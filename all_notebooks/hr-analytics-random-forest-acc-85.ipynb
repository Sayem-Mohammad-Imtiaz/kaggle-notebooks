{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nfrom sklearn.metrics import classification_report, accuracy_score\nfrom sklearn.metrics import classification_report, accuracy_score\nfrom sklearn.impute import KNNImputer\nimport pandas_profiling as pp\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import GridSearchCV\n\n\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-09T11:03:17.187267Z","iopub.execute_input":"2021-08-09T11:03:17.187645Z","iopub.status.idle":"2021-08-09T11:03:17.197546Z","shell.execute_reply.started":"2021-08-09T11:03:17.187615Z","shell.execute_reply":"2021-08-09T11:03:17.196475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# The Dataset\n","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/hr-analytics-job-change-of-data-scientists/aug_train.csv')\ndata.head(3)","metadata":{"execution":{"iopub.status.busy":"2021-08-09T11:03:17.320587Z","iopub.execute_input":"2021-08-09T11:03:17.32129Z","iopub.status.idle":"2021-08-09T11:03:17.383544Z","shell.execute_reply.started":"2021-08-09T11:03:17.321237Z","shell.execute_reply":"2021-08-09T11:03:17.382513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Description\nWith the help of these features we have to predict which data scientist is looking for a job change and thus, is a better prospect in terms of hiring.","metadata":{}},{"cell_type":"markdown","source":"* enrollee_id : Unique ID (Useless here)\n* city: City code\n* city_ development _index : Developement index of the city (scaled)\n* gender: Gender of candidate\n* relevent_experience: Relevant experience of candidate\n* enrolled_university: Type of University course enrolled if any\n* education_level: Education level of candidate\n* major_discipline :Education major discipline of candidate\n* experience: Total experience in years\n* company_size: No of employees in current company\n* company_type : Type of current company\n* lastnewjob: Difference in years between previous job and current job\n* training_hours: training hours completed\n\n    **target: 0 – Not looking for job change, 1 – Looking for a job change**","metadata":{}},{"cell_type":"markdown","source":"## Data Exploration\n\nFor EDA, I used this library that provides almost all the relevent information we need. So we don't have to manually look around the data ( Saves time:) )\n","metadata":{}},{"cell_type":"code","source":"pp.ProfileReport(data)","metadata":{"execution":{"iopub.status.busy":"2021-08-09T11:03:17.385063Z","iopub.execute_input":"2021-08-09T11:03:17.385622Z","iopub.status.idle":"2021-08-09T11:03:31.206662Z","shell.execute_reply.started":"2021-08-09T11:03:17.38558Z","shell.execute_reply":"2021-08-09T11:03:31.205844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing\n**Label encoding \"city\" feature**","metadata":{}},{"cell_type":"code","source":"le = LabelEncoder()\n\ndata['city'] = le.fit_transform(data['city'])\ndata['city'].head(5)","metadata":{"execution":{"iopub.status.busy":"2021-08-09T11:03:31.207986Z","iopub.execute_input":"2021-08-09T11:03:31.208299Z","iopub.status.idle":"2021-08-09T11:03:31.22128Z","shell.execute_reply.started":"2021-08-09T11:03:31.20827Z","shell.execute_reply":"2021-08-09T11:03:31.220488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Manual encoding ordinal features**\nSome of the categorical features in this dataset are ordinal, i.e,there is a clear ordering of the categories. So I have manually encoded these.","metadata":{}},{"cell_type":"code","source":"gender_map = {\n    'Female' : 2,\n    'Male' : 1,\n    'Other' : 0\n    }\n\nrelevent_experience_map = {\n    'Has relevent experience' : 1,\n    'No relevent experience' : 0\n    }\n\nenrolled_university_map = {\n    'no_enrollment' : 0,\n    'Part time course' : 1,\n    'Full time course' : 2\n    }\n\neducation_level_map = {\n    'Primary School' :    0,\n    'Graduate'       :    2,\n    'Masters'        :    3, \n    'High School'    :    1, \n    'Phd'            :    4\n    } \n\nmajor_map = {\n    'STEM' : 0,\n    'Business Degree' : 1,\n    'Humanities' : 2,\n    'Arts' : 3,\n    'Other' : 4,\n    'No Major' : 5\n    }\n\nexperience_map = {\n    '<1' : 0,\n    '1' : 1,\n    '2' : 2,\n    '3' : 3,\n    '4' : 4,\n    '5' : 5,\n    '6' : 6,\n    '7' : 7,\n    '8' : 8,\n    '9' : 9,\n    '10' : 10,\n    '11' : 11,\n    '12' : 12,\n    '13' : 13,\n    '14' : 14,\n    '15' : 15,\n    '16' : 16,\n    '17' : 17,\n    '18' : 18,\n    '19' : 19,\n    '20' : 20,\n    '>20' : 21\n    }\n\nsize_map = {\n    '<10' : 0,\n    '10/49' : 1,\n    '50-99' : 2,\n    '100-500' :3,\n    '500-999' :4,\n    '1000-4999': 5,\n    '5000-9999' : 6,\n    '10000+' : 7\n    }\n\ncompany_type_map = {\n    'Pvt Ltd'               :    0,\n    'Funded Startup'        :    1, \n    'Early Stage Startup'   :    2, \n    'Other'                 :    3, \n    'Public Sector'         :    4, \n    'NGO'                   :    5\n}\n\nlast_new_job_map = {\n    'never'        :    0,\n    '1'            :    1, \n    '2'            :    2, \n    '3'            :    3, \n    '4'            :    4, \n    '>4'           :    5\n}\n\ndata.loc[:,'education_level'] = data['education_level'].map(education_level_map)\ndata.loc[:,'company_size'] = data['company_size'].map(size_map)\ndata.loc[:,'company_type'] = data['company_type'].map(company_type_map)\ndata.loc[:,'last_new_job'] = data['last_new_job'].map(last_new_job_map)\ndata.loc[:,'major_discipline'] = data['major_discipline'].map(major_map)\ndata.loc[:,'enrolled_university'] = data['enrolled_university'].map(enrolled_university_map)\ndata.loc[:,'relevent_experience'] = data['relevent_experience'].map(relevent_experience_map)\ndata.loc[:,'gender'] = data['gender'].map(gender_map)\ndata.loc[:,'experience'] = data['experience'].map(experience_map)","metadata":{"execution":{"iopub.status.busy":"2021-08-09T11:03:31.222948Z","iopub.execute_input":"2021-08-09T11:03:31.223382Z","iopub.status.idle":"2021-08-09T11:03:31.267785Z","shell.execute_reply.started":"2021-08-09T11:03:31.22335Z","shell.execute_reply":"2021-08-09T11:03:31.266574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Handling missing data with knn\nAs we saw in the EDA report, there is a lot of missing data which we can deal with the help of KNN Imputer","metadata":{}},{"cell_type":"code","source":"knn_imputer = KNNImputer()\n#making a copy just in case\ncopy = data.copy()\n\ncopy = knn_imputer.fit_transform(copy)\n#rounding the knn values\ncopy[:, 3:] = np.round(copy[:, 3:])\ndata = pd.DataFrame(copy, columns = data.columns)","metadata":{"execution":{"iopub.status.busy":"2021-08-09T11:03:31.269072Z","iopub.execute_input":"2021-08-09T11:03:31.269371Z","iopub.status.idle":"2021-08-09T11:03:49.566683Z","shell.execute_reply.started":"2021-08-09T11:03:31.269344Z","shell.execute_reply":"2021-08-09T11:03:49.56575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# One Hot Encoding \nNow, the categorical features which are not ordinal will be one hot encoded","metadata":{}},{"cell_type":"markdown","source":" **Dividing numeric and categorical data**","metadata":{}},{"cell_type":"code","source":"numeric = data[[\"city_development_index\", \"training_hours\", \"target\"]].copy()\ncategory = data[[\"city\", \"gender\", \"relevent_experience\", \"enrolled_university\", \"education_level\", \"major_discipline\", \"experience\", \"company_size\", \"company_type\", \"last_new_job\"]].copy()\n\n#using the previously manual encoded columns\ncategory_ordinalencoded = category[['education_level', 'experience', 'company_size', 'last_new_job']]\n","metadata":{"execution":{"iopub.status.busy":"2021-08-09T11:03:49.567792Z","iopub.execute_input":"2021-08-09T11:03:49.568041Z","iopub.status.idle":"2021-08-09T11:03:49.576688Z","shell.execute_reply.started":"2021-08-09T11:03:49.568016Z","shell.execute_reply":"2021-08-09T11:03:49.5756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" **One Hot encoding the rest categorical columns**","metadata":{}},{"cell_type":"code","source":"#columns that need to be one hot encoded\none_how_columns = [ col for col in category.columns if col not in ['education_level', 'experience', 'company_size', 'last_new_job']]\n\n#onehotencoder\nohe = OneHotEncoder(sparse=False).fit(category.loc[:, one_how_columns])\ncategory_onehotEncoded = ohe.transform(category.loc[:, one_how_columns])\n\n#joining all the category columns\ncategory_preprocessed = np.concatenate([category_onehotEncoded, category_ordinalencoded], axis=1)\n\n#joining all the features\nX = np.concatenate([numeric.drop('target', axis=1).values, category_preprocessed], axis=1)\ny = numeric['target'].values","metadata":{"execution":{"iopub.status.busy":"2021-08-09T11:04:20.953688Z","iopub.execute_input":"2021-08-09T11:04:20.954034Z","iopub.status.idle":"2021-08-09T11:04:21.003252Z","shell.execute_reply.started":"2021-08-09T11:04:20.954004Z","shell.execute_reply":"2021-08-09T11:04:21.002171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Imbalanced data\nAs we can see clearly, the data is very imbalanced which needs to be dealt with if we want our model to train properly.\nFor this, I used SMOTE(Synthetic Minority Oversampling Technique) to make the data balanced. \n> If you want to know more about SMOTE, check this [link](https://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/)","metadata":{}},{"cell_type":"code","source":"X, y = SMOTE(random_state = 99).fit_resample(X, y)","metadata":{"execution":{"iopub.status.busy":"2021-08-09T11:04:26.3318Z","iopub.execute_input":"2021-08-09T11:04:26.332152Z","iopub.status.idle":"2021-08-09T11:04:26.90865Z","shell.execute_reply.started":"2021-08-09T11:04:26.332122Z","shell.execute_reply":"2021-08-09T11:04:26.907646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train-test split","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state = 99)","metadata":{"execution":{"iopub.status.busy":"2021-08-09T11:04:33.032317Z","iopub.execute_input":"2021-08-09T11:04:33.032648Z","iopub.status.idle":"2021-08-09T11:04:33.066025Z","shell.execute_reply.started":"2021-08-09T11:04:33.032619Z","shell.execute_reply":"2021-08-09T11:04:33.065095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Comparing models\n### Decision tree","metadata":{}},{"cell_type":"code","source":"dtc = DecisionTreeClassifier(criterion='entropy',random_state = 99)\ndtc.fit(X_train, y_train)\n\ny_dtc = dtc.predict(X_test)\naccuracy_score(y_test, y_dtc)","metadata":{"execution":{"iopub.status.busy":"2021-08-09T11:04:37.45546Z","iopub.execute_input":"2021-08-09T11:04:37.455828Z","iopub.status.idle":"2021-08-09T11:04:37.834339Z","shell.execute_reply.started":"2021-08-09T11:04:37.455798Z","shell.execute_reply":"2021-08-09T11:04:37.833345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Decision tree gives us a **79.2%** accuracy\n### SVM","metadata":{}},{"cell_type":"code","source":"#svm needs scaled data\nfrom sklearn.preprocessing import StandardScaler\nsc_X = StandardScaler()\ntrain_copy = X_train.copy()\ntest_copy = X_test.copy()\n\ntrain_copy = sc_X.fit_transform(train_copy)\ntest_copy = sc_X.transform(test_copy)\nsvm = SVC(kernel = 'rbf', random_state=0)\nsvm.fit(train_copy, y_train)\n\n#predicting test set\ny_svm = svm.predict(test_copy)\naccuracy_score(y_test, y_svm)","metadata":{"execution":{"iopub.status.busy":"2021-08-09T11:05:13.027823Z","iopub.execute_input":"2021-08-09T11:05:13.028314Z","iopub.status.idle":"2021-08-09T11:07:37.019202Z","shell.execute_reply.started":"2021-08-09T11:05:13.028269Z","shell.execute_reply":"2021-08-09T11:07:37.018461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"SVM gives us a **79.1%** accuracy\n### Random Forest","metadata":{}},{"cell_type":"code","source":"rf1 = RandomForestClassifier(random_state = 0)\nrf1.fit(X_train, y_train)\n\n#predicting test set\ny_rf1 = rf1.predict(X_test)\naccuracy_score(y_test, y_rf1)","metadata":{"execution":{"iopub.status.busy":"2021-08-09T11:09:31.375211Z","iopub.execute_input":"2021-08-09T11:09:31.375546Z","iopub.status.idle":"2021-08-09T11:09:35.723935Z","shell.execute_reply.started":"2021-08-09T11:09:31.375518Z","shell.execute_reply":"2021-08-09T11:09:35.722838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Random forest performs the best among these with an accuracy of **84.2%**\n\nSo, we'll finetune the random forest model to increase the performance.","metadata":{}},{"cell_type":"markdown","source":"# Tuning the Random Forest Model","metadata":{}},{"cell_type":"markdown","source":"## Base Model","metadata":{}},{"cell_type":"code","source":"rf = RandomForestClassifier(random_state = 99)\nrf.fit(X_train, y_train)\n\ny_base = rf.predict(X_test)\n\ncm_base =  confusion_matrix(y_test, y_base)\ncm_base","metadata":{"execution":{"iopub.status.busy":"2021-08-09T11:03:49.613743Z","iopub.status.idle":"2021-08-09T11:03:49.614301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Using RandomSearch to find the optimal parameters\n\nRandomizedSearchCV randomly goes through the combination of parameters and gives the best one found. It does not give the absolute best parameters but its usually pretty close and helps in reducing the iteratons in Gridsearch","metadata":{}},{"cell_type":"code","source":"# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\n\n# Use the random grid to search for best hyperparameters\n# Random search of parameters, using 3 fold cross validation, \n# search across 100 different combinations, and use all available cores\nrf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n# Fit the random search model\nrf_random.fit(X_train, y_train)\n\nrf_random.best_params_","metadata":{"execution":{"iopub.status.busy":"2021-08-09T11:03:49.615269Z","iopub.status.idle":"2021-08-09T11:03:49.615705Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Comparing the Random search model","metadata":{}},{"cell_type":"code","source":"best_random = rf_random.best_estimator_\ny_rand = best_random.predict(X_test)\n\n#confusion matrix\ncm_rand = confusion_matrix(y_test, y_rand)\ncm_rand","metadata":{"execution":{"iopub.status.busy":"2021-08-09T11:03:49.616779Z","iopub.status.idle":"2021-08-09T11:03:49.617172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Grid Search","metadata":{}},{"cell_type":"code","source":"# Create the parameter grid based on the results of random search \nparam_grid = {\n    'bootstrap': [False],\n    'max_depth': [None, 20, 50, 70, 100],\n    'max_features': ['auto', 'sqrt'],\n    'min_samples_leaf': [2],\n    'min_samples_split': [5],\n    'n_estimators': [400, 600, 800, 1200, 1400, 1600]\n}\n# Create a base model\nrfc = RandomForestClassifier()\n# Instantiate the grid search model\ngrid_search = GridSearchCV(estimator = rfc, param_grid = param_grid, \n                          cv = 3, n_jobs = -1, verbose = 2)\n\ngrid_search.fit(X_train, y_train)\ngrid_search.best_params_\n\ny_grid = grid_search.predict(X_test)\ncm_grid = confusion_matrix(y_test, y_grid)\ncm_grid\n","metadata":{"execution":{"iopub.status.busy":"2021-08-09T11:03:49.618006Z","iopub.status.idle":"2021-08-09T11:03:49.618396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**NOTE: I used random search twice and created the param_grid based on those two results (Yes, I got two different best_params_!)**","metadata":{}},{"cell_type":"markdown","source":"## Accuracy","metadata":{}},{"cell_type":"code","source":"print(classification_report(y_test, y_grid))\naccuracy_score(y_test, y_grid)","metadata":{"execution":{"iopub.status.busy":"2021-08-09T11:03:49.619341Z","iopub.status.idle":"2021-08-09T11:03:49.619751Z"},"trusted":true},"execution_count":null,"outputs":[]}]}