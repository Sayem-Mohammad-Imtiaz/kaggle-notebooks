{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Chapter 0: A brief introduction\n\nThis is my first public notebook. For now, the most interesting topic to me is NLP. So I decide to setup this notebook and update it regularly to share with friends what I have learnt and what I have experienced along the way.\n\nNLP means Natural Language Process. This process is to deal with text or speech so that we can gain insight of massive texts and speeches. After we preprocess text/speech data into useful representations, we can use computer to help us, in a highly efficient way, to dig out a lot from the massive texts/speeches, so that NLP can be used in:\n* Machine Translation\n* Speech Recognition\n* Sentiment Analysis\n* Question Answering System\n* Text Automatic Summariztion\n* Chatbots\n* Text Classification\n* Character Recognition\n* Spell Checking\n\n\nActually, since NLP is so important and attractive, there are more than one way to handle the process. To my knowledge, TensoFlow can do this job, spaCy is also very hot and spicy on this topic, [NLTK](https://www.nltk.org/) is another powerful tool. Hopefully, in the future, I can pick those treasures up one by one and collect them here, so my friends can find a good starting point in this notebook. \n\nOne thing I have to admit here is that I am not a smart man. I am not a genius, but an ordinary person with genuine interesting in data science. People always say there are two species can land on the top of a pyramid, the eagle and the snail.  For those \"eagles\", they can understand snipet of code in a blink of eye, to me, in my \"snail\" way, I have to type them word by word, test, debug, more test more debug, until I really understand them. In the same manner, I will build up this notebook little by little with detailed explaination of code and complete links to the references or other useful resources. I hope my friends can find enjoyment in my community. Let us work together!"},{"metadata":{},"cell_type":"markdown","source":"## 0.1 Some General Ideas on Machine Learning Script\n\nAfter reading some of the examples in Kaggle, you will realize 99% percent of them, no matter what topic they are talking about, no matter what kind of specific tools they are using, the steps of the scripts are similar. It is useful to summarize the steps at the very beginning, so that everyone will have a simplified map at hand to keep us at the right track.\n\n1. Loading in the data\n\n> Typical machine learning repository can be found from [UCI](https://archive.ics.uci.edu/ml/index.php). For a data scientist, the firt thing need to care about is the attribute information of the data. Those attributs can actually divided into the *factors* part and the *labels* part. For example, if you want to study the risk of diabetes, you man take into account the factors like age, sex, sudden weight loss, visual blurring, itching, etc. Then those factors will be associated to a certain label -- whether this guy has diabetes or not. \n\n> When data is not complex, we use Pandas to handl it.\n\n2. Splitting the whole dataset into training/testing sets\n3. Building a model\n4. Fitting the model \nThis is also called the optimisation process, the model training part.\n5. Evaluating the model\n6. Making prediction based on the model\n\nIt seems I have forgotten to mention **data visualization**.  No, I don't. The only reason is, data visualiztion, as a powerful tool, can be used in almost every step mentioned above. I will learn how to use it along the way. \n\nOne thing we need to keep in mind is the six steps listed above is only a simplifed road map, or a sketch of the \"pipeline\", we will meet more details later on. For example, when the data is loaded, we need a **preprocess** procedure to get rid of the noise, to figure out if the data is complete or not, to scale the data, to do the dimensionality reduction, etc. \n"},{"metadata":{},"cell_type":"markdown","source":"# Chapter 1: NLP with TensorFlow\nNLP can be realized on TensorFlow, I put them together in the following notebook.\nFor now, most of the contents are from [here](https://medium.com/@saitejaponugoti/nlp-natural-language-processing-with-tensorflow-b2751aa8c460). In the future, I will add more projects so that everyone can get what they want from the \"buffet table\"."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(tf.__version__)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Complete [official documentation of TensorFlow](https://www.tensorflow.org/api_docs)"},{"metadata":{},"cell_type":"markdown","source":"## 1.1 Tokenization\n<face size=\"5\">Since computer (tensorflow) cannot calculate character or words directly, we must figure out a process or some algorithms which can transform those uncalulatable information into numbers. This process is called **tokenization**.</face>"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# List of sample sentences that we want to tokenize\nsentences = ['I love my dog',\n             'I love my cat',\n            ]\n\n# intializing a tokenizer that can index\n# num_words is the maximum words that can be kept\n# tokenizer will automatically help in choosing most frequent words\ntokenizer = Tokenizer(num_words = 100)\n\n# fitting the sentences to using created tokenizer object\ntokenizer.fit_on_texts(sentences)\n\n# the full list of words is available as the tokenizer's word index\nword_index = tokenizer.word_index\n\n# the result will be a dictionary, key being the words and the value being the token for that word\nprint(word_index)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<face size = \"5\">The following code is to show the *tokenizer* is smart enough that it will NOT add a new token for \"dog!\".</face>"},{"metadata":{"trusted":true},"cell_type":"code","source":"sentences = ['I love my dog',\n             'I love my cat',\n             'you love my dog!'\n            ]\n\ntokenizer = Tokenizer(num_words = 100)\ntokenizer.fit_on_texts(sentences)\nword_index = tokenizer.word_index\n\n# Newly generated dictionary doesn't contain new token for \"dog!\"\nprint(word_index)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1.2 Sequencing:\nsequence the numbers (tokens) in the correct order."},{"metadata":{"trusted":true},"cell_type":"code","source":"sentences = ['I love my dog',\n             'I love my cat',\n             'you love my dog!',\n             'Do you think my dog is amazing?'\n            ]\n\ntokenizer = Tokenizer(num_words = 100)\ntokenizer.fit_on_texts(sentences)\nword_index = tokenizer.word_index\n\n# this creates sequence of tokens representing each sentence\nsequences = tokenizer.texts_to_sequences(sentences)\n\nprint(word_index)\nprint(sequences)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, we got the tensors which can be handled by tensorflow tools.\n\nBut before we move on, let us have a test first."},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data = [ \n    'i really love my dog',\n    'my dog loves my manatee'\n]\n\ntest_seq = tokenizer.texts_to_sequences(test_data)\n\nprint(test_seq)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the new test_data, the new words like \"really\" and \"manatee\" which are not tokenized cannot be seen in the sequences. Therefore, in other words, we are losing the lenght of the sequence. In order to fix this, we employ a trick named **OOV (out of vocabulary) token**. Actually, OOV is a property set to those words never been used anywhere, like \"<OOV>\""},{"metadata":{"trusted":true},"cell_type":"code","source":"sentences = ['I love my dog',\n             'I love my cat',\n             'you love my dog!',\n             'Do you think my dog is amazing?'\n            ]\n\n# adding a \"out of vocabulary\" word to the tokenizer\ntokenizer = Tokenizer(num_words = 100, oov_token = \"<OOV>\")\ntokenizer.fit_on_texts(sentences)\nword_index = tokenizer.word_index\nsequences = tokenizer.texts_to_sequences(sentences)\n\ntest_seq = ['i really love my dog',\n            'my dog loves my manatee'\n           ]\n\ntest_seq = tokenizer.texts_to_sequences(test_data)\n\nprint(word_index)\nprint(test_seq)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the output from the code above, we can see new words like \"really\" and \"manatee\" are all replaced by '<OOV>' and then tokenized as \"1\". The result can be explained like:\n> 'i really love my dog' = [5, 1, 3, 2, 4] = 'i \\< OOV\\> love my dog'"},{"metadata":{},"cell_type":"markdown","source":"## 1.3 Padding the sequences:\nIn sentences, different items contain different number of words, which means the sequences may be of different length. However, tensorflow cannot handle input vector of different length, so we need padding."},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.preprocessing.sequence import pad_sequences\n\nsentences = ['I love my dog',\n             'I love my cat',\n             'you love my dog!',\n             'Do you think my dog is amazing?'\n            ]\n\ntokenizer = Tokenizer(num_words = 100, oov_token = \"<OOV>\")\ntokenizer.fit_on_texts(sentences)\nword_index = tokenizer.word_index\nsequences = tokenizer.texts_to_sequences(sentences)\n\n# padding sequences\npadded = pad_sequences(sequences)\n\nprint(word_index)\nprint(sequences)\nprint(padded)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As a user, we can have some options over the paddings. If we want place the padding  at the end of the sequence, we can use `padding = \"post\"`. With the options decided, the function pad_Sequences might look like:\n\n`padded = pad_sequences(sequences, maxlen = 5, padding = 'post', truncating = 'post')`\n\nDetailed information can be found in [pad_sequences](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences) documentation."},{"metadata":{},"cell_type":"markdown","source":"Since the text has been tokenized now, next step for us is to use tensorflow. Let us start from the [dataset of News headlines](https://www.kaggle.com/rmisra/news-headlines-dataset-for-sarcasm-detection).\n\nThis dataset has 3 fields:\n> ___**is_sarcastic field**: \"1\" if sarcastic and 0 otherwise.___ <br>\n> ___**headline**: the headline if the news article___ <br>\n> ___**article_link**: link to the original news article.___"},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"# importing the json library\nimport json\nimport os\n\n# loading sarcasm.json file using the json library\n\n#with open(\"../input/news-headlines-dataset-for-sarcasm-detection/Sarcasm_Headlines_Dataset.json\", 'r') as f:\n#    data = json.load(f)\n\nf = open(\"../input/news-headlines-dataset-for-sarcasm-detection/Sarcasm_Headlines_Dataset.json\", 'r')   \n\ndata = []\nfor line in f.readlines():\n    dic = json.loads(line)\n    data.append(dic)\n#print(data)    \n# creating lists for sentences, labels and urls\nsentences = [] # headlines\nlabels = [] # labels\nurls = []\n\n# iterating through the json data and loading\n# the requisite values into our python lists\nfor item in data:\n    sentences.append(item['headline'])\n    labels.append(item['is_sarcastic'])\n    urls.append(item['article_link'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1.4 Splitting data into train and test sets:"},{"metadata":{"trusted":true},"cell_type":"code","source":"training_size = 20000\n\ntraining_sentences = sentences[0:training_size]\ntesting_sentences = sentences[training_size:]\n\ntraining_labels = labels[0:training_size]\ntesting_labels = labels[training_size:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since we have tokenized every word in the set, the test set has also been tokenized, which is not what we want. The reason is we want the words in the test set have not been tokenized so that we can use them to test the training set.\n\nThe following code is used to fix the problem:"},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab_size = 10000\nmax_length = 100\ntrunc_type = 'post'\npadding_type = 'post'\noov_tok = \"<OOV>\"\ntraining_size = 20000\n\ntokenizer = Tokenizer(num_words = vocab_size, oov_token = oov_tok)\n# fitting tokenizer only to training set\ntokenizer.fit_on_texts(training_sentences)\n\nword_index = tokenizer.word_index\n\n# creating training sequences and padding them \ntraining_sequences = tokenizer.texts_to_sequences(training_sentences)\ntraining_padded = pad_sequences(training_sequences, maxlen = max_length,\n                               padding = padding_type,\n                               truncating = trunc_type,\n                               )\n\n# creating testing sequences and padding them using same tokenizer\ntesting_sequences = tokenizer.texts_to_sequences(testing_sentences)\ntesting_padded = pad_sequences(testing_sequences, maxlen = max_length,\n                              padding = padding_type,\n                              truncating = trunc_type,\n                              )\n\n# converting all variables to numpy arrays, to be able to work with tf v2\ntraining_padded = np.array(training_padded)\ntraining_labels = np.array(training_labels)\ntesting_padded = np.array(testing_padded)\ntesting_labels = np.array(testing_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.rcParams['figure.figsize'] = [15, 15]  # make the plot bigger sothe subplot don't overlap\ntraining_padded.shape\n#training_padded.hist(); # use a semicolon to supress return value","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1.5 Word Embeddings:\n\nIn this step, basic sentiments will be taken into account, since the tokenized words--the numbers--cannot tell us preference, attitude or things like that. \n\nWell, here is where the context of **embeddings** comes in."},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_dim = 16\n\n# creating a model for sentiment analysis\nmodel = tf.keras.Sequential([\n    # addinging an Embedding layer for Neural Network to learn the vectors\n    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length = max_length),\n    # Global average pooling is similar to adding up vectors in this case\n    tf.keras.layers.GlobalAveragePooling1D(),\n    tf.keras.layers.Dense(24, activation = 'relu'),\n    tf.keras.layers.Dense(1, activation = 'sigmoid')\n])\n\nmodel.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n\nnum_epochs = 10\n\nhistory = model.fit(training_padded, training_labels, epochs = num_epochs,\n                   validation_data = (testing_padded, testing_labels))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1.6 Establishing Sentiment:\nThis section is to used the trained neural network to do the prediction."},{"metadata":{"trusted":true},"cell_type":"code","source":"# forming new sentences for testing, feel free to experiment\n# sentence 1 is bit sarcastic, whereas sentence two is a general statement.\n\nnew_sentence = [\n    \"granny starting to fear spider in the garden might be real\",\n    \"game of thrones season finale showing this sunday night\"\n]\n\n# Converting the sentences to sequences using tokenizer\nnew_sequences = tokenizer.texts_to_sequences(new_sentence)\n# padding the new sequences to make them have same dimensions\nnew_padded = pad_sequences(new_sequences, maxlen = max_length, \n                          padding = padding_type,\n                          truncating = trunc_type)\n\nnew_padded = np.array(new_padded)\n\nprint(model.predict(new_padded))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Chapter 2: NLP with spaCy\nThe following is from the [tutorial](https://www.kaggle.com/matleonard/intro-to-nlp)\n\nDetailed spaCy API documentation is [here](https://spacy.io/api/language) We may need to refer to this document from time to time to explain the following examples. "},{"metadata":{},"cell_type":"markdown","source":"![From Analytics Vidhya](https://cdn.analyticsvidhya.com/wp-content/uploads/2020/03/spacy_pipeline.png)\n\n<center>Pipeline of spaCy   </center>"},{"metadata":{},"cell_type":"markdown","source":"spaCy relies on **models** that are language-specific and come in different sizes. You can load a spaCy model with `spacy.load`.\n\nFor example, here's how you would load the English language model."},{"metadata":{"trusted":true},"cell_type":"code","source":"import spacy\nnlp = spacy.load('en')\n\nnlp.pipe_names","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"`\nnlp.pipe_names\n` shows the active pipeline componets. If we want to disable some of the components, we can use something like: `nlp.diable_pipes('tagger', 'parser')`."},{"metadata":{},"cell_type":"markdown","source":"## 2.1 Tokenize\n\nDifferent from the tokenizing process happened in the previous section about tensorflow. Token here is still a word, not a number we get after tokenization from the tensorflow. For spaCy, tokenization is a process of purifying raw text into simpler individual words which will make more sense in the future computation.\n\nAs you will see from the following example, in spaCy, punctuation is token, and contractions like \"don't\" will be splitted into \"do\" and \"n't\" and counted as two tokens. \n\nAlso in the following example, we displayed POS (Part-of-Speech) tagging. In English, the parts of speech tell us what is the function of a word and how it is used in a sentence. They may be Noun, Pronoun, Adjective, Verb, Adverb, etc. Punctuation is labelled as \"PUNCT\". "},{"metadata":{"trusted":true},"cell_type":"code","source":"doc = nlp(\"Tea is healthy and calming, don't you think?\")\n\nfor token in doc:\n    print(token)\nprint(\"~\"*40)    \nfor token in doc:\n    # print the token and its part-of-speech tag\n    print(token.text, \"-->\", token.pos_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"spacy.explain(\"AUX\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Almost all the POS tags shown in the above result are in the short form. If you want to what it stands for, you can use code like `spacy.explain(\"AUX\")` to figure it out."},{"metadata":{},"cell_type":"markdown","source":"## 2.1.1 Play with 'ner'\n\nAs you have seen from the previous sections, after we load the spacy and show the pipe names by using `nlp.pipe_name`, we can see one of the results is 'ner'. Then, what is 'ner'? It is short for **N**ame **E**ntity **R**ecognition. As long as there are name entities in the given text, it can be referred to by using `.ents`. See the following example:"},{"metadata":{"trusted":true},"cell_type":"code","source":"doc = nlp(\"D. Trump will win the 2020 U.S.A. election! I predicted it on 11/27th.\")\nprint(doc.ents)\nprint(f\"Entity \\t\\tType \\t\\tStart \\t\\t End\".format('Entity', 'Type', 'Start', 'End'))\nfor ent in doc.ents:\n    print(f\"{ent.text}\\t\\t{ent.label_}\\t\\t{ent.start_char}\\t\\t{ent.end_char}\")\n    #print(ent.text, ent.label_, ent.start_char, ent.end_char)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the result above, we can see not all the words or phrases are labelled as NAME ENTITY. Type is presented in the short form, we can find their meanings from [here](https://spacy.io/api/annotation). You can also use `spacy.explain('GPE')` to figure it out.\n\nA fun fact: if you use \"Trump\", this word will be labelled as \"ORG\", and if you use \"D. Trump\", you will get \"PERSON\".  "},{"metadata":{"trusted":true},"cell_type":"code","source":"#from spacy import displacy\ndisplacy.render(doc, style='ent')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.2 Text preprocessing\n\nWith a spaCy token, `token.lemma_` returns the lemma, while `token.is_stop` returns a boolean `True` if the token is a stopword (and `False` otherwise).\n\nLemma is the root of a family of words. All the other members of the family are derived from this root word (lemma). For example, \"walking\" and \"walks\" are all from \"walk\". After using `token.lemma_`, this word family will be presented by \"walk\" only. \n\nStopwords are the words like \"is\", \"the\", \"and\", \"but\". They glue the meaningful words together to form a complete sentence. However, they don't contain much information, so we need to get rid of them in the text preprocessing phase."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Token \\t\\tLemma \\t\\tStopword\".format('Token', 'Lemma', 'Stopword'))\nprint(\"-\"*40)\nfor token in doc:\n    print(f\"{str(token)}\\t\\t{token.lemma_}\\t\\t{token.is_stop}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.3 Pattern Matching\n\nTo mathch individual tokens, you need to create a `Matcher`. When you want to match a list of terms, it is easier and more efficient to use `PhraseMatcher`."},{"metadata":{"trusted":true},"cell_type":"code","source":"from spacy.matcher import PhraseMatcher\nmatcher = PhraseMatcher(nlp.vocab, attr = 'LOWER')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The matcher is created using the vocabulary of your model. Here we're using the small English model you loaded earlier. Setting `attr='LOWER'` will match the phrases on lowercased text. This provides case insensitive matching."},{"metadata":{"trusted":true},"cell_type":"code","source":"terms = ['Galaxy Note', 'iPhone 11', 'iPhone XS', 'Google Pixel']\npatterns = [nlp(text) for text in terms]\nmatcher.add(\"TerminologyList\", patterns)\nprint(patterns)\n#x = nlp('Galaxy Note')\n#for token in x:\n#    print(token)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Create a document from the text to search and use the phrase matcher to find where the terms occur in the text."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Borrowed from https://daringfireball.net/linked/2019/09/21/patel-11-pro\ntext_doc = nlp(\"Glowing review overall, and some really interesting side-by-side \"\n              \"photography tests pitting the iPhone 11 Pro against the \"\n              \"Galaxy Note 10 Plus and last year's iPhone XS and Google Pixel 3.\")\nmatches = matcher(text_doc)\nprint(matches)\nn = 0\nprint(f\"token \\t\\t\\tIndex\".format(token, 'index'))\nfor token in text_doc:\n    print(f\"{str(token)}\\t\\t\\t{n}\")\n    #print(f\"token \\t\\tn\".format(token, n))\n    n = n+1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"`print(matches)` prints out matches. We can see from the result, they are tuples. For example `(3766102292120407359, 17, 19)` is the first one. The big number `3766102292120407359` is the ID, `17` is the starting point while `19` is the ending point. \n* One thing need to be emphasize here is index of toeknized `text_doc` starts from zero not 1. \n* Another thing we need to know is if we want to print out matched word, the ending is 18 not 19. \n* The space behind the word \"side-by-side\" is important. If you neglect it, you will have a token like sidephotography, which is not what we want.\n\nThis can be seen from the following code."},{"metadata":{"trusted":true},"cell_type":"code","source":"match_id, start, end = matches[0]\nprint(nlp.vocab.strings[match_id], text_doc[start:end])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.4 Text Classification with SpaCy\n\nText classification can be used to the area like spam detection, sentiment analysis, and tagging customer queries"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\n\n# Loading the spam data\n# ham is the label for non-spam messages\nspam = pd.read_csv('../input/nlp-course/spam.csv')\nspam.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.5 Bag of Words\nMachine learning models don't learn from raw text data. Instead, you need to convert the text to something numeric. In the previous section about TensorFlow, we get those numeric information via a so-called 'tokenize' process. However, here in the case of spaCy, this text-to-number process is done by a variation of one-hot encoding.\n\n"},{"metadata":{},"cell_type":"markdown","source":"## Building a Bag of Words model\n\nspaCy handles the bag of words conversion and building a simple linear model for you with the `TextCategorizer` class.\n\nThe TextCategorizer is a spaCy **pipe**. Pipes are classes for processing and transforming tokens. When you create a spaCy model with `nlp = spacy.load('en_core_web_sm')`, there are default pipes tht perform part of speech tagging, entity recognition, and other transformations. When you run text through a model `doc = nlp(\"some text here\")`, the output of the pipe are attached to the tokens in the `doc` object. The lemmas for `token.lemma_`come from one of these pipes."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create the TextCategorizer with exclusive classes and \"bow\" architecture\ntextcat = nlp.create_pipe(\n    \"textcat\",        # name of the TextCategorizer\n    config = {        # configuration information written in a dictionary\n        \"exclusive_classes\": True,  # which means it is a binary classification, here it is either ham or spam\n        \"arhitecture\": \"bow\"    # bow is the acronym of bag of words. \n    }\n)\n\n# Add the TextCategorizer to the empty model\n#nlp.pipe_names\nnlp.add_pipe(textcat)\nnlp.pipe_names","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"`['tagger', 'parser', 'ner', 'textcat']`\n\nNER: Named Entity Recognition"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Add labels to text classifier\ntextcat.add_label(\"ham\")\ntextcat.add_label(\"spam\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training a Text Categorizer Model\n\nThe `train_labels` set below is a dictionary contains `{'ham': True, 'spam': False}`. This dictionay is nested in another dictionary with the key `'cats'`."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_texts = spam[\"text\"].values\ntrain_labels = [{'cats': {'ham': label == 'ham',\n                         'spam': label == 'spam'}}\n               for label in spam['label']]\nprint(train_labels[0:4]) # Print out the first four items in the dictionary to take a look.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Then we combine the texts and labels into a single list using `zip()` function."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = list(zip(train_texts, train_labels))\ntrain_data[:3]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now you are ready to train the model. First create an `optimizer` using `nlp.begin_training()`. This optimizer is used to update the model. `minibatch` returns a generator yielding minibatches for training.Then, the minibatches are split into texts and labels, then used with `nlp.update` to update the model's parameters.\n\nA detailed description of nlp.update method is here: https://spacy.io/api/language#update"},{"metadata":{"trusted":true},"cell_type":"code","source":"from spacy.util import minibatch\n\nspacy.util.fix_random_seed(1)\noptimizer = nlp.begin_training()\n\n# Create the batch generator with batch size = 8\nbatches = minibatch(train_data, size = 8)\n# Iterate through minibatches\nfor batch in batches:\n    # Each batch is a list of (text, label) but we need to\n    # send separate lists for texts and labels to update().\n    # This is a quick way to split a list of tuples into lists\n    texts, labels = zip(*batch)\n    nlp.update(texts, labels, sgd = optimizer)  \n    # ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import random\n\nrandom.seed(1)\nspacy.util.fix_random_seed(1)\noptimizer = nlp.begin_training()\n\nlosses = {}\nfor epoch in range(10):\n    random.shuffle(train_data)\n    # Create the batch generator with batch size = 8\n    batches = minibatch(train_data, size = 8)\n    # Iterate through minibatches\n    for batch in batches:\n        # Each batch is a list of (text, label) but we need to \n        # send separate lists for texts and labels to update().\n        # This is a quick way to split a list of tuples into lists\n        texts, labels = zip(*batch)\n        nlp.update(texts, labels, sgd = optimizer, losses = losses)\n    print(losses)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"One of the sample results from the code above:\n\n`{'ner': 0.0, 'parser': 0.0, 'textcat': 0.2844166667900331, 'tagger': 431.55187639655196}\n{'ner': 0.0, 'parser': 0.0, 'textcat': 0.42443664483654675, 'tagger': 804.3928032657932}\n{'ner': 0.0, 'parser': 0.0, 'textcat': 0.49458903677683475, 'tagger': 1079.4695060554511}\n{'ner': 0.0, 'parser': 0.0, 'textcat': 0.5406185587745101, 'tagger': 1329.9959928379762}\n{'ner': 0.0, 'parser': 0.0, 'textcat': 0.5782408489383685, 'tagger': 1538.3814673529532}\n{'ner': 0.0, 'parser': 0.0, 'textcat': 0.611205267520201, 'tagger': 1704.0516921713795}\n{'ner': 0.0, 'parser': 0.0, 'textcat': 0.6690017411580279, 'tagger': 1875.847960154827}\n{'ner': 0.0, 'parser': 0.0, 'textcat': 0.6983035982691773, 'tagger': 2054.010863930191}\n{'ner': 0.0, 'parser': 0.0, 'textcat': 0.7010045384345747, 'tagger': 2187.8098707783442}\n{'ner': 0.0, 'parser': 0.0, 'textcat': 0.7010062075913526, 'tagger': 2290.876525081628}`"},{"metadata":{},"cell_type":"markdown","source":"## Making Predictions\nMake predictions with `predict()` method. The input text needs to be tokenized with `nlp.tokenizer`."},{"metadata":{"trusted":true},"cell_type":"code","source":"texts = [\"Are you ready for the tea party????? It's gonnabe wild\",\n        \"URGENT Reply to this message for GURANTEED FREE TEA\"]\ndocs = [nlp.tokenizer(text) for text in texts]\n\n# Use textcat to get the scores for each doc\ntextcat = nlp.get_pipe('textcat')\nscores, _ = textcat.predict(docs)\n\nprint(scores)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# From the scores, find the label with the highest score/probability\npredicted_labels = scores.argmax(axis = 1)\nprint([textcat.labels[label] for label in predicted_labels])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Word Embeddings\nAnother way to represent the text numerically except for bag of words. It can take into account the context in which the words appear in the form of vectors. So it is also called word vectors. Words appear in the similar context will have similar vectors. For example, \"leopard\", \"lion\" and \"tiger\" will be close to each other whil they will be far away from \"planet\" and \"castle\".\n\nOnce the vectors are set up, we can perform mathematical manipulation over them. Magically, those operations, like subtracting, adding, will have some real-world meaning. For example, if we subtract the vector of \"man\" from that of \"woman\", we will get a new vector. And if we add this new vector to the vector of \"king\", the resulting vector will be close to the vector of \"queen\"! \n\n![Manipulation of word vectors](https://www.tensorflow.org/images/linear-relationships.png)\n\nThe vectors can sure be fed into a machine learning model. In **spaCy**, there is a model called Word2Vec. You can access them by loading a large language model like `en_core_web_lg`. Then they will be available on tokens from the `.vector` attribute."},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport spacy\n\n# Need to load the large model to get the vectors\nnlp = spacy.load('en_core_web_lg')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Disabling other pipes because we don't need them and it'll speed up this part a bit\ntext = \"These vectors can be used as features for machine learning models.\"\nwith nlp.disable_pipes():\n    vectors = np.array([token.vector for token in nlp(text)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vectors.shape\n#print(vectors[11])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The result shows there are 12 300-dimentional vectors for each word and the punctuation \".\". \n\nHowever, the vectors we got now is only the word-level embeddings. In practice, for modeling, we need document-level labels, which can be obtained from the word-level vectors via an easy approach -- averaging the vectors for each word in the documents.\n\nspaCy provide us with such functionality through `doc.vector`."},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\n\n# Loading the spam data\n# ham is the label for non-spam messages\nspam = pd.read_csv('../input/nlp-course/spam.csv')\n\nwith nlp.disable_pipes():\n    doc_vectors = np.array([nlp(text).vector for text in spam.text])\n    \ndoc_vectors.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Classification Models\n\nWith the document vectors, you can train scikit-learn models, xgboost models, or any other standard approach to modeling."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(doc_vectors, spam.label, test_size = 0.1, random_state = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"[SVMs](https://scikit-learn.org/stable/modules/svm.html#svm) provided by scikit-learn (`LinearSVC`) is used here. "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import LinearSVC\n\n# Set dual = False to speed up training, and it's not needed\nsvc = LinearSVC(random_state = 1, dual = False, max_iter = 10000)\nsvc.fit(X_train, y_train)\nprint(f\"Accuracy: {svc.score(X_test, y_test) * 100:.3f}%\", )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Document Similarity\n\nSince documents have now been presented in the form of vectors, the questions we are facing is how to measure the similarity between the vectors. The geomety tells us if there are two vectors in the space, and they are parallel to each other, then the angle between the two vectors is zero. If we do not care about the starting point and length of the two vectors, in this case, we can conclude those two vectors are identical to each other. They have the biggest similarity now. So, it is natural to use the angle between the vectors as a metric to tell if two vectors are similar to each other. The bigger the angle is, the smaller the similarity is. \n\nA little math here, although I hate this part, I cannot figure out any other way to explain this issue without using math. Suppose we a vector $\\bf{a}$ and a vector $\\bf{b}$, the angle between them is $\\theta$. Then the $\\theta$ can be calculated by:\n$$\\cos \\theta = \\frac{\\bf{a}\\bullet \\bf{b}}{\\|\\bf{a}\\|\\space\\|\\bf{b}\\|}$$\n$\\|\\bf{a}\\|$ and $\\|\\bf{b}\\|$ are the magnitude of those two vectors respectively. The following snippet is used to calculate the formula\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"def cosine_similarity(a, b):\n    return a.dot(b)/np.sqrt(a.dot(a) * b.dot(b))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a = nlp(\"Reply now for free tea\").vector\nb = nlp(\"According to legend, Emperor Shen Nung discovered tea when leaves from a wild tree blew into his pot of boiling water.\").vector\ncosine_similarity(a,b)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}