{"cells":[{"metadata":{},"cell_type":"markdown","source":"### Background:\nNational Institute of Diabetes and Digestive and Kidney Diseases. The objective of the dataset is to diagnostically predict whether or not a patient has diabetes, based on certain diagnostic measurements included in the dataset. Several constraints were placed on the selection of these instances from a larger database. In particular, all patients here are females at least 21 years old of Pima Indian heritage.\n\n### Content\nThe datasets consists of several medical predictor variables and one target variable, Outcome. Predictor variables includes the number of pregnancies the patient has had, their BMI, insulin level, age, and so on.\n\n### Objective of this excercise:\nTo learn implementation PipeLine in Machine Learning Logistic Regression"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# supress warnings\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# read data\ndata = pd.read_csv(\"/kaggle/input/pima-indians-diabetes-database/diabetes.csv\")\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# creating duplicate dataset\ndf = data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# visualise data with pairplot\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nsns.pairplot(df,hue=\"Outcome\",diag_kind='kde')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Most of the features spread are overlapping with respect to diabates/outcome\n- Age glucose and BMI ar having relatively better classified spread."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Feature Selection\n# define X and Y variabel\nX=df.drop('Outcome',axis=1)\ny=df[['Outcome']]\n\n# get feature importance using gradient boosting\nfrom sklearn.ensemble import GradientBoostingClassifier\ngb = GradientBoostingClassifier()\ngb.fit(X,y)\ngb.feature_importances_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check for Feature importance\nFeature_Importance_GB = pd.DataFrame({\"Feature_Importance_GB\" : gb.feature_importances_}, index=df.columns[:-1])\nFeature_Importance_GB.sort_values(by = 'Feature_Importance_GB', ascending = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- `Glucose`, `BMI`, `Age` and `DiabetesPedigreeFunction` turns out to be most important features"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets check the feature importance using SelectKBest (based on Pvalue)\nfrom sklearn.feature_selection import SelectKBest,f_classif\nskb = SelectKBest(f_classif,8)\nskb.fit_transform(X,y)\n\n\n\n# check for Feature importance\nKbest = pd.DataFrame({\"Pvalue\" : skb.pvalues_}, index=df.columns[:-1])\nKbest.sort_values(by = 'Pvalue', ascending = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Blood pressure is not having any significance to model\n- Glucose, BMI, Age seems to be most important faetures"},{"metadata":{"trusted":true},"cell_type":"code","source":"# check for the desinilarity in the data set by ttest\nplasdiabetic = df[df['Outcome']==1]['Glucose']\nplasnondiabetic = df[df['Outcome']==0]['Glucose']\nimport scipy.stats as stats\nprint(stats.ttest_ind(plasdiabetic,plasnondiabetic))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This shows the variance in the data set. which proves the mean of diabatic and nondiabatic spread are not same. So we can classify the dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Zero Model using Pipeline"},{"metadata":{"trusted":true},"cell_type":"code","source":"# fit and validate Zero model using Logistic regression in pipeline\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import PowerTransformer\nfrom sklearn.preprocessing import StandardScaler\n\n#train test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20,random_state=20)\n\n#define Pipeline\nfrom sklearn.pipeline import Pipeline\npipe = Pipeline((\n(\"pt\",PowerTransformer()),\n(\"sc\",StandardScaler()),    \n(\"lr\", LogisticRegression()),\n))\npipe.fit(X_train,y_train)\n\nprint(\"Testing Accuracy : \", pipe.score(X_test,y_test))\nprint(\"Training Accuracy: \", pipe.score(X_train,y_train))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Pipeline Intermediate Step\npipe.named_steps['lr'].coef_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Improving Feature Quality"},{"metadata":{"trusted":true},"cell_type":"code","source":"# check for basic ststistics in data set\ndf.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- `Glucose`, `BloodPressure`, `SkinThickness`, `Insulin`, `BMI` are the feature which shows zero as minimum, which is practically not possible. So we will replace these data as `np.nan`"},{"metadata":{"trusted":true},"cell_type":"code","source":"# replacing zero values with np.nan\ndf = pd.read_csv('../input/pima-indians-diabetes-database/diabetes.csv')\ndf['Glucose'].replace(0,np.nan, inplace = True)\ndf['BloodPressure'].replace(0,np.nan, inplace = True)\ndf['Insulin'].replace(0,np.nan, inplace = True)\ndf['BloodPressure'].replace(0,np.nan, inplace = True)\ndf['SkinThickness'].replace(0,np.nan, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check for info and null values\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train test split\nX=df.drop('Outcome',axis=1)\ny=df[['Outcome']]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20,random_state=20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model with entire dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"#With Pipeline\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.experimental import enable_iterative_imputer  # noqa\nfrom sklearn.impute import IterativeImputer\n\npipe = Pipeline((\n(\"it\", IterativeImputer()),\n(\"pt\",PowerTransformer()),\n(\"sc\", StandardScaler()),\n(\"lr\", LogisticRegression()),\n))\npipe.fit(X_train,y_train)\n\nprint(\"Testing Accuracy : \", pipe.score(X_test,y_test))\nprint(\"Training Accuracy: \", pipe.score(X_train,y_train))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Introducing SelectKBest in pipeline"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Including SelectKBest\n#With Pipeline\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_classif\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.feature_selection import RFE\npipe = Pipeline((\n(\"it\", IterativeImputer()),\n(\"pt\",PowerTransformer()),\n(\"sc\", StandardScaler()),\n(\"skb\",SelectKBest(f_classif,k=3)),\n(\"lr\", LogisticRegression()),\n))\npipe.fit(X_train,y_train)\nprint(\"Testing Accuracy : \", pipe.score(X_test,y_test))\nprint(\"Training Accuracy : \",pipe.score(X_train,y_train))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Introducing RFE in pipeline"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Including RFE\n#With Pipeline\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.feature_selection import RFE\n\npipe = Pipeline((\n(\"it\", IterativeImputer()),\n(\"pt\",PowerTransformer()),\n(\"sc\", StandardScaler()),\n(\"fs\",RFE(estimator = LogisticRegression(),n_features_to_select=3, step=1)),\n(\"lr\", LogisticRegression()),\n))\npipe.fit(X_train,y_train)\n\nprint(\"Testing Accuracy : \", pipe.score(X_test,y_test))\nprint(\"Training Accuracy: \", pipe.score(X_train,y_train))","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"pipe.named_steps['lr'].coef_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check for clssification report\npredicted = pipe.predict(X_test)\nfrom sklearn.metrics import confusion_matrix,classification_report,recall_score,precision_score,f1_score\nprint(confusion_matrix(y_test,predicted))\nprint(classification_report(y_test,predicted))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(recall_score(y_test,predicted,average=None))\nprint(precision_score(y_test,predicted,average=None))\nprint(f1_score(y_test,predicted,average=None))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Evaluating models using Cross Validation\nfrom sklearn.model_selection import cross_val_score\nscoreslr = cross_val_score(pipe, X_train, y_train, cv=10, scoring='accuracy')\nprint(scoreslr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check for mean accuracy and Standard deviation\nimport numpy as np\nprint(\"Average Accuracy of my model: \", np.mean(scoreslr))\nprint(\"SD of accuracy of the model : \", np.std(scoreslr,ddof=1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 95% Confidence Interval of Accuracy\nimport scipy.stats\nxbar = np.mean(scoreslr)\nn=10\ns = np.std(scoreslr,ddof=1)\nse = s/np.sqrt(n)\nstats.t.interval(0.95,df=n-1,loc=xbar,scale=se)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**The Recall score of the model is not good. The data is imbalance. We will use SMOTE to overcome it in the future action.**"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}