{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Introduction\n\nThis data set pertains to all types of structural permits. Data includes details on application/permit numbers, job addresses, supervisorial districts, and the current status of the applications. Data is uploaded weekly by DBI. Users can access permit information online through DBI’s Permit Tracking System which is 24/7 at www.sfdbi.org/dbipts.\n\nNote if you need to open permits in Excel, use one of the pre-filtered datasets:\n\n1. Building Permits on or after January 1, 2013 https://data.sfgov.org/d/p4e4-a5a7\n2. Building Permits before January 1, 2013 https://data.sfgov.org/d/4jpb-z4kk","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1. Reading the files\nThe dataset provided is in a .csv format. This is a structured dataset, with columns representing a range of things. For dealing with structured data, pandas is the most important library. We already imported pandas as pd when we used the import* command earlier. We will now use the read_csv function of pandas to read the data:","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# create new df\ndf = pd.read_csv('/kaggle/input/sf-building-permits/Building_Permits_on_or_after_January_1__2013.csv');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us look at the first few rows of the data. Since the dataset is large, this command does not show us the complete column-wise data. \nTo fix this, we will define the following function, where we set max.rows and max.columns to 100.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.set_option('display.max_columns',100)\ndf.head(1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's change the names of our columns so that it can be more convenient to work with them.\nTranslate to lowercase and remove characters from names","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# translate to lowercase\ndf.columns = map(str.lower, df.columns)\n# remove characters from names\ndf.columns = [c.replace(' ', '_') for c in df.columns]\ndf.columns = [c.replace('-', '') for c in df.columns]\ndf.head(1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now with the help of the library missingno we visualize empty values in our dataset. \nWhite passes are missing data in our dataset.\n\n**Visualising missing values for a sample of 250**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import missingno as msno\nmsno.matrix(df.sample(250))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The missingno correlation heatmap measures nullity correlation: how strongly the presence or absence of one variable affects the presence of another\n* **msno.bar** is a simple visualization of nullity by column:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"msno.bar(df.sample(1000))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"msno.heatmap(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Nullity correlation ranges from -1 (if one variable appears the other definitely does not) to 0 (variables appearing or not appearing have no effect on one another) to 1 (if one variable appears the other definitely also does).\n\nVariables that are always full or always empty have no meaningful correlation, and so are silently removed from the visualization—in this case for instance the datetime and injury number columns, which are completely filled, are not included.\n\nEntries marked <1 or >-1 are have a correlation that is close to being exactingly negative or positive, but is still not quite perfectly so. This points to a small number of records in the dataset which are erroneous. For example, in this dataset the correlation between 'exiting_use' and ' is <1, indicating that, contrary to our expectation, there are a few records which have one or the other, but not both. These cases will require special attention.\n\nThe heatmap works great for picking out data completeness relationships between variable pairs, but its explanatory power is limited when it comes to larger relationships and it has no particular support for extremely large datasets.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Dendrogram** allows you to more fully correlate variable completion, revealing trends deeper than the pairwise ones visible in the correlation heatmap:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"msno.dendrogram(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The dendrogram uses a hierarchical clustering algorithm (courtesy of scipy) to bin variables against one another by their nullity correlation (measured in terms of binary distance). At each step of the tree the variables are split up based on which combination minimizes the distance of the remaining clusters. The more monotone the set of variables, the closer their total distance is to zero, and the closer their average distance (the y-axis) is to zero.\n\nTo interpret this graph, read it from a top-down perspective. Cluster leaves which linked together at a distance of zero fully predict one another's presence—one variable might always be empty when another is filled, or they might always both be filled or both empty, and so on. In this specific example the dendrogram glues together the variables which are required and therefore present in every record.\n\nCluster leaves which split close to zero, but not at it, predict one another very well, but still imperfectly. If your own interpretation of the dataset is that these columns actually are or ought to be match each other in nullity (for example, as CONTRIBUTING FACTOR VEHICLE 2 and VEHICLE TYPE CODE 2 ought to), then the height of the cluster leaf tells you, in absolute terms, how often the records are \"mismatched\" or incorrectly filed—that is, how many values you would have to fill in or drop, if you are so inclined.\n\nAs with matrix, only up to 50 labeled columns will comfortably display in this configuration. However the dendrogram more elegantly handles extremely large datasets by simply flipping to a horizontal configuration.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# delete columns with a name delet_\ndf.drop(df.filter(regex='delete').columns, axis=1, inplace=True)\ndf.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's look at the number of unique values in each column.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.nunique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Primary data analysis / Primary visual data analysis","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* We select all the \"estimated cost\" and \"revised_cost\" values by the time the order was created.\n* Using the to_datetime function, we translate the string values in the date column into a time format.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data_loc = df.loc[:,['estimated_cost', 'revised_cost','permit_creation_date']]\ndata_cost = data_loc \ndata_cost.permit_creation_date = pd.to_datetime(data_cost.permit_creation_date)\ndata_cost = data_cost.set_index('permit_creation_date')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Using the dropna function, we delete all empty values, thus deleting all lines that will contain some kind of empty value in one of the parameters.\n* And using the groupby function, we group all our data by month. In this case, the value in other columns will be summarized. \n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data_cost = data_cost.dropna()\ndata_cost_m = data_cost.groupby(pd.Grouper(freq='M')).sum()\ndata_cost_m.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let's create a new chart, where on the X axis we will have the month of creating the order, and on the Y axis - the \"estimated cost\". We see here a slight cyclicality and a general tendency towards a decline in the total number of requests for construction.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(19,8))\n# Add title\nplt.title(\"Estimated costs and revised costs in 2013-2020\")\nsns.lineplot(data=data_cost_m)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's make the graph more visual and now group these same data by year. Here we will use the groupby function again, only as an argument to the function we will have not a month, but a year. And as before, the values ​​in the output columns during grouping will be summarized.\n\nHere you can already notice the general trend and we see that the total number of permits to the construction department has fallen compared to 2016. In 2019, activity in the construction industry of San Francisco was at the level of 2014. The general trend of the last five years - activity in the construction industry is falling.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data_cost_y = data_cost.groupby(pd.Grouper(freq='Y')).sum()\nplt.figure(figsize=(19,8))\n# Add title\nplt.title(\"Estimated costs and revised costs in 2013-2020\")\n# Line chart showing daily global streams of each song \nsns.lineplot(data=data_cost_y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's look at the number of building permits by day of the week. To do this, select the data from our previous data loc dataframe. Using the day_name function, we define for each date in the string the day of the week. Group all the data by day of the week. And display our data in a new chart.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data_cost_d = data_loc\ndata_cost_d = data_cost_d.dropna()\ndata_cost_d.permit_creation_date = data_cost_d.permit_creation_date.dt.day_name()\n\ncats = [ 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday']\ndata_cost_d  = data_cost_d.groupby(['permit_creation_date']).count().reindex(cats) \nplt.figure(figsize=(18,6))\ndata_cost_d.plot.bar()\nsns.lineplot(data=data_cost_d)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now create new latitude and longitude data for each object: \n* To do this, we will split the Location column using the split function. \n* And create a new dataframe where we will already have new data on latitude and longitude and \"estimated cost\".\n* Also here we will remove all zero values ​​from our new data frame.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df[['lat','long']] = df.location.str.split(\",\",expand=True)\ndata_location = df.loc[:,['long','lat','zipcode','estimated_cost']]\ndata_location = data_location.dropna()\ndata_location.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Using the apply function, we remove the opening and closing quotes from the latitude and longitude columns.\n* Using the info function, we see that there are 227,000 rows in our dataframe. Large enough dataframe for analysis.\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data_location.long = data_location.long.apply(lambda x: x.replace(')',''))\ndata_location.lat = data_location.lat.apply(lambda x: x.replace('(',''))\ndata_location.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using the to_numeric function, we translate the data in the latitude and longitude columns to float64 format - that is, a floating-point number.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data_location.lat = pd.to_numeric(data_location.lat)\ndata_location.long = pd.to_numeric(data_location.long)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Group all our dataframe “Data Location” by the zip code parameter. \n* At the same time, when grouping the data in the columns latitude, longitude and cost - all values will be taken as average values.\nThat is, they will not be summed up, as we already did before, but here the average values ​​will be taken.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data_location_mean = data_location.groupby(['zipcode'])['lat','long','estimated_cost'].mean()\ndata_location_mean.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using the folium library, we can display the average postcode price on a San Francisco map. \n* To do this, we will set the center of our map to a point with the long and lat values, which will be taken from our data frame by the average value in the latitude and longitude columns. \n* Jumpstart points to the scale of the map and with the argument Stamen Toner - points to the black and white style that we will use for the map.\n* Using the for loop and the Circle function, we can specify the average estimated_cost as a circle, where each circle will point to a zip code with the center of longitude and latitude. \n\nFrom this map, we can conclude that on Treasure Island we have the highest average cost of a building permit.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import folium\nfrom folium import Circle\n\n# map folium display\nlat = data_location_mean.lat.mean()\nlong = data_location_mean.long.mean()\nmap1 = folium.Map(location = [lat, long], zoom_start = 12, tiles='Stamen Toner')\nparam = 'estimated_cost'\nfor i in range(0,len(data_location_mean)):\n    Circle(\n        location = [data_location_mean.iloc[i]['lat'], data_location_mean.iloc[i]['long']],\n        radius= [data_location_mean.iloc[i]['estimated_cost']/5000],\n        fill = True).add_to(map1)\nmap1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's look at the total total cost of building permits depending on the zip code. \n* Group all the data that is in the Data Location data frame by zip code. Now the values from the longitude and latitude columns will be averaged when grouping. \n* And display our new dataframe Data Location.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data_location_lang_long = data_location.groupby(['zipcode'])['lat','long'].mean()\ndata_location_lang_long.head\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After the first grouping, using the assign function, we add to our frame date a new column called cost, which in turn was grouped by zip code, and in which it is no longer the average value - but the sum of all the values ​​in the group.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data_location_lang_long = data_location_lang_long.assign(cost = data_location.groupby(['zipcode'])['estimated_cost'].sum())\ndata_location_lang_long.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using the folium library, we will once again display our data on the total cost of all building permits for the postal code. From this we see that the total total cost of the appeal is mainly localized in downtown. And specifically on several of the main streets of San Francisco. At the same time, on a treasure island, where there was a very large average cost of building permits - the total cost of work at the level of suburban areas.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import folium\nfrom folium import Circle\n\n# map folium display\nlat = data_location_lang_long.lat.mean()\nlong = data_location_lang_long.long.mean()\nmap1 = folium.Map(location = [lat, long], zoom_start = 12)\n\nfor i in range(0,len(data_location_lang_long)):\n    Circle(\n        location = [data_location_lang_long.iloc[i]['lat'], data_location_lang_long.iloc[i]['long']],\n        radius= [data_location_lang_long.iloc[i]['cost']/20000000],\n        fill = True).add_to(map1)\nmap1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For the prediction problem, we choose the \"estimated cost\" parameter.\n* Using the heatmap function, we display the correlations between our parameters and see that the \"estimated cost\" parameter has practically no correlation with other parameters, which of course greatly complicates our task of predicting estimated_cost.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sn\nsn.heatmap(df.corr())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* In order to limit our selection, we first remove all empty values from the description column. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dfn = df.dropna(subset=['description'])\ndfn.description.isnull().values.any()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From our entire data frame, we will select only the data for which the description parameter has the value reroofing, that is, the work of deconstructing the old and creating a new roof. We select all the objects on which some changes on the roof have been made since 2014.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dfn = dfn[dfn['description'].str.match('reroofing')]\ndfn.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Present graphically our new data. To do this, we Select from our already cleared dfn dataframe - 'estimated_cost', 'existing_use', 'existing_units', 'zipcode', 'issued_date'. \n* And we delete all empty lines with empty values.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dfn.to_excel('eeeee.xls')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_unit = dfn.loc[:,['estimated_cost','existing_use', 'existing_units', 'zipcode','issued_date']]\ndf_unit = df_unit.dropna()\ndf_unit.head(15)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we will have large emissions due to the fact that hotels and industrial buildings are also taken into account here. \n* Therefore, we will limit our data frame to only one-story, two-story houses, offices and apartments.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_unit[df_unit.existing_use.str.contains(\"family|office|apartments\")]\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"On the new chart we display the average \"estimated cost\" by type of housing. On this graph you can see how much higher the average \"estimated cost\" of repairing roofs in office buildings is. Here you can certainly talk about some kind of “cartel conspiracy” :). At the same time, the average cost of repairing a roof for two and one family house is practically the same.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fam1 = df_unit[df_unit['existing_use']=='1 family dwelling']['estimated_cost'].mean()\nfam2 = df_unit[df_unit['existing_use']=='2 family dwelling']['estimated_cost'].mean()\noffice = df_unit[df_unit['existing_use']=='office']['estimated_cost'].mean()\napartments = df_unit[df_unit['existing_use']=='apartments']['estimated_cost'].mean()\n\ndata = {'1 family dwelling':fam1,'2 family dwelling':fam2,'Office':office,'Apartments':apartments}\ntypedf = pd.DataFrame(data = data,index=['Counts'])\ntypedf.plot(kind='barh', title=\"Average estimated cost by type\", figsize=(8,6));","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Group all these data by years. \n* We will select data for only one private house and apartment, since for the office the cost data is too high. \n* And here, as in previous examples, we will group our data with average values.\n\nOn the graph you can see that the value for apartments varies greatly from year to year, while for two family and one family houses estimed cost do not change so much. And in the second graph we see that the cost is growing from year to year - this way you can see inflation in the construction market. We can approximately see that inflation, for example, by the average cost of repairing a roof from 2014 to 2019 was approximately 30%. That is, inflation in the construction market is 6% per year.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_unit.issued_date = pd.to_datetime(df_unit.issued_date)\ndf_unit.issued_date = df_unit.issued_date.dt.year\nyears = list(range(2013, 2020)) \nkeywords = ['1 family dwelling','2 family dwelling','apartments']\n\nval_data = []\nfor year in years:\n    iss_data = []\n    for word in keywords:\n        v = df_unit[(df_unit['existing_use']==word) & (df_unit['issued_date']== year)]['estimated_cost'].mean()\n        iss_data.append(v)\n    val_data.append(iss_data)\n#print(val_data)\ndfnew = pd.DataFrame(data=val_data, index=years, columns=keywords)\ndfnew.head()\n\n\ndfnew.plot.bar(figsize=(12, 6)) \nplt.xlabel(\"Years\")\nplt.ylabel(\"Estimated cost of reroofing\")\nplt.title(\"Estimated cost of Bathroom by year\");\ndfnew.plot.line(figsize=(12, 6))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We set ourselves the task of determining the \"estimated cost\" by several parameters.\nAt the next stage, we need to determine all the characteristics by which we will determine the estimetet cost for roof repairs for a new facility. Unfortunately, we do not have data on the size of objects and the cost of, for example, the house itself, which would be the main parameter in determining the cost of work. We will work with those parameters that are in the public domain.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_corr = dfn.dropna(subset=['existing_use'])\ndf_corr.description.isnull().values.any()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*  To limit our selection and improve the prediction, we will take data for only 1 family houses.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_1fam = df_corr[df_corr.existing_use.str.contains('1 family')]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* First we find all the characteristics with numerical values in our data frame. \n* And sort them by correlation with our desired value \"estimated cost\".","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"num_feuture = df_1fam.select_dtypes(include=[np.number])\ncorr = num_feuture.corr()\nprint(corr['estimated_cost'].sort_values(ascending = False))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dfn[['lat','long']] = dfn.location.str.split(\",\",expand=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since we see that our \"estimated cost\" is little correlated with other parameters, so take 'permit_creation_date', 'zipcode', 'number_of_existing_stories', 'number_of_proposed_stories', 'current_police_districts' and other parameters that somehow correlate with the cost. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#df_pr = dfn.loc[:,['permit_creation_date', 'existing_use', 'existing_units','estimated_cost','zipcode','current_supervisor_districts', 'analysis_neighborhoods', ]]\n#df_pr = dfn.loc[:,['permit_creation_date', 'zipcode', 'existing_use', 'existing_construction_type', 'estimated_cost', 'long','lat' ]]#\n#df_pr = df_1fam.loc[:,['permit_creation_date', 'zipcode', 'number_of_existing_stories', 'number_of_proposed_stories',  'current_police_districts', 'existing_use', 'long','lat', 'record_id',  'estimated_cost',  ]]\n\ndf_pr = df_1fam.loc[:,['permit_creation_date', 'zipcode', 'number_of_existing_stories', 'number_of_proposed_stories',  'current_police_districts', 'long','lat', 'record_id',  'estimated_cost',  ]]\ndf_pr = df_pr.dropna()\n#df_pr = df_pr[df_pr.existing_use.str.contains('1 family')]\ndf_pr.permit_creation_date = pd.to_datetime(df_pr.permit_creation_date)\ndf_pr.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's look at the distribution of the \"estimated cost\" values ​​in the form of a histogram. Here we will see that we have some values ​​of $ 200,000 and there are very few of them. And a large number of small values.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"histplot = df_pr.estimated_cost.plot.hist(bins = 40)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* We will consider this data as outliers and delete them from our dataframe.\nThat is, we delete all the lines where the \"estimated cost\" will be more than 20,000 and less than 12,000. \n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"indexNames = df_pr[ (df_pr['estimated_cost'] > 20000)].index\ndf_pr.drop(indexNames , inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"indexNames = df_pr[ (df_pr['estimated_cost'] < 12000)].index\ndf_pr.drop(indexNames , inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And now our distribution will not look so one-sided.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"histplot = df_pr.estimated_cost.plot.hist(bins = 40)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using the Johnson library, we can look at the \"normality\" of the distribution of our values","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import scipy.stats as st\ny = df_pr['estimated_cost']\nplt.figure(figsize=(7,4))\nplt.figure(1); plt.title('Johnson SU')\nsns.distplot(y, kde=False, fit=st.johnsonsu)\nplt.figure(figsize=(7,4))\nplt.figure(figsize=(7,4))\nplt.figure(2); plt.title('Normal')\nsns.distplot(y, kde=False, fit=st.norm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_pr.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* As we did before, we remove the extra characters from the columns of longitude and latitude","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_pr.long= df_pr.long.apply(lambda x: x.replace(')',''))\ndf_pr.lat = df_pr.lat.apply(lambda x: x.replace('(',''))\ndf_pr.lat = pd.to_numeric(df_pr.lat)\ndf_pr.long = pd.to_numeric(df_pr.long)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Find the distance to the center of Downtown San Francisco. That is, we will take each object and from its point of latitude and longitude we will find the distance to the city center that is, to the value of the longitude and width of downtown San Francisco (37.7945742, -122.3999445).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from geopy.distance import vincenty\ndef distance_calc (row):\n    start = (row['lat'], row['long'])\n    stop = (37.7945742, -122.3999445)\n\n    return vincenty(start, stop).meters/1000\n\ndf_pr['distance'] = df_pr.apply (lambda row: distance_calc (row),axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Create a new Downtown proximity parameter, that is, the distance from the city center, in which we will have four different values. *\n\n1. Downtown itself, \n2.  <0.5H on foot to Downtown\n3. <1H Downtown on foot to Downtown\n4. \">=\" 10 -> Outside SF\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def downtown_proximity(dist):\n    '''\n    < 2 -> Near Downtown\n    >= 2, <6 -> <0.5H Downtown\n    >= 6, <10 -> <1H Downtown \n    >= 10 -> Outside SF\n\n    '''\n    if dist < 2:\n        return 'Downtown'\n    elif dist < 6:\n        return  '<0.5H Downtown'\n    elif dist < 10:\n        return '<1H Downtown'\n    elif dist >= 10:\n        return 'Outside SF'\n\ndf_pr['downtown_proximity'] = df_pr.distance.apply(downtown_proximity)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Display the average values ​​of the \"estimated cost\" by category in our new Downtown proximity column. Here you can see that the average cost differ by about 6 - 7%. In principle, houses in a rich area spend on the roof about 10% more than people whose houses are farther from the center.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"value_count=df_pr['downtown_proximity'].value_counts()\nplt.figure(figsize=(12,5))\nplt.title('Estimated cost rerofing depending on Downtown Proximity');\nsns.boxplot(x=\"downtown_proximity\", y=\"estimated_cost\", data=df_pr);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\nLook at the same data on the map and see the amount of data by distance from the center. Here, we chose the distance of 3 km to the city center as the main indicator - where the yellow dots show objects on the map that are located up to 3 km from the city center.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set()\nlocal_coord=[-122.3999445, 37.7945742] # the point near which we want to look at our variables\neuc_dist_th = 0.03 # distance treshhold\n\neuclid_distance=df_pr[['lat','long']].apply(lambda x:np.sqrt((x['long']-local_coord[0])**2+(x['lat']-local_coord[1])**2), axis=1)\n\n# indicate wethere the point is within treshhold or not\nindicator=pd.Series(euclid_distance<=euc_dist_th, name='indicator')\n\nprint(\"Data points within treshhold:\", sum(indicator));\n\n# a small map to visualize th eregion for analysis\nsns.lmplot('long', 'lat', data=pd.concat([df_pr,indicator], axis=1), hue='indicator', markers ='.', fit_reg=False, height=8);\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"On the new map we can mark all our objects in our 4 categories: city center, half an hour on foot, 1 hour, or outside the city.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.lmplot('long', 'lat', data=df_pr,markers ='.', hue='downtown_proximity', fit_reg=False, height=8)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Create a new column for the year \n* Delete the value that is no longer needed - permit_creation_date and location which we already used, in order to find the longitude and latitude.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#df_pr['month'] = df_pr.permit_creation_date.dt.month\ndf_pr['year'] = df_pr.permit_creation_date.dt.year\ndf_pr = df_pr.drop(columns=['permit_creation_date', 'long', 'lat'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"get_dummies - Convert categorical variable downtown_proximity into dummy/indicator variables (0,1).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#df_pr = pd.concat([df_pr, pd.get_dummies(df_pr.existing_use, prefix='existing_use')], axis=1)\ndf_pr = pd.concat([df_pr, pd.get_dummies(df_pr.downtown_proximity, prefix='dt_pr')], axis=1)\n\n#df_pr = df_pr.drop(columns=['existing_use'])\ndf_pr = df_pr.drop(columns=['downtown_proximity'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_pr.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\nWe can escalate the value in our data \n\n* Or we can simply subtract the minimum values from the data with large values.\n* For example, from the values of the year we subtract the minimum value - thereby we reduce the total value and these values will less affect our predictions.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#df_pr.existing_units = df_pr.existing_units.apply(lambda x: 10 if x > 10 else x)\ndf_pr.zipcode = df_pr.zipcode - df_pr.zipcode.min()\ndf_pr.year = df_pr.year - df_pr.year.min()\ndf_pr.record_id = df_pr.record_id - df_pr.record_id.min()\n#df_pr.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_pr.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_pr.hist(bins=50, figsize=(10, 10));","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we visualize our data using the technique of nonlinear dimensionality reduction and visualization of multidimensional variables. TSNE **\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.manifold import TSNE\ntsne=TSNE(perplexity = 3)\ntsne.fit(df_pr)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**\nt-Distributed Stochastic Neighbor Embedding** (t-SNE) is an unsupervised, non-linear technique primarily used for data exploration and visualizing high-dimensional data. In simpler terms, t-SNE gives you a feel or intuition of how the data is arranged in a high-dimensional space. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(tsne.embedding_[:,0], tsne.embedding_[:,1])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Separate the points, that is, our objects. And we will find projects in which the cost of construction work to create a new roof is more than \"$\"13,000 = orange and less than 13,000 in blue.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_pr['proofcost'] = df_pr.estimated_cost.apply(lambda x: True if x>=13000 else False )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(tsne.embedding_[df_pr.proofcost.values, 0], tsne.embedding_[df_pr.proofcost.values, 1], color='orange')\nplt.scatter(tsne.embedding_[~df_pr.proofcost.values, 0], tsne.embedding_[~df_pr.proofcost.values, 1], color='blue')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Delete the column 'proofcost' we do not need","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_pr = df_pr.drop(columns = 'proofcost')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#df_pr = df_pr.drop(['existing_construction_type'], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Creating, Training, Evaluating, Validating, and Testing ML Models","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Now we can start testing our model. First, we import all the libraries we need from the main sklearn library.\n\n**sklearn** is a Python module integrating classical machine learning algorithms in the tightly-knit world of scientific Python packages (numpy, scipy, matplotlib).\n\nIt aims to provide simple and efficient solutions to learning problems that are accessible to everybody and reusable in various contexts: machine-learning as a versatile tool for science and engineering.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import Ridge\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import r2_score, mean_squared_error\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score, KFold\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Defining Training/Test Sets**\n\n* divide all our data into training and validation data. In X we will have all the values ​​except the value. And in Y the value is only value.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_training = df_pr.drop(['estimated_cost'], axis = 1)\ny_training = df_pr['estimated_cost']\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Splitting into Validation**\n\n* Using the train_test_split function, we will separate our data regarding 80% training data and 20% validation data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split #to create validation data set\nX_train, X_valid, y_train, y_valid = train_test_split(X_training, y_training, test_size=0.2, random_state=0) \n#X_valid and y_valid are the validation sets","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Linear Regression Model\n**\n\nLet's look at the indicators of the model that the Linear Regression module will build for us. \n\n*  Train our data using the fit function.\n\nEach machine learning algorithm has a basic set of parameters that can be changed to improve its accuracy. During the fitting process, you run an algorithm on data for which you know the target variable, known as “labeled” data, and produce a machine learning model. \n\n* Compare the outcomes to real, observed values of the target variable to determine their accuracy.\n\nThen we predict based on our new cost model for the validation data frame X_valid. And we compare our obtained data with the initial data y_valid, calculating the determination coefficient for these values ​​and the standard deviation - the RMSE coefficient.\n\nR2 = 1 - sum of (valid value for each row - prediction) ^ 2 / sum of (valid value for each prediction - mean) ^ 2\nRMSE = sqrt (np.mean (np.square (y - y_pred)))\n\nWe got the value of RMSE = 2000 dollar.\nThose. When predicting the value using linear regression, our accuracy will be + + - 2000 dollar.\nWe can also see that due to the lack of important parameters in calculating the value, we obtained small values ​​of the determination coefficient near zero. This means that, now the forecasts do not match the actual values.\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"linreg = LinearRegression()\nlinreg.fit(X_train, y_train)\nlin_pred = linreg.predict(X_valid)\nr2_lin = r2_score(y_valid, lin_pred)\nrmse_lin = np.sqrt(mean_squared_error(y_valid, lin_pred))\nprint(\"R^2 Score: \" + str(r2_lin))\nprint(\"RMSE Score: \" + str(rmse_lin))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Try to predict the \"estimated cost\" for arbitrary parameters. Here I took an arbitrary zip code, and arbitrary values ​​for the rest as a parameter, and got values ​​with an accuracy of + - $ 2000.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"lin_pred = linreg.predict([[20.0, 1.0, 3.0, 4.0, 1163316512454, 4.825703, 0, 0, 0, 0, 1]])\nprint(\"Prediction for data with arbitrary values: \" + str(lin_pred[0]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We do the same for other regressors: DecisionTreeRegressor.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\nlinsvc = DecisionTreeRegressor()\nlinsvc.fit(X_train, y_train)\nlin_pred = linsvc.predict(X_valid)\nr2_lin = r2_score(y_valid, lin_pred)\nrmse_lin = np.sqrt(mean_squared_error(y_valid, lin_pred))\nprint(\"R^2 Score: \" + str(r2_lin))\nprint(\"RMSE Score: \" + str(rmse_lin))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"linsvc = linsvc.predict([[20.0, 1.0, 3.0, 4.0, 1163316512454, 4.825703, 0, 0, 0, 0, 1]])\nprint(\"Prediction for data with arbitrary values: \" + str(linsvc[0]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Decision Tree Regressor Model**\n\nHere, when training the model, we use standard hyperparameters. In order to configure these hyperparameters and to search for the best parameters specifically for our data, we will use GridSearchCV\n\nGridSearchCV is a library function that is a member of sklearn’s model_selection package. It helps to loop through predefined hyperparameters and fit your estimator (model) on your training set. So, in the end, you can select the best parameters from the listed hyperparameters.\nIn addition to that, you can specify the number of times for the cross-validation for each set of hyperparameters.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dtr = DecisionTreeRegressor()\nparameters_dtr = {\"criterion\" : [\"mse\", \"friedman_mse\", \"mae\"], \"splitter\" : [\"best\", \"random\"], \"min_samples_split\" : [2, 3, 5, 10], \n                  \"max_features\" : [\"auto\", \"log2\"]}\ngrid_dtr = GridSearchCV(dtr, parameters_dtr, verbose=1, scoring=\"r2\")\ngrid_dtr.fit(X_train, y_train)\n\nprint(\"Best DecisionTreeRegressor Model: \" + str(grid_dtr.best_estimator_))\nprint(\"Best Score: \" + str(grid_dtr.best_score_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dtr = grid_dtr.best_estimator_\ndtr.fit(X_train, y_train)\ndtr_pred = dtr.predict(X_valid)\nr2_dtr = r2_score(y_valid, dtr_pred)\nrmse_dtr = np.sqrt(mean_squared_error(y_valid, dtr_pred))\nprint(\"R^2 Score: \" + str(r2_dtr))\nprint(\"RMSE Score: \" + str(rmse_dtr))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#scores_dtr = cross_val_score(dtr, X_train, y_train, cv=10, scoring=\"r2\")\n#print(\"Cross Validation Score: \" + str(np.mean(scores_dtr)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next, we will test the model using the following other machine algorithms: Random Forest Regressor, Lasso, Ridge.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"rf = RandomForestRegressor()\nrf.fit(X_train, y_train)\nrf_pred = rf.predict(X_valid)\nr2_rf = r2_score(y_valid, rf_pred)\nrmse_rf = np.sqrt(mean_squared_error(y_valid, rf_pred))\nprint(\"R^2 Score: \" + str(r2_rf))\nprint(\"RMSE Score: \" + str(rmse_rf))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scores_rf = cross_val_score(rf, X_train, y_train, cv=10, scoring=\"r2\")\nprint(\"Cross Validation Score: \" + str(np.mean(scores_rf)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lasso = Lasso()\nlasso.fit(X_train, y_train)\nlasso_pred = lasso.predict(X_valid)\nr2_lasso = r2_score(y_valid, lasso_pred)\nrmse_lasso = np.sqrt(mean_squared_error(y_valid, lasso_pred))\nprint(\"R^2 Score: \" + str(r2_lasso))\nprint(\"RMSE Score: \" + str(rmse_lasso))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scores_lasso = cross_val_score(lasso, X_train, y_train, cv=10, scoring=\"r2\")\nprint(\"Cross Validation Score: \" + str(np.mean(scores_lasso)));","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ridge = Ridge()\nridge.fit(X_train, y_train)\nridge_pred = ridge.predict(X_valid)\nr2_ridge = r2_score(y_valid, ridge_pred)\nrmse_ridge = np.sqrt(mean_squared_error(y_valid, ridge_pred))\nprint(\"R^2 Score: \" + str(r2_ridge))\nprint(\"RMSE Score: \" + str(rmse_ridge))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scores_ridge = cross_val_score(ridge, X_train, y_train, cv=10, scoring=\"r2\");\nprint(\"Cross Validation Score: \" + str(np.mean(scores_ridge)));","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The obtained data on the coefficient of determination R ^ 2 and the standard error are written in the general resulting table. We see that the best results show Lasso algorithm.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model_performances = pd.DataFrame({\n    \"Model\" : [\"Linear Regression\", \"Decision Tree Regressor\", \"Random Forest Regressor\",\"Ridge\", \"Lasso\"],\n    \"R Squared\" : [str(r2_lin)[0:5], str(r2_dtr)[0:5], str(r2_rf)[0:5], str(r2_ridge)[0:5], str(r2_lasso)[0:5]],\n    \"RMSE\" : [str(rmse_lin)[0:8], str(rmse_dtr)[0:8], str(rmse_rf)[0:8], str(rmse_ridge)[0:8], str(rmse_lasso)[0:8]]\n})\nmodel_performances.round(4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_v = X_train.values\ny_train_v = y_train.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\ndecisiontree = DecisionTreeClassifier()\ndecisiontree.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import export_graphviz\nexport_graphviz(dtr_pred, out_file='tree.dot', feature_names = X_trai.columns, filled = True)\n!dot -Tpng tree.dot -o tree.png -Gdpi = 600\nfrom IPython.display import Image\nImage(filename = 'tree.png' )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Building\nWe have dealt with the categorical columns and the date values. We have also taken care of the missing values. Now we can finally power up and build the DecisionTree model we have been inching towards.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nX = df_pr.drop(['estimated_cost'], axis = 1).values\ny = df_pr['estimated_cost'].values\n\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(X, y,test_size = .3, random_state=0)\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\ny_df = df_pr['estimated_cost']\nX_df = df_pr.drop(['estimated_cost'], axis = 1)\n\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_df, y_df, test_size=0.25, random_state=5)\nfrom sklearn.linear_model import LogisticRegression\nmodel = LogisticRegression(max_iter = 500000)\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\naccuracy = model.score(X_test, y_test)\nprint(accuracy)\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import Ridge\nfrom sklearn.metrics import mean_squared_error\n\nridge=Ridge()\nparameters= {'alpha':[x for x in [0.1,0.2,0.4,0.5,0.7,0.8,1]]}\n\nridge_reg=GridSearchCV(ridge, param_grid=parameters)\nridge_reg.fit(X_train,Y_train)\nprint(\"The best value of Alpha is: \",ridge_reg.best_params_)\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"m = RandomForestRegressor(n_jobs=-1)\nm.fit(df_pr, y)\nm.score(df_pr,y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The n_jobs is set to -1 to use all the available cores on the machine. This gives us a score (r^2) of 0.99, which is excellent. The caveat here is that we have trained the model on the training set, and checked the result on the same. There’s a high chance that this model might not perform as well on unseen data (test set, in our case).\n\nThe only way to find out is to create a validation set and check the performance of the model on it. So let’s create a validation set and the train set will contain the rest.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y_df = df_pr['estimated_cost']\nX_df = df_pr.drop(['estimated_cost'], axis = 1)\n\ndef split_vals(a,n): \n    return a[:n].copy(), a[n:].copy()\n\nn_valid = int(len(df_pr)*0.25)  # same as Kaggle's test set size\nn_trn = len(df_pr)-n_valid\n#raw_train, raw_valid = split_vals(X_df, n_trn)\nX_train, X_valid = split_vals(X_df, n_trn)\ny_train, y_valid = split_vals(y_df, n_trn)\n\nX_train.shape, y_train.shape, X_valid.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here, we will train the model on our new set (which is a sample of the original set) and check the performance across both – train and validation sets.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import math \n#define a function to check rmse value\ndef rmse(x,y): \n    return math.sqrt(((x-y)**2).mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In order to compare the score against the train and test sets, the below function returns the RMSE value and score for both datasets.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def print_score(m):\n    res = [rmse(m.predict(X_train), y_train),\n           rmse(m.predict(X_valid), y_valid),\n           m.score(X_train, y_train), m.score(X_valid, y_valid)]\n    if hasattr(m, 'oob_score_'): res.append(m.oob_score_)\n    print(res)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The result of the above code is shown below. The train set has a score of 0.99, while the validation set has a score of 0.99.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"m = RandomForestRegressor(n_jobs=-1)\n%time m.fit(X_train, y_train)\nprint_score(m)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Creating a Validation set\nCreating a good validation set that closely resembles the test set is one of the most important tasks in machine learning. The validation score is representative of how our model performs on real-world data, or on the test data.\n\nKeep in mind that if there’s a time component involved, then the most recent rows should be included in the validation set. So, our validation set will be of the same size as the test set (last 25% rows from the training data).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def split_vals(a,n):\n   return a[:n].copy(), a[n:].copy()\n\nn_valid = int(len(df_pr)*0.25)  \nn_trn = len(df_pr)-n_valid\n\nraw_train, raw_valid = split_vals(df_pr, n_trn)\nX_train, X_valid = split_vals(X_df, n_trn)\ny_train, y_valid = split_vals(y_df, n_trn)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The data points from 0 to (length – 25%) are stored as the train set (x_train, y_train). A model is built using the train set and its performance is measured on both the train and validation sets as before.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"m = RandomForestRegressor(n_jobs=-1)\nm.fit(X_train, y_train)\nprint_score(m)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above code, we get the results:\n\n* RMSE on the validation set\n* R-square on validation set","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor\n\ndt_model = DecisionTreeRegressor(random_state = 42) \ndt_model.fit(train_X, train_Y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"m = RandomForestRegressor(n_estimators=1, max_depth=3, bootstrap=False, n_jobs=-1)\nm.fit(X_train, y_train)\nprint_score(m)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"draw_tree(m.estimators_[0], df_trn, precision=3)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = dt_model.predict([[116,2.0,1.0,5.0,10.0,81.0,9.781119,0,1,0,0,1,0,0]])\nprint(y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_pred = dt_model.predict(validation_X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"s = pd.Series(Y_pred)\nvalidation_Y.reindex()\ndf = pd.concat([s.reset_index(drop=True), validation_Y.reset_index(drop=True)], axis=1, ignore_index=True)\ndf.tail(15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y = df_pr.estimated_cost\nX = df_pr.drop(['estimated_cost'], axis = 1)\n\nfrom sklearn.model_selection import train_test_split\ntrain_X, validation_X, train_Y, validation_Y = train_test_split(X, Y, random_state = 42)\n\nprint(\"Training set: Xt:{} Yt:{}\".format(train_X.shape, train_Y.shape)) \nprint(\"Validation set: Xv:{} Yv:{}\".format(validation_X.shape, validation_Y.shape)) \nprint(\"-\") \nprint(\"Full dataset: X:{} Y:{}\".format(X.shape, Y.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nscore = accuracy_score(validation_Y, Y_pred)\nprint(score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor\n\nmodel = DecisionTreeRegressor(random_state = 42) \nmodel.fit(train_X, train_Y)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_absolute_error\n\n# instruct our model to make predictions for the prices on the validation set \nvalidation_predictions = model.predict(validation_X)\n\n# calculate the MAE between the actual prices (in validation_Y) and the predictions made \nvalidation_prediction_errors = mean_absolute_error(validation_Y, validation_predictions)\n\nvalidation_prediction_errors","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import KFold, cross_val_score\n\nkf = KFold(n_splits=10, random_state=17, shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features_for_trees=['street_number', 'number_of_existing_stories', 'existing_units',\n       'existing_construction_type', 'zipcode', 'sf_find_neighborhoods',\n       'distance', 'year', 'existing_use_1 family dwelling',\n       'existing_use_2 family dwelling', 'dt_pr_<0.5H Downtown',\n       'dt_pr_<1H Downtown', 'dt_pr_Downtown', 'dt_pr_Outside SF']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TOTAL = df_pr.count()[0] \nN_VALID = 0.25 # Three months \nTRAIN = int(TOTAL*N_VALID)\ndf_small = df_pr\nfeatures = ['street_number', 'number_of_existing_stories', 'existing_units',\n       'existing_construction_type', 'zipcode', 'sf_find_neighborhoods',\n       'distance', 'year', 'existing_use_1 family dwelling',\n       'existing_use_2 family dwelling', 'dt_pr_<0.5H Downtown',\n       'dt_pr_<1H Downtown', 'dt_pr_Downtown', 'dt_pr_Outside SF']\ndf_pr\ny_df = df_small['estimated_cost']\nX_train, X_val = X_df[:TRAIN], X_df[TRAIN:]\ny_train, y_val = y_df[:TRAIN], y_df[TRAIN:]\n#define a function to check rmse value\nimport  math \ndef rmse(x,y): \n    return math.sqrt(((x-y)**2).mean())\ndef print_score(m):\n    res = [rmse(m.predict(X_train), y_train),\n           rmse(m.predict(X_val), y_val),\n           m.score(X_train, y_train), m.score(X_valid, y_valid)]\n    if hasattr(m, 'oob_score_'): res.append(m.oob_score_)\n    print(res)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = RandomForestRegressor(n_estimators=40, bootstrap=True, min_samples_leaf=25)\nmodel.fit(X_train, y_train)\n#draw_tree(model.estimators_[0], X_train, precision=2)\nprint_score(model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.tree import DecisionTreeClassifier\n\nfrom sklearn.preprocessing import LabelEncoder\nlabelencoder = LabelEncoder()\n\n#training_scores_encoded = lab_enc.fit_transform(df_pr.estimated_cost)\n#print(training_scores_encoded)\n\n\ny = df_pr.estimated_cost\nX = df_pr.drop(['estimated_cost'], axis = 1)\nX_train, X_valid, y_train, y_valid = train_test_split(X,y, test_size=0.25, random_state = 17)\nX.shape, y.shape\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import accuracy_score\n\n\n\n\nml_arr =[SVC(), GaussianNB(), Perceptron(), SGDClassifier(), DecisionTreeClassifier(), RandomForestClassifier()]\n\n\nfor el in ml_arr:\n    el.fit(X_train, y_train)\n    Y_pred = el.predict(X_valid)\n    Y_pred.reshape(-1, 1)\n    #Y_pred = lab_enc.fit_transform(Y_pred)\n    #acc = round(el.score(y_valid, Y_pred) * 100, 2)\n    score = accuracy_score(y_valid, Y_pred)\n    print(score)\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# using scaled data\nX=pd.concat([train_df[dummies_names], X_train_scaled[numerical_features]], axis=1, ignore_index = True)\ncv_scores = cross_val_score(model, X, y, cv=kf, scoring='neg_mean_squared_error', n_jobs=-1)\nprint(np.sqrt(-cv_scores.mean()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 8 Cross-validation and adjustment of model hyperparameters\n\nLet's prepare cross validation samples. As far as there are not a lot of data we can easily divide it on 10 folds, that are taken from shuffled train data. Within every split we will train our model on 90% of train data and compute CV metric on the other 10%.\n\nWe fix the random state for the reproducibility.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import KFold, cross_val_score\n\nkf = KFold(n_splits=10, random_state=17, shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import Ridge\n\nmodel=Ridge(alpha=1)\ncv_scores = cross_val_score(model, X, y, cv=kf, scoring='neg_mean_squared_error', n_jobs=-1)\nprint(np.sqrt(-cv_scores.mean()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}