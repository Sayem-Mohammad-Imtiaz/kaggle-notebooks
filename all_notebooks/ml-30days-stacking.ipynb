{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#Setup libraries\nimport pandas as my_pd\nimport numpy as my_np\nimport optuna\nimport plotly\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OrdinalEncoder #categorical\nfrom sklearn.preprocessing import OneHotEncoder #categorical\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.ensemble import RandomForestRegressor\nfrom category_encoders.cat_boost import CatBoostEncoder #categorical\nfrom sklearn.preprocessing import StandardScaler #numerical\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.metrics import mean_squared_error\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-30T12:42:10.972928Z","iopub.execute_input":"2021-08-30T12:42:10.973426Z","iopub.status.idle":"2021-08-30T12:42:14.454838Z","shell.execute_reply.started":"2021-08-30T12:42:10.97334Z","shell.execute_reply":"2021-08-30T12:42:14.453784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#declaring num of folds\nnum_of_folds = 5","metadata":{"execution":{"iopub.status.busy":"2021-08-30T12:42:18.650912Z","iopub.execute_input":"2021-08-30T12:42:18.651273Z","iopub.status.idle":"2021-08-30T12:42:18.655012Z","shell.execute_reply.started":"2021-08-30T12:42:18.651244Z","shell.execute_reply":"2021-08-30T12:42:18.654311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"########for prep 1 level 0 setup############### \ndf = my_pd.read_csv(\"../input/30days-folds/train_folds.csv\")\ndf_test = my_pd.read_csv(\"../input/30-days-of-ml/test.csv\")\nsample_submission = my_pd.read_csv(\"../input/30-days-of-ml/sample_submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-08-29T14:36:27.615203Z","iopub.execute_input":"2021-08-29T14:36:27.615709Z","iopub.status.idle":"2021-08-29T14:36:29.995553Z","shell.execute_reply.started":"2021-08-29T14:36:27.61566Z","shell.execute_reply":"2021-08-29T14:36:29.994653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Extract list of features except id, target and kfold\nuseful_features = [c for c in df.columns if c not in (\"id\", \"target\", \"kfold\")]\n#Extract the list of categorical columns\nobject_cols = [col for col in useful_features if 'cat' in col]\n#Extract the list of numerical columns\nnumerical_cols = [col for col in useful_features if 'cont' in col]\n#Apply to test dataframe\ndf_test = df_test[useful_features]","metadata":{"execution":{"iopub.status.busy":"2021-08-29T14:36:30.003884Z","iopub.execute_input":"2021-08-29T14:36:30.004232Z","iopub.status.idle":"2021-08-29T14:36:30.047535Z","shell.execute_reply.started":"2021-08-29T14:36:30.004203Z","shell.execute_reply":"2021-08-29T14:36:30.04666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Generating the first prediction results\n#initialization\nfinal_test_predictions = []\nfinal_valid_predictions = {}\nscores = []","metadata":{"execution":{"iopub.status.busy":"2021-08-29T14:36:30.048715Z","iopub.execute_input":"2021-08-29T14:36:30.048984Z","iopub.status.idle":"2021-08-29T14:36:30.055033Z","shell.execute_reply.started":"2021-08-29T14:36:30.048959Z","shell.execute_reply":"2021-08-29T14:36:30.054063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Prep 1 Tuned parameters using optuna\nfor fold in range(num_of_folds):\n    xtrain =  df[df.kfold != fold].reset_index(drop=True)\n    xvalid = df[df.kfold == fold].reset_index(drop=True)\n    xtest = df_test.copy()\n    \n    valid_ids = xvalid.id.values.tolist()\n\n    ytrain = xtrain.target\n    yvalid = xvalid.target\n    \n    xtrain = xtrain[useful_features]\n    xvalid = xvalid[useful_features]\n    #Categorical Features ordinal approach\n    ordinal_encoder = OrdinalEncoder()\n    xtrain[object_cols] = ordinal_encoder.fit_transform(xtrain[object_cols])\n    xvalid[object_cols] = ordinal_encoder.transform(xvalid[object_cols])\n    xtest[object_cols] = ordinal_encoder.transform(xtest[object_cols])\n    \n    #standardization approach to numerical columns\n    num_scaler = StandardScaler()\n    xtrain[numerical_cols] = num_scaler.fit_transform(xtrain[numerical_cols])\n    xvalid[numerical_cols] = num_scaler.transform(xvalid[numerical_cols])\n    xtest[numerical_cols] = num_scaler.transform(xtest[numerical_cols])\n    #Best tuned parameters based on fold 0\n    lgbm_params = {\n        'n_estimators': 11692, \n        'max_depth': 4, \n        'learning_rate': 0.014266620585256642, \n        'num_leaves': 271, \n        'feature_fraction': 0.15945513102315187, \n        'bagging_fraction': 0.4728139218848143, \n        'bagging_freq': 6, \n        'max_bin': 2787, \n        'subsample': 1.0, \n        'colsample_bytree': 0.6, \n        'reg_alpha': 24.800000000000004, \n        'reg_lambda': 52.300000000000004, \n        'min_data_in_leaf': 10, \n        'min_child_samples': 73\n    }\n    \n    LGBM_model_1 = LGBMRegressor(**lgbm_params, random_state=42, n_jobs=-1)\n    LGBM_model_1.fit(xtrain, ytrain, early_stopping_rounds=300, eval_set=[(xvalid, yvalid)], verbose=False)\n    preds_valid = LGBM_model_1.predict(xvalid)\n    test_preds = LGBM_model_1.predict(xtest)\n    final_test_predictions.append(test_preds)\n    final_valid_predictions.update(dict(zip(valid_ids, preds_valid)))\n    rmse = mean_squared_error(yvalid, preds_valid, squared=False)\n    print(\"###1### fold = \", fold,\" RMSE = \", rmse)\n    scores.append(rmse)\n\nprint(\"Mean RMSE = \", my_np.mean(scores),\" Standard deviation = \", my_np.std(scores))","metadata":{"execution":{"iopub.status.busy":"2021-08-29T14:36:30.057966Z","iopub.execute_input":"2021-08-29T14:36:30.058404Z","iopub.status.idle":"2021-08-29T14:36:34.862879Z","shell.execute_reply.started":"2021-08-29T14:36:30.05836Z","shell.execute_reply":"2021-08-29T14:36:34.859478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_valid_predictions = my_pd.DataFrame.from_dict(final_valid_predictions, orient=\"index\").reset_index()\nfinal_valid_predictions.columns = [\"id\", \"pred_1\"]\nfinal_valid_predictions.to_csv(\"train_pred_1.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-08-29T14:36:34.863981Z","iopub.status.idle":"2021-08-29T14:36:34.864616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission.target = my_np.mean(my_np.column_stack(final_test_predictions), axis=1)\nsample_submission.columns = [\"id\", \"pred_1\"]\nsample_submission.to_csv(\"test_pred_1.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-08-29T14:36:34.865638Z","iopub.status.idle":"2021-08-29T14:36:34.866254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"########for prep 2 level 0 setup###############\ndf = my_pd.read_csv(\"../input/30days-folds/train_folds.csv\")\ndf_test = my_pd.read_csv(\"../input/30-days-of-ml/test.csv\")\nsample_submission = my_pd.read_csv(\"../input/30-days-of-ml/sample_submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-08-29T14:36:34.867193Z","iopub.status.idle":"2021-08-29T14:36:34.86785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Select all except id, target and  kfold columns\nuseful_features = [c for c in df.columns if c not in (\"id\", \"target\", \"kfold\")]\nobject_cols = [col for col in useful_features if 'cat' in col]\nnumerical_cols = [col for col in useful_features if col.startswith(\"cont\")]\ndf_test = df_test[useful_features]","metadata":{"execution":{"iopub.status.busy":"2021-08-29T14:36:34.868782Z","iopub.status.idle":"2021-08-29T14:36:34.869402Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#creating polynomial features from numerical features\npoly = PolynomialFeatures(degree=3, interaction_only=True, include_bias=False)\ntrain_poly = poly.fit_transform(df[numerical_cols])\ntest_poly = poly.fit_transform(df_test[numerical_cols])\n\ndf_poly = my_pd.DataFrame(train_poly, columns=[f\"poly_{i}\" for i in range(train_poly.shape[1])])\ndf_test_poly = my_pd.DataFrame(test_poly, columns=[f\"poly_{i}\" for i in range(test_poly.shape[1])])\n\ndf = my_pd.concat([df, df_poly], axis=1)\ndf_test = my_pd.concat([df_test, df_test_poly], axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-08-29T14:36:34.870421Z","iopub.status.idle":"2021-08-29T14:36:34.871099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Reselect to include the generated polynomial features \nuseful_features = [c for c in df.columns if c not in (\"id\", \"target\", \"kfold\")]\nobject_cols = [col for col in useful_features if 'cat' in col]\ndf_test = df_test[useful_features]","metadata":{"execution":{"iopub.status.busy":"2021-08-29T14:36:34.872062Z","iopub.status.idle":"2021-08-29T14:36:34.872744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#initialization...\nfinal_test_predictions = []\nfinal_valid_predictions = {}\nscores = []","metadata":{"execution":{"iopub.status.busy":"2021-08-29T14:36:34.873679Z","iopub.status.idle":"2021-08-29T14:36:34.874269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Prep 2 Tuned parameters using optuna\nfor fold in range(num_of_folds):\n    xtrain =  df[df.kfold != fold].reset_index(drop=True)\n    xvalid = df[df.kfold == fold].reset_index(drop=True)\n    xtest = df_test.copy()\n    \n    valid_ids = xvalid.id.values.tolist()\n\n    ytrain = xtrain.target\n    yvalid = xvalid.target\n    \n    xtrain = xtrain[useful_features]\n    xvalid = xvalid[useful_features]\n    #Ordinal encoding for categorical features\n    ordinal_encoder = OrdinalEncoder()\n    xtrain[object_cols] = ordinal_encoder.fit_transform(xtrain[object_cols])\n    xvalid[object_cols] = ordinal_encoder.transform(xvalid[object_cols])\n    xtest[object_cols] = ordinal_encoder.transform(xtest[object_cols])\n\n    #Best tuned parameters based on fold 0\n    lgbm_params = {\n        'n_estimators': 8843, \n        'max_depth': 3, \n        'learning_rate': 0.04029137517665557, \n        'num_leaves': 251, \n        'feature_fraction': 0.9196251401488119, \n        'bagging_fraction': 0.9993541403213315, \n        'bagging_freq': 14, \n        'max_bin': 2795, \n        'subsample': 0.1, \n        'colsample_bytree': 0.2, \n        'reg_alpha': 67.6, \n        'reg_lambda': 106.6, \n        'min_data_in_leaf': 106, \n        'min_child_samples': 59\n    }\n    \n    LGBM_model_2 = LGBMRegressor(**lgbm_params, random_state=42, n_jobs=-1)\n    LGBM_model_2.fit(xtrain, ytrain, early_stopping_rounds=300, eval_set=[(xvalid, yvalid)], verbose=False)\n    preds_valid = LGBM_model_2.predict(xvalid)\n    test_preds = LGBM_model_2.predict(xtest)\n    final_test_predictions.append(test_preds)\n    final_valid_predictions.update(dict(zip(valid_ids, preds_valid)))\n    rmse = mean_squared_error(yvalid, preds_valid, squared=False)\n    print(\"###2### fold = \", fold,\" RMSE = \", rmse)\n    scores.append(rmse)\n\nprint(\"Mean RMSE = \", my_np.mean(scores),\" Standard deviation = \", my_np.std(scores))","metadata":{"execution":{"iopub.status.busy":"2021-08-29T14:36:34.875213Z","iopub.status.idle":"2021-08-29T14:36:34.875868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_valid_predictions = my_pd.DataFrame.from_dict(final_valid_predictions, orient=\"index\").reset_index()\nfinal_valid_predictions.columns = [\"id\", \"pred_2\"]\nfinal_valid_predictions.to_csv(\"train_pred_2.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-08-29T14:36:34.876784Z","iopub.status.idle":"2021-08-29T14:36:34.877369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission.target = my_np.mean(my_np.column_stack(final_test_predictions), axis=1)\nsample_submission.columns = [\"id\", \"pred_2\"]\nsample_submission.to_csv(\"test_pred_2.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-08-29T14:36:34.878294Z","iopub.status.idle":"2021-08-29T14:36:34.878894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"########for prep 3 level 0 setup###############\ndf = my_pd.read_csv(\"../input/30days-folds/train_folds.csv\")\ndf_test = my_pd.read_csv(\"../input/30-days-of-ml/test.csv\")\nsample_submission = my_pd.read_csv(\"../input/30-days-of-ml/sample_submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-08-29T14:36:34.88003Z","iopub.status.idle":"2021-08-29T14:36:34.880805Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Target encoding approach with additive smoothing for categorical data\nfor col in object_cols:\n    temp_df = []\n    temp_test_feat = None\n    for fold in range(5):\n        #print(\"*****************Fold = \", fold)\n        xtrain =  df[df.kfold != fold].reset_index(drop=True)\n        xvalid = df[df.kfold == fold].reset_index(drop=True)\n        #feat = xtrain.groupby(col)[\"target\"].agg(\"median\")\n        # Compute the global mean\n        mean = df[\"target\"].mean()\n        #print(\"Global mean = \", mean)\n        my_agg = xtrain.groupby(col)[\"target\"].agg([\"count\", \"mean\"])\n        #print(\"my_agg :\", my_agg)\n        counts = my_agg[\"count\"]\n        #print(\"counts :\", counts)\n        means = my_agg[\"mean\"]\n        #print(\"means :\", means)\n        feat = (counts * means + 300 * mean) / (counts + 300)\n        #print(\"Feat :\", feat)\n        feat = feat.to_dict()\n        xvalid.loc[:, f\"tar_enc_{col}\"] = xvalid[col].map(feat)\n        temp_df.append(xvalid)\n        if temp_test_feat is None:\n            temp_test_feat = df_test[col].map(feat)\n        else:\n            temp_test_feat += df_test[col].map(feat)\n    \n    temp_test_feat /= 5\n    df_test.loc[:, f\"tar_enc_{col}\"] = temp_test_feat\n    df = my_pd.concat(temp_df)","metadata":{"execution":{"iopub.status.busy":"2021-08-29T14:36:34.881987Z","iopub.status.idle":"2021-08-29T14:36:34.882628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"useful_features = [c for c in df.columns if c not in (\"id\", \"target\", \"kfold\")]\n#make sure include the target encoding feature created\nobject_cols = [col for col in useful_features if col.startswith(\"cat\")]\nnumerical_cols = [col for col in useful_features if col.startswith(\"cont\")]\ndf_test = df_test[useful_features]","metadata":{"execution":{"iopub.status.busy":"2021-08-29T14:36:34.883556Z","iopub.status.idle":"2021-08-29T14:36:34.884174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#initialization\nfinal_test_predictions = []\nfinal_valid_predictions = {}\nscores = []","metadata":{"execution":{"iopub.status.busy":"2021-08-29T14:36:34.885483Z","iopub.status.idle":"2021-08-29T14:36:34.886126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Prep 3 Tuned parameters using optuna\nfor fold in range(num_of_folds):\n    xtrain =  df[df.kfold != fold].reset_index(drop=True)\n    xvalid = df[df.kfold == fold].reset_index(drop=True)\n    xtest = df_test.copy()\n    \n    valid_ids = xvalid.id.values.tolist()\n\n    ytrain = xtrain.target\n    yvalid = xvalid.target\n    \n    xtrain = xtrain[useful_features]\n    xvalid = xvalid[useful_features]\n    #One hot encoding for categorical features\n    OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n    OH_cols_train = my_pd.DataFrame(OH_encoder.fit_transform(xtrain[object_cols]))\n    OH_cols_valid = my_pd.DataFrame(OH_encoder.transform(xvalid[object_cols]))\n    OH_cols_test = my_pd.DataFrame(OH_encoder.transform(xtest[object_cols]))\n    \n    # One-hot encoding removed index; put it back\n    OH_cols_train.index = xtrain.index\n    OH_cols_valid.index = xvalid.index\n    OH_cols_test.index = xtest.index\n     \n    # Remove categorical columns (will replace with one-hot encoding)\n    num_X_train = xtrain.drop(object_cols, axis=1)\n    num_X_valid = xvalid.drop(object_cols, axis=1)\n    num_X_test = xtest.drop(object_cols, axis=1)\n\n    # Add one-hot encoded columns to numerical features\n    OH_X_train = my_pd.concat([num_X_train, OH_cols_train], axis=1)\n    OH_X_valid = my_pd.concat([num_X_valid, OH_cols_valid], axis=1)\n    OH_X_test = my_pd.concat([num_X_test, OH_cols_test], axis=1)\n\n   ##########Numerical##########\n    #standardization approach to numerical columns\n    num_scaler = StandardScaler()\n    OH_X_train[numerical_cols] = num_scaler.fit_transform(OH_X_train[numerical_cols])\n    OH_X_valid[numerical_cols] = num_scaler.transform(OH_X_valid[numerical_cols])\n    OH_X_test[numerical_cols] = num_scaler.transform(OH_X_test[numerical_cols])\n    \n    #Best tuned parameters based on fold 0\n    lgbm_params = {\n        'n_estimators': 3929, \n        'max_depth': 3, \n        'learning_rate': 0.07366853778420801, \n        'num_leaves': 153, \n        'feature_fraction': 0.11974384505538949, \n        'bagging_fraction': 0.6957549032324082, \n        'bagging_freq': 11, \n        'max_bin': 4479, \n        'subsample': 0.30000000000000004, \n        'colsample_bytree': 0.30000000000000004, \n        'reg_alpha': 20.500000000000004, \n        'reg_lambda': 92.7, \n        'min_data_in_leaf': 105, \n        'min_child_samples': 135\n    }\n    \n    LGBM_model_3 = LGBMRegressor(**lgbm_params, random_state=42, n_jobs=-1)\n    LGBM_model_3.fit(OH_X_train, ytrain, early_stopping_rounds=300, eval_set=[(OH_X_valid, yvalid)], verbose=False)\n    preds_valid = LGBM_model_3.predict(OH_X_valid)\n    test_preds = LGBM_model_3.predict(OH_X_test)\n    final_test_predictions.append(test_preds)\n    final_valid_predictions.update(dict(zip(valid_ids, preds_valid)))\n    rmse = mean_squared_error(yvalid, preds_valid, squared=False)\n    print(\"###3### fold = \", fold,\" RMSE = \", rmse)\n    scores.append(rmse)\n\nprint(\"Mean RMSE = \", my_np.mean(scores),\" Standard deviation = \", my_np.std(scores))","metadata":{"execution":{"iopub.status.busy":"2021-08-29T14:36:34.887065Z","iopub.status.idle":"2021-08-29T14:36:34.887662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_valid_predictions = my_pd.DataFrame.from_dict(final_valid_predictions, orient=\"index\").reset_index()\nfinal_valid_predictions.columns = [\"id\", \"pred_3\"]\nfinal_valid_predictions.to_csv(\"train_pred_3.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-08-29T14:36:34.888557Z","iopub.status.idle":"2021-08-29T14:36:34.889176Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission.target = my_np.mean(my_np.column_stack(final_test_predictions), axis=1)\nsample_submission.columns = [\"id\", \"pred_3\"]\nsample_submission.to_csv(\"test_pred_3.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-08-29T14:36:34.890088Z","iopub.status.idle":"2021-08-29T14:36:34.890703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"########for prep 4 level 0 setup###############\ndf = my_pd.read_csv(\"../input/30days-folds/train_folds.csv\")\ndf_test = my_pd.read_csv(\"../input/30-days-of-ml/test.csv\")\nsample_submission = my_pd.read_csv(\"../input/30-days-of-ml/sample_submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-08-29T14:36:34.891639Z","iopub.status.idle":"2021-08-29T14:36:34.892272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Extract categorical and numerical features\nuseful_features = [c for c in df.columns if c not in (\"id\", \"target\", \"kfold\")]\nobject_cols = [col for col in useful_features if 'cat' in col]\nnumerical_cols = [col for col in useful_features if col.startswith(\"cont\")]\ndf_test = df_test[useful_features]","metadata":{"execution":{"iopub.status.busy":"2021-08-29T14:36:34.893186Z","iopub.status.idle":"2021-08-29T14:36:34.893849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Initialization\nfinal_test_predictions = []\nfinal_valid_predictions = {}\nscores = []","metadata":{"execution":{"iopub.status.busy":"2021-08-29T14:36:34.894782Z","iopub.status.idle":"2021-08-29T14:36:34.895401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Pred 4 Tuned parameters using optuna\nfor fold in range(num_of_folds):\n    xtrain =  df[df.kfold != fold].reset_index(drop=True)\n    xvalid = df[df.kfold == fold].reset_index(drop=True)\n    xtest = df_test.copy()\n    \n    valid_ids = xvalid.id.values.tolist()\n\n    ytrain = xtrain.target\n    yvalid = xvalid.target\n    \n    xtrain = xtrain[useful_features]\n    xvalid = xvalid[useful_features]\n    #Categorical Features ordinal approach\n    ordinal_encoder = OrdinalEncoder()\n    xtrain[object_cols] = ordinal_encoder.fit_transform(xtrain[object_cols])\n    xvalid[object_cols] = ordinal_encoder.transform(xvalid[object_cols])\n    xtest[object_cols] = ordinal_encoder.transform(xtest[object_cols])\n    \n    #Log transformation to numerical columns\n    logtransformer = FunctionTransformer(my_np.log1p)\n    xtrain[numerical_cols] = logtransformer.fit_transform(xtrain[numerical_cols])\n    xvalid[numerical_cols] = logtransformer.transform(xvalid[numerical_cols])\n    xtest[numerical_cols] = logtransformer.transform(xtest[numerical_cols])\n    \n    #Best tuned parameters based on fold 1\n    lgbm_params = {\n        'n_estimators': 3625, \n        'max_depth': 11, \n        'learning_rate': 0.034230498273737156, \n        'num_leaves': 21, \n        'feature_fraction': 0.13619234154861645, \n        'bagging_fraction': 0.6606506089603802, \n        'bagging_freq': 12, \n        'max_bin': 2498, \n        'subsample': 0.6, \n        'colsample_bytree': 0.2, \n        'reg_alpha': 24.500000000000004, \n        'reg_lambda': 12.6, \n        'min_data_in_leaf': 113, \n        'min_child_samples': 136\n    }\n    \n    LGBM_model_4 = LGBMRegressor(**lgbm_params, random_state=42, n_jobs=-1)\n    LGBM_model_4.fit(xtrain, ytrain, early_stopping_rounds=300, eval_set=[(xvalid, yvalid)], verbose=False)\n    preds_valid = LGBM_model_4.predict(xvalid)\n    test_preds = LGBM_model_4.predict(xtest)\n    final_test_predictions.append(test_preds)\n    final_valid_predictions.update(dict(zip(valid_ids, preds_valid)))\n    rmse = mean_squared_error(yvalid, preds_valid, squared=False)\n    print(\"###4### fold = \", fold,\" RMSE = \", rmse)\n    scores.append(rmse)\n\nprint(\"Mean RMSE = \", my_np.mean(scores),\" Standard deviation = \", my_np.std(scores))\n\nfinal_valid_predictions = my_pd.DataFrame.from_dict(final_valid_predictions, orient=\"index\").reset_index()\nfinal_valid_predictions.columns = [\"id\", \"pred_4\"]\nfinal_valid_predictions.to_csv(\"train_pred_4.csv\", index=False)\n\nsample_submission.target = my_np.mean(my_np.column_stack(final_test_predictions), axis=1)\nsample_submission.columns = [\"id\", \"pred_4\"]\nsample_submission.to_csv(\"test_pred_4.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-08-29T14:36:34.896301Z","iopub.status.idle":"2021-08-29T14:36:34.896896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"########for prep 5 level 0 setup###############\ndf = my_pd.read_csv(\"../input/30days-folds/train_folds.csv\")\ndf_test = my_pd.read_csv(\"../input/30-days-of-ml/test.csv\")\nsample_submission = my_pd.read_csv(\"../input/30-days-of-ml/sample_submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-08-30T12:43:01.370071Z","iopub.execute_input":"2021-08-30T12:43:01.37045Z","iopub.status.idle":"2021-08-30T12:43:05.719479Z","shell.execute_reply.started":"2021-08-30T12:43:01.370421Z","shell.execute_reply":"2021-08-30T12:43:05.718381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Extract list of features except id, target and kfold\nuseful_features = [c for c in df.columns if c not in (\"id\", \"target\", \"kfold\")]\n#Extract the list of categorical columns\nobject_cols = [col for col in useful_features if 'cat' in col]\n#Extract the list of numerical columns\nnumerical_cols = [col for col in useful_features if 'cont' in col]\n#Apply to test dataframe\ndf_test = df_test[useful_features]","metadata":{"execution":{"iopub.status.busy":"2021-08-30T12:43:13.908545Z","iopub.execute_input":"2021-08-30T12:43:13.908915Z","iopub.status.idle":"2021-08-30T12:43:13.952949Z","shell.execute_reply.started":"2021-08-30T12:43:13.908879Z","shell.execute_reply":"2021-08-30T12:43:13.952067Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Initialization\nfinal_test_predictions = []\nfinal_valid_predictions = {}\nscores = []","metadata":{"execution":{"iopub.status.busy":"2021-08-30T12:43:19.936722Z","iopub.execute_input":"2021-08-30T12:43:19.937358Z","iopub.status.idle":"2021-08-30T12:43:19.941856Z","shell.execute_reply.started":"2021-08-30T12:43:19.93731Z","shell.execute_reply":"2021-08-30T12:43:19.940974Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Pred 5 Tuned parameters\nfor fold in range(num_of_folds):\n    xtrain =  df[df.kfold != fold].reset_index(drop=True)\n    xvalid = df[df.kfold == fold].reset_index(drop=True)\n    xtest = df_test.copy()\n    \n    valid_ids = xvalid.id.values.tolist()\n\n    ytrain = xtrain.target\n    yvalid = xvalid.target\n    \n    xtrain = xtrain[useful_features]\n    xvalid = xvalid[useful_features]\n    #Categorical Features ordinal approach\n    #Catboost Encoder Categorical columns\n    CBE_encoder = CatBoostEncoder()\n    xtrain[object_cols] = CBE_encoder.fit_transform(xtrain[object_cols], ytrain)\n    xvalid[object_cols] = CBE_encoder.transform(xvalid[object_cols])\n    xtest[object_cols] = CBE_encoder.transform(xtest[object_cols]) \n    \n    #standardization approach to numerical columns\n    num_scaler = StandardScaler()\n    xtrain[numerical_cols] = num_scaler.fit_transform(xtrain[numerical_cols])\n    xvalid[numerical_cols] = num_scaler.transform(xvalid[numerical_cols])\n    xtest[numerical_cols] = num_scaler.transform(xtest[numerical_cols])\n    #Best tuned parameters based on fold 1\n    lgbm_params = {\n        'n_estimators': 14625, \n        'max_depth': 9, \n        'learning_rate': 0.01168710233322345, \n        'num_leaves': 10, \n        'feature_fraction': 0.32238328745223305, \n        'bagging_fraction': 0.7090167545870307, \n        'bagging_freq': 4, \n        'max_bin': 2065, \n        'subsample': 0.4, \n        'colsample_bytree': 1.0, \n        'reg_alpha': 17.6, \n        'reg_lambda': 53.400000000000006, \n        'min_data_in_leaf': 78, \n        'min_child_samples': 110\n    }\n    \n    LGBM_model_5 = LGBMRegressor(**lgbm_params, random_state=42, n_jobs=-1)\n    LGBM_model_5.fit(xtrain, ytrain, early_stopping_rounds=300, eval_set=[(xvalid, yvalid)], verbose=False)\n    preds_valid = LGBM_model_5.predict(xvalid)\n    test_preds = LGBM_model_5.predict(xtest)\n    final_test_predictions.append(test_preds)\n    final_valid_predictions.update(dict(zip(valid_ids, preds_valid)))\n    rmse = mean_squared_error(yvalid, preds_valid, squared=False)\n    print(\"###5### fold = \", fold,\" RMSE = \", rmse)\n    scores.append(rmse)\n\nprint(\"Mean RMSE = \", my_np.mean(scores),\" Standard deviation = \", my_np.std(scores))","metadata":{"execution":{"iopub.status.busy":"2021-08-30T12:43:43.155623Z","iopub.execute_input":"2021-08-30T12:43:43.156227Z","iopub.status.idle":"2021-08-30T13:00:05.939265Z","shell.execute_reply.started":"2021-08-30T12:43:43.156184Z","shell.execute_reply":"2021-08-30T13:00:05.938134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_valid_predictions = my_pd.DataFrame.from_dict(final_valid_predictions, orient=\"index\").reset_index()\nfinal_valid_predictions.columns = [\"id\", \"pred_5\"]\nfinal_valid_predictions.to_csv(\"train_pred_5.csv\", index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission.target = my_np.mean(my_np.column_stack(final_test_predictions), axis=1)\nsample_submission.columns = [\"id\", \"pred_5\"]\nsample_submission.to_csv(\"test_pred_5.csv\", index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Reinitialize the dataframe for predicted results merging\ndf = my_pd.read_csv(\"../input/30days-folds/train_folds.csv\")\ndf_test = my_pd.read_csv(\"../input/30-days-of-ml/test.csv\")\nsample_submission = my_pd.read_csv(\"../input/30-days-of-ml/sample_submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-08-29T14:36:34.897744Z","iopub.status.idle":"2021-08-29T14:36:34.898331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Prep for level 0 merging of predicted data\ndf1 = my_pd.read_csv(\"train_pred_1.csv\")\ndf2 = my_pd.read_csv(\"train_pred_2.csv\")\ndf3 = my_pd.read_csv(\"train_pred_3.csv\")\ndf4 = my_pd.read_csv(\"train_pred_4.csv\")\ndf5 = my_pd.read_csv(\"train_pred_5.csv\")\n\ndf_test1 = my_pd.read_csv(\"test_pred_1.csv\")\ndf_test2 = my_pd.read_csv(\"test_pred_2.csv\")\ndf_test3 = my_pd.read_csv(\"test_pred_3.csv\")\ndf_test4 = my_pd.read_csv(\"test_pred_4.csv\")\ndf_test5 = my_pd.read_csv(\"test_pred_5.csv\")\n\ndf = df.merge(df1, on=\"id\", how=\"left\")\ndf = df.merge(df2, on=\"id\", how=\"left\")\ndf = df.merge(df3, on=\"id\", how=\"left\")\ndf = df.merge(df4, on=\"id\", how=\"left\")\ndf = df.merge(df5, on=\"id\", how=\"left\")\n\ndf_test = df_test.merge(df_test1, on=\"id\", how=\"left\")\ndf_test = df_test.merge(df_test2, on=\"id\", how=\"left\")\ndf_test = df_test.merge(df_test3, on=\"id\", how=\"left\")\ndf_test = df_test.merge(df_test4, on=\"id\", how=\"left\")\ndf_test = df_test.merge(df_test5, on=\"id\", how=\"left\")","metadata":{"execution":{"iopub.status.busy":"2021-08-29T14:36:34.89919Z","iopub.status.idle":"2021-08-29T14:36:34.899776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Extract the level 0 predicted results cols.\nuseful_features = [\"pred_1\", \"pred_2\", \"pred_3\", \"pred_4\", \"pred_5\"]\ndf_test = df_test[useful_features]\nprint(df.head())\nprint(df_test.head())","metadata":{"execution":{"iopub.status.busy":"2021-08-29T14:36:34.900594Z","iopub.status.idle":"2021-08-29T14:36:34.901178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#initialization for level 1 prediction results\nfinal_test_predictions = []\nfinal_valid_predictions = {}\nscores = []","metadata":{"execution":{"iopub.status.busy":"2021-08-29T14:36:34.902046Z","iopub.status.idle":"2021-08-29T14:36:34.902601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Level 1 Random Forest Pred 1 with tuned parameters\nfor fold in range(num_of_folds):\n    xtrain =  df[df.kfold != fold].reset_index(drop=True)\n    xvalid = df[df.kfold == fold].reset_index(drop=True)\n    xtest = df_test.copy()\n    \n    valid_ids = xvalid.id.values.tolist()\n\n    ytrain = xtrain.target\n    yvalid = xvalid.target\n    \n    xtrain = xtrain[useful_features]\n    xvalid = xvalid[useful_features]\n    \n    #Best tuned parameters based on fold 0\n    rf_params = { 'n_estimators': 600, 'max_depth': 5, 'max_features': 0.5700919740571448, 'min_samples_split': 15, 'min_samples_leaf': 15 }\n    \n    RF_model = RandomForestRegressor(**rf_params, random_state=42, n_jobs=-1)\n    RF_model.fit(xtrain, ytrain)\n    preds_valid = RF_model.predict(xvalid)\n    test_preds = RF_model.predict(xtest)\n    final_test_predictions.append(test_preds)\n    final_valid_predictions.update(dict(zip(valid_ids, preds_valid)))\n    rmse = mean_squared_error(yvalid, preds_valid, squared=False)\n    print(\"###RF### fold = \", fold,\" RMSE = \", rmse)\n    scores.append(rmse)\n#Save to files\nprint(\"Mean RMSE = \", my_np.mean(scores),\" Standard deviation = \", my_np.std(scores))\nfinal_valid_predictions = my_pd.DataFrame.from_dict(final_valid_predictions, orient=\"index\").reset_index()\nfinal_valid_predictions.columns = [\"id\", \"lvl1pred_1\"]\nfinal_valid_predictions.to_csv(\"lvl1_train_pred_1.csv\", index=False)\n\nsample_submission.target = my_np.mean(my_np.column_stack(final_test_predictions), axis=1)\nsample_submission.columns = [\"id\", \"lvl1pred_1\"]\nsample_submission.to_csv(\"lvl1_test_pred_1.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-08-29T14:36:34.903477Z","iopub.status.idle":"2021-08-29T14:36:34.904088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Reinitialization for XBG model\nfinal_test_predictions = []\nfinal_valid_predictions = {}\nscores = []\nsample_submission = my_pd.read_csv(\"../input/30-days-of-ml/sample_submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-08-29T23:30:13.655788Z","iopub.execute_input":"2021-08-29T23:30:13.656077Z","iopub.status.idle":"2021-08-29T23:30:13.74423Z","shell.execute_reply.started":"2021-08-29T23:30:13.656036Z","shell.execute_reply":"2021-08-29T23:30:13.743381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Level 1 XGB Pred 2 with tuned parameters\nfor fold in range(num_of_folds):\n    xtrain =  df[df.kfold != fold].reset_index(drop=True)\n    xvalid = df[df.kfold == fold].reset_index(drop=True)\n    xtest = df_test.copy()\n    \n    valid_ids = xvalid.id.values.tolist()\n\n    ytrain = xtrain.target\n    yvalid = xvalid.target\n    \n    xtrain = xtrain[useful_features]\n    xvalid = xvalid[useful_features]\n    \n    #Best tuned parameters based on fold 0\n    xgb_params = {\n        'n_estimators': 3630, \n        'max_depth': 2, \n        'learning_rate': 0.04303064869486619, \n        'colsample_bytree': 0.677745729203771, \n        'subsample': 0.2482377859197017, \n        'alpha': 0.019836014804809354, \n        'lambda': 1.1315039974499819e-08, \n        'min_child_weight': 692.2453262362711\n    }\n    \n    XGB_model = XGBRegressor(**xgb_params, random_state=42, n_jobs=-1)\n    XGB_model.fit(xtrain, ytrain, early_stopping_rounds=200, eval_set=[(xvalid, yvalid)], verbose=False)\n    preds_valid = XGB_model.predict(xvalid)\n    test_preds = XGB_model.predict(xtest)\n    final_test_predictions.append(test_preds)\n    final_valid_predictions.update(dict(zip(valid_ids, preds_valid)))\n    rmse = mean_squared_error(yvalid, preds_valid, squared=False)\n    print(\"###XGB### fold = \", fold,\" RMSE = \", rmse)\n    scores.append(rmse)\n\nprint(\"Mean RMSE = \", my_np.mean(scores),\" Standard deviation = \", my_np.std(scores))\nfinal_valid_predictions = my_pd.DataFrame.from_dict(final_valid_predictions, orient=\"index\").reset_index()\nfinal_valid_predictions.columns = [\"id\", \"lvl1pred_2\"]\nfinal_valid_predictions.to_csv(\"lvl1_train_pred_2.csv\", index=False)\n\nsample_submission.target = my_np.mean(my_np.column_stack(final_test_predictions), axis=1)\nsample_submission.columns = [\"id\", \"lvl1pred_2\"]\nsample_submission.to_csv(\"lvl1_test_pred_2.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-08-29T14:36:34.906401Z","iopub.status.idle":"2021-08-29T14:36:34.906955Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Reinitialization for LightGB model\nfinal_test_predictions = []\nfinal_valid_predictions = {}\nscores = []\nsample_submission = my_pd.read_csv(\"../input/30-days-of-ml/sample_submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-08-29T23:30:21.329579Z","iopub.execute_input":"2021-08-29T23:30:21.330027Z","iopub.status.idle":"2021-08-29T23:30:21.389566Z","shell.execute_reply.started":"2021-08-29T23:30:21.329997Z","shell.execute_reply":"2021-08-29T23:30:21.388703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Level 1 LightGB Pred 3 with tuned parameters\nfor fold in range(num_of_folds):\n    xtrain =  df[df.kfold != fold].reset_index(drop=True)\n    xvalid = df[df.kfold == fold].reset_index(drop=True)\n    xtest = df_test.copy()\n    \n    valid_ids = xvalid.id.values.tolist()\n\n    ytrain = xtrain.target\n    yvalid = xvalid.target\n    \n    xtrain = xtrain[useful_features]\n    xvalid = xvalid[useful_features]\n\n    #Best tuned parameters based on fold 0\n    lgbm_params = {\n        'n_estimators': 13103, \n        'max_depth': 1, \n        'learning_rate': 0.01007362838130684, \n        'num_leaves': 46, \n        'feature_fraction': 0.1011590209710771, \n        'bagging_fraction': 0.8409771566248516, \n        'bagging_freq': 1, \n        'max_bin': 954, \n        'subsample': 0.1, \n        'colsample_bytree': 1.0, \n        'reg_alpha': 42.900000000000006, \n        'reg_lambda': 133.2, \n        'min_data_in_leaf': 50, \n        'min_child_samples': 20\n    }\n    \n    LGBM_model = LGBMRegressor(**lgbm_params, random_state=42, n_jobs=-1)\n    LGBM_model.fit(xtrain, ytrain, early_stopping_rounds=200, eval_set=[(xvalid, yvalid)], verbose=False)\n    preds_valid = LGBM_model.predict(xvalid)\n    test_preds = LGBM_model.predict(xtest)\n    final_test_predictions.append(test_preds)\n    final_valid_predictions.update(dict(zip(valid_ids, preds_valid)))\n    rmse = mean_squared_error(yvalid, preds_valid, squared=False)\n    print(\"###LGBM### fold = \", fold,\" RMSE = \", rmse)\n    scores.append(rmse)\n\nprint(\"Mean RMSE = \", my_np.mean(scores),\" Standard deviation = \", my_np.std(scores))\n\nfinal_valid_predictions = my_pd.DataFrame.from_dict(final_valid_predictions, orient=\"index\").reset_index()\nfinal_valid_predictions.columns = [\"id\", \"lvl1pred_3\"]\nfinal_valid_predictions.to_csv(\"lvl1_train_pred_3.csv\", index=False)\n\nsample_submission.target = my_np.mean(my_np.column_stack(final_test_predictions), axis=1)\nsample_submission.columns = [\"id\", \"lvl1pred_3\"]\nsample_submission.to_csv(\"lvl1_test_pred_3.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-08-29T14:36:34.909244Z","iopub.status.idle":"2021-08-29T14:36:34.909836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Prep for the predicted level 1 results merging and linear regression\ndf = my_pd.read_csv(\"../input/30days-folds/train_folds.csv\")\ndf_test = my_pd.read_csv(\"../input/30-days-of-ml/test.csv\")\nsample_submission = my_pd.read_csv(\"../input/30-days-of-ml/sample_submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-08-29T23:30:27.757767Z","iopub.execute_input":"2021-08-29T23:30:27.75813Z","iopub.status.idle":"2021-08-29T23:30:32.060125Z","shell.execute_reply.started":"2021-08-29T23:30:27.758099Z","shell.execute_reply":"2021-08-29T23:30:32.05891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1 = my_pd.read_csv(\"lvl1_train_pred_1.csv\")\ndf2 = my_pd.read_csv(\"lvl1_train_pred_2.csv\")\ndf3 = my_pd.read_csv(\"lvl1_train_pred_3.csv\")\n\ndf_test1 = my_pd.read_csv(\"lvl1_test_pred_1.csv\")\ndf_test2 = my_pd.read_csv(\"lvl1_test_pred_2.csv\")\ndf_test3 = my_pd.read_csv(\"lvl1_test_pred_3.csv\")\n\ndf = df.merge(df1, on=\"id\", how=\"left\")\ndf = df.merge(df2, on=\"id\", how=\"left\")\ndf = df.merge(df3, on=\"id\", how=\"left\")\n\ndf_test = df_test.merge(df_test1, on=\"id\", how=\"left\")\ndf_test = df_test.merge(df_test2, on=\"id\", how=\"left\")\ndf_test = df_test.merge(df_test3, on=\"id\", how=\"left\")","metadata":{"execution":{"iopub.status.busy":"2021-08-29T14:36:34.912193Z","iopub.status.idle":"2021-08-29T14:36:34.912813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Extract the level 1 predicted results cols.\nuseful_features = [\"lvl1pred_1\", \"lvl1pred_2\", \"lvl1pred_3\"]\ndf_test = df_test[useful_features]\nprint(df_test.head())","metadata":{"execution":{"iopub.status.busy":"2021-08-29T14:36:34.913684Z","iopub.status.idle":"2021-08-29T14:36:34.914264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Using Ridge Regression model\nfinal_predictions_ridge = []\nscores = []\nfor fold in range(num_of_folds):\n    xtrain =  df[df.kfold != fold].reset_index(drop=True)\n    xvalid = df[df.kfold == fold].reset_index(drop=True)\n    xtest = df_test.copy()\n\n    ytrain = xtrain.target\n    yvalid = xvalid.target\n    \n    xtrain = xtrain[useful_features]\n    xvalid = xvalid[useful_features]\n    #Tuned Alpha parameter\n    model = Ridge(alpha= 3.4583520855110406)\n    model.fit(xtrain, ytrain)\n    \n    preds_valid = model.predict(xvalid)\n    test_preds = model.predict(xtest)\n    final_predictions_ridge.append(test_preds)\n    rmse = mean_squared_error(yvalid, preds_valid, squared=False)\n    print(\"###Ridge### fold = \", fold,\" RMSE = \", rmse)\n    scores.append(rmse)\n\nprint(\"Mean RMSE = \", my_np.mean(scores),\" Standard deviation = \", my_np.std(scores))","metadata":{"execution":{"iopub.status.busy":"2021-08-29T14:36:34.915133Z","iopub.status.idle":"2021-08-29T14:36:34.915842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Prepare the final submission file\nsample_submission.target = my_np.mean(my_np.column_stack(final_predictions_ridge), axis=1)\nsample_submission.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-08-29T14:36:34.916843Z","iopub.status.idle":"2021-08-29T14:36:34.917446Z"},"trusted":true},"execution_count":null,"outputs":[]}]}