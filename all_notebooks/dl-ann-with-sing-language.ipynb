{"cells":[{"metadata":{"_uuid":"e61e7d716ca32a843aa9a45b253191e3e1f65d6d"},"cell_type":"markdown","source":"# Cihan Yatbaz\n###  04 / 12 / 2018\n---\n\n\n1.  [Introduction:](#0)\n2. [Preparing Dataset :](#1)\n3. [Artificial Neural Network (ANN):](#2)\n    1. [Initializing parameters  :](#3)\n    1. [Forward propagation  :](#4)\n    1. [Loss and Cost Function  :](#5)\n    1. [Backward propagation  :](#6)\n    1. [Update parameters  :](#7)\n    1. [Prediction  :](#8)\n    1. [Creat Model  :](#9)\n\n5. [L Layer Neural Network :](#10)\n6. [CONCLUSION :](#11)"},{"metadata":{"_uuid":"e153944aa57b0a1a4f8c8265298c21bcc2eabdc8"},"cell_type":"markdown","source":"<a id=\"0\"></a> <br>\n## 1) Introduction\n* We will be working on this kernel Sign Language data. We'll introduce 80% of the sign language we have, and we will try to predict the remaining 20%.\n* In this Kernel we will do the Artificial Neural Network (ANN) step by step.\n* Let's start by creating our libraries\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode('utf8'))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a2874c4be019c3fe77785cfcf888b1b3556a80de"},"cell_type":"markdown","source":"<a id=\"1\"></a> <br>\n## 2) Preparing Dataset\n---\n* Now we'll upload our library and then let's see the 0 and 1 signs we'll work on."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# Load datasets\nx_data = np.load('../input/Sign-language-digits-dataset/X.npy')\ny_data = np.load('../input/Sign-language-digits-dataset/Y.npy')\nimg_size = 64  # pixel size\n\n # for sign zero\nplt.subplot(1,2,1)  \nplt.imshow(x_data[260])  # Get 260th index\nplt.axis(\"off\")\n\n# for sign one\nplt.subplot(1,2,2)\nplt.imshow(x_data[900])  # Get 900th index\nplt.axis(\"off\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c229c6598f8e306c97629662a1db82b06d513de9"},"cell_type":"markdown","source":"* Now we will concatenate our pictures consisting of 0 and 1.\n* We have image of 255 one sign, 255 zero sign"},{"metadata":{"trusted":true,"_uuid":"94df5df2ac7a32bb9509a8dc19583dbe959bf4b5"},"cell_type":"code","source":"# From 0 to 204 is zero sign, from 205 to 410 is one sign\nX = np.concatenate((x_data[204:409], x_data[822:1027] ), axis=0)\n\n# We will create their labels. After that, we will concatenate on the Y.\nz = np.zeros(205)\no = np.ones(205)\nY = np.concatenate((z,o), axis=0).reshape(X.shape[0],1)\nprint(\"X shape: \", X.shape)\nprint(\"Y shape: \", Y.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0afe952f0f0f2f0261974b431b4cf86f9d5fb9c6"},"cell_type":"markdown","source":"* The shape of the X is (410, 64, 64)\n    * 410 means that we have 410 images (zero and one signs)\n    * 64 means that our image size is 64x64 (64x64 pixels)\n* The shape of the Y is (410,1)\n    * 410 means that we have 410 labels (0 and 1)"},{"metadata":{"_uuid":"f6b9a154ac1095298fa3e84cc35a25913a7acba5"},"cell_type":"markdown","source":"* Now we reserve 80% of the values as 'train' and 20% as 'test'.\n* Then let's create x_train, y_train, x_test, y_test arrays\n\n"},{"metadata":{"trusted":true,"_uuid":"644d40ec00a7f7064f54592b73b4761a8f7f4ce2"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(X,Y, test_size = 0.2, random_state=42)\n# random_state = Use same seed while randomizing\nprint(x_train.shape)\nprint(y_train.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9bccdc766efd1b2df705604d3845cad16582e5be"},"cell_type":"markdown","source":"  * Since our data in X is 3D, we need to flatten it to 2D to use Deep Learning.\n  * Since our data in Y is 2D, we don't need to flatten."},{"metadata":{"trusted":true,"_uuid":"87a89292b7af645c19f9f860a2cc25fb7f2f5693"},"cell_type":"code","source":"x_train_flatten = x_train.reshape(x_train.shape[0], x_train.shape[1] * x_train.shape[2])\nx_test_flatten = x_test.reshape(x_test.shape[0], x_test.shape[1] * x_test.shape[2])\nprint('x_train_flatten: {} \\nx_test_flatten: {} '.format(x_train_flatten.shape, x_test_flatten.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3feb0f9c74f064fda28c964948546f7524c06a9f"},"cell_type":"markdown","source":"* Now x and y 2D\n* 4096 = 64 * 64"},{"metadata":{"trusted":true,"_uuid":"c05672fe82b5f0336dc33b77dadb27c9517789b8"},"cell_type":"code","source":"# Here we will change the location of our samples and features. '(328,4096) -> (4096,328)' \nx_train = x_train_flatten.T\nx_test = x_test_flatten.T\ny_train = y_train.T\ny_test = y_test.T\n\nprint(\"x train: \", x_train.shape)\nprint(\"x test: \", x_test.shape)\nprint(\"y train: \", y_train.shape)\nprint(\"y test: \", y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e54ac09dbe535c4898c0a0ddc5bedaca3384aa78"},"cell_type":"markdown","source":"<a id=\"2\"></a> <br>\n## 3) Artificial Neural Network (ANN)"},{"metadata":{"_uuid":"d5efe47cfef549aa729b247323198a80d8114b4d"},"cell_type":"markdown","source":"<a id=\"3\"></a> <br>\n### A) Initializing parameters"},{"metadata":{"trusted":true,"_uuid":"205bb2cec91750677cc4e7b74ef8343fc7f037f9"},"cell_type":"code","source":"# Now let's create the parameter and sigmoid function. \n# So what we need is dimension 4096 that is number of pixel as a parameter for our initialize method(def)\n\ndef initialize_weights_and_bias(dimension):\n    w = np.full((dimension, 1), 0.01)\n    b = 0.0\n    return w,b\n\n# Sigmoid function\n# z = np.dot(w.T, x_train) +b\ndef sigmoid(z):\n    y_head = 1/(1 + np.exp(-z))  # sigmoid function finding formula\n    return y_head\nsigmoid(0)  # 0 should result in 0.5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"95fb9487503317c4ad84d9acf28d6752bc70899e"},"cell_type":"code","source":"def initialize_parameters_and_layer_sizes_NN(x_train, y_train):\n    parameters = {\n        \"weight1\": np.random.randn(3,x_train.shape[0]) * 0.1, \n# the reason we say 3, 4096: The number of rows in our weight has to be 3 because we have 3 nodes\n        \"bias1\": np.zeros((3,1)), # That is same for bias \n        \"weight2\": np.random.randn(y_train.shape[0], 3) * 0.1,\n        \"bias2\": np.zeros((y_train.shape[0],1))\n    }\n    return parameters","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"695e4e6e7e3368b9a8041b9b6a6cb7cec9a34716"},"cell_type":"markdown","source":"<a id=\"4\"></a> <br>\n### B) Forward propagation"},{"metadata":{"trusted":true,"_uuid":"37387bfea46ea23a9269998abe81434b56c1d32d"},"cell_type":"code","source":"# We only process 2 times because we use tanh function.\ndef forward_NN(x_train, parameters):\n    Z1 = np.dot(parameters[\"weight1\"], x_train) + parameters[\"bias1\"]\n    A1 = np.tanh(Z1)  # We can do this easily with the numpy library\n    Z2 = np.dot(parameters[\"weight2\"],A1) + parameters[\"bias2\"]\n    A2 = sigmoid(Z2)\n    \n    cache = {\n        \"Z1\": Z1,\n        \"A1\": A1,\n        \"Z2\": Z2,\n        \"A2\": A2\n    }\n    return A2, cache","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"96e8cf1df6fba0c0dcfb2950c466610ebd8eba8a"},"cell_type":"markdown","source":"<a id=\"5\"></a> <br>\n### C) Loss and Cost Function"},{"metadata":{"trusted":true,"_uuid":"5eca557861d05245b1cbc131333420de4aebe0b6"},"cell_type":"code","source":"# Compute cost\n# We take A2 as input and use it here\ndef compute_cost_NN(A2, Y, parameters):\n# We multiply the y_head value (A2) with the actual single value (Y).\n    logprobs = np.multiply(np.log(A2),Y)\n# cost : We collect all our losts\n    cost = - np.sum(logprobs) / Y.shape[1]\n    return cost","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ad1a861c5c17d5a8e48c0e3ca777ae52a3f7fac2"},"cell_type":"markdown","source":"<a id=\"6\"></a> <br>\n### D) Backward propagation"},{"metadata":{"trusted":true,"_uuid":"82e0812efe211913fb6dd96a7bc8cb69ae3f6bd5"},"cell_type":"code","source":"def backward_NN(parameters, cache, X, Y):\n    # We are doing derivative transactions\n    dZ2 = cache[\"A2\"] - Y\n    dW2 = np.dot(dZ2, cache[\"A1\"].T) / X.shape[1]\n    # keepdims : He's holding as an Array. We are writing in array, even though the result of our collection is different (consten).\n    db2 = np.sum(dZ2, axis=1, keepdims=True) / X.shape[1]\n    dZ1 = np.dot(parameters[\"weight2\"].T, dZ2) * (1 - np.power(cache[\"A1\"], 2))\n    dW1 = np.dot(dZ1, X.T) / X.shape[1]\n    db1 = np.sum(dZ1, axis=1, keepdims=True) / X.shape[1]\n    # grads : We're storing grads. Changes in Weight1, bias1, Weight2, bias2. We store the derivatives of these according to them.\n    grads = {\n        \"dweight1\": dW1,\n        \"dbias1\": db1,\n        \"dweight2\": dW2,\n        \"dbias2\": db2,\n    }\n    return grads","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"80caabad1b735c735da5062b81718e27aa3068c3"},"cell_type":"markdown","source":"<a id=\"7\"></a> <br>\n### E) Update parameters"},{"metadata":{"trusted":true,"_uuid":"a19d01b577ed87e92e609960c2224a9ac2b166ac"},"cell_type":"code","source":"# Learning_rate: It is a hyper parameter. A parameter we'll find by trying.\ndef update_NN(parameters, grads, learning_rate = 0.003):\n    parameters = {\n        \"weight1\": parameters[\"weight1\"] - learning_rate * grads[\"dweight1\"],\n        \"bias1\": parameters[\"bias1\"] - learning_rate * grads[\"dbias1\"],\n        \"weight2\": parameters[\"weight2\"] - learning_rate * grads[\"dweight2\"],\n        \"bias2\": parameters[\"bias2\"] - learning_rate * grads[\"dbias2\"],\n    }\n    return parameters","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d1c7ea262ec2b2651eb004185e81495838490315"},"cell_type":"markdown","source":"<a id=\"8\"></a> <br>\n### F) Prediction\n---\n* Our trained model is ready. \n* We will prediction our trained model.\n* We will check whether it is."},{"metadata":{"trusted":true,"_uuid":"af4abe7dc1a48e21b9c95d9d7384f843a3d1c086"},"cell_type":"code","source":"def predict_NN(parameters, x_test):\n    #x_test is a input for forward propagation\n    A2, cache = forward_NN(x_test, parameters)\n    Y_prediction = np.zeros((1, x_test.shape[1]))\n    # if z is bigger than 0.5, our predictioan is sign one(y_head=1),\n    # if z is smaller than 0.5, our predictioan is sign zero(y_head=0),    \n    for i in range(A2.shape[1]):\n        if A2[0,i] <= 0.5:\n            Y_prediction[0,i] = 0\n        else:\n            Y_prediction[0,i] = 1\n    return Y_prediction","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f605cd9a9c3e46cf4c4188bfc6e0bf9cc795f95a"},"cell_type":"markdown","source":"<a id=\"9\"></a> <br>\n### G) Creat Model\n---\n* Now we're gonna combine them all."},{"metadata":{"trusted":true,"_uuid":"f8d6c9af4768236ee9277ca119f6c52c73c27fea"},"cell_type":"code","source":"# Layer Neural Network\ndef layer_neural_network(x_train, y_train, x_test, y_test, num_iterations):\n    # We store Cost and Indexes for analysis.\n    cost_list = []\n    index_list = []\n    # initialize parameters and layer sizes\n    # We determine how many nodes in our layer.\n    parameters = initialize_parameters_and_layer_sizes_NN(x_train, y_train)\n    \n    for i in range(0, num_iterations):\n        # forward propagation\n        A2, cache = forward_NN(x_train, parameters)\n        # compute cost\n        cost = compute_cost_NN(A2, y_train, parameters)\n        # backward propagation\n        grads  = backward_NN(parameters, cache, x_train, y_train)\n        # update parameters\n        parameters = update_NN(parameters, grads)\n        \n        # It will store cost in cost_list per hundred steps. Same goes for index.\n        if i % 100 == 0:\n            cost_list.append(cost)\n            index_list.append(i)\n            print(\"Cost after iteration %i: %f\" %(i,cost))\n    \n    plt.plot(index_list, cost_list)\n    plt.xticks(index_list, rotation='vertical')\n    plt.xlabel(\"Number of iteration\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n    \n    # predict\n    y_prediction_test = predict_NN(parameters, x_test)\n    y_prediction_train = predict_NN(parameters, x_train)\n    \n    # Print test/train errors\n    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_train - y_train)) * 100))\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))    \n\n    return parameters\n\nparameters = layer_neural_network(x_train, y_train, x_test, y_test, num_iterations = 2500) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d6f48f583885422d303f80747108ad97ed54c033"},"cell_type":"markdown","source":"<a id=\"10\"></a> <br>\n## 4) L Layer Neural Network"},{"metadata":{"_uuid":"2906da4ff76cc5a1590172d4e4bb69accf745dd0"},"cell_type":"markdown","source":"* Activation Functions\n    * Sigmoid : The sigmoid function, which is a little more advanced in the step function, is usually used as an activation function at the output node when binary estimates are made.\n    * Relu : It replaces negative values with zero and is the most used activation function.\n    * Tanh : It takes a real-value input and takes it to the [-1, 1] range."},{"metadata":{"trusted":true,"_uuid":"8ec15ab6d3b5df454a97f58329720a58ad5a032f"},"cell_type":"markdown","source":"* Hidden Layer : As the number increases, it starts to explore more complex things. And discovers the data better. Can define objects more easily as their numbers increase.\n* Relu is also faster than others because the derivative is easy to take."},{"metadata":{"trusted":true,"_uuid":"8ff9c1e02006226aea80bef401076e1a897cdc16"},"cell_type":"code","source":"# We need to get transposing when using Keras. Because it's easier.\n# reshaping\nx_train, x_test, y_train, y_test = x_train.T, x_test.T, y_train.T, y_test.T  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7ede05902e22e7cec9f8392ffbe77b2923dd1349"},"cell_type":"markdown","source":"#### Implementing with keras library"},{"metadata":{"trusted":true,"_uuid":"7d4e050be4bcca200e6a843b17ea3d550bf4b33c"},"cell_type":"code","source":"#Evaluating the ANN\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom keras.models import Sequential # initialize neural network library\nfrom keras.layers import Dense # build our layers library\n# build classifier: We are building a structure that will form the neural network.\ndef build_classifier():\n    classifier = Sequential()  # initialize neural network\n    \n    # Dense: It's building the layers.\n    #   - units=8: We have eight node.\n    #   - kernel_initializer: The values we first define in weights. It will be randomly distributed with uniform.  \n    #   - relu: If input < 0, x = 0 indicates(sample: relu(-6)=0). If input > 0, x = x indicates(sample: relu(6)=6).\n    #   - input_dim: 4096 px.\n    classifier.add(Dense(units=8, kernel_initializer='uniform', activation='relu', input_dim = x_train.shape[1]))\n    classifier.add(Dense(units=4, kernel_initializer='uniform', activation='relu'))\n    classifier.add(Dense(units=2, kernel_initializer='uniform', activation='relu'))\n    # - sigmoid: We're adding our last output layer. That will be our output layer.\n    classifier.add(Dense(units=1, kernel_initializer='uniform', activation='sigmoid'))            \n    \n    # We will find loss and cost.\n    # - adam: Adaptive momentum. If we use Adam, learning_rate is not fixed. It updates Learning_rate and enables us to learn more quickly.\n    # - loss: The same lost function that we use in Linear Regression.\n    # - metrics: Evaluation metric. We choose Accuracy.\n    classifier.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n    return classifier\n\n# - build_fn: This parameter calls the neural network we built.\n# epochs: number of iteration\nclassifier = KerasClassifier(build_fn = build_classifier, epochs = 100)\n    \n# cross_val_score: It gives us more than one accuracy and we get a more effective result by taking averages of them. \n# estimator: We determine the classifier to use.\n# cv : Find 3 times accuracy and then we'll average it, after that we find a more effective and more accurate result.\naccuracies = cross_val_score(estimator = classifier, X = x_train, y = y_train, cv =3)\nmean = accuracies.mean()\nvariance = accuracies.std()\nprint(\"Accuracy mean:\" + str(mean))\nprint(\"Accuracy variance:\" + str(variance))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e044aaa661254c5cf686c45a7577becbaf7c4fa5"},"cell_type":"markdown","source":"<a id=\"11\"></a> <br>\n> # CONCLUSION \n* If you want a more detailed kernel. Check out DATAI TEAM's Deep Learning Tutorial for Beginners Kernel.  https://www.kaggle.com/kanncaa1/deep-learning-tutorial-for-beginners\n---\n<br> **Thank you for your votes and comments.**                                                                                                                                             \n<br>**If you have any suggest, May you write for me, I will be happy to hear it.**"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}