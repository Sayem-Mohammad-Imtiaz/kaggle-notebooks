{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n# Some of these I did not use because I didn't need to but I put them in there just incase i would need them later\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.tree import DecisionTreeClassifier # Our model\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.ensemble import RandomForestRegressor\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.tree import DecisionTreeClassifier ,plot_tree\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-25T03:26:11.829965Z","iopub.execute_input":"2021-06-25T03:26:11.830617Z","iopub.status.idle":"2021-06-25T03:26:13.110667Z","shell.execute_reply.started":"2021-06-25T03:26:11.830494Z","shell.execute_reply":"2021-06-25T03:26:13.109641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Introduction\nPurpose/Hope to predict: I hope to predict if a patient will die if they have certain symptoms of a patient with heart failure\nGoal for quality of predictions: below 20% mae\nHypotheses: \n        - The most accurate model will be the random forest model because it uses multiple decision trees to predict and I will most likely be using alot of variables from the big dataset\n        - features like high blood pressure, diabetes, sex, smoking will have the highest affect on my mae\n        \n\n# Accesing the data\n\nNow that the notebook is setup by importing different libraries and code to access the data we can now start by accessing the data and turning it into a variable.\nThe code cell above contributed to setting up the notebook by importing different libraries and code. In the code cell under code is used to access the data file by setting up a path to it. the .describe is also used to print out the data in a table. This table just uses statistical variables like count, mean, min, max etc.\n# Why I used heart failure data\n\nThis dataset had many variables I could use to predict a y value which was given in the data set (DEATH_EVENT). This dataset also had a gold medal and at the time 1138 upvotes.\n","metadata":{}},{"cell_type":"code","source":"train_file_path = '../input/heart-failure-clinical-data/heart_failure_clinical_records_dataset.csv'\n\n# Create a new Pandas DataFrame with our training data\nheart_train_data = pd.read_csv(train_file_path)\n\n#printing the data\nheart_train_data.describe(include='all')","metadata":{"execution":{"iopub.status.busy":"2021-06-25T03:26:13.112212Z","iopub.execute_input":"2021-06-25T03:26:13.112552Z","iopub.status.idle":"2021-06-25T03:26:13.189386Z","shell.execute_reply.started":"2021-06-25T03:26:13.112521Z","shell.execute_reply":"2021-06-25T03:26:13.188408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prepare the Data\nWe now need to start preparing the data by describing our x and y. Our X will be a list of variables from the data. These variables are used to predict our y variable which in this case will be if they die or not.\n\nBefore we can get a reference to our prediction target 'y' we first need to prepare our data so that there aren't any rows with missing values as our machine learning model doesn't know how to handle them.\n\n## Select Features and Drop Missing Values\nWe will need to filter our data to data that is relevent and doesn't have any missing values. Using the dropna(axis=0) line of code allows us to drop rows of data that have missing values so all our data has the same amount of rows.\n","metadata":{}},{"cell_type":"code","source":"# Let's reduce our data to only the features we need and the target.\n# We need to keep the prediction as part of our DataFrame for now.\n#I have experimented with different variables inorder to decrease the mean absolute error from 46% to 18.54%\nselected_columns = ['age', 'diabetes','high_blood_pressure', 'DEATH_EVENT', 'sex', 'smoking', 'anaemia', 'serum_creatinine', 'time', 'serum_sodium', 'platelets', 'ejection_fraction']\n\n# Create our new training set containing only the features we want\nprepared_data = heart_train_data[selected_columns]\n\n# Drop rows from the selected_colums data that contain missing values\nprepared_data = prepared_data.dropna(axis=0)\n\n# Check that you still have a good 'count' value.\nprepared_data.describe()","metadata":{"execution":{"iopub.status.busy":"2021-06-25T03:26:13.191141Z","iopub.execute_input":"2021-06-25T03:26:13.191447Z","iopub.status.idle":"2021-06-25T03:26:13.259803Z","shell.execute_reply.started":"2021-06-25T03:26:13.19142Z","shell.execute_reply":"2021-06-25T03:26:13.25878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Separate Features From Target\nNow that we have a set of data (aka DataFrame) without any missing values. We now need to take out 'DEATH_EVENT' from our X value and set y as 'DEATH_EVENT'so we can use our X to predict our y.\n\n\n","metadata":{}},{"cell_type":"code","source":"y = prepared_data.DEATH_EVENT\n\n# Drop the DEATh_event column (axis=1 indicates column, axis=0 indicates row)\nX = prepared_data.drop('DEATH_EVENT', axis=1)\n\n#Delete hashtag below to see specific data\nX.head()\n#y.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-25T03:26:13.261268Z","iopub.execute_input":"2021-06-25T03:26:13.261527Z","iopub.status.idle":"2021-06-25T03:26:13.280256Z","shell.execute_reply.started":"2021-06-25T03:26:13.261502Z","shell.execute_reply":"2021-06-25T03:26:13.279128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## One Hot Encode Categorical Data \nOne Hot Encoding is necessary for categorical non-numerical data. In this case, our categorical data is for 'Sex' and can be 'male' or 'female'. We must do this because Decision Tree models in Scikit cannot work with non-numerical data. \n\nOne Hot Encoding separates each of the options for 'Sex' into a separate column, where a 1 means that the row contains this category value and a zero indicates it must be another categorical value. Watch this video (https://www.youtube.com/watch?v=v_4KWmkwmsU) for more information about how and why this works.\n\n\nThe Pandas get_dummies function is the easiest way to One Hot Encode categorical data. Here's how it's done","metadata":{"_kg_hide-input":true}},{"cell_type":"code","source":"#one_hot_X = pd.get_dummies(X)\n\n#one_hot_X.head()","metadata":{"_kg_hide-output":true,"_kg_hide-input":false,"execution":{"iopub.status.busy":"2021-06-25T03:26:13.28171Z","iopub.execute_input":"2021-06-25T03:26:13.282072Z","iopub.status.idle":"2021-06-25T03:26:13.288114Z","shell.execute_reply.started":"2021-06-25T03:26:13.28204Z","shell.execute_reply":"2021-06-25T03:26:13.287222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train a Model and Make Predictions\nNow that we have data our model can digest, let's train a model on our data and make some predictions.","metadata":{}},{"cell_type":"code","source":"#specify the model. \n#For model reproducibility, set a numeric value for random_state when specifying the model\n#HF for Heart Failure\nHF_model = DecisionTreeRegressor(random_state=1)\n\n# Fit the model\nHF_model.fit(X, y)","metadata":{"execution":{"iopub.status.busy":"2021-06-25T03:26:13.289512Z","iopub.execute_input":"2021-06-25T03:26:13.289919Z","iopub.status.idle":"2021-06-25T03:26:13.308673Z","shell.execute_reply.started":"2021-06-25T03:26:13.289878Z","shell.execute_reply":"2021-06-25T03:26:13.307482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Making predictions using the data chosen in the X variable\npredictions = HF_model.predict(X)\n#predictions = HF_model.predict(X.head())\nprint(predictions)","metadata":{"execution":{"iopub.status.busy":"2021-06-25T03:26:13.310192Z","iopub.execute_input":"2021-06-25T03:26:13.310832Z","iopub.status.idle":"2021-06-25T03:26:13.322564Z","shell.execute_reply.started":"2021-06-25T03:26:13.310787Z","shell.execute_reply":"2021-06-25T03:26:13.321391Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Splitting the data\nWe need to split the data into training and testing so the data can be fit to further models","metadata":{}},{"cell_type":"code","source":"train_X, val_X, train_y, val_y = train_test_split(X, y, random_state = 1)","metadata":{"execution":{"iopub.status.busy":"2021-06-25T03:26:13.324813Z","iopub.execute_input":"2021-06-25T03:26:13.32516Z","iopub.status.idle":"2021-06-25T03:26:13.33204Z","shell.execute_reply.started":"2021-06-25T03:26:13.325129Z","shell.execute_reply":"2021-06-25T03:26:13.331081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Specify the model\nHF__model = DecisionTreeRegressor(random_state=1)\n\n# Fit HF__model with the training data.\nHF__model.fit(train_X, train_y)","metadata":{"execution":{"iopub.status.busy":"2021-06-25T03:26:13.333541Z","iopub.execute_input":"2021-06-25T03:26:13.333908Z","iopub.status.idle":"2021-06-25T03:26:13.348072Z","shell.execute_reply.started":"2021-06-25T03:26:13.333875Z","shell.execute_reply":"2021-06-25T03:26:13.347312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Making predictions of the split data\nval_predictions = HF__model.predict(val_X)\nprint(val_predictions)","metadata":{"execution":{"iopub.status.busy":"2021-06-25T03:26:13.349699Z","iopub.execute_input":"2021-06-25T03:26:13.350031Z","iopub.status.idle":"2021-06-25T03:26:13.362631Z","shell.execute_reply.started":"2021-06-25T03:26:13.350003Z","shell.execute_reply":"2021-06-25T03:26:13.36152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculating the mean absolute error\n#The failure rate is 18.6% reocurring \nval_mae = (mean_absolute_error(val_y, val_predictions))\n\nprint(val_mae)","metadata":{"execution":{"iopub.status.busy":"2021-06-25T03:26:13.36394Z","iopub.execute_input":"2021-06-25T03:26:13.364294Z","iopub.status.idle":"2021-06-25T03:26:13.370776Z","shell.execute_reply.started":"2021-06-25T03:26:13.36426Z","shell.execute_reply":"2021-06-25T03:26:13.370008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Calculating the best amount of leaf nodes\nI came across a problem here with the Mean Absolute Error (mae) being 0 which is most likely something to do with the training data and the testing data being the same so the predictions will be similar.","metadata":{}},{"cell_type":"code","source":"def get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y):\n    model = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=0)\n    model.fit(train_X, train_y)\n    preds_val = model.predict(val_X)\n    mae = mean_absolute_error(val_y, preds_val)\n    return(mae)","metadata":{"execution":{"iopub.status.busy":"2021-06-25T03:26:13.371945Z","iopub.execute_input":"2021-06-25T03:26:13.37221Z","iopub.status.idle":"2021-06-25T03:26:13.381493Z","shell.execute_reply.started":"2021-06-25T03:26:13.372185Z","shell.execute_reply":"2021-06-25T03:26:13.380816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_leaf_nodes = 100\ncandidate_max_leaf_nodes = [5, 25, 50, 100, 250, 500]\n# I tried to loop it with the candidate_max_leaf_nodes list but it didn't work\nmy_mae = get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y)\nprint(\"Max leaf nodes: %d  \\t\\t Mean Absolute Error:  %d\" %(max_leaf_nodes, my_mae))","metadata":{"execution":{"iopub.status.busy":"2021-06-25T03:26:13.382607Z","iopub.execute_input":"2021-06-25T03:26:13.383123Z","iopub.status.idle":"2021-06-25T03:26:13.405083Z","shell.execute_reply.started":"2021-06-25T03:26:13.383093Z","shell.execute_reply":"2021-06-25T03:26:13.40442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Random Forest model\nThis is a random forest model. It takes random data from my selected data in order to make a prediction. ","metadata":{}},{"cell_type":"code","source":"# Defining the model\nrf_model = RandomForestRegressor(random_state=1)\n\n# fitting the model\nrf_model.fit(train_X, train_y)\n\n# Calculating the mean absolute error of my Random Forest model on the validation data\nrf_val_predictions = rf_model.predict(val_X)\nrf_val_mae = mean_absolute_error(rf_val_predictions, val_y)\n\nprint(\"Validation MAE for Random Forest Model: {}\".format(rf_val_mae))","metadata":{"execution":{"iopub.status.busy":"2021-06-25T03:26:13.406006Z","iopub.execute_input":"2021-06-25T03:26:13.40639Z","iopub.status.idle":"2021-06-25T03:26:13.618634Z","shell.execute_reply.started":"2021-06-25T03:26:13.406354Z","shell.execute_reply":"2021-06-25T03:26:13.617949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Decision tree classifier model\nI used this model to help me visualise my data","metadata":{}},{"cell_type":"code","source":"# change the value of max_depth to change the size of the decision tree\ndeath_predictor = DecisionTreeClassifier(max_depth=10)\n\n# fitting the model on my training data\ndeath_predictor.fit(train_X, train_y)\n\n# Plotting the tree\nplt.figure(figsize = (20,10))\nplot_tree(death_predictor,\n          feature_names=X.columns,\n          class_names=['Died', 'survived'],\n          filled=True)\nplt.show()\n#  Printing the mae\nval_mae = (mean_absolute_error(val_y, val_predictions))\nprint(val_mae)","metadata":{"execution":{"iopub.status.busy":"2021-06-25T03:26:13.619589Z","iopub.execute_input":"2021-06-25T03:26:13.619999Z","iopub.status.idle":"2021-06-25T03:26:18.021311Z","shell.execute_reply.started":"2021-06-25T03:26:13.619968Z","shell.execute_reply":"2021-06-25T03:26:18.020646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conclusion\nMy data was pretty accurate with a prediction success rate of 81.46%. I used different variables and models in order to get the lowest mean absolute error (mae). I originally had less variables in order to get a 18.66% success rate in the decision tree regressor but after a little more experimenting i reached 18.54% in the random forest model. Using different models I was allowed to have a larger chance of finding a lower mae. During this investigation I have found out different models that can be used to make predictions on patients chance of death if they have had heart failure.\n\nChanging variables in the X varaibles allowed me to get a smaller MAE in order to have more accurate predictions. By using different amount of variables I have decreased my MAE by small amounts. By using different variables i have decreased my MAE by larger amounts from 40%-20%. This happened because those variables are used to evaluate my mae.\n\nI used a decision tree regressor, classifier and random forest models because the regressor and random forest models were used to compare eachother to find the lowest MAE and the classifier was used to help visualise the data set. The random forest model had a lower MAE then the regressor in the end but before I made slight changes to the selected_coloumns the regressor had a MAE of 18.66%. I used more variables inorder to decrease my MAE in my random forest model\n\nHypotheses accuracy: My hypotheses was right. The random tree forest was the most accurate model at making predictions and the variables I stated made the largest difference on my mae.","metadata":{}}]}