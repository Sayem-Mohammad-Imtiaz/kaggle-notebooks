{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h1 style=\"text-align:center;color:brown;\">Univariate Time Series Autoregression</h1>\n\n<img src=\"https://idoraquel.s3.eu-central-1.amazonaws.com/userimages/Drawing-3.sketchpad.png\" />","metadata":{}},{"cell_type":"markdown","source":"<h2 style=\"text-align:center;color:brown;\">The goal of this notebook</h2>\n<p>\nThis notebook is for <b>beginners</b>. My main goal is to show how the time (or any other sequence) feature can be used in datasets.\n</p>\n\n<p>\nYou probably already know how the linear regression works.\n</p>\n\n<p><i><u>Linear regression:</u></i> Given number of features, you are updating the weights in order to minimize the error.</p>\n<p><i><u>Autoregression of order N:</u></i> Given sequence of target values over some time, we are modelling the same linear regression. But features now are the target values for the previous N time steps.</p> ","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom statsmodels.tsa.stattools import adfuller\n\n# Skew the distribution a bit with a constantly growing trend. \n# For the little number, the data is still stationary according to the Dickey-Fuller test\nHB_GROW = 1\n# number of steps that model considers. \n# try to change this number to 5 and see the difference :)\nAR_ORDER = 50\n\ndata = np.random.rand(10000) ** 2\nseason1 = np.sin(np.arange(0, 10000 / 5, 0.2))\nseason2 = np.cos(np.arange(0, 10000 / 10, 0.1))\nnoise = np.random.randn(10000,)\nstationary_data = data + season1 + season2 + noise\ndata = stationary_data + np.linspace(0, HB_GROW, 10000)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T11:00:42.608248Z","iopub.execute_input":"2021-06-20T11:00:42.60916Z","iopub.status.idle":"2021-06-20T11:00:42.624933Z","shell.execute_reply.started":"2021-06-20T11:00:42.609106Z","shell.execute_reply":"2021-06-20T11:00:42.623604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stats = adfuller(data)\nassert stats[1] < 0.05, \\\n       \"The data is not stationary, please decrease the HB_GROW\" # still stationary ?","metadata":{"execution":{"iopub.status.busy":"2021-06-20T11:00:42.627671Z","iopub.execute_input":"2021-06-20T11:00:42.628337Z","iopub.status.idle":"2021-06-20T11:00:43.629063Z","shell.execute_reply.started":"2021-06-20T11:00:42.628289Z","shell.execute_reply":"2021-06-20T11:00:43.627955Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 5))\nax1.title.set_text(\"Whole range\")\nax1.plot(data[:2000])\nax2.title.set_text(\"First 300 points\")\nax2.plot(data[:300]);","metadata":{"execution":{"iopub.status.busy":"2021-06-20T11:00:43.631963Z","iopub.execute_input":"2021-06-20T11:00:43.632529Z","iopub.status.idle":"2021-06-20T11:00:44.036993Z","shell.execute_reply.started":"2021-06-20T11:00:43.632465Z","shell.execute_reply":"2021-06-20T11:00:44.035954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Autoregression order\np = AR_ORDER\ntrain = []\n# Max index should have been increased by 1 (len(data) - p + 1), \n# but last slice would never have the target value to predict.\n# So i just skipped it on the start\nmax_start_index = len(data) - p\nfor li in range(0, max_start_index):\n    sl = data[li:li+p]\n    train.append(sl)\n\ntrain = np.array(train)\nprint(f\"Train set consists of {train.shape[0]} examples {train.shape[1]} features each one\")\n\ntargets = data[p:]\n\nassert train.shape == (10000 - p, p)\nassert targets.shape == (10000 - p,)\n\n\n# Now we have smth like this (but with order 10, not 5)\n# train_seq   target\n# 1 2 3 4 5   6\n# 2 3 4 5 6   7\n# 3 4 5 6 7   8\n# Ensure that first target is the same as the last element in second training example \n# or the second from the end in second example, etc. (because slice is shifting repeatedly by 1)\nassert targets[0] == train[1, -1]\nassert targets[0] == train[2, -2]\nassert targets[0] == train[3, -3]","metadata":{"execution":{"iopub.status.busy":"2021-06-20T11:00:44.038805Z","iopub.execute_input":"2021-06-20T11:00:44.039182Z","iopub.status.idle":"2021-06-20T11:00:44.069438Z","shell.execute_reply.started":"2021-06-20T11:00:44.039146Z","shell.execute_reply":"2021-06-20T11:00:44.067226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2 style=\"text-align:center;color:brown;\">Validation &amp; test sets</h2>\n\n<p><i><u style=\"color:red\">Important:</u></i> When it comes to splitting the dataset, we always should consider the time. Namely, evaluate the data on the <b>only unseen</b> data. \nIn other words, in the future</p>","metadata":{}},{"cell_type":"code","source":"TRAIN_SET_SLICE = slice(None, 10000)\nVAL_SET_SLICE = slice(6000, 8000)\nTEST_SET_SLICE = slice(8000, 10000 - p)\nX_train, y_train = train[TRAIN_SET_SLICE, :], targets[TRAIN_SET_SLICE]\nX_val, y_val = train[VAL_SET_SLICE, :], targets[VAL_SET_SLICE]\nX_test, y_test = train[TEST_SET_SLICE, :], targets[TEST_SET_SLICE]\n\n# a bit of tests never hurts\nassert X_val.shape == (2000, p)\nassert y_test.shape == (2000 - p,)\nassert all(y_val == targets[6000:8000])","metadata":{"execution":{"iopub.status.busy":"2021-06-20T11:00:44.070936Z","iopub.execute_input":"2021-06-20T11:00:44.071307Z","iopub.status.idle":"2021-06-20T11:00:44.080037Z","shell.execute_reply.started":"2021-06-20T11:00:44.071274Z","shell.execute_reply":"2021-06-20T11:00:44.078683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2 style=\"text-align:center;color:brown;\">\n    AR Model\n</h2>\n<p>In order to make it more intuitive, i won't import any blackbox models from package (although they are great :))</p>\n<p>Instead, we will construct it using simple Dense layer from the keras (tensorflow)</p>","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.layers import Input, Dense\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import SGD\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping","metadata":{"execution":{"iopub.status.busy":"2021-06-20T11:00:44.081798Z","iopub.execute_input":"2021-06-20T11:00:44.082208Z","iopub.status.idle":"2021-06-20T11:00:44.098113Z","shell.execute_reply.started":"2021-06-20T11:00:44.082171Z","shell.execute_reply":"2021-06-20T11:00:44.097091Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inputs = Input(shape=(p,))\noutputs = Dense(1)(inputs)\nmodel = Model(inputs, outputs)\nopt = SGD(learning_rate=1e-3)\nmodel.compile(optimizer=opt, loss=\"mse\")\nmodel.summary()\n\nreduce_lr = ReduceLROnPlateau(patience=10, factor=0.3, min_lr=1e-10)\nearly_stop = EarlyStopping(patience=20)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T11:00:44.099438Z","iopub.execute_input":"2021-06-20T11:00:44.099995Z","iopub.status.idle":"2021-06-20T11:00:44.156694Z","shell.execute_reply.started":"2021-06-20T11:00:44.099943Z","shell.execute_reply":"2021-06-20T11:00:44.155714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"r = model.fit(X_train, \n              y_train, \n              validation_data=(X_val, y_val),\n              batch_size=32,\n              epochs=500, \n              verbose=False, \n              shuffle=True, callbacks=[reduce_lr, early_stop])","metadata":{"execution":{"iopub.status.busy":"2021-06-20T11:00:44.158048Z","iopub.execute_input":"2021-06-20T11:00:44.158354Z","iopub.status.idle":"2021-06-20T11:01:05.193705Z","shell.execute_reply.started":"2021-06-20T11:00:44.158324Z","shell.execute_reply":"2021-06-20T11:01:05.192532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(r.history[\"loss\"], color=\"blue\")\nplt.plot(r.history[\"val_loss\"], color=\"orange\");","metadata":{"execution":{"iopub.status.busy":"2021-06-20T11:01:05.196072Z","iopub.execute_input":"2021-06-20T11:01:05.196373Z","iopub.status.idle":"2021-06-20T11:01:05.349035Z","shell.execute_reply.started":"2021-06-20T11:01:05.196345Z","shell.execute_reply":"2021-06-20T11:01:05.346891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Please have a look at the orange curve (validation loss over epochs). It is so \"nervous\" because the data is **not clearly stationary**.\n\nIn other words, the model can't converge because of amount of noise\n\nConsider *decreasing* HB_GROW to 0.1 or remove this line\n> \\+ np.linspace(0, HB_GROW, 10000)","metadata":{}},{"cell_type":"code","source":"model.evaluate(X_test, y_test)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T11:01:05.3508Z","iopub.execute_input":"2021-06-20T11:01:05.351099Z","iopub.status.idle":"2021-06-20T11:01:05.877498Z","shell.execute_reply.started":"2021-06-20T11:01:05.351072Z","shell.execute_reply":"2021-06-20T11:01:05.876449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_pred = model.predict(X_test)\nplt.plot(y_test[:100])\nplt.plot(test_pred[:100], color=\"green\");","metadata":{"execution":{"iopub.status.busy":"2021-06-20T11:01:05.87906Z","iopub.execute_input":"2021-06-20T11:01:05.879377Z","iopub.status.idle":"2021-06-20T11:01:06.153755Z","shell.execute_reply.started":"2021-06-20T11:01:05.879346Z","shell.execute_reply":"2021-06-20T11:01:06.152574Z"},"trusted":true},"execution_count":null,"outputs":[]}]}