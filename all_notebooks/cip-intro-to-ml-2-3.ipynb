{"cells":[{"metadata":{},"cell_type":"markdown","source":"# CIP: INTRO TO MACHINE LEARNING - BUILDING A SMART ZOOKEEPER\n_Notebook curated by [Prajna Soni](pvs262@nyu.edu)_\nThis notebook is curated to help you understand the basics of machine learning and artificial intelligence so that you can follow along without having to type all of the code yourself. \n\nWe will use **[Python](http://www.python.org)**, a high-level, general-purpose programming language, in this notebook. It is a great language to get started on and there is a lot of documentation, including tutorials online so you can continue learning it after this workshop too!\n\n_(For more info, check out [Kaggle Learn](https://www.kaggle.com/learn/overview))_\n\n**Quick-access Resource List:** \n* [Markdown Cheatsheet](https://www.markdownguide.org/cheat-sheet/)\n* [Example Open Challenge](https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge)\n* [Python Documentation](https://docs.python.org/3/tutorial/introduction.html)\n* [Numpy Beginner's Guide](https://numpy.org/doc/stable/user/absolute_beginners.html)\n* [Pandas Cheatsheet](https://pandas.pydata.org/pandas-docs/stable/user_guide/10min.html)\n* [Scikit-learn Documentation](https://scikit-learn.org/stable/)"},{"metadata":{},"cell_type":"markdown","source":"**RECAP. CLASSIFIERS**\n\n1. **Classification and Class Probability Estimation** - BINARY: Is this animal aquatic? MULTICLASS: Which class does this animal belong to? (think bank fraud)\n2. **Regression** - How much rain can we expect tomorrow? (think ______)\n3. **Similarity Matching** - Are Alia and Prajna similar? (think Netflix)\n4. **Clustering** - Which majors exist in this group of students? (think ______)\n5. **Co-occurrence Grouping** - Which items are most likely to be bought together at the supermarket? (think store offers)\n6. **Profiling** - What are the typical characteristics/behaviours of Carrie? (think ______)\n7. **Link Prediction** - Are Alia and Prajna friends? (think Facebook)\n\n_Source: https://www.juvo.be/blog/popular-data-mining-algorithms_\n\n**Supervised vs Unsupervised** - named based on the teacher-class analogy\n\n**Supervised Methods** - have a specific target defined \n- \"Can we find groups of students who are likely to major in Computer Science?\"\n    \n**Unsupervised Methods** - no defined target \n- \"Do our students fall into groups?\"\n\n"},{"metadata":{},"cell_type":"markdown","source":"**A. BINARY CLASSIFICATION PROBLEM**\n\n_Adapted from lecture slides by [Prof. George Valkanas](https://www.linkedin.com/in/gvalk/) (NYU Stern)_\n\nLet's learn the basics of classification through a **binary classification problem**. Binary classification means that the machine model has to classify individuals into one of two possible groups. In this problem, we will look at classifying responses of a pop-up lemonade-charity campaign.\n\n\n\nImagine you want to set up a temporary lemonade stall to raise money for a charity. \n1. You have a limited number of posters that you can distribute to houses around your neighbourhood.\n2. You have historic data from previous campaigns on characteristics of neighbours and whether they purchased a lemonade.\n\nLet's think about this. \n1. **Why does historic data matter?**\n    _Your answer here_\n2. **Why do characteristics of neighbours matter?**\n    _Your answer here_\n3. **Why is their previous response to the campaign important?**\n    _Your answer here_"},{"metadata":{},"cell_type":"markdown","source":"Distribution of neighbours and their responses:\n\nRed = no purchase\n\nBlue = yes purchase"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import the libraries we will be using\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport math\nimport matplotlib.patches as patches\nimport matplotlib.pylab as plt\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn import tree\nfrom sklearn import metrics\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nfrom IPython.display import Image\nfrom sklearn.tree import export_graphviz\n\n\n%matplotlib inline\n\n#-----------------------------------------------------------------------------------\n\n# A function that picks a color for an instance, depending on its target variable\n# We use 0 for \"No\" and \"1\" for yes.\n# The function returns a list of items, one item for each instance (in the order given)\ndef Color_Data_Points(target):\n    color = [\"red\" if t == 0 else \"blue\" for t in target]\n    return color\n\n\n# A function to plot the data in a scatter plot\n# data: The data we want to visualize\n# v1: The name used to access the X-axis variable information in the data parameter\n# v2: The name used to access the Y-axis variable information in the data parameter\n# tv: The name used to access the target variable information in the data parameter\ndef Plot_Data(data, v1, v2, tv):\n\n    # Make the plot square\n    plt.rcParams['figure.figsize'] = [12.0, 8.0]\n    \n    # Color\n    color = Color_Data_Points(data[tv])\n    \n    # Plot and label\n    plt.scatter(data[v1], data[v2], c=color, s=50)\n    plt.xlabel(v1)\n    plt.ylabel(v2)\n    plt.xlim([min(data[v1]) , max(data[v1]) ])\n    plt.ylim([min(data[v2]) , max(data[v2]) ])\n\n#-----------------------------------------------------------------------------------\n    \n# Set the randomness\nnp.random.seed(36)\n\n# Number of users, i.e. number of instances in our dataset\nn_users = 100\n\n# Features that we know about each user. The attributes below are for illustration purposes only!\nvariable_names = [\"name\", \"age\", \"years_neighbour\"]\nvariables_keep = [\"years_neighbour\", \"age\"]\ntarget_name = \"response\"\n\n# Generate data with the \"datasets\" function from SKLEARN (package)\n# This function returns two variables: predictors and target\n\npredictors, target = datasets.make_classification(n_features=3, n_redundant=0, \n                                                  n_informative=2, n_clusters_per_class=2,\n                                                  n_samples=n_users)\n\n# We will write this data in a dataframe (pandas package)\n\ndata = pd.DataFrame(predictors, columns=variable_names)\n\n# We want to take each column of the dataframe to change the values \n\ndata['age'] = data['age'] * 10 + 50\ndata['years_neighbour'] = (data['years_neighbour'] + 6)/2\ndata[target_name] = target\n\n# Our variables (features) will be stored in one variable called X\nX = data[variables_keep]\n\n# Our target will be stored in one variable called Y\nY = data[target_name]\n\n# Show the first 5 values of our data\npd.concat([X, Y], axis=1).head(5)\n\nplt.figure(figsize=[7,6])\nPlot_Data(data, \"years_neighbour\", \"age\", \"response\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's explore the individual features - does the age of the customer influence their decision to buy lemonade or does the number of years they have been your neighbour influence their decision to buy lemonade more?"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"plt.rcParams['figure.figsize'] = [15.0, 2.0]\n\ncolor = color = Color_Data_Points(data[\"response\"])\nplt.scatter(X['age'], Y, c=color, s=50)\nplt.xlabel('age')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.rcParams['figure.figsize'] = [15.0, 2.0]\n\ncolor = color = Color_Data_Points(data[\"response\"])\nplt.scatter(X['years_neighbour'], Y, c=color, s=50)\nplt.xlabel('years_neighbour')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Which feature/characteristic has a better split?** \n\n_Your answer here_ years_neighbour"},{"metadata":{},"cell_type":"markdown","source":"If you had to set a condition to split based on the graphs above, how would you decide to distribute the posters?\n\nWould you bet that neighbours who had lived next you you for 3 years or longer were more likely to buy your lemonade?"},{"metadata":{"trusted":true},"cell_type":"code","source":"# A function that creates the surface of a decision tree\n\ndef Decision_Surface(data, target, model):\n    # Get bounds\n    x_min, x_max = data[data.columns[0]].min(), data[data.columns[0]].max()\n    y_min, y_max = data[data.columns[1]].min(), data[data.columns[1]].max()\n\n    # Create a mesh\n    xx, yy = np.meshgrid(np.arange(x_min, x_max,0.01), np.arange(y_min, y_max,0.01))\n    meshed_data = pd.DataFrame(np.c_[xx.ravel(), yy.ravel()])\n\n    plt.figure(figsize=[12,7])\n    Z = model.predict(meshed_data).reshape(xx.shape)\n\n    plt.title(\"Decision surface\")    \n    plt.ylabel(\"age\")\n    plt.xlabel(\"years_customer\")\n\n    color = Color_Data_Points(target)\n    cs = plt.contourf(xx, yy, Z, levels=[-1,0,1], colors=['#ff6666', '#66b2ff'] )\n    plt.scatter(data[data.columns[0]], data[data.columns[1]], color=color, edgecolor='black' )\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's define the model (tree)\nmy_tree = DecisionTreeClassifier(max_depth=3,criterion=\"entropy\") \n#entropy is one measure of determining where to split\n\n# Let's tell the model what is the data\nmy_tree.fit(X, Y)\n\n#Let's print an image with the results\nDecision_Surface(X,Y,my_tree)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print ( \"Accuracy = %.3f\" % (metrics.accuracy_score(my_tree.predict(X), Y)) )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**UNDERSTANDING ACCURACY**\n* False Positive -\n* False Negative - "},{"metadata":{"trusted":true},"cell_type":"code","source":"tree.export_graphviz(my_tree, out_file = 'tree.dot', feature_names = variables_keep, class_names = ['no', 'yes'], filled = True)\n# Convert to png\nfrom subprocess import call\ncall(['dot', '-Tpng', 'tree.dot', '-o', 'tree.png', '-Gdpi=600'])\n\n# Display in python\nimport matplotlib.pyplot as plt\nplt.figure(figsize = (14, 18))\nplt.imshow(plt.imread('tree.png'))\nplt.axis('off');\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we've seen how decision trees work in classifying instances based on features/characteristics. Let's make the problem a little harder whilst using a real dataset! Let's teach a computer how to identify what class an animal belongs to - let's train a smart zoo-keeper!"},{"metadata":{},"cell_type":"markdown","source":"**B. SMART ZOOKEEPER**\n\nWe have data about the characteristics of different animals in the zoo, and we want to train our smart zoo-keeping model so it can tell us whether an animal is a Mammal, a Bird, a Reptile, a Fish, a Amphibian, a Bug, or a Invertebrate.\n\nLet's explore the data set so we can see what kind of characteristics we have and how the data is stored."},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"zoo_class = pd.read_csv(\"../input/zoo-animal-classification/class.csv\")\n#zoo_class = pd.read_csv(\"../input/class.csv\")\n#zoo_class = pd.read_csv(\"../class.csv\")\n\n\nzoo = pd.read_csv(\"../input/zoo-animal-classification/zoo.csv\")\nzoo_class.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**INTERPRETING THIS DATA**\n\nWe can now see the first five lines of this data. We have 16 characteristics/features about each animal, with each boolean (true/false) characteristic marked as a 1 (true) or a 0 (false) and numeric characteristics with their respective answers.\n\nE.g. predator is a boolean - true or false, legs is a numeric feature - 4 legs or 0 legs, etc. \n\nWhat else can we see from this set of characteristics?"},{"metadata":{"trusted":true},"cell_type":"code","source":"#PREPARING THE DATASET\n# Here, we split the dataset into the characteristics/features (X) and the class (Y)\nX = zoo.drop('class_type', axis=1)\nX = X.drop('animal_name',  axis=1)\nY = zoo['class_type']\n\n# We then split the data into a training (80%) and testing (20%) dataset\ntrain_X, test_X, train_Y, test_Y = train_test_split(X, Y, test_size=0.20, random_state=262, stratify=Y)\n\n#Let's look at the first 5 rows of our train_X data set. \n#This is the dataset with characteristics/features that we will be training on\ntrain_X.head()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**TRAIN AND TEST SPLIT**\n\nWe split our dataset into a training set that the machine \"sees\" as it trains and learns, and a separate test set which it has never \"seen\" before.\n\n**Why is it important to split our dataset into training and testing set?**\n\n_Your answer here_"},{"metadata":{"trusted":true},"cell_type":"code","source":"#declare decision tree classifier classifiying based on entropy \nzkTree = DecisionTreeClassifier(max_depth = 3, criterion ='entropy')\n#train decision tree classifier on training data\nzkTree.fit(train_X, train_Y)\n#get predicted results for given test_X\nzkTreePred = zkTree.predict(test_X)\nacc_score = metrics.accuracy_score(test_Y, zkTreePred)\nprint(acc_score)\nmetrics.confusion_matrix(test_Y, zkTreePred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's visualise the tree\ntree.export_graphviz(zkTree, out_file = 'zkTree.dot', feature_names = X.columns, class_names = zoo_class['Class_Type'], filled = True)\n# Convert to png\nfrom subprocess import call\ncall(['dot', '-Tpng', 'zkTree.dot', '-o', 'zkTree.png', '-Gdpi=600'])\n\n# Display in python\nimport matplotlib.pyplot as plt\nplt.figure(figsize = (14, 18))\nplt.imshow(plt.imread('zkTree.png'))\nplt.axis('off');\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**UNDERSTANDING THE RESULTS**\n\nAbove, we now have the accuracy score of our newly trained Zoo Keeper (zkTree) as well as the Confusion Matrix*.\n\n_*A confusion matrix is a table of the predicted values versus the actual values so you know where the model is predicting false values._\n\nLet's change the max_depth of the decision tree classifier and see how that changes things. \n\n"},{"metadata":{},"cell_type":"markdown","source":"**C. APPLICATIONS OF MACHINE LEARNING**\n\nCan you think of applications of machine learning in real life?\n\n_Your answer here_"},{"metadata":{},"cell_type":"markdown","source":"**WELL DONE FOR MAKING IT THIS FAR!**\n\nThis is the end of the workshop but feel free to ask questions or email the facilitators with any questions you may have: \n* Prajna Soni (prajna.soni@nyu.edu)"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}