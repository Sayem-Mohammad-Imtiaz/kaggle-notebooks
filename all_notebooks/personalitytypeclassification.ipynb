{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nPW = 8\nPH = 6\nplt.rcParams['figure.figsize'] = (PW, PH) \nplt.rcParams['image.cmap'] = 'gray'\n\nimport re\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\npd.options.display.max_rows = 250\npd.options.display.max_columns = 500\npd.options.display.max_colwidth = 500\n\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk.stem.snowball import SnowballStemmer\nimport string \nimport warnings\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4effce69eee8e93821b56bd7e9459361d7b17d12"},"cell_type":"code","source":"df = pd.read_csv(\"../input/mbti_1.csv\", encoding=\"utf-8\")\nprint(\"Number of users\", len(df))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b332845bbebea2df9c63fc3c5662b1c0aa152e90"},"cell_type":"markdown","source":"## Exploratory Analysis"},{"metadata":{"trusted":true,"_uuid":"d613742df9ab4476faa98603aeaa2919064d4bca"},"cell_type":"code","source":"df.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3406ad6106cea477f6b58f7cb6e16799b058ab59"},"cell_type":"code","source":"#Personality Types\ngroups = df.groupby(\"type\").count()\ngroups.sort_values(\"posts\", ascending=False, inplace=True)\nprint (\"Personality types\", groups.index.values)\n\n#Priors used below for Random Guessing Estimation\npriors = groups[\"posts\"] / groups[\"posts\"].sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7f74745c30f00659be49bf61ea33310b6aa0156d"},"cell_type":"code","source":"groups[\"posts\"].plot(kind=\"bar\", title=\"Number of Users per Personality type\");","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f800b7bccadc2db59f283ace188483c8267821e2"},"cell_type":"code","source":"df[\"LenPre\"] = df[\"posts\"].apply(len)\nsns.distplot(df[\"LenPre\"]).set_title(\"Distribution of Lengths of all 50 Posts\");","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"c595daea7433ade4c34e57ed5020b9b5ea77a3ee"},"cell_type":"code","source":"def preprocess_text(df, remove_special=True):\n    #Remove links \n    df[\"posts\"] = df[\"posts\"].apply(lambda x: re.sub(r'https?:\\/\\/.*?[\\s+]', '', x.replace(\"|\",\" \") + \" \"))\n    \n    #Keep EOS\n    df[\"posts\"] = df[\"posts\"].apply(lambda x: re.sub(r'\\.', ' EOSTokenDot ', x + \" \"))\n    df[\"posts\"] = df[\"posts\"].apply(lambda x: re.sub(r'\\?', ' EOSTokenQuest ', x + \" \"))\n    df[\"posts\"] = df[\"posts\"].apply(lambda x: re.sub(r'!', ' EOSTokenExs ', x + \" \"))\n    \n    #Strip Punctation\n    df[\"posts\"] = df[\"posts\"].apply(lambda x: re.sub(r'[^\\w\\s]','',x))\n\n    #Remove Non-words\n    df[\"posts\"] = df[\"posts\"].apply(lambda x: re.sub(r'[^a-zA-Z\\s]','',x))\n\n    #To lower\n    df[\"posts\"] = df[\"posts\"].apply(lambda x: x.lower())\n\n    #Remove multiple letter repating words\n    df[\"posts\"] = df[\"posts\"].apply(lambda x: re.sub(r'([a-z])\\1{2,}[\\s|\\w]*','',x)) \n\n    #Remove short/long words\n    df[\"posts\"] = df[\"posts\"].apply(lambda x: re.sub(r'(\\b\\w{0,3})?\\b','',x)) \n    df[\"posts\"] = df[\"posts\"].apply(lambda x: re.sub(r'(\\b\\w{30,1000})?\\b','',x))\n\n    #Remove Personality Types Words\n    #This is crutial in order to get valid model accuracy estimation for unseen data. \n    if remove_special:\n        pers_types = ['INFP' ,'INFJ', 'INTP', 'INTJ', 'ENTP', 'ENFP', 'ISTP' ,'ISFP' ,'ENTJ', 'ISTJ','ENFJ', 'ISFJ' ,'ESTP', 'ESFP' ,'ESFJ' ,'ESTJ']\n        pers_types = [p.lower() for p in pers_types]\n        p = re.compile(\"(\" + \"|\".join(pers_types) + \")\")\n\n    df[\"posts\"] = df[\"posts\"].apply(lambda x: p.sub(' PTypeToken ',x))\n    return df\n\n#Used for class balancing. When class balancing is used dataset becomes very small.\ndef subsample(df):\n    groups = df.groupby(\"type\").count()\n    groups.sort_values(\"posts\", ascending=False, inplace=True)\n    \n    min_num = groups[\"posts\"][-1]\n    min_ind = groups.index[-1]\n    ndf = df[df[\"type\"] == min_ind]\n\n    for pt in groups.index[:-1]:\n        print(min_num,pt)\n        tdf = df[df[\"type\"] == pt].sample(min_num)\n        ndf = pd.concat([ndf, tdf])\n    return ndf","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ea7bbbd665ad840a5ad98e6f16446a662ad9716b"},"cell_type":"markdown","source":" ## Data Preprocessing"},{"metadata":{"trusted":true,"_uuid":"22c23943133d8e712fa8991227382232adb5e0c3"},"cell_type":"code","source":"#Number of Posts per User\ndf[\"NumPosts\"] = df[\"posts\"].apply(lambda x: len(x.split(\"|||\")))\n\nsns.distplot(df[\"NumPosts\"], kde=False).set_title(\"Number of Posts per User\");","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8d698738d1bf724f2957c3348f26fd8c515b31b9"},"cell_type":"code","source":"#Split to posts\ndef extract(posts, new_posts):\n    for post in posts[1].split(\"|||\"):\n        new_posts.append((posts[0], post))\n\nposts = []\ndf.apply(lambda x: extract(x, posts), axis=1)\nprint(\"Number of users\", len(df))\nprint(\"Number of posts\", len(posts))\n\ndf = pd.DataFrame(posts, columns=[\"type\", \"posts\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9ff0ad0a9d2aa51eba8b02f25222805e37343f8e"},"cell_type":"code","source":"df[\"Len\"] = df[\"posts\"].apply(len)\nsns.distplot(df[\"Len\"]).set_title(\"Post lengths\");","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9771dd4d5861c4c9a2e2f86f2aa5e27f5c0ea205","collapsed":true},"cell_type":"code","source":"#Preprocess Text\ndf = preprocess_text(df) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4532584822ca4a60752f7bc4f3755787a0f9af8f"},"cell_type":"code","source":"df[\"Len\"] = df[\"posts\"].apply(len)\nsns.distplot(df[\"Len\"]).set_title(\"Post lengths\");","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f111942d8563336b9456fcac6a32bf0875a68626"},"cell_type":"code","source":"#Remove posts with less than X words\nmin_words = 15\nprint(\"Number of posts\", len(df)) \ndf[\"nw\"] = df[\"posts\"].apply(lambda x: len(re.findall(r'\\w+', x)))\ndf = df[df[\"nw\"] >= min_words]\nprint(\"Number of posts\", len(df)) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f0f77a28ed8712133fe9c598f2437529b1530a79"},"cell_type":"code","source":"df[\"Len\"] = df[\"posts\"].apply(len)\nsns.distplot(df[\"Len\"]).set_title(\"Post lengths\");","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1fd41f03294a427e861d5e804caffdab486d5297"},"cell_type":"code","source":"#Remove long post\nmax_length = 350\nprint(\"Number of posts\", len(df)) \ndf = df[df[\"Len\"] < 350]\nprint(\"Number of posts\", len(df)) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"62084de96b06f0ea37e4977a2e3892902929246a"},"cell_type":"code","source":"df[\"Len\"] = df[\"posts\"].apply(len)\nsns.distplot(df[\"Len\"]).set_title(\"Post lengths\");","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3f2d011336cb3325bf10c6485eb2bcc2bb0e755d","collapsed":true},"cell_type":"code","source":"#Drop nw Len\ndf.drop([\"nw\", \"Len\"],axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"86c8abe100a1da9d17d8eafe9facee3110cba741","collapsed":true},"cell_type":"code","source":"#Subsample - Used for class balancing. \n#df = subsample(df)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"569d407d9f3dd42a696a5c4cbf5933d496a048eb"},"cell_type":"markdown","source":"## Word  Stemming"},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"3bb20b294e8e692fc495fcbb6cbedde6753a8cb9"},"cell_type":"code","source":"#Stem\nstemmer = SnowballStemmer(\"english\")\n\ndf[\"posts\"] = df[\"posts\"].apply(lambda x: \" \".join(stemmer.stem(p) for p in x.split(\" \")))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0b5588a4f42e358aca5a5dca24519f16b7ef9eda"},"cell_type":"markdown","source":"## Preprocessed Posts"},{"metadata":{"trusted":true,"_uuid":"f8abf39f5bd1b3024691fc197c299820d9c27073"},"cell_type":"code","source":"df.iloc[np.random.choice(len(df),10),:]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"07d1f0f005e2273c91ec07721577a75f17f714b1"},"cell_type":"markdown","source":"##  Bag of Words  Model"},{"metadata":{"trusted":true,"_uuid":"b06d80ff5e1e8e171a81cbf3508502ed2f8be04d","collapsed":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.preprocessing import LabelEncoder, RobustScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"54c80036822150e616b0320136c7f8d6d7522e6c","collapsed":true},"cell_type":"code","source":"#Split train/test\nvect = CountVectorizer(stop_words='english') \nX =  vect.fit_transform(df[\"posts\"]) \n\nle = LabelEncoder()\ny = le.fit_transform(df[\"type\"])\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, stratify=y, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"907188cacf92d48055f3fb19f69c76cd00af192a"},"cell_type":"markdown","source":"## Word Lengths Disttribution"},{"metadata":{"trusted":true,"_uuid":"115c21b307b21809d2ba53a3503d7acb21141bbc"},"cell_type":"code","source":"wdf = pd.DataFrame( vect.get_feature_names(),columns=[\"word\"])\nwdf[\"len\"] = wdf.word.apply(len)\nsns.distplot(wdf[\"len\"], kde=False).set_title(\"Words Lentghs Distribution\");","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"056cc585337989dd06ece93980bfcabf015686b1"},"cell_type":"markdown","source":" ## Random Guessing Estimation"},{"metadata":{"trusted":true,"_uuid":"d4155c2276cb60ab878256ca5702cc9f56b9e05e","collapsed":true},"cell_type":"code","source":"#Evaluating Acccuarcy across four categories indipendently \ndef cat_accuracy(yp_test, y_test, le):\n    ype = np.array(list(\"\".join(le.inverse_transform(yp_test))))\n    ye = np.array(list(\"\".join(le.inverse_transform(y_test))))\n    return (ype == ye).mean()\n\ndef predict_random_guess(priors, lp):\n    return np.random.choice(priors.index, lp, p=priors.values)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"855236a9ea3166ca09f07c0b2d02711cfc8f6f9e"},"cell_type":"code","source":"num_iter = 100\nmc16 = np.zeros(num_iter)\nmc4 = np.zeros(num_iter)\n\nwarnings.filterwarnings(action='ignore', category=DeprecationWarning)\nfor i in range(100):\n    mc16[i] = np.mean(le.transform(predict_random_guess(priors, len(y_test))) == y_test)\n    mc4[i] = cat_accuracy(le.transform(predict_random_guess(priors, len(y_test))), y_test, le)\n\n\nprint (\"Random Guessing 16 Types:\", mc16.mean(), mc16.std())\nprint (\"Random Guessing 4 Categories:\", mc4.mean(), mc4.std())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2f9386f2931e98c1ee9d9ed17accc70d30ca82d8"},"cell_type":"markdown","source":"## NaiveBayes"},{"metadata":{"trusted":true,"_uuid":"619a998fc529e1de414d7d6ffcfa1208cdfe5f3e"},"cell_type":"code","source":"clf = MultinomialNB()\nclf.fit(X_train, y_train)\n\nyp_train = clf.predict(X_train)\nprint(\"Train Accuracy:\", np.mean(yp_train == y_train))\n\nyp_test = clf.predict(X_test)\nprint(\"Test Accuracy:\", np.mean(yp_test == y_test))\nprint(\"******\")\nprint(\"Categorical Train Accuracy:\", cat_accuracy(yp_train, y_train, le))\nprint(\"Categorical Test Accuracy:\", cat_accuracy(yp_test, y_test, le))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7d5c922b0a3a61393ac61067310172d1df3e4253"},"cell_type":"markdown","source":"## Plot Predictions"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"9b5be23db24468cee952a04be3d176b2bf4f0a67"},"cell_type":"code","source":"dft = pd.DataFrame(le.inverse_transform(yp_test),columns=[\"pred\"])\ndft[\"cnt\"] =  1\ndft[\"same\"] = (yp_test == y_test)\ndft[\"same\"] = dft[\"same\"].astype(int)\n\ngroupsn = dft.groupby(\"pred\").sum()\ngroupsn.sort_values(\"cnt\", ascending=False, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2328e6d66e3a43c90cf71e881c930fb180985f40"},"cell_type":"code","source":"f, ax = plt.subplots(1,2,figsize=(2*PW,PH))\ngroupsn[\"cnt\"].plot(kind=\"bar\", title=\"Distribution of Predicted User Personality Types\", ax=ax[0]);\ngroupsn[\"same\"].plot(kind=\"bar\", title=\"Distribution of Correctly Classified User Personality Types\", ax=ax[1]);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a628c1d6b443ed0bc8a2af18866ed4318e629a81"},"cell_type":"markdown","source":"## Sequential Models"},{"metadata":{"trusted":true,"_uuid":"cb7fa546f75b119660aa8d4153ab31860dca3895"},"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing import sequence\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense,  Dropout, Flatten\nfrom keras.layers import LSTM, Conv1D, Input, MaxPooling1D\nfrom keras.layers.embeddings import Embedding\nfrom sklearn.preprocessing import OneHotEncoder\n\nfrom scipy import spatial\nfrom sklearn.utils import class_weight","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8348d0b8793507580abaf827027b8e3feb54f239","collapsed":true},"cell_type":"code","source":"def cat_accu_seq(X_test, y_test, model):\n    yp_test = model.predict(X_test)\n\n    yp_test_d =  np.argmax(yp_test, axis=1)\n    y_test_d =  np.argmax(y_test, axis=1)\n    \n    return  cat_accuracy(yp_test_d, y_test_d, le )\n\ndef calc_weights(df, le, ohe):\n    groups = df.groupby(\"type\").count()\n    groups.sort_values(\"posts\", ascending=False, inplace=True)\n    \n    p = groups[\"posts\"]#.to_dict()\n    ohe.transform([[x] for x in le.transform(p.index.values)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dbdac0a2eec7b084de1b1f63c8e308569931ed2e","collapsed":true},"cell_type":"code","source":"#Prepare X and y\nX =  df.posts\n\nle = LabelEncoder()\ny = le.fit_transform(df[\"type\"])\n\nohe = OneHotEncoder(n_values='auto',  sparse=False)\ny = ohe.fit_transform(y.reshape(-1, 1))","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"d406d46d99c4657196363802b88914d0cf149bc4"},"cell_type":"code","source":"#Tokenize words\nmax_nb_words = 200000\n\ntokenizer = Tokenizer(num_words=max_nb_words)\ntokenizer.fit_on_texts(X)\nsequences = tokenizer.texts_to_sequences(X)\nword_index = tokenizer.word_index\nprint('Found %s unique tokens.' % len(word_index))\n\n#Retokenize\nmax_nb_words = len(word_index)\ntokenizer = Tokenizer(num_words=max_nb_words)\ntokenizer.fit_on_texts(X)\nsequences = tokenizer.texts_to_sequences(X)\nword_index = tokenizer.word_index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"7338f71d1bd3f6bb1f8a5e1e4875a582eed834d4"},"cell_type":"code","source":"#Constants\nptypes_num = 16\nmax_post_len = np.max([ len(x) for x in sequences])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"30ad08d26e56ba5b978fb7f784f8b16f7053a7ae"},"cell_type":"code","source":"#Pad Sequen\nsequences = sequence.pad_sequences(sequences, maxlen=max_post_len) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"75f3254ac542af7c47651dedac4bb0cd57ea14a4"},"cell_type":"code","source":"#Split Train/Test\nX_train, X_test, y_train, y_test = train_test_split(sequences, y, test_size=0.1, stratify=y, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"79b7002a3df66218f5e8a1cc0b88802015d9111c","collapsed":true},"cell_type":"code","source":"#Check Stratification\n#temp = [np.argmax(x) for x in y_test]\n#pd.DataFrame(temp)[0].value_counts().plot(kind=\"bar\");","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7017bed8d35f9d135fd1d9c559f8f8588545c571"},"cell_type":"markdown","source":"## LSTM"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"bd17eb8df352e36cf51b18d6058029e82b4c17ed"},"cell_type":"code","source":"#Parameters\nbatch_size = 512\nepochs = 5\nembedding_vecor_length = 32\nlstm_size = 32","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4e30e992f128c16f5d32144dc1df085004c9b9f7"},"cell_type":"code","source":"#Model\nmodel = Sequential()\nmodel.add(Embedding(max_nb_words, embedding_vecor_length, input_length=max_post_len))\nmodel.add(Dropout(0.25))\n\nmodel.add(LSTM(lstm_size))\nmodel.add(Dropout(0.25))\n\nmodel.add(Dense(ptypes_num, activation='softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nprint(model.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cf82b373ebf438e326e40730fac8e92741f10cda","collapsed":true},"cell_type":"code","source":"#Calculate Class Weights\nwy = le.fit_transform(df[\"type\"])\ncw = class_weight.compute_class_weight('balanced', np.unique(wy), wy)\n\n#Fit Model\nmodel.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, class_weight=cw, validation_data=(X_test, y_test));","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4b01f7afecda2775c37cc8ecf6a1746f98a44d54","collapsed":true},"cell_type":"code","source":"scores_tr = model.evaluate(X_train, y_train, verbose=0)\nscores_ts = model.evaluate(X_test, y_test, verbose=0)\n\nprint(\"Train Accuracy:\",scores_tr[1])\nprint(\"Test Accuracy:\", scores_ts[1])\nprint(\"******\")\nprint(\"Categorical Train accuracy:\", cat_accu_seq(X_train, y_train, model))\nprint(\"Categorical Test Accuracy:\", cat_accu_seq(X_test, y_test, model))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ae201c4b52b1a2819ca3684045c9a35e8cf3c6ff"},"cell_type":"markdown","source":"## Plot Predictions"},{"metadata":{"trusted":true,"_uuid":"4a0c591ef0598834d8acfc8c93bc5d2011f72612","collapsed":true},"cell_type":"code","source":"yp_test =  np.argmax(model.predict(X_test), axis=1)\n\ndft = pd.DataFrame(le.inverse_transform(yp_test),columns=[\"pred\"])\ndft[\"cnt\"] =  1\ndft[\"same\"] = (yp_test == np.argmax(y_test, axis=1))\ndft[\"same\"] = dft[\"same\"].astype(int)\n\ngroupsn = dft.groupby(\"pred\").sum()\ngroupsn.sort_values(\"cnt\", ascending=False, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"154b62388bc11da59e34840f79ef544426d06f5f","collapsed":true},"cell_type":"code","source":"f, ax = plt.subplots(1,2,figsize=(2*PW,PH))\ngroupsn[\"cnt\"].plot(kind=\"bar\", title=\"Distribution of Predicted User Personality Types\", ax=ax[0]);\ngroupsn[\"same\"].plot(kind=\"bar\", title=\"Distribution of Correctly Classified User Personality Types\", ax=ax[1]);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"d9887c367c4d22f30843844559c2b11f16a7e024"},"cell_type":"code","source":"exit()","execution_count":null,"outputs":[]}],"metadata":{"anaconda-cloud":{},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}