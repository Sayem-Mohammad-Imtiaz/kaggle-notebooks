{"cells":[{"metadata":{},"cell_type":"markdown","source":"The data consists of 5 columns:\n\n* Variance of Wavelet Transformed image (continuous)\n* Skewness of Wavelet Transformed image (continuous)\n* Curtosis of Wavelet Transformed image (continuous)\n* Entropy of image (continuous)\n* Class (integer)\n\nWhere class indicates whether or not a Bank Note was authentic.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n\ndata = pd.read_csv('../input/bank-note-authentication-uci-data/BankNote_Authentication.csv')\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"So, the data has no missing values.\\n\\n\\nNo. of observations in our dataset is {data.shape[0]}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display(\"Let's have a look on relationships between features of our data\")\nsns.pairplot(data,hue='class');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x='class',data=data)\nplt.title('Classes (Authentic 1 vs Fake 0)');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['class'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our data is not balanced. Standard classifier algorithms like Decision Tree and Logistic Regression have a bias towards classes which have higher number of instances. They tend to only predict the majority class data. The features of the minority class are treated as noise and are often ignored. Thus, there is a high probability of misclassification of the minority class as compared to the majority class. It can affect our model badly. So, here I choose over-sampling.\n\nOver-Sampling increases the number of instances in the minority class by randomly replicating them in order to present a higher representation of the minority class in the sample. This is done in order to obtain approximately the same number of instances for both the classes. I have used resample library from python.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.utils import resample,shuffle\ndf_majority = data[data['class']==0]\ndf_minority = data[data['class']==1]\ndf_minority_upsampled = resample(df_minority,replace=True,n_samples=762,random_state = 123)\nbalanced_df = pd.concat([df_minority_upsampled,df_majority])\nbalanced_df = shuffle(balanced_df)\nbalanced_df['class'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#importing librarires needed for modelling\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report,confusion_matrix\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.pipeline import Pipeline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*First, I apply Random Forest algorithm from sklearn and then by creating a pipeline.*","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = StandardScaler()\nscaled_features = scaler.fit_transform(balanced_df.drop('class',axis=1))\ndf_feat = pd.DataFrame(scaled_features,columns=balanced_df.columns[:-1])\n\nX = df_feat\ny = balanced_df['class']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\nrfc = RandomForestClassifier(n_estimators=200)\nrfc.fit(X_train,y_train)\nrfc_preds = rfc.predict(X_test)\nprint(classification_report(y_test,rfc_preds))\nprint(confusion_matrix(y_test,rfc_preds))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score=cross_val_score(rfc,X,y,cv=5)\n(100*score.mean()).round(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"my_pipeline= Pipeline([('scaler',StandardScaler()),('rfc',RandomForestClassifier())])\nmy_pipeline.fit(X_train,y_train)\npp_preds = my_pipeline.predict(X_test)\nprint(classification_report(y_test,pp_preds))\nprint(confusion_matrix(y_test,pp_preds))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Benefit of creating a pipeline is that it automates our preprocessing and modelling part. Here, we did not handle missing values otherwise it can also be included in pipeline. Then, when predicting for new unseen data we simply pass it through pipeline and need not apply standardization and imputation on it separately.\n\nAlso, during k-fold cross validation pipeline helps to achieve more accurate results. Because of pipeline, standardization is not fitted on test data during k-folds otherwise train-test data leakage occurs.*","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"score1=cross_val_score(my_pipeline,X,y,cv=5)\n(100*score1.mean()).round(2)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}