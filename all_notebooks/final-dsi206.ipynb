{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Real or Not? NLP with Disaster Tweets\nPredict which Tweets are about real disasters and which ones are not\n\nในปัจจุบันนี้ไม่ว่าบนโลกจะเกิดเหตุการณ์อะไรขึ้น ผู้คนมักใช้ Tweeter ในการติดตามข่าวสารที่เกิดขึ้นเป็นจำนวนมากซึ่ง เป็นแหล่งข่าวที่มีการอัพเดตอยู่ตลอดและว่องไวและผู้คนมักจะ tweet ข้อความข่าวสารเหล่านั้น โดยเฉพาะข่าวสารที่เกี่ยวกับภัยพิบัติ tweeter กลายเป็นแอพพลิเคชั่นสำคัญที่เอาไว้ใช้เฝ้าระวังเหตุฉุกเฉินแบบเรียลไทม์ แต่คำบางคำที่อาจจะเกี่ยวข้องกับภัยพิบัตินั้น ก็นำมาใช้เขียนในเชิงเปรียบเทียบที่คนเราสามารถอ่านแล้วเข้าใจได้ แต่ machine นั้นก็ไม่สามารถเข้าใจได้อย่างชัดเจน และเมื่อมีปัญหาที่เกิดขึ้น ทางกลุ่มเราจึงได้เข้าร่วมการแข่งขันการสร้างโมเดลที่คาดการณ์ว่าทวีตใดเกี่ยวกับภัยพิบัติจริงและทวีตใดที่ไม่ใช่"},{"metadata":{},"cell_type":"markdown","source":"# Data Set\nประกอบด้วย\n\n1. sample_submission.csv ที่เก็บข้อมูลของ id และ target ที่บ่งบอกว่า id นั้นได้พูดถึงภัยพิบัติจริงหรือไม่ ให้ 1 พูดถึงภัยพิบัติจริงและ 0 ไม่ได้พูดถึงภัยพิบัติ\n\n2. test.csv ชุดการทดสอบ  ซึ่งประกอบด้วย\n>     1. id : การระบุเลขที่มีการโพสต์ที่ไม่ซ้ำกัน\n> \n>     2. Keyword : คำสำคัญสำหรับการทำนาย\n> \n>     3. location : ตำแหน่งที่อยู่ของผู้ใช้งานทวีต\n> \n>     4. text : ข้อความที่ผู้ใช้โพสต์\n> \n3. train.csv ชุดการฝึก  ซึ่งประกอบด้วย\n>     1. id : การระบุเลขที่มีการโพสต์ที่ไม่ซ้ำกัน\n> \n>     2. Keyword : คำสำคัญสำหรับการทำนาย\n> \n>     3. location : ตำแหน่งที่อยู่ของผู้ใช้งานทวีต\n> \n>     4. text : ข้อความที่ผู้ใช้โพสต์\n> \n>     5. target : สิ่งนี้บ่งชี้ว่าทวีตนั้นเกี่ยวกับภัยพิบัติจริงให้เป็น (1) หรือไม่จริงให้เป็น (0)"},{"metadata":{},"cell_type":"markdown","source":"# Data Processing\nขั้นตอนแรกเราได้ทำการ import module ต่างๆที่จำเป็นต้องใช้ในโปรแกรม \n*  numpy สำหรับการคำนวณทางคณิตศาสตร์ \n*  pandas ที่ทำให้เราจัดการกับข้อมูลต่างๆได้ง่ายขึ้นและสามารถแสดงข้อมูลออกมาเป็นตารางได้ \n* nltk เป็นเครื่องมือการประมวลผลภาษาธรรมชาติ เพื่อให้คอมพิวเตอร์สามารถตีความและเข้าใจภาษามนุษย์ได้ \n* re ที่ทำให้สามารถค้นหากลุ่มตัวหนังสือที่มีรูปแบบตามที่ต้องการจากข้อความหรือกลุ่มตัวอักษรได้ \n* string เป็นลำดับของตัวอักษรหลายตัวเรียงต่อกัน ซึ่งในภาษา Python นั้นการที่จะประกาศ String\n  ค่าของมันจะอยู่ในเครื่องหมาย Double quote หรือ Single quote เท่านั้น \n*  sklearn เป็น module นึงของภาษา Python ที่เราจะใช้จัดการเกี่ยวกับ supervised machine learning \n*  math ที่เป็นฟังก์ชันที่ใช้คำนวณทางคณิตศาสตร์ เพื่อหาค่าทางคณิตศาสตร์ \n*  os เพื่อเช็ค dataset ว่ามีไฟล์อะไรบ้าง"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport nltk\nimport re\nimport string\nimport pandas as pd\nimport sklearn as sk\nimport math  \n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"กำหนดชุดข้อมูล train และ test ในการอ่านไฟล์และโหลดข้อมูล csv ไปยังดาต้าเฟรมและได้แสดงข้อมูลของ train เป็นจำนวน 10 แถว\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/nlpgettingstarted/train.csv')\ntest = pd.read_csv('../input/nlpgettingstarted/test.csv')\ntrain.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"และได้แสดงตารางให้เห็นจำนวนของ target 0 เป็นทวีตที่ไม่เกี่ยวข้องกับภัยพิบัติมีจำนวน 4,342 ข้อความ และ 1 เป็นทวีตที่เกี่ยวข้องกับภัยพิบัติมีจำนวน 3,271 ข้อความ"},{"metadata":{"trusted":true},"cell_type":"code","source":"temp = train.groupby('target').count()['text'].reset_index()\ntemp['label'] = temp['target'].apply(lambda x : 'Disaster Tweet' if x==1 else 'Non Disaster Tweet')\ntemp","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"แทนที่ช่องว่าง NaN ด้วยคำว่า \n\n     no_location ในคอลัมน์ location\n\n     no_keyword ในคอลัมน์ Keyword"},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in ['keyword', 'location']:\n    train[col] = train[col].fillna(f'no_{col}')\nfor col in ['keyword', 'location']:\n    test[col] = test[col].fillna(f'no_{col}')\n    \n    #train.head()\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# clean data"},{"metadata":{},"cell_type":"markdown","source":"ต่อมาเราได้ทำการ clean ข้อมูล\n\n* ขั้นแรกเราต้องการลบอิโมจิออกโดยใช้ regrex_pattern ในการตรวจสอบข้อมูลแพตเทิร์น(Pattern) คือ อิโมจิ, สัญลักษร์และภาษาภาพ, สัญลักษณ์ของการขนส่งและแผนที่ และอิโมจิรูปธงชาติใน ios และใช้ re.compile() รวบรวมรูปแบบ (Pattern) เป็นวัตถุเก็บในรูปแบบunicode แล้วทำการลบออกโดยใช้ฟังก์ชัน remove_emo\n* ขั้นตอนต่อมาคือ การลบข้อความ เช่น ลบทุกตัวที่ไม่ใช่ตัวเลขและอักษรภาษาอังกฤษรวมถึงขีดล่างเทียบเท่า [A-Za-z0–9_],ขีดและอักขระ,url, แปลงอักษรตัวพิมพ์ใหญ่ทั้งหมดในสตริงข้อความให้เป็นตัวพิมพ์เล็ก,สเปซบาร์หรือแท็บหรือขึ้นบรรทัดใหม่และเครื่องหมาย \" \" ให้ถูกแทนที่ด้วยช่องว่าง และสุดท้ายคือ ตัวเลขที่มีเลข 0 ถึง 9\n* ขั้นตอนสุดท้ายคือการลบเครื่องหมายวรรคตอนออก โดยการใช้ clean_sting ในการลบ all list ที่มีเครื่องหมายวรรคตอนอยู่ \n* และได้ apply การลบอิโมจิ ข้อความ และเครื่องหมายวรรคตอนออกทั้งในชุดฝึกฝนและชุดทดสอบ text \n\nและแสดงข้อมูลเป็นจำนวน 10 แถว"},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_emo(text) :\n    regrex_pattern = re.compile(pattern = \"[\"\n        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           \"]+\", flags = re.UNICODE)\n    return regrex_pattern.sub(r'',text)\ntrain['text'] = train['text'].apply(lambda x : remove_emo(x))\ntest['text'] = test['text'].apply(lambda x : remove_emo(x))\n\ndef removetext(text):\n    text = re.sub(r'[^\\w]', ' ', text)#Remove all \n    text = re.sub(\"\\'\\w+\", '', text) #Remove ticks and the next character\n    text = re.sub(\"https*\\S+\", \" \", text)#Remove URL\n    text = text.lower()\n    text = re.sub('\\s{2,}', \" \", text)#Replace the over spaces\n    text=re.sub(r'[0-9]', ' ', text)\n    return text\ntrain['text'] = train['text'].apply(lambda x : removetext(x))\ntest['text'] = test['text'].apply(lambda x : removetext(x))\n\ndef punctuation(text):\n    all_list = [char for char in text if char not in string.punctuation]\n    clean_str = ''.join(all_list)\n    return clean_str\ntrain['text'] = train['text'].apply(punctuation)\ntest['text'] = test['text'].apply(punctuation)\ntrain.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# remove stop word\nต่อไปเราต้องการที่จะลบคำที่พบบ่อยๆ เมื่อลบแล้วแต่ยังคงความหมายเดิม ซึ่งเป็นคำที่ไม่จำเป็น เช่น of, is, the, and เป็นต้น เพื่อกรองข้อมูลที่ไร้ประโยชน์ออก \n\nลบโดยการใช้คำสั่ง stop และใช้ stopword จากโมเดล nltk ที่เก็บคลังภาษาอังกฤษที่พบบ่อยๆแต่ไม่จำเป็นออกจากประโยค"},{"metadata":{"trusted":true},"cell_type":"code","source":"stop = nltk.corpus.stopwords.words('english')\ntrain['text'] = train['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\ntest['text'] = test['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\ntrain.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Token\nจากนั้น import sent_tokenize เพื่อตัดคำจากช่องว่าง มหัพภาค บรรทัดใหม่ และ word_tokenize เพื่อตัดคำออกมาเป็น token\n\nและได้แสดงผลออกในตาราง text_tokens "},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.tokenize import sent_tokenize, word_tokenize\ntrain['text_tokens'] = train['text'].apply(lambda x: word_tokenize(x))\ntrain.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Lemmatizer\nเราได้เลือกวิธี Lemmatizer เพื่อทำให้ได้คำที่เป็นรูปแบบพื้นฐาน (รูปแบบพจนานุกรม) เพื่อให้ได้ความแม่นยำในการทำนายคำมากขึ้น\n\nโดย import WordNetLemmatizer ใน nltk เพื่อทำ Lemmatization กับชุดฝึกฝน text_tokens และเก็บไปยังชุดฝึกฝน text_clean_tokens\n \nและได้แสดงผลในตาราง text_clean_tokens"},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.stem import WordNetLemmatizer\ndef word_lemmatizer(text):\n    lem_text = [WordNetLemmatizer().lemmatize(i) for i in text]\n    return lem_text\ntrain['text_clean_tokens'] = train['text_tokens'].apply(lambda x: word_lemmatizer(x))\ntrain.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"จากที่เราใช้วิธี Lemmatizer เพื่อให้ได้ซึ่งคำที่ลดรูปออกมาเป็นคำที่เป็นรูปแบบพื้นฐานโดยสิ้นเชิง ที่ช่วยให้เราสามารถคำนวณได้แม่นยำ เช่น wildfires \n\nเมื่อเราทำวิธี stemming ก็จะได้คำว่า wildfir และเราทำวิธี Lemmatizer ก็จะได้คำว่า wildfire ซึ่งเป็นคำที่เป็นรูปแบบพื้นฐาน เป็นต้น"},{"metadata":{},"cell_type":"markdown","source":"# CountVectorizer\nเราเลือกใช้วิธีCountVwectorizer เพื่อนับจำนวนครั้งที่คำหนึ่งคำปรากฏในเอกสารซึ่งส่งผลให้เกิดการเอนเอียงในความโปรดปรานของคำที่พบบ่อยที่สุด \n\nสิ่งนี้กลายเป็นการละเว้นคำที่หายากซึ่งอาจช่วยได้ในการประมวลผลข้อมูลของเราอย่างมีประสิทธิภาพมากขึ้น\n\n* ขั้นแรก import TfidfTransformer สำหรับใช้ในการคำนวณจำนวนคำ (TF) และ counterVectorizer เพื่อคำนวณ inverse document frequency (IDF) จาก sklearn.feature_extraction และทำการหา TFIDF โดยการใช้ Tfidfvectorizer ที่แปลงข้อความเป็นเวกเตอร์ \n* คำนวณค่า IDF จาก counterVectorizer แล้วเก็บใน cv ที่กำหนดการแบ่งข้อมูลเพื่อคำนวณค่าความแม่นยำหรือค่าความผิดพลาด\n* แล้วทำทำกับ fit และ tranform ชุดฝึกฝน text ไปเก็บใน train_vectors และ ทำการ tranform ชุดทดสอบ text ไปยัง text_vectors\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import model_selection\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\ncv = CountVectorizer()\ntrain_vectors = cv.fit_transform(train['text'])\ntest_vectors = cv.transform(test[\"text\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# TFIDF\nเราต้องการที่จะคัดแยกคำตามความสำคัญเพื่อที่จะทำนายคำที่เกี่ยวกับภัยพิบัติจริงๆนั้น ทางกลุ่มจึงได้เลือกวิธีการ TFIDF เพื่อหาค่าการทำนาย \n\nโดยใช้ TfidfVectorizer และกำหนดให้           \n          \n          min_df = 2 หมายถึง ให้ลบคำที่พบเพียงแค่ 2 document\n\n          max_df = 0.5 หมายถึง ให้ลบคำที่พบมากถึง 50 % ใน 1 document\nและได้ทำ tfidf และทำการ fit และ tranform ค่าในชุดฝึกฝน text และเก็บใน train_vectors แล้วทำวิธีเดียวกันกับชุดทดสอบ text แล้วเก็บค่าไปยัง test_vectors "},{"metadata":{"trusted":true},"cell_type":"code","source":"tfidf = TfidfVectorizer(min_df=2, max_df=0.5, ngram_range=(1, 1))\ntrain_vectors = tfidf.fit_transform(train['text'])\ntest_vectors = tfidf.transform(test[\"text\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"เราได้ import LogisticRegression และ model_selection และได้เก็บอยู่ในตัวแปร clf ที่มีเก็บฟังก์ชั่น LogisticRegression \n\n    และได้กำหนดตัวแปร score เพื่อคำนวณคะแนนที่ learning และทำนายออกมาใน clt โดยใช้ f1-score ซึ่งคือ \n    การหาค่าเฉลี่ยระหว่าง Precision กับ Recall และกำหนด cv = 8 ซึ่งเป็นการแบ่งข้อมูลเพื่อคำนวณค่าความแม่นยำหรือ\n    ค่าความผิดพลาดจำนวน 8 รอบ เพราะรอบที่ 8 เป็นรอบที่ให้ค่า มีการทำนายที่สูงกว่ารอบอื่นๆ"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn import model_selection\n\nclf = LogisticRegression(C=3.1)\nscores = model_selection.cross_val_score(clf, train_vectors, train[\"target\"], cv=8, scoring=\"f1\")\nprint(scores)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"การแสดงผลความแม่นยำของ Model with Cross Validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Accuracy : \",scores.mean() * 100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"ปรับ simple Logistic Regression ให้เหมาะสม"},{"metadata":{"trusted":true},"cell_type":"code","source":"clf.fit(train_vectors, train[\"target\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"เป็นการนำผลลัพธ์ที่ได้ สุดท้ายเราทำการ predict กับ test_vectors เก็บค่าที่ได้ลงในตาราง submission ในคอลัมน์ target \n\nจากนั้นโหลดเข้าไฟล์ submission.csv เพื่อส่งผลการทำนาย"},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv('../input/nlpgettingstarted/sample_submission.csv')\nsubmission[\"target\"] = clf.predict(test_vectors)\nsubmission.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"แสดงผลลัพธ์ของ submission จำนวน 30 แถว"},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.head(30)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Reference"},{"metadata":{},"cell_type":"markdown","source":"arupjyoti_dutta 2020 ,Removing stop words with NLTK in Python , view 21 Nov 2020 ,https://www.geeksforgeeks.org/removing-stop-words-nltk-python/\n\nKatharine Jarmul 2017,Detecting Fake News with Scikit-Learn , view 21 Nov 2020 , https://www.datacamp.com/community/tutorials/scikit-learn-fake-news\n\nkoPytok 2018,Corpora/stopwords not found when import nltk library,view 21 Nov 2020,https://stackoverflow.com/questions/41610543/corpora-stopwords-not-found-when-import-nltk-library\n\nNaim Mhedhbi 2020 , Text Classification Step by Step , view 21 Nov 2020 , https://www.kaggle.com/naim99/text-classification-step-by-step/output\n\nZhi Li 2019,A Beginner’s Guide to Word Embedding with Gensim Word2Vec Model ,view 21 Nov 2020 ,https://towardsdatascience.com/a-beginners-guide-to-word-embedding-with-gensim-word2vec-model-5970fa56cc92\n\ndef love(x) 2017 ,CountVectorizer, TfidfVectorizer, Predict Comments, view 22 Nov 2020 ,https://www.kaggle.com/adamschroeder/countvectorizer-tfidfvectorizer-predict-comments#CountVectorizer----Brief-Tutorial\n\nJason Brownlee 2017, Deep Learning for Natural Language Processing ,view 22 Nov 2020 , https://machinelearningmastery.com/prepare-text-data-machine-learning-scikit-learn/\n\nKavita Ganesan 2020 , AI Implementation, Hands-On NLP Scikit-learn’s Tfidft,view 21 Nov 2020https://kavita-ganesan.com/tfidftransformer-tfidfvectorizer-usage-differences/#.X7o9lWj7SM8"},{"metadata":{},"cell_type":"markdown","source":"# รายชื่อสมาชิก\nน.ส.นันท์นภัส บุญเชิด 6209656021\n\nน.ส.สุภาวินี แจ๊ะซ้าย 6209656039\n\nน.ส.ณัฐนิช เพิ่มหรรษา 6209656328"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}