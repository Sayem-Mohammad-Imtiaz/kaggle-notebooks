{"cells":[{"metadata":{},"cell_type":"markdown","source":" \n# Classification of Delhi Metro stations.\n<img src='https://upload.wikimedia.org/wikipedia/commons/6/65/Delhi_Metro_logo.svg' style=\"width: 150px;\"/>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Introduction\n\nDelhi Metro is a rapid transit system serving Delhi and its satellite cities in the National Capital Region of India. As of now, there are a total of 229 metro stations including the Airport Express stations. The first section of the Delhi Metro opened on 25 December 2002 with the Red Line,[2] and has since been expanded to around 347.66 km(216.03 miles) of route length as of 4 October 2019. The network has nine operational lines and is built and operated by the Delhi Metro Rail Corporation Limited (DMRC). The Delhi Metro Rail Corporation makes 2,700 trips per day carrying 1.5 million passengers, who on an average travel a distance of 17 kilometres each.\n\nFor this project, we will try to look at the places surrounding these metro stations and classify them accordng to the similarity of nearby venues. Every one use metro transit to migrate from one place to another for reasons which can be personal of professional. If there are more professional places like companies, offices surrounding a station then it will mostly be used by working professionals. Then there are some stations with many unversities or colleges nearby and is used by Students mostly. Stations which have places like amusement parks, malls, monuments are used by people for recreation. \n\nWe can classify stations by primary usage analyzing the data that contains the number of nearby venues according to their category. This can help plan further extension of the network and find places for new development.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Data","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"In this section we will describe our base data which we will analyze to reach the goal we want. \n\nLet us import some of the libraries that we will be required to move further in the project.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"import numpy as np # library to handle data in a vectorized manner\n\nimport pandas as pd # library for data analsysis\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)\n\nimport json # library to handle JSON files\n\nfrom geopy.geocoders import Nominatim \n# convert an address into latitude and longitude values\n\nimport requests # library to handle requests\n\n# Matplotlib and associated plotting modules\nimport matplotlib.colors as colors\nimport matplotlib.pyplot as plt\n\n# import k-means from clustering stage\nfrom sklearn.cluster import KMeans\n\n#!conda install -c conda-forge folium=0.5.0 --yes # uncomment this line if you haven't completed the Foursquare API lab\nimport folium # map rendering library\n\nprint('Libraries imported.')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Because we are working on the data that is based on places. It will be great to visualize using a world map. Since, our project revolves around the National Capital of India, Delhi. We wil visualize a simple map of delhi using folium library.\n\nWe will require the Latitude and Longitude of Delhi to focus it on the map. To get these coordinates we use geopy.geocoders that can perform geocoding of the given Address.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"address = 'Delhi, India'\n\ngeolocator = Nominatim(user_agent=\"ny_explorer\")\nlocation = geolocator.geocode(address)\nlatitude = location.latitude\nlongitude = location.longitude\nprint('The geograpical coordinate of Delhi are {}, {}.'.format(latitude, longitude))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"map_delhi = folium.Map(location=[latitude, longitude], zoom_start=11)  \nmap_delhi.save(outfile= \"test_map.html\")\nmap_delhi","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we will need a data that will contain the list of all the metro stations under DMRC in Delhi. For this we will use this url\n\nhttps://en.wikipedia.org/wiki/List_of_Delhi_Metro_stations\n\nWe have to scrape the relevant table data from this url like Station name, Line. \n\n#### Assumption :-\n- There are some stations with more than one line that pass through it. So we have assumed that only the line that is written first on the above url will be the data of our choice to nullify the ambiguity that we may face while plotting on the graph.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"from bs4 import BeautifulSoup\n\nwiki_url = 'https://en.wikipedia.org/wiki/List_of_Delhi_Metro_stations'\nwiki_page = requests.get(wiki_url).text\nwiki_doc = BeautifulSoup(wiki_page, 'lxml')\n\nrows = wiki_doc.find('table', {'class': 'wikitable sortable'}).findAll('tr')\n\n\ndf = pd.DataFrame()\n \n\n\nlst = []\nform = '{ \"name\": \"%s\",\\\n          \"details\": {\"line\":\"[%s]\",\\\n                      \"latitude\":0.0,\\\n                      \"longitude\":0.0 }}'\nStation=[]\nLine = []\nfor row in rows[1:]:\n    items = row.find_all('td')\n    try:\n        if len(items)==8:\n            Station.append(items[0].find('a').contents[0])\n            Line.append(items[2].find('a').find('span').find('b').contents[0])\n            lst.append(form % (items[0].find('a').contents[0],\n               items[2].find('a').find('span').find('b').contents[0]))\n    \n    except Exception as e:\n        continue\n\nstring = '['+','.join(lst)+']'\n\ndata = json.loads(string)\n\nf = open('metro.json', 'w+')\nf.write(json.dumps(data, indent=4))\nf.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(len(Station))\nprint(len(Line))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Adding the stations and Corresponding Line to the empty dataframe that we have made above with the name 'df'.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"df['Station']=Station\ndf['Line']=Line\ndf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now that we have all the station and lines in our dataframe but we will need to use their coordinates that are unique to them and can be used to plot each station on the map.\n\nAgain we will use the geocoder to get the corresponsing latitude and longitude value of all station. \n\n#### Assumption\n- While using the address there are few shortcomings like there can be more than one address with same name. for example 'Gandhinagar' is in Gujarat as well as in Delhi. so we have to use few try except blocks that will search for the place with more accuracy to less accuracy. \n- If we dont find the coordinates even after this we have used None as lat. and long. values. Store them in two list and then update in the dataframe","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"Latitude = []\nLongitude = []\nfor stat in Station:\n    try:\n        try:\n            try:\n                address = \"{} metro station, Delhi, India\".format(stat)\n                geolocator = Nominatim(user_agent=\"ny_explorer\")\n                location = geolocator.geocode(address)\n                lat = location.latitude\n                long = location.longitude\n            except Exception as e:\n                address = \"{}, Delhi, India\".format(stat)\n                geolocator = Nominatim(user_agent=\"ny_explorer\")\n                location = geolocator.geocode(address)\n                lat = location.latitude\n                long = location.longitude\n        except Exception as e:\n            address = \"{}, India\".format(stat)\n            geolocator = Nominatim(user_agent=\"ny_explorer\")\n            location = geolocator.geocode(address)\n            lat = location.latitude\n            long = location.longitude\n    except Exception as d:\n        lat=None\n        long=None\n    Latitude.append(lat)\n    Longitude.append(long)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df['Latitude'] = Latitude\ndf['Longitude'] = Longitude\ndf.head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# df.to_csv('DELHI_METRO_DATA.csv',index=False)\ndf=pd.read_csv('DELHI_METRO_DATA.csv')\ndf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Converting all the line with a integer value so that we can use it easily.\n- Blue line and Green line diverge or branch so simplicity we have taken them as the same line.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"linetonum = {\"Yellow Line\": 1, \"Red Line\": 2,\"Blue Line\": 3,'Blue Line branch':3, \"Pink Line\": 4,\"Magenta Line\": 5, \"Green Line\": 6,'Green Line branch':6, \"Violet Line\": 7, \"Orange Line\": 8,\"Grey Line\": 9}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our data contains few NaN values for which our code was unable to find the coordinates value so we have remove their rows. But you can also replace them with coordinated found from web.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"data = df.dropna(axis=0)\ndata","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"data.replace({\"Line\": linetonum},inplace=True)\ndata","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For a clear visualization we have used the line with their actual color in hexadecimal code. For example, Station on 'Red Line' will be marked with red color. ","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"colors_dict = {1:'#FFFF00', 2:'#FF0000',3:'#0000FF', 4:'#FFC0CB',5:'#FF00FF', 6:'#008000',7:'#EE82EE', 8:'#FFA500',9:'#808080'} ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using the code to get coordinate of Delhi.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"address = 'Delhi, India'\n\ngeolocator = Nominatim(user_agent=\"ny_explorer\")\nlocation = geolocator.geocode(address)\nlatitude = location.latitude\nlongitude = location.longitude\nprint('The geograpical coordinate of Delhi are {}, {}.'.format(latitude, longitude))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plotting the map with all stations marked with corresponding line color using folium library.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"map_delhi_metro = folium.Map(location=[latitude, longitude], zoom_start=10)\n\nfor line, station, lat,long in zip(data['Line'], data['Station'],data['Latitude'], data['Longitude']):\n    folium.Circle(\n        [lat,long],\n        popup=station,\n        radius=20,\n        color=colors_dict[line]\n    ).add_to(map_delhi_metro)\nmap_delhi_metro.save(outfile= \"outlier_map.html\")\nmap_delhi_metro","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Zooming out the map will give us a outlier whose error in getting coordinates is very high. We can see that station named 'Lal Qila' is plotted near Telangana state so we will correct it by using the real coordinate dof located using map and replacing the value.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"data.at[97,'Latitude'] = 28.656682\ndata.at[97,'Longitude'] = 77.236612\ndata\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Sort the rows of dataframe by column 'Line'\ndata_sort = data.sort_values(by ='Line' )\ndata_sort","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"data_sort.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"map_delhi_metro = folium.Map(location=[latitude, longitude], zoom_start=10)\n#add markers\nfor line, station, lat,long in zip(data_sort['Line'], data_sort['Station'],data_sort['Latitude'], data_sort['Longitude']):\n    folium.Circle(\n        [lat,long],\n        popup=station,\n        radius=30,\n        fill=True,\n        color=colors_dict[line]\n    ).add_to(map_delhi_metro)   \n\nmap_delhi_metro","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we have a good visualiztion of each station and also we easily can trace the path of each line. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Methodology","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We have the the data related to the metro station under DMRC, their latitudes and longitudes values with the line of Metro that passed through them.\n\nFor further processing we will be utilizing the **FOURSQUARE API** and explore the venues in a specific radius around that using their coordinates values.\n\nWe will use explore query under venues of the api calls. You can read more about the various Endpoints provided Foursquare api on the following link.\n**https://developer.foursquare.com/docs/places-api/endpoints/**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"__The advantage of using Foursquare api is that venues are classiied in particular categories which will make it easy to calculate the number of venues surround a given location for each category and that is actually the base of our classification.__","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Make a account on Foursquare Developer portal and get your credentials below.\nMake sure you do not share these values with anyone.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"CLIENT_ID = 'Your Client ID'\nCLIENT_SECRET = 'Your Client Secret'\nVERSION = '20180605'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All the categories that i have mentioned has some unique id so we need to execute a api call to get all the valid categories with the unique id.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"categories_url = 'https://api.foursquare.com/v2/venues/categories?client_id={}&client_secret={}&v={}'.format(\n            CLIENT_ID, \n            CLIENT_SECRET, \n            VERSION)\n            \n# make the GET request\nresults = requests.get(categories_url).json()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"len(results['response']['categories'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"categories_list = []\n# Let's print only the top-level categories and their IDs and also add them to categories_list\n\ndef print_categories(categories, level=0, max_level=0):    \n    if level>max_level: return\n    out = ''\n    out += '-'*level\n    for category in categories:\n        print(out + category['name'] + ' (' + category['id'] + ')')\n        print_categories(category['categories'], level+1, max_level)\n        categories_list.append((category['name'], category['id']))\n        \nprint_categories(results['response']['categories'], 0, 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we need to define a function which will take latiude and longitude values to check the address and category_id for which we will we want the count of nearby venues.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"def get_venues_count(lat,long, radius, categoryId):\n    explore_url = 'https://api.foursquare.com/v2/venues/explore?client_id={}&client_secret={}&v={}&ll={},{}&radius={}&categoryId={}'.format(\n                CLIENT_ID, \n                CLIENT_SECRET, \n                VERSION,\n                lat,\n                long,\n                radius,\n                categoryId)\n    try:\n        return requests.get(explore_url).json()['response']['totalResults']\n    except Exception as e:\n        return 0","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Checking the progress of our data for a final time to make sure we are going in right direction","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"data_sort.reset_index(inplace=True,drop=True)\ndata_sort","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"stations_venues_df = data_sort.copy()\nfor c in categories_list:\n    stations_venues_df[c[0]] = 0\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Calling the function to get all the count of nearby venues for each category for all station one by one and place them in a dataframe.\n\nNote:-\n- Here i may seem to start from 180 thats' because in Foursquare Regular acoount we can only call 950 per day and for our dataset we have 221 stations and 10 category for each station so i have run in several three days or you can use different accounts and just see from where you are going to see all 0 counts for each category.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"for i, row in stations_venues_df[stations_venues_df.index > 179].iterrows():\n    print(i)\n    for c in categories_list:        \n        stations_venues_df.loc[i, c[0]] = get_venues_count(stations_venues_df.Latitude.iloc[i],stations_venues_df.Longitude.iloc[i], radius=1000,categoryId=c[1])\n    stations_venues_df.to_csv('stations_venues.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"stations_venues = pd.read_csv('stations_venues.csv', index_col=0)\nstations_venues","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"stations_venues.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Visualization and Modeling","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Using a Box Plot to see the the distrubution of venue count for each category.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(20, 10))\nplt.xticks(rotation='vertical')\nsns.boxplot\n\nax = sns.boxplot(data = stations_venues)\nax.set_ylabel('Count of venues', fontsize=25)\nax.set_xlabel('Venue category', fontsize=25)\nax.tick_params(labelsize=20)\nplt.xticks(rotation=45, ha='right')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ignoring the Latitude and Longitude and analysing the categories distribution we can see that most of the venues are in the category of Food, Professional places and Shops & Services. While others are comparitively less in number.\n\nTo make a good visualization let us Normalize the data so that every cell value will be in between 1 and 0 using the MinMaxScaler and removing the Latitude and longitude column through slicing.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\n\nX = stations_venues.values[:,4:]\ncluster_dataset = MinMaxScaler().fit_transform(X)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we need to define a dataframe that we will use for clustering using the normalized value. \n\nNote: We use the Normalize Venues count values for each category as a base of our clustering the station. That is for what reason that station is beiing used by passengers. ","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"cluster_df = pd.DataFrame(cluster_dataset)\ncluster_df.columns = [c[0] for c in categories_list]\ncluster_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plotting a Box Plot again to see how Normalization affects the distribution.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.figure(figsize=(20, 10))\nplt.xticks(rotation='vertical')\nsns.boxplot\n\nax = sns.boxplot(data = cluster_df)\nax.set_ylabel('Count of venues', fontsize=25)\nax.set_xlabel('Venue category', fontsize=25)\nax.tick_params(labelsize=20)\nplt.xticks(rotation=45, ha='right')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Clustering using KMeans Clsutering","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"So we have the data and using an unsupervised clustering Algorithm known as KMeans Clusteing we will Create clusters.\nTo get started You may know that 'K' Means Numbers of clusters in which data is to be categorized. \nNow problem is we dont know how to evaluate the best K value so we will run the Kmeans algorithm on different values of K. Then we will use some measures or metrics to see which K value will be the most suitable.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"First we will use the 'ELBOW METHOD' to check the most appropriate value of K.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.cluster import KMeans \n\nSum_of_squared_distances = []\nK = range(2,11)\nfor k in K:\n    km = KMeans(n_clusters=k)\n    km = km.fit(cluster_df)\n    Sum_of_squared_distances.append(km.inertia_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plotting the curve ","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.plot(K, Sum_of_squared_distances, 'bx-')\nplt.xlabel('k')\nplt.ylabel('Sum_of_squared_distances')\nplt.title('Elbow Method For Optimal k')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that we cannot make clear prediction from the Elbow curve i.e a value of K after which sum_of_squared_distances is almost stable.\n\nNext Let us try to use anothe method which is known as Silhoutte Score Method.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.metrics import silhouette_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sil = []\n\nkmax = 10\n\n# dissimilarity would not be defined for a single cluster, thus, minimum number of clusters should be 2\nfor k in range(2,kmax+1):\n  kmeans = KMeans(n_clusters = k).fit(cluster_df)\n  labels = kmeans.labels_\n  sil.append(silhouette_score(cluster_df, labels, metric = 'euclidean'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"K1=range(2,kmax+1)\nplt.plot(K1, sil, 'bx-')\nplt.xlabel('k')\nplt.ylabel('silhouette_score')\nplt.title('silhouette_score Method For Optimal k')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here too we see that value of silhoutte score is max at cluster 2 but we know that we will need to create more than two clusters. Also because we cannot find a Global maxima we have a local maxima when the value of 4. So for our Clustering let us use K value as 4.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"kclusters = 4\n\n# run k-means clustering\nkmeans = KMeans(n_clusters=kclusters, random_state=0).fit(cluster_df)\n\nkmeans_labels = kmeans.labels_\nkmeans_labels","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"(unique, counts) = np.unique(kmeans_labels, return_counts=True)\nfrequencies = np.asarray((unique, counts)).T\n\nprint(frequencies)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After seeing the frequency of values for each cluster we can replace the labels in Descending order of No. of stations. i.e count.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"replace_labels = {0:3,1:1,2:2,3:0}\nfor i in range(len(kmeans_labels)):\n    kmeans_labels[i] = replace_labels[kmeans_labels[i]]\n\nstations_clusters_df = stations_venues.copy()\nstations_clusters_df['Cluster'] = kmeans_labels\nstations_clusters_minmax_df = cluster_df.copy()\nstations_clusters_minmax_df['Cluster'] = kmeans_labels\nstations_clusters_minmax_df['Station'] = stations_venues['Station']\nstations_clusters_minmax_df['Latitude'] = stations_venues['Latitude']\nstations_clusters_minmax_df['Longitude'] = stations_venues['Longitude']","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"import matplotlib.ticker as ticker\n\nfig, axes = plt.subplots(1,kclusters, figsize=(20, 10), sharey=True)\n\naxes[0].set_ylabel('Count of venues (relative)', fontsize=25)\n#plt.set_xlabel('Venue category', fontsize='x-large')\n\nfor k in range(kclusters):\n    #Set same y axis limits\n    axes[k].set_ylim(0,1.1)\n    axes[k].xaxis.set_label_position('top')\n    axes[k].set_xlabel('Cluster ' + str(k), fontsize=25)\n    axes[k].tick_params(labelsize=20)\n    plt.sca(axes[k])\n    plt.xticks(rotation='vertical')\n    sns.boxplot(data = stations_clusters_minmax_df[stations_clusters_minmax_df['Cluster'] == k].drop('Cluster',1), ax=axes[k])\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Before analyzing the Box-Plot above let us create a map of clustered station using folium.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"address = 'Delhi, India'\n\ngeolocator = Nominatim(user_agent=\"ny_explorer\")\nlocation = geolocator.geocode(address)\nlatitude = location.latitude\nlongitude = location.longitude\nprint('The geograpical coordinate of Delhi are {}, {}.'.format(latitude, longitude))","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"\ncluster_map_delhi = folium.Map(location=[28.6517178,77.2219388], zoom_start=10)\n\n\n#adding markers\nfor i, station, lat,long, cluster in zip(stations_clusters_minmax_df.index,\n                                         stations_clusters_minmax_df['Station'],\n                                         stations_clusters_minmax_df['Latitude'],\n                                         stations_clusters_minmax_df['Longitude'],\n                                         stations_clusters_minmax_df['Cluster']):\n        \n    \n    colors=['blue','green','orange','red']\n    \n    station_series = stations_clusters_minmax_df.iloc[i]\n    top_categories_dict = {}\n    for cat in categories_list:\n        top_categories_dict[cat[0]] = station_series[cat[0]]\n    top_categories = sorted(top_categories_dict.items(), key = lambda x: x[1], reverse=True)\n    popup='<b>{}</b><br>Cluster {}<br>1. {} {}<br>2. {} {}<br>3. {} {}'.format(\n        station,\n        cluster,\n        top_categories[0][0],\n        \"{0:.2f}\".format(top_categories[0][1]),\n        top_categories[1][0],\n        \"{0:.2f}\".format(top_categories[1][1]),\n        top_categories[2][0],\n        \"{0:.2f}\".format(top_categories[2][1]))\n    folium.CircleMarker(\n        [lat,long],\n        fill=True,\n        fill_opacity=0.5,\n        popup=folium.Popup(popup, max_width = 300),\n        radius=5,\n        color=colors[cluster]\n    ).add_to(cluster_map_delhi)\ncluster_map_delhi.save('Cluster_DMRC_Stations.html')\ncluster_map_delhi","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Results and Insights from Clusters and Plots","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Here is how we can characterize the clusters by looking at venue scores and Map visualization.\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Cluster 0 (Blue)","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"cluster_0= stations_clusters_minmax_df[stations_clusters_minmax_df['Cluster']==0]\ncluster_0","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Data Analysis Shows that **Blue cluster/ Cluster 0** have high value in almost all the fields of College & University, Professional & Other Places, Outdoors & Recreation, Shops & Service and also in other categories for some stations. \n\n1. From the map, if we compare it with the Line station map we see that almost every Station on Red line falls in this cluster. \n2. Moreover, there are small patches in the north and south delhi region where there are many colleges of Delhi university and other institutions.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Cluster 1 (GREEN)","execution_count":null},{"metadata":{"scrolled":false,"trusted":false},"cell_type":"code","source":"cluster_1= stations_clusters_minmax_df[stations_clusters_minmax_df['Cluster']==1]\ncluster_1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Data Analysis Shows that **Green cluster/ Cluster 1** have high value in almost all the fields of Residence, Professional & Other Places, Outdoors & Recreation, Arts & Entertainment , Shops & Services and College & University than other categories.\n\n1. From the map, We see that stations like Jor Bagh, Civil Lines and many other places which are considered to be Posh or are Residential areas are in this cluster.\n2. Talking about shops and services some Famous Market areas like Chandni Chowk, Sarojini Nagar etc. are present in this cluster. \n3. Moreover, there are Many places of Recreation and outdoors like Hauz Khas, Chhatarpur, Kalkaji, Akshardham etc. are clustered in this cluster.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### CLUSTER 2 (YELLOW)","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"cluster_2= stations_clusters_minmax_df[stations_clusters_minmax_df['Cluster']==2]\ncluster_2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Data Analysis Shows that Yellow cluster/ Cluster 2 have high value in almost all the fields of Event, Residence, Outdoors & Recreation, Arts & Entertainment , Travel & Transport than other categories.\n\n1. Analysis shows us that this cluster is geographically centered at almost central part of Delhi and including stations like Kashmere Gate, Welcome which have interchanges so mostly visited by people who travel alot and thats one of the reason there is more of Travel and Transport facility.\n2. From the map, We see that there are important stations like AIIMS, Supreme Court in this cluster.\n3. This actually shows that there is a need of continuous development in these areas. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### CLUSTER 3 (RED)","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"cluster_3= stations_clusters_minmax_df[stations_clusters_minmax_df['Cluster']==3]\ncluster_3","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Data Analysis Shows that Red cluster/ Cluster 3 have high value in almost all the fields of Event, Nightlife, Outdoors & Recreation, Professional & Other Places, College & University than other categories.\n\nAnalysis shows us that this cluster consists of industrial and professional places like Faridabad, some Noida sectors. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<hr>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Discussions","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Note that Foursquare data isn't very precise and accurate. The Foursquare counts for the venues that we can explore via Foursquare data. This doesn’t take into account other attributes that are essential and impotant to take while making clusters like Venue's size, services it provide, small shops or 'Rehdis' that are usually present almost everywhere. Moreover we do not take into account the Food while analysis as this is the attribute almost equal everywhere.\n\nAlso using Geocoder has some limitations in how accurate the geocoding of an address will be returned. There is also ambiguity with some places due to similar names or changes in names.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Conclusion\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Foursquare data is limited but can provide insights into a city’s development. This data could be combined with other sources to provide more accurate results.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}