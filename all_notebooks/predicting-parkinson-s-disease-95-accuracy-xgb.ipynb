{"cells":[{"metadata":{},"cell_type":"markdown","source":"Introduction:-\n\nParkinsonâ€™s disease is a progressive disorder of the central nervous system affecting movement and inducing tremors and stiffness. It has 5 stages to it and affects more than 1 million individuals every year in India. This is chronic and has no cure yet. It is a neurodegenerative disorder affecting dopamine-producing neurons in the brain.\n\nProblem Statement:- Prediction of PARKINSON'S Disease.\n\nThe Dataset contains following columns:-\n\n1) name - object\n\n2) MDVP:Fo(Hz) - float64\n\n3) MDVP:Fhi(Hz) - float64\n\n4) MDVP:Flo(Hz) - float64\n\n5) MDVP:Jitter(%) - float64\n\n6) MDVP:Jitter(Abs) - float64\n\n7) MDVP:RAP - float64\n\n8) MDVP:PPQ - float64\n\n9) Jitter:DDP - float64\n\n10) MDVP:Shimmer - float64\n \n11) MDVP:Shimmer(dB) - float64\n\n12) Shimmer:APQ3 - float64\n\n13) Shimmer:APQ5 - float64\n\n14) MDVP:APQ - float64\n\n15) Shimmer:DDA - float64\n\n16) NHR - float64\n\n17) HNR - float64\n\n18) status - int64\n\n19) RPDE - float64\n\n20) DFA - float64\n\n21) spread1 - float64\n\n22) spread2 - float64\n\n23) D2 - float64\n\n24) PPE - float64\n\nData visualization done with using various plots.\n\nMachine learning algorithm used\n\n1) XGBOOST\n\n2) Decision Tree Classifier\n\n3) Naive Bayes"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n\nfrom sklearn.metrics import accuracy_score\n\n\nfrom sklearn.linear_model import LogisticRegression\nfrom xgboost import XGBClassifier\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\n\nfrom sklearn.linear_model import LogisticRegression\nfrom xgboost import XGBClassifier\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/parkinsonsxyz/parkinsons2.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot histograms for each variable\ndf.hist(figsize=(20,12))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['status'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"percentage_of_disease = 147 / (147 + 48) * 100\npercentage_of_not_having_disease = 48 / (147 + 48) * 100\nprint('percentage of having disease' , percentage_of_disease)\nprint('percentage of not having disease' , percentage_of_not_having_disease)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot(x='status',kind='count',data=df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Health status:-\n\n1= parkinsons\n\n0=healthy\n\nMore number of peoples are suffering from parkisons disease."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"col={'MDVP:Fo(Hz)': 1, 'MDVP:Fhi(Hz)':2, 'MDVP:Flo(Hz)':3, 'MDVP:Jitter(%)':4,\n       'MDVP:Jitter(Abs)':5, 'MDVP:RAP':6, 'MDVP:PPQ':7, 'Jitter:DDP':8,\n       'MDVP:Shimmer':9, 'MDVP:Shimmer(dB)':10, 'Shimmer:APQ3':11, 'Shimmer:APQ5':12,\n       'MDVP:APQ':13, 'Shimmer:DDA':14, 'NHR':15, 'HNR':16, 'RPDE':17, 'DFA':18, 'spread1':19,\n       'spread2':20, 'D2':21, 'PPE':22}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,30))\n\nfor variable,i in col.items():\n                     plt.subplot(5,5,i)\n                     plt.boxplot(df[variable])\n                     plt.title(variable)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Removing the outliers from the features.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"q1 = df.quantile(0.25)\nq2 = df.quantile(0.5)\nq3 = df.quantile(0.75)\nIQR = q3-q1\nprint(IQR)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_out = df[~((df < (q1 - 1.5* IQR)) | (df > (q3 + 1.5 * IQR))).any(axis =1)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,30))\n\nfor variable,i in col.items():\n                     plt.subplot(5,5,i)\n                     plt.boxplot(df_out[variable])\n                     plt.title(variable)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#sns.pairplot(df_out , hue = 'status')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Splitting the dependent and independent variables before modelling."},{"metadata":{"trusted":true},"cell_type":"code","source":"x=df.drop(['status'],axis=1)\ny=df['status']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Scaling the values between -1 and 1 before applying the three models."},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler=MinMaxScaler((-1,1))\nx=scaler.fit_transform(x)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Splitting up the datasets into 80% training set and 20% test set."},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train,x_test,y_train,y_test=train_test_split(x, y, test_size=0.2, random_state=7)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = np.array(df_out.drop(['status'],axis=1))\ny = np.array(df_out['status'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Applying XGBoost Classifier first, I have not done any hyperparameter tuning and optimization for this dataset but you can can try this will surely help in increasing the accuracy upto a certain level."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix\nxgb= XGBClassifier()\nxgb.fit(x_train,y_train)\ny_pred=xgb.predict(x_test)\nprint(classification_report(y_test,y_pred))\naccuracy1=xgb.score(x_test,y_test)\nprint (accuracy1*100,'%')\ncm = confusion_matrix(y_test, y_pred)\nsns.heatmap(cm, annot= True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**XG BOOST CLASSIFIER IS GIVING US A GOOD ACCURACY OF 95% WITH GOOD PRECISION AND RECALL SCORE.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier \nfrom sklearn.tree import DecisionTreeClassifier\n\ndes_class=DecisionTreeClassifier()\ndes_class.fit(x_train,y_train)\ndes_predict=des_class.predict(x_test)\nprint(classification_report(y_test,des_predict))\naccuracy3=des_class.score(x_test,y_test)\nprint(accuracy3*100,'%')\ncm = confusion_matrix(y_test, des_predict)\nsns.heatmap(cm, annot= True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**DECISION TREE CLASSIFIER IS GIVING A ACCURACY OF 85% WHICH IS NOT THAT BAD BUT LESSS IN COMPARISION XG BOOST CLASSIFIER. IN HEALTHCARE SECTOR WE NEED TO MAKE SURE THAT WE ARE HAVING A GOOD ACCURACY AS WE CANNOT COMPROMISE WRONG PREDICTIONS WITH PATIENTS LIFE WHILE PREDICTING DISEASE.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes  import GaussianNB \nfrom sklearn.metrics import classification_report, confusion_matrix\nnvclassifier = GaussianNB()\nnvclassifier .fit(x_train,y_train)\ny_pred=nvclassifier .predict(x_test)\nprint(classification_report(y_test,y_pred))\nprint(accuracy_score(y_pred,y_test)*100,'%')\ncm = confusion_matrix(y_test, y_pred)\nsns.heatmap(cm, annot= True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***XG BOOST CLASSIFIER IS GIVING US A GOOD ACCURACY OF 95% WITH GOOD PRECISION AND RECALL SCORE.We can stick with this model for now.***"},{"metadata":{},"cell_type":"markdown","source":"**Thank You please upvote if you like the kernel.**"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}