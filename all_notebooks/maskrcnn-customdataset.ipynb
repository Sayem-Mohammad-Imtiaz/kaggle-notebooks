{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"## setting up environment\n!git clone --quiet https://github.com/matterport/Mask_RCNN.git\n%cd Mask_RCNN\n\n!pip install -q PyDrive\n!pip install -r requirements.txt\n!python setup.py install\n!pip install tensorflow==1.5.0\n!pip install keras==2.3.1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1uANhery-FjzCOXdPe2kNXgB_5XfRX2go' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1uANhery-FjzCOXdPe2kNXgB_5XfRX2go\" -O dataset.zip && rm -rf /tmp/cookies.txt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!unzip dataset.zip","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%writefile custom_data_train.py\n\nimport os\nimport sys\nimport json\nimport datetime\nimport numpy as np\nimport skimage.draw\nimport cv2\nfrom mrcnn.visualize import display_instances\nimport matplotlib.pyplot as plt\n\n# Root directory of the project\nROOT_DIR = \"/kaggle/working/Mask_RCNN/\"\n\n# Import Mask RCNN\nsys.path.append(ROOT_DIR)  # To find local version of the library\nfrom mrcnn.config import Config\nfrom mrcnn import model as modellib, utils\n\n# Path to trained weights file\nCOCO_WEIGHTS_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_coco.h5\")\n\n# Directory to save logs and model checkpoints, if not provided\n# through the command line argument --logs\nDEFAULT_LOGS_DIR = os.path.join(ROOT_DIR, \"logs\")\n\n\n\nclass CustomConfig(Config):\n    \"\"\"Configuration for training on the custom  dataset.\n    Derives from the base Config class and overrides some values.\n    \"\"\"\n    # Give the configuration a recognizable name\n    NAME = \"object\"\n\n    # We use a GPU with 12GB memory, which can fit two images.\n    # Adjust down if you use a smaller GPU.\n    IMAGES_PER_GPU = 2\n\n    # Number of classes (including background)\n    NUM_CLASSES = 1 + 2  # Background + phone,laptop and mobile\n\n    # Number of training steps per epoch\n    STEPS_PER_EPOCH = 10\n\n    # Skip detections with < 90% confidence\n    DETECTION_MIN_CONFIDENCE = 0.9\n\n############################################################\n#  Dataset\n############################################################\n\nclass CustomDataset(utils.Dataset):\n\n    def load_custom(self, dataset_dir, subset):\n        \"\"\"Load a subset of the Dog-Cat dataset.\n        dataset_dir: Root directory of the dataset.\n        subset: Subset to load: train or val\n        \"\"\"\n        # Add classes. We have only one class to add.\n        self.add_class(\"object\", 1, \"Horse\")\n        self.add_class(\"object\", 2, \"Man\")\n\n        # Train or validation dataset?\n        assert subset in [\"train\", \"val\"]\n        dataset_dir = os.path.join(dataset_dir, subset)\n        print(dataset_dir)\n        # Load annotations\n        # VGG Image Annotator saves each image in the form:\n        # { 'filename': '28503151_5b5b7ec140_b.jpg',\n        #   'regions': {\n        #       '0': {\n        #           'region_attributes': {},\n        #           'shape_attributes': {\n        #               'all_points_x': [...],\n        #               'all_points_y': [...],\n        #               'name': 'polygon'}},\n        #       ... more regions ...\n        #   },\n        #   'size': 100202\n        # }\n        # We mostly care about the x and y coordinates of each region\n        annotations1 = json.load(open('Dataset/train/via_project.json'))\n        # print(annotations1)\n        annotations = list(annotations1.values())  # don't need the dict keys\n\n        # The VIA tool saves images in the JSON even if they don't have any\n        # annotations. Skip unannotated images.\n        annotations = [a for a in annotations if a['regions']]\n        \n        # Add images\n        for a in annotations:\n            # print(a)\n            # Get the x, y coordinaets of points of the polygons that make up\n            # the outline of each object instance. There are stores in the\n            # shape_attributes (see json format above)\n            polygons = [r['shape_attributes'] for r in a['regions']] \n            objects = [s['region_attributes']['name'] for s in a['regions']]\n            print(\"objects:\",objects)\n            #name_dict = {\"laptop\": 1,\"tab\": 2,\"phone\": 3}\n            name_dict = {\"Horse\": 1,\"Man\": 2} #,\"xyz\": 3}\n            # key = tuple(name_dict)\n            num_ids = [name_dict[a] for a in objects]\n     \n            # num_ids = [int(n['Event']) for n in objects]\n            # load_mask() needs the image size to convert polygons to masks.\n            # Unfortunately, VIA doesn't include it in JSON, so we must read\n            # the image. This is only managable since the dataset is tiny.\n            print(\"numids\",num_ids)\n            image_path = os.path.join(dataset_dir, a['filename'])\n            image = skimage.io.imread(image_path)\n            height, width = image.shape[:2]\n\n            self.add_image(\n                \"object\",  ## for a single class just add the name here\n                image_id=a['filename'],  # use file name as a unique image id\n                path=image_path,\n                width=width, height=height,\n                polygons=polygons,\n                num_ids=num_ids\n                )\n\n    def load_mask(self, image_id):\n        \"\"\"Generate instance masks for an image.\n       Returns:\n        masks: A bool array of shape [height, width, instance count] with\n            one mask per instance.\n        class_ids: a 1D array of class IDs of the instance masks.\n        \"\"\"\n        # If not a Dog-Cat dataset image, delegate to parent class.\n        image_info = self.image_info[image_id]\n        if image_info[\"source\"] != \"object\":\n            return super(self.__class__, self).load_mask(image_id)\n\n        # Convert polygons to a bitmap mask of shape\n        # [height, width, instance_count]\n        info = self.image_info[image_id]\n        if info[\"source\"] != \"object\":\n            return super(self.__class__, self).load_mask(image_id)\n        num_ids = info['num_ids']\n        mask = np.zeros([info[\"height\"], info[\"width\"], len(info[\"polygons\"])],\n                        dtype=np.uint8)\n        for i, p in enumerate(info[\"polygons\"]):\n            # Get indexes of pixels inside the polygon and set them to 1\n        \trr, cc = skimage.draw.polygon(p['all_points_y'], p['all_points_x'])\n\n        \tmask[rr, cc, i] = 1\n\n        # Return mask, and array of class IDs of each instance. Since we have\n        # one class ID only, we return an array of 1s\n        # Map class names to class IDs.\n        num_ids = np.array(num_ids, dtype=np.int32)\n        return mask, num_ids #np.ones([mask.shape[-1]], dtype=np.int32)\n\n    def image_reference(self, image_id):\n        \"\"\"Return the path of the image.\"\"\"\n        info = self.image_info[image_id]\n        if info[\"source\"] == \"object\":\n            return info[\"path\"]\n        else:\n            super(self.__class__, self).image_reference(image_id)\ndef train(model):\n    \"\"\"Train the model.\"\"\"\n    # Training dataset.\n    dataset_train = CustomDataset()\n    dataset_train.load_custom(\"Dataset\", \"train\")\n    dataset_train.prepare()\n\n    # Validation dataset\n    dataset_val = CustomDataset()\n    dataset_val.load_custom(\"Dataset\", \"val\")\n    dataset_val.prepare()\n\n    # *** This training schedule is an example. Update to your needs ***\n    # Since we're using a very small dataset, and starting from\n    # COCO trained weights, we don't need to train too long. Also,\n    # no need to train all layers, just the heads should do it.\n    print(\"Training network heads\")\n    model.train(dataset_train, dataset_val,\n                learning_rate=config.LEARNING_RATE,\n                epochs=10,\n                layers='heads')\n\t\t\t\t\n\t\t\t\t\n\t\t\t\t\nconfig = CustomConfig()\nmodel = modellib.MaskRCNN(mode=\"training\", config=config,\n                                  model_dir=DEFAULT_LOGS_DIR)\n\nweights_path = COCO_WEIGHTS_PATH\n        # Download weights file\nif not os.path.exists(weights_path):\n  utils.download_trained_weights(weights_path)\n\nmodel.load_weights(weights_path, by_name=True, exclude=[\n            \"mrcnn_class_logits\", \"mrcnn_bbox_fc\",\n            \"mrcnn_bbox\", \"mrcnn_mask\"])\n\ntrain(model)\t\t\t","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!wget https://github.com/matterport/Mask_RCNN/releases/download/v2.0/mask_rcnn_coco.h5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install tensorflow==1.13.1\n!pip install keras==2.1.1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!python custom_data_train.py train --dataset=Dataset/ --weights=coco","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}