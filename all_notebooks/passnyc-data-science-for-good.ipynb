{"cells":[{"metadata":{"_uuid":"33b054013e337a16e72bb4c308ec2139d666beea"},"cell_type":"markdown","source":"# Introduction\n\nIn this notebook we tackle the Data Science for Good Challenge posted by NYCPASS. The main goal of this challenge is to support NYCPASS in their endeavour of enabling underrepresented students gain access to specialized high schools in NYC by providing a number of support programs. \n\nOur analysis will begin from a relatively broad perspective, where we will try to identify the main variables determining success for students. In doing that we will start from the premise that access to these specialized schools not only depends on academic criteria but also on personal elements. As we will show below, the data points towards a scenario where the academic component is strongly affected by elements like the economic situation of students and schools, however, it will also provide hints telling us that some students may be failing to even take the test perhaps due to a lack of confidence or a supportive environment.\n\nThe notebook is structured in the following sections\n\n- **Data preparation**: here we import all the required data, including the minimum of two NYC Kaggle datasets demanded by competition rules, and carried out processing of the data. In a first read or for readers more interested in the actual data analysis, we recommend skipping this section.\n- **Data analysis**: this section contains the actual data analysis. It is itself broken down into two subsections: the big picture, and a more in-depth analysis, where in the former we look at the data from a more global perspective, whereas on the latter we attempt to look into more specific details. In the latter section we also take the time to dwelve into the difference in performance between community and non-community schools, a characteristic which plays a significant role.\n- **Targeting NYCPASS's support**: in this section we use the insights gained in the data analysis section to develop an ad-hoc score which should enable PASSNYC in making a decision of which schools to prioritize. We present a sample of the highest priority schools, according to this ad-hoc score, and also a geographical visualization of schools where we emphasize those in more urgent need of assistance.\n- **Conclusions**: here we present a brief conclusion of our findings."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"# Data Preparation"},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true,"collapsed":true,"_uuid":"69461c7bfb499281f3670c08451d54ad5cb7f10c"},"cell_type":"code","source":"import pandas as pd\nimport numpy as np","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true,"_uuid":"7ac46dd92d347f5b5417a818f8ebea6051ae7d50","collapsed":true},"cell_type":"code","source":"# Read in main data\n\n## This DataFrame will be our main DataFrame, so we will add additional data from other sources below\nschoolExplorer2016File = '../input/data-science-for-good/2016 School Explorer.csv'\ndf = pd.read_csv(schoolExplorer2016File,header=0)\n\n# Convert percentage fields into numerical values, add new features, etc.\n\ndf['Average Proficiency'] = df.loc[:,'Average ELA Proficiency':'Average Math Proficiency'].mean(axis=1)\n\n# Normalize percentage rates\npercentageFields =  list(df.filter(like='Percent').columns.values) + \\\n                    list(df.filter(like='Rate').columns.values) + \\\n                    list(df.filter(like='%').columns.values)\n        \nfillRate = {}\nminRate = 0.01 # Arbitrary value \nfor f in percentageFields:\n    # Transform to float\n    fillRate[f] = df[f].dropna().apply(lambda x: float(x[:-1])/100).mean()\n    df.fillna(value={f : str(fillRate[f])+'%'},inplace=True)\n    df[f]=df[f].apply(lambda x: float(x[:-1])/100)\n    \n    # Normalize\n    df.loc[df[f]<minRate, f] = minRate\n    \ndef remComma(x):\n    if isinstance(x,str):\n        x = float(x[1:].replace(',',''))\n        \n    return x\n    \nfillRate['School Income Estimate'] = df['School Income Estimate'].dropna().apply(lambda x: float(x[1:].replace(',',''))).mean()\ndf['School Income Estimate']=df['School Income Estimate'].apply(remComma)\ndf.fillna(value={'School Income Estimate' : fillRate['School Income Estimate']},inplace=True);\n\n# There is a typo on the 'Grade 3 Math - All Students Tested' label (Tested->tested)\ndf['Grade 3 Math - All Students Tested'] = df['Grade 3 Math - All Students tested']\ndf.drop(labels=['Grade 3 Math - All Students tested'], axis=1, inplace=True)\n\n# Compute percentage of outstanding students in sample\nfor g in range(3,9): \n    df['Grade ' + str(g) + ' ELA 4s %'] = df['Grade ' +str(g)+' ELA 4s - All Students'].map(float) / df['Grade ' + str(g) + ' ELA - All Students Tested']\n    df['Grade ' + str(g) + ' Math 4s %'] = df['Grade ' +str(g)+' Math 4s - All Students'].map(float) / df['Grade ' + str(g) + ' Math - All Students Tested']\n    df['Grade ' + str(g) + ' 4s %'] = df[['Grade ' + str(g) + ' ELA 4s %', 'Grade ' + str(g) + ' Math 4s %']].mean(axis=1)\n    \n# Simply fill with zeros the entries for schools with no students taking exams\ndf[df.filter(like='4s %').columns]=df[df.filter(like='4s %').columns].fillna(-0.1)\n\n# Convert coordinates into Web Mercator Format (we will need this when plotting maps)\ndef wgs84_to_web_mercator(df, lon=\"Longitude\", lat=\"Latitude\"):\n    \"\"\" \n        Converts decimal longitude/latitude to Web Mercator format\n        Code taken from the Bokeh tutorial on Geoplotting\n        https://hub.mybinder.org/user/bokeh-bokeh-notebooks-xq55q0f7/notebooks/tutorial/09%20-%20Geographic%20Plots.ipynb\n    \"\"\"\n    k = 6378137 # earth's radius in m\n    df[lon+'WebMercator'] = df[lon] * (k * np.pi/180.0)\n    df[lat+'WebMercator'] = np.log(np.tan((90 + df[lat]) * np.pi/360.0)) * k\n    return df\n\nwgs84_to_web_mercator(df)\n\n# Sort entries by Location Code (will be useful later on)\ndf.sort_values(by='Location Code', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"f3cd17172ec82ff796659682030df184cffc1be1","collapsed":true},"cell_type":"code","source":"# Read in NYC DOE high school directory data\n\ndoeHSDir2016File = '../input/nyc-high-school-directory/2016-doe-high-school-directory.csv'\ndoeHSDir2015File = '../input/nyc-high-school-directory/2014-2015-doe-high-school-directory.csv'\nHSDir2016 = pd.read_csv(doeHSDir2016File,header=0)\nHSDir2015 = pd.read_csv(doeHSDir2015File,header=0)\n\n# Prepare DOE data\n\nlabelCols = {'dbn':'DBN'}\nnumericCols = {'total_students':'Total Students'}\nstringCols = {'extracurricular_activities':'Extracurricular Activities',\n                   'language_classes':'Language Classes',\n                   'advancedplacement_courses':'Advanced Placement Courses',\n                   'school_sports':'School Sports',\n                   'partner_highered':'Partner Higher Ed',\n                   'partner_cultural':'Partner Cultural',\n                   'partner_financial':'Partner Financial'}\n\ninterestingCols = {**labelCols, **numericCols, **stringCols}\n    \nHSDir2016.rename(columns=interestingCols,inplace=True)\nHSDir2015.rename(columns=interestingCols,inplace=True)\n\nHSDir2016.sort_values(by='DBN',inplace=True)\nHSDir2015.sort_values(by='DBN',inplace=True)\n\nhsDirFilter = HSDir2015['DBN'].isin(HSDir2016['DBN'].values)\n\n# Generate a unified DataFrame for both 2015 and 2016.\nHSDir = HSDir2016[list(interestingCols.values())].copy()\n\ncommonSchoolFilter2016 = HSDir['DBN'].isin(HSDir2015['DBN'].values)\ncommonSchoolFilter2015 = HSDir2015['DBN'].isin(HSDir['DBN'].values)\n\nHSDir.loc[commonSchoolFilter2016 , 'Total Students'] = 0.5*(HSDir.loc[commonSchoolFilter2016 , 'Total Students'].values + HSDir2015.loc[commonSchoolFilter2015 , 'Total Students'].values)\n\ndef cleanSemicolons(s):\n    if isinstance(s,str):\n        s=s.replace(';',',')\n    \n    return s\n\nfor col in stringCols.values():\n    for l, r in zip(HSDir.loc[commonSchoolFilter2016 , col].values , HSDir2015.loc[commonSchoolFilter2015 , col].values):\n        if isinstance(l,str) and isinstance(r,str):\n            l += ',' + r\n        elif isinstance(r,str):\n            l = r\n            \n    HSDir.loc[commonSchoolFilter2016 , col] = HSDir.loc[commonSchoolFilter2016 , col].map(cleanSemicolons)\n    \nHSDir = pd.concat([HSDir , HSDir2015.loc[~hsDirFilter,interestingCols.values()]], ignore_index=True)\nHSDir.sort_values(by='DBN', inplace=True)\n\n# Now we add all this data to the 'main' DataFrame\nHSDirFilter = HSDir['DBN'].isin(df['Location Code'].values)\nDFFilter = df['Location Code'].isin(HSDir['DBN'].values)\n\ndef countCommas(s):\n    if isinstance(s,str):\n        return 1 + s.count(',')\n    else:\n        return 0\n\ndf['Total Students'] = pd.Series(np.NaN, index=df.index)\ndf.loc[DFFilter , 'Total Students'] = HSDir.loc[HSDirFilter, 'Total Students'].values \n\n# Fill missing values with a simple estimate. We distinguish between school type because of the analysis below\ncommSchoolFilter = df['Community School?']=='Yes'\ndf.loc[~DFFilter & commSchoolFilter , 'Total Students'] = df.loc[commSchoolFilter,'Total Students'].mean()\ndf.loc[~DFFilter & ~commSchoolFilter , 'Total Students'] = df.loc[~commSchoolFilter,'Total Students'].mean()\n\nfor col in stringCols.values():\n    df[col] = pd.Series(np.NaN, index=df.index)\n    df.loc[DFFilter , col] = HSDir.loc[HSDirFilter, col].map(countCommas).values\n    df[col].fillna(0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"5134125b3dad1ac476c2267f5b72401c751b0273","collapsed":true},"cell_type":"code","source":"# Read in NYC School Demographics and Accountability data\n\nschoolDemographicsFile = '../input/ny-school-demographics-and-accountability-snapshot/2006-2012-school-demographics-and-accountability-snapshot.csv'\nHSDemo = pd.read_csv(schoolDemographicsFile,header=0)\n\n# Prepare NYC School Demographics and Accountability data\n\nHSDemo.sort_values(by='DBN')\n\n## Replace spurious strings being read in in numeric fields by NaNs\ndef replaceString(x):\n    if x == '    ':\n        x = np.NaN\n    else:\n        x = float(x)\n    \n    return x\n\ngradeCountCols = HSDemo.filter(like='grade').columns.values\nfor l in gradeCountCols:\n    HSDemo.loc[:,l] = HSDemo.loc[:,l].map(replaceString).values\n\n    # Fill NaNs by zeros in student counts for all grades\n    HSDemo.loc[:,l].fillna(np.NaN, inplace=True)\n\n# Consider only average counts\nHSDemo = HSDemo.groupby('DBN').mean()\nHSDemo.reset_index(inplace=True)\n\ngenderCols = {'male_per':'Male %', 'female_per':'Female %'}\nHSDemo.rename(columns=genderCols,inplace=True)\n\n## Insert student counts to main DataFrame\nHSDemoFilter = HSDemo['DBN'].isin(df['Location Code'].values)\nDFFilter = df['Location Code'].isin(HSDemo['DBN'].values)\n\nfor col in list(gradeCountCols)+list(genderCols.values()):\n    df[col] = pd.Series(np.NaN, index=df.index)\n    df.loc[DFFilter,col] = HSDemo.loc[HSDemoFilter, col].values","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"collapsed":true,"_uuid":"660f520d1f9c763c6f2b249320dc7b388fcb70eb"},"cell_type":"code","source":"# Read in TNYTimes data\nTNYTimesFile = '../input/the-new-york-times-nyc-shsat-data/nyc-shsat-data.csv'\nTNYTimes = pd.read_csv(TNYTimesFile,header=0)\n\n# Prepare TNYTimes data\n\nTNYTimes.drop(labels=['Unnamed: 4','Source: New York City Department of Education. ^Data suppressed for values of 5 students or fewer.'], axis=1, inplace=True)\nTNYTimes.sort_values(by='DBN', inplace=True)\n\ndef replaceSmall(s):\n    '''All values below 6 were replaced by an \\'s\\' in the original data. This functions replaces that placeholder for NaNs.'''\n    if s=='s' or s=='s^':\n        s = 0 # np.NaN\n    return s\n\nTNYTimes['Offers'] = TNYTimes['Offers'].map(replaceSmall)\nTNYTimes['Testers'] = TNYTimes['Testers'].map(replaceSmall)\n\n# Insert data into main DataFrame\ndf['SHSAT Takers'] = 0 # np.NaN\ndf['SHSAT Offers'] = 0 # np.NaN\n\nhsFilter = TNYTimes['DBN'].isin(df['Location Code'].values)\ndfFilter = df['Location Code'].isin(TNYTimes['DBN'].values)\n\ndf.loc[dfFilter , 'SHSAT Takers'] = TNYTimes.loc[hsFilter, 'Testers'].map(float).values\ndf.loc[dfFilter , 'SHSAT Offers'] = TNYTimes.loc[hsFilter, 'Offers'].map(float).values\n\ndf['SHSAT Takers %'] = df['SHSAT Takers'] / df['Total Students']\ndf['SHSAT Offers %'] = df['SHSAT Offers'] / df['SHSAT Takers']\ndf['SHSAT Offers %'].fillna(0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e455e33be026bc348baabd5d9899a9cb88582a7a"},"cell_type":"markdown","source":"# Data Analysis"},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true,"collapsed":true,"_uuid":"fa9c510e72e2ef34aa40b4e2f1dccf8d94999f40"},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sb; sb.set()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b7d58db7bc7103727592feaf88e1bc95ed89e938"},"cell_type":"markdown","source":"## The big picture\nTo begin our analysis we try to find simple trends in the data that can be used to identify which variables appear to be playing a key role in a school's performance. This will allows us to gain some intuition regarding correlations present in the data. "},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"ece6c464e8834622546c6991b5fdc1a33fce0695"},"cell_type":"code","source":"meanAverageProficiency = df['Average Proficiency'].mean()\nmedianAverageProficiency = df['Average Proficiency'].median()\nmedianSchoolIncomeEstimate = df['School Income Estimate'].median()\ndf.fillna(value={'Average Proficiency' : meanAverageProficiency},inplace=True);\n\nprint('Mean Average Proficiency = ' + str(meanAverageProficiency))\nprint('Median Average Proficiency = ' + str(medianAverageProficiency))\nprint('Median School Income Estimate = ' + str(medianSchoolIncomeEstimate))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ff9c161503a46c08140e15a2d22562423de7411d"},"cell_type":"markdown","source":"### Three key variables for academic proficiency\n\nWe were able to identify three variables exhibiting a clear effect on average proficiency, namely:\n- Economic Need Index\n- Chronical Absence\n- Estimated School Income\n\nAs we will see below, these variables serve not only as a proxy for academic performance but could also, possibly, hint towards issues arising outside school boundaries, e.g., in a student's household, hindering a student's chances of entering a specialized school. "},{"metadata":{"_uuid":"31120cf7be19ad7d7d2ee8c474dad2db180dc74f"},"cell_type":"markdown","source":"#### Economic Need Index (ENI)\n\nAs the data below shows, there is a clear anticorrelation between a school's Economic Need Index and the average proficiency of it's students, i.e., the larger the ENI a school exhibits the lower its students are likely to score on the SHSAT. Statistically this anticorrelation can be measured using Pearson's $R$ coefficient ($|R| \\leq 1$, with the extremes showing maximal (anti-)correlations), for which we find $R=-0.76$. The black line indicates the median of all school average proficiencies."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"63cf7e7dfb781a7310d6aaa8bca66f4e3f6abe95"},"cell_type":"code","source":"g=sb.jointplot(df['Economic Need Index'],df['Average Proficiency'],\n           kind='kde',color='b');\n\nXs = [df['Economic Need Index'].min(), df['Economic Need Index'].max()]\nMeds=[meanAverageProficiency,meanAverageProficiency]\ng.ax_joint.plot(Xs, Meds, '--k')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"639396e05b0f1b1ec9c86556232f951050fe4652"},"cell_type":"markdown","source":"#### Chronical Absence\n\nChronical absence is the second variable showing a strong degree of anticorrelation with average performance. In this case we find $R=-0.5$. \n\nJust as interestingly the distribution appears to exhibit a bimodal structure separating schools which exhibited higher scores from those which exhibited lower scores. "},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"30c7e9d4932be33d21b3e7301a5f9d5c9d949ba2"},"cell_type":"code","source":"g=sb.jointplot(df['Percent of Students Chronically Absent'],df['Average Proficiency'],\n           kind='kde',color='b')\n\nXs = [df['Percent of Students Chronically Absent'].min(), df['Percent of Students Chronically Absent'].max()]\nMeds=[meanAverageProficiency,meanAverageProficiency]\ng.ax_joint.plot(Xs, Meds, '--k')\n\nsb.jointplot(df['Economic Need Index'],df['Percent of Students Chronically Absent'],\n           kind='kde',color='b');","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bd5e37586b47d2a72890d399d2faa4435edcd364"},"cell_type":"markdown","source":"To identify these clusters we employ the so-called Ward Agglomerative Clustering technique and plot joint and marginal distributions by cluster, where we denote the cluster of schools with low chronical absence/high average proficiency in blue and the high chronical absence/low average proficiency in red.\n\nWe find that the cluster with schools obtaining higher scores (blue) is centered around a chronical absence rate of about $10\\%$ and exhibits a median score about $70\\%$ higher than that of the cluster of schools with lower scores. This latter cluster is centered at a chronical absence of about $30\\%$."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"a16d646494df127d550f051aa2a6c1710e7121a4"},"cell_type":"code","source":"from sklearn.cluster import AgglomerativeClustering\n\nclusterer = AgglomerativeClustering(n_clusters=2, linkage= 'ward')\nclusterLabels = clusterer.fit_predict(df[['Average Proficiency','Percent of Students Chronically Absent']])\n\ncluster0 = clusterLabels == 0\ncluster1 = clusterLabels == 1\n\ng = sb.JointGrid(df['Percent of Students Chronically Absent'][cluster1],df['Average Proficiency'][cluster1],\n                xlim=[df['Percent of Students Chronically Absent'].min(),df['Percent of Students Chronically Absent'].max()],\n                ylim=[df['Average Proficiency'].min(),df['Average Proficiency'].max()])\n\ng = g.plot_joint(sb.kdeplot, cmap='Blues_d')\ng = g.plot_marginals(sb.distplot, color=\"b\")\ng.x = df['Percent of Students Chronically Absent'].values[cluster0]\ng.y = df['Average Proficiency'].values[cluster0]\ng = g.plot_joint(sb.kdeplot, cmap='Reds_d')\ng = g.plot_marginals(sb.distplot, color=\"r\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9c810013eaf1ec83c9824b36a032521edb891874"},"cell_type":"markdown","source":"#### Estimated School Income (ESI)\n\nSchool income appears to be a variable providing a clear threshold separating two subgroups of schools with low scores and high scores. What we find is that ESI exhibits a median of about $48$K USD (shown by the red dashed line); there is, however, a considerable number of schools lying significantly below this value, the vast majority of which have average proficiencies below the median for all schools (shown by the black dashed line). This provides a hint that schools with incomes significantly below the median are very likely to exhibit scores below the median score."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"f975d0565284580ee43bace504eebe1cf40dc635"},"cell_type":"code","source":"g=sb.jointplot(df['School Income Estimate'],df['Average Proficiency'],\n           kind='kde',color='b',n_levels=15)\n\nXs = [df['School Income Estimate'].min(), df['School Income Estimate'].max()]\nMeds=[meanAverageProficiency,meanAverageProficiency]\ng.ax_joint.plot(Xs, Meds, '--k')\n\nYs = [df['Average Proficiency'].min(), df['Average Proficiency'].max()]\nMeds=[medianSchoolIncomeEstimate,medianSchoolIncomeEstimate]\n\ng.ax_joint.plot(Meds, Ys, '--r');","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9429bd582050489dc1951cbe975f597c990d7767"},"cell_type":"markdown","source":"### SHSAT participation rates"},{"metadata":{"_kg_hide-input":true,"trusted":true,"collapsed":true,"_uuid":"1780cbf181702de28f283e93d0498be77efd4a39"},"cell_type":"code","source":"ratingFields = df.filter(like='Rating').columns.values\nratingValues = df['Supportive Environment Rating'].dropna().unique()\n\ndfTakersByRating = pd.DataFrame(columns=ratingFields,index=['Meeting/Exceeding Target','Not Meeting/Approaching Target'])\n\nfor rf in ratingFields:\n    filt = df[rf].isin(['Exceeding Target','Meeting Target'])\n    dfTakersByRating.loc['Meeting/Exceeding Target',rf] = df.loc[filt, 'SHSAT Takers %'].mean()\n    \n    filt = df[rf].isin(['Approaching Target','Not Meeting Target'])\n    dfTakersByRating.loc['Not Meeting/Approaching Target',rf] = df.loc[filt, 'SHSAT Takers %'].mean()\n\ndfTakersByRating = dfTakersByRating.reset_index(col_fill='Value').rename(columns={'index':'Score'})\ndfTakersByRating = dfTakersByRating.melt(id_vars=['Score'], value_vars=ratingFields, var_name='Rating', value_name='Average SHSAT Takers %')\ndfTakersByRating['Rating'] = dfTakersByRating['Rating'].map(lambda s: s.replace(' Rating',''))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d9e28578ac733f713b87469cff756fe292e68ba8"},"cell_type":"markdown","source":"Part of gaining access to a specialized school requires that students not only be prepared fo the demands of the SHSAT but also, and perhaps just as importantly, that the students feel confident/supported enough to actually take the exam.\n\nThe results in the figure below show quite clearly how schools offering collaborative teachers, a supportive environment, rigorous instruction, as well as proper student achievement, manage to get more of their students to take the SHSAT. Interestingly, strong family-community ties appear to reduce the number of students taking the exam. Understanding this will require additional efforts.\n\nThis observation is important since it shows that it should be possible, in principle, to achieve an increased enrollment of underrepresented minorities in specialized high schools also by endowing students with the confidence that they too are capable of succeeding both in the exam as well as inside a specialized school."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"c237e406241802f953e5477115ecc7b5ed2c4e99"},"cell_type":"code","source":"fig, (ax1) = plt.subplots(figsize=(21, 8), ncols=1)\nsb.barplot(x=\"Rating\", y=\"Average SHSAT Takers %\", hue='Score', data=dfTakersByRating, ax=ax1)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6ff1f1d8016128065707558f91415f8f17e402de"},"cell_type":"markdown","source":"## A more in-depth analysis"},{"metadata":{"_uuid":"195c4efc0d3b40c944c2ed18dd29f0c712beb9bd"},"cell_type":"markdown","source":"### The case of community schools\n\nAccording to our estimates, community schools account for roughly $5\\%$ of the total student population in NYC. This might seem like giving them additional consideration might not be worth the effort; however, as we shall show below, community schools do show signs worthy of concern in terms of academic performance and access to specialized high schools, which make them natural candidates for NYCPASS' programs."},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true,"collapsed":true,"_uuid":"8d3a0390534f61e3892213cf8eeb8c81f85feed0"},"cell_type":"code","source":"dfBySchoolType = df.groupby('Community School?').mean()\ndfBySchoolType.reset_index(inplace=True)\n\npercentCols=['Percent Asian','Percent Black / Hispanic', 'Percent White']\n\ndfMinorityPercentage = dfBySchoolType.melt(id_vars=['Community School?'], value_vars=percentCols,var_name='Group', value_name='Percentage')\n\ndfMinorityPercentage['Group'] = dfMinorityPercentage['Group'].map(lambda s: s.replace('Percent ','')).values","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6b4be7b04c44d87c573deecb0f1497bfd6f04d0e"},"cell_type":"markdown","source":"##### Low academic performance and high rates of chronical absence\n\nAs we show below community schools systematically exhibit deficiencies when compared to non-community schools; first, their overall scores fall on the lowest quartile of non-community school scores, with only a few outliers obtaining results comparable to the median performance of non-community schools. Second, the median percent of students which are chronically absent is nearly twice as large for community schools when compared to non-community schools. This strengthens our observation above regarding the strong connection between the number of chronically absent students and lower academic performance, even more, it shows that community schools appear to suffer severyle from this connection."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"aea7390b5eb20de68d4440f1d9f97fd63faf25da"},"cell_type":"code","source":"fig, (ax1,ax2) = plt.subplots(figsize=(21, 8), ncols=2)\nsb.boxplot(df['Community School?'],df['Average Proficiency'], ax=ax1)\nsb.boxplot(df['Community School?'],df['Percent of Students Chronically Absent'], ax=ax2)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"98fba606fa453cc984c16ee86688d44649bd814f"},"cell_type":"markdown","source":"##### Large concentration of Black/Hispanic students\nAs the following plot illustrates not only do community schools suffer from a severe lower than average academic performance but their student bodies tend to have a much larger proportion of Black/Latino students. Since it is precisely these communities which tend to suffer from access rates to specialized schools, this makes community schools a group of very interesting targets for NYCPASS' support programs."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"ea9cb0ae887a1c42ebf1b091a1631dfbff18c94a"},"cell_type":"code","source":"fig, (ax1) = plt.subplots(figsize=(21, 8), ncols=1)\nsb.barplot(x=\"Group\", y=\"Percentage\", hue='Community School?', data=dfMinorityPercentage, ax=ax1)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6a727576e2e58175143370d2d57c1f65073207bd"},"cell_type":"markdown","source":"##### A need for support systems beyond classroom topics?\n\nA point which draws our attention is the fact that community schools exhibit average numbers of extracurricular activities, advanced placement programs, language classes and school sports. Naively, we would have expected such numbers to be proxies for improvements inside the classroom, however, as we have pointed out above, they remain notorious under performers. This leads us to think that the most pressing issues for kids in these schools might originate somewhere outside of school, i.e., it leads us to conjecture that these kids might be exposed to detrimental factors like family/gang violence, as well as perhaps depression, etc.\n\nWe believe NYPASS' programs focusing on the strengthening of family ties, as well as emotional and mental coaching or mentoring could prove useful for kids in community schools."},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true,"collapsed":true,"_uuid":"8319e9896f191e2ce455a41b1b9996820aa231cf"},"cell_type":"code","source":"specialActivities=['Extracurricular Activities','Advanced Placement Courses','Language Classes','School Sports']\ndfActivities = dfBySchoolType[['Community School?']+specialActivities]\ndfActivities = dfActivities.melt(id_vars=['Community School?'], value_vars=specialActivities,var_name='Activity', value_name='Average Offer')","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"9cbd2f9d8c8523454edf8b3080c283f08810f035"},"cell_type":"code","source":"fig, (ax1) = plt.subplots(figsize=(17, 8), ncols=1)\nsb.barplot(x=\"Activity\", y=\"Average Offer\", hue='Community School?', data=dfActivities, ax=ax1)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b3fc75ab91d7faf99652a7b8204306ffc0d6b7fb"},"cell_type":"markdown","source":"## Targeting PASSNYC's support"},{"metadata":{"_uuid":"dfc9927f7fce59ab74cb54121aec14f9f6284171"},"cell_type":"markdown","source":"Taking all the analysis carried out above, we now address the question of which schools to focus on using PASSNYC's support programs. To do this we develop an ad hoc score for schools in the 2016 School Explorer dataset.\n\nTo build up the ad-hoc score we choose the following parameters and assign a weight to each of them\n\n- Economic Need Index: the school with the largest ENI obtains the full weight, whereas the school with the lowest ENI obtains zero weight.\n- Chronical Absence: the school with the largest Chronical Absence obtains the full weight, whereas the school with the lowest Chronical Absence obtains zero weight. \n- Community School: if a school is a community school it receives full weight, if not, it receives zero weight.\n- School Income Estimate: the school with the lowest SIE obtains the full weight, whereas the school with the lowest SIE obtains zero weight. \n- Percentage of Black/Latino Students: the school with the largest percentage obtains the full weight, whereas the school with the lowest percentage obtains zero weight.\n\nIt is important that this approach is can readily be adapted to other forms of choosing both the variables to be used in the scoring as well as their relative importance."},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true,"_uuid":"7c23c722b6fef96baa8d83b0c93c49301b1af5af","collapsed":true},"cell_type":"code","source":"scoringWeights = {}\nscoringWeights['Economic Need Index'] = 0.2\nscoringWeights['Percent of Students Chronically Absent'] = 0.2\nscoringWeights['School Income Estimate'] = 0.3\nscoringWeights['Percent Black / Hispanic'] = 0.3\n                                                                      \nscoringFields = ['Economic Need Index','Percent of Students Chronically Absent','School Income Estimate','Percent Black / Hispanic']\n\nscoringParams = {}\nfor field in scoringFields:\n    scoringParams[field] = {'max':df[field].max() , 'min':df[field].min()}\n    \nscores = pd.DataFrame(index=[df.index], columns=['DBN','Name','Percent Black / Hispanic','Score'])\nscores['DBN'] = df['Location Code'].values\nscores['Name'] = df['School Name'].values\nscores['Percent Black / Hispanic'] = df['Percent Black / Hispanic'].values\nscores['Score'] = 0\nfor field in scoringFields:\n    scores['Score'] += scoringWeights[field] * (df[field].values - scoringParams[field]['min'])/abs(scoringParams[field]['max']-scoringParams[field]['min'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c9d251d8b1293421fc834ba17d96880cd58c0bd5"},"cell_type":"markdown","source":"Below we show a sample of candidate schools for PASSNYC's programs"},{"metadata":{"trusted":true,"_uuid":"07b5817c7abe7339f99ea48a474d95c1a366d96d","_kg_hide-input":true},"cell_type":"code","source":"scores.sort_values(by='Score',ascending=False).head(15)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c30cd14a2cda7ff80922f0128b238542f7a0266b"},"cell_type":"markdown","source":"### Geographical Visualization\n\nTo wrap up our analysis we show a map of NYC with schools shown as circles using a color coding given by their ENI (yellow - higher priority, purple - low priority) and their size given by the ad-hoc score.\n\nTo obtain more data from a specific school the reader need only use the mouse to hover over a given data point."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"c984f14caf70342c5646f0701bee113b90728c23"},"cell_type":"code","source":"from bokeh.io import output_notebook, show\noutput_notebook()\nfrom bokeh.plotting import figure\nfrom bokeh.models import WMTSTileSource, ColumnDataSource, Circle, ColorBar, BasicTicker, HoverTool\nfrom bokeh.models.mappers import ColorMapper, LinearColorMapper\nfrom bokeh.palettes import Viridis5\n\n# web mercator coordinates\nNYC = x_range,y_range = ((df['LongitudeWebMercator'].min(),df['LongitudeWebMercator'].max()) ,\n                         (df['LatitudeWebMercator'].min(),df['LatitudeWebMercator'].max()))\n\np = figure( tools='hover, pan, wheel_zoom',\n            x_range=x_range, y_range=y_range, \n            width=1250,\n            height=850\n          )\np.axis.visible = False\n\nhover = p.select_one(HoverTool)\nhover.point_policy = \"follow_mouse\"\nhover.tooltips = [(\"Score\", \"@Score\"),(\"ENI\", \"@ENI\"), (\"Black/Hispanic %\", \"@BHPercent\")]\n\nurl = 'http://a.basemaps.cartocdn.com/dark_all/{Z}/{X}/{Y}.png'\nattribution = \"Tiles by Carto, under CC BY 3.0. Data by OSM, under ODbL\"\n\np.add_tile(WMTSTileSource(url=url, attribution=attribution))\n\ncolor_mapper = LinearColorMapper(palette=Viridis5)\n\nsource = ColumnDataSource(\n    data=dict(\n        lat=df['LatitudeWebMercator'].values,\n        lon=df['LongitudeWebMercator'].values,\n        Score=df['Percent Black / Hispanic'].values,\n        ScoreScaled=np.exp(df['Percent Black / Hispanic'].values*2.5),\n        ENI=df['Economic Need Index'].values,\n        BHPercent=df['Percent Black / Hispanic'].values*100        \n    )\n)\n\ncircle = Circle(x=\"lon\", y=\"lat\", size=\"ScoreScaled\", fill_color={'field': 'ENI', 'transform': color_mapper}, fill_alpha=0.5, line_color=None)\np.add_glyph(source, circle)\n\ncolor_bar = ColorBar(color_mapper=color_mapper, ticker=BasicTicker(),\n                     label_standoff=12, border_line_color=None, location=(0,0))\n\np.add_layout(color_bar, 'right')\n\nshow(p)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"274bfe10114c2ffbd30082e709c96f9fb828ec45"},"cell_type":"markdown","source":"## Summary\n\nIn summary we have carried out an analysis of the numerous factors potentially playing a key role in the success of a student in being accepted at a specialized school. To do this we have employed numerous data sources, some of which were already as Kaggle datasets, while some others were obtained from external sources (The New York Times).\n\nWe found a very strong correlation between variables like Economic Need Index, Chronical Absence and School Estimated Income with a school's academic performance. We also found that it may be important for a school to meet certain quality standards in terms of as this appears to correlate positively with the number of students actually taking the SHSAT.\n\nIn a more detailed analysis we looked into community schools which appear to suffer particularly badly from low academic performance levels and the information available appears to hint towards academic problems having a root cause extending beyond school limits, i.e., we believe students in such schools could greatly profit from support programs focusing on strengthening ties as well as providing mental coaching, in order to foster the students' self-confidence when facing the SHSAT.\n\nWe close this analysis by developing an ad-hoc score which takes into account all previous observations and can be used as a simple guideline when deciding which schools should be prioritized by PASSNYC when implementing support programs. This score is greatly flexible and can easily be tuned to incorporate modifications in the relative weights of its parameters should more sofisticated approaches be imployed in the future."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"0e61918ded5194853f4dc8927217e7d26012a963"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}