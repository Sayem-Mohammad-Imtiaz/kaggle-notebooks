{"cells":[{"metadata":{"_uuid":"bf68704a-99d8-41a3-a50a-0553aafc20aa","_cell_guid":"9a70ceb3-18a3-4981-b576-174ce2441fed","trusted":true},"cell_type":"code","source":"# Data Manupilation\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Basket Analysis\nfrom mlxtend.frequent_patterns import apriori, association_rules\n\n# Deep Learning\nimport tensorflow as tf\nimport torch\nimport torch.nn as nn \nfrom torch.utils.data import DataLoader\nimport torch.nn.functional as F\nfrom sklearn import metrics\nfrom transformers import BertTokenizer, BertConfig, BertForSequenceClassification\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case = True)\nfrom tqdm import tqdm\n\n# Text\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\nstopw = stopwords.words('english')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3ae9d08a-4fdd-4f2d-9657-7c3a13858f56","_cell_guid":"74392d79-9321-43ef-a288-629af35729e5","trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/topic-modeling-for-research-articles-20/Train.csv')\ndf_test = pd.read_csv('../input/topic-modeling-for-research-articles-20/Test.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c7f7de44-ddb2-4804-ac0e-5ee08bd9b9dd","_cell_guid":"ed3e2881-a947-4657-bbbb-ba2933f45bd4","trusted":true},"cell_type":"markdown","source":"Each text has more than one topic. For example, one text can be about computer science, statistics and machine learning at the same time. \n\nLet's find out the total number of topics for each text with the sum function."},{"metadata":{"_uuid":"afc219a5-7cc6-49c3-aec8-16fac8670a74","_cell_guid":"b6301430-c09a-47d4-98ad-452e2b7d8da4","trusted":true},"cell_type":"code","source":"df['no_topics'] = df.iloc[:,2:].sum(axis = 1)\ndf.sort_values(by = 'no_topics', ascending = False).head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['no_topics'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1aa36eab-84a8-4144-8270-423c0fdc9fe1","_cell_guid":"8d5bf0fa-e66a-4864-ac76-95cf2e7af2ac","trusted":true},"cell_type":"markdown","source":"* As you can see, highest total number of topic is 7. Actually, all of the texts have more than one topics.\n* I picked one of the text that has 7 different topics. You can see whole text and text topics below."},{"metadata":{"_uuid":"09241488-1971-4d98-965d-7258e8dddbf6","_cell_guid":"3b795e33-8d77-4b80-9506-6bc899fc4f2b","trusted":true},"cell_type":"code","source":"df.loc[12706,'ABSTRACT' ]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6ed152dc-e24a-4e5a-9335-cb11bae5b1ae","_cell_guid":"f82b0cfd-480d-4343-9c31-6b4cc950101d","trusted":true},"cell_type":"code","source":"df.loc[df['id'] == 11080].T","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"beb79ad0-4da7-4ca3-bdd6-6cf941f06d0f","_cell_guid":"5b446ba4-a82e-4546-b049-bde10ee976cb","trusted":true},"cell_type":"markdown","source":"# Basket Analysis\n* I want to reduce the number of topic for related topics.\n* Basically, I want to create main category for each text so that I can classifiy them. \n* To find the relationships of topics, I used basket analysis."},{"metadata":{"_uuid":"37e2e109-8dfa-46fb-a776-d4d614fd2560","_cell_guid":"e7698bdc-cf60-4d09-b992-d412fb8aad54","trusted":true},"cell_type":"code","source":"frequency_result = apriori(df.iloc[:,2:31], min_support = 0.01, use_colnames=True).sort_values('support', ascending = False).reset_index(drop= True)\nassociation = association_rules(frequency_result, metric = 'lift', min_threshold =1 )","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"254fee5c-ae12-4342-8dd3-f9c042e5528d","_cell_guid":"85cc6438-d51d-43a6-a8fd-a9e1e61fd0df","trusted":true},"cell_type":"code","source":"#print(association.sort_values(by = ['confidence','support'], ascending = False).to_string())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"059c26b5-76ab-493d-af3a-c9da89ef76d9","_cell_guid":"1c6b2d5f-4522-4dce-8524-a664f3162ec8","trusted":true},"cell_type":"code","source":"association.sort_values(by = ['confidence','support'], ascending = False).head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"671fff37-6bb9-402b-b5c5-a3f124918039","_cell_guid":"1fbed641-c055-4dca-916c-15dc63808806","trusted":true},"cell_type":"markdown","source":"**Result of Basket Analysis**\n\n\n* We can see most related categories from the table.\n* I looked confidence value to find out the relationship.\n* For example, fourth row shows that Strongly Collerated Electrons and Physics was together 6.3% of the whole dataset. And based on the confidence figure, Strongly Collerated Electronslso and Physics are highly correlated.\n* Hence, we can say that Strongly Collerated Electrons is subcategory of Physics."},{"metadata":{"_uuid":"ea9750e5-ff3c-4d78-956f-c78704496f0d","_cell_guid":"8aa5bf0f-aae2-48d1-ae2b-74636121872b","trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6cccc8b2-7d03-432d-9cf0-f481d09eead4","_cell_guid":"ef176f8f-c6b6-4db1-8bde-e37826e4087b","trusted":true},"cell_type":"markdown","source":"* Based on confidence figure, I have decided to shrink the categories into 3 main categories.\n* Machine Learning and AI -> Statistics, Statistics Theory, Machine Learning, Artificial Intelligence, Computer Vision and Pattern Recognition\n* Physics -> Physics, Superconductivity, Strongly Correlated Electrons,  Astrophysics of Galaxies, Cosmology and Nongalactic Astrophysics, Earth and Planetary Astrophysics, Fluid Dynamics, Instrumentation and Methods for Astrophysics, Materials Science\n* Mathematics -> Mathematics, Number Theory, Analysis of PDEs, Differential Geometry, Optimization and Control, Representation Theory"},{"metadata":{"_uuid":"b1bf4716-5ff9-4e55-8955-d88bae4ba0a2","_cell_guid":"987b0727-17cf-440c-bd08-c40a40eed4df","trusted":true},"cell_type":"code","source":"df_test.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f56fdc8c-7d77-42fe-b75c-26173eab67f4","_cell_guid":"c3ec600a-9e8a-4f94-abe0-ce9d8f9ab51c","trusted":true},"cell_type":"code","source":"df['category'] = df.iloc[:,2:31].apply((lambda x: 'Machine Learning and AI' if (x['Statistics'] == 1 or x['Statistics Theory']==1 or x['Machine Learning']==1 or x['Artificial Intelligence'] == 1 or x['Computer Vision and Pattern Recognition'] ==1) \n                                        else 'Physics' if (x['Physics'] ==1 or x['Superconductivity'] == 1 or x['Strongly Correlated Electrons'] == 1 or  x['Astrophysics of Galaxies'] == 1 or x['Cosmology and Nongalactic Astrophysics'] ==1 or x['Earth and Planetary Astrophysics'] ==1 or x['Fluid Dynamics'] ==1 or x['Instrumentation and Methods for Astrophysics'] ==1 or x['Materials Science'] ==1) \n                                        else 'Mathematics' if(x['Mathematics'] ==1 or x['Number Theory'] == 1 or x['Analysis of PDEs'] == 1 or x['Differential Geometry'] ==1 or x['Optimization and Control'] == 1 or x['Representation Theory'] ==1) else 'other') , axis = 1)\n\ndf_test['category'] = df_test.iloc[:,2:31].apply((lambda x: 'Machine Learning and AI' if x['Statistics'] == 1 or x['Computer Science']==1 else 'Physics' if x['Physics'] else 'Mathematics' if x['Mathematics'] else 'other'), axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a5ca56c6-a3e7-45a7-af4d-acc485e24de7","_cell_guid":"cb8ed1c0-5f79-4723-87d4-a03bf74f983b","trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bb41f592-b392-479f-8d1a-f23488822831","_cell_guid":"96f32210-79ae-479c-8611-02db59aeb4c7","trusted":true},"cell_type":"code","source":"df['category'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"38121af9-bbbb-4509-9da7-ed604c91dfaf","_cell_guid":"ae03cf3d-cd24-4ffd-93a4-13581911ddc7","trusted":true},"cell_type":"markdown","source":"* As we can see, almost half of the texts are about machine learning and AI. \n* I dropped the other category for train dataset. So, we will have three main categories."},{"metadata":{"_uuid":"2c4e388d-aef4-4720-9153-3f1dd1481753","_cell_guid":"41c57d17-9a83-4c3f-9c22-83183fe432fe","trusted":true},"cell_type":"code","source":"df_train = df[['ABSTRACT', 'category']]\ndf_train = df_train[df_train['category'] != 'other' ]\ndf_train =df_train.rename(columns = {'ABSTRACT' : 'text'})\n\ndf_test = df_test[['ABSTRACT', 'category']]\ndf_test = df_test[df_test['category'] != 'other' ]\ndf_test =df_test.rename(columns = {'ABSTRACT' : 'text'})","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f8689218-3060-4e1e-a7da-579a6d377a2d","_cell_guid":"6832544b-1e2e-4b90-aab8-19811bbf8399","trusted":true},"cell_type":"code","source":"df_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# The total number of words "},{"metadata":{"_uuid":"117ab44a-d7f2-4fc5-8e89-31c75af5f82c","_cell_guid":"278392df-9fd3-4761-97d5-3b5f9fe7d616","trusted":true},"cell_type":"code","source":"df_train['len_text'] = df_train['text'].apply(lambda x: len(x.split()) )\ndf_test['len_text'] = df_test['text'].apply(lambda x: len(x.split()) )","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9b8d6909-3aac-4696-bc38-83c16ef1656d","_cell_guid":"44273678-d973-4ec2-ab53-36965ff011c0","trusted":true},"cell_type":"code","source":"df_train.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ee21de68-8aec-445e-bd46-a03321123d70","_cell_guid":"3bb5e904-24c2-40c9-ac54-d3934c66930e","trusted":true},"cell_type":"code","source":"len(df_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b9ff54a0-6e42-423d-b040-a16bd7c28e88","_cell_guid":"e50a03bb-3c15-4515-b17a-4ced6c4c973f","trusted":true},"cell_type":"code","source":"sns.set(font_scale=1.2)\nsns.set_style(\"white\")\nfig, ax = plt.subplots(2,sharex = True, figsize = (8,8), gridspec_kw={\"height_ratios\": (.2, .85)})\nsns.boxplot(df_train['len_text'], ax = ax[0])\nsns.distplot(df_train['len_text'], ax = ax[1], kde =False)\nax[0].set(xlabel = '', yticks = [])\nax[0].set_title('The Distibution of Length of Texts', fontsize = 20)\nax[1].set_xlabel('Length of Text', fontsize = 14)\nax[1].set_ylabel('Count', fontsize = 14)\nsns.despine(ax = ax[0])\nsns.despine(ax=ax[0], left=True)\nplt.subplots_adjust(hspace = .05)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1e1e9358-2d64-486c-8d43-466b2140ee47","_cell_guid":"21de73b9-36aa-484f-b15c-bf648d77a3a9","trusted":true},"cell_type":"markdown","source":"* As you can see from the chart above, most of the text lengths are between 0 and 300. There 3 outliers in the texts."},{"metadata":{"_uuid":"41efb85e-11e4-4ad7-8377-7e2d68ebe5e2","_cell_guid":"3ab957ac-e619-4081-9730-147f3f1db1d5","trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b54e5072-0e51-42f0-a7b9-3381bbd4c136","_cell_guid":"da918f29-1404-4b6b-a352-7694744e8372","trusted":true},"cell_type":"code","source":"enc = LabelEncoder()\ndf_train['label'] = enc.fit_transform(df_train['category'])\ndf_test['label'] = enc.fit_transform(df_test['category'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dbc43c71-6860-41f6-b300-0b8ae66a2981","_cell_guid":"2fcfff97-2186-4ddd-a0a5-2f5f883dbd92","trusted":true},"cell_type":"code","source":"df_train['category'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ed993a36-08d5-4c65-9b00-2ddab8b1a882","_cell_guid":"83f26ed9-2d2e-4071-9c29-6e412fe45ec5","trusted":true},"cell_type":"code","source":"df_train.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dea8b522-a2e2-4073-8dc2-7457ec6cb047","_cell_guid":"6f518f30-fdc2-4fdb-a537-97c2252f896b","trusted":true},"cell_type":"code","source":"fig ,ax = plt.subplots(figsize = (8,8))\nfor col in df_train['category'].unique():\n    ax = sns.distplot(df_train.loc[(df_train['category'] == col) & (df_train['len_text'] < 350), 'len_text'], kde = False)\n    \nax.legend(df_train['category'].unique())\nax.set_title('The Distribution of Length of Texts by Category', fontsize = 18)\nax.set_xlabel('Length of Texts')\nsns.despine(left = True)\nax.set_ylabel('Count')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ee852889-4f94-4e07-81c0-8a3820346b3d","_cell_guid":"812200f7-a075-45e5-8bc2-20fe56fc30e0","trusted":true},"cell_type":"markdown","source":"The length of texts for Mathemathics is lower than other categories."},{"metadata":{"_uuid":"cc4a5501-ebe3-478d-8fff-3303b2dfc69a","_cell_guid":"c4a42038-2d8e-4dad-811d-c6247888fe9e","trusted":true},"cell_type":"code","source":"category_encoding = df_train.groupby(['category'])['label'].min().rename('label').reset_index()\ncategory_encoding","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9a166fcd-25c2-4313-8b95-6e4019d0020d","_cell_guid":"c2593f80-448f-4f08-b65f-b99010ede5a6","trusted":true},"cell_type":"markdown","source":"* This is the example of bert tokenization. The category names are encoded. [input_ids, token_type_ids, attention_mask]."},{"metadata":{"_uuid":"6eee4705-d145-4e97-8b45-b084bc7bfbb6","_cell_guid":"706ab9e9-c7f0-4693-b315-aa80421b303e","trusted":true},"cell_type":"markdown","source":"# **Cleaning the Text**\n* It is better to get rid of punctuation and stopwords from the text."},{"metadata":{"_uuid":"de7fee9d-c42d-4583-8dca-0789285c9d37","_cell_guid":"2086aa41-d6a1-4e65-bbdd-a81774df9f91","trusted":true},"cell_type":"code","source":"def clean_text(text):\n    text = re.sub(r'[^a-zA-Z\\']',' ',text)\n    text = text.split()\n    text = [word for word in text if word not in stopw]\n    text = ' '.join(text)\n    text = re.sub(r'  ', ' ', text)\n    text = re.sub(r'   ', ' ', text)\n    return text","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"42e283d4-c23c-493b-bb40-4cfc9b513a6b","_cell_guid":"2b94062f-171d-43fa-981b-b832ebc6bb2f","trusted":true},"cell_type":"code","source":"print(\"---- Text ----\\n\")\nprint(df_train.loc[0, 'text'])\nprint(\"\\n---- Cleaned Text ----\\n\")\nprint(clean_text(df_train.loc[0, 'text']))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fb652f52-fa76-4afa-aff2-2909a34500e1","_cell_guid":"963bfc07-991c-4070-8733-3cbfa75fd66d","trusted":true},"cell_type":"code","source":"df_train['cleaned_text'] = df_train['text'].apply(lambda x: clean_text(x))\ndf_test['cleaned_text'] = df_test['text'].apply(lambda x: clean_text(x))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"74e9fae0-6b51-4a23-ab3c-2e2b82776661","_cell_guid":"2c3075a0-30dc-433c-b1d9-cfd8f7332aa4","trusted":true},"cell_type":"markdown","source":"# N-Grams\n* This function returns the unique words(bigram) or sequence of words and its occurrence frequency."},{"metadata":{"_uuid":"ba69760f-e225-4a5e-880a-c958dd262cb9","_cell_guid":"701decc4-ac4d-428c-ac5a-3d579fef507f","trusted":true},"cell_type":"code","source":"def n_gram(df, category = '', n_gram = 1, text_column= 'cleaned_text'):\n    n_gram_result = {}\n    \n    if category != '':\n        df = df[df['category'] == category]\n    \n    for text in tqdm(df[text_column]):\n        for i in range(len(text)):\n            words = text.split()[i:i+n_gram]\n            words = ' '.join(words)\n\n            if (len(words.split()) % n_gram) > 0 or words == '':\n                continue\n\n            elif words in n_gram_result.keys():\n                n_gram_result[words] += 1\n            else:\n                n_gram_result[words] = 1\n                \n                \n    n_gram_result = pd.DataFrame([n_gram_result]).T.reset_index()\n    n_gram_result.columns = [category + '_n_grams_'+ str(n_gram), category+ '_counts_' + str(n_gram)]\n\n\n    return n_gram_result","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b420f6af-fe93-47b2-9ba8-7ebc1e0502ec","_cell_guid":"f138bd3c-4f0b-4085-9b48-ca751fa3e85a","trusted":true},"cell_type":"markdown","source":"# Bigram, 2-Gram, and 3-Gram\n* The new dataframe is created to find top 30 sequence of words in terms of categories."},{"metadata":{"_uuid":"2e2455a0-ce54-4c2a-a9d5-75577b48d6ca","_cell_guid":"c57baa7f-7987-4fa3-ba1c-f94238fa1fed","trusted":true},"cell_type":"code","source":"temp_result = {}\nfor category in df_train['category'].unique():\n    for i in range(1,4):\n        result = n_gram(df_train, category = category, n_gram = i, text_column = 'cleaned_text').sort_values(by = category + \"_counts_\"+ str(i), ascending = False).head(30).reset_index(drop =True)\n        temp_result[category + \"_\", str(i)] = result\n\n\nn_gram_result = pd.DataFrame({})\nfor i in temp_result.keys():\n    n_gram_result = pd.concat([n_gram_result , temp_result[i]], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"157d815b-98ca-446d-84c5-b1e7cfcd42b3","_cell_guid":"59d4539b-09b8-42ef-913c-ab3babe7ecd7","trusted":true},"cell_type":"markdown","source":"# Physics N-Gram"},{"metadata":{"_uuid":"0ec0f5f2-48e2-4b3b-bbb6-925bb8066347","_cell_guid":"710f260e-7f36-445c-8c0b-c537076dd720","trusted":true},"cell_type":"code","source":"n_gram_result.iloc[:,0:6]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8832b16c-379b-4300-a992-2a095340d854","_cell_guid":"dc990fad-cf70-4ed2-b435-55beb0e41dcf","trusted":true},"cell_type":"markdown","source":"# Machine Learning and AI N-Gram"},{"metadata":{"_uuid":"7ac2af7a-9198-41ea-8d4d-69fda3b2d150","_cell_guid":"7bbe45ea-f9c4-4e10-b375-26156d244de9","trusted":true},"cell_type":"code","source":"n_gram_result.iloc[:,6:12]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c7814fb4-a315-4b44-998b-974e28c05747","_cell_guid":"96be218f-46cc-4355-9aea-fac53978e866","trusted":true},"cell_type":"markdown","source":"# Mathematics N-Gram"},{"metadata":{"_uuid":"9fe7abdb-667f-4eb4-a9c8-476ef50a9d5c","_cell_guid":"d5e44611-e76d-4f4b-8f2a-efeef2614106","trusted":true},"cell_type":"code","source":"n_gram_result.iloc[:,12:18]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"52f5388a-dae1-4998-a86f-c6528bf6d938","_cell_guid":"91d18116-6b3c-46d7-8385-b476ee79b18d","trusted":true},"cell_type":"markdown","source":"# Tensorflow with Glove Embeddings"},{"metadata":{"_uuid":"c42bf196-ddea-4a3b-8178-6e6cd37ad62f","_cell_guid":"6714adec-d931-4412-9036-b4abb67ca2a6","trusted":true},"cell_type":"markdown","source":"* Glove provides vector representation for words. I will use Common Crawl (840B tokens, 2.2M vocab, cased, 300d vectors).\n* It provides 300 dimensinal vector. Every dimension represents different feature of word. For example, one is positivity value of word, another one is negativity value of word, and so on.\n* We will find all words in the dataset and will create emmbedding matrix for all words.\n* The bidirectional layer will be used in neural network. Bidirectional layer is impoartant to train text data because it trains the sentence or text as is and reversed version of sentence.\n* **For example, \"I like a bar that plays jazz music\" - \"I like a bar of white chocolate not small piece\". As you can see first 3 words are same. If we don't train dataset with bidirectional layers, we cannot understand real meaning of the bar.**"},{"metadata":{"_uuid":"86c60819-2625-405b-8edf-f8dd1db0882a","_cell_guid":"eb0e4291-e4f8-431c-9293-d20ffdee5811","trusted":true},"cell_type":"code","source":"from tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.utils import to_categorical","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"708bbe5a-691c-402f-9042-455291a8b561","_cell_guid":"b21bf22c-18ae-4f32-bc89-c741fcbe8bff","trusted":true},"cell_type":"code","source":"glove_embeddings = np.load('../input/pickled-glove840b300d-for-10sec-loading/glove.840B.300d.pkl', allow_pickle=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e36e7254-4e41-44bd-aa0d-138fd6f913d8","_cell_guid":"4787e17b-7173-4f38-b5ef-6877a02b05d7","trusted":true},"cell_type":"code","source":"print(\"There are {} words and every word has {} dimensions in Glove Dictionary. I used the word 'sister' as an example.\".format(len(glove_embeddings.keys()),len(glove_embeddings['sister']),))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"71f22339-7bbf-4eb5-b885-eed53f695ab0","_cell_guid":"5100bd8d-c9e0-44cd-819f-00eac43c8729","trusted":true},"cell_type":"markdown","source":"# Uncovered words in the Glove Embeddings\n* We will find the words that Glove doesn't include and proportion of uncovered words."},{"metadata":{"_uuid":"335e66b0-4ff3-4766-9aa6-52fb3f553763","_cell_guid":"8819d783-cf64-40d4-832b-64d65695d60e","trusted":true},"cell_type":"code","source":"covered_words_by_category = {'Physics' : {},'Machine Learning and AI':{},'Mathematics' : {} }\nuncovered_words_by_category = {'Physics' : {},'Machine Learning and AI':{},'Mathematics' : {} }\nfor category in df_train['category'].unique():\n    for text in tqdm(df_train.loc[df['category'] == category, 'cleaned_text']):\n        text = text.split()\n        for word in text:\n            if word in glove_embeddings.keys():\n                if word not in covered_words_by_category[category].keys():\n                    covered_words_by_category[category][word] = 1\n                else:\n                    covered_words_by_category[category][word] += 1\n\n            else:\n                if word not in uncovered_words_by_category[category].keys():\n                    uncovered_words_by_category[category][word] = 1\n                else:\n                    uncovered_words_by_category[category][word] += 1\n                    \n                    \ncovered_words ={}\nuncovered_words = {}\nfor text in tqdm(df_train['cleaned_text']):\n    text = text.split()\n    for word in text:\n        if word in glove_embeddings.keys():\n            if word not in covered_words.keys():\n                covered_words[word] = 1\n            else:\n                covered_words[word] += 1\n\n        else:\n            if word not in uncovered_words.keys():\n                uncovered_words[word] = 1\n            else:\n                uncovered_words[word] += 1","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"53697a71-871f-4450-bdb9-49b5c583dcc9","_cell_guid":"9a62962b-93bb-4fe4-b7e9-8c0eaff1edbf","trusted":true},"cell_type":"code","source":"print(\"---There are {} words in the whole dataset, and {:.2f}% of the words aren't covered by Glove---\".format((len(uncovered_words) + len(covered_words)),len(uncovered_words) / (len(uncovered_words)+len(covered_words))*100))\nprint(\"---There are {} words in the Physics category, and {:.2f}% of the words aren't covered by Glove---\".format((len(uncovered_words_by_category['Physics']) + len(covered_words_by_category['Physics'])),len(uncovered_words_by_category['Physics']) / (len(uncovered_words_by_category['Physics'])+len(covered_words_by_category['Physics']))*100))\nprint(\"---There are {} words in the Machine Learning and AI category, and {:.2f}% of the words aren't covered by Glove---\".format((len(uncovered_words_by_category['Machine Learning and AI']) + len(covered_words_by_category['Machine Learning and AI'])),len(uncovered_words_by_category['Machine Learning and AI']) / (len(uncovered_words_by_category['Machine Learning and AI'])+len(covered_words_by_category['Machine Learning and AI']))*100))\nprint(\"---There are {} words in the Mathematics category, and {:.2f}% of the words aren't covered by Glove---\".format((len(uncovered_words_by_category['Mathematics']) + len(covered_words_by_category['Mathematics'])),len(uncovered_words_by_category['Mathematics']) / (len(uncovered_words_by_category['Mathematics'])+len(covered_words_by_category['Mathematics']))*100))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"80ad7db6-a9f3-4990-9b03-b47d48f2d161","_cell_guid":"15fce050-33d1-492b-bbd1-04fbcdb20e19","trusted":true},"cell_type":"markdown","source":"* In glove embeddings, it is helpful to clean the text. Glove embeddings doesn't cover 25% of the words because there are too many words in these three topics. If we don't clean the text, proportion of uncovered words is around 70%."},{"metadata":{"_uuid":"80b849ca-dc5f-4d8a-858c-fb2b9f2323cb","_cell_guid":"e97ed74d-f88a-4997-b0d3-bf5a5a5d660a","trusted":true},"cell_type":"code","source":"covered_words_by_category = pd.DataFrame(covered_words_by_category).reset_index().rename(columns = {'index' :'words'})\nuncovered_words_by_category = pd.DataFrame(uncovered_words_by_category).reset_index().rename(columns = {'index' :'words'})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Physics uncoverd words**"},{"metadata":{"_uuid":"88db9594-490c-4d98-ad70-5ebe569f2796","_cell_guid":"e6b0c8d4-0c76-4c43-b201-fc661b4ac085","trusted":true},"cell_type":"code","source":"uncovered_words_by_category[['words', 'Physics']].sort_values(by = 'Physics', ascending = False).head(20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Machine Learning and AI uncoverd words**"},{"metadata":{"_uuid":"5bd9c6d5-914e-4f11-9a1d-39762121cd77","_cell_guid":"3c085465-690c-434b-bbe6-34a16fbc79cd","trusted":true},"cell_type":"code","source":"uncovered_words_by_category[['words', 'Machine Learning and AI']].sort_values(by = 'Machine Learning and AI', ascending = False).head(20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Mathematics uncovered words**"},{"metadata":{"_uuid":"b8c7bbe5-fe3d-44a3-95b5-2d3e52ae141b","_cell_guid":"49672bbf-f016-4755-aaf4-537ca52f5d28","trusted":true},"cell_type":"code","source":"uncovered_words_by_category[['words', 'Mathematics']].sort_values(by = 'Mathematics', ascending = False).head(20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Embedding Matrix\n**Creating embedding matrix for all the words that are in the whole dataset.**"},{"metadata":{"_uuid":"ce009b3f-e36e-4202-8aba-d2cbced3cd6a","_cell_guid":"9c610510-0e12-4e56-bdea-84347b4805e0","trusted":true},"cell_type":"code","source":"tokenizer_keras = Tokenizer(num_words = 36073, oov_token = '<OOV>' )\ntokenizer_keras.fit_on_texts(df_train['text'])\nword_index = tokenizer_keras.word_index\nvocab_size_keras = len(word_index)\nembedding_dim = 300\nword_embeddings = np.zeros((vocab_size_keras , embedding_dim))\n\nfor word, i in word_index.items():\n    if word in glove_embeddings.keys():\n        word_embeddings[i-1] = glove_embeddings[word]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b69399da-c54a-4723-8451-dd73f13d04b8","_cell_guid":"e7c2b231-ceab-4d5c-b452-687cf5574a72","trusted":true},"cell_type":"code","source":"word_tokens = pd.DataFrame([word_index]).T.reset_index().rename(columns = {'index': 'words', 0: 'tokens'})\nkeras_word_embeddings = pd.DataFrame(word_embeddings).reset_index().rename(columns = {'index':'tokens'})\nkeras_word_embeddings['tokens'] = keras_word_embeddings['tokens'] + 1","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"79cad0e8-2237-48c2-8227-a952c8ccdb01","_cell_guid":"479a6861-8c8f-4d94-b70c-5d16c99f7cf9","trusted":true},"cell_type":"code","source":"word_tokens.merge(keras_word_embeddings , how = 'left' , on = 'tokens')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9f96a27f-afe0-43a5-94a2-8679910bce06","_cell_guid":"fe8f3cfa-da97-4f29-b845-0c9f09446c02","trusted":true},"cell_type":"code","source":"def prepare_data(df, tokenizer, text_column = 'cleaned_text',label_column = 'labels', max_len = 256):\n    '''\n    This function converts the text data into tokens. max_len is the number of words that we want to use in each text. \n    So, if the text includes more than 256 words, the post words of the text is going to be deleted.(trun)\n    If the text includes less than 256 words, 0 will be added into vector. (padding)\n    '''\n    sequences = tokenizer.texts_to_sequences(df[text_column])\n    padded = pad_sequences(sequences, maxlen = max_len, padding = 'post', truncating = 'post')\n    labels = tf.keras.utils.to_categorical(df[label_column])\n    return padded, labels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2bc5d456-4d8e-4671-af2b-3b063a5b2554","_cell_guid":"d607e201-f73b-4f91-8077-317c84888162","trusted":true},"cell_type":"code","source":"df_train.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"532b2793-5f0f-4b02-9a66-1013cb911e8f","_cell_guid":"b27ee679-e6f9-4a8e-ba1c-e790fca5e0d4","trusted":true},"cell_type":"code","source":"max_len = 128\npadded, labels = prepare_data(df_train, tokenizer_keras, text_column = 'cleaned_text', label_column = 'label', max_len= max_len)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Creating the validation data**"},{"metadata":{"_uuid":"116044c1-cf29-4547-8980-1461936f9836","_cell_guid":"6a392bf2-dbeb-45c7-af3c-f58b9907d6fe","trusted":true},"cell_type":"code","source":"training_portion =0.75\ntraining_size = int(len(df_train) * training_portion)\npadded_training = padded[:training_size]\nlabels_training = labels[:training_size]\npadded_val = padded[training_size:]\nlabels_val = labels[training_size:]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e1e2286a-f710-4e9e-b26f-9fc077ce8c84","_cell_guid":"166ed06b-30df-4b73-a27f-ca4abc31c452","trusted":true},"cell_type":"code","source":"len(word_embeddings)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0634ad77-b8b0-4cd7-9e24-7fb95fcc4d76","_cell_guid":"4a38940f-4552-4f46-9888-3664d90e36e2","trusted":true},"cell_type":"code","source":"padded_training[6]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fb32279b-f1cf-4070-8079-25a771ac2630","_cell_guid":"473abc54-439d-4d4b-80c2-19814cd83f08","trusted":true},"cell_type":"code","source":"model_glove = tf.keras.Sequential()\nmodel_glove.add(tf.keras.layers.Embedding(vocab_size_keras , embedding_dim,input_length = max_len, weights = [word_embeddings], trainable = False))\nmodel_glove.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences = True)))\nmodel_glove.add(tf.keras.layers.Dropout(0.5))\nmodel_glove.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)))\nmodel_glove.add(tf.keras.layers.Dropout(0.5))\nmodel_glove.add(tf.keras.layers.Dense(32, activation = 'relu'))\nmodel_glove.add(tf.keras.layers.Dropout(0.5))\nmodel_glove.add(tf.keras.layers.Dense(3, activation = 'softmax'))\n\nmodel_glove.compile(optimizer = 'Adam', loss= 'categorical_crossentropy', metrics = ['accuracy'])\nmodel_glove.summary()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"de62476a-91bb-4cf0-b41b-0f3172a33b1b","_cell_guid":"29848bb0-a1e8-4d08-b79b-3813c6de43fb","trusted":true},"cell_type":"code","source":"model_glove.fit(padded_training, labels_training, epochs = 10, verbose = 1, validation_data = (padded_val, labels_val))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"46c4f191-89b0-473a-8d48-7a7f30f9935b","_cell_guid":"91a8fc95-c348-4fc3-b41b-6721738f5bc1","trusted":true},"cell_type":"markdown","source":"**Testing Model with Glove**"},{"metadata":{"_uuid":"776c05ad-3478-4020-833d-d9159767d259","_cell_guid":"652235c5-6b7d-4d0f-8913-4905d85ae536","trusted":true},"cell_type":"code","source":"test_data, test_label = prepare_data(df_test, tokenizer_keras, text_column = 'cleaned_text', label_column = 'label', max_len = 128)\noutput_label = np.argmax(test_label, axis =1 ).flatten()\npredicted_label = np.argmax(model_glove.predict(test_data), axis = 1).flatten()\ntest_accuracy = np.sum(output_label == predicted_label) / len(output_label)\nprint(\"The test set includes {} texts and the accuracy is {:.2f}\".format(len(output_label), test_accuracy * 100))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f83d5302-15fc-41cf-ac5a-533463c473ef","_cell_guid":"dafd162b-4b19-4266-9e2a-c9544c993284","trusted":true},"cell_type":"markdown","source":"# Tensorflow with Bert"},{"metadata":{"_uuid":"d8971e7f-3ea7-43eb-9910-bffb1e09174e","_cell_guid":"d89b9d7f-a11b-485f-a744-fa175cd45ef2","trusted":true},"cell_type":"code","source":"tf.keras.backend.clear_session()\nimport tensorflow_hub as hub\nfrom sklearn.model_selection import StratifiedKFold","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3b7dad80-9540-4cef-ba02-d20f626ecb13","_cell_guid":"82289878-06b7-4966-8fb4-3352a52204b9","trusted":true},"cell_type":"code","source":"def processing_data( row, row_label,category_encoding = category_encoding ,max_len = max_len):\n    \n    '''\n    Bert needs 3 different token vectors, so we need to convert data.\n    '''\n    label = category_encoding[category_encoding['category'] == row_label]['label'].values[0]\n    temp_input_ids = tokenizer.encode(row, max_length = max_len)\n    pad_len = max_len - len(temp_input_ids)  \n    input_ids =temp_input_ids + [0] * pad_len\n    attention_masks= [1] * len(temp_input_ids)+ [0] * pad_len\n    token_type_ids =  [0] * max_len\n    return np.array(input_ids), np.array(attention_masks), np.array(token_type_ids), np.array(label)\n\n\ndef encode(df, max_len = max_len, text_column = 'cleaned_text', category_encoding = category_encoding):\n    '''\n    This function is created to convert 3 special vectors into numpy array.\n    '''\n    ids, masks, token_ids, labels  = map(list, zip(*df[[text_column, 'category']].apply(lambda x: processing_data(x[text_column], x.category, category_encoding, max_len = max_len) , axis = 1)))\n    ids = np.array(ids, dtype = 'float32')\n    masks = np.array(masks, dtype = 'float32')\n    token_ids = np.array(token_ids, dtype = 'float32')\n    labels = tf.keras.utils.to_categorical(labels)\n    return ids, masks, token_ids, labels","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"44c89a72-85f8-4c15-8c3f-d98bb8dec902","_cell_guid":"7f3bf201-34c7-4614-bb86-3e98e69717c6","trusted":true},"cell_type":"code","source":"category_encoding","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8de4bebe-9319-420a-be00-b5fdc802e474","_cell_guid":"4355745d-ca4d-446f-a2a2-2d4b40b688b8","trusted":true},"cell_type":"code","source":"def build_model(max_len = max_len, no_category = 3):\n    ids = tf.keras.Input(shape = (max_len, ), dtype = tf.int32)\n    masks = tf.keras.Input(shape = (max_len, ), dtype = tf.int32)\n    token_ids = tf.keras.Input(shape = (max_len), dtype = tf.int32)\n    \n    bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",  trainable=True)\n    \n    pooled_output , sequence_output = bert_layer([ids, masks, token_ids])\n    \n    output = sequence_output[:,0,:]\n\n    out = tf.keras.layers.Dense(no_category, activation = 'softmax')(output)\n    \n    model = tf.keras.models.Model(inputs = [ids, masks, token_ids], outputs = out)\n    \n    model.compile(loss = 'categorical_crossentropy', optimizer= tf.optimizers.Adam(learning_rate = 3e-5), metrics = ['accuracy'])\n                  \n    return model","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5edfcdc5-e8e1-4256-9d13-bfae2078d2e6","_cell_guid":"7686f468-4bee-4158-a915-167d08623422","trusted":true},"cell_type":"code","source":"tf.keras.backend.clear_session()\nskf = StratifiedKFold(n_splits = 4, shuffle = True)\nids, masks, token_ids, labels = encode(df_train, max_len = max_len)\n\nfor i , (train_index, val_index) in enumerate(skf.split(ids, labels.argmax(1))):\n    ids_train = ids[train_index,:]\n    masks_train = masks[train_index,:]\n    token_ids_train = token_ids[train_index, :]\n    labels_train = labels[train_index, :]\n    ids_val = ids[val_index,:]\n    masks_val = masks[val_index, :] \n    token_ids_val = token_ids[val_index, :]\n    labels_val = labels[val_index, :]\n    print(\"Fold :{}\".format(i+1))\n    model = build_model(max_len = max_len)\n    \n    model.fit((ids_train, masks_train, token_ids_train), labels_train, verbose = 1, epochs = 4, batch_size = 32, validation_data = ((ids_val, masks_val, token_ids_val), labels_val))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"381e0c19-e7e5-4faa-a181-060119bdb2be","_cell_guid":"6855c9c8-fe14-4c3d-a50d-19efc21ad507","trusted":true},"cell_type":"code","source":"def flat_accuracy(output, prediction):\n    prediction_flat = np.argmax(prediction, axis=1).flatten()\n    output_flat = np.argmax(output, axis=1).flatten()\n    return np.sum(prediction_flat == output_flat) / len(output_flat)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"07a186dd-8696-4c5e-a81f-5614c224a6f9","_cell_guid":"aabf89ad-1152-498c-9770-3c7afa2caaba","trusted":true},"cell_type":"code","source":"ids_test, masks_test, token_ids_test, labels_test = encode(df_test, max_len = 128)\nprediction = model.predict((ids_test, masks_test, token_ids_test))\noutput = tf.keras.utils.to_categorical(df_test['label'])\ntest_result = flat_accuracy(output, prediction)\nprint(\"The test set includes {} texts and the accuracy is {:.2f}\".format(len(output), test_result * 100))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"310f711c-c4d0-476f-9a55-8c9b240ea228","_cell_guid":"201a321c-4d9c-43ea-b8f6-fca36331c876","trusted":true},"cell_type":"markdown","source":"# Classification for both Research and BBC Datasets\n* I am adding extra data for broader classification.\n* I have already implemented Bert Classification in another noteboook. You can reach from the link below.\n* https://www.kaggle.com/cempek/bbc-multiclass-glove-tf-vs-bert-pytorch-tf-99-5\n* We had 3 categories in previous dataset. I am now adding 5 more categories from BBC dataset includes less technical topics.\n* BBC dataset includes news about politics, entertainment, sport, tech, and business."},{"metadata":{"_uuid":"67ec8ce9-3097-40c1-930e-57a72f00b978","_cell_guid":"4f3afafe-72b6-44cf-b106-245493f1b868","trusted":true},"cell_type":"code","source":"df_bbc = pd.read_csv('../input/bbc-fulltext-and-category/bbc-text.csv')\ndf_bbc.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a48645a0-d848-4d5c-843f-30272eef3b2f","_cell_guid":"5a41c938-12cc-4836-820d-ef1f159cef62","trusted":true},"cell_type":"markdown","source":"* We retrieve sample data from the research dataset since BBC dataset has around 450 news in each category."},{"metadata":{"_uuid":"4156d80a-9083-4e04-8d07-9f341ad17202","_cell_guid":"d54b59f5-01ca-4c9d-93fb-ea1aa1484a66","trusted":true},"cell_type":"code","source":"df_research = df_train.groupby('category').sample(n = 600, random_state = 1)\n#df_research = df_train.groupby('category').apply(lambda x: x.sample(500)).reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c5719d78-d445-48b9-a72a-fce537684d48","_cell_guid":"775f2e51-742b-4379-8407-e456e73f79dd","trusted":true},"cell_type":"code","source":"df_merge = pd.concat([df_research[['text', 'category']], df_bbc]).reset_index(drop = True)\ndf_merge['category'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"27b5f49b-d939-4115-bc78-874df842489d","_cell_guid":"34a23f54-40ef-4af9-bd34-f2e4ff9abe0b","trusted":true},"cell_type":"markdown","source":"**Splitting data into test and training**"},{"metadata":{"_uuid":"c5d685fc-8645-467a-8bca-a3f98891bfbc","_cell_guid":"b511a511-e58c-4b31-b206-7659aabadbc7","trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(df_merge['text'], df_merge['category'], test_size = 0.05, random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"29661f93-6662-4910-a1bc-b5cdf8521a26","_cell_guid":"1f823b40-ce80-4a92-8b9e-8b88b3e0e466","trusted":true},"cell_type":"code","source":"df_merge = pd.concat([X_train, y_train], axis = 1)\ndf_merge_test = pd.concat([X_test, y_test], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dc0746b0-ecf5-425e-8a1f-9bd286107dde","_cell_guid":"cd198be5-25a3-4f3b-ad27-24f8b01f66ce","trusted":true},"cell_type":"code","source":"enc2 = LabelEncoder()\ndf_merge['label'] = enc2.fit_transform(df_merge['category'])\ndf_merge['cleaned_text'] = df_merge['text'].apply(lambda x: clean_text(x))\ndf_merge_test['label'] = enc2.fit_transform(df_merge_test['category'])\ndf_merge_test['cleaned_text'] = df_merge_test['text'].apply(lambda x: clean_text(x))\ndf_merge_test = df_merge_test.reset_index(drop= True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7e08b31e-b650-422b-90d5-f4e3e868b251","_cell_guid":"9c79a509-fa51-4945-9534-22495d0d7dd3","trusted":true},"cell_type":"code","source":"category_encoding_merge = df_merge.groupby(['category'])['label'].min().rename('label').reset_index()\ncategory_encoding_merge","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9c25c5c1-13a6-43da-a947-6490318abc02","_cell_guid":"511a5df8-28de-44ff-9e87-f61149b8f8a5","trusted":true},"cell_type":"markdown","source":"I have used same model for this merged dataset, I just changed the number category in the model."},{"metadata":{"_uuid":"7041ac50-810a-40a7-9f65-6a4cc47db740","_cell_guid":"7d49f90b-fd8c-4c42-b5d4-8276931ca1b6","trusted":true},"cell_type":"code","source":"tf.keras.backend.clear_session()\nskf = StratifiedKFold(n_splits = 4, shuffle = True)\nids, masks, token_ids, labels = encode(df_merge, max_len = max_len, category_encoding = category_encoding_merge)\n\nfor i , (train_index, val_index) in enumerate(skf.split(ids, labels.argmax(1))):\n    ids_train = ids[train_index,:]\n    masks_train = masks[train_index,:]\n    token_ids_train = token_ids[train_index, :]\n    labels_train = labels[train_index, :]\n    ids_val = ids[val_index,:]\n    masks_val = masks[val_index, :] \n    token_ids_val = token_ids[val_index, :]\n    labels_val = labels[val_index, :]\n    print(\"Fold :{}\".format(i+1))\n    model_merge = build_model(max_len = max_len, no_category = 8)\n    \n    model_merge.fit((ids_train, masks_train, token_ids_train), labels_train, verbose = 1, epochs = 4, batch_size = 32, validation_data = ((ids_val, masks_val, token_ids_val), labels_val))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a5ceb61a-7cfa-4f65-b659-3c3646bd97bb","_cell_guid":"dbe839a9-3c20-43db-a675-054dfa84c939","trusted":true},"cell_type":"code","source":"ids_test, masks_test, token_ids_test, labels_test = encode(df_merge_test, max_len = max_len, category_encoding = category_encoding_merge)\nprediction = model_merge.predict((ids_test, masks_test, token_ids_test))\noutput = tf.keras.utils.to_categorical(df_merge_test['label'])\ntest_result = flat_accuracy(output, prediction)\nprint(\"The test set includes {} texts and the accuracy is {:.2f}\".format(len(output), test_result * 100))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Result\n* In the research dataset, Bert performed better than bidirectional LSTM model.\n* After I merged the Research and BBC datasets, I have received better results(more than 95%) in the test data. \n* It is hard to predict the text that has very similar topic like Math and Physics, so the accuracy increased in the merged dataset which includes topics such as politics, entertainment, sport, business, machine learning and so on."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}