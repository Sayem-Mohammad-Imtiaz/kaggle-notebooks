{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Belarus Used Cars Prices"},{"metadata":{},"cell_type":"markdown","source":"Hello, I am using this data set to build my first linear regression in scikit-learn. I would really appreciate any feed back if I have made a small or serious error. I am attempting to go through the entire machine learning work flow. Looking forward to getting started on kaggle and I hope this notebook at least provides some value to beginners like my self."},{"metadata":{},"cell_type":"markdown","source":"## Data Source"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Uncomment and run cellto get to the data source\n\nimport webbrowser\n#webbrowser.open('https://www.kaggle.com/slavapasedko/belarus-used-cars-prices')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Description"},{"metadata":{},"cell_type":"markdown","source":"**Context** <br>\nThis is a file that represents market of cars on sale. <br>\n\n**Content** <br>\nThis data was collected on the Internet and represents the car market. Dataset was collected at the dawn of 2019. <br>\n\n**Columns**\n1. make - machine firm <br>\n2. model - model :) <br>\n3. price USD - price in dollars (target variable) <br>\n4. year - production year <br>\n5. condition - represents the condition at the sale moment (with mileage, for parts, etc) <br>\n6. mileage - mileage in kilometers <br>\n7. fuel_type - type of the fuel (electro, petrol, diesel) <br>\n8. volume of the engine <br>\n9. color <br>\n10. transmission <br>\n11. drive unit <br>\n12. segment (this feature was collected manually, so it could be wrong) <br>\n\n**Inspiration** <br>\nThis is a dataset for linear regression training skills, also we can visualize the data"},{"metadata":{},"cell_type":"markdown","source":"# Import Libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data"},{"metadata":{},"cell_type":"markdown","source":"## Read in Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/belarus-used-cars-prices/cars.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Information"},{"metadata":{},"cell_type":"markdown","source":"First check the information of the data set. See what each feature data type is and whether or not it has missing values."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After looking through each feature the data types appear to be correct. Some of the features have missing values and we will deal with them later on. <br>"},{"metadata":{},"cell_type":"markdown","source":"## Peak at Data"},{"metadata":{},"cell_type":"markdown","source":"Lets view the head and the tail of the data set"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['transmission'].unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Descriptive Statistics"},{"metadata":{},"cell_type":"markdown","source":"Lets also get some summary statistics of our data."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe(include='all')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Alot of features are in fact catagorical so we will need to convert them."},{"metadata":{},"cell_type":"markdown","source":"# Missing Data"},{"metadata":{},"cell_type":"markdown","source":"## Explore Missing Data"},{"metadata":{},"cell_type":"markdown","source":"Lets first see which features have missing data and how many there are"},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_data = df.isnull().sum() \nmissing_data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will start by removing the features that have more than 5% of there values missing"},{"metadata":{"trusted":true},"cell_type":"code","source":"na_index_filter = missing_data[missing_data / len(df) <= 0.05].index","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we can us this index to keep the columns we want. This will only drop the segment column."},{"metadata":{"trusted":true},"cell_type":"code","source":"df1 = df[na_index_filter]\ndf1.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Impute Data / Remove rows"},{"metadata":{},"cell_type":"markdown","source":"The volume feature have less than 1 % of missing values so we will remove those rows from the data set."},{"metadata":{"trusted":true},"cell_type":"code","source":"(df1['volume(cm3)'].isnull().sum() / len(df)) * 100","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The dive_unit feature has 3.4 %. We can try to remove them or impute them if it makes sense."},{"metadata":{"trusted":true},"cell_type":"code","source":"(df1['drive_unit'].isnull().sum() / len(df)) * 100","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets check the unique values."},{"metadata":{"trusted":true},"cell_type":"code","source":"df1['drive_unit'].unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets remove all of the rows for now as it will reduce less then 5% of the total rows of the data set. If we decide later we can come back and impute the volume feature with then mean/median and the drive unit feature with the mode."},{"metadata":{"trusted":true},"cell_type":"code","source":"df1 = df1.dropna(axis=0)\ndf1.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Type Conversion"},{"metadata":{},"cell_type":"markdown","source":"## Covert Year to Object"},{"metadata":{"trusted":true},"cell_type":"code","source":"df1['year']= df1['year'].astype('object')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Categorical Variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"text_cols = df1.select_dtypes(include=['object']).columns\ntext_cols","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have 7 categorical variables in our data set. Lets see how many unique values each one has."},{"metadata":{"trusted":true},"cell_type":"code","source":"for cat in text_cols:\n    print(cat + ':' + str(len(df1[cat].unique())))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Covert Text Columns to Category Type"},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in text_cols:\n    df1[col]= df1[col].astype('category')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now lets make sure each text col was converted to type category"},{"metadata":{"trusted":true},"cell_type":"code","source":"df1.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A view of each category codes unique values"},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in text_cols:    \n    print(col + ':' + str(df1[col].cat.codes.unique()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text_cols","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Transformation"},{"metadata":{},"cell_type":"markdown","source":"## Convert mileage(kilometers) to Miles"},{"metadata":{"trusted":true},"cell_type":"code","source":"df1['mileage(miles)_sqrt'] = np.sqrt(df1['mileage(kilometers)']*0.621371)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Selection"},{"metadata":{},"cell_type":"markdown","source":"## Numerical Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"num_features = df1.select_dtypes(include=[int, float]).columns.drop('priceUSD')\nnum_features","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Correaltion Matrix"},{"metadata":{},"cell_type":"markdown","source":"We can make a correlation table and a heat map since all the values are now numeric. "},{"metadata":{"trusted":true},"cell_type":"code","source":"CORR_MAP = df1.corr()\nCORR_MAP","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Heat Map"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10, 7))\nsns.heatmap(CORR_MAP, annot=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Scatter Plots"},{"metadata":{},"cell_type":"markdown","source":"#### Numerical Feature Selection"},{"metadata":{"trusted":true},"cell_type":"code","source":"price_corr = df1.corr()['priceUSD'].sort_values().drop('priceUSD')\nprice_corr","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets select only the features that have greater than the absolute value of 0.25"},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_drop = price_corr[np.abs(price_corr) <= 0.25].index\nfeature_drop","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1 = df1.drop(feature_drop, axis=1)\ndf1.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Catagorical Data"},{"metadata":{},"cell_type":"markdown","source":"Lets convert text data to the categorical type"},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_features = df1.select_dtypes(include=['category']).columns\ncat_features","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets look at some data visualizations between our categorical variables and sales price"},{"metadata":{},"cell_type":"markdown","source":"# Dummy Encoding"},{"metadata":{},"cell_type":"markdown","source":"Now we can dummy encode our catagical data"},{"metadata":{"trusted":true},"cell_type":"code","source":"dummy_cols = pd.DataFrame()\nfor text_col in text_cols:\n    col_dummies = pd.get_dummies(df1[text_col])\n    df1 = pd.concat([df1, col_dummies], axis=1)\n    del df1[text_col]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We now have a new data frame were each level is a new column and the values are all 1's and 0's"},{"metadata":{"trusted":true},"cell_type":"code","source":"df1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_price = df1.corr()['priceUSD'].sort_values()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_price = corr_price.drop('priceUSD')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"other_index = corr_price[np.abs(corr_price) > 0.15].index                            ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_df = df1[other_index]\nnew_df['priceUSD'] = df1['priceUSD']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training and Test Set"},{"metadata":{},"cell_type":"markdown","source":"## Training and Test Sets"},{"metadata":{},"cell_type":"markdown","source":"Break dataset into a training set where we can train our model and a test test to see how well our model is predicting the target variable on unseen data>"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_rows = len(df1) * .75\ntest_rows = len(df1) - train_rows\nprint(train_rows)\nprint(test_rows)\ntrain_rows + test_rows == len(df1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = df1.iloc[:40720]\ntest_df = df1.iloc[40720:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Features and Target Variables"},{"metadata":{},"cell_type":"markdown","source":"We can a variables that hold our features and target variable"},{"metadata":{"trusted":true},"cell_type":"code","source":"features = train_df.drop('priceUSD', axis=1).columns\ntarget = 'priceUSD'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Linear Regression "},{"metadata":{},"cell_type":"markdown","source":"## Import Packages from SK Learn"},{"metadata":{},"cell_type":"markdown","source":"Import classes to run regression and calculate error metric"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## First Model with few categorical features"},{"metadata":{},"cell_type":"markdown","source":"## Model Building"},{"metadata":{},"cell_type":"markdown","source":"Instantiate model"},{"metadata":{"trusted":true},"cell_type":"code","source":"lm = LinearRegression()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Train the model on the train dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"lm.fit(train_df[features], train_df[target])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Predictions"},{"metadata":{},"cell_type":"markdown","source":"Make predictions on the test set"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_predictions = lm.predict(test_df[features])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets make a series that contains the actual target values from the test set."},{"metadata":{"trusted":true},"cell_type":"code","source":"actual_target = test_df[target].reset_index(drop=True)\nactual_target.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we can concat the actual and predicted values."},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_df = pd.concat([actual_target, pd.Series(test_predictions)], axis=1, ignore_index=True)\npred_df.columns = ['Actual', 'Predicted']\npred_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Visual of the predictions vs the actuals"},{"metadata":{},"cell_type":"markdown","source":"Visual of the first 25 predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_df = pred_df.head(50)\nplot_df.plot(kind='bar',figsize=(16,10))\nplt.grid(which='major', linestyle='-', linewidth='0.5', color='green')\nplt.grid(which='minor', linestyle=':', linewidth='0.5', color='black')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## RMSE"},{"metadata":{},"cell_type":"markdown","source":"The rmse value is very high. This is not unexpected as our features were not highly correlated with our target."},{"metadata":{"trusted":true},"cell_type":"code","source":"rmse = np.sqrt(mean_squared_error(test_df[target], test_predictions))\nrmse","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The rmse value is very high. This is not unexpected as our features were not highly correlated with our target."},{"metadata":{"trusted":true},"cell_type":"code","source":"rmse = np.sqrt(mean_squared_error(test_df[target], test_predictions))\nrmse","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Second Model with more categorical features"},{"metadata":{},"cell_type":"markdown","source":"## Model Building"},{"metadata":{},"cell_type":"markdown","source":"Instantiate model"},{"metadata":{"trusted":true},"cell_type":"code","source":"lm = LinearRegression()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Train the model on the train dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"lm.fit(train_df[features], train_df[target])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Predictions"},{"metadata":{},"cell_type":"markdown","source":"Make predictions on the test set"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_predictions = lm.predict(test_df[features])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets make a series that contains the actual target values from the test set."},{"metadata":{"trusted":true},"cell_type":"code","source":"actual_target = test_df[target].reset_index(drop=True)\nactual_target.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we can concat the actual and predicted values."},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_df = pd.concat([actual_target, pd.Series(test_predictions)], axis=1, ignore_index=True)\npred_df.columns = ['Actual', 'Predicted']\npred_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Visual of the predictions vs the actuals"},{"metadata":{},"cell_type":"markdown","source":"Visual of the first 25 predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_df = pred_df.head(50)\nplot_df.plot(kind='bar',figsize=(16,10))\nplt.grid(which='major', linestyle='-', linewidth='0.5', color='green')\nplt.grid(which='minor', linestyle=':', linewidth='0.5', color='black')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## RMSE"},{"metadata":{},"cell_type":"markdown","source":"The rmse value is very high. This is not unexpected as our features were not highly correlated with our target."},{"metadata":{"trusted":true},"cell_type":"code","source":"rmse = np.sqrt(mean_squared_error(test_df[target], test_predictions))\nrmse","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusion"},{"metadata":{},"cell_type":"markdown","source":"This was a good exercise going through the machine learning workflow. I walked away with more questions than was expected so I have much to learn. I made alot of decisions that I was unsure about, especially with how to deal with categorical features."}],"metadata":{"hide_input":false,"kernelspec":{"display_name":"Python (py3_ds)","language":"python","name":"py3_ds"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.2"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{"height":"calc(100% - 180px)","left":"10px","top":"150px","width":"384px"},"toc_section_display":true,"toc_window_display":true},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"position":{"height":"522.85px","left":"1550px","right":"20px","top":"120px","width":"350px"},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":true}},"nbformat":4,"nbformat_minor":1}