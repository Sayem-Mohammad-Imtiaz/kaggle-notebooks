{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Business Understanding: Social Mediaâ€™s Influences on Policy Making\n\n\n\n\n# Data Understanding\n\n"},{"metadata":{},"cell_type":"markdown","source":"**About Data Understanding**\n\nIn this section, we will give an overview of how the dataset looks like. We will introduce the source of the dataset and the size of this dataset -- the number of instances and attributes (which is the same as the number of rows and columns). Specifically, we will discuss the data type of each attribute and the meaning associated with each attribute. \n\n**Data Understanding of the Dataset**\n\nIn this project, we investigate the Airbnb ratings dataset (https://www.kaggle.com/samyukthamurali/airbnb-ratings-dataset). Our goal is to discover which attributes are the most influential to the ratings of the Airbnbs in the U.S.. The original dataset contains four datasets: LA_Listings, NY_Listings, airbnb_ratings_new and  airbnb-reviews. The first three datasets contain 59.9k instances and 35 attributes, including customer ID, host ID, locations, layouts, furnishings, prices of the residences, review scores ratings, etc. The last dataset contains 1325 instances and 6 attributes, which are customer ID, host ID, review ID, reviewer name, date and comments. \n"},{"metadata":{},"cell_type":"markdown","source":"# Data Preparation \n\nTo begin the exloratory data analysis and train machine learning model, we need to do the data preparation first.\n\nWe will import, combine, and filter the data we need and output an csv file for the further use.\n\n### Import Data"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Then, import libraries and define functions for plotting the data using matplotlib."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns \nimport pandas as pd\nimport scipy.stats","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, let's read in the data and prepare to combine the csvs into one.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Read the LA_Lising.csv\ndf = pd.read_csv('/kaggle/input/airbnb-ratings-dataset/LA_Listings.csv', encoding='ISO-8859-1')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Read the NY_Listings.csv\ndf2 = pd.read_csv('/kaggle/input/airbnb-ratings-dataset/NY_Listings.csv', encoding='ISO-8859-1')\ndf2.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finnally, we need to read 'airbnb_ratings_new.csv'."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Read the airbnb_ratings_new.csv\ndf3 = pd.read_csv('/kaggle/input/airbnb-ratings-dataset/airbnb_ratings_new.csv', encoding='ISO-8859-1')\npd.set_option('display.max_columns', None)\ndf3.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that in 'airbnb_ratings_new.csv' file, this dataset includes airbnb list from a lot of different countries such as Italy, China Hong Kong, Austria... But we only want to analysis the listings inside U.S because the price changes a lot in diffrent countris, so we need to filter the 'Country' collumn."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_filtered = df3[df3['Country'] == 'United States']\n\ndf_filtered.head()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, let's get more infomation with our datasets."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2.describe()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_filtered.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, let's combine those three datasets into one:"},{"metadata":{"trusted":true},"cell_type":"code","source":"combinedDf = df.append(df2)\ndf_final = combinedDf.append(df_filtered)\n\ndf_final.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Now, 'df_final' has 295,452 lines of data and ready to use.**"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}