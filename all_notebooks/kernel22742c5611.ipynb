{"cells":[{"metadata":{"id":"XH3vfalNVr3M"},"cell_type":"markdown","source":"# Readme\ni didn't done this on the github, because i just push it to colab.\nand u can just open the link to run the whole project \nhttps://colab.research.google.com/drive/1sdzln9IUD7AUtkxJ1U5SPrgQbP9mhOWC#scrollTo=XH3vfalNVr3M\n\nand all you have to do is open this link, and upload the dataset. \nthen run all the code. \n\n","execution_count":null},{"metadata":{"id":"1PDttl4HYI3p","outputId":"7077c68c-e827-4a5e-b78c-eabc79cb3f26","trusted":false},"cell_type":"code","source":"import json\nimport re\nfrom random import seed, randrange\nfrom math import log\nfrom sklearn.utils import shuffle\nimport pandas as pd\nfrom nltk.corpus import stopwords\nfrom csv import reader\nfrom sklearn.model_selection import train_test_split\n\nimport nltk\nnltk.download(\"stopwords\")","execution_count":null,"outputs":[]},{"metadata":{"id":"IkgatNm2WQ7H","trusted":false},"cell_type":"code","source":"\n","execution_count":null,"outputs":[]},{"metadata":{"id":"jvYMq__B2FBG"},"cell_type":"markdown","source":"**import the data set**\n\n```\n```\n\n","execution_count":null},{"metadata":{"id":"X_FEt8LKYNhh","outputId":"30c8ae55-f5a5-473c-f376-98609cdcfc08","trusted":false},"cell_type":"code","source":"df = pd.read_csv('bgg-13m-reviews.csv',index_col=0)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"IdzkhFpH2Uiz"},"cell_type":"markdown","source":"**Data Preprocessing**","execution_count":null},{"metadata":{"id":"Liu0vzC6YQYj","outputId":"4278e38b-8aed-4bc5-bf12-cb8647f582eb","trusted":false},"cell_type":"code","source":"df = df.dropna()\ndf.describe()","execution_count":null,"outputs":[]},{"metadata":{"id":"t6Ejg3HCYSZR","outputId":"908c43ee-e940-44b5-f67c-eaa9703f7619","trusted":false},"cell_type":"code","source":"reviews = df[['rating','comment']]\nreviews.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"8saDu5MJYT-C","outputId":"ac3e5657-59cf-4a9b-d9d7-830151d9d709","trusted":false},"cell_type":"code","source":"reviews = shuffle(reviews)\nreviews.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"jT6qF9S64PT4"},"cell_type":"markdown","source":"# Word Segmentation\n\nWe just claimed word segmentation is a hard problem, but in fact the segmentation part is quite easy! We’ll give a quick overview of the segmentation algorithm which assumes that we can evaluate a segmentation for optimality.\n\nFirst, we note that by an elementary combinatorial argument there are 2^{n-1} segmentations of a word with n letters. To see this, imagine writing a segmentation of “homebuiltairplanes” with vertical bars separating the letters, as in “home | built | ai |  rpla | nes”. The maximum number of vertical bars we could place is one less than the number of letters, and every segmentation can be represented by describing which of the n-1 gaps contain vertical bars and which do not. We can hence count up all segmentations by counting up the number of ways to place the bars. A computer scientist should immediately recognize that these “bars” can represent digits in a binary number, and hence all binary numbers with n-1 digits correspond to valid segmentations, and these range from 0 to 2^{n-1} - 1, giving 2^{n-1} total numbers.\n\n\n# first:\n we implement the “splitPairs” function, which accepts a string s as input and returns a list containing all possible split pairs (u,v) where s = uv. We achieve this by a simple list comprehension (gotta love list comprehensions!) combined with string slicing\n# secondly\nNote that the last entry in this list is crucial, because we may not want to segment the input word at all, and in the following we assume that “splitPairs” returns all of our possible choices of action. Next we define the “segment” function, which computes the optimal segmentation of a given word. In particular, we assume there is a global function called “wordSeqFitness” which reliably computes the fitness of a given sequence of words, with respect to whether or not it’s probably the correct segmentation.","execution_count":null},{"metadata":{"id":"sH4pCw3XYViz","trusted":false},"cell_type":"code","source":"\nwith open(data_drict,'r',encoding='utf-8') as f:\n    row_data = reader(f)\n    review = []\n    rate = []\n    for row in row_data:\n        if  row[0] != '' and row[3] !='':\n            rate.append(round(float(row[2])))\n            content = row[3].lower()\n            content = content.replace(\"\\r\", \"\").strip()\n            content = content.replace(\"\\n\", \"\").strip()\n            content = re.sub(\"[%s]+\"%('.,|?|!|:|;\\\"\\-|#|$|%|&|\\|(|)|*|+|-|/|<|=|>|@|^|`|{|}|~\\[\\]'), \"\", content)\n            sentence = content.split(' ')\n            for i in stopwords:\n                while i in sentence:\n                    sentence.remove(i)\n            content = ' '.join(sentence)\n            review.append(content)\n\n","execution_count":null,"outputs":[]},{"metadata":{"id":"bAqvk4VQYXqA","trusted":false},"cell_type":"code","source":"x = [review for review in reviews['comment']]\ny = [round(r) for r in reviews['rating']]","execution_count":null,"outputs":[]},{"metadata":{"id":"O7krijAy8-F7"},"cell_type":"markdown","source":"# Divide Data\n ","execution_count":null},{"metadata":{"id":"ZURD0755YZtm","outputId":"898f9f63-b5f0-4b17-f426-30397bda1754","trusted":false},"cell_type":"code","source":"x_train, x_test, y_train, y_test = train_test_split(review, rate, test_size=0.3, random_state=0)\nprint('Size of Train Set: ', len(x_train))\nprint('Size of Test Set: ', len(x_test))\n","execution_count":null,"outputs":[]},{"metadata":{"id":"zTsVK0pL-ejV"},"cell_type":"markdown","source":"# Text Feature Extraction\n\nn a large text corpus, some words will be very present (e.g. “the”, “a”, “is” in English) hence carrying very little meaningful information about the actual contents of the document. If we were to feed the direct count data directly to a classifier those very frequent terms would shadow the frequencies of rarer yet more interesting terms.\n\nIn order to re-weight the count features into floating point values suitable for usage by a classifier it is very common to use the tf–idf transform.\n\nTf means term-frequency while tf–idf means term-frequency times inverse document-frequency:\ntf-idf(t,d) = tf(t,d) * idf(t)\n\n\nUsing the TfidfTransformer’s default settings, TfidfTransformer(norm='l2', use_idf=True, smooth_idf=True, sublinear_tf=False) the term frequency, the number of times a term occurs in a given document, is multiplied with idf component, which is computed as\nidf(t) = log(1+n/1+df(t)) +1\n\nwhere n is the total number of documents in the document set, and   df(t) is the number of documents in the document set that contain term . The resulting tf-idf vectors are then normalized by the Euclidean norm.\n\nThe following sections contain further explanations and examples that illustrate how the tf-idfs are computed exactly and how the tf-idfs computed in scikit-learn’s TfidfTransformer and TfidfVectorizer differ slightly from the standard textbook notation that defines the idf as\nidf(t) = log(n/1+df(t))\n\nIn the TfidfTransformer and TfidfVectorizer with smooth_idf=False, the “1” count is added to the idf instead of the idf’s denominator:\n\nidf(t) = log(n/df(t)) +1\n\n\n\n","execution_count":null},{"metadata":{"id":"G7tQfzLeYb5X","trusted":false},"cell_type":"code","source":"# Get all the words in the training set non-repeatedly and record the index of each word\n\nwords_index_dict = {}\nindex = 0\nfor rating in x_train:\n    for word in rating:\n        if word in words_index_dict:\n          continue\n        else:\n            words_index_dict[word]=index\n            index+=1\n\n","execution_count":null,"outputs":[]},{"metadata":{"id":"h7y_RniJYeZB","trusted":false},"cell_type":"code","source":"\ndef set_tf(idf):\n    temp = []\n    for cont in idf:\n        temp.append(log(len(x_train)/(cont+1)))\n    return temp\ntf={}\nidf = [0 for _ in range(len(words_index_dict))]\nfor review_index, review in enumerate(x_train):\n    review_counts = pd.value_counts(review)\n    for word_index, word in enumerate(review):\n        if word not in words_index_dict:\n          continue\n        else:\n            tf[(review_index,words_index_dict[word])] = review_counts[word]/len(review)\n            idf[words_index_dict[word]]+=1\nidf = set_tf(idf)\n\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"id":"trWVlnhe-njT"},"cell_type":"markdown","source":"# Algorithms\n\nNaive Bayes Classifier. Naive Bayes is a kind of classifier which uses the Bayes Theorem. It predicts membership probabilities for each class such as the probability that given record or data point belongs to a particular class. The class with the highest probability is considered as the most likely class.\n\n\n![alt text](https://www.analyticsvidhya.com/wp-content/uploads/2015/09/Bayes_rule-300x172-300x172.png)\n\n# Naive Bayes Classifier – Example\n\nBecause it is a supervied learning algorithm, we have a dataset with samples and labels accordingly. First, Naive Bayes Classifier calculates the probability of the classes. What does it mean exactly? Calculating that if we choose a random sample, what is the probability it belongs to a given class?\n\n![alt text](https://www.globalsoftwaresupport.com/wp-content/uploads/2018/02/naivebayes7.png)\n\n\n\nAfter the training procedure we want to classify the new sample (circle with question mark). Then we have to consider the neighborhood of that sample. We can make predictions based on Bayes theorem.\n![alt text](https://www.globalsoftwaresupport.com/wp-content/uploads/2018/02/naivebayes8.png)","execution_count":null},{"metadata":{"id":"LRMv5rx2VpP2"},"cell_type":"markdown","source":"","execution_count":null},{"metadata":{"id":"pqj6XrokhfDy","trusted":false},"cell_type":"code","source":"class Naive_Bayes:\n    def __init__(self, data):\n        self.d = data.iloc[:, 1:]\n        self.headers = self.d.columns.values.tolist()\n        self.prior = np.zeros(len(self.d['Class'].unique()))\n        self.conditional = {}\n    \n    def build(self):\n        y_unique = self.d['Class'].unique()\n        for i in range(0,len(y_unique)):\n            self.prior[i]=(sum(self.d['Class']==y_unique[i])+1)/(len(self.d['Class'])+len(y_unique))\n            \n        for h in self.headers[:-1]:\n            x_unique = list(set(self.d[h]))\n            x_conditional = np.zeros((len(self.d['Class'].unique()),len(set(self.d[h]))))\n            for j in range(0,len(y_unique)):\n                for k in range(0,len(x_unique)):\n                    x_conditional[j,k]=(self.d.loc[(self.d[h]==x_unique[k])&(self.d['Class']==y_unique[j]),].shape[0]+1)/(sum(self.d['Class']==y_unique[j])+len(x_unique))\n        \n            x_conditional = pd.DataFrame(x_conditional,columns=x_unique,index=y_unique)   \n            self.conditional[h] = x_conditional       \n        return self.prior, self.conditional\n    \n    def predict(self, X):\n        classes = self.d['Class'].unique()\n        ans = []\n        for sample in X:\n            prob = []\n            for i in range(len(self.prior)):\n                p_i = self.prior[i]\n                for j, h in enumerate(self.headers[:-1]):\n                    p_i *= self.conditional[h][sample[j]][i]\n                prob.append(p_i)\n            ans.append(classes[np.argmax(prob)])\n        return ans","execution_count":null,"outputs":[]},{"metadata":{"id":"yQCWxLJfYgTf"},"cell_type":"markdown","source":"for key in tf:\n    tf[key]*=idf[key[1]]\n    tfidf=dict()\nfor rating in range(11):\n    tfidf[rating]=[0 for _ in range(len(words_index_dict))]\nfor key, value in tf.items():\n    label = y_train[key[0]]\n    word_index = key[1]\n    tfidf[label][word_index]+=value\nfor i in range(len(tfidf)):\n    row_sum = sum(tfidf[i])\n    tfidf[i]=[x/row_sum for x in tfidf[i]]\n","execution_count":null},{"metadata":{"id":"XuWaZmwdYs-r","trusted":false},"cell_type":"code","source":"label_count = [0 for _ in range(11)] + [len(x_train)]\nfor rating in y_train:\n    label_count[rating]+=1","execution_count":null,"outputs":[]},{"metadata":{"id":"2ra0QTBxYuzX","trusted":false},"cell_type":"code","source":"nb = Naive_Bayes()\n","execution_count":null,"outputs":[]},{"metadata":{"id":"KGXhsA2nY1wR","trusted":false},"cell_type":"code","source":"\naccuracy = sum([nb.predict(x_test[i]) == y_test[i] for i in range((len(x_test)))])/len(x_test)\nprint(\"Accuracy of Test set is:\", accuracy)","execution_count":null,"outputs":[]},{"metadata":{"id":"uNK_3blx5dKO"},"cell_type":"markdown","source":"# Reference\nhttps://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction\n\nhttps://jeremykun.com/2012/01/15/word-segmentation/\n\n\n\n> \n\n","execution_count":null},{"metadata":{"id":"-bxsUnybCCTo"},"cell_type":"markdown","source":"","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}