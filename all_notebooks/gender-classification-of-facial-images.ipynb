{"cells":[{"metadata":{},"cell_type":"markdown","source":"<h1>Gender Classification of Facial Images Using CNN</h1>\n\n<h2>This Notebook Covers</h2>\n<h3><ol><li><a href=\"#1\">Exploratory Data Analysis &amp; Data Cleaning</a></li>\n    <li><a href=\"#2\">Data Visualization</a></li>\n    <li><a href=\"#3\">Image Augmentation</a></li>\n    <li><a href=\"#4\">Model Development</a></li>\n    <li><a href=\"#5\">Model Evaluation</a></li>\n    <li><a href=\"#6\">Error Analysis</a></li>"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport random\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense,Conv2D,MaxPool2D,Dropout,Flatten\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.utils import to_categorical\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom  IPython.display import display\nfrom tensorflow.random import set_seed\nnp.random.seed(11)\nset_seed(11)\nrandom.seed(11)\n!PYTHONHASHSEED=0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"../input/age-gender-and-ethnicity-face-data-csv/age_gender.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"1\"></a><h2>Exploratory Data Analysis &amp; Data Cleaning</h2>"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>As seen above in the column 'pixels', the pixel data is in the form of a string where each pixel is separated by space. The function below converts the string into pixel array one row at a time."},{"metadata":{"trusted":true},"cell_type":"code","source":"def img_arr(x):\n    '''\n    Function to convert pixel data (string) into array of pixels\n    '''\n    x=x.reset_index(drop=True)\n    n = len(x) #number of rows\n    for i in range(n):\n        if i==0:\n            arr = np.array(x[i].split()).astype(np.int16) #Initializing the array\n        else:\n            arr = np.append(arr,np.array(x[i].split()).astype(np.int16),axis=0) #Appending data to the array\n    return arr.reshape(n,48,48,1) #reshaping the array to 4-dim image pixel array","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Splitting dataset into X and y\nX = df.iloc[:,4].copy()\ny = df.iloc[:,2].copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# As seen below the class is fairly balanced\ny.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y.value_counts().plot(kind=\"bar\")\nplt.title(\"Label Distribution\")\nplt.xlabel(\"Labels\")\nplt.ylabel(\"Count\");","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#splitting the data into train and te sets. 'te' set will be further split into validation and test sets \nX_train,X_te,y_train,y_te = train_test_split(X,y,test_size=0.3,random_state=11)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#splitting 'te' set into validation and test set\nX_val,X_test,y_val,y_test = train_test_split(X_te,y_te,test_size=0.15,random_state=11)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Converting the string of pixels into image array for each of train, val and test set\nX_train = img_arr(X_train)\nX_test = img_arr(X_test)\nX_val = img_arr(X_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = y_train.values\ny_test = y_test.values\ny_val = y_val.values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"2\"></a><h2>Data Visualization</h2>\n<h3>The code below displays 100 random faces and their genders. This helps in identifying anomalies in labeling and also helps in framing rules for image augmentation."},{"metadata":{"trusted":true},"cell_type":"code","source":"rows=20 #rows in subplots\ncols=5 #columns in subplots\nsamp = random.sample(range(X_train.shape[0]),rows*cols) #selecting 100 random samples\nx_samp = X_train[samp,:,:,:]\ny_samp = y_train[samp]\n\nfig,ax = plt.subplots(rows,cols,figsize=(12,45))\nr = 0\nc = 0\nfor i in range(rows*cols):\n    aa = x_samp[i,:,:,:].reshape(48,48)\n    ax[r,c].axis(\"off\")\n    ax[r,c].imshow(aa,cmap=\"gray\")\n    ax[r,c].set_title(f\"Gender: {'Female' if y_samp[i]==1 else 'Male'}\")\n    c+=1\n    if c == cols:\n        c=0\n        r+=1\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>From the above data visualization, it is found that 0 indicates Male and 1 indicates Female. Also, there are a few images with wrong labels."},{"metadata":{"trusted":true},"cell_type":"code","source":"set_seed(11)\nrandom.seed(11)\nnp.random.seed(11)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"3\"></a><h2>Image Augmentation</h2>\n<h3>Image augmentation is a process of transorming images with a set of pre-specified rules. Image augmentation doesn't result in additional images, rather it randomly transforms the images in every epoch and inputs to the CNN. This enables the CNN to train on multiple tranforms of the original image and prevents overfitting.</h3>\n<h3>We must transform the training images only, validation and test images must be left untouched, except for scaling"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data_gen = ImageDataGenerator(rotation_range=30,\n                                   width_shift_range=1,\n                                    brightness_range=[0.8,1.2],\n                                    zoom_range=[0.8,1.2],\n                                    rescale=1/255\n                                   )\n\n\nval_data_gen = ImageDataGenerator(rescale=1/255)\n\ntest_data_gen = ImageDataGenerator(rescale=1/255)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>The plot below shows random images in their original and augmented form"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,ax = plt.subplots(10,5,figsize=(15,25))\nfor n in range(10):    \n    r = random.sample(range(X_train.shape[0]),1)[0]\n    ax[n,0].imshow(X_train[r].reshape(48,48),cmap=\"gray\")\n    ax[n,0].set_title(\"Original\")\n    ax[n,0].axis(\"off\")\n    for i in range(1,5):\n        ax[n,i].imshow(train_data_gen.random_transform(X_train[r]).reshape(48,48),cmap=\"gray\")\n        ax[n,i].set_title(\"Augmented\")\n        ax[n,i].axis(\"off\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"set_seed(11)\nrandom.seed(11)\nnp.random.seed(11)\ntraining_data = train_data_gen.flow(X_train,y_train,\n                                   seed=11)\n\nval_data = val_data_gen.flow(X_val,y_val,\n                                   seed=11,shuffle=False)\n\ntest_data = test_data_gen.flow(X_test,y_test,\n                                   seed=11,shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"INPUT_SHAPE = (48,48,1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"4\"></a><h2>Model Development</h2>\nThe CNN below is inspired by VGG16 and to match the current data the network architecture is modified accordingly"},{"metadata":{"trusted":true},"cell_type":"code","source":"random.seed(11)\nset_seed(11)\nnp.random.seed(11)\nmodel = Sequential()\n\nmodel.add(Conv2D(filters=64,kernel_size=3,strides=1,activation=\"relu\",input_shape=INPUT_SHAPE))\nmodel.add(Conv2D(filters=64,kernel_size=3,strides=1,activation=\"relu\"))\nmodel.add(MaxPool2D(pool_size=(2,2),strides=(2,2),padding=\"same\"))\n\nmodel.add(Conv2D(filters=128,kernel_size=3,strides=1,activation=\"relu\"))\nmodel.add(Conv2D(filters=128,kernel_size=3,strides=1,activation=\"relu\"))\nmodel.add(MaxPool2D(pool_size=(2,2),strides=(2,2),padding=\"same\"))\n\nmodel.add(Flatten())\n\nmodel.add(Dense(units=512,activation=\"relu\"))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(units=1024,activation=\"relu\"))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(units=1,activation=\"sigmoid\"))\n\nmodel.compile(optimizer=\"adam\",loss=\"binary_crossentropy\",metrics=[\"binary_accuracy\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"early_stop = EarlyStopping(monitor=\"val_loss\",patience=5,mode=\"min\") #Ensure the model doesn't overfit","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"random.seed(11)\nset_seed(11)\nnp.random.seed(11)\nhistory = model.fit(training_data,batch_size=32,epochs=500,callbacks=early_stop,validation_data=val_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Dataframe capturing the accuracy and loss per epoch\nloss_df = pd.DataFrame(history.history)\nloss_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loss_df.plot();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>Since, we got an idea on the optimum number of epochs to run from the above model training, now we'll concatenate the X_train, X_val and y_train, y_val to train the model on a larger dataset for a better performance. While training the model above I found 20 epochs as the optimum (might change due to randomness). Hence we'll train the final model for 20 epochs."},{"metadata":{"trusted":true},"cell_type":"code","source":"Final_train = np.append(X_train,X_val,axis=0)\nFinal_val = np.append(y_train,y_val,axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_training_data = train_data_gen.flow(Final_train,Final_val,\n                                   seed=11)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"random.seed(11)\nset_seed(11)\nnp.random.seed(11)\nfinal_model_history = model.fit(final_training_data,batch_size=32,epochs=20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"5\"></a><h2>Model Evaluation"},{"metadata":{"trusted":true},"cell_type":"code","source":"model.evaluate(test_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction = model.predict(test_data).flatten()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(prediction)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction = np.round(prediction) #rounding so that the prediction >0.5 becones 1 and everything else becomes 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(confusion_matrix(y_test,prediction),annot=True,cbar=False,fmt=\"d\")\nplt.xlabel(\"Prediction\")\nplt.ylabel(\"Actual\");","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test,prediction))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"6\"></a><h2>Error Analysis</h2>\n<h3>Analyzing the errors visually may help in tuning image augmentation parameters as well as the model architecture. It also gives an idea of how the model may perform in the future and determine if the model matches human level performance."},{"metadata":{"trusted":true},"cell_type":"code","source":"error_index = (y_test != prediction)#finding error indices\ny_test_error = y_test[error_index]\nX_test_error = X_test[error_index]\nprediction_error = prediction[error_index]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>Below we visualize the errors and identify actual label vs predicted labels"},{"metadata":{"trusted":true},"cell_type":"code","source":"rows=int(np.floor(sum(error_index)/3)) #rows in subplots\ncols=3 #columns in subplots\nx_samp = X_test_error\ny_samp = y_test_error\n\nfig,ax = plt.subplots(rows,cols,figsize=(15,200))\nr = 0\nc = 0\nfor i in range((rows*cols)-1):\n    aa = x_samp[i].reshape(48,48)\n    ax[r,c].axis(\"off\")\n    ax[r,c].imshow(aa,cmap=\"gray\")\n    actual_lab = \"Female\" if y_samp[i]==1 else \"Male\"\n    pred_lab = \"Female\" if int(prediction_error[i])==1 else \"Male\"\n    ax[r,c].set_title(f'Actual: {actual_lab}\\nPred: {pred_lab}')\n    c+=1\n    if c == cols:\n        c=0\n        r+=1\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>From the above error analysis, we can interpret that the model majorly misclassfied images of babies and kids (which even a human finds difficult to classify). This shows that beard and moustache and hair length might be the important features captured by the model for classifying the gender."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}