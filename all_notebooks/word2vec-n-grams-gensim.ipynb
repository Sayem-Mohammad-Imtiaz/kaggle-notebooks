{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-07T04:17:31.095317Z","iopub.execute_input":"2021-08-07T04:17:31.095764Z","iopub.status.idle":"2021-08-07T04:17:31.10764Z","shell.execute_reply.started":"2021-08-07T04:17:31.095729Z","shell.execute_reply":"2021-08-07T04:17:31.106064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gensim\nimport re\nimport pickle\nimport string\nimport nltk\nfrom functools import partial\n\nfrom nltk.corpus import stopwords\nsw = stopwords.words('english')\npuncts = re.compile('[%s]' % re.escape(string.punctuation))\nfrom nltk.stem import WordNetLemmatizer\nlemmatizer = WordNetLemmatizer()","metadata":{"execution":{"iopub.status.busy":"2021-08-07T04:17:31.109474Z","iopub.execute_input":"2021-08-07T04:17:31.109823Z","iopub.status.idle":"2021-08-07T04:17:31.124302Z","shell.execute_reply.started":"2021-08-07T04:17:31.109793Z","shell.execute_reply":"2021-08-07T04:17:31.123082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.read_pickle('../input/simplenormal-wikipedia-sections/wikipedia_sections.pkl.xz')['text'].tolist()\nif not data[-1]:\n    del(data[-1])\ndata = data[int(0.75*len(data)):]","metadata":{"execution":{"iopub.status.busy":"2021-08-07T04:17:31.126543Z","iopub.execute_input":"2021-08-07T04:17:31.127299Z","iopub.status.idle":"2021-08-07T04:18:18.473445Z","shell.execute_reply.started":"2021-08-07T04:17:31.127254Z","shell.execute_reply":"2021-08-07T04:18:18.472242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def clean_text(text):\n    text = text.replace('\\n', ' ')\n    text = text.replace('\\r', ' ')\n    text = text.replace('\\t', ' ')\n    text = text.strip(' ')\n    text = re.sub('\\s+', ' ', text)\n    text = text.encode('ascii','ignore').decode('utf-8')\n    text = re.sub(r'https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)', \"\", text)\n    text = re.sub(r'[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)', \"\", text)\n    text = re.sub('[A-Za-z0-9_.]+@[A-Za-z0-9_.]+', '', text)\n    text = re.sub(r'#[a-zA-Z0-9_.]+', '', text)\n    text = re.sub('[0-9]+', '', text)\n    text = puncts.sub(' ', text)\n    text = re.sub('  ',' ',text)\n    text = text.lower()\n    text = text.split()\n    text = [lemmatizer.lemmatize(i) for i in text if i not in sw]\n    return text\n\ndef clean_batch(texts, clean_function):\n    return list(map(clean_function, texts))","metadata":{"execution":{"iopub.status.busy":"2021-08-07T04:18:18.475139Z","iopub.execute_input":"2021-08-07T04:18:18.475448Z","iopub.status.idle":"2021-08-07T04:18:18.485835Z","shell.execute_reply.started":"2021-08-07T04:18:18.475419Z","shell.execute_reply":"2021-08-07T04:18:18.484699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = clean_batch(data, clean_text)","metadata":{"execution":{"iopub.status.busy":"2021-08-07T04:18:18.487039Z","iopub.execute_input":"2021-08-07T04:18:18.487393Z","iopub.status.idle":"2021-08-07T04:45:37.596472Z","shell.execute_reply.started":"2021-08-07T04:18:18.487362Z","shell.execute_reply":"2021-08-07T04:45:37.592209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_file(filename):\n    with open(filename, 'rb') as f:\n        data = pickle.load(f)\n    return data","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bigrams = load_file('../input/ngrams-gensim/bigrams.pkl')\nbigrams.add_vocab(data)\nbigrams = bigrams.freeze()\nwith open('./bigrams.pkl', 'wb') as f:\n    pickle.dump(bigrams, f)","metadata":{"execution":{"iopub.status.busy":"2021-08-07T04:45:37.597987Z","iopub.status.idle":"2021-08-07T04:45:37.598637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trigrams = load_file('../input/ngrams-gensim/trigrams.pkl')\ntrigrams.add_vocab(bigrams[data])\ntrigrams = trigrams.freeze()\nwith open('./trigrams.pkl', 'wb') as f:\n    pickle.dump(trigrams, f)","metadata":{"execution":{"iopub.status.busy":"2021-08-07T04:45:37.60011Z","iopub.status.idle":"2021-08-07T04:45:37.600726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}