{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"\n\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport matplotlib as mp\nimport scipy\nimport scipy.stats\nimport tensorflow as tf\n#import tensorflow_hub as hub\nimport json\nimport pickle\nimport urllib\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelBinarizer\n\nprint(tf.__version__)\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nfrom wordcloud import WordCloud, STOPWORDS\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## import the dataframe \n"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"\n\npath = os.path.join(dirname, filename)\ndf = pd.read_csv(path) \n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\ndf.columns\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data=df.Text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\ndf.head()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wc=WordCloud(width=200,height=100,background_color='black',stopwords=STOPWORDS\n            ).generate(str(data))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## overall wordcloud"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nfig=plt.figure(figsize=(10,10),facecolor='k',edgecolor='w')\nplt.imshow(wc,interpolation='bicubic')\nplt.axis(\"off\")\nplt.tight_layout()\nplt.show()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nfrom nltk.corpus import stopwords\n\nfrom nltk.stem.wordnet import WordNetLemmatizer\n\nimport string\nimport gensim\nfrom gensim import corpora\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## cleanup of punctuations fillers stopwords etc"},{"metadata":{"trusted":true},"cell_type":"code","source":"stop = set(stopwords.words('english'))\n\nexclude = set(string.punctuation)\n\nlemmma= WordNetLemmatizer() #base word conversion for bbetter tuning and performance\n\ndef clean(doc):\n    stop_free=\" \".join([i for i in doc.lower().split() if i not in stop])\n    punc_free=\"\".join([char for char in stop_free if char not in exclude])\n    normalisation = \" \".join(lemmma.lemmatize(word) for word in punc_free.split(' '))\n    stop_free1=\" \".join([i for i in normalisation.lower().split() if i not in ['like','know','go','ah','okay','ok','so','thats','there','right','no','good','think','yes','yeah','thing','stuff','this','it','that','the']])\n    return stop_free1\n\ndocument=df.Text.to_list()\n\n\ndoc_clean=[clean(docu).split() for docu in document ]\n\ndoc_clean[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\ndf['text_clean']=pd.Series(doc_clean)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\ndf.head()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dictionary=corpora.Dictionary(doc_clean)\n\nprint(dictionary)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## create document for processing with word frequencies corpus"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\ndoc_word_freqcies=[dictionary.doc2bow(term) for term in doc_clean]\ndoc_word_freqcies[:30]\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nfrom gensim.models import LdaModel\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## create LDA Model in gensim"},{"metadata":{"trusted":true},"cell_type":"code","source":"model=LdaModel(doc_word_freqcies,num_topics=4,id2word=dictionary,passes=800) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Analyze the types of topics"},{"metadata":{"trusted":true},"cell_type":"code","source":"types= model.show_topics()\nfor t in types:\n    print(t)\n    print('----------------')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"diction={}\nfor i in range(4):\n    words=model.show_topic(i,topn=20)\n    #print(words)\n    diction[\"Topic number\" + \"{}\".format(i)]=[i[0] for i in words]\n    \n    \npd.DataFrame(diction)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nimport pyLDAvis.gensim\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Vis=pyLDAvis.gensim.prepare(model,doc_word_freqcies,dictionary,sort_topics=False)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## visualize topics with word distribution \ntopics found\nAI,\nCars,\nFlamehtrower,\nEnergy, solar and tunnel"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\npyLDAvis.display(Vis)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\n# Create Corpus: Term Document Frequency\ncorpus = [dictionary.doc2bow(text) for text in doc_clean]\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def format_topics_sentences(ldamodel=None, corpus=corpus, texts=data):\n    # Init output\n    sent_topics_df = pd.DataFrame()\n\n    # Get main topic in each document\n    for i, row_list in enumerate(ldamodel[corpus]):\n        row = row_list[0] if ldamodel.per_word_topics else row_list            \n        # print(row)\n        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n        # Get the Dominant topic, Perc Contribution and Keywords for each document\n        for j, (topic_num, prop_topic) in enumerate(row):\n            if j == 0:  # => dominant topic\n                wp = ldamodel.show_topic(topic_num)\n                topic_keywords = \", \".join([word for word, prop in wp])\n                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n            else:\n                break\n    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n\n    # Add original text to the end of the output\n    contents = pd.Series(texts)\n    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n    return(sent_topics_df)\n\n\ndf_topic_sents_keywords = format_topics_sentences(ldamodel=model, corpus=corpus, texts=doc_clean)\n\n# Format\ndf_dominant_topic = df_topic_sents_keywords.reset_index()\ndf_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\ndf_dominant_topic.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nvc=df_dominant_topic.Dominant_Topic.value_counts()\nvc\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\n#topic 2 and 4  is \n#topic 3 is\n#topic 5 IS \n#topic 1 is\ndic={1.0:\"2\",2.0:\"3\",3.0:\"4\",0.0:'1'}#,4.0:\"5\",0.0:'1'}#,5.0:\"6\",6.0:'7',0.0:\"1\",8.0:'9',7.0:'8'}\nvc=df_dominant_topic.Dominant_Topic.value_counts()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dt=df_dominant_topic[[\"Dominant_Topic\"]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\ndt\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dt.Dominant_Topic=dt.Dominant_Topic.apply(lambda row: dic[row])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\npd.DataFrame(dt.Dominant_Topic.value_counts())\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Word cloud of topics and add the topics into dataframe"},{"metadata":{},"cell_type":"markdown","source":"## we will also do some sentiment analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\n# lda is assumed to be the variable holding the LdaModel object\nimport matplotlib.pyplot as plt\nfor t in range(model.num_topics):\n    plt.figure()\n#   plt.imshow(WordCloud().fit_words(model.show_topic(t, 200)))\n    plt.imshow(WordCloud().fit_words(dict(model.show_topic(t, 200))))\n    plt.axis(\"off\")\n    plt.title(\"Topic #\" + str(t))\n    plt.show()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import OrderedDict\n\ndata_lda = {i: OrderedDict(model.show_topic(i,25)) for i in range(4)}\n#data_lda","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## visualisation of heatmap between words"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\n\ndf_lda = pd.DataFrame(data_lda)\nprint(df_lda.shape)\ndf_lda = df_lda.fillna(0).T\nprint(df_lda.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\ng=sns.clustermap(df_lda.corr(), center=0, cmap=\"RdBu\", metric='cosine'\n                 , linewidths=.75, figsize=(10, 10))\nplt.setp(g.ax_heatmap.yaxis.get_majorticklabels(), rotation=0)\nplt.show()\n#plt.setp(ax_heatmap.get_yticklabels(), rotation=0)  # For y axis","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nfrom textblob import TextBlob\ndf['pop']=df.text_clean.apply(lambda tw: TextBlob(' '.join(tw)).sentiment.polarity)\ndf\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['sent']=df['pop'].apply(lambda p: 'positive' if p>0 else  \n                           ( 'negative' if p<0  else 'neutral'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## visualisation of sentiment"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10, 10))\ndf['Time']=pd.to_datetime(df.Timestamp.str.strip(':'),format='[%H:%M:%S]')\n\ndf['Timest']=df.Time.apply(lambda x: x.hour*60 + x.second + x.minute*60)\nsns.scatterplot(x=df['Timest'], y=df['pop'],data=df,hue=df['sent']);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\ndf['mint']=df.Time.apply(lambda x:x.minute)\ndf.groupby('mint')['pop'].describe()['mean'].plot(figsize=(10,10))\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\ndf['hr']=df.Time.apply(lambda x:x.hour)\ndf.groupby('hr')['pop'].describe()['mean'].plot(figsize=(10,10))\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## polarity went up with time in the podcast"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nprint(df.sent.value_counts())\nsns.countplot(x='sent', data = df);\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,6))\nsns.distplot(df['pop'], bins=30)\nplt.title('Sentiment Distribution',size = 10)\nplt.xlabel('Polarity',size = 10)\nplt.ylabel('Frequency',size = 10)\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"count = df.groupby(['min','sent'])['text_clean'].count().reset_index().rename(\n    columns={'tweet_clean':'count'})\ncount","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"count = df.groupby(['hr','sent'])['text_clean'].count().reset_index().rename(\n    columns={'tweet_clean':'count'})\ncount","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"times = count.hr.unique()\nneut=[]\nfor hr in count.hr.unique():\n    a=count[count['hr']==hr]\n    if len(list(a[a['sent']=='neutral'].sent))>0:#list(a['sent'].unique()):\n        c=list(a[a['sent']=='neutral']['text_clean'])[0]\n        neut.append(c)\n    else:\n        neut.append(0)\npos = count.loc[count['sent'] == 'positive']['text_clean'].reset_index(drop = True)\n\nneg=[]\nfor hr in count.hr.unique():\n    a=count[count['hr']==hr]\n    if len(list(a[a['sent']=='negative'].sent))>0:#list(a['sent'].unique()):\n        c=list(a[a['sent']=='negative']['text_clean'])[0]\n        neg.append(c)\n    else:\n        neg.append(0)                        \nplt.figure(figsize=(10,10))\nplt.xticks(rotation='45')\nlin1=plt.plot(times, pos, 'ro-', label='positive')\nlin2=plt.plot(times, neut, 'g^-', label='neutral')\nlin3=plt.plot(times, neg, 'b--', label='negative')\nplt.legend()\nplt.show","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"count = df.groupby(['mint','sent'])['text_clean'].count().reset_index().rename(\n    columns={'tweet_clean':'count'})\ncount\n\n\ntimes = count.mint.unique()\nneut=[]\nfor mint in count.mint.unique():\n    a=count[count['mint']==mint]\n    if len(list(a[a['sent']=='neutral'].sent))>0:#list(a['sent'].unique()):\n        c=list(a[a['sent']=='neutral']['text_clean'])[0]\n        neut.append(c)\n    else:\n        neut.append(0)\npos = count.loc[count['sent'] == 'positive']['text_clean'].reset_index(drop = True)\n\nneg=[]\nfor mint in count.mint.unique():\n    a=count[count['mint']==mint]\n    if len(list(a[a['sent']=='negative'].sent))>0:#list(a['sent'].unique()):\n        c=list(a[a['sent']=='negative']['text_clean'])[0]\n        neg.append(c)\n    else:\n        neg.append(0)                        \nplt.figure(figsize=(10,10))\nplt.xticks(rotation='45')\nlin1=plt.plot(times, pos, 'ro-', label='positive')\nlin2=plt.plot(times, neut, 'g^-', label='neutral')\nlin3=plt.plot(times, neg, 'b--', label='negative')\nplt.legend()\nplt.show","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\ndf_dominant_topic.Dominant_Topic.shape\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['topic']=df_dominant_topic.Dominant_Topic","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\ndf.columns\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_nlp=df[['Time','hr','mint','topic','pop','sent']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nprint(df_nlp.topic.value_counts())\nsns.countplot(x='topic', data = df_nlp);\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nsns.heatmap(df_nlp.corr())\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}