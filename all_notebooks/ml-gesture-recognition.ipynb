{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Gesture Recognition"},{"metadata":{},"cell_type":"markdown","source":"import libraries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nfrom sklearn.model_selection import train_test_split,GridSearchCV\nfrom sklearn.metrics import confusion_matrix,classification_report,accuracy_score,plot_confusion_matrix\nfrom sklearn import tree\n\nimport matplotlib.pyplot as plt\n\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras import backend as K\n\nfrom sklearn.metrics import ConfusionMatrixDisplay\n\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\n\nfrom sklearn.ensemble import AdaBoostClassifier\nimport matplotlib.pyplot as plt\nimport copy\nfrom sklearn.model_selection import KFold\nimport time\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import f1_score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Read dataset and split into training and testing sets"},{"metadata":{"trusted":true},"cell_type":"code","source":"rock_data = pd.read_csv(\"../input/emg-4/0.csv\", header=None )\nscissors_data = pd.read_csv(\"../input/emg-4/1.csv\", header=None )\npaper_data = pd.read_csv(\"../input/emg-4/2.csv\", header=None )\nok_data = pd.read_csv(\"../input/emg-4/3.csv\", header=None )\ndata = pd.concat([rock_data, scissors_data, paper_data, ok_data], axis = 0)\n\n\nX = data.drop(data.columns[-1],axis=1)\ny = data[data.columns[-1]]\n\n#Split data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n\nX_train.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Check the number of times each gesture occurs in training data.\ny_train.value_counts()\nprint(\"Dataset samples:\",len(y))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Function to plot learning curves for each model"},{"metadata":{"trusted":true},"cell_type":"code","source":"#https://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html#sphx-glr-auto-examples-model-selection-plot-learning-curve-py\nprint(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.datasets import load_digits\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.model_selection import ShuffleSplit\n\ndef plot_learning_curve(estimator, title, X, y, axes=None, ylim=None, cv=None,\n                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n    if axes is None:\n        _, axes = plt.subplots(1, 1, figsize=(20, 5))\n\n    axes[0].set_title(title)\n    if ylim is not None:\n        axes[0].set_ylim(*ylim)\n    axes[0].set_xlabel(\"Training examples\")\n    axes[0].set_ylabel(\"Predictive Accuracy\")\n\n    train_sizes, train_scores, test_scores, fit_times, _ = \\\n        learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs,\n                       train_sizes=train_sizes,\n                       return_times=True,verbose=2)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    fit_times_mean = np.mean(fit_times, axis=1)\n    fit_times_std = np.std(fit_times, axis=1)\n\n    # Plot learning curve\n    axes[0].grid()\n    axes[0].fill_between(train_sizes, train_scores_mean - train_scores_std,\n                         train_scores_mean + train_scores_std, alpha=0.1,\n                         color=\"r\")\n    axes[0].fill_between(train_sizes, test_scores_mean - test_scores_std,\n                         test_scores_mean + test_scores_std, alpha=0.1,\n                         color=\"g\")\n    axes[0].plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n                 label=\"Training score\")\n    axes[0].plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n                 label=\"Cross-validation score\")\n    axes[0].legend(loc=\"best\")\n    print(\"CV Scores:\",test_scores_mean)\n\n    return plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def r2(a,b):\n    this_correlation = np.corrcoef(a, b)[0,1]\n    this_r2 = this_correlation**2\n    return this_r2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Function to plot confusion matrices"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_confusion(title, model, X_train, y_train, X_test, y_test):\n    svm_confusion_matrix = plot_confusion_matrix(model, X_train, y_train,\n                      display_labels=['Rock','Scissors', 'Paper', 'Ok'],\n                      cmap=plt.cm.YlOrBr)\n    svm_confusion_matrix.ax_.set_title(title + \" Confusion Matrix (Training Set)\")\n    plt.show()\n\n    svm_confusion_matrix = plot_confusion_matrix(model, X_test, y_test,\n                          display_labels=['Rock','Scissors', 'Paper', 'Ok'],\n                          cmap=plt.cm.YlOrBr)\n    svm_confusion_matrix.ax_.set_title(title + \" Confusion Matrix (Testing Set)\")\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Decision Tree"},{"metadata":{},"cell_type":"markdown","source":"Grid Search decision tree parameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Estimate parameters\ndecision_tree = tree.DecisionTreeClassifier(max_depth=10, \n                                  criterion='entropy',\n                                  min_samples_leaf=10,\n                                  min_samples_split=5,\n                                  random_state=0)\ndecision_tree.fit(X_train, y_train)\n\ny_train_pred = decision_tree.predict(X_train)\ny_test_pred = decision_tree.predict(X_test)\n\nprint('Decision Tree Train Accuracy' , accuracy_score(y_train, y_train_pred))\nprint('Decision Tree Test Accuracy' , accuracy_score(y_test, y_test_pred))\n\ncv = KFold(n_splits=10, random_state=0, shuffle=True)\n\nfig, axes = plt.subplots(1,1, figsize=(8, 5))\nplot_learning_curve(decision_tree, \"Initial Decision Tree Learning Rate\", \n                    X_train, y_train, axes=[axes], ylim=(0.4, 1.01),\n                    cv=cv, n_jobs=4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dt_params = {\n    \"criterion\":['gini','entropy'],\n    \"max_depth\":range(5,30),\n    \"min_samples_leaf\":range(1,5),\n    \"min_samples_split\":range(1,5)\n}\ndecision_tree = tree.DecisionTreeClassifier()\n\n\ngrid = GridSearchCV(decision_tree,\n                    param_grid = dt_params,\n                    cv=10,\n                    verbose=1,\n                    n_jobs=-1\n)\ngrid.fit(X_train,y_train)\nprint(grid.best_params_)\n#Best params:\n#{'criterion': 'gini', 'max_depth': 20, 'min_samples_leaf': 1, 'min_samples_split': 3}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Run Decision tree"},{"metadata":{"trusted":true},"cell_type":"code","source":"start_time = time.time();\ndecision_tree = tree.DecisionTreeClassifier(max_depth=20,\n                                  criterion='gini',\n                                  random_state=0,\n                                  min_samples_leaf=1,\n                                  min_samples_split=3)\ndecision_tree.fit(X_train, y_train)\nprint(\"Training time:\",time.time()-start_time)\n\nstart_time = time.time();\ny_train_pred = decision_tree.predict(X_train)\ny_test_pred = decision_tree.predict(X_test)\nprint(\"Prediction Time:\",time.time()-start_time, \"for\",len(X_train) + len(X_test),\"samples\")\n\nprint('Decision Tree Train Accuracy' , accuracy_score(y_train, y_train_pred))\nprint(\"Decision Tree Train r2 score:\",r2(y_train, y_train_pred))\nprint('Decision Tree Test Accuracy' , accuracy_score(y_test, y_test_pred))\nprint(\"Decision Tree Test r2 score:\",r2(y_test, y_test_pred))\nprint(\"F1 SCORE:\",round(f1_score(y_test, y_test_pred, average='micro'),3))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Calculate Decision Tree Learning Rate"},{"metadata":{"trusted":true},"cell_type":"code","source":"cv = KFold(n_splits=10, random_state=0, shuffle=True)\n\nestimator = tree.DecisionTreeClassifier(max_depth=20,\n                                  criterion='gini',\n                                  random_state=0,\n                                  min_samples_leaf=1,\n                                  min_samples_split=3)\nfig, axes = plt.subplots(1,1, figsize=(8, 5))\nplot_learning_curve(estimator, \"Decision Tree Learning Rate\", X_train, y_train, axes=[axes], ylim=(0.6, 1.01),\n                    cv=cv, n_jobs=4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# SVM Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Initial model\nsvm = SVC(kernel='poly',\n          C=1,\n          degree=3,\n          random_state=0)\nsvm.fit(X_train, y_train)\n\ny_train_pred = svm.predict(X_train)\ny_test_pred = svm.predict(X_test)\n\nprint(\"Using Poly Kernel:\")\nprint('SVM Train Accuracy' , accuracy_score(y_train, y_train_pred))\nprint('SVM Test Accuracy' , accuracy_score(y_test, y_test_pred))\n\ncv = KFold(n_splits=10, random_state=0, shuffle=True)\nfig, axes = plt.subplots(1,1, figsize=(8, 5))\nplot_learning_curve(svm, \"Initial Poly Learning Rate\", X_train, y_train, axes=[axes], ylim=(0.25, 0.8),\n                    cv=cv, n_jobs=4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"GridSearch poly kernel SVM parameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"svm_params1 = {\n    \"kernel\":['poly'],\n    \"degree\":range(1,5),\n    \"C\":[0.01,0.1,1,10]\n}\n\nsvm = SVC()\ngrid = GridSearchCV(svm,\n                    param_grid = svm_params1,\n                    cv=10,\n                    verbose=1,\n                    n_jobs=-1\n)\ngrid.fit(X_train,y_train)\nprint(grid.best_params_)\n#{'C': 10, 'degree': 2, 'kernel': 'poly'}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"GridSearch Rbf parameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"svm_params1 = {\n    \"kernel\":['rbf'],\n    \"gamma\":['scale','auto']\n}\n\nsvm = SVC()\ngrid = GridSearchCV(svm,\n                    param_grid = svm_params1,\n                    cv=10,\n                    verbose=1,\n                    n_jobs=-1\n)\ngrid.fit(X_train,y_train)\nprint(grid.best_params_)\n#{'gamma': 'scale', 'kernel': 'rbf'}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Run Rbf Kernel SVM"},{"metadata":{"trusted":true},"cell_type":"code","source":"start_time = time.time();\nsvm = SVC(kernel='rbf',\n          gamma='scale',\n          random_state=0)\nsvm.fit(X_train, y_train)\n\nprint(\"Execution Time:\",time.time()-start_time)\n\nstart_time = time.time();\ny_train_pred = svm.predict(X_train)\ny_test_pred = svm.predict(X_test)\nprint(\"Prediction Time:\",time.time()-start_time, \"for\",len(X_train) + len(X_test),\"samples\")\n\nprint(\"\\nUsing RBF Kernel:\")\nprint('SVM Train Accuracy' , accuracy_score(y_train, y_train_pred))\nprint(\"SVM Train r2 score:\",r2(y_train, y_train_pred))\nprint('SVM Test Accuracy' , accuracy_score(y_test, y_test_pred))\nprint(\"SVM Test r2 score:\",r2(y_test, y_test_pred))\nprint(\"F1 SCORE:\",round(f1_score(y_test, y_test_pred, average='micro'),3))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plot RBF Learning Curve"},{"metadata":{"trusted":true},"cell_type":"code","source":"cv = KFold(n_splits=10, random_state=0, shuffle=True)\nfig, axes = plt.subplots(1,1, figsize=(8, 5))\nplot_learning_curve(svm, \"RBF Learning Rate\", X_train, y_train, axes=[axes], ylim=(0.5, 1.01),\n                    cv=cv, n_jobs=4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Run Poly SVM"},{"metadata":{"trusted":true},"cell_type":"code","source":"start_time = time.time();\nsvm = SVC(kernel='poly',\n          C=10,\n          degree=2,\n          random_state=0)\nsvm.fit(X_train, y_train)\n\nprint(\"Execution Time:\",time.time()-start_time)\n\nstart_time = time.time();\ny_train_pred = svm.predict(X_train)\ny_test_pred = svm.predict(X_test)\nprint(\"Prediction Time:\",time.time()-start_time, \"for\",len(X_train) + len(X_test),\"samples\")\n\nprint(\"Using Poly Kernel:\")\nprint('SVM Train Accuracy' , accuracy_score(y_train, y_train_pred))\nprint(\"SVM Train r2 score:\",r2(y_train, y_train_pred))\nprint('SVM Test Accuracy' , accuracy_score(y_test, y_test_pred))\nprint(\"SVM Test r2 score:\",r2(y_test, y_test_pred))\nprint(\"F1 SCORE:\",round(f1_score(y_test, y_test_pred, average='micro'),3))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plot Poly kernel SVM learning rate"},{"metadata":{"trusted":true},"cell_type":"code","source":"cv = KFold(n_splits=10, random_state=0, shuffle=True)\nfig, axes = plt.subplots(1,1, figsize=(8, 5))\nplot_learning_curve(svm, \"Poly Learning Rate\", X_train, y_train, axes=[axes], ylim=(0.7, 1.01),\n                    cv=cv, n_jobs=4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# KNN Algorithm"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Initial parameter estimates\nknn = KNeighborsClassifier(n_neighbors=10,\n                           algorithm='auto',\n                           leaf_size=10)\nknn.fit(X_train, y_train)\n\ny_train_pred = knn.predict(X_train)\ny_test_pred = knn.predict(X_test)\n\nprint('KNN Train Accuracy' , accuracy_score(y_train, y_train_pred))\nprint('KNN Test Accuracy' , accuracy_score(y_test, y_test_pred))\n\ncv = KFold(n_splits=10, random_state=0, shuffle=True)\nfig, axes = plt.subplots(1,1, figsize=(8, 5))\nplot_learning_curve(knn, \"Initial KNN Learning Rate\", X_train, y_train, axes=[axes], ylim=(0.45, 0.85),\n                    cv=cv, n_jobs=4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"GridSearch knn parameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"knn_params = {\n    \"n_neighbors\":range(1,10),\n    \"leaf_size\":range(25,35),\n    \"algorithm\":['auto', 'ball_tree', 'kd_tree', 'brute']\n}\n\nknn = KNeighborsClassifier()\ngrid = GridSearchCV(knn,\n                    param_grid = knn_params,\n                    cv=10,\n                    verbose=1,\n                    n_jobs=-1\n)\ngrid.fit(X_train,y_train)\nprint(grid.best_params_)\n#{'algorithm': 'auto', 'leaf_size': 33, 'n_neighbors': 4}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Run KNN model"},{"metadata":{"trusted":true},"cell_type":"code","source":"start_time = time.time();\nknn = KNeighborsClassifier(n_neighbors=4,\n                           algorithm='auto',\n                           leaf_size=33)\nknn.fit(X_train, y_train)\n\nprint(\"Execution Time:\",time.time()-start_time)\n\n\nstart_time = time.time();\ny_train_pred = knn.predict(X_train)\ny_test_pred = knn.predict(X_test)\n\nprint(\"Prediction Time:\",time.time()-start_time, \"for\",len(X_train) + len(X_test),\"samples\")\n\nprint(\"K = 4\")\nprint('Decision Tree Train Accuracy' , accuracy_score(y_train, y_train_pred))\nprint(\"Decision Tree Train r2 score:\",r2(y_train, y_train_pred))\nprint('KNN Test Accuracy' , accuracy_score(y_test, y_test_pred))\nprint(\"KNN Test r2 score:\",r2(y_test, y_test_pred))\nprint(\"F1 SCORE:\",round(f1_score(y_test, y_test_pred, average='micro'),3))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plot KNN Learning rate"},{"metadata":{"trusted":true},"cell_type":"code","source":"cv = KFold(n_splits=10, random_state=0, shuffle=True)\nfig, axes = plt.subplots(1,1, figsize=(8, 5))\nplot_learning_curve(knn, \"KNN Learning Rate\", X_train, y_train, axes=[axes], ylim=(0.45, 0.8),\n                    cv=cv, n_jobs=4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# BOOSTING"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Initial estimates\nbooster = AdaBoostClassifier(\n    tree.DecisionTreeClassifier(max_depth=2, \n                                  criterion='entropy',\n                                  min_samples_leaf=1,\n                                  min_samples_split=2,\n                                  random_state=0),\n    n_estimators=50,\n    learning_rate=0.1,\n    random_state=0) \nbooster.fit(X_train, y_train)\n\ny_test_pred = booster.predict(X_test)\ny_train_pred = booster.predict(X_train)\nprint('Boosted Tree Train Accuracy' , accuracy_score(y_train, y_train_pred))\nprint('Boosted Tree Test Accuracy' , accuracy_score(y_test, y_test_pred))\n\ncv = KFold(n_splits=10, random_state=0, shuffle=True)\nfig, axes = plt.subplots(1,1, figsize=(8, 5))\nplot_learning_curve(booster, \"Initial ADA Booster Learning Rate\", X_train, y_train, axes=[axes], ylim=(0.55, 0.95),\n                    cv=cv, n_jobs=4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Run Boosting algorithm"},{"metadata":{"trusted":true},"cell_type":"code","source":"start_time = time.time();\nbooster = AdaBoostClassifier(\n    tree.DecisionTreeClassifier(max_depth=6,#Tested values between 1 and 20\n                                  criterion='gini',\n                                  random_state=0,\n                                  min_samples_leaf=1,\n                                  min_samples_split=3),\n    n_estimators=500,\n    learning_rate=0.5,#0.5: 0.908  #tested values between 0.01 and 1\n    random_state=0)\nbooster.fit(X_train, y_train)\n\nprint(\"Execution Time:\",time.time()-start_time)\nstart_time = time.time();\n\ny_test_pred = booster.predict(X_test)\ny_train_pred = booster.predict(X_train)\nprint(\"Prediction Time:\",time.time()-start_time, \"for\",len(X_train) + len(X_test),\"samples\")\n\nprint('Boosted Tree Train Accuracy' , accuracy_score(y_train, y_train_pred))\nprint(\"Boosted Tree Train r2 score:\",r2(y_train, y_train_pred))\nprint('Boosted Tree Test Accuracy' , accuracy_score(y_test, y_test_pred))\nprint(\"Boosted Tree Test r2 score:\",r2(y_test, y_test_pred))\nprint(\"F1 SCORE:\",round(f1_score(y_test, y_test_pred, average='micro'),3))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Calculate Boosting accuracy by iterations"},{"metadata":{"trusted":true},"cell_type":"code","source":"if False:#Turn to true to run cell.  Takes > 15 minutes.\n    start_time = time.time();\n    train_accuracies = [];\n    test_accuracies = [];\n    for i in range(1,1001,50):\n        booster = AdaBoostClassifier(\n            tree.DecisionTreeClassifier(max_depth=6,#Tested values between 1 and 20\n                                      criterion='gini',\n                                      random_state=0,\n                                      min_samples_leaf=1,\n                                      min_samples_split=3),\n            n_estimators=i,\n            learning_rate=0.5,\n            random_state=0) \n        booster.fit(X_train, y_train)\n        y_test_pred = booster.predict(X_test)\n        y_train_pred = booster.predict(X_train)\n        train_accuracies.append(accuracy_score(y_train, y_train_pred))\n        test_accuracies.append(accuracy_score(y_test, y_test_pred))\n        print(i,end=\" \")\n    print(\"Training Scores:\",train_accuracies)\n    print(\"Testing Scores:\",test_accuracies)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(range(1,1001,50),train_accuracies,marker='o',label=\"Training Set\")\nplt.plot(range(1,1001,50),test_accuracies,marker='o',label=\"Testing Set\")\nplt.xlabel('Estimators')\nplt.ylabel('Classification Accuracy')\nplt.title(\"Adaptive Boosting Accuracy by Estimator Count\")\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Show Booster Learning Rate"},{"metadata":{"trusted":true},"cell_type":"code","source":"cv = KFold(n_splits=10, random_state=0, shuffle=True)\nfig, axes = plt.subplots(1,1, figsize=(8, 5))\nplot_learning_curve(booster, \"ADA Booster Learning Rate\", X_train, y_train, axes=[axes], ylim=(0.8, 1.01),\n                    cv=cv, n_jobs=4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Neural Network"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Initial parameters\nnn = MLPClassifier(activation='relu',\n                   hidden_layer_sizes=(32,),\n                   solver='lbfgs',\n                   verbose=True,\n                   max_iter=50,\n                   random_state=1,\n                   early_stopping=True)\n\nnn.fit(X_train, y_train)\ny_test_pred = nn.predict(X_test)\ny_train_pred = nn.predict(X_train)\n\nprint('Neural Network Train Accuracy' , accuracy_score(y_train, y_train_pred))\nprint('Neural Network Test Accuracy' , accuracy_score(y_test, y_test_pred))\n\ncv = KFold(n_splits=10, random_state=0, shuffle=True)\nfig, axes = plt.subplots(1,1, figsize=(8, 5))\nplot_learning_curve(nn, \"Initial Neural Network Learning Rate\", X_train, y_train, axes=[axes], ylim=(0.65, 1.01),\n                    cv=cv, n_jobs=4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Gridsearch Network parameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"nn_params = {\n    \"hidden_layer_sizes\":[(64,),(),(64,32),(32,)],\n    \"activation\":['identity', 'logistic', 'tanh', 'relu'],\n    \"solver\":['lbfgs', 'sgd', 'adam'],\n    \"verbose\":[True]\n}\n\nnn = MLPClassifier()\ngrid = GridSearchCV(nn,\n                    param_grid = nn_params,\n                    cv=10,\n                    verbose=1,\n                    n_jobs=-1\n)\ngrid.fit(X_train,y_train)\nprint(grid.best_params_)\n#{'activation': 'relu', 'hidden_layer_sizes': (64,), 'solver': 'lbfgs', 'verbose': True}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Run neural network"},{"metadata":{"trusted":true},"cell_type":"code","source":"start_time = time.time();\nnn = MLPClassifier(activation='relu',\n                   hidden_layer_sizes=(64,),\n                   solver='lbfgs',\n                   verbose=True,\n                   max_iter=200,\n                   random_state=1,\n                   early_stopping=True)\n\nnn.fit(X_train, y_train)\nprint(\"Execution Time:\",time.time()-start_time)\nstart_time = time.time();\n\ny_test_pred = nn.predict(X_test)\ny_train_pred = nn.predict(X_train)\nprint(\"Prediction Time:\",time.time()-start_time, \"for\",len(X_train) + len(X_test),\"samples\")\n\nprint('Neural Network Train Accuracy' , accuracy_score(y_train, y_train_pred))\nprint('Neural Network Train r2 score:',r2(y_train, y_train_pred))\nprint('Neural Network Test Accuracy' , accuracy_score(y_test, y_test_pred))\nprint(\"Neural Network Test r2 score:\",r2(y_test, y_test_pred))\nprint(\"F1 SCORE:\",round(f1_score(y_test, y_test_pred, average='micro'),3))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Calculate network accuracy by iteration"},{"metadata":{"trusted":true},"cell_type":"code","source":"nn_scores_train = [];\nnn_scores = [];\nfor i in range(20):\n    nn = MLPClassifier(activation='relu',\n                   hidden_layer_sizes=(64,),\n                   solver='lbfgs',\n                   verbose=False,\n                   max_iter=i*10+1,\n                   random_state=1)\n    \n    nn.fit(X_train,y_train);\n    y_test_pred = nn.predict(X_test)\n    nn_scores_train.append(accuracy_score(y_train, y_train_pred))\n    nn_scores.append(accuracy_score(y_test, y_test_pred))\n    print(\"I\",i)\nprint(\"Training Scores:\",nn_scores_train)\nprint(\"Testing Scores:\",nn_scores)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(range(1,201,10),nn_scores_train,marker='o',label=\"Training Set\")\nplt.plot(range(1,201,10),nn_scores,marker='o',label=\"Testing Set\")\nplt.xlabel('Network Iterations')\nplt.ylabel('Classification Accuracy')\nplt.title(\"Neural Network Accuracy by Iteration Number\")\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Draw network learning rate"},{"metadata":{"trusted":true},"cell_type":"code","source":"cv = KFold(n_splits=10, random_state=0, shuffle=True)\nfig, axes = plt.subplots(1,1, figsize=(8, 5))\nplot_learning_curve(nn, \"Neural Network Learning Rate\", X_train, y_train, axes=[axes], ylim=(0.65, 1.01),\n                    cv=cv, n_jobs=4)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}