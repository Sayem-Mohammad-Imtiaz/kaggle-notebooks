{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":22,"outputs":[{"output_type":"stream","text":"['imdb-review-dataset', 'corpus-of-russian-news-articles-from-lenta']\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### Поиск гиперпараметров\nАвтоматический поиск гиперпараметров позволяет заметно упростить настройку модели. \nНиже приведён код, позволяющий декларативно перечислить множества значений гиперпараметров.\nДалее, из этих множеств фиксированное количество раз (число итераций поиск) берутся значения (случайно или фиксированно), которые передаются в описываемую программистом функцию итерации (обучения модели на заданном наборе параметров). Функция также принимает текущее состояния поиска и обновляет его через возвращаемое значение. Это позволяет сохранять лучшую модель и сравнивать ее с предыдущей, или накапливать статистику по мере поиска."},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nfrom collections import defaultdict\n\n# Абстрактный класс множества значений. Объект множества сам отвечает за выборку заданного (size) числа значений из него.\nclass HyperSampler:\n    def sample(self, size):\n        raise NotImplementedError\n\n# Простая реализация, в которой функция выборки просто задается пользователем.\n# Вариант использования - LambdaSampler(lambda size: np.random.randn(size))  - выборка из стандартного нормального распределения\nclass LambdaSampler(HyperSampler):\n    def __init__(self, fn):\n        super().__init__()\n        self._fn = fn\n\n    def sample(self, size):\n        return self._fn(size)\n\n# В данной реализации выборка не случайна. Задается массив значений, из которого берется первые size значений,\n# по одному на итерацию поиска\n# По этой причине размер массива должен быть не менее числа итераций поиска гиперпараметра.\nclass FixedArraySampler(HyperSampler):\n    def __init__(self, array):\n        super().__init__()\n        self._arr = np.asarray(array)\n\n    def sample(self, size):\n        if len(self._arr) < size:\n            raise ValueError(\"len(self._arr) < size\")\n        return self._arr[:size]\n\n# Выборка из категориального равномерного распределения. \n# Вы задаёте множество значений, и из него случайным образом отбирается size элементов \n# (с заменой, т.е. один элемент может быть выбран > 1 раза)\nclass RandomArraySampler(HyperSampler):\n    def __init__(self, array):\n        super().__init__()\n        self._arr = np.asarray(array)\n\n    def sample(self, size):\n        idx = np.random.choice(len(self._arr), size=size)\n        return self._arr[idx]\n\n# Обертка над LambdaSampler    \ndef h_lambda(fn):\n    return LambdaSampler(fn)\n\n# Обертка над RandomArraySampler, позволяющая писать так h_enum(32,64,128) вместо RandomArraySampler([32,64,128])\ndef h_enum(*values):\n    return RandomArraySampler(values)\n\n# Обертка над RandomArraySampler из существующей коллекции (lst = [1,2,3], h_enum(lst)), по сути для абстракции и сокращения имени\ndef h_set(values):\n    return RandomArraySampler(values)\n\n#Аналогичные обертки над FixedArraySampler\ndef h_fixed_enum(*values):\n    return FixedArraySampler(values)\n\ndef h_fixed_set(values):\n    return FixedArraySampler(values)\n\n# Разбиение словаря на два непересекающихся в зависимости от значения критерия\ndef split_dictionary(d, criterion):\n    a, b = {}, {}\n    for k, v in d.items():\n        if criterion(k, v):\n            a[k] = v\n        else:\n            b[k] = v\n    return a, b\n\ndef hyper_search(num_trials, parameters,\n                            iteration_function,\n                            initial_state,\n                            progress_bar=None):\n    \"\"\"Функция поиска гиперпараметров\n    \n    Параметры:\n    num_trials -- число итераций поиска\n    parameters -- словарь, ключами которого служат имена параметров, \n        а значениями - либо фиксированные значения (тогда они будут передаваться на каждую итерацию одинаковыми),\n        либо экземпляры класса HyperSampler, из которых на каждую новую итерацию выбирается новое значение\n    iteration_function -- задаваемая пользователем функция итерации. \n        Её первым аргументом является текущее состояние поиска (любой объект),\n        вторым - номер итерации,\n        Также в неё распаковывается словарь с ключами из parameters,\n        поэтому она либо должна перечислить каждый из этих ключей в своём списке параметров,\n        либо принимать неограниченное количество именованных параметров (**kvargs)\n        Функция возвращает новое состояние поиска.\n        Таким образом поиск представляет собой reduce-алгоритм\n    initial_state -- начальное состояние поиска. Может быть любым объектом, включая None.\n    В частности, если вы несколько раз запускали hyper_search, вы можете скормить сюда результат предыдущего запуска.\n    progress_bar -- Имеет три возможных значения - None (не отображать прогресс), 'tqdm' - текстовый progress bar, 'tqdm_notebook' - для jupyter\n    \n    Возвращаемое значение:\n    Конечное состояние поиска (результат вызова iteration_function на последней итерации)\n     \"\"\"\n    # Разделяем словарь по признаку необходимости выборки\n    sampled, fixed = split_dictionary(parameters, lambda _, v: isinstance(v, HyperSampler))\n    # Сразу, наперед, отбираем num_trials значений для каждого нефиксированного параметра\n    random_queue = {}\n    for name, population in sampled.items():\n        random_queue[name] = population.sample(num_trials)\n\n    if progress_bar == 'tqdm':\n        from tqdm import tqdm\n        tqdm_function = lambda iterable, **kws: tqdm(iterable,**kws)\n    elif progress_bar == 'tqdm_notebook':\n        from tqdm import tqdm_notebook\n        tqdm_function = lambda iterable, **kws: tqdm_notebook(iterable, **kws)\n    else:\n        tqdm_function = lambda iterable, **kws: iterable\n\n    # Превращаем numpy-скаляры в соотв. классы python, чтобы не сломать некоторые функции.\n    def decay(x):\n        if isinstance(x, np.generic):\n            return x.item()\n        return x\n\n    current_state = initial_state\n\n    for trial in tqdm_function(range(num_trials), desc='Trial #'):\n        # Собираем значения гиперпараметров на текущую итерацию вместе\n        current_trial_random = {k: decay(sample[trial]) for k,sample in random_queue.items()}\n        iteration_setting = dict(current_trial_random, **fixed)\n        iteration_setting.update(fixed)\n        # Вызываем функцию итерации и вызываем \n        current_state = iteration_function(current_state, trial, **iteration_setting)\n\n    return current_state","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport torch.nn as nn","execution_count":63,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.random.seed(5771)\ntorch.manual_seed(5661)","execution_count":40,"outputs":[{"output_type":"execute_result","execution_count":40,"data":{"text/plain":"<torch._C.Generator at 0x7f32e4371630>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"class PersistentModelWrapper:\n    def __init__(self, path, initial_criterion):\n        self.path = path\n        self.criterion = initial_criterion\n\n    def update(self, model, optimizer, criterion):\n        self.criterion = criterion\n        torch.save(\n            {'model_state': model.state_dict(), 'optimizer_state': optimizer.state_dict(), 'criterion': criterion},\n            self.path)\n\n    def load_model_data(self):\n        return torch.load(self.path)\n\n    def restore(self, model, optimizer):\n        model_data = self.load_model_data()\n        model.load_state_dict(model_data['model_state'])\n        optimizer.load_state_dict(model_data['optimizer_state'])\n","execution_count":5,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"!pip install pytorch-pretrained-bert","execution_count":6,"outputs":[{"output_type":"stream","text":"Collecting pytorch-pretrained-bert\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n\u001b[K    100% |████████████████████████████████| 133kB 7.4MB/s ta 0:00:01\n\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from pytorch-pretrained-bert) (1.16.3)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.6/site-packages (from pytorch-pretrained-bert) (4.31.1)\nRequirement already satisfied: torch>=0.4.1 in /opt/conda/lib/python3.6/site-packages (from pytorch-pretrained-bert) (1.0.1.post2)\nRequirement already satisfied: regex in /opt/conda/lib/python3.6/site-packages (from pytorch-pretrained-bert) (2019.4.14)\nRequirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from pytorch-pretrained-bert) (2.21.0)\nRequirement already satisfied: boto3 in /opt/conda/lib/python3.6/site-packages (from pytorch-pretrained-bert) (1.9.134)\nRequirement already satisfied: urllib3<1.25,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->pytorch-pretrained-bert) (1.22)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->pytorch-pretrained-bert) (2019.3.9)\nRequirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->pytorch-pretrained-bert) (2.6)\nRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->pytorch-pretrained-bert) (3.0.4)\nRequirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.6/site-packages (from boto3->pytorch-pretrained-bert) (0.9.4)\nRequirement already satisfied: botocore<1.13.0,>=1.12.134 in /opt/conda/lib/python3.6/site-packages (from boto3->pytorch-pretrained-bert) (1.12.134)\nRequirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /opt/conda/lib/python3.6/site-packages (from boto3->pytorch-pretrained-bert) (0.2.0)\nRequirement already satisfied: docutils>=0.10 in /opt/conda/lib/python3.6/site-packages (from botocore<1.13.0,>=1.12.134->boto3->pytorch-pretrained-bert) (0.14)\nRequirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /opt/conda/lib/python3.6/site-packages (from botocore<1.13.0,>=1.12.134->boto3->pytorch-pretrained-bert) (2.6.0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.6/site-packages (from python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\"->botocore<1.13.0,>=1.12.134->boto3->pytorch-pretrained-bert) (1.12.0)\nInstalling collected packages: pytorch-pretrained-bert\nSuccessfully installed pytorch-pretrained-bert-0.6.2\n\u001b[33mYou are using pip version 19.0.3, however version 19.1.1 is available.\nYou should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM, BertForSequenceClassification","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import logging\nlogging.basicConfig(level=logging.INFO)","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logger = logging.getLogger(__name__)\n","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')","execution_count":10,"outputs":[{"output_type":"stream","text":"WARNING:pytorch_pretrained_bert.tokenization:The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\nINFO:pytorch_pretrained_bert.file_utils:https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-vocab.txt not found in cache, downloading to /tmp/tmpgdjeol4r\n100%|██████████| 995526/995526 [00:00<00:00, 2028901.77B/s]\nINFO:pytorch_pretrained_bert.file_utils:copying /tmp/tmpgdjeol4r to cache at /tmp/.pytorch_pretrained_bert/96435fa287fbf7e469185f1062386e05a075cadbf6838b74da22bf64b080bc32.99bcd55fc66f4f3360bc49ba472b940b8dcf223ea6a345deb969d607ca900729\nINFO:pytorch_pretrained_bert.file_utils:creating metadata file for /tmp/.pytorch_pretrained_bert/96435fa287fbf7e469185f1062386e05a075cadbf6838b74da22bf64b080bc32.99bcd55fc66f4f3360bc49ba472b940b8dcf223ea6a345deb969d607ca900729\nINFO:pytorch_pretrained_bert.file_utils:removing temp file /tmp/tmpgdjeol4r\nINFO:pytorch_pretrained_bert.tokenization:loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-vocab.txt from cache at /tmp/.pytorch_pretrained_bert/96435fa287fbf7e469185f1062386e05a075cadbf6838b74da22bf64b080bc32.99bcd55fc66f4f3360bc49ba472b940b8dcf223ea6a345deb969d607ca900729\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"text = \"[CLS] Каким образом так получилось, что мы стоим на краю этой дороги [SEP]\"\ntokenized_text = tokenizer.tokenize(text)\nmasked_index = 5\ntokenized_text[masked_index] = '[MASK]'\nprint(tokenized_text)","execution_count":11,"outputs":[{"output_type":"stream","text":"['[CLS]', 'Как', '##им', 'образом', 'так', '[MASK]', '##ось', ',', 'что', 'мы', 'сто', '##им', 'на', 'краю', 'этой', 'дороги', '[SEP]']\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\nsegments_ids = [0 for _ in range(len(indexed_tokens))]\nprint(indexed_tokens)","execution_count":12,"outputs":[{"output_type":"stream","text":"[101, 23220, 13478, 20417, 12123, 103, 16353, 117, 10791, 35818, 108804, 13478, 10122, 50629, 18079, 33949, 102]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokens_tensor = torch.tensor([indexed_tokens])\nsegments_tensors = torch.tensor([segments_ids])","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # model = BertForMaskedLM.from_pretrained('bert-base-multilingual-cased').eval()\n# model = model.cuda()\n# with torch.no_grad():\n#     predictions = model.bert(tokens_tensor.cuda(), segments_tensors.cuda())\n# topk = torch.topk(predictions[0,masked_index],10)\n# print(topk)","execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class InputExample(object):\n    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n\n    def __init__(self, guid, text_a, text_b=None, label=None):\n        \"\"\"Constructs a InputExample.\n        Args:\n            guid: Unique id for the example.\n            text_a: string. The untokenized text of the first sequence. For single\n            sequence tasks, only this sequence must be specified.\n            text_b: (Optional) string. The untokenized text of the second sequence.\n            Only must be specified for sequence pair tasks.\n            label: (Optional) string. The label of the example. This should be\n            specified for train and dev examples, but not for test examples.\n        \"\"\"\n        self.guid = guid\n        self.text_a = text_a\n        self.text_b = text_b\n        self.label = label","execution_count":15,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class InputFeatures(object):\n    \"\"\"A single set of features of data.\"\"\"\n\n    def __init__(self, input_ids, input_mask, segment_ids, label_id):\n        self.input_ids = input_ids\n        self.input_mask = input_mask\n        self.segment_ids = segment_ids\n        self.label_id = label_id","execution_count":16,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# class DataProcessor(object):\n#     \"\"\"Base class for data converters for sequence classification data sets.\"\"\"\n\n#     def get_train_examples(self, data_dir):\n#         \"\"\"Gets a collection of `InputExample`s for the train set.\"\"\"\n#         raise NotImplementedError()\n\n#     def get_dev_examples(self, data_dir):\n#         \"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"\n#         raise NotImplementedError()\n\n#     def get_labels(self):\n#         \"\"\"Gets the list of labels for this data set.\"\"\"\n#         raise NotImplementedError()\n\n#     @classmethod\n#     def _read_tsv(cls, input_file, quotechar=None):\n#         \"\"\"Reads a tab separated value file.\"\"\"\n#         with open(input_file, \"r\", encoding=\"utf-8\") as f:\n#             reader = csv.reader(f, delimiter=\"\\t\", quotechar=quotechar)\n#             lines = []\n#             for line in reader:\n#                 if sys.version_info[0] == 2:\n#                     line = list(unicode(cell, 'utf-8') for cell in line)\n#                 lines.append(line)\n#             return lines","execution_count":17,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def convert_examples_to_features(examples, label_list, max_seq_length,\n                                 tokenizer, output_mode, total_examples=None):\n    \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n\n    label_map = {label : i for i, label in enumerate(label_list)}\n\n    features = []\n    for (ex_index, example) in enumerate(examples):\n        if ex_index % 10000 == 0:\n            logger.info(\"Writing example {} of {}\".format(ex_index, total_examples))\n\n        tokens_a = tokenizer.tokenize(example.text_a)\n\n        tokens_b = None\n        if example.text_b:\n            tokens_b = tokenizer.tokenize(example.text_b)\n            # Modifies `tokens_a` and `tokens_b` in place so that the total\n            # length is less than the specified length.\n            # Account for [CLS], [SEP], [SEP] with \"- 3\"\n            _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\n        else:\n            # Account for [CLS] and [SEP] with \"- 2\"\n            if len(tokens_a) > max_seq_length - 2:\n                tokens_a = tokens_a[:(max_seq_length - 2)]\n\n        # The convention in BERT is:\n        # (a) For sequence pairs:\n        #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n        #  type_ids: 0   0  0    0    0     0       0 0    1  1  1  1   1 1\n        # (b) For single sequences:\n        #  tokens:   [CLS] the dog is hairy . [SEP]\n        #  type_ids: 0   0   0   0  0     0 0\n        #\n        # Where \"type_ids\" are used to indicate whether this is the first\n        # sequence or the second sequence. The embedding vectors for `type=0` and\n        # `type=1` were learned during pre-training and are added to the wordpiece\n        # embedding vector (and position vector). This is not *strictly* necessary\n        # since the [SEP] token unambiguously separates the sequences, but it makes\n        # it easier for the model to learn the concept of sequences.\n        #\n        # For classification tasks, the first vector (corresponding to [CLS]) is\n        # used as as the \"sentence vector\". Note that this only makes sense because\n        # the entire model is fine-tuned.\n        tokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\"]\n        segment_ids = [0] * len(tokens)\n\n        if tokens_b:\n            tokens += tokens_b + [\"[SEP]\"]\n            segment_ids += [1] * (len(tokens_b) + 1)\n\n        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n\n        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n        # tokens are attended to.\n        input_mask = [1] * len(input_ids)\n\n        # Zero-pad up to the sequence length.\n        padding = [0] * (max_seq_length - len(input_ids))\n        input_ids += padding\n        input_mask += padding\n        segment_ids += padding\n\n        assert len(input_ids) == max_seq_length\n        assert len(input_mask) == max_seq_length\n        assert len(segment_ids) == max_seq_length\n\n        if output_mode == \"classification\":\n            label_id = label_map[example.label]\n        elif output_mode == \"regression\":\n            label_id = float(example.label)\n        else:\n            raise KeyError(output_mode)\n\n        if ex_index < 5:\n            logger.info(\"*** Example ***\")\n            logger.info(\"guid: %s\" % (example.guid))\n            logger.info(\"tokens: %s\" % \" \".join(\n                    [str(x) for x in tokens]))\n            logger.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n            logger.info(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n            logger.info(\n                    \"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))\n            logger.info(\"label: %s (id = %d)\" % (example.label, label_id))\n\n        features.append(\n                InputFeatures(input_ids=input_ids,\n                              input_mask=input_mask,\n                              segment_ids=segment_ids,\n                              label_id=label_id))\n    return features\n","execution_count":18,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def _truncate_seq_pair(tokens_a, tokens_b, max_length):\n    \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n\n    # This is a simple heuristic which will always truncate the longer sequence\n    # one token at a time. This makes more sense than truncating an equal percent\n    # of tokens from each, since if one sequence is very short then each token\n    # that's truncated likely contains more information than a longer sequence.\n    while True:\n        total_length = len(tokens_a) + len(tokens_b)\n        if total_length <= max_length:\n            break\n        if len(tokens_a) > len(tokens_b):\n            tokens_a.pop()\n        else:\n            tokens_b.pop()","execution_count":19,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import gc\ngc.collect()","execution_count":20,"outputs":[{"output_type":"execute_result","execution_count":20,"data":{"text/plain":"0"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"imdb_df = pd.read_csv('../input/imdb-review-dataset/imdb_master.csv', encoding='latin-1')","execution_count":23,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"imdb_df.sample(10)","execution_count":24,"outputs":[{"output_type":"execute_result","execution_count":24,"data":{"text/plain":"       Unnamed: 0   type     ...       label         file\n68730       68730  train     ...       unsup  26859_0.txt\n15866       15866   test     ...         pos  1780_10.txt\n98915       98915  train     ...       unsup   9024_0.txt\n34023       34023  train     ...         neg   6872_2.txt\n24435       24435   test     ...         pos  9493_10.txt\n91669       91669  train     ...       unsup  47503_0.txt\n80152       80152  train     ...       unsup  37138_0.txt\n76857       76857  train     ...       unsup  34172_0.txt\n94688       94688  train     ...       unsup    521_0.txt\n53942       53942  train     ...       unsup  13549_0.txt\n\n[10 rows x 5 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>type</th>\n      <th>review</th>\n      <th>label</th>\n      <th>file</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>68730</th>\n      <td>68730</td>\n      <td>train</td>\n      <td>Noticed this on cable last night and wasn't su...</td>\n      <td>unsup</td>\n      <td>26859_0.txt</td>\n    </tr>\n    <tr>\n      <th>15866</th>\n      <td>15866</td>\n      <td>test</td>\n      <td>The Dekalog 5 may be considered a violent accu...</td>\n      <td>pos</td>\n      <td>1780_10.txt</td>\n    </tr>\n    <tr>\n      <th>98915</th>\n      <td>98915</td>\n      <td>train</td>\n      <td>I wasn't expecting much from this film, but I ...</td>\n      <td>unsup</td>\n      <td>9024_0.txt</td>\n    </tr>\n    <tr>\n      <th>34023</th>\n      <td>34023</td>\n      <td>train</td>\n      <td>This show had pretty good stories, but bad dia...</td>\n      <td>neg</td>\n      <td>6872_2.txt</td>\n    </tr>\n    <tr>\n      <th>24435</th>\n      <td>24435</td>\n      <td>test</td>\n      <td>I just re-watched a few episodes of this serie...</td>\n      <td>pos</td>\n      <td>9493_10.txt</td>\n    </tr>\n    <tr>\n      <th>91669</th>\n      <td>91669</td>\n      <td>train</td>\n      <td>By 1950 Hollywood gave Dean Stockwell a lead r...</td>\n      <td>unsup</td>\n      <td>47503_0.txt</td>\n    </tr>\n    <tr>\n      <th>80152</th>\n      <td>80152</td>\n      <td>train</td>\n      <td>Viewers preferring a straightforward story and...</td>\n      <td>unsup</td>\n      <td>37138_0.txt</td>\n    </tr>\n    <tr>\n      <th>76857</th>\n      <td>76857</td>\n      <td>train</td>\n      <td>I appreciate that the series tries to give us ...</td>\n      <td>unsup</td>\n      <td>34172_0.txt</td>\n    </tr>\n    <tr>\n      <th>94688</th>\n      <td>94688</td>\n      <td>train</td>\n      <td>I picked this one up as a \"blind\" rental and w...</td>\n      <td>unsup</td>\n      <td>521_0.txt</td>\n    </tr>\n    <tr>\n      <th>53942</th>\n      <td>53942</td>\n      <td>train</td>\n      <td>\"I just want to get my clothes on and get the ...</td>\n      <td>unsup</td>\n      <td>13549_0.txt</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"dev_df = imdb_df[(imdb_df.type == 'train') & (imdb_df.label != 'unsup')]","execution_count":25,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df = imdb_df[(imdb_df.type == 'test')]","execution_count":26,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import model_selection","execution_count":27,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df, val_df = model_selection.train_test_split(dev_df, test_size=0.05, stratify=dev_df.label)","execution_count":28,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for row in train_df.iterrows():\n    print(row[1].type)\n    break","execution_count":29,"outputs":[{"output_type":"stream","text":"train\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def df_to_examples_imdb(df):\n    for idx,row in df.iterrows():\n        yield InputExample(idx,row.review,label=row.label)","execution_count":30,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features = convert_examples_to_features(df_to_examples_imdb(train_df),\n                                              ['neg','pos'],\n                                              max_seq_length=230,\n                                              tokenizer=tokenizer,\n                                              output_mode=\"classification\")","execution_count":31,"outputs":[{"output_type":"stream","text":"INFO:__main__:Writing example 0 of None\nINFO:__main__:*** Example ***\nINFO:__main__:guid: 44840\nINFO:__main__:tokens: [CLS] This comic book style film is fun ##ny , has nic ##ely pace ##d action and a great futur ##istic style to it . Writer Steven de Souza , who also wrote Commando , gives Ar ##nie pl ##enty of lines to dis ##h out : \" Sen ##d me a copy , \" after signing a contract and sta ##bbi ##ng a pen into the lawyer ##s back ; \" What a pain in the neck , \" after stran ##gli ##ng sub ##zero with bar ##bed wire ; \" He had to split , \" after sl ##ici ##ng his body between his legs ; and finally , as Kill ##ian sl ##ams through a bill ##board bearing his own face , Ar ##nie conclude ##s , \" Now that hit the spot . \" Fun ##nil ##y enough , bears some similar ##ities total reca ##ll , another sci - fi fl ##ick starring Schwarz ##ene ##gger . [SEP]\nINFO:__main__:input_ids: 101 10747 31761 12748 13351 10458 10124 41807 10756 117 10393 46267 44096 32547 10162 14204 10111 169 14772 33864 29025 13351 10114 10271 119 64609 17569 10104 53558 117 10479 10379 13954 95531 117 24952 18484 11297 20648 110319 10108 19515 10114 27920 10237 10950 131 107 18082 10162 10911 169 39740 117 107 10662 46529 169 16108 10111 16527 37801 10376 169 66558 10708 10105 38055 10107 12014 132 107 12489 169 38576 10106 10105 63938 117 107 10662 74536 20986 10376 13987 106821 10169 18121 33627 68033 132 107 10357 10374 10114 24137 117 107 10662 38523 13439 10376 10226 14333 10948 10226 51863 132 10111 21256 117 10146 30943 11630 38523 35224 11222 169 34497 25690 66455 10226 12542 13295 117 18484 11297 79460 10107 117 107 17121 10189 14946 10105 28504 119 107 47989 33099 10157 21408 117 77911 11152 13213 17285 11339 103778 11231 117 12864 58803 118 14045 58768 20898 27519 39050 12061 25749 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\nINFO:__main__:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\nINFO:__main__:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\nINFO:__main__:label: pos (id = 1)\nINFO:__main__:*** Example ***\nINFO:__main__:guid: 44065\nINFO:__main__:tokens: [CLS] Marlene Go ##rri ##s has established herself as one of the world ' s great directors . This sensitive , visual ##ly beautiful film is based on a story by Vladimir Na ##bok ##ov and capture ##s well that writer ' s dark iron ##y . John Tur ##tur ##ro gives what I consider to be his fines ##t performance ( I am usually not a fan of his ) ; and Emily Watson is br ##ill ##iant as well . Well worth seeing . [SEP]\nINFO:__main__:input_ids: 101 90440 14439 24874 10107 10393 13245 32262 10146 10464 10108 10105 11356 112 187 14772 44416 119 10747 73330 117 24559 10454 42235 10458 10124 11610 10135 169 13617 10155 18124 10685 30980 11024 10111 32083 10107 11206 10189 17556 112 187 25100 32374 10157 119 10421 105549 15698 10567 24952 12976 146 44856 10114 10347 10226 32392 10123 14432 113 146 10392 15910 10472 169 10862 10108 10226 114 132 10111 25282 19229 10124 33989 19503 24096 10146 11206 119 37025 43509 57039 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\nINFO:__main__:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\nINFO:__main__:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\nINFO:__main__:label: pos (id = 1)\nINFO:__main__:*** Example ***\nINFO:__main__:guid: 26922\nINFO:__main__:tokens: [CLS] In ##vesti ##gative reporter Darren M ##c ##G ##avi ##n ( as Carl Ko ##lch ##ak ) is back ; this time , he ' s after \" The Night St ##rang ##ler \" . Once again , police officials and fellow journalists either dis ##beli ##eve , or want to cover - up , the super ##natural angle . Producer - director Dan Curtis presents the same basic story as his preceding \" Night \" , with understand ##ably less success . < br / > < br / > Mr . Curtis ass ##emble ##s a fun supporting cast , included are \" Dark Shadows \" alumni George Di ##C ##enz ##o and Ivo ##r Francis . Jo Ann P ##flug ( as Louise Harper ) heads up a sex ##y collection of bell ##y - dancer ##s . And , although I ' ve never seen it mentioned any ##where , that must be Roger Davis as Mr . M ##c ##G ##avi ##n ' s dini ##ng companion in an early scene , fe ##ign ##ing dis ##beli ##ef in the existence of va ##mpi ##res ! < br / > < br / > * * * * The Night St ##rang ##ler ( 1 / 16 / 73 ) Dan Curtis ~ Darren M ##c ##G ##avi ##n , Jo Ann P [SEP]\nINFO:__main__:input_ids: 101 10167 63996 61477 41408 53337 150 10350 11447 30266 10115 113 10146 12225 30186 93049 10710 114 10124 12014 132 10531 10635 117 10261 112 187 10662 107 10117 12703 10838 24141 10815 107 119 23075 13123 117 15034 27730 10111 24619 85851 16106 27920 35439 19293 117 10345 21528 10114 16068 118 10741 117 10105 25212 56124 30891 119 46020 118 12461 14261 25658 41175 10105 11561 25090 13617 10146 10226 100369 107 12703 107 117 10169 49151 38565 15306 17001 119 133 33989 120 135 133 33989 120 135 12916 119 25658 13935 91259 10107 169 41807 32403 18922 117 12742 10301 107 16193 47628 107 91992 10955 12944 10858 29925 10133 10111 46024 10129 13738 119 20977 15879 153 74412 113 10146 18605 15390 114 42399 10741 169 18549 10157 14903 10108 53822 10157 118 81070 10107 119 12689 117 14779 146 112 10323 14794 15652 10271 24909 11178 30935 117 10189 14982 10347 13513 14281 10146 12916 119 150 10350 11447 30266 10115 112 187 51974 10376 86389 10106 10151 11732 18167 117 34778 58445 10230 27920 35439 16822 10106 10105 20616 10108 10321 35407 11234 106 133 33989 120 135 133 33989 120 135 115 115 115 115 10117 12703 10838 24141 10815 113 122 120 10250 120 12545 114 14261 25658 198 53337 150 10350 11447 30266 10115 117 20977 15879 153 102\nINFO:__main__:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\nINFO:__main__:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","name":"stderr"},{"output_type":"stream","text":"INFO:__main__:label: neg (id = 0)\nINFO:__main__:*** Example ***\nINFO:__main__:guid: 49716\nINFO:__main__:tokens: [CLS] Ok ##ay , let ' s face it . this is a god - aw ##ful movie . The plot ( such as it is ) is ho ##rri ##ble , the acting worse . But the movie was made for one reason and one reason only , like all of those aw ##ful Mario Lan ##za movies . . . just to hear the voice of the star , in this case Pa ##varo ##tti in his prime . Ok ##ay , so may ##be the Lan ##za movies were also an ex ##cus ##e for him to hit on women , but this movie is about hearing Luciano . That alone is worth watching the movie . A big opera star st ##uck on himself faces his fear ##s , finds hu ##mil ##ity and love along the way , and belt ##s out a lot of hit numbers , too . < br / > < br / > I must ad ##mit I ' m prej ##udi ##ced on a number of levels . I ' m Italian . I ' m a big Pa ##varo ##tti fan ( is there anything about Pa ##varo ##tti that isn ' t big , including his fan base ? ) . And when I first saw this movie I was going out on my own , [SEP]\nINFO:__main__:input_ids: 101 84591 13998 117 13595 112 187 13295 10271 119 10531 10124 169 22009 118 56237 14446 18379 119 10117 32473 113 11049 10146 10271 10124 114 10124 13173 24874 11203 117 10105 25086 110353 119 16976 10105 18379 10134 11019 10142 10464 27949 10111 10464 27949 10893 117 11850 10435 10108 12676 56237 14446 14011 19670 10637 39129 119 119 119 12820 10114 62064 10105 21264 10108 10105 16624 117 10106 10531 13474 26907 77640 12683 10106 10226 19287 119 84591 13998 117 10380 11387 11044 10105 19670 10637 39129 10309 10379 10151 11419 14319 10112 10142 10957 10114 14946 10135 13190 117 10473 10531 18379 10124 10978 51191 35491 119 13646 24087 10124 43509 84532 10105 18379 119 138 22185 13335 16624 28780 31746 10135 14764 48343 10226 44929 10107 117 31478 26506 55177 11949 10111 16138 12400 10105 13170 117 10111 62705 10107 10950 169 19826 10108 14946 20953 117 16683 119 133 33989 120 135 133 33989 120 135 146 14982 10840 15772 146 112 181 22907 35772 38039 10135 169 11487 10108 21559 119 146 112 181 11667 119 146 112 181 169 22185 26907 77640 12683 10862 113 10124 11155 42819 10978 26907 77640 12683 10189 98370 112 188 22185 117 11198 10226 10862 11404 136 114 119 12689 10841 146 10422 17112 10531 18379 146 10134 19090 10950 10135 15127 12542 117 102\nINFO:__main__:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\nINFO:__main__:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\nINFO:__main__:label: pos (id = 1)\nINFO:__main__:*** Example ***\nINFO:__main__:guid: 38301\nINFO:__main__:tokens: [CLS] This is one of the most guilty pl ##eas ##ure movies ever ! < br / > < br / > I am em ##barra ##ssed to say that my favorite character is T ##IS ##H , but still enjoy watching her make her space out ##fit \" like super cool \" with a \" like totally bit ##chin \" belt and stick on r ##hine ##stone ##s on her face . < br / > < br / > But any ##ways , the movie is actually one of the few \" family \" movies that holds your interest . I know that the begin ##ing drag ##s , parti ##cua ##rly if you know what is going to happen , but the second half is probably one of the most nerve w ##rack ##ing segments in a family film . < br / > < br / > I would ##n ' t stand up in front of millions of people and pro ##cla ##im to love this movie , in fact rent ##ing it is pretty em ##barra ##ssing itself , but I ' ll ad ##mit it here with the internet to hide behind . < br / > < br / > [SEP]\nINFO:__main__:input_ids: 101 10747 10124 10464 10108 10105 10992 56044 20648 42658 12101 39129 17038 106 133 33989 120 135 133 33989 120 135 146 10392 10266 97607 50987 10114 23763 10189 15127 55768 15092 10124 157 19088 12396 117 10473 12647 84874 84532 10485 13086 10485 16199 10950 48865 107 11850 25212 67420 107 10169 169 107 11850 110240 17684 43849 107 62705 10111 84081 10135 186 76648 23314 10107 10135 10485 13295 119 133 33989 120 135 133 33989 120 135 16976 11178 36869 117 10105 18379 10124 24376 10464 10108 10105 13824 107 11365 107 39129 10189 28278 20442 17644 119 146 21852 10189 10105 16135 10230 71840 10107 117 14869 75545 52347 12277 13028 21852 12976 10124 19090 10114 84630 117 10473 10105 11132 13877 10124 23282 10464 10108 10105 10992 95252 191 71189 10230 58202 10106 169 11365 10458 119 133 33989 120 135 133 33989 120 135 146 10894 10115 112 188 14603 10741 10106 14589 10108 18123 10108 11426 10111 11284 60582 11759 10114 16138 10531 18379 117 10106 18638 60727 10230 10271 10124 108361 10266 97607 63400 17587 117 10473 146 112 22469 10840 15772 10271 19353 10169 10105 18938 10114 96935 17155 119 133 33989 120 135 133 33989 120 135 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\nINFO:__main__:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\nINFO:__main__:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\nINFO:__main__:label: pos (id = 1)\nINFO:__main__:Writing example 10000 of None\nINFO:__main__:Writing example 20000 of None\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"val_features = convert_examples_to_features(df_to_examples_imdb(val_df),\n                                              ['neg','pos'],\n                                              max_seq_length=230,\n                                              tokenizer=tokenizer,\n                                              output_mode=\"classification\")","execution_count":32,"outputs":[{"output_type":"stream","text":"INFO:__main__:Writing example 0 of None\nINFO:__main__:*** Example ***\nINFO:__main__:guid: 49448\nINFO:__main__:tokens: [CLS] As many people know , Mexican cinema was very poor after the so - called Golden Age of the Mexican Cinema , fortuna ##tely , during the late 90 ' s , and early 21st century , great movies like La Ley de Hero ##des , Bajo California , Amore ##s Per ##ros , Y Tu Ma ##m ##Ã ¡ Tam ##bi ##Ã ##© ##n and , of course , El Coronel No Tiene Qui ##en le Es ##cri ##ba , appeared . El Coronel . . . , is a won ##der ##ful movie , that rete ##lls the classic story by Gabriel Ga ##rc ##Ã ##a M ##Ã ¡ r ##quez , by eli ##minat ##ing the magic real ##ism elements , and replacing them with the c ##rude reality lived in Mexico , not only by people like the Colonel , who wait for their pension ##s , but by more than the half of the Mexican population , who live in complete poverty . The film ' s characters , sat ##iri ##cally represent classic characters found in Mexican society , such as the nationalist Colonel , the cold and even amb ##iti ##ous priest , the hy ##po ##crit ##e , but at the same time loyal com ##pad ##re , the tol ##eran ##t and patient wife , the hidden homosexual , [SEP]\nINFO:__main__:input_ids: 101 10882 11299 11426 21852 117 15323 18458 10134 12558 23247 10662 10105 10380 118 11552 14428 17385 10108 10105 15323 23630 117 50516 100102 117 10939 10105 13002 10919 112 187 117 10111 11732 38408 11943 117 14772 39129 11850 10159 34121 10104 26550 10920 117 46083 11621 117 82010 10107 11982 12333 117 162 20108 13744 10147 110905 199 27324 11645 110905 110889 10115 10111 117 10108 15348 117 10224 55152 10657 32005 35921 10136 10141 10912 99590 10537 117 14565 119 10224 55152 119 119 119 117 10124 169 11367 11304 14446 18379 117 10189 32641 22881 10105 36592 13617 10155 15447 69699 46382 110905 10113 150 110905 199 186 48041 117 10155 18166 104130 10230 10105 55909 13486 13397 17464 117 10111 45857 11345 10169 10105 171 97417 26926 17603 10106 10490 117 10472 10893 10155 11426 11850 10105 24782 117 10479 83279 10142 10455 70838 10107 117 10473 10155 10798 11084 10105 13877 10108 10105 15323 11077 117 10479 12962 10106 17876 38491 119 10117 10458 112 187 19174 117 20694 19334 72762 30382 36592 19174 11823 10106 15323 19912 117 11049 10146 10105 99451 24782 117 10105 41626 10111 13246 10559 13903 13499 40981 117 10105 15165 13520 87349 10112 117 10473 10160 10105 11561 10635 84340 10212 29700 10246 117 10105 53183 28415 10123 10111 38607 14384 117 10105 57786 91175 117 102\nINFO:__main__:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\nINFO:__main__:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\nINFO:__main__:label: pos (id = 1)\nINFO:__main__:*** Example ***\nINFO:__main__:guid: 47114\nINFO:__main__:tokens: [CLS] Like another review ##er said , this movie is not a heavy me ##lod ##rama , but it deals with har ##sh real ##ities . A very very play ##ful movie that does not d ##well for a moment . Some very good acting and some won ##der ##ful sm ##iles as well . [SEP]\nINFO:__main__:input_ids: 101 15105 12864 17030 10165 12415 117 10531 18379 10124 10472 169 18296 10911 51861 46582 117 10473 10271 78037 10169 10453 13264 13486 17285 119 138 12558 12558 12253 14446 18379 10189 15107 10472 172 15862 10142 169 14316 119 13885 12558 15198 25086 10111 11152 11367 11304 14446 39709 38278 10146 11206 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\nINFO:__main__:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\nINFO:__main__:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\nINFO:__main__:label: pos (id = 1)\nINFO:__main__:*** Example ***\nINFO:__main__:guid: 41679\nINFO:__main__:tokens: [CLS] I went looking for this movie in typical fan ob ##session . I just wanted to check it out . I was not ex ##pect ##ing much of anything . After all , a musician , an actor and a screenplay writer ? Not possible for so much talent to reside in one person . Right ? ? < br / > < br / > Wrong ! ! Ob ##session aside , it quickly became one of my favorite ##s ! The story line and characters are not lost in the typical hy ##ped up Hollywood special effects . The story plu ##cks at your em ##otions and pull ##s you along . As the credits roll by , you suddenly real ##ize you were g ##lue ##d until the end . < br / > < br / > At times , the acting seems a little over the top . I do , however , believe it ' s done with come ##dic intent and very fit ##ting of the character . Other ##wise , I would ##n ' t have expected the level of acting witness ##ed . < br / > < br / > It ' s worth seeing more than once . I find my ##self lau ##ghing hy ##ster ##ically or gas ##ping une ##xpected ##ly over something I either [SEP]\nINFO:__main__:input_ids: 101 146 13446 34279 10142 10531 18379 10106 36772 10862 17339 88080 119 146 12820 22591 10114 43662 10271 10950 119 146 10134 10472 11419 51511 10230 13172 10108 42819 119 11301 10435 117 169 35041 117 10151 14066 10111 169 93124 17556 136 16040 14128 10142 10380 13172 27411 10114 76481 10106 10464 15042 119 23488 136 136 133 33989 120 135 133 33989 120 135 66256 106 106 43019 88080 95167 117 10271 23590 11179 10464 10108 15127 55768 10107 106 10117 13617 12117 10111 19174 10301 10472 14172 10106 10105 36772 15165 16898 10741 14642 14478 21274 119 10117 13617 13651 18676 10160 20442 10266 94409 10111 80870 10107 13028 12400 119 10882 10105 48357 25520 10155 117 13028 80263 13486 19181 13028 10309 175 75483 10162 11444 10105 11572 119 133 33989 120 135 133 33989 120 135 11699 13465 117 10105 25086 34208 169 16745 10491 10105 12364 119 146 10149 117 13800 117 30587 10271 112 187 20378 10169 10678 55170 58692 10111 12558 21635 12141 10108 10105 15092 119 14490 48339 117 146 10894 10115 112 188 10529 25973 10105 13277 10108 25086 82871 10336 119 133 33989 120 135 133 33989 120 135 10377 112 187 43509 57039 10798 11084 14907 119 146 17860 15127 43310 27207 90427 15165 12765 52917 10345 16091 15398 10231 101239 10454 10491 26133 146 16106 102\nINFO:__main__:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n","name":"stderr"},{"output_type":"stream","text":"INFO:__main__:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\nINFO:__main__:label: pos (id = 1)\nINFO:__main__:*** Example ***\nINFO:__main__:guid: 42675\nINFO:__main__:tokens: [CLS] I give this movie an A + for the she ##er camp of it ! As Dietrich ' s daughter Maria Riva wrote in the book on her mother , \" If one sees The Garden of Allah in the context of high camp , it can be very am ##using . \" And how ! I lau ##ghed with del ##ight at the over ##wr ##ough ##t score and the as ##tou ##nding ##ly , ri ##dic ##ulo ##usly , fant ##astic ##ally me ##lod ##rama ##tic dialogue . View ##ers who ' ve read the accounts of Boyer ' s tou ##pe ##e ( it kept coming uns ##tu ##ck in the heat ) will s ##nick ##er every time it makes an appearance . < br / > < br / > Dietrich and Boyer rarely look at each other when giving their lines - - instead they gaz ##e dream ##ily off into the distance , pre ##sum ##ably so their faces can be photo ##graphe ##d at the best angle and with the most advantage ##ous light ( if you ' re starring in a tur ##key might as well look good ! ) . Dietrich ' s costumes are out of this world . As Riva notes in her book , Dietrich managed to st ##eal Paramount ' s Travis Ban ##ton [SEP]\nINFO:__main__:input_ids: 101 146 18090 10531 18379 10151 138 116 10142 10105 10833 10165 16700 10108 10271 106 10882 33826 112 187 15243 11066 85576 13954 10106 10105 12748 10135 10485 15293 117 107 14535 10464 41369 10117 17590 10108 22734 10106 10105 30798 10108 11846 16700 117 10271 10944 10347 12558 10392 95179 119 107 12689 14796 106 146 27207 75395 10169 10127 27521 10160 10105 10491 21428 73603 10123 17704 10111 10105 10146 21052 61029 10454 117 29956 55170 22540 61289 117 69262 97656 19777 10911 51861 46582 13275 51077 119 30789 10901 10479 112 10323 24944 10105 44546 10108 30259 112 187 52586 11355 10112 113 10271 26546 23959 15826 10991 11263 10106 10105 33955 114 11337 187 71609 10165 14234 10635 10271 20562 10151 19099 119 133 33989 120 135 133 33989 120 135 33826 10111 30259 57879 25157 10160 11948 10684 10841 24426 10455 19515 118 118 17427 10689 34055 10112 51442 32464 11898 10708 10105 18527 117 12229 31417 38565 10380 10455 48343 10944 10347 38171 53118 10162 10160 10105 12504 30891 10111 10169 10105 10992 38119 13499 15765 113 12277 13028 112 11639 27519 10106 169 32461 25975 20970 10146 11206 25157 15198 106 114 119 33826 112 187 58176 10301 10950 10108 10531 11356 119 10882 85576 19899 10106 10485 12748 117 33826 22391 10114 28780 30759 40322 112 187 43346 21631 11183 102\nINFO:__main__:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\nINFO:__main__:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\nINFO:__main__:label: pos (id = 1)\nINFO:__main__:*** Example ***\nINFO:__main__:guid: 29154\nINFO:__main__:tokens: [CLS] When will the hur ##ting stop ? I never want to see another version of a Christmas Carol again . They keep on making movies with the same story , falling over each other in trying to make the movie better then the rest , but sad ##ly fail to do so , as this is not a good story . Moral ##istic , old - fashion ##ed , conservative happy - thinking . As if people learn . The numerous different versions of this film prove that we [UNK] . [SEP]\nINFO:__main__:input_ids: 101 12242 11337 10105 52824 12141 20517 136 146 14794 21528 10114 12888 12864 11674 10108 169 17265 23059 13123 119 11696 23819 10135 14293 39129 10169 10105 11561 13617 117 54756 10491 11948 10684 10106 32862 10114 13086 10105 18379 18322 11059 10105 17333 117 10473 81708 10454 84891 10114 10149 10380 117 10146 10531 10124 10472 169 15198 13617 119 64432 29025 117 12898 118 35055 10336 117 50202 54214 118 56294 119 10882 12277 11426 42671 119 10117 19083 12902 20713 10108 10531 10458 35905 10189 11951 100 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\nINFO:__main__:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\nINFO:__main__:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\nINFO:__main__:label: neg (id = 0)\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def features_to_tensors(list_of_features):\n    all_text_tensor = torch.tensor([f.input_ids for f in list_of_features], dtype=torch.long)\n    all_mask_tensor = torch.tensor([f.input_mask for f in list_of_features], dtype=torch.long)\n    all_segment_tensor = torch.tensor([f.segment_ids for f in list_of_features], dtype=torch.long)\n    all_label_tensor = torch.tensor([f.label_id for f in list_of_features], dtype=torch.long)\n    return all_text_tensor, all_mask_tensor, all_segment_tensor, all_label_tensor","execution_count":33,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from torch.utils.data import TensorDataset,DataLoader","execution_count":34,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_text_tensor, train_mask_tensor, train_segment_tensor, train_label_tensor = features_to_tensors(train_features)\nval_text_tensor, val_mask_tensor, val_segment_tensor, val_label_tensor = features_to_tensors(val_features)","execution_count":35,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset = TensorDataset(train_text_tensor, train_mask_tensor, train_segment_tensor, train_label_tensor)\nval_dataset = TensorDataset(val_text_tensor, val_mask_tensor, val_segment_tensor, val_label_tensor)","execution_count":36,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_label_tensor[:2])","execution_count":37,"outputs":[{"output_type":"stream","text":"tensor([1, 1])\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pytorch_pretrained_bert import BertAdam","execution_count":38,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# bert_model = BertForSequenceClassification.from_pretrained('bert-base-multilingual-cased',num_labels=2).cuda()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!nvidia-smi","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Batch size: 16, 32\n# • Learning rate (Adam): 5e-5, 3e-5, 2e-5\n# • Number of epochs: 3, 4","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Опишем настройки для дообучения, основываясь на секции 3.5 (https://arxiv.org/abs/1810.04805)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from pytorch_pretrained_bert import BertConfig","execution_count":41,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os","execution_count":58,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class BertPersistentWrapper:\n    def __init__(self, prefix, initial_criterion, num_labels):\n        self.prefix = prefix\n        self.model_path = prefix + '_model.bin'\n        self.config_path = prefix + '_config.bin'\n#         self.vocab_path = prefix + '_vocab.bin'\n        self.criterion = initial_criterion\n        self.num_labels = num_labels\n\n    def update(self, model, criterion):\n        self.criterion = criterion\n        model_to_save = model.module if hasattr(model, 'module') else model\n        torch.save(model_to_save.state_dict(), self.model_path)\n        model_to_save.config.to_json_file(self.config_path)\n#         tokenizer.save_vocabulary(output_vocab_file)\n\n#     def load_model_data(self):\n#         return torch.load(self.path)\n\n    def restore(self):\n        config = BertConfig.from_json_file(self.config_path)\n        model = BertForSequenceClassification(config, num_labels=self.num_labels)\n        model.load_state_dict(torch.load(self.model_path))\n        return model\n    \n    def destroy(self):\n        os.remove(self.model_path)\n        os.remove(self.config_path)","execution_count":59,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class SearchState:\n    def __init__(self, best_model, parameter_stats):\n        self.best_model = best_model\n        self.parameter_stats = parameter_stats\n        \nSearchState = namedtuple('SearchState',['best_model', 'parameter_stats'])","execution_count":111,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def bert_wrapper_test():\n    model = BertForSequenceClassification.from_pretrained('bert-base-multilingual-cased',num_labels=2)\n    print(model.num_labels)\n    bw = BertPersistentWrapper('wrapper_test',10, 2)\n    bw.update(model,5)\n    res_model = bw.restore()\n    print(res_model)\n    print(res_model.num_labels)\n    bw.destroy()\n    \nbert_wrapper_test()","execution_count":62,"outputs":[{"output_type":"stream","text":"INFO:pytorch_pretrained_bert.modeling:loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased.tar.gz from cache at /tmp/.pytorch_pretrained_bert/731c19ddf94e294e00ec1ba9a930c69cc2a0fd489b25d3d691373fae4c0986bd.4e367b0d0155d801930846bb6ed98f8a7c23e0ded37888b29caa37009a40c7b9\nINFO:pytorch_pretrained_bert.modeling:extracting archive file /tmp/.pytorch_pretrained_bert/731c19ddf94e294e00ec1ba9a930c69cc2a0fd489b25d3d691373fae4c0986bd.4e367b0d0155d801930846bb6ed98f8a7c23e0ded37888b29caa37009a40c7b9 to temp dir /tmp/tmph5u6v7z8\nINFO:pytorch_pretrained_bert.modeling:Model config {\n  \"attention_probs_dropout_prob\": 0.1,\n  \"directionality\": \"bidi\",\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"max_position_embeddings\": 512,\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pooler_fc_size\": 768,\n  \"pooler_num_attention_heads\": 12,\n  \"pooler_num_fc_layers\": 3,\n  \"pooler_size_per_head\": 128,\n  \"pooler_type\": \"first_token_transform\",\n  \"type_vocab_size\": 2,\n  \"vocab_size\": 119547\n}\n\nINFO:pytorch_pretrained_bert.modeling:Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\nINFO:pytorch_pretrained_bert.modeling:Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n","name":"stderr"},{"output_type":"stream","text":"2\nBertForSequenceClassification(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): BertLayerNorm()\n      (dropout): Dropout(p=0.1)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): BertLayerNorm()\n              (dropout): Dropout(p=0.1)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1)\n          )\n        )\n        (1): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): BertLayerNorm()\n              (dropout): Dropout(p=0.1)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1)\n          )\n        )\n        (2): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): BertLayerNorm()\n              (dropout): Dropout(p=0.1)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1)\n          )\n        )\n        (3): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): BertLayerNorm()\n              (dropout): Dropout(p=0.1)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1)\n          )\n        )\n        (4): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): BertLayerNorm()\n              (dropout): Dropout(p=0.1)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1)\n          )\n        )\n        (5): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): BertLayerNorm()\n              (dropout): Dropout(p=0.1)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1)\n          )\n        )\n        (6): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): BertLayerNorm()\n              (dropout): Dropout(p=0.1)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1)\n          )\n        )\n        (7): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): BertLayerNorm()\n              (dropout): Dropout(p=0.1)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1)\n          )\n        )\n        (8): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): BertLayerNorm()\n              (dropout): Dropout(p=0.1)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1)\n          )\n        )\n        (9): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): BertLayerNorm()\n              (dropout): Dropout(p=0.1)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1)\n          )\n        )\n        (10): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): BertLayerNorm()\n              (dropout): Dropout(p=0.1)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1)\n          )\n        )\n        (11): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): BertLayerNorm()\n              (dropout): Dropout(p=0.1)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (dropout): Dropout(p=0.1)\n  (classifier): Linear(in_features=768, out_features=2, bias=True)\n)\n2\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls","execution_count":61,"outputs":[{"output_type":"stream","text":"__notebook_source__.ipynb  wrapper_test_config.bin  wrapper_test_model.bin\r\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import gc\ngc.collect()","execution_count":66,"outputs":[{"output_type":"execute_result","execution_count":66,"data":{"text/plain":"8"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm import tqdm_notebook","execution_count":65,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_bert(hyper_state, hyper_trial,\n               n_epochs,\n               gradient_accumulation_steps,\n               batch_size,\n               learning_rate,\n               warmup_proportion,\n               train_dataset,\n               val_dataset,\n               num_labels,\n               device):\n    print('Trial', hyper_trial)\n    print('n_epochs = {}, effective_batch_size={}, lr={}, warmup={}'.format(n_epochs,\n                                                                            batch_size * gradient_accumulation_steps,\n                                                                            learning_rate,\n                                                                            warmup_proportion))\n    model = BertForSequenceClassification.from_pretrained('bert-base-multilingual-cased', num_labels=num_labels).cuda()\n    num_train_optimization_steps = n_epochs * int(len(train_dataset) / batch_size / gradient_accumulation_steps)\n\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n\n    optimizer = BertAdam(model.parameters(), lr=learning_rate, warmup=warmup_proportion,\n                         t_total=num_train_optimization_steps)\n\n    best_model = BertPersistentWrapper(f'model{hyper_trial}.md', 0.0, num_labels)\n\n    for epoch in tqdm_notebook(range(n_epochs), desc='Epoch'):\n        model.train()\n        tr_loss = 0.0\n        nb_tr_examples, nb_tr_steps = 0, 0\n        for step, batch in enumerate(tqdm_notebook(train_loader, desc=\"Iteration\")):\n            batch = tuple(t.to(device) for t in batch)\n            input_ids, input_mask, segment_ids, label_ids = batch\n\n            # define a new function to compute loss values for both output_modes\n            logits = model(input_ids, segment_ids, input_mask, labels=None)\n            loss_fct = nn.CrossEntropyLoss()\n            loss = loss_fct(logits.view(-1, num_labels), label_ids.view(-1))\n\n            if gradient_accumulation_steps > 1:\n                loss = loss / gradient_accumulation_steps\n\n            loss.backward()\n            tr_loss += loss.item()\n            nb_tr_examples += input_ids.size(0)\n            nb_tr_steps += 1\n            if (step + 1) % gradient_accumulation_steps == 0:\n                optimizer.step()\n                optimizer.zero_grad()\n\n        tr_loss /= len(train_loader)\n        print('Epoch {}, training_loss={}'.format(epoch, tr_loss))\n\n        model.eval()\n        with torch.no_grad():\n            running_corrects = 0\n            running_total = 0\n\n            running_loss = 0.0\n            for batch in tqdm_notebook(val_loader):\n                batch = tuple(t.to(device) for t in batch)\n                input_ids, input_mask, segment_ids, label_ids = batch\n\n                logits = model(input_ids, segment_ids, input_mask, labels=None)\n                preds = logits.view(-1, num_labels).argmax(dim=1)\n\n                running_total += input_ids.size(0)\n                running_corrects += (preds == label_ids.view(-1)).sum().item()\n\n                loss_fct = nn.CrossEntropyLoss()\n                loss = loss_fct(logits.view(-1, num_labels), label_ids.view(-1))\n                running_loss += loss.item()\n\n        val_loss = running_loss / len(val_loader)\n        val_accuracy = running_corrects / running_total\n        print('Epoch {}, val_loss={}, val_accuracy={}'.format(epoch, val_loss, val_accuracy))\n\n        if val_accuracy > best_model.criterion:\n            best_model.update(model, val_accuracy)\n\n    del model\n    torch.cuda.empty_cache()\n#     print('n_epochs = {}, effective_batch_size={}, lr={}, warmup={}'.format(n_epochs,\n#                                                                             batch_size * gradient_accumulation_steps,\n#                                                                             learning_rate,\n#                                                                             warmup_proportion))\n    param_stats = dict(\n        n_epochs=n_epochs,\n        gradient_batch=batch_size * gradient_accumulation_steps,\n        lr=learning_rate,\n        accuracy=val_accuracy\n    )\n    \n    if not hyper_state:\n        return SearchState(best_model, [param_stats])\n    \n    hyper_state.parameter_stats.append(param_stats)\n    if best_model.criterion > hyper_state.best_model.criterion:\n        hyper_state.best_model.destroy()\n        hyper_state.best_model = best_model\n    else:\n        best_model.destroy()\n    return hyper_state\n                ","execution_count":112,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"settings_for_random_search = {\n    'learning_rate': h_enum(5e-5, 3e-5, 2e-5),\n    'warmup_proportion': 0.1,\n    'gradient_accumulation_steps': h_enum(1,2),\n    'batch_size': 16,\n    'n_epochs': h_enum(3,4),\n    'train_dataset': train_dataset,\n    'val_dataset': val_dataset,\n    'num_labels': 2,\n    'device': torch.device('cuda')\n}","execution_count":113,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result_state = hyper_search(6,settings_for_random_search,train_bert,None,'tqdm_notebook')\n","execution_count":114,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(IntProgress(value=0, description='Trial #', max=4, style=ProgressStyle(description_width='initi…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"15f66a64965544a884e269f6d0d14846"}},"metadata":{}},{"output_type":"stream","text":"Trial 0\nn_epochs = 1, effective_batch_size=16, lr=5e-05, warmup=0.1\n","name":"stdout"},{"output_type":"stream","text":"INFO:pytorch_pretrained_bert.modeling:loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased.tar.gz from cache at /tmp/.pytorch_pretrained_bert/731c19ddf94e294e00ec1ba9a930c69cc2a0fd489b25d3d691373fae4c0986bd.4e367b0d0155d801930846bb6ed98f8a7c23e0ded37888b29caa37009a40c7b9\nINFO:pytorch_pretrained_bert.modeling:extracting archive file /tmp/.pytorch_pretrained_bert/731c19ddf94e294e00ec1ba9a930c69cc2a0fd489b25d3d691373fae4c0986bd.4e367b0d0155d801930846bb6ed98f8a7c23e0ded37888b29caa37009a40c7b9 to temp dir /tmp/tmp2oit5kow\nINFO:pytorch_pretrained_bert.modeling:Model config {\n  \"attention_probs_dropout_prob\": 0.1,\n  \"directionality\": \"bidi\",\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"max_position_embeddings\": 512,\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pooler_fc_size\": 768,\n  \"pooler_num_attention_heads\": 12,\n  \"pooler_num_fc_layers\": 3,\n  \"pooler_size_per_head\": 128,\n  \"pooler_type\": \"first_token_transform\",\n  \"type_vocab_size\": 2,\n  \"vocab_size\": 119547\n}\n\nINFO:pytorch_pretrained_bert.modeling:Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\nINFO:pytorch_pretrained_bert.modeling:Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n","name":"stderr"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(IntProgress(value=0, description='Epoch', max=1, style=ProgressStyle(description_width='initial…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"11fde21b989d4d479ed9d968f61c2a27"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"HBox(children=(IntProgress(value=0, description='Iteration', max=16, style=ProgressStyle(description_width='in…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e89bdc0cf7004dfb9ccc15776f8ed037"}},"metadata":{}},{"output_type":"error","ename":"RuntimeError","evalue":"CUDA out of memory. Tried to allocate 43.12 MiB (GPU 0; 15.90 GiB total capacity; 14.11 GiB already allocated; 17.88 MiB free; 438.52 MiB cached)","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-114-36bf18606fdf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresult_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhyper_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msettings_for_random_search\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_bert\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'tqdm_notebook'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-2-e4a8760e64e0>\u001b[0m in \u001b[0;36mhyper_search\u001b[0;34m(num_trials, parameters, iteration_function, initial_state, progress_bar)\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0miteration_setting\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfixed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0;31m# Вызываем функцию итерации и вызываем\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m         \u001b[0mcurrent_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miteration_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0miteration_setting\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcurrent_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-112-607c58a48682>\u001b[0m in \u001b[0;36mtrain_bert\u001b[0;34m(hyper_state, hyper_trial, n_epochs, gradient_accumulation_steps, batch_size, learning_rate, warmup_proportion, train_dataset, val_dataset, num_labels, device)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0;31m# define a new function to compute loss values for both output_modes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegment_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m             \u001b[0mloss_fct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pytorch_pretrained_bert/modeling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids, attention_mask, labels)\u001b[0m\n\u001b[1;32m    987\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 989\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpooled_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_all_encoded_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    990\u001b[0m         \u001b[0mpooled_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpooled_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpooled_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pytorch_pretrained_bert/modeling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids, attention_mask, output_all_encoded_layers)\u001b[0m\n\u001b[1;32m    731\u001b[0m         encoded_layers = self.encoder(embedding_output,\n\u001b[1;32m    732\u001b[0m                                       \u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 733\u001b[0;31m                                       output_all_encoded_layers=output_all_encoded_layers)\n\u001b[0m\u001b[1;32m    734\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoded_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m         \u001b[0mpooled_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpooler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pytorch_pretrained_bert/modeling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, output_all_encoded_layers)\u001b[0m\n\u001b[1;32m    404\u001b[0m         \u001b[0mall_encoder_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer_module\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0moutput_all_encoded_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m                 \u001b[0mall_encoder_layers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pytorch_pretrained_bert/modeling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask)\u001b[0m\n\u001b[1;32m    390\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 392\u001b[0;31m         \u001b[0mintermediate_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    393\u001b[0m         \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintermediate_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlayer_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pytorch_pretrained_bert/modeling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    363\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 365\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate_act_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    366\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pytorch_pretrained_bert/modeling.py\u001b[0m in \u001b[0;36mgelu\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0mAlso\u001b[0m \u001b[0msee\u001b[0m \u001b[0mhttps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0marxiv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morg\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m1606.08415\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m     \"\"\"\n\u001b[0;32m--> 124\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 43.12 MiB (GPU 0; 15.90 GiB total capacity; 14.11 GiB already allocated; 17.88 MiB free; 438.52 MiB cached)"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame.from_records(result_state.parameter_stats)","execution_count":121,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'result_state' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-121-3776837d3427>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameter_stats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'result_state' is not defined"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"bert_model = result_state.best_model.restore()","execution_count":76,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"torch.cuda.empty_cache()","execution_count":119,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bert_model = bert_model.cuda()","execution_count":85,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import gc\ngc.collect()","execution_count":117,"outputs":[{"output_type":"execute_result","execution_count":117,"data":{"text/plain":"2617"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict_loader(bert_model ,loader, device='cuda'):\n    bert_model.eval()\n    predictions = []\n    correct_predictions = []\n    with torch.no_grad():\n        for batch in tqdm_notebook(loader):\n            batch = tuple(t.to(device) for t in batch)\n            input_ids, input_mask, segment_ids, label_ids = batch\n            logits = bert_model(input_ids, segment_ids, input_mask, labels=None)\n            predictions.extend(logits.argmax(dim=1).tolist())\n            correct_predictions.extend(label_ids.tolist())\n#             break\n    return predictions, correct_predictions","execution_count":91,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred, y_test = predict_loader(bert_model, DataLoader(val_dataset, batch_size=16))","execution_count":92,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(IntProgress(value=0, max=79), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"897277eac8944036857d15e331d87eb2"}},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics","execution_count":93,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(metrics.classification_report(y_test, y_pred))","execution_count":94,"outputs":[{"output_type":"stream","text":"              precision    recall  f1-score   support\n\n           0       0.87      0.88      0.88       625\n           1       0.88      0.87      0.88       625\n\n   micro avg       0.88      0.88      0.88      1250\n   macro avg       0.88      0.88      0.88      1250\nweighted avg       0.88      0.88      0.88      1250\n\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(metrics.accuracy_score(y_test, y_pred))","execution_count":95,"outputs":[{"output_type":"stream","text":"0.876\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_features = convert_examples_to_features(df_to_examples_imdb(test_df),\n                                              ['neg','pos'],\n                                              max_seq_length=230,\n                                              tokenizer=tokenizer,\n                                              output_mode=\"classification\")","execution_count":96,"outputs":[{"output_type":"stream","text":"INFO:__main__:Writing example 0 of None\nINFO:__main__:*** Example ***\nINFO:__main__:guid: 0\nINFO:__main__:tokens: [CLS] Once again Mr . Cost ##ner has drag ##ged out a movie for far longer than necessary . As ##ide from the ter ##rif ##ic sea rescue sequences , of which there are very few I just did not care about any of the characters . Most of us have ghost ##s in the close ##t , and Cost ##ner ' s character are realized early on , and then for ##gott ##en until much later , by which time I did not care . The character we should really care about is a very co ##cky , over ##con ##fi ##dent Ashton Ku ##tch ##er . The problem is he comes off as ki ##d who think ##s he ' s better than anyone else around him and shows no signs of a c ##lut ##tere ##d close ##t . His only ob ##sta ##cle appears to be winning over Cost ##ner . Finally when we are well past the half way point of this st ##ink ##er , Cost ##ner tells us all about Ku ##tch ##er ' s ghost ##s . We are told why Ku ##tch ##er is driven to be the best with no prior in ##kling or for ##esh ##adow ##ing . No magic here , it was all I could do to keep from turning it off an hour in . [SEP]\nINFO:__main__:input_ids: 101 23075 13123 12916 119 102455 11129 10393 71840 18832 10950 169 18379 10142 13301 20165 11084 27039 119 10882 13315 10188 10105 12718 52070 11130 14931 48022 56549 117 10108 10319 11155 10301 12558 13824 146 12820 12172 10472 11131 10978 11178 10108 10105 19174 119 14361 10108 19626 10529 100766 10107 10106 10105 16065 10123 117 10111 102455 11129 112 187 15092 10301 74160 11732 10135 117 10111 11059 10142 99185 10136 11444 13172 10873 117 10155 10319 10635 146 12172 10472 11131 119 10117 15092 11951 14819 30181 11131 10978 10124 169 12558 11170 30742 117 10491 23486 14403 21029 70912 49869 38732 10165 119 10117 18077 10124 10261 21405 11898 10146 10879 10162 10479 27874 10107 10261 112 187 18322 11084 51747 40843 12166 10957 10111 15573 10192 45033 10108 169 171 40846 45417 10162 16065 10123 119 11597 10893 17339 10972 19478 20296 10114 10347 16542 10491 102455 11129 119 51857 10841 11951 10301 11206 17781 10105 13877 13170 12331 10108 10531 28780 39717 10165 117 102455 11129 27024 19626 10435 10978 49869 38732 10165 112 187 100766 10107 119 12865 10301 21937 31237 49869 38732 10165 10124 39803 10114 10347 10105 12504 10169 10192 20972 10106 56886 10345 10142 38806 96580 10230 119 10657 55909 19353 117 10271 10134 10435 146 12174 10149 10114 23819 10188 48448 10271 11898 10151 24730 10106 119 102\nINFO:__main__:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\nINFO:__main__:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\nINFO:__main__:label: neg (id = 0)\nINFO:__main__:*** Example ***\nINFO:__main__:guid: 1\nINFO:__main__:tokens: [CLS] This is an example of why the majority of action films are the same . Gene ##ric and bor ##ing , there ' s really nothing worth watching here . A complete waste of the then bare ##ly - tap ##ped talents of Ice - T and Ice Cube , who ' ve each proven many times over that they are capable of acting , and acting well . Don ' t both ##er with this one , go see New Jack City , Rico ##chet or watch New York Under ##cover for Ice - T , or Boyz n the Hood , Higher Learning or Friday for Ice Cube and see the real deal . Ice - T ' s ho ##rri ##bly c ##liche ##d dialogue alone makes this film gra ##te at the teeth , and I ' m still won ##dering what the he ##ck Bill Pax ##ton was doing in this film ? And why the he ##ck does he always play the exact same character ? From Alien ##s on ##ward , every film I ' ve seen with Bill Pax ##ton has him playing the exact same ir ##rita ##ting character , and at least in Alien ##s his character died , which made it somewhat gra ##tif ##ying . . . < br / > < br / > Overall [SEP]\nINFO:__main__:input_ids: 101 10747 10124 10151 14351 10108 31237 10105 19471 10108 14204 14280 10301 10105 11561 119 22787 18570 10111 27728 10230 117 11155 112 187 30181 33338 43509 84532 19353 119 138 17876 59158 10108 10105 11059 21766 10454 118 66956 16898 77869 10108 22694 118 157 10111 22694 80825 117 10479 112 10323 11948 101330 11299 13465 10491 10189 10689 10301 29298 10108 25086 117 10111 25086 11206 119 11740 112 188 11408 10165 10169 10531 10464 117 11783 12888 10287 12342 10773 117 21012 42298 10345 34481 10287 10482 12594 51652 10142 22694 118 157 117 10345 105858 182 10105 33058 117 35471 36639 10345 30767 10142 22694 80825 10111 12888 10105 13486 19918 119 22694 118 157 112 187 13173 24874 31748 171 12337 10162 51077 24087 20562 10531 10458 63706 10216 10160 10105 75839 117 10111 146 112 181 12647 11367 54406 12976 10105 10261 11263 13160 77984 11183 10134 30918 10106 10531 10458 136 12689 31237 10105 10261 11263 15107 10261 19540 12253 10105 45809 11561 15092 136 12222 49333 10107 10135 16988 117 14234 10458 146 112 10323 15652 10169 13160 77984 11183 10393 10957 14879 10105 45809 11561 10478 27821 12141 15092 117 10111 10160 16298 10106 49333 10107 10226 15092 12482 117 10319 11019 10271 43203 63706 23631 40018 119 119 119 133 33989 120 135 133 33989 120 135 58877 102\nINFO:__main__:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\nINFO:__main__:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\nINFO:__main__:label: neg (id = 0)\nINFO:__main__:*** Example ***\nINFO:__main__:guid: 2\nINFO:__main__:tokens: [CLS] First of all I hat ##e those mor ##onic rapper ##s , who could ' nt act if they had a gun presse ##d against their for ##eh ##ead ##s . All they do is curs ##e and shoot each other and acting like c ##lich ##Ã ##© ' e version of gang ##sters . < br / > < br / > The movie doesn ' t take more than five minutes to explain what is going on before we ' re already at the ware ##house There is not a single sy ##mpa ##thetic character in this movie , except for the home ##less gu ##y , who is also the only one with half a brain . < br / > < br / > Bill Pax ##ton and William Sad ##ler are both hill bill ##ies and Sad ##lers character is just as much a villa ##in as the gang ##sters . I did ' nt like him right from the start . < br / > < br / > The movie is filled with point ##less violence and Walter Hills special ##ty : people falling through windows with glass flying every ##where . There is pretty much no plot and it is a big problem when you root for no - one . Everybody dies , except from Pax ##ton and the [SEP]\nINFO:__main__:input_ids: 101 12128 10108 10435 146 11250 10112 12676 24984 56177 39742 10107 117 10479 12174 112 86459 19833 12277 10689 10374 169 23103 33834 10162 11327 10455 10142 25723 30297 10107 119 11101 10689 10149 10124 57887 10112 10111 53839 11948 10684 10111 25086 11850 171 11666 110905 110889 112 173 11674 10108 16330 47035 119 133 33989 120 135 133 33989 120 135 10117 18379 47798 112 188 13574 10798 11084 12403 15304 10114 67004 12976 10124 19090 10135 11360 11951 112 11639 19034 10160 10105 88902 15562 11723 10124 10472 169 11376 12261 31285 64899 15092 10106 10531 18379 117 23423 10142 10105 11816 14985 75980 10157 117 10479 10124 10379 10105 10893 10464 10169 13877 169 34467 119 133 33989 120 135 133 33989 120 135 13160 77984 11183 10111 10694 48691 10815 10301 11408 41473 34497 11624 10111 48691 36090 15092 10124 12820 10146 13172 169 19863 10245 10146 10105 16330 47035 119 146 12172 112 86459 11850 10957 13448 10188 10105 15148 119 133 33989 120 135 133 33989 120 135 10117 18379 10124 39287 10169 12331 14985 26342 10111 12506 18888 14478 11195 131 11426 54756 11222 40115 10169 32362 34676 14234 30935 119 11723 10124 108361 13172 10192 32473 10111 10271 10124 169 22185 18077 10841 13028 47887 10142 10192 118 10464 119 56965 15229 117 23423 10188 77984 11183 10111 10105 102\n","name":"stderr"},{"output_type":"stream","text":"INFO:__main__:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\nINFO:__main__:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\nINFO:__main__:label: neg (id = 0)\nINFO:__main__:*** Example ***\nINFO:__main__:guid: 3\nINFO:__main__:tokens: [CLS] Not even the Beatles could write songs everyone like ##d , and although Walter Hill is no mo ##p - top he ' s second to none when it comes to thought pro ##vo ##king action movies . The nine ##ties came and social platforms were changing in music and film , the emerge ##nce of the Rap ##per turned movie star was in full swing , the acting took a back seat to each man ' s over ##power ##ing regional accent and transparent acting . This was one of the many ice - t movies i saw as a ki ##d and loved , only to watch them later and c ##ringe . Bill Pax ##ton and William Sad ##ler are fire ##men with basic lives until a burning building tenant about to go up in fl ##ames hands over a map with gold implications . I hand it to Walter for quickly and ne ##at ##ly setting up the main characters and location . But i fault everyone involved for turning out Lam ##e - o performances . Ice - t and cu ##be must have been red hot at this time , and while I ' ve enjoyed both their careers as rapper ##s , in my opinion they fell flat in this movie . It ' s about nine ##ty minutes of one [SEP]\nINFO:__main__:input_ids: 101 16040 13246 10105 21301 12174 28685 15457 48628 11850 10162 117 10111 14779 12506 12289 10124 10192 46912 10410 118 12364 10261 112 187 11132 10114 46638 10841 10271 21405 10114 18957 11284 11244 15629 14204 39129 119 10117 19964 14197 13383 10111 12142 51325 10309 43068 10106 11839 10111 10458 117 10105 99467 12150 10108 10105 47957 12713 21031 18379 16624 10134 10106 13375 74772 117 10105 25086 12149 169 12014 17687 10114 11948 10817 112 187 10491 65211 10230 16454 59649 10111 107078 25086 119 10747 10134 10464 10108 10105 11299 24642 118 188 39129 177 17112 10146 169 10879 10162 10111 82321 117 10893 10114 34481 11345 10873 10111 171 98557 119 13160 77984 11183 10111 10694 48691 10815 10301 13559 11418 10169 25090 21418 11444 169 78514 12585 70039 10978 10114 11783 10741 10106 58768 55497 27925 10491 169 14876 10169 18128 86309 119 146 15230 10271 10114 12506 10142 23590 10111 10554 10526 10454 29421 10741 10105 12126 19174 10111 18214 119 16976 177 110640 48628 16247 10142 48448 10950 44068 10112 118 183 22744 119 22694 118 188 10111 10854 11044 14982 10529 10590 10680 29698 10160 10531 10635 117 10111 11371 146 112 10323 52072 11408 10455 110196 10146 39742 10107 117 10106 15127 32282 10689 25194 31307 10106 10531 18379 119 10377 112 187 10978 19964 11195 15304 10108 10464 102\nINFO:__main__:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\nINFO:__main__:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\nINFO:__main__:label: neg (id = 0)\nINFO:__main__:*** Example ***\nINFO:__main__:guid: 4\nINFO:__main__:tokens: [CLS] Brass pictures ( movies is not a fit ##ting word for them ) really are somewhat brass ##y . Their all ##uring visual quali ##ties are re ##mini ##sce ##nt of expensive high class TV commercial ##s . But un ##fort ##una ##tely Brass pictures are feature films with the pret ##ense of want ##ing to enter ##tain viewers for over two hours ! In this they fail mise ##rab ##ly , their unde ##nia ##ble , but rather soft and fl ##abb ##y than steam ##y , er ##otic quali ##ties non with ##stand ##ing . < br / > < br / > Sens ##o ' 45 is a remake of a film by Luc ##hino Visconti with the same title and Ali ##da Vall ##i and Far ##ley Granger in the lead . The original tells a story of sense ##less love and lu ##st in and around Venice during the Italian wars of independence . Brass moved the action from the 19th into the 20th century , 1945 to be exact , so there are Mussolini mural ##s , men in black shirt ##s , German uniform ##s or the tatt ##ered gar ##b of the partisans . But it is just window dress ##ing , the historic context is completely negli ##gi ##ble . < br / > < br / > Anna [SEP]\nINFO:__main__:input_ids: 101 79134 54156 113 39129 10124 10472 169 21635 12141 12307 10142 11345 114 30181 10301 43203 109555 10157 119 17551 10435 31653 24559 15510 14197 10301 11639 37249 24176 10368 10108 58069 11846 13596 10813 17331 10107 119 16976 10119 23044 14212 100102 79134 54156 10301 19072 14280 10169 10105 49775 23643 10108 21528 10230 10114 31006 37879 52717 10142 10491 10551 19573 106 10167 10531 10689 84891 15858 55645 10454 117 10455 23650 11335 11203 117 10473 16863 44898 10111 58768 39211 10157 11084 45833 10157 117 10163 45079 15510 14197 10446 10169 15418 10230 119 133 33989 120 135 133 33989 120 135 105607 10133 112 10827 10124 169 48702 10108 169 10458 10155 26589 107485 54936 10169 10105 11561 12887 10111 13518 10229 73740 10116 10111 26747 12105 108923 10106 10105 14107 119 10117 11364 27024 169 13617 10108 15495 14985 16138 10111 14657 10562 10106 10111 12166 39208 10939 10105 11667 68756 10108 31412 119 79134 13059 10105 14204 10188 10105 19794 10708 10105 18604 11943 117 10670 10114 10347 45809 117 10380 11155 10301 39019 92494 10107 117 10588 10106 15045 81050 10107 117 12026 51135 10107 10345 10105 34791 45452 47243 10457 10108 10105 71997 119 16976 10271 10124 12820 39051 67348 10230 117 10105 23704 30798 10124 27185 15866 11210 11203 119 133 33989 120 135 133 33989 120 135 12300 102\nINFO:__main__:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\nINFO:__main__:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\nINFO:__main__:label: neg (id = 0)\nINFO:__main__:Writing example 10000 of None\nINFO:__main__:Writing example 20000 of None\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_text_tensor, test_mask_tensor, test_segment_tensor, test_label_tensor = features_to_tensors(test_features)","execution_count":97,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_dataset = TensorDataset(test_text_tensor, test_mask_tensor, test_segment_tensor, test_label_tensor)\n","execution_count":98,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_loader = DataLoader(test_dataset,batch_size=16,shuffle=False)\ny_pred, y_test = predict_loader(bert_model, test_loader)\nprint(metrics.classification_report(y_test, y_pred))\nprint(metrics.accuracy_score(y_test, y_pred))","execution_count":100,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(IntProgress(value=0, max=1563), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"200d7434df414069aa73266db1ce325d"}},"metadata":{}},{"output_type":"stream","text":"              precision    recall  f1-score   support\n\n           0       0.89      0.87      0.88     12500\n           1       0.88      0.89      0.88     12500\n\n   micro avg       0.88      0.88      0.88     25000\n   macro avg       0.88      0.88      0.88     25000\nweighted avg       0.88      0.88      0.88     25000\n\n0.88076\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}