{"cells":[{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true},"cell_type":"markdown","source":"### **In this notebook I will try to do an exploratory data analysis and clustering for the travel review ratings dataset, found in the UCI machine learning repository.**"},{"metadata":{},"cell_type":"markdown","source":"<h1 id=\"Exploratory_Data_Analysis\">\n1. Exploratory Data Analysis\n<a class=\"anchor-link\" href=\"https://www.kaggle.com/johnmantios/travel-review-ratings-dataset/edit/run/47115230#Exploratory_Data_Analysis\">Â¶</a>\n</h1>"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nimport os\n\n            \ndata = pd.read_csv('../input/travel-review-ratings/google_review_ratings.csv') \n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's take a general first idea of our data with info()"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will rename the columns for ease of understanding"},{"metadata":{"trusted":false},"cell_type":"code","source":"column_names = ['user_id', 'churches', 'resorts', 'beaches',\n                'parks', 'theatres', 'museums', 'malls', 'zoo',\n                'restaurants', 'pubs_bars', 'local_services',\n                'burger_pizza_shops', 'hotels_other_lodgings',\n                'juice_bars', 'art_galleries', 'dance_clubs',\n                 'swimming_pools', 'gyms', 'bakeries', 'beauty_spas',\n                'cafes', 'view_points', 'monuments', 'gardens', 'Unnamed: 25']\n\ndata.columns = column_names","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's check how many null values we have"},{"metadata":{"trusted":false},"cell_type":"code","source":"data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seems that we will drop the entire unnamed 25th column and we will impute the 2 rows with zeros supposing that the user did not give any rating to these categories (the 2 null values in category 12 and 24).\nAlso we will drop the 'User' row as it is of no use to us.Finally, we will map the local_services column from object to float"},{"metadata":{"trusted":false},"cell_type":"code","source":"data.drop('Unnamed: 25', axis = 1, inplace = True)\n\ndata.drop('user_id', axis = 1, inplace = True)\n\ndata = data.fillna(0)\n\ndata['local_services'].astype('float')\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So, we have one string in our local_services column.Let's impute it with the column's input_data['local_services'][input_data['local_services']mean"},{"metadata":{"trusted":false},"cell_type":"code","source":"local_services_mean = data['local_services'][data['local_services'] != '2\\t2.']\n\ndata['local_services'][data['local_services'] == '2\\t2.'] = np.mean(local_services_mean.astype('float'))\n\ndata['local_services'] = data['local_services'].astype('float')\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Great.All of our data is type 'float'. Let's check some of the descriptive statistics"},{"metadata":{"trusted":false},"cell_type":"code","source":"pd.set_option('display.max_columns', 30)\ndata.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's visualize our first plot: we will examine the number of reviews under each category"},{"metadata":{"trusted":false},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport warnings\n%matplotlib inline\nimport plotly.express as px\n\n# Plotting pretty figures and avoid blurry images\n%config InlineBackend.figure_format = 'retina'\n# Larger scale for plots in notebooks\n\n\n# Ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Enable multiple cell outputs\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = 'all'\n\n\ncolumn_names = ['churches', 'resorts', 'beaches', 'parks', 'theatres', 'museums', 'malls', 'zoo',\n                'restaurants', 'pubs_bars', 'local_services',\n                'burger_pizza_shops', 'hotels_other_lodgings', 'juice_bars', \n                'art_galleries', 'dance_clubs', 'swimming_pools',\n                'gyms', 'bakeries', 'beauty_spas', 'cafes', 'view_points', 'monuments', 'gardens']\n\n\n\ncounts = data[column_names[:]].astype(bool).sum(axis=0).sort_values()\n\ntest = []\nfor i in range(len(counts.index)):\n    test.append(counts.index[i])\n    \n\n\n\nfig = px.bar(counts, \n             x=counts, \n             y=test,\n             color=counts,\n                             labels={\n                     \"total ratings\": \"this is x\",\n                     \"categories\": \"this is y)\"\n                 },\n                height = 800,\n                title=\"Number of reviews under each category\")\n\nfig.show()\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can observe, bakeries, gyms and beauty spas are the venues where users show the least amount of interest in rating them.\nLet's see how many reviews were given for each category. \n"},{"metadata":{"trusted":false},"cell_type":"code","source":"reviews = data[column_names[:]].astype(bool).sum(axis=1).value_counts()\n\n\nfig = px.bar(reviews, \n             x=reviews.index, \n             y=reviews.values,\n             color=reviews.values,\n                height = 800,\n                title=\"Number of categories VS Number of reviews\")\n\nfig.show()\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can easily understand that a total of 3725 users have given a review for all 24 categories and only 6 gave a review for 15 of all of them. \nLet's check now the average rating for each category"},{"metadata":{"trusted":false},"cell_type":"code","source":"avg_rating = data[column_names[:]].mean().sort_values()\n\n\n\nfig = px.bar(avg_rating,\n            x = avg_rating.index,\n            y = avg_rating.values,\n            color = avg_rating.values,\n            height = 800,\n            title = \"Average rating for each category\")\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We clearly see that gyms are users' least favorite venue with an average rating of 0.82 and on the other hand, malls are the leaders  when it comes to ratings with an average score of 3.35."},{"metadata":{},"cell_type":"markdown","source":"Let's analyze the outliers"},{"metadata":{"trusted":false},"cell_type":"code","source":"\nfig = px.box(data, y = ['churches', 'resorts', 'beaches', 'parks', 'theatres', 'museums', 'malls', 'zoo',\n                'restaurants', 'pubs_bars', 'local_services',\n                'burger_pizza_shops', 'hotels_other_lodgings', 'juice_bars', \n                'art_galleries', 'dance_clubs', 'swimming_pools',\n                'gyms', 'bakeries', 'beauty_spas', 'cafes', 'view_points', 'monuments', 'gardens'] )\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df = pd.DataFrame(data)\n\nQ1 = df.quantile(0.25)\nQ3 = df.quantile(0.75)\nIQR = Q3 - Q1\n\npd.set_option('display.max_info_rows', 30)\n\n((df < (Q1 - 1.5 * IQR)) | (df > (Q3 + 1.5 * IQR))).sum()\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Outliers are expected for large sample sizes. Since the reviews in google do not occur by human mistake , we should not discard them. \n\nOutlier handling is even more difficult in unsupervised learning, since we are both trying to learn what the clusters are, and what data points correspond to \"no\" clusters."},{"metadata":{"trusted":false},"cell_type":"code","source":"import seaborn as sns\n\nplt.figure(figsize=(15,10))\ncor = data.corr() #Calculate the correlation of the above variables\nsns.heatmap(cor, square = True) #Plot the correlation as heat map","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"data = data.drop_duplicates()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"data.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Not clear at all"},{"metadata":{},"cell_type":"markdown","source":"It would help us significantly if we would basket the various categories into higher levels, both in terms of analysis and clustering"},{"metadata":{"trusted":false},"cell_type":"code","source":"entertainment = ['theatres', 'dance_clubs', 'malls']\nfood_travel = ['restaurants', 'pubs_bars', 'burger_pizza_shops', 'juice_bars', 'bakeries', 'cafes']\nplaces_of_stay = ['hotels_other_lodgings', 'resorts']\nhistorical = ['churches', 'museums', 'art_galleries', 'monuments']\nnature = ['beaches', 'parks', 'zoo', 'view_points', 'gardens']\nservices = ['local_services', 'swimming_pools', 'gyms', 'beauty_spas']","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_categories = pd.DataFrame(columns = ['entertainment', 'food_travel', 'places_of_stay', 'historical', 'nature', 'services'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_categories['entertainment'] = data[entertainment].mean(axis=1)\ndf_categories['food_travel'] = data[food_travel].mean(axis = 1)\ndf_categories['places_of_stay'] = data[places_of_stay].mean(axis = 1)\ndf_categories['historical'] = data[historical].mean(axis = 1)\ndf_categories['nature'] = data[nature].mean(axis = 1)\ndf_categories['services'] = data[services].mean(axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_categories.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"\nfig = px.box(df_categories, y = ['entertainment', 'food_travel', 'places_of_stay', 'historical', 'nature', 'services'] )\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1 id=\"Clustering\">\n2. Clustering\n<a class=\"anchor-link\" href=\"https://www.kaggle.com/johnmantios/travel-review-ratings-dataset/edit/run/47115230#Clustering\">Â¶</a>\n</h1>"},{"metadata":{},"cell_type":"markdown","source":"Let's assess the clusterability of the dataset using the hopkins statistic. According to the pyclustertend library, on a scale from 0 to 1, the lower the score, the better the clusterability of the dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install pyclustertend\n\nfrom pyclustertend import hopkins \n\nhopkins(df_categories, df_categories.shape[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1 id=\"agglomerative\">\n2.1 Agglomerative Clustering\n<a class=\"anchor-link\" href=\"https://www.kaggle.com/johnmantios/travel-review-ratings-dataset/edit/run/47115230#agglomerative\">Â¶</a>\n</h1>"},{"metadata":{},"cell_type":"markdown","source":"We begin the clustering process with the agglomerative clustering algorithm for one simple reason: it is a hierarchical clustering algorithm, so we simplify the problem of having to choose beforehand the number of clusters in our model.Hierarchical clustering does not avoid the problem with choosing the number of clusters. Simply - it constructs the tree spaning over all samples, which shows which samples (later on - clusters) merge together to create a bigger cluster. This happens recursively till you have just two clusters (this is why the default number of clusters is 2) which are merged to the whole dataset."},{"metadata":{},"cell_type":"markdown","source":"Firstly, we are going to determine which linkage method to use. In order to do that we will calculate the cophenet index. Cophenet index is a measure of the correlation between the distance of points in feature space and distance on the dendrogram. If the distance between these points increases as the dendrogram distance between the clusters does then the Cophenet index is closer to 1. So, values closer to 1 mean a better linkage method."},{"metadata":{"scrolled":false,"trusted":false},"cell_type":"code","source":"from scipy.cluster.hierarchy import dendrogram\nfrom scipy.cluster import hierarchy\nfrom sklearn.cluster import AgglomerativeClustering\nfrom scipy.spatial.distance import pdist\nimport plotly.graph_objects as go\n\nZ = hierarchy.linkage(df_categories, 'ward')\n\nc, coph_dists = hierarchy.cophenet(Z, pdist(df_categories, 'hamming'))\n\nward = c\n\n\nA = hierarchy.linkage(df_categories,'average')\n\nc, coph_dists = hierarchy.cophenet(A, pdist(df_categories, 'hamming'))\n\naverage = c\n\n\nB = hierarchy.linkage(df_categories,'single')\n\nc, coph_dists = hierarchy.cophenet(B, pdist(df_categories, 'hamming'))\n\nsingle = c\n\n\nC = hierarchy.linkage(df_categories,'complete')\n\nc, coph_dists = hierarchy.cophenet(C, pdist(df_categories, 'hamming'))\n\ncomplete = c\n\n\nD = hierarchy.linkage(df_categories,'weighted')\n\nc, coph_dists = hierarchy.cophenet(D, pdist(df_categories, 'hamming'))\n\nweighted = c\n\n\nE = hierarchy.linkage(df_categories,'centroid')\n\nc, coph_dists = hierarchy.cophenet(E, pdist(df_categories, 'hamming'))\n\ncentroid = c\n\nF = hierarchy.linkage(df_categories,'median')\n\nc, coph_dists = hierarchy.cophenet(F, pdist(df_categories, 'hamming'))\n\nmedian = c\n\n\nmetrics=['ward', 'average', 'single', 'complete', 'weighted', 'centroid', 'median']\n\n\nfig = go.Figure([go.Bar(x=metrics, y=[ward, average, single, complete, weighted, centroid, median])])\nfig.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We clearly see that the average linkage method is the preferred one."},{"metadata":{},"cell_type":"markdown","source":"Let's calculate some useful metrics that will help us decide the number of clusters. Since the ground truth labels are not known we will use such metrics like the silhouette coefficient, the Davies-Bouldin score and the Calinski-Harabasz Index."},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.metrics import silhouette_score\nfor n_clusters in range(2,10):\n    clusterer = AgglomerativeClustering (n_clusters=n_clusters, distance_threshold = None)\n    preds = clusterer.fit_predict(df_categories)\n    \n\n    score = silhouette_score (df_categories, preds)\n    print (\"For n_clusters = {}, silhouette score is {})\".format(n_clusters, score))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the various silhouette score we can see that although 2 clusters would be a better choice for our data, the score itself is pretty low"},{"metadata":{},"cell_type":"markdown","source":"Let's check the Davies-Bouldin score.We want a values as close to 0 as possible"},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.metrics import davies_bouldin_score\n\nfor n_clusters in range(2,10):\n    clusterer = AgglomerativeClustering (n_clusters=n_clusters, distance_threshold = None)\n    preds = clusterer.fit_predict(df_categories)\n    \n    score = davies_bouldin_score (df_categories , preds)\n    print (\"For n_clusters = {}, the Davies-Bouldin score is {})\".format(n_clusters, score))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Again, 6 clusters have the lowest score from a reasonable range of clusters"},{"metadata":{},"cell_type":"markdown","source":"Calinski-Harabasz index.We want as high a score as possible"},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.metrics import calinski_harabasz_score\n\nfor n_clusters in range(2,10):\n    clusterer = AgglomerativeClustering (n_clusters=n_clusters, distance_threshold = None)\n    preds = clusterer.fit_predict(df_categories)\n    \n    score = calinski_harabasz_score(df_categories, preds)\n    print (\"For n_clusters = {}, the Calinski-Harabasz score is {})\".format(n_clusters, score))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here the metric shows that 2 clusters would be better than 3."},{"metadata":{"trusted":false},"cell_type":"code","source":"model = AgglomerativeClustering(distance_threshold=0, n_clusters = None)\nmodel = model.fit(df_categories)\n\nZ = hierarchy.linkage(model.children_, 'average')\n\nplt.figure(figsize=(20,10))\n\ndn = hierarchy.dendrogram(Z)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"By looking at the above dendrogram, we observe 3 distinct colors in the dendrogram, but this will not determine how many clusters are formed. Following the main criteria of cutting the dendrogram appropriately, we discover that there are basically 5 clusters. Observing the height of each dendrogram division we decided to go with 4000 where the line would be drawn.\n\nNow, let's plot our data using the labels that the algorithm generated.We are going to make a scatterplot."},{"metadata":{"trusted":false},"cell_type":"code","source":"df = pd.DataFrame(data)\n\nmodel = AgglomerativeClustering(distance_threshold=None, n_clusters = 3)\nmodel = model.fit(df_categories)\ny_agg=model.fit_predict(df_categories)\n\ndf_agg = df_categories.copy()\ndf_agg[\"AggLabels\"] = y_agg\n\n#uncomment the following line and let your machine explode!\nsns.pairplot( df_agg, hue=\"AggLabels\")  \n\ndf_agg[\"AggLabels\"].value_counts(0)\n\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1 id=\"DBSCAN\">\n2.2 DBSCAN\n<a class=\"anchor-link\" href=\"https://www.kaggle.com/johnmantios/travel-review-ratings-dataset/edit/run/47115230#DBSCAN\">Â¶</a>\n</h1>"},{"metadata":{},"cell_type":"markdown","source":"Following, we apply to our data set the DBSCAN algorithm.DBSCAN works by running a connected components algorithm across the different core points. If two core points share border points, or a core point is a border point in another core pointâs neighborhood, then theyâre part of the same connected component, which forms a cluster.\nA low min_samples parameter means it will build more clusters from noise, so we shouldn't choose it too small.\nThe DBSCAN paper suggests to choose minPts based on the dimensionality, and eps based on the elbow in the k-distance graph.\nFor eps, we can try to do a knn distance histogram and choose a \"knee\" there, but there might be no visible one, or multiple.\n\nIn the more recent publication\n\nSchubert, E., Sander, J., Ester, M., Kriegel, H. P., & Xu, X. (2017).\nDBSCAN Revisited, Revisited: Why and How You Should (Still) Use DBSCAN.\nACM Transactions on Database Systems (TODS), 42(3), 19.\n\n\nthe authors suggest to use a larger minpts for large and noisy data sets, and to adjust epsilon depending on whether you get too large clusters (decrease epsilon) or too much noise (increase epsilon). Clustering requires iterations.\n\nAfter some trial and error, the min_samples value with the less noise is 43"},{"metadata":{},"cell_type":"markdown","source":"Let's apply the Knee method using KNN to find the optimal eps value for our model. Since KNN is a supervised learning algorithm and our data is not labeled, we will apply a general rule of thumb popularized by the \"Pattern Classification\" book by Duda et al., saying that the optimal K value usually found is the square root of N, where N is the total number of samples."},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.neighbors import NearestNeighbors\n\nnearest_neighbors = NearestNeighbors(n_neighbors=73) #sqrt(5456) = 73\nnearest_neighbors.fit(df_categories)\ndistances, indices = nearest_neighbors.kneighbors(df_categories)\ndistances = np.sort(distances, axis=0)[:, 1]\n#print(distances)\nplt.figure(figsize=(20,10))\nplt.plot(distances)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Optimal value for eps where a 'knee' is formed is 0.6."},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.cluster import DBSCAN\n\nmodel2 = DBSCAN(eps = 0.6, min_samples = 43)\n\nmodel2 = model2.fit(df_categories)\n\nnp.unique(model2.labels_)\n\ny_db=model2.labels_\n\ndf_db = df_categories.copy()\ndf_db[\"DBLabels\"] = y_db\n\ndf_db[\"DBLabels\"].value_counts(0)\n\n\nsns.pairplot( df_db, hue=\"DBLabels\")  \n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_db","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's remove the noise"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Get names of indexes for which column DB_Labels has value -1\nindexNames = df_db[ df_db['DBLabels'] == -1 ].index\n# Delete these row indexes from dataFrame\ndf_db.drop(indexNames , inplace=True)\n\ndf_db['DBLabels'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_db","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sns.pairplot( df_db, hue=\"DBLabels\")  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"y_db = df_db['DBLabels']\nsilhouette_score (df_db, y_db)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1 id=\"EM\">\n2.3 EM using GMM\n<a class=\"anchor-link\" href=\"https://www.kaggle.com/johnmantios/travel-review-ratings-dataset/edit/run/47115230#EM\">Â¶</a>\n</h1>"},{"metadata":{},"cell_type":"markdown","source":"At its simplest, GMM is also a type of clustering algorithm. As its name implies, each cluster is modelled according to a different Gaussian distribution. This flexible and probabilistic approach to modelling the data means that rather than having hard assignments into clusters like k-means, we have soft assignments. This means that each data point could have been generated by any of the distributions with a corresponding probability."},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.mixture import GaussianMixture as GMM\n\nn_components = np.arange(1, 21)\nmodels = [GMM(n, covariance_type='full', random_state=0).fit(df_categories)\n          for n in n_components]\n\nplt.plot(n_components, [m.bic(df_categories) for m in models], label='BIC')\nplt.plot(n_components, [m.aic(df_categories) for m in models], label='AIC')\nplt.legend(loc='best')\nplt.xlabel('n_components');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The choice of number of components measures how well GMM works as a density estimator, not how well it works as a clustering algorithm. From the above plot it shows that optimal number of components is 13 (where the gradient stops decreasing)"},{"metadata":{"trusted":false},"cell_type":"code","source":"from matplotlib.patches import Ellipse\n\ndf_gmm = df_categories.copy()\n\ndf_gmm['gmm'] = GMM(n_components=6, random_state=42).fit_predict(df_categories)\n\nsns.pairplot( df_gmm, hue=\"gmm\")  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1 id=\"kmeans\">\n2.4 KMeans\n<a class=\"anchor-link\" href=\"https://www.kaggle.com/johnmantios/travel-review-ratings-dataset/edit/run/47115230#kmeans\">Â¶</a>\n</h1>"},{"metadata":{},"cell_type":"markdown","source":"Kmeans algorithm is an iterative algorithm that tries to partition the dataset into K pre-defined distinct non-overlapping subgroups (clusters) where each data point belongs to only one group\nIt tries to make the intra-cluster data points as similar as possible while also keeping the clusters as different (far) as possible. It assigns data points to a cluster such that the sum of the squared distance between the data points and the clusterâs centroid (arithmetic mean of all the data points that belong to that cluster) is at the minimum. The less variation we have within clusters, the more homogeneous (similar) the data points are within the same cluster."},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.cluster import KMeans\n\nfor n_clusters in range(2,10):\n    clusterer = KMeans(n_clusters = n_clusters)\n    preds = clusterer.fit_predict(df_categories)\n    \n\n    score = silhouette_score (df_categories, preds)\n    print (\"For n_clusters = {}, silhouette score is {})\".format(n_clusters, score))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from scipy.spatial.distance import cdist\n\ndistortions = [] \ninertias = [] \nmapping1 = {} \nmapping2 = {} \nK = range(1,10) \n  \nfor k in K: \n    #Building and fitting the model \n    kmeanModel = KMeans(n_clusters=k).fit(df_categories) \n    kmeanModel.fit(df_categories)     \n      \n    distortions.append(sum(np.min(cdist(df_categories, kmeanModel.cluster_centers_, \n                      'euclidean'),axis=1)) / df_categories.shape[0]) \n    inertias.append(kmeanModel.inertia_) \n  \n\n    mapping2[k] = kmeanModel.inertia_ \n    \nfor key,val in mapping2.items(): \n    print(str(key)+' : '+str(val))\n    \nplt.plot(K, inertias, 'bx-') \nplt.xlabel('Values of K') \nplt.ylabel('Inertia') \nplt.title('The Elbow Method using Inertia') \nplt.show()     ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We'll go with 2 clusters."},{"metadata":{"trusted":true},"cell_type":"code","source":"model4 = KMeans(n_clusters = 2, n_init = 40)\n\nmodel4 = model4.fit(df_categories)\n\ndf_kmeans = df_categories.copy()\n\ndf_kmeans['kmeans'] = model4.fit_predict(df_categories)\n\nsns.pairplot( df_kmeans, hue = 'kmeans')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}