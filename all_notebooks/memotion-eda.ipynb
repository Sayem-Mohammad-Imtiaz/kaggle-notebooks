{"cells":[{"metadata":{},"cell_type":"markdown","source":"# In this notebook we explore the dataset and try to find out what the data can tell us"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nnp.random.seed(42)\nimport os\nimport pickle\nimport collections\nfrom PIL import Image\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport re\nfrom tqdm import notebook","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Reading the files"},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"IMAGES_DIR = '/kaggle/input/memotion-dataset-7k/memotion_dataset_7k/images'\nimage_filenames = os.listdir(IMAGES_DIR)\nfile_extentions = [filename.split('.')[-1] for filename in image_filenames]\n\nimages_paths = [os.path.join(IMAGES_DIR,filename) for filename in image_filenames]\n\nREF_FILE = '/kaggle/input/memotion-dataset-7k/memotion_dataset_7k/reference_df_pickle'\nLABELS_FILE = '/kaggle/input/memotion-dataset-7k/memotion_dataset_7k/labels_pd_pickle'\n\nwith open(REF_FILE, 'rb') as handle:\n    reference_df_ = pickle.load(handle)\n\nwith open(LABELS_FILE, 'rb') as handle:\n    labels_pd_ = pickle.load(handle)\n    \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## First off, inspecting the file extentions, we can see that the memes are mostly in _jpg_ format.<br> But there are some other formats as well"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nimage_formats = collections.Counter(file_extentions)\nprint(f'Num Images: {len(images_paths)}')\n\nprint('Image formats found: ', image_formats)\nimage_formats_df = pd.DataFrame.from_dict(image_formats, orient='index').reset_index()\nimage_formats_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Lets open up the given columns in the given pickle"},{"metadata":{"trusted":true},"cell_type":"code","source":"labels_pd_.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_train_val_split(train_frac, df, id_col):\n    \"\"\"\n    Splits dataframe into train and val keeping percentage of\n    labels same in both splits.\n    Args:\n        train_frac: Fraction of samples to use for train\n        df: pd.DataFrame to split\n        id_col: Column that uniquely identifies every row.\n    Returns:\n        split_df\n    \"\"\"\n    val_frac = 1 - train_frac\n    assert val_frac + train_frac == 1\n    labels = set(df.label)\n    split_df = None\n    df = df.sample(frac=1) #shuffle df\n\n    for lbl in notebook.tqdm(labels, total = len(labels)):\n        lbl_df = df[df.label == lbl].copy()\n        temp_df_train = lbl_df.sample(frac=train_frac).copy()\n        temp_df_val = lbl_df[~lbl_df[id_col].isin(temp_df_train[id_col])].copy()\n        temp_df_train['split'] = 'train'\n        temp_df_val['split'] = 'val'\n        if not isinstance(split_df,pd.DataFrame):\n            split_df = temp_df_train.copy()\n            split_df = pd.concat([split_df, temp_df_val])\n        else:\n            split_df = pd.concat([split_df, temp_df_train, temp_df_val])\n    \n    assert len(split_df) == len(df)\n    return split_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Now lets inspect the amount of samples given for each task\n"},{"metadata":{},"cell_type":"markdown","source":"## Task A : Sentiment Classification\n### Definition: : Given an Internet meme, the first task is to classify it as a positive, negative or neutral meme.\n\n- Negative and Very Negative => -1\n- Positive and Very Positive => 1\n- Neutral => 0"},{"metadata":{},"cell_type":"markdown","source":"### Sample Count of Task A"},{"metadata":{"trusted":true},"cell_type":"code","source":"#  Negative and Very Negative => -1\n# Positive and Very Positive => 1\n# Neutral => 0\n\ntask_a_labels = {\n    'negative': -1 ,\n    'very_negative': -1,\n    'neutral' : 0,\n    'positive' : 1,\n    'very_positive': 1,\n}\n\ntask_a_labels_df = labels_pd_[['image_name','overall_sentiment']].copy()\ntask_a_labels_df['label'] = task_a_labels_df['overall_sentiment'].map(task_a_labels)\ntask_a_labels_df.label.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"task_a_split_df = get_train_val_split(\n    train_frac = 0.90,\n    df = task_a_labels_df,\n    id_col= 'image_name',\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Task B: Humor Classification\n### Definition : Given an Internet meme, the system has to identify the type of humor expressed. The categories are sarcastic, humorous, and offensive meme. If a meme does not fall under any of these categories, then it is marked as another meme. A meme can have more than one category.\n\nLabel Mapping:\n- Not humorous => 0 and Humorous (funny, very funny, hilarious) => 1\n- Not Sarcastic => 0 and Sarcastic (general, twisted meaning, very twisted) => 1\n- Not offensive => 0 and Offensive (slight, very offensive, hateful offensive) => 1\n- Not Motivational => 0 and Motivational => 1"},{"metadata":{"trusted":true},"cell_type":"code","source":"print( f' Humor labels: {set(labels_pd_[\"humour\"])}')\nprint( f' Sarcasm labels: {set(labels_pd_[\"sarcasm\"])}')\nprint( f' Offensive labels: {set(labels_pd_[\"offensive\"])}')\nprint( f' Motivational labels: {set(labels_pd_[\"motivational\"])}')\n\n\n\nhumour_labels_dict = {'funny':1, 'hilarious':1, 'not_funny':0, 'very_funny':1}\nsarcasm_labels_dict = {'general':1, 'twisted_meaning':1, 'not_sarcastic':0, 'very_twisted':1}\nmotivational_labels_dict = { 'motivational':1, 'not_motivational':0 }\noffensive_labels_dict = { 'hateful_offensive':1, 'slight':1, 'not_offensive':0, 'very_offensive':1}\n\ntask_b_labels_df = labels_pd_.copy()\n\ntask_b_labels_df['humour'] = labels_pd_['humour'].map(humour_labels_dict)\ntask_b_labels_df['sarcasm'] = labels_pd_['sarcasm'].map(sarcasm_labels_dict)\ntask_b_labels_df['offensive'] = labels_pd_['offensive'].map(offensive_labels_dict)\ntask_b_labels_df['motivational'] = labels_pd_['motivational'].map(motivational_labels_dict)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Sample Count of Task B"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(task_b_labels_df.humour.value_counts(),'\\n')\nprint(task_b_labels_df.sarcasm.value_counts(),'\\n')\nprint(task_b_labels_df.offensive.value_counts(),'\\n')\nprint(task_b_labels_df.motivational.value_counts(),'\\n')\n\nprint('Total:\\n',\n     pd.concat(\n        [\n            task_b_labels_df['humour'],\n            task_b_labels_df['sarcasm'],\n            task_b_labels_df['offensive'],\n            task_b_labels_df['motivational'],\n        ],\n        ignore_index= True,\n        axis = 0,\n    ).value_counts()      \n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Analysing The Images"},{"metadata":{},"cell_type":"markdown","source":"### From exploring the image files we can see that the some of the images are somewhat corrupted."},{"metadata":{"trusted":true},"cell_type":"code","source":"image_sizes = [Image.open(filepath).size for filepath in images_paths]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### We can also see that there is a lot of variation in image sizes. There is no standard format"},{"metadata":{"trusted":true},"cell_type":"code","source":"image_widths = [size_[0] for size_ in image_sizes]\nimage_heights = [size_[1] for size_ in image_sizes]\nimage_size_df = pd.DataFrame(data = {'Width':image_widths, 'Height':image_heights })","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Sampling the image sizes"},{"metadata":{"trusted":true},"cell_type":"code","source":"image_size_df.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"image_size_df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = go.Figure()\n\n\nfig_1 = go.Histogram(x=image_size_df['Height'], nbinsx= 100, name='Height') #\nfig_2 = go.Histogram(x=image_size_df['Width'], nbinsx=100, name = 'Width')\n\n\nfig.add_trace(fig_1)\nfig.add_trace(fig_2)\n\nfig.show(interactive = False)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Analyse of the texts\nAlmost every meme has a corresponding OCR extracted text. We will discard the null."},{"metadata":{"trusted":true},"cell_type":"code","source":"labels_pd_.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### We set up a basic text cleaner."},{"metadata":{"trusted":true},"cell_type":"code","source":"class TextCleaner:\n    \"\"\"Basic Text cleaner that removes excess whitespaces and URLs\"\"\"\n    \n#     url_re = r\"\\b(?:https?://|www\\.)[a-z0-9-]+(\\.[a-z0-9-]+)+(?:[/?].*)?\"\n    \n    url_re_1 = r\"\\b(?:https?://|www\\.)[a-z0-9-]+(\\.[a-z0-9-]+)+(?:[/?].*)?\" #removes most urls\n    url_re_2 = r\"(w{3}\\.)*[a-zA-Z0-9]+\\.{1}(co){1}[m]{0,1}\\s{0,1}\" # removes any.com urls\n    url_re_3 = r\"(w{3}\\.)*[a-zA-Z0-9]+\\.{1}(net){1}\\s{0,1}\" # removes any.net urls\n    \n    def clean(self, text):\n        text = str(text)\n        excess_whitespace_removed = ' '.join(text.split())\n        s1 = re.sub(self.url_re_1, \"\", excess_whitespace_removed)\n        s2 = re.sub(self.url_re_2, \"\", s1)\n        s3 = re.sub(self.url_re_3, \"\", s2)\n        \n        return s3\n\n\ntext_cleaner = TextCleaner() \ns = \"Je veux que: https://site.english.com/this/is/a/url/path/component#fragment quickmeme.net meme.co asy.com 9gag.com\"\nprint(f\" Text: {s}\\n Cleaned Text:  {text_cleaner.clean(s)}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text_df = labels_pd_[['image_name','text_corrected']].copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#check if df contains any columns with null values\ntext_df.columns[text_df.isna().any()].tolist()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are some images with no corresponding texts. We will discard them from the analysis."},{"metadata":{"trusted":true},"cell_type":"code","source":"#images with no text\nnulls_samples = text_df[pd.isnull(text_df).any(axis=1)]\nnulls_samples","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#lets drop the null values\ntext_df.dropna(subset=['text_corrected'],inplace=True)\n\n#reset index\ntext_df.index = pd.RangeIndex(len(text_df.index))\n# text_df[pd.isnull(text_df).any(axis=1)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets run the text cleaner"},{"metadata":{"trusted":true},"cell_type":"code","source":"# text_len_df = text_df.copy()\n\ntext_df.loc[:,'char_len'] = text_df.text_corrected\\\n                                .map(text_cleaner.clean)\\\n                                .str.len()\n\ntext_df.loc[:,'word_len'] = text_df.text_corrected\\\n                                .map(text_cleaner.clean)\\\n                                .map(lambda x: [str(word) for word in str(x).split()])\\\n                                .map(len)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels_pd_.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Lets find some stats the text both at char level and word level"},{"metadata":{"trusted":true},"cell_type":"code","source":"labels_df = labels_pd_.copy()\nlabels_df.drop(['text_ocr', 'text_corrected'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"char_df = text_df.sort_values(['char_len'],ascending=True)\nchar_df = pd.merge(char_df, labels_df, how='inner', on=['image_name'])\n\nword_df = text_df.sort_values(['word_len'],ascending=True)\nword_df = pd.merge(word_df, labels_df, how='inner', on=['image_name'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nchar_df.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Taking a look at the longest and shortest text samples."},{"metadata":{},"cell_type":"markdown","source":"#### Char-wise"},{"metadata":{"trusted":true},"cell_type":"code","source":"char_df.head(5)[['text_corrected','char_len','overall_sentiment']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"char_df.tail(5)[['text_corrected','char_len','overall_sentiment']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Word Wise"},{"metadata":{"trusted":true},"cell_type":"code","source":"word_df.head(5)[['text_corrected','word_len','overall_sentiment']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_df.tail(5)[['text_corrected','word_len','overall_sentiment']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Stats on text lenghts reaffirm that memes don't really have any standard format. They can have any length of words/chars."},{"metadata":{},"cell_type":"markdown","source":"### Character Lengths"},{"metadata":{"trusted":true},"cell_type":"code","source":"text_df['char_len'].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Word Lenghts"},{"metadata":{"trusted":true},"cell_type":"code","source":"text_df['word_len'].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets plot them and see"},{"metadata":{"trusted":true},"cell_type":"code","source":"_fig_text = go.Figure()\n\nchar_len_fig = go.Histogram(x=text_df['char_len'], name=\"Num chars\", nbinsx=100)\nword_len_fig = go.Histogram(x=text_df['word_len'], name=\"Num words\", nbinsx=100)\n\n\n_fig_text.add_trace(char_len_fig, )\n_fig_text.add_trace(word_len_fig, )\n\n\n\n_fig_text.show(interactive=False)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Lets see if we can detect the language of the texts. Although most of them should be in English. \n\nFor this we use  [CLD3](https://github.com/google/cld3/). We will only accept the inferenced langauge if the reliability is atleast 50%, other wise we will mark it as unknown."},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"!python -m pip install -U pycld3 langcodes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import cld3\nimport langcodes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def detect_language(text:str) -> str:\n    \"\"\"Detects the language of the string. \n    Returns 'unknown' if the probability is < 0.5 and is not reliable\n    \"\"\"\n    lang, probability, is_reliable, _ = cld3.get_language(text)\n    if probability >= 0.5 and is_reliable:\n        return lang\n    else:\n#         print(lang,probability ,is_reliable)\n        return 'unknown'\n\ndef detect_languages(text:str, num:int = 3) :\n    \"\"\"Detects the language of the string. \n    Returns 'unknown' if the probability is < 0.5 and is not reliable\n    \"\"\"\n    langs = []\n    for lng in cld3.get_frequent_languages(\n        text,\n        num_langs=3\n    ):  \n        lang, probability, is_reliable, _ = lng\n        if probability >= 0.5 and is_reliable:\n            langs.append(lang)\n    \n    return tuple(langs)\n    \ndef get_language_name(lang:str) -> str:\n    \"\"\"Converts language code to language name\"\"\"\n    return langcodes.Language.get(lang).language_name('en')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text_df.loc[:,'cld3_preds'] = text_df.text_corrected\\\n                                .map( text_cleaner.clean )\\\n                                .map( detect_language )\\\n                                .map( get_language_name )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### We can see that it detected 61 different languages. Most are in english, some are even unknown. \nCould lack of proper grammar and manipulation of the spelling in an attempt of be funny be the reason? \nFor example, 'doge' instead of 'dog' ?\nAlso some text samples are just too short to detect the language."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Languages detected: \", set(text_df['cld3_preds']))\nprint(\"Num Languages detected: \", len(set(text_df['cld3_preds'])))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text_df['cld3_preds'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with pd.option_context('display.max_colwidth', -1): \n    print(\n        text_df[~text_df.cld3_preds.isin(['English','unknown'])][['text_corrected','cld3_preds']].sample(5)\n    )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Can we find out what the memes are about? Lets try LSA to discover some topics"},{"metadata":{"trusted":true},"cell_type":"code","source":"#import modules\nimport os.path\nfrom gensim import corpora\nfrom gensim.models import LsiModel\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom gensim.models.coherencemodel import CoherenceModel\n# import matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class LSAHelpers:\n    # initialize regex tokenizer\n    tokenizer = RegexpTokenizer(r'\\w+')\n    \n    text_cleaner = TextCleaner()\n    # create English stop words list\n    en_stop = set(stopwords.words('english'))\n    # Create p_stemmer of class PorterStemmer\n    p_stemmer = PorterStemmer()\n    # list for tokenized documents in loop\n    \n    \n    \n    def preprocess_data(self,document:str):\n        \"\"\"\n        Input  : docuemnt list\n        Purpose: preprocess text (tokenize, removing stopwords, and stemming)\n        Output : preprocessed text\n        \"\"\"\n        \n        document = text_cleaner.clean(document)\n        \n        raw = document.lower()\n        \n        tokens = self.tokenizer.tokenize(raw)\n        # remove stop words from tokens\n        stopped_tokens = [i for i in tokens if not i in self.en_stop]\n        # stem tokens\n        stemmed_tokens = [self.p_stemmer.stem(i) for i in stopped_tokens]\n        \n        return stemmed_tokens\n\n    def _prepare_corpus(self, doc_clean):\n        \"\"\"\n        Input  : clean documents\n        Purpose: create term dictionary of our courpus and Converting list of documents (corpus) into Document Term Matrix\n        Output : term dictionary and Document Term Matrix\n        \"\"\"\n        # Creating the term dictionary of our courpus, where every unique term is assigned an index. dictionary = corpora.Dictionary(doc_clean)\n        self.dictionary = corpora.Dictionary(doc_clean)\n        # Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n        self.doc_term_matrix = [self.dictionary.doc2bow(doc) for doc in doc_clean]\n        # generate LDA model\n    \n    def create_gensim_lsa_model(self,doc_clean,number_of_topics):\n        \"\"\"\n        Input  : clean document, number of topics and number of words associated with each topic\n        Purpose: create LSA model using gensim\n        Output : return LSA model\n        \"\"\"\n        self._prepare_corpus(doc_clean)\n        # generate LSA model\n        lsamodel = LsiModel(self.doc_term_matrix, num_topics=number_of_topics, id2word = self.dictionary)  # train model\n#         print(lsamodel.print_topics(num_topics=number_of_topics, num_words=words))\n        self.lsamodel = lsamodel\n    \n    def compute_coherence_values(self,doc_clean, stop, start = 2, step = 3):\n        \"\"\"\n        Input   : dictionary : Gensim dictionary\n                  corpus : Gensim corpus\n                  texts : List of input texts\n                  stop : Max num of topics\n        purpose : Compute c_v coherence for various number of topics\n        Output  : model_list : List of LSA topic models\n                  coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n        \"\"\"\n        coherence_values = []\n        model_list = []\n        for num_topics in notebook.tqdm(range(start, stop, step)):\n            # generate LSA model\n            model = LsiModel(self.doc_term_matrix, num_topics=num_topics, id2word = self.dictionary)  # train model\n            model_list.append(model)\n            coherencemodel = CoherenceModel(model=model, texts=doc_clean, dictionary=self.dictionary, coherence='c_v')\n            coherence_values.append(coherencemodel.get_coherence())\n        self.model_list = model_list\n        self.coherence_values = coherence_values\n    \n    def plot_coherence(self, doc_clean,start, stop, step):\n\n        model_list, coherence_values = self.compute_coherence_values(doc_clean , stop, start, step)\n        \n        fig = go.Figure(data=go.Scatter(x=range(start, stop, step), y=self.coherence_values))\n        fig.show()\n                                                                \n        # Show graph\n        x = range(start, stop, step)\n        plt.plot(x, coherence_values)\n        plt.xlabel(\"Number of Topics\")\n        plt.ylabel(\"Coherence score\")\n        plt.legend((\"coherence_values\"), loc='best')\n        plt.show()\n    \n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lsa_helper = LSAHelpers()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clean_text = text_df.text_corrected.map(lsa_helper.preprocess_data)\nlsa_helper._prepare_corpus(clean_text)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"with pd.option_context('display.max_colwidth', -1): \n    #print whatever\n    print(clean_text.sample(10))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"lsa_helper.compute_coherence_values(clean_text , stop=20, start=1, step=1) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_coherence = max(lsa_helper.coherence_values)\nnum_topics = lsa_helper.coherence_values.index(best_coherence) + 1\nprint(f'Best Coherence {best_coherence} with {num_topics} Topics')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"coherence_fig = go.Figure(\n    data=go.Scatter(x=list(range(1, 35, 1)), y=lsa_helper.coherence_values),\n)\n\ncoherence_fig.update_layout(\n    title=\"LSA on Text\",\n    xaxis_title=\"Coherence Value\",\n    yaxis_title=\"Number of topics\",\n    font=dict(\n        family=\"Courier New, monospace\",\n        size=18,\n        color=\"#7f7f7f\"\n    )\n)\ncoherence_fig.show(interactive=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Lets see what the memes are based on according to LSA"},{"metadata":{"trusted":true},"cell_type":"code","source":"lsa_helper.model_list[num_topics-1].print_topics()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusion\nWe did some exploratory data analysis on the memotion dataset. We looked at the sample counts for Task A and B. We found out stats on the images. We can see that moajority of the images are in the 'jpg' format, but there are other types in the mix as well. The images have various heights and widths and there is large variance in the same. The same can be said for texts. The texts can range from a single word to a large number of words. Both is which tell was that there are no standard format for memes. We also tried to find out language of the memes. We saw that there are a large number of unknowns, even thought the dataset is probably compiled with memes that are in the english langauge. We also attempted LSA to see if we can find out what the memes are about. "}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}