{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Setting up environment","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import warnings\nfrom sklearn.exceptions import ConvergenceWarning\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nwarnings.filterwarnings(\"ignore\", category=ConvergenceWarning)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load data","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n# Load dataset\norig_dataset = pd.read_csv(\"../input/hotel-booking-demand/hotel_bookings.csv\");\norig_dataset.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Split data\nSplit data into train and test dataset. <br> \nSelect and explore only train dataset to avoid training bias from test set.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt \n    \n# Explore missing data in dataset\nprint(orig_dataset.isnull().sum())\n\n# Drop duplicate rows\norig_dataset.drop_duplicates(inplace= True)\n\n# Convert data type\norig_dataset['children'] = orig_dataset['children'].astype('Int64')\n\n# Handling null value\norig_dataset.drop(['company', 'agent'], axis=1, inplace= True)   # Drop features with lots of missing value out\norig_dataset['country'].fillna(method='ffill', inplace=True)  # Impute missing data with LOCF method\norig_dataset = orig_dataset[orig_dataset['children'].notna()]   # Drop missing data rows\n\nprint(orig_dataset.isnull().sum())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Select features and label\nfeatures = orig_dataset.drop(['is_canceled'], axis=1)\nlabel = orig_dataset['is_canceled']\n\n# Split train test data\nX_train, X_test, y_train, y_test = train_test_split(features, label, test_size=0.2, random_state= 3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"explore_data = pd.concat([X_train, y_train], axis=1).copy()\n\nexplore_data.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## EDA - Exploratory Data Analysis\nInvestigate the hidden pattern, information in data","metadata":{}},{"cell_type":"markdown","source":"### 1. Time series analysis\nThe below graphs show the amount of lodging detail in each time of year.","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Create dict for sorting purpose\nmonth_dict = {'January': 0, 'February': 1, 'March': 2, 'April': 3, 'May': 4, 'June': 5, 'July': 6, 'August': 7, 'September': 8, 'October': 9, 'November': 10, 'December': 11} \n\n_, axs = plt.subplots(4, 1, figsize = (15,15))\n\n# Amount of lodging in each month separated by years\nloding_trend = pd.crosstab(explore_data['arrival_date_month'], explore_data['arrival_date_year'])\nloding_trend = loding_trend.sort_values(by=['arrival_date_month'], key=lambda x: x.map(month_dict))\nsns.lineplot(data=loding_trend, ax=axs[0])\n\n# Amount of hotel reserved in each month\nhotel_type = pd.crosstab(explore_data['arrival_date_month'], explore_data['hotel'])\nhotel_type = hotel_type.sort_values(by=['arrival_date_month'], key=lambda x: x.map(month_dict))\nsns.lineplot(data= hotel_type, ax=axs[1])\n\n# Amount of customer type lodge the hotel in each month\ncustomer_type_month = pd.crosstab(explore_data['arrival_date_month'], explore_data['customer_type'])\ncustomer_type_month = customer_type_month.sort_values(by=['arrival_date_month'], key=lambda x: x.map(month_dict))\nsns.lineplot(data= customer_type_month, ax=axs[2])\n\n# Amount of customer country lodge the hotel in each month\ntop10_country = explore_data['country'].value_counts().head(10).index\ncountry_month = explore_data.loc[explore_data['country'].isin(top10_country), ['arrival_date_month', 'country']]\ncountry_month = pd.crosstab(country_month['arrival_date_month'], country_month['country'])\ncountry_month = country_month.sort_values(by=['arrival_date_month'], key=lambda x: x.map(month_dict))\nsns.lineplot(data= country_month, ax=axs[3])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Observations\nExploring time series is exposed the useful information as listed below\n* Around third quarter of each year will have the most of lodging transactions and will start decrease in the fourth quarter. <br>\n* City and resort hotel have similar demand growth along the year. <br>\n* Transient type has significantly larger demand than the others along the year <br>\n* BEL is the country that most lodging the hotel.","metadata":{}},{"cell_type":"markdown","source":"### 2. Bivariate analysis\nThis section is trying to find the hidden pattern or relation between interested features and a target feature. <br>\nMoreover, trying to prove the assumptions that came in to my mind.","metadata":{}},{"cell_type":"markdown","source":"**2.1 Cancellation due to days in waiting list**","metadata":{}},{"cell_type":"code","source":"# Find ratio of cancellation due to days in waiting list\n\n# Separate into three groups\ncancel_because_waiting1  = explore_data.loc[(explore_data['days_in_waiting_list'] >= 0) & (explore_data['days_in_waiting_list'] < 100), ['days_in_waiting_list', 'is_canceled'] ]\ncancel_because_waiting2  = explore_data.loc[(explore_data['days_in_waiting_list'] >= 100) & (explore_data['days_in_waiting_list'] < 200), ['days_in_waiting_list', 'is_canceled'] ]\ncancel_because_waiting3  = explore_data.loc[(explore_data['days_in_waiting_list'] >= 200) , ['days_in_waiting_list', 'is_canceled'] ]\n\n# Calculate in percentage\ncancel_because_waiting1 = cancel_because_waiting1['is_canceled'].value_counts(normalize=True) * 100\ncancel_because_waiting2 = cancel_because_waiting2['is_canceled'].value_counts(normalize=True) * 100\ncancel_because_waiting3 = cancel_because_waiting3['is_canceled'].value_counts(normalize=True) * 100\n\n# Plot\n_, axs = plt.subplots(1, 3, figsize=(12, 18))\naxs[0].pie(cancel_because_waiting1, labels = ['Not canceled', 'Canceled'], autopct='%1.1f%%', startangle=90)\naxs[1].pie(cancel_because_waiting2, labels = ['Not canceled', 'Canceled'], autopct='%1.1f%%', startangle=90)\naxs[2].pie(cancel_because_waiting3, labels = ['Not canceled', 'Canceled'], autopct='%1.1f%%', startangle=90)\n\naxs[0].title.set_text('Less than 100 days')\naxs[1].title.set_text('Between 100 and 200 days')\naxs[2].title.set_text('More than 200 days')\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Observations\nInvestigating the cancellation due to days in waiting list. <br>\nI assumed that longer time the customers waiting in the list will make them likely decide to cancel the lodging because of dissatifaction of service. <br>\nI have try below steps to visualize data:\n1. Split the time which customers use to wait in the list into three groups. Less than 100 days, more than 100 days but less than 200 days and more than 200 days.\n2. Calculate the percentage amount of cancel or not cancel for each group.\n3. Visualize each group in pie charts. <br>\n\nThe apparent result shows that the longer time used is not significantly affect the cancellation decision.\n","metadata":{}},{"cell_type":"markdown","source":"**2.2 Cancellation due to prvious cancellation**","metadata":{}},{"cell_type":"code","source":"# Find ratio of cancellation due to previous cancellation\n\n# Separate into three segments\nprevious_cancel1 = explore_data.loc[ explore_data['previous_cancellations'] < 9, ['previous_cancellations', 'is_canceled']]\nprevious_cancel2 = explore_data.loc[ (explore_data['previous_cancellations'] >= 9) & (explore_data['previous_cancellations'] < 18), ['previous_cancellations', 'is_canceled']]\nprevious_cancel3 = explore_data.loc[ explore_data['previous_cancellations'] >= 18, ['previous_cancellations', 'is_canceled']]\n\n# Calculate in percentage \nprevious_cancel1 = previous_cancel1['is_canceled'].value_counts(normalize=True) * 100\nprevious_cancel2 = previous_cancel2['is_canceled'].value_counts(normalize=True) * 100\nprevious_cancel3 = previous_cancel3['is_canceled'].value_counts(normalize=True) * 100\n\n# Simulate\n_, axs = plt.subplots(1, 3, figsize=(12, 18))\naxs[0].pie(previous_cancel1, labels = ['Not canceled', 'Canceled'], autopct='%1.1f%%', startangle=90, colors=['forestgreen', 'firebrick'])\naxs[1].pie(previous_cancel2, labels = ['Not canceled', 'Canceled'], autopct='%1.1f%%', startangle=90, colors=['forestgreen', 'firebrick'])\naxs[2].pie(previous_cancel3, labels = ['Canceled'], autopct='%1.1f%%', startangle=90, colors=['firebrick'])\n\naxs[0].title.set_text('Less than 9 times')\naxs[1].title.set_text('Between 9 and 18 times')\naxs[2].title.set_text('More than 18 times')\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**2.3 Cancellation due to distribution channel**","metadata":{}},{"cell_type":"code","source":"# Compare percentage of cancellation for each distribution channel\n\n# Create cross tab\nchannels = pd.crosstab(explore_data['is_canceled'], explore_data['distribution_channel']) \n\n# Calculate percentage separated by channel\nchannels = pd.DataFrame([ channels[channel] / channels[channel].sum() * 100 for channel in channels ])\n\nx = channels.reset_index().rename(columns={'index': 'Channel', 0:'Not canceled', 1: 'Canceled'})\nx = pd.concat( [x, pd.DataFrame([100] * 5)], axis=1)\nsns.barplot(x='Channel', y=0, data=x, color='salmon')\nsns.barplot(x='Channel', y='Canceled', data=x, color='red')\nplt.title('Portion of cancellation for each channel')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**2.4 Cancellation due to customer segment**","metadata":{}},{"cell_type":"code","source":"# Compare percentage of cancellation for each distribution channel\n\n# Create cross tab\ncus_types = pd.crosstab(explore_data['is_canceled'], explore_data['customer_type']) \n\n# Calculate percentage separated by channel\ncus_types = pd.DataFrame([ cus_types[cus_type] / cus_types[cus_type].sum() * 100 for cus_type in cus_types ])\n\nx = cus_types.reset_index().rename(columns={'index': 'Types', 0:'Not canceled', 1: 'Canceled'})\nx = pd.concat( [x, pd.DataFrame([100] * 5)], axis=1)\nsns.barplot(x='Types', y=0, data=x, color='salmon')\nsns.barplot(x='Types', y='Canceled', data=x, color='red')\nplt.title('Portion of cancellation for each customer type')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**2.5 Cancellation due to repeated guest**","metadata":{}},{"cell_type":"code","source":"# Explore relation between customer used to be guest before and cancellation\ncross_cancel = pd.crosstab(explore_data['is_canceled'], explore_data['is_repeated_guest'])\ncross_cancel = cross_cancel.rename({0: 'Not repeated guest', 1: 'Repeated guest'}, axis=1)\ncross_cancel = pd.DataFrame([cross_cancel[guest] / cross_cancel[guest].sum() * 100 for guest in cross_cancel ]).reset_index()\ncross_cancel = cross_cancel.rename({'index': 'Is repeated guest', 0: 'Not canceled', 1: 'Canceled'}, axis=1)\ncross_cancel = pd.concat([cross_cancel, pd.DataFrame([100] * 2)], axis=1)\n\n# Visualize\nsns.barplot(x= 'Is repeated guest', y= 0, data= cross_cancel, color='salmon')\nsns.barplot(x= 'Is repeated guest', y= 'Canceled', data= cross_cancel, color='red')\nplt.title('Portion of cancellation for each repeated and not repeated guest')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature engineering\nInvestigate features to find the appropriate features for prediction","metadata":{}},{"cell_type":"code","source":"explore_data.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Investigate imbalanced data\nsns.catplot(x= 'is_canceled', kind= 'count', data=explore_data)\nplt.title('Portion of cancellation in dataset')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Construct new features from features","metadata":{}},{"cell_type":"code","source":"# Find the total amount of guest\nexplore_data.loc[:, 'total_guest'] = explore_data['adults'] + explore_data['children'] + explore_data['babies']\nX_train.loc[:, 'total_guest'] = (X_train['adults'] + X_train['children'] + X_train['babies']).copy()\nX_test.loc[:, 'total_guest'] = (X_test['adults'] + X_test['children'] + X_test['babies']).copy()\n\n# Find the problability of customers who likely to cancel the lodging by determining from previous cancellations\nexplore_data.loc[:, 'cancellation_rate'] = explore_data['previous_cancellations'] / (explore_data['previous_cancellations'] + explore_data['previous_bookings_not_canceled'])\n# In case customers have never lodge, the rate will be -1\nexplore_data.loc[:, 'cancellation_rate'] = explore_data['cancellation_rate'].fillna(-1)\n\nX_train.loc[:, 'cancellation_rate'] = (X_train['previous_cancellations'] / (X_train['previous_cancellations'] + X_train['previous_bookings_not_canceled'])).copy()\nX_train.loc[:, 'cancellation_rate'].fillna(-1, inplace=True)\n\nX_test.loc[:, 'cancellation_rate'] = (X_test['previous_cancellations'] / (X_test['previous_cancellations'] + X_test['previous_bookings_not_canceled'])).copy()\nX_test.loc[:, 'cancellation_rate'].fillna(-1, inplace= True)\n\n# Find the total nights customers stay\nexplore_data.loc[:, 'total_nights_stay'] = explore_data['stays_in_week_nights'] + explore_data['stays_in_weekend_nights']\nX_train.loc[:, 'total_nights_stay'] = (X_train['stays_in_week_nights'] + X_train['stays_in_weekend_nights']).copy()\nX_test.loc[:, 'total_nights_stay'] = (X_test['stays_in_week_nights'] + X_test['stays_in_weekend_nights']).copy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Categorize data into two types\nnumerical_data = explore_data.select_dtypes(include=['int64', 'float64']).columns\ncategorical_data = explore_data.select_dtypes(include=['object']).columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Declare variable for keeping selected categorical features\nselected_num = []\n\n# Explore corrlation of numerical variables\nplt.figure(figsize= (12,8))\nfeatures_corr = explore_data[numerical_data].corr()\nsns.heatmap(features_corr)\n\n# Select features from numerical features\nsig_features = features_corr['is_canceled'].sort_values().drop('is_canceled', axis=0)\nselected_num.extend(sig_features.head(3).index.tolist())\nselected_num.extend(sig_features.tail(3).index.tolist())\nplt.title('Heat map of numerical correlation')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from scipy.stats import chisquare\nfrom scipy.stats import chi2_contingency\nfrom scipy.stats import chi2\n\n# Declare variable for keeping selected numerical features\nselected_cate = []\n\n# Explore chi-square of categorical variables\nfor cat in categorical_data:\n    contingency_tab = pd.crosstab(explore_data[cat], explore_data['is_canceled'])\n    stat, p, dof, expected = chi2_contingency(contingency_tab)\n    # Select features with p-value less than 0.05     \n    if p < 0.05 and cat != 'reservation_status':\n        selected_cate.append(cat)\n    print('p-value of {} is {}'.format(cat, p))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Selected categorical features: {}'.format(selected_cate))\nprint('Selected numerical features: {}'.format(selected_num))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler, OrdinalEncoder\nfrom category_encoders import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\n\nmonths = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December']\n\nselected_features = selected_cate.copy()\nselected_cate.remove('arrival_date_month')\n\n# Deine pipeline for transforming each feaure type separately\npreprocessor = ColumnTransformer([\n        ('scaler', StandardScaler(), pd.Index(selected_num)),\n        ('one_hot', OneHotEncoder(), pd.Index(selected_cate)),\n        ('ordinal_enc', OrdinalEncoder(categories= [months]), pd.Index(['arrival_date_month']))\n])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modeling\nTry multiple algorithms to fit the data","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report\nfrom sklearn import svm\n\n# Define model training pipelines\nlgs = Pipeline([('preprocess', preprocessor),\n                ('logistic', LogisticRegression(max_iter=300))])\n\nsvm = Pipeline([('preprocess', preprocessor),\n                ('svm', svm.SVC(max_iter=300))])\n\ntree = Pipeline([('preprocess', preprocessor),\n                ('decisionTree', DecisionTreeClassifier())])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\n\nselected_features =  selected_num + selected_cate + ['arrival_date_month']\nX_train = X_train[selected_features].copy()\nX_test = X_test[selected_features].copy()\n\nlgs_cv = cross_val_score(lgs, X_train, y_train, cv=5)\nsvm_cv = cross_val_score(svm, X_train, y_train, cv=5)\ntree_cv = cross_val_score(tree, X_train, y_train, cv=5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Logistic Regression mean: {}'.format(lgs_cv.mean()))\nprint('SVM mean: {}'.format(svm_cv.mean()))\nprint('Decision Tree mean: {}'.format(tree_cv.mean()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fit the model\nlgs_model = lgs.fit(X_train, y_train)\nsvm_model = svm.fit(X_train, y_train)\ntree_model = tree.fit(X_train, y_train)\n\n# Predict the result\nlgs_pred = lgs_model.predict(X_test)\nsvm_pred = svm_model.predict(X_test)\ntree_pred = tree_model.predict(X_test)\n\n# Show classification report\nprint(classification_report(y_test, lgs_pred), end='\\n\\n')\nprint(classification_report(y_test, svm_pred), end='\\n\\n')\nprint(classification_report(y_test, tree_pred), end='\\n\\n')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Parameter Tuning\nSelect best type of model which can be fitted to data from past section and tune the parameters with GridSearchCV.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\n# Define options\ntree_params = {\n  'decisionTree__criterion': ['gini', 'entropy'],\n  'decisionTree__splitter': ['best', 'random'],\n  'decisionTree__max_depth': [3,4,5],\n  'decisionTree__min_samples_leaf': [1,2,3],\n  'decisionTree__min_samples_split': [2,3,4]\n} \n\n\n# Define GridSearchCV\nmodel = GridSearchCV(estimator=tree,\n            param_grid=tree_params,\n            scoring='accuracy',\n            cv=10)\n\n# Train, predict and evaluate\nmodel.fit(X_train, y_train)\nprint('Best params are: {}'.format(model.best_params_))\nprint('Best training accuracy: {}'.format(model.best_score_))\ny_pred = model.predict(X_test)\nprint('Test set accuracy: {}'.format(accuracy_score(y_test, y_pred)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}