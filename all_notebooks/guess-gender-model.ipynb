{"cells":[{"source":"# Guessing people's natural gender by their name\n\nIn portuguese (Brazil official idiom), people's names are usually related to their natural gender (male or female only). That been said the sole name of a person contains patterns that can reveal their natural gender.\n\nThis notebook shows one approach using Machine Learning to discover people's natural gender using only their name.\n\nPs. The dataset used only contains a list of common names used in Brazil.\n\n# Step 1: Reading Data\n\nFirst step of this process is read the data.\nThe data is formed by two columns containing the person name and gender.\n\nThe gender field contains values `Masc` for male and `Fem` for female.","metadata":{"_cell_guid":"2e327838-58e2-479b-af1a-efd6be1f7e0d","_uuid":"1d4e29b2ba7f864288dcbc72d900729c4c2ebe1f"},"cell_type":"markdown"},{"outputs":[],"source":"import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport unicodedata\n\ndata = pd.read_csv('../input/brazilian-names.csv', header=None) \\\n    .rename(columns={0 : 'name', 1 : 'gender'})\n\ndata.head()","metadata":{"collapsed":true,"_cell_guid":"4832c9c5-4371-4ae6-9193-ad4a89e7a946","_uuid":"40122601c19addb4f16094ee9260986fd224812d","scrolled":true},"execution_count":null,"cell_type":"code"},{"source":"# Step 2: Data Preparation\n\nThis section covers the way the data (mere names) was prepared the be trained using a classification model.\n\nThis section is split in 3 sub sections:\n* Name Normalization;\n* Numeric representation;\n* Interpole the data to a fixed length.\n\n## Step 2.1: Name Normalization\n\nThis step defines a function `norm_name` that normalizes names. The normalization takes the following steps:\n* Remove word accents. (Ã£ -> a);\n* Lower case the names;\n* Remove duplicated letters (aa -> a, nn -> n).\n\nAfter the function definition it is applied to the original dataset.","metadata":{"_cell_guid":"3a10d565-6350-4ba6-b62b-a57e3ff2b50c","_uuid":"a15e4ed3bdfbb1c7c9a378e46975820e49cb88e3"},"cell_type":"markdown"},{"outputs":[],"source":"import string\nimport itertools\n\ndef norm_name(data):\n    unaccents = ''.join(x for x in unicodedata.normalize('NFKD', data) if x in string.ascii_letters).lower()\n    return ''.join(ch for ch, _ in itertools.groupby(unaccents))\n\ndata['name'] = data['name'].map(norm_name)\ndata['gender'] = data['gender'].map(lambda r: r[0])\ndata.drop_duplicates()\ndata.head()","metadata":{"collapsed":true,"_cell_guid":"b5c51a8f-1d3a-4b90-96c3-7163c830e92e","_uuid":"0d8d41997e12ea1b49330f958cfc1911a0a5ae20"},"execution_count":null,"cell_type":"code"},{"source":"## Step 2.2: Numeric representation\n\nThis step defines how the represent name strings in a numeric way.\n\nThe main idea consist to convert all character of the string into their numeric representation, this should convert `'a'..'z'` to `97..122`. \n\nAs part of the data preparation process, this numeric representation should be scaled to numbers between 0 and 1. So the numeric list must be linear down scaled, so the function must convert `'a'..'z'` to `0..1`.\n\nThe function used on the name`albatroz` should return `0.0, 0.44, 0.04, 0.0, 0.76, 0.68, 0.56, 1.0`.","metadata":{"_cell_guid":"f373c184-4677-4261-a5c1-30db529131c4","_uuid":"1e287d12ed347cdf1b782bcb8dc139774ca690e2"},"cell_type":"markdown"},{"outputs":[],"source":"def numeric_rep(name): return [float(ord(c) - ord('a'))/(ord('z') - ord('a')) for c in name]\n\nnumeric_converter = lambda row: ', '.join([str(i) for i in numeric_rep(row)])\nnumeric_df = pd.DataFrame.from_items([('name', data['name']), \n                         ('numeric_name', data['name'].map(numeric_converter))])\nnumeric_df.head()","metadata":{"collapsed":true,"_cell_guid":"f596f3b1-0db4-4479-86f5-33262ec331f3","_uuid":"fffa9ad41d4a7fbb3cffa3460769b3749a2e75da"},"execution_count":null,"cell_type":"code"},{"source":"## Step 2.3: Interpole the data onto a fixed length\n\nThis is the most complex step of the data preparation section. Now that all strings have a numerical and normalized (linear down scaled) representation, there's still another problem. Their numerical representation has different lengths.\n\nFor instance `abel (0.0, 0.04, 0.16, 0.44)` have a numeric representation of 4 numbers and `abelardo (0.0, 0.04, 0.16, 0.44, 0.0, 0.68, 0.12, 0.56)` have a numeric representation of 8 numbers.\n\nThe `interpole_name` function has the objective to scale the length of the numeric representation to a given size (`flen` param). ","metadata":{"_cell_guid":"09fd0d73-bd8f-4922-b48e-0fc34dc9c13b","_uuid":"d513098b0562b18353cb75621f3a317452dbadbd"},"cell_type":"markdown"},{"outputs":[],"source":"import numpy as np\n\ndef interpole_name(name, flen = 10):\n    y = numeric_rep(name)\n    x = range(len(y))\n    nx = np.linspace(0, len(y)-1, flen)\n    return np.interp(nx, x, y)","metadata":{"collapsed":true,"_cell_guid":"15937739-b83a-46d3-9d69-fb49a50d7981","_uuid":"e5134d9166149aef784b651ee91cdb615749ed9a"},"execution_count":null,"cell_type":"code"},{"source":"So `abel` can be expanded to size 10, so the representation `(0.0, 0.04, 0.16, 0.44)` will change to `(0.0, 0.013, 0.026, 0.04, 0.08, 0.12, 0.16, 0.25, 0.34, 0.44)`. \n\nThe same way `abelardo` can be shrunk to size 5, the representation `(0.0, 0.04, 0.16, 0.44, 0.0, 0.68, 0.12, 0.56)` will change into `(0.0, 0.13, 0.22, 0.54, 0.56)`.\n\nThe images below shows how theses names can be visualized with both were expanded to size 50.","metadata":{"_cell_guid":"bafc2660-a1fc-4456-8c8c-5c35e4b98163","_uuid":"da07ebff65e89f77bf5f56105602f7e47934c017"},"cell_type":"markdown"},{"outputs":[],"source":"import matplotlib.pyplot as plt\n\nf, ((ax1, ax2), (ax3, ax4)) = plt.subplots(ncols=2, nrows=2, figsize = (14, 6))\nax1.set_title('abel')\nax1.stem(range(len('abel')), numeric_rep('abel'))\nax2.set_title('abelardo')\nax2.stem(range(len('abelardo')), numeric_rep('abelardo'))\n\nax3.set_title('abel (expanded to 50)')\nax3.stem(range(50), interpole_name('abel', 50))\nax4.set_title('abelardo (expanded to 50)')\nax4.stem(range(50), interpole_name('abelardo', 50))\n\nf.subplots_adjust(hspace=0.3)\nplt.show()","metadata":{"collapsed":true,"_cell_guid":"800e7fc6-39ac-413b-88bf-33d77cb2a182","_uuid":"5d0a8554544cde3c88b124d1001e8e9213a7edbf"},"execution_count":null,"cell_type":"code"},{"source":"As shown in the images above, expanding the numerical representation can be a good way to maintain the original pattern, however shrink it can represent some significant loss of information. An alternative to this approach would be to use the last N numbers (crop the last N characters) in the list. Since the portugueses names seems to define the person gender on the last characters of their names, this seems to be a good approach.\n\nAfter the definition of the 3 previous steps, they're used to convert the original dataset to a transformed dataframe.  The new dataframe contains the name converted into an/a expanded/cropped length of 10 and the gender (now named label) changed into 0 (for male) or 1 (for female).\n\nThe `convertdf` function does this whole job.","metadata":{"_cell_guid":"b7a8a5cd-9a9f-4fb8-8940-4b275980f058","_uuid":"85406725d87cacd97aa590ce25ec5362df23751a"},"cell_type":"markdown"},{"outputs":[],"source":"def convertdf(data, length = 10):\n    dt = data.copy()\n    aux = dt['name'].apply(lambda x: pd.Series(interpole_name(x[-length:], length)))\n    for i in range(length):\n        dt['x'+str(i+1)] = aux[i]\n    \n    dt['label'] = dt['gender'].map(lambda r: 0 if r == 'M' else 1)\n    return dt.drop('gender', 1)\n    \ntdf = convertdf(data, 10)\ntdf.head()","metadata":{"collapsed":true,"_cell_guid":"0e7440da-3b47-4288-a83f-a8190d8f91e7","_uuid":"2bfbf4a847ec19017f3bb598060f95040257cbc4"},"execution_count":null,"cell_type":"code"},{"source":"# Step 3: Model Selection\n\nNow is time to train the model that is going to do the classification. Scikit provides a set of algorithms that can be used. But witch one of them is more appropriate for the analyzed dataset?\n\nTo help on the algorithm selection, Scikit provides the `cross_val_score` function. On this analysis, this function is used together it the `KFold` class, so the data is divided in multiple chunks of test/train split and then the average accuracy are calculated.\n\nThis session shows the code used to compare the accuracy of the following algorithms: `LogisticRegression`, `LinearDiscriminantAnalysis`, `KNeighborsClassifier`, `DecisionTreeClassifier`, `GaussianNB`, `SVC`, `RandomForestClassifier` and `XGBClassifier`.","metadata":{"_cell_guid":"68f5beb3-19b2-452f-9e09-2e727487554e","_uuid":"324d1d0485742593c55e7b3eacbbea843dd27073"},"cell_type":"markdown"},{"outputs":[],"source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\n\nmodels = [('LR', LogisticRegression()),\n         ('LDA', LinearDiscriminantAnalysis()),\n         ('KNN', KNeighborsClassifier()),\n         ('CART', DecisionTreeClassifier()),\n         ('NB', GaussianNB()),\n         ('SVM', SVC()),\n         ('RF', RandomForestClassifier()),\n         ('XGB', XGBClassifier())]\n\nseed = 1073\nresults = []\nnames = []\nscoring = 'accuracy'\nX = tdf.iloc[:,1:11]\nY = tdf['label']\nfor name, model in models:\n    kfold = KFold(n_splits=10, random_state=seed)\n    cv_results = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)","metadata":{"collapsed":true,"_cell_guid":"631e672f-9d35-4a83-91b7-f91f9b721bcb","_uuid":"c2ac2ff2f7229819271ab22a8dd1bbf3480b76cf"},"execution_count":null,"cell_type":"code"},{"source":"The output of the comparison shows the algorithms, the mean (better higher) and standard deviation (better lower) of the accuracy calculated on all tests executed by `cross_val_score`.\n\nA visual comparison of the results can be analyzed using a boxplot.","metadata":{"_cell_guid":"3549721f-d43b-42b2-9378-873e90877b4d","_uuid":"ceb80bda4db31f04ceda77f1322bc24075bd04eb"},"cell_type":"markdown"},{"outputs":[],"source":"fig = plt.figure()\n\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(111)\nplt.boxplot(results)\nax.set_xticklabels(names)\nplt.show()","metadata":{"collapsed":true,"_cell_guid":"ddb62bb2-e91f-4ee8-9122-c233a9cf39f8","_uuid":"88b77968d7561d8cc9e1aa0d0535268951e47944"},"execution_count":null,"cell_type":"code"},{"source":"# Step 4: Hyper-Parameter tuning\n\nThe numeric and visual results shows that for this dataset, `SVM` an `LDA` showed betters results among the others. However this comparison based on the algorithms with their \"base form\", without any extra parameter setup. Setting a right set of parameters with the right values can improve significantly their results, but what kind of values can be set on every parameter to tune up the algorithm? To answer this question a combination os parameter must be tested and the results must be compared. Scikit provides a Hyper-Parameters tuning package.\n\nWith the help of this package a set of parameters are setup and the tuning process will search for the best combination of parameters looking for the best cross validation score possible.\n\nAn exhaustive search considers all parameter combinations. Since this approach can take too much time, this analysis make use of a randomized search of combinations.","metadata":{"_cell_guid":"c6b8f5d3-9282-47e9-8c18-f39b89f91882","_uuid":"a55b805daa323812724677b9ef59728d7fab3b19"},"cell_type":"markdown"},{"outputs":[],"source":"from sklearn.model_selection import RandomizedSearchCV\n\n# a set of possible values for many possible parameters for a SVM model.\nparameters = {\n    'C':            np.logspace(-3, 2, 6),\n    'kernel':       ['linear', 'rbf'],                   # precomputed,'poly', 'sigmoid'\\n\",\n    'degree':       np.arange( 0, 100+0, 1 ).tolist(),\n    'gamma':        np.logspace(-3, 2, 6),\n    'coef0':        np.arange( 0.0, 10.0+0.0, 0.1 ).tolist(),\n    'shrinking':    [True],\n    'probability':  [False],\n    'tol':          np.arange( 0.001, 0.01+0.001, 0.001 ).tolist(),\n    'cache_size':   [2000],\n    'class_weight': [None],\n    'verbose':      [False],\n    'max_iter':     [-1],\n    'random_state': [None],\n    }\n\nmodel = RandomizedSearchCV( n_iter              = 500,\n                            estimator           = SVC(),\n                            param_distributions = parameters,\n                            n_jobs              = 4,\n                            iid                 = True,\n                            refit               = True,\n                            cv                  = 5,\n                            verbose             = 1,\n                            pre_dispatch        = '2*n_jobs'\n                            )         # scoring = 'accuracy'\nmodel.fit( X, Y )\nprint( model.best_estimator_ )\nprint( model.best_score_ )\nprint( model.best_params_ )","metadata":{"collapsed":true,"_cell_guid":"40b26098-0c02-4f04-92bd-54b84a41f385","_uuid":"42cce31860dd2ec7450f68f2278284d5d8000ff5"},"execution_count":null,"cell_type":"code"},{"source":"# Step 5: Training the final model\n\nThe output above shows a good set of parameters that can be used to improve the original `SVC()` used on the Model Selection Step. These parameters can be finally used to train the model.","metadata":{"_cell_guid":"f5bd18d0-9ecf-4905-9fe7-77b6edc4b7e6","_uuid":"f0f8193b129ca5e9a776e34ba995637e2ed0b430"},"cell_type":"markdown"},{"outputs":[],"source":"clf = SVC(C=10.0, cache_size=2000, class_weight=None, coef0=0.4,\n  decision_function_shape='ovr', degree=80, gamma=0.1,\n  kernel='rbf', max_iter=-1, probability=False, random_state=None,\n  shrinking=True, tol=0.01, verbose=False) #0.829180327869\n\nclf.fit(X, Y)","metadata":{"collapsed":true,"_cell_guid":"dd1abed9-effb-4a15-9f2d-3c5610a375f8","_uuid":"a6841e42b3cf963b8381e6195fe5d0ccdbe09646"},"execution_count":null,"cell_type":"code"},{"source":"After the final model trained, it can now be used to predict the gender of new names.","metadata":{"_cell_guid":"7011227e-f4a3-4aef-8d7b-14da364132b9","_uuid":"36921f8c8f5dfd8820c88dbcd380f76c0199d627"},"cell_type":"markdown"},{"outputs":[],"source":"sample_names = [\"Messias\", \"Thiago\", \"Marcelo\", \"Renata\", \"Larissa\", \n                \"Altino\", \"Desiree\", \"Andrew\", \"Jefferson\", \"Tatiene\"]\nguessed_genders = clf.predict([interpole_name(norm_name(x)) for x in sample_names])\ngenders = ['M' if g == 0 else 'F' for g in guessed_genders]\n\npd.DataFrame.from_items([('names', sample_names), ('guessed_gender', genders)])","metadata":{"collapsed":true,"_cell_guid":"796e8a28-b79f-4268-b9fe-97f26f4d94bb","_uuid":"8a1640f5a6c1c0694c97ca6c4e8169cab15f997d"},"execution_count":null,"cell_type":"code"},{"source":"# Step 6: Final Considerations\n\nThe generated SVC model can also be persisted using the default scikit learning dump function (look at the official [docs](http://scikit-learn.org/stable/modules/model_persistence.html)) or it can use the PMML format.\n\nTo persist in the PMML format, the lib [sklearn2pmml](https://github.com/jpmml/sklearn2pmml) must be installed.\nThe additional lines of code can the executed to generate the PMML file.\n\n```python\nfrom sklearn2pmml import PMMLPipeline\nfrom sklearn2pmml import sklearn2pmml\n\ngender_clf = PMMLPipeline([\n\t(\"classifier\", clf)\n])\n\nsklearn2pmml(gender_clf, \"GenderGuess.pmml\", with_repr = True)\n```\n\nThe PMML model can be used on other programming languages besides python. \nThis [project](https://github.com/pintowar/gguess) shows the PMML generated on this notebook on a Java/Groovy Application.","metadata":{"_cell_guid":"e4433797-e107-403b-9272-43f88768443d","_uuid":"ee346b4dc19516484fb380197e20d2f1db287194"},"cell_type":"markdown"}],"metadata":{"language_info":{"version":"3.6.3","name":"python","codemirror_mode":{"version":3,"name":"ipython"},"mimetype":"text/x-python","file_extension":".py","nbconvert_exporter":"python","pygments_lexer":"ipython3"},"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"}},"nbformat_minor":1,"nbformat":4}