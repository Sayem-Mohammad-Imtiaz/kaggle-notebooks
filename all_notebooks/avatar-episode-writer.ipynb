{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Time to get serious"},{"metadata":{},"cell_type":"markdown","source":"![](http://cdn.vox-cdn.com/thumbor/mXo5ObKpTbHYi9YslBy6YhfedT4=/95x601:1280x1460/1200x800/filters:focal(538x858:742x1062)/cdn.vox-cdn.com/uploads/chorus_image/image/66699060/mgidarccontentnick.comc008fa9d_d.0.png)"},{"metadata":{},"cell_type":"markdown","source":"Lets have a little fun with fastai. Im going to use a language model from fastai to generate new episodes of Avatar the last airbender. Cause we all wanted more episodes."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"from fastai.text.all import *","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_lm = pd.read_csv('../input/avatar-the-last-air-bender/avatar.csv',encoding='latin1') ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_lm.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_lm.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_lm['chapter'].unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The data object rows are broken up by scene. Each full_text entry is the full text for a scene. So lets make a function that gets the full script for each episode. "},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_scripts(df):\n    scripts = []\n    for episode in df['chapter'].unique():\n        script = ''\n        for i, obj in df.iterrows():\n            if obj['chapter'] == episode:\n                script += obj['character'] + \": \" + obj['full_text'] + \" \"\n        \n        scripts.append(script)\n    return pd.DataFrame(scripts)\n\nscripts_ = get_scripts(df_lm)\nscripts_ = scripts_.apply(lambda x: x.replace(\"\\n\", \" \"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scripts_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dls = TextDataLoaders.from_df(scripts_, is_lm=True)\ndls.show_batch(max_n=3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The TextDataLoaders object just created a dataset where the independent variable is a sequence of antecedent from the avatar corpus and the dependent variable is a sequence of words index offset by one. It also takes care of the tokenization process for us."},{"metadata":{"trusted":true},"cell_type":"code","source":"learn = language_model_learner(dls, AWD_LSTM, pretrained=True, drop_mult=0.5, metrics=[accuracy, perplexity])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.lr_find()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.fit_one_cycle(5, 2e-2, \n                    #cbs=EarlyStoppingCallback(min_delta=0.005, monitor='valid_loss', patience=2)\n                   )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.unfreeze()\nlearn.lr_find()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.fit_one_cycle(8, slice(1e-5, 2e-3))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.recorder.plot_loss()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start = \"The scene opens\"\ndef write_episode(start):\n\n    N_WORDS = 400\n    N_SENTENCES = 1\n    preds = [learn.predict(start, N_WORDS) \n             for _ in range(N_SENTENCES)]\n\n    return (\" \".join(preds)) + \". THE END.\"\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ep = write_episode(start)\nprint(ep)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"THE END"},{"metadata":{},"cell_type":"markdown","source":"![](http://imgix.bustle.com/uploads/image/2020/6/1/65220bb4-d554-403f-a682-d704bd217007-atla-iroh-tea.jpg?w=1200&h=630&fit=crop&crop=faces&fm=jpg)"},{"metadata":{},"cell_type":"markdown","source":"More tweeks obviously need to be made. Let me know in the comments if any of you have suggestions. "}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}