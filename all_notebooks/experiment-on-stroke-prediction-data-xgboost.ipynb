{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\"\"\"\nCreated on Tue May  4 00:11:06 2021\n\n@author: YUSUF BARAN TANRIVERDÄ°\n\n\n\"\"\"\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\n\n# accuracy_score average_precision_score\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import confusion_matrix, recall_score, roc_auc_score, balanced_accuracy_score, precision_score\nfrom sklearn.metrics import roc_curve\n\ndata = pd.read_csv('../input/stroke-prediction-dataset/healthcare-dataset-stroke-data.csv')\n","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Turn categorical values into label encoding\n# ref: https://pbpython.com/categorical-encoding.html\n\ndef label_encoding(data, feature_name):\n    # encodes with an alphabetical order  of original titles\n    data[feature_name] = data[feature_name].astype('category')\n    data[feature_name] = data[feature_name].cat.codes\n    return\n\n\nlabel_encoding(data, 'gender')\nlabel_encoding(data, 'ever_married')\nlabel_encoding(data, 'work_type')\nlabel_encoding(data, 'Residence_type')\nlabel_encoding(data, 'smoking_status')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"SPECIAL CASES\n1) NaN values\nA very common way to replace missing values is using a median.\nReplace using median \n\nref: https://towardsdatascience.com/data-cleaning-with-python-and-pandas-detecting-missing-values-3e9c6ebcf78b\n\n2) UNKNOWN values of smoking status\n What to do? \n They shouldn't play as a factor category- \n i.e they should be neutral on the subject of smoking is whether harmful or not!\n For now, let's pass them as another category.\n ","metadata":{}},{"cell_type":"code","source":"median = data['bmi'].median()\ndata['bmi'].fillna(median, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.pairplot(data.iloc[:, 5:], hue='stroke') \nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It is seen that data is imbalanced heavily. Let's experiment with that before handling.","metadata":{}},{"cell_type":"markdown","source":"**Create Datasets**","metadata":{}},{"cell_type":"code","source":"# create datasets    \nxy = data.iloc[:, 1:].to_numpy()\nnp.random.shuffle(xy)\n\n\nx = xy[:, :-1]\ny = xy[:, -1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Some Useful Functions**","metadata":{}},{"cell_type":"code","source":"def convert_one_hot_vectors(y):\n    target_list = []\n    one_hot = np.eye(2)\n    for target in y:\n        if target == 0:\n            target_list.append(one_hot[0])\n        if target == 1:\n            target_list.append(one_hot[1])\n    y = np.asarray(target_list)\n    \n    #x, y = shuffle_data(x, y)\n    return y\n\ndef get_accuracy(y_preds, y_labels):\n    matches = 0\n    for boolean in y_preds == y_labels:\n        if boolean == True: matches = matches +1\n    return matches/ len(y_labels)* 100\n\n#ref:https://scikit-learn.org/stable/modules/model_evaluation.html\ndef confusion_matrix_scorer(y_preds, y_labels):\n    cm = confusion_matrix(y_labels, y_preds)\n    return {'tn': cm[0, 0], 'fp': cm[0, 1],\n            'fn': cm[1, 0], 'tp': cm[1, 1]}\n\n#ref: https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-classification-in-python/\ndef plot_roc_auc_curve(y_preds, y_labels, model_name):\n    ns_auc = roc_auc_score(y_labels, np.zeros(y_labels.shape))\n    lr_auc = roc_auc_score(y_labels, y_preds)\n    # summarize scores\n    print('No Skill: ROC AUC=%.3f' % (ns_auc))\n    print(f'{model_name}: ROC AUC=%.3f' % (lr_auc))\n    # calculate roc curves\n    ns_fpr, ns_tpr, _ = roc_curve(y, np.zeros(y.shape))\n    lr_fpr, lr_tpr, _ = roc_curve(y, preds)\n    # plot the roc curve for the model\n    ns_curve, = plt.plot(ns_fpr, ns_tpr, linestyle='--', label='No Skill')\n    plt.plot(lr_fpr, lr_tpr, marker='.', label=f'{model_name}')\n    # axis labels\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    # show the legend\n    plt.legend()\n    # show the plot\n    plt.show()\n    ns_curve.remove()\n\ndef print_metrics(y_preds, y_labels, model_name):\n    print(\"\\n MODEL: \", model_name )\n    \n    print(\"Accuracy:\", get_accuracy(y_preds, y_labels))\n    print(\"Null rate of test:\", get_accuracy(y_preds, np.zeros(y.shape)))\n    print(\"\\n CONFUSION MATRIX\")\n    print(confusion_matrix_scorer(y_preds, y_labels))\n\n    # recall is tp/ (tp+fn)\n    print(\"Recall: \", recall_score(y_preds, y_labels))\n    \n    # precision is tp/(tp+fp)\n    print(\"Precision:\", precision_score(y_preds, y_labels))\n    \n    # (recall+ specificity)/2\n    print(\"Balanced Accuracy:\", balanced_accuracy_score(y_labels, y_preds)* 100)\n\n\ndef fit_LinearRegression():\n    regressor = LinearRegression()\n    tmp_y = convert_one_hot_vectors(y)\n    regressor.fit(x,tmp_y)\n        \n    probs = regressor.predict(x)\n    return  probs, regressor","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"EXP1 LINEAR REGRESSION","metadata":{}},{"cell_type":"code","source":"probs, _  = fit_LinearRegression()\npreds = np.argmax(probs, axis= 1)\nprint_metrics(preds, y, 'Linear Regression (imbalanced data)')\n# balanced accuracy in this case would be 0.5\n\n\nprint(\"PREVELANCE -> pr(stroke|sample) = \",len(data[data['stroke'] == 1])/len(data))\n# see the scatter plot also.\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It always give 'negative'. Imbalanced target problem?   Yes, solutions might be:\n    \nSMOTE\n\nMODEL CHANGING\n\nDIFFERENT METRIC EVALUATION \n\n(ref: https://towardsdatascience.com/regression-for-imbalanced-data-with-application-edf93517247c)","metadata":{}},{"cell_type":"markdown","source":"EXP2 SMOTE - REGRESSION","metadata":{}},{"cell_type":"code","source":"from imblearn.over_sampling import SMOTENC\n\n#CALL RAW DATA\nx = xy[:, :-1]\ny = xy[:, -1]\n\n#SMOTE the data with SMOTENC algorithm\ncat_col_index = [0, 4, 5, 6, 9]\noversample = SMOTENC(categorical_features=cat_col_index, k_neighbors=2)\nx, y = oversample.fit_resample(x, y)\n\ndf = pd.DataFrame(x, columns= ['gender', 'age', 'hypertension', 'heart_disease', 'ever_married','work_type', 'Residence_type', 'avg_glucose_level', 'bmi', 'smoking_status'])\n\ndf['stroke'] = y\n# print(df)\n#sns.pairplot(df.iloc[:, 5:], hue='stroke') \n#plt.show()\n#sns.pairplot(df.iloc[:, 5:], hue='stroke') \n#plt.show()\nprint(\" \\n AFTER SMOTENC...\")\n\nprobs, linearModelv2 = fit_LinearRegression()\npreds = np.argmax(probs, axis= 1)\n\nprint_metrics(preds, y, 'Linear Regression after SMOTE')\n\nprint(\" NEW PREVELANCE ->pr(stroke|sample) = \", len(df[df['stroke'] == 1])/len(df))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.pairplot(df.iloc[:, 5:], hue='stroke') \nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"EXP3 DEEP LEARNING METHODS\n\nREFERENCES:\n https://towardsdatascience.com/handling-overfitting-in-deep-learning-models-c760ee047c6e\n\n https://towardsdatascience.com/dont-overfit-how-to-prevent-overfitting-in-your-deep-learning-models-63274e552323\n\n https://towardsdatascience.com/multi-layer-perceptron-using-tensorflow-9f3e218a4809\n","metadata":{}},{"cell_type":"code","source":"# import libs and define some useful functions\nfrom keras.layers import *\nimport tensorflow as tf\nimport keras\n\ndef get_evaluation_accuracy(history):\n    return 100 * history['accuracy']\n\n\ndef plot_metrics(model_name, history, metric_name):\n    plt.figure()\n    e = range(1, len(history.history[metric_name]) + 1)\n    plt.plot(e, history.history[metric_name], 'bo', label=metric_name)\n    plt.plot(e, history.history[f'val_{metric_name}'], 'b', label=f'val_{metric_name}')\n    plt.xlabel('Epoch')\n    plt.ylabel(f'{metric_name}')\n    plt.legend(loc='lower right')\n    plt.title(f'Comparing training and validation loss of {model_name}')\n    plt.savefig(f'./{model_name}.png')\n\ndef metrics_table_for_evaluation(model_name, history):\n    tp = history['true_pos']\n    fp = history['false_pos']\n    tn = history['true_neg']\n    fn = history['false_neg']\n    \n    cm = pd.DataFrame(index=('predicted YES', 'predicted NO'), columns=('actual YES', 'actual NO'))\n    cm.iloc[0,0] = np.mean(tp)\n    cm.iloc[0,1] = np.mean(fn)\n    cm.iloc[1,0] = np.mean(fp)\n    cm.iloc[1,1] = np.mean(tn)\n    \n    mcs = pd.DataFrame(index=[model_name], columns=('recall (TPR)', 'specificty', 'sensitivity', 'precision', 'false positive rate', 'accuracy'))\n    \n    mcs.iloc[0,0] = np.mean(tp)/ (np.mean(tp)+ np.mean(fp))\n    mcs.iloc[0,1] = np.mean(tn)/ (np.mean(tn)+ np.mean(fp)) \n    mcs.iloc[0,2] = np.mean(tp)/ (np.mean(tp)+ np.mean(fn)) \n    mcs.iloc[0,3] = np.mean(tp)/ (np.mean(tp)+ np.mean(fp)) \n    mcs.iloc[0,4] = np.mean(fp)/ (np.mean(tn)+ np.mean(fp)) \n    mcs.iloc[0,5] = np.mean(history['accuracy']) \n    \n    print(cm)\n    print(mcs)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#HYPERPARAMATERS\n\nLEARNING_RATE = 0.001\nDROPOUT_RATE = 0.5\nEPOCHS= 100\nBATCH_SIZE = 10\nLOSS_FN = 'binary_crossentropy'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create train_dataset\n# test split %90\n\nboundary = int(len(x) *0.1)\nx_train = x[boundary:]\ny_train = y[boundary:]\n    \n# create test_dataset\nx_test = x[:boundary]\ny_test = y[:boundary]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"DEFINE MODEL with a dropout layer.","metadata":{}},{"cell_type":"code","source":"def dropout_dnn(number_of_classes=2, lr=LEARNING_RATE, dr=DROPOUT_RATE):\n    model = keras.Sequential([\n        Dense(512, activation='relu'),\n        Dense(256, activation='relu'),\n        Dropout(dr),\n        Dense(64, activation='relu'),\n        Dropout(dr),\n        Dense(16, activation='relu'),\n        Dense(1, activation='sigmoid')\n    ])\n    model.compile(loss=LOSS_FN,\n                  metrics=['accuracy', \n                           tf.keras.metrics.TrueNegatives(name='true_neg'),\n                           tf.keras.metrics.FalsePositives(name='false_pos'),\n                           tf.keras.metrics.FalseNegatives(name='false_neg'),\n                           tf.keras.metrics.TruePositives(name='true_pos'),\n                           ],\n                  optimizer=tf.keras.optimizers.Adam(lr))\n    return model\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To stop training when no change happens:","metadata":{}},{"cell_type":"code","source":"es_callback = keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Tranining**","metadata":{}},{"cell_type":"code","source":"densenet = dropout_dnn()\ndropout_history = densenet.fit(x_train, y_train, validation_split=0.2, \n                               batch_size=BATCH_SIZE, verbose=1, epochs=EPOCHS\n                               )\n\n#save model\ntf.keras.models.save_model(\n    densenet,\n    './models/densenet.h5'\n)\n\n\npd.DataFrame.from_dict(dropout_history.history).to_csv('./dropout_histor.csv', index=False)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_metrics(history=dropout_history, metric_name='loss', model_name='densenet')\nplot_metrics(history=dropout_history, metric_name='accuracy', model_name='densenet')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eval_history = densenet.evaluate(x_test, y_test, return_dict=True)\n\nmetrics_table_for_evaluation('MLP test', eval_history)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"metrics_table_for_evaluation('MLP train', dropout_history.history)\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"NULL RATE\")\ndensenet.evaluate(x_test, np.zeros(y_test.shape))\n\nprint(\"FULL RATE\")\nindices = [i for i in range(len(y)) if y[i]==1]\ndensenet.evaluate(x_test, np.ones(y_test.shape))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"EXP4 XGBOOST","metadata":{}},{"cell_type":"code","source":"from collections import Counter\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom xgboost import XGBClassifier","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# define model\nmodel = XGBClassifier()\n# define evaluation procedure\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=1, random_state=1)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit(x, y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evals_result = model.score(x_test, y_test)\n\nevals_result","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}