{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This notebook work on using Voting ensemble technique to classify disaster tweets from non disaster tweets","metadata":{}},{"cell_type":"markdown","source":"The Classification steps:\n* Preprocessing\n    * remove urls, stopwords, punctuations, and small words\n    * translate emojis   \n* Feature Extraction and Analysis\n    * use unigram to explore frequent words on each class\n    * use word cloud as explanatory analysis for unigram output\n    * use bigram to explore frequent bigrams on each class\n    * use bar graph to visualize bigram results\n    * apply unigram analysis on both location and keyword columns\n    * use bar graph to visualize unigram of location and keyword columns\n    * format feature vector for both training and testing data\n* Model Implementation\n    * use Naive Bayes, Suppprt vector machine, K nearest neighbor, and logistic regression algorithms\n    * split training data (0.33 for testing) and test it on each algorithm \n    * apply cross validation on each algorithm\n    * use class report to evaluate each run\n    * apply voting ensemble technique on the four algorithms\n    * apply test feature vectors for each algorthim separately then on voting ensemble technique","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-28T15:48:07.70556Z","iopub.execute_input":"2021-07-28T15:48:07.705981Z","iopub.status.idle":"2021-07-28T15:48:07.716188Z","shell.execute_reply.started":"2021-07-28T15:48:07.705942Z","shell.execute_reply":"2021-07-28T15:48:07.715108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#load necessary libraries\nimport nltk\nfrom nltk.corpus import stopwords\nimport re, string\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\nfrom emoji import UNICODE_EMOJI\nimport emoji\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import train_test_split, cross_validate\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report","metadata":{"execution":{"iopub.status.busy":"2021-07-28T15:48:07.789319Z","iopub.execute_input":"2021-07-28T15:48:07.78967Z","iopub.status.idle":"2021-07-28T15:48:07.79681Z","shell.execute_reply.started":"2021-07-28T15:48:07.789639Z","shell.execute_reply":"2021-07-28T15:48:07.795596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#load train and test data to dataframes\ntrain_df=pd.read_csv('/kaggle/input/nlp-getting-started/train.csv',encoding=\"utf-8\")\ntest_df=pd.read_csv('/kaggle/input/nlp-getting-started/test.csv',encoding=\"utf-8\")\n#visulaize first top 10 columns of train data\ntrain_df.head(10)","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","execution":{"iopub.status.busy":"2021-07-28T15:48:07.821167Z","iopub.execute_input":"2021-07-28T15:48:07.8217Z","iopub.status.idle":"2021-07-28T15:48:07.872769Z","shell.execute_reply.started":"2021-07-28T15:48:07.821667Z","shell.execute_reply":"2021-07-28T15:48:07.87172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#visulaize first top 10 columns of test data\ntest_df.head(10)","metadata":{"execution":{"iopub.status.busy":"2021-07-28T15:48:07.924304Z","iopub.execute_input":"2021-07-28T15:48:07.924722Z","iopub.status.idle":"2021-07-28T15:48:07.938517Z","shell.execute_reply.started":"2021-07-28T15:48:07.924689Z","shell.execute_reply":"2021-07-28T15:48:07.936977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing","metadata":{}},{"cell_type":"code","source":"#preprocessing train data\n#extract hashtags\ntrain_df[\"hashtags\"]=train_df[\"text\"].apply(lambda x:re.findall(r\"#(\\w+)\",x.lower()))\ntest_df[\"hashtags\"]=test_df[\"text\"].apply(lambda x:re.findall(r\"#(\\w+)\",x.lower()))\n\n#translate emojis to text\ntrain_df[\"clean_text\"]=train_df[\"text\"].apply(lambda x: emoji.demojize(x))\ntest_df[\"clean_text\"]=test_df[\"text\"].apply(lambda x: emoji.demojize(x))\n\n#length feature\ntrain_df[\"len_text\"] = train_df[\"clean_text\"].apply(lambda x: len(x.split()))\ntest_df[\"len_text\"] = test_df[\"clean_text\"].apply(lambda x: len(x.split()))\n\n#remove urls\ntrain_df[\"clean_text\"]=train_df[\"clean_text\"].apply(lambda x: re.sub(r\"http:\\S+\",'',x))\ntrain_df[\"clean_text\"]=train_df[\"clean_text\"].apply(lambda x: re.sub(r\"https:\\S+\",'',x))\ntest_df[\"clean_text\"]=test_df[\"clean_text\"].apply(lambda x: re.sub(r\"http:\\S+\",'',x))\ntest_df[\"clean_text\"]=test_df[\"clean_text\"].apply(lambda x: re.sub(r\"https:\\S+\",'',x))\n\n#tokenize tweets\ntrain_df[\"clean_text\"]=train_df[\"clean_text\"].apply(lambda x: nltk.word_tokenize(x.strip().lower()))\ntest_df[\"clean_text\"]=test_df[\"clean_text\"].apply(lambda x: nltk.word_tokenize(x.strip().lower()))\n\n#remove punctuations from tweets\ntrain_df[\"clean_text\"]=train_df[\"clean_text\"].apply(lambda x: [re.sub(r'['+string.punctuation+']','',y.strip()) for y in x])\ntest_df[\"clean_text\"]=test_df[\"clean_text\"].apply(lambda x: [re.sub(r'['+string.punctuation+']','',y.strip()) for y in x])\n\n#load stopwords set\nstopwrds = set(stopwords.words('english'))\n#remove stop words from tweets\ntrain_df[\"clean_text\"]=train_df[\"clean_text\"].apply(lambda x: [y for y in x if (y.strip() not in stopwrds)])\ntest_df[\"clean_text\"]=test_df[\"clean_text\"].apply(lambda x: [y for y in x if (y.strip() not in stopwrds)])\n\n#remove new lines in tweets\ntrain_df[\"clean_text\"]=train_df[\"clean_text\"].apply(lambda x: [re.sub('\\\\n','',y.strip()) for y in x])\ntest_df[\"clean_text\"]=test_df[\"clean_text\"].apply(lambda x: [re.sub('\\\\n','',y.strip()) for y in x])\n\n#remove spaces and small words from tweets\ntrain_df[\"clean_text\"]=train_df[\"clean_text\"].apply(lambda x: [y.strip() for y in x if (y.strip() != \"\") and len(y.strip())>2])\ntest_df[\"clean_text\"]=test_df[\"clean_text\"].apply(lambda x: [y.strip() for y in x if (y.strip() != \"\") and len(y.strip())>2])\n\n#convert tokens of tweets to text\ntrain_df[\"clean_text\"]=train_df[\"clean_text\"].apply(lambda x: ' '.join(x))\ntest_df[\"clean_text\"]=test_df[\"clean_text\"].apply(lambda x: ' '.join(x))\n\n#convert tokens hashtags to text\ntrain_df[\"hashtags\"]=train_df[\"hashtags\"].apply(lambda x: ' '.join(x))\ntest_df[\"hashtags\"]=test_df[\"hashtags\"].apply(lambda x: ' '.join(x))\n\n#convert lower cases of keyword and location\ntrain_df[\"keyword\"]=train_df[\"keyword\"].apply(lambda x: x if str(x).lower() == \"nan\" else x.lower())\ntrain_df[\"location\"]=train_df[\"location\"].apply(lambda x: x if str(x).lower() == \"nan\" else x.lower())\ntest_df[\"keyword\"]=test_df[\"keyword\"].apply(lambda x: x if str(x).lower() == \"nan\" else x.lower())\ntest_df[\"location\"]=test_df[\"location\"].apply(lambda x: x if str(x).lower() == \"nan\" else x.lower())\n\n#visualize data \ntrain_df.head(10)","metadata":{"execution":{"iopub.status.busy":"2021-07-28T15:48:07.993778Z","iopub.execute_input":"2021-07-28T15:48:07.994137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Analysis ","metadata":{}},{"cell_type":"markdown","source":"**Data Statistics**","metadata":{}},{"cell_type":"code","source":"#total data length\nprint(\"length of train data\",len(train_df))\nprint(\"length of test data\",len(test_df))\n\n# unique location and keyword size of data\nprint(\"Checking train location column values\",len(train_df.location.unique()))\nprint(\"Checking train keyword column values\",len(train_df.keyword.unique()))\nprint(\"Checking test location column values\",len(test_df.location.unique()))\nprint(\"Checking test keyword column values\",len(test_df.keyword.unique()))\n\n#number of disaster tweets\nprint(\"disaster tweets\", len(train_df[train_df[\"target\"]==1]) )\nprint(\"non-disaster tweets\", len(train_df[train_df[\"target\"]==0]) )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Graphical analysis**","metadata":{}},{"cell_type":"code","source":"plt.subplots(1,2,figsize=(10,5))\n#visualize top 20 train unique keywords\nplt.subplot(1,2,1)\ntrain_df.keyword.value_counts()[:20].plot(kind=\"bar\",title=\"Unique Keywords\")\n\n#visualize top 20 train unique locations\nplt.subplot(1,2,2)\ntrain_df.location.value_counts()[:20].plot(kind=\"bar\",title=\"Unique Locations\")\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.subplots(1,2,figsize=(10,5))\n#visualize top 20 disaster tweets and their keywords bar graph\nplt.subplot(1,2,1)\ntrain_df[train_df[\"target\"]==1].keyword.value_counts()[:20].plot(kind=\"bar\",title=\"Disaster tweets keywords\")\n\n#visualize top 20 non disaster tweets and their keywords bar graph\nplt.subplot(1,2,2)\ntrain_df[train_df[\"target\"]==0].keyword.value_counts()[:20].plot(kind=\"bar\",title=\"Non-Disaster tweets keywords\")\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.subplots(1,2,figsize=(10,5))\n#visualize top 20 disaster tweets and their locations bar graph\nplt.subplot(1,2,1)\ntrain_df[train_df[\"target\"]==1].location.value_counts()[:20].plot(kind=\"bar\",title=\"Disaster tweets Locations\")\n\n#visualize top 20 non disaster tweets and their locations bar graph\nplt.subplot(1,2,2)\ntrain_df[train_df[\"target\"]==0].location.value_counts()[:20].plot(kind=\"bar\",title=\"Non-Disaster tweets Locations\")\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nsns.countplot(x = \"len_text\" ,data  = train_df[train_df[\"target\"]==1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(x = \"len_text\" ,data  = train_df[train_df[\"target\"]==0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Extraction","metadata":{}},{"cell_type":"code","source":"plt.subplots(1,2,figsize=(15,15))\nplt.subplot(1,2,1)\n#Uigram Frequency distribution for disaster tweets\n#convert disaster tweets into single string\ntxt=' '.join(train_df[train_df[\"target\"]==1][\"clean_text\"])\ndisaster_unigram=nltk.FreqDist(nltk.word_tokenize(txt))\n\n#visualize unigram frequency distribution for disaster tweets using wordcloud\ndisaster_wc = WordCloud(width=800, height=400, max_words=100).generate_from_frequencies(disaster_unigram)\nplt.title(\"Disaster Unigram Frequency Distribution\")\nplt.imshow(disaster_wc, interpolation=\"bilinear\")\n\nplt.subplot(1,2,2)\n#Uigram Frequency distribution for non disaster tweets\n#convert non disaster tweets into single string\ntxt=' '.join(train_df[train_df[\"target\"]==0][\"clean_text\"])\nnondisaster_unigram=nltk.FreqDist(nltk.word_tokenize(txt))\n\n#visualize unigram frequency distribution for non disaster tweets using wordcloud\nnondisaster_wc = WordCloud(width=800, height=400, max_words=100).generate_from_frequencies(nondisaster_unigram)\nplt.title(\"Non Disaster Unigram Frequency Distribution\")\nplt.imshow(nondisaster_wc, interpolation=\"bilinear\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.subplots(1,2,figsize=(15,10))\n\nplt.subplot(1,2,1)\n#Bigram Frequency distribution for disaster tweets\n#convert disaster tweets into single string\ntxt=' '.join(train_df[train_df[\"target\"]==1][\"clean_text\"])\ndisaster_bigram=nltk.FreqDist(nltk.bigrams(nltk.word_tokenize(txt)))\ntmplst=disaster_bigram.most_common(30)\n\n#visualize Bigram frequency distribution for disaster tweets using bar graph\nwrd,cnt=zip(*tmplst)\nwrd=[ x+\",\"+y for (x,y) in wrd]\nplt.barh(wrd,cnt)\nplt.title(\"Disaster Bigram BarGraph\")\n\nplt.subplot(1,2,2)\n#Bigram Frequency distribution for non disaster tweets\n#convert non disaster tweets into single string\ntxt=' '.join(train_df[train_df[\"target\"]==0][\"clean_text\"])\nnondisaster_bigram=nltk.FreqDist(nltk.bigrams(nltk.word_tokenize(txt)))\ntmplst=nondisaster_bigram.most_common(30)\n\n#visualize Bigram frequency distribution for non disaster tweets using bar graph\nwrd,cnt=zip(*tmplst)\nwrd=[ x+\",\"+y for (x,y) in wrd]\nplt.barh(wrd,cnt)\nplt.title(\"Non Disaster Bigram BarGraph\")\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.subplots(1,2,figsize=(15,15))\nplt.subplot(1,2,1)\n#Uigram Frequency distribution for disaster hashtags\n#convert disaster hashtags into single string\ntxt=' '.join(train_df[train_df[\"target\"]==1][\"hashtags\"])\ndisaster_unigram_hash=nltk.FreqDist(nltk.word_tokenize(txt))\n\n#visualize unigram frequency distribution for disaster hashtags using wordcloud\ndisaster_wc = WordCloud(width=800, height=400, max_words=100).generate_from_frequencies(disaster_unigram_hash)\nplt.title(\"Disaster Unigram Frequency Distribution hashtags\")\nplt.imshow(disaster_wc, interpolation=\"bilinear\")\n\nplt.subplot(1,2,2)\n#Uigram Frequency distribution for non disaster hashtags\n#convert non disaster hashtags into single string\ntxt=' '.join(train_df[train_df[\"target\"]==0][\"hashtags\"])\nnondisaster_unigram_hash=nltk.FreqDist(nltk.word_tokenize(txt))\n\n#visualize unigram frequency distribution for non disaster hashtags using wordcloud\nnondisaster_wc = WordCloud(width=800, height=400, max_words=100).generate_from_frequencies(nondisaster_unigram_hash)\nplt.title(\"Non Disaster Unigram Frequency Distribution hashtags\")\nplt.imshow(nondisaster_wc, interpolation=\"bilinear\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Convert Tweet to train feature vector**","metadata":{}},{"cell_type":"code","source":"#compute unigram feature vector for tweet likelihood to disaster\ntrain_df[\"unigram_disas\"]=train_df[\"clean_text\"].apply(lambda x: sum([disaster_unigram.get(wrd) for wrd in nltk.word_tokenize(x) if disaster_unigram.get(wrd)!=None])/len(disaster_unigram))\n\n#compute unigram feature vector for tweet likelihood to non disaster\ntrain_df[\"unigram_nondisas\"]=train_df[\"clean_text\"].apply(lambda x: sum([nondisaster_unigram.get(wrd) for wrd in nltk.word_tokenize(x) if nondisaster_unigram.get(wrd)!=None])/len(nondisaster_unigram))\n\n#compute unigram feature vector for hashtags likelihood to disaster\ntrain_df[\"unigram_disas_hash\"]=train_df[\"hashtags\"].apply(lambda x: sum([disaster_unigram_hash.get(wrd) for wrd in nltk.word_tokenize(x) if disaster_unigram_hash.get(wrd)!=None])/len(disaster_unigram_hash))\n\n#compute unigram feature vector for hashtags likelihood to non disaster\ntrain_df[\"unigram_nondisas_hash\"]=train_df[\"hashtags\"].apply(lambda x: sum([nondisaster_unigram_hash.get(wrd) for wrd in nltk.word_tokenize(x) if nondisaster_unigram_hash.get(wrd)!=None])/len(nondisaster_unigram_hash))\n\n#compute bigram feature vector for tweet likelihood to disaster\ntrain_df[\"bigram_disas\"]=train_df[\"clean_text\"].apply(lambda x: sum([disaster_bigram.get(wrd) for wrd in nltk.bigrams(nltk.word_tokenize(x)) if disaster_bigram.get(wrd)!=None])/len(disaster_bigram))\n\n#compute bigram feature vector for tweet likelihood to non disaster\ntrain_df[\"bigram_nondisas\"]=train_df[\"clean_text\"].apply(lambda x: sum([nondisaster_bigram.get(wrd) for wrd in nltk.bigrams(nltk.word_tokenize(x)) if nondisaster_bigram.get(wrd)!=None])/len(nondisaster_bigram))\n\nkey_disas=nltk.FreqDist(train_df[train_df[\"target\"]==1][\"keyword\"])\n#compute unigram keyword to disaster\ntrain_df[\"key_disas\"]=train_df[\"keyword\"].apply(lambda x: sum([key_disas.get(x) if (x in key_disas.keys() and str(x).lower()!=\"nan\") else 0])/len(key_disas))\n\nkey_nondisas=nltk.FreqDist(train_df[train_df[\"target\"]==0][\"keyword\"])\n#compute unigram keyword to non disaster\ntrain_df[\"key_nondisas\"]=train_df[\"keyword\"].apply(lambda x: sum([key_nondisas.get(x) if (x in key_nondisas.keys() and str(x).lower() != \"nan\") else 0])/len(key_nondisas))\n\nloc_disas=nltk.FreqDist(train_df[train_df[\"target\"]==1][\"location\"])\n#compute unigram location to disaster\ntrain_df[\"loc_disas\"]=train_df[\"location\"].apply(lambda x: sum([loc_disas.get(x) if (x in loc_disas.keys() and str(x).lower()!=\"nan\") else 0])/len(loc_disas))\n\nloc_nondisas=nltk.FreqDist(train_df[train_df[\"target\"]==0][\"location\"])\n#compute unigram location to non disaster\ntrain_df[\"loc_nondisas\"]=train_df[\"location\"].apply(lambda x: sum([loc_nondisas.get(x) if (x in loc_nondisas.keys() and str(x).lower() != \"nan\") else 0])/len(loc_nondisas))\ntrain_df.head(5)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#define feature vectors for training dataset\ntrain_feature_vectors = train_df[['unigram_disas', 'unigram_nondisas', 'unigram_disas_hash', 'unigram_nondisas_hash', 'bigram_disas',\n          'bigram_nondisas', 'key_disas', 'key_nondisas', 'loc_disas','loc_nondisas']]\ntrain_feature_vectors.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Convert Tweet to test feature vector**","metadata":{}},{"cell_type":"code","source":"#compute unigram feature vector for tweet likelihood to disaster\ntest_df[\"unigram_disas\"]=test_df[\"clean_text\"].apply(lambda x: sum([disaster_unigram.get(wrd) for wrd in nltk.word_tokenize(x) if disaster_unigram.get(wrd)!=None])/len(disaster_unigram) )\n\n#compute unigram feature vector for tweet likelihood to non disaster\ntest_df[\"unigram_nondisas\"]=test_df[\"clean_text\"].apply(lambda x: sum([nondisaster_unigram.get(wrd) for wrd in nltk.word_tokenize(x) if nondisaster_unigram.get(wrd)!=None])/len(nondisaster_unigram))\n\n#compute unigram feature vector for hashtags likelihood to disaster\ntest_df[\"unigram_disas_hash\"]=test_df[\"hashtags\"].apply(lambda x: sum([disaster_unigram_hash.get(wrd) for wrd in nltk.word_tokenize(x) if disaster_unigram_hash.get(wrd)!=None])/len(disaster_unigram_hash))\n\n#compute unigram feature vector for hashtags likelihood to non disaster\ntest_df[\"unigram_nondisas_hash\"]=test_df[\"hashtags\"].apply(lambda x: sum([nondisaster_unigram_hash.get(wrd) for wrd in nltk.word_tokenize(x) if nondisaster_unigram_hash.get(wrd)!=None])/len(nondisaster_unigram_hash))\n\n#compute bigram feature vector for tweet likelihood to disaster\ntest_df[\"bigram_disas\"]=test_df[\"clean_text\"].apply(lambda x: sum([disaster_bigram.get(wrd) for wrd in nltk.bigrams(nltk.word_tokenize(x)) if disaster_bigram.get(wrd)!=None])/len(disaster_bigram) if x.strip()!='' else 0)\n\n#compute bigram feature vector for tweet likelihood to non disaster\ntest_df[\"bigram_nondisas\"]=test_df[\"clean_text\"].apply(lambda x: sum([nondisaster_bigram.get(wrd) for wrd in nltk.bigrams(nltk.word_tokenize(x)) if nondisaster_bigram.get(wrd)!=None])/len(nondisaster_bigram) if x.strip()!='' else 0)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#compute unigram keyword to disaster\ntest_df[\"key_disas\"]=test_df[\"keyword\"].apply(lambda x: sum([key_disas.get(x) if (x in key_disas.keys() and str(x).lower()!=\"nan\") else 0])/len(key_disas))\n\n#compute unigram keyword to non disaster\ntest_df[\"key_nondisas\"]=test_df[\"keyword\"].apply(lambda x: sum([key_nondisas.get(x) if (x in key_nondisas.keys() and str(x).lower() != \"nan\") else 0])/len(key_nondisas))\n\n#compute unigram location to disaster\ntest_df[\"loc_disas\"]=test_df[\"location\"].apply(lambda x: sum([loc_disas.get(x) if (x in loc_disas.keys() and str(x).lower()!=\"nan\") else 0])/len(loc_disas))\n\n#compute unigram location to non disaster\ntest_df[\"loc_nondisas\"]=test_df[\"location\"].apply(lambda x: sum([loc_nondisas.get(x) if (x in loc_nondisas.keys() and str(x).lower() != \"nan\") else 0])/len(loc_nondisas))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#define feature vectors for testing dataset\ntest_feature_vectors = test_df[['unigram_disas', 'unigram_nondisas', 'unigram_disas_hash', 'unigram_nondisas_hash', 'bigram_disas',\n          'bigram_nondisas', 'key_disas', 'key_nondisas', 'loc_disas','loc_nondisas']]\ntest_feature_vectors.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Building & Training","metadata":{}},{"cell_type":"code","source":"#split train data \nY=train_df['target']\nX_train, X_test, y_train, y_test = train_test_split(train_feature_vectors, Y, test_size=0.33, random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Naive Bayes Classifier\nnb_clf = GaussianNB()\n#train classifier on train data after splitting\nnb_clf.fit(X_train,y_train)\nprint(nb_clf.get_params())\nprint(\"split training score\",nb_clf.score(X_test,y_test))\n\n#train over all trained data applying cross validation\nprint(\"NB cross validation scores\",cross_validate(nb_clf,train_feature_vectors,Y,cv=5))\nprint(classification_report(Y, nb_clf.predict(train_feature_vectors)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Support Vector Machine\nsvm_clf = SVC(probability=True)\n#train classifier on train data after splitting\nsvm_clf.fit(X_train,y_train)\nprint(svm_clf.get_params())\nprint(\"split training score\",svm_clf.score(X_test,y_test))\n\n#train over all trained data applying cross validation\nprint(\"SVM cross validation scores\",cross_validate(svm_clf,train_feature_vectors,Y,cv=5))\nprint(classification_report(Y, svm_clf.predict(train_feature_vectors)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#K nearest neighbor\nknn_clf = KNeighborsClassifier()\n#train classifier on train data after splitting\nknn_clf.fit(X_train,y_train)\nprint(knn_clf.get_params())\nprint(\"split training score\",knn_clf.score(X_test,y_test))\n\n#train over all trained data applying cross validation\nprint(\"KNN cross validation scores\",cross_validate(knn_clf,train_feature_vectors,Y,cv=5))\nprint(classification_report(Y, knn_clf.predict(train_feature_vectors)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Logistic Regression\nlogReg_clf = LogisticRegression()\n#train classifier on train data after splitting\nlogReg_clf.fit(X_train,y_train)\nprint(logReg_clf.get_params())\nprint(\"split training score\",logReg_clf.score(X_test,y_test))\n\n#train over all trained data applying cross validation\nprint(\"LR cross validation score\",cross_validate(logReg_clf,train_feature_vectors,Y,cv=5))\nprint(classification_report(Y, logReg_clf.predict(train_feature_vectors)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Testing","metadata":{}},{"cell_type":"code","source":"#Naive Bayes Model on test features\nnb_clf.predict(test_feature_vectors)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#SVM Model on test features\nsvm_clf.predict(test_feature_vectors)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#KNN Model on test features\nknn_clf.predict(test_feature_vectors)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Logistic regression model on test features\nlogReg_clf.predict(test_feature_vectors)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Voting Classifier on NB,SVM,LR,and KNN models","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import VotingClassifier\nvoting_clf = VotingClassifier(estimators=[('NB', nb_clf), ('SVM', svm_clf), ('KNN', knn_clf),('LogReg',logReg_clf)], voting='soft')\n#train over all trained data applying cross validation\nvoting_clf.fit(train_feature_vectors,Y)\nprint(\"Voting score\",voting_clf.score(train_feature_vectors,Y))\nprint(classification_report(Y, voting_clf.predict(train_feature_vectors)))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#testing model on test feature vectors\nvals=voting_clf.predict(test_feature_vectors)\n\n#save in submission dataframe\nsubmission=pd.read_csv('/kaggle/input/nlp-getting-started/sample_submission.csv')\nsubmission['id']=test_df['id']\nsubmission['target']=vals\nsubmission.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv('sample_submission.csv',index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}