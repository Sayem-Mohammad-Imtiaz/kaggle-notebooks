{"cells":[{"metadata":{},"cell_type":"markdown","source":"# In this Notebook, I have build a Model by finetuning OpenAI's GPT2 on Hillary Clinton's Tweet Data during US Presidential Election 2016. This Model generates artificial Tweet in the style of Hillary Clinton. Please do upvote this notebook if you liked the content. Thanks!!!"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"!pip install transformers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"!pip install simpletransformers==0.32.3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nfrom nltk.corpus import stopwords\nimport re\nfrom wordcloud import WordCloud, STOPWORDS \nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom transformers import (GPT2Config,GPT2LMHeadModel,GPT2Tokenizer)\nimport torch\nfrom string import punctuation as pnc\nfrom collections import Counter\nimport gc\npd.set_option('display.max_colwidth', -1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tweets = pd.read_csv('/kaggle/input/clinton-trump-tweets/tweets.csv')\ndisplay(tweets.head(1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tweets = tweets[['handle','text','is_retweet']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Number of Tweets : \",len(tweets))\nprint(\"Null Count in the 3 columns : \")\nprint(tweets.isna().sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Number of Tweets from Doland and Hillary : \")\ntweets['handle'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tweets['tweetLen'] = tweets['text'].apply(lambda x : len(x.split(\" \")))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"doland_tweets = tweets[tweets['handle']=='realDonaldTrump']\nprint(\"Doland Tweets : \")\ndisplay(doland_tweets['text'].head(5))\nhillary_tweets = tweets[tweets['handle']=='HillaryClinton']\nprint(\"Hillary Tweets : \")\ndisplay(hillary_tweets['text'].head(5))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Number of Words in Doland Vs Hillary Tweets"},{"metadata":{"trusted":true},"cell_type":"code","source":"doland_tweets['tweetLen'].hist(bins=32)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hillary_tweets['tweetLen'].hist(bins=32)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Word Cloud across Doland Vs Hillary Tweets"},{"metadata":{"trusted":true},"cell_type":"code","source":"def getWordCloud(df,col):\n  comment_words = '' \n  stopwords = set(STOPWORDS) \n    \n  for val in df[col]: \n        \n      val = str(val) \n      tokens = val.split() \n        \n      for i in range(len(tokens)): \n          tokens[i] = tokens[i].lower() \n        \n      comment_words += \" \".join(tokens)+\" \"\n    \n  wordcloud = WordCloud(width = 800, height = 800, \n                  background_color ='white', \n                  stopwords = stopwords, \n                  min_font_size = 10).generate(comment_words) \n    \n                       \n  plt.figure(figsize = (5, 5), facecolor = None) \n  plt.imshow(wordcloud) \n  plt.axis(\"off\")\n  plt.tight_layout(pad = 0) \n    \n  plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"getWordCloud(doland_tweets,'text')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"getWordCloud(hillary_tweets,'text')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Most of the Tweets have a URL in it. \"https\" occurs in both Doland and Hillary Tweet Word Cloud"},{"metadata":{},"cell_type":"markdown","source":"## Let's see which handles did Doland and Hillary tag in their Tweets"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Prints only the top 20 frequently occured Twitter handles\ndef getTwitterHandlesTagged(df, col):\n    taggedHandlesList = []\n    for tweet in df[col].tolist():\n        taggedHandles = [x for x in tweet.split(\" \") if x.startswith('@')]\n        taggedHandlesList = taggedHandlesList + taggedHandles\n    \n    print(list({k: v for k, v in sorted(dict(Counter(taggedHandlesList)).items(), key=lambda item: item[1], reverse = True)}.items())[:20])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"getTwitterHandlesTagged(doland_tweets, 'text')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"getTwitterHandlesTagged(hillary_tweets, 'text')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Let's see which Tags did Doland and Hillary use in their Tweets"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Prints only the top 20 frequently occured tags\ndef getTags(df, col):\n    tagsList = []\n    for tweet in df[col].tolist():\n        tags = [x for x in tweet.split(\" \") if x.startswith('#')]\n        tagsList = tagsList + tags\n    print(list({k: v for k, v in sorted(dict(Counter(tagsList)).items(), key=lambda item: item[1], reverse = True)}.items())[:20])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"getTags(doland_tweets, 'text')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"getTags(hillary_tweets, 'text')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Let's see tweets where Hillary mentioned herself"},{"metadata":{"trusted":true},"cell_type":"code","source":"hillary_tweets['textLwr'] = hillary_tweets['text'].str.lower()\nhillary_tweets['hasHillaySubString'] = hillary_tweets['textLwr'].str.contains('hillary')\ndisplay(hillary_tweets[hillary_tweets['hasHillaySubString'] == True]['text'].head(10))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Most of the above tweets are in quotes and has \"-Hillary\" at the end"},{"metadata":{},"cell_type":"markdown","source":"## Let's see the Quote Authors in Hillary's Tweet"},{"metadata":{"trusted":true},"cell_type":"code","source":"def getQuoteAuthor(df, col):\n    quoteAuthorList = []\n    for tweet in df[col].tolist():\n        quoteAuthor = [x for x in tweet.split(\" \") if x.startswith('—')]\n        quoteAuthorList = quoteAuthorList + quoteAuthor\n    print(list({k: v for k, v in sorted(dict(Counter(quoteAuthorList)).items(), key=lambda item: item[1], reverse = True)}.items())[:20])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"getQuoteAuthor(hillary_tweets, 'text')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"getQuoteAuthor(doland_tweets, 'text')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Preprocessing the Tweets. I have removed the New Line Characters, URLs, Tagged handles,Tags and Quote Authors to preprocess Hillary Tweets such that these frequent Tags, Tagged Handles, Quote Authors and random URL does not get generated every time."},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"def removeTagTaggedHandlesQuoteAuthor(text):\n    text = \" \".join([x for x in text.split(\" \") if not x.startswith(\"@\")])\n    text = \" \".join([x for x in text.split(\" \") if not x.startswith(\"#\")])\n    text = \" \".join([x for x in text.split(\" \") if not x.startswith(\"—\")])\n    return text\n    \nhillary_tweets['preProcessedText'] = hillary_tweets['text'].str.replace('http\\S+|www.\\S+', '', case=False)\nhillary_tweets['preProcessedText'] = hillary_tweets['preProcessedText'].str.replace('\\n','')\nhillary_tweets['preProcessedText'] = hillary_tweets['preProcessedText'].apply(removeTagTaggedHandlesQuoteAuthor)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Split in Training and Validation DataSet and save as text file. For now we will only generate artificial Tweets in Hillary Style."},{"metadata":{"trusted":true},"cell_type":"code","source":"hillary_preprocessedtweets = hillary_tweets['preProcessedText']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hillary_preprocessedtweets_train, hillary_preprocessedtweets_eval = train_test_split(hillary_preprocessedtweets,test_size = 0.05)\nprint(\"Number of tweets in training data  : \",len(hillary_preprocessedtweets_train))\nprint(\"Number of tweets in validation data : \",len(hillary_preprocessedtweets_eval))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hillary_preprocessedtweets_train.to_csv('/kaggle/working/hillary_preprocessedtweets_train.txt', header=None, index=None, sep=' ')\nhillary_preprocessedtweets_eval.to_csv('/kaggle/working/hillary_preprocessedtweets_eval.txt', header=None, index=None, sep=' ')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training the Model. We will finetune GPT2 Model(Simple Transformer) using the Hillary's Tweets."},{"metadata":{"trusted":true},"cell_type":"code","source":"from simpletransformers.language_modeling import LanguageModelingModel\nimport logging\n\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger()\nlogger.warning(\"Is this working?\") \ntransformers_logger = logging.getLogger(\"transformers\")\ntransformers_logger.setLevel(logging.WARNING)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"args = {\n    \"reprocess_input_data\": True,\n    \"overwrite_output_dir\": True,\n    \"num_train_epochs\": 10,\n    \"train_batch_size\": 32,\n    \"mlm\": False,\n    \"dataset_type\" : \"simple\",\n    \"block_size\" : 24,\n    \"max_seq_length\" : 24,\n    \"evaluate_during_training\": True,\n    \"evaluate_during_training_steps\": 50,\n    \"evaluate_during_training_verbose\": True,\n    \"use_cached_eval_features\": True,\n    \"save_eval_checkpoints\" : False,\n    \"save_model_every_epoch\" : False,\n    \"early_stopping_patience\" : 2,\n    \"use_early_stopping\" : True,\n    \"save_optimizer_and_scheduler \" : False,\n    \"fp16\" : False\n}\n\nhillary_model = LanguageModelingModel(\n    'gpt2', \n    'gpt2',\n    args=args,\n    use_cuda=True,\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"print(\"Get Value of all the hyperparameters  : \")\nfor key in hillary_model.args:\n  print(key, '->', hillary_model.args[key])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"hillary_model.train_model(\"hillary_preprocessedtweets_train.txt\", eval_file=\"hillary_preprocessedtweets_eval.txt\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### The Best Model Weight is stored in /kaggle/working/outputs/best_model/pytorch_model.bin"},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls ./outputs/best_model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Loading the best model weight in GPT2LMHeadModel"},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"config_class, model_class, tokenizer_class = GPT2Config, GPT2LMHeadModel, GPT2Tokenizer\nBestModel = model_class.from_pretrained('gpt2')\nBestModel.load_state_dict(torch.load(\"./outputs/best_model/pytorch_model.bin\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Generate Text. We have to feed a sequence and the model will generate additional sequences in context of the US 2016 presidential election tweets by Hillary. Here I have taken only 2 additional sentences as output."},{"metadata":{"trusted":true},"cell_type":"code","source":"prompt_texts = [\"I will reduce Gun violence.\",\"Donald will build a wall\",\"I will make our health care system better\",\"Come rally with us\",\"America is in financial stress\",\"We have to preserve secularism\",\"We will win the election\"]\ntokenizer = tokenizer_class.from_pretrained('gpt2')\nfor prompt_text in prompt_texts:\n  encoded_prompt = tokenizer.encode(prompt_text, add_special_tokens=False, return_tensors=\"pt\")\n  generated = BestModel.generate(encoded_prompt,max_length = 128, num_beams = 2, repetition_penalty = 5.0,verbose=False)\n  generated = generated.tolist()[0]\n  text = tokenizer.decode(generated, clean_up_tokenization_spaces=True)\n  print(\".\".join(text.split(\".\")[:3]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# The Model is working in most of the cases. But sometimes it might not be coherent with the input sequence. It might get better if we finetune on a larger data set."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}