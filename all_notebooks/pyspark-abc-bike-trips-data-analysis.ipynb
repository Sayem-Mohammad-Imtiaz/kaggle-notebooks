{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Brief introduction to PySpark\n\n<br>\n> **Contents**\n1. Spark workflow and data structure\n2. Common SQL / PySpark functions and keywords comparison\n3. Window functions\n4. Data exploration of Ford GoBike dataset\n5. ETL example\n\n<br>\n---------------------------------------\n## 1. Spark workflow, RDD, DataFrame\n\nSpark is a unified analytics engine for large-scale data processing. It is built on a paradigm of functional programming - operations (transformations) in the pipeline are \"lazy\", they are not executed and applyied immediatelly, they are \"delayed\" until a result is needed. \n<br>\nFundamental data structure in Spark is RDD (Resilient Distributed Dataset). RDDs are spread across many machines in the cluster. <br>\nDataFrame is based on RDD, it is a distributed collection of data organized into named columns like a table in relational database. <br>If you want to learn more, everything is covered in detail at [databricks blog](https://databricks.com/blog/2016/07/14/a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets.html).\n<br>\n<br>\n**RDD characteristics:**\n* immutable\n* in-memory\n* lazy evaluated\n* parallel\n* structured and unstructured data\n* two types of operations: transformations and actions\n\n**DataFrame characteristics:** \n* immutable\n* in-memory\n* resilient\n* distributed\n* parallel\n* structured\n* allows SQL/Hive queries\n\nFirst we need to install pyspark and import all dependencies. Remember that Spark works on Java 8, so you need to install this particular distribution on your machine as well."},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"!pip install pyspark","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pyspark\nfrom pyspark.sql import SparkSession, Row, functions as f\nfrom pyspark.sql.window import Window\nfrom pyspark.sql.types import StringType, TimestampType, DoubleType, FloatType, IntegerType, LongType, StructField, StructType\nfrom IPython.display import Image\n\nimport random\nrandom.seed(1990)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"`SparkSession` class is an entry point for any Spark application. It allows you to interact with Spark API. \n<br>\n`getOrCreate()` returns a new Spark app or points to already existing one."},{"metadata":{"trusted":true},"cell_type":"code","source":"spark = SparkSession.builder.appName(\"spark_app\").getOrCreate()\nspark","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's generate first DataFrame with some random data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Regions\ngeo_id = [random.choice([\"regA\",\"regB\",\"regC\",\"regD\",\"regE\"]) for x in range(500)]\n\n# Products\nprod_id = [random.choice([\"prodA\",\"prodB\",\"prodC\",\"prodD\",\"prodE\",\"prodF\",\n                          \"prodG\",\"prodH\",\"prodI\",\"prodJ\",\"prodK\",\"prodL\"]) for x in range(500)]\n\n# Values\nvalue = [random.uniform(1000,10000) for x in range(500)]\nvalue[5] = None\nvalue[15] = None\nvalue[245] = None","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = spark.createDataFrame([Row(prod=p, geo=g, val=v) for p,g,v in zip(prod_id, geo_id, value)])\ndf.show(7)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.createOrReplaceTempView(\"train_df\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"geo_df = spark.createDataFrame([Row(geo_id = \"regA\", geo_name = \"Europe\"),\n                                Row(geo_id = \"regB\", geo_name = \"Asia\"),\n                                Row(geo_id = \"regC\", geo_name = \"N_America\"),\n                                Row(geo_id = \"regD\", geo_name = \"S_America\"),\n                                Row(geo_id = \"regE\", geo_name = \"Africa\")])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"geo_df.createOrReplaceTempView(\"geo_df\")\ngeo_df.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<br>\n## 2. SQL / Spark functions comparison"},{"metadata":{"trusted":true},"cell_type":"code","source":"# if True instead of Spark syntax a SQL equivalent will be executed\nsql = False","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# SQL and Spark equivalent:\nspark.sql(\"SELECT prod FROM train_df\").show(7) if sql else \\\ndf.select(\"prod\").show(7)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prod_ids = [\"prodA\",\"prodB\",\"prodC\",\"prodD\",\"prodE\",\"prodF\", \"prodG\",\"prodH\",\"prodI\",\"prodJ\",\"prodK\",\"prodL\"]\nprod_names = [\"smarfone\", \"PC\", \"laptop\", \"headphones\", \"tv\", \"speaker\", \n              \"keyboard\", \"mouse\", \"charger\", \"powerbank\", \"microphone\", \"camera\"]\n\nprod_df = spark.createDataFrame([Row(prod_id = i, prod_name = n) for i,n in zip(prod_ids, prod_names)])\nprod_df.createOrReplaceTempView(\"prod_df\")\nprod_df.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# SQL example of INNER JOIN\nspark.sql(\"SELECT geo_df.geo_name, prod_df.prod_name, train_df.val FROM train_df \\\n            INNER JOIN prod_df ON prod_df.prod_id = train_df.prod \\\n            INNER JOIN geo_df ON geo_df.geo_id = train_df.geo\").show(7)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Spark join() is also 'inner' by default\ndf.groupBy(\"prod\").agg(f.round(f.sum(\"val\"), 2).alias(\"total value\"))\\\n                  .sort(\"total value\")\\\n                  .join(prod_df, df.prod == prod_df.prod_id)\\\n                  .select(\"prod_name\", \"total value\")\\\n                  .show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# WHERE / where()\n# SQL and Spark equivalent:\nspark.sql(\"SELECT * FROM train_df WHERE prod != 'prodA' AND val > 9900\").show() if sql else \\\ndf.where(df[\"prod\"] != \"prodA\").where(f.col(\"val\") > 9900).show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# LIKE / like()\n# SQL and Spark equivalent:\nspark.sql(\"select * from train_df where prod like '%A'\").show(7) if sql else \\\ndf.where(df.prod.like('%A')).show(7)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# orderBy()\ndf.groupBy([\"prod\",\"geo\"]).sum().orderBy(\"geo\").show(7)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# SUM, AVG, COUNT / sum(), avg(), count()\n# SQL and Spark equivalent:\nq = (\"SELECT prod, geo, SUM(val) val_sum, AVG(val) val_avg, COUNT(*), COUNT(val) FROM train_df GROUP BY prod, geo\")\nspark.sql(q).show(7) if sql else \\\ndf.groupBy([\"prod\",\"geo\"]).agg(f.sum(\"val\").alias(\"val_sum\"), f.avg(\"val\").alias(\"val_avg\"), \n                               f.count(\"*\"), f.count(\"val\")).show(7)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Select unique combinations\n# DISTINCT / distinct()\nspark.sql(\"SELECT DISTINCT prod, geo FROM train_df\").show(5) if sql else \\\ndf.select(\"prod\", \"geo\").distinct().show(5)\n\n# alternative\n# df.dropDuplicates([\"prod\", \"geo\"]).show(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop rows with any null values\n# SQL and Spark equivalent:\nspark.sql(\"select * from train_df where val is not null\").count() if sql else \\\ndf.dropna(\"any\").count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Show rows with null values in val column\ndf.where(df.val.isNull()).show()\n# df.where(f.isnull(\"val\")).show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Replace null values with 1\n# SQL and Spark equivalent:\nq = \"SELECT prod, geo, IF(val is null, 1, val) AS val FROM train_df\"\nspark.sql(q).show() if sql else \\\ndf.fillna(1).show(7)\n# df.fillna({\"val\": 1}).show(7)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Replacing values with REGEXP_REPLACE / replace()\n# SQL and Spark equivalent:\nspark.sql(\"SELECT geo, REGEXP_REPLACE(prod, 'prodE', 'Product E') AS prod, val FROM train_df\").show(3) if sql else \\\ndf.replace(\"prodE\", \"Product E\").show(3)\n\n# or with dictionary\n# df.replace({\"prodA\": \"Product A\", \"prodB\": \"Product B\"}).show(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Rename columns\n# SQL and Spark equivalent:\nspark.sql(\"SELECT prod, geo, val AS volume FROM train_df\").show(3) if sql else \\\ndf.withColumnRenamed(\"val\", \"volume\").show(3)\n\n# df.select(df.val.alias(\"volume\")).show(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# New column\n# SQL and Spark equivalent:\nspark.sql(\"SELECT *, val/1000 AS minival FROM train_df\").show(3) if sql else \\\ndf.withColumn(\"minival\", df[\"val\"] / 1000).show(3)\n# df.select(\"*\", (df.val/1000).alias(\"minival)).show(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# CASE WHEN THEN/ when() otherwise()\n# SQL and Spark equivalent:\nq = \"SELECT prod, CASE WHEN val > 7500 THEN 1 WHEN val < 2500 THEN 3 ELSE 2 END AS out FROM train_df\"\nspark.sql(q).show(5) if sql else \\\ndf.select(df.prod, f.when(df.val > 7500, 1).when(df.val < 2500, 3).otherwise(2).alias(\"out\")).show(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# SUBSTRING / substring() \n# SQL and Spark equivalent:\nspark.sql(\"SELECT SUBSTRING(prod, 4, 2) AS id FROM train_df\").show(5) if sql else \\\ndf.select(f.substring(\"prod\", 4, 2).alias(\"id\")).show(5)\n# or\n# df.select(df.prod.substr(4, 2).alias(\"id\")).show(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"<br>\n## 3. Window functions\nWindow function calculates a return value for every input row of a table based on a group of rows. "},{"metadata":{"trusted":true},"cell_type":"code","source":"windowSpec = Window.partitionBy('prod')\n\n# SQL and Spark equivalent:\nspark.sql(\"SELECT prod, val, SUM(val) OVER (PARTITION BY prod) AS prod_val FROM train_df\").show(3) if sql else \\\ndf.select(\"prod\", \"val\", f.sum(\"val\").over(windowSpec).alias(\"prod_val\")).show(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"windowSpec = Window.partitionBy(\"prod\").orderBy(\"geo\")\n\n# rank() function returns rank value, if it's the same it returns the same score\n# then the number of row index\n# dense_rank() returns rank value in natural order\ndf.withColumn(\"ranked\", f.rank().over(windowSpec))\\\n  .withColumn(\"ranked_dense\", f.dense_rank().over(windowSpec))\\\n  .withColumn(\"row_number\", f.row_number().over(windowSpec)).show(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"windowSpec = Window.partitionBy(\"prod\").orderBy(\"val\")\n\n# Cumulative sum\ndf.withColumn(\"sum_from_start\", f.sum(df.val).over(windowSpec)).show(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Stop the app\nspark.stop()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<br>\n## 4. Bike trips data exploration\n\nBike rental systems is a growing part of mobility market. I will analyze Lyft bikeshare system data from 2017 and 2018. <br>\nThe data is available at https://www.lyft.com/bikes/bay-wheels/system-data \n<br>\n<br>\nWe need to merge multiple csv files to one Spark DataFrame.\n<br>\n-------------------------------------------------------------------------------------\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"spark = SparkSession.builder.appName('bike_app').master(\"local[*]\").getOrCreate()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Merge all 2018 csv files to one DataFrame\nfiles = \"../input/ford-gobike-data/2018*.csv\"\ngoBike = spark.read.csv(files, header=True, inferSchema=True)\nprint(f\"Total Records = {goBike.count()}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Add 2017 data\ngoBike = goBike.drop(\"bike_share_for_all_trip\")\ngoBike = goBike.unionAll(spark.read.csv(\"../input/ford-gobike-data/2017-fordgobike-tripdata.csv\", header=True, inferSchema=True))\nprint(f\"Total Records = {goBike.count()}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"># [↑ Almost 2 mln records]()"},{"metadata":{"trusted":true},"cell_type":"code","source":"goBike.printSchema()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Exemplary record\ngoBike.show(1, vertical=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Some data is missing\ngoBike.where(goBike.member_birth_year.isNull()).count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 1. Remove rows with null values\ngoBike = goBike.dropna(\"any\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 2. Distribution of \"member_gender\" variable\ngoBike.groupBy(\"member_gender\").count().show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":">About 75% of users are men\n># [Men: 3x more often]()"},{"metadata":{"trusted":true},"cell_type":"code","source":"# 3. Count min, max, avg age of customers\ngoBike.select((2020 - goBike[\"member_birth_year\"]).alias(\"age\")).describe()\\\n      .select(\"summary\", f.round(\"age\", 0).alias(\"age\")).show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Age of the oldest cyclist in the dataset. Probably a random number entered in registry. \n> # [139 years old (ಠ_ಠ)]()"},{"metadata":{"trusted":true},"cell_type":"code","source":"# 4. Count total number of unique bikes\ngoBike.select(\"bike_id\").distinct().count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 5. Count total number of unique stations\ngoBike.select(\"start_station_id\").union(goBike.select(\"end_station_id\")).distinct().count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 6. Check bike with shortest and longest rental time\ngoBike.groupBy(\"bike_id\").agg(f.sum(\"duration_sec\").alias(\"total_time\")).orderBy(\"total_time\").show(1)\ngoBike.groupBy(\"bike_id\").agg(f.sum(\"duration_sec\").alias(\"total_time\")).orderBy(f.desc(\"total_time\")).show(1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> The longest journey\n> # [11 days]()"},{"metadata":{"trusted":true},"cell_type":"code","source":"# 7. Calculate average time of single rental\ngoBike.select(f.avg(\"duration_sec\").alias(\"average\")).show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Average rental\n> # [13 minutes]()"},{"metadata":{"trusted":true},"cell_type":"code","source":"# 8. Find stations with the most traffic between them\ngoBike.select(f.when(goBike[\"start_station_id\"] > goBike[\"end_station_id\"], \n                     f.array(goBike[\"start_station_id\"], goBike[\"end_station_id\"]))\\\n                     .otherwise(f.array(goBike[\"end_station_id\"], goBike[\"start_station_id\"]))\\\n                     .alias(\"route\"))\\\n      .groupBy(\"route\")\\\n      .count()\\\n      .orderBy(f.desc(\"count\"))\\\n      .show(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Show stations from route above\ngoBike.filter((goBike.start_station_id == 6) | (goBike.start_station_id == 15)) \\\n      .select(\"start_station_name\").distinct().show(truncate=False)\n# Source: google maps\nImage(\"../input/images/popular_route.JPG\", width=800)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 9. Find rush hour\ngoBike.select(f.hour(\"start_time\").alias(\"hour\"))\\\n.groupBy(\"hour\").count().orderBy(f.desc(\"count\")).show(7)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":">Rush hours during day\n># [8am & 5pm]()"},{"metadata":{"trusted":true},"cell_type":"code","source":"# 10. Find average rentals grouped by weekday\ngoBike.select(f.date_format(\"start_time\", \"dd.MM.yyyy\").alias(\"date\"), \\\n              f.date_format(\"start_time\", \"E\").alias(\"weekday\")) \\\n      .groupBy(\"date\", \"weekday\").count() \\\n      .groupBy(\"weekday\").agg(f.avg(\"count\").alias(\"avg_use\")) \\\n      .orderBy(\"avg_use\").show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":">Most popular day for a ride\n># [Tuesday]()"},{"metadata":{"trusted":true},"cell_type":"code","source":"# 11. Calculate average distance between stations for all trips\nfrom math import radians, cos, sin, asin, sqrt\nfrom pyspark.sql.types import FloatType\n\n# Credit: Michael Dunn https://stackoverflow.com/a/4913653\ndef haversine(lon1, lat1, lon2, lat2):\n    \"\"\"\n    Calculate the great circle distance between two points \n    on the earth (specified in decimal degrees)\n    \"\"\"\n    # convert decimal degrees to radians \n    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n\n    # haversine formula \n    dlon = lon2 - lon1 \n    dlat = lat2 - lat1 \n    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n    c = 2 * asin(sqrt(a)) \n    r = 6371 # Radius of earth in kilometers\n    return c * r\n\nhaversine_udf = f.udf(haversine, FloatType())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"goBike.select(haversine_udf(\"start_station_longitude\", \"start_station_latitude\", \\\n                            \"end_station_longitude\", \"end_station_latitude\").alias(\"distance\")) \\\n.agg(f.avg(\"distance\").alias(\"avg distance [km]\")).show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":">Avarege distance of a single bike trip\n># [1.59 km]()\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"spark.stop()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5. ETL pipeline example\n\nNow based on our data let's create exemplary ETL process:\n* Extract: Load the data from source to a DataFrame with defined structure.\n* Transform: Create new DataFrame with daily data\n* Load: Save transformed frame to file or pandas\n\n<br>\n<br>\n### EXTRACT\nThis time let's define a structure of loaded csv data:"},{"metadata":{"trusted":true},"cell_type":"code","source":"spark = SparkSession.builder.appName('etl_app').master(\"local[*]\").getOrCreate()\n\ndef load_with_schema(spark):\n\n    schema = StructType([\n        StructField(\"duration_sec\", IntegerType(), True),\n        StructField(\"start_time\", TimestampType(), True),\n        StructField(\"end_time\", TimestampType(), True),\n        StructField(\"start_station_id\", StringType(), True),\n        StructField(\"start_station_name\", StringType(), True),\n        StructField(\"start_station_latitude\", DoubleType(), True),\n        StructField(\"start_station_longitude\", DoubleType(), True),\n        StructField(\"end_station_id\", StringType(), True),\n        StructField(\"end_station_name\", StringType(), True),\n        StructField(\"end_station_latitude\", DoubleType(), True),\n        StructField(\"end_station_longitude\", DoubleType(), True),\n        StructField(\"bike_id\", IntegerType(), True),\n        StructField(\"user_type\", StringType(), True),\n        StructField(\"member_birth_year\", IntegerType(), True),\n        StructField(\"member_gender\", StringType(), True)\n    ])\n\n    df = spark \\\n        .read \\\n        .format(\"csv\") \\\n        .schema(schema)         \\\n        .option(\"header\", \"true\") \\\n        .load(\"../input/ford-gobike-data/*.csv\")\n\n    return df\n\ngoBike = load_with_schema(spark)\nprint(f\"Total Records = {goBike.count()}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"> **ZADANIE 2**: Utwórz DataFrame `dataDaily` zawierający dane zagregowane do poziomu dnia. Zbiór ma zawierać następujące informacje (kolumny): \n- 'date' : data \n- 'avg_duration_sec' : średni czas wypożyczeń danego dnia\n- 'n_trips' : liczba wypożyczeń danego dnia\n- 'n_bikes' : liczba unikatowych rowerów użytych danego dnia\n- 'n_routes' : liczba unikatowych kombinacji stacji (x -> y == y -> x) danego dnia\n- 'n_subscriber' : liczba wypożyczeń dokonanych przez subskrybentów danego dnia"},{"metadata":{},"cell_type":"markdown","source":"<br>\n### TRANSFORM\nWe will create a new DataFrame `dailyData` with aggregated data by single day. <br>\nLet's transform our data into these new columns: \n\n- 'date' : date \n- 'avg_duration_sec' : average time of rental of that day\n- 'n_trips' : total number of trips of that day\n- 'n_bikes' : total number of unique bikes rented that day\n- 'n_routes' : total number of unique trips combinations (x -> y == y -> x) of that day\n- 'n_subscriber' : total number of subsribers' rentals of that day"},{"metadata":{"trusted":true},"cell_type":"code","source":"dailyData = goBike.withColumn(\"date\", f.date_format(\"start_time\", \"dd.MM.yyyy\")) \\\n                  .groupBy(\"date\") \\\n                  .agg(f.avg(\"duration_sec\").alias(\"avg_duration_sec\"), \n                       f.count(\"*\").alias(\"n_trips\"), \n                       f.countDistinct(\"bike_id\").alias(\"n_bikes\"), \n                       f.sum(f.when(goBike.user_type == \"Subscriber\", 1).otherwise(0))\n                       .alias(\"n_subscriber\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp = goBike.select(f.date_format(\"start_time\", \"dd.MM.yyyy\").alias(\"date\"), \n                    f.when(goBike[\"start_station_id\"] > goBike[\"end_station_id\"], \n                           f.array(goBike[\"start_station_id\"], goBike[\"end_station_id\"]))\\\n                    .otherwise(f.array(goBike[\"end_station_id\"], goBike[\"start_station_id\"])).alias(\"route\"))\\\n            .groupBy(\"date\").agg(f.countDistinct(\"route\").alias(\"n_routes\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dailyData = dailyData.join(temp, \"date\")\ndailyData.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"<br>\n### LOAD\nBe aware of high computational cost of these operations depending on how big is your DataFrame\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Option 1: save data to .parquet file\n# goBike.write.parquet('path/to/location/transformed.parquet')\n\n# Option 2: save data to .csv\n# goBike.write.csv('path/to/location/transformed.csv')\n\n# Option 3: save data to pandas\n# goBike.toPandas()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"spark.stop()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"That's it. If you like it, please upvote. Thank you"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}