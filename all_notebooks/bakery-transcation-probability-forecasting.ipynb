{"cells":[{"metadata":{"_uuid":"fb3400ace590c080e0352e2dbc9bd0813c2dc3df"},"cell_type":"markdown","source":"# Forecasting the probability of an item sold during the 24 span of day"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\n\n\nimport os\nprint(os.listdir(\"../input\"))\ninputData=pd.read_csv(r\"../input/BreadBasket_DMS.csv\")","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"mergedDateTime = inputData.Date +' '+inputData.Time\ninputData.index = (pd.to_datetime(mergedDateTime))\ninputData.drop([\"Time\",\"Date\",\"Transaction\"],axis=1,inplace=True)\ninputData.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"21be4444dc50fe05118d20d3b91758f7fa26452f"},"cell_type":"markdown","source":"# Group the data into one hourly bunches - Split into test and train sets"},{"metadata":{"trusted":true,"_uuid":"8176151c767a52bb05ecd1d829bbf7c8f98a0d2b"},"cell_type":"code","source":"\nstartDate,endDate = \"2016-10\",\"2017-02\"\ntrainingData = inputData[startDate:endDate]\n# Group the data on hourly spans\ngroup = pd.DataFrame({\"ItemCount\":trainingData.groupby([trainingData.index.map(lambda t: t.hour),\"Item\"]).size()}).reset_index();\n\n# Now lets find the probabilities\n# Note this is an hourly probability - so we only consider items falling within\n# the hour period\nitemName = \"Coffee\"\ntrainItemProbability = group[group[\"Item\"] == itemName]\ntrainItemProbability.rename(index=int, columns={\"level_0\": \"Hour\"},inplace=True)\ntrainItemProbability.drop([\"Item\"],axis= 1,inplace=True)\ntotal = np.float(trainItemProbability[\"ItemCount\"].sum())  \ntrainItemProbability[\"ItemCount\"]=trainItemProbability[\"ItemCount\"].apply(lambda v:(v /total))\n\n# Since the item span may not be in the 24 hours range,\n# we add in averages samples. This will ne done to testing data as well\n# Will help us keep sanity during testing\n\nhours = np.arange(0,24,1)\ntrainItemProbability24hrs = pd.DataFrame(0,index=hours,columns=trainItemProbability.columns.values)\ndef expand_to_24_hours(data24hrs,data):\n    for row in range(0,len(data)):\n        oneRow = data.iloc[row,:]\n        data24hrs.iloc[np.int(oneRow.Hour)] = oneRow\n    return data24hrs\ntrainItemProbability = expand_to_24_hours(trainItemProbability24hrs,trainItemProbability)\nx = trainItemProbability[\"Hour\"]\ny = trainItemProbability[\"ItemCount\"]\n\ndel total,group\n\n\ntestingData = inputData[endDate:]  \ngroup = pd.DataFrame({\"ItemCount\":testingData.groupby([testingData.index.map(lambda t: t.hour),\"Item\"]).size()}).reset_index();\n\ntestItemProbability = group[group[\"Item\"] == itemName]\ntestItemProbability.rename(index=int, columns={\"level_0\": \"Hour\"},inplace=True)\ntestItemProbability.drop([\"Item\"],axis= 1,inplace=True)\ntotal = np.float(testItemProbability[\"ItemCount\"].sum())  \ntestItemProbability[\"ItemCount\"]=testItemProbability[\"ItemCount\"].apply(lambda v:(v /total))\n\n\n\nhours = np.arange(0,24,1) # This will be our new index\ntestItemProbability24hrs = pd.DataFrame(0,index=hours,columns=testItemProbability.columns.values)\ntestItemProbability = expand_to_24_hours(testItemProbability24hrs,testItemProbability)\n\nfig = plt.figure(figsize = (15,5))\nax = fig.gca()\nxTrain = trainItemProbability24hrs[\"Hour\"]\nyTrain = trainItemProbability24hrs[\"ItemCount\"]\nxTest = testItemProbability24hrs[\"Hour\"]\nyTest = testItemProbability24hrs[\"ItemCount\"]\nplt.scatter(xTrain,yTrain,label=\"Train\")\nplt.scatter(xTest,yTest,label=\"Test\")\nplt.legend()\nplt.xlabel('Time span',fontsize=10)\nplt.ylabel('Probability',fontsize=10)\nax.tick_params(labelsize=10)\nplt.title('Probability of {} sold during the day'.format(itemName),fontsize=20)\nplt.grid()\nplt.ioff()\nplt.show()\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bb062185b12a0e242cf5580cbfa47b9215a23437"},"cell_type":"markdown","source":"# Probability Forecasting"},{"metadata":{"trusted":true,"_uuid":"6dce615ede19067f0fb4b306da8218a7c760dc16"},"cell_type":"code","source":"# Lets also select particular hour range for our predictions\nstartHour = 3\nendHour = 22\ncondition = np.logical_and((trainItemProbability24hrs.index >= startHour),(trainItemProbability24hrs.index <= endHour))\ntrainSamples = trainItemProbability24hrs[condition]\ncondition = np.logical_and((testItemProbability24hrs.index >= startHour),(testItemProbability24hrs.index <= endHour))\ntestSamples = testItemProbability24hrs[condition]\n# Naive \n\npredicted = trainSamples.copy();\npredicted[\"Naive\"] = trainSamples.ItemCount\nfig = plt.figure(figsize = (15,5))\nax = fig.gca()\nplt.scatter(trainSamples.index,trainSamples.ItemCount,label=\"Train\",c='r',marker='.')\nplt.scatter(testSamples.index,testSamples.ItemCount,label=\"Test\",c='g',marker='+')\nplt.scatter(predicted.index,predicted.ItemCount,label=\"Predicted\",c='b',marker='*')\nplt.legend()\nplt.xlabel('Time span',fontsize=10)\nplt.ylabel('Probability',fontsize=10)\nax.tick_params(labelsize=10)\nplt.title('Naive Forecast',fontsize=20)\nplt.grid()\nplt.ioff()\nplt.show()\n\nrms = sqrt(mean_squared_error(testSamples.ItemCount, predicted.Naive))\nprint(rms)\n\n# Simple Average\n\npredicted[\"Average\"] = trainSamples['ItemCount'].rolling(1).mean()\nfig = plt.figure(figsize = (15,5))\nax = fig.gca()\nplt.scatter(trainSamples.index,trainSamples.ItemCount,label=\"Train\",c='r',marker='.')\nplt.scatter(testSamples.index,testSamples.ItemCount,label=\"Test\",c='g',marker='+')\nplt.scatter(predicted.index,predicted.Average,label=\"Predicted\",c='b',marker='*')\nplt.legend()\nplt.xlabel('Time span',fontsize=10)\nplt.ylabel('Probability',fontsize=10)\nax.tick_params(labelsize=10)\nplt.title('Simple Average Forecast',fontsize=20)\nplt.grid()\nplt.ioff()\nplt.show()\n\nrms = sqrt(mean_squared_error(testSamples.ItemCount, predicted.Average))\nprint(rms)\n\n\n# Moving Average\n\npredicted[\"MAverage\"] = trainSamples['ItemCount'].rolling(4).mean()\n# Find nans and replace. Nans appear as we initialized a DataFrame with zeros\npredicted.fillna(0,inplace=True)\nfig = plt.figure(figsize = (15,5))\nax = fig.gca()\nplt.scatter(trainSamples.index,trainSamples.ItemCount,label=\"Train\",c='r',marker='.')\nplt.scatter(testSamples.index,testSamples.ItemCount,label=\"Test\",c='g',marker='+')\nplt.scatter(predicted.index,predicted.MAverage,label=\"Predicted\",c='b',marker='*')\nplt.legend()\nplt.xlabel('Time span',fontsize=10)\nplt.ylabel('Probability',fontsize=10)\nax.tick_params(labelsize=10)\nplt.title('Moving Average Forecast',fontsize=20)\nplt.grid()\nplt.ioff()\nplt.show()\n\nrms = sqrt(mean_squared_error(testSamples.ItemCount, predicted.MAverage))\nprint(rms)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b06718c8a6459034e3bf5ada4b743eadd406e758"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}