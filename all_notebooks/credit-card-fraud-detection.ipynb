{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Credit Card Fraud Detection","metadata":{}},{"cell_type":"markdown","source":"# Predict whether the Credit Card Transaction is Fraud or not?","metadata":{}},{"cell_type":"markdown","source":"# Context :\nIt is important that credit card companies are able to recognize fraudulent credit card transactions\nso that customers are not charged for items that they did not purchase.","metadata":{}},{"cell_type":"markdown","source":"# The Problem :\nThe challenge is to recognize fraudulent credit card transactions so that the customers of credit\ncard companies are not charged for items that they did not purchase.","metadata":{}},{"cell_type":"markdown","source":"# **Content**\n\nIt is important that credit card companies are able to recognize fraudulent credit card transactions so that customers are not charged for items that they did not purchase.\n\nThe dataset contains transactions made by credit cards in September 2013 by European cardholders. This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\n\nIt contains only numerical input variables which are the result of a PCA transformation. Unfortunately, due to confidentiality issues, we cannot provide the original features and more background information about the data. Features V1, V2, … V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are 'Time' and 'Amount'. Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature 'Amount' is the transaction Amount, this feature can be used for example-dependant cost-sensitive learning. Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise.\n\nGiven the class imbalance ratio, we recommend measuring the accuracy using the Area Under the Precision-Recall Curve (AUPRC). Confusion matrix accuracy is not meaningful for unbalanced classification.\n\n","metadata":{}},{"cell_type":"markdown","source":"**Main challenges involved in credit card fraud detection are:**\n* Enormous Data is processed every day and the model build must be fast enough to respond to the\nscam in time.\n* Imbalanced Data i.e most of the transactions (99.8%) are not fraudulent which makes it really\nhard for detecting the fraudulent ones\n* Data availability as the data is mostly private.\n* Misclassified Data can be another major issue, as not every fraudulent transaction is caught and\nreported.\n* Adaptive techniques used against the model by the scammers.\n* How to tackle these challenges?\n* The model used must be simple and fast enough to detect the anomaly and classify it as a\nfraudulent transaction as quickly as possible.\n* Imbalance can be dealt with by properly using some methods which we will talk about in the next\nparagraph\n* For protecting the privacy of the user the dimensionality of the data can be reduced.\n* A more trustworthy source must be taken which double-checks the data, at least for training the\nmodel.\n* We can make the model simple and interpretable so that when the scammer adapts to it with just\nsome tweaks we can have a new model up and running to deploy.","metadata":{}},{"cell_type":"markdown","source":"# **Data** **Analysis**\n\n**PANDAS**:\n\n  Pandas provide high performance, fast, easy to use data structures and data analysis tools for manipulating numeric data and time series. Pandas is built on the numpy library and written in languages like Python, Cython, and C. In pandas, we can import data from various file formats like JSON, SQL, Microsoft Excel, etc.\n\n**NUMPY**:\n\nIt is the fundamental library of python, used to perform scientific computing. It provides high-performance multidimensional arrays and tools to deal with them. A numpy array is a grid of values (of the same type) that are indexed by a tuple of positive integers, numpy arrays are fast, easy to understand, and give users the right to perform calculations across arrays.\n","metadata":{}},{"cell_type":"markdown","source":"# **DATA** **VISUALIZATION**:\n\nData Visualization is the graphic representation of data. It converts a huge dataset into small graphs, thus aids in data analysis and predictions.\n\n**MATPLOTLIB**:\n\nIt is a Python library used for plotting graphs with the help of other libraries like Numpy and Pandas. It is a powerful tool for visualizing data in Python. It is used for creating statical interferences and plotting 2D graphs of arrays.\n\n**SEABORN**:\nIt is also a Python library used for plotting graphs with the help of Matplotlib, Pandas, and Numpy. It is built on the roof of Matplotlib and is considered as a superset of the Matplotlib library. It helps in visualizing univariate and bivariate data.","metadata":{}},{"cell_type":"markdown","source":"# Importing the required Libraries","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns","metadata":{"execution":{"iopub.status.busy":"2021-08-27T15:08:08.169159Z","iopub.execute_input":"2021-08-27T15:08:08.169561Z","iopub.status.idle":"2021-08-27T15:08:09.038304Z","shell.execute_reply.started":"2021-08-27T15:08:08.169474Z","shell.execute_reply":"2021-08-27T15:08:09.037628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Reading the Data Set","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('../input/credit-card-fraud-detection-data/Credit_Card_Fraud_Detection.csv')","metadata":{"execution":{"iopub.status.busy":"2021-08-27T15:08:09.041562Z","iopub.execute_input":"2021-08-27T15:08:09.042999Z","iopub.status.idle":"2021-08-27T15:08:12.745146Z","shell.execute_reply.started":"2021-08-27T15:08:09.042963Z","shell.execute_reply":"2021-08-27T15:08:12.744419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Examining the Data","metadata":{}},{"cell_type":"code","source":"df.head(3)","metadata":{"execution":{"iopub.status.busy":"2021-08-27T15:08:12.746372Z","iopub.execute_input":"2021-08-27T15:08:12.746707Z","iopub.status.idle":"2021-08-27T15:08:12.783642Z","shell.execute_reply.started":"2021-08-27T15:08:12.746681Z","shell.execute_reply":"2021-08-27T15:08:12.782804Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.tail(3)","metadata":{"execution":{"iopub.status.busy":"2021-08-27T15:08:12.78494Z","iopub.execute_input":"2021-08-27T15:08:12.785198Z","iopub.status.idle":"2021-08-27T15:08:12.807081Z","shell.execute_reply.started":"2021-08-27T15:08:12.785174Z","shell.execute_reply":"2021-08-27T15:08:12.806278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"markdown","source":"# Exploratory Data Analysis\n\n\n\n*   **head**()Understand your data using the head() function to look at the first few rows.\n\n* **shape**()Review the dimensions of your data with the shape \nproperty.\n* **info**()To know the information about the data\n* **Dtyes** Look at the data types for each attribute with the dtypes property.\n* **describeReview** the distribution of your data with the describe() function.\n* **Correlation** Calculate pairwise correlation between your variables using the corr() function.\n\n\n","metadata":{}},{"cell_type":"code","source":"df.shape","metadata":{"execution":{"iopub.status.busy":"2021-08-27T15:08:12.808118Z","iopub.execute_input":"2021-08-27T15:08:12.808355Z","iopub.status.idle":"2021-08-27T15:08:12.813769Z","shell.execute_reply.started":"2021-08-27T15:08:12.808331Z","shell.execute_reply":"2021-08-27T15:08:12.812904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# info()\n\nThis method prints information about a DataFrame including the index dtype and column dtypes, non-null values and memory usage","metadata":{}},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2021-08-27T15:08:12.81482Z","iopub.execute_input":"2021-08-27T15:08:12.815086Z","iopub.status.idle":"2021-08-27T15:08:12.858106Z","shell.execute_reply.started":"2021-08-27T15:08:12.815062Z","shell.execute_reply":"2021-08-27T15:08:12.857164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Interpretation:\n* By understanding the info() , we can say that all the columns are float type expect class column\n* There is no null values are present, we can check the memory usage i.e 31.4 MB","metadata":{}},{"cell_type":"code","source":"print(\"There are {} rows and {} columns are present in the Data Set\".format(df.shape[0],df.shape[1]))","metadata":{"execution":{"iopub.status.busy":"2021-08-27T15:08:12.859529Z","iopub.execute_input":"2021-08-27T15:08:12.860057Z","iopub.status.idle":"2021-08-27T15:08:12.865683Z","shell.execute_reply.started":"2021-08-27T15:08:12.860017Z","shell.execute_reply":"2021-08-27T15:08:12.86452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Describe():","metadata":{}},{"cell_type":"code","source":"df.describe()","metadata":{"execution":{"iopub.status.busy":"2021-08-27T15:08:12.867779Z","iopub.execute_input":"2021-08-27T15:08:12.868151Z","iopub.status.idle":"2021-08-27T15:08:13.274006Z","shell.execute_reply.started":"2021-08-27T15:08:12.868124Z","shell.execute_reply":"2021-08-27T15:08:13.273152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Outlier Detection","metadata":{}},{"cell_type":"code","source":"df.select_dtypes(\"number\").head()","metadata":{"execution":{"iopub.status.busy":"2021-08-27T15:08:13.27589Z","iopub.execute_input":"2021-08-27T15:08:13.27623Z","iopub.status.idle":"2021-08-27T15:08:13.328616Z","shell.execute_reply.started":"2021-08-27T15:08:13.276192Z","shell.execute_reply":"2021-08-27T15:08:13.327782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualize the outliers using Box Plot\n\nplt.style.use('fivethirtyeight')\nplt.rcParams['figure.figsize'] = (15, 4)\n\nplt.subplot(1, 2, 1)\nsns.boxplot(df['Amount'], color = \"blue\")\n\nplt.subplot(1, 2, 2)\nsns.boxplot(df['Class'], color = \"green\")\n\nplt.suptitle('Outliers Present in the Data')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-27T15:08:13.329872Z","iopub.execute_input":"2021-08-27T15:08:13.330232Z","iopub.status.idle":"2021-08-27T15:08:13.636048Z","shell.execute_reply.started":"2021-08-27T15:08:13.330193Z","shell.execute_reply":"2021-08-27T15:08:13.635196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Removing outliers from amt Column from train dataset\n\n# Shape before removing outliers\nprint(\"Before Removing Outliers \", df.shape)\n\n# Filtering the amt having more than 18000\ndf = df[df[\"Amount\"] < 16000]\n\n# Shape after removing outliers\nprint(\"After Removing Outliers \", df.shape)","metadata":{"execution":{"iopub.status.busy":"2021-08-27T15:08:13.637121Z","iopub.execute_input":"2021-08-27T15:08:13.637374Z","iopub.status.idle":"2021-08-27T15:08:13.703782Z","shell.execute_reply.started":"2021-08-27T15:08:13.637347Z","shell.execute_reply":"2021-08-27T15:08:13.702868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Handling Missing Values**\nThere are broadly divide into two ways to treat missing values\n\nDelete --> Delete the missing values\n* 2.impute -->\n     * imputing by a simple static: Replace the missing values by another value according to MEAN,MEDIAN,MODE\n     * Predictive Techniques: Use statitical models such as K-NN,SVM etc to predict and replace missing values fillna\n* Otherwise deletion is often safer and recongineed . You may loose data but will not make false predections\n* Caution : Always have backup of the orginal data .if you are deleting missing values","metadata":{}},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-08-27T15:08:13.704745Z","iopub.execute_input":"2021-08-27T15:08:13.705114Z","iopub.status.idle":"2021-08-27T15:08:13.729257Z","shell.execute_reply.started":"2021-08-27T15:08:13.705087Z","shell.execute_reply":"2021-08-27T15:08:13.728368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install missingno","metadata":{"execution":{"iopub.status.busy":"2021-08-27T15:08:13.730311Z","iopub.execute_input":"2021-08-27T15:08:13.730585Z","iopub.status.idle":"2021-08-27T15:08:20.801275Z","shell.execute_reply.started":"2021-08-27T15:08:13.730556Z","shell.execute_reply":"2021-08-27T15:08:20.8002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot missng values in bar graph\nimport missingno as msno\nmsno.bar(df)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-27T15:08:20.802973Z","iopub.execute_input":"2021-08-27T15:08:20.80334Z","iopub.status.idle":"2021-08-27T15:08:23.151155Z","shell.execute_reply.started":"2021-08-27T15:08:20.803299Z","shell.execute_reply":"2021-08-27T15:08:23.150513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is no missing value in this dataset","metadata":{}},{"cell_type":"code","source":"df.columns","metadata":{"execution":{"iopub.status.busy":"2021-08-27T15:08:23.152095Z","iopub.execute_input":"2021-08-27T15:08:23.152477Z","iopub.status.idle":"2021-08-27T15:08:23.157256Z","shell.execute_reply.started":"2021-08-27T15:08:23.152439Z","shell.execute_reply":"2021-08-27T15:08:23.156671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# creating index in a list\nlst=['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',\n       'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20',\n       'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount',\n       'Class']","metadata":{"execution":{"iopub.status.busy":"2021-08-27T15:08:23.158133Z","iopub.execute_input":"2021-08-27T15:08:23.158498Z","iopub.status.idle":"2021-08-27T15:08:23.166041Z","shell.execute_reply.started":"2021-08-27T15:08:23.158461Z","shell.execute_reply":"2021-08-27T15:08:23.165426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Histogram**\n\nA histogram is a graphical representation of the distribution of data given by the user.\nIts appearance is similar to Bar-Graph except it is continuous.\nThe towers or bars of a histogram are called bins.\nThe height of each bin shows how many values from that data fall into that range.\n\n### **Skewed**\n\n* These distributions are sometimes called asymmetric or asymmetrical distributions as they don’t show any kind of symmetry.\n* Symmetry means that one half of the distribution is a mirror image of the other half.\n* For example, the normal distribution is a symmetric distribution with no skew. The tails are exactly the same.\n\n### **Normal** **Distribution**\n* A normal distribution, sometimes called the bell curve.\n* The bell curve is symmetrical. Half of the data will fall to the left of the mean; half will fall to the right.\n* The mean, mode and median are all equal.\n* The curve is symmetric at the center (i.e. around the mean, μ).\n* Exactly half of the values are to the left of center and exactly half the values are to the right.\n* The total area under the curve is 1.\n* The Standard Normal Model\n* A standard normal model is a normal distribution with a mean of 0 and a standard deviation of 1.\n\n### **Left side skewed**\n\n* A left-skewed distribution has a long left tail.\n* Left-skewed distributions are also called negatively-skewed distributions.\n* That’s because there is a long tail in the negative direction on the number line.\n* The mean is also to the left of the peak.\n\n### **Right side skewd**\n\n* A right-skewed distribution has a long right tail.\n* Right-skewed distributions are also called positive-skew distributions.\n* That’s because there is a long tail in the positive direction on the number line.","metadata":{}},{"cell_type":"code","source":"for i in lst[1:]: # iterating all the rows\n    df[i].hist(bins=50,figsize=(10,6))\n    \n    # Width of each bin is = (max value of data – min value of data) / total number of bins \n    # hist means histogram, here we using with the help of matplotlib , it gives some bins to understand bars\n    \n    plt.yscale('log') \n    #the type of conversion of the scale, to convert y-axes to logarithmic scale we pass the “log” keyword or the matplotlib. scale\n    #LogScale class to the yscale method\n    plt.title(i)\n    \n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-27T15:08:23.166951Z","iopub.execute_input":"2021-08-27T15:08:23.16732Z","iopub.status.idle":"2021-08-27T15:08:38.724282Z","shell.execute_reply.started":"2021-08-27T15:08:23.167283Z","shell.execute_reply":"2021-08-27T15:08:38.723247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Interpretation**:\n\n* V1 ---> its a left side skwed, mean is on the left, Here data points are most on left side and very few amount of outliers occurs.\n\n* V2 ---> its looks like bell cure i.e Uniform distribution some slightly a left side skwed, mean is on the left, Here data points are most uniform and very few amount of outliers occurs.\n\n* V3 ---> its a left side skwed mean is on the left, Here data points are most on left side and very few amount of outliers occurs\n\n* V4 --->its looks like bell cure i.e Uniform distribution some slightly a right side skwed, mean is on the right, Here data points are most uniform and very few amount of outliers occurs on 2 bins.\n\n* V5 ---> It looks unifrom distribution and one bin outlier far away to this data.\n\n* V6 ---> It looks unifrom distribution and one bin outlier far away to this data.\n\n* V7 ---> It looks unifrom distribution and one bin outlier far away to this data.\n\n* V8 --->its looks like bell cure i.e Uniform distribution some slightly a right side skwed, mean is on the right, Here data points are most uniform and very few amount of outliers occurs on 2 bins\n\n* V9 ---> its looks like bell cure i.e Uniform distribution some slightly a left side skwed, mean is on the left, Here data points are most uniform and very few amount of outliers occurs\n\n* V10 ---> it looks unifrom distribution small amount of data oustide.\n\n* V11 ---> Its a Completely Uniform Distribution\n\n* V12--->its looks like bell cure i.e Uniform distribution some slightly a right side skwed, mean is on the right, Here data points are most uniform and very few amount of outliers occurs on 1 bin.\n\n* V13 ---> Its a Completely Uniform Distribution\n\n* V14 --->its looks like bell cure i.e Uniform distribution some slightly a right side skwed, mean is on the right, Here data points are most uniform and very few amount of outliers occurs on positive side aswellas neagative.\n\n* V15 ---> It looks unifrom distribution and one bin outlier far away to this data.\n\n* V16 ---> It looks unifrom distribution and one bin outlier far away to this data.\n\n* V17 ---> its looks like bell cure i.e Uniform distribution some slightly a left side skwed, mean is on the left, Here data points are most uniform and very few amount of data is on left but there in group.\n\n* V18 ---> its looks like bell cure i.e Uniform distribution some slightly a right side skwed, mean is on the right, Here data points are most uniform and very few amount of data is on right but there in group.\n\n* V19 ---> Its a Completely Uniform Distribution\n\n* V20 ---> Its a Completely Uniform Distribution\n\n* V21 ---> Its a Completely Uniform Distribution\n\n* V22 ---> Its a Completely Uniform Distribution\n\n* V23 ---> Its a Completely Uniform Distribution\n\n* V24 --->its looks like bell cure i.e Uniform distribution some slightly a right side skwed, mean is on the right, Here data points are most uniform and very few amount of outliers occurs on positive side.\n\n* V25 ---> it looks unifrom distribution very small amount of data is far.\n\n* V26 ---> it looks unifrom distribution very small amount of data is far.\n\n* V27 ---> it looks unifrom distribution very small amount of data is far.\n\n* V28 ---> it looks unifrom distribution very small amount of data is far.\n\n* Amount ---> its a left side skwed, mean is on the left, Here data points are most on left side and very few amount of outliers occurs.\n\n* Class ------> It has around not fraud 1.3 Lakhs and 1 is fraud i.e nearly 400","metadata":{}},{"cell_type":"code","source":"df['Class'].value_counts()   ","metadata":{"execution":{"iopub.status.busy":"2021-08-27T15:08:38.727184Z","iopub.execute_input":"2021-08-27T15:08:38.727451Z","iopub.status.idle":"2021-08-27T15:08:38.737329Z","shell.execute_reply.started":"2021-08-27T15:08:38.727425Z","shell.execute_reply":"2021-08-27T15:08:38.736498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df.drop(columns=['Time','Class']) ","metadata":{"execution":{"iopub.status.busy":"2021-08-27T15:08:38.738415Z","iopub.execute_input":"2021-08-27T15:08:38.738648Z","iopub.status.idle":"2021-08-27T15:08:38.742861Z","shell.execute_reply.started":"2021-08-27T15:08:38.738625Z","shell.execute_reply":"2021-08-27T15:08:38.741964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" This shows that data is highly unbalanced.","metadata":{}},{"cell_type":"markdown","source":"# **Correlation** :","metadata":{}},{"cell_type":"code","source":"df.corr()","metadata":{"execution":{"iopub.status.busy":"2021-08-27T15:08:38.744301Z","iopub.execute_input":"2021-08-27T15:08:38.74465Z","iopub.status.idle":"2021-08-27T15:08:39.59223Z","shell.execute_reply.started":"2021-08-27T15:08:38.744613Z","shell.execute_reply":"2021-08-27T15:08:39.591232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cor=df.corr()\nplt.figure(figsize=(16,10))\nsns.heatmap(cor)","metadata":{"execution":{"iopub.status.busy":"2021-08-27T15:08:39.593464Z","iopub.execute_input":"2021-08-27T15:08:39.593731Z","iopub.status.idle":"2021-08-27T15:08:41.199511Z","shell.execute_reply.started":"2021-08-27T15:08:39.593704Z","shell.execute_reply":"2021-08-27T15:08:41.198603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Heat Map","metadata":{}},{"cell_type":"markdown","source":"**HeatMap** - A heatmap is a graphical representation of data in which data values are represented as colors. That is, it uses color in order to communicate a value to the reader. This is a great tool to assist the audience towards the areas that matter the most when you have a large volume of data.\n\n* It shows the relationship between two columns or variables\n* If correlation is equal to zero i.e No Correlated\n* If correlation is equal to one i.e Perfect Correlated\n* If correlation is between less than zero to less than 0.45 i.e small positive correlated\n* If correlation is between greater than 0.5 to 0.9 then it is i.e large positive correlated\n* If correlation is negative to -0.45 is small neagtive correlated\n* If correlation is negative between greater than - 0.5 to - 0.9 then it is i.e large negative correlated","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(20,15))\n#plotting the figure size based on width and height\n\nsns.heatmap(df.corr(),cmap='PiYG',annot=True,linewidths=1,fmt='0.2f')","metadata":{"execution":{"iopub.status.busy":"2021-08-27T15:08:41.201004Z","iopub.execute_input":"2021-08-27T15:08:41.20138Z","iopub.status.idle":"2021-08-27T15:08:46.355709Z","shell.execute_reply.started":"2021-08-27T15:08:41.201341Z","shell.execute_reply":"2021-08-27T15:08:46.355064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Interpretation**:\n\n* V1, V3, V5, V8, V9, V10,V11, V12, V13, V14, V15, V16, V19, V22,V23, V25, V26 are small neagtive correlated with amount\n* V2 is large neative correlated with amount\n* V11 does not correlated with amount i.e 0.0\n* V4, V6, V7, V14, V17, V18, V20,V21, V27 V28 are Small positive correlated.","metadata":{}},{"cell_type":"markdown","source":"# **countplot**():\n\nCountplot(): method is used to Show the counts of observations in each categorical bin using bars.\n###Counter\nCounter is a container which stores the count of elements in a dictionary format where element is the key and its value corrosponds to it's count.","metadata":{}},{"cell_type":"code","source":"sns.countplot(x='Class', data = df)\nfrom collections import Counter\ncounter = Counter( df [ 'Class' ])\nprint(counter)","metadata":{"execution":{"iopub.status.busy":"2021-08-27T15:08:46.358722Z","iopub.execute_input":"2021-08-27T15:08:46.359149Z","iopub.status.idle":"2021-08-27T15:08:46.683368Z","shell.execute_reply.started":"2021-08-27T15:08:46.359109Z","shell.execute_reply":"2021-08-27T15:08:46.682523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are 284315 number is not fraud and remaining 492 is fraud.","metadata":{}},{"cell_type":"markdown","source":"Split the dataset into training and testing","metadata":{}},{"cell_type":"code","source":"# separating the data for analysis\nlegit = df[df.Class == 0]\nfraud = df[df.Class == 1]","metadata":{"execution":{"iopub.status.busy":"2021-08-27T15:08:46.68515Z","iopub.execute_input":"2021-08-27T15:08:46.685399Z","iopub.status.idle":"2021-08-27T15:08:46.726703Z","shell.execute_reply.started":"2021-08-27T15:08:46.685374Z","shell.execute_reply":"2021-08-27T15:08:46.725771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(legit.shape)\nprint(fraud.shape)","metadata":{"execution":{"iopub.status.busy":"2021-08-27T15:08:46.727865Z","iopub.execute_input":"2021-08-27T15:08:46.728129Z","iopub.status.idle":"2021-08-27T15:08:46.732846Z","shell.execute_reply.started":"2021-08-27T15:08:46.728102Z","shell.execute_reply":"2021-08-27T15:08:46.731961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"legit.Amount.describe()","metadata":{"execution":{"iopub.status.busy":"2021-08-27T15:08:46.734017Z","iopub.execute_input":"2021-08-27T15:08:46.734264Z","iopub.status.idle":"2021-08-27T15:08:46.758626Z","shell.execute_reply.started":"2021-08-27T15:08:46.734239Z","shell.execute_reply":"2021-08-27T15:08:46.757701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# statistical measures of the data\nlegit.describe()","metadata":{"execution":{"iopub.status.busy":"2021-08-27T15:08:46.759873Z","iopub.execute_input":"2021-08-27T15:08:46.760141Z","iopub.status.idle":"2021-08-27T15:08:47.160974Z","shell.execute_reply.started":"2021-08-27T15:08:46.760115Z","shell.execute_reply":"2021-08-27T15:08:47.160131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fraud.Amount.describe()","metadata":{"execution":{"iopub.status.busy":"2021-08-27T15:08:47.162026Z","iopub.execute_input":"2021-08-27T15:08:47.162248Z","iopub.status.idle":"2021-08-27T15:08:47.170849Z","shell.execute_reply.started":"2021-08-27T15:08:47.162224Z","shell.execute_reply":"2021-08-27T15:08:47.170043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# compare the values for both transactions\ndf.groupby('Class').mean()","metadata":{"execution":{"iopub.status.busy":"2021-08-27T15:08:47.171921Z","iopub.execute_input":"2021-08-27T15:08:47.172171Z","iopub.status.idle":"2021-08-27T15:08:47.261081Z","shell.execute_reply.started":"2021-08-27T15:08:47.172146Z","shell.execute_reply":"2021-08-27T15:08:47.26024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Under-Sampling\n\nBuild a sample dataset containing similar distribution of normal transactions and Fraudulent Transactions\n\nNumber of Fraudulent Transactions --> 492","metadata":{}},{"cell_type":"code","source":"legit_sample = legit.sample(n=492)","metadata":{"execution":{"iopub.status.busy":"2021-08-27T15:08:47.262264Z","iopub.execute_input":"2021-08-27T15:08:47.262515Z","iopub.status.idle":"2021-08-27T15:08:47.27413Z","shell.execute_reply.started":"2021-08-27T15:08:47.262489Z","shell.execute_reply":"2021-08-27T15:08:47.273315Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Concatenating two DataFrames","metadata":{}},{"cell_type":"code","source":"new_dataset = pd.concat([legit_sample, fraud], axis=0)\nnew_dataset.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-27T15:08:47.275223Z","iopub.execute_input":"2021-08-27T15:08:47.275458Z","iopub.status.idle":"2021-08-27T15:08:47.302186Z","shell.execute_reply.started":"2021-08-27T15:08:47.275434Z","shell.execute_reply":"2021-08-27T15:08:47.301223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_dataset['Class'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-08-27T15:08:47.303298Z","iopub.execute_input":"2021-08-27T15:08:47.303531Z","iopub.status.idle":"2021-08-27T15:08:47.30996Z","shell.execute_reply.started":"2021-08-27T15:08:47.303508Z","shell.execute_reply":"2021-08-27T15:08:47.308964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_dataset.groupby('Class').mean()","metadata":{"execution":{"iopub.status.busy":"2021-08-27T15:08:47.311267Z","iopub.execute_input":"2021-08-27T15:08:47.311625Z","iopub.status.idle":"2021-08-27T15:08:47.337158Z","shell.execute_reply.started":"2021-08-27T15:08:47.311588Z","shell.execute_reply":"2021-08-27T15:08:47.336214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Splitting the data into Features & Targets","metadata":{}},{"cell_type":"code","source":"x = new_dataset.drop(columns='Class', axis=1)\ny = new_dataset['Class']\nprint(x)","metadata":{"execution":{"iopub.status.busy":"2021-08-27T15:08:47.33817Z","iopub.execute_input":"2021-08-27T15:08:47.338391Z","iopub.status.idle":"2021-08-27T15:08:47.354516Z","shell.execute_reply.started":"2021-08-27T15:08:47.338367Z","shell.execute_reply":"2021-08-27T15:08:47.35371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(y)","metadata":{"execution":{"iopub.status.busy":"2021-08-27T15:08:47.355618Z","iopub.execute_input":"2021-08-27T15:08:47.35589Z","iopub.status.idle":"2021-08-27T15:08:47.361431Z","shell.execute_reply.started":"2021-08-27T15:08:47.355861Z","shell.execute_reply":"2021-08-27T15:08:47.360489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Split the data into Training data & Testing Data","metadata":{}},{"cell_type":"markdown","source":"# Train_Test_Split\n\nTrain-Test Split Evaluation\nThe train-test split is a technique for evaluating the performance of a machine learning algorithm.\nIt can be used for classification or regression problems and can be used for any supervised learning algorithm.\n\nThe procedure involves taking a dataset and dividing it into two subsets. The first subset is used to fit the model and is referred to as the training dataset. The second subset is not used to train the model; instead, the input element of the dataset is provided to the model, then predictions are made and compared to the expected values. This second dataset is referred to as the test dataset.\n\nTrain Dataset: Used to fit the machine learning model.\n\nTest Dataset: Used to evaluate the fit machine learning model.\n\nThe objective is to estimate the performance of the machine learning model on new data: data not used to train the model.\n\ncommon split percentages include:\n\nTrain: 80%, Test: 20%\n","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, stratify=y, random_state=2)","metadata":{"execution":{"iopub.status.busy":"2021-08-27T15:08:47.362577Z","iopub.execute_input":"2021-08-27T15:08:47.362862Z","iopub.status.idle":"2021-08-27T15:08:47.538759Z","shell.execute_reply.started":"2021-08-27T15:08:47.362813Z","shell.execute_reply":"2021-08-27T15:08:47.537929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train.shape,x_test.shape,x_train.shape,x_test.shape","metadata":{"execution":{"iopub.status.busy":"2021-08-27T15:08:47.541558Z","iopub.execute_input":"2021-08-27T15:08:47.54181Z","iopub.status.idle":"2021-08-27T15:08:47.546528Z","shell.execute_reply.started":"2021-08-27T15:08:47.541785Z","shell.execute_reply":"2021-08-27T15:08:47.545916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(x.shape, x_train.shape, x_test.shape)","metadata":{"execution":{"iopub.status.busy":"2021-08-27T15:08:47.54746Z","iopub.execute_input":"2021-08-27T15:08:47.547883Z","iopub.status.idle":"2021-08-27T15:08:47.557501Z","shell.execute_reply.started":"2021-08-27T15:08:47.547855Z","shell.execute_reply":"2021-08-27T15:08:47.556517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Training\n\n# Logistic Regression","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nmodel = LogisticRegression()","metadata":{"execution":{"iopub.status.busy":"2021-08-27T15:08:47.558479Z","iopub.execute_input":"2021-08-27T15:08:47.55895Z","iopub.status.idle":"2021-08-27T15:08:47.640441Z","shell.execute_reply.started":"2021-08-27T15:08:47.5589Z","shell.execute_reply":"2021-08-27T15:08:47.639593Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# training the Logistic Regression Model with Training Data\nmodel.fit(x_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2021-08-27T15:08:47.641582Z","iopub.execute_input":"2021-08-27T15:08:47.641851Z","iopub.status.idle":"2021-08-27T15:08:47.81376Z","shell.execute_reply.started":"2021-08-27T15:08:47.64181Z","shell.execute_reply":"2021-08-27T15:08:47.812885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Evaluation\n\n# Accuracy Score","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\n# accuracy on training data\nx_train_prediction = model.predict(x_train)\ntraining_data_accuracy = accuracy_score(x_train_prediction, y_train)","metadata":{"execution":{"iopub.status.busy":"2021-08-27T15:08:47.815144Z","iopub.execute_input":"2021-08-27T15:08:47.815662Z","iopub.status.idle":"2021-08-27T15:08:47.825228Z","shell.execute_reply.started":"2021-08-27T15:08:47.815618Z","shell.execute_reply":"2021-08-27T15:08:47.824181Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Accuracy on Training data : ', training_data_accuracy)","metadata":{"execution":{"iopub.status.busy":"2021-08-27T15:08:47.826733Z","iopub.execute_input":"2021-08-27T15:08:47.827297Z","iopub.status.idle":"2021-08-27T15:08:47.834189Z","shell.execute_reply.started":"2021-08-27T15:08:47.827258Z","shell.execute_reply":"2021-08-27T15:08:47.833239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# accuracy on test data\nx_test_prediction = model.predict(x_test)\ntest_data_accuracy = accuracy_score(x_test_prediction, y_test)","metadata":{"execution":{"iopub.status.busy":"2021-08-27T15:08:47.835713Z","iopub.execute_input":"2021-08-27T15:08:47.836426Z","iopub.status.idle":"2021-08-27T15:08:47.845955Z","shell.execute_reply.started":"2021-08-27T15:08:47.836229Z","shell.execute_reply":"2021-08-27T15:08:47.844942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Accuracy score on Test Data : ', test_data_accuracy)","metadata":{"execution":{"iopub.status.busy":"2021-08-27T15:08:47.847556Z","iopub.execute_input":"2021-08-27T15:08:47.848114Z","iopub.status.idle":"2021-08-27T15:08:47.853971Z","shell.execute_reply.started":"2021-08-27T15:08:47.848071Z","shell.execute_reply":"2021-08-27T15:08:47.85294Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Classification Algorithm","metadata":{}},{"cell_type":"markdown","source":"**KNeighborsClassifier**\n\n* The K-Nearest Neighbors classifier (KNN) is one of the simplest yet most commonly used classifiers in supervised machine learning.\n* KNN is often considered a lazy learner.\n* it doesn’t technically train a model to make predictions.\n* Instead an observation is predicted to be the class of that of the largest proportion of the k nearest observations. For example, if an observation with an unknown class is surrounded by an observation of class 1, then the observation is classified as class 1.","metadata":{}},{"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier","metadata":{"execution":{"iopub.status.busy":"2021-08-27T15:08:47.855457Z","iopub.execute_input":"2021-08-27T15:08:47.856192Z","iopub.status.idle":"2021-08-27T15:08:47.925808Z","shell.execute_reply.started":"2021-08-27T15:08:47.856144Z","shell.execute_reply":"2021-08-27T15:08:47.924798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = KNeighborsClassifier(n_neighbors=5,metric='minkowski',p=2)\nmodel.fit(x_train,y_train)","metadata":{"execution":{"iopub.status.busy":"2021-08-27T15:08:47.927083Z","iopub.execute_input":"2021-08-27T15:08:47.927442Z","iopub.status.idle":"2021-08-27T15:08:47.943971Z","shell.execute_reply.started":"2021-08-27T15:08:47.927405Z","shell.execute_reply":"2021-08-27T15:08:47.942922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluation","metadata":{}},{"cell_type":"code","source":"y_pred=model.predict(x_test)\n\n#predict the models and probabilities\ny_pred_proba=model.predict_proba(x_test)[:,1]","metadata":{"execution":{"iopub.status.busy":"2021-08-27T15:08:47.945531Z","iopub.execute_input":"2021-08-27T15:08:47.94611Z","iopub.status.idle":"2021-08-27T15:08:47.968505Z","shell.execute_reply.started":"2021-08-27T15:08:47.946065Z","shell.execute_reply":"2021-08-27T15:08:47.967626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Confusion Matrix","metadata":{}},{"cell_type":"markdown","source":"**Confusion** **matrix**\nA confusion matrix is a table that is often used to describe the performance of a classification model\n**true positives (TP)**: These are cases in which we predicted yes (they have the disease), and they do have the disease.\n\n**true negatives (TN):** We predicted no, and they don't have the disease.\n\n**false positives (FP):** We predicted yes, but they don't actually have the disease. (Also known as a \"Type I error.\")\n\n**false negatives (FN):** We predicted no, but they actually do have the disease. (Also known as a \"Type II error.\")\n\n**precision** - What proportion of positive identifications was actually correct?\n**recall** - What proportion of actual positives was identified correctly?\n\n**F1 Score**\n\n* F1 Score is the weighted average of Precision and Recall\n* F1 is usually more useful than accuracy, especially if you have an uneven class distribution.","metadata":{}},{"cell_type":"code","source":"import numpy as np  \n\nnp.random.seed(1001)\n# np.random.seed   it can generate same random numbers on multiple executions of the code on the same machine","metadata":{"execution":{"iopub.status.busy":"2021-08-27T15:08:47.969546Z","iopub.execute_input":"2021-08-27T15:08:47.970046Z","iopub.status.idle":"2021-08-27T15:08:47.97381Z","shell.execute_reply.started":"2021-08-27T15:08:47.970008Z","shell.execute_reply":"2021-08-27T15:08:47.973021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, classification_report\n\n#Importing cohen_kappa_score and roc_auc_score metrices\nfrom sklearn.metrics import cohen_kappa_score, roc_auc_score\nfrom sklearn.metrics import roc_curve, auc\n\n#importing visualizing library\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#logloss to check is there loss or difference\nfrom sklearn.metrics import log_loss\n\n#Creating a Function name called Classification Metric\ndef classification_metric(y_test,y_pred,y_prob,label,n=1,verbose=False):\n    \"\"\"\n    Note: only for binary classification\n    confusionmatrix(y_true,y_pred,labels=['No','Yes'])\n    \"\"\"\n    # confusion matrix\n    \n    cm = confusion_matrix(y_test,y_pred)\n    row_sum = cm.sum(axis=0)\n    cm = np.append(cm,row_sum.reshape(1,-1),axis=0)\n    col_sum = cm.sum(axis=1)\n    cm = np.append(cm,col_sum.reshape(-1,1),axis=1)\n\n    labels = label+['Total']\n    \n    plt.figure(figsize=(10,6))\n    #plotting a fig size as 10 width and 6 height\n    \n    \n    sns.heatmap(cm,annot=True,cmap='summer',fmt='0.2f',xticklabels=labels,\n                yticklabels=labels,linewidths=3,cbar=None,)\n    #create a heapmap using seaborn libarary and used various parametere\n\n    plt.xlabel('Predicted Values')\n    #ploting the values on x- axis as Predicted values\n    \n    plt.ylabel('Actual Values')\n    #ploting the values on y- axis as actual values\n    \n    plt.title('Confusion Matrix')\n    # Mentioning the title of the figure\n    \n    plt.show()\n    #show the image\n    \n    print('*'*30+'Classifcation Report'+'*'*30+'\\n\\n')\n    #showing * are to put a  line to style\n    \n    #created classification report\n    cr = classification_report(y_test,y_pred)\n    \n    #print the classifiaction report\n    print(cr)\n    \n    print('\\n'+'*'*36+'Kappa Score'+'*'*36+'\\n\\n')\n    \n    \n    # Kappa score\n    kappa = cohen_kappa_score(y_test,y_pred) # Kappa Score\n    print('Kappa Score =',kappa)\n    \n    print('\\n'+'*'*30+'Area Under Curve Score'+'*'*30+'\\n\\n')\n    # Kappa score\n    roc_a = roc_auc_score(y_test,y_pred) # Kappa Score\n    print('AUC Score =',roc_a)\n    \n    # ROC\n    \n    \n    plt.figure(figsize=(8,5))\n    #plot the figuare based on width and height sizes\n    \n    fpr,tpr, thresh = roc_curve(y_test,y_prob)\n    #fpr false positive rate\n    #tpr true positive rate\n    \n    plt.plot(fpr,tpr,'r')\n    print('Number of probabilities to build ROC =',len(fpr))\n    if verbose == True:\n        for i in range(len(thresh)):\n            if i%n == 0:\n                plt.text(fpr[i],tpr[i],'%0.2f'%thresh[i])\n                plt.plot(fpr[i],tpr[i],'v')\n\n\n    plt.xlabel('False Positive Rate')\n    #fpr on x -axis \n    \n    plt.ylabel('True Positive Rate')\n    #tpr on y axis\n    \n    plt.title('Receiver Operating Characterstic')\n    #mentioning the title of the figuare\n    \n    plt.legend(['AUC = {}'.format(roc_a)])\n    #assign the legend to the figuare\n    \n    plt.plot([0,1],[0,1],'b--',linewidth=2.0)\n    #mentioning then line width as 2.0\n    \n    plt.grid()\n    # show the grid lines to the image\n    \n    plt.show()\n    #display the image\n    \n  # A point beyond which there is a change in the manner a program executes  \nclass threshold():\n    '''\n    Setting up the threshold points\n    '''\n    def __init__(self):\n        self.th = 0.5\n        \n    def predict_threshold(self,y):\n        if y >= self.th:\n            return 1\n        else:\n            return 0","metadata":{"execution":{"iopub.status.busy":"2021-08-27T15:08:47.974807Z","iopub.execute_input":"2021-08-27T15:08:47.975199Z","iopub.status.idle":"2021-08-27T15:08:47.990927Z","shell.execute_reply.started":"2021-08-27T15:08:47.975163Z","shell.execute_reply":"2021-08-27T15:08:47.990189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"classification_metric(y_test,y_pred,y_pred_proba,['no','yes'],n=1,verbose=True)","metadata":{"execution":{"iopub.status.busy":"2021-08-27T15:08:47.991934Z","iopub.execute_input":"2021-08-27T15:08:47.992361Z","iopub.status.idle":"2021-08-27T15:08:48.290345Z","shell.execute_reply.started":"2021-08-27T15:08:47.992324Z","shell.execute_reply":"2021-08-27T15:08:48.289456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Kappa Score :\n\nIt can also be used to assess the performance of a classification model.\nwe know that Cohen’s kappa is a useful evaluation metric when dealing with imbalanced data\nCohen's kappa coefficient (κ) is a statistic that is used to measure inter-rater reliability\nCohen’s kappa tries to correct the evaluation bias by taking into account the correct classification by a random guess\n\n## AUC\n\nIts provies an aggregative measure of performance occurs all posibile classification thresholds\nit talks about linearty about the dataset\nAUC starts from 0 to 1\nThe higher the AUC, the better the performance of the model at distinguishing between the positive and negative classes.\n## ROC (Receiver Operating Characterstic curve):\n\nIt shows the performance of the model through all thresholds\ncurve plot between two parameters Tpr (Sensivity) Fpr (specifity)","metadata":{}},{"cell_type":"markdown","source":"* Above we can understand that Kappa Score is 0.116 is very less\n* AUC score is moderate i.e 0.53 , we wnat to increase\n* **Reason**: The dataset is an imbalance dataset so, the kappa score and auc score are low, we can make it balanace by using some of techniques","metadata":{}},{"cell_type":"markdown","source":"* Above we can understand that Kappa Score is 0.116 is very less\n* AUC score is moderate i.e 0.53 , we wnat to increase\n* **Reason**: The dataset is an imbalance dataset so, the kappa score and auc score are low, we can make it balanace by using some of techniques","metadata":{}},{"cell_type":"markdown","source":"##Solution for unbalanced dataset:\n* is oversamplling techique i.e is Syntetic Minority Oversample Techique(SMOTE)\n* Works based on K-NN","metadata":{}},{"cell_type":"markdown","source":"#Synthetic Minority Oversampling Technique\n\n* **SMOTE:** Synthetic Minority Oversampling Technique\n* SMOTE is an oversampling technique where the synthetic samples are generated for the minority class.\n* This algorithm helps to overcome the overfitting problem posed by random oversampling.\n* It focuses on the feature space to generate new instances with the help of interpolation between the positive instances that lie together.\n* General idea to carry out this technique is to bring the minority class values ( either 0 or 1 ) to a comparable number in terms of the other class . In other words to match up the length of the other class.","metadata":{}},{"cell_type":"markdown","source":"# **Linear Discriminent Analysis**","metadata":{}},{"cell_type":"code","source":"from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\n#Initialize the Linear Discriminant Analysis Classifier\nmodel = LinearDiscriminantAnalysis()\n\n#Train the model using Training Dataset\nmodel.fit(x_train, y_train)\n\n# Prediction using test data\ny_pred = model.predict(x_test)\n\n# Calculate Model accuracy by comparing y_test and y_pred\nacc_lda = round( accuracy_score(y_test, y_pred) * 100, 2 )\nprint( 'Accuracy of Linear Discriminant Analysis Classifier: ', acc_lda )","metadata":{"execution":{"iopub.status.busy":"2021-08-27T15:08:48.291503Z","iopub.execute_input":"2021-08-27T15:08:48.291756Z","iopub.status.idle":"2021-08-27T15:08:48.395453Z","shell.execute_reply.started":"2021-08-27T15:08:48.291729Z","shell.execute_reply":"2021-08-27T15:08:48.394515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **GaussianNB**","metadata":{}},{"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\n\n#Initialize the Gaussian Naive Bayes Classifier\nmodel = GaussianNB()\n\n#Train the model using Training Dataset\nmodel.fit(x_train, y_train)\n\n# Prediction using test data\ny_pred = model.predict(x_test)\n\n# Calculate Model accuracy by comparing y_test and y_pred\nacc_ganb = round( accuracy_score(y_test, y_pred) * 100, 2 )\nprint( 'Accuracy of Gaussian Naive Bayes : ', acc_ganb )","metadata":{"execution":{"iopub.status.busy":"2021-08-27T15:08:48.396505Z","iopub.execute_input":"2021-08-27T15:08:48.396743Z","iopub.status.idle":"2021-08-27T15:08:48.411625Z","shell.execute_reply.started":"2021-08-27T15:08:48.396719Z","shell.execute_reply":"2021-08-27T15:08:48.410521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Decision Tree**","metadata":{}},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\n\n#Initialize the Decision Tree Classifier\nmodel = DecisionTreeClassifier()\n\n#Train the model using Training Dataset\nmodel.fit(x_train, y_train)\n\n# Prediction using test data\ny_pred = model.predict(x_test)\n\n# Calculate Model accuracy by comparing y_test and y_pred\nacc_dtree = round( accuracy_score(y_test, y_pred) * 100, 2 )\nprint( 'Accuracy of  Decision Tree Classifier : ', acc_dtree )\n","metadata":{"execution":{"iopub.status.busy":"2021-08-27T15:08:48.413021Z","iopub.execute_input":"2021-08-27T15:08:48.413391Z","iopub.status.idle":"2021-08-27T15:08:48.462198Z","shell.execute_reply.started":"2021-08-27T15:08:48.413352Z","shell.execute_reply":"2021-08-27T15:08:48.461135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Random Forest**","metadata":{}},{"cell_type":"code","source":"#Import Library for Random Forest\nfrom sklearn.ensemble import RandomForestClassifier\n\n#Initialize the Random Forest\nmodel = RandomForestClassifier()\n\n#Train the model using Training Dataset\nmodel.fit(x_train, y_train)\n\n# Prediction using test data\ny_pred = model.predict(x_test)\n\n# Calculate Model accuracy by comparing y_test and y_pred\nacc_rf = round( accuracy_score(y_test, y_pred) * 100, 2 )\nprint( 'Accuracy of  Random Forest : ', acc_rf )","metadata":{"execution":{"iopub.status.busy":"2021-08-27T15:08:48.463602Z","iopub.execute_input":"2021-08-27T15:08:48.463987Z","iopub.status.idle":"2021-08-27T15:08:48.798422Z","shell.execute_reply.started":"2021-08-27T15:08:48.463948Z","shell.execute_reply":"2021-08-27T15:08:48.797526Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}