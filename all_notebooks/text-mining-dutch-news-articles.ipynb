{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Text Mining Dutch News Articles\n\nIn this article, I analyze Dutch news articles. I gathered the news articles by scraping them from the NOS [website](https://nos.nl/). The NOS is one of the organizations of the public broadcasting system in the Netherlands and established in the Media Act. Its task is to provide the media supply for the national public media service in the field of news, sports, and events. The NOS is also one of the biggest online news websites in the Netherlands.\n\nAs of the publishing of this post, the dataset contains 218,609 news articles spanning more than 10 years (the first article in the archive is published on 2020-01-01). In addition to the article content, the dataset contains the article title, the category, and the publishing grade. The NOS also publishes liveblogs. I do not consider these news articles. Therefore they are excluded from the dataset.","metadata":{}},{"cell_type":"code","source":"!pip install -U textblob-nl\n!pip install -U textstat\n\nimport unicodedata\nimport re\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as mtick\nimport seaborn as sns\nimport textstat\n\nfrom matplotlib.ticker import PercentFormatter\nfrom tqdm.notebook import tqdm\n\nfrom textblob import TextBlob\nfrom textblob_nl import PatternAnalyzer\n\n# set language to dutch \ntextstat.set_lang(\"nl\")\n\n# progress bar when apply functions to dataframe\ntqdm.pandas()\n\n# better quality plots\n%config InlineBackend.figure_format = \"retina\"\n\n# Title font size\nTITLE_SIZE = 14","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-output":true,"_kg_hide-input":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get data\nnos_df = (pd.read_csv(\"/kaggle/input/dutch-news-articles/dutch-news-articles.csv\",\n                      parse_dates=[\"datetime\"], encoding=\"utf-8\") \n          # sort values by date published\n          .sort_values(\"datetime\")\n          )\n# Convert category colum to category\nnos_df.category = pd.Categorical(nos_df.category)\n\n# Display first three rows\nnos_df.head(3)","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Before analyzing the news articles, the raw data needs to be cleaned. The texts contain a lot of white spaces and some weird characters. I use regular expressions and some Python string methods to remove extra white spaces and these wrong characters.","metadata":{}},{"cell_type":"code","source":"# lambda function to clean multiple spaces and non-breaking spaces\ndef clean_string(x):\n    x = re.sub(\" +\", \" \", x)\n    x = x.replace(\"\\n\", \" \")\n    x = unicodedata.normalize(\"NFKD\", x)\n    return x\n\n# clean content \nnos_df.content = nos_df.content.progress_apply(clean_string)\n\n# clean title\nnos_df.title = nos_df.title.progress_apply(clean_string)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After cleaning the content of an article will look something like this:","metadata":{}},{"cell_type":"code","source":"nos_df.content.iloc[1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get list of all categories\ncategories = nos_df['category'].unique().tolist()\n\n# Translate categories from Dutch to English\ntranslate_categories = {\n    \"Buitenland\": \"Foreign\",\n    \"Binnenland\": \"Domestic\",\n    \"Politiek\": \"Politics\",\n    \"Economie\": \"Economy\",\n    \"Koningshuis\": \"Royal family\",\n    \"Opmerkelijk\": \"Remarkable\",\n    \"Tech\": \"Tech\",\n    \"Cultuur & Media\": \"Culture & Media\",\n    \"Regionaal nieuws\": \"Regional news\"\n}\n\n# Apply mapping\nnos_df[\"category_en\"] = nos_df.category.map(translate_categories)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Article Counts\n\nThe first thing to look at is the article count over time between 2010 and 2020. To get a less stochastic article count, I decided to group them by months over time. From 2010 to the middle of 2016, the monthly article count increased from around 1000 to a little more than 2000. After 2016 we see a downwards trend in the number of monthly articles published to 1100 in 2020.","metadata":{}},{"cell_type":"code","source":"# Function that groups times series\ndef group_timeseries(df, column, normalize=False, count=False):\n    # Group by year and month and select colimn\n    group = df.groupby([df.datetime.dt.year, df.datetime.dt.month])[column]\n\n    # Value count and unstack to dataframe\n    if count:\n        df = group.value_counts(normalize=normalize).unstack()\n\n    # Count \n    else:\n        df = group.count()\n\n    # Select year and month index\n    years = pd.Series(df.index.get_level_values(level=0).values.astype(str))\n    months = pd.Series(df.index.get_level_values(level=1).values.astype(str))\n\n    # Combine year and month to new date\n    dates = pd.to_datetime(years + \"-\" + months)\n\n    # Set date as index\n    df = df.reset_index(drop=True)\n    df.index = dates\n\n    return df\n\n# Overall absolute count over articles\nall_overtime = group_timeseries(nos_df, \"category_en\", count=False)\n\n# Get absolute and normalized grouped time series\ncat_timeseries_absolute = group_timeseries(nos_df, \"category_en\", count=True)\ncat_timeseries_normalized = group_timeseries(nos_df, \"category_en\", normalize=True, count=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Intialize figure\nfig, ax = plt.subplots(figsize=(14, 4), nrows=1)\n\n# Plot absolute number of acticles over time\nall_overtime.plot(ax=ax, legend=None, lw=2, color=\"gray\")\nall_overtime.plot(kind=\"area\", ax=ax, legend=None, lw=2, alpha=0.1, color=\"gray\")\nax.set_title(\"Number of articles publised every month on NOS.nl\", fontsize=TITLE_SIZE)\nax.set_ylabel(\"# articles\")\nax.set_ylim(0)\n\n# Despine and tightlayout\nsns.despine()\nfig.tight_layout(pad=3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"More interesting is probably to look at the change of monthly article count when we split the articles based on category. If we look at the absolute count over time, we see the same trend as the overall article count. This is most likely caused by the \"Binnenland\" (domestic) and the \"Buitenland\" (foreign) categories, which display the same tends.","metadata":{}},{"cell_type":"code","source":"# Intialize figure\nfig, ax = plt.subplots(figsize=(14, 8), nrows=2)\n\n# Figure 1: Absolute number of acticles per category\n(cat_timeseries_absolute\n .plot(lw=2,\n       ax=ax[0],\n       legend=None))\nax[0].set_title(\"Absolute number of articles per category over time\", fontsize=TITLE_SIZE)\n\n# Figure 2: Normalized number of acticles per category\n(cat_timeseries_normalized\n .plot(lw=2, \n       legend=None, \n       ax=ax[1]) )\nax[1].set_title(\"Normalized number of articles per category over time\", fontsize=TITLE_SIZE)\n\n# Format yaxis ticks as percent\nax[1].yaxis.set_major_formatter(PercentFormatter(xmax=1))\n\n# Only one legend for both axes\nhandles, labels = ax[1].get_legend_handles_labels()\nfig.legend(handles, labels, frameon=False, ncol=10, loc=\"lower center\")\n\n# Despine and tightlayout\nsns.despine()\nfig.tight_layout(pad=3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Therefore I choose to normalize the counts based on category. The second graph shows the share of a news category in the total number of articles in a specific month. For example, there is a large bump in the domestic news share (and drop in foreign news) in the first quarter of 2020 caused by the initial Coronavirus outbreak. In contrast, most of 2011, the foreign news articles were most prevalent, probably caused by the start of the [Arabic Spring](https://en.wikipedia.org/wiki/Arab_Spring), the [capture of Osama Binladen](https://en.wikipedia.org/wiki/Killing_of_Osama_bin_Laden), and the [terrorist attacks in Norway](https://en.wikipedia.org/wiki/2011_Norway_attacks).","metadata":{}},{"cell_type":"markdown","source":"## Word Count\n\nThe word count of an article can be an indicator of the quality of the news article. Blumenstock et al. [[1]](https://dl.acm.org/doi/10.1145/1367497.1367673) show that longer Wikipedia articles are more likely of better quality. Longer articles are more often featured than shorter Wikipedia articles. They state that word count is a robust method of article quality.\n\nLet us first look at the overall word count of all the articles published by the NOS. As expected, the word count of the article is heavily right-skewed (when used for modeling probably better to use the log). Also, the word count of the title is more normally distributed, which is expected.","metadata":{}},{"cell_type":"code","source":"# Calculate word count of title and article\nnos_df[\"title_word_count\"] = nos_df[\"title\"].str.split(\" \").str.len()\nnos_df[\"content_word_count\"] = nos_df[\"content\"].str.split(\" \").str.len()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialze figure\nfig, ax = plt.subplots(figsize=(14, 4), ncols=2)\n\n# Plot article word count distribution\nnos_df.content_word_count.plot(kind='hist', bins=50, ax=ax[0], edgecolor=\"k\", color=\"gray\")\nax[0].set_title(\"Article word count\", fontsize=TITLE_SIZE)\n\n# Plot article title word count distribution\nnos_df.title_word_count.value_counts().sort_index().plot(kind='bar', ax=ax[1], edgecolor=\"k\", ylabel=\"Frequency\", color=\"gray\")\nax[1].set_title(\"Title word count\", fontsize=TITLE_SIZE)\n\n# Despine\nsns.despine();","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Tech and Culture & Media have the highest median word count. This makes sense to me. These articles are often less time-dependent. Meaning that their publishing is less dependent on the daily news cycle. They are still relevant at a later moment than political news. This suggests that writers have more time to write these articles, can incorporate more detail, leading to longer pieces. The “remarkable” and “regional news” category have the lowest median news count. This is expected most of these articles report small news events, leading to shorter pieces.","metadata":{}},{"cell_type":"code","source":"categories_en = nos_df['category_en'].unique().tolist()\n\n# Initialize plots\nfig, axes = plt.subplots(3, 3, figsize=(14, 8), sharex=True)\n\n# Loop over axes and plot category\nfor cat, ax in enumerate(axes.flatten()):\n\n    # Select df with dataframe\n    category_df = nos_df[nos_df.category_en == categories_en[cat]].content_word_count\n\n    # Bar plot word count category\n    category_df.plot(kind=\"kde\", ax=ax, title=f\"{categories_en[cat]} (median: {category_df.median()})\", color=\"gray\", lw=2)\n\n    # Limit x axise\n    ax.set_xlim(0, 2000)\n\n# Plot meta data\nfig.suptitle(\"Word count articles by category\", fontsize=TITLE_SIZE, y=1)\nfig.tight_layout(pad=3)\n\nsns.despine();","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Readability\n\nAnother important of a news article is how easy it is to read. Readability measures how hard it is to read a text. There are several methods to calculate readability. These methods are not that reliable, but they give some insight into the readability of a written text. In this analysis, I use the Flesch reading-ease test to calculate readability. Douma [[2]](https://research.wur.nl/en/publications/de-leesbaarheid-van-landbouwbladen-een-onderzoek-naar-en-een-toep) slightly tweaked the values in the Flesch reading-ease test formula such that the measures can also be used for Dutch texts.\n\n\n<details>\n  <summary><b>More on the calculation and interpretation of the Flesch-Douma formula</b></summary>\n    <br>\n  The Flesch-Douma formula equates readability to the following formula:\n    <br>\n  $$ \\text{readability} = 206.835 - 0.93 \\bigg{(}\\frac{\\text{total words}}{\\text{total sentences}}\\bigg{)} - 77.0 \\bigg{(}\\frac{\\text{total syllables}}{\\text{total words}}\\bigg{)}$$\n    <br>\n The readability score is related to education level in the following manner:\n    \n| Score  | School level       | Notes                                                                   |\n|--------|--------------------|-------------------------------------------------------------------------|\n| 100–90 | 5th grade          | Very easy to read. Easily understood by an average 11-year-old student. |\n| 90–80  | 6th grade          | Easy to read. Conversational English for consumers.                     |\n| 80-70  | 7th grade          | Fairly easy to read.                                                    |\n| 70-60  | 8th & 9th grade    | Plain English. Easily understood by 13- to 15-year-old students.        |\n| 60-50  | 10th to 12th grade | Fairly difficult to read.                                               |\n| 50-30  | College            | Difficult to read.                                                      |\n| 30-10  | College graduate   | Very difficult to read. Best understood by university graduates.        |\n| 10-0   | Professional       | Extremely difficult to read. Best understood by university graduates.   |\n\n</details>\n<br>\nThe articles categorized as Foreign, Politics, Tech, and Economy are the heardest to read with a readability score below 60, which is about 10th to 12th-grade-level reading. So somewhat hard to read. Categories “remarkable news”, “regional news” and “culture & media” are easier to read with a score above 60. However, the overall median scores are not that different across categories.","metadata":{}},{"cell_type":"code","source":"# Calculate reading ease\nflesh_reading_ease = lambda x: textstat.flesch_reading_ease(str(x))\nnos_df[\"content_flesh_reading_ease\"]  = nos_df.content.progress_apply(flesh_reading_ease)\n\n# filter scores\nreading = nos_df[(nos_df.content_flesh_reading_ease > 0) & (nos_df.content_flesh_reading_ease < 150)]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get column order sorted by median\norder = reading.groupby([\"category_en\"])[[\"content_flesh_reading_ease\"]].median().unstack().sort_values(ascending=False).index\nreading_order = reading[[\"category_en\", \"content_flesh_reading_ease\"]].pivot(columns=\"category_en\")[order][\"content_flesh_reading_ease\"]\n\n# Boxplot\nreading_order.boxplot(grid=False, figsize=(12, 8), vert=False, color=\"gray\")\n\n# Meta data\nplt.suptitle(\"Flesh Reading Ease Score for NOS Articels by Category\")\nplt.xlabel(\"Flesh Reading Ease Score\")\nsns.despine();","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Sentiment, Polarity, and Subjectivity\n\nLet us look at the sentiment of the texts. Are they positive or negative? Furthermore, we can also analyze the texts for subjectivity. When looking at all the articles together, we see that most of the news articles are neutral in their polarity. Most of the articles hover somewhere in the middle between subjective and objective in their subjectivity measure.","metadata":{}},{"cell_type":"code","source":"# Extract sentiment\nnos_df[\"sentiment\"] = nos_df.content.progress_apply(lambda x: tuple(TextBlob(text=x, analyzer=PatternAnalyzer()).sentiment))\n\n# Get polarity from tuple \nnos_df[\"polarity\"] = nos_df[\"sentiment\"].str.get(0)\n\n# Get subjectivity from tuple \nnos_df[\"subjectivity\"] = nos_df[\"sentiment\"].str.get(1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialze figure\nfig, ax = plt.subplots(figsize=(14, 3), ncols=2)\n\n# Plot polarity\nnos_df[\"polarity\"].plot(kind=\"hist\", bins=50, edgecolor=\"k\", color=\"gray\", ax=ax[0], title=f\"Polarity (median: {nos_df.polarity.median():.2f})\")\n\n# Plot subjectivity\nnos_df[\"subjectivity\"].plot(kind=\"hist\", bins=50, edgecolor=\"k\", color=\"gray\", ax=ax[1], title=f\"Subjectivity (median: {nos_df.subjectivity.median():.2f})\")\n\n# Limits\nax[0].set_xlim(-1, 1)\nax[1].set_xlim(0, 1)\n\n# Meta-data plot\nfig.tight_layout()\nsns.despine()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Even if we parse the articles by news category we do not see many differences in the distributions of polarity and subjectivity scores.","metadata":{}},{"cell_type":"code","source":"# Initialize plots\nfig, axes = plt.subplots(3, 3, figsize=(14, 8), sharex=True)\n\n# Loop over axes and plot category\nfor cat, ax in enumerate(axes.flatten()):\n\n    # Select df with dataframe\n    polarity_category_df = nos_df[nos_df.category_en == categories_en[cat]].polarity\n    subjectivity_category_df = nos_df[nos_df.category_en == categories_en[cat]].subjectivity\n\n    # Bar plot word count category\n    polarity_category_df.plot(kind=\"kde\", ax=ax, color=\"gray\", lw=2, title=f\"{categories_en[cat]}\",label=f\"Polarity median: {round(polarity_category_df.median(), 2)}\")\n    subjectivity_category_df.plot(kind=\"kde\", ax=ax, lw=2, label=f\"Subjectivity median: {round(subjectivity_category_df.median(), 2)}\")\n    ax.legend()\n\n# Plot meta data\nfig.suptitle(\"Polarity and subjectivity distribution across categories.\", fontsize=TITLE_SIZE, y=1)\nfig.tight_layout(pad=3)\n\nsns.despine();","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Informative words\n\nWe can use TF-IDF, short for term frequency-inverse document frequency, to extract the most informative words out of every article. TF-IDF is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. In this case, I removed stop words, because most of the time they don’t contain any useful information. Also, I ignore terms that appear in less than 0.5% of the documents. Let’s see if what the most informative terms are in the news articles in every category.","metadata":{}},{"cell_type":"code","source":"# Read Dutch stopwords\nstop_words = list(pd.read_csv(\"https://raw.githubusercontent.com/stopwords-iso/stopwords-nl/master/stopwords-nl.txt\", header=None).values.flatten())\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Fit TF-IDF vectorizer\ntfidf = TfidfVectorizer(min_df=0.005, stop_words=stop_words)\ntfidf.fit(nos_df.content);\n\n# transform nos_df article text to tfidf vectors\ntfidf_vect = tfidf.transform(nos_df.content)\n\n# Store dataframe\ntfidf_df = pd.DataFrame(tfidf_vect.toarray(), columns=tfidf.get_feature_names())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Insert category column in TF-IDF matrix\ntfidf_df['category_en'] = nos_df.category_en\n\n# Get unqiue categories\ncategories = tfidf_df['category_en'].unique().tolist()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialize plots\nfig, axes = plt.subplots(3, 3, figsize=(14, 8), sharex=True)\n\n# Loop over axes and plot category\nfor cat, ax in enumerate(axes.flatten()):\n  \n    # Sort words by hight TF-IDF mean and by category\n    top = tfidf_df[tfidf_df.category_en == categories[cat]].mean().sort_values().tail(15)\n  \n    # Bar plot most imporant words for every category\n    top.plot(kind=\"barh\", ax=ax, title=categories[cat], color=\"gray\", edgecolor=\"k\")\n\n# Plot meta data\nfig.suptitle(\"How important is a word to a document in a the articles based on category\", y=1.1)\nfig.tight_layout()\n\nsns.despine();","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The TF-IDF result is pretty interesting. For example, we can see that the most informative terms (translated to English) in articles categorized as politics are: chamber, cabinet, minister, political parties (VVD, PvdA, CDA, D66), member of parliament, and secretary of state. All words that are associated with politics. The most informative terms in the economic news articles are also associated with the economy. For example, percent, Euro, company, billion, banks, and money. In the Royal Family category, we see that the words King, Queen, Prince, Princess, Willem (the king’s first name) are all highly informative.","metadata":{}}]}