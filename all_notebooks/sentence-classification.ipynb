{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%load_ext autoreload\n%autoreload 2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install 'tensorflow_text'","metadata":{"id":"QKHd1RGc-WKs","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import json\nimport textwrap\n\nimport pandas as pd\nimport tensorflow_hub as hub\nimport numpy as np\nimport tensorflow_text\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom tabulate import tabulate","metadata":{"id":"7A8LROpU6URG","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tr_data = pd.read_csv(\"../input/dodiom-dataset/tr_corpus_second_run.csv\", converters={\n    'idiom_indices': eval,\n    'idiom_words': eval,\n    'lemmas': eval,\n    'words': eval\n})\ntr_data = tr_data[tr_data.likes + tr_data.dislikes + tr_data.reports >= 0]\ntr_data = tr_data[tr_data.rating >= 0.0]\ntr_data = tr_data.reset_index()\nprint(len(tr_data))\nprint(tr_data.columns)","metadata":{"id":"_bU_zYr665n1","outputId":"7bd79011-5b47-4a55-a484-de78f856e9e9","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-multilingual/3\")","metadata":{"id":"197wliFzETKM","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nidiom_embeds = {idiom: embed([idiom])[0].numpy() for idiom in tr_data.idiom.unique()}\nsentence_embeds = embed([x for x in tr_data.submission])\nsentence_idiom_embeds = embed([\" \".join(x) for x in tr_data.idiom_words])","metadata":{"id":"4xe7GAFmZfaw","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def is_seperate(arr):\n    for i in range(len(arr) - 1):\n        if arr[i+1] != arr[i] + 1:\n            return False\n    return True","metadata":{"id":"tRdasmAQW5oy","outputId":"5cf5e78e-70f1-4a96-f7cb-c8f7939a53de","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tr_data[\"embed\"] = [list(np.concatenate((\n    sentence_embeds[ix],\n    idiom_embeds[idiom],\n    sentence_idiom_embeds[ix],\n    np.array([is_seperate([tr_data.iloc[0].idiom_indices])], dtype=float)\n    ))) for ix, idiom in enumerate(tr_data.idiom)]","metadata":{"id":"7vJMRU1L1IUO","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#X = np.array([np.concatenate((sentence_embeds[ix], idiom_embeds[idiom], sentence_idiom_embeds[ix])) for ix, idiom in enumerate(tr_data.idiom)])\nX = np.arange(len(tr_data))\ny = np.array([int(x == \"idiom\") for x in tr_data.category])","metadata":{"id":"y-LECXr2EROa","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X.shape, y.shape)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True)\n\ndf_train = tr_data.iloc[X_train]\ndf_test = tr_data.iloc[X_test]\n\nX_train = np.stack(df_train.embed, axis=0)\nX_test = np.stack(df_test.embed, axis=0)\n\nprint(len([x for x in df_train.idiom.unique()]), \"idioms\")","metadata":{"id":"LOeNxAJyGVDn","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.shape, y_train.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_data = pd.DataFrame({\n    \"idiom\": [x for x in tr_data.idiom.unique()],\n    \"train pos\": [len(df_train[(df_train.idiom == x) & (df_train.category == \"idiom\")]) for x in tr_data.idiom.unique()],\n    \"train neg\": [len(df_train[(df_train.idiom == x) & (df_train.category == \"nonidiom\")]) for x in tr_data.idiom.unique()],\n    \"test pos\": [len(df_test[(df_test.idiom == x) & (df_test.category == \"idiom\")]) for x in tr_data.idiom.unique()],\n    \"test neg\": [len(df_test[(df_test.idiom == x) & (df_test.category == \"nonidiom\")]) for x in tr_data.idiom.unique()],\n})\ndf_data","metadata":{"id":"SLw5BCB5HaTB","outputId":"cdfb32b5-e22f-4f51-8c29-cabd8c10689a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = tf.keras.Sequential([\n    tf.keras.layers.Dense(256, activation='relu'),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dense(128, activation='relu'),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n])\n\nearlystopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\nopt = tf.keras.optimizers.Adam(learning_rate=0.0001)\nmodel.compile(optimizer=opt,\n              loss=tf.keras.losses.BinaryCrossentropy(),\n              metrics=['accuracy'])\nhist = model.fit(X_train, y_train,\n                 validation_data=(X_test, y_test),\n                 batch_size=32, \n                 epochs=100,\n                 callbacks=[earlystopping])","metadata":{"id":"iVQTfLYKFacT","outputId":"9628afa3-a082-4817-a943-29525f749aa9","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5), edgecolor=\"black\", facecolor=\"white\")\n\nax1.plot(hist.history[\"accuracy\"], label=\"train accuracy\")\nax2.plot(hist.history[\"loss\"], label=\"train loss\")\n\nax1.plot(hist.history[\"val_accuracy\"], label=\"test accuracy\")\nax2.plot(hist.history[\"val_loss\"], label=\"test loss\")\n\nax1.set_ylim(0, 1.1)\nax1.set_xlabel(\"Epoch\")\nax2.set_xlabel(\"Epoch\")\nax1.set_ylabel(\"Accuracy\")\nax2.set_ylabel(\"Loss\")\nax1.legend(loc=\"best\")\nax2.legend(loc=\"best\")\n\nplt.tight_layout()\nplt.show()","metadata":{"id":"09ObpfAXfL_d","outputId":"ccd4284e-14a5-44db-930e-f4bbe7bc41b9","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"idioms = []\nposs = []\nnegs = []\ntps = []\nfps = []\ntns = []\nfns = []\nprecisions = []\nrecalls = []\nf1s = []\n\nmisclass = []\n\ndef wrap(text: str) -> str:\n    return \"\\n\".join(textwrap.wrap(text))\n    \n\nfor idiom in tr_data.idiom.unique():\n    if len(df_test[df_test.idiom == idiom].embed) == 0:\n        continue\n    \n    idioms.append(idiom)\n\n    X = np.stack(df_test[df_test.idiom == idiom].embed, axis=0)\n    y = np.array([int(x == \"idiom\") for x in df_test[df_test.idiom == idiom].category])\n    out = model(X).numpy().flatten()\n\n    outr = np.array(np.rint(out), dtype=int)\n    for ix, yi in enumerate(y):\n        if yi != outr[ix]:\n            item = df_test[df_test.idiom == idiom].iloc[ix]\n            misclass.append([len(misclass) + 1, item.idiom, wrap(item.submission), item.category, out[ix]])\n\n    tpm = tf.keras.metrics.TruePositives()\n    tpm.update_state(y, out)\n    tp = int(tpm.result().numpy())\n    tps.append(tp)\n\n    fpm = tf.keras.metrics.FalsePositives()\n    fpm.update_state(y, out)\n    fp = int(fpm.result().numpy())\n    fps.append(fp)\n\n    tnm = tf.keras.metrics.TrueNegatives()\n    tnm.update_state(y, out)\n    tn = int(tnm.result().numpy())\n    tns.append(tn)\n\n    fnm = tf.keras.metrics.FalseNegatives()\n    fnm.update_state(y, out)\n    fn = int(fnm.result().numpy())\n    fns.append(fn)\n\n    poss.append(tp + fn)\n    negs.append(fp + tn)\n\n    if tp + fp != 0:\n        precision = tp / (tp + fp)\n    else:\n        precision = 0\n    precisions.append(precision)\n\n    if tp + fn != 0:\n        recall = tp / (tp + fn)\n    else:\n        recall = 0\n    recalls.append(recall)\n\n    if precision + recall != 0:\n        f1_score = 2 * ((precision * recall) / (precision + recall))\n    else:\n        f1_score = 0\n    f1s.append(f1_score)\n\nprint(tabulate(misclass, headers=[\"index\", \"Idiom\", \"Submission\", \"Category\" , \"Prediction\"]))","metadata":{"id":"by_TEsv-wwII","outputId":"f05c0ca5-5a1f-4a2d-9e00-0cfad4ce8e68","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eval_data = pd.DataFrame({\n    \"idiom\": idioms,\n    \"POS\": poss,\n    \"NEG\": negs,\n    \"TP\": tps,\n    \"FP\": fps,\n    \"TN\": tns,\n    \"FN\": fns,\n    \"Precision\": precisions,\n    \"Recall\": recalls,\n    \"F1\": f1s\n})\neval_data.sort_values(\"F1\", ascending=False).reset_index()","metadata":{"id":"mN8FxZcu3mGW","outputId":"d676c830-d519-400b-a45d-ba0621eef8da","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Macro avg. F1: {eval_data.F1.mean():.3f}\")","metadata":{"id":"jDjS_1xaBJv7","outputId":"187415c3-4ff8-4c00-ef04-2b73eb026350","trusted":true},"execution_count":null,"outputs":[]}]}