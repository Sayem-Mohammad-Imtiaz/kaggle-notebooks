{"cells":[{"metadata":{"_cell_guid":"fe30c858-0fff-40ee-899f-6306887293da","_uuid":"8aa94cc48c58151c779a7e4967d3f6e204d158e0"},"cell_type":"markdown","source":"# Spam SMS Classification using KNN\n#### as in Machine Learning Class on jiuzhang.com \nhttps://www.kaggle.com/uciml/sms-spam-collection-dataset"},{"metadata":{"_cell_guid":"d6b1ab40-34da-4645-98c4-4b61c97895f6","_uuid":"0259bd59e4fad41ffd78aa5a510a83ef07daab7f","collapsed":true},"cell_type":"markdown","source":"Start from loading data"},{"metadata":{"_cell_guid":"a89760f4-9075-42be-b4c4-c8c87d175c88","_uuid":"4429a082ddabcb23261daec29ee1ae9e68ba2a2e","trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport numpy as np\n\ndata_dir = \"../input/\"\n\ndf = pd.read_csv(data_dir + '/spam.csv', encoding='latin-1')\nprint ('show what kind of data we are dealing with')\nprint (df.head())\n\n# split into train and test\ndata_train, data_test, labels_train, labels_test = train_test_split(\n    df.v2,\n    df.v1, \n    test_size=0.2, \n    random_state=0)   \nprint('')\nprint ('Now print each SMS text after train/test split')\nprint (data_train[:10])\nprint('')\nprint ('Now print labels of SMS texts')\nprint (labels_train[:10])","execution_count":3,"outputs":[]},{"metadata":{"_cell_guid":"00614a72-9f5d-40ad-9e45-1a4b2f74298e","_uuid":"96eeae4a1d59c0ebbdb537097102db4614beda1b"},"cell_type":"markdown","source":"\nNow we want to count the number of all unique words"},{"metadata":{"_cell_guid":"721995ec-f0ee-4d0f-95f1-7c6c9903ff30","_uuid":"9c2746c72a6aab56a5b1efbf5f641559dbc89e14","trusted":false},"cell_type":"code","source":"\ndef GetVocabulary(data): \n    vocab_set = set([])\n    for document in data:\n        words = document.split()\n        for word in words:\n            vocab_set.add(word) \n    return list(vocab_set)\n\nvocab_list = GetVocabulary(data_train)\nprint ('Number of all the unique words : ' + str(len(vocab_list)))\n\n","execution_count":2,"outputs":[]},{"metadata":{"_cell_guid":"d7a2259d-801e-4e0d-a880-177e5b433e00","_uuid":"bfd54ce276dc4227055e55e74eec3032f792165d"},"cell_type":"markdown","source":"Now, we will vectorize each SMS\n"},{"metadata":{"_cell_guid":"772db8d0-2106-43ff-a3c4-b404d25c3ab2","_uuid":"5d3734d61dd084d0c8a2f46fa1c1c3c26f8ce04a","trusted":false},"cell_type":"code","source":"\ndef Document2Vector(vocab_list, data):\n    word_vector = np.zeros(len(vocab_list))\n    words = data.split()\n    for word in words:\n        if word in vocab_list:\n            word_vector[vocab_list.index(word)] += 1\n    return word_vector\n\nprint (data_train[1:2,])\nans = Document2Vector(vocab_list,\"the the the\")\nprint (data_train.values[2])\n","execution_count":3,"outputs":[]},{"metadata":{"_cell_guid":"5033458e-55fc-4a18-aba3-1b1ecbfce2ab","_uuid":"f8e259bb2ef260294478c4ec7929cdd10b68d84a","trusted":false},"cell_type":"code","source":"train_matrix = []\nfor document in data_train.values:\n    word_vector = Document2Vector(vocab_list, document)\n    train_matrix.append(word_vector)\n\nprint (len(train_matrix))","execution_count":4,"outputs":[]},{"metadata":{"_cell_guid":"5367c702-7954-4b16-a864-222908ddfab2","_uuid":"ed22ab8ea5890c59212d5063568d7a0813c2c287"},"cell_type":"markdown","source":"Now, we will use Naive Bayes model to \"train\" the training set, and get probability of each feature(word)"},{"metadata":{"_cell_guid":"45c35f19-8fb5-42ca-b5a5-9e3a10cf09cd","_uuid":"773bfa1e4819a5986eda2ad09dc47f7e4decc1e0","trusted":false},"cell_type":"code","source":"# this is not using SKlearn model\ndef NaiveBayes_train(train_matrix,labels_train):\n    num_docs = len(train_matrix)\n    num_words = len(train_matrix[0])\n    \n    spam_vector_count = np.ones(num_words);\n    ham_vector_count = np.ones(num_words)  #initialize the count as 1 for each word\n    spam_total_count = num_words;\n    ham_total_count = num_words                  #this is Laplacian smooth\n    \n    spam_count = 0\n    ham_count = 0\n    for i in range(num_docs):\n        if i % 500 == 0:\n            print ('Train on the doc id:' + str(i))\n            \n        if labels_train[i] == 'spam':\n            ham_vector_count += train_matrix[i]\n            ham_total_count += sum(train_matrix[i])\n            ham_count += 1\n        else:\n            spam_vector_count += train_matrix[i]\n            spam_total_count += sum(train_matrix[i])\n            spam_count += 1\n    \n    print (ham_count)\n    print (spam_count)\n    \n    p_spam_vector = np.log(ham_vector_count/ham_total_count)#return probability vector\n    p_ham_vector = np.log(spam_vector_count/spam_total_count)#return a priori probabiligy\n    return p_spam_vector, np.log(spam_count/num_docs), p_ham_vector, np.log(ham_count/num_docs)\n\n    \np_spam_vector, p_spam, p_ham_vector, p_ham = NaiveBayes_train(train_matrix, labels_train.values)","execution_count":5,"outputs":[]},{"metadata":{"_cell_guid":"30ef732e-8668-414c-90d2-71b24403186b","_uuid":"aef1a42f58b8aa2805bb13939416e605530c05a0","trusted":false},"cell_type":"code","source":"data_test.values.shape","execution_count":6,"outputs":[]},{"metadata":{"_cell_guid":"e4415b37-0855-49bd-8926-afe63c668268","_uuid":"1d737630d3f2df2b3d28355f57cabbf594c2bf23","trusted":false},"cell_type":"code","source":"\n    \ndef Predict(test_word_vector,p_spam_vector, p_spam, p_ham_vector, p_ham):\n    \n    spam = sum(test_word_vector * p_spam_vector) + p_spam\n    ham = sum(test_word_vector * p_ham_vector) + p_ham\n    if spam > ham:\n        return 'spam'\n    else:\n        return 'ham'\n\npredictions = []\ni = 0\nfor document in data_test.values:\n    if i % 200 == 0:\n        print ('Test on the doc id:' + str(i))\n    i += 1    \n    test_word_vector = Document2Vector(vocab_list, document)\n    ans = Predict(test_word_vector, p_spam_vector, p_spam, p_ham_vector, p_ham)\n    predictions.append(ans)\n\nprint (len(predictions))","execution_count":7,"outputs":[]},{"metadata":{"_cell_guid":"33beb08e-dfd5-4b74-81bb-4740bd9813b6","_uuid":"b04b025a8ebfbfccbf85acce2714a2563b09e670","trusted":false},"cell_type":"code","source":"# now, evaluate the model\n\nfrom sklearn.metrics import accuracy_score,classification_report,confusion_matrix\nfrom sklearn.model_selection import cross_val_score\n\n\nprint (accuracy_score(labels_test, predictions))\nprint (classification_report(labels_test, predictions))\nprint (confusion_matrix(labels_test, predictions))\n","execution_count":8,"outputs":[]},{"metadata":{"_uuid":"667139f42e4c65b0f0f680199bcccef9c29565b7"},"cell_type":"markdown","source":"What we learned in this study:\n    1. Naive Bayes is a simplistic model that predicts spam/ham by calculating the probability of representative keywords\n    2. Advantages of Naive Bayes are relatively simple, good performance, fast training\n    3. However, it assumes, by default, features are independent, that is, word and word are not related\n    4. One method to improve word-word correlation is to change Ngram to >1. For example, \"white house\" should be considered as one word\n    5. In short, weakness of Naive Bayes is that it cannot learn interdependency between features, because it simply assumes features are independent of each other"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":2},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython2","version":"2.7.13"}},"nbformat":4,"nbformat_minor":1}