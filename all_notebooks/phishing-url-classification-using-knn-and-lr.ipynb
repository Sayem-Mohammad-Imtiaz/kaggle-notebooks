{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom nltk.tokenize import RegexpTokenizer  \nfrom nltk.stem.snowball import SnowballStemmer\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.feature_extraction.text import CountVectorizer  \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import make_pipeline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df= pd.read_csv(\"../input/phishing-site-urls/phishing_site_urls.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Removind Duplicates in Dataset Data.\n\nThe data set contains some duplicate data. You should probably remove them. Duplicates are an extreme case of nonrandom sampling, and they bias your fitted model. Including them will essentially lead to the model overfitting this subset of points."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.drop_duplicates()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Let study the data!\n\nThe dataset contains a total of 507,196 unique rows and 2 columns. The data consists of URLs and each URL has a lable which denotes if it is a phishy URL with the label 'bad' and for a non phishy URL it has the lable 'good'."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x=\"Label\",data=df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Preprocession\n\nNow we have to gather all the proper words (tokens) from the URLs using RegexpTokenizer() method from the nltk method.\n\nWe pass \"r'[A-Za-z]+'\" to consider only alphabets for forming tokens."},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = RegexpTokenizer(r'[A-Za-z]+')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['text_tokenized'] = df.URL.map(lambda t: tokenizer.tokenize(t))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### SnowballStemmer\n\nSnowball is a small string processing language, gives root words\n\nDifference Between Porter Stemmer and Snowball Stemmer:\n* Snowball Stemmer is more aggressive than Porter Stemmer.\n* Some issues in Porter Stemmer were fixed in Snowball Stemmer.\n* There is only a little difference in the working of these two."},{"metadata":{"trusted":true},"cell_type":"code","source":"root_words = SnowballStemmer(\"english\")\ndf['root_words'] = df['text_tokenized'].map(lambda l: [root_words.stem(word) for word in l])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Taking all the root words into a sentence. \nThis is done to pass into CountVectorizer function later on."},{"metadata":{"trusted":true},"cell_type":"code","source":"df['text_sent'] = df['root_words'].map(lambda l: ' '.join(l))\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Visualizing Data\nSpliting Data into data with bad lables and good lables"},{"metadata":{"trusted":true},"cell_type":"code","source":"bad_sites = df[df.Label == 'bad']\ngood_sites = df[df.Label == 'good']\nbad_sites.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"good_sites.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(list(STOPWORDS)[:10])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Wordcloud for the good urls."},{"metadata":{"trusted":true},"cell_type":"code","source":"data = good_sites.text_sent\ndata.reset_index(drop=True, inplace=True)\ntext = str(data)\n\n\nstopwords = set(STOPWORDS).union({'com','http','www'})  \nwordcloud = WordCloud(width = 800, height = 800, background_color ='white', stopwords = stopwords, max_words = 400, min_font_size = 10).generate(text)\n  \n               \nplt.figure(figsize = (8, 8), facecolor = None)\nplt.imshow(wordcloud)\nplt.title(\"Most common words used in Good Urls\", fontdict={'size': 20, 'color': 'navy', 'verticalalignment': 'bottom'})\nplt.axis(\"off\")\nplt.tight_layout(pad = 0)\n  \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Wordcloud for the bad urls."},{"metadata":{"trusted":true},"cell_type":"code","source":"data = bad_sites.text_sent\ndata.reset_index(drop=True, inplace=True)\ntext = str(data)\n\n\nstopwords = set(STOPWORDS).union({'com','http','www'})  \nwordcloud = WordCloud(width = 800, height = 800, background_color ='white', stopwords = stopwords, max_words = 400, min_font_size = 10).generate(text)\n  \n               \nplt.figure(figsize = (8, 8), facecolor = None)\nplt.imshow(wordcloud)\nplt.title(\"Most common words used in Bad Urls\", fontdict={'size': 20, 'color': 'navy', 'verticalalignment': 'bottom'})\nplt.axis(\"off\")\nplt.tight_layout(pad = 0)\n  \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model Creation\n#### CountVectorizer\n\nCountVectorizer tokenizes(tokenization means breaking down a sentence or paragraph or any text into words) the text along with performing very basic preprocessing like removing the punctuation marks, converting all the words to lowercase, etc.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"c = CountVectorizer()\ncv = c.fit_transform(df.text_sent)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The text has been preprocessed, tokenized(word-level tokenization: means each word is a separate token), and represented as a sparse matrix. The best part is it ignores single character during tokenization like I and a."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(list(c.vocabulary_)[:10])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('The length of vocabulary', len(c.get_feature_names()))\nprint('The shape is', cv.shape)\n\n#This means 507196 unique urls are there in the dataset and 350837 unique words in the dataset","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Splitting the dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"Xtrain, Xtest, Ytrain, Ytest = train_test_split(cv, df.Label,test_size=0.3, random_state=5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### KNN (K-Nearest Neighbor Algorithm)"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = KNeighborsClassifier(n_neighbors=2)\nmodel.fit(Xtrain, Ytrain)\nprint(model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.score(Xtest, Ytest)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ypred = model.predict(Xtest)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"con_mat = pd.DataFrame(confusion_matrix(ypred, Ytest),columns = ['Predicted:Bad', 'Predicted:Good'],index = ['Actual:Bad', 'Actual:Good'])\nplt.figure(figsize= (6,4))\nsns.heatmap(con_mat, annot = True,fmt='g', cmap=\"Blues\", annot_kws = {'size': 14})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('\\nCLASSIFICATION REPORT\\n')\nprint(classification_report(ypred, Ytest, target_names =['Bad','Good']))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Logistic Regression\nLogistic Regression is a classification algorithm. It's a technique for predicting a binary outcome from a series of independent variables.\n\nA binary outcome is one in which there are only two options: the occurrence occurs (1) or it does not occur (0). Independent variables are variables or factors that have the ability to affect the result (or dependent variable).\n\nWhen dealing with binary data, the best method of analysis to use is logistic regression. When the performance or dependent variable is dichotomous or categorical in nature (e.g., \"yes\" or \"no,\" \"pass\" or \"fail,\" and so on), you're dealing with binary results.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = LogisticRegression(max_iter=507197)\nlr.fit(Xtrain,Ytrain)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr.score(Xtest,Ytest)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ypred = lr.predict(Xtest)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"con_mat = pd.DataFrame(confusion_matrix(ypred, Ytest),columns = ['Predicted:Bad', 'Predicted:Good'],index = ['Actual:Bad', 'Actual:Good'])\nplt.figure(figsize= (6,4))\nsns.heatmap(con_mat, annot = True,fmt='g', cmap=\"Blues\", annot_kws = {'size': 14})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('\\nCLASSIFICATION REPORT\\n')\nprint(classification_report(ypred, Ytest, target_names =['Bad','Good']))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Conclusion\nFrom this we can see that Logistic Regression gives a better accuracy when comaperd to the K-Nearest Neigbor Algorithm. Hence Logistic Regression is the appropriate algorithm to use for classification of the URLs."},{"metadata":{"trusted":true},"cell_type":"code","source":"Xtrain, Xtest, Ytrain, Ytest = train_test_split(df.URL, df.Label,test_size=0.3, random_state=5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pipeline_ls = make_pipeline(CountVectorizer(tokenizer = RegexpTokenizer(r'[A-Za-z]+').tokenize,stop_words='english'), LogisticRegression(max_iter=507197))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pipeline_ls.fit(Xtrain,Ytrain)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bad = ['yeniik.com.tr/wp-admin/js/login.alibaba.com/login.jsp.php','fazan-pacir.rs/temp/libraries/ipad','tubemoviez.exe','svision-online.de/mgfi/administrator/components/com_babackup/classes/fx29id1.txt']\ngood = ['youtube.com/','youtube.com/watch?v=qI0TQJI3vdU','bestbuy.com/','restorevisioncenters.com/html/technology.html']\n\nresult1 = pipeline_ls.predict(bad)\nresult2 = pipeline_ls.predict(good)\n\nprint(result1)\nprint(result2)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}