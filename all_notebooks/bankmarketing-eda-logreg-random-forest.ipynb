{"cells":[{"metadata":{},"cell_type":"markdown","source":"![](https://csbcorrespondent.com/sites/default/files/styles/blog_feature_full/public/blog/BANK%20MARKETING%20ANALYTICS.jpg?itok=SwPf4x34)"},{"metadata":{},"cell_type":"markdown","source":"### Introduction\nThe data is related with direct marketing campaigns of a Portuguese banking institution. The marketing campaigns were based on phone calls. Often, more than one contact to the same client was required, in order to access if the product (bank term deposit) would be ('yes') or not ('no') subscribed.\n\n### Input variables:\n\n### Bank client data\n1. **age** (numeric)\n2. **job** : type of job (categorical: 'admin.','blue-collar','entrepreneur','housemaid','management','retired','self-employed','services','student','technician','unemployed','unknown')\n3. **salary** : amount of salary (numeric)\n4. **marital** : marital status (categorical: 'divorced','married','single','unknown'; note: 'divorced' means divorced or widowed)\n5. **education** (categorical: 'basic.4y', 'basic.6y', 'basic.9y', 'high.school', 'illiterate', 'professional.course', 'university.degree', 'unknown')\n6. **targeted** : has been targeted for subscription of term deposit? (categorical: 'no','yes')\n7. **default** : has credit in default? (categorical: 'no','yes','unknown')\n8. **balance** : balance in the account (numeric)\n9. **housing** : has housing loan? (categorical: 'no','yes','unknown')\n10. **loan** : has personal loan? (categorical: 'no','yes','unknown')\n\n### Related with the last contact of the current campaign\n11. **contact** : contact communication type (categorical: 'cellular','telephone')\n12. **day** : last contact day of the week (categorical: '1:mon','2:tue','3:wed','4:thu','5:fri')\n13. **month** : last contact month of year (categorical: 'jan', 'feb', 'mar', ..., 'nov', 'dec')\n14. **duration**: last contact duration, in seconds (numeric)\n\nImportant note: this attribute highly affects the output target (e.g., if duration=0 then y='no'). Yet, the duration is not known before a call is performed. Also, after the end of the call y is obviously known. Thus, this input should only be included for benchmark purposes and should be discarded if the intention is to have a realistic predictive model.\n\n### Other attributes\n15. **campaign**: number of contacts performed during this campaign and for this client (numeric, includes last contact)\n16. **pdays**: number of days that passed by after the client was last contacted from a previous campaign (numeric; 999 means client was not previously contacted)\n17. **previous**: number of contacts performed before this campaign and for this client (numeric)\n18. **poutcome**: outcome of the previous marketing campaign (categorical: 'failure','nonexistent','success')\n\n\n### Output variable (desired target)\n19. **response** - has the client subscribed a term deposit? (binary: 'yes','no')"},{"metadata":{},"cell_type":"markdown","source":"## Importing Libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.rcdefaults()\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Importing Dataset"},{"metadata":{"ExecuteTime":{"end_time":"2020-12-20T05:38:57.112422Z","start_time":"2020-12-20T05:38:52.438053Z"},"trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/banckingmarket/bank.csv')\nX1 = df[['job','balance']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Describe the pdays column, make note of the mean, median and minimum values. Anything fishy in the values?\n"},{"metadata":{"ExecuteTime":{"end_time":"2020-12-20T05:38:57.144426Z","start_time":"2020-12-20T05:38:57.114424Z"},"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-12-20T05:39:01.040454Z","start_time":"2020-12-20T05:38:57.146426Z"},"trusted":true},"cell_type":"code","source":"# NOT CONSIDERING -1 VALUES IN pdays COLUMN\nvalues = []\nfor i,row in df.iterrows():\n    if row[\"pdays\"] > -1:\n        values.append(row[\"pdays\"])","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-12-20T05:39:01.056456Z","start_time":"2020-12-20T05:39:01.042453Z"},"trusted":true},"cell_type":"code","source":"quartile = [0.25,0.50,0.75]\nquartiles = []\nindex = ['count','mean','std','min','25%','50%','75%','max']\nfor i in quartile:\n    quartiles.append(np.quantile(values,i))\nsummary = [len(values),\n           np.mean(values),\n           np.std(values),\n           np.min(values),\n           quartiles[0],\n           quartiles[1],\n           quartiles[2],\n           np.max(values)\n]\nfor i,j in zip(summary,index):\n    print(f'{j}   {i}')","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-12-20T05:39:01.072457Z","start_time":"2020-12-20T05:39:01.058456Z"},"trusted":true},"cell_type":"code","source":"pdays_mean = np.mean(values)\npdays_median = np.median(values)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The difference in mean is 184.38"},{"metadata":{},"cell_type":"markdown","source":"# Plot a horizontal bar graph with the median values of balance for each education level value. Which group has the highest median?"},{"metadata":{"ExecuteTime":{"end_time":"2020-12-20T05:39:01.104458Z","start_time":"2020-12-20T05:39:01.074457Z"},"trusted":true},"cell_type":"code","source":"# Replacing unkown by the minimum level of education in the education column\ndf.loc[(df['education']=='unknown'),'education'] = 'primary'\nprint(df['education'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Creating different datasets for each education level"},{"metadata":{"ExecuteTime":{"end_time":"2020-12-20T05:39:01.13646Z","start_time":"2020-12-20T05:39:01.10646Z"},"trusted":true},"cell_type":"code","source":"df1 = df[df['education'] == 'secondary']\ndf2 = df[df['education'] == 'tertiary']\ndf3 = df[df['education'] == 'primary']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Calculating mean of column 'balance' for each education level"},{"metadata":{"ExecuteTime":{"end_time":"2020-12-20T05:39:01.152462Z","start_time":"2020-12-20T05:39:01.140464Z"},"trusted":true},"cell_type":"code","source":"med1 = np.median(df1['balance'])  # 392.0\nmed2 = np.median(df2['balance'])  # 577.0\nmed3 = np.median(df3['balance'])  # 432.0\ncompare = [med1,med2,med3]\nedu_list = ['primary','secondary','tertiary']\nprint(f'Tertiary education has the highest median that is {med2}')","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-12-20T05:39:01.375711Z","start_time":"2020-12-20T05:39:01.154462Z"},"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (5,3))\nplt.barh(edu_list, compare, align='center', alpha=0.5)\nfor index, value in enumerate(compare):\n    plt.text(value, index, str(value))\nprint('Horizontal bar graph displaying the median values of column \"balance\" for the different levels of education')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Make a box plot for pdays. Do you see any outliers?"},{"metadata":{"ExecuteTime":{"end_time":"2020-12-20T05:39:01.550954Z","start_time":"2020-12-20T05:39:01.377713Z"},"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots()\n\nmy_data = [df['pdays'],values]\nax.boxplot(my_data)\nplt.show()\nprint('Described pdays column using boxplot. \\n 1. Considering all the values of pdays column including \"-1\"\\n 2. Considering only the non-negative values of pdays column')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{},"cell_type":"markdown","source":"# First, perform bi-variate analysis to identify the features that are directly associated with the target variable"},{"metadata":{},"cell_type":"markdown","source":"### Plotting the categorical variables"},{"metadata":{"ExecuteTime":{"end_time":"2020-12-20T05:39:02.495026Z","start_time":"2020-12-20T05:39:01.552952Z"},"trusted":true},"cell_type":"code","source":"for i in ['marital','education','targeted','default','housing','loan','month','poutcome']:\n    df[i].unique()\n    fig, ax = plt.subplots()\n    fig.set_size_inches(6,3)\n    sns.countplot(x = i, data = df)\n    ax.set_xlabel(i)\n    ax.set_ylabel('Count')\n    ax.set_xticklabels(ax.get_xticklabels(),rotation= 45)\n    sns.despine()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### ViolinPlot for jobs and balance column"},{"metadata":{"ExecuteTime":{"end_time":"2020-12-20T05:39:03.659473Z","start_time":"2020-12-20T05:39:02.497028Z"},"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(13,8))\nsns.countplot(X1['job'])\nplt.show()\nplt.figure(figsize=(13,8))\nsns.violinplot(\n    x='job',\n    y='balance',\n    data=X1\n)\nplt.show()\nprint('In the graphs above, we have displayed the distribution of jobs among the customers through a countplot graph and the balance the customers of different professions have using violin plot')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Through the countplot we can see that most of the customers are in blue-collar profession.\nThrough the violin plot, we can infer that a few customers working in management have the highest balance as compared to other jobs"},{"metadata":{"ExecuteTime":{"end_time":"2020-12-20T05:39:04.283507Z","start_time":"2020-12-20T05:39:03.661475Z"},"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,10))\nsns.heatmap(df.corr(), annot = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here, we have drawn a heatmap to establish correlation between the features and the 'response' variable"},{"metadata":{},"cell_type":"markdown","source":"# Convert the response variable to a convenient form\n"},{"metadata":{"ExecuteTime":{"end_time":"2020-12-20T05:39:04.314977Z","start_time":"2020-12-20T05:39:04.285509Z"},"trusted":true},"cell_type":"code","source":"# Encoding the 'response' variable with 1 and 0\ndf1.loc[(df1['response']=='yes'),'response'] = 1\ndf1.loc[(df1['response']=='no'),'response'] = 0\ndf1['response']=df1['response'].astype('int64')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" # Make suitable plots for associations with numerical features and categorical features’"},{"metadata":{"ExecuteTime":{"end_time":"2020-12-20T05:39:45.090691Z","start_time":"2020-12-20T05:39:04.315976Z"},"trusted":true},"cell_type":"code","source":"# Replacing unknown by mode of the job column\ndf.loc[(df['job']=='unknown'),'job'] = df['job'].mode().get(0)\n\n# Replacing unkown by the minimum level of education in the education column\ndf.loc[(df['education']=='unknown'),'education'] = 'primary'\n\nsns.pairplot(data=df,x_vars = ['job','marital','education','targeted','default','housing','loan','day','previous'],\n             y_vars = ['age','salary','balance','month','duration','campaign','pdays','previous'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" Here, we have plotted the categorical variables against numerical variables."},{"metadata":{},"cell_type":"markdown","source":"# Are the features about the previous campaign data useful?\n"},{"metadata":{"ExecuteTime":{"end_time":"2020-12-20T05:39:45.375711Z","start_time":"2020-12-20T05:39:45.09269Z"},"trusted":true},"cell_type":"code","source":"plt.subplot(2,1,1)\ndf['previous'].value_counts().nlargest(5).plot(kind='barh')\nplt.xlabel('Count')\nplt.ylabel('Previous Contact')\n\nplt.subplot(2,1,2)\ndf['poutcome'].value_counts().nlargest(5).plot(kind='barh')\nplt.xlabel('Count')\nplt.ylabel('Previous Outcome')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From these graphs, it is clear that whoever is not previously contacted is marked outcome as Unknown."},{"metadata":{},"cell_type":"markdown","source":"# Are pdays and poutcome associated with the target?"},{"metadata":{"ExecuteTime":{"end_time":"2020-12-20T05:39:45.407714Z","start_time":"2020-12-20T05:39:45.377712Z"},"trusted":true},"cell_type":"code","source":"df1=df[df['poutcome']==1]\ndf2=df[df['poutcome']==0]\n\nprint('Response of people who is marked success in previous Campaign\\n',df1['response'].value_counts(),'\\n')\nprint('Response of people who is marked failure in previous Campaign\\n',df2['response'].value_counts())\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see, that those who responded positively in previous campaign sill have high response in current campaign, while those who responded negatively still have same opinion.\n\nThis shows that, the results of previous campaign is still affecting the current campaign."},{"metadata":{"ExecuteTime":{"end_time":"2020-12-20T05:39:45.567724Z","start_time":"2020-12-20T05:39:45.410714Z"},"trusted":true},"cell_type":"code","source":"df['pdays'].value_counts().nlargest(5).plot(kind='barh')\nplt.xlabel('Count')\nplt.ylabel('Previous Contct Days')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the graph of 'previous' and 'poutcome', its clear that people who is not contacted before in previous campagins are marked '-1'"},{"metadata":{},"cell_type":"markdown","source":"# Before the predictive modeling part, make sure to perform –\n"},{"metadata":{},"cell_type":"markdown","source":"##  The necessary transformations for the categorical variables and the numeric variables"},{"metadata":{"ExecuteTime":{"end_time":"2020-12-20T05:39:45.630658Z","start_time":"2020-12-20T05:39:45.569727Z"},"trusted":true},"cell_type":"code","source":"for i in df:\n    if df[i].dtypes == object and i != 'contact':\n        df[i] = df[i].astype('category').cat.codes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we have converted categorical columns into numerical columns"},{"metadata":{},"cell_type":"markdown","source":"##  Handle variables corresponding to the previous campaign\n"},{"metadata":{},"cell_type":"markdown","source":"There is only one feature which is doubtfull which is pdays because of -1 value, as 'no previous contact person is marked with 999' but while checking the values, there is no record of '999'\n\nWhile according to the 'previous' and 'poutcome' coulumn, the records which should marked to be '999' re marked as '-1'. There is no need to change the value of -1 to 999 as no other value is olliding with '-1'. So -1 can be considered as it is.   "},{"metadata":{},"cell_type":"markdown","source":"# CLEANING THE DATASET"},{"metadata":{"ExecuteTime":{"end_time":"2020-12-20T05:39:45.661658Z","start_time":"2020-12-20T05:39:45.633657Z"},"trusted":true},"cell_type":"code","source":"per = df['contact'].value_counts()['unknown']\ntotal = df['contact'].count()\nprint(f'Null values percentage = {per/total*100}')\ndf.drop(['contact'],axis=1,inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The null-percentage of contact column is almost 30% of the total values of the column therefore, we drop 'contacts' column"},{"metadata":{},"cell_type":"markdown","source":"## Train test split"},{"metadata":{"ExecuteTime":{"end_time":"2020-12-20T05:39:45.943836Z","start_time":"2020-12-20T05:39:45.66366Z"},"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX = df.iloc[:,:-1].values\ny = df.iloc[:,-1].values\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.2,random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# LOGISTIC REGRESSION"},{"metadata":{},"cell_type":"markdown","source":"### Scaling the features"},{"metadata":{"ExecuteTime":{"end_time":"2020-12-20T05:39:45.975733Z","start_time":"2020-12-20T05:39:45.944837Z"},"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### RFE TO REMOVE UNNECESSARY FEATURES"},{"metadata":{"ExecuteTime":{"end_time":"2020-12-20T05:39:49.340473Z","start_time":"2020-12-20T05:39:45.980736Z"},"trusted":true},"cell_type":"code","source":"# Splitting df into two dataframes X and y\nX = df.iloc[:,:-1]\ny = df.iloc[:,-1]\n\n# Extracting the columns of X and storing them in 'cols' list\ncols = list(X.columns)\n\n# Importing the necessary libraries\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LogisticRegression\n\nrfe = RFE(estimator = LogisticRegression())\nrfe.fit(X,y)\nX = rfe.transform(X)\n\ntemp = pd.Series(rfe.support_,index = cols)\nselected_features_rfe = temp[temp==True].index\n\nX=df[selected_features_rfe]\nX.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Calculating VIF"},{"metadata":{"ExecuteTime":{"end_time":"2020-12-20T05:39:50.523878Z","start_time":"2020-12-20T05:39:49.34346Z"},"trusted":true},"cell_type":"code","source":"from statsmodels.stats.outliers_influence import variance_inflation_factor\nvif = pd.DataFrame()\ndef calc_VIF(X):\n    vif['variables'] = X.columns\n    vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n    return(vif)\n\ncalc_VIF(df)\nimportant_features=[]\nfor i,row in vif.iterrows():\n    if row[\"VIF\"] < 2.5 and row[\"variables\"] != \"response\":\n        print(f'{row[\"variables\"]} ----> {row[\"VIF\"]}')\n        important_features.append(row[\"variables\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The features shown above are the best features according to VIF, where Variance inflation factor is a measure of the amount of multicollinearity in a set of multiple regression variables. A high VIF indicates that the associated independent variable is highly collinear with the other variables in the model"},{"metadata":{},"cell_type":"markdown","source":"### Calculating p-value"},{"metadata":{"ExecuteTime":{"end_time":"2020-12-20T05:39:51.548791Z","start_time":"2020-12-20T05:39:50.52588Z"},"trusted":true},"cell_type":"code","source":"type(X)\nimport statsmodels.api as sm\nfrom scipy import stats\nX2 = sm.add_constant(df)\nest = sm.OLS(y,X2)\nest2 = est.fit()\nprint(est2.summary())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The smaller the p-value shows that that feature is not suitable for the model as it violates the null-hypothesis which is the feature is good for the model. \n\nThe larger the p-value the better the feature is for the model. From the above table  previous, housing, education and marital are some of the best features"},{"metadata":{},"cell_type":"markdown","source":"# In this model, we will follow the features provided by VIF "},{"metadata":{"ExecuteTime":{"end_time":"2020-12-20T05:39:51.564792Z","start_time":"2020-12-20T05:39:51.550793Z"},"trusted":true},"cell_type":"code","source":"X = df[important_features]\nfeatures = X.columns\nX.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Converting X, y dataframe into arrays"},{"metadata":{"ExecuteTime":{"end_time":"2020-12-20T05:39:51.580794Z","start_time":"2020-12-20T05:39:51.566795Z"},"trusted":true},"cell_type":"code","source":"X=X.values\ny = df['response'].values","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-12-20T05:39:51.6588Z","start_time":"2020-12-20T05:39:51.581795Z"},"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, accuracy_score\n# Training the model\nclassifier = LogisticRegression(random_state = 0)\nclassifier.fit(X_train,y_train)\n\n# Testing the model\ny_pred=classifier.predict(X_test)\n\n# Printing the accuracy score\nprint(accuracy_score(y_test,y_pred))\n\n# Printing the confusion matrix\ncm = confusion_matrix(y_test,y_pred)\nprint(cm)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### k-Fold Cross Validation"},{"metadata":{"ExecuteTime":{"end_time":"2020-12-20T05:39:53.974972Z","start_time":"2020-12-20T05:39:51.660801Z"},"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import KFold\nimport numpy\nfrom sklearn.model_selection import cross_val_score\ncv = KFold(n_splits = 10, random_state = 1, shuffle = True)\nscores = cross_val_score(classifier,X,y,scoring = 'accuracy', cv = cv, n_jobs = -1)\n# report performance\nprint('Accuracy: %.3f (%.3f)' % (numpy.mean(scores), numpy.std(scores)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Precision, Accuracy and Recall of our model"},{"metadata":{"ExecuteTime":{"end_time":"2020-12-20T05:39:53.990977Z","start_time":"2020-12-20T05:39:53.976976Z"},"trusted":true},"cell_type":"code","source":"recall = cm[0][0]/(cm[0][0] + cm[1][0])\nprecision = cm[0][0]/(cm[0][0] + cm[0][1])\nnumpy.mean(scores)\nprint(f'Recall is -> {recall}\\nPrecision is -> {precision}\\nAccuracy is -> {numpy.mean(scores)}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Important Features"},{"metadata":{"ExecuteTime":{"end_time":"2020-12-20T05:39:54.006978Z","start_time":"2020-12-20T05:39:53.992978Z"},"trusted":true},"cell_type":"code","source":"features","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n# RANDOM FOREST CLASSIFICATION"},{"metadata":{"ExecuteTime":{"end_time":"2020-12-20T05:39:54.181943Z","start_time":"2020-12-20T05:39:54.008977Z"},"trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/banckingmarket/bank.csv')\ndf.drop(['contact'],axis = 1, inplace = True)\n\nfor i in df:\n    if df[i].dtypes == object:\n        df[i] = df[i].astype('category').cat.codes\n\nX = df.iloc[:, :-1].values\ny = df.iloc[:, -1].values\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-12-20T05:40:25.513656Z","start_time":"2020-12-20T05:39:54.183943Z"},"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n\naccuracy=[]\nestimators_count=[]\nfor i in range(1,50,2):\n    rf = RandomForestClassifier(n_estimators = i, criterion = 'entropy', random_state = 0)\n    rf.fit(X_train, y_train)\n    y_pred = rf.predict(X_test)\n    accuracy.append(accuracy_score(y_test,y_pred))\n    estimators_count.append(i)\n    print(f'{i} {accuracy_score(y_test,y_pred).round(4)}')\n    \nplt.plot(estimators_count,accuracy)\nplt.xlabel('Number of Estimators')\nplt.ylabel('Accuracy')\nplt.title('No_of_Estimators VS Accuracy')\nplt.grid(b=None)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-12-20T05:40:26.535726Z","start_time":"2020-12-20T05:40:25.515657Z"},"trusted":true},"cell_type":"code","source":"classifier = RandomForestClassifier(n_estimators = 20, criterion = 'entropy', random_state = 0)\nclassifier.fit(X_train, y_train)\n \ny_pred = classifier.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Accuracy using  k-fold Cross Validation"},{"metadata":{"ExecuteTime":{"end_time":"2020-12-20T05:40:29.739964Z","start_time":"2020-12-20T05:40:26.537728Z"},"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\ncv = KFold(n_splits = 10, random_state = 1, shuffle = True)\nscores = cross_val_score(classifier,X,y,scoring = 'accuracy', cv = cv, n_jobs = -1)\n# report performance\nprint('Accuracy: %.4f (%.4f)' % (numpy.mean(scores), numpy.std(scores)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Creating the confusion matrix and printing the recall, precision and accuracy of the model"},{"metadata":{"ExecuteTime":{"end_time":"2020-12-20T05:40:29.771702Z","start_time":"2020-12-20T05:40:29.741966Z"},"trusted":true},"cell_type":"code","source":"cm = confusion_matrix(y_test, y_pred)\naccuracy_score(y_test, y_pred)\n\nrecall = cm[0][0]/(cm[0][0] + cm[1][0])\nprecision = cm[0][0]/(cm[0][0] + cm[0][1])\nnp.mean(scores)\nprint(f'Recall is -> {recall.round(4)}\\nPrecision is -> {precision.round(4)}\\nAccuracy is -> {numpy.mean(scores).round(4)}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Displaying the feature ranking"},{"metadata":{"ExecuteTime":{"end_time":"2020-12-20T05:40:29.787707Z","start_time":"2020-12-20T05:40:29.773705Z"},"trusted":true},"cell_type":"code","source":"importances = classifier.feature_importances_\nfeature_names = df.iloc[:,:-1].columns\nindices = np.argsort(importances)[::-1]\n\nprint(\"Feature ranking\\n\")\nfor i in range(X_train.shape[1]):\n    print(\"%d.   %s   = %f\" % (i + 1, df.columns[indices[i]], importances[indices[i]]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Compare the performance of the Random Forest and the logistic model "},{"metadata":{},"cell_type":"markdown","source":"The evaluation is done above just after the prediction. The results for RF and Logistic Regression using k-Fold Cross Validation are .9002 and .888 respectively which clearly shows that the RF is working better than the Logistic Regresion but, on the other hand, RF is taking more time to training.\n"},{"metadata":{},"cell_type":"markdown","source":"Here we have chosen k-Fold Cross validation as the response variable have large amount of 'no' as compared to 'yes', which shows that the dataset is not balanced. IFf we use accuracy as our metric, any random model can give a very good accuracy, but at the end it would be a random model. TO conquer this problem, we are using k_Fold Cross Validation. "},{"metadata":{},"cell_type":"markdown","source":"RF has better performance than the Logisitc model as confirmed by k-Fold Cross Validation result."},{"metadata":{},"cell_type":"markdown","source":"In Logistic regression, for selection of features, we have followed VIF and for RF, we have used its inbuilt feature_importance_ attribute to check the features which both the models are using for training and prediction.\n\nIn VIF, we have default, balance, loan, duration, campaign and previous as important features while in RF we have duration, balance, age, day, month, pdays as important features. Therefore, we can say that only 2 features as common from the two models.\n\nAlso, according to the EDA, previous and pdays are having same values but on different scales. So, this can also be considered as common."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}