{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import shuffle\nimport random\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Load dataset\ndata_original = pd.read_csv(os.path.join(dirname, filename))\ndata_original.head()\n\n# Select columns\ndata_select = data_original[['0', '1', '2', '1558']]\ndata_select.head()\n\n# Remove NAs from any column\n#data_select = data_select[data_select[['0','1','2','1558']].notna()]\ndata_select = data_select[data_select['0'].notna()]\ndata_select = data_select[data_select['1'].notna()]\ndata_select = data_select[data_select['2'].notna()]\ndata_select = data_select[data_select['1558'].notna()]\n\n# Determine data type for each column\n#data_select[['0', '1', '2']] = data_select[['0', '1', '2']].astype(float)\n\ndata_select['0'] = pd.to_numeric(data_select['0'], errors = 'coerce')\ndata_select['1'] = pd.to_numeric(data_select['1'], errors = 'coerce')\ndata_select['2'] = pd.to_numeric(data_select['2'], errors = 'coerce')\ndata_select[['1558']] = data_select[['1558']].astype('string')\n\n# Identify frequency of ads and non-ads\ndata_select.groupby('1558').count()\n\n# Convert last column to 1 or 0. 1 is ad, 0 is non-ad\ndata_select['1558'] = np.where(data_select['1558'] == 'ad.', 1, 0)\ndata_select['1558'] = data_select['1558'].astype(float)\n\n# Remove NaN\ndata_select = data_select.dropna()\n\n# Rename columns\ndata_select.columns = ['height', 'width', 'aspect_ratio', 'category']\n#data_select.head()\n#data_select.tail()\n#data_select.dtypes\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split data into train and development sets\nrandom.seed(1)\ndata_select = shuffle(data_select)\n\ntrain_set, test_set = train_test_split(data_select, train_size=1895, test_size=(2369-1895))\n\nX_train = train_set[['height', 'width', 'aspect_ratio']]\nY_train = train_set[['category']]\n\nX_test = test_set[['height', 'width', 'aspect_ratio']]\nY_test = test_set[['category']]\n\nX_train = X_train.T\nY_train = Y_train.T\nX_test = X_test.T\nY_test = Y_test.T\n\n# Convert to numpy array\nX_train = X_train.to_numpy()\nY_train = Y_train.to_numpy()\nX_test = X_test.to_numpy()\nY_test = Y_test.to_numpy()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create logistic regression with neural network framework\n# List of helper functions\n# sigmoid()\n# initialise_with_zeros()\n# propagate()\n# optimise()\n# predict()\n# model()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sigmoid()\ndef sigmoid(z):\n    # Compute the sigmoid of z\n    # Arguments:\n    # z - A scalar or numpy array of any size\n    # Return:\n    # s - sigmoid(z)\n    s = 1 / (1 + np.exp(-z))\n    return s","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Test sigmoid()\nprint(\"sigmoid([0, 2]) = \" + str(sigmoid(np.array([0, 2]))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# initialise_with_zeros()\ndef initialise_with_zeros(dim):\n    # This function creates a vector of zeros of shape (dim, 1) for w and initialises b to zero.\n    # Arguments:\n    # dim - size of the vector w we want (or number of parameters in this case)\n    # Return:\n    # w - initialised vector of shape (dim, 1)\n    # b - initialised scalar (corresponds to the bias)\n    w = np.zeros(shape=(dim, 1))\n    b = 0\n    \n    assert(w.shape == (dim, 1))\n    assert(isinstance(b, float) or isinstance(b, int))\n    \n    return w, b","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Test initialise_with_zeros()\ndim = 3\nw, b = initialise_with_zeros(dim)\nprint(\"w = \" + str(w))\nprint(\"b = \" + str(b))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# propagate()\ndef propagate(w, b, X, Y):\n    # Implement the cost function and its gradient\n    # Arguments:\n    # w - weights, a numpy array of size (num_px, 1)\n    # b - bias, a scalar\n    # X - data of size (num_px, number of examples m)\n    # Y - true \"label\" vector of size (1, number of examples m). 1 is ad and 0 is non-ad.\n    # Return\n    # cost - negative log-likelihood cost for logistic regression\n    # dw - gradient of the loss with respect to w, thus same shape as w\n    # db - gradient of the loss with respect to b, thus same shape as b\n    m = X.shape[1]\n    \n    # Forward propagation\n    A = sigmoid(np.dot(w.T, X) + b)\n    \n    # Compute cost\n    cost = (-1/m) * np.sum(Y * np.log(A) + (1-Y) * np.log(1-A))\n    \n    # Backward propagation\n    dw = (1/m) * np.dot(X, (A - Y).T)\n    db = (1/m) * np.sum(A - Y)\n    \n    assert(dw.shape == w.shape)\n    assert(db.dtype == float)\n    cost = np.squeeze(cost)\n    assert(cost.shape == ())\n    \n    grads = {\n        \"dw\" : dw,\n        \"db\" : db\n    }\n\n    return grads, cost\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Test propagate()\n# w, b, X, Y = np.array([[1.], [2.]]), 2., np.array([[1., 2., -1.], [3., 4., -3.2]]), np.array([[1, 0, 1]])\n# w, b, X, Y = np.array([[1.], [2.], [1.2]]), 2., np.array([[1., 2., -1.], [3., 4., -3.2], [1., -1.1, 2.]]), np.array([[1, 0, 1]])\nw, b, X, Y = np.array([[1.], [2.], [1.2]]), 2., X_train, Y_train\ngrads, cost = propagate(w, b, X, Y)\n#print(\"dw: \" + str(grads[\"dw\"]))\n#print(\"db: \" + str(grads[\"db\"]))\n#print(\"cost: \" + str(cost))\n\ncost.shape\ncost\n#X_train.shape\n#Y_train.shape\n\n#np.array([[1., 2., -1.], [3., 4., -3.2], [1., -1.1, 2.], [1.,1.,1.]]).shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# optimise()\ndef optimise(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n    # Update parameters w and b using gradient descent\n    # Arguments:\n    # w - weights, a numpy array of size (num_px, 1)\n    # b - bias, a scalar\n    # X - data of shape (num_px, number of examples m)\n    # Y - true \"label\" vector of shape (1, number of examples m)\n    # num_iterations - number of iterations of the optimisation loop\n    # learning_rate - learning rate of the gradient descent update rule\n    # print_cost - True to print the loss every 100 iterations\n    costs = []\n    \n    for i in range(num_iterations):\n        grads, cost = propagate(w, b, X, Y)\n        \n        # Retrieve derivatives\n        dw = grads[\"dw\"]\n        db = grads[\"db\"]\n        \n        # Update rule\n        w = w - learning_rate * dw\n        b = b - learning_rate * db\n        \n        # Record costs\n        if i % 100 == 0:\n            costs.append(cost)\n            \n        # Print the costs after every 100 iterations\n        if print_cost and i % 100 == 0:\n            print(\"Cost after iteration %i: %f\" %(i, cost))\n            \n    params = {\n        \"w\" : w,\n        \"b\" : b\n    }\n    \n    grads = {\n        \"dw\" : dw,\n        \"db\" : db\n    }\n        \n    return params, grads, costs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Test optimise()\nparams, grads, costs = optimise(w, b, X, Y, num_iterations = 1000, learning_rate = 0.009, print_cost = True)\nprint(\"w: \" + str(params[\"w\"]))\nprint(\"b: \" + str(params[\"b\"]))\nprint(\"dw: \" + str(grads[\"dw\"]))\nprint(\"db: \" + str(grads[\"db\"]))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# predict()\ndef predict(w, b, X):\n    # Predict whether label is 0 or 1 using logistic regression via parameters w and b\n    # Arguments:\n    # w - weights, a numpy array of size (num_px, 1)\n    # b - bias, a scalar\n    # X - data, a numpy array of shape (num_px, number of examples m)\n    # Returns:\n    # Y_prediction - predicted labels 0 or 1, a numpy array of shape (1, number of examples m)\n    m = X.shape[1]\n    Y_prediction = np.zeros(shape = (1, m))\n    w = w.reshape(X.shape[0], 1)\n    \n    # Compute vector A to derive probabilities between 0 and 1 of image being ad or not\n    A = sigmoid(np.dot(w.T,X) + b)\n    \n    for i in range(A.shape[1]):\n        if A[0,i] <= 0.5:\n            Y_prediction[0,i] = 0\n        else:\n            Y_prediction[0,i] = 1\n    \n    assert(Y_prediction.shape == (1,m))\n    \n    return Y_prediction\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Test predict()\nw = np.array([[0.1124579], [0.23106775]])\nb = -0.3\nX = np.array([[1.,-1.1,-3.2], [1.2, 2., 0.1]])\nprint(\"predictions: \" + str(predict(w, b, X)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model()\ndef model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.01, print_cost = False):\n    # Builds the logistic regression model based on the helper functions created above\n    # Arguments:\n    # X_train - train set, numpy array of size (num_px, number of examples m)\n    # Y_train - train label vector, numpy array of size (1, number of examples m)\n    # X_test - test set, numpy array of size (num_px, number of examples m)\n    # Y_test - test label vector, numpy array of size (1, number of examples m)\n    # num_iterations - a scalar\n    # learning_rate - a scalar\n    # print_cost - True to print cost for every 100 iterations\n    # Returns:\n    # d - dictionary of costs, predictions, and parameters w and b, learning rate and number of iterations\n    \n    # Initialise parameters w and b with zeros\n    w, b = initialise_with_zeros(X_train.shape[0])\n    \n    # Gradient descent\n    parameters, grads, costs = optimise(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost)\n    \n    # Retrieve parameters w and b\n    w = parameters[\"w\"]\n    b = parameters[\"b\"]\n    \n    # Predict train and test sets\n    Y_prediction_train = predict(w, b, X_train)\n    Y_prediction_test = predict(w, b, X_test)\n    \n    # Print train and test set errors\n    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n    \n    d = {\n        \"costs\": costs,\n        \"Y_prediction_train\": Y_prediction_train,\n        \"Y_prediction_test\": Y_prediction_test,\n        \"w\": w,\n        \"b\": b,\n        \"learning_rate\": learning_rate,\n        \"num_iterations\": num_iterations\n    }\n    \n    return d","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d = model(X_train, Y_train, X_test, Y_test, num_iterations = 3000, learning_rate = 0.0001, print_cost = True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot learning curve with costs\ncosts = np.squeeze(d[\"costs\"])\nplt.plot(costs)\nplt.ylabel('cost')\nplt.xlabel('iterations (per hundreds)')\nplt.title(\"Learning rate: \" + str(d[\"learning_rate\"]))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}