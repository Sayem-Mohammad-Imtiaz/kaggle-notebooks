{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nfrom collections import defaultdict, Counter\nfrom tqdm import tqdm\nfrom catboost import CatBoostClassifier\nfrom matplotlib import pyplot as plt\nimport time\nimport gc\nfrom scipy.stats import entropy\nfrom gensim.models import Word2Vec\nfrom sklearn.metrics import *","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_path = \"../input/2020-digix-advertisement-ctr-prediction/test_data_A.csv\"\ntrain_path = \"../input/2020-digix-advertisement-ctr-prediction/train_data/train_data.csv\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train dataset庞大（4千万），故以100万份为一批读取，并从每批中选取5%作为新的train dataset（n代表取100万中的第n份5%）\ndef load_data(train_data_path, test_data_path, n):\n    chunkSize = 10 ** 6\n    num_of_chunk = 0\n    train = pd.DataFrame()\n\n    for chunk in tqdm(pd.read_csv(train_data_path, iterator=True, sep='|', chunksize=chunkSize)):\n        num_of_chunk += 1\n        train = pd.concat([train, chunk.iloc[n*50000:(n+1)*50000,:]], axis=0)\n        #print('Processing Chunk No. ' + str(num_of_chunk))  \n    train.reset_index(drop=True, inplace=True)\n    loop = True\n    chunks = []\n    test = pd.read_csv(test_data_path, iterator=True, sep='|')\n    while loop:\n        try:\n            #print(index)\n            chunk = test.get_chunk(chunkSize)\n            chunks.append(chunk)\n            #index += 1\n        except StopIteration:\n            loop = False\n            print(\"testing data iteration stopped.\")\n    for i in tqdm(chunks):\n        test = pd.concat(chunks, ignore_index=True)\n    test['label'] = np.nan\n    data = pd.concat([train, test], axis=0)\n    return data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# train dataset庞大（4千万），故以100万份为一批读取，并从每批中随机抽取5%作为新的train dataset\nchunkSize = 10 ** 6\nnum_of_chunk = 0\ntrain = pd.DataFrame()\n    \nfor chunk in tqdm(pd.read_csv(train_path, iterator=True, sep='|', chunksize=chunkSize)):\n    num_of_chunk += 1\n    train = pd.concat([train, chunk.sample(frac=.05, replace=False, random_state=123)], axis=0)\n    #print('Processing Chunk No. ' + str(num_of_chunk))     \n    \ntrain.reset_index(drop=True, inplace=True)\n\n# train的长度，稍后df重新分割索引用途\ntrain_len = len(train)\ntrain_len\n\ntrain['useage'] = 'train'\ntrain['id'] = np.nan\nloop = True\nchunks = []\n\ntest = pd.read_csv(test_path, iterator=True, sep='|')\nwhile loop:\n    try:\n        #print(index)\n        chunk = test.get_chunk(chunkSize)\n        chunks.append(chunk)\n        #index += 1\n\n    except StopIteration:\n        loop = False\n        print(\"testing data iteration stopped.\")\n\nfor i in tqdm(chunks):\n    test = pd.concat(chunks, ignore_index=True)\n\ntest['useage'] = 'test'\ntest['label'] = np.nan\ndata = pd.concat([train, test])","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# 减小数据容量\ndef reduce_mem(df):\n    starttime = time.time()\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if pd.isnull(c_min) or pd.isnull(c_max):\n                continue\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('-- Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction),time spend:{:2.2f} min'.format(end_mem,\n                                                                                                           100*(start_mem-end_mem)/start_mem,\n                                                                                                           (time.time()-starttime)/60))\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = load_data(train_path, test_path, 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Age interval**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"age = Counter(age for age in df['age'])\nitems = sorted(age.items())\nx_age_count = [i[:][0] for i in items]\ny_age_count = [i[:][1] for i in items]\nplt.figure(figsize = (5, 5))\nsns.barplot(y = y_age_count, x = x_age_count)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Top 20 resident city of a user:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"city = Counter(city for city in df['city'])\ncity_items = city.most_common(20)\nx_city_count = [i[:][0] for i in city_items]\ny_city_count = [i[:][1] for i in city_items]\nplt.figure(figsize = (10, 10))\nsns.barplot(y = y_city_count, x = x_city_count)\nplt.xticks(rotation=90)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"city_rank = Counter(city_rank for city_rank in df['city_rank'])\ncity_rank_items = sorted(city_rank.items())\nx_city_rank_count = [i[:][0] for i in city_rank_items]\ny_city_rank_count = [i[:][1] for i in city_rank_items]\nplt.figure(figsize = (5, 5))\nsns.barplot(y = y_city_rank_count, x = x_city_rank_count)\n#plt.xticks(rotation=90)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##########################cate feature#######################\ncate_cols = ['slot_id','net_type','task_id','adv_id','adv_prim_id','age','app_first_class','app_second_class','career','city','consume_purchase','uid','dev_id','tags']\nfor f in tqdm(cate_cols):\n    map_dict = dict(zip(df[f].unique(), range(df[f].nunique())))\n    df[f + '_count'] = df[f].map(df[f].value_counts())\ndf = reduce_mem(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##########################groupby feature#######################\ndef group_fea(df,key,target):\n    tmp = df.groupby(key, as_index=False)[target].agg({\n        key+target + '_nunique': 'nunique',\n    }).reset_index()\n    del tmp['index']\n    print(\"**************************{}**************************\".format(target))\n    return tmp\n\nfeature_key = ['uid','age','career','net_type']\nfeature_target = ['task_id','adv_id','dev_id','slot_id','spread_app_id','indu_name']\n\nfor key in tqdm(feature_key):\n    for target in feature_target:\n        tmp = group_fea(df,key,target)\n        df = df.merge(tmp,on=key,how='left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df = df[df[\"pt_d\"]==8].copy().reset_index()\ntrain_df = df[df[\"pt_d\"]<8].reset_index()\ndel df\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#统计做了groupby特征的特征\ngroup_list = []\nfor s in train_df.columns:\n    if '_nunique' in s:\n        group_list.append(s)\nprint(group_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##########################target_enc feature#######################\nfrom sklearn.model_selection import StratifiedKFold\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=2020)\nenc_list = group_list + ['net_type','task_id','adv_id','adv_prim_id','age','app_first_class','app_second_class','career','city','consume_purchase','uid','uid_count','dev_id','tags','slot_id']\nfor f in tqdm(enc_list):\n    train_df[f + '_target_enc'] = 0\n    test_df[f + '_target_enc'] = 0\n    for i, (trn_idx, val_idx) in enumerate(skf.split(train_df, train_df['label'])):\n        trn_x = train_df[[f, 'label']].iloc[trn_idx].reset_index(drop=True)\n        val_x = train_df[[f]].iloc[val_idx].reset_index(drop=True)\n        enc_df = trn_x.groupby(f, as_index=False)['label'].agg({f + '_target_enc': 'mean'})\n        val_x = val_x.merge(enc_df, on=f, how='left')\n        test_x = test_df[[f]].merge(enc_df, on=f, how='left')\n        val_x[f + '_target_enc'] = val_x[f + '_target_enc'].fillna(train_df['label'].mean())\n        test_x[f + '_target_enc'] = test_x[f + '_target_enc'].fillna(train_df['label'].mean())\n        train_df.loc[val_idx, f + '_target_enc'] = val_x[f + '_target_enc'].values\n        test_df[f + '_target_enc'] += test_x[f + '_target_enc'].values / skf.n_splits","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#线下数据集的切分\nX_train = train_df[train_df[\"pt_d\"]<=6].copy()\ny_train = X_train[\"label\"].astype('int32')\nX_valid = train_df[train_df[\"pt_d\"]>6]\ny_valid = X_valid[\"label\"].astype('int32')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#筛选特征\ndrop_fea = ['pt_d','label','communication_onlinerate','index']\nfeature= [x for x in X_train.columns if x not in drop_fea]\nprint(len(feature))\nprint(feature)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}