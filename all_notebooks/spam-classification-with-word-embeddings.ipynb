{"nbformat_minor":1,"cells":[{"metadata":{},"source":"# Introduction\nThis is a not very sophisticated approach to text classification employing word embeddings (from GloVe) and a keras ANN.\n\nInteresting because it performs better than it has any right to, given the little or no attention I've paid to engineering features from the text itself.\n\nCredit to Jason Brownlee and his fabulous [site](https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/). This is just a hack of his code, but I don't think he'll mind ;-)","cell_type":"markdown"},{"metadata":{},"source":"# 1. Read data\nRead the data from CSV and apply some basic pre-processing (remove non-ascii characters, convert our target variable to an integer label).","cell_type":"markdown"},{"execution_count":null,"outputs":[],"metadata":{"_cell_guid":"8c21760d-0639-4bac-b96e-ded323c966f6","_uuid":"8a770c31a53e04c287a94bf27fb95c20b5724e9a"},"source":"import numpy as np\nimport pandas as pd\nfrom string import printable\nst = set(printable)\ndata = pd.read_csv(\"../input/sms-spam-collection-dataset/spam.csv\",\n                   names=[\"labelStr\",\"text\"],\n                   skiprows=1,\n                   usecols=[0,1],\n                   encoding=\"latin-1\")\n\ndata[\"text\"] = data[\"text\"].apply(lambda x: ''.join([\"\" if  i not in st else i for i in x]))\ndata[\"label\"]=data[\"labelStr\"].apply(lambda x: 1 if x == \"spam\" else 0)\n\ndocs = data[\"text\"].values\nlabels = data[\"label\"].values\n\nprint(len(docs))","cell_type":"code"},{"metadata":{},"source":"# 2. Preprocessing\nTokenize text, convert words / tokens to indexed integers.\nTake each document and convert to a sequence of max length 20 (pad with zeroes if shorter).","cell_type":"markdown"},{"execution_count":null,"outputs":[],"metadata":{"_cell_guid":"c73c3111-280c-4006-bb0b-67e2c151df7e","_uuid":"ce3fe6048d5609f516cf090b2fc3627959e807bc"},"source":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n# prepare tokenizer\nt = Tokenizer()\nt.fit_on_texts(docs)\nvocab_size = len(t.word_index) + 1\n# integer encode the documents\nencoded_docs = t.texts_to_sequences(docs)\nprint(vocab_size)\n# pad documents to a max length of 4 words\nmax_length = 20\npadded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\nprint(len(padded_docs))","cell_type":"code"},{"metadata":{},"source":"# 3. Import embeddings\nThe clever part: import a dictionary of word embeddings that translates each word into a 100 dimensional vector.  \nMore info on the project that created this dataset [here](https://nlp.stanford.edu/projects/glove/).  \nThe dataset itself was already available on [Kaggle](https://www.kaggle.com/rtatman/glove-global-vectors-for-word-representation) which makes life somewhat easier.","cell_type":"markdown"},{"execution_count":null,"outputs":[],"metadata":{"_cell_guid":"b9dde316-3ccb-4121-8138-dd79fed1e12e","_uuid":"e4e37cd1f017715d56130f40af6c2ace77a8c114"},"source":"# load the whole embedding into memory\nembeddings_index = dict()\nf = open(\"../input/glove-global-vectors-for-word-representation/glove.6B.100d.txt\")\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()\nprint('Loaded %s word vectors.' % len(embeddings_index))","cell_type":"code"},{"metadata":{},"source":"Very nice, but we only need the subset of these 400,000 words that appear in our docs:","cell_type":"markdown"},{"execution_count":null,"outputs":[],"metadata":{"_cell_guid":"8b2c5bbb-5328-4945-84ac-6c81697484d1","_uuid":"d27f59e286847ff796b14e37eb009599f83e3979"},"source":"# create a weight matrix for words in training docs\nembedding_matrix = np.zeros((vocab_size, 100))\nfor word, i in t.word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector\n\nprint(embedding_matrix.shape)","cell_type":"code"},{"metadata":{"_cell_guid":"2c2b25b4-989e-4218-a974-268e74145d7b","_uuid":"44e75e1ece27dc994db5a6c4538ce33f5cc1c066","collapsed":true},"source":"# 4. Network architecture\nThis is wonderfully simple:\n  1. Embedding layer has to come first and takes the embedding matrix and input_length arguments we specified earlier. I've set this to be not trainable to begin with; we'll trust that Stanford have done a better job with their mega dataset than we will with our text messages.\n  2. A flattening layer (translates the outputs of the embedding into a vector of length 2,000: sequence length of 20 x embedding width of 100).\n  3. Sigmoid output node to output probability estimate of class membership.\n\nWe train the network using rmsprop with log-loss as the loss metric.","cell_type":"markdown"},{"execution_count":null,"outputs":[],"metadata":{"_cell_guid":"429f85f9-5e2b-4baf-b2fd-aca68bfd95f8","_uuid":"1c85acc949a734adcbd376def6f5523377581097"},"source":"from keras.models import Sequential\nfrom keras.layers import Dense, Flatten\nfrom keras.layers.embeddings import Embedding\n# define the model\n\nmodel = Sequential()\nmodel.add(Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=20, trainable=False))\nmodel.add(Flatten())\nmodel.add(Dense(1, activation='sigmoid'))\n# compile the model\nmodel.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n# summarize the model\nprint(model.summary())\n","cell_type":"code"},{"metadata":{},"source":"# 5. Training and Evaluation\nIs it any good? Let's find out.  \nDivide our dataset using a holdout strategy:","cell_type":"markdown"},{"execution_count":null,"outputs":[],"metadata":{"_cell_guid":"9158312d-1a6f-48c5-b1c0-e7b6e3c02110","_uuid":"46354f64c9b69385ddea388e98a5a13245c9228d","collapsed":true},"source":"# split dataset into train and test\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(padded_docs, labels, test_size=0.2, random_state=42)","cell_type":"code"},{"metadata":{},"source":"Train the model for ten epochs using the training data.  \nEvaluate using the test data.","cell_type":"markdown"},{"execution_count":null,"outputs":[],"metadata":{"_cell_guid":"9248bcdc-b61e-4ae9-99e1-797ecd3e8df2","_uuid":"90fc7bd5e49a54af0a19ac93234ce0da306808d2"},"source":"# fit the model\nmodel.fit(X_train, y_train, epochs=10, verbose=0)\n# evaluate the model\nloss, accuracy = model.evaluate(X_test, y_test, verbose=0)\nprint('Accuracy: %f' % (accuracy*100))","cell_type":"code"},{"metadata":{},"source":"OK, it's nowhere near as good as [Jessica's](https://www.kaggle.com/jessicayung) [example](https://www.kaggle.com/jessicayung/word-char-count-top-words-xgboost-1-0-test) using xgboost, but then she put some thought into what features she ought to use!  ","cell_type":"markdown"}],"metadata":{"language_info":{"nbconvert_exporter":"python","codemirror_mode":{"version":3,"name":"ipython"},"version":"3.6.3","pygments_lexer":"ipython3","name":"python","file_extension":".py","mimetype":"text/x-python"},"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"}},"nbformat":4}