{"cells":[{"metadata":{"_uuid":"02702343701abcefca4cc377f9d918baab7798fe"},"cell_type":"markdown","source":"## Context\nThis database contains 76 attributes, but all published experiments refer to using a subset of 14 of them. In particular, the Cleveland database is the only one that has been used by ML researchers to this date. The \"goal\" field refers to the presence of heart disease in the patient. It is integer valued from 0 (no presence) to 4.\n\n## Content\n\n**Attribute Information: **\n- age = age in years\n- sex = (1 = male; 0 = female)\n- cp = chest pain type\n- trestbps = resting blood pressure (in mm Hg on admission to the hospital)\n- chol = serum cholestoral in mg/dl\n- fbs = (fasting blood sugar > 120 mg/dl) (1 = true; 0 = false)\n- restecg = resting electrocardiographic results\n- thalach = maximum heart rate achieved\n- exang = exercise induced angina (1 = yes; 0 = no)\n- oldpeak = ST depression induced by exercise relative to rest\n- slope = the slope of the peak exercise ST segment\n- ca = number of major vessels (0-3) colored by flourosopy\n- thal = 3 = normal; 6 = fixed defect; 7 = reversable defect\n- target = 1 or 0 (1 being heart disease, 0 being no heart disease\n\n## Get the Data\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"Data = pd.read_csv('../input/heart.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b3d94a2d60727e2395f75b84d8ea6beda63f4614"},"cell_type":"code","source":"!pip install -q tensorflow==2.0.0-alpha0\nimport tensorflow as tf\ntf.random.set_seed(123)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6e4ff1b803da4f33302efd782a63672030e4c7c5"},"cell_type":"code","source":"Data.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"88106fd03a62435b5dc65501fe7579b88097b736"},"cell_type":"markdown","source":"### Split up the data to training set and test set"},{"metadata":{"trusted":true,"_uuid":"d3539bc72d8fa89d494cde602fcf626265777577"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ntrain_set,test_set = train_test_split(Data,test_size=0.2, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3a45b023d8160cf378de4754b8c9cf2038345437"},"cell_type":"code","source":"train_X = train_set.drop('target',axis=1)\ntrain_y = train_set['target']\n\ntest_X = test_set.drop('target',axis=1)\ntest_y = test_set['target']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7e6987446fe66b82df814e02a2c6344bb92a9839"},"cell_type":"code","source":"%matplotlib inline\nimport matplotlib.pyplot as plt\ntrain_X.hist(bins=20, figsize=(20,15))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a6fe06cd9a28f59e2b00c6d993520ec1d6de18a0"},"cell_type":"markdown","source":"From the graph below we can see most of the people in this dataset have an age between 50 and 60"},{"metadata":{"trusted":true,"_uuid":"66a63f7ad7bf992ad612ff76053bd19d2880b586"},"cell_type":"code","source":"train_X.age.hist(bins=20);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b6fc0e8c01beb5cc149697546871449028f8eb4f"},"cell_type":"markdown","source":"Most of the data is male"},{"metadata":{"trusted":true,"_uuid":"9a49886221690f05008561478844b36b1bd10059"},"cell_type":"code","source":"train_X.sex.value_counts().plot(kind='barh');","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d4aed28f91b3106732bfdd035029fcd2c3cede1a"},"cell_type":"markdown","source":"From the code below it seems that women are less susseptible to heart disease"},{"metadata":{"trusted":true,"_uuid":"f3c5294bf3ee6c50f305b0dd9af2c2ce065da891"},"cell_type":"code","source":"pd.concat([train_X, train_y], axis=1).groupby('sex').target.mean().plot(kind='barh').set_xlabel('% target');","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"35f55b929a8503766ec985173444614b96556ef2"},"cell_type":"markdown","source":"## Prepare the Data\n\n- to deal with categorical data i will use one hot vector"},{"metadata":{"trusted":true,"_uuid":"17d0ea4076973a34ec672d85729230597c1afef4"},"cell_type":"code","source":"fc = tf.feature_column\nCATEGORICAL_COLUMNS = ['sex', 'cp', 'fbs', 'restecg', 'exang', \n                       'slope', 'ca','thal']\nNUMERIC_COLUMNS = ['age', 'trestbps','chol','thalach','oldpeak']\n  \ndef one_hot_cat_column(feature_name, vocab):\n  return tf.feature_column.indicator_column(\n      tf.feature_column.categorical_column_with_vocabulary_list(feature_name,\n                                                 vocab))\nfeature_columns = []\nfor feature_name in CATEGORICAL_COLUMNS:\n  # Need to one-hot encode categorical features.\n  vocabulary = train_X[feature_name].unique()\n  feature_columns.append(one_hot_cat_column(feature_name, vocabulary))\n  \nfor feature_name in NUMERIC_COLUMNS:\n  feature_columns.append(tf.feature_column.numeric_column(feature_name,\n                                           dtype=tf.float32))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d1192bc69d28a314e6eaef55e182be8170616ef1"},"cell_type":"code","source":"#tf.keras.layers.DenseFeatures(feature_columns)(example).numpy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"184757b23e8f387f67f46d07c686e2f312df5456"},"cell_type":"code","source":"# Use entire batch since this is such a small dataset.\nNUM_EXAMPLES = len(train_y)\n\ndef make_input_fn(X, y, n_epochs=None, shuffle=True):\n  def input_fn():\n    dataset = tf.data.Dataset.from_tensor_slices((dict(X), y))\n    if shuffle:\n      dataset = dataset.shuffle(NUM_EXAMPLES)\n    # For training, cycle thru dataset as many times as need (n_epochs=None).    \n    dataset = dataset.repeat(n_epochs)\n    # In memory training doesn't use batching.\n    dataset = dataset.batch(NUM_EXAMPLES)\n    return dataset\n  return input_fn\n\n# Training and evaluation input functions.\ntrain_input_fn = make_input_fn(train_X, train_y)\neval_input_fn = make_input_fn(test_X, test_y, shuffle=False, n_epochs=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b4fc7dec47dd68cffaba3134431b7b9c0ad891ba"},"cell_type":"markdown","source":"## Train the model\n1. using logistic regretion \n2. using Boosted Trees"},{"metadata":{"trusted":true,"_uuid":"702c3cd176dfd926c9e267e69692bf71333daf71"},"cell_type":"code","source":"#Logistic\nlinear_est = tf.estimator.LinearClassifier(feature_columns)\n\n# Train model.\nlinear_est.train(train_input_fn, max_steps=100)\n\n# Evaluation.\nresult = linear_est.evaluate(eval_input_fn)\n#clear_output()\nprint(pd.Series(result))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a65f4eb94ea392cc1ffa31ed27158d0856aa38ac","scrolled":true},"cell_type":"code","source":"# Boosted Trees \nn_batches = 1\nest = tf.estimator.BoostedTreesClassifier(feature_columns,\n                                          n_batches_per_layer=n_batches)\n\n# The model will stop training once the specified number of trees is built, not \n# based on the number of steps.\nest.train(train_input_fn, max_steps=100)\n\n# Eval.\nresult = est.evaluate(eval_input_fn)\nprint(pd.Series(result))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"93dcb5f406d4c764c46c8528c31ca8f0161a3758"},"cell_type":"markdown","source":"- the logistic model did better, so if you want the best predictions it might make more sense to use the logistic model. \n- for determing feature importance i will use the Boosted Trees model "},{"metadata":{"trusted":true,"_uuid":"7dfc0d70b881575dfaefb3a3fd00d49c9209efe8"},"cell_type":"code","source":"pred_dicts = list(linear_est.predict(eval_input_fn))\nprobs = pd.Series([pred['probabilities'][1] for pred in pred_dicts])\n\nprobs.plot(kind='hist', bins=20, title='predicted probabilities');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a15c0d7257812c673d87ef21bdf20a6947745cae"},"cell_type":"code","source":"from sklearn.metrics import roc_curve\nfrom matplotlib import pyplot as plt\n\nfpr, tpr, _ = roc_curve(test_y, probs)\nplt.plot(fpr, tpr)\nplt.title('ROC curve')\nplt.xlabel('false positive rate')\nplt.ylabel('true positive rate')\nplt.xlim(0,)\nplt.ylim(0,);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6028d4e26dbfc336777fce784a3e14b559a705ef"},"cell_type":"markdown","source":"## Feature Importants\n- first will try and improve the Boosted Trees model"},{"metadata":{"trusted":true,"_uuid":"8cfbb987b4a93f127eb7011224b17e7aa6c1b824","scrolled":true},"cell_type":"code","source":"params = {\n  'n_trees': 500,\n  'max_depth': 1,\n  'n_batches_per_layer': 1,\n  # You must enable center_bias = True to get DFCs. This will force the model to \n  # make an initial prediction before using any features (e.g. use the mean of \n  # the training labels for regression or log odds for classification when\n  # using cross entropy loss).\n  'center_bias': True\n}\n\nest = tf.estimator.BoostedTreesClassifier(feature_columns, **params)\n# Train model.\nest.train(train_input_fn, max_steps=100)\n\n# Evaluation.\nresults = est.evaluate(eval_input_fn)\npd.Series(results).to_frame()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6933440077b1786137678c3c2e973b86b544fbd3"},"cell_type":"markdown","source":"- got a bit better acc of .83 in this model and .82 in the old model (still not as good as the logitic model)"},{"metadata":{"trusted":true,"_uuid":"d9cc24bab6d82fc3f48b49ce06247e7d157cb3d1"},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nsns_colors = sns.color_palette('colorblind')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f1fb92d1aac1a7b00fb6e6a03620352528589714"},"cell_type":"code","source":"pred_dicts = list(est.experimental_predict_with_explanations(eval_input_fn))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8ffa1c4789d2b5d5f40b5a196f53fa5d909905bf"},"cell_type":"markdown","source":"### Directional Feature Contributions (DFCs) \n- this is only to look at what factors effected a spesific person"},{"metadata":{"trusted":true,"_uuid":"1278094094a79272db79d0f23f751a9d6d05eb7a"},"cell_type":"code","source":"# Create DFC Pandas dataframe.\nlabels = test_y.values\nprobs = pd.Series([pred['probabilities'][1] for pred in pred_dicts])\ndf_dfc = pd.DataFrame([pred['dfc'] for pred in pred_dicts])\ndf_dfc.describe().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1ae79be86a4d0b34d258b71fb68b40da892f2cd6"},"cell_type":"code","source":"bias = pred_dicts[0]['bias']\ndfc_prob = df_dfc.sum(axis=1) + bias\nnp.testing.assert_almost_equal(dfc_prob.values,\n                               probs.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d558e14a4fca768377f2a0d00e64885aaa70767a"},"cell_type":"code","source":"# Boilerplate code for plotting :)\ndef _get_color(value):\n    \"\"\"To make positive DFCs plot green, negative DFCs plot red.\"\"\"\n    green, red = sns.color_palette()[2:4]\n    if value >= 0: return green\n    return red\n\ndef _add_feature_values(feature_values, ax):\n    \"\"\"Display feature's values on left of plot.\"\"\"\n    x_coord = ax.get_xlim()[0]\n    OFFSET = 0.15\n    for y_coord, (feat_name, feat_val) in enumerate(feature_values.items()):\n        t = plt.text(x_coord, y_coord - OFFSET, '{}'.format(feat_val), size=12)\n        t.set_bbox(dict(facecolor='white', alpha=0.5))\n    from matplotlib.font_manager import FontProperties\n    font = FontProperties()\n    font.set_weight('bold')\n    t = plt.text(x_coord, y_coord + 1 - OFFSET, 'feature\\nvalue',\n    fontproperties=font, size=12)\n    \ndef plot_example(example):\n  TOP_N = 8 # View top 8 features.\n  sorted_ix = example.abs().sort_values()[-TOP_N:].index  # Sort by magnitude.\n  example = example[sorted_ix]\n  colors = example.map(_get_color).tolist()\n  ax = example.to_frame().plot(kind='barh',\n                          color=[colors],\n                          legend=None,\n                          alpha=0.75,\n                          figsize=(10,6))\n  ax.grid(False, axis='y')\n  ax.set_yticklabels(ax.get_yticklabels(), size=14)\n\n  # Add feature values.\n  _add_feature_values(test_set.iloc[ID][sorted_ix], ax)\n  return ax","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b6c1c084bb11058dca5024e7c64975a08d8a7701"},"cell_type":"code","source":"# Plot results.\nID = 27\nexample = df_dfc.iloc[ID]  # Choose ith example from evaluation set.\nTOP_N = 8  # View top 8 features.\nsorted_ix = example.abs().sort_values()[-TOP_N:].index\nax = plot_example(example)\nax.set_title('Feature contributions for example {}\\n pred: {:1.2f}; label: {}'.format(ID, probs[ID], labels[ID]))\nax.set_xlabel('Contribution to predicted probability', size=14);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c17c9166b6aeb954301ad18a6006cb7e6348d32f"},"cell_type":"markdown","source":"### Gain-based feature importances"},{"metadata":{"trusted":true,"_uuid":"5d91fe7826b962469165bb03770e8e5ec6c8b383"},"cell_type":"code","source":"importances = est.experimental_feature_importances(normalize=True)\ndf_imp = pd.Series(importances)\n\n# Visualize importances.\nN = 8\nax = (df_imp.iloc[0:N][::-1]\n    .plot(kind='barh',\n          color=sns_colors[0],\n          title='Gain feature importances',\n          figsize=(10, 6)))\nax.grid(False, axis='y')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ae8a9daae69b0fa0ed379968631d45293fcba3fb"},"cell_type":"markdown","source":"### Avrage DFC Over all Data"},{"metadata":{"trusted":true,"_uuid":"9d66afbb2ca3c50e5ce57f39b64ccec3a238357f"},"cell_type":"code","source":"# Plot.\ndfc_mean = df_dfc.abs().mean()\nN = 8\nsorted_ix = dfc_mean.abs().sort_values()[-N:].index  # Average and sort by absolute.\nax = dfc_mean[sorted_ix].plot(kind='barh',\n                       color=sns_colors[1],\n                       title='Mean |directional feature contributions|',\n                       figsize=(10, 6))\nax.grid(False, axis='y')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f7ad6b3908429a002e5f33acd1caf7d1f2c39505"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6ae7f4406eb2696db45d3abcc1db69d899ec12c5"},"cell_type":"code","source":"# see how much a feature contributes based on its value\nFEATURE = 'oldpeak'\nfeature = pd.Series(df_dfc[FEATURE].values, index=test_set[FEATURE].values).sort_index()\nax = sns.regplot(feature.index.values, feature.values, lowess=True);\nax.set_ylabel('contribution')\nax.set_xlabel(FEATURE);\nax.set_xlim(0, 10);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"70d0fb7a232f35e8131b087afd2c50b8af39842b"},"cell_type":"markdown","source":"### Permutation feature importance\n- in this part I was able to use the logistic model from earlier so I did"},{"metadata":{"trusted":true,"_uuid":"bc01a0ef03a99e0255d8a6f2cd087667b562442d"},"cell_type":"code","source":"def permutation_importances(est, X_eval, y_eval, metric, features):\n    \"\"\"Column by column, shuffle values and observe effect on eval set.\n    \n    source: http://explained.ai/rf-importance/index.html\n    A similar approach can be done during training. See \"Drop-column importance\"\n    in the above article.\"\"\"\n    baseline = metric(est, X_eval, y_eval)\n    imp = []\n    for col in features:\n        save = X_eval[col].copy()\n        X_eval[col] = np.random.permutation(X_eval[col])\n        m = metric(est, X_eval, y_eval)\n        X_eval[col] = save\n        imp.append(baseline - m)\n    return np.array(imp)\n\ndef accuracy_metric(est, X, y):\n    \"\"\"TensorFlow estimator accuracy.\"\"\"\n    eval_input_fn = make_input_fn(X,\n                                  y=y,\n                                  shuffle=False,\n                                  n_epochs=1)\n    return est.evaluate(input_fn=eval_input_fn)['accuracy']\nfeatures = CATEGORICAL_COLUMNS + NUMERIC_COLUMNS\nimportances = permutation_importances(linear_est, test_set, test_y, accuracy_metric,\n                                      features)\ndf_imp = pd.Series(importances, index=features)\n\nsorted_ix = df_imp.abs().sort_values().index\nax = df_imp[sorted_ix][-5:].plot(kind='barh', color=sns_colors[2], figsize=(10, 6))\nax.grid(False, axis='y')\nax.set_title('Permutation feature importance');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ec7129865750ad6fb6876932eed3e2410bab63e6"},"cell_type":"markdown","source":"for more about feature importants info check out this [blog post](https://explained.ai/rf-importance/index.html). also for more info about how the code works check out the tensorflow [website](https://www.tensorflow.org/alpha/tutorials/estimators/boosted_trees)"},{"metadata":{"trusted":true,"_uuid":"438d352009411db3e6aab3becb78884da849edd5"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"25a2fb4eaf71967e998d926b7e0701cecf5ebde8"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}