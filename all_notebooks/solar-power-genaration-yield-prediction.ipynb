{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Prepare Data"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"sensor_data = '../input/solar-power-generation-data/Plant_1_Weather_Sensor_Data.csv'\ngen_data = '../input/solar-power-generation-data/Plant_1_Generation_Data.csv'\n\nsensor_df = pd.read_csv(sensor_data)\ngen_df = pd.read_csv(gen_data)\n\nsensor_df = sensor_df.drop(['PLANT_ID','SOURCE_KEY'],axis='columns')\ngen_df = gen_df.drop(['PLANT_ID'], axis='columns')\ngen_df[\"DATE_TIME\"] = pd.to_datetime(gen_df[\"DATE_TIME\"])\nsensor_df[\"DATE_TIME\"] = pd.to_datetime(sensor_df[\"DATE_TIME\"])\ndf = pd.merge(sensor_df,gen_df,on=\"DATE_TIME\",how=\"inner\")\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train = df.groupby(['DATE_TIME']).sum().iloc[:-1,-2:-1]\nfor feature_name in x_train.columns:\n    x_train[feature_name] = (x_train[feature_name] - x_train[feature_name].min())/(x_train[feature_name].max()-x_train[feature_name].min())\ny_train = df.groupby(['DATE_TIME']).sum().iloc[1:,-2:-1]\nfor feature_name in y_train.columns:\n    y_train[feature_name] = (y_train[feature_name] - y_train[feature_name].min())/(y_train[feature_name].max()-y_train[feature_name].min())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train.iloc[:277,:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train.iloc[24:30,:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train = x_train.values\ny_train = y_train.values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# K-Fold Split"},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nfrom sklearn.model_selection import KFold\nk = 10\nkf = KFold(n_splits=k)\nkf_data = {\"train\" : [],\"valid\" : [], \"test_list\" : []}\nfor train_index, valid_index in kf.split(x_train):\n    kf_data['train'].append(torch.utils.data.TensorDataset(torch.from_numpy(x_train[train_index]),torch.from_numpy(y_train[train_index])))\n    kf_data['valid'].append(torch.utils.data.TensorDataset(torch.from_numpy(x_train[valid_index]),torch.from_numpy(y_train[valid_index])))\n    kf_data['test_list'].append((x_train[valid_index],y_train[valid_index]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Define Network Architecture"},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch.nn as nn\nimport torch.nn.functional as F\n\nuse_cuda = torch.cuda.is_available()\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        hidden_1 = 200\n        hidden_2 = 200\n        hidden_3 = 100\n        # linear layer (1 -> hidden_1)\n        self.fc1 = nn.Linear(1, hidden_1)\n        # linear layer (n_hidden -> hidden_2)\n        self.fc2 = nn.Linear(hidden_1, hidden_2)\n        # linear layer (n_hidden2 -> hidden_3)\n        self.fc3 = nn.Linear(hidden_2, hidden_3)\n        # linear layer (n_hidden3 -> 1)\n        self.fc4 = nn.Linear(hidden_3, 1)\n        # dropout layer (p=0.5)\n        # dropout prevents overfitting of data\n        self.dropout = nn.Dropout(0.5)\n\n    def forward(self, x):\n        # flatten image input\n        x = x.view(-1, 1)\n        # add hidden layer, with relu activation function\n        x = F.relu(self.fc1(x))\n        # add dropout layer\n        x = self.dropout(x)\n        # add hidden layer, with relu activation function\n        x = F.relu(self.fc2(x))\n        # add dropout layer\n        x = self.dropout(x)\n        # add hidden layer, with relu activation function\n        x = F.relu(self.fc3(x))\n        # add dropout layer\n        x = self.dropout(x)\n        # add output layer\n        x = self.fc4(x)\n        return x","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train"},{"metadata":{"trusted":true},"cell_type":"code","source":"def train(n_epochs, loaders, model, optimizer, criterion, use_cuda, save_path):\n    \"\"\"returns trained model\"\"\"\n    # initialize tracker for minimum validation loss\n    valid_loss_min = np.Inf\n\n    for epoch in range(1, n_epochs+1):\n        # initialize variables to monitor training and validation loss\n        train_loss = 0.0\n        valid_loss = 0.0\n\n        ###################\n        # train the model #\n        ###################\n        model.train()\n        for data, target in loaders['train']:\n            # move to GPU\n            if use_cuda:\n                data, target = data.cuda(), target.cuda()\n            #clear gradient\n            optimizer.zero_grad()\n            ## find the loss and update the model parameters accordingly\n            output = model(data.float())\n            loss = torch.sqrt(criterion(output, target.float()))\n            loss.backward()\n            optimizer.step()\n            ## record the average training loss, using something like\n            ## train_loss = train_loss + ((1 / (batch_idx + 1)) * (loss.data - train_loss))\n            train_loss += loss.item()*data.size(0)\n\n        ######################    \n        # validate the model #\n        ######################\n        model.eval()\n        for data, target in loaders['valid']:\n            # move to GPU\n            if use_cuda:\n                data, target = data.cuda(), target.cuda()\n            ## update the average validation loss\n            output = model(data.float())\n            loss = torch.sqrt(criterion(output, target.float()))\n            valid_loss += loss.item()*data.size(0)\n\n        # calculate average losses\n        train_loss = train_loss/len(loaders['train'].dataset)\n        valid_loss = valid_loss/len(loaders['valid'].dataset)\n\n        # print training/validation statistics \n        print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n            epoch,\n            train_loss,\n            valid_loss))\n\n        ## TODO: save the model if validation loss has decreased\n        if valid_loss <= valid_loss_min:\n            print('Valid loss decreased ({:.6f} --> {:.6f}). Saving model ...'.format(\n            valid_loss_min,\n            valid_loss))\n            torch.save(model.state_dict(), save_path)\n            valid_loss_min = valid_loss\n    # return trained model\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 32\nfor i in range(k):\n    model = Net()\n    if use_cuda:\n        model.cuda()\n    criterion = nn.MSELoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-6)\n    train_loader = torch.utils.data.DataLoader(kf_data['train'][i], batch_size=batch_size, shuffle=True)\n    valid_loader = torch.utils.data.DataLoader(kf_data['valid'][i], batch_size=batch_size, shuffle=True)\n    loaders = {'train' : train_loader, 'valid' : valid_loader}\n    print()\n    print(f'Fold {i + 1}')\n    model = train(500, loaders, model, optimizer,criterion, use_cuda, 'model_fold_'+str(i+1)+'.pth')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# TEST"},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\ndef test(loaders, model, criterion, use_cuda,print_every=15):\n\n    # monitor test loss and accuracy\n    test_loss = 0.\n    correct = 0.\n    total = 0.\n\n    model.eval()\n    for batch_idx, (data, target) in enumerate(loaders['test']):\n        # move to GPU\n        if use_cuda:\n            data, target = data.cuda(), target.cuda()\n        # forward pass: compute predicted outputs by passing inputs to the model\n        output = model(data.float())\n        # calculate the loss\n        loss = torch.sqrt(criterion(output, target.float()))\n        # update average test loss \n        test_loss = test_loss + ((1 / (batch_idx + 1)) * (loss.data - test_loss))\n        # convert output probabilities to predicted class\n        pred = output.data.max(1, keepdim=True)[0]\n        \n\n    print('Test RMSELoss: {:.6f}\\n'.format(test_loss))\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"testdata = []\nfor i in range(k):\n    testdata.append(torch.utils.data.TensorDataset(torch.from_numpy(kf_data['test_list'][i][0]),torch.from_numpy(kf_data['test_list'][i][1])))\n    test_loader = torch.utils.data.DataLoader(testdata[i], batch_size=batch_size)\n    loaders['test'] = test_loader\n    model.load_state_dict(torch.load('model_fold_'+str(i+1)+'.pth'))\n    print()\n    print(f'Fold {i + 1}')\n    test(loaders, model, criterion, use_cuda)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Plot Results of each fold"},{"metadata":{"trusted":true},"cell_type":"code","source":"def prediction(input_data,model_path):\n    model.load_state_dict(torch.load(model_path))\n    out = []\n    if use_cuda:\n        model.cuda()\n    model.eval()\n    for batch_idx, (data, target) in enumerate(input_data):\n        if use_cuda:\n                data, target = data.cuda(), target.cuda()\n        output = model(data.float())\n        out.append(list(np.squeeze(output.data.max(1, keepdim=True)[0]).cpu().numpy()))\n    result = []\n    for i in out:\n        for j in i:\n            result.append(j)\n    return result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for test_index in range(k):\n    test_loader = torch.utils.data.DataLoader(testdata[test_index], batch_size=batch_size)\n    predict = prediction(test_loader,'model_fold_'+str(test_index+1)+'.pth')\n    \n    x_plot = range(len(kf_data['test_list'][test_index][1]))\n\n    plt.plot(x_plot, kf_data['test_list'][test_index][1], label = \"DATA\")\n    plt.plot(x_plot, predict, label = \"PREDICT\")\n\n    plt.xlabel('Time')\n    plt.ylabel('Normalized Daily_yield')\n    plt.title('Compared Results of Fold ' + str(test_index+1))\n\n    plt.legend()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# next n days Prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"def n_day_prediction(data,model_path,n):\n    result = []\n    out = []\n    model.load_state_dict(torch.load(model_path))\n    if use_cuda:\n        model.cuda() \n    model.eval()\n    for i in range(n):\n        output = model(data.float())\n        out.append(list(np.squeeze(output.data.max(1, keepdim=True)[0]).cpu().numpy()))\n        result = []\n        for i in out:\n            for j in i:\n                result.append(j)\n        data = result[:]\n        data = torch.from_numpy(np.array(data))\n    return result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n = 2\nfirst_point = torch.from_numpy(x_train[:277*n]) #3days data\n\nfor test_index in range(k):\n    npred = n_day_prediction(first_point,'model_fold_'+str(test_index+1)+'.pth',n)\n    x_plot = range(len(npred))\n\n    plt.plot(x_plot, npred, label = \"PREDICT\")\n    plt.xlabel('Time')\n    \n    plt.ylabel('Normalized Predicted Daily_yield')\n    plt.title('Predicted in next ' + str(3*n) + ' days for Fold ' + str(test_index+1))\n\n    plt.legend()\n    plt.show()\n    '''\n    Total_yield = dialy_yield(prediction) + Total_yield(before)\n    '''","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}