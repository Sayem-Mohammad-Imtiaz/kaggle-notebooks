{"cells":[{"metadata":{"_uuid":"a12bec911d641ed4292a6f7c303be8ee7061b5a0"},"cell_type":"markdown","source":"## Comparing Classification Methods LR-KNN-SVM-NB-RF\n\n* [Logistic Regression Classification](#1.)\n* [K-Nearest Neighbour (KNN) Classification](#2.)\n* [SVM Classification](#3.)\n* [Naive Bayes Classification](#4.)\n* [Decision Tree Classification](#5.)\n* [Random Forest Classification](#6.)\n\n![](https://iili.io/JGe6Ss.png)\n\n\n## Comparing Clustering Methods K-Means-Hierarchical\n\n* [K-Means Clustering](#7.)\n* [Hierarchical Clustering](#8.)\n\n![](https://iili.io/JGebi7.png)\n\n\n## Comparing Regression Methods (MLR-PR-SVR-DT-RF)\n\n* [Linear Regression](#9.)\n* [Polynomial Regression](#10.)\n* [Support Vector Regression , Scaling](#11.)\n* [Decision Tree](#12.)\n* [Random Forest](#13.)\n\n\n![](https://iili.io/JGkfWB.png)\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"#1. libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.datasets import load_iris","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d24b37570fa55323b214fb225d1102b31197605f"},"cell_type":"code","source":"#2. data preprocessing\n#2.1. data loading\ndf = pd.read_csv('../input/iris/Iris.csv')\n\ndf.sample(15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = df.iloc[:,1:5].values \n#y = df.iloc[:,5:].values \ny = df.Species.values\n\n#2.2 data standardization\n\nfrom sklearn.cross_validation import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.33, random_state=0)\n\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\n\nX_train = sc.fit_transform(x_train)\nX_test = sc.transform(x_test)\nprint(\"X_train: \\n\", X_train, \"\\n\")\nprint(\"X_test: \\n\", X_test, \"\\n\")","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"# Comparing Classification Methods LR-KNN-SVM-NB-RF\n\n\n<a id=\"1.\"></a> \n# Logistic Regression Classification"},{"metadata":{"trusted":true,"_uuid":"067abd37f1690f6cff3041b14d0f0d3b2f783f0e"},"cell_type":"code","source":"# 1. Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\nlogr = LogisticRegression(random_state=0)\nlogr.fit(X_train, y_train)\n\ny_pred = logr.predict(X_test)\n#print(y_pred)\n#print(y_test)\n\n# print('Logistic Regression confusion matrix')\ncm = confusion_matrix(y_test, y_pred)\n\n# %% cm visualization\n\nimport seaborn as sns\nf, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(cm,annot=True, linewidths=0.5, linecolor=\"white\", fmt=\".0f\", ax=ax, square=True, cmap=\"GnBu_r\")\nplt.xlabel(\"y_pred\")\nplt.ylabel(\"y_true\")\nplt.title(\"Logistic Regression (as classifer)\\n(confusion matrix)\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9efade0de23fff6f5cf5cb1ed842b9969b81fa83"},"cell_type":"markdown","source":"\n<a id=\"2.\"></a> \n# K-Nearest Neighbour (KNN) Classification"},{"metadata":{"trusted":true,"_uuid":"6fc2ca5f1b616493a65b5bc1ab5533659fb0308c"},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier(n_neighbors=1, metric='minkowski')\nknn.fit(X_train, y_train)\n\ny_pred = knn.predict(X_test)\n\ncm = confusion_matrix(y_test, y_pred)\n# print('KNN confusion matrix')\n#print(cm)\n\nimport seaborn as sns\nf, ax = plt.subplots(figsize =(5,5))\nsns.heatmap(cm,annot=True, linewidths=0.5, linecolor=\"white\", fmt=\".0f\", ax=ax, square=True, cmap=\"GnBu_r\")\nplt.xlabel(\"y_pred\")\nplt.ylabel(\"y_true\")\nplt.title(\"k-nearest-neighbor Classification\\n(confusion matrix)\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8013dede0f9324ea3a846317bd2844797674d17f"},"cell_type":"markdown","source":"\n<a id=\"3.\"></a> \n# SVM Classification"},{"metadata":{"trusted":true,"_uuid":"18efbdf138ae357394582738e4adcbe62863579b"},"cell_type":"code","source":"# 3. Support Vector Classifier (SVC) (SVM classifier)\nfrom sklearn.svm import SVC\nsvc = SVC(kernel='poly')\nsvc.fit(X_train, y_train)\n\ny_pred = svc.predict(X_test)\n\ncm = confusion_matrix(y_test, y_pred)\n# print('SVC confusion matrix')\n#print(cm)\n\nimport seaborn as sns\nf, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(cm, annot=True, linewidths=0.5, linecolor=\"white\", fmt=\".0f\", ax=ax, square=True, cmap=\"GnBu_r\")\nplt.xlabel(\"y_pred\")\nplt.ylabel(\"y_true\")\nplt.title(\"Support Vector Classification\\n(confusion matrix)\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6ea11ff10a1ba562fd9ff2376772331c5106c2eb"},"cell_type":"markdown","source":"\n<a id=\"4.\"></a> \n# Naive Bayes Classification"},{"metadata":{"trusted":true,"_uuid":"51328a34373b73c627777b970a48c8220f29bffd"},"cell_type":"code","source":"# 4. Naive Bayes Classification\nfrom sklearn.naive_bayes import GaussianNB\ngnb = GaussianNB()\ngnb.fit(X_train, y_train)\n\ny_pred = gnb.predict(X_test)\n\ncm = confusion_matrix(y_test,y_pred)\n# print('GNB')\n\nimport seaborn as sns\nf, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(cm, annot=True, linewidths=0.5, linecolor=\"white\", fmt=\".0f\", ax=ax, square=True, cmap=\"GnBu_r\")\nplt.xlabel(\"y_pred\")\nplt.ylabel(\"y_true\")\nplt.title(\"Gaussian Naive Bayes\\n(confusion matrix)\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1e295fa434cb03fe879f9a4cf55aa2ee86f4d3dc"},"cell_type":"markdown","source":"\n<a id=\"5.\"></a> \n# Decision Tree Classification"},{"metadata":{"trusted":true,"_uuid":"418969c939200079f921448926e1dfd37c0eb502"},"cell_type":"code","source":"# 5. Decision Tree Classification\nfrom sklearn.tree import DecisionTreeClassifier\ndtc = DecisionTreeClassifier(criterion='entropy')\n\ndtc.fit(X_train, y_train)\ny_pred = dtc.predict(X_test)\n\ncm = confusion_matrix(y_test,y_pred)\n# print('DTC')\n\nimport seaborn as sns\nf, ax = plt.subplots(figsize =(5,5))\nsns.heatmap(cm, annot=True, linewidths=0.5, linecolor=\"white\", fmt=\".0f\", ax=ax, square=True, cmap=\"GnBu_r\")\nplt.xlabel(\"y_pred\")\nplt.ylabel(\"y_true\")\nplt.title(\"Decision Tree Classifier\\n(confusion matrix)\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"72c6e52d91b787d4fc9293ce01ea8baa3a160e72"},"cell_type":"markdown","source":"\n<a id=\"6.\"></a> \n# Random Forest Classification"},{"metadata":{"trusted":true,"_uuid":"c172506731073027e2a622e9245c6a6aab542c84"},"cell_type":"code","source":"# 6. Random Forest Classification\nfrom sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier(n_estimators=10, criterion='entropy')\nrfc.fit(X_train, y_train)\n\ny_pred = rfc.predict(X_test)\ncm = confusion_matrix(y_test,y_pred)\n# print('RFC')\n#print(cm)\n\nimport seaborn as sns\nf, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(cm, annot=True, linewidths=0.5, linecolor=\"white\", fmt=\".0f\", ax=ax, square=True, cmap=\"GnBu_r\")\nplt.xlabel(\"y_pred\")\nplt.ylabel(\"y_true\")\nplt.title(\"Random Forest Classification\\n(confusion matrix)\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7d939ccec27ce9ff8fbf398ece7114549deacff2"},"cell_type":"code","source":"# 7. ROC, TPR, FPR\ny_proba = rfc.predict_proba(X_test)\nprint(y_test, len(y_test))\nprint(y_proba[:, 0], len(y_proba[:, 0]))\n\nfrom sklearn import metrics\nfpr, tpr, thold = metrics.roc_curve(y_test, y_proba[:, 0], pos_label='Iris-virginica')\nprint(\"FPR:\\n\", fpr)\nprint(\"\\nTPR:\\n\", tpr)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Comparing Clustering Methods: K-Means vs. Hierarchical"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# load the CUSTOMER dataset (gender, age, net_worth, salary)  -- musterlier is Turkish for customers\ndf = pd.read_csv('../input/comparingalgosedited/musteriler.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#X = df.drop(columns=['id','gender','age']) # leaving net_worth and salary\nX = df.drop(columns=['id','gender'])\n# print(X)\nx1 = X.net_worth\ny1 = X.salary\n\nplt.figure(figsize=(10,6))\nplt.xlabel(\"Net Worth\")\nplt.ylabel(\"Salary\")\nplt.title(\"Salary vs. Net Worth\")\nplt.scatter(x1, y1)\nplt.show()\n# print(df.info())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n<a id=\"7.\"></a> \n# K-Means Clustering"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import KMeans\nimport matplotlib.colors\n\ncolors = {0:'red', 1:'green', 2:'blue', 3:'yellow', 4:'orange', 5:'purple', 6:'teal', 7:'gray', 8:'pink', 9:'brown'}\nnum_plots = len(colors)\n\nresults = []\n\nfig, ax = plt.subplots(5, len(colors), figsize = (20, 25))\nfig.subplots_adjust(hspace=0.4, wspace=0.4)\n\n# train num_plots (10) models with k-means++, with number of clusters varying from 1 to 10\nfor i in range(num_plots):\n    kmeans = KMeans(n_clusters=i+1, init='k-means++', random_state=123)\n    kmeans.fit(X)\n    \n    plt.subplot(5, 2, i+1)\n    plt.scatter(x1, y1, c=[colors[i] for i in kmeans.labels_])\n    plt.xlabel(\"net worth\")\n    plt.ylabel(\"salary\")\n    plt.title('k =' + str(i+1))\n    #print(kmeans.labels_)\n    results.append(kmeans.inertia_)\n\nplt.show() # show all the individual subplots\n\nplt.plot(range(1,11), results, marker=\"o\")\nplt.title(\"Inertia\")\nplt.xlabel(\"k\")\nplt.ylabel(\"inertia\")\nplt.show() # show the inertia plot","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"centers_df = X.drop(columns=['age'])\ncenters_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# change this to see centers of centroids for any particular k of this k-means clustering example\nnumber_of_clusters=6\n\nkmeans = KMeans(n_clusters=number_of_clusters, init='k-means++')\nkmeans.fit(centers_df)\n\nclusters = kmeans.fit_predict(centers_df)\ndf[\"label\"] = clusters\n\nprint(\"Clusters:\\n\", clusters)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"kmeans cluster centers:\\n\", kmeans.cluster_centers_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,6))\nplt.scatter(df.net_worth, df.salary, c=[colors[i] for i in clusters])\nplt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], color=\"orange\", marker='s', s=300)\nplt.title(\"Centroids overlaid for k = \" + str(number_of_clusters) + \"\\n[orange squares]\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"8.\"></a> \n# Hierarchical Clustering\n\n## Dendogram\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.cluster.hierarchy import linkage, dendrogram\n\nmerge = linkage(X, method=\"ward\")\nplt.figure(figsize=(20,10))\n\ndendrogram(merge, leaf_rotation=90)\n\nplt.xlabel(\"data points\")\nplt.ylabel(\"euclidean distance\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Basic Agglomerative Algo (hierarchical clustering)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import AgglomerativeClustering\nac = AgglomerativeClustering(n_clusters=4, affinity='euclidean', linkage='ward')\nY_prediction = ac.fit_predict(X)\n\nplt.figure(figsize=(12,8))\nplt.scatter(X.salary, X.net_worth, s=100, c=[colors[i] for i in Y_prediction])\nplt.title('Hierarchical Clustering')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Comparing Regression Methods (MLR-PR-SVR-DT-RF)\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nfrom sklearn.metrics import r2_score\nimport statsmodels.api as sm\n\n# load 'results2' dataset (title, education_level, salary) -- maaslar is Turkish for 'results'\ndf = pd.read_csv('../input/comparingalgosedited/maaslar2.csv')\n\nx = df.drop(columns=['title','salary']) # keep only education_level as x\ny = df.drop(columns=['title','education_level']) # keep only salary as y\nX = x.values\nY = y.values\n\nregr_xlabel = \"Education Level\"\nregr_ylabel = \"Salary\"\n\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# correlation coefficients\ndf.corr()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"9.\"></a> \n# Linear Regression\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\n\nlin_reg = LinearRegression()\nlin_reg.fit(X, Y)\nmodel = sm.OLS(lin_reg.predict(X), X) # ordinary least squares (OLS)\n\nmodel.fit().summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# R2: how well does the function fit the data? (0-1)\nprint(\"Linear R2 value:\")\nprint(r2_score(Y, lin_reg.predict(X)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(X, Y, color='red')\nplt.plot(x, lin_reg.predict(X), color='blue')\nplt.xlabel(regr_xlabel)\nplt.ylabel(regr_ylabel)\nplt.title(\"Linear Regression\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n<a id=\"10.\"></a> \n# Polynomial Regression\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import PolynomialFeatures\n\n# degree -- the order of the polynomial function (e.g. degree=2 would be f(x)= ax + bx^2)\npoly_degree = 4\n\npoly_reg = PolynomialFeatures(degree=poly_degree)\nx_poly = poly_reg.fit_transform(X)\n#print(x_poly)\n\nlin_reg2 = LinearRegression()\nlin_reg2.fit(x_poly, y)\nmodel2 = sm.OLS(lin_reg2.predict(poly_reg.fit_transform(X)), X)\n\n# R2: how well does the function fit the data? (0-1)\nprint(\"Polynomial R2 value:\")\nprint(r2_score(Y, lin_reg2.predict(poly_reg.fit_transform(X)) ))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(X, Y, color='red')\nplt.plot(X, lin_reg2.predict(poly_reg.fit_transform(X)), color='blue')\nplt.xlabel(regr_xlabel)\nplt.ylabel(regr_ylabel)\nplt.title(\"Polynomial Regression\\n(degree=\" + str(poly_degree) + \")\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"11.\"></a> \n# Support Vector Regression (SVR) | Scaling\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\nsc1 = StandardScaler()\nx_scaled = sc1.fit_transform(X)\nsc2 = StandardScaler()\ny_scaled = sc2.fit_transform(Y)\n\nfrom sklearn.svm import SVR\n\nsvr_reg = SVR(kernel='rbf') # Google 'ML SVR kernel methods' for more info\nsvr_reg.fit(x_scaled, y_scaled) # train the model\n\nmodel3 = sm.OLS(svr_reg.predict(x_scaled), x_scaled) # predict \nmodel3.fit().summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# R2: how well does the function fit the data? (0-1)\nprint(\"SVR R2 value:\")\nprint(r2_score(y_scaled, svr_reg.predict(x_scaled)) )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(x_scaled, y_scaled, color='red')\nplt.plot(x_scaled, svr_reg.predict(x_scaled), color='blue')\nplt.xlabel(regr_xlabel)\nplt.ylabel(regr_ylabel)\nplt.title(\"SVR Regression\\n(kernel=rbf\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"12.\"></a> \n# Decision Tree\n\n* [Random Forest](#13.)\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor\n\nr_dt = DecisionTreeRegressor(random_state=0)\nr_dt.fit(X, Y)\n\nprint(\"Decision Tree OLS (ordinary least squares)\")\nmodel4 = sm.OLS(r_dt.predict(X), X)\n\nmodel4.fit().summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# R2: how well does the function fit the data? (0-1)\nprint(\"Decision Tree R2 value:\", r2_score(Y, r_dt.predict(X)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Z = X + 0.5\nK = X - 0.4\n\nplt.scatter(X, Y, color='red')\nplt.plot(x, r_dt.predict(X), color='blue')\nplt.plot(x, r_dt.predict(Z), color='green')\nplt.plot(x, r_dt.predict(K), color = 'yellow')\nplt.xlabel(regr_xlabel)\nplt.ylabel(regr_ylabel)\nplt.title(\"Decision Tree (OLS)\") # OLS=ordinary least squares\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n<a id=\"13.\"></a> \n# Random Forest\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nnum_decision_trees=10\n\nrf_reg = RandomForestRegressor(n_estimators=num_decision_trees, random_state=0)\nrf_reg.fit(X, Y)\n\nprint(\"Random Forest OLS\\n(using\", num_decision_trees, \"decision trees)\")\nmodel5 = sm.OLS(rf_reg.predict(X), X)\n\nmodel5.fit().summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(X, Y, color='red')\nplt.plot(x, rf_reg.predict(X), color='blue')\nplt.plot(x, rf_reg.predict(Z), color='green')\nplt.xlabel(regr_xlabel)\nplt.ylabel(regr_ylabel)\nplt.title(\"Random Forest OLS\\n(using \" + str(num_decision_trees) + \" decision trees)\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# R2: how well does the function fit the data? (0-1)\n# Summary of all R2 scores by algorithm\n\nprint('R2 Values by Algo (\\'goodness of fit\\', 0-1)\\n-------------------------------------------')\nprint(\"Linear R2 value:\\t\", r2_score(Y, lin_reg.predict((X))))\nprint(\"Polynomial R2 value:\\t\", r2_score(Y, lin_reg2.predict(poly_reg.fit_transform(X)) ))\nprint(\"SVR R2 value:\\t\\t\", r2_score(y_scaled, svr_reg.predict(x_scaled)))\nprint(\"Decision Tree R2 value:\\t\", r2_score(Y, r_dt.predict(X)))\nprint(\"Random Forest R2 value:\\t\", r2_score(Y, rf_reg.predict(X)))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}