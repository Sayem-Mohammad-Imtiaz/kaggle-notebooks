{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Hello. In this notebook I tried to make a cluster modeling using the credit card dataset, which has a very interesting feature to it: Most of the data is very clumped together and a bit of data wrangling is necessary in order to have satisfying results"},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data=pd.read_csv(\"/kaggle/input/ccdata/CC GENERAL.csv\")\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Removing null values and reseting index after drops"},{"metadata":{"trusted":true},"cell_type":"code","source":"data=data.dropna()\ndata=data.reset_index().drop(\"index\",axis=1)\ndata.drop(\"CUST_ID\", axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# In the plots below we can see the data clumping: A high number of occurences are found at the extremes of the ranges"},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in list(data.columns)[:-1]:\n    plt.figure(figsize=(10,5))\n    sns.distplot(data[i], bins=100)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Tenure is a feature that doesn't tell us much ( 85% of the customers have 12 month tenure), so we will remove this feature from the modeling"},{"metadata":{"trusted":true},"cell_type":"code","source":"len(data[data[\"TENURE\"]==12])/len(data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# We can make a pre-clustering encoding in order to make the groups more distinctive between one another"},{"metadata":{"trusted":true},"cell_type":"code","source":"#grouping variables with the same order of magnitude\nv1=['BALANCE', 'PURCHASES', 'ONEOFF_PURCHASES', 'INSTALLMENTS_PURCHASES', 'CASH_ADVANCE', 'CREDIT_LIMIT',\n        'PAYMENTS', 'MINIMUM_PAYMENTS']\nv2=['BALANCE_FREQUENCY', 'PURCHASES_FREQUENCY', 'ONEOFF_PURCHASES_FREQUENCY', 'PURCHASES_INSTALLMENTS_FREQUENCY', \n         'CASH_ADVANCE_FREQUENCY', 'PRC_FULL_PAYMENT']\nv3=['PURCHASES_TRX', 'CASH_ADVANCE_TRX']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data[v1].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data[v2].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe()[\"BALANCE\"][\"std\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# For the v1 and v3 groups we can see that the mean almost (and sometimes does) comprehends the 3 first quartiles of the data, indicating a great amount of outliers in the data. For this values, we will create a cutoff beyond 3 standard deviations (arbitrary value) to avoid the outliers"},{"metadata":{"trusted":true},"cell_type":"code","source":"def cutoff_function(col,data,metrics):\n    std_value=metrics[col][\"std\"]\n    mean_value=metrics[col][\"mean\"]\n    return data[data[col]<=(mean_value+3*std_value)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"metrics=data.describe()\nfor i in v1:\n    data=cutoff_function(i,data,metrics)\nfor i in v3:\n    data=cutoff_function(i,data,metrics)\ndata=data.reset_index().drop(\"index\",axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in list(data.columns)[:-1]:\n    plt.figure(figsize=(10,5))\n    sns.distplot(data[i], bins=100)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# For the v1 and v3 features we are going to take the following approach: We will group in 10 ranges, and the range size is going to be equal to a half the standard deviation. Most of the values will be categorized in the first ranges, which comprehend most of the customers, and the remaining high spending customers will be separated in different groups."},{"metadata":{"trusted":true},"cell_type":"code","source":"def range_function(col,data,metrics):\n    col_s=list(data[col])\n    std=metrics[col][\"std\"]\n    col_s=pd.Series(col_s/std)\n    return pd.Series(col_s.apply(apply_funct))\n\ndef apply_funct(val):\n    if val<=0.5:\n        return 1\n    elif val<=1:\n        return 2\n    elif val<=1.5:\n        return 3\n    elif val<=2:\n        return 4\n    elif val<=2.5:\n        return 5\n    elif val<=3:\n        return 6\n    elif val<=3.5:\n        return 7\n    elif val<=4:\n        return 8\n    elif val<=4.5:\n        return 9\n    elif val<=5:\n        return 10\n    else:\n        return 11","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"metrics=data.describe()\nfor i in v1:\n    col_name=i+\"_RANGE\"\n    v4.append(col_name)\n    data[col_name]=range_function(i,data,metrics)\nfor i in v3:\n    col_name=i+\"_RANGE\"\n    v5.append(col_name)\n    data[col_name]=range_function(i,data,metrics)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# For the v2 group, we will simply group them in 0.1 ranges"},{"metadata":{"trusted":true},"cell_type":"code","source":"def v2_range(val):\n    if val<=0.1:\n        return 1\n    elif val<=0.2:\n        return 2\n    elif val<=0.3:\n        return 3\n    elif val<=0.4:\n        return 4\n    elif val<=0.5:\n        return 5\n    elif val<=0.6:\n        return 6\n    elif val<=0.7:\n        return 7\n    elif val<=0.8:\n        return 8\n    elif val<=0.9:\n        return 9\n    else:\n        return 10","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in v2:\n    col_name=i+\"_RANGE\"\n    data[col_name]=data[i].apply(v2_range)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Dropping all original features, leaving only Tenure behind alongside the new features"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.drop(v1+v2+v3+[\"TENURE\"], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Our data is already standardized and we can move onto the clustering methods"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import KMeans","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Given dataset size and amount of clusters, only KMeans will be implemented on this kernel"},{"metadata":{},"cell_type":"markdown","source":"# We'll be using the elbow method to determine the best K value"},{"metadata":{"trusted":true},"cell_type":"code","source":"inertia=[]\nfor i in range(1,20):\n    km=KMeans(n_clusters=i)\n    km.fit(data)\n    inertia.append(km.inertia_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(range(1,20), inertia, \"--o\")\nplt.title(\"Elbow method\")\nplt.xlabel(\"K\")\nplt.ylabel(\"Inertia\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# The elbow is around 4 or 5 depending on the user's evaluation"},{"metadata":{"trusted":true},"cell_type":"code","source":"km=KMeans(n_clusters=4)\nkm.fit(data)\nclusters=km.fit_predict(data)\ndata[\"Clusters\"]=clusters","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for column in data:\n    g=sns.FacetGrid(data,col=\"Clusters\")\n    g.map(sns.distplot,column,kde=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"km=KMeans(n_clusters=5)\nkm.fit(data)\nclusters=km.fit_predict(data)\ndata[\"Clusters\"]=clusters","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for column in data:\n    g=sns.FacetGrid(data,col=\"Clusters\")\n    g.map(sns.distplot,column,kde=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# In K=5 we can determine the customer segmentation more easily:\n\n* Cluster 0: Customers who make a lot of low value purchases and tend to make higher value purchases with installments\n* Cluster 1: High spending customers, making a lot of high value purchases (smaller cluster)\n* Cluster 2: Low spending customers\n* Cluster 3: Customers who don't use their credit cards so often (the wide range in balance and limit differentiate this cluster from cluster 2)\n* Cluster 4: Customers who tend to make all their purchases in one-go, and tend to make less transactions"},{"metadata":{},"cell_type":"markdown","source":"# The analysis above has considered the most proeminent features in each cluster, and a different segmentation of ranges could have improved the contrast between clusters."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}