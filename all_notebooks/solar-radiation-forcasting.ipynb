{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# <u>27/04/2021  to 02/05/2021 work</u>","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <u>Data Collection and Preperation</u>\n**guys here we can understand how to combine multiple data_sets from a folder kindly go through this guys**","metadata":{}},{"cell_type":"code","source":"# files=os.listdir(r'E:\\solar_radiation_data')\n# files","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# path=r'E:\\solar_radiation_data'\n\n# #blank dataframe\n# final=pd.DataFrame()\n\n# for file in files:\n#     df=pd.read_csv(path+\"/\"+file,encoding='utf-8')\n#     final=pd.concat([df,final])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final = pd.read_csv(\"../input/solar-hourly-data/combined_data_from_2016_2019_with_out_extra_features.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final.head(n=-5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <u>Correlation Analysis</u>","metadata":{}},{"cell_type":"code","source":"final.corr()['GHI']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**since it is observed that \"GHi=clearsky Ghi\" we can remove clearsky Ghi**\n**we can also find that \"fill flag\",\"precipitable water\" and \"pressure\" has very Low impact on GHI so there is no \nuse of considering those columns**\n**<u>so by this correlation analysis I am going to elimanate \"clearsky GHI\",\"fill flag\",\"precipitable water\" and \"pressure\" columns from our Data</u>**\n","metadata":{}},{"cell_type":"code","source":"# let's rename our data to df from final to make it easy for analysis \ndf = final.drop([ \"Clearsky GHI\",\"Fill Flag\"], axis = 1)\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features = df.columns\nfor feature in features:\n    if (len(df[feature].unique())<50):\n        print('for {}:{}'.format(feature,df[feature].unique()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**from above Observation we can conclude other than date&time columns on cloud type contains no continuous values, rest of all other features contains Continuous features**","metadata":{}},{"cell_type":"markdown","source":"## <u>Univariate Analysis</u>","metadata":{}},{"cell_type":"code","source":"for feature in features:\n    plt.plot(df[feature], '+')\n    plt.xlabel(feature)\n    plt.title('analysis of'+feature)\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <u>Bivariate Analysis</u>","metadata":{}},{"cell_type":"code","source":"for feature in features:\n    df.plot(kind='scatter', x=feature, y='GHI')\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**1. we can observe GHI increases between 6 to 18 hours from hour and GHI plot\n  2. we can observe at cloud_type 5,9 there is no impact on GHI\n  3. we can also observe increasing Solar_Zenith_angle Decreases GHI\n  4. and can say increasing temperature can increase GHI\n  \n***<u>this is the observation from bivariate analysis between features and \"GHI\"</u>***\n\n****for <u>\"relative Humidity,wind direction,wind speed and dew point\"</u> we can't find any relation from this analysis let's try with another****\n  ","metadata":{}},{"cell_type":"code","source":"for feature in features:\n    df.plot(kind='scatter', y=feature, x='Month')\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***from above analysis between months and features we canclude that\n   1. between march and may GHI was at peak level then its get down fall\n   2. wind speed was maximum in month of july and started increasing from April and down fall starts from august***","metadata":{}},{"cell_type":"code","source":"for feature in features:\n    df.plot(kind='scatter', y=feature, x='Year')\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It is found that surface_albedo is maximum in year 2018","metadata":{}},{"cell_type":"markdown","source":"so far analysis can conclude droping DHI, CLearsky DHI,DNI, CLearsky DNI columns","metadata":{}},{"cell_type":"code","source":"df.corr()['GHI']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.dtypes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features = df.columns\nfeatures","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(1,len(features)):\n    data=df.copy()\n#     data[feature]=np.log(data[feature])\n    data.boxplot(column=features[i],grid = True)\n    plt.ylabel(features[i])\n    plt.title(features[i])\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.corr()['GHI']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = df.drop(['GHI'],axis =1)\ny = df['GHI']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <u>Model Creation</u>","metadata":{}},{"cell_type":"code","source":"# # if not installed pycaret\n# import sys\n# !{sys.executable} -m pip install numpy\n# !pip install pycaret","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Auto Pycaret","metadata":{}},{"cell_type":"code","source":"import pycaret\nfrom pycaret.regression import *\nreg = setup(data=df, target='GHI',remove_outliers=True,feature_selection_threshold=0.2, silent=True, feature_selection=True, transformation=False, train_size=0.8)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"compare_models()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train_Test_Split","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.3, random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Fitting","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Logistic Regression","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\nlr.fit(x_train,y_train)\nY_pred_lr = lr.predict(x_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import r2_score\nprint('R2 Score For Logistic Regression:',round(r2_score(Y_pred_lr,y_test)*100,2))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## K Nearest Neighbours","metadata":{}},{"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier(n_neighbors=7)\nknn.fit(x_train,y_train)\nY_pred_knn=knn.predict(x_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import r2_score\nprint('R2 Score For K Nearest Neighbours :',round(r2_score(Y_pred_knn,y_test)*100,2))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## xgboost","metadata":{}},{"cell_type":"code","source":"# !pip install xgboost","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import xgboost as xgb\n\nxgb_model = xgb.XGBRegressor(verbosity=0) \nxgb_model.fit(x_train, y_train)\n\nY_pred_xgb = xgb_model.predict(x_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import r2_score\nprint('R2 Score For xgboost Regressor :',round(r2_score(Y_pred_xgb,y_test)*100,2))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Y_pred_x = xgb_model.predict(x_train)\nprint('R2 Score For xgboost Regressor :',round(r2_score(Y_pred_x,y_train)*100,2))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Feeling that the model was overfitted because testing accuracy was arround 95% and training accuracy was 98% ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Neural Networks","metadata":{}},{"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense\nimport tensorflow as tf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Sequential()\nmodel.add(Dense(50, activation='relu',input_dim=15))\nmodel.add(Dense(40, activation='relu'))\nmodel.add(Dense(30, activation='relu'))\nmodel.add(Dense(20, activation='relu'))\nmodel.add(Dense(10, activation='relu'))\nmodel.add(Dense(1))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(optimizer='RMSProp', loss='mean_squared_error')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit(x_train,y_train,epochs=10)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Y_pred_nn = model.predict(x_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import r2_score\nprint('R2 Score For Neural Networks :',round(r2_score(Y_pred_nn,y_test)*100,2))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Y_pred_n = model.predict(x_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import r2_score\nprint('R2 Score For Neural Networks :',round(r2_score(Y_pred_n,y_train)*100,2))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b>finally I found Neural Networks was performing well from my conclusion with out any overfitting Problem</b>","metadata":{}},{"cell_type":"markdown","source":"# Testing the Model","metadata":{}},{"cell_type":"code","source":"x_test[:10]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"k = model.predict(x_test[:10])\nk","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test[:10]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1 = pd.DataFrame(k,columns=['predicted_values'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"here we can see that predited values are acceptable and model that we created also Generalised model with good training \nand testing accuracies with out overfitting","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Flatten, Dense, Dropout, BatchNormalization, AveragePooling2D","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.models import Sequential \nfrom keras.layers import Dense, Activation ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.initializers import RandomNormal\nmodel_relu_ADAM_BN_drop = Sequential()\n\nmodel_relu_ADAM_BN_drop.add(Dense(128, activation='relu', input_dim=15, kernel_initializer=RandomNormal(mean=0.0, stddev=0.062, seed=None)))\nmodel_relu_ADAM_BN_drop.add(BatchNormalization())\nmodel_relu_ADAM_BN_drop.add(Dropout(0.3))\n\nmodel_relu_ADAM_BN_drop.add(Dense(64, activation='relu', kernel_initializer=RandomNormal(mean=0.0, stddev=0.125, seed=None)) )\nmodel_relu_ADAM_BN_drop.add(BatchNormalization())\nmodel_relu_ADAM_BN_drop.add(Dropout(0.3))\n\nmodel_relu_ADAM_BN_drop.add(Dense(64, activation='relu', kernel_initializer=RandomNormal(mean=0.0, stddev=0.250, seed=None)) )\nmodel_relu_ADAM_BN_drop.add(BatchNormalization())\nmodel_relu_ADAM_BN_drop.add(Dropout(0.3))\n\nmodel_relu_ADAM_BN_drop.add(Dense(1))\n\nprint(model_relu_ADAM_BN_drop.summary())\n\nmodel_relu_ADAM_BN_drop.compile(optimizer='adam', loss='mean_squared_error')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model_relu_ADAM_BN_drop.fit(x_train, y_train, batch_size=128, epochs=10 , verbose=1, validation_data=(x_test, y_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Y_pred_rab = model_relu_ADAM_BN_drop.predict(x_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import r2_score\nprint('R2 Score For Neural Networks :',round(r2_score(Y_pred_rab,y_test)*100,2))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Y_pred_r =  model_relu_ADAM_BN_drop.predict(x_train)\nY_pred_rab[:10]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import r2_score\nprint('R2 Score For Neural Networks :',round(r2_score(Y_pred_r,y_train)*100,2))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test[:10]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# here we can see that the prediction were some how in accurate so we can't consider this model out of which Neural Networks performed well","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 26/04/2021 work","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df = pd.read_csv('C:/Users/HP/OneDrive/Desktop/combined_data_from_2016_2019_with_extra_features.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df.head(n=200)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df.corr()","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df = df.drop(['Clearsky GHI','DHI','Clearsky DHI','DNI','Clearsky DNI'],axis =1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(df.corr()[\"GHI\"].abs().sort_values(ascending=False))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df = df.drop(['Ozone','Fill Flag'],axis = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(df.corr()[\"GHI\"].abs().sort_values(ascending=True))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(np.percentile(df.Dew_Point,[99])[0])\n# print(np.percentile(df.Dew_Point,[100])[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(np.percentile(df.Solar_Zenith_Angle,[0])[0])\n# print(np.percentile(df.Solar_Zenith_Angle,[3])[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# features = df.columns\n# features","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Linear analysis\n# for feature in features:\n#     data=df.copy()\n#     plt.scatter(data[feature],data['GHI'])\n#     plt.xlabel(feature)\n#     plt.ylabel('GHI')\n#     plt.title(feature)\n#     plt.show()\n        ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ## We will be using logarithmic transformation to extract more relavent information from the data\n\n\n# for feature in features:\n#     data=df.copy()\n#     data[feature]=np.log(data[feature])\n#     data['GHI']=np.log(data['GHI'])\n#     plt.scatter(data[feature],data['GHI'])\n#     plt.xlabel(feature)\n#     plt.ylabel('GHI')\n#     plt.title(feature)\n#     plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for feature in features:\n#     data=df.copy()\n#     data[feature]=np.log(data[feature])\n#     data.boxplot(column=feature,grid = True)\n#     plt.ylabel(feature)\n#     plt.title(feature)\n#     plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## found many Outliers from Temp,Dew_poin, Solar_zenith_angle, Surface_albedo Columns","metadata":{}},{"cell_type":"code","source":"# from pycaret.regression import *\n# reg = setup(data=df, target='GHI',remove_outliers=True, normalize=True, silent=True,train_size=0.8)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}