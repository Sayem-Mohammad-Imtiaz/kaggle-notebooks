{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Exploratory Data Analysis - Airbnb London Dataset\n\nPortions of this EDA were based on the following articles:\n* https://towardsdatascience.com/exploratory-data-analysis-eda-python-87178e35b14\n* https://www.kaggle.com/ekami66/detailed-exploratory-data-analysis-with-python\n* https://levelup.gitconnected.com/a-complete-exploratory-data-analysis-with-python-45a57f5ef4c9\n* https://www.kaggle.com/ash316/eda-to-prediction-dietanic","metadata":{}},{"cell_type":"markdown","source":"## About London\n\nSource: Wikipedia\n\n![London](https://upload.wikimedia.org/wikipedia/commons/thumb/e/e4/Palace_of_Westminster_from_the_dome_on_Methodist_Central_Hall_%28cropped%29.jpg/1000px-Palace_of_Westminster_from_the_dome_on_Methodist_Central_Hall_%28cropped%29.jpg)\n\nLondon is the capital and largest city of England and the United Kingdom. The city stands on the River Thames in the south-east of England, at the head of its 50-mile (80 km) estuary leading to the North Sea. London has been a major settlement for two millennia, and was originally called Londinium, which was founded by the Romans. The City of London, London's ancient core and financial centre—an area of just 1.12 square miles (2.9 km2) and colloquially known as the Square Mile—retains boundaries that closely follow its medieval limits. The adjacent City of Westminster has for centuries been the location of much of the national government. Thirty-one additional boroughs north and south of the river also comprise modern London. The London region is governed by the mayor of London and the London Assembly.\n\nLondon is one of the world's most important global cities. It exerts a considerable impact upon the arts, commerce, education, entertainment, fashion, finance, healthcare, media, professional services, research and development, tourism and transportation. It is one of the largest financial centres in the world and in 2019, London had the second highest number of ultra high-net-worth individuals in Europe, after Paris. And in 2020, London had the second-highest number of billionaires of any city in Europe, after Moscow. London's universities form the largest concentration of higher education institutes in Europe, and London is home to highly ranked institutions such as Imperial College London in natural and applied sciences, the London School of Economics and social sciences, as well as the comprehensive University College London. In 2012, London became the first city to have hosted three modern Summer Olympic Games.\n\nLondon has a diverse range of people and cultures, and more than 300 languages are spoken in the region. Its estimated mid-2018 municipal population (corresponding to Greater London) was roughly 9 million, which made it the third-most populous city in Europe. London accounts for 13.4% of the U.K. population. Greater London Built-up Area is the fourth-most populous in Europe, after Istanbul, Moscow, and Paris, with 9,787,426 inhabitants at the 2011 census. The London metropolitan area is the third-most populous in Europe, after Istanbul and the Moscow Metropolitan Area, with 14,040,163 inhabitants in 2016.\n\nLondon contains four World Heritage Sites: the Tower of London; Kew Gardens; the site comprising the Palace of Westminster, Westminster Abbey, and St Margaret's Church; and the historic settlement in Greenwich where the Royal Observatory, Greenwich defines the Prime Meridian (0° longitude) and Greenwich Mean Time. Other landmarks include Buckingham Palace, the London Eye, Piccadilly Circus, St Paul's Cathedral, Tower Bridge, Trafalgar Square and The Shard. London has numerous museums, galleries, libraries and sporting events. These include the British Museum, National Gallery, Natural History Museum, Tate Modern, British Library and West End theatres. The London Underground is the oldest underground railway network in the world.","metadata":{}},{"cell_type":"markdown","source":"## London neighborhoods\n\n![London_neighborhoods](https://assets.londonist.com/uploads/2016/08/i730/absolute_profit_from_airbnb_london.jpg)","metadata":{}},{"cell_type":"markdown","source":"## 1. Data import","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\nplt.style.use('bmh')\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport os\ndfdict = dict()\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        name = Path(os.path.join(dirname, filename)).stem\n        dfdict[name] = pd.read_csv(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Let's look at the shape of the data","metadata":{}},{"cell_type":"code","source":"with pd.option_context('display.max_rows', 6, 'display.max_columns', None):  # show all df columns\n    for name, df_ in dfdict.items():\n        display(name)\n        display(df_.shape)\n        display(df_.head(6))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The main file is `listings`. Taking a quick look at its data, we can observe 76534 property listings on the London Airbnb dataset, which provides 74 columns of information, such as listing and picture URL's, the date when the listing was scraped from Airbnb's website, name and description of the property and a textual overview of its neighborhood, data about the host (id, url, name, location, short-bio, if the host was verified by Airbnb and some statistics about his/her response time). There are also columns containing geographical data about the property, such as latitude and longitude, its neighborhood, the property and room type (room, flat, etc.), how many persons it accommodates, how many beds, bedroom and bathrooms it has, room amenities, and, of course, information about price. The final group of columns regards several statistics about availability, guest reviews, and property rating (number of stars?).\n\n","metadata":{}},{"cell_type":"markdown","source":"## 2. Analyzing columns\n\nLet's perform an initial analysis of columns, including data types, removal of irrelevant columns, or columns with too much missing values, and finally we'll setup an index for each dataframe.","metadata":{}},{"cell_type":"markdown","source":"### 2.1. Check data types\nLet’s see how Pandas determined the types of each column when loading them","metadata":{}},{"cell_type":"code","source":"for name, df_ in dfdict.items():\n    display(name)\n    display(df_.info())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.2. Dropping irrelevant information or with too much missing values \n\nFrom these informations above we can already see that some features won't be relevant in our exploratory analysis as there are too much missing values (such as `license` and `bathrooms`). Plus there is so much features to analyse that it may be better to concentrate on the ones which can give us real insights.","metadata":{}},{"cell_type":"code","source":"# Remove columns with almost no values: listings: bathrooms, neighbourhood_group_cleansed, calendar_updated, license\ndfdict['listings'].drop(columns=['bathrooms', 'neighbourhood_group_cleansed', 'calendar_updated', 'license'], inplace=True)\n# Remove irrelevant columns\ndfdict['listings'].drop(columns=['listing_url', 'picture_url', 'host_url', 'host_name', 'host_thumbnail_url', 'host_picture_url'], inplace=True)\ndfdict['listings'].drop(columns=['minimum_minimum_nights', 'maximum_minimum_nights', 'minimum_maximum_nights', 'maximum_maximum_nights', \n                                 'minimum_nights_avg_ntm', 'maximum_nights_avg_ntm'], inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.3. Analyzing the date column on calendar and reviews dataframes","metadata":{}},{"cell_type":"code","source":"print(dfdict['calendar']['date'].head())\nprint(dfdict['reviews']['date'].head())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We need to convert the following columns from object to datetime64: `calendar.date, reviews.date, listings: host_since, calendar_last_scraped, first_review, last_review.`","metadata":{}},{"cell_type":"code","source":"dfdict['calendar']['date'] = pd.to_datetime(dfdict['calendar']['date'])\ndfdict['reviews']['date'] = pd.to_datetime(dfdict['reviews']['date'])\ndfdict['listings']['last_scraped'] = pd.to_datetime(dfdict['listings']['last_scraped'])\ndfdict['listings']['host_since'] = pd.to_datetime(dfdict['listings']['host_since'])\ndfdict['listings']['calendar_last_scraped'] = pd.to_datetime(dfdict['listings']['calendar_last_scraped'])\ndfdict['listings']['first_review'] = pd.to_datetime(dfdict['listings']['first_review'])\ndfdict['listings']['last_review'] = pd.to_datetime(dfdict['listings']['last_review'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that the dataset date range is [2010-08-18, 2011-10-09]","metadata":{}},{"cell_type":"markdown","source":"### 2.4. Converting other columns","metadata":{}},{"cell_type":"code","source":"def convert_price(df_column):\n    return df_column.str.replace('$', '', regex = 'true').str.replace(',', '', regex = 'true').astype(float)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfdict['listings']['price'] = convert_price(dfdict['listings']['price'])\ndfdict['calendar']['price'] = convert_price(dfdict['calendar']['price'])\ndfdict['calendar']['adjusted_price'] = convert_price(dfdict['calendar']['adjusted_price'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Other fields that need to be converted: \n\n% => host_response_rate, host_acceptance_rate;\n\nint => accommodates, bathrooms, bedrooms, beds, minimum_nights, maximum_nights, availability_30, availability_60, availability_90, availability_365, number_of_reviews, number_of_reviews_ltm, number_of_reviews_l30d, calculated_host_listings_count, calculated_host_listings_count_entire_homes, calculated_host_listings_count_private_rooms, calculated_host_listings_count_shared_rooms);\n\n? => license => we do not know its datatype and it does not contain enough data.","metadata":{}},{"cell_type":"code","source":"def convert_boolean(df_column):\n    return df_column.replace({'f': 0, 't': 1}).astype('boolean')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert t/f fields to boolean\n# calendar dataframe : available\n# listings dataframe : (host_is_superhost, host_has_profile_pic, host_identity_verified, calendar_updated, has_availability, instant_bookable)\ndfdict['calendar']['available'] = convert_boolean(dfdict['calendar']['available'])\ndfdict['listings']['host_is_superhost'] = convert_boolean(dfdict['listings']['host_is_superhost'])\ndfdict['listings']['host_has_profile_pic'] = convert_boolean(dfdict['listings']['host_has_profile_pic'])\ndfdict['listings']['host_identity_verified'] = convert_boolean(dfdict['listings']['host_identity_verified'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfdict['listings']['has_availability'] = convert_boolean(dfdict['listings']['has_availability'])\ndfdict['listings']['instant_bookable'] = convert_boolean(dfdict['listings']['instant_bookable'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert 'host_acceptance_rate', 'host_response_rate', removing the %\ndfdict['listings']['host_acceptance_rate'] = dfdict['listings']['host_acceptance_rate'].str.replace('%', '', regex = 'true').str.replace(',', '', regex = 'true').astype(float)\ndfdict['listings']['host_response_rate'] = dfdict['listings']['host_response_rate'].str.replace('%', '', regex = 'true').str.replace(',', '', regex = 'true').astype(float)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert % fields to float: listings.host_response_rate, listings.host_acceptance_rate\ndisplay(dfdict['calendar'].info())\n# Convert floats to int: listings.bathrooms, listings.bedrooms, listings.beds\ndisplay(dfdict['listings'].info())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have now converted all important columns from `object` to their appropriate datatype, except for columns containing large texts or arrays of values. Let's take a look at the index of each dataframe.","metadata":{}},{"cell_type":"markdown","source":"### 2.5. Check current indices","metadata":{}},{"cell_type":"code","source":"for name, df_ in dfdict.items():\n    display(name, df_.index, '------------------------')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's look for candidate indices for each dataframe.","metadata":{}},{"cell_type":"markdown","source":"#### 2.5.1. Calendar dataframe","metadata":{}},{"cell_type":"code","source":"print(len(dfdict['calendar'].index))\nprint(dfdict['calendar'].groupby(['listing_id', 'date'])['available'].transform('nunique')) #count(distinct)\nprint(dfdict['calendar'].groupby(['listing_id', 'date'])['available'].count())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As seen above, `['listing_id', 'date']` is a good index for calendar table, i.e., they have a one-to-one correspondance with each line. Let's set this index.","metadata":{}},{"cell_type":"code","source":"# Calendar appears to have listing_id and date as index\ndfdict['calendar'].set_index(['listing_id', 'date'], inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 2.5.2. Listings and Reviews dataframes\n\nThese two dataframes have a predefined `id` field.","metadata":{}},{"cell_type":"code","source":"print('[Listings] number of records: ', len(dfdict['listings'].index))\nprint('[Listings] Unique id values: ', len(dfdict['listings']['id'].unique()))\nprint('[Reviews] Listings: number of records: ', len(dfdict['reviews'].index))\nprint('[Reviews] Unique id values: ', len(dfdict['reviews']['id'].unique()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Observe that the dataset contains 76534 listed properties and over 1 million reviews. There are also 27 million lines on the calendar dataframe.","metadata":{}},{"cell_type":"markdown","source":"#### Both Listings and Reviews dataframes can have column 'id' as index, since it has unique values\n\nLet's set `id` as index for these 2 dataframes.","metadata":{}},{"cell_type":"code","source":"dfdict['listings'].set_index('id', inplace=True)\ndfdict['reviews'].set_index('id', inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3. Data Cleaning\n\nThe next step in the process of EDA is Data Cleaning. It is very important to get rid of the irregularities and clean the data after sourcing it into our system.\nIrregularities are of different types of data.\n\n* Missing Values\n* Incorrect Format\n* Incorrect Headers\n* Anomalies/Outliers","metadata":{}},{"cell_type":"markdown","source":"### 3.1. Check for null values","metadata":{}},{"cell_type":"code","source":"for name, df_ in dfdict.items():\n    display(name)\n    display(df_.isnull().sum())  # isna() does the same thing","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see, on the `calendar` dataframe, 4 columns contain missing values. We'll simply ignore the NaN values when plotting the graphs and analyzing the data.\n\nOn the `listings` dataframe, we'll ignore pure textual columns (with large sentences/text), such as: `name, description, neighborhood_overview, host_about, host_neighborhood and neighborhood`. \n\nFor now, we'll ignore the `reviews` dataframe, since it only contains textual (reviewer's comments) data.\n\nLet’s see how to handle the other missing values:  \n\nhost_response_time                              43221\n\nhost_response_rate                              43221\n\nhost_acceptance_rate                            34703\n\nhost_is_superhost                                  46\n\nbathrooms_text                                    181\n\nbedrooms                                         4838\n\nbeds                                             1219\n\nfirst_review                                    22194\n\nlast_review                                     22194\n\nreview_scores_rating                            23937\n\nreview_scores_accuracy                          23999\n\nreview_scores_cleanliness                       23990\n\nreview_scores_checkin                           24046\n\nreview_scores_communication                     23997\n\nreview_scores_location                          24045\n\nreview_scores_value                             24046\n\nreviews_per_month                               22194\n\nWe can handle missing values by dropping the missing records or by imputing the values. Or we can simply choose to ignore them at this moment. **That's exactly what we are going to do.**","metadata":{}},{"cell_type":"markdown","source":"### 3.3. Handling Outliers\n\nLet's take a look at some outliers, regarding property price.\n\nOuliers can be handled by dropping the records or imputing with the values or leaving them as is, if it makes more sense.","metadata":{}},{"cell_type":"code","source":"print(dfdict['listings']['price'].describe(percentiles=[0.25, 0.5, 0.75, 0.9, 0.95, 0.98, 0.99, 0.997]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"99.7% of all properties cost less than 1250 GBP. We could consider values above that as outliers.\n\nBesides, 90% of all properties cost less than 200 GBP.","metadata":{}},{"cell_type":"markdown","source":"## 4. Feature analysis","metadata":{}},{"cell_type":"markdown","source":"### 4.1. Univariate Analysis\n\nIf we analyze data over a single variable/column from a dataset, it is known as Univariate Analysis.","metadata":{}},{"cell_type":"markdown","source":"#### 4.1.1. Categorical Unordered Univariate Analysis\n\nAn unordered variable is a categorical variable that has no defined order. If we take our data as an example, the neighbourhood_cleansed column in the dataset is divided into many sub-categories like ..., etc. There is no weight or measure given to any value in the ‘neighbourhood_cleansed’ column.\nNow, let’s analyze the neighbourhood_cleansed category by using plots. Since neighbourhood_cleansed is a category, we will plot the bar plot.","metadata":{}},{"cell_type":"code","source":"# Let's calculate the percentage of each job status category.\ndisplay(dfdict['listings'].neighbourhood_cleansed.value_counts(normalize=True))\n\n# plot the bar graph of percentage job categories\nplt.figure(figsize = (12, 6))\ndfdict['listings'].neighbourhood_cleansed.value_counts(normalize=True).plot.barh()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"By the above bar plot, we can infer that the data set contains more number of Westminster bnb's compared to other neighborhoods. Other neighborhoods are also frequent, such as Tower Hamlets, Hackney, Kensignton and Chelsea, Camden and Islington.","metadata":{}},{"cell_type":"markdown","source":"#### 4.1.2. Categorical Ordered Univariate Analysis\n\nOrdered variables are those variables that have a natural rank of order. Some examples of categorical ordered variables from our dataset are:\n\nproperty_type: Entire cottage, Campsite, Shared Room, ...\n\nBedrooms, beds: 1, 2, 3, ...\n\nNow, let’s analyze the property_type from the dataset. Since we’ve already seen a bar plot, let’s see how a Pie Chart looks like.","metadata":{}},{"cell_type":"code","source":"print('bedrooms: ', dfdict['listings']['bedrooms'].unique())\nprint('beds: ', dfdict['listings']['beds'].unique())\nprint('review_scores_rating', dfdict['listings']['review_scores_rating'].unique())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### What types of property do we have?","metadata":{}},{"cell_type":"code","source":"dfdict['listings']['property_type'].unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# calculate the percentage of each property type.\ndisplay(dfdict['listings']['property_type'].value_counts(normalize=True))\n\n# plot the pie chart of property categories\nplt.figure(figsize = (20, 12))\ndfdict['listings']['property_type'].value_counts(normalize=True).plot.pie(autopct='%1.0f%%', pctdistance=1.1, labeldistance=1.2, rotatelabels=True)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"By the above analysis, we can infer that the data set has a large number of Entire apartments, followed by Private rooms in apartment and then by Private rooms in houses and Entire houses. Also, among the properties with very small percentage, we have 'Shared room in bus', 'Shared room in hotel', 'Room in minsu', 'Shared room in tent' and 'Earth house'. ","metadata":{}},{"cell_type":"markdown","source":"#### 4.1.3. Numerical features\n\nIf the column or variable is numerical, then we’ll analyze it by calculating its mean, median, standard deviation, etc. We can get those values by using the describe function.","metadata":{}},{"cell_type":"code","source":"for name, df_ in dfdict.items():\n    display(name)\n    display(df_.describe())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 4.1.3. a) Visualize Numerical Data Distributions — Histogram Plot of all features","metadata":{}},{"cell_type":"markdown","source":"Let's get all the types of our data from our dataset and take only the numerical ones.","metadata":{}},{"cell_type":"code","source":"df_num = dfdict['listings'].select_dtypes(include = ['float64', 'int64'])\ndf_num.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params = {'axes.titlesize':'8', 'xtick.labelsize':'12', 'ytick.labelsize':'12'}\nplt.rcParams.update(params)\ndf_num.drop(columns=['scrape_id', 'host_id']).hist(figsize=(20, 20), bins=50, xlabelsize=8, ylabelsize=8, ); # ; avoid having the matplotlib verbose informations","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Features such as `beds` and `bedrooms` seem to share a similar distribution to the one we have with `price`.\n\nRegarding `review_scores_rating`, remark that the majority of scores are concentrated above 75%.","metadata":{}},{"cell_type":"markdown","source":"#### 4.1.3. b) Visualize Price Distributions — Seaborn Histogram\n\nLet's take a look at how the property price is distributed.","metadata":{}},{"cell_type":"code","source":"print(dfdict['listings']['price'].describe(percentiles=[0.25, 0.5, 0.75, 0.9, 0.95, 0.98, 0.99, 0.997]))\nplt.figure(figsize=(9, 8))\nsns.distplot(dfdict['listings']['price'], color='g', bins=100, hist_kws={'alpha': 0.4});","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"With this information we can see that the prices are skewed right and some outliers lie above ~1250. Notice that the percentile 99.7% corresponds to this price of 1250 GBP.\n\nLet's regenerate the graph after removing these outliers (`price > 1250 GBP`).","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(9, 8))\nsns.distplot(dfdict['listings'][dfdict['listings']['price'] <= 1250]['price'], color='g', bins=100, hist_kws={'alpha': 0.4});","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 4.1.3. d) Box plot of 'price', 'review_scores_cleanliness' and 'review_scores_rating'\n","metadata":{}},{"cell_type":"markdown","source":"#### Property prices\n\nFirst, let's take a look at the property price distribution.\n\nSince there are some price outliers above the 1200 pounds range, we will filter them out when doing the box plots.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(10,8))\nplt.title('Price distribution for properties that cost < GBP 1200')\nsns.boxplot(y='price', x='room_type', data = dfdict['listings'][dfdict['listings']['price'] < 1200])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As expected, the average price of Entire home/apt is greater than Private rooms. Shared rooms are the cheapest properties.","metadata":{}},{"cell_type":"markdown","source":"#### Review scores\n\nLet's see the distribution of review scores, grouped by room type.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(10,8))  # 'review_scores_cleanliness', 'review_scores_rating'\nplt.title('Review scores distribution for London Airbnb properties.')\nsns.boxplot(y='review_scores_rating', x='room_type', data = dfdict['listings'][['room_type', 'review_scores_rating']])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that the majority of the review scores is above 80, regardless of the room type.","metadata":{}},{"cell_type":"markdown","source":"#### 4.1.3. e) Calculate and Visualize Correlations — Seaborn Heat Map\n\n Let's see if some variables are linked between each other and then try to explain their relation with common sense.","metadata":{}},{"cell_type":"code","source":"corr = df_num.corr()\nplt.figure(figsize=(12, 10))\n\nsns.heatmap(corr[(corr >= 0.1) | (corr <= -0.1)], \n            cmap='viridis', vmax=1.0, vmin=-1.0, linewidths=0.1,\n            annot=True, annot_kws={\"size\": 8}, square=True);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Only a few features seem to be correlated with each other. For example, \n\n* availability 30/60/90/365\n* bedrooms/beds/accomodates\n* review scores rating/accuracy/cleanliness/checkin/communication/value: the correlation between these columns indicates that, in a lot of cases, if a host has a good score (guest evaluation), s/he also has good scores related to cleanliness of the room/property, good communication with the guest and value, which makes sense.\n\nThe rest of the variables have very low correlation with each other. Against intuition, correlation between property price and review scores is very, very low...","metadata":{}},{"cell_type":"markdown","source":"#### 4.1.4.a) Visualize Categorical Data Distributions — Histogram Plot of all features\n\nWe'll now visualize the non-numerical features.","metadata":{}},{"cell_type":"code","source":"df_not_num = dfdict['listings'].select_dtypes(include = ['O'])\nprint('There are {} non numerical features including:\\n{}'.format(len(df_not_num.columns), df_not_num.columns.tolist()))\ndf_not_num.head(4)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### But we need to remove columns with large text or sentences! E.g., name, description, neighborhood_overview, host_about.","metadata":{}},{"cell_type":"code","source":"# 'host_location' is a pretty large column, with the host's full address! We'll not consider it.\n# 'host_neighbourhood' has way too many values, let's ignore it.\n# 'host_verifications' and 'amenities' are multi-value column. \n# TODO Include 'host_acceptance_rate', 'host_response_rate'\ndf_not_num = df_not_num[['neighbourhood_cleansed', 'property_type', 'room_type', 'host_response_time', 'bathrooms_text']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" Let's look at their distribution.","metadata":{}},{"cell_type":"code","source":"ncols = 1\n\nfig, axes = plt.subplots(round(len(df_not_num.columns) / ncols), ncols, figsize=(20, 40))\n#plt.xticks(rotation=90)\n\nfor i, ax in enumerate(fig.axes):\n    if i < len(df_not_num.columns):\n        ax.set_xticklabels(ax.xaxis.get_majorticklabels(), rotation=45)\n        ax.set_xticklabels(ax.get_xticklabels(), rotation = 90, ha=\"right\")\n        sns.countplot(x=df_not_num.columns[i], alpha=0.7, data=df_not_num, ax=ax)\n\nfig.tight_layout()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have already analyzed neighborhood frequency in a previous graph.\n\nWe can see that the majority of properties are of the following types: `Private room in apartment`, `Entire apartment`, `Entire townhouse`, `Entire house`, `Private room in townhouse`, `Entire condominium` and `Entire serviced apartment`.\n\nWhen it comes to `room_type`, the vast majority are `private rooms` and `entire home/apartments`.\n\nNormally, hosts give a response to their guests within an hour (more than 16000 hosts lie in this category). Less than 4000 hosts take more than a day/a few days or more to answer. ","metadata":{}},{"cell_type":"markdown","source":"## 4.2. Bivariate Analysis\n\nIf we analyze data by taking two variables/columns into consideration from a dataset, it is known as Bivariate Analysis.","metadata":{}},{"cell_type":"markdown","source":"### 4.2.1. Numeric-Numeric Analysis\n\nAnalyzing the two numeric variables from a dataset is known as numeric-numeric analysis. We can analyze it in three different ways.\n\n* Scatter Plot\n* Pair Plot\n* Correlation Matrix","metadata":{}},{"cell_type":"markdown","source":"#### 4.2.1. a) Scatter plot\n\nLet’s take the columns ‘price’, ‘review_scores_cleanliness’ and 'review_scores_rating' from our dataset and see what we can infer by plotting to scatter plot.","metadata":{}},{"cell_type":"code","source":"# plot the scatter plot of neighborhood and price variable in data\nplt.figure(figsize=(6, 5))\nplt.scatter(dfdict['listings'].review_scores_cleanliness, dfdict['listings'].price)\nplt.xticks(rotation=90)\nplt.title('review_scores_cleanliness x property price', fontsize =20)\nplt.show()\n\n# plot the scatter plot of review_scores_rating and price variable in data\ndfdict['listings'].plot.scatter(x=\"review_scores_rating\",y=\"price\")\nplt.title('review_scores_rating x property price', fontsize =20)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is not a clear correlation between property rating and the price. \n\nBut... In the second graph, we can see that high-priced properties (`> 5000 GBP`) have only high review-scores ratings (`above 80`). ","metadata":{}},{"cell_type":"markdown","source":"#### 4.2.1. b) Pair Plot\n\nNow, let’s plot Pair Plots for some other numerical columns. We’ll use the seaborn library for plotting Pair Plots.","metadata":{}},{"cell_type":"code","source":"# plot the pair plot of beds, price and property review scores in dataframe\nplt.figure(figsize = (10, 5))\nsns.pairplot(data = dfdict['listings'], vars=['beds', 'price', 'review_scores_cleanliness', 'review_scores_rating', \n                                              'review_scores_location', 'review_scores_value', 'host_acceptance_rate'])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It is difficult to find a trend between 2 different variables in the above graph.","metadata":{}},{"cell_type":"markdown","source":"#### 4.2.1. c) Correlation Matrix\n\nSince we cannot use more than two variables as x-axis and y-axis in Scatter and Pair Plots, it is difficult to see the relation between three numerical variables in a single graph. In those cases, we’ll use the correlation matrix.","metadata":{}},{"cell_type":"code","source":"# Creating a matrix using beds, accommodates, price and some review scores as rows and columns\nxpto = dfdict['listings'][['beds','accommodates','price', 'review_scores_cleanliness', 'review_scores_rating',\n                          'review_scores_location', 'review_scores_value', 'host_acceptance_rate']].corr()\n\n# plot the correlation matrix of these columns in data dataframe\nsns.heatmap(xpto, annot=True, cmap = 'Reds')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let's try to find which features are strongly correlated with `price`. We'll reuse our df_num dataset (created in 4.1.3.a) to do so.","metadata":{}},{"cell_type":"code","source":"df_num.corr()['price']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_num_corr = df_num.corr()['price'][:-1] # -1 because it is the latest row in df_num\ngolden_features_list = df_num_corr[abs(df_num_corr) > 0.1].sort_values(ascending=False)\nprint(\"There are {} correlated columns with respect to 'price':\\n{}\".format(len(golden_features_list), golden_features_list))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Correlation is very weak for these variables (`c < 0.2`).","metadata":{}},{"cell_type":"markdown","source":"### 4.2.2. Numeric - Categorical Analysis\n\nAnalyzing the one numeric variable and one categorical variable from a dataset is known as numeric-categorical analysis. We analyze them mainly using mean, median, and box plots.\n\nLet’s take price and neighborhood columns from our dataset.\nFirst check for mean value using groupby.","metadata":{}},{"cell_type":"markdown","source":"#### 4.2.2. a) Comparing mean and median values of price","metadata":{}},{"cell_type":"code","source":"# groupby the listings df to find the mean of the property price according to the city neighborhood.\ndfdict['listings'].groupby('neighbourhood_cleansed')['price'].mean()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is a lot of price difference between the neighborhoods. `City of London` has an average price of 258 pounds, while `Bexley` costs on average 59 pounds. This gives us a price difference of 199 pounds!\n\nLet’s calculate the median,","metadata":{}},{"cell_type":"code","source":"# groupby the listings df to find the median of the property price according to the city neighborhood.\ndfdict['listings'].groupby('neighbourhood_cleansed')['price'].median()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The median appears to be less sensitive to outliers. Now, we have: `City of London` with a median price of 119 pounds and `Bexley` with a median cost of 40 pounds. The median indicates a price difference of 79 pounds, against an average price difference of 199 pounds.","metadata":{}},{"cell_type":"markdown","source":"#### 4.2.2. b) Plot the bar graph of neighborhood x average value of price","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize = (12, 6))\ndfdict['listings'].groupby('neighbourhood_cleansed')['price'].mean().plot.bar()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"By the above graph, we can infer that the property price is, on average, higher for more central locations, like City of London and Westminster. ","metadata":{}},{"cell_type":"markdown","source":"#### 4.2.2. c) Boxplot of the price in function of the neighborhood\n\nBesides median and average values, it is important to observe the interquantile ranges (25%-75%), minimum and maximum values. The box plot gives us this information visually.","metadata":{}},{"cell_type":"code","source":"# plot the box plot of price according to neighborhood, after removing outliers where price > 300 GBP\nplt.figure(figsize = (20, 12))\ndf_filtered = dfdict['listings'][dfdict['listings'].price <= 300]\nsns.boxplot(df_filtered.price, df_filtered.neighbourhood_cleansed, orient=\"h\", palette=\"Set2\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see, when we plot the Box Plot, it paints a very different picture compared to mean and median. `City of London`, for instance, has the greatest minimum property price, and, together with `Kensignton and Chelsea` and `Westminster`, has the highest prices considering the IQR [25%-75&].","metadata":{}},{"cell_type":"markdown","source":"#### 4.2.2. d) Time series plot of price and availability\n\nLet's use the `calendar` dataframe to analyze the evolution of price and availability through time.\n\nThere are 27 million lines on the calendar dataframe. Let's see how many different dates we have.","metadata":{}},{"cell_type":"code","source":"dfdict['calendar']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The property listings refer to periods starting at Feb 2021 until Feb 2022.","metadata":{}},{"cell_type":"code","source":"print('There are', dfdict['calendar'].reset_index()['date'].nunique(), 'days and', \n      dfdict['calendar'].reset_index()['listing_id'].nunique(), 'unique listings on calendar df.')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's define a new 0/1 column to show availability.","metadata":{}},{"cell_type":"code","source":"dfdict['calendar']['occupied'] = dfdict['calendar']['available'].astype(float) * 100.0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Average price by day","metadata":{}},{"cell_type":"code","source":"avg_daily_price = dfdict['calendar'].reset_index().groupby('date').mean()\n#print(avg_daily_price)\n# Plotting the Graph\nplt.figure(figsize=(10, 5))\nprice_plot_by_day = avg_daily_price['price'].plot(title='Average property prices')\nprice_plot_by_day.set_xlabel('Date')\nprice_plot_by_day.set_ylabel('Property price')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Property prices have their lowest values on the start of the dataset (Mar, Apr, May), and they start to rise until July (summer season). From July on, they remain stable. Finally price hit a peak near Christmas and New Year's eve. \n\nThere is also a strange valley in Fev 2022...","metadata":{}},{"cell_type":"markdown","source":"#### Average availability by day","metadata":{}},{"cell_type":"code","source":"avg_daily_occupancy = dfdict['calendar'].reset_index().groupby('date').mean()\nprint(avg_daily_occupancy)\n# Plotting the Graph\nplt.figure(figsize=(10, 5))\noccupancy_plot_by_day = avg_daily_occupancy['occupied'].plot(title='Average property occupancy (%)')\noccupancy_plot_by_day.set_xlabel('Date')\noccupancy_plot_by_day.set_ylabel('Property Occupancy')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Average % Property Occupancy for 2021 (i.e. the % of properties already booked, by day) oscillates between 27% and almost 50%, depending on the month.","metadata":{}},{"cell_type":"markdown","source":"**Note: I gave up joining the `listing` and `calendar` dataframes, because of OutOfMemory errors in Kaggle Jupyter Kernel :-(**","metadata":{}},{"cell_type":"markdown","source":"df = dfdict['calendar'].join(dfdict['listings'], on='listing_id', rsuffix='_listing')","metadata":{}},{"cell_type":"markdown","source":"df.head(6)\nsns.lineplot(x=\"date\", y=\"occupied\",\n             hue=\"\", style=\"event\",\n             data=avg_daily_price)","metadata":{}},{"cell_type":"markdown","source":"### 4.2.3. Categorical — Categorical Analysis\n\nLet's see how the different categories like neighborhood, property_type, etc., are associated with each other. ","metadata":{}},{"cell_type":"markdown","source":"#### 4.2.3. a) Category plot of neighborhood, room type and price in dataframe `listings`","metadata":{}},{"cell_type":"code","source":"first_10_neihborhoods = dfdict['listings']['neighbourhood_cleansed'].unique()[0:10]\nprint('first_10_neihborhoods: ', first_10_neihborhoods)\ndf_filtered = dfdict['listings'][dfdict['listings']['price'] < 400]\ndf_filtered = df_filtered[df_filtered['neighbourhood_cleansed'].isin(first_10_neihborhoods)]\ng = sns.catplot(y=\"neighbourhood_cleansed\", x=\"price\", hue=\"room_type\", kind=\"bar\", data=df_filtered, height=8.27, aspect=11.7/8.27)\ng.set_xticklabels(rotation=90)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Impressive. Sometimes a hotel room can be more expensive than an entire home/apt! For example, this happens in Hounslow and Richmond upon Thames.","metadata":{}},{"cell_type":"markdown","source":"## 4.3. Multivariate Analysis\n\nIf we analyze data by taking more than two variables/columns into consideration from a dataset, it is known as Multivariate Analysis.\nLet’s see how ‘neighborhood_cleansed’, ‘bedrooms’, and ‘price’ vary with each other.\nWe’ll create a pivot table with the three columns and after that, we’ll create a heatmap.","metadata":{}},{"cell_type":"code","source":"result = pd.pivot_table(data=dfdict['listings'], index='neighbourhood_cleansed', columns='bedrooms',values='price')\nprint('Pivot table:\\n', result)\n\n# create heat map of neighbourhood vs price vs availability_rate\nplt.figure(figsize = (12, 6))\nax = plt.axes()\nsns.heatmap(result, annot=False, cmap = 'RdYlGn', center=0.117, ax=ax)\nax.set_title('Property price x neighborhood x number of beds')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Based on the Heatmap above, we can infer that more expensive properties usually have more beds available. Additionally, on more expensive neighborhoods, such as City of London, Westminster and Islington, smaller properties have high prices too. ","metadata":{}},{"cell_type":"markdown","source":"### TODO Other ideas: heat maps (using latitude and longitude) and tag clouds (using textual information from property listings, reviews, etc.).","metadata":{}},{"cell_type":"markdown","source":"## ","metadata":{}}]}