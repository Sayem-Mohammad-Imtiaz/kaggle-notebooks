{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Case Study of equipment’s signal quality"},{"metadata":{},"cell_type":"markdown","source":"PROJECT OBJECTIVE: The need is to build a regressor which can use these parameters to determine the signal strength or \nquality\nDOMAIN:  Electronics and Telecommunication \nCONTEXT: A communications equipment manufacturing company has a product which is responsible for emitting informative signals. \nCompany wants to build a machine learning model which can help the company to predict the equipment’s signal quality using \nvarious parameters.\nDATA DESCRIPTION: The data set contains information on various signal tests performed: \n        1. Parameters: Various measurable signal parameters. \n        2. Signal_Quality: Final signal strength or quality "},{"metadata":{"trusted":true},"cell_type":"code","source":"#%tensorflow_version 2.x\nimport tensorflow as tf\ntf.__version__","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. Import data. "},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport scipy.stats as stats \nimport matplotlib.pyplot as plt\nfrom tensorflow import keras\n#from keras.models import Sequential\n#from keras.layers import Dense\n#from sklearn.model_selection import StratifiedKFold\n%matplotlib inline\n#Test Train Split\nfrom sklearn.model_selection import train_test_split\n#Feature Scaling library\nfrom sklearn.preprocessing import StandardScaler\n#import pickle\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Flatten, Dense\nfrom tensorflow.keras import regularizers, optimizers\nfrom sklearn.metrics import r2_score\nfrom tensorflow.keras.models import load_model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Initialize the random number generator\nimport random\nseed = 7\nnp.random.seed(seed)\n\n# Ignore the warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Read the data as a data frame\nmydata = pd.read_csv('../input/part-123-signalcsv/Part- 123 - Signal.csv')\nmydata.head(20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"2. Data analysis & visualisation "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Shape of the data \nmydata.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are 1599 rows and 12 columns in data"},{"metadata":{},"cell_type":"markdown","source":"There are 1599 rows and 12 columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Data type of each attribute \nmydata.info()   # it gives information about the data and data types of each attribute","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All the parameters are floating point and the signal strength is an integer."},{"metadata":{},"cell_type":"markdown","source":"Apart from Signal Strength rest all features are floating point."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the presence of missing values\nnull_counts = mydata.isnull().sum()  # This prints the columns with the number of null values they have\nprint (null_counts)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are no null values in the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# 5 point summary of numerical attributes\nmydata.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looking the 11 parameters :\nParameter 3 ranges between 0 and 1.\nMaximum value of Parameter 5 is 0.6\nParameter 8 has a very low range between 0.9 and 1.004\nStandard deviation is lowest for Parameter 8, it is 0.001887\n'Signal_Strength' has classes as - 3.5, 4.0,5.0, 6.0, 7.0 and 7.5 "},{"metadata":{"trusted":true},"cell_type":"code","source":"# studying the distribution of continuous attributes\ncols = list(mydata)\nfor i in np.arange(len(cols)):\n    sns.distplot(mydata[cols[i]], color='blue')\n    #plt.xlabel('Experience')\n    plt.show()\n    print('Distribution of ',cols[i])\n    print('Mean is:',mydata[cols[i]].mean())\n    print('Median is:',mydata[cols[i]].median())\n    print('Mode is:',mydata[cols[i]].mode())\n    print('Standard deviation is:',mydata[cols[i]].std())\n    print('Skewness is:',mydata[cols[i]].skew())\n    print('Maximum is:',mydata[cols[i]].max())\n    print('Minimum is:',mydata[cols[i]].min())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Mean, median and mode are almost overlapping or too close to each other ecept in Parameter 7\nParameter 3 is trimodal and Signal strength is a classification variable.\nAll of them are positively skewed.\nStandard deviation is maximum for Parameter7, it is 32.895324478299074"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(mydata['Signal_Strength'])    # Distibution of the column 'Signal_Strength'\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"class 5.0 in 'Signal_Strength' has the highest count."},{"metadata":{"trusted":true},"cell_type":"code","source":"#plt.figure(figsize = (50,50))\nsns.pairplot(mydata,diag_kind='kde')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1.Parameter 6 and Parameter 7 are highly correlated with each other and visce versa and they have almost 0 correlation with other Parameters\n2.Parameter 1 is positively correlated to Parameter 3 and Parameter 8 and negatively correlated to Parameter 2 and Parameter 9.\n3.Parameter 4 is has very low correlation with other Parameters."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the presence of outliers\nl = len(mydata)\ncol = list(mydata.columns)\n#col.remove('condition')\nfor i in np.arange(len(col)):\n    sns.boxplot(x= mydata[col[i]], color='cyan')\n    plt.show()\n    print('Boxplot of ',col[i])\n    #calculating the outiers in attribute \n    Q1 = mydata[col[i]].quantile(0.25)\n    Q2 = mydata[col[i]].quantile(0.50)\n    Q3 = mydata[col[i]].quantile(0.75) \n    IQR = Q3 - Q1\n    L_W = (Q1 - 1.5 *IQR)\n    U_W = (Q3 + 1.5 *IQR)    \n    print('Q1 is : ',Q1)\n    print('Q2 is : ',Q2)\n    print('Q3 is : ',Q3)\n    print('IQR is:',IQR)\n    print('Lower Whisker, Upper Whisker : ',L_W,',',U_W)\n    bools = (mydata[col[i]] < (Q1 - 1.5 *IQR)) |(mydata[col[i]] > (Q3 + 1.5 * IQR))\n    print('Out of ',l,' rows in data, number of outliers are:',bools.sum())   #calculating the number of outliers","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Parameter 4 has the highest number of outliers which is 155."},{"metadata":{"trusted":true},"cell_type":"code","source":"#  function to treat outliers\ndef detect_treate_outliers(df,operation):\n    cols=[]\n    IQR_list=[]\n    lower_boundary_list=[]\n    upper_boundary_list=[]\n    outliers_count=[]\n    for col in df.columns:\n        #print('col',col)\n        if((df[col].dtype =='int64' or df[col].dtype =='float64') and (col != 'HR')):\n            #print('Inside if')\n            IQR = df[col].quantile(0.75) - df[col].quantile(0.25)\n            lower_boundary = df[col].quantile(0.25) - (1.5 * IQR)\n            upper_boundary = df[col].quantile(0.75) + (1.5 * IQR)\n            up_cnt = df[df[col]>upper_boundary][col].shape[0]\n            #print('Upper count=',up_cnt)\n            lw_cnt = df[df[col]<lower_boundary][col].shape[0]\n            #print('lower count=',lw_cnt)\n            if(up_cnt+lw_cnt) > 0:\n                cols.append(col)\n                IQR_list.append(IQR)\n                lower_boundary_list.append(lower_boundary)\n                upper_boundary_list.append(upper_boundary)\n                outliers_count.append(up_cnt+lw_cnt)\n                if operation == 'update':\n                    df.loc[df[col] > upper_boundary,col] = upper_boundary\n                    df.loc[df[col] < lower_boundary,col] = lower_boundary\n                else:\n                    pass\n            else:\n                pass\n   #print('cols=',cols)\n   # print('IQR_list=',IQR_list)\n   # print('lower_boundary_list=',lower_boundary_list)\n   # print('upper_boundary_list=',upper_boundary_list)\n   # print('outliers_count=',outliers_count)\n    ndf = pd.DataFrame(list(zip(cols,IQR_list,lower_boundary_list,upper_boundary_list,outliers_count)),columns=['Features','IQR','Lower Boundary','Upper Boundary','Outlier Count'])\n    #print('Data=',ndf)\n    #print('Columns having outliers=',cols)\n    if operation == 'update':\n        return (len(cols),df)\n    else:\n        return (len(cols),ndf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Removing outliers by replacing the data below lower whisker with it and above upper whisker with it respectively.\ncount,df=detect_treate_outliers(mydata,'update')\nif count>0:\n    print('Updating dataset')\n    mydata=df","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# studying correlation between the attributes\nb_corr=mydata.corr()\nplt.subplots(figsize =(12, 7)) \nsns.heatmap(b_corr,annot=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since high correlation coefficient value lies between ± 0.50 and ± 1\nParameter 1 is highly correlated with Parameter 3 and Parameter 8, Parameter 9.\nParameter 6 and 7 are highly correlated.\nBut since, the correlation is not too high near 0.8 or above not dropping the features."},{"metadata":{},"cell_type":"markdown","source":"3. Design, train, tune and test a neural network regressor. "},{"metadata":{"trusted":true},"cell_type":"code","source":"X = mydata.drop(\"Signal_Strength\", axis=1)\ny = mydata['Signal_Strength']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# splitting to create test data\nX_vtrain, X_test, y_vtrain, y_test = train_test_split(X, y, test_size=.30, random_state=seed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_vtrain.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# splitting to create training and validation data\nX_train, X_val, y_train, y_val = train_test_split(X_vtrain, y_vtrain, test_size=.20, random_state=seed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Initialize Sequential model\nmodel_reg = tf.keras.models.Sequential()\n\n# Normalize input data\nmodel_reg.add(tf.keras.layers.BatchNormalization(input_shape=(11,)))\n\n# Add final Dense layer for prediction - Tensorflow.keras declares weights and bias automatically\nmodel_reg.add(tf.keras.layers.Dense(1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compile the model - add mean squared error as loss and stochastic gradient descent as optimizer\nmodel_reg.compile(optimizer='sgd', loss='mse')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_reg.fit(X_train, y_train, validation_data=(X_val,y_val),epochs=100, batch_size=10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"4. Pickle the model for future use."},{"metadata":{"trusted":true},"cell_type":"code","source":"# save the model\nmodel_reg.save(\"model_reg.h5\") #using h5 extension\nprint(\"model saved!!!\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# load the model\nmodel_rr = load_model('model_reg.h5')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"error when trying to pickle is - \nTypeError: cannot pickle 'weakref' object\nand to resolve 'weakref' object we need to import dill and weakref butit cannot be saved with pickle, so \nI have used save() to save the model and load_model() to load it.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Save the Modle to file in the current working directory\n\n#Pkl_Filename = \"Pickle_RR_Model.pkl\"  \n#with open(Pkl_Filename, 'wb') as file:  \n#    pickle.dump(model_reg, file)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load the Model back from file\n\n#with open(Pkl_Filename, 'rb') as file:  \n#    Pickled_RR_Model = pickle.load(file)\n\n#Pickled_RR_Model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = model_rr.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(y_pred[0])\nprint(y_pred[1])\nprint(y_pred[2])\nprint(y_pred[3])\nprint(y_pred[4])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(y_test.head())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The first 5 elements of y_pred and y_test are close."},{"metadata":{"trusted":true},"cell_type":"code","source":"score_r = r2_score(y_test,y_pred)\nprint(score_r)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#summary of regression model\nmodel_rr.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Part 2"},{"metadata":{},"cell_type":"markdown","source":"# PROJECT OBJECTIVE: The need is to build a classifier which can use these parameters to determine the signal strength or quality ."},{"metadata":{},"cell_type":"markdown","source":"Steps 1 and 2 are same as for the regressor above"},{"metadata":{},"cell_type":"markdown","source":"3. Design, train, tune and test a neural network classifier."},{"metadata":{"trusted":true},"cell_type":"code","source":"# counting the number of classes in output\nmydata['Signal_Strength'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"yc = to_categorical(y, num_classes=8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# splitting data for test of categorial \nXcv_train, Xc_test, ycv_train, yc_test = train_test_split(X, yc, test_size=.30, random_state=seed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Shape of y_train:\", ycv_train.shape)\nprint(\"One value of y_train:\", ycv_train[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# splitting data for  train and validation of categorial \nXc_train, Xc_val, yc_train, yc_val = train_test_split(Xcv_train, ycv_train, test_size=.20, random_state=seed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Shape of y_train:\", yc_train.shape)\nprint(\"One value of y_train:\", yc_train[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_class = Sequential()\nmodel_class.add(Dense(11, activation='relu'))\nmodel_class.add(Dense(8, activation='relu'))\nmodel_class.add(Dense(8, activation='softmax'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compile the model\nmodel_class.compile(loss=\"categorical_crossentropy\", metrics=[\"accuracy\"], optimizer=\"sgd\")\n\n# Fit the model\nmodel_class.fit(x=Xc_train, y=yc_train, batch_size=20, epochs=100, validation_data=(Xc_val, yc_val))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"4. Pickle the model for future use."},{"metadata":{"trusted":true},"cell_type":"code","source":"# save the model\nmodel_class.save(\"model_class.h5\") #using h5 extension\nprint(\"model saved!!!\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# load the model\nmodel_cl = load_model('model_class.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# calculate score of training data\nscore = model_cl.evaluate(Xc_train, yc_train, verbose=0)\nprint(score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# score of test data\nscore_t = model_cl.evaluate(Xc_test, yc_test, verbose=0)\nprint( score_t)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#summary of classification model\nmodel_cl.summary()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"tensorflow-env","language":"python","name":"tensorflow-env"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"nbformat":4,"nbformat_minor":4}