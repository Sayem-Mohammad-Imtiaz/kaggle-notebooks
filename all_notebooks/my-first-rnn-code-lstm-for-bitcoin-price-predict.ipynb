{"cells":[{"metadata":{},"cell_type":"markdown","source":"### Vin√≠cius Rodrigues Ferraz"},{"metadata":{},"cell_type":"markdown","source":"## Recurrent Neural Networks"},{"metadata":{},"cell_type":"markdown","source":"## Important Note: The BTC Results are poor. That happens because I'm using an simple Neural Network with an simple input data (only the values of a few previous days). The objective of this BTC work is only to see how to code an RNN(LSTM)."},{"metadata":{},"cell_type":"markdown","source":"**RNNs are Neural Networks that allows previous outputs to be used as inputs (inputs and outputs have the same shape), while having hidden states.  \nRNNs are used to deal with sequential/temporal informations, since each epoch in RNN is an time step.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.display import Image\nImage(\"../input/images/RNN.png\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**As I made in the ANN Notebook, first I'll find some RNN work to take as a base, so I can learn in the process and apply it form myself later.**"},{"metadata":{},"cell_type":"markdown","source":"**NOTES:**  \n   - **I'll use [This Work](https://www.datatechnotes.com/2018/12/rnn-example-with-keras-simplernn-in.html) from datatechnotes to learn how to code an RNN**\n       - **All merits of this first work must be given to the author.**\n   - **After this, to apply what I've learned, I'll try to make on my own a RNN predict the next dinner. This is an example that I saw when I was learning about RNNs.**\n   - **Last, I'll try to make on my own a _BTC Price Predict_ applying LSTMs.**"},{"metadata":{},"cell_type":"markdown","source":"## Imports and Setup"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom keras.models import Sequential\nfrom keras.layers import Dense, SimpleRNN,Embedding","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Generating sequential dataset**"},{"metadata":{"trusted":true},"cell_type":"code","source":"n = 1000\ntrain_point = int(0.8*n)\n\nt = np.arange(0,n)\nX = np.sin((0.02*t)+2*np.random.rand(n)) #Multiplications done to make the graph \"clean\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Visualizing the data**"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.DataFrame(X)\ndf.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Now we will make the Train and the Test Data**\n****\n**RNN requires sequential Data but also requires a step**"},{"metadata":{"trusted":true},"cell_type":"code","source":"step = 4","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**For RNN our input need to be the same shape. When we make a step it brokes the last numbers like:**  \ndata = 1-10 | step = 4  \nx = 1 y = 5  \nx = 2 y = 6  \n...  \nx = 10 y = 14 (14 not in data)"},{"metadata":{"trusted":true},"cell_type":"code","source":"values = df.values\ntrain,test = values[0:train_point,:], values[train_point:n,:]\n#train.shape = (800,1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"step = 4\n# add step elements into train and test\ntest = np.append(test,np.repeat(test[-1,],step))\ntrain = np.append(train,np.repeat(train[-1,],step))\ntrain.shape # = (804,1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# convert into dataset matrix\ndef convertToMatrix(data, step):\n    X, Y =[], []\n    for i in range(len(data)-step):\n        d=i+step  \n        X.append(data[i:d,])\n        Y.append(data[d,])\n    return np.array(X), np.array(Y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainX,trainY =convertToMatrix(train,step)\nprint(trainX.shape)\ntestX,testY =convertToMatrix(test,step)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\ntestX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))\ntrainX.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainY.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Split ok**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#(batch_size, timesteps, input_dim)\ndef RNN():\n    model = Sequential()\n    model.add(SimpleRNN(units=32, input_shape=(1,step), activation=\"relu\"))\n    model.add(Dense(8,activation=\"relu\"))\n    model.add(Dense(1))\n    model.compile(loss=\"mean_squared_error\",optimizer=\"adam\")\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#print(trainX.shape,trainy.shape)\nmodel = RNN()\nmodel.fit(trainX,trainY, epochs=100, batch_size=16, verbose=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainPredict = model.predict(trainX)\ntestPredict= model.predict(testX)\npredicted=np.concatenate((trainPredict,testPredict),axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainScore = model.evaluate(trainX, trainY, verbose=0)\nprint(trainScore)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"index = df.index.values\nplt.plot(index,df)\nplt.plot(index,predicted)\nplt.axvline(df.index[train_point], c=\"lightgreen\",lw=2)\nplt.show() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**OK! It works!  \nNow, I will apply it on another sample.**"},{"metadata":{},"cell_type":"markdown","source":"**Since RNNs can treat sequence problems let's supose this case:  \n My roomate always make the same dinner, based on the day of week.  \n     Monday    - Pizza  \n     Tuesday   - Burguer  \n     Wednesday - Pancake  \n     Thursday  - spaghetti  \n     Friday    - Meat  \n     Saturday  - Chicken  \n     Sunday    - Sausage**"},{"metadata":{},"cell_type":"markdown","source":"**To make the work easy, I'll set numbers instead of strings**"},{"metadata":{},"cell_type":"markdown","source":"**Dataset Creation**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#dayofweek = [\"Monday, Tuesday, Wednesday, Thursday, Friday, Saturday, Sunday\"]\ndayofweek = [1,2,3,4,5,6,7]\n#dinner = [\"Pizza\",\"Burguer\",\"Pancake\",\"Spaghetti\",\"Meat\",\"Chicken\",\"Sausage\"]\ndinner = [1,2,3,4,5,6,7]\n\n#Repeating for greater dataset\ndayofweek = np.array(144*dayofweek) # = 1008 days\ndinner = np.array(144*dinner)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Train/Test Split**"},{"metadata":{},"cell_type":"markdown","source":"**Time-step will be 2, so, given the previous 2 days, the RNN will discover what's the next food  \nGiven Monday(1) and Tuesday(2) RNN have to return Pancake(3)**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Time-step\ntstep = 2\n\n# train = ~80% | test = ~20%\ntp = 800\n\ntrainX, trainY = dayofweek[0:tp], dinner[0:tp]\ntestX, testY = dayofweek[tp:], dinner[tp:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"testX.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Fixing dimensions**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def makematrix(dataX, dataY, tstep):\n    X, Y = [], []\n    for i in range(len(dataX)-tstep):\n        X.append(dataX[i:i+tstep,])\n        Y.append(dataY[i+tstep])\n    return np.array(X), np.array(Y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainX, trainY = makematrix(trainX, trainY, tstep)\ntestX, testY = makematrix(testX, testY, tstep)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"testX.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**RNNs require 3 dimensional inputs, so we have to reshape it**"},{"metadata":{"trusted":true},"cell_type":"code","source":"trainX = np.reshape(trainX, (trainX.shape[0], 1, tstep))\ntestX = np.reshape(testX, (testX.shape[0], 1, tstep))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**I'll create a graph function to plot future models**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def graph(hist):\n    # Plots Loss Line.\n    plt.plot(hist.history['loss'])\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    \n    lss = str(hist.history['loss'][-1])\n    plt.title(str('Loss='+lss))\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Defining the Model.  \n**Our model is a Recurrent Neural Network with 32 recurrent units and 1 hidden layer with 8 neurons  \nRelu is used as the activation function.  \nMean Squared Error is used as Loss Function with Adam optimizer.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def RNN():\n    model = Sequential()\n    import keras\n    keras.initializers.RandomNormal(mean=0.0, stddev=0.05, seed=5)\n    model.add(SimpleRNN(units=32, input_shape=(1,tstep), kernel_initializer=\"normal\",activation=\"relu\"))\n    model.add(Dense(1))\n    keras.optimizers.Adam(decay=1e-6,learning_rate=0.0001, beta_1=0.9, beta_2=0.999, amsgrad=False )\n    model.compile(loss=\"mean_squared_error\",optimizer=\"adam\")\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = RNN()\nhist = model.fit(trainX, trainY, epochs=350, batch_size=32, verbose=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainPredict = model.predict(trainX)\ntestPredict = model.predict(testX)\npredicted = np.concatenate((trainPredict,testPredict),axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainScore = model.evaluate(trainX, trainY, verbose=0)\nprint(trainScore)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"graph(hist)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Remembering**  \n**dayofweek = \n    [\"Monday, Tuesday, Wednesday, Thursday, Friday, Saturday, Sunday\"]  \ndayofweek = \n    [----1----,-----2----,-------3-----,------4-----,----5----,----6-----,----7----]**\n\n**dinner = \n    [\"Pizza\",\"Burguer\",\"Pancake\",\"Spaghetti\",\"Meat\",\"Chicken\",\"Sausage\"]  \ndinner =\n    [----1---,-----2-----,------3-----,------4------,----5----,----6-----,-----7-----]**"},{"metadata":{"trusted":true},"cell_type":"code","source":"seq = np.array([[[1, 2]],[[4, 5]],[[7, 1]]])\n# NextDay: [Wednesday],[Saturday],[Tuesday]\n# Returns: Pancake, Chicken, Burguer\n#           3.0      6.0      2.0\n\nfor i in model.predict(seq):\n    if round(i[0]) == 1:\n        print('\\n',round(i[0],3))\n        print(\"Pizza\")\n    elif round(i[0]) == 2:\n        print('\\n',round(i[0],3))\n        print(\"Burguer\")\n    elif round(i[0]) == 3:\n        print('\\n',round(i[0],3))\n        print(\"Pancake\")\n    elif round(i[0]) == 4:\n        print('\\n',round(i[0],3))\n        print(\"Spaghetti\")\n    elif round(i[0]) == 5:\n        print('\\n',round(i[0],3))\n        print(\"Meat\")\n    elif round(i[0]) == 6:\n        print('\\n',round(i[0],3))\n        print(\"Chicken\")\n    else:\n        print(\"Sausage\")\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### It works!\n****\n****\n****\n**OK! Now I'll try to predict BTC prices, based on both these previous works  \nBut this time I'll use LSTM**\n<br><br>\n**For sure the results won't be too close to the real value, since I'm using simple inputs and an simple Neural Network.**"},{"metadata":{},"cell_type":"markdown","source":"## Long Short-Term Memory"},{"metadata":{},"cell_type":"markdown","source":"**LSTMs are an special kind of RNNs.  \nLSTMs can memorize past terms better than RNN**"},{"metadata":{"trusted":true},"cell_type":"code","source":"Image(\"../input/images/LSTM.png\",width=400, height=400)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Forgetting: The Irrelevant Informations are forgetted by the Forget Gate.  \nPossibilities: All output possibilities, given the input.  \nIgnoring: Words to be ignored in actual LSTM cell  \nSelection: Selected words, given the input**\n<br></br><br></br>\n**First, the LSTM select the possibilities giving P possible words (Possibilities).  \nThem some of them (let's say the I ignored words) will be ignored by the Ignoring.   \nNon-ignored words will pass (Filtered possibilities).  \nSelected words from the forgetting will be removed(forgetted) from the Filtered Possibilities, and new words (Non-forgetted) will be: 1- collected by the memory 2- moved forward (both called Collected Possibilities).  \nFinally, LSTM will \"\"merge\"\" Selected Words with Collected Possibilities giving prediction. Also this prediction comes to the next LSTM cell as part of the input**"},{"metadata":{"trusted":true},"cell_type":"code","source":"Image(\"../input/images/LSTM2.png\", width=550, height=550)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nsc = MinMaxScaler()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"btcdf = pd.read_csv('../input/bitcoin-historical-price/Bitcoin_Historical_Price.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"btcdf['Date'] = pd.to_datetime(btcdf['Date'])\nbtcdf.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"btcdf['Close'] = btcdf['Close']/btcdf['Close'].max()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"group = btcdf.groupby('Date')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Real_Price = group['Close'].mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Real_Price[:3]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Real_Price.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1772 values - I'll split at 1400 = ~80%"},{"metadata":{"trusted":true},"cell_type":"code","source":"split_point = 1400\nstep = 30\nX_train = Real_Price[:split_point]\ny_train = Real_Price[step:split_point+step] # 30 days after train\nX_test = Real_Price[split_point:-step] # Values until the 30th last value\ny_test = Real_Price[split_point+step:] # 30 days after test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def makematrix(dataX, dataY, step):\n    X, Y = [], []\n    for i in range(len(dataX)-step):\n        X.append(dataX[i:i+step,])\n        Y.append(dataY[i+step])\n    return np.array(X), np.array(Y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, y_train = makematrix(X_train, y_train, step)\nX_test, y_test = makematrix(X_test, y_test, step)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Everything is OK!  \nReshaping to 3 dimensional**"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = np.reshape(X_train, (X_train.shape[0], 1, step))\nX_test = np.reshape(X_test, (X_test.shape[0], 1, step))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#X_train = sc.fit_transform(X_train)\n#X_test = sc.fit_transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.layers import LSTM","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def BTC_RNN():\n    model = Sequential()\n    model.add(LSTM(units=64, activation=\"sigmoid\", input_shape=(1,step)))\n    model.add(Dense(units=1))\n    import keras\n    keras.optimizers.Adam(decay=1e-6,learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False )\n    model.compile(loss=\"mean_squared_error\",optimizer=\"adam\")\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = BTC_RNN()\nhist1 = model.fit(X_train, y_train, batch_size=32, epochs=5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"graph(hist1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = y_test\ny = model.predict(X_test)\nplt.figure(figsize=(20,4))\nplt.plot(X,color='orange', label='Real BTC Prices')\nplt.plot(y,color='b', label = 'Predicted BTC Prices')\nplt.legend(loc=2, prop={'size': 12})\nplt.title('BTC Price Prediction', fontsize=20)\nplt.xlabel('Time', fontsize=15)\nplt.ylabel('BTC Price(USD)', fontsize=15)\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":1}