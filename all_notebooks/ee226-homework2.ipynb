{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# EE226 - Coding 2\n## Streaming algorithm & Locality Sensitive Hashing","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Streaming: DGIM","metadata":{}},{"cell_type":"markdown","source":"DGIM is an efficient algorithm in processing large streams. When it's infeasible to store the flowing binary stream, DGIM can estimate the number of 1-bits in the window. In this coding, you're given the *stream_data.txt* (binary stream), and you need to implement the DGIM algorithm to count the number of 1-bits. Write code and ask the problems below.","metadata":{}},{"cell_type":"markdown","source":"### Your task","metadata":{}},{"cell_type":"markdown","source":"1. Set the window size to 1000, and count the number of 1-bits in the current window.","metadata":{}},{"cell_type":"code","source":"import time\nimport sys as sys\n\nbuckets = []\nwindow_size = 1000\ncurrent_time = 4000\n\ndef Merge_buckets():\n    for i in range(len(buckets) - 1, 2, -1):\n        if buckets[i]['bitsum'] == buckets[i-3]['bitsum']:\n            buckets[i-3]['bitsum'] += buckets[i-2]['bitsum']\n            buckets[i-3]['timestamp'] = buckets[i-2]['timestamp']\n            del buckets[i-2]\n        \ndef DGIM():\n    bitsum = 0\n    start_time = time.time()\n    with open('../input/coding2/stream_data.txt','r') as f:\n        bits = f.readline().split()\n        start_stamp = 0 if current_time <= window_size else current_time - window_size\n        for i in range(min(window_size, current_time)):\n            bit = bits[start_stamp + i]\n            if len(buckets) > 0:\n                if buckets[0][\"timestamp\"] <= start_stamp:\n                    del bucket[0]\n            if int(bit) == 1:\n                bucket = {\"timestamp\":start_stamp + i,\"bitsum\":1}\n                buckets.append(bucket)\n                Merge_buckets()\n    for i in range(len(buckets)):\n        bitsum += buckets[i][\"bitsum\"]\n    bitsum -= buckets[0]['bitsum'] / 2\n    return bitsum if len(buckets) > 0 else 0, time.time() - start_time\n\nbit_sum, DGIM_time = DGIM()\nprint(\"The number of 1-bits in the current window:\",bit_sum,\" Using time: \",DGIM_time)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"2. Write a function that accurately counts the number of 1-bits in the current window, and compare the difference between its running time and space and the DGIM algorithm.","metadata":{}},{"cell_type":"code","source":"import csv\nimport numpy as np\n\ndef Acc_count_bits():\n    bitsum = 0\n    start_time = time.time()\n    with open('../input/coding2/stream_data.txt','r') as f:\n        bits = f.readline().split()\n        start_stamp = 0 if current_time <= window_size else current_time - window_size\n        for i in range(min(window_size, current_time)):\n            bit = bits[start_stamp + i]\n            if int(bit) == 1:\n                bitsum += 1\n    return bitsum, time.time()-start_time\n\nacc_bit_sum, acc_time = Acc_count_bits()\nprint(\"The number of 1-bits counted by DGIM in the current window:\",bit_sum,\" Using time: \",DGIM_time)\nprint(\"The accurate number of 1-bits in the current window:\",acc_bit_sum,\" Using time: \", acc_time)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the result we can see that using brute force costs less running time than DGIM, but using DGIM will cost much less space than brute force.","metadata":{}},{"cell_type":"markdown","source":"### Locality Sensitive Hashing","metadata":{}},{"cell_type":"markdown","source":"The locality sensitive hashing (LSH) algorithm is efficient in near-duplicate document detection. In this coding, you're given the *docs_for_lsh.csv*, where the documents are processed into set of k-shingles (k = 8, 9, 10). *docs_for_lsh.csv* contains 201 columns, where column 'doc_id' represents the unique id of each document, and from column '0' to column '199', each column represents a unique shingle. If a document contains a shingle ordered with **i**, then the corresponding row will have value 1 in column **'i'**, otherwise it's 0. You need to implement the LSH algorithm and ask the problems below.","metadata":{}},{"cell_type":"markdown","source":"### Your task","metadata":{}},{"cell_type":"markdown","source":"Use minhash algoirthm to create signature of each document, and find 'the most similar' documents under Jaccard similarity. \nParameters you need to determine:\n1) Length of signature (number of distinct minhash functions) *n*. Recommanded value: n > 20.\n\n2) Number of bands that divide the signature matrix *b*. Recommanded value: b > n // 10.","metadata":{}},{"cell_type":"code","source":"import csv\nimport numpy as np\n\n# Load the file\nwith open('../input/coding2/docs_for_lsh.csv','r') as f:\n    data = csv.reader(f)\n    data_matrix = []\n    for row in data:\n        data_matrix.append(list(map(int,row[1:])))\n    del(data_matrix[0])\n    matrix = np.array(data_matrix)\n    matrix = matrix.T\n    print(\"Loading the csv files...\")\n    print(matrix.shape)\n    print(matrix)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import random\n\n# Minhashing\ndef MinHash(data, b, r):\n    n = b*r\n    signature = []    \n    for i in range(n):                          \n        permutation = []                        # permutation pi for minhash\n        signal_signature = []                  \n        for num in range(1,data.shape[0]+1):\n            permutation.append(num)        \n        random.shuffle(permutation)             # initialize/randomize the permutation vector   \n        for j in range(data.shape[1]):\n            for k in range(data.shape[0]):\n                index = permutation.index(k+1)  # find the earliest '1', if find, then break  \n                if data[index][j] == 1:\n                    signal_signature.append(k+1)\n                    break\n                else:\n                    pass          \n        signature.append(signal_signature)\n    return np.array(signature)\n\nb = 10\nr = 5\nres_signature = MinHash(matrix,b,r)\nprint('MinHashing...')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Problem: For document 0 (the one with id '0'), list the 30 most similar document ids (except document 0 itself). You can valid your results with the sklearn.metrics.jaccard_score() function.\n\nTips: You can adjust your parameters to hash the documents with similarity s > 0.8 into the same bucket.","metadata":{}},{"cell_type":"code","source":"import hashlib\nfrom sklearn.metrics import jaccard_score\n\n# LSH functions\ndef LSH(signature, b, r):  \n    length, docnum = signature.shape    \n    buckets = {}                      # the hash buckets for LSH result, a dictionary type    \n    start = 0                         # the beginning location   \n    for i in range(b):\n        for j in range(docnum):\n            md5 = hashlib.md5()       # create hash object, here I use md5 in hashlib\n            signal_band = str(signature[start:start+r,j])\n            hashed_band = md5.update(signal_band.encode())    # need to encode\n            hashed_band = md5.hexdigest() \n            if hashed_band not in buckets:                 # put the hashed bands in the buckets,\n                buckets[hashed_band] = [j]                 # key for hashed id,\n            elif j not in buckets[hashed_band]:            # value for document id.\n                buckets[hashed_band].append(j)\n        start += r \n    return buckets\n\nprint('LSH...')\nLSH_table = LSH(res_signature,b,r)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Nearest Neighbor Searching\ndef NNS(LSH_table, num):\n    res = {}\n    for key in LSH_table:\n        if num in LSH_table[key] and len(LSH_table) != 1:    # avoid the bucket only contains the searched num itselt\n            for docnum in LSH_table[key]:\n                if docnum == num:\n                    pass\n                else:\n                    if docnum in res:\n                        res[docnum] += 1\n                    else:\n                        res[docnum] = 1\n    return res\n\nresult = NNS(LSH_table,0)\nresult = sorted(result.items(),key=lambda item:item[1])     # sort the result dictionary in an ascending order by the value of each key \n\nnearest_neighbor_num = 30\nnearest_neighbor = []\nfor i in range(len(result)-1,len(result)-nearest_neighbor_num-1,-1):   # find the nearest documents\n    nearest_neighbor.append(result[i])\nprint('The nearest documents with times in the buckets are {}. '.format(nearest_neighbor))\n\ncheck_data = matrix.T\n\nLSH_neighbor = []\n# Check the accuracy by calculating the jaccard score between the found documents and the document 0\nfor i in range(len(nearest_neighbor)):\n    check_doc = nearest_neighbor[i][0]\n    score = jaccard_score(check_data[check_doc],check_data[0], pos_label=1, average = 'binary')\n    print(\"doc {}'s score with doc 0 is : {}\".format(check_doc,score))\n    LSH_neighbor.append((check_doc,score))\nprint('The LSH results of nearest documents are {}'.format(LSH_neighbor))","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}