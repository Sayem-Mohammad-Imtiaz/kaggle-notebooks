{"cells":[{"metadata":{"trusted":false,"_uuid":"98885167be4b77dd08644f561cc1742294fe364b"},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport sklearn\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.style.use('fast')\nimport re\nimport nltk\nfrom nltk import word_tokenize\nfrom nltk.corpus import stopwords\nfrom sklearn.model_selection import train_test_split\nimport string\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics import classification_report\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.text import text_to_word_sequence\nfrom sklearn.metrics import accuracy_score\nstop_words = set(stopwords.words('english'))\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import roc_auc_score\nimport itertools\nimport seaborn as sns\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import StratifiedKFold\nfrom datetime import datetime\nfrom sklearn.model_selection import cross_val_score\nfrom scipy.sparse import hstack\nfrom wordcloud import WordCloud, STOPWORDS\nfrom sklearn.multiclass import OneVsRestClassifier\nimport pickle\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.svm import SVC","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"4ccc64274ffa9cdfdb4a9ebd8ae9b2c88769e5f2"},"cell_type":"code","source":"data = pd.read_csv('../input/Tweets.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"280e22d8659b7012b68774c750514fff3edec1ba"},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"3c0164d04609a8edb32f45ae82116be97b3c6bd8"},"cell_type":"code","source":"print(data.shape)\nprint(data.airline_sentiment.unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"e30a72a78de7b76a737b4d29467a9165ab1b1dbe"},"cell_type":"code","source":"data.isnull().sum()/data.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"4988c5c44e82ffca419555335a59694ce27eed37"},"cell_type":"code","source":"Index=[1, 2, 3]\nprint(data.airline_sentiment.value_counts())\nplt.bar(Index, data.airline_sentiment.value_counts())\nplt.xticks(Index, ['negative', 'neutral', 'positive'], rotation = 45)\nplt.ylabel('Number of tweets')\nplt.xlabel('Sentiment expressed in tweets')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"1d28eb01290309e18936579598af806959aca7ff"},"cell_type":"code","source":"print(data.airline_sentiment.value_counts() / data.shape[0])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e427fbbfb08fc1371bee4ad6cd44b2b572bd3ac9"},"cell_type":"markdown","source":"60% of the tweets are of negative sentiment"},{"metadata":{"trusted":false,"_uuid":"35e77839eb31dd15592fc157da49976cfc639ec1"},"cell_type":"code","source":"df=data.groupby([\"airline\",\"airline_sentiment\"]).size().unstack()\nprint(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"93a9f2a6a047ccd5468cd16e665ab35586cfffd9"},"cell_type":"code","source":"ax=df.plot.bar(stacked=True)\nplt.ylabel('Number of Tweets')\nplt.xticks(rotation=45)\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"2b4a1b4a3b57b5d3d6970093019b1b268679acfc"},"cell_type":"code","source":"df=data.groupby([\"airline\",\"airline_sentiment\"]).size().unstack()\ndf=df.div(df.sum(axis=1),axis='index')\nprint(df)\nax=df.plot.bar(stacked=True)\nplt.ylabel('Fraction of Tweets')\nplt.xticks(rotation=45)\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"eda590648c30455f9811b4fa32f01084326006fa"},"cell_type":"markdown","source":"Normalized sentiment by airline plotted to see how the relative number of the individual sentiments varies across different airlines."},{"metadata":{"trusted":false,"_uuid":"ef2421de222356c0634444294d08d61b39d6619d"},"cell_type":"code","source":"negative_tweets=data[(data.airline_sentiment==\"negative\") & (data.negativereason !=\"Can't Tell\")]\ndf=negative_tweets.groupby([\"negativereason\"]).size().sort_values()\ndf.plot.bar()\nplt.ylabel('Number of Tweets')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"77ab8c3740c2f73f2f3032dec0195b63087dfc06"},"cell_type":"markdown","source":"Plot for the reasons for negative comment reported in the tweets. The plot shows that the most common reason for negative sentiment was customer service issue, followed by late fight and canceled flights."},{"metadata":{"trusted":false,"_uuid":"9159b3033e7ea3e416f5e6801e3ab9d7f0d2f595"},"cell_type":"code","source":"df=negative_tweets.groupby([\"airline\",\"negativereason\"]).size().unstack()\nax=df.plot.bar(stacked=True)\nplt.ylabel('Number of Tweets')\nplt.xticks(rotation=45)\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7b775330f83bea32efc9f68e23e68630a8d354f6"},"cell_type":"markdown","source":"Negative Reasons for reviews per airlines"},{"metadata":{"trusted":false,"_uuid":"ae155d2a25145c96f18a991ccb6d9a909218ddfd"},"cell_type":"code","source":"df=negative_tweets.groupby([\"airline\",\"negativereason\"]).size().unstack()\ndf=df.div(df.sum(axis=1),axis='index')#rowsum\nax=df.plot.bar(stacked=True)\nplt.ylabel('Fraction of Tweets')\nplt.xticks(rotation=45)\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5beba25389cb4e5fdd6faa360ad74c8c925b3048"},"cell_type":"markdown","source":"# Objectives\n#### 1. Classify positive, negative, and neutral tweets\n#### 2. If tweet is negative specify negative reasons (such as \"late flight\" etc).\n\n"},{"metadata":{"_uuid":"a799f83898a4de47be58ba12833586fb04af649d"},"cell_type":"markdown","source":"Normalized Negative Reasons for reviews per airlines"},{"metadata":{"trusted":false,"_uuid":"7ea3ffe49c5354bf8c1a190f599dfbe0e53988b3"},"cell_type":"code","source":"data_bak = data.copy","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6563fa7b7ebf8977f588efd8443680cd21643f55"},"cell_type":"markdown","source":"Preprocessing"},{"metadata":{"trusted":false,"_uuid":"df637500d0b615363efcf75dee2060cd96d62af1"},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5446c7f67a288b8f4670dd835f4ecf2d7ba9d8f4"},"cell_type":"markdown","source":"### Text Cleanup, Preprocessing\n    Remove @Mentions, #Hashtags of tweets\n    Convert Emoji to Text\n    Remove Stop Words\n    Convert to Lower Case\n    Convert words with Apostrophe to proper words\n    Lemmatize Text"},{"metadata":{"trusted":false,"_uuid":"1a4056d56925a7b6769d3c9d2a0a5962542adb54"},"cell_type":"code","source":"# apostrophe lookup dict\nAPPO = {\n\"aren't\" : \"are not\",\n\"can't\" : \"cannot\",\n\"couldn't\" : \"could not\",\n\"didn't\" : \"did not\",\n\"doesn't\" : \"does not\",\n\"don't\" : \"do not\",\n\"hadn't\" : \"had not\",\n\"hasn't\" : \"has not\",\n\"haven't\" : \"have not\",\n\"he'd\" : \"he would\",\n\"he'll\" : \"he will\",\n\"he's\" : \"he is\",\n\"i'd\" : \"I would\",\n\"i'd\" : \"I had\",\n\"i'll\" : \"I will\",\n\"i'm\" : \"I am\",\n\"isn't\" : \"is not\",\n\"it's\" : \"it is\",\n\"it'll\":\"it will\",\n\"i've\" : \"I have\",\n\"let's\" : \"let us\",\n\"mightn't\" : \"might not\",\n\"mustn't\" : \"must not\",\n\"shan't\" : \"shall not\",\n\"she'd\" : \"she would\",\n\"she'll\" : \"she will\",\n\"she's\" : \"she is\",\n\"shouldn't\" : \"should not\",\n\"that's\" : \"that is\",\n\"there's\" : \"there is\",\n\"they'd\" : \"they would\",\n\"they'll\" : \"they will\",\n\"they're\" : \"they are\",\n\"they've\" : \"they have\",\n\"we'd\" : \"we would\",\n\"we're\" : \"we are\",\n\"weren't\" : \"were not\",\n\"we've\" : \"we have\",\n\"what'll\" : \"what will\",\n\"what're\" : \"what are\",\n\"what's\" : \"what is\",\n\"what've\" : \"what have\",\n\"where's\" : \"where is\",\n\"who'd\" : \"who would\",\n\"who'll\" : \"who will\",\n\"who're\" : \"who are\",\n\"who's\" : \"who is\",\n\"who've\" : \"who have\",\n\"won't\" : \"will not\",\n\"wouldn't\" : \"would not\",\n\"you'd\" : \"you would\",\n\"you'll\" : \"you will\",\n\"you're\" : \"you are\",\n\"you've\" : \"you have\",\n\"'re\": \" are\",\n\"wasn't\": \"was not\",\n\"we'll\":\" will\",\n\"didn't\": \"did not\",\n\"tryin'\":\"trying\"\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"cc921172d1fb50a4592bc9e6b494084aa53e2ef9"},"cell_type":"code","source":"lemmatizer = WordNetLemmatizer()\nimport emoji\n\ntest_str = \"Thank you @VirginAmerica for you amazing üëç customer support team on Tuesday 11/28 at @EWRairport and returning my lost bag in less than 24h! #efficiencyiskey #virginamerica\"\n\ndef clean_text(text):\n    text = re.sub(r'(?:@[\\w_]+)', \" \", text) # @-mentions\n    text = re.sub(r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\", \" \", text) # hash-tags\n    text = emoji.demojize(text).replace('_','')\n    #text = re.sub(r'\\d+', '', text)\n    text = re.sub(r'\\b\\w{1,1}\\b', '', text)\n    #text = re.sub(r'http.?://[^\\s]+[\\s]?', '', text) #Remove URLs\n    return text\n\ntest_str = clean_text(test_str)\n\ndef cleanupText(s):\n    stopset = set(stopwords.words('english'))\n    stopset.add('wikipedia')\n\n    tokens =sequence=text_to_word_sequence(s, \n                                        filters=\"\\\"!'#$%&()*+,-ÀöÀô./:;‚Äò‚Äú<=¬∑>?@[]^_`{|}~\\t\\n\",\n                                        lower=True,\n                                        split=\" \")\n    tokens=[APPO[token] if token in APPO else token for token in tokens]\n    for token in tokens:\n        lemmatizer.lemmatize(token, 'v')\n    cleanup = \" \".join(filter(lambda word: word not in stopset, tokens))\n    return cleanup\n\nprint(cleanupText(test_str))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"b11cd818bdf883ece18dbb9a8e6761c83654fb00"},"cell_type":"code","source":"data.text = data.text.map(lambda text : clean_text(text))\ndata.text = data.text.apply(cleanupText)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"7ded4c2930bd76f03f32314ad3e471d5ccce93d7"},"cell_type":"code","source":"df = data[data['airline_sentiment']=='negative']\nwords = ' '.join(df['text'])\nwordcloud = WordCloud(stopwords=STOPWORDS,\n                      background_color='black',\n                      width=3000,\n                      height=2500\n                     ).generate(words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"40d852940f4111debf71091ca3b29ad4e3a67e2e"},"cell_type":"code","source":"plt.figure(1,figsize=(12, 12))\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"94162691da62ab52f3ff53ae7674e229ff9739b9"},"cell_type":"code","source":"df = data[data['airline_sentiment']=='positive']\nwords = ' '.join(df['text'])\nwordcloud = WordCloud(stopwords=STOPWORDS,\n                      background_color='black',\n                      width=3000,\n                      height=2500\n                     ).generate(words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"32ef990eefdc9700181d58f17c688177f51e5561"},"cell_type":"code","source":"plt.figure(1,figsize=(12, 12))\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"aa14ec399ef355cab8d6abd637d4a3be5f90782b"},"cell_type":"markdown","source":"Bag Of Words Model"},{"metadata":{"trusted":false,"_uuid":"fa053407cf8be616ec15706b9d8bde854ae49165"},"cell_type":"code","source":"\ndef get_metrics(y_test, y_predicted):  \n    # true positives / (true positives+false positives)\n    precision = precision_score(y_test, y_predicted, pos_label=None,\n                                    average='weighted')             \n    # true positives / (true positives + false negatives)\n    recall = recall_score(y_test, y_predicted, pos_label=None,\n                              average='weighted')\n    \n    # harmonic mean of precision and recall\n    f1 = f1_score(y_test, y_predicted, pos_label=None, average='weighted')\n    \n    # true positives + true negatives/ total\n    accuracy = accuracy_score(y_test, y_predicted)\n    return accuracy, precision, recall, f1\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"002b33c2dc06b31f2d274a3ef06e31cbb924c383"},"cell_type":"markdown","source":"Converting Target Label which is text to Numerical Form using Label Encoder"},{"metadata":{"trusted":false,"_uuid":"a7e5e7c2920e75d4eb78ba2d67fe6aed681898b8"},"cell_type":"code","source":"lb = LabelEncoder()\ndata['sentiment_encoded'] = lb.fit_transform(data['airline_sentiment'])\ndata[['airline_sentiment', 'sentiment_encoded']]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"56a8c95fde3c52f72937571b5f15d4180a4f9c65"},"cell_type":"markdown","source":"### Feature Engineering on text data\n    Trying 3 ways:\n    Count Vectorizer\n    TF-IDF Vectorizer\n    Count Vectorizer with NGrams(BiGrams)"},{"metadata":{"trusted":false,"_uuid":"2efd3fcfc2beeed249a51a2325724bfbaad0103a"},"cell_type":"code","source":"count_vect = CountVectorizer(decode_error='ignore',stop_words='english')\ncount_vect.fit_transform(data.text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"5fe2066b9da14ef1981adeed060c2dbccbb1f58d"},"cell_type":"code","source":"print(data.airline.unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"270b2986970abda2690f84a69fe1601c4e1eaaf4"},"cell_type":"code","source":"X = data.text\nprint(X.shape)\ny = data['sentiment_encoded']\nprint(y.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"239ed142b0f1eac1b25f023d80a132cc92c6a387"},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, test_size=0.20, shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"fd4a4fca6556b6e86995736efbac131d10fb803d"},"cell_type":"code","source":"X_train_count = count_vect.transform(X_train)\nX_test_count = count_vect.transform(X_test)\nprint(X_train_count.shape)\nprint(X_test_count.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"2eeef667614ef81d47c449a3bbb1db3a5cb9b80d"},"cell_type":"code","source":"y[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"e14379eefd5fb045bb53a96b10633cfd64579624"},"cell_type":"code","source":"classifier = MultinomialNB()\nclassifier.fit(X_train_count, y_train)\n\n# Predicting the Test set results\ny_pred = classifier.predict(X_test_count)\naccuracy, precision, recall, f1 = get_metrics(y_test, y_pred)\nprint(\"accuracy = %.3f, precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracy, precision, recall, f1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"e0ca9dc5a2e40f98f5cc4dbb635990b393c3a862"},"cell_type":"code","source":"classifier = LogisticRegression(multi_class='multinomial', random_state = 0, n_jobs = -1, solver = 'sag', C=1, max_iter = 2000)\nclassifier.fit(X_train_count, y_train)\n\n# Predicting the Test set results\ny_pred = classifier.predict(X_test_count)\naccuracy, precision, recall, f1 = get_metrics(y_test, y_pred)\nprint(\"accuracy = %.3f, precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracy, precision, recall, f1))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2eba0c1882001309bdc785b3c802442fcf510e07"},"cell_type":"markdown","source":"TF_IDF"},{"metadata":{"trusted":false,"_uuid":"ac34d6c41906b93e577d9b5f08a52484b1d12c06"},"cell_type":"code","source":"word_vectorizer = TfidfVectorizer(\n    sublinear_tf=True,\n    strip_accents='unicode',\n    analyzer='word',\n    token_pattern=r'\\w{1,}',\n    stop_words='english',\n    ngram_range=(1, 2),\n    max_features=None,\n    smooth_idf = True\n)\nword_vectorizer.fit_transform(data.text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"a4c3a6a2b3857a9686227b0391e6e826e7f91dea"},"cell_type":"code","source":"X_train_word = word_vectorizer.transform(X_train)\nX_test_word = word_vectorizer.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"62230a4af548cfcb20a1ebca7d4c2f015a601755"},"cell_type":"code","source":"classifier = MultinomialNB()\nclassifier.fit(X_train_word, y_train)\n\n# Predicting the Test set results\ny_pred = classifier.predict(X_test_word)\naccuracy, precision, recall, f1 = get_metrics(y_test, y_pred)\nprint(\"accuracy = %.3f, precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracy, precision, recall, f1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"f49abb09e22726e10c062b2c31fa574ea032de2c"},"cell_type":"code","source":"classifier = LogisticRegression(multi_class='multinomial', random_state = 0, n_jobs = -1, solver = 'sag', C=1)\nclassifier.fit(X_train_word, y_train)\n\n# Predicting the Test set results\ny_pred = classifier.predict(X_test_word)\naccuracy, precision, recall, f1 = get_metrics(y_test, y_pred)\nprint(\"accuracy = %.3f, precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracy, precision, recall, f1))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8022ad0a9362009194c963ed2253c614ae322b8e"},"cell_type":"markdown","source":"Count NGrams"},{"metadata":{"trusted":false,"_uuid":"afd37bbb5a7e5872c547751a2f0a461f1a7bf5e8"},"cell_type":"code","source":"count_vect_ngram = CountVectorizer(decode_error='ignore',stop_words='english', ngram_range=(1, 2), token_pattern=r'\\w{1,}')\ncount_vect_ngram.fit_transform(data.text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"93586a3345f1b89721aa27762291d3f4c58870fe"},"cell_type":"code","source":"X_train_ngram = count_vect_ngram.transform(X_train)\nX_test_ngram = count_vect_ngram.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"ca48150ba66d7762e58982d8688059ab8145578c"},"cell_type":"code","source":"classifier = MultinomialNB()\nclassifier.fit(X_train_ngram, y_train)\n\n# Predicting the Test set results\ny_pred = classifier.predict(X_test_ngram)\naccuracy, precision, recall, f1 = get_metrics(y_test, y_pred)\nprint(\"accuracy = %.3f, precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracy, precision, recall, f1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"9119619977718eb86e555da4cea439b6fd4e93b3"},"cell_type":"code","source":"classifier = LogisticRegression(multi_class='multinomial', random_state = 0, n_jobs = -1, solver = 'sag', max_iter=2000)\nclassifier.fit(X_train_ngram, y_train)\n\n# Predicting the Test set results\ny_pred = classifier.predict(X_test_ngram)\naccuracy, precision, recall, f1 = get_metrics(y_test, y_pred)\nprint(\"accuracy = %.3f, precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracy, precision, recall, f1))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a09ac658ccabe25853c78d36126f06b97037cd85"},"cell_type":"markdown","source":"### Count Vectorizer with Unigrams + BiGrams gives Best Results so we will only use this going forward"},{"metadata":{"_uuid":"6571a83dec60ba50fa1cb17c8f7a152dec5a161a"},"cell_type":"markdown","source":"Becoz The Setiment of Tweets have different Distributions based on the airline we will also add Airline Class to feature vector"},{"metadata":{"_uuid":"435c162ab588c6a6a454b7ec99a1db90a7d82aff"},"cell_type":"markdown","source":"Adding Airline Feature using One Hot Encoding"},{"metadata":{"trusted":false,"_uuid":"161c0d76e79f68036808738f129a0e4a2ca0b28c"},"cell_type":"code","source":"new_features = pd.get_dummies(data['airline'],prefix='airline')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"6d1ce7867e18024cb07838d15659349744561afc"},"cell_type":"code","source":"X = data.text\nprint(data.shape)\nprint(new_features.shape)\nprint(type(X))\nprint(type(new_features))\nX = pd.concat([X, new_features], axis = 1)\n#print(X)\nprint(X.shape)\ny = data['sentiment_encoded']\nprint(y.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"695f06658b951478eb62e9c44c516341eef60403"},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, test_size=0.20, shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"12560ec10e9739c05021d522ea2a49743153dba7"},"cell_type":"markdown","source":"Count NGrams"},{"metadata":{"trusted":false,"_uuid":"dcbeec855aac35b015606512c0581556ae147b91"},"cell_type":"code","source":"X_train_ngram = count_vect_ngram.transform(X_train['text'])\nX_test_ngram = count_vect_ngram.transform(X_test['text'])\nX_train_ngram = hstack([X_train_ngram, X_train.iloc[:, 1:]])\nX_test_ngram = hstack([X_test_ngram, X_test.iloc[:, 1:]])\nprint(X_train_ngram.shape)\nprint(X_test_ngram.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"a8da57e2ad0cbc00d0217808f6474732795bc4eb"},"cell_type":"code","source":"classifier = MultinomialNB()\nclassifier.fit(X_train_ngram, y_train)\n\n# Predicting the Test set results\ny_pred = classifier.predict(X_test_ngram)\naccuracy, precision, recall, f1 = get_metrics(y_test, y_pred)\nprint(\"accuracy = %.3f, precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracy, precision, recall, f1))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"90b217b6be41a3a471a3824b3b8280ebd2437092"},"cell_type":"markdown","source":"### One vs rest strategy involves training a single classifier per class, with the samples of that class as positive samples and all other samples as negatives"},{"metadata":{"trusted":false,"_uuid":"48cf5b6398a4328fbd79a0d02f204a10447bc14b"},"cell_type":"code","source":"classifier = OneVsRestClassifier(SVC(random_state = 0, kernel='linear',gamma=0.01, C = 0.1, probability=True))\nclassifier.fit(X_train_ngram, y_train)\n\n# Predicting the Test set results\ny_pred = classifier.predict(X_test_ngram)\naccuracy, precision, recall, f1 = get_metrics(y_test, y_pred)\nprint(\"accuracy = %.3f, precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracy, precision, recall, f1))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f512b2c44aee1b53cfdd287786d8a0e8f95478d9"},"cell_type":"markdown","source":"### Best Result"},{"metadata":{"trusted":false,"_uuid":"00f4a534d5b0fc1f87cbb3540e41a47c2f84acb6"},"cell_type":"code","source":"classifier = OneVsRestClassifier(LogisticRegression(random_state = 0, n_jobs = -1, solver = 'sag', C=1, max_iter= 2000))\nclassifier.fit(X_train_ngram, y_train)\nfinal_classifier_sentiment = classifier\n\n# Predicting the Test set results\ny_pred = classifier.predict(X_test_ngram)\naccuracy, precision, recall, f1 = get_metrics(y_test, y_pred)\nprint(\"accuracy = %.3f, precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracy, precision, recall, f1))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"de9203e55b2c5887ce9c09ee2c01268d174fb8ee"},"cell_type":"markdown","source":"# Objective 2: \n## If tweet is negative specify negative reasons (such as \"late flight\" etc).\n"},{"metadata":{"_uuid":"b8c92623e3d36ae9ccb07643cce1343b1af50811"},"cell_type":"markdown","source":"## Use negative tweets to train a classifier for the reason of the negative tweets"},{"metadata":{"_uuid":"be360e3a8e31f6ef45b23f635da44baaa1989d5a"},"cell_type":"markdown","source":"Feature Selection"},{"metadata":{"trusted":false,"_uuid":"0b56c603f9803a4475e460ea4e23908bcb72fedc"},"cell_type":"code","source":"data[data['sentiment_encoded'] == 0].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"a9f7ef2daf3f57b01d4a3f9aa061e43d46871ba0"},"cell_type":"code","source":"negative_data = data[data['sentiment_encoded'] == 0]\ncols = ['negativereason', 'airline', 'text'] \nnegative_data[cols].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"43c74cfde05cb97b8f49e519f0894618ff9c623d"},"cell_type":"code","source":"negative_data.negativereason.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"0147169ca62f1428eb047ce9d1e13f858249646c"},"cell_type":"code","source":"negative_data.negativereason.value_counts() / negative_data.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1de2376da1732b0251214024f51db86bb0368454"},"cell_type":"markdown","source":"### We will try to classify only the first two cause of negative tweets because the others don‚Äôt have enough amount of data. All the others tweets are classified as others."},{"metadata":{"trusted":false,"_uuid":"4882ed238ee662540fa83aaf7fbf6bca020a36ca"},"cell_type":"code","source":"def fix_reason(text):\n    if text not in ['Customer Service Issue', 'Late Flight']:\n        text = 'Others'\n    return text\n\nnegative_data['negativereason_fixed'] = negative_data.negativereason.map(lambda text : fix_reason(text))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"1d9765926ec6d3a0de2e0488c635ea7a95ead90b"},"cell_type":"code","source":"cols = ['negativereason', 'negativereason_fixed', 'airline', 'text']\nnegative_data[cols].tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"e60839a23e11ba5d8a3b47021f1c0ea1e9f4312a"},"cell_type":"code","source":"negative_data.negativereason_fixed.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"ab7e26bf1b665f0ff2803ec3eb129216f32a024f"},"cell_type":"code","source":"negative_data.negativereason_fixed.value_counts() / negative_data.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fb4c616dc3c991a85f948ef53f9febce8e0eb276"},"cell_type":"markdown","source":"#### Using Label Encoding to convert negativereason categorical feature to numeric form"},{"metadata":{"trusted":false,"_uuid":"0306f16fae584b9bdb2c4afd8d79e3e5f680244a"},"cell_type":"code","source":"negative_data['negativereason_fixed_encoded'] = lb.fit_transform(negative_data['negativereason_fixed'])\nnegative_data[['negativereason_fixed', 'negativereason_fixed_encoded']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"9011e91b09e3c016ba870a5494ebe4f5b70a77cf"},"cell_type":"code","source":"new_features = pd.get_dummies(negative_data['airline'],prefix='airline')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ebc8bdc4e0fcd284e30278a3db6fa6a97935ba7d"},"cell_type":"markdown","source":"Count NGrams"},{"metadata":{"trusted":false,"_uuid":"243e3ec6db93328c27040c7b3b36fade4d40270e"},"cell_type":"code","source":"X = negative_data.text\nprint(negative_data.shape)\nprint(new_features.shape)\nprint(type(X))\nprint(type(new_features))\nX = pd.concat([X, new_features], axis = 1)\n#print(X)\nprint(X.shape)\ny = negative_data['negativereason_fixed_encoded']\nprint(y.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"b32909c7efcb6c4069aa39b9d3d338f0e983c0f3"},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, test_size=0.20, shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"1d76233b9bbc84596bb5fdaf08536dc807067b05"},"cell_type":"code","source":"X_train_ngram = count_vect_ngram.transform(X_train['text'])\nX_test_ngram = count_vect_ngram.transform(X_test['text'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"2ee128ae77e27db83eb0da4c2a422df3f3984c1d"},"cell_type":"code","source":"X_train_ngram = hstack([X_train_ngram, X_train.iloc[:, 1:]])\nX_test_ngram = hstack([X_test_ngram, X_test.iloc[:, 1:]])\nprint(X_train_ngram.shape)\nprint(X_test_ngram.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"5f2144b75b66f50157317ffde7a2abd7f5a7488f"},"cell_type":"code","source":"classifier = MultinomialNB()\nclassifier.fit(X_train_ngram, y_train)\n\n# Predicting the Test set results\ny_pred = classifier.predict(X_test_ngram)\naccuracy, precision, recall, f1 = get_metrics(y_test, y_pred)\nprint(\"accuracy = %.3f, precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracy, precision, recall, f1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"638dafd7baa5748a2c2f3014df50d985a5784237"},"cell_type":"code","source":"classifier = OneVsRestClassifier(SVC(random_state = 0, kernel='linear',gamma=0.01, C = 0.1, probability=True))\nclassifier.fit(X_train_ngram, y_train)\n\n# Predicting the Test set results\ny_pred = classifier.predict(X_test_ngram)\naccuracy, precision, recall, f1 = get_metrics(y_test, y_pred)\nprint(\"accuracy = %.3f, precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracy, precision, recall, f1))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8efb3c8580981542670fd37222f69bf190a9d73c"},"cell_type":"markdown","source":"### Best Result"},{"metadata":{"trusted":false,"_uuid":"8e3c56e0b6b02ee165852fe3e24a19372fc83637"},"cell_type":"code","source":"classifier = OneVsRestClassifier(LogisticRegression(random_state = 0, n_jobs = -1, solver = 'sag', C=1, max_iter= 2000))\nclassifier.fit(X_train_ngram, y_train)\nfinal_classifier_negativereason = classifier\n\n# Predicting the Test set results\ny_pred = classifier.predict(X_test_ngram)\naccuracy, precision, recall, f1 = get_metrics(y_test, y_pred)\nprint(\"accuracy = %.3f, precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracy, precision, recall, f1))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{},"version_major":2,"version_minor":0}}},"nbformat":4,"nbformat_minor":1}