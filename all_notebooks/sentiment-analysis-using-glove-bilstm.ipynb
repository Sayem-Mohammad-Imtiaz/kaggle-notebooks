{"cells":[{"metadata":{},"cell_type":"markdown","source":"Here, I have predicted the sentiment score of the Title and the Headline of the news articles.\n\nThe target columns are:\n\n- `SentimentTitle`, which is the sentiment score of the Title\n- `SentimentHeadline`, which is the sentiment score of the Headline\n\nI have used `GloVe Embeddings` for the words and created a `BiLSTM` Network to predict the sentiment"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scipy.stats as stats\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, precision_recall_curve\n\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom spacy.lang.en import English\nfrom spacy.lang.en.stop_words import STOP_WORDS\nlemma = WordNetLemmatizer()\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Bidirectional, LSTM, Dropout, BatchNormalization\nfrom keras.layers.embeddings import Embedding","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nltk.download('stopwords')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embeddings_index = dict()\nf = open('../input/glove-global-vectors-for-word-representation/glove.6B.50d.txt')\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/news-popularity-in-multiple-social-media-platforms/train_file.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_val = pd.DataFrame(train.isnull().sum())\nmissing_val = missing_val.reset_index()\nmissing_val","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[train['Source'].isna()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.dropna(inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.describe().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['Topic'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### EDA & Data Visualization\n\n**NOTE:** I used the same EDA as in my other notebook where I used Custom Transformers in scikit-learn"},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\nstopwords = nltk.corpus.stopwords.words('english')\nstopwords.extend(['Palestinian','Palestine','Microsoft','Economy','Obama','Barack'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud\nplt.figure(figsize=(12,6))\ntext = ' '.join(train.Title[train['Topic']=='economy'])\nwc = WordCloud(background_color='white',stopwords=stopwords).generate(text)\nplt.imshow(wc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud\nplt.figure(figsize=(12,6))\ntext = ' '.join(train.Title[train['Topic']=='obama'])\nwc = WordCloud(background_color='white',stopwords=stopwords).generate(text)\nplt.imshow(wc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud\nplt.figure(figsize=(12,6))\ntext = ' '.join(train.Title[train['Topic']=='microsoft'])\nwc = WordCloud(background_color='white',stopwords=stopwords).generate(text)\nplt.imshow(wc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud\nplt.figure(figsize=(12,6))\ntext = ' '.join(train.Title[train['Topic']=='palestine'])\nwc = WordCloud(background_color='white',stopwords=stopwords).generate(text)\nplt.imshow(wc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(style='darkgrid',palette='Set1')\n_ = sns.jointplot(x='SentimentTitle',y='SentimentHeadline',data=train,kind = 'reg')\n_.annotate(stats.pearsonr)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Bar graph exploring total sentiment for the different topics\n\ntrain.groupby('Topic').agg('sum')[['SentimentHeadline', 'SentimentTitle']].plot(kind='bar', figsize=(25, 7),\n                                                          stacked=True, color=['b', 'r', 'g']);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,15))\n_ = sns.heatmap(train[['Facebook','GooglePlus','LinkedIn','SentimentTitle','SentimentHeadline']].corr(), square=True, cmap='Blues',linewidths=0.5,linecolor='w',annot=True)\nplt.title('Correlation matrix ')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Separating Title and headline, so that they can be trained separately"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_title = train.loc[:,'Title'].values\ny_train_title = train.loc[:,['SentimentTitle']].values\n\nX_train_headline = train.loc[:,'Headline'].values\ny_train_headline = train.loc[:,['SentimentHeadline']].values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Creating separate dataframes for Title and Headline"},{"metadata":{"trusted":true},"cell_type":"code","source":"title_df=pd.DataFrame()\ntitle_df['X_train_title']=X_train_title\ntitle_df['y_train_title']=y_train_title\n\nheadline_df=pd.DataFrame()\nheadline_df['X_train_headline']=X_train_headline\nheadline_df['y_train_headline']=y_train_headline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Text preprocessing function"},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.corpus import stopwords\ndef preprocess_text(texts):\n    texts = texts.lower() \n    texts = re.sub(r'[^\\x00-\\x7F]+',' ', texts) \n    splitwords = texts.split()\n    splitwords = filter(lambda x: x[0]!= '@' , texts.split()) \n    splitwords = [word for word in splitwords if word not in set(stopwords.words('english'))] \n    texts = \" \".join(splitwords)\n    return texts","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Applying preprocessor function to the title and headline text"},{"metadata":{"trusted":true},"cell_type":"code","source":"title_df['X_train_title'] = title_df.X_train_title.apply(preprocess_text)\ndisplay(title_df.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"headline_df['X_train_headline'] = headline_df.X_train_headline.apply(preprocess_text)\ndisplay(headline_df.head())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Using GloVe to create word embeddings for our Title and Headline columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating Embeddings for the titles\n\nmax_len_title = title_df.X_train_title.apply(lambda x: len(x.split())).max()\n\ntok_title = Tokenizer()\ntok_title.fit_on_texts(title_df.X_train_title)\nvocab_size_title = len(tok_title.word_index) + 1\nencoded_title = tok_title.texts_to_sequences(title_df.X_train_title)\npadded_title = pad_sequences(encoded_title, maxlen=max_len_title, padding='post')\n\nvocab_size_title = len(tok_title.word_index) + 1\n\ntitle_embedding_matrix = np.zeros((vocab_size_title, 50))\nfor word, i in tok_title.word_index.items():\n    t_embedding_vector = embeddings_index.get(word)\n    if t_embedding_vector is not None:\n        title_embedding_matrix[i] = t_embedding_vector","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating Embeddings for the Headlines\n\nmax_len_headline = headline_df.X_train_headline.apply(lambda x: len(x.split())).max()\n\ntok_headline = Tokenizer()\ntok_headline.fit_on_texts(headline_df.X_train_headline)\nvocab_size_headline = len(tok_headline.word_index) + 1\nencoded_headline = tok_headline.texts_to_sequences(headline_df.X_train_headline)\npadded_headline = pad_sequences(encoded_headline, maxlen=max_len_headline, padding='post')\n\nvocab_size_headline = len(tok_headline.word_index) + 1\n\nheadline_embedding_matrix = np.zeros((vocab_size_headline, 50))\nfor word, i in tok_headline.word_index.items():\n    h_embedding_vector = embeddings_index.get(word)\n    if h_embedding_vector is not None:\n        headline_embedding_matrix[i] = h_embedding_vector","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Creating training and testing sets from our data for both title and headline respectively. I have used 15% of the data for testing"},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train_title, x_valid_title, Y_train_title, y_valid_title = train_test_split(padded_title, y_train_title, shuffle = True, test_size = 0.15)\n\nx_train_headline, x_valid_headline, Y_train_headline, y_valid_headline = train_test_split(padded_headline, y_train_headline, shuffle = True, test_size = 0.15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import math\nfrom math import exp\nfrom keras import backend as K","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Defining a custom activation function by changing the pre-existing tanh parameter"},{"metadata":{"trusted":true},"cell_type":"code","source":"def mod_tanh(x):\n    return K.tanh(0.6*x)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Defining separate LSTM Networks for Title and Headline\n\n**Some key novelties in the network:**\n- The loss function used for the network is `mean squared error`, the reason being that the output was required to be continuous\n- The activation function used in the last layer of the network was a custom `tanh` function defined above, because the outputs were required in the range of [-1, 1]"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Model for title\ntitle_model = Sequential()\ntitle_model.add(Embedding(vocab_size_title, 50, input_length=max_len_title, weights=[title_embedding_matrix], trainable=True))\ntitle_model.add(Bidirectional(LSTM(20, return_sequences=True)))\ntitle_model.add(Dropout(0.3))\ntitle_model.add(BatchNormalization())\ntitle_model.add(Bidirectional(LSTM(20, return_sequences=True)))\ntitle_model.add(Dropout(0.3))\ntitle_model.add(BatchNormalization())\ntitle_model.add(Bidirectional(LSTM(20)))\ntitle_model.add(Dropout(0.3))\ntitle_model.add(BatchNormalization())\ntitle_model.add(Dense(64, activation='relu'))\ntitle_model.add(Dense(64, activation='relu'))\ntitle_model.add(Dense(1, activation=mod_tanh))\ntitle_model.compile(loss='mse', optimizer='adam', metrics=['mse', 'mae'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Model for Headline\nheadline_model = Sequential()\nheadline_model.add(Embedding(vocab_size_headline, 50, input_length=max_len_headline, weights=[headline_embedding_matrix], trainable=True))\nheadline_model.add(Bidirectional(LSTM(20, return_sequences=True)))\nheadline_model.add(Dropout(0.3))\nheadline_model.add(BatchNormalization())\nheadline_model.add(Bidirectional(LSTM(20, return_sequences=True)))\nheadline_model.add(Dropout(0.3))\nheadline_model.add(BatchNormalization())\nheadline_model.add(Bidirectional(LSTM(20)))\nheadline_model.add(Dropout(0.3))\nheadline_model.add(BatchNormalization())\nheadline_model.add(Dense(64, activation='relu'))\nheadline_model.add(Dense(64, activation='relu'))\nheadline_model.add(Dense(1, activation=mod_tanh))\nheadline_model.compile(loss='mse', optimizer='adam', metrics=['mse', 'mae'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Title model training"},{"metadata":{"trusted":true},"cell_type":"code","source":"title_model.fit(x_train_title, Y_train_title, epochs = 10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Headline model training"},{"metadata":{"trusted":true},"cell_type":"code","source":"headline_model.fit(x_train_headline, Y_train_headline, epochs = 10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Now we shall predict on the validation sets and then see what score we obtain"},{"metadata":{"trusted":true},"cell_type":"code","source":"title_valid_pred = title_model.predict(x_valid_title)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"headline_valid_pred = headline_model.predict(x_valid_headline)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Calculating the Mean Absolute errors for both Title and Headline sentiments"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_absolute_error\nmae_title=mean_absolute_error(y_valid_title,title_valid_pred)\nmae_headline=mean_absolute_error(y_valid_headline,headline_valid_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Here we caclulate our final score. Score is calulated as\n\n**max(0, 1 - ((0.4(mean abs error of title)+(0.6(mean abs error of headline)))**"},{"metadata":{"trusted":true},"cell_type":"code","source":"score=1-((0.4*mae_title)+(0.6*mae_headline))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Score = {} \\nScore(out of 100%) = {}%\".format(score,round(score*100, 2)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### We achieved a score of 93.15%\n\n### This score is an indication of how close our predicted values were to the target values. It cannot exacly be termed as accurcacy, because this is not a classification problem. Our sentiment score is a real number between -1 and 1"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}