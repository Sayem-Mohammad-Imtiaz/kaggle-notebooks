{"cells":[{"metadata":{},"cell_type":"markdown","source":"# <center> <font color='#AD3D6F'> SPEED DATING EXPERIMENT  </font> "},{"metadata":{},"cell_type":"markdown","source":"# # <center> <font color='#E17327'> Python : your brand new wingman ! </center>\n*Wingman : a friend who supports you when trying to meet or talk to possible romantic partners.*  "},{"metadata":{},"cell_type":"markdown","source":"## <font color='darkblue'> 0. Kaggle link to dataset</font>\n"},{"metadata":{},"cell_type":"markdown","source":"https://www.kaggle.com/annavictoria/speed-dating-experiment"},{"metadata":{},"cell_type":"markdown","source":"## <font color='darkblue'> 1. Context </font>\n"},{"metadata":{},"cell_type":"markdown","source":"I have always been intrigued by **dating**. As the use of dating apps is booming, I feel like technology has significantly impacted our dating habits and our expectations, but also somehow debunked the myth of the prince charming and the sleeping beauty. At the era of globalization and with the power of social media, people are hyper-connected, and therefore lonelier than ever. \n\nBut in a logic of time optimization, wouldn’t it be great to only meet people that you are more likely to match with? It would avoid deceptions, broken hearts and mostly, enable people to focus on real potential romantic partners. \n \n <font color='darkblue'> **What is the secret of finding love at first sight ?** <br> \n    **Can an algorithm provoke fate?**</font> \n    \nThis dataset was conducted by Columbia University in order to investigate dating preferences. It gathers data from participants in experimental speed dating events from 2002 to 2004. During these events, participants went on 4-minute dates with partners of the opposite sex (*Pretty oldschool data set...*). After this first date, participants had to decide whether they wanted to go on a second date, and to rate their date on six attributes : Attractiveness, Sincerity, Intelligence, Fun, Ambition, and Shared Interests.\n\nTo make this study even more interesting, the dataset also includes questionnaire data gathered before, during and after the dating events. Very important information was gathered on people's self-perception of their qualities and what they are looking for in the perfect romantic partner. Demographics, dating habits and lifestyle information will be necessary for us to debunk this myth of finding the perfect soulmate.\n"},{"metadata":{},"cell_type":"markdown","source":"## <font color='darkblue'> 2. Problem definition </font>\n"},{"metadata":{},"cell_type":"markdown","source":"The goal of this study is to crack the case of Love. Love has always been full of mysteries, but I am convinced that there is some logic behind it. As this data set is huge, I decided to start this study by answering some general questions to understand the experiment and get ahold of the dataset. These questions are: \n\n-\tHow serious were people about this experiment?  \n-\tHow picky are they ? \n-\tDid people find their soulmate ? \n-\tWere hearts broken during this experiment ? \n-\tPrince charming or just the master of speed dating ? \n-\tWhat makes a man/woman THE perfect date ? \n-\tDo we under or over estimate ourselves ?     \n\nAfter answering all these questions, I will tackle the main problem of this study:\n\n**Predicting if two people are going to be a match after a 4-minutes date regarding their personal features and what they are looking for in a romantic partner.**\n\nThis calls for a binary classification algorithm, that given all the features on 2 participants, can predict if they would match or not.   \nThis algorithm can be very useful to optimize those speed dating events. As I explained it earlier, people are a lot more involved in finding love today and way more afraid to end up alone. This feeling pushes people to try harder to find love and to participate even more to speed dating events. But instead of making all the participants meet one another, wouldn’t it be better to assemble only people with the highest probability of matching? Not only would it be a considerable gain of time, it will also make “Love” easier by avoiding disastrous dates and focusing on people who are relationship material.       \n"},{"metadata":{},"cell_type":"markdown","source":"## <font color='darkblue'> 3. Data exploration </font>\n"},{"metadata":{},"cell_type":"markdown","source":"### <font color=\"#3268ca\"> 3.1) Importing the dataset </font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# To support both python 2 and python 3\nfrom __future__ import division, print_function, unicode_literals\n\n# Common imports\nimport numpy as np\nimport os\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport sklearn.linear_model\n\n# to make this notebook's output stable across runs\nnp.random.seed(42)\n\n# To plot pretty figures\n%matplotlib inline\nimport matplotlib\nimport matplotlib.pyplot as plt\nplt.rcParams['axes.labelsize'] = 14\nplt.rcParams['xtick.labelsize'] = 12\nplt.rcParams['ytick.labelsize'] = 12","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Speed_dating_data = pd.read_csv('../input/speed-dating-experiment/Speed Dating Data.csv',encoding=\"ISO-8859-1\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <font color=\"#3268ca\"> 3.2) First steps of the exploration  "},{"metadata":{"trusted":true},"cell_type":"code","source":"Speed_dating_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Speed_dating_data.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are 195 features ! This number is huge!! We will have to do an important work on features selection.   "},{"metadata":{"trusted":true},"cell_type":"code","source":"Speed_dating_data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Looking for empty cells \nSpeed_dating_data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see, many features have a lot of missing values. Depending on the features chosen, we will have to make sure that the data is cleaned. We can't delete all the missing values now, because cleaning it for all the 195 features deletes all the dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Were there the same amount of male and female participants ?\nSpeed_dating_data.groupby(['gender'])['iid'].count().reset_index()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that there is the same amount of women and men in this experiment. This enables us to signifiantly compare men and women's dating behaviours. In fact, this dataset is balanced.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"#The age of the participants \nint_corr = Speed_dating_data[np.isfinite(Speed_dating_data['age'])]['age']\nplt.hist(int_corr.values, color='#900C3F')\nplt.xlabel('Age')\nplt.ylabel('Participants')\nplt.title('The age of the participants')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Participants are relatively **young**, aged between 20 and 30 for the majority. "},{"metadata":{},"cell_type":"markdown","source":"### <font color=\"#3268ca\"> 3.3) General questions on the experiment "},{"metadata":{},"cell_type":"markdown","source":" #### <font color='#900C3F'> 3.3.1) How serious were people about this experiment ?"},{"metadata":{},"cell_type":"markdown","source":"Participants were asked about their goal in participating in this speed dating sessions. They had to choose between 6 possibilities:  \n\n<font color='grey'> What is your primary goal in participating in this event? \n- Seemed like a fun night out = 1\n- To meet new people = 2\n- To get a date = 3\n- Looking for a serious relationship = 4\n- To say I did it = 5\n- Other = 6\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Encode the answers to the question above\nreplace_map = {'goal': {1: \"Seemed like a fun night out\", \n                        2: \"To meet new people\", \n                        3: \"To get a date\" , \n                        4: \"Looking for a serious relationship\" ,\n                        5: \"To say I did it\" ,\n                        6: \"Other\" }}\nGoal=Speed_dating_data.replace(replace_map)\nGoal['goal'].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"intentions=Goal.groupby(['goal'])['iid'].count().reset_index()\nintentions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (20,5))\n\nplt.bar(Goal.groupby(['goal'])['iid'].count().reset_index()['goal'], \n        Goal.groupby(['goal'])['iid'].count().reset_index()['iid'], color='#701C3F',\n        width= 0.3, align='center')\nplt.title(\"The primary goal in participating in this event\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that people take part in these events mostly because it seemed like a fun night out and in order to meet new people. Only a few of them were there loking for a serious relastionship or to get a date. This shows people willigness to have a good time and to get to know people, but the sincerety of some of them might me questionable.  "},{"metadata":{},"cell_type":"markdown","source":"#### <font color='#900C3F'> 3.3.2) How picky are we ? "},{"metadata":{},"cell_type":"markdown","source":"The feature \"dec\" gives the decision of the participant after the 4 minutes date. If both participants said yes, a match is recorded.  \n\n*Reminder : Yes --> 1 and No -->0* \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Encode the answers to this question \nreplace_map2 = {'dec': {1: \"Yes\", 0: \"No\"}}\nDecision=Speed_dating_data.replace(replace_map2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Decision.groupby(['gender','dec'])['iid'].count().reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (20,5))\n\nplt.subplot(131)\n\n\nplt.bar(Decision[Decision['gender']==0].groupby(['dec'])['iid'].count().reset_index()['dec'], \n        Decision[Decision['gender']==0].groupby(['dec'])['iid'].count().reset_index()['iid'], color=\"#E75480\",\n        width= 0.1, align='center')\nplt.title(\"Decision of women after their dates with men\")\nplt.subplot(132)\n\nplt.bar(Decision[Decision['gender']==1].groupby(['dec'])['iid'].count().reset_index()['dec'],\n        Decision[Decision['gender']==1].groupby(['dec'])['iid'].count().reset_index()['iid'], color=\"darkblue\",\n               width= 0.1, align='center')\nplt.title(\"Decision of men after dating women\")\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\nWe can see that men seem to be reasonably picky. *They said yes to 47% of the women they met.* <br> \nOn the opposite side, women are way more picky. *They said yes to 36% of the men they met.* \n\n**Men tend to be more easily rejected than women after a 4-minuntes date.**"},{"metadata":{},"cell_type":"markdown","source":"#### <font color='#900C3F'> 3.3.3) Did people find their soulmate ? \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (10,5))\n\nplt.bar(Speed_dating_data[Speed_dating_data['date_3']==1].groupby(['numdat_3'])['iid'].count().reset_index()['numdat_3'],\n        Speed_dating_data[Speed_dating_data['date_3']==1].groupby(['numdat_3'])['iid'].count().reset_index()['iid'], color='#900C3F',\n               width= 0.8, align='center')\nplt.title('Number of dates participants went on after recontacting their matches')\n\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"People who received matches and recontacted the people they met, generally went on 1 date to confirm their interest in one another. This shows that many prople might of have found their soulmate amoung the match *- A soulmate, or at least a second date.*    "},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"#Encode the answers to this question \nreplace_map3 = {'match': {0: \"No Match\", 1: \"Match\"}}\nmatches=Speed_dating_data.replace(replace_map3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Counting how many matches were recorded \nmatches.groupby(['match'])['iid'].count().reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plotting the number of matches recorded \n\nplt.bar(matches.groupby(['match'])['iid'].count().reset_index()['match'], \n        matches.groupby(['match'])['iid'].count().reset_index()['iid'], color='#700C3F', width=0.2 )\nplt.title(\"Recorded matches during this experiment\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Based on all the records that we have, only **16% of the dates resulted in \na match. <br>**\nThis shows how hard it can be to find an optimal romantic partner and proves the utility of the oncoming classification algorithm. In fact, by meeting everyone in a speed dating event, people will go on an average of 74% useless dates.      <br>\n\nThus,this algorithm is not balanced regarding matches. That means that our model after will be more trained to classify \"No matches\" than \"mathes\". "},{"metadata":{},"cell_type":"markdown","source":"#### <font color='#900C3F'> 3.3.4) Were hearts broken during this experiment ? \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Generate the number of people who wanted to go on a second date, and the decision of their partners.  \nSpeed_dating_data[Speed_dating_data['dec']==1].groupby(['match'])['iid'].count().reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Encode the answers to this question \nreplace_map4 = {'match': {0: \"Broken hearts\", 1: \"Love at first sight\"}}\nhearts=Speed_dating_data.replace(replace_map4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plotting these results \nplt.bar(hearts[hearts['dec']==1].groupby(['match'])['iid'].count().reset_index()['match'], \n        hearts[hearts['dec']==1].groupby(['match'])['iid'].count().reset_index()['iid'], color='#600C3F', width=0.2 )\nplt.title(\"Recorded matches during this experiment\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see it, many people felt a one sided connection with their date, that apparently was not mutual. 61% of the dates ended-up \"breaking\" someone's hearts.  <BR>\n   It could be interesting to manage the participant's feeling by protecting them from feeling rejected and not appreciated. To avoid such disillusions, the use of an algorithm that would predict the matches can be very useful."},{"metadata":{},"cell_type":"markdown","source":" #### <font color='#900C3F'> 3.3.5) Prince charming or just the master of speed dating ? "},{"metadata":{},"cell_type":"markdown","source":"Let's count how many matches each participant has, to see if some people are just **good daters**. <br>\n\n*Dating is just like sports: you become better with training and some people are more gifted.* "},{"metadata":{"trusted":true},"cell_type":"code","source":"data1=Speed_dating_data.groupby(['iid'])['match'].sum().reset_index()\ndata1.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that the mean is 2 and that  25% of the participants didn't have any date and 25% of them got between 4 and 14 dates. This shows that the participants are really heterogenous, and the we have both unexperimented and expert daters.      "},{"metadata":{},"cell_type":"markdown","source":"#### <font color='#900C3F'> 3.3.6) What features make you want to go on a second date ? "},{"metadata":{},"cell_type":"markdown","source":"Let's first take a look at the average evaluation for each feature, depending if there was a match or not. This will show us the important features and the ones that drive the participant's decision.   "},{"metadata":{"trusted":true},"cell_type":"code","source":"#Selection the features I am interested in : The way the participants percieved their partner after the 4-minute date. \nDATA3=Speed_dating_data[['iid','gender','match','dec','attr','sinc','intel','fun','amb','shar','like']].drop_duplicates().reset_index()\nreplace_map5 = {'match': { 1: \"Match\", 0:\"No match\" }}\nDATA3.replace(replace_map5, inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Features=DATA3.groupby(['match'])['attr','sinc','intel','fun','amb','shar'].mean()\nFeatures.rename(columns={'attr': 'Attractive', 'sinc': 'Sincere','intel': 'Intelligent','fun': 'Fun','amb': 'Ambitious','shar': 'Shared interests'}, inplace=True)\nFeatures","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating a row with the difference between Match and No match\nlist_cols = Features.columns\nFeatures2 = pd.DataFrame({\"match\":['Match','No match','Difference (Match - No match)']})\n\nfor col in list_cols:\n    \n    col_to_list = list(Features[col].unique())\n    diff = col_to_list[0]-col_to_list[1]\n    col_to_list.append(diff)\n    \n    Features2[col] = col_to_list","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Features2.set_index('match',inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Features2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plot these results\nFeatures2.plot(kind='bar', figsize=(10,3))\nplt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\nplt.title('The features of participants who were matched or rejected')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As these numbers show it, participants wanted to go on a second date with people who were **more attracitve**, **funnier**, and who **shared more interests**. Sincerity, ambition and intelligence don't seem to influence one's decision to go on a second date. <BR>\nIsn't it good news that people are not judged only on their attractiveness ? The importance of Fun and shared interests shows that participants are really looking for genuine connections. \n    "},{"metadata":{},"cell_type":"markdown","source":"#### <font color='#900C3F'> 3.3.7) Do we *under* or *over* estimate ouverselves ? \n"},{"metadata":{},"cell_type":"markdown","source":"The features I will be using to answer this question are :\n\n- **\"Please rate your opinion of your own attributes**, on a scale of 1-10 (1=awful, 10=great) -- Be honest!\"<br>\n*The answers are encoded as : attr3_s, sinc3_s , intel3_s, fun3_s and amb3_s*\n    \n- **\"The rating by partner the night of the event**, for all 6 attributes, on a scale of 1-10 (1=awful, 10=great).\"<br> \n*The answers are encoded as : attr_o; sinc_o, intel_o, fun_o and amb_o*\n\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"Reality=Speed_dating_data.groupby(['gender'])['attr3_s','sinc3_s','intel3_s','fun3_s','amb3_s','attr_o','sinc_o','intel_o','fun_o','amb_o'].mean()\nReality\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating an 'all' column that is always =1, to code a groupby\nSpeed_dating_data['all']  = 0\nReality2=Speed_dating_data.groupby(['all'])['attr3_s','sinc3_s','intel3_s','fun3_s','amb3_s',\n                                            'attr_o','sinc_o','intel_o','fun_o','amb_o'].mean()\nReality2\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_diff = pd.DataFrame()\n\nlist_cols = Reality2.columns\nnew_col_names = ['Attractive','Sincere','Intelligent','Fun','Ambitious']\nlist_values = []\nfor i in range(5):\n    calc = float(Reality2[list_cols[i]]-Reality2[list_cols[i+5]])\n    list_values.append(calc)\n    df_diff[new_col_names[i]] = [float(Reality2[list_cols[i]]),\n                                 float(Reality2[list_cols[i+5]]),\n                                 calc]\n    \ndf_diff['Different perceptions'] = ['Personal rating','Others rating','Difference']\ndf_diff.set_index('Different perceptions',inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_diff","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plotting these results\n\ndf_diff.plot(kind='bar', figsize=(15,5))\nplt.title('Is our self-perception distorted ? ', fontsize=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that the difference between the personal rating and others' rating is always positive, which means participants tend to **over-estimate** themselves. <br>\nThe feature they over-estimate the most is **fun**. This maks sense because fun is very subjective. Everyone is fun in its own way. The second attribute people over-estimate is **attractiveness**.  \nMaybe people don't over-estimate themselves but they just underestimate people they meEt. In fact, it must be hard to circle someone in only 4 minutes.  Not to mention the stress factor due to the short time, people must not be at their fullest potential.  "},{"metadata":{},"cell_type":"markdown","source":"### <font color=\"#3268ca\"> 3.3) Looking for correlations"},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nsns.pairplot(DATA3)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that the most liked people tend to be very **attractive** and to **share many common interests** with the participant. "},{"metadata":{},"cell_type":"markdown","source":"## <font color='darkblue'> **4. Features Selection** </font>\n"},{"metadata":{},"cell_type":"markdown","source":"The goal of this study is to **Predict if two people are going to be a match after a 4-minutes date regarding their personal features.**\n\nThe features that will be needed for this study are : \n- The gender and iid\n- Personal features on a scale of 1-10 (1=awful, 10=great) *--> attr3_s, sinc3_s, intel3_s, fun3_s, amb3_s*\n- The other person's features on a scale of 1-10 (1=awful, 10=great) *--> attr, sinc, intel, fun, amb* \n- Match\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"dating_data=Speed_dating_data[['gender','iid',\n                     'attr3_s','sinc3_s','intel3_s','fun3_s','amb3_s',\n                     'attr','sinc','intel','fun','amb','shar','match']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <font color='darkblue'> 5.Data Processing Step 1 (Cleaning, etc.)"},{"metadata":{"trusted":true},"cell_type":"code","source":"dating_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dating_data.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <font color=\"#3268ca\"> 5.1) Looking for non available values"},{"metadata":{"trusted":true},"cell_type":"code","source":"dating_data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are a lot of missing values !    "},{"metadata":{"trusted":true},"cell_type":"code","source":"dating_data.isnull().sum().max()/dating_data.shape[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"52% of the dataset contains non available values. \nWe have several options de tackle this issue, among them : \n- Deleting rows with empty values \n- Replace the empty values with the mean \n\nI think that I will delete the empty rows because replacing the missing values with the mean would totally biase our algorithm. I want the algorithm to learn from real evaluations and not only to be mainstream individuals.      "},{"metadata":{"trusted":true},"cell_type":"code","source":"#deleting non available values\ndating_data_clean=dating_data.dropna().reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dating_data_clean.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <font color=\"#3268ca\"> 5.2) Taking care of categorical features"},{"metadata":{"trusted":true},"cell_type":"code","source":"dating_data_clean.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Luckily, in this dataset the gender has already been encoded as an integrer. Therefore, there are no categorical features to take care of.     "},{"metadata":{},"cell_type":"markdown","source":"### <font color=\"#3268ca\"> 5.3) Drop duplicates"},{"metadata":{"trusted":true},"cell_type":"code","source":"dating_data_clean_without_duplicates = dating_data_clean.drop_duplicates()\nprint(dating_data_clean.shape)\nprint(dating_data_clean_without_duplicates.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are no duplicates ! "},{"metadata":{},"cell_type":"markdown","source":"## <font color='darkblue'> 5. Data Processing Step 2 :  Features Engineering</font>\n"},{"metadata":{},"cell_type":"markdown","source":"### <font color=\"#3268ca\"> 5.1) Low variance features"},{"metadata":{},"cell_type":"markdown","source":"Search for features that have low variance"},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_variances = dating_data_clean.std().sort_values(ascending=True)\nfeatures_low_variance = feature_variances[feature_variances < 0.1].index.values.tolist()\nfeatures_low_variance","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"None of the features have low variance. This means we can keep all of them, and that all of them will have an impact on our model.     "},{"metadata":{},"cell_type":"markdown","source":"### <font color=\"#3268ca\"> 5.2) Correlation study"},{"metadata":{},"cell_type":"markdown","source":"Let's take a look at which features have the less correlation with the final result : the match. "},{"metadata":{"trusted":true},"cell_type":"code","source":"correlations = dating_data_clean.corr().abs().unstack().sort_values(ascending=False).drop_duplicates()\ncorrelations = correlations[correlations != 1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"match_correlations = correlations.loc['match']\nmatch_correlations[match_correlations > 0.1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lowest_correlation = match_correlations[match_correlations < 0.1].axes[0].to_list()\nlowest_correlation","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see, sincerity and intelligence of the participant is not very important for the decision. Gender does not have a significative correlation with the matching result. I find this surprising, as of all the results plotted before showed that it was harder for men to be liked and therefore to have a match.     \n\nIn order to be able to compare both individuals features, I decide to keep sincerity and intelligence of one of the participants among the features because the sincerety and the intelligence of the other is significantly correlated to the matching result.     "},{"metadata":{},"cell_type":"markdown","source":"## <font color='darkblue'> **6. Model Selection** </font>\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating the training and the testing sets\nfrom sklearn.model_selection import train_test_split\nX=dating_data_clean[['gender',\n                     'attr3_s','sinc3_s','intel3_s','fun3_s','amb3_s',\n                     'attr','sinc','intel','fun','amb','shar']]\ny=dating_data_clean['match']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=3, stratify=y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Due to the small dataset we are working with (3373 rows), the classification models are really unstable. Therefore, results vary a lot each time I run the code. \n\nI will run two classifiers for this problem: \n- A logistic Regression\n- A random Forest Classifier"},{"metadata":{},"cell_type":"markdown","source":"### <font color='#FF781F'> **6.1) Logistic Regression** "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nfrom sklearn.metrics import accuracy_score\nimport time\n\nt0 = time.time()\n\nmodel_log_reg = LogisticRegression(C=3, random_state=43)\nlog_reg = model_log_reg.fit(X_train, y_train)\npredict_train_log_reg = log_reg.predict(X_train)\npredict_test_log_reg = log_reg.predict(X_test)\nt1 = time.time()\nduration = t1 - t0\nLog_reg_before=duration\nLog_reg_score_before=accuracy_score(y_test, predict_test_log_reg)\nprint(\"The duration of the training is \" , duration)\nprint('Training Accuracy Score:', accuracy_score(y_train, predict_train_log_reg))\nprint('Validation Accuracy Score :', accuracy_score(y_test, predict_test_log_reg))\n\n#Saving these values to compare before and after the dimentionality reduction \nLog_reg_before=duration\nLog_reg_score_before=accuracy_score(y_test, predict_test_log_reg)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This model is very efficient ! I has a similar accuracy score for the training and testing set which means it avoids overfitting. These scores are pretty high wich means our model avoids also underfitting.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plotting the confusion matrix: \nfrom sklearn.metrics import confusion_matrix\n\nconfusion_matrix(y_test, model_log_reg.predict(X_test))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see, 81 values were predicted negative while they were actually positive. This is a little bit worrying because I don't want my algorithm to make a participant miss it's soulmate. \nAlso, only 4 values were correctly predicted as matches, which confirms the fact that our dataset is not balances regarding the matching results. Since we have a lot more \"Non matches\", the algorithm is better as predecting what is not a match than what is a match.   "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report\nprint(classification_report(y_test, model_log_reg.predict(X_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\nlogit_roc_auc = roc_auc_score(y_test, model_log_reg.predict(X_test))\nfpr, tpr, thresholds = roc_curve(y_test, model_log_reg.predict_proba(X_test)[:,1])\nplt.figure()\nplt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.legend(loc=\"lower right\")\nplt.savefig('Log_ROC')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"ROC curves typically feature true positive rate on the Y axis, and false positive rate on the X axis. This means that the top left corner of the plot is the “ideal” point - a false positive rate of zero, and a true positive rate of one. The further the logistic Regression is from the red line, the better the model is.  As we can see it above, this graph is great and whos that our model is working just fine !  "},{"metadata":{},"cell_type":"markdown","source":"### <font color='#FF781F'> **6.2) Random Forest Classifier** "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nimport time\n\nt0 = time.time()\n\nmodel = RandomForestClassifier()\nrf_model = model.fit(X_train, y_train)\npredict_train_rf = rf_model.predict(X_train)\npredict_test_rf = rf_model.predict(X_test)\nt1 = time.time()\nduration = t1 - t0\nprint(\"The duration of the training is \" , duration)\nprint('Training Accuracy:', accuracy_score(y_train, predict_train_rf))\nprint('Validation Accuracy:', accuracy_score(y_test, predict_test_rf))\n\n#Saving these values to compare before and after the dimentionality reduction \nRf_duration_before=duration\nRf_score_before=accuracy_score(y_test, predict_test_rf)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Oh wow ! A 99% training accuracy, it smells like Overfitting ...  "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\n\nconfusion_matrix(y_test, model.predict(X_test))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <font color='darkblue'> **7. Learning Curves analysis** </font>\n"},{"metadata":{},"cell_type":"markdown","source":"### <font color='#3268ca'> **7.1) The function that plots learning curves** "},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.datasets import load_digits\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.model_selection import ShuffleSplit\n\n\ndef plot_learning_curve(estimator, title, X, y, axes=None, ylim=None, cv=None,\n                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 3)):\n    \"\"\"\n    Generate 3 plots: the test and training learning curve, the training\n    samples vs fit times curve, the fit times vs score curve.\n\n    Parameters\n    ----------\n    estimator : object type that implements the \"fit\" and \"predict\" methods\n        An object of that type which is cloned for each validation.\n\n    title : string\n        Title for the chart.\n\n    X : array-like, shape (n_samples, n_features)\n        Training vector, where n_samples is the number of samples and\n        n_features is the number of features.\n\n    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n        Target relative to X for classification or regression;\n        None for unsupervised learning.\n\n    axes : array of 3 axes, optional (default=None)\n        Axes to use for plotting the curves.\n\n    ylim : tuple, shape (ymin, ymax), optional\n        Defines minimum and maximum yvalues plotted.\n\n    cv : int, cross-validation generator or an iterable, optional\n        Determines the cross-validation splitting strategy.\n        Possible inputs for cv are:\n\n          - None, to use the default 5-fold cross-validation,\n          - integer, to specify the number of folds.\n          - :term:`CV splitter`,\n          - An iterable yielding (train, test) splits as arrays of indices.\n\n        For integer/None inputs, if ``y`` is binary or multiclass,\n        :class:`StratifiedKFold` used. If the estimator is not a classifier\n        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n\n        Refer :ref:`User Guide <cross_validation>` for the various\n        cross-validators that can be used here.\n\n    n_jobs : int or None, optional (default=None)\n        Number of jobs to run in parallel.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    train_sizes : array-like, shape (n_ticks,), dtype float or int\n        Relative or absolute numbers of training examples that will be used to\n        generate the learning curve. If the dtype is float, it is regarded as a\n        fraction of the maximum size of the training set (that is determined\n        by the selected validation method), i.e. it has to be within (0, 1].\n        Otherwise it is interpreted as absolute sizes of the training sets.\n        Note that for classification the number of samples usually have to\n        be big enough to contain at least one sample from each class.\n        (default: np.linspace(0.1, 1.0, 5))\n    \"\"\"\n    if axes is None:\n        _, axes = plt.subplots(1, 3, figsize=(20, 3))\n\n    axes[0].set_title(title)\n    if ylim is not None:\n        axes[0].set_ylim(*ylim)\n    axes[0].set_xlabel(\"Training examples\")\n    axes[0].set_ylabel(\"Score\")\n\n    train_sizes, train_scores, test_scores, fit_times, _ = \\\n        learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs,\n                       train_sizes=train_sizes,\n                      return_times=True)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    fit_times_mean = np.mean(fit_times, axis=1)\n    fit_times_std = np.std(fit_times, axis=1)\n\n    # Plot learning curve\n    axes[0].grid()\n    axes[0].fill_between(train_sizes, train_scores_mean - train_scores_std,\n                         train_scores_mean + train_scores_std, alpha=0.1,\n                         color=\"r\")\n    axes[0].fill_between(train_sizes, test_scores_mean - test_scores_std,\n                         test_scores_mean + test_scores_std, alpha=0.1,\n                         color=\"g\")\n    axes[0].plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n                 label=\"Training score\")\n    axes[0].plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n                 label=\"Cross-validation score\")\n    axes[0].legend(loc=\"best\")\n\n    # Plot n_samples vs fit_times\n    axes[1].grid()\n    axes[1].plot(train_sizes, fit_times_mean, 'o-')\n    axes[1].fill_between(train_sizes, fit_times_mean - fit_times_std,\n                         fit_times_mean + fit_times_std, alpha=0.1)\n    axes[1].set_xlabel(\"Training examples\")\n    axes[1].set_ylabel(\"fit_times\")\n    axes[1].set_title(\"Scalability of the model\")\n\n    # Plot fit_time vs score\n    axes[2].grid()\n    axes[2].plot(fit_times_mean, test_scores_mean, 'o-')\n    axes[2].fill_between(fit_times_mean, test_scores_mean - test_scores_std,\n                         test_scores_mean + test_scores_std, alpha=0.1)\n    axes[2].set_xlabel(\"fit_times\")\n    axes[2].set_ylabel(\"Score\")\n    axes[2].set_title(\"Performance of the model\")\n\n    return plt","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <font color='#3268ca'> **7.2) Logistic regression** "},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(3, 2, figsize=(10, 15))\n\nX, y = X_train , y_train\n\ntitle = \"Learning Curves (Naive Bayes)\"\n# Cross validation with 100 iterations to get smoother mean test and train\n# score curves, each time with 20% data randomly selected as a validation set.\ncv = ShuffleSplit(n_splits=100, test_size=0.2, random_state=0)\n\nestimator = LogisticRegression()\nplot_learning_curve(estimator, title, X, y, axes=axes[:, 0], ylim=(0.7, 1.01),\n                    cv=cv, n_jobs=3)\n\ntitle = \"Learning Curves (SVM, RBF kernel, $\\gamma=0.001$)\"\n# SVC is more expensive so we do a lower number of CV iterations:\ncv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\nestimator = SVC(gamma=0.001)\nplot_learning_curve(estimator, title, X, y, axes=axes[:, 1], ylim=(0.7, 1.01),\n                    cv=cv, n_jobs=3)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see it: \n- the accruacy are pretty high ( between 0.80 and 0.85)\n- The training accuracy is above the cross-validation score\n- Both score tend to converge towards a 0.82 score as the training examples increase \n\nThis means that the Logistic Regression classifier fits the data set. <BR>\nMore data would have been useful to make this model even more accurate.  "},{"metadata":{},"cell_type":"markdown","source":"### <font color='#3268ca'> **7.3) Random Forest Classifier** "},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(3, 2, figsize=(10, 15))\n\nX, y = X_train , y_train\n\ntitle = \"Learning Curves (Naive Bayes)\"\n# Cross validation with 100 iterations to get smoother mean test and train\n# score curves, each time with 20% data randomly selected as a validation set.\ncv = ShuffleSplit(n_splits=100, test_size=0.2, random_state=0)\n\nestimator = RandomForestClassifier()\nplot_learning_curve(estimator, title, X, y, axes=axes[:, 0], ylim=(0.7, 1.01),\n                    cv=cv, n_jobs=3)\n\ntitle = \"Learning Curves (SVM, RBF kernel, $\\gamma=0.001$)\"\n# SVC is more expensive so we do a lower number of CV iterations:\ncv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\nestimator = SVC(gamma=0.001)\nplot_learning_curve(estimator, title, X, y, axes=axes[:, 1], ylim=(0.7, 1.01),\n                    cv=cv, n_jobs=3)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"These learning curves confirm that the Random Forest Classifier **Overfits** a lot. The training score is way higher than the cross-validation score, and they never converge. \nHow to prevent overfitting ? \n- We can train the model with **more data** : This is the optimal solution. As mentioned earlier, the model is trained with few elements. This makes the model very unstable and very likely to overfit.  \n- **Remove some of the features** using dimensionality reduction. Let's try this option.    "},{"metadata":{},"cell_type":"markdown","source":"## <font color='darkblue'> **8. Dimensionality Reduction** </font>\n "},{"metadata":{},"cell_type":"markdown","source":"### <font color='#3268ca'> **8.1) PCA method** "},{"metadata":{},"cell_type":"markdown","source":"Compute the minimum number of dimensions required to preserve **95%** of the training data"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\n\npca = PCA()\npca.fit(X_train)\ncumsum = np.cumsum(pca.explained_variance_ratio_)\nd = np.argmax(cumsum >= 0.95) + 1\nprint(d)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This means that instead of working with 13 features, we can work with only 10. \n\nUnfortunatly, we won't be able to plot the results of this algorithm because even the PCA uses 10 features and we only know how to plot two features.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"pca = PCA(n_components = 10)\nX_test_reduced = pca.fit_transform(X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test_reduced.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <font color='#3268ca'> **8.2) Logistic Rgression** "},{"metadata":{"trusted":true},"cell_type":"code","source":"LogReg_clf_reduced =LogisticRegression(C=3, random_state=43)\nfrom sklearn.metrics import accuracy_score\nimport time\n\nt0 = time.time()\nLogReg_clf_reduced.fit(X_test_reduced, y_train)\nt1 = time.time()\nduration = t1 - t0\n\nprint(\"The duration of the training before dimensionality reduction is \" , Log_reg_before)\n\nprint(\"The duration of the training with dimensionality reduction is \" , duration)\n\n\nX_test_reduced2 = pca.transform(X_test)\n\ny_pred = LogReg_clf_reduced.predict(X_test_reduced2)\n\nprint(\"The validation accuracy score before dimensionality reduction is \" , Log_reg_score_before)\nprint(\"The validation accuracy score with dimensionality reduction is \" , accuracy_score(y_test, y_pred))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Dimensionality reduction didn't really impact the accuracy score, but it has **significantly lowered the duration of the training**."},{"metadata":{},"cell_type":"markdown","source":"### <font color='#3268ca'> **8.3) Random Forest** "},{"metadata":{"trusted":true},"cell_type":"code","source":"Rf_clf_reduced =RandomForestClassifier()\nfrom sklearn.metrics import accuracy_score\nimport time\n\nt0 = time.time()\nRf_clf_reduced.fit(X_test_reduced, y_train)\nt1 = time.time()\nduration = t1 - t0\nprint(\"The duration of the training before dimensionality reduction is \" , Rf_duration_before)\n\nprint(\"The duration of the training with dimensionality reduction is \" , duration)\n\nRf_duration_before=duration\nRf_score_before=accuracy_score(y_test, predict_test_rf)\n\n\nprint(\"The validation accuracy score before dimensionality reduction is \" , Rf_score_before)\nprint(\"The validation accuracy score with dimensionality reduction is \" , accuracy_score(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Dimensionality reduction slightly lowered the accuracy score and increased a little the duration of the training. \n\n**Contrary to the logistic regression, dimensionaliy reduction is not useful at all to the Random Forest model.**    "},{"metadata":{},"cell_type":"markdown","source":"## <font color='darkblue'> **9. Results Interpretation and ideas to go further and perfect the algorithm** </font>\n"},{"metadata":{},"cell_type":"markdown","source":"I decided to choose the **Logistic Regression model**. This model can give you with a **82%** accuracy wether you will match with someone or no only using your personal features (your gender, attractiveness, intelligence, sincerity, fun and ambition)  and the other person's features (its attractiveness, intelligence, sincerity, fun and ambition). Isn't it great ? \n\n\nThe main reason I chose this model is that it successfully avoided overfitting and underfitting problems. The Random Forest classifier is clearly overfitting, which creates a huge bias. \n\nBut still, to perfect this algorithm, more data is mandatory. In fact, the confusion matrix was not really satisfying. As mentioned earlier, 80 matches were falsly classified as non-matches. This is very problematic ! Imagine passing by the love of your life because of an algorithm ... In addition to adding more data, this data should be more balanced  to generate better resultats. This means it should contain almost as many matches and fails *(also colled \"Non matches\")*.  \n\nUnfortunatly, I wasn't able to plot the results of this algorithm, even after applying dimensionality reduction, because the number of features is superior to 2. \n\nEven if there is still a lot of work to perfect this algorithm, I feel like we learned a lot about dating and especially about men and women's dating behaviours. \nSome of the most interesting conclusions :  \n- In speed datings, people are really willing to have a good time and to get to know people, but the sincerety of some of them might me questionable.\n- Men tend to be more easily rejected than women OR women are more picky than men.\n- Finding love can be very grueling and you might have to go on numerous boring dates to find *the one* (Provided he/she exists...) \n- Attracitveness, fun, and shared interests make people want to go on a second date. Participants should focus on them to make a great first impression in 4 minutes.  \n- The fact of liking somebody is very subjective. As we say : there is no accounting for tastes ! So neither is there for attractiveness and fun. This means that to find love, people just need to be themselves to meet people that will have the same taste and therefore people that fully appreciate their value.   "}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}