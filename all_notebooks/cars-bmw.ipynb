{"cells":[{"metadata":{"_uuid":"8882021d-49a2-4962-b6ee-2ca010bd29c3","_cell_guid":"7dfd79d7-a1e3-46bd-8bf5-b92d5e2796ff","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import plotly.express as ply\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df=pd.read_csv('../input/used-car-dataset-ford-and-mercedes/bmw.csv')   \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"CHECK for NULL and NAN values","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().any().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isna().any().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As seen we have Model,Transmission type, Fuel type is object for which we have to convert these categorical values into int.We will use Label Encoding technique for this later.For now lets lokk at the unique values in those categorical features.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['model'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['transmission'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['fuelType'].unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Exploratory Data Analysis","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Checkong the correlation of all the features using heatmap.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_=df.corr()\nsns.heatmap(corr_,annot=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looking at the different models and their values","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(17,5))\nsns.countplot(df['model'])\nplt.title('Model')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Pie Chart","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_ser_=df.model.value_counts()\ndf_ser=pd.DataFrame(df_ser_)\nlabels=df['model'].unique()\nsizes=df_ser['model']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig1,ax1=plt.subplots()\nax1.pie(sizes,explode=None,labels=labels,autopct='%1.1f%%',shadow=True,startangle=90)\nax1.axis('equal')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Cars with different fuel type","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(df['fuelType'])\nplt.title('Fuel Type')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Cars with different Transmission type","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(df['transmission'])\nplt.title('Transmission type')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Cars with different Enigine Size","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(df['engineSize'])\nplt.title('Engine Size')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model Vs Price","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,10))\nplt.scatter(df['model'],df['price'])\nplt.title('Model Vs Fuel type')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Engine size Vs Miles Per Gallon","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(50,30))\nsns.jointplot(x='engineSize',y='mpg',data=df)\nplt.xlabel('engineSize')\nplt.ylabel('mpg')\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Fuel type Vs MPG","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,10))\nplt.scatter(df['fuelType'],df['mpg'])\nplt.xlabel('Fuel Type')\nplt.ylabel('MPG')\nplt.title('Fuel type vs MPG')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model Vs MPG","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,10))\nplt.scatter(df['model'],df['mpg'])\nplt.xlabel('Model')\nplt.ylabel('MPG')\nplt.title('Model vs MPG')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model Vs Transmission","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,10))\nplt.scatter(df['fuelType'],df['transmission'])\nplt.xlabel('Fuel ')\nplt.ylabel('Transmission')\nplt.title('Model vs Transmission')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Tax Vs Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,10))\nplt.scatter(df['tax'],df['model'])\nplt.xlabel('Tax')\nplt.ylabel('Model')\nplt.title('Model vs MPG')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we have seen different graphs explaining relationships between different variables.Let us now Encode the Categorical variable using Label Encode.We can also plot different graphs by taking different parameters from the data.We can dive deep into the data more by using .groupby() method and grouping the models based on years and other parameters/attributes.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nLE=LabelEncoder()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['model']=LE.fit_transform(df['model'])\ndf['fuelType']=LE.fit_transform(df['fuelType'])\ndf['transmission']=LE.fit_transform(df['transmission'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As one can see in the info() the categorical columns have now been encoded and dtype has been changed to int64 from object which makes it easier for us and to the model to perform operations on it.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Selecting Features i.e Dependent and independent variables for prediction","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"features=['model','year','transmission','mileage','fuelType','tax','mpg','engineSize']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X=df[features]\nY=df['price']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Splitting the data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.3,random_state=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Having splited the data into Train and Test categories,we will apply Linear Regression ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nLR=LinearRegression()\nlr=LR.fit(X_train,Y_train)\npred=lr.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Preformance metrics","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import r2_score,mean_squared_error","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('R-Squared: ',r2_score(Y_test,pred)*100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vals=pd.DataFrame({'Predicted':pred,'Actual':Y_test})\nvals","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As one can see that the accuracy is around 74%,it is highly recomended to optimize the parameters or to implement the other regression techniques to get better accuracy.Now I'll with Ridge regression and ElasticNet which is the combination of Ridge and Lasso Regressions.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Ridge is a L2 norm where it distributes the coefficients across all the features.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import Ridge,ElasticNet\nridge=Ridge(alpha=2,max_iter=1000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ridge.fit(X_train,Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Ridge_predict=ridge.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ridge.score(X_test,Y_test)*100 ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ridge havent shown better accuracy than the Linear Regression.Lets try with Elastic Net.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"EN=ElasticNet(alpha=1,l1_ratio=1.001,max_iter=1000)\nEN.fit(X_train,Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"EN_pred=EN.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"EN.score(X_test,Y_test)*100","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Even the elastic net haven't given us the good enough result when compared to that of Linear and Ridge Regressions.Lets try with Gradient boosting and  Extreme Gradient Booosting algorithims.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Boosting algorithims comes in first place when one think of either regression or classification problems.Since it's introduction in '04 's it has become popular among Data Science enthusiasts and Data Scientists because of its accurate implementations and computing capability od boosting tree algorithms.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Gradient Boosting Regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingRegressor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"GB=GradientBoostingRegressor(random_state=0)\nGB.fit(X_test,Y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"GB_pred=GB.predict(X_test)\nGB_pred","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Score of Gradient Boosting","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Performance Score(GB): ',GB.score(X_test,Y_test)*100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Bammm!!!!!!! Finally satiisfactory results.Lets check with Extreme Gradient Boosting(XGB) also.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBRegressor\nXGB=XGBRegressor()\nXGB.fit(X_train,Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"XGB_pred=XGB.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Score","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print('performance score(XGB): ',XGB.score(X_test,Y_test)*100) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Print the values for both Gradient Boosting and XGB ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#For XGB\nvalues=pd.DataFrame({'Predicted':XGB_pred,'Actual':Y_test})\nvalues","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#For GB\nvalues=pd.DataFrame({'Predicted':GB_pred,'Actual':Y_test})\nvalues","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Linear Regression accuracy score: ',r2_score(Y_test,pred)*100)\nprint('Ridge Regression accuracy score: ',ridge.score(X_test,Y_test)*100 )\nprint('Elastic_Net Regression accuracy score: ',EN.score(X_test,Y_test)*100)\nprint('Gradient_Boosting Regression accuracy score: ',GB.score(X_test,Y_test)*100)\nprint('XGB Regression accuracy score: ',XGB.score(X_test,Y_test)*100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally!...We have managed to achive a 95% score with Gradient Boosting and XGB(extreme Gradient Boosting) algorithms.As always my first choise was GB and XGB however to show-up the problem with different regression algorithims I have used 'Linear Regression','Ridge Regression'(ps note: Could have tried with Lasso Regression also),'Elastic Net','Gradient Boosting','XGBoosting'.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}