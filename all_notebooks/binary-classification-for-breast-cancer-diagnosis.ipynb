{"nbformat":4,"nbformat_minor":1,"cells":[{"source":"### Problem Statement\nThe problem in hand is a binary classification problem involving a large number of features. Here the inputs are the characteristics of the test culture (such as the radius, smoothness, compactness etc. of the distribution of cells), while the output is binary, i.e., benign or malign. \n\nSo the method to approach this would be to understand the significance of the features, execute some strategies for feature reduction, apply a binary classification algorithm and iterate this process, until performance saturates. \n\nIn short, the objective of this study is to build a predictive model that wil improve the accuracy, objectivity and reproducibility of breast cancer diagnosis by FNA. \n","cell_type":"markdown","metadata":{"_cell_guid":"16b9afd7-22fd-470f-94ea-7f21f965c90f","_uuid":"75d0b95480da7bdd6512b8712d47e785d63c5c23"}},{"source":"### Data Exploration & Exploratory Visualization\n\nThe dataset has 569 rows and 33 columns. Amongst the 33 columns, the first two are `ID number` and `Diagnonsis (M=malignant, B = benign)`. And the last column is an unnamed column with only NaN values, so it is removed right away. The other 30 columns correspond to mean, standard deviation and the largest values (points on the tails) of the distributions of the following 10 features computed for the cellnuclei;\n\n- radius (mean of distances from center to points on the perimeter)\n- texture (standard deviation of gray-scale values) \n- perimeter \n- area \n- smoothness (local variation in radius lengths) \n- compactness (perimeter^2 / area - 1.0) \n- concavity (severity of concave portions of the contour) \n- concave points (number of concave portions of the contour) \n- symmetry \n- fractal dimension (\"coastline approximation\" - 1)\n\n\nAll feature values are recorded with four significant digits. The class distribution of the samples is such that 357 are benign and 212 are malignant, which is imbalanced, but not so bad. ","cell_type":"markdown","metadata":{"_cell_guid":"a71462c5-6d7e-4609-ad77-eb98002a5aae","_uuid":"7859c94ff0b3a4e00f84140dca5a53b76473ec5d"}},{"source":"import numpy as np \nimport pandas as pd \nimport seaborn as sns \nfrom IPython.display import display # Allows the use of display() for DataFrames\n# Pretty display for notebooks\n%matplotlib inline\nimport matplotlib.pyplot as pl\nimport matplotlib.patches as mpatches\nimport importlib\nimportlib.import_module('mpl_toolkits.mplot3d').Axes3D\nfrom time import time\nfrom sklearn.metrics import f1_score, accuracy_score\nfrom collections import Counter\nimport re\nfrom sklearn.model_selection import cross_validate\nimport math\nimport random \nfrom collections import defaultdict\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import cross_validate\nimport re\n\nrandom.seed(50)","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"52942f7b-e58d-4275-86f0-ced1bcea06f9","_uuid":"d7dc365d2933b6675c57c98d438356e4cc1e6125","collapsed":true,"_execution_state":"idle"}},{"source":"Output and features are first extracted from the complete dataset. ","cell_type":"markdown","metadata":{"_cell_guid":"3d2dc74b-f62d-49fc-aed7-5103287e6278","_uuid":"a03e816d472a7120c672de87cb50c20764f89498"}},{"source":"def als_split_data(data):\n    output = data['diagnosis']\n    features = pd.DataFrame(data=data)\n    cols = ['id', 'diagnosis']\n    for col in cols:\n        if col in features.columns:\n            features = features.drop(col, axis=1)\n    return output, features\n\n","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"f6e15071-3034-4ba5-bc3f-94fdbe1a9b2a","_uuid":"7466c29c10fdb5da4c33a6c2fa75de92d6456089","collapsed":true}},{"source":"data = pd.read_csv('../input/breast-cancer.csv')\ndata.columns\n# Removing the last unnamed columns\ndata = data.drop(['Unnamed: 32'], axis =1)\noutput, features = als_split_data(data)\ndata.head()","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"c9bd4680-5a5d-4ce5-8b85-1820d2e478d0","_uuid":"4a65810840012b075b5a359994931bec8acf9ab0","collapsed":true,"_execution_state":"idle"}},{"source":"The above is an example of what the dataset looks like. Below is a statistical description of the 30 dataset features. As mentioned before, the 30 features correspond to the mean, standard deviation and \"worst values\" of the measured cell culture characteristics such as radius, texture, area, smoothness etc. ","cell_type":"markdown","metadata":{"_cell_guid":"231b5bce-70ac-4962-9fd9-bd230999e6af","_uuid":"7d60af755412af4eb9b58fedb6558a25752fb46a"}},{"source":"display(features.describe())","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"60308baf-344a-41fb-8580-cef707ce5aa8","_uuid":"54600377cdbec016505dcb970bb1988afbc260a2","collapsed":true,"_execution_state":"idle"}},{"source":"From what it seems, `area` is a parameter that is well spread out, i.e., it has a large standard deviation, but it's probably only because `area` is a squared function of the radius. In the next few steps, I will do further data exploration in the following order\n\n1. Visualize the distribution statistics; apply appropriate data preprocessing steps\n2. Look for outliers\n3. Observe feature correlations that will guide in feature selection","cell_type":"markdown","metadata":{"_cell_guid":"ae2569b0-71d7-4bcd-9c3a-f38dd2694727","_uuid":"d3409130385ba762589e0310746bad7eec3d3634"}},{"source":"### Data Preprocessing, Outlier Detection","cell_type":"markdown","metadata":{"_cell_guid":"34484707-d705-4ec4-888c-bad94080b60b","_uuid":"f047be499db06e9b24f852fc6aaa91314bd78709"}},{"source":"The following plots shows histograms of 6 features - 'area_mean', 'texture_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean', 'symmetry_mean'","cell_type":"markdown","metadata":{"_cell_guid":"557a7771-f1f3-46b9-80c7-99f8fb40a752","_uuid":"55b8dc2c5ff0f7f17023c419551b2978034b55cc"}},{"source":"def vs_distribution(data):\n    \"\"\"\n    Visualization code for displaying skewed distributions of features\n    \"\"\"\n    \n    # Create figure\n    fig = pl.figure(figsize = (18,15));\n\n    # Skewed feature plotting\n    for i, feature in enumerate(data.columns[:10]):\n        ax = fig.add_subplot(5, 5, i+1)\n        ax.hist(data[feature], bins = 25, color = '#00A0A0')\n        ax.set_title(\"'%s'\"%(feature), fontsize = 14)\n        ax.set_xlabel(\"Value\")\n        ax.set_ylabel(\"Number of Records\")\n        #ax.set_ylim((0, 2000))\n        #ax.set_yticks([0, 500, 1000, 1500, 2000])\n        #ax.set_yticklabels([0, 500, 1000, 1500, \">2000\"])\n\n        fig.suptitle(\"Distributions Features\", \\\n            fontsize = 16, y = 1.03)\n\n    fig.tight_layout()\n    fig.show()","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"b7825303-f9c0-4961-af93-1f88995befae","_uuid":"54f49d3c5d8ec9349d26c4648d4e8c8e333cd239","collapsed":true}},{"source":"vs_distribution(features)","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"54d5428c-ef79-465b-b7cb-40975d9bcf08","_uuid":"92fc789d068bf16bb63498fd95ab9dc02c04ab05","collapsed":true}},{"source":"I am first going to look for outliers, remove them and observe the distributions again. \n\nIt is to be noted that the features contain `mean`, `se` and `worst` values of the measurements of the 10 features describe in section I. Since `worst` and `se` values determine the quality of measured data, I am going to observe only the last 20 features for removing outliers. ","cell_type":"markdown","metadata":{"_cell_guid":"869118b7-e0c1-4ddc-a5ea-f63ca351dfae","_uuid":"b25731ee47a66257405ac4e2dc1fa140b5a90af5"}},{"source":"I am using `als.print_outliers()` for this. This function looks for outliers in each of the features, that are lying `how_far` steps away from its respective interquartile range. Each of these bad points are counted in a dictionary and finally bad points to be discarded are selected as those that occurred with highest frequency i.e., points that were bad in most features (as determined by the `worst_th` parameter).","cell_type":"markdown","metadata":{"_cell_guid":"909e1430-b6ce-43f7-9dd2-c9b88017b474","_uuid":"f0cbf2bff252a0588a4e32de43d74b17f346496f"}},{"source":"# For each feature print the rows which have outliers in all features \ndef als_print_outliers(data, how_far=2, worst_th=6, to_display=False):\n    # Select the last 10 features as they are the worst collected during measurements\n    data = data.iloc[:,11:30]\n    really_bad_data = defaultdict(int)\n    for col in data.columns:\n        Q1 = np.percentile(data[col], 25)\n        Q3 = np.percentile(data[col], 75)\n        step = (Q3-Q1)*how_far\n        bad_data = list(data[~((data[col]>=Q1-step)&(data[col]<=Q3+step))].index)\n        for i in bad_data:\n            really_bad_data[i]+= 1\n        # Display the outliers\n    max_ind = max(really_bad_data.values())\n    worst_points = [k for k, v in really_bad_data.items() if v > max_ind-worst_th]\n    if to_display:\n        print(\"Data points considered outliers are:\") \n        display(data.ix[worst_points,:])\n    return worst_points\n    ","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"19310af4-fd83-4ec2-8ffe-fa3408b347ba","_uuid":"86b028a2e2175826463455d9aa1ef4ab9d7d210a","collapsed":true}},{"source":"outlier_indices = als_print_outliers(features, worst_th=3)","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"724ac13d-26b5-40c5-b0d4-6e56d27745e9","_uuid":"d716338ff7cb8d44f8c7fa6f9ffee294570fafeb","collapsed":true}},{"source":"# Cleaning dataset by dropping outliers (cl)\ndata_cl = data.drop(data.index[outlier_indices]).reset_index(drop=True) # cleaned data\noutput_cl, features_cl = als_split_data(data_cl)\n\nvs_distribution(features_cl)","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"69e32b8c-5a5a-4e75-9adc-289d15e5d7e1","_uuid":"f0a4bd0add063ca1c65b6ae37eb80da9792a42c5","collapsed":true}},{"source":"print('Size of new dataset is {0:.2f} % of the original'.format(100.0*len(data_cl)/len(data)))","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"8c16d101-5c6d-4448-b7f1-5bf977131d39","_uuid":"3142e2e7baf4e0cb682aea1b542c8a8fde6f93bb","collapsed":true}},{"source":"As can be seen from the above two plots, the distribution characteristics have definitely changed, especially w.r.t to the features depicting dimensions of the cell cultures, if not for features like `concavity_mean`, `concave_points_mean`, `symmetry_mean` and `fractional_dimension_mean`","cell_type":"markdown","metadata":{"_cell_guid":"ebb40a40-d074-46ed-a005-867cb96593ff","_uuid":"794e997fc69249c6825758705a04fd753b08f071"}},{"source":"Before proceeding with other visualization techniques, I will apply logarithmic transormations to features (to see if it will remove the skewness) and apply min-max scaling as well. These data preprocessing steps might be useful in further steps where I will be analysing the effects of each of these features on classification.","cell_type":"markdown","metadata":{"_cell_guid":"17c05c6d-1329-4a89-b5ef-61879bf8556a","_uuid":"b932d4f1723209799c33b89a1c7acdb66dafd351"}},{"source":"def als_transform_log_minmax(data):\n    cols = data.columns\n    data_transformed = pd.DataFrame(data=data)\n    scaler = MinMaxScaler()\n    for col in cols:\n        data_transformed[col] = data[col].apply(lambda x: np.log(x+1))\n        data_transformed[col] = scaler.fit_transform(data[col].values.reshape(-1,1))\n    return data_transformed","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"a71d7e8c-6dd7-4f34-afe4-f8d910f9b6a6","_uuid":"bac4c9423971a49a7e92c51e1010e5a32e41fabe","collapsed":true}},{"source":"# Applying log transfromation and minmax scaling (tr)\nfeatures_cl_tr = als_transform_log_minmax(features_cl) # cleaned, transformed data\ndata_cl_tr = pd.concat([output_cl, features_cl_tr], axis=1)","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"d40c2f0d-560b-4463-b5ce-240c11dbcddd","_uuid":"3040605c2ebf4e0d3ff55ddf2dcf1b10d2005f11","collapsed":true}},{"source":"vs_distribution(features_cl_tr)","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"513502a8-6f2d-45c6-976d-a12390f27633","_uuid":"064602fea9937f2b89774ed4866c267c7a72568b","collapsed":true}},{"source":"Skewness has reduced in most plots except for `concavity_mean`, `concave_points_mean` and `fractal_dimension_mean`. I am going to look for outliers again and check the distribution statistics again ","cell_type":"markdown","metadata":{"_cell_guid":"b1dd897b-0fbe-4d80-addf-3f51d59747de","_uuid":"ab2ee86e92b558f13fb8095815a10653773a58ba"}},{"source":"outlier_indices = als_print_outliers(features_cl_tr,worst_th=3)","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"1b98a51f-1769-4ec3-99bc-9f005c8f80ad","_uuid":"330a9798d2f172353024b87a6116a1aa4456b666","collapsed":true}},{"source":"# Cleaning dataset again - dropping outliers\ndata_cl_tr_cl = data_cl_tr.drop(data_cl.index[outlier_indices]) # cleaned transformed cleaned data\noutput_cl_tr_cl, features_cl_tr_cl = als_split_data(data_cl_tr_cl)","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"a30c064e-df9c-4e1b-b8fc-d749ba29fabe","_uuid":"773319689e8a5b4abb3c9c1b896b525da447e0e0","collapsed":true}},{"source":"print('Size of new dataset is {0:.2f} % of the original'.format(100.0*len(data_cl_tr_cl)/len(data)))","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"263e76a8-15c2-4f8c-8123-b0b6e781d422","_uuid":"dc2346478e7af7fb019fabc5e08b0f33a8d3b80d","collapsed":true}},{"source":"Below is a plot showing the size distribution of benign and malign samples, before and after data transformation operations.","cell_type":"markdown","metadata":{"_cell_guid":"5fedbb00-8dc8-423c-9d26-67345da7d909","_uuid":"3b2c412002e2f5a574c07ba5fdc38801bae1507f"}},{"source":"def als_encode_diagnosis(d):\n    if d== 'B':\n        ed = 0\n    else:\n        ed = 1\n    return ed","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"d2b84a48-0b16-4bde-94ff-5e4562bbe57d","_uuid":"f35599977670a07eba50a25bd08bb9208dee25ce","collapsed":true}},{"source":"def vs_show_output_classes(data, data_clean):\n    '''\n    Visualization code for histogram of classes\n    '''\n    \n    # Create figure\n    fig = pl.figure(figsize=(10,6))\n    \n    encoded_data = data.apply(lambda x: als_encode_diagnosis(x))\n    encoded_data_clean = data_clean.apply(lambda x: als_encode_diagnosis(x))\n\n    ax = fig.add_subplot(111)\n    n, bins, patches = ax.hist(encoded_data, bins=np.arange(3), alpha=0.5, color='b', label='Lost data', width=0.5)\n    n_c, bins_c, patches_c = ax.hist(encoded_data_clean, bins=np.arange(3), color='k', label = 'Data filtered for outliers',width=0.5)\n    '''\n    colors = ['r', 'g']\n    for i in range(2):\n        patches[i].set_fc(colors[i])\n        patches_c[i].set_fc(colors[i])\n    '''    \n    ax.set_title('Barplot of output classes', fontsize=16)\n    ax.set_xticks([b+0.25 for b in bins[:-1]])\n    ax.set_xticklabels(['Benign', 'Malign'], fontsize=16)\n    \n    ax.legend(fontsize=16)\n    ax.set_ylabel('Number of records', fontsize=16)\n    fig.tight_layout()\n    fig.show()","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"2c9315ed-31c8-4322-9aba-558418edb329","_uuid":"76d21ffcff41d4a4cc682c15745cc1b86e8a5dc2","collapsed":true}},{"source":"vs_show_output_classes(output, output_cl_tr_cl)","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"31ec8d06-ea25-4b34-84ca-c322b3d8a10f","_uuid":"71fecf26e957a2d670182d607aca5a7b92b4a3b6","collapsed":true,"_execution_state":"idle"}},{"source":"As seen, we haven't lost too much data. Outliers seem to have been removed almost equally in both classes and since this is a classification problem, I believe this data pre-processing step would definitely be useful in producing a robust model.","cell_type":"markdown","metadata":{"_cell_guid":"83ec0d17-a5cb-4b6d-9649-dfa51c0fcc6b","_uuid":"bbc1f4e46d8968ebe1da137e6e32018ad77aec2f"}},{"source":"I am now going to create violin plots of the features split across the diagnosis type. Violin plots represent probability distributions of samples, similar to histograms and box plots. But, rather than showing counts of data points, violin plots use kernel density estimation (KDE) to compute empirical distributions of the samples. ","cell_type":"markdown","metadata":{"_cell_guid":"89895b44-678e-4e15-a78a-925bc31eba67","_uuid":"0f434a50bd6e6b886fae45273571516ebc06ad15"}},{"source":"### Visualizing feature effects","cell_type":"markdown","metadata":{"_cell_guid":"9c994d20-1ff1-4c8f-8749-27a59e5d284b","_uuid":"fd7d8c2184c6a4c228f8f6dd482a9a02a06731b8"}},{"source":"def als_return_select_cols(data, **kwargs):\n    checks = ['radius', 'area', 'perimeter']\n    cols = [c for c in data.columns for ch in checks if re.search('{}(.)'.format(ch), c)]\n    if kwargs['which']=='mean_non_dims':\n        cols = [c for c in data.columns if c not in cols]\n        cols = [c for c in cols if re.search('(.)_mean', c)]\n    elif kwargs['which']=='se_non_dims':\n        cols = [c for c in data.columns if c not in cols]\n        cols = [c for c in cols if re.search('(.)_se', c)]\n    elif kwargs['which']=='worst_non_dims':\n        cols = [c for c in data.columns if c not in cols]\n        cols = [c for c in cols if re.search('(.)_worst', c)]\n    elif kwargs['which']=='all':\n        cols = data.columns\n    return cols","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"a5100c9d-5813-4a6a-9256-f5cf073fb8ab","_uuid":"5423a4a3bcfe998d75bec2f51d5af186eec59d4b","collapsed":true}},{"source":"def vs_violin_swarm_plots(data, **kwargs):\n    cols = ['diagnosis'] + als_return_select_cols(data, which=kwargs['which'])\n    d = pd.melt(data[cols], id_vars = 'diagnosis', var_name = 'features', value_name = 'value')\n    \n    sns.set(font_scale=1.5)\n    fig, ax = pl.subplots(figsize=(15,10))\n    ax = sns.violinplot(x='features', y = 'value', hue='diagnosis', data=d, split=True, inner='quart')\n    for tick in ax.get_xticklabels():\n        tick.set_rotation(45)    \n    fig.tight_layout()\n    pl.subplots_adjust(bottom=0.2)\n    pl.show()","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"2f67e831-060f-4ab6-9317-eca7a71a5dd0","_uuid":"45a473bae4bddb5d2a99d5156c5d8d8993f1d553","collapsed":true}},{"source":"vs_violin_swarm_plots(data_cl_tr_cl,which='only_dims') # first 10 features","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"d44858d8-54b5-4e08-98cb-b7bb915430bd","_uuid":"4bd6d23db8e80e4e914903b3a89a4b50da483a7b","collapsed":true}},{"source":"vs_violin_swarm_plots(data_cl_tr_cl,which='mean_non_dims') # Next 10 features","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"6dd661bf-a0eb-44e8-a5e7-01a065e5cd90","_uuid":"022e9bed4e1e50150558cde9619cb1c8ea0f4eb6","collapsed":true}},{"source":"vs_violin_swarm_plots(data_cl_tr_cl,which='se_non_dims') # Last 10 features","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"b253e18c-b06a-495e-9c38-99cc7ca2c10b","_uuid":"8b3187599ba5791cfec4446760ada911f725dc15","collapsed":true}},{"source":"vs_violin_swarm_plots(data_cl_tr_cl,which='worst_non_dims') # Last 10 features","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"5a800d54-a8bc-4ee8-ab8f-d78c4b49d467","_uuid":"d987d7447a9d594081821b3a116a5ad631f93e3b","collapsed":true}},{"source":"There are many conclusions that can be made based on the above plot. \n\n1. Firstly, it appears that parameters corresponding to cell dimensions (radius, perimeter and area) as well as cell concavity and concave points have the right tails of their distribution belonging to the malign class and left tails belonging to the benign class. In other words, these features seem ideally useful for classification.\n\n2. Except for texture se most other se features do not have distributions that are far spread out. Moreover, they do not seem to be useful in classification itself, except maybe for concavity se, concave points se and fractal dimension se\n\n3. Texture and smoothness features (mean, se, worst) seem to be equally distributed in both the classes\n\n4. None of the symmetry features seem very useful for classification","cell_type":"raw","metadata":{"_cell_guid":"06f55e4b-1211-4cd8-879b-504136e1ddff","_uuid":"eb4c86e824a29d92fb17090a8446a697c56f3613"}},{"source":"I am now going to discover any feature corerlations. Now, radius, area and perimeter would be features that are correlated - as seen even their distribution characteristics are similar as seen in the above violin plots. This is established in the following correlation heat map.","cell_type":"markdown","metadata":{"_cell_guid":"ba62dc40-ee02-486b-9e2e-57bbed6b8097","_uuid":"1301dbdf1fcdb0a8056e5dd0fe48a701de39bbe7"}},{"source":"### Visualizing feature correlations","cell_type":"markdown","metadata":{"_cell_guid":"9b68770d-a760-473e-9ee5-92a90378df1e","_uuid":"4ac5dffd0eb29d3d906878ad5b10c41045bf87a1"}},{"source":"def vs_observe_correlations(data, **kwargs):\n    cols = als_return_select_cols(data, which=kwargs['which'])\n    fig,ax = pl.subplots(figsize=(10,7))\n    sns.heatmap(data[cols].corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)\n    fig.tight_layout()\n    pl.show()","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"e9c74344-0223-45de-8905-b0513a1b3633","_uuid":"a65729a73001887087842d7da95a854578240b3a","collapsed":true}},{"source":"vs_observe_correlations(features_cl_tr_cl, which='only_dims')","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"f9033a69-04b4-44b9-b3b1-919945001243","_uuid":"d331be453551f7e79d7e233734ee9c6e81ca60d8","collapsed":true}},{"source":"Almost all numbers in the correlation matrix are high indicating high correlation between the features, as expected. So I can make a random choice to select all the `area` features (i.e., `area_mean`, `area_se`, `area_worst`). But again, from the above heatmap, `area_mean` is highly correlated with `area_se` and `area_worst`. So it wouldn't be a bad decision to only choose `area_mean` from these 9 features. ","cell_type":"markdown","metadata":{"_cell_guid":"8de90bba-e8f8-488a-b7c3-70cb48613f20","_uuid":"5f339318b3aae4919cfff8debd1409481c42f8af"}},{"source":"vs_observe_correlations(features_cl_tr_cl, which='mean_non_dims')","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"c7c41c85-b854-430f-99ea-8c1b8d55fccb","_uuid":"675c4303eca277bc673aa236c5617c18cfbeb20c","collapsed":true}},{"source":"The above heatmap again shows high values in the centre. Implying high correlation between `compactness_mean`, `concavity_mean`, `concave_points_mean`","cell_type":"markdown","metadata":{"_cell_guid":"9b1a4011-2f4a-4bfc-8e33-35690b5bdec3","_uuid":"b2ce745c50bb00b9729c51638c49468412ef2942"}},{"source":"vs_observe_correlations(features_cl_tr_cl, which='se_non_dims')","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"6f5ca81d-e563-46ae-b453-4505ebf82af0","_uuid":"d6a093cf904b03f2019bce60aa8e7ec193435981","collapsed":true}},{"source":"Similarly the above heatmap indicates high correlation between `compactness_se`, `concavity_se`, `concave_points_se`","cell_type":"markdown","metadata":{"_cell_guid":"f5e876fe-54c7-48b1-96d7-7c3f037a9179","_uuid":"76707317cc2c72a6ddf64f35c952ecae67aef5bd"}},{"source":"vs_observe_correlations(features_cl_tr_cl, which='worst_non_dims')","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"338335ed-b772-470a-b8bb-11e033c7730f","_uuid":"c21ab2383181089333d40d99a059fbdd6d009414","collapsed":true}},{"source":"And there is a high correlation between `compactness_worst`, `concavity_worst`, `concave_points_worst`","cell_type":"markdown","metadata":{"_cell_guid":"4334c1a3-0c69-4cc6-ae2d-c73969056022","_uuid":"327f060bb64016c249eae6ab76e738cf61cf1250"}},{"source":"The final conclusions of feature selection implies that keeping one of the correlated features in each of the list item below might probably aid in classification:\n- radius_mean, radius_se, radius_worst\n- perimeter_mean, perimeter_se, perimeter_worst\n- area_se, area_worst\n- smoothness_se, compactness_se, concave_points_se, concavity_se, symmetry_se\n- compactness_worst, concavity_worst, concave_points_worst","cell_type":"markdown","metadata":{"_cell_guid":"43e3b40c-3bc4-4d19-9372-7ff55183ac2f","_uuid":"95328968eab1cc1c34d6308cc3377cde0a4ee220"}},{"source":"So now the data has been scaled, transformed to reduce skweness, outliers have been removed, and feature correlations have been inspected. The next study would be feature trasnformation. By applying PCA to the `good_data`, new dimensions that best maximizes the variance of features can be discovered. In addition to finding these dimensions, PCA also reports the captured variance of each dimension.","cell_type":"markdown","metadata":{"_cell_guid":"6d90ad66-fd28-435a-982d-914fd0623d6a","_uuid":"a5646f9e838b1fba9d7dae7bd990bfd4ba27608a"}},{"source":"from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nclf = LinearDiscriminantAnalysis()\noutput_float = output_cl_tr_cl.apply(lambda x: als_encode_diagnosis(x))\ncoeffs = clf.fit(features_cl_tr_cl[:450], output_float[:450]).coef_.T\nLDA_F = features_cl_tr_cl[:450].dot(coeffs)\n\npreds = clf.predict(features_cl_tr_cl[451:])\nfig, ax = pl.subplots()\nax.scatter(np.arange(len(preds)), preds-output_float[451:], c = preds, cmap='winter')\nax.legend()","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"e90758cf-0ca1-415c-a0aa-06f781360031","_uuid":"8d534c03c3b82b922963d8e870919907518e51a7","collapsed":true}},{"source":"### Feature Transformation by PCA","cell_type":"markdown","metadata":{"_cell_guid":"45dae910-a7b3-46d2-a37e-9871705c2864","_uuid":"11b0b61449f8bbc762be01421af040b10e778cc5"}},{"source":"# Applying PCA\nfrom sklearn.decomposition import PCA ","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"d59ea923-b539-46d2-82b5-0f91ceac84f9","_uuid":"4e4cfece1cf69cdfd7ce6098fc819b244b82d4ee","collapsed":true}},{"source":"def vs_plot_pca_variance(pca):\n    x = np.arange(1,len(pca.components_)+1)\n    fig, ax = pl.subplots(figsize=(10,6))\n    \n    # plot the cumulative variance\n    ax.plot(x, np.cumsum(pca.explained_variance_ratio_), '-o', color='black')\n\n    # plot the components' variance\n    ax.bar(x, pca.explained_variance_ratio_, align='center', alpha=0.5)\n\n    # plot styling\n    ax.set_ylim(0, 1.05)\n    \n    for i,j in zip(x, np.cumsum(pca.explained_variance_ratio_)):\n        ax.annotate(str(j.round(2)),xy=(i+.2,j-.02))\n    ax.set_xticks(range(1,len(pca.components_)+1))\n    ax.set_xlabel('PCA components')\n    ax.set_ylabel('Explained Variance')\n    \n    fig.tight_layout()\n    pl.show()\n    ","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"4f34cc37-6708-4787-b39b-42738ed85c4d","_uuid":"daff97333256c5f13604354a9c3bc8556a2e94eb","collapsed":true}},{"source":"pca = PCA(n_components = 6).fit(features_cl_tr_cl)\nvs_plot_pca_variance(pca)","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"c63e6a03-c55d-48f5-8f83-b0e1523e1828","_uuid":"bab2b4250002a197417746d6bf2e9c9766e5fe65","collapsed":true}},{"source":"The above graph indicates that 92% of variance in the data can be achieved with just 6 dimensions instead of the 30 features. ","cell_type":"markdown","metadata":{"_cell_guid":"94930d4c-c060-4609-bb64-2c45f5ce3e06","_uuid":"3a713e9ef3cca313e3e5d4f447869544b4c8ccf2"}},{"source":"def vs_pca_results(good_data, pca, **kwargs):\n    cols = als_return_select_cols(good_data, which=kwargs['which'])\n    cols_indices = [i for i, j in enumerate(good_data.keys()) if j in cols]\n    # Dimension indexing\n    dimensions = ['Dimension {}'.format(i) for i in range(1,len(pca.components_)+1)]\n\n    # PCA components\n    components = pd.DataFrame(np.round(pca.components_[:,cols_indices], 4), columns = good_data.keys()[cols_indices])\n    components.index = dimensions\n\n    # PCA explained variance\n    ratios = pca.explained_variance_ratio_.reshape(len(pca.components_), 1)\n    variance_ratios = pd.DataFrame(np.round(ratios, 4), columns = ['Explained Variance'])\n    variance_ratios.index = dimensions\n\n    # Create a bar plot visualization\n    fig, ax = pl.subplots(figsize = (14,8))\n\n    # Plot the feature weights as a function of the components\n    components.plot(ax = ax, kind = 'bar');\n    ax.set_ylabel(\"Feature Weights\")\n    ax.set_xticklabels(dimensions, rotation=0)","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"26804ee9-abe4-4bdd-8e51-c405c05d4cba","_uuid":"1dc57f7d990bf6acfa1e96faafc63442ce0c0526","collapsed":true}},{"source":"# Generate PCA results plot\nvs_pca_results(features_cl_tr_cl, pca, which='only_dims')","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"429d21ce-726a-4b9d-a77b-1fc103ff9965","_uuid":"d42932eeb2943ab5c53494013f90862f662b099d","collapsed":true}},{"source":"vs_pca_results(features_cl_tr_cl, pca, which='mean_non_dims')","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"b81754ba-bd53-41c2-8a76-cc8e8d33e1ff","_uuid":"ea7765d7a27904ff5d41191ab5eee3ac8a2f9398","collapsed":true}},{"source":"vs_pca_results(features_cl_tr_cl, pca, which='se_non_dims')","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"5df67b9d-2267-45ba-851b-afac582f6f86","_uuid":"5b85e87fe095f049226b443289b96bf8ec111bde","collapsed":true}},{"source":"vs_pca_results(features_cl_tr_cl, pca, which='worst_non_dims')","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"ebfc04ab-5e88-4240-af8a-e6005a321103","_uuid":"e1b2d5a6c3e18d55753434cd2c96cee29f8b6ee3","collapsed":true}},{"source":"The above plots indicate the following:\n\n1. The first principal component dimension has all positive weights.\n\n2. The second principal component dimension has positive weights for all features except those related to cell dimensions (radius, perimeter, mean). The first two dimensions contribute to upto 72% variance. \n\n3. `texture_mean`, `texture_se` and `texture_worst`, all three contribute heavily to the third principal component dimension\n\n4. `radius, perimeter, area` \"mean\" and \"worst\" features contribute only to the first two principal component dimensions","cell_type":"markdown","metadata":{"_cell_guid":"13ac70ac-e2cd-4e68-b7a4-f1680f55fe11","_uuid":"34d116e903137d59495ba9291d500c2939496e6d"}},{"source":"def als_return_reduced_data(good_data, pca):\n    dimensions = ['Dimension {}'.format(i) for i in range(1,len(pca.components_)+1)]\n    reduced_data = pd.DataFrame(data=pca.transform(good_data), columns=dimensions)\n    return reduced_data","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"302757ff-b1fd-4bdc-a881-659a0bbd8904","_uuid":"e7bf9b1c4d88d7b31f0ae6b0d6b0e0474c2793b8","collapsed":true}},{"source":"reduced_features = als_return_reduced_data(features_cl_tr_cl, pca)\noutput_float = output_cl_tr_cl.apply(lambda x: als_encode_diagnosis(x))","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"e7e106bc-c41c-497b-b810-dd6235cdd07f","_uuid":"13858e0d554e83bf20e32a1f0122ce59a775c7a5","collapsed":true}},{"source":"def vs_scatter_two_dimensions(reduced_features, output_float):\n    fig, ax = pl.subplots(figsize=(8,5))\n    ax.scatter(reduced_features.loc[:,'Dimension 1'], reduced_features.loc[:, 'Dimension 2'], c=output_float, cmap='winter')\n    ax.set_xlabel('Dimension 1')\n    ax.set_ylabel('Dimension 2')\n    ax.set_title('Projections of features on first two principal components')\n    ax.set_xticklabels([])\n    ax.set_yticklabels([])\n    \n    fig.tight_layout()\n    pl.show()","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"b48b3770-f7b7-4e46-87d8-27ae83c0f530","_uuid":"01be55737714a7d9e39db52610b3e3ec747d8c64","collapsed":true}},{"source":"vs_scatter_two_dimensions(reduced_features, output_float)","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"504b477f-176d-41ef-a735-d11121387c2c","_uuid":"d7d4a33e60d67e25971e508204f97a6880cb72aa","collapsed":true}},{"source":"As can be seen from the above plots, quite a good classification is achieved with just 2 dimensions. It is not clear from the plots if adding the 3rd dimension improved classification. This can be known for sure only by applying classification models to the reduced features and analysing the scores. That is another reason why PCA is so important when dealing with data having a lot of features. It will minimize noise by only using the most important set of independent features","cell_type":"markdown","metadata":{"_cell_guid":"440e357d-2c16-4020-816f-8ed24cf71a3a","_uuid":"8b3531d47cd03e97612e3054fa4182e686cca961"}},{"source":"","cell_type":"markdown","metadata":{"_cell_guid":"b9cffaf2-046f-4031-876f-a2e899560494","_uuid":"a35f77aa0f21e774f50a2d1307bd842e1ba01fe1"}},{"source":"### Algorithms and Techniques","cell_type":"markdown","metadata":{"_cell_guid":"ce15a5a8-367d-4e8e-8aaa-7b244de9fd09","_uuid":"5dab3314c533373b647f9b0eb5df3a189d48b3a4"}},{"source":"The reduced features from above will now be used in a binary classification algorithm. There are many algorithms suitable for this problem and below, I have provided an analysis of a few of them that I think can be used. \n\n1) Gaussian Naive Bayes (GaussianNB)\n\n    * Naive Bayes method is a supervised learning alogrithm based on Bayes' theorem with the naive assumption of independence between various pairs of features. And Gaussian Naive Bayes method assumes the likelihood of the features to be Gaussian.\n    \n    One real-world application of this method is text prediction based on a sample text data. Given a test sentence, Naive-Bayes can predict the next word based on the word with the highest probability of occurence, as derived from the sample data. Subsequent words can similarly be predicted based on high probability words occurring next to the first predicted word and so on.\n\n    * The advantage of this algorithm is that it is simple, fast and requires relatively little training data. \n\n    * However, it is commonly known as a a bad algorithm due to its overly simplified assumption of independence between feature pairs. \n\n    * The problem in hand is binary classification. And the data we are using is the reduced feature matrix after applying PCA. This implies that the transformed features are independent of each other and thus it is likely that the algorithm successful. However, if the feature-output is not so simple to be captured by this Naive algorithm, it definitely wouldn't be suitable. Neverthless, it is worth giving this a shot. \n\n2) Decision Trees/Random forests\n\n    * Decision trees is a non-parametric model that can classify data based on a tree of decision rules. Random forests on the other hand, is an ensemble learning method, that operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. \n\n    One real-world application of this could be predicting the performance of a car with a series of labels such as - average, good, excellent, extra-ordinary etc. A car's performance is related to various features - like its make, mileage, size & speed of engine etc. By asking a series of decision rules on a number of these features - we might be able to build a pretty good model for predicting the car performance. \n    \n    * The advantages of this method are many. They are simple to understand, visualize and interpret. It requires little preparation, and it can handle both categorical and numerical variables without much pre-processing. \n\n    * While the performance of a decision tree might improve with the number of rules set, it could very easily run into a problem of overfitting, if the right questions are not asked. They are also unstable to small variations in data. And often they can create biased trees if some classes dominate. Random forests on the other hand has the advantage of not falling into the issue of overfitting\n\n    * Since the problem in hand is quite similar to the example provided above, this algorithm is quite worth a shot.\n \n3) K-Nearest Neighbors Classifier\n\n    * The k-nearest-neighbours algorithm is another \"lazy\" non-parametric model. Thus it can be used in the same stance as decision trees. \n    \n4) Support Vector Classifier\n\n    * Support vector machine constructs hyperplanes in infinite-dimensional spaces to achieve classification. Intuitively, a good separation is achieved by the hyperplane that has the largest distance to the nearest training-data point of any class (so-called functional margin), since in general the larger the margin the lower the generalization error of the classifier.\n\n    * The advantage of the method is very clear, in the sense that it is a very formal approach to classification. The disadvantage however is the high training time complexity (more than quadratic with the number of samples) which makes it makes it difficult to apply this algorithm to a couple of 10000 samples","cell_type":"markdown","metadata":{"_cell_guid":"6a048b60-2e10-48ea-b540-44442ac14763","_uuid":"408c117f4ab8b997dffa4babc4d6f506271486b9"}},{"source":"## Methodology","cell_type":"markdown","metadata":{"_cell_guid":"f008c532-b091-4579-a0fd-2567c90079f8","_uuid":"32a4ebf9f481815c35c275cfd542ff091d7fe0d5"}},{"source":"- Step 1:\n    I m going to begin with applying models on the reduced data from above:\n        1. Gaussian Naive Bayes (Since it is a simple model for initial testing)\n        2. Decision tree/random forests (since this is a classification problem)\n        3. Clustering (since it is a non-parametric approach as compared to sophisticated plane-separation approaches)\n        4. SVM/Logistic regression \n\n- Step 2:\n    Based on my understanding of the correlation maps above, I'll forcedly remove some features that I think are not important, apply PCA on the left over and use the above models agains to see if accuracy improves. (I can also use the feature selection modules during this process)\n    \n\n- Step 3:\n    I will pick the best 3 models from among steps 1 & 2. And I will fine-tune the model hyperparameters.\n    \n\n- Step 4:\n    I will also finally try some adaboost methods - which is a collection of weak learners on the selected features as well as on the reduced features. ","cell_type":"markdown","metadata":{"_cell_guid":"f6322f00-7f18-4f76-a96f-543c28899af8","_uuid":"5ce5c2bd868c8b0fdb8cfeefec3392d939a0e60c"}},{"source":"def als_print_evaluation_metrics(clf, x, y, scoring, cv=5, only_times=True, print_times=True):\n    scores = cross_validate(clf, x, y, cv=cv, scoring=scoring, return_train_score=True)\n    if print_times:\n        print('Average fit time is:   {:.3f}s'.format(np.mean(scores['fit_time'])))\n        print('Average score time is: {:.3f}s\\n'.format(np.mean(scores['score_time'])))\n    if not only_times:\n        print(' {: >7} {: >10} |  {: >3}    |  {: >3}    |  {: >3}    |'.format(' ', ' ', 'Avg', 'Min', 'Max'))\n        for f in ['train', 'test']:\n            for s in scoring:\n                key = [sc for sc in scores.keys() if re.search('{}(.){}'.format(f,s),sc)]\n                print(' {: >7} {: >10} |  {: >.3f}  |  {: >.3f}  |  {: >.3f}  |'.format(f, s, np.mean(scores[key[0]]), np.min(scores[key[0]]), np.max(scores[key[0]])))","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"19286cb9-bf3a-4fa7-8e51-60012a2355bf","_uuid":"69ef19e60bb0b41d85acf33f827aa233184e488d","collapsed":true}},{"source":"def vs_plot_evaluation_metrics(clfs, clf_labels, x, y, cv=5):\n    scoring = ['accuracy', 'precision', 'recall']\n    scores = {}\n    for label, clf in zip(clf_labels, clfs):\n        scores[label] = cross_validate(clf, x, y, cv=cv, scoring=scoring, return_train_score=True)\n    colors = ['b', 'g', 'r', 'k', 'c']\n    lab2 = ['Avg', 'Min', 'Max']\n    fig, ax = pl.subplots(2,3, figsize = (20,15))\n    for i, f in enumerate(['train', 'test']):\n        for j, s in enumerate(scoring):\n            minval=1\n            for k, lab1 in enumerate(clf_labels):\n                scs = scores[lab1]\n                key = [sc for sc in scs.keys() if re.search('{}(.){}'.format(f,s),sc)][0]\n                alphac = [1,0.2,0.5]\n                for l, lval in enumerate([np.mean(scs[key]), np.min(scs[key]), np.max(scs[key])]):\n                    lab = lab1 + ' ' + lab2[l]\n                    if lval < minval:\n                        minval = lval\n                    ax[i,j].bar(k+1+(0.23*l), lval, 0.23, color=colors[k], label=lab, alpha=alphac[l]) \n            ax[i,j].legend()\n            ax[i,j].set_ylim(minval-0.01,1)\n            ax[i,j].set_xlim(0,8)\n            ax[i,j].set_title('{} {} scores'.format(f,s))\n            ax[i,j].set_ylabel('Score')\n            ax[i,j].set_xticklabels([])\n    fig.tight_layout()\n    pl.show()","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"4d76425d-97cc-48c6-b087-75537e9ec308","_uuid":"7d875459fa053134891ad1c0b92fb3e7800e4e33","collapsed":true}},{"source":"from sklearn.model_selection import cross_val_score\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\n\nclf_GNB = GaussianNB()\nclf_RF = RandomForestClassifier()\nclf_KNN = KNeighborsClassifier()\nclf_SVM = SVC()\nclfs = [clf_GNB, clf_RF, clf_KNN, clf_SVM]\nclf_labels = ['GNB', 'RF', 'KNN', 'SVC']\nvs_plot_evaluation_metrics(clfs, clf_labels, reduced_features, output_float, cv=5)\nscoring=['accuracy', 'precision', 'recall', 'f1']\nfor label, clf in zip(clf_labels, clfs):\n    print('{}:'.format(label))\n    als_print_evaluation_metrics(clf, reduced_features, output_float, scoring, cv=5)","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"ee730828-955f-4920-8936-aea742d016a3","_uuid":"122dc94ff62a5959870323cf333a1511baa21a2d","collapsed":true}},{"source":"The above plot leads to the following conclusions:\n1. Gaussian Naive Bayes is the least performing of all as suspected\n2. Random Forests have the best training scores (sometimes even perfect), but the testing scores are not so good. This indicates a biased model\n3. KNN and SVC are comparable. \n    - KNN training accuracy scores seem higher than SVC. But KNN training precision scores are slightly lower than SVC\n    - It can be observed that the variance of scores across the folds (as seen from the mean, min, max values) is lower in KNN for the test recall scores, but the behavior is vice-versa for the test precision scores\n    - Also, KNN has low fitting time but higher score time and it is vice versa for SVC. However, in this case since the dataset is so small, it doesn't really matter\n\nIt would be wise to print the f-beta test scores for KNN and SVC models to make the final decision on best model","cell_type":"markdown","metadata":{"_cell_guid":"9bc512d9-ed0c-4c95-ac95-87761c49ffb1","_uuid":"05f4ab6330f1919e7af284b4a0b961150a6dc6ab"}},{"source":"scoring=['f1']\nfor label, clf in zip(['KNN', 'SVC'], [clf_KNN, clf_SVM]):\n    print('{}:'.format(label))\n    als_print_evaluation_metrics(clf, reduced_features, output_float, scoring, cv=5, only_times=False, print_times=False)\n    print('\\n')","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"aa46c925-fd03-4cc4-aa8c-a4ac73f7d3ac","_uuid":"2beb98b637275ec59491e8d0def53f5a2c1514df","collapsed":true}},{"source":"The next step of the strategy is to remove some of the features observed from data exploration/visualization (i.e. feature selection), followed by PCA transformation and use of classifier model on that. ","cell_type":"markdown","metadata":{"_cell_guid":"22da4444-4dc3-4949-9207-7239217d73c6","_uuid":"ee59651d2b4a92fdee558cd616cf023d2bffad5a"}},{"source":"cols = ['radius_mean', 'radius_se', 'radius_worst', 'perimeter_mean', 'perimeter_se', 'perimeter_worst', 'area_se', 'area_worst', 'smoothness_se', 'compactness_se', 'concave points_se', 'concavity_se', 'symmetry_se']\nselected_features = pd.DataFrame(features_cl_tr_cl)\nfor col in cols:\n    selected_features = selected_features.drop([col],axis=1)\npca = PCA(n_components = 6).fit(selected_features)\nvs_plot_pca_variance(pca)","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"f33ea415-c064-4092-83b2-7867dc517126","_uuid":"e2ec3adfb0ac59d063eb77f0f43f63eca5bc7f38","collapsed":true}},{"source":"selected_reduced_features = als_return_reduced_data(selected_features, pca)\nclf_GNB = GaussianNB()\nclf_RF = RandomForestClassifier()\nclf_KNN = KNeighborsClassifier()\nclf_SVM = SVC()\nclfs = [clf_GNB, clf_RF, clf_KNN, clf_SVM]\nclf_labels = ['GNB', 'RF', 'KNN', 'SVC']\noutput_float = output_cl_tr_cl.apply(lambda x: als_encode_diagnosis(x))\nvs_plot_evaluation_metrics(clfs, clf_labels, selected_reduced_features, output_float, cv=5)","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"58f3d3df-50ae-49a8-bfc3-ef4cf79c412f","_uuid":"dbe6320f4c8d11bf20405028f07538517b6d5ed3","collapsed":true}},{"source":"The results do not indicate any model improvement, rather there might be a performance drop due to the selection of features before transformation. There is also an indication of overfitting, since training scores are much better, while testing score are worse.","cell_type":"markdown","metadata":{"_cell_guid":"0a2e1cd0-4175-4d6d-afed-a82fd182c97e","_uuid":"ceb0265d29d4255ada40e956285daefeda552715"}},{"source":"The final step for model refinement is fine tuning of the model hyper parameters. ","cell_type":"markdown","metadata":{"_cell_guid":"45660f1f-f39a-4ba2-a8dd-bbd6597693a8","_uuid":"9865564295a383fe8765b8ee09f80be95247779a","collapsed":true}},{"source":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import accuracy_score, fbeta_score, precision_score, make_scorer\nfrom sklearn.model_selection import ShuffleSplit\n\nknn = KNeighborsClassifier()\nparameters = {'n_neighbors':list(range(2,7)), 'algorithm':['auto', 'ball_tree', 'kd_tree', 'brute']}\nscoring=make_scorer(fbeta_score, beta=0.5)\nclf = GridSearchCV(knn, parameters, scoring=scoring, cv=5)\nclf.fit(reduced_features, output_float)\nresults_pd = pd.DataFrame(clf.cv_results_)","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"0cb8820c-62d8-427f-8e42-1003d4b7ff2d","_uuid":"b8bba847d6d89ad152a879dff339d5041e057637","collapsed":true}},{"source":"select_cols = ['mean_train_score', 'mean_test_score', 'param_algorithm', 'param_n_neighbors', 'rank_test_score', 'std_test_score']\nresults_pd[select_cols].sort_values(['rank_test_score'], ascending=True).reset_index(drop=True).head(8)","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"958e7c2b-c4d8-4494-aa30-eabca33563a1","_uuid":"70cefa7b3a1a432ba4ae149cd0eb321f4c90b4ae","collapsed":true}},{"source":"The results indicate that all algorithms give same result. Only the `n_neighbors` parameter has any significant effect on the scores","cell_type":"markdown","metadata":{"_cell_guid":"aae8c183-1ff2-4495-9f7a-7da154051e8c","_uuid":"260c9c9a5e72a0fb2416a4d85da383bd55ebbb9f"}},{"source":"best_clf = KNeighborsClassifier(n_neighbors=4, algorithm='auto')\nnext_best_clf = KNeighborsClassifier(n_neighbors=5, algorithm='auto')","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"274a538f-a890-47d6-81ec-018a00e1e286","_uuid":"23e719ca9f3538cd4eeff60a75262e07d60a50f8","collapsed":true}},{"source":"Challenges faced during coding:\n\nThe application of classification models was easy due to classes available for each of them in scikit. So the real challenge was in the section above - which composes of data preprocessing, visualization, feature selection, transformation etc. Also, since the features were all numerical, the only thing I had to do was encode the diagnosis to a float value. So the challenges in coding this project was actually quite minimal. ","cell_type":"markdown","metadata":{"_cell_guid":"ea4ffde5-4e04-4f4b-8941-d401dc183d74","_uuid":"edaca5436852dd73b3d69262d484b95dc29d719e"}},{"source":"## Results","cell_type":"markdown","metadata":{"_cell_guid":"39422b9c-4189-4a92-a7bc-6b7b5f7abde6","_uuid":"60a8843bdfd37f72788624cde3ced413ba7bbe7a"}},{"source":"scoring=['accuracy', 'precision', 'recall', 'f1']\nprint('Best obtained from grid search:\\n')\nals_print_evaluation_metrics(best_clf, reduced_features, output_float, scoring, only_times=False)\nprint('\\nSecond best model from grid search, it has lower variance of test scores across folds:\\n')\nals_print_evaluation_metrics(next_best_clf, reduced_features, output_float, scoring, only_times=False)","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"df4bfd45-59a9-4635-b4e7-b8f0ed92e009","_uuid":"d295775b1464f6e82c77830ecc68f6fb3c44e1af","collapsed":true}},{"source":"#### Robustness of model\n\nIn this section, I am going to further test the model of some other data. Now since I have no other sources, I am going to do the following:\n\n1. Corrupt the output, i.e., change some `benign` diagnoses to `malign`. \n2. We will know that the model is robust if the above change results in more false negatives\n\n","cell_type":"markdown","metadata":{"_cell_guid":"b6530849-69f4-49df-9d22-6bedad3605eb","_uuid":"bca1d95c6ae19b84df4f43884e8ee4f6f73b1f18"}},{"source":"def als_corrupt_output(output_float, f):\n    positives = output_float[output_float==1]\n    turnovers = int(len(positives)*f)\n    turnover_index = np.random.choice(positives.index, turnovers)\n    output_float[turnover_index] = 0\n    return output_float\n               ","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"89fe197e-a987-4697-847d-c4d4e034e21a","_uuid":"63b2573766ce010c72ad75b283cae0702ddc6988","collapsed":true}},{"source":"from sklearn.cross_validation import train_test_split\nfrom sklearn.metrics import confusion_matrix \n# Split the 'features' and 'income' data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(reduced_features, \n                                                    output_float, \n                                                    test_size = 0.2, \n                                                    random_state = 0)\nbest_clf.fit(X_train, y_train)\ny_preds = best_clf.predict(X_test)\nprint('Confusion matrix of best model tested on original testing data:')\nprint(pd.DataFrame(confusion_matrix(y_test, y_preds), columns=['TP', 'FN'], index=['FN', 'TN']))\nprint('\\n')\nprint('Confusion matrix of best model tested on testing data corrupted with false benign diagnoses, by 30%:')\ny_test_corrupted = als_corrupt_output(y_test, 0.3)\nprint(pd.DataFrame(confusion_matrix(y_test_corrupted, y_preds), columns=['TP', 'FN'], index=['FN', 'TN']))\n","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"eda5e098-6819-45b8-985f-f542e5eedbe4","_uuid":"c522f64901a6720bcf6efe7227d955eda8e71bb3","collapsed":true}},{"source":"The trend is just as expected. The number of false negatives have gone up. This gives me the confidence that the model performance is not random, rather it has clearly recognized the \"hyper-plane of separation\" between the 2 classes. More experiments like these can be conducted.\n","cell_type":"markdown","metadata":{"_cell_guid":"e7181318-5832-4676-81a2-f09392f2da58","_uuid":"4474af75124b7b6b532651e4536e60e94bba324e"}},{"source":"## Conclusion","cell_type":"markdown","metadata":{"_cell_guid":"75a437c4-dc53-4faf-9100-86b2751d3123","_uuid":"c1a790408165857c25d9492a331693523bb198bc"}},{"source":"Thus, as I had set out, I have addressed the binary classification problem of cancer diagnosis from FNA tests, based on the following strategy \n\n1. I did an extensive data exploratory and visualization analysis of the 30 features constituting the test results. \n    - Log transformation and min max scaling was applied\n    \n    - Outliers were detected based on points lying outside the interquartile range. Points that were identified as outliers in the most number of features were dropped. This process resulted in losing about 4% of the data. And the loss was more or less equally distributed, this maintaining the class balance. \n    \n    - Violin plots, which are kernel density estimation plots divided by class type were plotted for all the features. It allowed to visualize which features were most likely to affect classification. \n    \n    - Correlation heat maps were also plotted to further identify the relation between the features\n   \n    - The above visualization studies would serve as a guide for heuristic feature selection    \n\n2. Following the above analysis, I attempted feature transformation based on PCA. The feature weights composing the first 6 principal component dimensions were represented. And scatter plots of first two and three dimensions were also plotted to visualize the separation achieved with just 3 transformed features. \n\n3. After feature transformation, I used the cleaned, reduced, transformed features in classification algorithms like Gaussian Bayes, Random Forests, (non-parametric) K-nearest neighbors and support vector classifiers. The classifiers' scores were estimated across 5 cross validation folds, and the mean, worst and best values of them were plotted to compare them. The KNN classifier came first among them on various aspects\n\n4. I also tried to drop features that were observed as not useful from the first step and then followed it with a PCA and classification. It seemed that dropping those features actually resulted in a small drop of the results \n\n5. Model hyper parameters were also optimized by a simple grid search algorithm as available in sklearn scikit \n\n6. The final test scores that I have are accuracy: 0.973, precision: 0.995 and recall: 0.93. \n\n7. I have also printed out a second-best classifier which has slightly lower scores, but also lower variance of testing scores across the cross validation splits. However, in order to make strong conclusions out of this, it is defnitely necessary to have more data points","cell_type":"markdown","metadata":{"_cell_guid":"d6db7106-81f3-48fc-8af2-0f426a2f8d4c","_uuid":"428a123f643830ee1c4b0fb8065f18232739e049"}},{"source":"#### Free form visualization\n\nTo understand the significance of the data exploratory and transformatory analyses, I have shown 3 different scatter plots:\n1. Scatter plot of 3 'non-significant' features (`smoothness_mean`, `concavity_mean`, `compactness_se`). I am calling these non-significant for many reasons - they did not contribute much to the first few principal component dimensions, they did not seem to determine classification (as was seen from the violin plots) and they seemed to be correlated as seen from the heat maps as well \n\n2. Scatter plot of 3 'significant' features (`area_mean`, `texture_mean`, `fractal_dimension_mean`) for all of the opposite reasons mentioned above\n\n3. Scatter plot of first 3 principal component dimensions.\n\nAs can be seen in the plots, the separation of malign/benign cases progressively gets better. Visually it can be seen it will be easiest to find the separating hyperplane in the third case. Thus the more un-correlated the features are, the easier it is to separate the output. This is in my opinion, the single key to this problem. ","cell_type":"markdown","metadata":{"_cell_guid":"acd9a68b-19b6-498c-b616-cc3b6d19bdb7","_uuid":"25e4965b5f261863cf0dbb3ff1863e788b072078"}},{"source":"def vs_biplot(good_data, reduced_data, output_float, pca):\n    '''\n    Produce a biplot that shows a scatterplot of the reduced\n    data and the projections of the original features.\n    \n    good_data: original data, before transformation.\n               Needs to be a pandas dataframe with valid column names\n    reduced_data: the reduced data (the first two dimensions are plotted)\n    pca: pca object that contains the components_ attribute\n\n    return: a matplotlib AxesSubplot object (for any additional customization)\n    \n    This procedure is inspired by the script:\n    https://github.com/teddyroland/python-biplot\n    '''\n\n    fig = pl.figure(figsize = (22,10))\n    ax1 = fig.add_subplot(1,3,3, projection='3d')\n    # scatterplot of the reduced data    \n    xs = reduced_data.loc[:, 'Dimension 1']\n    ys = reduced_data.loc[:, 'Dimension 2']\n    zs = reduced_data.loc[:, 'Dimension 3']\n    \n    ax1.scatter(xs, ys, zs, c=output_float, cmap='winter')\n    feature_vectors = pca.components_.T\n\n    # we use scaling factors to make the arrows easier to see\n    arrow_size, text_pos = 5.6, 6\n\n    # projections of the original features\n    for i, v in enumerate(feature_vectors):\n        ax1.plot([0, arrow_size*v[0]], [0, arrow_size*v[1]], [0, arrow_size*v[2]], lw=1.5, color='red')\n        ax1.text(v[0]*text_pos, v[1]*text_pos, v[2]*text_pos, good_data.columns[i], color='black', \n                 ha='center', va='center', fontsize=14)\n\n    ax1.set_xlabel(\"Dimension 1\", fontsize=14)\n    ax1.set_ylabel(\"Dimension 2\", fontsize=14)\n    ax1.set_zlabel(\"Dimension 3\", fontsize=14)\n\n    ax1.set_title(\"Scatter on first 3 PCs\", fontsize=18);\n    \n    ax2 = fig.add_subplot(1,3,1, projection='3d')\n    cols = ['smoothness_mean', 'concavity_mean', 'compactness_se']\n    # scatterplot of the reduced data    \n    xs = good_data.loc[:, cols[0]]\n    ys = good_data.loc[:, cols[1]]\n    zs = good_data.loc[:, cols[2]]\n    \n    ax2.scatter(xs, ys, zs, c=output_float, cmap='winter')\n    ax2.set_xlabel(cols[0], fontsize=14)\n    ax2.set_ylabel(cols[1], fontsize=14)\n    ax2.set_zlabel(cols[2], fontsize=14)\n\n    ax2.set_title(\"Scatter on any 3 'non-significant' features\", fontsize=18);\n    \n    ax3 = fig.add_subplot(1,3,2, projection='3d')\n    cols = ['area_mean', 'texture_mean', 'fractal_dimension_mean']\n    # scatterplot of the reduced data    \n    xs = good_data.loc[:, cols[0]]\n    ys = good_data.loc[:, cols[1]]\n    zs = good_data.loc[:, cols[2]]\n    \n    ax3.scatter(xs, ys, zs, c=output_float, cmap='winter')\n    ax3.set_xlabel(cols[0], fontsize=14)\n    ax3.set_ylabel(cols[1], fontsize=14)\n    ax3.set_zlabel(cols[2], fontsize=14)\n\n    ax3.set_title(\"Scatter on any 3 'significant' features\", fontsize=18);\n    \n    fig.tight_layout()\n    pl.show()\n    ","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"01f63fc5-4397-4e7f-8e89-3e1ff92de7fb","_uuid":"a4cb76073c0a8e121c5ab6ba31ef78e4d6686962","collapsed":true}},{"source":"vs_biplot(features_cl_tr_cl, reduced_features, output_float, pca)","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"2e283eb7-18aa-47cf-8219-9c281e8e4882","_uuid":"87ff250d67c7d6c7ee2aa022571312fe180081d6","collapsed":true}},{"source":"#### Reflection on the challenges faced\n\nAs I have repeatedly emphasized throughout, this project is about understanding the features, twisting and turning them and literally, as an analogy, being able to identify who are the \"guys\" in-charge. While this is straight-forward by using principal component analysis, the result is transformed dimensions that does not make much physical sense. On the other hand, analyses of the significant features based on the data visualization exercises proved to be very difficult, just because there were so many and it was difficult to come at a robust conclusion. This was evident at the step where I observed higher classification error when only some features were selected, I obviously did not drop the best candidates. \n\nAnother problem I faced was the size of the dataset. It was too small for me to identify and reject most outliers. It was also too small for me to attempt cross-validation with higher number of folds, as more folds implied significantly reducing the test-set size, resulting more variance in the testing scores across folds. \n\n#### Improvement\n1. A more useful analysis would be to perform feature selection using a simple classifier, maybe a decision tree. This would allow us to further explore the effects of feature selection before feature tranformation \n\n2. Deep learning algorithms could be tested as well, as it appears that the decision boundary between malign and benign cases is very non-linear. But again the dataset size is very small.\n\n3. Generally there is a limitation of data. There are only a total of 500 odd points. This presents a challenge in  analysing the effects of overfitting as well as the effects of increasing the number of folds in cross validation etc. ","cell_type":"markdown","metadata":{"_cell_guid":"7c5665b7-676c-48a5-a634-005c90cc0aef","_uuid":"50f0649d08dce566a5f6638f5abfa8ad9aaf235a","collapsed":true}}],"metadata":{"kernelspec":{"name":"python3","language":"python","display_name":"Python 3"},"language_info":{"name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","version":"3.6.3","mimetype":"text/x-python"}}}