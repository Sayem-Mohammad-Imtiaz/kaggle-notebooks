{"cells":[{"metadata":{},"cell_type":"markdown","source":"# EDA and Choosing Best Classifier including Hyperparameters \n\nBelow I will go through a realtively simple but comprehensive pipeline of how to properly analyze, process and impute data and, apply Machine Learning algorithms to perform binary classification. Most important points or perks of the below notebooks are \n\n1. Imputing Missing Data. \n    \n    1.1. Imputing via Mean of Feature and via Interpolation. \n\n2. Learn about the Dataset through properly analyzing distributions. \n    \n    2.1. Feature Distribution based on Outcome. \n\n    2.2. Feature distribution using Pair Plot of Seaborn.  \n    \n    2.3. Are there outliers? Using Box plots.\n\n\n3. Learn to simply deal with Outliers. \n\n\n4. Apply and Test Several ML Algorithms. \n    \n    4.1. Create a pipeline of standardizing the features and ML Algorithms. \n    \n    4.2. Select the best hyperparameter for each model. \n    \n    4.3. Store the models and corresponding scores in a dataframe. \n\nEnjoy!    "},{"metadata":{},"cell_type":"markdown","source":"## 1. Necessary Imports "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import time\nimport numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Load the CSV File with Pandas  "},{"metadata":{"trusted":true},"cell_type":"code","source":"diab_df = pd.read_csv(\"../input/pima-indians-diabetes-database/diabetes.csv\")\ndiab_df.head(6)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. Check Basic Information About the Dataframe "},{"metadata":{"trusted":true},"cell_type":"code","source":"print ('dataframe shape: ', diab_df.shape)\nprint ('null values in the entire dataframe: ', diab_df.isnull().values.any()) # check for NaNs\nprint ('total number of null values: ', diab_df.isnull().sum().sum()) # total number of NaNs\nprint ('number of positive (1) and negative (0) diabetic patients: ', '\\n', diab_df.Outcome.value_counts())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that Number of negative patients are higher (almost twice) than positive patient. Later when we split the data for training and testing, we would like to keep this in mind. \nFor now, let's change the column name 'DiabetesPedigreeFunction' as this is a rather long name. Change it to something short. "},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"diab_df.rename(columns={'DiabetesPedigreeFunction': 'DiabPedgFunct'}, inplace=True)\n\nprint ('check dataframe columns :', diab_df.columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4. Knowing the Data (Data Analysis)\n\n_So the dataframe has no null (NaN) values_. Don't get fooled by this though, because as you can see at least for the first 6 rows, 'Insulin' and 'SkinThickness' have entries with value 0. These are impossible entries, so we have to test ways to impute those zero values. \n\nSo first we check for 0 values in specific columns (Glucose, BloodPressure, SkinThickness, Insulin, BMI) where measuring 0 does not make sense (should be treated as missing value). We can check later that Age column has no 0 values.  We will proceed step by step. \n\n### 4.1. Fraction of Missing Values (How much data aren't available)  "},{"metadata":{"trusted":true},"cell_type":"code","source":"glucose_val_count0 = diab_df['Glucose'].value_counts()[0]\n# print (glucose_val_count0)\n\nbp_val_count0 = diab_df['BloodPressure'].value_counts()[0]\n# print (bp_val_count0)\n\nskin_th_count0 = diab_df['SkinThickness'].value_counts()[0]\n# print (skin_th_count0)\n\ninsulin_count0 = diab_df['Insulin'].value_counts()[0]\n# print(insulin_count0)\n\nBMI_count0 = diab_df['BMI'].value_counts()[0]\n# print(BMI_count0)\n\n# Age_count0 = diab_df['Age'].value_counts()[30]\n# print (Age_count0) # no zero values for age, gives a keyerror \n\nval_list0 = [glucose_val_count0/diab_df.shape[0], bp_val_count0/diab_df.shape[0], skin_th_count0/diab_df.shape[0], \n             insulin_count0/diab_df.shape[0], BMI_count0/diab_df.shape[0]]\n\nlabels0 = ['Glucose', 'BP', 'SkinThick', 'Insulin', 'BMI'] \nx = np.arange(len(labels0))\n\nfig = plt.figure(figsize=(6, 5))\nplt.bar(x, height=val_list0, width=0.4, align='center', color='magenta', alpha=0.7)\nplt.xticks(ticks=x, labels=labels0, fontsize=12)\nplt.title('Fraction of Missing Values', fontsize=14)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see here that for 'Insulin' almost half the values are missing.  "},{"metadata":{},"cell_type":"markdown","source":"### 4.2. Check Distribution of Features \n\nBelow we will plot _Histogram Plots_ of the features to see how they are distributed. "},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(12, 7))\nfig.add_subplot(241)\nplt.hist(diab_df['Age'], bins=int(np.sqrt(diab_df.shape[0])), density= True, color='lime', alpha=0.6)\nplt.xlabel('Age', fontsize=12)\n\nfig.add_subplot(242)\nplt.hist(diab_df['BloodPressure'], bins=int(np.sqrt(diab_df.shape[0])), density= True, color='lime', alpha=0.6)\nplt.xlabel('Blood Pressure', fontsize=12)\n\nfig.add_subplot(243)\nplt.hist(diab_df['BMI'], bins=int(np.sqrt(diab_df.shape[0])), density= True, color='lime', alpha=0.6)\nplt.xlabel('BMI', fontsize=12)\n\nfig.add_subplot(244)\nplt.hist(diab_df['DiabPedgFunct'], bins=int(np.sqrt(diab_df.shape[0])), density= True, color='lime', alpha=0.6)\nplt.xlabel('Diabetes Pedigree Function', fontsize=12)\n\nfig.add_subplot(245)\nplt.hist(diab_df['Glucose'], bins=int(np.sqrt(diab_df.shape[0])), density= True, color='lime', alpha=0.6)\nplt.xlabel('Glucose', fontsize=12)\n\nfig.add_subplot(246)\nplt.hist(diab_df['Insulin'], bins=int(np.sqrt(diab_df.shape[0])), density= True, color='lime', alpha=0.6)\nplt.xlabel('Insulin', fontsize=12)\n\nfig.add_subplot(247)\nplt.hist(diab_df['Pregnancies'], bins=int(np.sqrt(diab_df.shape[0])), density= True, color='lime', alpha=0.6)\nplt.xlabel('Pregnancies', fontsize=12)\n\nfig.add_subplot(248)\nplt.hist(diab_df['SkinThickness'], bins=int(np.sqrt(diab_df.shape[0])), density= True, color='lime', alpha=0.6)\nplt.xlabel('Skin  Thickness', fontsize=12)\n\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.3. Strategy to Impute Missing Values\n#### Try Imputing with _Mean Value_ of Feature\n\nWe can once again verify from the histogram plots that Insulin and SkinThickness features have lots of zero values. Also BloodPressure, BMI, Glucose and SkinThickness features follow nearly _Normal_ distribution. So we start to replace the null values with the distribution mean (in case of a perfectly normal distribution the mean, median and mode should be same). We will use DataFrame [replace](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.replace.html) method. However, since Insulin and SkinThickness have lots of missing values, this strategy may not be good enough. \n\nTo get started we first replace the zero values with NaNs so that handling them later would be easier.   "},{"metadata":{"trusted":true},"cell_type":"code","source":"diab_df[['Glucose','BloodPressure','SkinThickness','Insulin','BMI']] = diab_df[['Glucose','BloodPressure','SkinThickness','Insulin','BMI']].replace(0, np.NaN)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# replace using mean \n\ndiab_df['BMI_New'] = diab_df['BMI'].replace(np.NaN, diab_df['BMI'].mean())\ndiab_df['BloodPressure_New'] = diab_df['BloodPressure'].replace(np.NaN, diab_df['BloodPressure'].mean())\ndiab_df['Glucose_New'] = diab_df['Glucose'].replace(np.NaN, diab_df['Glucose'].mean())\ndiab_df['Insulin_New'] = diab_df['Insulin'].replace(np.NaN, diab_df['Insulin'].mean())\ndiab_df['SkinThickness_New'] = diab_df['SkinThickness'].replace(np.NaN, diab_df['SkinThickness'].mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We plot the distributions again to see after imputation with the mean value, how those distributions look like. "},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(10, 8))\n\nfig.add_subplot(231)\nplt.hist(diab_df['BMI_New'], density=True, bins=int(np.sqrt(diab_df.shape[0])), color='lime', alpha=0.7)\nplt.xlabel('BMI', fontsize=12)\n\nfig.add_subplot(232)\nplt.hist(diab_df['BloodPressure_New'], density=True, bins=int(np.sqrt(diab_df.shape[0])), color='lime', alpha=0.7)\nplt.xlabel('BP', fontsize=12)\n\nfig.add_subplot(233)\nplt.hist(diab_df['Glucose_New'], density=True, bins=int(np.sqrt(diab_df.shape[0])), color='lime', alpha=0.7)\nplt.xlabel('Glucose', fontsize=12)\n\nfig.add_subplot(234)\nplt.hist(diab_df['Insulin_New'], density=True, bins=int(np.sqrt(diab_df.shape[0])), color='lime', alpha=0.7)\nplt.xlabel('Insulin', fontsize=12)\n\nfig.add_subplot(235)\nplt.hist(diab_df['SkinThickness_New'], density=True, bins=int(np.sqrt(diab_df.shape[0])), color='lime', alpha=0.7)\nplt.xlabel('Skin Thickness', fontsize=12)\n\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we see the problem of imputing with mean value when a lot of values of a particular feature are missing. Even though the distributions for BMI, BP and Glucose looks reasonable, distributions of Insulin and SkinThickness resemble a lot with [Cauchy distribution](https://en.wikipedia.org/wiki/Cauchy_distribution) with low $\\gamma$ value. They just look abnormal because of our imputing strategy. So for these 2 distributions we have to take a different strategy.  \n\n#### Impute Using _Interpolation._ \n\nFor Skin thickness and Insulin we do a linear interpolation to replace the NaN values. This method is available for Pandas Series. [_Interpolation_](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.interpolate.html). Pandas has a nice [documentation](https://pandas.pydata.org/pandas-docs/stable/user_guide/missing_data.html) about different ways to handle missing data. \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"diab_df = diab_df.astype(float)\ndiab_df['SkinThickness_New1'] = diab_df.SkinThickness.interpolate(method='linear', limit=400, limit_direction='both')\ndiab_df['Insulin_New1'] = diab_df.Insulin.interpolate(method='linear', limit=600, limit_direction='both')\n\n\nfig = plt.figure(figsize=(6, 5))\n\nfig.add_subplot(121)\nplt.hist(diab_df['SkinThickness_New1'], density=True, color='lime', alpha=0.6)\nplt.xlabel('Skin Thickness', fontsize=12)\n\nfig.add_subplot(122)\nplt.hist(diab_df['Insulin_New1'], density=True, color='lime', alpha=0.7)\nplt.xlabel('Insulin_New1', fontsize=12)\n\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### check the min and max values of the above 2 features. \n\nprint ('Insulin max and min; ', max(diab_df['Insulin_New1']) , min(diab_df['Insulin_New1']))\nprint ('Skin Thickness max and min; ', max(diab_df['SkinThickness_New1']), min(diab_df['SkinThickness_New1']))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After successful imputation we only keep the relevant columns for our analysis. "},{"metadata":{"trusted":true},"cell_type":"code","source":"#### select the relevant features\ndiab_df_selected = diab_df[['Pregnancies', 'Glucose_New', 'BloodPressure_New', 'SkinThickness_New1', 'Insulin_New1',\n       'BMI_New', 'DiabPedgFunct', 'Age', 'Outcome']]   ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.4. Distribution of Features Based on Outcome\n\nBelow we would like to see how each feature is distributed based on the Outcome i.e. positive or negative diabetic patients. \nIf we see a(some) feature(s) where positive and negative patients have widely separated histograms, we can say that that feature plays an important role to classify the patient.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"features = diab_df_selected.drop(['Outcome'], axis=1)\n\nfeatures_arr = features.to_numpy()\nfeature_names_list = features.columns.to_list()\n\npositive_diab = features_arr[diab_df_selected.Outcome==1]\nnegative_diab = features_arr[diab_df_selected.Outcome==0]\n\nfig,axes =plt.subplots(4,2, figsize=(10, 8))\nax = axes.ravel()\n\nfor i in range(8):\n    _,bins= np.histogram(features_arr[:, i], bins=int(np.sqrt(768)) )\n    # plt.close()\n    ax[i].hist(positive_diab[:, i], bins=bins, histtype='stepfilled', edgecolor='red', linewidth=1.2, fill=False, alpha=0.8,)\n    ax[i].hist(negative_diab[:, i], bins=bins, color='green', alpha=0.6)\n    ax[i].set_title(feature_names_list[i],fontsize=12)\nax[0].legend(['Positive','Negative'],loc='best',fontsize=11)\nplt.tight_layout()\nplt.show() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the age distribution we can categorically say that young people have less chance of diabetes.  But there are no such particular feature where we see a wide separation between the two different classes. So for the machine learning part we may need to include all the features to classify the patients. \n\nThis will be even more meaningful and prominent if we plot the correlation plot of different features. \n\n### 4.5. Feature Correlation  "},{"metadata":{"trusted":true},"cell_type":"code","source":"# features = list(diab_df_selected.columns[0:7])\n# feature_corr = diab_df_selected[features].corr() # alternate way \n\n\nfeature_corr = features.corr() \n\nfig = plt.figure(figsize=(10, 7))\ng1 = sns.heatmap(feature_corr, cmap='coolwarm', vmin=0., vmax=1., )\ng1.set_xticklabels(g1.get_xticklabels(), rotation=40, fontsize=10)\ng1.set_yticklabels(g1.get_yticklabels(), rotation=40, fontsize=10)\nplt.title('Correlation Plot of Features', fontsize=12)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Correlation between the variables are very low. Some moderate correlation between Age and Pregnancy and, BMI_New and BloodPressure_New exist.  \nThis also tell us that we can ignore proceeding via PCA (this is for ML part), because we want all our features to be present. \n\nWe can see the feature dependence in detail with the pair plot. "},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(font_scale=1.2)\nsns.pairplot(diab_df_selected, hue='Outcome', palette='Set2')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.6. Are there Outliers ? How to Deal with Them?  \n\nTo understand the feature distribution we will now consider [Box Plots](https://en.wikipedia.org/wiki/Box_plot) and mainly focus on the outliers. "},{"metadata":{"trusted":true},"cell_type":"code","source":"f, axes = plt.subplots(2, 3, figsize=(12, 8))\n\nsns.set(font_scale=0.8)\nsns.boxplot(x=diab_df_selected['Age'], ax=axes[0][0],)\nsns.boxplot(x=diab_df_selected['Glucose_New'], ax=axes[1][0])\nsns.boxplot(x=diab_df_selected['Insulin_New1'], ax=axes[1][1]) ### many outliers !!!\nsns.boxplot(x=diab_df_selected['BMI_New'], ax=axes[0][1])\nsns.boxplot(x=diab_df_selected['BloodPressure_New'], ax=axes[0][2])\nsns.boxplot(diab_df_selected['Pregnancies'], ax=axes[1][2])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see apart except Glucose, all features have outliers. Specially Insulin have lots of outliers. We can use dataframe [describe](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.describe.html) method to get the info about Insulin features.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"print (diab_df_selected['Insulin_New1'].describe())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see $75\\%$ count is reached for a value of 190.13 whereas the maximum value is at 846.0, no wonder we have tons of outliers. "},{"metadata":{},"cell_type":"markdown","source":"#### Select Features Only When Z score is less than 3$\\sigma$. \n\nConsider $3\\sigma$ standard deviation, so that 99.7% data are included and everything beyond that would be neglected. This we do for all features.    "},{"metadata":{"trusted":true},"cell_type":"code","source":"diab_df_selected_Zscore = diab_df_selected[(np.abs(stats.zscore(diab_df_selected)) < 3).all(axis=1)]\n\nprint ('check new dataframe shape after rejecting outliers: ', diab_df_selected_Zscore.shape) # 50 rows are gone","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's plot the distributions again to see whether now the distributions have less outliers or not.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"f, axes = plt.subplots(2, 3, figsize=(12, 8))\n\nsns.set(font_scale=0.8)\nsns.boxplot(x=diab_df_selected_Zscore['Age'], ax=axes[0][0],)\nsns.boxplot(x=diab_df_selected_Zscore['Glucose_New'], ax=axes[1][0])\nsns.boxplot(x=diab_df_selected_Zscore['Insulin_New1'], ax=axes[1][1]) \nsns.boxplot(x=diab_df_selected_Zscore['BMI_New'], ax=axes[0][1])\nsns.boxplot(x=diab_df_selected_Zscore['BloodPressure_New'], ax=axes[0][2])\nsns.boxplot(diab_df_selected_Zscore['Pregnancies'], ax=axes[1][2])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5. Testing Machine Learning Algortihms\n\nAfter feature engineering, we are now ready to prepare our data for testing various ML algorithms.  \n\n### 5.1. Select Features and Labels "},{"metadata":{"trusted":true},"cell_type":"code","source":"Outcome_arr = diab_df_selected_Zscore['Outcome'].to_numpy()\nfeatures_Zscore = diab_df_selected_Zscore.drop(['Outcome'], axis=1)\nfeatures_Zscore_arr = features_Zscore.to_numpy()\n\nprint ('check shapes for features and outcome: ', features_Zscore_arr.shape, Outcome_arr.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5.2. Split the Data in Train and Test Set \n\nSince the positive and negative samples are not equally distributed, we will use stratify based on outcomes. "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(features_Zscore_arr, Outcome_arr, test_size=0.20, random_state=42, shuffle=True, stratify=Outcome_arr)\n\nprint ('check shape of training data: ', X_train.shape, y_train.shape)\nprint ('check shape of test data: ', X_test.shape, y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5.3. Necessary Import for Machine Learning Part "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import Pipeline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5.4. Create a List of Classifiers  \n\nFor classification task, we would like to check the following classifiers\n\n* Support Vector Machine. \n* Logistic Regression. \n* Random Forest. \n* Adaboost (Base Classifier as decision tree). \n* Naive Bayes.  \n\nApart from the classifier since we would also like to pre-process the features a bit, we include [StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html). For tree  based method standardization has no effect. So we neglect it there. \n\nP.S: If you are interseted in learning about in detail how SVM, Logistic Regression and Decision Tree classifiers work, please check my articles. \n1. [Complete Theory of SVM](https://towardsdatascience.com/understanding-support-vector-machine-part-1-lagrange-multipliers-5c24a52ffc5e). \n2. [Understanding Logit of Logistic Regression](https://towardsdatascience.com/logit-of-logistic-regression-understanding-the-fundamentals-f384152a33d1). \n3. [Understanding Decision Tree Classification](https://towardsdatascience.com/understanding-decision-tree-classification-with-scikit-learn-2ddf272731bd). "},{"metadata":{"trusted":true},"cell_type":"code","source":"pipelines = [ [('scaler', StandardScaler()), ('SVM', SVC())], [('scaler', StandardScaler()), ('LR', LogisticRegression())], \n             [ ('RF', RandomForestClassifier())],  [('ADB', AdaBoostClassifier(DecisionTreeClassifier(max_depth=2)))], \n             [('scaler', StandardScaler()), ('GNB', GaussianNB())]]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5.5. Create the Grids for Parameter Search for Each Classifier  "},{"metadata":{"trusted":true},"cell_type":"code","source":"svm_param_grid = {'SVM__C': [0.01, 0.05, 0.1, 0.5, 1, 5, 10, 30, 40, 50, 75, 100, 200],  \n                  'SVM__kernel': ['linear']}\n\nLR_param_grid = {'LR__C': [0.01, 0.05, 0.1, 0.5, 1., 2., 5., 10.], 'LR__class_weight':['balanced']}\n\nRF_param_grid = {'RF__criterion': ['gini', 'entropy'], 'RF__n_estimators': [30, 50, 75, 100, 125, 150, 200], 'RF__max_depth': [2, 3, 4]}\n\nADB_param_grid = {'ADB__n_estimators': [20, 40, 50, 75, 100, 200], 'ADB__learning_rate': [0.01, 0.05, 0.1, 0.5, 1., 2]}\n\nGNB_param_grid = {'GNB__priors': [[0.35, 0.65], [0.4, 0.6]], 'GNB__var_smoothing': [1e-9, 1e-8]}\n\nall_param_grid = [svm_param_grid, LR_param_grid, RF_param_grid, ADB_param_grid, GNB_param_grid]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5.6. Create a Pipeline of Standardization and Classifier "},{"metadata":{"trusted":true},"cell_type":"code","source":"all_pipelines = []\n\nfor p in pipelines:\n    all_pipelines.append(Pipeline(p))\nprint ('check one of the pipelines: ', all_pipelines[4])  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5.7. Select the Best Parameter for Each Classifier using GridSearchCV"},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm.notebook import tqdm\n\ngrid_scores = []\ngrid_best_params = []\n\ntime1 = time.time()\n\nfor x in tqdm(range(len(all_pipelines))):\n    grid = GridSearchCV(all_pipelines[x], param_grid=all_param_grid[x], cv=5)\n    grid.fit(X_train, y_train)\n    score = grid.score(X_test, y_test)\n    grid_scores.append(score)\n    grid_best_params.append(grid.best_params_)\nprint ('!!!!! out of the loop !!!!!')  \nprint ('time taken: ', time.time() - time1, 'seconds')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5.8. Results \n\n#### 5.8.1. Check the best hyperparameter for every model.  \n\nPrint out the best hyperparameters for every model.   "},{"metadata":{"trusted":true},"cell_type":"code","source":"print ('Below are the Selected Best Hyperparameter for Each Classifier: ')\nprint ('\\n')\nfor x in range(len(grid_best_params)):\n    print (grid_best_params[x])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 5.8.2. Score for Each Model \n\nPrint out and check the score for each classifier. Best seems like SVM with Linear kernel with an accuracy of $78.4\\%$.   "},{"metadata":{"trusted":true},"cell_type":"code","source":"models = ['SVM', 'Logistic Reg', 'Random Forest', 'AdaBoost', 'GaussianNB']\nscore_dict = dict(zip(models, grid_scores))\nprint ('check score for each model : \\n', score_dict)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5.9. Store the Result in Dataframe\n\nWe finally create a dataframe to store Model Name and Score. "},{"metadata":{"trusted":true},"cell_type":"code","source":"score_df = pd.DataFrame(score_dict.items(), columns=['Model', 'Score'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}