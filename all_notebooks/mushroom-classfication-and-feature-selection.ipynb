{"cells":[{"metadata":{},"cell_type":"markdown","source":"Hi, I'm going to apply 3 supervised machine learning classification models on the given dataset to classify mushrooms as poisonous or edible.\n\n1. Logistic Regression\n2. K-Nearest Neighbours(K-NN)\n3. Support Vector Classfier(SVC)\n\nI'll proceed by converting categorical variables into dummy/indicator variables, then applying 3 feature selection techniques to reduce 23 categorical variables (which will become 95 variables after conversion to dummy variables) to only 20 variables and choose the best feature elemination technique for given dataset. Then training different classification models over these 20 features. Here the goal is to choose best feature selection technique for such datasets with optimum accuracy."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Import important libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport sklearn\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Load our msuhroom dataset\nmain_data=pd.read_csv('../input/mushroom-classification/mushrooms.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"main_data.head()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we see that initaly we total 23 features including target variable"},{"metadata":{},"cell_type":"markdown","source":"# 1) EDA:"},{"metadata":{},"cell_type":"markdown","source":"Look that the variable in the analysis are only qualitatives,all in their nominal state ,that is there are no levels more important than another in each of analyzed variable.The categories of each variable are represented by letters,this meanning was transformed into literal in order to bring an interpretation to the problem."},{"metadata":{"trusted":true},"cell_type":"code","source":"main_data['class']=main_data['class'].replace({'p':'Poisonous','e':'Edible'})\nmain_data['cap-shape']=main_data['cap-shape'].replace({'b':'Bell','c':'Conical','x':'Convex','f':'Flat',\n                                                               'k':'Knobbed','s':'Sunken'})\nmain_data['cap-surface']=main_data['cap-surface'].replace({'f':'Fibrous','g':'Grooves','y':'scaly','s':'Smooth'})\nmain_data['cap-color']=main_data['cap-color'].replace({'n':'Brown','b':'Buff',\n                                                               'c':'Cinnamon','g':'Gray','r':'Green','p':'Pink',\n                                                               'u':'Purple','e':'Red','w':'White','y':'Yellow'})\nmain_data['bruises']=main_data['bruises'].replace({'t':'Brusies','f':'No'})\nmain_data['odor']=main_data['odor'].replace({'a':'Almond','l':'Anise','c':'Creosote','y':'Fishy','f':'Foul',\n                                                     'm':'Musty','n':'None','p':'Pungent','s':'Spicy'})\nmain_data['gill-attachment']=main_data['gill-attachment'].replace({'a':'Attached','d':'Decending','f':'Free',\n                                                                           'n':'Notched'})\nmain_data['gill-spacing']=main_data['gill-spacing'].replace({'c':'Close','w':'Crowded','d':'Distant'})\nmain_data['gill-size']=main_data['gill-size'].replace({'b':'Broad','n':'Narrow'})\nmain_data['gill-color']=main_data['gill-color'].replace({'k':'Black','n':'Brown','b':'Buff','h':'Chocolate',\n                                                                 'g':'Gray'})\nmain_data['stalk-shape']=main_data['stalk-shape'].replace({'e':'Enlarging','t':'Tapering'})\nmain_data['stalk-root']=main_data['stalk-root'].replace({'b':'Bulbous','c':'Club','u':'Cup','e':'Equal',\n                                                                 'z':'Rhizomorphs','r':'Rooted','?':'Missing'})\nmain_data['stalk-surface-above-ring']=main_data['stalk-surface-above-ring'].replace({'f':'Fibrous','y':'Scaly',\n                                                                                         'k':'Silky','s':'Smooth'})\nmain_data['stalk-surface-below-ring']=main_data['stalk-surface-below-ring'].replace({'f':'Fibrous','y':'Scaly',\n                                                                                         'k':'Silky','s':'Smooth'})\nmain_data['stalk-color-above-ring']=main_data['stalk-color-above-ring'].replace({'n':'Brown','b':'Buff',\n                                                                                         'c':'Cinnamon','g':'Gray',\n                                                                                         'o':'Orange','p':'Pink',\n                                                                                         'e':'Red','w':'White',\n                                                                                         'y':'Yellow'})\nmain_data['stalk-color-below-ring']=main_data['stalk-color-below-ring'].replace({'n':'Brown','b':'Buff',\n                                                                                         'c':'Cinnamon','g':'Gray',\n                                                                                         'o':'Orange','p':'Pink','e':'Red',\n                                                                                         'w':'White','y':'Yellow'})\nmain_data['veil-type']=main_data['veil-type'].replace({'p':'Partial','u':'Universal'})\nmain_data['veil-color']=main_data['veil-color'].replace({'n':'Brown','o':'Orange','w':'White','y':'Yellow'})\nmain_data['ring-number']=main_data['ring-number'].replace({'n':'None','o':'One','t':'Two'})\nmain_data['ring-type']=main_data['ring-type'].replace({'c':'Cowbebby','e':'Evanescent','f':'Flaring','l':'Large',\n                                                               'n':'None','p':'Pendant','s':'Sheating','z':'Zone'})\nmain_data['spore-print-color']=main_data['spore-print-color'].replace({'k':'Black','n':'Brown','b':'Buff',\n                                                                               'h':'Chocolate','r':'Green','o':'Orange',\n                                                                               'u':'Purple','w':'White','y':'Yellow'})\nmain_data['population']=main_data['population'].replace({'a':'Abundant','c':'Clustered','n':'Numerous',\n                                                                 's':'Scattered','v':'Several','y':'Solitary'})\nmain_data['habitat']=main_data['habitat'].replace({'g':'Grasses','l':'Leaves','m':'Meadows','p':'Paths','u':'Urban',\n                                                           'w':'Waste','d':'Woods'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"main_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Spores of a mushroom contains all the necessary materials to form a new fungus.\n\nIn the below figure ,it is interestion to note that the largest spore-point-color categorie for Poisonous Mushrooms are Chocolate and White ,implying that the higest frequency of poisonous mushrooms contains that color.\n\nWhile Ediable mushrooms are mostly Black and Brown in their spores ,But there is small existence of ediable mushroom that contains the coloration of chocolate and white spores "},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,6))\nsns.countplot(x='spore-print-color',data=main_data,hue='class')\nplt.title('Spore color counts',fontsize=20)\nplt.xlabel('Spore Point Color',fontsize=15)\nplt.ylabel('Count',fontsize=15)\nplt.xticks(rotation=45)\nplt.legend(loc='upper right')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Regarding the odor of mushrooms,we see that the essential characterstic of a poisonous mushroom is that it has a unpleasant odor ,being referred by Foul.\n\nEdible mushrooms tend to have no smell at all."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,6))\nsns.countplot(x='odor',data=main_data,hue='class')\nplt.xlabel('Odor',fontsize=15)\nplt.ylabel('Count',fontsize=15)\nplt.xticks(rotation=45)\nplt.legend(loc='upper right')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see from the below bar graph that mushroom habitted on paths and leaves are mostly poisonous."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,6))\nsns.countplot(x='habitat',data=main_data,hue='class')\nplt.xlabel('Habitat',fontsize=15)\nplt.ylabel('Count',fontsize=15)\nplt.xticks(rotation=45)\nplt.legend(loc='upper right')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,6))\ns=sns.countplot(x='class',data=main_data)\nplt.xlabel('Class',fontsize=15)\nplt.ylabel('Count',fontsize=15)\nplt.xticks(fontsize=12)\nplt.legend(loc='upper right')\n\n\nfor p in s.patches:\n    s.annotate(format(p.get_height(), '.1f'), \n               (p.get_x() + p.get_width() / 2., p.get_height()), \n                ha = 'center', va = 'center', \n                xytext = (0, 9), \n                textcoords = 'offset points')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above Bar graph we can see that we have 4208 values of Ediable mushroom and 3916 values of Poisonous mushrooms.Hence we can say that our data is not biased towards a specific class means that our dataset is Balanced."},{"metadata":{"trusted":true},"cell_type":"code","source":"main_data.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the main_data we can see that in veil-type their is only kind of value that is Partial so we are going to drop this column.It is not significant to Ml models"},{"metadata":{},"cell_type":"markdown","source":"#  2) Identify & Handel Missing values \n\nMissing values are the one of the most common problems we can encounter when we try to prepare our data for machine learning. The reason for this might be human erroe ,interruption in the data flow.First of all we have to identify missing values after we can do imputation or we can drop the rows who has missing values."},{"metadata":{"trusted":true},"cell_type":"code","source":"main_data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we see that in our dataset their is no missing value so lets move to next step"},{"metadata":{"trusted":true},"cell_type":"code","source":"main_data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"main_data=main_data.drop('veil-type',axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"main_data.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3) Encoding categorical varible \n\nSince the Data is of object type,it is not possible to make prediction/classfication .So we need a way to convert the data to either an integer type or a float type.To do this we need to encode the data .There are several ways to do this :\n\n**1)** Label encoder and One Hot Encoder of sklearn :It is a two step process .First of all we have to apply label encoder it convert data in machine readable,but it assigns a unique Number (Starting from 0) to each value of column.This may lead to the \"priority\" issue in training of dataset.A label with High Value may be considered to have high priority.Through Label Encoding is straight but is has the disadvantage that the numeric values can be missintrepted by the ML model.\nThis ordering issue is address in another common alternative approch called One Hot Encoder of sklearn library.\nIn this stratigy ,each category value is converted into a new column and assigned a **1 or 0 ** (Notation for True /False) to the column .\n\n\n**2)**  pd.get_dummies(): This is pandas,s dummy variable function .it is used for data manipulation .It converts categorical data into dummy or indicator variable.It return the one hot encoded columns.This is one step process.\n\n\nWe are gone use pd.get_dummies coz it is easy to impliment."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nencoder=LabelEncoder()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for c in main_data.columns:\n    main_data[c]=encoder.fit_transform(main_data[c])\n    \n ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoded_data=pd.get_dummies(main_data.iloc[:,1:],columns=main_data.iloc[:,1:].columns,drop_first=True)\nencoded_data=pd.concat([main_data.iloc[:,0],encoded_data],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoded_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In class variable \n\n1=Poisonous,\n0=Edible"},{"metadata":{},"cell_type":"markdown","source":"Let,s split our encoded dataset into dependent and independent variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"X=encoded_data.iloc[:,1:]\nY=encoded_data.iloc[:,0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4) Feature Selection:\nFeature Selecytion is a process of identifying and selecting a subset of input features that are most relevent to the target variable.Their are several techniques but we are gone use three of then and comapre our models on them .\n\n\n"},{"metadata":{},"cell_type":"markdown","source":"# 4.1) Correlation\n\nCorrelation is a statistic method that measures the degree to which two varibles move in relation to each other .The correlation cofficient 'r' lies between -1 and 1.If the correlation cofficient is -1 means both variable are negatively strong  related and if r is 1 menas they are positively strong related and if r is 0 it means ther is no relation between them.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"cor_cofe=encoded_data.corr(method='pearson').abs()\ncor_cofe=cor_cofe.iloc[:,0].to_frame()  # it convert the cofficent into data frame\ncor_cofe.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cor_cofe=cor_cofe.rename(columns={'class':'Correlation'})\ncor_cofe=cor_cofe.drop('class',axis=0)  # Because class is our target variable","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We only want the feature who's correlation cofficient is eual to or greater than 0.5"},{"metadata":{"trusted":true},"cell_type":"code","source":"cor_cofe=cor_cofe[cor_cofe['Correlation']>= 0.5]\ncor_cofe","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_corr=X[cor_cofe.index]    # Features Seected using correlation metrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_corr.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4.2) SelectKBest(chi ^2)\n\nUnivariate feature selection works by selecting the best features using univariate statistical tests such as Chi-square .It determine the strength of the relationship of the feature with the response variable.\n\n**Chi-square**\n\nTest isa an example of the test of independence between categorical variable [To estimate wheather two categorical variable are independent of one another]\n\n\nThe chai-square test of independence works by computing the categories coded data that we have collected(Knowns as the Observed frequencies) with the frequencies that we would expect to get in each cell of the table by change alone(Known as the expected frequencies)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import SelectKBest,chi2\nX_K_best=SelectKBest(chi2,k=20)\nX_K=X_K_best.fit_transform(X,Y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scores=X_K_best.scores_/1000\nscores_data=pd.DataFrame({'Features': X.columns,'Scores':scores})\nplt.figure(figsize=(30,8))\nsns.barplot(x='Features',y='Scores',data=scores_data)\nplt.xticks(rotation=90)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"top_features=scores_data.sort_values(by='Scores',ascending=False)\ntop_features=top_features.head(20)\ntop_features=top_features.reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"top_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_scores=X[top_features['Features']]    #Features selected by SelectKbest using chi^2 test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_scores.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4.3) Recursive Feature Elimination(RFE)\n\nUnlike the univariate method ,RFE starts by fitting a model on the entire set of features and computing an importance score for each predictor.The weakest features are then removed ,the model is re-fitted and importance scores are computed again until the specific number of features are used.Feature important score are ranked by the model,s coef_ or feature_importances_ attributes. "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LogisticRegression\nestimator=LogisticRegression()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for k in range(2, 30):  \n    selector = RFE(estimator, n_features_to_select=k, step=2)\n    selector = selector.fit(X, Y)\n    selector.support_\n    selector.ranking_\n\n    sel_fea  = [i for i,j in zip(X.columns,selector.ranking_) if j==1]\n\n    X_new = X[sel_fea]\n\n    from sklearn.model_selection import train_test_split\n    X_train, X_test, Y_train, Y_test = train_test_split(X_new, Y, test_size=0.33, random_state=42)\n    from sklearn.linear_model import LogisticRegression\n    classifier = LogisticRegression()\n    classifier.fit(X_train, Y_train)\n    Y_pred1 = classifier.predict(X_test)\n\n    from sklearn.metrics import accuracy_score\n    acc = accuracy_score(Y_pred1,Y_test)\n    print(\"Features: %s\"%k, \" Accuracy: %f\"%acc)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Applying RFE with 8 Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"selector = RFE(estimator, n_features_to_select=8, step=2)\nselector = selector.fit(X, Y)\nselector.support_\nselector.ranking_\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sel_cofe=[i for i ,j in zip(X.columns,selector.ranking_)if j==1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_new=X[sel_cofe]      # Features selected by RFE","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# 5) Now i'm going to apply 3 supervised machine learning classfication model on the given dataset:\n\n1 Logistc Regression\n\n2 K-Nearest Neighbours(K-NN)\n\n3 Support Vector classfier(SVC)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing 3 above models and apply train test split on the datasets that are selected using Feature selection technique.\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\n\n# Train test split on the features which are selected using correlation matrix \nX_train_corr,X_test_corr,Y_train_corr,Y_test_corr=train_test_split(X_corr,Y,test_size=0.25)\n\n# Train test split on the features which are selected using SelectKbest with chi^2\nX_train_score,X_test_score,Y_train_score,Y_test_score=train_test_split(X_scores,Y,test_size=0.25)\n\n# Train test split on the features which are selected using RFE\nX_train_new,X_test_new,Y_train_new,Y_test_new=train_test_split(X_new,Y,test_size=0.25)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# 1) Logistic Regression:\n\nLR is a statistical technique used to predict probelty of binary response on one or more independent variables. \n\nNow i'm going to apply LR on feature  that are  selected using correlation,SelectKbest and RFE .After i'm going to compare the accuracy of all the model and select the best . \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# LR on features which are selected using correlation matrix\naccuracy_LR_corr=[]\nLR_model=LogisticRegression()    \nLR_model.fit(X_train_corr,Y_train_corr)   \nY_pred_corr=LR_model.predict(X_test_corr)\nacc_LR_corr=accuracy_score(Y_pred_corr,Y_test_corr)*100\naccuracy_LR_corr.append(acc_LR_corr)\nprint(\"Accuracy:%f\"% acc_LR_corr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# LR on features which are selected using SelectKbest with chi^2\n\nLR_model=LogisticRegression()\naccuracy_LR_chi=[]\nLR_model.fit(X_train_score,Y_train_score)\nY_pred_score=LR_model.predict(X_test_score)\nacc_LR_chi=accuracy_score(Y_pred_score,Y_test_score)*100\naccuracy_LR_chi.append(acc_LR_chi)\nprint(\"Accuracy:%f\"% acc_LR_chi)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# LR on features which are selected using RFE\n\nLR_model=LogisticRegression()\naccuracy_LR_rfe=[]\nLR_model.fit(X_train_new,Y_train_new)\nY_pred_new=LR_model.predict(X_test_new)\nacc_LR_rfe=accuracy_score(Y_pred_new,Y_test_new)*100\naccuracy_LR_rfe.append(acc_LR_rfe)\nprint(\"Accuracy:%f\"% acc_LR_rfe)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# 2) K-Nearest Neighbours(K-NN):\n\nK-NN is a simple algorithm that stores all avialable cases and classifies new cases based on similarity measure.\n\nI'm going to run a for loop over KNN with a range (1,20) to find the best n_neighbours for our KNN."},{"metadata":{"trusted":true},"cell_type":"code","source":"# KNN on features which are selected using correlation matrix\naccuracy_knn_corr=[]\nlist_nn=range(1,20)\nfor k in list_nn:\n    KNN_model=KNeighborsClassifier(n_neighbors=k,metric='minkowski',p=2)\n    KNN_model.fit(X_train_corr,Y_train_corr)\n    y_pred_knn=KNN_model.predict(X_test_corr)\n    acc_knn_corr=accuracy_score(y_pred_knn,Y_test_corr)*100\n    accuracy_knn_corr.append(acc_knn_corr)\n    print(\"Neighbours:%s\"%k, \"Accuracy:%f\"%acc_knn_corr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,4))\nplt.grid()\nplt.plot(list_nn,accuracy_knn_corr,'-og')\nplt.xlabel('Values of K',fontsize=14)\nplt.ylabel('Accuracy',fontsize=14)\nplt.xticks(range(1,20))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# KNN on features which are selected using SelectKbest with chi^2\naccuracy_knn_chi=[]\nfor k in range(1,20):\n    KNN_model=KNeighborsClassifier(n_neighbors=k,metric='minkowski',p=2)\n    KNN_model.fit(X_train_score,Y_train_score)\n    Y_pred_knn=KNN_model.predict(X_test_score)\n    acc_knn_chi=accuracy_score(Y_pred_knn,Y_test_score)*100\n    accuracy_knn_chi.append(acc_knn_chi)\n    print(\"Neighbours:%s\"%k, \"Accuracy:%f\"%acc_knn_chi)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,4))\nplt.grid()\nplt.plot(accuracy_knn_chi,'o-g')\nplt.xlabel('Values of K',fontsize=14)\nplt.ylabel('Accuracy',fontsize=14)\nplt.xticks(range(1,20))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# KNN on features which are selected using RFE\naccuracy_knn_rfe=[]\nfor k in range(1,20):\n    KNN_model=KNeighborsClassifier(n_neighbors=k,metric='minkowski',p=2)\n    KNN_model.fit(X_train_new,Y_train_new)\n    Y_pred_knn=KNN_model.predict(X_test_new)\n    acc_knn_rfe=accuracy_score(Y_pred_knn,Y_test_new)*100\n    accuracy_knn_rfe.append(acc_knn_rfe)\n    print(\"Neighbours:%s\"%k, \"Accuracy:%f\"%acc_knn_rfe)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,4))\nplt.grid()\nplt.plot(accuracy_knn_rfe,'o-g')\nplt.xlabel('Values of K',fontsize=14)\nplt.ylabel('Accuracy',fontsize=14)\nplt.xticks(range(1,20))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# 3) Support Vector classfier(SVC):\n\nSVC is a supervised classfication method that separates data using hyperplanes,Hyperplanes acts like a decision boundary for classes.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# SVC on features which are selected using correlation matrix\nkernal_list=[\"linear\",\"rbf\",\"sigmoid\"]\naccuracy_svc_corr=[]\nfor k in kernal_list:\n    svc_model=SVC(kernel=k)\n    svc_model.fit(X_train_corr,Y_train_corr)\n    Y_pred_svc=svc_model.predict(X_test_corr)\n    acc_svc_corr=accuracy_score(Y_pred_svc,Y_test_corr)*100\n    accuracy_svc_corr.append(acc_svc_corr)\n    print(\"Kernal:%s\"%k, \"Accuracy:%f\"%acc_svc_corr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# SVC on features which are selected using SelectKbest with chi^2\nkernal_list=[\"linear\",\"rbf\",\"sigmoid\"]\naccuracy_svc_chi=[]\nfor k in kernal_list:\n    svc_model=SVC(kernel=k)\n    svc_model.fit(X_train_score,Y_train_score)\n    Y_pred_svc=svc_model.predict(X_test_score)\n    acc_svc_chi=accuracy_score(Y_pred_svc,Y_test_score)*100\n    accuracy_svc_chi.append(acc_svc_chi)\n    print(\"Kernal :%s\"%k,\"Accuracy:%f\"%acc_svc_chi)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# SVC on features which are selected using RFE\nkernal_list=[\"linear\",\"rbf\",\"sigmoid\"]\naccuracy_svc_rfe=[]\naccuracy=[]\nfor k in kernal_list:\n    svc_model=SVC(kernel=k)\n    svc_model.fit(X_train_new,Y_train_new)\n    Y_pred_svc=svc_model.predict(X_test_new)\n    acc_svc_rfe=accuracy_score(Y_pred_svc,Y_test_new)*100\n    accuracy_svc_rfe.append(acc_svc_rfe)\n    print(\"Kernal:%s\"%k, \"Accuracy:%f\"%acc_svc_rfe)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we are gona comapre all of our model with respect to the feature selection technique on the basis of accuracy."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(18,6))\n\n\nplt.subplot(1,3,1)\nplt.grid()\nplt.plot(acc_LR_corr,'o-y',label='LR',markersize=8,markeredgecolor='k')\nplt.plot(accuracy_knn_corr,'o-g',label='KNN',markersize=8,markeredgecolor='k')\nplt.plot(accuracy_svc_corr,'o-r',label='SVC',markersize=8,markeredgecolor='k')\nplt.ylabel('Accuracy',fontsize=12)\nplt.title(\"Corr\",fontsize=17)\nplt.legend()\n\n\nplt.subplot(1,3,2)\nplt.grid()\nplt.plot(accuracy_LR_chi,'o-y',label='LR',markersize=8,markeredgecolor='k')\nplt.plot(accuracy_knn_chi,'o-g',label='KNN',markersize=8,markeredgecolor='k')\nplt.plot(accuracy_svc_chi,'o-r',label='SVC',markersize=8,markeredgecolor='k')\nplt.ylabel('Accuracy',fontsize=12)\nplt.title('Chi^2',fontsize=17)\nplt.legend()\n\n\nplt.subplot(1,3,3)\nplt.grid()\nplt.plot(accuracy_LR_rfe,'o-y',label='LR',markersize=8,markeredgecolor='k')\nplt.plot(accuracy_knn_rfe,'o-g',label='KNN',markersize=8,markeredgecolor='k')\nplt.plot(accuracy_svc_rfe,'o-r',label='SVC',markersize=8,markeredgecolor='k')\nplt.ylabel('Accuracy',fontsize=12)\nplt.title('RFE',fontsize=17)\nplt.legend(fontsize=12)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"On the above line plot the first one (left one)is on the features which are selected using correlation metrix ,as we can clearly see that approx all the model have same accuracy of approx 98 percentage .\n\nAnd the middle one is on the features which are selected usinng SelectKbest with chi^2 ,as we can see our SVC model with rbf kernal works well as compare to other.Here we get a maximum accuracy of around 99.1 percent.\n\nThe last one and the Right one is on the features which are selected using RFE(Logistic Regression),as we can see that all our works well with accuracy of 100 percent exclude SVC(sigmoid).\n\n\n**Final conclusion is that from the three feature selection technique RFE(Logistic Regression) select the best feature for our model,s with accuracy of 100 perercnt.**"},{"metadata":{},"cell_type":"markdown","source":"If you like my work, an upvote will motivate me to persue this never ending ML/Data Science journey. I am new to this field, if you feel I made some mistakes or have any suggestions please comment. I trust this community will help me to hone my skills."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}