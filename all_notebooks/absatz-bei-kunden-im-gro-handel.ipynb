{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Projektarbeit: Absatz bei Kunden im Großhandel (Statistical Learning)\n* **Ziel des Projekts**: Einteilung der Großhandelskunden in sinnvolle Gruppen mithilfe einer Clustermethode","metadata":{}},{"cell_type":"markdown","source":"## 1. Datenexploration\nIn diesem Kapitel erforscht man die Daten durch Visualisierungen, um zu verstehen, wie jedes Merkmal mit den anderen zusammenhängt. Das Ziel der Datenexploration ist es, zu sehen, ob etwas von Interesse ist.","metadata":{}},{"cell_type":"code","source":"# Für dieses Projekt erforderliche Bibliotheken importieren\nimport sys\n#!{sys.executable} -m pip install pandas-profiling\nimport numpy as np\nimport pandas as pd\nfrom IPython.display import display \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\nfrom pandas_profiling import ProfileReport\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\n\n%matplotlib inline\n\n#Daten importieren\ntry:\n    df = pd.read_csv(\"../input/wholesale-customers-data-set/Wholesale customers data.csv\")\n    print(\"Wholesale customers dataset has {} samples with {} features each.\".format(*df.shape))\nexcept:\n    print(\"Dataset could not be loaded.\")","metadata":{"execution":{"iopub.status.busy":"2021-07-16T18:01:13.399723Z","iopub.execute_input":"2021-07-16T18:01:13.40024Z","iopub.status.idle":"2021-07-16T18:01:13.41653Z","shell.execute_reply.started":"2021-07-16T18:01:13.400198Z","shell.execute_reply":"2021-07-16T18:01:13.41546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = df.rename(columns={'Delicassen': 'Delicatessen'}) #Rechtschreibung korrigieren\ndata.drop(['Region', 'Channel'], axis = 1, inplace = True)\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-16T18:01:13.418006Z","iopub.execute_input":"2021-07-16T18:01:13.418348Z","iopub.status.idle":"2021-07-16T18:01:13.458176Z","shell.execute_reply.started":"2021-07-16T18:01:13.41831Z","shell.execute_reply":"2021-07-16T18:01:13.457094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# allgemeine Beschreibung des Datensatzes\ndisplay(data.describe())","metadata":{"execution":{"iopub.status.busy":"2021-07-16T18:01:13.460268Z","iopub.execute_input":"2021-07-16T18:01:13.460936Z","iopub.status.idle":"2021-07-16T18:01:13.497198Z","shell.execute_reply.started":"2021-07-16T18:01:13.460884Z","shell.execute_reply":"2021-07-16T18:01:13.495968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Ein detailierter Bericht von allen Variablen\ndata.profile_report()","metadata":{"execution":{"iopub.status.busy":"2021-07-16T18:01:13.499122Z","iopub.execute_input":"2021-07-16T18:01:13.499427Z","iopub.status.idle":"2021-07-16T18:01:29.826652Z","shell.execute_reply.started":"2021-07-16T18:01:13.499396Z","shell.execute_reply":"2021-07-16T18:01:29.825133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Zuerst wollen wir *Channel* und *Region* einzig untersuchen, da die beide Merkmalen wahrscheinlich mit unserer Analyse nicht relevant sind. Es erscheint, dass es nicht so viele Unterschiede zwischen den beiden.","metadata":{}},{"cell_type":"code","source":"# Channel plotten\na = df.pop('Channel')\ndata = data.join(a)\nsubjects = ['Fresh', 'Milk', 'Grocery', 'Frozen', 'Detergents_Paper', 'Delicatessen']\nchannel = data.groupby('Channel')[subjects].sum()\nchannel1 = channel.T\nprint(channel1)\ncolors = ['c','coral','limegreen']\nax = channel1.plot(kind='bar', figsize = (9,7),stacked= True, title = '', color = colors)\nax.tick_params(axis='x', rotation=0)\nax.legend(['Channel 1', 'Channel 2'],loc=1, prop={'size': 16})","metadata":{"execution":{"iopub.status.busy":"2021-07-16T18:01:29.828402Z","iopub.execute_input":"2021-07-16T18:01:29.828829Z","iopub.status.idle":"2021-07-16T18:01:30.065223Z","shell.execute_reply.started":"2021-07-16T18:01:29.828788Z","shell.execute_reply":"2021-07-16T18:01:30.064063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Region plotten\na = df.pop('Region')\ndata = data.join(a)\nregion = data.groupby('Region')[subjects].sum()\nregion1 = region.T\nprint(region1)\nax = region1.plot(kind='bar', figsize = (8,6),stacked= True, title = '', color = colors)\nax.tick_params(axis='x', rotation=0)\nax.legend(['Region 1', 'Region 2','Region 3'],loc=1, prop={'size': 16})","metadata":{"execution":{"iopub.status.busy":"2021-07-16T18:01:30.066579Z","iopub.execute_input":"2021-07-16T18:01:30.066902Z","iopub.status.idle":"2021-07-16T18:01:30.315629Z","shell.execute_reply.started":"2021-07-16T18:01:30.066842Z","shell.execute_reply":"2021-07-16T18:01:30.314515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.1 Samples\nUm die Daten besser zu verstehen, wäre es am besten, ein paar Beobachtungen als Stichproben auszuwählen und sie genauer zu untersuchen. Im Codeblock unten werden drei Indizes der Wahl zur Liste hinzugefügt, die die zu verfolgenden Kunden repräsentieren werden.","metadata":{}},{"cell_type":"code","source":"data.drop(['Region', 'Channel'], axis = 1, inplace = True)\n\n# drei beliebige Beobachtungen\nindices = [43, 344, 122]\n\n# Ein DataFrame für die Stichproben erzeugen\nsamples = pd.DataFrame(data.loc[indices], columns = data.columns)\nprint (\"Chosen samples of wholesale customers dataset:\")\ndisplay(samples)\n\n# Durchschnitt berechnen \nmean_data = data.describe().loc['mean', :]\nsamples_bar = samples.append(mean_data)\n\n# Index kontruieren\nsamples_bar.index = indices + ['mean']\n\n# Barplot\nax=samples_bar.plot(kind='bar', figsize=(14,8))\nax.tick_params(rotation=0)","metadata":{"execution":{"iopub.status.busy":"2021-07-16T18:01:30.3181Z","iopub.execute_input":"2021-07-16T18:01:30.31858Z","iopub.status.idle":"2021-07-16T18:01:30.647497Z","shell.execute_reply.started":"2021-07-16T18:01:30.318532Z","shell.execute_reply":"2021-07-16T18:01:30.646313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Vergleichen mit durchschnittlichen Wert\n(samples - data.mean()) / data.std()","metadata":{"execution":{"iopub.status.busy":"2021-07-16T18:01:30.650071Z","iopub.execute_input":"2021-07-16T18:01:30.650406Z","iopub.status.idle":"2021-07-16T18:01:30.670356Z","shell.execute_reply.started":"2021-07-16T18:01:30.650376Z","shell.execute_reply":"2021-07-16T18:01:30.669141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.2 Überprüfung auf Missings und Ausreißer\nDas Erkennen von Missings und Ausreißern des Datensatzes ist ein wichtiger Teil der Datenexplorationsarbeit. Wir überprüfen auf Ausreißer durch Visualisierung mittels *Pairplot*, *Barplot* und *Boxplot*.","metadata":{}},{"cell_type":"code","source":"# Überprüfen, ob es Missings gibt\nprint(data.isnull().values.any())\n# Es gibt keine Null-Werte bzw. Missings im Datensatz","metadata":{"execution":{"iopub.status.busy":"2021-07-16T18:01:30.672731Z","iopub.execute_input":"2021-07-16T18:01:30.673121Z","iopub.status.idle":"2021-07-16T18:01:30.680769Z","shell.execute_reply.started":"2021-07-16T18:01:30.673086Z","shell.execute_reply":"2021-07-16T18:01:30.679754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Überprüfen, ob es Ausreißer gibt\n_ = sns.pairplot(data, diag_kind = 'kde')","metadata":{"execution":{"iopub.status.busy":"2021-07-16T18:01:30.682266Z","iopub.execute_input":"2021-07-16T18:01:30.682595Z","iopub.status.idle":"2021-07-16T18:01:37.938798Z","shell.execute_reply.started":"2021-07-16T18:01:30.682564Z","shell.execute_reply":"2021-07-16T18:01:37.937751Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (20,8))\n_ = sns.barplot(data=data, palette=\"Set2\")","metadata":{"execution":{"iopub.status.busy":"2021-07-16T18:01:37.940189Z","iopub.execute_input":"2021-07-16T18:01:37.940513Z","iopub.status.idle":"2021-07-16T18:01:38.356067Z","shell.execute_reply.started":"2021-07-16T18:01:37.94048Z","shell.execute_reply":"2021-07-16T18:01:38.355283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (20,8))\n_ = sns.boxplot(data=data, orient='h', palette=\"Set2\")","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-07-16T18:01:38.357215Z","iopub.execute_input":"2021-07-16T18:01:38.35767Z","iopub.status.idle":"2021-07-16T18:01:38.617953Z","shell.execute_reply.started":"2021-07-16T18:01:38.357618Z","shell.execute_reply":"2021-07-16T18:01:38.617176Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Implementierung\n## 2.1 Datenverarbeitung\nVor der Clusteranalyse müssen folgende Voraussetzungen erfüllt sein:\n* **Normalisierung**: Da die Daten verzehrt sind, fangen wir mit der Normalisierung an, indem wir einfach den natürlichen Logarithmus auf die Daten verwenden.\n* **Ausreißer**: Es ist festgestellt, der Datensatz von vielen Ausreißer zerstört wird. Ausgewählt ist die Methode nach Tukey mit Hilfe des Interquartilabstandes, um Ausreißer zu erkennen.\n* **Dimensionsreduktion**: mittels Hauptkomponentenanalyse (PCA). Sie dient dazu, umfangreiche Datensätze zu struktieren, zu vereinfachen und zu veranschaulichen.","metadata":{}},{"cell_type":"markdown","source":"## Normalisierung","metadata":{}},{"cell_type":"code","source":"# die Daten mit dem natürlichen Logarithmus skalieren\nlog_data = np.log(data)\n\n# die Stichprobedaten mit dem natürlichen Logarithmus skalieren\nlog_samples = np.log(samples)\n\n# eine Streumatrix für jedes Paar neu transformierter Merkmale erstellen\n_ = sns.pairplot(log_data, diag_kind = 'kde')\n\n# die Protokolltransformierten Beispieldaten anzeigen\ndisplay(log_samples)\nax=log_samples.plot(kind='bar', figsize=(8,6))\nax.tick_params(rotation=0)\nax.set_title('Samples nach dem Log-Transformation')","metadata":{"execution":{"iopub.status.busy":"2021-07-16T18:01:38.619076Z","iopub.execute_input":"2021-07-16T18:01:38.619514Z","iopub.status.idle":"2021-07-16T18:01:46.102454Z","shell.execute_reply.started":"2021-07-16T18:01:38.619465Z","shell.execute_reply":"2021-07-16T18:01:46.101395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Ausreißerbehandlung","metadata":{}},{"cell_type":"code","source":"outliers_list = []\n# Für jedes Merkmal die Datenpunkte mit extrem hohen oder niedrigen Werten finden\nfor feature in log_data.keys():\n    \n    # Q1 (25. Perzentil der Daten) für das gegebene Merkmal berechnen\n    Q1 = np.percentile(log_data[feature], 25)\n    \n    # Q3 (75. Perzentil der Daten) für das gegebene Merkmal berechnen\n    Q3 = np.percentile(log_data[feature], 75)\n    \n    # den Interquartilabstand verwenden, um einen Ausreißerschritt zu berechnen (1,5-facher Interquartilabstand)\n    step = (Q3 - Q1) * 1.5\n    \n    # Ausreißer anzeigen\n    print(\"Data points considered outliers for the feature '{}':\".format(feature))\n    outliers = list(log_data[~((log_data[feature] >= Q1 - step) & (log_data[feature] <= Q3 + step))].index.values)\n    display(log_data[~((log_data[feature] >= Q1 - step) & (log_data[feature] <= Q3 + step))])\n    outliers_list.extend(outliers)\n    \nprint(\"List of Outliers -> {}\".format(outliers_list))\nduplicate_outliers_list = list(set([x for x in outliers_list if outliers_list.count(x) >= 2]))\nduplicate_outliers_list.sort()\nprint(\"\\nList of Common Outliers -> {}\".format(duplicate_outliers_list))\n\n# die Indizes für Datenpunkte auswählen, die wir entfernen möchten\noutliers  = duplicate_outliers_list\n\n# die Ausreißer entfernen, falls welche angegeben wurden\ngood_data = log_data.drop(log_data.index[outliers]).reset_index(drop = True)\ndisplay(good_data)","metadata":{"execution":{"iopub.status.busy":"2021-07-16T18:01:46.103671Z","iopub.execute_input":"2021-07-16T18:01:46.104212Z","iopub.status.idle":"2021-07-16T18:01:46.221345Z","shell.execute_reply.started":"2021-07-16T18:01:46.104157Z","shell.execute_reply":"2021-07-16T18:01:46.220224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Überprüfen, ob die Stichproben Ausreißer sind\n# => Sie sind nicht Ausreißer. Die obere Auswertung kann so bleiben.\nprint(43 in outliers_list)\nprint(344 in outliers_list)\nprint(122 in outliers_list)","metadata":{"execution":{"iopub.status.busy":"2021-07-16T18:01:46.223222Z","iopub.execute_input":"2021-07-16T18:01:46.223646Z","iopub.status.idle":"2021-07-16T18:01:46.230081Z","shell.execute_reply.started":"2021-07-16T18:01:46.223598Z","shell.execute_reply":"2021-07-16T18:01:46.228924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dimensionsreduktion","metadata":{}},{"cell_type":"code","source":"#Hauptkomponentenanalyse\nfrom sklearn.decomposition import PCA\n\n# PCA auf die guten Daten mit der gleichen Anzahl von Dimensionen wie Merkmale anwenden\npca = PCA()\npca.fit_transform(good_data)\n\n# die gleiche PCA-Transformation auf die drei Stichproben-Datenpunkte anwenden\npca_samples = pca.transform(log_samples)","metadata":{"execution":{"iopub.status.busy":"2021-07-16T18:01:46.23167Z","iopub.execute_input":"2021-07-16T18:01:46.232081Z","iopub.status.idle":"2021-07-16T18:01:46.295216Z","shell.execute_reply.started":"2021-07-16T18:01:46.232045Z","shell.execute_reply":"2021-07-16T18:01:46.294062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Die Varianz, die durch jede Hauptkomponente erklärt wird\nexplained_variances = pca.explained_variance_ratio_\n\nprint(\"Proportion of the variance explained by each dimension\")\nprint(\"\\n\".join([\"{}: {:1.3f}\".format(i+1,val) for i,val in enumerate(explained_variances)]))\n\n# Alle Varianz, die durch die n-te Hauptkomponente erklärt wird\ncumulative_variance = [explained_variances[:i+1].sum() for i in range(len(explained_variances))]\nprint(\"\\nTotal variance explained by the first N principal compoments\")\nprint(\"\\n\".join([\"{}: {:1.3f}\".format(i+1,val) for i,val in enumerate(cumulative_variance)]))","metadata":{"execution":{"iopub.status.busy":"2021-07-16T18:01:46.298666Z","iopub.execute_input":"2021-07-16T18:01:46.29908Z","iopub.status.idle":"2021-07-16T18:01:46.308404Z","shell.execute_reply.started":"2021-07-16T18:01:46.299044Z","shell.execute_reply":"2021-07-16T18:01:46.307296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def pca_results(good_data, pca):\n\t'''\n\tCreate a DataFrame of the PCA results\n\tIncludes dimension feature weights and explained variance\n\tVisualizes the PCA results\n\t'''\n\n\t# Dimensionsindexierung\n\tdimensions = dimensions = ['Dimension {}'.format(i) for i in range(1,len(pca.components_)+1)]\n\n\t# PCA Komponenten\n\tcomponents = pd.DataFrame(np.round(pca.components_, 4), columns = list(good_data.keys()))\n\tcomponents.index = dimensions\n\n\t# PCA explained variance\n\tratios = pca.explained_variance_ratio_.reshape(len(pca.components_), 1)\n\tvariance_ratios = pd.DataFrame(np.round(ratios, 4), columns = ['Explained Variance'])\n\tvariance_ratios.index = dimensions\n\n\t# eine Barplot-Visualisierung\n\tfig, ax = plt.subplots(figsize = (14,8))\n\n\t# die Merkmalsgewichte als Funktion der Komponenten plotten\n\tcomponents.plot(ax = ax, kind = 'bar');\n\tax.set_ylabel(\"Feature Weights\")\n\tax.set_xticklabels(dimensions, rotation=0)\n\n\n\t# die erklärten Varianzverhältnisse anzeigen\n\tfor i, ev in enumerate(pca.explained_variance_ratio_):\n\t\tax.text(i-0.40, ax.get_ylim()[1] + 0.05, \"Explained Variance\\n          %.4f\"%(ev))\n\n\t# Einen verketteten DataFrame zurückgeben\n\treturn pd.concat([variance_ratios, components], axis = 1)","metadata":{"execution":{"iopub.status.busy":"2021-07-16T18:01:46.310209Z","iopub.execute_input":"2021-07-16T18:01:46.310662Z","iopub.status.idle":"2021-07-16T18:01:46.326515Z","shell.execute_reply.started":"2021-07-16T18:01:46.310613Z","shell.execute_reply":"2021-07-16T18:01:46.325362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pca_results = pca_results(good_data, pca)","metadata":{"execution":{"iopub.status.busy":"2021-07-16T18:01:46.328532Z","iopub.execute_input":"2021-07-16T18:01:46.329106Z","iopub.status.idle":"2021-07-16T18:01:46.747204Z","shell.execute_reply.started":"2021-07-16T18:01:46.328961Z","shell.execute_reply":"2021-07-16T18:01:46.746149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# PCA mit nur zwei Dimensionen an die guten Daten anpassen\npca = PCA(n_components=2,random_state=0)\npca.fit(good_data)\n\n# eine PCA-Transformation der guten Daten anwenden\nreduced_data = pca.transform(good_data)\n\n# eine PCA-Transformation auf die drei Stichproben-Datenpunkte anwenden\npca_samples = pca.transform(log_samples)\n\n# einen DataFrame für die reduzierten Daten erstellen\nreduced_data = pd.DataFrame(reduced_data, columns = ['Dimension 1', 'Dimension 2'])\n#display(reduced_data)","metadata":{"execution":{"iopub.status.busy":"2021-07-16T18:01:46.748632Z","iopub.execute_input":"2021-07-16T18:01:46.748996Z","iopub.status.idle":"2021-07-16T18:01:46.766562Z","shell.execute_reply.started":"2021-07-16T18:01:46.748961Z","shell.execute_reply":"2021-07-16T18:01:46.765436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(1, 1, figsize=(10, 6))\nfig.suptitle('Scatterplot of First Two Principal Components', fontsize=15)\nimg = ax.scatter(reduced_data[\"Dimension 1\"], reduced_data[\"Dimension 2\"],\n                 c=\"#9A0EEA\", s=100, alpha=0.4, linewidths=0)","metadata":{"execution":{"iopub.status.busy":"2021-07-16T18:01:46.768484Z","iopub.execute_input":"2021-07-16T18:01:46.769113Z","iopub.status.idle":"2021-07-16T18:01:46.975901Z","shell.execute_reply.started":"2021-07-16T18:01:46.76907Z","shell.execute_reply":"2021-07-16T18:01:46.974527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.2 Modellierung\nEine wichtige Frage ist es, welche Clustering-Methode geeignet ist. Hier ist das Gaußsische Mischungsmodell ausgewählt, da es uns aufgefallen ist, dass die Daten eine Tendenz zur Überlappung haben. Daher ist GMM die beste Option.","metadata":{}},{"cell_type":"markdown","source":"## Cluster erzeugen","metadata":{}},{"cell_type":"code","source":"# GMM und silhouette_score importieren\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.metrics import silhouette_score\n\n# Unterschiedliche Werte für die Anzahl der zu verwendenden Cluster (von 2 bis 11)\nnum_clusters_list = range(2, 11+1)\nnum_cluster_values = len(num_clusters_list)\n\n# Initialisieren die Listen, um Vorhersagen, Schwerpunkte und Bewertungspunkte zu speichern\npreds_list = [[]] * num_cluster_values         # Predictions\nsample_preds_list = [[]] * num_cluster_values  # Predictions for sample data\ncenters_list = [0] * num_cluster_values        # Centers\nscore_list = [0] * num_cluster_values          # scores\n\n# For each value of clusters to consider, perform clustering and make\n# Vorhersagen, die Zentren speichern und die Punktzahl berechnen\nfor i, num_clusters  in enumerate(num_clusters_list):\n    # den Clustering-Algorithmus auf die reduzierten Daten anwenden\n    clusterer = GaussianMixture(n_components=num_clusters, covariance_type='diag', random_state=4)\n    clusterer.fit(reduced_data)\n\n    # Vorhersage des Clusters für jeden Datenpunkt\n    preds_list[i] = clusterer.predict(reduced_data)\n\n    # die Clusterzentren finden\n    centers_list[i] = clusterer.means_\n\n    # Vorhersage des Clusters für jeden transformierten Stichproben-Datenpunkte\n    sample_preds_list[i] = clusterer.predict(pca_samples)\n\n    # den mittleren Silhouetteskoeffizienten für die Anzahl der ausgewählten Cluster berechnen\n    score_list[i] = silhouette_score(reduced_data, preds_list[i],\n                                     metric='euclidean', sample_size=None,\n                                     random_state=4)","metadata":{"execution":{"iopub.status.busy":"2021-07-16T18:01:46.977621Z","iopub.execute_input":"2021-07-16T18:01:46.978086Z","iopub.status.idle":"2021-07-16T18:01:47.516959Z","shell.execute_reply.started":"2021-07-16T18:01:46.978034Z","shell.execute_reply":"2021-07-16T18:01:47.515423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import operator\n\n# Holt den k-Wert, der zum besten Clustering-Modell führte\nbest_k_index = max(enumerate(score_list), key=operator.itemgetter(1))[0]\nk = num_clusters_list[best_k_index]\n\n# Holt die Vorhersagen, Zentren und Punkte für das beste Clustering-Modell\npreds = preds_list[best_k_index]\ncenters = centers_list[best_k_index]\nscore = score_list[best_k_index]\n\nprint(\"Bestes Ergebnis kann es erreichen wenn k={} (score={:0.3f})\".format(k, score))","metadata":{"execution":{"iopub.status.busy":"2021-07-16T18:01:47.5232Z","iopub.execute_input":"2021-07-16T18:01:47.523758Z","iopub.status.idle":"2021-07-16T18:01:47.543642Z","shell.execute_reply.started":"2021-07-16T18:01:47.523708Z","shell.execute_reply":"2021-07-16T18:01:47.542219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Farbpalette für Clusterpunkte\nPALLETTE =  [\"#FF8000\", \"#5BA1CF\", \"#9621E2\", \"#FF4F40\", \"#73AD21\",\n            \"#FFEC48\", \"#DE1BC2\", \"#29D3D1\", \"#B4F924\", \"#666666\", \"#AF2436\"]\n\n# Für jeden der Anzahl der berücksichtigten Clusterwerte ein Unterdiagramm erstellen\nf, axes = plt.subplots(int(np.ceil(len(num_clusters_list) / 2.0)), 2,\n                       figsize=(10, 14), sharex=True, sharey=True)\naxes = [item for row in axes for item in row] # Unroll axes to a flat list\n\nfor i in range(len(num_clusters_list)):\n    # Datenpunkte plotten, wobei je nach Clusterzuordnung eine andere Farbe zugewiesen wird\n    ax = axes[i]\n    colors  = [PALLETTE[y] for y in preds_list[i]]\n    ax.scatter(reduced_data[\"Dimension 1\"],\n               reduced_data[\"Dimension 2\"],\n               c=colors, s=100, alpha=0.4, linewidths=0)\n\n    # den Titel für den Nebenplot festlegen, einschließlich der Anzahl der verwendeten Cluster und\n    # die Silhouetten-Punktzahl.\n    ax.set_title(\n        \"K = {k}    (Silhouettenkoeffizieten = {score:.3f})\"\\\n        \"\".format(k=num_clusters_list[i], score=score_list[i]),\n        fontdict= {\"style\": \"italic\", \"size\": 10})\n\n# den Titel für die Figur festlegen\nt = f.suptitle('Ergebnissen des Clusterings (and Silhouettenkoeffizienten) mithilfe GMM',\n               fontsize=15,\n               fontdict={\"fontweight\": \"extra bold\"})","metadata":{"execution":{"iopub.status.busy":"2021-07-16T18:01:47.546685Z","iopub.execute_input":"2021-07-16T18:01:47.547245Z","iopub.status.idle":"2021-07-16T18:01:49.081037Z","shell.execute_reply.started":"2021-07-16T18:01:47.547192Z","shell.execute_reply":"2021-07-16T18:01:49.080029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def cluster_viz(reduced_data, labels, centers=None, reduced_samples=None, title=\"\", legend_labels=[\"Segment 1\",\"Segment 2\"]):\n   \n    f, ax = plt.subplots(1, 1,  figsize=(12, 10))\n\n    PALLETTE =  [\"#FF8000\", \"#5BA1CF\"]\n    colors  = [PALLETTE[y] for y in labels]\n\n    classes = np.unique(labels)\n    for class_id in classes:\n        ax.scatter(reduced_data[\"Dimension 1\"][labels==class_id],\n                   reduced_data[\"Dimension 2\"][labels==class_id],\n                   label=legend_labels[class_id],\n                   c=PALLETTE[class_id], s=100, alpha=0.4, linewidths=0)\n\n#     ax.scatter(reduced_data[\"Dimension 1\"][labels==class_id],\n#        reduced_data[\"Dimension 2\"][labels==class_id],\n#        c=colors, s=100, alpha=0.4, linewidths=0)\n\n\n    # Mittelpunkte plotten\n    if centers is not None:\n        for i, c in enumerate(centers):\n            ax.scatter(x = c[0], y = c[1], color = 'white', edgecolors=PALLETTE[i], \\\n                       alpha=1, linewidth=2, marker = 'o', s=300);\n            ax.scatter(x = c[0], y = c[1], marker='${}$'.format(i), alpha=1, s=100);\n\n    # die Stichprobe-Datenpunkte plotten\n    if reduced_samples is not None:\n        ax.scatter(x = reduced_samples[:,0], y = reduced_samples[:,1], \\\n                   s = 300,\n                   linewidth = 2,\n                   color = 'black',\n                   facecolors = 'none',\n                   edgecolors='black',\n                   marker = 'o');\n\n        for i in range(len(reduced_samples)):\n            ax.scatter(x = reduced_samples[i,0]+0.4, y = reduced_samples[i,1], marker='$({})$'.format(i), alpha = 1, color='black', s=350);\n\n    ax.legend(loc=\"lower right\", frameon=False)\n\n    # Titel des Plots festlegen\n    ax.set_title(title);","metadata":{"execution":{"iopub.status.busy":"2021-07-16T18:01:49.082228Z","iopub.execute_input":"2021-07-16T18:01:49.082663Z","iopub.status.idle":"2021-07-16T18:01:49.095812Z","shell.execute_reply.started":"2021-07-16T18:01:49.082631Z","shell.execute_reply":"2021-07-16T18:01:49.094973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cluster_viz(reduced_data,\n            labels=preds,\n            centers=centers,\n            reduced_samples=pca_samples,\n            title=\"Customer Segments Learned by Model on PCA-Reduced Data\")","metadata":{"execution":{"iopub.status.busy":"2021-07-16T18:01:49.096905Z","iopub.execute_input":"2021-07-16T18:01:49.097324Z","iopub.status.idle":"2021-07-16T18:01:49.7343Z","shell.execute_reply.started":"2021-07-16T18:01:49.097292Z","shell.execute_reply":"2021-07-16T18:01:49.733097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Zentroid berechnen","metadata":{}},{"cell_type":"code","source":"# die Inversion der Transformation der Zentren\nlog_centers = pca.inverse_transform(centers)\n\n# die Zentren potenzieren (Umkehrung der Log-Transformation)\ntrue_centers = np.exp(log_centers)\n\n# die wahren Zentrenn anzeigen\nsegments = ['Segment {}'.format(i) for i in range(0,len(centers))]\ntrue_centers = pd.DataFrame(np.round(true_centers), columns = data.keys())\ntrue_centers.index = segments\ndisplay(true_centers)","metadata":{"execution":{"iopub.status.busy":"2021-07-16T18:01:49.735841Z","iopub.execute_input":"2021-07-16T18:01:49.736207Z","iopub.status.idle":"2021-07-16T18:01:49.755502Z","shell.execute_reply.started":"2021-07-16T18:01:49.736175Z","shell.execute_reply":"2021-07-16T18:01:49.753995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"(true_centers - data.mean())","metadata":{"execution":{"iopub.status.busy":"2021-07-16T18:01:49.757429Z","iopub.execute_input":"2021-07-16T18:01:49.758002Z","iopub.status.idle":"2021-07-16T18:01:49.784559Z","shell.execute_reply.started":"2021-07-16T18:01:49.75792Z","shell.execute_reply":"2021-07-16T18:01:49.78299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_preds = sample_preds_list[best_k_index]\n\n# die Vorhersagen ausdrucken\npotential_cust_segments = [\"Einzelhandel\", \"Restaurant\"]\nfor i, pred in enumerate(sample_preds):\n    print(\"Client {} predicted to be in Segment {} ({})\".format(i, pred,\n                                                 potential_cust_segments[pred]))","metadata":{"execution":{"iopub.status.busy":"2021-07-16T18:01:49.786889Z","iopub.execute_input":"2021-07-16T18:01:49.787424Z","iopub.status.idle":"2021-07-16T18:01:49.797484Z","shell.execute_reply.started":"2021-07-16T18:01:49.787369Z","shell.execute_reply":"2021-07-16T18:01:49.796426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"(true_centers - data.mean()) / data.std()","metadata":{"execution":{"iopub.status.busy":"2021-07-16T18:01:49.798506Z","iopub.execute_input":"2021-07-16T18:01:49.798839Z","iopub.status.idle":"2021-07-16T18:01:49.822495Z","shell.execute_reply.started":"2021-07-16T18:01:49.798797Z","shell.execute_reply":"2021-07-16T18:01:49.821177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Wiedereinführung des Merkmals *Channel* in den Datensatz","metadata":{}},{"cell_type":"code","source":"# die vollständigen Daten laden\ntry:\n    full_data = pd.read_csv(\"../input/wholesale-customers-data-set/Wholesale customers data.csv\")\nexcept:\n    print(\"Dataset could not be loaded.\")","metadata":{"execution":{"iopub.status.busy":"2021-07-16T18:01:49.823911Z","iopub.execute_input":"2021-07-16T18:01:49.824205Z","iopub.status.idle":"2021-07-16T18:01:49.8517Z","shell.execute_reply.started":"2021-07-16T18:01:49.824176Z","shell.execute_reply":"2021-07-16T18:01:49.850443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def channel_results(reduced_data, outliers, pca_samples):\n\n\n\t# Überprüfen,ob das Dataset ladbar ist.\n\ttry:\n\t    full_data = pd.read_csv(\"../input/wholesale-customers-data-set/Wholesale customers data.csv\")\n\texcept:\n\t    print(\"Dataset could not be loaded. Is the file missing?\")       \n\t    return False\n\n\t# den Channel-DataFrame erstellen\n\tchannel = pd.DataFrame(full_data['Channel'], columns = ['Channel'])\n\tchannel = channel.drop(channel.index[outliers]).reset_index(drop = True)\n\tlabeled = pd.concat([reduced_data, channel], axis = 1)\n\t\n\t# das Cluster-Plot generieren\n\tfig, ax = plt.subplots(figsize = (14,8))\n\n\t# Farbkarte\n\tcmap = cm.get_cmap('gist_rainbow')\n\n\t# die Punkte basierend auf dem zugewiesenen Kanal färben\n\tlabels = ['Hotel/Restaurant/Cafe', 'Retailer']\n\tgrouped = labeled.groupby('Channel')\n\tfor i, channel in grouped:   \n\t    channel.plot(ax = ax, kind = 'scatter', x = 'Dimension 1', y = 'Dimension 2', \\\n\t                 color = cmap((i-1)*1.0/2), label = labels[i-1], s=30);\n\t    \n\t# die transformierte Datenpunkte plotten \n\tfor i, sample in enumerate(pca_samples):\n\t\tax.scatter(x = sample[0], y = sample[1], \\\n\t           s = 200, linewidth = 3, color = 'black', marker = 'o', facecolors = 'none');\n\t\tax.scatter(x = sample[0]+0.25, y = sample[1]+0.3, marker='$%d$'%(i), alpha = 1, s=125);\n\n\t# Set plot title\n\tax.set_title(\"PCA-Reduced Data Labeled by 'Channel'\\nTransformed Sample Data Circled\");","metadata":{"execution":{"iopub.status.busy":"2021-07-16T18:01:49.853254Z","iopub.execute_input":"2021-07-16T18:01:49.853569Z","iopub.status.idle":"2021-07-16T18:01:49.866208Z","shell.execute_reply.started":"2021-07-16T18:01:49.853539Z","shell.execute_reply":"2021-07-16T18:01:49.864693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# die Clustering-Ergebnisse basierend auf \"Channel\"-Daten anzeigen\nchannel_results(reduced_data, outliers, pca_samples)","metadata":{"execution":{"iopub.status.busy":"2021-07-16T18:01:49.867406Z","iopub.execute_input":"2021-07-16T18:01:49.8678Z","iopub.status.idle":"2021-07-16T18:01:50.271383Z","shell.execute_reply.started":"2021-07-16T18:01:49.867747Z","shell.execute_reply":"2021-07-16T18:01:50.270161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3. Projekterweiterung\nVorhersagen von Segmentierung der Neukunden durch die logistische Regression.\n* **Problemstellung**: Angenommen, wir haben 10 neue Kunden. Zu welchem Segment gehören sie? \n* **Erklärende Variablen**: Fresh, Milk, Grocery, Detergents_Paper, Delicatessen\n* **Zielvariablen**: Segment","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2021-07-16T18:01:50.272729Z","iopub.execute_input":"2021-07-16T18:01:50.273098Z","iopub.status.idle":"2021-07-16T18:01:50.277765Z","shell.execute_reply.started":"2021-07-16T18:01:50.273063Z","shell.execute_reply":"2021-07-16T18:01:50.276653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Daten auf den ursprünglichen Zustand transformieren\ndata_preds = preds_list[best_k_index]\n\nnew_data = np.exp(good_data)\n\nnew_data = pd.DataFrame(new_data, columns = data.keys())\nsegment = preds_list[best_k_index]\n\n# Ein neues Merkmal hinzufügen\nnew_data['Segment'] = segment\n\n# Neuer Datensatz sieht so aus\nnew_data.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-16T18:01:50.279564Z","iopub.execute_input":"2021-07-16T18:01:50.280023Z","iopub.status.idle":"2021-07-16T18:01:50.309116Z","shell.execute_reply.started":"2021-07-16T18:01:50.279974Z","shell.execute_reply":"2021-07-16T18:01:50.307989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Datensatz in der Merkmale und Zielvariable aufteilen\nfeature_cols = ['Fresh', 'Milk', 'Grocery', 'Frozen','Detergents_Paper','Delicatessen']\nX = new_data[feature_cols] # erklärende Variablen\ny = new_data.Segment # Zielvariablen","metadata":{"execution":{"iopub.status.busy":"2021-07-16T18:01:50.310683Z","iopub.execute_input":"2021-07-16T18:01:50.311142Z","iopub.status.idle":"2021-07-16T18:01:50.317737Z","shell.execute_reply.started":"2021-07-16T18:01:50.311095Z","shell.execute_reply":"2021-07-16T18:01:50.316752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# X und y in Trainings- und Testsätze aufteilen (25-75)\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.25,random_state=0)","metadata":{"execution":{"iopub.status.busy":"2021-07-16T18:01:50.319337Z","iopub.execute_input":"2021-07-16T18:01:50.319914Z","iopub.status.idle":"2021-07-16T18:01:50.331897Z","shell.execute_reply.started":"2021-07-16T18:01:50.319865Z","shell.execute_reply":"2021-07-16T18:01:50.330776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Instanziieren des Modells (mit den Standardparametern)\n# Das neue Model definieren\nlr_model = LogisticRegression()\n\n# das Modell mit Daten anpassen\nlr_model.fit(X_train,y_train)\n\n# Mit dem Testingdatensatz testen\ny_pred=lr_model.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2021-07-16T18:01:50.33375Z","iopub.execute_input":"2021-07-16T18:01:50.334489Z","iopub.status.idle":"2021-07-16T18:01:50.367192Z","shell.execute_reply.started":"2021-07-16T18:01:50.334435Z","shell.execute_reply":"2021-07-16T18:01:50.366081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"a = pd.DataFrame({'Actual value': y_test, 'Predicted value':y_pred})\na.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-16T18:01:50.368964Z","iopub.execute_input":"2021-07-16T18:01:50.369424Z","iopub.status.idle":"2021-07-16T18:01:50.387044Z","shell.execute_reply.started":"2021-07-16T18:01:50.369377Z","shell.execute_reply":"2021-07-16T18:01:50.386136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Validierung mit der Konfusionsmatrix\n# die Metrikklasse importieren\nfrom sklearn import metrics\ncnf_matrix = metrics.confusion_matrix(y_test, y_pred)\ncnf_matrix","metadata":{"execution":{"iopub.status.busy":"2021-07-16T18:01:50.388533Z","iopub.execute_input":"2021-07-16T18:01:50.389012Z","iopub.status.idle":"2021-07-16T18:01:50.400491Z","shell.execute_reply.started":"2021-07-16T18:01:50.388961Z","shell.execute_reply":"2021-07-16T18:01:50.399685Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class_names=[0,1] #Name der Klasse\nfig, ax = plt.subplots()\ntick_marks = np.arange(len(class_names))\nplt.xticks(tick_marks, class_names)\nplt.yticks(tick_marks, class_names)\n\n# Heatmap erstellen\nsns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\nax.xaxis.set_label_position(\"top\")\nplt.tight_layout()\nplt.title('Confusion matrix', y=1.1)\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')","metadata":{"execution":{"iopub.status.busy":"2021-07-16T18:01:50.401804Z","iopub.execute_input":"2021-07-16T18:01:50.402348Z","iopub.status.idle":"2021-07-16T18:01:50.995816Z","shell.execute_reply.started":"2021-07-16T18:01:50.402296Z","shell.execute_reply":"2021-07-16T18:01:50.994669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\nprint(\"Precision:\",metrics.precision_score(y_test, y_pred))\nprint(\"Recall:\",metrics.recall_score(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2021-07-16T18:01:50.997123Z","iopub.execute_input":"2021-07-16T18:01:50.997426Z","iopub.status.idle":"2021-07-16T18:01:51.009303Z","shell.execute_reply.started":"2021-07-16T18:01:50.997395Z","shell.execute_reply":"2021-07-16T18:01:51.008097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Ergebnisse","metadata":{}},{"cell_type":"code","source":"# Angenommen, die neuen Kunden haben folgende Ausgaben\nnew_customer = {'Fresh': [15000,22276,7000,30500,11020,1000,9858,34000,9621,25000],\n                  'Milk': [4312,2000,3353,9831,5100,5000,12000,11500,1373,3470],\n                  'Grocery': [11008,9876,43245,11246,5611,14100,2200,23000,985,18000],\n                'Frozen': [9882,3712,631,16500,5686,800,230,18800,8530,6200],\n                'Detergents_Paper': [1451,4231,3256,1221,5545,2500,650,1609,980,3800],\n                'Delicatessen': [400,1001,3534,9832,10000,800,916,1800,60,2489]\n                  }\n\npredict = pd.DataFrame(new_customer,columns= ['Fresh', 'Milk','Grocery','Frozen','Detergents_Paper','Delicatessen'])\ny_pred = lr_model.predict(predict)\npredict['Segment'] = y_pred\ndisplay(predict)","metadata":{"execution":{"iopub.status.busy":"2021-07-16T18:01:51.010873Z","iopub.execute_input":"2021-07-16T18:01:51.01117Z","iopub.status.idle":"2021-07-16T18:01:51.035899Z","shell.execute_reply.started":"2021-07-16T18:01:51.011143Z","shell.execute_reply":"2021-07-16T18:01:51.034827Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Fazit: \n\n* Mithilfe der Clusteringsmethode können wir die Kunden im Großhandel in zwei Segmente klassifizieren.\n\n* Nach dem Clustering kann man das Modell auf die Verkauf/Marketing/Lieferservice-Strategien verwenden.","metadata":{}}]}