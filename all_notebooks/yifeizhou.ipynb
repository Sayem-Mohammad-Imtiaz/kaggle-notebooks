{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"1. *load data and packages"},{"metadata":{},"cell_type":"markdown","source":"Group member:\nYifei Zhou\nYinghong Xu\nQian Qiao\nDaniel Saunders"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/womens-ecommerce-clothing-reviews/Womens Clothing E-Commerce Reviews.csv\",index_col = 0)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Embedding, LSTM, SpatialDropout1D,SimpleRNN,Flatten\nfrom keras.layers import Dropout\nfrom sklearn.model_selection import train_test_split\nfrom keras.utils.np_utils import to_categorical\nimport re\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nfrom nltk.sentiment.util import *\nimport scikitplot as skplt\nfrom wordcloud import WordCloud\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer \nfrom nltk.stem import PorterStemmer, LancasterStemmer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nimport matplotlib.pyplot as plt \nfrom textblob import TextBlob\nimport nltk\nimport warnings\nwarnings.filterwarnings('ignore') ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"2.feature preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"# only take the review column as predictor and recommended IND as target in the dataset \n# rename the predictor and target\ndata = df[[\"Title\",\"Review Text\",\"Recommended IND\"]]\ndata = data.rename(columns = {\"Review Text\":\"text\",\"Recommended IND\":\"sentiment\"})\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# calculate the null values in the dataset\ndata.text.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# the null values only occupies a very small proportion thus we can directly delete them\ndata = data[~data.text.isna()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(data.sentiment.isna().sum(),data.text.isna().sum())\n# now no na values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's check whether the sentiment columns contain other values or not\ndata.sentiment.unique()\n# Great, no other values, just binary result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def count_exclamation_mark(string_text):\n    count = 0\n    for char in string_text:\n        if char == '!':\n            count += 1\n    return count","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# calculate the ! number in text\ndata['count_exc'] = data['text'].apply(count_exclamation_mark)\ndata.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# transfer all the text into lower case\ndata['text'] = data['text'].str.lower()\n# clear all the non-related notation\ndata['text'] = data['text'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]','',x)))\ndata.head(6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# generate new feature, the length of text\ndata['text_length'] = data['text'].apply(len)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# view the distribution of the target variable\nprint(len(data[data.sentiment == 1]))\nprint(len(data[data.sentiment == 0 ]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# generate new feature, the polarity of the one review text\ndata['Polarity'] = data['text'].apply(lambda x: TextBlob(x).sentiment.polarity)\ndata.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# manully set the stop words in this situation\nstop_words = list(set(stopwords.words('english')))\nclothes_list =['dress','sweater','shirt',\n               'skirt','material', 'white', 'black',\n              'jeans', 'fabric', 'color','order', 'wear']\n\nfor i in clothes_list:\n    stop_words.append(i)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def stopwords_removal(messy_str):\n    messy_str = word_tokenize(messy_str)\n    return [word.lower() for word in messy_str \n            if word.lower() not in stop_words ]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# remove all the words which are in the stop word list\ndata['text'] = data['text'].apply(stopwords_removal)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# stemming transformation of text\nporter = PorterStemmer()\ndef stem_update(text_list):\n    text_list_new = []\n    for word in text_list:\n        word = porter.stem(word)\n        text_list_new.append(word) \n    return text_list_new","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['text'] = data['text'].apply(stem_update)\ndata['text'].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['text'] = data['text'].apply(lambda x:' '.join(x))\ndata['text'].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create word cloud\npos_df = data[data.sentiment== 1]\nneg_df = data[data.sentiment== 0]\npos_df.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pos_words =[]\nneg_words = []\n\nfor review in pos_df.text:\n    pos_words.append(review) \npos_words = ' '.join(pos_words)\npos_words[:60]\n\nfor review in neg_df.text:\n    neg_words.append(review)\nneg_words = ' '.join(neg_words)\nneg_words[:200]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# word cloud for positive word\nwordcloud = WordCloud().generate(pos_words)\n\nwordcloud = WordCloud(background_color=\"white\",max_words=len(pos_words),\\\n                      max_font_size=40, relative_scaling=.5, colormap='summer').generate(pos_words)\nplt.figure(figsize=(13,13))\nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# word cloud for negative word\nwordcloud = WordCloud().generate(neg_words)\n\nwordcloud = WordCloud(background_color=\"white\",max_words=len(neg_words),\\\n                      max_font_size=40, relative_scaling=.5, colormap='gist_heat').generate(neg_words)\nplt.figure(figsize=(13,13))\nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# tokenlizing(vectorizing) the text, which transforms the data into tensor format\nsamples = data[\"text\"].tolist()\nmaxlen = 100\nmax_words = 10000\ntokenizer = Tokenizer(num_words=max_words)\ntokenizer.fit_on_texts(samples)\nsequences = tokenizer.texts_to_sequences(samples)#transfer string into number\nword_index = tokenizer.word_index\nprint('Found %s unique tokens.' % len(word_index))\nX = pad_sequences(sequences, maxlen=maxlen)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# generate the target label\nlabels =  pd.get_dummies(data['sentiment']).values\nprint('Shape of data tensor:', X.shape)\nprint('Shape of label tensor:', labels.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# generate the random dataset\nindices = np.arange(data.shape[0])\nnp.random.shuffle(indices)\ndata2 = X[indices]\nlabels = labels[indices]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data2.shape# contains all the text information","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a = data[['Polarity','text_length']].values #contains the created features information\na.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"we split the dataset into three parts, training ,validation , test, the ratio is : 75% :25 :25%"},{"metadata":{"trusted":true},"cell_type":"code","source":"# the train,val,test is the data about text feature and train2,val2,test2 is the data about two creating new features,finailly we merge them together \ntraining_samples = 11320\nvalidation_samples = 15848\nx_train = data2[:training_samples]\nx_train2 = a[:training_samples]\ny_train = labels[:training_samples]\nx_val = data2[training_samples: validation_samples] \nx_val2 = a[training_samples: validation_samples] \ny_val = labels[training_samples: validation_samples]\nx_test = data2[validation_samples:]\nx_test2 = a[validation_samples:]\ny_test = labels[validation_samples:]\n# for text feature, we still need following preprocessing step\nx_train = pad_sequences(x_train, maxlen=maxlen)\nx_val = pad_sequences(x_val, maxlen=maxlen)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# concat all the features\nx_train = np.hstack((x_train2,x_train))\nx_val = np.hstack((x_val2,x_val))\nx_test = np.hstack((x_test2,x_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# This is the baseline, cause in the dataset, lable 1 occupies 82% percent\n(np.sum(data['sentiment'] == 1)/data.shape[0]) * 100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# First, let's build the simple embedding model\ndef build_model():\n    model = Sequential()\n    model.add(Embedding(max_words, 102, input_length=maxlen+2))\n    model.add(Flatten())\n    model.add(Dense(128, activation='relu'))\n    model.add(Dense(64, activation='relu'))\n    model.add(Dense(32, activation='relu'))\n    model.add(Dense(2, activation='softmax'))\n    model.compile(optimizer='rmsprop',\n              loss='binary_crossentropy',\n              metrics=['acc'])\n    return model\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = build_model()\nmodel.summary()\nhistory = model.fit(x_train, y_train,\n                    epochs=7,\n                    batch_size=64,\n                    validation_data=(x_val, y_val))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# First, let's build the simple NN model. Considering that the word vector will be a sparse matrix thus we add one embedding layer\ndef build_model():\n    \n    model = Sequential()\n    model.add(Embedding(max_words, 102,input_length=maxlen+2))\n    model.add(Flatten())\n    model.add(Dense(128, activation='relu'))\n    model.add(Dropout(0.3))\n    model.add(Dense(64, activation='relu'))\n    model.add(Dropout(0.2))\n    model.add(Dense(32, activation='relu'))\n    model.add(Dropout(0.2))\n    model.add(Dense(2, activation='softmax'))\n    model.compile(optimizer='rmsprop',\n              loss='binary_crossentropy',\n              metrics=['acc'])\n    return model\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = build_model()\nmodel.summary()\nhistory = model.fit(x_train, y_train,\n                    epochs=7,\n                    batch_size=64,\n                    validation_data=(x_val, y_val))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"acc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(1, len(acc) + 1)\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\nplt.figure()\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.show()\n#  training loss keep decreasing while val loss keep increasing , overfitting","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.evaluate(x_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# recursion NN is a classic method to process text problem\ndef build_RNN():\n    model = Sequential() \n    model.add(Embedding(max_words, 102, input_length=maxlen+2)) \n    model.add(Dropout(0.3))\n    model.add(SimpleRNN(32)) \n    model.add(Dropout(0.3))\n    model.add(Dense(2, activation='softmax'))\n    model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc']) \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"RNN_model = build_RNN()\nRNN_model.summary()\nhistory_RNN = RNN_model.fit(x_train, y_train,\n                    epochs=7,\n                    batch_size=64,\n                    validation_data=(x_val, y_val))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"acc = history_RNN.history['acc']\nval_acc = history_RNN.history['val_acc']\nloss = history_RNN.history['loss']\nval_loss = history_RNN.history['val_loss']\nepochs = range(1, len(acc) + 1)\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\nplt.figure()\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.show()\n# training loss keeps decreasing while val loss keeps decreasing,overfitting","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"RNN_model.evaluate(x_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# RNN and embedding model both exist some problems, let's try LSTM, another advanced version of RNN\ndef build_LSTM():\n    embed_dim = 128\n    lstm_out = 196\n    max_features = 2000\n    model = Sequential()\n    model.add(Embedding(max_features, embed_dim,input_length = x_train.shape[1]))\n    model.add(SpatialDropout1D(0.4))\n    model.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\n    model.add(Dense(2,activation='softmax'))\n    model.compile(loss = 'binary_crossentropy', optimizer='adam',metrics = ['accuracy'])\n    return model\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"LSTM_model = build_LSTM()\nLSTM_model.summary()\nhistory_LSTM = LSTM_model.fit(x_train, y_train,\n                    epochs=7,\n                    batch_size=64,\n                    validation_data=(x_val, y_val))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"acc = history_LSTM.history['accuracy']\nval_acc = history_LSTM.history['val_accuracy']\nloss = history_LSTM.history['loss']\nval_loss = history_LSTM.history['val_loss']\nepochs = range(1, len(acc) + 1)\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\nplt.figure()\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.show()\n# training loss decreases and val loss flucates,better than previous two ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"LSTM_model.evaluate(x_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Because our dataset is not balanced, thus we use another metrics---tpr and tnr to evalute our model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pos_count, neg_count, pos_correct, neg_correct = 0, 0, 0, 0\nfor x in range(len(x_test)):\n    \n    result = model.predict(x_test[x].reshape(1,x_test.shape[1]),batch_size=1,verbose = 2)[0]\n   \n    if np.argmax(result) == np.argmax(y_test[x]):\n        if np.argmax(y_test[x]) == 0:\n            neg_correct += 1\n        else:\n            pos_correct += 1\n       \n    if np.argmax(y_test[x]) == 0:\n        neg_count += 1\n    else:\n        pos_count += 1\n\n\nprint(\"Embedding model's ablity to identify the positive samples and negative samples\")\nprint(\"pos_acc\", pos_correct/pos_count*100, \"%\")\nprint(\"neg_acc\", neg_correct/neg_count*100, \"%\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pos_count, neg_count, pos_correct, neg_correct = 0, 0, 0, 0\nfor x in range(len(x_test)):\n    \n    result = RNN_model.predict(x_test[x].reshape(1,x_test.shape[1]),batch_size=1,verbose = 2)[0]\n   \n    if np.argmax(result) == np.argmax(y_test[x]):\n        if np.argmax(y_test[x]) == 0:\n            neg_correct += 1\n        else:\n            pos_correct += 1\n       \n    if np.argmax(y_test[x]) == 0:\n        neg_count += 1\n    else:\n        pos_count += 1\n\n\nprint(\"RNN's ablity to identify the positive samples and negative samples\")\nprint(\"pos_acc\", pos_correct/pos_count*100, \"%\")\nprint(\"neg_acc\", neg_correct/neg_count*100, \"%\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pos_count, neg_count, pos_correct, neg_correct = 0, 0, 0, 0\nfor x in range(len(x_test)):\n    \n    result = LSTM_model.predict(x_test[x].reshape(1,x_test.shape[1]),batch_size=1,verbose = 2)[0]\n   \n    if np.argmax(result) == np.argmax(y_test[x]):\n        if np.argmax(y_test[x]) == 0:\n            neg_correct += 1\n        else:\n            pos_correct += 1\n       \n    if np.argmax(y_test[x]) == 0:\n        neg_count += 1\n    else:\n        pos_count += 1\n\n\nprint(\"LSTM ablity to identify positive samples and negative samples\")\nprint(\"pos_acc\", pos_correct/pos_count*100, \"%\")\nprint(\"neg_acc\", neg_correct/neg_count*100, \"%\")\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"we can clearly see that LSTM has better loss performance, test accuracy and more powerful ability to identify the negative samples\nthus our final model is LSTM"},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's try some interest samples\nreview_sample_1 = 'the poor quality and size is not suitable! '\nreview_sample_2 = 'Oh! nice experience'\nreview_sample_3 = 'ehh...OK OK, price is cheap! quality is also\"cheap\"'\nreview_sample_4 = 'good! very good! everything is good! Only one thing is not very great:what I buy is a shirt but get a pant'\ndef get_result(review):\n    print(review)\n    #vectorizing the review by the pre-fitted tokenizer instance\n    length = np.array(len(review))\n    polarity = np.array(TextBlob(review).sentiment.polarity)\n    length = length.reshape(1,-1)\n    polarity = polarity.reshape(1,-1)\n    rw = tokenizer.texts_to_sequences([review])\n    #padding the review to have exactly the same shape as `embedding_2` input\n    rw = pad_sequences(rw, maxlen=100, dtype='int32', value=0)\n    rw = np.hstack((rw,length,polarity))\n    sentiment = LSTM_model.predict(rw,batch_size=1,verbose = 2)[0]\n    if(np.argmax(sentiment) == 0):\n        print(\"negative\")\n    elif (np.argmax(sentiment) == 1):\n        print(\"positive\")\nfor i in [review_sample_1,review_sample_2,review_sample_3,review_sample_4]:\n    get_result(i)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}