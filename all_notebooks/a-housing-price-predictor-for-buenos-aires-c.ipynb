{"cells":[{"metadata":{},"cell_type":"markdown","source":"<h1 align=\"center\">Machine Learning regressions to predict Buenos Aires City housing prices</h1>  \n\n<font color=\"green\">Upvotes and suggestions are highly appreciated :)</font>  \n***Note: sections 4 and 5 are still under development, I really do appreciate advice in regards to them.***","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"***Table of contents***  \n1. [Exploratory data analysis](#s1)  \n    1. [A first look to the columns, column selection by relevance](#s1p1) \n    \n        * [Temporary columns](#tcols)\n        * [Geospatial columns](#gcols)\n        * [Other columns](#ocols)\n        \n    2. [Feature plots and distributions](#s1p2)  \n    \n        * [Publication density map](#pdm)  \n        * [Median prices diagram](#mpd)  \n        * [Price distribution](#pd)  \n        * [<font color=\"red\">(to-do)Boxplots (useful for imputation criteria)</font>](#b)\n    \n2. [Feature engineering](#s2)   \n\n    * [A second column selection](#ascs)  \n    * [Core training data](#ctd)   \n    * [<font color=\"green\">First model training and results</font>](#fmt)\n    * [Feature creation](#df)\n    * [Feature importance measurement](#fem)\n    * [<font color=\"red\">Detailed Spatial Clustering</font>](#dsc)\n    * [Imputation](#idt)\n    \n    \n3. [Model selection](#s3)\n    * [XGBoost](#xgb)\n    * [Random Forest Regressor](#rfr)\n    * [LGBMRegressor](#LGBM)\n    * [CatBoostingRegressor](#LGBM)  \n    * [Model saving](#ms)\n\n4. [Model optimization](#mo)\n\n5. [Applications and conclusions](#s4)\n    * [Predictions on real-world properties](#prwp)\n    * [Conclusions](#conclusions)\n","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"###### This notebook is part of a wider project that aims to create property price predictors. The previous notebook tackled this whithin <a href=\"https://www.kaggle.com/msorondo/property-price-predictions-great-buenos-aires-n\">northern Great Buenos Aires</a>. This notebook continues with the previous project by taking some insights for filtering and selecting the features and models... Still, it will introduce lots of modifications in order to increase the performance of the models to train. This notebook will be also used to refine the previous one.\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 1. Exploratory Data Analysis (EDA)<a id=\"s1\"></a>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.plotting.register_matplotlib_converters()\n%matplotlib inline\nimport seaborn as sns\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We´ll first take a look to the dataset and to have an idea of the data in it...","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_crude = pd.read_csv(\"../input/argentina-venta-de-propiedades/ar_properties_crude.csv\", index_col=\"id\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_crude.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Take only Buenos Aires properties, valuated in dollars (ARS have been hugely devaluated).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#CABA = Ciudad Autónoma de Buenos Aires = Buenos Aires City\ndf_CABA_dolar = df_crude[(df_crude[\"l1\"]==\"Argentina\") & (df_crude[\"l2\"]==\"Capital Federal\") & (df_crude[\"currency\"]==\"USD\")]\ndf_CABA_dolar","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1.1. A first look at the columns, column selection by relevance<a id=\"s1p1\">","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_CABA_dolar.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_CABA_dolar[\"ad_type\"].unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We´ll drop this one, it carries no relevant information at all.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_CABA_dolar = df_CABA_dolar.drop(columns=[\"ad_type\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"tcols\"></a>\n### Temporary columns\nStart date, end date and creation date give some relevant information, but they are not good predictors of the price. We want to predict the price and these dates depend on the owner´s choice (which obviously is not significantly correlated with the price). Same as the price period.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_CABA_dolar = df_CABA_dolar.drop(columns=[\"start_date\", \"end_date\", \"created_on\", \"price_period\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Geospatial columns <a id=\"gcols\"></a>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We´ll also drop \"l1\", \"l2\" and \"currency\" (they were used to filter by country and by city previously)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_CABA_dolar = df_CABA_dolar.drop(columns=[\"l1\", \"l2\" ,\"currency\"])\nmissing_percentage = df_CABA_dolar.isnull().sum()*100/len(df_CABA_dolar.index)\nmissing_percentage","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\"l3\" gives information in regards to te neighboorhood. \"l4\", \"l5\" and \"l6\" give even more accurate details, but they have a huge amount of missing values (almost all missing) and we´ve still got \"lat\" and \"lon\" give high-precision information in regards to the location of the house, this could be useful to increase the precition of the prediction. We´ll keep \"lat\", \"lon\" and \"l3\". ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_CABA_dolar = df_CABA_dolar.drop(columns=[\"l4\",\"l5\",\"l6\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"ocols\"></a>\n### Other columns\nEven though we could use NLP techniques to analyze \"title\" and \"description\" columns, this would tremendously increase the length and complexity of this notebook with probably not much more benefits. Rent operations will be excluded as well.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_CABA_dolar[\"operation_type\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_CABA_dolar = df_CABA_dolar[df_CABA_dolar[\"operation_type\"]==\"Venta\"]\ndf_CABA_dolar = df_CABA_dolar.drop(columns=[\"title\", \"description\",\"operation_type\"])\ndf_CABA_dolar","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"s1p2\"></a>\n## 1.2. Plots and distributions","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import folium\nfrom folium import Marker\nfrom folium.plugins import HeatMap","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"pdm\"></a>\n### Publication density map (excepting publications with missing geolocation)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"map_2 = folium.Map(width = 700, height = 500, location=[-34.586662, -58.436620], titles=\"cartodbposition\", zoom_start=12)\ndf_CABA_dolar_noLatNorLonMissing = df_CABA_dolar[df_CABA_dolar[\"lat\"].notnull() & df_CABA_dolar[\"lon\"].notnull()]\nHeatMap(data=df_CABA_dolar_noLatNorLonMissing[[\"lat\",\"lon\"]], radius=12).add_to(map_2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"map_2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The number of properties published look quite well distributed. Let´s look at the unique value counts from \"l3\"...","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_CABA_dolar[\"l3\"].value_counts().head(8)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"mpd\"></a>\n### Median prices diagram ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_CABA_dolar.groupby(by=[\"l3\"], axis=0)[\"price\"].median().sort_values(ascending=False).head(7)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_CABA_dolar.groupby(by=[\"l3\"], axis=0)[\"price\"].median().sort_values(ascending=False).tail(7)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"pd\"></a>\n### Price distribution","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\nplt.figure(figsize=(10,7))\nplt.ticklabel_format(style='plain', axis='x')\nsns.distplot(df_CABA_dolar[\"price\"])\n\nplt.ylim(0,10**-7)\nplt.xlim(0,4000000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_CABA_dolar.describe()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Property type histogram","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,10))\nplt.hist(x=df_CABA_dolar[\"property_type\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As expected, most of the properties are apartments.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id=\"s2\"></a>\n# 2. Feature engineering\n\nLet´s start by renaming the dataframe to simplify it.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"properties = df_CABA_dolar\ncurrent_missing_percentages = (properties.isnull().sum()/properties.shape[0]).sort_values(ascending=False)\ncurrent_missing_percentages","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(properties[properties[\"surface_covered\"].isnull() & properties[\"surface_total\"].isnull()].shape[0])\nproperties[properties[\"surface_covered\"].isnull() & properties[\"surface_total\"].isnull()].shape[0]/properties.shape[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"ascs\"></a>\n## A second column selection","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"There are some features that add important information to perform prediction, but still have huge amounts of missing values %, we´ll examine correlation to see how to deal with them.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def columns_correlation_with_target(df,target):\n    for feature in df.select_dtypes(exclude=[\"object\"]).columns:\n        if feature!=target:\n            print(\"Correlation between \", feature, \" and \", target, \": \", df[target].corr(df[feature]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"columns_correlation_with_target(properties,\"price\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lat and lon were still not clustered so there´s no problem with them not correlating with price.\nRooms, bedrooms and bathrooms are significantly correlated with price, with bedrooms being the least ones.  \nBedrooms columns will have to be dropped, they do correlate with price but they have a huge amount of missing values and are not worth of imputation (mainly because in Argentina \"bedrooms\" isn´t usually used as a reference, and rooms and bathrooms already give substantial information).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"properties = properties.drop(columns=[\"bedrooms\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"properties","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"ctd\"></a>\n## Core training data\nIn order to proceed to deal with missing values and then create features we´ll start by tasting how a basic, not so much pocessed dataframe performs in a model. It will include all of the latter features except for lat and lon, and we´ll remove rows with missing values.\nThen we´ll compare this approach with another one that imputes the values.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"core_properties = properties.drop(columns=[\"lat\",\"lon\"])\ncore_properties = core_properties.dropna(axis=0)\ncore_properties.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Note that it is still a huge dataset.  \n<a id=\"fmt\"></a>\n## First model training + cross validation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.compose import ColumnTransformer\nX_core = core_properties.drop(columns=[\"price\"])\ny_core = core_properties[\"price\"]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_core_train, X_core_valid, y_core_train, y_core_valid = train_test_split(X_core,y_core)\n\nOHE4 = OneHotEncoder(handle_unknown=\"ignore\",sparse=False)\n\nobj_cols = [col for col in X_core_valid.columns if X_core_valid[col].dtype==\"object\"]\n\nOHE4_cat_train = pd.DataFrame(OHE4.fit_transform(X_core_train[obj_cols]))\nOHE4_cat_valid = pd.DataFrame(OHE4.transform(X_core_valid[obj_cols]))\n\nOHE4_cat_train.index = X_core_train.index\nOHE4_cat_valid.index = X_core_valid.index\n\nnum_cols = [col for col in X_core_valid.columns if X_core_valid[col].dtype==\"float64\"]\nnum_cols\nencoded_X_core_train = pd.concat([OHE4_cat_train,X_core_train[num_cols]],axis=1)\nencoded_X_core_valid = pd.concat([OHE4_cat_valid, X_core_valid[num_cols]], axis=1)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"RFR2 = RandomForestRegressor(random_state=4).fit(encoded_X_core_train,y_core_train)\nRFR2_preds = RFR2.predict(encoded_X_core_valid)\nRFR2_MAE = mean_absolute_error(RFR2_preds,y_core_valid)\nRFR2_MAE","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Not bad for a first attempt.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id=\"fc\"></a>\n## Feature Creation","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We´ll create and test new features to increase the performance...","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"bath_rooms_ratio = core_properties[\"bathrooms\"]/core_properties[\"rooms\"]\nsurf_covered_by_total= core_properties[\"surface_covered\"]/core_properties[\"surface_total\"]\nl3_type = core_properties[\"l3\"]+core_properties[\"property_type\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"newFeatureTester function","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def newFeatureTester(df, new_column):\n    X_core[\"new_feature\"] = new_column\n    \n    X_train, X_test, y_train, y_test = train_test_split(X_core,y_core)\n    \n    objec_cols = [col for col in X_test.columns if X_test[col].dtype==\"object\"]\n    \n    OHE = OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n    OHEncoded_cats_train = pd.DataFrame(OHE.fit_transform(X_train[objec_cols]))\n    OHEncoded_cats_test = pd.DataFrame(OHE.transform(X_test[objec_cols]))\n    \n    OHEncoded_cats_train.index = X_train.index\n    OHEncoded_cats_test.index = X_test.index\n    \n    numericals_train = X_train.select_dtypes(exclude=[\"object\"])\n    numericals_test = X_test.select_dtypes(exclude=[\"object\"])\n    \n    OHEncoded_train = pd.concat([OHEncoded_cats_train,numericals_train], axis=1)\n    OHEncoded_test = pd.concat([OHEncoded_cats_test, numericals_test], axis=1) \n    \n    model = RandomForestRegressor(random_state=3).fit(OHEncoded_train,y_train)\n    preds = model.predict(OHEncoded_test)\n    \n    mae = mean_absolute_error(preds,y_test)\n    \n    print(\"MAE with \", new_column.name,\": \" , mae)\n    \n    mae_avg_price = mae/(y_core.mean())\n    \n    print(\"MAE/AVG PRICE: \", new_column.name,\": \",  mae_avg_price)\n    \n    return [mae,mae_avg_price]\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"With this function, we´ll try to separately measure the impact of each new feature on the model´s prediction.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"res = pd.DataFrame({\"bath_rooms_ratio\" : newFeatureTester(core_properties,bath_rooms_ratio),\n\"surf_covered_by_total\" : newFeatureTester(core_properties,surf_covered_by_total),\n\"l3_type\":newFeatureTester(core_properties,l3_type)},index=[\"MAE\",\"MAE/AVG Price\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"res","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There doesn´t seem to be substantial improvements (if any). Let´s use them all into one training set and measure the feature importance to choose with more confidence.\n<a id=\"fem\"></a>\n## Feature importance","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dict_new_features = {\"bath_rooms_ratio\":bath_rooms_ratio, \"surf_covered_by_total\":surf_covered_by_total, \"l3_type\":l3_type}\ndf_new_features = pd.DataFrame(dict_new_features)\ndf_new_features\n\nX_core_plus_new = pd.concat([X_core,df_new_features], axis=1)\nX_core_plus_new.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"OneHotEncode categoricals...","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import eli5\nfrom eli5.sklearn import PermutationImportance","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X_core_plus_new,y_core)\nobject_cols = [col for col in X_train.columns if X_train[col].dtype==\"object\"]\n\nOHE2 = OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n\nlabeled_obj_cols_train = pd.DataFrame(OHE2.fit_transform(X_train[object_cols]))\nlabeled_obj_cols_test = pd.DataFrame(OHE2.transform(X_test[object_cols]))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#OneHotEncoder removed indexes, put them back...\nlabeled_obj_cols_train.index = X_train.index\nlabeled_obj_cols_test.index= X_test.index\n\nnumeric_X_train = X_train.select_dtypes(exclude=[\"object\"])\nnumeric_X_test = X_test.select_dtypes(exclude=[\"object\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labeled_X_train  =  pd.concat([labeled_obj_cols_train,numeric_X_train], axis=1)\nlabeled_X_test = pd.concat([labeled_obj_cols_test,numeric_X_test], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"RFReg = RandomForestRegressor(random_state=7).fit(labeled_X_train,y_train)\n\npermutator = PermutationImportance(RFReg,random_state=2).fit(labeled_X_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"permutator.estimator","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"colnames_labeled = labeled_X_train.columns.tolist()\ncolnames_labeled_all_as_strings = [str(name) for name in colnames_labeled]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"eli5.show_weights(permutator, feature_names=colnames_labeled_all_as_strings, top=len(colnames_labeled_all_as_strings))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = permutator.predict(labeled_X_test)\nmae = mean_absolute_error(preds,y_test)\nprint(\"Mean absolute error: \",mae, \" . Error in relation to mean price: \", mae/y_test.mean(), \"% .\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The error seems to be considerably lower (~ USD 5K less) when combining all of the new features. \nThere are some columns resulting from the One Hot Encoding that deteriorate the prediction but the totality of them increases it quite a bit. The only feature that seems not worth using is \"surf_covered_by_total\".\nWe´ll update the DataFrame to eliminate this feature.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_core_plus_new = X_core_plus_new.drop(columns=[\"surf_covered_by_total\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"dsc\"></a>\n## (for later stages of development)Detailed Spatial Clustering\nWe´ll use Density-Based Spatial Clustering of Applications with Noise algorithm to unsupervisedly find highly detailed clusters that relate numeric geospatial columns with price. I chose this one over the others because of its capability to detect multiple clusters in high-density maps without having to pre-establish the number of clusters (like K-Means does).\nAnother advantage of this algorithm is that it performs very well with high-dimentional spaces, this lets us go further and add the price component.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id=\"idt\"></a>\n## Imputation","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### On numerical columns only","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We´ll compare the following model with the one trained on the core DataFrame...","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.impute import SimpleImputer\nproperties_with_missing = properties.drop(columns=[\"lat\",\"lon\"])\nproperties_with_missing_numericals = properties_with_missing.dropna(subset=[\"l3\",\"property_type\"])\nproperties_with_missing_numericals","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This way we almost double the amount of properties to perform prediction.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X = properties_with_missing_numericals.drop(columns=[\"price\"])\ny=properties_with_missing_numericals[\"price\"]\nX_train, X_test, y_train, t_test = train_test_split(X,y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"obj_cols = [col for col in X_test.columns if X_test[col].dtype==\"object\"]\nOHE3 = OneHotEncoder(handle_unknown=\"ignore\",sparse=False)\n\nOHE3_cat_X_train = pd.DataFrame(OHE3.fit_transform(X_train[obj_cols]))\nOHE3_cat_X_test = pd.DataFrame(OHE3.transform(X_test[obj_cols]))\n#One Hot Encoder lost indexes, put them back...\nOHE3_cat_X_train.index = X_train.index\nOHE3_cat_X_test.index = X_test.index\n\nnumerical_X_train = X_train.select_dtypes(exclude=[\"object\"])\nnumerical_X_test = X_test.select_dtypes(exclude=[\"object\"])\n\nOHE3_X_train = pd.concat([OHE3_cat_X_train,numerical_X_train], axis=1)\nOHE3_X_test = pd.concat([OHE3_cat_X_test,numerical_X_test],axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Given the vast amount of outliers across all columns, we´ll impute for the median.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"imputer = SimpleImputer(strategy=\"median\")\n\nimputed_X_train = pd.DataFrame(imputer.fit_transform(OHE3_X_train))\nimputed_X_test = pd.DataFrame(imputer.transform(OHE3_X_test))\n#imputer removed column names, put them back    \nimputed_X_train.columns = OHE3_X_train.columns\nimputed_X_test.columns = OHE3_X_test.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"RFR_new = RandomForestRegressor(random_state = 1).fit(imputed_X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions_RFR_new = RFR_new.predict(imputed_X_test)\nmae_RFR_new = mean_absolute_error(predictions_RFR_new,t_test)\nmae_RFR_new","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from joblib import dump","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"dump(RFR_new,\"imputed_RFR.joblib\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The imputation icreased the error. We´ll avoid it.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id=\"s3\"></a>\n# 3. Model selection","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"I´ll first train a Gradient Boosting Regressor, then compare with the Random Forest with no parameter tuning and then select the one that best performed for further optimization.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id=\"xgb\"></a>\n## XGBoosting Regressor","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"X_core_train, X_core_valid, y_core_train, y_core_valid = train_test_split(labeled_X_train,y_train)\nOHE4 = OneHotEncoder(handle_unknown=\"ignore\",sparse=False)\n\nOHE4_cat_train = pd.DataFrame(OHE4.fit_transform(X_core_train[obj_cols]))\nOHE4_cat_valid = pd.DataFrame(OHE4.transform(X_core_valid[obj_cols]))\n\nOHE4_cat_train.index = X_core_train.index\nOHE4_cat_valid.index = X_core_valid.index\n\nnum_cols = [col for col in X_core_valid.columns if X_core_valid[col].dtype==\"float64\"]\nnum_cols\nencoded_X_core_train = pd.concat([OHE4_cat_train,X_core_train[num_cols]],axis=1)\nencoded_X_core_valid = pd.concat([OHE4_cat_valid, X_core_valid[num_cols]], axis=1)\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBRegressor\nXGBR2 = XGBRegressor(random_state=6,n_estimators=900,early_stopping_rounds=10, \n                     eval_set=[encoded_X_core_valid,y_core_valid],verbose=False).fit(encoded_X_core_train,y_core_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"XGBR2_preds = XGBR2.predict(encoded_X_core_valid)\nXGBR2_MAE = mean_absolute_error(y_core_valid,XGBR2_preds)\nXGBR2_MAE","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dump(XGBR2,\"XGBR2.joblib\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"rfr\"></a>\n## Random Forest Regressor","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"encoded_X_core_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labeled_X_train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Training WITHOUT labeled \"l3_type column\"","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"RFR2 = RandomForestRegressor(random_state=4).fit(encoded_X_core_train,y_core_train)\nRFR2_preds = RFR2.predict(encoded_X_core_valid)\nRFR2_MAE = mean_absolute_error(RFR2_preds,y_core_valid)\nRFR2_MAE","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Training WITH labeled \"l3_type column\"","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"RFR3 = RandomForestRegressor(random_state=4).fit(labeled_X_train,y_core_train)\nRFR3_preds = RFR3.predict(labeled_X_test)\nRFR3_MAE = mean_absolute_error(RFR3_preds,y_core_valid)\nRFR3_MAE","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The difference isn´t worth the extra training time.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from joblib import dump, load\ndump(RFR2, 'baseline_random_forest.joblib')#37.6k","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"lgbm\"></a>\n## LGBMRegressor","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in X_train.columns:\n    if X_train[col].dtype==\"object\":\n        X_train[col] = X_train[col].astype('category')\n        X_test[col] = X_test[col].astype('category')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"LGBM = LGBMRegressor(random_state=12).fit(X_train,y_train)\npreds_LGBM = LGBM.predict(X_test)\nmean_absolute_error(preds_LGBM,y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let´s try with one hot encoded data...","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"LGBM2 = LGBMRegressor(random_state=12).fit(labeled_X_train,y_train)\npreds_LGBM2 = LGBM2.predict(labeled_X_test)\nmean_absolute_error(preds_LGBM2,y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"cbr\"></a>\n## CatBoostRegressor","execution_count":null},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"from catboost import CatBoostRegressor\n\nCBR = CatBoostRegressor(random_state=9,cat_features=[\"l3\",'l3_type', 'property_type']).fit(X_train,y_train)\npreds = CBR.predict(X_test)\nMAE = mean_absolute_error(preds,y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MAE","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"One hot encoded version...","execution_count":null},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"CBR2 = CatBoostRegressor(random_state=9).fit(labeled_X_train,y_train)\npreds = CBR2.predict(labeled_X_test)\nMAE = mean_absolute_error(preds,y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MAE","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All of the gradient boosting techniques were outperformed by the random forest regressor, yet the extreme gradient booster got quite near and has a bigger tuning margin. We´ll optimize both.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id=\"ms\"></a>\n## Model saving","execution_count":null},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"dump(RFR2, 'baseline_random_forest.joblib')#37.6k\ndump(RFR3, 'l3_types_random_forest.joblib')#37.6k","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dump(XGBR2,\"XGBR2.joblib\")#40k","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dump(RFR_new,\"imputed_RFR.joblib\")#70k","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"mo\"></a>\n# 4. Model Optimization","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"parameters_forsearch = {\n    \"n_estimators\" : [100,250,500,750]\n}\nsearch = GridSearchCV(RandomForestRegressor(),parameters_forsearch,cv=2)\nsearch.fit(encoded_X_core_train,y_core_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(search.best_params_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We´ll keep RFR2 model by now, it has the best n_estimators.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id=\"s4\"></a>\n# 5. Applications and conclusions.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id=\"prwp\"></a>\n## (Applications)Predictions on real world properties:\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"current_model = load(\"baseline_random_forest.joblib\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"conclusions\"></a>\n# Conclusions","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}