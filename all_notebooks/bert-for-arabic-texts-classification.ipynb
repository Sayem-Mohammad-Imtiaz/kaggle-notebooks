{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# content \n1. [**Introduction**](#Introduction)  \n2. [**Word Representation**](#Word_Representation)\n3. [**Transformers**](#Transformers)\n4. [**USING BERT MODEL FRO TEXT CLASSIFICATION**](#BERT_MODELS)\n5. [Dataset](#Dataset)\n6. [Loading the Arabic BERT](#our_arabic_bert_model)\n7. [take a look](#let's_code)\n8. [BERT Inputs](#Model_Inputs)\n9. [BERT Outputs](#Model_Outputs)\n10. [Custom DataSet Module](#our_custom_dataset_module)\n11.[Try the model](#try_the_model)\n\n\n","metadata":{}},{"cell_type":"markdown","source":"# Introduction\n\n<div dir=\"auto\">\n    الحمد لله الخالق المبدع الذي اتقن كل شيء خلقه و بدأ خلق الانسان من طين والصلاة والسلام علي محمد خير خلقه هادي البشرية\n و معلمها اما بعد  \n    \n فان الادراك الحسي عموما و اللغوي بشكل خاص لهو من أعقد المهام التي يقوم بها العقل البشري و التي لطالما عجزت الحواسيب علي مضاهاتها بعكس العمليات الحاسوبية المتسلسلة  sequential_computing التي أظهرت فيها قدرات فائقة متفوقة علي الاداء البشري الذي لم يخلق لتلك المهام حتي جائت فكرة محاكاة عمل خلايا العقل البشري __حتي بالرغم من عدم فهمنا الكامل لطبيعة عمل تلك الخلايا أو كيفية تعلمها__ وذلك من خلال الشبكات العصبية الاصطناعية فسبحان الخالق المبدع  \n وكان مجال الادراك اللغوي و معالجة اللغات الطبيعية __Natural Language Processing__ من أبرز المجالات التي ساهمت الشبكات العصبية الاصطناعية في تحقيق قفزة هائلة في أدائهاالزوايا بينها\n</div>","metadata":{}},{"cell_type":"markdown","source":"# Word_Representation \n<div dir='rtl'>\n\n  الكلمة هي الوحدة الاساسية لبناء أي لغة و كان التحدي الاول أو المهمة الأساسية هي افهام الكمبيوتر للكلمة \n    *semantic* و كذلك تمييز المعني من خلال السياق الذي وردت به الكلمة *discourse*  بلغته الوحيدة التي يفهمها وهي الارقام فكانت الفكرة هي تمثيل الكلمات بمتجهات في فضاء معين بابعاد معينة بحيث يمكن للكمبيوتر قياس المسافات بين تلك المتجهات  بالرقمية وقياس الزوايا بينها بمعني أخر ادراك الفروقات و التشابهات بين المرادفات المختلفة \n\n  - البداية كانت بالتمثبل الساذج للكلمة في فضاء بعده هو عدد جميع مرادفات اللغة __one hot vector__ , و هو تمثيل افترض استقلال جميع الكلمات و عدم وجود أي علاقة بينها  اي تمثيل مليوني البعد دون اي دلالة \n  - ثم كانت فكرة عطاء صفات عددية لمتجة الكلمة بناء علي عدد تكراراتها و ظهورها داخل النصوص مما اضاف بعض الدلالة علي متجهات الكلمات و متجهات الجمل __count vectorizing__   اما عن طريق __binary counting__  أو أسلوب __TFIDF__  \n  - ثم بدأت ثمارالشبكات العصبية في الظهور بحيث أمكن انتاج متجهات للكلمة __Word2Vec Embeddings__ و ئلك عن طريق تدريب الشبكة العصبية علي كمية كبيرة من البيانات علي أداء مهمة وهمية  __CBOW OR Ngrams__ ثم نخرج من هذا التدريب بمتجه مدرب __pretrained__  لكل مفرد\n   ة يحمل الكثير من معناها و دلالاتها و يمكننا استخدامه في جميع الوظائف المتعلقة باللغة دون الحاجة لاعادة تدريبه\n  - و في تطور سريع لأداء الشبكات العصبية و اسهاماتها كانت الورقة البحثية  [Attention Is All You Need](https://arxiv.org/abs/1706.03762v5) في العام 2017 و التي اضافت فكرة __self_attention__ للتركيز علي فهم الكلمة في السياقات المختلفة لها و فهم العلاقة بين الكلمات داخ الجملة حتي وان طالت الجملة و بعدت المسافة كما انها كانت الحل لمشكلات الشبكات العصبية التكرارية التي التي عانت من مشكلات تلاشي الانحدارات عند تدريبها لجمل طويلة  كما انها كانت تعد تحوي العديد من الحسابات المتسلسلة البطيئة ","metadata":{}},{"cell_type":"markdown","source":"# Transformers\n<div dir='auto'>\n      ظهرت المحولات لحل مشكلة تمثيل سياق الجملة عوضا عن تمثيل كلماتها بشكل بمستقل و استفادت المحولات من من فكرة الانتباه الذاتي السابق ذكرها والمحولات عموما هي الية لاعطاء الكلمات داخل النص تمثيل يعبر عن سياق النص فيكون تمثيل الكلمة داخل جملة ما مختلف عن تمثيلها داخل جملة أخري و تقوم الفكرة علي اعادة وزن متجهات الكلمات وفقا للسياق  \n     اختلاف المحولات فيما بينها هو اختلاف في بنية النموذج نفسة من طبقات الادخال و طبقات الانتباه و عدد رؤوس الانتباه  و من هذة المحولات \n    </div>\n\n   - [GPT-2_OPEN-AI_2018](https://github.com/openai/gpt-2/blob/master/model_card.md) Generative Pretrained for Transformers \n   - [BERT_google-ai_2018](https://arxiv.org/abs/1810.04805) Bidirectional Encoder Representation from Transformer  \n   - [T5_google-ai_2019](https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html) Text_TO_Text Transfer Transformer","metadata":{}},{"cell_type":"markdown","source":"# BERT_MODELS\n## or Bidirectional Encoder Represention from Transformers\n## how to use it\n\n    \n","metadata":{}},{"cell_type":"markdown","source":"# <div dir='rtl'>نماذج بيرت هي محولات تم تدريبها علي بياتات تحت اشراف جزئي للقيام بمهتين علي التوازي و هما \n ا </div>\n - MLM or Masked Language Model \n - NSP or Next Sentence Prediction\n <div dir= 'rtl'>\n  ","metadata":{}},{"cell_type":"markdown","source":"<div dir='rtl'>\n    \n    \n- يتم في المهمة الأولي أخفاء كلمات عشوائيا من النص و تدريب النموذج علي توقعها (املأ الفراغات)\n   و في هذةا لمهمة يتعلم النموذج السياق داخل الجملة نفسها\n- و في المهمة الثانية التي تشبة التصنيف الثنائي (ما ان كانت تلك الجملة تصلح تالية لتلك أم لا)و في هذة المهمة يتعلم النموذج السياق بين الجمل","metadata":{}},{"cell_type":"markdown","source":"# let's_code","metadata":{}},{"cell_type":"markdown","source":"### first we need to import the needed libraries\n- I will use the transformers library to load the pre_trained bert model  \n- I will use the pretrained model from [AUB](https://sites.aub.edu.lb/mindlab/2020/02/28/arabert-pre-training-bert-for-arabic-language-understanding/) this model is pretrained by the American Univerisity of Beirut\n\n- pytorch and pytorch_lightning to train the model \n","metadata":{}},{"cell_type":"code","source":"!pip install arabic-stopwords","metadata":{"execution":{"iopub.status.busy":"2021-07-24T07:48:48.680014Z","iopub.execute_input":"2021-07-24T07:48:48.680548Z","iopub.status.idle":"2021-07-24T07:48:56.981956Z","shell.execute_reply.started":"2021-07-24T07:48:48.680497Z","shell.execute_reply":"2021-07-24T07:48:56.980695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport transformers\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import  AutoModel,AutoTokenizer\nimport pytorch_lightning as pl\nfrom torchmetrics.functional import f1\nseed = pl.seed_everything(79)\nimport arabicstopwords.arabicstopwords as ast\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sb","metadata":{"execution":{"iopub.status.busy":"2021-07-24T07:48:56.984002Z","iopub.execute_input":"2021-07-24T07:48:56.984337Z","iopub.status.idle":"2021-07-24T07:48:58.115128Z","shell.execute_reply.started":"2021-07-24T07:48:56.984303Z","shell.execute_reply":"2021-07-24T07:48:58.114021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# setting paddas for better view for our texts\npd.set_option('display.max_colwidth',None)","metadata":{"execution":{"iopub.status.busy":"2021-07-24T07:48:58.116695Z","iopub.execute_input":"2021-07-24T07:48:58.116985Z","iopub.status.idle":"2021-07-24T07:48:58.122281Z","shell.execute_reply.started":"2021-07-24T07:48:58.116949Z","shell.execute_reply":"2021-07-24T07:48:58.121193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"../input/arabic-reviews# Dataset \nthe data I used for this notebook is an edited version form [Arabic 100k Reviews](https://www.kaggle.com/abedkhooli/arabic-100k-reviews)\nThe dataset combines reviews from hotels, books, movies, products and a few airlines after dropping all mixed_labeld reviews It has two classes ( Negative and Positive) \nThe hotels and book reviews were a subset of [HARD and BRAD](https://github.com/elnagara/HARD-Arabic-Dataset )  The rest were selected from airlines reviews collected manually.\n<div dir='rtl'>\nهنا وفي هذة البيانات استبعدنا المراجعات المختلطة الانطباعات (mixed_labeled) و ذلك للتبسيط و حتي يمكننا اختيار طول الجملة بأريحية دون التقيد بوجوب شمول معظم النص فاصبح لدينا 66666 نص موزعين بالتساوي بين الايجاب والسلب","metadata":{}},{"cell_type":"code","source":"reviews = pd.read_csv('../input/arabic-reviews/arabic_reviews.csv')","metadata":{"execution":{"iopub.status.busy":"2021-07-24T07:48:58.365754Z","iopub.execute_input":"2021-07-24T07:48:58.366154Z","iopub.status.idle":"2021-07-24T07:48:59.72998Z","shell.execute_reply.started":"2021-07-24T07:48:58.366113Z","shell.execute_reply":"2021-07-24T07:48:59.729189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reviews.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-24T07:49:00.266346Z","iopub.execute_input":"2021-07-24T07:49:00.26696Z","iopub.status.idle":"2021-07-24T07:49:00.301104Z","shell.execute_reply.started":"2021-07-24T07:49:00.266923Z","shell.execute_reply":"2021-07-24T07:49:00.300208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**to ake a look of the reviews length distribution to to choose our sequence length**","metadata":{}},{"cell_type":"code","source":"reviews['text_len'] = reviews.loc[:,'text'].apply(lambda x:len(x.split()))","metadata":{"execution":{"iopub.status.busy":"2021-07-24T07:49:03.362274Z","iopub.execute_input":"2021-07-24T07:49:03.36262Z","iopub.status.idle":"2021-07-24T07:49:03.674312Z","shell.execute_reply.started":"2021-07-24T07:49:03.36259Z","shell.execute_reply":"2021-07-24T07:49:03.673412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sb.displot(reviews.text_len,bins=300)","metadata":{"execution":{"iopub.status.busy":"2021-07-24T07:49:07.198503Z","iopub.execute_input":"2021-07-24T07:49:07.198858Z","iopub.status.idle":"2021-07-24T07:49:08.069298Z","shell.execute_reply.started":"2021-07-24T07:49:07.198828Z","shell.execute_reply":"2021-07-24T07:49:08.068097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sb.violinplot(x=reviews.label, y=reviews.text_len)","metadata":{"execution":{"iopub.status.busy":"2021-07-24T07:49:09.997967Z","iopub.execute_input":"2021-07-24T07:49:09.998325Z","iopub.status.idle":"2021-07-24T07:49:10.505218Z","shell.execute_reply.started":"2021-07-24T07:49:09.998295Z","shell.execute_reply":"2021-07-24T07:49:10.504095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reviews.describe()","metadata":{"execution":{"iopub.status.busy":"2021-07-24T07:49:12.617477Z","iopub.execute_input":"2021-07-24T07:49:12.618049Z","iopub.status.idle":"2021-07-24T07:49:12.637326Z","shell.execute_reply.started":"2021-07-24T07:49:12.617996Z","shell.execute_reply":"2021-07-24T07:49:12.636153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div dir=\"rtl\">\n    \n### نري بوضوح ان قيم أطوال معظم النصوص يتمركز حول المتوسط مع وجود نصوص شاذة شديدة الطول و أخري قصيرة و بما أننا أستبعدنا النصوص المختلطة الانطباعات فان طول 64 كلمة سيكون مناسب و كافي للنموذج لكشف الانطباع ","metadata":{}},{"cell_type":"markdown","source":"# Some Preprocessing helps our model","metadata":{}},{"cell_type":"code","source":"#getting a stopwords_list\nstop_words = ast.stopwords_list()","metadata":{"execution":{"iopub.status.busy":"2021-07-24T07:49:15.172501Z","iopub.execute_input":"2021-07-24T07:49:15.17287Z","iopub.status.idle":"2021-07-24T07:49:15.177166Z","shell.execute_reply.started":"2021-07-24T07:49:15.172836Z","shell.execute_reply":"2021-07-24T07:49:15.176142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# a small function to remove stop words\ndef remove_stop_words(text):\n    return ' '.join(word for word in text.split() if word not in stop_words)","metadata":{"execution":{"iopub.status.busy":"2021-07-24T07:49:16.156221Z","iopub.execute_input":"2021-07-24T07:49:16.156588Z","iopub.status.idle":"2021-07-24T07:49:16.161415Z","shell.execute_reply.started":"2021-07-24T07:49:16.156555Z","shell.execute_reply":"2021-07-24T07:49:16.160214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reviews['text'] = reviews['text'].apply(remove_stop_words)","metadata":{"execution":{"iopub.status.busy":"2021-07-24T07:49:16.780128Z","iopub.execute_input":"2021-07-24T07:49:16.780476Z","iopub.status.idle":"2021-07-24T07:49:17.80211Z","shell.execute_reply.started":"2021-07-24T07:49:16.780448Z","shell.execute_reply":"2021-07-24T07:49:17.801163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## encoding our categorical labels\nreviews['label'] = reviews['label'].astype('category').cat.codes","metadata":{"execution":{"iopub.status.busy":"2021-07-24T07:49:18.76672Z","iopub.execute_input":"2021-07-24T07:49:18.767082Z","iopub.status.idle":"2021-07-24T07:49:18.778716Z","shell.execute_reply.started":"2021-07-24T07:49:18.767037Z","shell.execute_reply":"2021-07-24T07:49:18.777781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reviews.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-24T07:49:24.234352Z","iopub.execute_input":"2021-07-24T07:49:24.234687Z","iopub.status.idle":"2021-07-24T07:49:24.244926Z","shell.execute_reply.started":"2021-07-24T07:49:24.234658Z","shell.execute_reply":"2021-07-24T07:49:24.244108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## *Note* :   the more time you spend cleaning your data the better performance you can get from your model specially with arabic texts","metadata":{}},{"cell_type":"markdown","source":"- some useful libraries to clean your arabic text\n    - [farasa](https://farasa.qcri.org/) is very good for stemming and segmentation \n    - [pyarabic](https://github.com/linuxscout/pyarabic) for normalization\n    - [cleantext](https://pypi.org/project/cleantext/) for general purpose\n    ","metadata":{}},{"cell_type":"markdown","source":"# our_arabic_bert_model \n##### I will use a pre_trained BERT model that is trained by [aub](https://sites.aub.edu.lb/mindlab) the american univerisiy in Beirut \n## you can use any other pre_trained model if you want , even you can train your own model","metadata":{}},{"cell_type":"code","source":"#load your pre_trained model with all its weights \nmodel_name= 'aubmindlab/bert-base-arabertv02'  \ntokenizer =AutoTokenizer.from_pretrained(model_name) \nmodel=AutoModel.from_pretrained(model_name,output_hidden_states=True)","metadata":{"execution":{"iopub.status.busy":"2021-07-24T07:49:27.597541Z","iopub.execute_input":"2021-07-24T07:49:27.598201Z","iopub.status.idle":"2021-07-24T07:49:50.37143Z","shell.execute_reply.started":"2021-07-24T07:49:27.598165Z","shell.execute_reply":"2021-07-24T07:49:50.370252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model_Inputs\n<div dir='rtl'>\n    \n# كيف يتوقع النموذج شكل المدخلات ؟\n\n# يتوقع نموذج بيرت أستقبال ثلاث أنواع من المدخلات \n- أرقام الرموز أو الفردات ليحولها الي متجهات لكل رمز  __ids__ for word embeddings \n-متجه يوضح ما ان كان الرمز من الجملة الأولي أم الثاني __لاحظ أن النموذج يتوقع ادخال زوج من الجمل__ __token_type__\n- متجه لموقف كل كلمة ياخذ القيمة واحد ان كانت موجودة أو صفر ان كانت رمز ال(padding) ليتم تجاهلها: __attention_mask__\n    ","metadata":{}},{"cell_type":"markdown","source":"# let's take an Example","metadata":{}},{"cell_type":"code","source":"example = 'اللهم صلي و سلم و بارك علي نبينا و قدوتنا و سائر الأنبياء و المرسلين'\ntokenizer.tokenize(example)","metadata":{"execution":{"iopub.status.busy":"2021-07-24T07:49:50.373002Z","iopub.execute_input":"2021-07-24T07:49:50.373312Z","iopub.status.idle":"2021-07-24T07:49:50.381752Z","shell.execute_reply.started":"2021-07-24T07:49:50.373281Z","shell.execute_reply":"2021-07-24T07:49:50.380727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <div dir='rtl'>لاحظ أن الترميز هنا مقطعي __subwords tokenization__","metadata":{}},{"cell_type":"code","source":"inputs = tokenizer.encode_plus(example,return_tensors='pt')\ninputs","metadata":{"execution":{"iopub.status.busy":"2021-07-24T07:49:50.383367Z","iopub.execute_input":"2021-07-24T07:49:50.383727Z","iopub.status.idle":"2021-07-24T07:49:50.399729Z","shell.execute_reply.started":"2021-07-24T07:49:50.383697Z","shell.execute_reply":"2021-07-24T07:49:50.39851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n# <div dir='rtl'>و هذه هي المدخلات التي يتوقعها نموذج بيرت             ","metadata":{}},{"cell_type":"markdown","source":"# Model_Outputs","metadata":{}},{"cell_type":"code","source":"outputs = model(inputs['input_ids'],inputs['attention_mask'],inputs['token_type_ids'])['hidden_states']\noutputs","metadata":{"execution":{"iopub.status.busy":"2021-07-24T08:03:32.996132Z","iopub.execute_input":"2021-07-24T08:03:32.996486Z","iopub.status.idle":"2021-07-24T08:03:33.084856Z","shell.execute_reply.started":"2021-07-24T08:03:32.996456Z","shell.execute_reply":"2021-07-24T08:03:33.083672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"outputs['last_hidden_state'].shape","metadata":{"execution":{"iopub.status.busy":"2021-07-24T07:49:58.284998Z","iopub.execute_input":"2021-07-24T07:49:58.285413Z","iopub.status.idle":"2021-07-24T07:49:58.290105Z","shell.execute_reply.started":"2021-07-24T07:49:58.285379Z","shell.execute_reply":"2021-07-24T07:49:58.289354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div dir='rtl'>\n    \n    \n# هنا لدينا  جملة واحد من 19 رمز لكل رمز متجة من 768 بعد و هذة هي مخرجات الطبقة الأخيرة من النموذج   \n# فكم طبقة لدينا ؟","metadata":{}},{"cell_type":"code","source":"len(outputs['hidden_states'])","metadata":{"execution":{"iopub.status.busy":"2021-07-24T07:50:06.255201Z","iopub.execute_input":"2021-07-24T07:50:06.255804Z","iopub.status.idle":"2021-07-24T07:50:06.2605Z","shell.execute_reply.started":"2021-07-24T07:50:06.255761Z","shell.execute_reply":"2021-07-24T07:50:06.259798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div dir='rtl'>\n    \n# اذن لدينا 13 طبقة مماثلة للطبقة الأخيرة و هي 12 طبقات النموذج بالاضافة الي الطبقة الاولي __word embedding__ ","metadata":{}},{"cell_type":"markdown","source":"## our control Panel","metadata":{}},{"cell_type":"code","source":"train_size = 0.8\nbatch_size = 128\nlr = .00001\nmax_len = 64","metadata":{"execution":{"iopub.status.busy":"2021-07-24T07:50:11.21227Z","iopub.execute_input":"2021-07-24T07:50:11.212913Z","iopub.status.idle":"2021-07-24T07:50:11.217758Z","shell.execute_reply.started":"2021-07-24T07:50:11.212861Z","shell.execute_reply":"2021-07-24T07:50:11.216567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# our_custom_dataset_module","metadata":{}},{"cell_type":"code","source":"class MyDataset(Dataset):\n    def __init__(self, dataframe, tokenizer, max_len , train=True):\n        self.train_set = dataframe.sample(frac=train_size,random_state=seed)\n        self.test_set = dataframe.drop(self.train_set.index).reset_index(drop=True)\n        self.tokenizer = tokenizer\n        self.data = self.train_set if train else self.test_set\n        self.text = self.data.text\n        self.max_len = max_len\n        \n    def __len__(self):\n        return len(self.text)\n\n    def __getitem__(self, index):\n        text = str(self.text.iloc[index])\n        text = \" \".join(text.split())\n\n        inputs = self.tokenizer.encode_plus(\n            text,\n            None,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            truncation=True,\n            padding='max_length',\n            return_token_type_ids=True,\n        )\n        self.labels = self.data.label.iloc[index]\n        ids = inputs['input_ids']\n        mask = inputs['attention_mask']\n        token_type_ids = inputs[\"token_type_ids\"]\n\n       \n        return {\n            'ids': torch.tensor(ids, dtype=torch.long),\n            'mask': torch.tensor(mask, dtype=torch.long),\n            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n            'labels': torch.tensor(self.labels, dtype=torch.long)\n        }","metadata":{"execution":{"iopub.status.busy":"2021-07-24T07:50:14.662311Z","iopub.execute_input":"2021-07-24T07:50:14.662977Z","iopub.status.idle":"2021-07-24T07:50:14.673435Z","shell.execute_reply.started":"2021-07-24T07:50:14.662926Z","shell.execute_reply":"2021-07-24T07:50:14.672325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div dir='rtl'>\n    \n    \n# يمكننا أن نستخدم نموذج بيرت بأكثر من طريقة\n    - يمكن ان نستخدم هيكلية النموذج بدون أوزان و نقوم بتدريبة علي بياناتنا الخاصة\n    - يمكن أن نستخدم فقط الطبقة الأخيرة من النموذج كتمثيل رقمي للكلمات داخل الجملة ثم نضيف طبقة التصنيف\n    -  يمكن أن نستخدم عدد من الطبقات الأخيرة من النموذج ثم  طبقة تصنيف و ذلك بعدة طرق مثل الصف concatinating   أو التجميع pooling  أو المتوسط\n    - في كل الحالات يمكننا أن نثبت أوزان النموذج و نقوم بأمثلة أوزان طبقة التصنيف فقط أو نقوم بأمثلة جميع الاوزان \n","metadata":{}},{"cell_type":"markdown","source":"<div dir='rtl'>\n    \n# هنا سوف نقوم بتدريب أوزان طبقة التصنيف فقط و تثبيت أوزان النموذج المسبقة التدريب و سوف نقوم باستخدام مخرجات أخر أربعة طبقت من النموذج عن طريق صفها و أرسالها لطبقة التصنيف","metadata":{}},{"cell_type":"markdown","source":"# try_the_model","metadata":{}},{"cell_type":"code","source":"for param in model.parameters():\n    param.requires_grad = False","metadata":{"execution":{"iopub.status.busy":"2021-07-24T07:50:20.776785Z","iopub.execute_input":"2021-07-24T07:50:20.77733Z","iopub.status.idle":"2021-07-24T07:50:20.78494Z","shell.execute_reply.started":"2021-07-24T07:50:20.777288Z","shell.execute_reply":"2021-07-24T07:50:20.783937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Bert(pl.LightningModule):\n    def __init__(self,batch_size=batch_size,lr=lr):\n        super(Bert,self).__init__()\n        self.l1 = model\n        self.l2 = nn.Linear(3072,2)\n        self.drop = nn.Dropout(.3)\n        self.batch_size = batch_size\n        self.lr=lr\n        \n    def forward(self, ids, mask, token_type_ids):\n        out = self.l1(ids, mask, token_type_ids)['hidden_states']\n        out = torch.stack(out,dim=0)\n        #   هذا هو متجه الطلمة الأولي للنص لأخر اربع طبقات   cls\n        out = torch.cat([out[-1][:,0,:], out[-2][:,0,:], out[-3][:,0,:], out[-4][:,0,:]],1)\n        out = self.drop(self.l2(out))\n        return out\n        \n    def training_step(self,batch,batch_idx):\n        ids = batch['ids']\n        mask = batch['mask']\n        token_type_ids = batch['token_type_ids']\n        x = self(ids=ids, mask = mask, token_type_ids = token_type_ids)\n        y = batch['labels']\n        loss = F.cross_entropy(x,y)\n        x=x.max(1)[1]\n        f1_score =f1(x,y,num_classes=2)\n        self.log('train_f1',f1_score,on_step=True, on_epoch=True, prog_bar=True, logger=True)\n        self.log('training_loss',loss,on_step=True, on_epoch=True, prog_bar=True, logger=True)\n        return loss\n       \n    def validation_step(self,batch,batch_inx):\n        ids = batch['ids']\n        mask = batch['mask']\n        token_type_ids = batch['token_type_ids']\n        x= self(ids=ids, mask = mask, token_type_ids = token_type_ids)\n        y= batch['labels']\n        loss = F.cross_entropy(x,y)\n        x=x.max(1)[1]\n        f1_score = f1(x,y,num_classes=2)\n        self.log('val_f1',f1_score,on_step=True, on_epoch=True, prog_bar=True, logger=True)\n        self.log('val_loss',loss,on_step=True, on_epoch=True, prog_bar=True, logger=True)\n        return loss\n    def configure_optimizers(self):\n        \n        return optim.RMSprop(self.l2.parameters(),lr=self.lr)\n    \n    def train_dataloader(self):\n        train_set = MyDataset(reviews,tokenizer,max_len,train=True)\n        train_loader = DataLoader(train_set,shuffle=True,batch_size=batch_size,num_workers=0)\n        return train_loader\n    \n    def val_dataloader(self):\n        val_set = MyDataset(reviews,tokenizer,max_len,train=False)\n        val_loader = DataLoader(val_set,shuffle=True,batch_size=batch_size,num_workers=0)\n        return val_loader\n \n        \narabic = Bert()","metadata":{"execution":{"iopub.status.busy":"2021-07-24T08:05:53.690015Z","iopub.execute_input":"2021-07-24T08:05:53.690399Z","iopub.status.idle":"2021-07-24T08:05:53.705645Z","shell.execute_reply.started":"2021-07-24T08:05:53.690364Z","shell.execute_reply":"2021-07-24T08:05:53.704457Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"early_stop = pl.callbacks.EarlyStopping('val_loss',patience=3,mode ='min')","metadata":{"execution":{"iopub.status.busy":"2021-07-24T07:50:36.716528Z","iopub.execute_input":"2021-07-24T07:50:36.716884Z","iopub.status.idle":"2021-07-24T07:50:36.721185Z","shell.execute_reply.started":"2021-07-24T07:50:36.71685Z","shell.execute_reply":"2021-07-24T07:50:36.72038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer=pl.Trainer(callbacks=early_stop)","metadata":{"execution":{"iopub.status.busy":"2021-07-24T07:50:44.444585Z","iopub.execute_input":"2021-07-24T07:50:44.44531Z","iopub.status.idle":"2021-07-24T07:50:44.45141Z","shell.execute_reply.started":"2021-07-24T07:50:44.445264Z","shell.execute_reply":"2021-07-24T07:50:44.450623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#trainer.fit(arabic)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-07-24T08:05:57.932846Z","iopub.execute_input":"2021-07-24T08:05:57.933216Z","iopub.status.idle":"2021-07-24T08:07:38.027687Z","shell.execute_reply.started":"2021-07-24T08:05:57.933181Z","shell.execute_reply":"2021-07-24T08:07:38.027083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div dir='rtl'>\n    \n# حاول تجريب الاستراتيجيات الأخري لأستخدام النموذج و حاول أستخدامة في مهمات أخري و بيانات مختلفة و اترك انطباعك أو استفساراتك أو أفكارك هنا لنتشارك ","metadata":{}},{"cell_type":"markdown","source":"<div dir='rtl'>\n    \n# اللهم انفعنا بما علمتنا ، وعلّمنا ما ينفعنا ، وزدنا علماً","metadata":{}}]}