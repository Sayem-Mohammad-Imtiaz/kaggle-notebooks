{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"scrolled":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Organization Overview: \nThe Toronto Police Service (TPS) is a municipal police force in Toronto, Ontario, Canada. It is the largest municipal police service in Canada, and third largest police force in Canada after the Ontario Provincial Police (OPP) and the Royal Canadian Mounted Police (RCMP).\n"},{"metadata":{},"cell_type":"markdown","source":"## Datasets Overview:\n\n* Toronto Police KSI (Killed/Seriously Injured): Identify when, how and where most impactful Killed and Seriously Injured accidents occur to reduce the incidents in neighbourhoods. \n\n* Toronto Police MCI (Major Crime Indicator): Help Police forces to identify occurrence of MCI based on area, time of day, weekday so that Police Patrols can be delegated accordingly. \n"},{"metadata":{},"cell_type":"markdown","source":"## Descriptive Analysis\n\n#### We tried to answer below questions and make some predictions after analysing it\n* 1.  Total number of KSI accidents in the City of Toronto in percentage\n* 2.  Total number of different crime types in the City of Toronto in percentage \n* 3.  Trend Visualization for all crimes and KSI accidents by year.\n* 4.  What time of the day has the most accidents involved - Daylight, Early Eve, Late Eve, Night - added new attribute\n* 5.  Visualization of Location by neighbourhood heat map on both KSI and MSI dataset\n\n## AI Solution\n\n* 6.  Time Seris Forecasting - Forecast next year general trend (Yearly,Monthly)\n* 7.  Clustering Neighbourhoods Risk Level"},{"metadata":{},"cell_type":"markdown","source":"## Import Packages"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"scrolled":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nfrom pandas.plotting import autocorrelation_plot, scatter_matrix\n\n#visualization \nimport matplotlib.pyplot as plt\nimport seaborn as sea\n\nfrom pandas import DataFrame, Series\nimport statsmodels.formula.api as sm\n\nimport scipy, scipy.stats\n\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns; sns.set()\nimport statsmodels.api as sm\n\nfrom dateutil.relativedelta import relativedelta # working with dates with style\nfrom scipy.optimize import minimize              # for function minimization\n\nimport statsmodels.formula.api as smf            # statistics and econometrics\nimport statsmodels.tsa.api as smt\nimport statsmodels.api as sm\nimport scipy.stats as scs\n\nfrom itertools import product                    # some useful functions\nfrom tqdm import tqdm_notebook\n\n# Importing everything from forecasting quality metrics\nfrom sklearn.metrics import r2_score, median_absolute_error, mean_absolute_error\nfrom sklearn.metrics import median_absolute_error, mean_squared_error, mean_squared_log_error\nimport time\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Create a dataframe from the data in csv"},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"mci_df= pd.read_csv('../input/mcitoronto/MCI_2014_to_2019.csv')\nksi_df = pd.read_csv('../input/ksi-toronto/Motor Vehicle Collisions with KSI Data.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Check MCI dataset"},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"\nmci_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Check KSI dataset"},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"ksi_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Cleaning data"},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"# Drop Na\nmci_df=mci_df.dropna()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"# Filter columns to be used\ndf_MCI=mci_df[['Hood_ID','Division','MCI','occurrencedate','occurrencehour']]\ndf_MCI['occurrencedate']=pd.to_datetime(df_MCI['occurrencedate']).dt.date\n# Filter year\ndf_MCI=df_MCI.loc[pd.to_datetime(df_MCI['occurrencedate']).dt.year>=2014]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"#Combine same type in the same hour\ndf_MCI_count=df_MCI\ndf_MCI_count[\"Count\"] = 1\ndf_MCI_count=df_MCI_count.groupby(['Hood_ID','Division','MCI','occurrencedate','occurrencehour']).count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"#Sort by Date\ndf_MCI_count=df_MCI_count.sort_values(['occurrencedate','occurrencehour']).reset_index()\n#Rename\ndf_MCI_count=df_MCI_count.rename(columns={\"MCI\": \"Type\", \"occurrencedate\": \"Date\",\"occurrencehour\": \"Hour\"})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"#Finish MCI dataset\ndf_MCI_count.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"#Select KSI columns\ndf_Accident=ksi_df[['Hood_ID','Division','INJURY','DATE','HOUR']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"#Rename to match MCI dataset , Seperate DATE to Month and Day Columns  and \n\ndf_Accident=df_Accident.rename(columns={\"INJURY\": \"Type\", \"DATE\": \"Date\",\"HOUR\": \"Hour\"})\n\ndf_Accident['Date']=pd.to_datetime(df_Accident['Date']).dt.date\n\n\n#Only watch 2014 +\ndf_Accident=df_Accident.loc[pd.to_datetime(df_Accident['Date']).dt.year>=2014]\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"df_Accident[\"Count\"] = 1\ndf_Accident=df_Accident.groupby(['Hood_ID','Division','Type','Date','Hour']).count().reset_index()\ndf_Accident.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"#\ndf_Accident['Type']=df_Accident['Type']+' Collision'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"frames = [df_MCI_count, df_Accident]\ndf_All= pd.concat(frames)\ndf_All=df_All.sort_values(by='Date').reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"df_All['Year'] = pd.to_datetime(df_All['Date']).dt.year\ndf_All['Month'] = pd.to_datetime(df_All['Date']).dt.month\ndf_All['Day']= pd.to_datetime(df_All['Date']).dt.day","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"df_All.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"# output for csv for further investgation\ndf_All.to_csv(\"./output.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Analysis 1.Total number of KSI accidents in the City of Toronto in percentage"},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"df_Accident['Year'] = pd.to_datetime(df_Accident['Date']).dt.year\ndf_Accident['Month'] = pd.to_datetime(df_Accident['Date']).dt.month\ndf_Accident['Day']= pd.to_datetime(df_Accident['Date']).dt.day\ndf_Accident.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"pivot_KSI=df_Accident.pivot_table(index=['Year','Type'],values='Count',aggfunc=np.sum)\npivot_KSI","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"years = df_Accident['Year'].unique()\nfor year in years:\n    y = pivot_KSI.iloc[pivot_KSI.index.get_level_values('Year') == year]['Count']\n    total = np.sum(y)\n    plt.pie(y, labels = df_Accident['Type'].unique(),autopct='%1.2f%%', startangle=90 )\n    plt.title(\"Total number of KSI Collision in the City of Toronto in percentage in \" + str(year) + \" : \" + str(total))\n    plt.show() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,5))\nplt.title = ('Collisions by Year')\nsea.barplot(x=\"Year\", y=\"Count\",hue='Type',data=pivot_KSI.reset_index())\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Analysis 2. Total number of different crime types in the City of Toronto in percentage"},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"df_MCI_count['Year'] = pd.to_datetime(df_MCI_count['Date']).dt.year\ndf_MCI_count['Month'] = pd.to_datetime(df_MCI_count['Date']).dt.month\ndf_MCI_count['Day']= pd.to_datetime(df_MCI_count['Date']).dt.day\ndf_MCI_count.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"pivot_MCI=df_MCI_count.pivot_table(index=['Year','Type'],values='Count',aggfunc=np.sum)\npivot_MCI","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"crime_type=['Assault','Auto Theft','Break and Enter','Robbery','Theft Over']\nyears = df_MCI_count['Year'].unique()\nfor year in years:\n    y = pivot_MCI.iloc[pivot_KSI.index.get_level_values('Year') == year]['Count']\n    total = np.sum(y)\n    plt.pie(y, labels = crime_type,autopct='%1.2f%%', startangle=90 )\n    plt.title(\"Total number of Crimes in the City of Toronto in percentage in \" + str(year) + \" : \" + str(total))\n    plt.show() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,5))\nplt.title = ('Crimes by Year')\nsea.barplot(x=\"Year\", y=\"Count\",hue='Type',data=pivot_MCI.reset_index())\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Analysis 3. Trend Visualization for all crimes and KSI accidents by year."},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"%matplotlib inline\nplt.rcParams['figure.figsize'] = [15, 15]\n# Draw Plot\ndef plot_df(df, x, y, title=\"\", xlabel='Year', ylabel='Count', dpi=100):\n    plt.figure(figsize=(6,3), dpi=dpi)\n    plt.plot(x, y, color='tab:red')\n    plt.gca().set(title=title, xlabel=xlabel, ylabel=ylabel)\n    plt.show()\n    \nfor i in (df_All['Type'].unique()):\n    df=df_All.loc[df_All['Type']==i]\n    plot_df(df_All, x=df_All.Year.unique(), y=df.groupby('Year')['Count'].agg('sum'), title=i) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Analysis 4: What time of the day has the most accidents/crimes involved - Daylight, Early Eve, Late Eve, Night - added new attribute"},{"metadata":{},"cell_type":"markdown","source":"- First we will define the new attribute \"TIMEOFDAY\". We define labels or buckets as \n - 12AM-4AM - [00 to 4 hours]\n - 4AM-8AM - [4 to 8 hours]\n - 8AM-12PM - [8 to 12 hours]\n - 12PM-4PM - [12 to 16 hours]\n - 4PM-8PM - [16 to 20 hours]\n - 8PM-12PM - [20 to Midnight]"},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"bins = [0, 4, 8, 12, 16, 20, np.inf]\nlabels = ['12AM-4AM', '4AM-8AM','8AM-12PM', '12PM-4PM', '4PM-8PM', '8PM-12PM']\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### KSI accidents"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_Accident[\"TIMEOFDAY\"] = pd.cut(df_Accident[\"Hour\"], bins, labels = labels)\ndf_Accident.groupby('TIMEOFDAY')['Count'].agg('sum')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"df_Accident_time = pd.DataFrame(df_Accident.groupby(['TIMEOFDAY','Type'])['Count'].agg('sum'))\ndf_Accident_time","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Visualization"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nplt.figure(figsize=(12,5))\nplt.title = ('Time of the day for accidents')\nsea.barplot(x=\"TIMEOFDAY\", y=\"Count\",hue='Type',data=df_Accident_time.reset_index())\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Analysis"},{"metadata":{},"cell_type":"markdown","source":"- It is clear that most of the accidents occured during hours start from 4PM to 8PM, which is the time when people try to reach home after work. \n- Another point to be noted here is, 12PM to 4PM has the second highest accidents, around lunch hours to afternoon. \n- Most of the accidents occured in Daylight from 8AM to 8PM, which is mainly office hours. "},{"metadata":{},"cell_type":"markdown","source":"### MCI crimes"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_MCI_count[\"TIMEOFDAY\"] = pd.cut(df_MCI_count[\"Hour\"], bins, labels = labels)\ndf_MCI_count.groupby('TIMEOFDAY')['Count'].agg('sum')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_MCI_time = pd.DataFrame(df_MCI_count.groupby(['TIMEOFDAY','Type'])['Count'].agg('sum'))\ndf_MCI_time","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Visulization"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,5))\n\nsea.barplot(x=\"TIMEOFDAY\", y=\"Count\",hue='Type',data=df_MCI_time.reset_index())\nplt.title = ('Time of the day for crimes')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Analysis\n\n- It is clear that most of the crimes occured during hours start from 4PM to 8PM, which is the time when people try to reach home after work. ,Most of them are Assualt\n- Another point to be noted here is, 4AM-8AM is the least crimes time period.\n- Most of the Break and Enter happened from 12 AM to 4 AM, which is midnight"},{"metadata":{},"cell_type":"markdown","source":"## Analysis 5. Visualization of Location by neighbourhood heat map on both KSI and MSI dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"import geopandas as gpd\nsns.set(style=\"darkgrid\")\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"regions = gpd.read_file('../input/folder/forAnalysis/Neighbourhoods/Neighbourhoods.shp')\n\nregions['neighbourhood'] = regions['FIELD_7'].str.replace(' \\(.+\\)', '').str.lower()\nregions.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_Accident_Neighbourhood = df_Accident.groupby(['Hood_ID'])['Count'].agg('sum')\ndf_Accident_Neighbourhood.sort_values(ascending=False).head(10)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"merged = regions.set_index('FIELD_5').join(df_Accident_Neighbourhood)\nmerged = merged.reset_index()\nmerged = merged.fillna(0)\nmerged[['FIELD_7', 'FIELD_11', 'FIELD_12', 'geometry', 'Count']].sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# we are using the maximum and minimum count values from the previous cell.\n# setting additionally properties for the plot such as titles, turning of the axis for better visibility\n# and setting the color scheme to look like a heat map.\nfig, ax = plt.subplots(1, figsize=(20, 10))\nax.axis('off')\nax.set_title('Heat Map of KSI collisons in Toronto, Ontario', fontdict={'fontsize': '40', 'fontweight' : '3'})\n\n\n# Create colorbar as a legend\n# empty array for the data range\n# add the colorbar to the figure\n# set the color bar label text size\ncolor = 'Oranges'\nvmin, vmax = 0, 200\nsm = plt.cm.ScalarMappable(cmap=color, norm=plt.Normalize(vmin=vmin, vmax=vmax))\nsm._A = []\ncbar = fig.colorbar(sm)\ncbar.ax.tick_params(labelsize=20)\n\n\n# actually plot the map\nmerged.plot('Count', cmap=color, linewidth=0.8, ax=ax, edgecolor='0.8', figsize=(40,20))\nfor idx, row in merged.iterrows():\n    if(row['Count'] > 140):\n        plt.annotate(s=row['FIELD_7'], xy=(row['FIELD_11'], row['FIELD_12']),\n                 horizontalalignment='center', fontsize='large', color='black', wrap=True)\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_MCI_Neighbourhood = df_MCI_count.groupby(['Hood_ID'])['Count'].agg('sum')\ndf_MCI_Neighbourhood.sort_values(ascending=False).head(10)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MCI_merged = regions.set_index('FIELD_5').join(df_MCI_Neighbourhood)\nMCI_merged = MCI_merged.reset_index()\nMCI_merged = MCI_merged.fillna(0)\nMCI_merged[['FIELD_7', 'FIELD_11', 'FIELD_12', 'geometry', 'Count']].sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# we are using the maximum and minimum count values from the previous cell.\n# setting additionally properties for the plot such as titles, turning of the axis for better visibility\n# and setting the color scheme to look like a heat map.\nfig, ax = plt.subplots(1, figsize=(20, 10))\nax.axis('off')\nax.set_title('Heat Map of Crimes in Toronto, Ontario', fontdict={'fontsize': '40', 'fontweight' : '3'})\n\n\n# Create colorbar as a legend\n# empty array for the data range\n# add the colorbar to the figure\n# set the color bar label text size\ncolor = 'Blues'\nvmin, vmax = 0,8000\nsm = plt.cm.ScalarMappable(cmap=color, norm=plt.Normalize(vmin=vmin, vmax=vmax))\nsm._A = []\ncbar = fig.colorbar(sm)\ncbar.ax.tick_params(labelsize=20)\n\n\n# actually plot the map\nMCI_merged.plot('Count', cmap=color, linewidth=0.8, ax=ax, edgecolor='0.8', figsize=(40,20))\nfor idx, row in MCI_merged.iterrows():\n    if(row['Count'] > 6000):\n        plt.annotate(s=row['FIELD_7'], xy=(row['FIELD_11'], row['FIELD_12']),\n                 horizontalalignment='center', fontsize='large', color='black', wrap=True)\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## AI Solution 6. TIME SERIES FORECASTING - Forecast next year general trend (Yearly,Monthly)\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings                                  # do not disturbe mode\nwarnings.filterwarnings('ignore')\n\n# Load packages\nimport numpy as np                               # vectors and matrices\nimport pandas as pd                              # tables and data manipulations\nimport matplotlib.pyplot as plt                  # plots\nimport seaborn as sns                            # more plots\nfrom datetime import datetime\n\nfrom dateutil.relativedelta import relativedelta # working with dates with style\nfrom scipy.optimize import minimize              # for function minimization\n\nimport statsmodels.formula.api as smf            # statistics and econometrics\nimport statsmodels.tsa.api as smt\nimport statsmodels.api as sm\nimport scipy.stats as scs\n\nfrom itertools import product                    # some useful functions\nfrom tqdm import tqdm_notebook\n\n# Importing everything from forecasting quality metrics\nfrom sklearn.metrics import r2_score, median_absolute_error, mean_absolute_error\nfrom sklearn.metrics import median_absolute_error, mean_squared_error, mean_squared_log_error\n\ndata = pd.read_csv('../input/newoutput/output (1).csv')\n\ndata['Time'] = pd.to_datetime(data['Month'])\n\ndf = data.groupby(['Time']).sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_2 = data.groupby(['Time', 'Type']).sum()\ndef create_sub_df(type_of_crime):\n  return df_2[np.in1d(df_2.index.get_level_values(1), type_of_crime)]\nassault = create_sub_df('Assault')\nassault.index = assault.index.droplevel(1)\nauto_theft = create_sub_df('Auto Theft')\nauto_theft.index = auto_theft.index.droplevel(1)\nbreak_and_enter = create_sub_df('Break and Enter')\nbreak_and_enter.index = break_and_enter.index.droplevel(1)\nfatal_collision = create_sub_df('Fatal Collision')\nfatal_collision.index = fatal_collision.index.droplevel(1)\nmajor_collision = create_sub_df('Major Collision')\nmajor_collision.index = major_collision.index.droplevel(1)\nminimal_collision = create_sub_df('Minimal Collision')\nminimal_collision.index = minimal_collision.index.droplevel(1)\nminor_collision = create_sub_df('Minor Collision')\nminor_collision.index = minor_collision.index.droplevel(1)\nnone_collision = create_sub_df('None Collision')\nnone_collision.index = none_collision.index.droplevel(1)\nrobbery = create_sub_df('Robbery')\nrobbery.index = robbery.index.droplevel(1)\nthetf_over = create_sub_df('Theft Over')\nthetf_over.index = thetf_over.index.droplevel(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# MAPE\ndef mean_absolute_percentage_error(y_true, y_pred): \n    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n    \ndef tsplot(y, lags=None, figsize=(12, 7), style='bmh'):\n    if not isinstance(y, pd.Series):\n        y = pd.Series(y)\n\n    with plt.style.context(style):    \n        fig = plt.figure(figsize=figsize)\n        layout = (2, 2)\n        ts_ax = plt.subplot2grid(layout, (0, 0), colspan=2)\n        acf_ax = plt.subplot2grid(layout, (1, 0))\n        pacf_ax = plt.subplot2grid(layout, (1, 1))\n        y.plot(ax=ts_ax)\n        p_value = sm.tsa.stattools.adfuller(y)[1]\n        ts_ax.set_title('Time Series Analysis Plots\\n Dickey-Fuller: p={0:.5f}'.format(p_value))\n        smt.graphics.plot_acf(y, lags=lags, ax=acf_ax)\n        smt.graphics.plot_pacf(y, lags=lags, ax=pacf_ax)\n        plt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tsplot(df.Count, lags=50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ads_diff = df.Count - df.Count.shift(12)\ntsplot(ads_diff[12:], lags=50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# setting initial values and some bounds for them\nps = range(2, 5)\nd = 0 \nqs = range(2, 5)\nPs = range(0, 2)\nD = 1 \nQs = range(0, 2)\ns = 12 # season length is still 12\n\n# creating list with all the possible combinations of parameters\nparameters = product(ps, qs, Ps, Qs)\nparameters_list = list(parameters)\nlen(parameters_list)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef optimizeSARIMA(parameters_list, d, D, s):\n    results = []\n    best_aic = float(\"inf\")\n\n    for param in tqdm_notebook(parameters_list):\n        # we need try-except because on some combinations model fails to converge\n        try:\n            model=sm.tsa.statespace.SARIMAX(df.Count, order=(param[0], d, param[1]), \n                                            seasonal_order=(param[2], D, param[3], s)).fit(disp=-1)\n        except:\n            continue\n        aic = model.aic\n        # saving best model, AIC and parameters\n        if aic < best_aic:\n            best_model = model\n            best_aic = aic\n            best_param = param\n        results.append([param, model.aic])\n\n    result_table = pd.DataFrame(results)\n    result_table.columns = ['parameters', 'aic']\n    # sorting in ascending order, the lower AIC is - the better\n    result_table = result_table.sort_values(by='aic', ascending=True).reset_index(drop=True)\n    \n    return result_table\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result_table = optimizeSARIMA(parameters_list, d, D, s)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"p, q, P, Q = result_table.parameters[0]\nmodel_total=sm.tsa.statespace.SARIMAX(df.Count, order=(p, d, q), seasonal_order=(P, D, Q, s)).fit(disp=-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ss = df.Count\nss.columns = ['actual']\nss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plotSARIMA(series, model, n_steps):\n    # adding model values\n    data = series.copy()\n    data['actual'] = data[:]\n    data['sarima_model'] = model.fittedvalues\n    # making a shift on s+d steps, because these values were unobserved by the model due to the differentiating\n    data['sarima_model'][:s+d] = np.NaN\n    \n    # forecasting on n_steps forward\n    forecast = model.predict(start = data.shape[0]-2, end = data.shape[0]+n_steps-2)\n    forecast = data.sarima_model.append(forecast)\n    \n    # calculate error, again having shifted on s+d steps from the beginning\n    error = mean_absolute_percentage_error(data['actual'][s+d:], data['sarima_model'][s+d:])\n\n    plt.figure(figsize=(15, 7))\n    #plt.title(\"Mean Absolute Percentage Error: {0:.2f}%\".format(error))\n    plt.plot(forecast, color='r', label=\"model\")\n    #plt.axvspan(data.index[-1], forecast.index[-1], alpha=0.5, color='lightgrey')\n    plt.plot(data.actual, label='actual')\n    plt.legend()\n    plt.show()\n    return(forecast)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Total Prediction\nprediction_total = plotSARIMA(df.Count, model_total, 11)\nprediction_total","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Total Crimes prediction for 2020')\nprediction_total['2020-03-01']+prediction_total['2020-04-01']+prediction_total['2020-05-01']+prediction_total['2020-06-01']+prediction_total['2020-07-01']+prediction_total['2020-08-01']+prediction_total['2020-09-01']+prediction_total['2020-10-01']+prediction_total['2020-11-01']+prediction_total['2020-12-01']+prediction_total['2020-01-01']+prediction_total['2020-02-01']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Assault Prediction\nmodel_assault=sm.tsa.statespace.SARIMAX(assault.Count, order=(p, d, q), seasonal_order=(P, D, Q, s)).fit(disp=-1)\nprediction_assault = plotSARIMA(assault.Count, model_assault, 11)\nprediction_assault","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Auto Theft Prediction\nmodel_auto_theft=sm.tsa.statespace.SARIMAX(auto_theft.Count, order=(p, d, q), seasonal_order=(P, D, Q, s)).fit(disp=-1)\nprediction_auto_theft = plotSARIMA(auto_theft.Count, model_auto_theft, 11)\nprediction_auto_theft","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Break and Enter Prediction\nmodel_break_and_enter=sm.tsa.statespace.SARIMAX(break_and_enter.Count, order=(p, d, q), seasonal_order=(P, D, Q, s)).fit(disp=-1)\nprediction_break_and_enter = plotSARIMA(break_and_enter.Count, model_break_and_enter, 11)\nprediction_break_and_enter","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fatal Collision Prediction\nmodel_fatal_collision=sm.tsa.statespace.SARIMAX(fatal_collision.Count, order=(p, d, q), seasonal_order=(P, D, Q, s)).fit(disp=-1)\nprediction_fatal_collision = plotSARIMA(fatal_collision.Count, model_fatal_collision, 11)\nprediction_fatal_collision","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Major Collision Prediction\nmodel_major_collision=sm.tsa.statespace.SARIMAX(major_collision.Count, order=(p, d, q), seasonal_order=(P, D, Q, s)).fit(disp=-1)\nprediction_major_collision = plotSARIMA(major_collision.Count, model_major_collision, 11)\nprediction_major_collision","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# AI Solution 7. Clustering Neighbourhoods"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_All[\"TIMEOFDAY\"] = pd.cut(df_All[\"Hour\"], bins, labels = labels)\ndf_All_time = pd.DataFrame(df_All.groupby(['Hood_ID','TIMEOFDAY','Type'])['Count'].agg('sum'))\ndf_All_time.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_neighbourhoods = df_All_time.pivot_table('Count', ['Hood_ID'], ['Type'],aggfunc=np.sum)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_neighbourhoods=df_neighbourhoods.fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_neighbourhoods.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_neighbourhoods = regions.set_index('FIELD_5')[['FIELD_7']].join(df_neighbourhoods)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_neighbourhoods.set_index('FIELD_7', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_neighbourhoods.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import preprocessing\ndf_neighbourhoods = df_neighbourhoods.apply(lambda x: x.astype('float64'))\ndf_norm = df_neighbourhoods.apply(preprocessing.scale, axis=0)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.cluster.hierarchy import dendrogram, linkage\n#Average\nZ = linkage(df_norm, method='average')\nplt.figure(figsize=(60,20))\nfig.subplots_adjust(right=3)\nplt.xlabel('HoodID')\ndendrogram(Z, labels=df_norm.index, color_threshold=5.5)\nplt.axhline(y=5.5, color='black', linewidth=0.5, linestyle='dashed')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Single\nZ = linkage(df_norm, method='single')\nplt.figure(figsize=(60,20))\nfig.subplots_adjust(right=3)\nplt.xlabel('HoodID')\ndendrogram(Z, labels=df_norm.index, color_threshold=5.5)\nplt.axhline(y=5.5, color='black', linewidth=0.5, linestyle='dashed')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Ward\nZ = linkage(df_norm, method='ward')\nplt.figure(figsize=(60,20))\nfig.subplots_adjust(right=3)\nplt.xlabel('HoodID')\ndendrogram(Z, labels=df_norm.index, color_threshold=5.5)\nplt.axhline(y=5.5, color='black', linewidth=0.5, linestyle='dashed')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Complete\nZ = linkage(df_norm, method='complete')\nplt.figure(figsize=(60,20))\nfig.subplots_adjust(right=3)\nplt.xlabel('HoodID')\ndendrogram(Z, labels=df_norm.index, color_threshold=5.5)\nplt.axhline(y=5.5, color='black', linewidth=0.5, linestyle='dashed')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Median\nZ = linkage(df_norm, method='median')\nplt.figure(figsize=(60,20))\nfig.subplots_adjust(right=3)\nplt.xlabel('HoodID')\ndendrogram(Z, labels=df_norm.index, color_threshold=5.5)\nplt.axhline(y=5.5, color='black', linewidth=0.5, linestyle='dashed')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Weighted\nZ = linkage(df_norm, method='weighted')\nplt.figure(figsize=(60,20))\nfig.subplots_adjust(right=3)\nplt.xlabel('HoodID')\ndendrogram(Z, labels=df_norm.index, color_threshold=5.5)\nplt.axhline(y=5.5, color='black', linewidth=0.5, linestyle='dashed')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Centroid\nZ = linkage(df_norm, method='centroid')\nplt.figure(figsize=(60,20))\nfig.subplots_adjust(right=3)\nplt.xlabel('HoodID')\ndendrogram(Z, labels=df_norm.index, color_threshold=5.5)\nplt.axhline(y=5.5, color='black', linewidth=0.5, linestyle='dashed')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import KMeans\n\n# Fit a k-Means clustering with k=6 clusters\nkmeans = KMeans(n_clusters=3, random_state=0).fit(df_norm)\n\n# Cluster membership\nmemb = pd.Series(kmeans.labels_, index=df_norm.index)\nprint('\\033[1m'+'k-Means cluster membership:'+'\\033[0m')\nfor key, item in memb.groupby(memb):\n    print(key, ': ', ', '.join(item.index))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"centroids = pd.DataFrame(kmeans.cluster_centers_, columns=df_norm.columns)\npd.set_option('precision', 3) # round to 3 decimal places\nprint(centroids)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pandas.plotting import parallel_coordinates\ncentroids['cluster'] = ['Cluster {}'.format(i) for i in centroids.index]\n\nplt.figure(figsize=(30,6))\nfig.subplots_adjust(right=3)\nax = parallel_coordinates(centroids, class_column='cluster', colormap='Dark2', linewidth=5)\nplt.legend(loc='center left', bbox_to_anchor=(0.915, 0.5))\nplt.xlim(-0.5,10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"High Risk Neighbourhoods: Cluster 1\n* York University Heights (27)\n* Moss Park (73)\n* Waterfront Communities-The Island (77)\n* West Humber-Clairville (1)\n* Wexford/Maryvale (119)\n* Woburn (137)\n* Bay Street Corridor (76)\n* Church-Yonge Corridor (75)\n* Downsview-Roding-CFB (26)\n* Islington-City Centre West (14)\n\n\n\nMedium Risk Neighbourhoods: Cluster 0\n\n*  Yorkdale-Glen Park (31), Malvern (132), Milliken (130), Mimico (includes Humber Bay Shores) (17), Mount Olive-Silverstone-Jamestown (2), Newtonbrook East (50), Niagara (82), Rockcliffe-Smythe (111), Rouge (131), South Parkdale (85), South Riverdale (70), Steeles (116), Tam O'Shanter-Sullivan (118), West Hill (136), Willowdale East (51), Agincourt North (129), Agincourt South-Malvern West (128), Annex (95), Banbury-Don Mills (42), Bedford Park-Nortown (39), Bendale (127), Birchcliffe-Cliffside (122), Clairlea-Birchmount (120), Don Valley Village (47), Dorset Park (126), Dovercourt-Wallace Emerson-Junction (93), Eglinton East (138), Glenfield-Jane Heights (25), High Park-Swansea (87), Humber Summit (21), Humbermede (22), Junction Area (90), Kennedy Park (124), Kensington-Chinatown (78), L'Amoreaux (117)\n\n\nLow Risk Neighbourhoods: Cluster 2\n\n* Wychwood (94), Yonge-Eglinton (100), Yonge-St.Clair (97), Lambton Baby Point (114), Lansing-Westgate (38), Lawrence Park North (105), Lawrence Park South (103), Leaside-Bennington (56), Little Portugal (84), Long Branch (19), Maple Leaf (29), Markland Wood (12), Morningside (135), Mount Dennis (115), Mount Pleasant East (99), Mount Pleasant West (104), New Toronto (18), Newtonbrook West (36), North Riverdale (68), North St.James Town (74), O'Connor-Parkview (54), Oakridge (121), Oakwood Village (107), Old East York (58), Palmerston-Little Italy (80), Parkwoods-Donalda (45), Pelmo Park-Humberlea (23), Playter Estates-Danforth (67), Pleasant View (46), Princess-Rosethorn (10), Regent Park (72), Rexdale-Kipling (4), Roncesvalles (86), Rosedale-Moore Park (98), Runnymede-Bloor West Village (89), Rustic (28), Scarborough Village (139), St.Andrew-Windfields (40), Stonegate-Queensway (16), Taylor-Massey (61), The Beaches (63), Thistletown-Beaumond Heights (3), Thorncliffe Park (55), Trinity-Bellwoods (81), University (79), Victoria Village (43), Westminster-Branson (35), Weston (113), Weston-Pellam Park (91), Willowdale West (37), Willowridge-Martingrove-Richview (7), Woodbine Corridor (64), Woodbine-Lumsden (60), Alderwood (20), Bathurst Manor (34), Bayview Village (52), Bayview Woods-Steeles (49), Beechborough-Greenbrook (112), Black Creek (24), Blake-Jones (69), Briar Hill-Belgravia (108), Bridle Path-Sunnybrook-York Mills (41), Broadview North (57), Brookhaven-Amesbury (30), Cabbagetown-South St.James Town (71), Caledonia-Fairbank (109), Casa Loma (96), Centennial Scarborough (133), Clanton Park (33), Cliffcrest (123), Corso Italia-Davenport (92), Danforth (66), Danforth East York (59), Dufferin Grove (83), East End-Danforth (62), Edenbridge-Humber Valley (9), Elms-Old Rexdale (5), Englemount-Lawrence (32), Eringate-Centennial-West Deane (11), Etobicoke West Mall (13), Flemingdon Park (44), Forest Hill North (102), Forest Hill South (101), Greenwood-Coxwell (65), Guildwood (140), Henry Farm (53), High Park North (88), Highland Creek (134), Hillcrest Village (48), Humber Heights-Westmount (8), Humewood-Cedarvale (106), Ionview (125), Keelesdale-Eglinton West (110), Kingsview Village-The Westway (6), Kingsway South (15)\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"memb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_memb=memb\ndf_memb=pd.DataFrame(df_memb)\n\ndf_memb['cluster'] = memb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"risk_merged = regions.set_index('FIELD_7').join(df_memb)\nrisk_merged = risk_merged.reset_index()\nrisk_merged = risk_merged.fillna(0)\nrisk_merged.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def risk_func(x):\n    return {\n        2: 0,\n        0: 1,\n        1: 2\n    }[x]\n    \nrisk_merged['risk']=risk_merged['cluster'].map(risk_func)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# we are using the maximum and minimum count values from the previous cell.\n# setting additionally properties for the plot such as titles, turning of the axis for better visibility\n# and setting the color scheme to look like a heat map.\nfig, ax = plt.subplots(1, figsize=(30, 15))\nax.axis('off')\nax.set_title('Risk Level for Neighbourhoods in Toronto, Ontario', fontdict={'fontsize': '40', 'fontweight' : '3'})\n\n\n# Create colorbar as a legend\n# empty array for the data range\n# add the colorbar to the figure\n# set the color bar label text size\ncolor = 'Oranges'\nvmin, vmax = 0,3\nsm = plt.cm.ScalarMappable(cmap=color, norm=plt.Normalize(vmin=vmin, vmax=vmax))\nsm._A = []\ncbar = fig.colorbar(sm)\ncbar.ax.tick_params(labelsize=20)\n\n\n# actually plot the map\nrisk_merged.plot('risk', cmap=color, linewidth=0.8, ax=ax, edgecolor='0.8', figsize=(40,20))\nfor idx, row in risk_merged.iterrows():\n    if(row['risk'] > 1):\n        plt.annotate(s=row['FIELD_7'], xy=(row['FIELD_11'], row['FIELD_12']),\n                 horizontalalignment='center', fontsize='large', color='red', wrap=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}