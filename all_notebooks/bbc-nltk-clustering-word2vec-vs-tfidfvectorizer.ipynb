{"cells":[{"metadata":{},"cell_type":"markdown","source":"# The BBC News Clustering\n* The dataset has 2225 News and all are labeled.\n* There are 5 different categories for these news.\n* Even though the news are labeled, they will be used to test it after prediction.\n* The all labels will be predicted by unsupervised learning.\n## Summary\n* The two different libraries are used to predcit news' classes.\n* **1)** Word2Vec \n* * Firstly, the prediction is done without cleaning the words, just stopwords will be removed. \n* * Also, the two different stem algorithms will be used to clean words and will be compared all results. \n* * The purpose of stemming is removing inflection in words. For expample, 'loves' ---> 'love'\n* * The stemming libraries ---> WordNetLemmatizer and PorterStemmer (There are more than these 2)\n* * Lastly, nltk.kmeansclusterer library is used to predict clusters.\n* **2)** TfidfVectorizer\n* * This library more convenient than first one because we didn't use any stemming algorithm because it is not necessary.\n* * Kmeans library is used to predict clusters.","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Importing the libraries\nimport os \nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\n# Stemming Libraries\nfrom nltk.stem import WordNetLemmatizer\nlm = WordNetLemmatizer()\nfrom nltk.stem.porter import PorterStemmer\npm = PorterStemmer()\nfrom nltk.probability import FreqDist\nfrom gensim.models import Word2Vec\nfrom nltk.cluster import KMeansClusterer\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_bbc = pd.read_csv('../input/bbc-fulltext-and-category/bbc-text.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_bbc.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"category_counts = df_bbc.category.value_counts()\ncategories = category_counts.index\nprint(categories)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfig = plt.figure(figsize = (12,5))\nax = fig.add_subplot(111)\nsns.barplot(x = category_counts.index , y = category_counts)\nfor a, p in enumerate(ax.patches):\n    ax.annotate(f'{categories[a]}\\n' + format(p.get_height(), '.0f'), xy = (p.get_x() + p.get_width() / 2.0, p.get_height()), xytext = (0,-25), size = 13, color = 'white' , ha = 'center', va = 'center', textcoords = 'offset points', bbox = dict(boxstyle = 'round', facecolor='none',edgecolor='white', alpha = 0.5) )\nplt.xlabel('Categories', size = 15)\nplt.ylabel('The Number of News', size= 15)\nplt.xticks(size = 12)\n\nplt.title(\"The number of News by Categories\" , size = 18)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Text Preprocessing","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data = np.asarray(df_bbc)\nno_cluster = len(categories)\n\ntemp_text = []\ntemp_text_lemma = []\ntemp_text_stem = []\n\ncleaned_texts = []\ncleaned_texts_lemma = []\ncleaned_texts_stem = []\n\nfor i in range(len(data)):\n    temp_text.append([])\n    temp_text_lemma.append([])\n    temp_text_stem.append([])\n    temp_text[i] = re.sub('[^a-zA-Z]', ' ', data[i][1] )                                                                      # Remove all punctuations\n    temp_text[i] = temp_text[i].lower()\n    temp_text[i] = temp_text[i].split()\n    temp_text_lemma[i] = [lm.lemmatize(word) for word in temp_text[i] if not word in set(stopwords.words('english')) ]           # First stemming method \n    temp_text_stem[i] = [pm.stem(word) for word in temp_text[i] if not word in set(stopwords.words('english'))]                  # Second stemming method\n    temp_text[i] = [word for word in temp_text[i] if not word in set(stopwords.words('english')) ]                         # we didn't use stemming method, just get rid of stopwords\n    cleaned_texts.append(temp_text[i])\n    cleaned_texts_lemma.append(temp_text_lemma[i])\n    cleaned_texts_stem.append(temp_text_stem[i])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Clustering\n# Vectorizing the Words by Word2Vec","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Vectorize all words\n\nfrom gensim.models import Word2Vec\nfrom nltk.cluster import KMeansClusterer\n\ndef word_sentinizer(txt, model):\n    text_vect = []\n    no_words = 0\n    for word in txt:\n        if no_words ==  0:\n            text_vect = model[word]\n        else:\n            text_vect = np.add(text_vect, model[word])\n        no_words += 1\n    return np.asarray(text_vect) / no_words\n\n# Vectorizing withot cleaning\nX = []\nmodel = Word2Vec(cleaned_texts, min_count = 1)\nfor text in cleaned_texts:\n    X.append(word_sentinizer(text, model))\n    \n# Vectorizing with WordNetLemmatizer \nX_lemma = []\nmodel_lemma = Word2Vec(cleaned_texts_lemma, min_count = 1)\nfor text in cleaned_texts_lemma:\n    X_lemma.append(word_sentinizer(text, model_lemma))\n    \n\n# Vectorizing with PorterStemmer      \nX_stem = []\nmodel_stem = Word2Vec(cleaned_texts_stem, min_count = 1)\nfor text in cleaned_texts_stem:\n    X_stem.append(word_sentinizer(text, model_stem))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Clustering vectorized words\nkclusterer = KMeansClusterer(no_cluster, distance= nltk.cluster.util.cosine_distance, repeats = 100)\n\nassigned_clusterers = kclusterer.cluster(X, assign_clusters = True)\nassigned_clusterers_lemma = kclusterer.cluster(X_lemma, assign_clusters = True)\nassigned_clusterers_stem = kclusterer.cluster(X_stem, assign_clusters = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Finding Clusters' Topic","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* This functions is created to stack the real news classes and predicted classes.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Stacking output and predicted results\ndef stack_pred_actual(assigned_clusterers,cleaned_texts,data):\n    cluster_results = np.asarray(assigned_clusterers) \n    cluster_results = cluster_results.reshape(len(cluster_results), 1)\n    cleaned_texts = np.asarray(cleaned_texts)\n    cleaned_texts = cleaned_texts.reshape(len(cleaned_texts), 1)\n    results = np.hstack((cleaned_texts,cluster_results, data[:,0].reshape(len(data), 1)))\n    return results\nresults = stack_pred_actual(assigned_clusterers , cleaned_texts , data)\nresults_lemma = stack_pred_actual(assigned_clusterers_lemma , cleaned_texts_lemma , data)\nresults_stem = stack_pred_actual(assigned_clusterers_stem , cleaned_texts_stem , data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* This function is created to combine the texts based on thier clusters to find clusters' topics. So we will 5 long texts by clusters.\n* We combined the text by clusters because we need to discover which cluster belogns to which topic.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def merge_cluster_news(no_cluster , results):  \n    text_by_clusters = []\n    for i in range(no_cluster):\n        text_by_clusters.append([[],[]])\n\n        for k in range(len(results)):\n            if results[k,1] == i:\n                temp = \" \".join(results[k,0])\n                text_by_clusters[i][0].append(str(temp))\n                text_by_clusters[i][1] = i\n\n        text_by_clusters[i][0] = \" \".join(text_by_clusters[i][0])\n    return text_by_clusters\n\ntext_by_clusters =  merge_cluster_news(no_cluster , results)\ntext_by_clusters_lemma = merge_cluster_news(no_cluster , results_lemma)\ntext_by_clusters_stem = merge_cluster_news(no_cluster , results_stem)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* The word cloud has been created just for the one model that is not cleaned. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# First way to find the clusters' topic. ---> Creating word cloud for each cluster\n\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(1,5, figsize = (25,5))\nfor i in range(len(text_by_clusters)):\n    wordcloud = WordCloud(background_color = 'white',\n                              width = 1200,\n                              height = 1200).generate(text_by_clusters[i][0]) \n    ax[i].imshow(wordcloud)\n    ax[i].grid(False)\n    ax[i].axis('off')\n    ax[i].title.set_text(str(text_by_clusters[i][1]))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* The purpose of this function is finding the clusters' topic based of top 20 common words.\n* The key words has been found from these 20 words and it is used for desicion of each classes cluster number and assined.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Second way to find the clusters' topic. ---> Finding most common 20 words and print them based on their cluster\ndef find_cluster_word2vec(text_by_clusters):\n    topic = [[],[],[]]\n    business = 0\n    sport = 0\n    tech = 0\n    politics = 0\n    entertainment = 0\n    array = []\n    for i, text in enumerate(text_by_clusters):\n        tokenized_words = nltk.tokenize.word_tokenize(text[0])\n        word_dist = FreqDist(tokenized_words)\n        for word, frequency in word_dist.most_common(20):\n            topic[0].append(int(text[1]))\n            topic[1].append(word)\n            topic[2].append(frequency)\n\n    topic = np.array(topic).T\n    topic= pd.DataFrame(topic)\n    topic[0] = topic[0].astype(int)\n\n    for i in range(len(np.unique(topic.iloc[:,0]))):\n        common_words = topic[topic.iloc[:,0] == i].iloc[:,1]\n\n        print(f'Cluster {i}: most common words are {[common_words.iloc[a] for a in range(len(common_words))]}')\n        \n        if np.isin(np.array(common_words[:]), ['election','minister','labour']).sum() > 0:\n            politics =i\n        elif np.isin(np.array(common_words[:]), ['market','growth']).sum() > 0:\n            business =i\n        elif np.isin(np.array(common_words[:]), ['mobile','technology','technolog','comput']).sum() > 0:\n            tech =i\n        elif np.isin(np.array(common_words[:]), ['show','club']).sum() > 0:\n            entertainment =i\n        elif np.isin(np.array(common_words[:]), ['back','england',]).sum() > 0:\n            sport =i\n\n    return {'politics' : [politics],'business': [business],'tech': [tech],'entertainment': [entertainment],'sport': [sport]}\n\nprint('\\n--------Without Cleaning CLuster Predictions--------\\n')\npredicted_classes = pd.DataFrame(find_cluster_word2vec(text_by_clusters),index = ['cluster_numbers'])\nprint(predicted_classes)\n\nprint('\\n--------WordNetLemmatizer CLuster Predictions--------\\n')\npredicted_classes_lemma = pd.DataFrame(find_cluster_word2vec(text_by_clusters_lemma), index = ['cluster_numbers'])\nprint(predicted_classes_lemma)\n\nprint('\\n--------PorterStemmer CLuster Predictions--------\\n')\npredicted_classes_stem = pd.DataFrame(find_cluster_word2vec(text_by_clusters_stem), index = ['cluster_numbers'])\nprint(predicted_classes_stem)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Results for Word2Vec","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def confusion_matrix(results, predicted_classes):\n    temp_array = np.zeros((5,5), dtype = int)\n    cm = pd.DataFrame(temp_array, index = predicted_classes.keys(), columns = predicted_classes.keys())\n\n    for i in range(len(results)):\n        cm.loc[results[i][2], predicted_classes.T[predicted_classes.T == results[i][1]].dropna().index.values[0]] +=1\n    return cm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cm = confusion_matrix(results, predicted_classes)\ncm_lemma = confusion_matrix(results_lemma, predicted_classes_lemma)\ncm_stem = confusion_matrix(results_stem, predicted_classes_stem)\nimport seaborn as sns\naxes = []\nfig, ax = plt.subplots(1,3, figsize = (15,5))\naxes.append(sns.heatmap(cm, annot = True, cmap=\"YlGnBu\",fmt=\"d\", ax = ax[0]))\naxes.append(sns.heatmap(cm_lemma, annot = True, cmap=\"YlGnBu\",fmt=\"d\", ax = ax[1]))\naxes.append(sns.heatmap(cm_stem, annot = True, cmap=\"YlGnBu\",fmt=\"d\", ax = ax[2]))\nfor i in range(len(axes)):\n    axes[i].set_xlabel('Predicted', fontsize = 16)\n    axes[i].set_ylabel('Actual', fontsize = 16)\n    axes[i].tick_params('x', rotation = 45)\naxes[0].set_title('Without Cleaning', fontsize = 20)\naxes[1].set_title('WordNetLemmatizer', fontsize = 20)\naxes[2].set_title('PorterStemmer', fontsize = 20)\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# # Scatter Plot for Predicted Classes\n* TNSE library is used to reduce 100 dimension vector to 2 dimension(coordinats) to demonstrate in scatter plot.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.manifold import TSNE\nimport seaborn as sns\ndef find_coords(X,assigned_clusterers,predicted_classes):\n    tsne = TSNE(n_components = 2)                      \n    X_tsne = tsne.fit_transform(X)\n    df_coords = pd.DataFrame(X_tsne , columns = ['x', 'y'] )\n    df_coords['clusters'] = assigned_clusterers\n    for i in range(len(df_coords)):\n        df_coords.loc[i,'pred_labels'] = predicted_classes.T[predicted_classes.T == df_coords.loc[i, 'clusters']].dropna().index.values[0]\n    return df_coords\n\ndf_coords = find_coords(X,assigned_clusterers,predicted_classes)\ndf_coords_lemma =find_coords(X_lemma,assigned_clusterers_lemma,predicted_classes_lemma)\ndf_coords_stem = find_coords(X_stem,assigned_clusterers_stem,predicted_classes_stem)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1,3, figsize = (21,7))\naxes = []\naxes.append(sns.scatterplot(x =df_coords.x, y = df_coords.y, hue =df_coords.pred_labels, palette = 'Set2', ax = ax[0]))\naxes.append(sns.scatterplot(x =df_coords_lemma.x, y = df_coords_lemma.y, hue =df_coords_lemma.pred_labels, palette = 'Set2', ax = ax[1]))\naxes.append(sns.scatterplot(x = df_coords_stem.x, y = df_coords_stem.y, hue = df_coords_stem.pred_labels, palette = 'Set2', ax = ax[2]))\naxes[0].set_title('Without Cleaning', fontsize = 18)\naxes[1].set_title('WordNetLemmatizer', fontsize = 18)\naxes[2].set_title('PorterStemmer', fontsize = 18)\n[axes[i].axis(False) for i in range(3)]\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* As you can see, PorterStemmer has perfomed better that fisrt two methods. In PorterStemmer, almost all classes distinguished its own class.\n* On the other hand, in first two methods, entertainment classes is not seperated very well, it is distributed to other classes. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def accuracy(results, business_class =0, sport_class = 0 , entertainment_class =0, tech_class =0, politics_class =0):\n    false = 0\n    \n    evaluating = {'cluster' : [], 'no_record' : [], 'correct_pred' : [], 'wrong_pred' : [] , 'accuracy' : []}\n    clusters, counts = np.unique(results[:,2], return_counts = True)\n    clusters = np.asarray([clusters, counts]).T\n    for i in range(len(clusters)):\n        evaluating['cluster'].append(clusters[i,0])\n        evaluating['no_record'].append(clusters[i,1])\n        evaluating['correct_pred'].append(0)\n        evaluating['wrong_pred'].append(0)\n        evaluating['accuracy'].append(0)\n    evaluating = pd.DataFrame(evaluating)    \n    \n    for i in range(len(results)):\n        if ((results[i][1] == business_class) and (results[i][2]== 'business')):\n            evaluating.iloc[0,2] += 1      \n        elif ((results[i][1] == sport_class) and (results[i][2]== 'sport')):\n            evaluating.iloc[3,2] += 1\n        elif ((results[i][1] == entertainment_class) and (results[i][2]== 'entertainment')):\n            evaluating.iloc[1,2] += 1\n        elif ((results[i][1] == tech_class) and (results[i][2]== 'tech')):\n            evaluating.iloc[4,2] += 1\n        elif ((results[i][1] == politics_class) and (results[i][2]== 'politics')):\n            evaluating.iloc[2,2] += 1\n        else:\n            false +=1\n    evaluating['wrong_pred'] = evaluating['no_record'] - evaluating['correct_pred']\n    evaluating['accuracy'] = round(evaluating['correct_pred'] / evaluating['no_record'],2)\n    return evaluating\n\n\nevaluating = accuracy(results, business_class =int(predicted_classes.business), sport_class = int(predicted_classes.sport) , entertainment_class =int(predicted_classes.entertainment), tech_class =int(predicted_classes.tech), politics_class =int(predicted_classes.politics))\nevaluating_lemma = accuracy(results_lemma, business_class =int(predicted_classes_lemma.business), sport_class = int(predicted_classes_lemma.sport) , entertainment_class =int(predicted_classes_lemma.entertainment), tech_class =int(predicted_classes_lemma.tech), politics_class =int(predicted_classes_lemma.politics))\nevaluating_stem = accuracy(results_stem, business_class =int(predicted_classes_stem.business), sport_class = int(predicted_classes_stem.sport) , entertainment_class =int(predicted_classes_stem.entertainment), tech_class =int(predicted_classes_stem.tech), politics_class =int(predicted_classes_stem.politics))\nprint('\\n             -----Without Cleaning Results-----')\nprint(evaluating)\nprint('\\n             -----WordNetLemmatizer Results-----')\nprint(evaluating_lemma)\nprint('\\n             -----PorterStemmer Results-----')\nprint(evaluating_stem)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def print_evaluation(evaluating):\n    evaluating.iloc[:,3] = evaluating.iloc[:,1] - evaluating.iloc[:,2]\n\n    print(f'{round((sum(evaluating.iloc[:,2])/sum(evaluating.iloc[:,1]))*100,2)} of --all the news-- are predicted as correctly.')\n\n    for i in range(len(evaluating)):\n        print(f'{round((evaluating.iloc[i,2]/evaluating.iloc[i,1])*100,2)} of the --{evaluating.iloc[i,0]}-- news is predicted correctly.')\n\n\n        \nprint('\\n             -----Without Cleaning Results-----')\nprint_evaluation(evaluating)\nprint('\\n             -----WordNetLemmatizer Results-----')\nprint_evaluation(evaluating_lemma)\nprint('\\n             -----PorterStemmer Results-----')\nprint_evaluation(evaluating_stem)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Vectorizing the Word by TfidfVectorizer\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* TfidfVectorizer is another library to vectorize the words.\n* There is a limitation about tokenization but it is performing well and faster.\n* There is no need to clean the data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# sklearn word vectorizing\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.cluster import KMeans\n\ncorpus =[]\nfor i in range(len(data)):\n    corpus.append(data[i][1])\nvectorizer = TfidfVectorizer(stop_words = \"english\")\nX_tf = vectorizer.fit_transform(corpus)\nmodel_tf = KMeans(n_clusters = no_cluster, init = 'k-means++' , max_iter = 100, n_init = 1)\nmodel_tf.fit(X_tf)\n\norder_centroids = model_tf.cluster_centers_.argsort()[:,::-1]        # order_centroids variable includes all words words' vectors for each cluster and it is ordered\nterms = vectorizer.get_feature_names()                               # this variable includes all words","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* In Word2Vec, the vector had 100 dimension but in this method, dimension has been decided based on the number of unique words.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(np.shape(X_lemma))\nprint(np.shape(X_tf))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Results for TfidfVectorizer ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* The purpose of this function is finding the clusters' topic based of top 20 common words","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def find_cluster(no_cluster,terms, order_centroids):     \n    business = 0\n    sport = 0\n    tech = 0\n    politics = 0\n    entertainment = 0\n    for i in range(no_cluster):\n        common_words = [terms[word_number] for word_number in order_centroids[i,:20]]\n\n        print(f'Cluster {i}: most common words are {common_words}')\n        \n        if np.isin(np.array(common_words[:]), ['election','minister','labour']).sum() > 0:\n            politics =i\n        elif np.isin(np.array(common_words[:]), ['economy','oil']).sum() > 0:\n            business =i\n        elif np.isin(np.array(common_words[:]), ['software','microsoft','computer','technology']).sum() > 0:\n            tech =i\n        elif np.isin(np.array(common_words[:]), ['oscar','actress','award']).sum() > 0:\n            entertainment =i\n        elif np.isin(np.array(common_words[:]), ['match','cup','coach']).sum() > 0:\n            sport =i\n    return {'politics' : [politics],'business': [business],'tech': [tech],'entertainment': [entertainment],'sport': [sport]}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted_classes_tf = find_cluster(no_cluster, terms, order_centroids)                    # The clusters' numbers are assigned based on key words in top 20 words.\n\npredicted_classes_tf = pd.DataFrame(predicted_classes_tf, index = ['cluster_numbers'])\n\nassigned_cluster_tf = model_tf.predict(X_tf)                                               # All of the news clusters predictions.                 \n\nresults_tf = stack_pred_actual(assigned_cluster_tf,corpus,data)                            # The real and prediction classes are stacked.\nprint('\\n--------TfidfVectorizer CLuster Predictions--------')\nprint(predicted_classes_tf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cm_tf = confusion_matrix(results_tf, predicted_classes_tf)\nsns.heatmap(cm_tf, annot = True, cmap=\"YlGnBu\",fmt=\"d\")\nplt.xlabel('Predicted', size = 16)\nplt.ylabel(\"Actual\" , size = 16)\nplt.title('TfidfVectorizer', size = 20)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* There are two libraries to reduce numbers of dimension for words vectors which are TSNE and UMAP.\n* Both has been used to demonstrate.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import umap\ncoords_finder = umap.UMAP(metric = 'cosine')\ncoords = coords_finder.fit_transform(X_tf)\ndf_coords_tf1 = find_coords(X_tf,assigned_cluster_tf,predicted_classes_tf)\ndf_coords_tf2 = df_coords_tf1.copy()\ndf_coords_tf2['x'] = coords[:,0]\ndf_coords_tf2['y'] = coords[:,1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1,2,figsize = (20,8))\nax1 = sns.scatterplot(x = df_coords_tf1. x , y = df_coords_tf1.y, hue = df_coords_tf1.pred_labels, palette = \"Set2\", ax = ax[0])\nax2 = sns.scatterplot(x = df_coords_tf2. x , y = df_coords_tf2.y, hue = df_coords_tf2.pred_labels, palette = \"Set2\", ax = ax[1])\nax1.axis(False)\nax2.axis(False)\nax1.set_title(\"TfidfVectorizer Clusters(TSNE)\", size = 18)\nax2.set_title(\"TfidfVectorizer Clusters(UMAP)\", size = 18)\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"total_true_pred = 0\nfor i in range(len(cm_tf)):    \n    total_true_pred += cm_tf.iloc[i,i]\nprint(f'{round((total_true_pred / len(data) ) * 100, 2)}% of --all the news-- is predicted correctly.')\nfor i in range(len(cm_tf)):\n    print(f'{round((cm_tf.iloc[i,i] / cm_tf.iloc[i,:].sum()) * 100, 2)}% of the --{cm_tf.columns[i]}-- news is predicted correctly.')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusion\n* In Word2Vec\n* * The accuracy of the model that is not used any stemming is around 75.0% and always the lowest one.\n* * The best clustering has been predicted by PorterStemmer.\n* * Entertainment results are performed worst. The reason is that entertainment contains more general words than others.\n* TfidfVectorizer\n* * Although there is limitation in the method about tokeinizing the words, it performed very well and is definitely better than Word2Vec.\n* * Unlike Word2Vec, in this library, Sport clustering result is great, almost 100% correct prediction.\n* * Most common words are more related to its real clusters, so it is more seperable.\n* Both Vectorizing methods are trained over 10 times, results show that they are not consistent. \n* They give similar results in every training but sometimes one class can perform worse than previous one especiall in Word2Vec.\n","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}