{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \n\ndf = pd.read_csv('../input/superheroes-nlp-dataset/superheroes_nlp_dataset.csv')\ndf.head(3)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"cdf = df[['creator', 'history_text']]\ncdf.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Null Values')\nprint(cdf.isnull().sum())\nprint('________________________')\nprint(cdf['creator'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cdf = cdf.dropna()\nprint(cdf.isnull().sum())\nprint('--------------------------------')\nprint(cdf['creator'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mask1 = cdf.loc[(cdf['creator'] == 'Marvel Comics' )]\nmask2 = cdf.loc[(cdf['creator'] == 'DC Comics' )]\nframes = [mask1, mask2]\ncdf = pd.concat(frames)\ncdf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(cdf.isnull().sum())\nprint('--------------------------------')\nprint(cdf['creator'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nsns.countplot(cdf['creator'], palette=\"plasma\")\nfig = plt.gcf()\nfig.set_size_inches(8,5)\nplt.title('Score')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features = []\ntarget = []","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in cdf['history_text']:\n    features.append(i)\n    \nfor i in cdf['creator']:\n    target.append(i)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Cloud_Marvel = cdf['history_text'].loc[(cdf.creator == 'Marvel Comics')]\nCloud_Marvel = Cloud_Marvel.sum()\nCloud_DC = cdf['history_text'].loc[(cdf.creator == 'DC Comics')]\nCloud_DC = Cloud_DC.sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline \nfrom wordcloud import WordCloud\nwordcloud = WordCloud(max_font_size=300,background_color=\"black\").generate(Cloud_Marvel)\nplt.figure()\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline \nfrom wordcloud import WordCloud\nwordcloud = WordCloud(max_font_size=300,background_color=\"black\").generate(Cloud_DC)\nplt.figure()\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modeling\n\n> The problems is simple, then I'll use simple models."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Stopwords"},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.corpus import stopwords  \nstopwords = set(stopwords.words('english'))  \nstopwords","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Stemmer"},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.stem import PorterStemmer\n\nstemmer = PorterStemmer()\nanalyzer = CountVectorizer().build_analyzer()\n\ndef stemmed_words(doc):\n    return (stemmer.stem(w) for w in analyzer(doc))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Vectorizer"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# Unigram and Bigram no TF , 1 & 2\n\nvect1 = CountVectorizer(stop_words = stopwords)\nvect2 = CountVectorizer(stop_words = stopwords , ngram_range = (1,2))\n\nX_t1 = vect1.fit_transform(features)\nX_t2 = vect2.fit_transform(features)\n\n# Unigram and Bigram with TF 3 & 4\n\ntf1 = TfidfVectorizer(use_idf=False, norm = \"l1\", stop_words = stopwords)\ntf2 = TfidfVectorizer(use_idf=False, norm = \"l1\", stop_words = stopwords, ngram_range = (1, 2))\n\nX_t3 = tf1.fit_transform(features)\nX_t4 = tf2.fit_transform(features)\n\n# Unigram and Bigram with TF-IDF 5 & 6\n\ntf_idf1 = TfidfVectorizer(norm=\"l1\", stop_words = stopwords)\ntf_idf2 = TfidfVectorizer(norm = \"l1\", stop_words = stopwords, ngram_range = (1, 2))\n\nX_t5 = tf_idf1.fit_transform(features)\nX_t6 = tf_idf2.fit_transform(features)\n\n# Unigram and Bigram stemmed\n\nstem_vect1 = CountVectorizer(analyzer=stemmed_words,stop_words = stopwords)\nstem_vect2 = CountVectorizer(analyzer=stemmed_words,stop_words = stopwords,ngram_range = (1,2))\n\nX_t7 = stem_vect1.fit_transform(features)\nX_t8 = stem_vect2.fit_transform(features)\n\n# Unigram and Bigram stemmed with term frequency\n\nstem_tf1 = TfidfVectorizer(analyzer=stemmed_words, use_idf=False, norm = \"l1\", stop_words = stopwords)\nstem_tf2 = TfidfVectorizer(analyzer=stemmed_words, use_idf=False, norm = \"l1\", stop_words = stopwords, ngram_range = (1, 2))\n\nX_t9 = stem_tf1.fit_transform(features)\nX_t10 = stem_tf2.fit_transform(features)\n\n# Unigram and Bigram stemmed with term frequency using IDF\nstem_tf_idf1 = TfidfVectorizer(analyzer=stemmed_words, norm=\"l1\", stop_words = stopwords)\nstem_tf_idf2 = TfidfVectorizer(analyzer=stemmed_words, norm = \"l1\", stop_words = stopwords, ngram_range = (1, 2))\n\nX_t11 = stem_tf_idf1.fit_transform(features)\nX_t12 = stem_tf_idf2.fit_transform(features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train1, X_test1, y_train1, y_test1 = train_test_split(X_t1, target, test_size = .2, random_state = 42)\nX_train2, X_test2, y_train2, y_test2 = train_test_split(X_t2, target, test_size = .2, random_state = 42)\nX_train3, X_test3, y_train3, y_test3 = train_test_split(X_t3, target, test_size = .2, random_state = 42)\nX_train4, X_test4, y_train4, y_test4 = train_test_split(X_t4, target, test_size = .2, random_state = 42)\nX_train5, X_test5, y_train5, y_test5 = train_test_split(X_t5, target, test_size = .2, random_state = 42)\nX_train6, X_test6, y_train6, y_test6 = train_test_split(X_t6, target, test_size = .2, random_state = 42)\nX_train7, X_test7, y_train7, y_test7 = train_test_split(X_t7, target, test_size = .2, random_state = 42)\nX_train8, X_test8, y_train8, y_test8 = train_test_split(X_t8, target, test_size = .2, random_state = 42)\nX_train9, X_test9, y_train9, y_test9 = train_test_split(X_t9, target, test_size = .2, random_state = 42)\nX_train10, X_test10, y_train10, y_test10 = train_test_split(X_t10, target, test_size = .2, random_state = 42)\nX_train11, X_test11, y_train11, y_test11 = train_test_split(X_t11, target, test_size = .2, random_state = 42)\nX_train12, X_test12, y_train12, y_test12 = train_test_split(X_t12, target, test_size = .2, random_state = 42)\n\nX_train = [X_train1, X_train2, X_train3, X_train4, X_train5, X_train6, X_train7, X_train8, X_train9, X_train10, X_train11, X_train12]\nX_test = [X_test1, X_test2, X_test3, X_test4, X_test5, X_test6, X_test7, X_test8, X_test9, X_test10, X_test11, X_test12]\ny_train = [y_train1, y_train2, y_train3, y_train4, y_train5, y_train6, y_train7, y_train8, y_train9, y_train10, y_train11, y_train12]\ny_test = [y_test1, y_test2, y_test3, y_test4, y_test5, y_test6, y_test7, y_test8, y_test9, y_test10, y_test11, y_test12]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Multinomial Naive Bayes"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\nNB = []\nfor a, b in zip(X_train, y_train):\n    NB.append(MultinomialNB().fit(a, b))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"NB_acc = []\n\nfor a, b, c in zip(NB, X_test, y_test):\n    predict = a.predict(b)\n    acc = accuracy_score(c, predict)\n    NB_acc.append(acc)\n    \nprint(NB_acc)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nLR = []\nfor a, b in zip(X_train, y_train):\n    LR.append(LogisticRegression(solver = 'liblinear', random_state=42).fit(a, b))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"LR_acc = []\n\nfor a, b, c in zip(LR, X_test, y_test):\n    predict = a.predict(b)\n    acc = accuracy_score(c, predict)\n    LR_acc.append(acc)\n    \nprint(LR_acc)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## XG BOOST"},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost as xgb\nXGBoost = []\nfor a, b in zip(X_train, y_train):\n    XGBoost.append(xgb.XGBClassifier(objective=\"binary:logistic\", random_state=42).fit(a, b))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"XGBoost_acc = []\n\nfor a, b, c in zip(XGBoost, X_test, y_test):\n    predict = a.predict(b)\n    acc = accuracy_score(c, predict)\n    XGBoost_acc.append(acc)\n    \nprint(XGBoost_acc)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Support Vector Machine"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\nSVM = []\nfor a, b in zip(X_train, y_train):\n    SVM.append(SVC().fit(a, b))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"SVM_acc = []\n\nfor a, b, c in zip(SVM, X_test, y_test):\n    predict = a.predict(b)\n    acc = accuracy_score(c, predict)\n    SVM_acc.append(acc)\n    \nprint(SVM_acc)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nForest = []\nfor a, b in zip(X_train, y_train):\n    Forest.append(RandomForestClassifier(max_depth=2, random_state=42).fit(a,b))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Forest_acc = []\n\nfor a, b, c in zip(Forest, X_test, y_test):\n    predict = a.predict(b)\n    acc = accuracy_score(c, predict)\n    Forest_acc.append(acc)\n    \nprint(Forest_acc)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Decision Tree"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import tree as tree_model \nTree = []\nfor a, b in zip(X_train, y_train):\n    Tree.append(tree_model.DecisionTreeClassifier().fit(a, b))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Tree_acc = []\n\nfor a, b, c in zip(Tree, X_test, y_test):\n    predict = a.predict(b)\n    acc = accuracy_score(c, predict)\n    Tree_acc.append(acc)\n    \nprint(Tree_acc)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# The Best"},{"metadata":{"trusted":true},"cell_type":"code","source":"d = {\n    'MultinomialNB': NB_acc, 'Logistic Regression': LR_acc,\n    'XGBoost': XGBoost_acc, 'SVM': SVM_acc, 'Random Forest': Forest_acc,\n     'Decision Tree': Tree_acc\n    }\n\nbest_acc =pd.DataFrame(data=d)\nbest_acc.head(12)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nax = sns.heatmap(best_acc, annot=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Some tests"},{"metadata":{"trusted":true},"cell_type":"code","source":"DC = [\"A boy lost your parents with eight years old, he was train and later became a vigilant in his city!\",\n           \n\"A baby falls down in a farm, he was born on the planet Krypton and was given the name Kal-El at birth. As a baby, his parents sent him to Earth in a small spaceshipmoments before his planet was destroyed in a natural cataclysm.Now he resides in the fictional American city of Metropolis, where he works as a journalist for the Daily Planet.\",\n\n\"He is a vigilant in Star City, His main weapon is a bow and arrow, his favorite color is Green\",\n           \n\"Princess Diana of an all-female Amazonian race rescues US pilot Steve. Upon learning of a war, she ventures into the world of men to stop Ares, the god of war, from destroying mankind.\",\n      \n\"He is the fastest man alive, sometimes actually, when he was a kid, his mom was murdered from a yellow man, a yellow thing\"          \n          ]\n\nMarvel = [\"A wealthy American business magnate, playboy, and ingenious scientist, he suffers a severe chest injury during a kidnapping. When his captors attempt to force him to build a weapon of mass destruction, he instead creates a mechanized suit of armor to save his life and escape captivity.\",\n \"The history is simple, He is the Wakanda King\",\"His Father is Odin\",\n           \n\"He was a normal scientist falling in love for a beautiful scientist, but now when he is Angry, he become a green monster, and everybody call him HULK\",\n\"He is a good person, a good hero, an old hero, he is a captain, a leader, currently he is an avenger.\"\n     ]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nMarvel1 = vect1.transform(Marvel)\nMarvel2 = vect2.transform(Marvel)\nMarvel3 = tf1.transform(Marvel)\nMarvel4 = tf2.transform(Marvel)\nMarvel5 = tf_idf1.transform(Marvel)\nMarvel6 = tf_idf2.transform(Marvel)\nMarvel7 = stem_vect1.transform(Marvel)\nMarvel8 = stem_vect2.transform(Marvel)\nMarvel9 = stem_tf1.transform(Marvel)\nMarvel10 = stem_tf2.transform(Marvel)\nMarvel11 = stem_tf_idf1.transform(Marvel)\nMarvel12 = stem_tf_idf2.transform(Marvel)\n\nDC1 = vect1.transform(DC)\nDC2 = vect2.transform(DC)\nDC3 = tf1.transform(DC)\nDC4 = tf2.transform(DC)\nDC5 = tf_idf1.transform(DC)\nDC6 = tf_idf2.transform(DC)\nDC7 = stem_vect1.transform(DC)\nDC8 = stem_vect2.transform(DC)\nDC9 = stem_tf1.transform(DC)\nDC10 = stem_tf2.transform(DC)\nDC11 = stem_tf_idf1.transform(DC)\nDC12 = stem_tf_idf2.transform(DC)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## The better\nTesting with the better algorithm we have, MultinomialNB V2 (vectorizer with bigram)"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(NB[1].predict(DC2))\nprint('-'*20)\nprint(NB[1].predict(Marvel2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Other tests"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(LR[1].predict(DC2))\nprint('-'*20)\nprint(LR[1].predict(Marvel2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(SVM[11].predict(DC12))\nprint('-'*20)\nprint(SVM[11].predict(Marvel12))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusions\n\nThe better in all tests is the MultinomialNB V2, trained with the dataset of bigrams.\n\nIn this dataset, we can see some words there is only in a histories from DC Comics, like cities, so Gotham for example is a powerful keyword that does not exist in texts from Marvel creator.\n\n\n\n"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}