{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n#for dirname, _, filenames in os.walk('/kaggle/input'):\n#  for filename in filenames:\n#      print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install --no-deps '../input/timm-package/timm-0.1.26-py3-none-any.whl'\n!pip install --no-deps '../input/pycocotools/pycocotools-2.0-cp37-cp37m-linux_x86_64.whl'\n# !pip install effdet","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import sys\nsys.path.insert(0, \"../input/timm-efficientdet-pytorch\")\nsys.path.insert(0, \"../input/omegaconf\")\n\nimport json\nimport numpy as np\nimport pandas as pd\nimport cv2\nimport os\nimport re\nfrom datetime import datetime\nimport time\nimport random\n\nfrom PIL import Image\nfrom tqdm import tqdm\nfrom glob import glob\nimport albumentations as A\nfrom albumentations.pytorch.transforms import ToTensorV2\n\nimport torch\nimport torchvision\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom torchvision.models.detection import FasterRCNN\nfrom torchvision.models.detection.rpn import AnchorGenerator\nfrom torch.utils.data import Dataset,DataLoader\nfrom torch.utils.data.sampler import SequentialSampler, RandomSampler\nfrom sklearn.model_selection import StratifiedKFold\n\n\nfrom matplotlib import pyplot as plt\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TRAIN_ROOT_PATH = '/kaggle/input/train-dhaka-ai-and-poribohon/train_p/'\nIMG_SIZE = 512\n\ntrain_image_path = '/kaggle/input/train-dhaka-ai-and-poribohon/train_p/'\n\n\ntypes = ('*.jpg','*.jpeg','*.png','*.JPG','*.PNG') # the tuple of file types\ntrain_images = []\nfor type in types:\n  train_images.extend(sorted(glob(train_image_path + type)))\n\n\n\nprint(f' train images : {len(train_images)} items')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from random import sample\n\ndef get_ids(path_list):\n  path_list = random.sample(path_list,len(path_list))\n  id_list = [path.split('/')[-1].split('.')[0] for path in path_list]\n  return np.array(id_list)\n\ntrain_ids = get_ids(train_images)\n\n\nprint(len(train_ids))\nprint(train_ids)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import preprocessing\n#getting the labels\nmarking_train = pd.read_csv('/kaggle/input/train-dhaka-ai-and-poribohon/train_p/train_p.csv')\n\nwh_train = marking_train\nwh_train['w'] = marking_train['xmax'] - marking_train['xmin']\nwh_train['h'] = marking_train['ymax'] - marking_train['ymin']\nwh_train = wh_train.drop(['xmax','ymax'], axis = 1)\n\nwh_train = wh_train.rename(columns={\"class\" : \"classname\",\"width\": \"image_width\", \"height\": \"image_height\", \"xmin\": \"x\", \"ymin\": \"y\"})\nmarking_train = wh_train\nmarking_train['image_id'] = marking_train['image_name']\nfor i in range(0,marking_train['image_name'].size) :\n  marking_train['image_id'].iloc[i] = marking_train['image_name'].iloc[i].split('.')[0]\n#marking_train = marking_train.drop(['image_name'], axis = 1)\nle = preprocessing.LabelEncoder()\ntargets = le.fit_transform(marking_train['classname'].tolist())\nmarking_train['class_id'] = targets\nmarking_train = marking_train[['image_id','image_name','image_width','image_height','classname','class_id','x','y','w','h']]\nmarking_train = marking_train.drop(['image_name'], axis=1)\nmarking_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corrupt_files = ['113','114','118','231','Pias (359)', 'Pias (360)']\n\nfor i in corrupt_files :\n  # Get indexes where name column has value i\n  if marking_train[marking_train['image_id'] == i].index.any() :\n    indexNames = marking_train[marking_train['image_id'] == i].index\n    # Delete these row indexes from dataFrame\n    marking_train.drop(indexNames , inplace=True)\n\nmarking_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#code for each vehicle\ncolumn_values = marking_train[[\"classname\", \"class_id\"]].values.ravel()\nunique_values = pd.unique(column_values)\nprint(unique_values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = marking_train[(marking_train[['image_width']] != 0).all(axis=1)]\ndf.sort_values(by='image_width',ascending=True)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['x2'] = df['x'] + df['w'] \ndf['y2'] = df['y'] + df['h']\n\ndf = df.rename(columns={'x' : 'x1', 'y' : 'y1'})\ndf = df.drop(columns=['class_id','w','h'])\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['classname'] = df['classname'].replace(['three_wheelers_(CNG)'],'three wheelers (CNG)')\ndf['classname'] = df['classname'].replace(['human_hauler'],'human hauler')\ndf['classname'] = df['classname'].replace(['auto_rickshaw'],'auto rickshaw')\ndf['classname'] = df['classname'].replace(['army_vehicle'],'army vehicle')\nlen(df['classname'].unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"my_list = ['114','113','118','Navid_06_aug2','Numan_  (33)_aug1','Dipto_357_aug1','Pias (309)', 'Numan_(210)', 'Numan_(301)', 'Navid_295','Navid_650', 'Numan_(112)', 'Dipto_ 117', 'Dipto_752','truck (683)','rickshaw (201)','truck (684)','truck (643)']\n\nfor i in my_list :\n  df.drop(df.loc[df['image_id']==i].index, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class_map = dict(zip(df['classname'].unique(),list(range(1,22))))\nprint(df['classname'].unique())\npd.DataFrame(df['classname'].unique(),columns = ['classname']).to_csv('class.csv',index=False)\n\ndf['class'] = df['classname'].apply(lambda x : class_map[x])\n# df['id'] = df['id'].astype(np.str)\ndf[['x1','y1','x2','y2']] = df[['x1','y1','x2','y2']].astype(np.float)\ndf = df.drop(index = df[df['x1']==df['x2']].index)\ndf.to_csv('train.csv',index=False)\n# df = pd.read_csv('../input/facemask/train.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['class'].nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['image_id'].nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"count = 0\nERROR_LIST = []\nfor id in train_ids:\n    if id not in df['image_id'].unique():\n        count = count+1\n        ERROR_LIST.append(id)\nprint(count)\nprint(len(ERROR_LIST))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_ids2 =set(train_ids) - set(ERROR_LIST)\nprint(len(train_ids))\nprint(len(train_ids2))\n\ntrain_ids = train_ids2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# df = pd.read_csv('../input/facemask/FaceMask.csv')\n# df = df.rename(columns = {'classname':'class'})\n# df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n\ndf_folds = df[['image_id']].copy()\ndf_folds.loc[:, 'bbox_count'] = 0\ndf_folds = df_folds.groupby('image_id').count()\ndf_folds.loc[:, 'fold'] = 0\n\nfor fold_number, (train_index, val_index) in enumerate(skf.split(X=df_folds.index, y=df_folds.bbox_count)):\n    df_folds.loc[df_folds.iloc[val_index].index, 'fold'] = fold_number\n    \n# df_folds.to_csv('train_folds.csv',index=False)\n# df_folds = pd.read_csv('../input/facemask/train_folds.csv')\ndf_folds.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def getExtension(filename):\n  left = TRAIN_ROOT_PATH\n  types = ('*.jpg','*.jpeg','*.png','*.JPG','*.PNG') # the tuple of file types\n  train_images = []\n  for type in types:\n    train_images.extend(sorted(glob(TRAIN_ROOT_PATH + type)))\n  for i in train_images:\n    x = i\n    x = x.split('.')[0]\n    if filename == x:\n        return i.lstrip(left+x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(getExtension(TRAIN_ROOT_PATH+'Dipto_532'))\nprint(getExtension(TRAIN_ROOT_PATH+'Navid_582'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_train_transforms():\n    return A.Compose(\n        [\n#             A.RandomSizedCrop(min_max_height=(256, 256), height=1024, width=1024, p=0.5),\n            \n            A.OneOf([\n                A.HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit= 0.2, \n                                     val_shift_limit=0.2, p=0.9),\n                A.RandomBrightnessContrast(brightness_limit=0.2, \n                                           contrast_limit=0.2, p=0.9),\n            ],p=0.9),\n            A.ToGray(p=0.01),\n            A.HorizontalFlip(p=0.5),\n            A.VerticalFlip(p=0.01),\n            A.RandomSizedBBoxSafeCrop(512, 512, erosion_rate=0.0, interpolation=1, p=1.0),\n            A.Resize(height=512, width=512, p=1),\n            A.Cutout(num_holes=20, max_h_size=32, max_w_size=32, fill_value=0, p=0.5),\n            A.Blur(blur_limit=5, p=0.5),\n            A.ShiftScaleRotate(shift_limit=0.0625,scale_limit=0.1, rotate_limit=20 ,p=0.5),\n            ToTensorV2(p=1.0),\n        ], \n        p=1.0, \n        bbox_params=A.BboxParams(\n            format='pascal_voc',\n            min_area=0, \n            min_visibility=0,\n            label_fields=['labels']\n        )\n    )\n\ndef get_valid_transforms():\n    return A.Compose(\n        [\n            A.Resize(height=512, width=512, p=1.0),\n            ToTensorV2(p=1.0),\n        ], \n        p=1.0, \n        bbox_params=A.BboxParams(\n            format='pascal_voc',\n            min_area=0, \n            min_visibility=0,\n            label_fields=['labels']\n        )\n    )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"LIST_OF_ERROR_IMAGES = []\nclass DatasetRetriever(Dataset):\n\n    def __init__(self, marking, image_ids, transforms=None, test=False):\n        super().__init__()\n\n        self.image_ids = image_ids\n        self.marking = marking\n        self.transforms = transforms\n        self.test = test\n\n    def __getitem__(self, index: int):\n        image_id = self.image_ids[index]\n        image, boxes, labels = self.load_image_and_boxes(index)\n        \n#         if self.test or random.random() > 0.5:\n#             image, boxes = self.load_image_and_boxes(index)\n#         else:\n#             image, boxes = self.load_cutmix_image_and_boxes(index)\n        \n        target = {}\n        target['boxes'] = boxes\n        target['labels'] = labels\n        target['img_scale'] = torch.tensor([1.])\n        target['image_id'] = torch.tensor([index])\n        target['img_size'] = torch.tensor([(IMG_SIZE, IMG_SIZE)])\n\n        try :\n          if self.transforms:\n            for i in range(10):\n                sample = self.transforms(**{\n                    'image': image,\n                    'bboxes': target['boxes'],\n                    'labels': target['labels']\n                })\n                if len(sample['bboxes']) > 0:\n                    image = sample['image']\n                    target['boxes'] = torch.stack(tuple(map(torch.tensor, zip(*sample['bboxes'])))).permute(1, 0)\n                    target['boxes'][:,[0,1,2,3]] = target['boxes'][:,[1,0,3,2]]  #yxyx: be warning\n                    target['labels'] = torch.tensor(sample['labels'])\n                    break\n\n          return image, target, image_id\n        except:\n          print(\"\\nERROR IN THIS IMAGE : \",image_id)\n          LIST_OF_ERROR_IMAGES.append(image_id)\n          \n          image_id = self.image_ids[1]\n          image, boxes, labels = self.load_image_and_boxes(1)\n          \n  #         if self.test or random.random() > 0.5:\n  #             image, boxes = self.load_image_and_boxes(index)\n  #         else:\n  #             image, boxes = self.load_cutmix_image_and_boxes(index)\n          \n          target = {}\n          target['boxes'] = boxes\n          target['labels'] = labels\n          target['img_scale'] = torch.tensor([1.])\n          target['image_id'] = torch.tensor([index])\n          target['img_size'] = torch.tensor([(IMG_SIZE, IMG_SIZE)])\n\n          if self.transforms:\n            for i in range(10):\n                sample = self.transforms(**{\n                    'image': image,\n                    'bboxes': target['boxes'],\n                    'labels': target['labels']\n                })\n                if len(sample['bboxes']) > 0:\n                    image = sample['image']\n                    target['boxes'] = torch.stack(tuple(map(torch.tensor, zip(*sample['bboxes'])))).permute(1, 0)\n                    target['boxes'][:,[0,1,2,3]] = target['boxes'][:,[1,0,3,2]]  #yxyx: be warning\n                    target['labels'] = torch.tensor(sample['labels'])\n                    break\n\n          return image, target, image_id\n\n    def __len__(self) -> int:\n        return self.image_ids.shape[0]\n\n    def load_image_and_boxes(self, index):\n        \n        path = TRAIN_ROOT_PATH\n        try:\n            image_id = self.image_ids[index]\n            ext = getExtension(str(path + image_id))\n            image = cv2.imread(f'{path}/{image_id}{ext}', cv2.IMREAD_COLOR)\n            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n            image /= 255.0\n            records = self.marking[self.marking['image_id'] == image_id]\n            boxes = records[['x1', 'y1', 'x2', 'y2']].values\n            return image, boxes , records['class']\n        except:\n            image_id = self.image_ids[0]\n            ext = getExtension(str(path + image_id))\n            image = cv2.imread(f'{path}/{image_id}{ext}', cv2.IMREAD_COLOR)\n            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n            image /= 255.0\n            records = self.marking[self.marking['image_id'] == image_id]\n            boxes = records[['x1', 'y1', 'x2', 'y2']].values\n            return image, boxes , records['class']\n        \n            \n        \n        #return image, boxes , records['class']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fold_number = 0\n\ntrain_dataset = DatasetRetriever(\n    image_ids=df_folds[df_folds['fold'] != fold_number].index.values,\n    marking=df,\n    transforms=get_train_transforms(),\n    test=False,\n)\n\nvalidation_dataset = DatasetRetriever(\n    image_ids=df_folds[df_folds['fold'] == fold_number].index.values,\n    marking=df,\n    transforms=get_valid_transforms(),\n    test=True,\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(train_dataset))\nprint(len(validation_dataset))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig=plt.figure(figsize=(30, 30))\ncolumns = 4\nrows = 3\n\n#c_map = dict(zip(list(range(1,21)),pd.read_csv('../input/facemask/classname.csv')['classname'].unique()))\n\nfor i in range(1, columns*rows +1):\n    image, target, image_id = train_dataset[i]\n    boxes = target['boxes'].cpu().numpy().astype(np.int32)\n    classnames = target['labels'].cpu().numpy()\n    numpy_image = image.permute(1,2,0).cpu().numpy()\n\n    #fig, ax = plt.subplots(1, 1, figsize=(16, 8))\n\n    for box in boxes:\n        cv2.rectangle(numpy_image, (box[1], box[0]), (box[3],  box[2]), (0, 1, 0), 2)\n    \n    fig.add_subplot(rows, columns, i)\n    plt.imshow(numpy_image)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig=plt.figure(figsize=(30, 30))\ncolumns = 4\nrows = 3\n\n#c_map = dict(zip(list(range(1,21)),pd.read_csv('../input/facemask/classname.csv')['classname'].unique()))\n\nfor i in range(1, columns*rows +1):\n    image, target, image_id = validation_dataset[i]\n    boxes = target['boxes'].cpu().numpy().astype(np.int32)\n    classnames = target['labels'].cpu().numpy()\n    numpy_image = image.permute(1,2,0).cpu().numpy()\n\n    #fig, ax = plt.subplots(1, 1, figsize=(16, 8))\n\n    for box in boxes:\n        cv2.rectangle(numpy_image, (box[1], box[0]), (box[3],  box[2]), (0, 1, 0), 2)\n    \n    fig.add_subplot(rows, columns, i)\n    plt.imshow(numpy_image)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class AverageMeter(object):\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class TrainGlobalConfig:\n    num_workers = 8\n    batch_size = 2\n    n_epochs = 5\n    lr = 0.001\n\n    folder = 'effdet5checkpoint/'\n    verbose = True\n    verbose_step = 1\n    step_scheduler = False  # do scheduler.step after optimizer.step\n    validation_scheduler = True  # do scheduler.step after validation stage loss\n    \n    SchedulerClass = torch.optim.lr_scheduler.ReduceLROnPlateau\n    scheduler_params = dict(\n        mode='min',\n        factor=0.25,\n        patience=2,\n        verbose=False, \n        threshold=0.0001,\n        threshold_mode='abs',\n        cooldown=0, \n        min_lr=1e-8,\n        eps=1e-08\n    )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Fitter:\n    \n    def __init__(self, model, device, config):\n        self.config = config\n        self.epoch = 0\n\n        self.base_dir = f'./{config.folder}'\n        if not os.path.exists(self.base_dir):\n            os.makedirs(self.base_dir)\n        \n        self.log_path = f'{self.base_dir}/log.txt'\n        self.best_summary_loss = 10**5\n\n        self.model = model\n        self.device = device\n\n        param_optimizer = list(self.model.named_parameters())\n        no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n        optimizer_grouped_parameters = [\n            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n        ] \n\n        self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=config.lr)\n        self.scheduler = config.SchedulerClass(self.optimizer, **config.scheduler_params)\n        self.log(f'Fitter prepared. Device is {self.device}')\n        \n        self.load('../input/lastcheckpoint/last-checkpoint.bin')\n\n    def fit(self,df_folds,df):\n        fold_number = 0\n        for e in range(self.config.n_epochs):\n            fold_number = (fold_number+1)%10\n\n            train_dataset = DatasetRetriever(\n                image_ids=df_folds[df_folds['fold'] != fold_number].index.values,\n                marking=df,\n                transforms=get_train_transforms(),\n                test=False,\n            )\n\n            validation_dataset = DatasetRetriever(\n                image_ids=df_folds[df_folds['fold'] == fold_number].index.values,\n                marking=df,\n                transforms=get_valid_transforms(),\n                test=True,\n            )\n            \n            train_loader = torch.utils.data.DataLoader(\n                train_dataset,\n                batch_size=self.config.batch_size,\n#               sampler=RandomSampler(train_dataset),\n                shuffle=True,\n                pin_memory=False,\n                drop_last=True,\n                num_workers=self.config.num_workers,\n                collate_fn=collate_fn,\n            )\n            validation_loader = torch.utils.data.DataLoader(\n                validation_dataset, \n                batch_size=self.config.batch_size,\n                num_workers=self.config.num_workers,\n                shuffle=False,\n#         sampler=SequentialSampler(validation_dataset),\n                pin_memory=False,\n                collate_fn=collate_fn,\n            )\n            \n            if self.config.verbose:\n                lr = self.optimizer.param_groups[0]['lr']\n                timestamp = datetime.utcnow().isoformat()\n                self.log(f'LR: {lr}')\n\n            t = time.time()\n            summary_loss = self.train_one_epoch(train_loader)\n\n            self.log(f'[RESULT]: Train. Epoch: {self.epoch}, summary_loss: {summary_loss.avg:.5f}, time: {(time.time() - t):.5f}')\n            self.save(f'{self.base_dir}/last-checkpoint.bin')\n\n            t = time.time()\n            summary_loss = self.validation(validation_loader)\n\n            self.log(f'[RESULT]: Val. Epoch: {self.epoch}, summary_loss: {summary_loss.avg:.5f}, time: {(time.time() - t):.5f}')\n            if summary_loss.avg < self.best_summary_loss:\n                self.best_summary_loss = summary_loss.avg\n                self.model.eval()\n                self.save(f'{self.base_dir}/best-checkpoint-{str(self.epoch).zfill(3)}epoch.bin')\n                for path in sorted(glob(f'{self.base_dir}/best-checkpoint-*epoch.bin'))[:-3]:\n                    os.remove(path)\n\n            if self.config.validation_scheduler:\n                self.scheduler.step(metrics=summary_loss.avg)\n\n            self.epoch += 1\n\n    def validation(self, val_loader):\n        self.model.eval()\n        summary_loss = AverageMeter()\n        \n        tk0 =  tqdm(enumerate(val_loader),total = len(validation_dataset)/TrainGlobalConfig.batch_size)\n        for step, (images, targets, image_ids) in tk0:\n            if self.config.verbose:\n                if step % self.config.verbose_step == 0:\n                    tk0.set_postfix( loss = summary_loss.avg)\n                    \n            with torch.no_grad():\n                images = torch.stack(images)\n                batch_size = images.shape[0]\n                images = images.to(self.device).float()\n                boxes = [target['boxes'].to(self.device).float() for target in targets]\n                labels = [target['labels'].to(self.device).float() for target in targets]\n\n                loss, _, _ = self.model(images, boxes, labels)\n                summary_loss.update(loss.detach().item(), batch_size)\n\n        return summary_loss\n\n    def train_one_epoch(self, train_loader):\n        self.model.train()\n        summary_loss = AverageMeter()\n        \n        tk0 =  tqdm(enumerate(train_loader),total = int(len(train_dataset)/TrainGlobalConfig.batch_size))\n        for step, (images, targets, image_ids) in tk0:\n            if self.config.verbose:\n                if step % self.config.verbose_step == 0:\n                    tk0.set_postfix( loss = summary_loss.avg)\n            \n            images = torch.stack(images)\n            images = images.to(self.device).float()\n            batch_size = images.shape[0]\n            boxes = [target['boxes'].to(self.device).float() for target in targets]\n            labels = [target['labels'].to(self.device).float() for target in targets]\n            '''\n            print(images.shape)\n            print(len(boxes))\n            print(len(labels))\n            '''\n\n            self.optimizer.zero_grad()\n            \n            loss, _, _ = self.model(images, boxes, labels)\n            \n            loss.backward()\n\n            summary_loss.update(loss.detach().item(), batch_size)\n\n            self.optimizer.step()\n\n            if self.config.step_scheduler:\n                self.scheduler.step()\n\n        return summary_loss\n    \n    def save(self, path):\n        self.model.eval()\n        torch.save({\n            'model_state_dict': self.model.model.state_dict(),\n            'optimizer_state_dict': self.optimizer.state_dict(),\n            'scheduler_state_dict': self.scheduler.state_dict(),\n            'best_summary_loss': self.best_summary_loss,\n            'epoch': self.epoch,\n        }, path)\n\n    def load(self, path):\n        checkpoint = torch.load(path)\n        self.model.model.load_state_dict(checkpoint['model_state_dict'])\n        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n        self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n        self.best_summary_loss = checkpoint['best_summary_loss']\n        self.epoch = checkpoint['epoch'] + 1\n        \n    def log(self, message):\n        if self.config.verbose:\n            print(message)\n        with open(self.log_path, 'a+') as logger:\n            logger.write(f'{message}\\n')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def collate_fn(batch):\n    return tuple(zip(*batch))\n\ndef run_training(df_folds,df):\n    device = torch.device('cuda:0')\n    net.to(device)\n\n    \n\n    fitter = Fitter(model=net, device=device, config=TrainGlobalConfig)\n    fitter.fit(df_folds,df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from effdet import get_efficientdet_config, EfficientDet, DetBenchTrain\nfrom effdet.efficientdet import HeadNet\n\ndef get_net():\n    config = get_efficientdet_config('tf_efficientdet_d5')\n    net = EfficientDet(config, pretrained_backbone=True)\n#     checkpoint = torch.load('../input/efficientdet/efficientdet_d5-ef44aea8.pth')\n#     net.load_state_dict(checkpoint)\n    config.num_classes = 21\n    config.image_size = 512\n    net.class_net = HeadNet(config, num_outputs=config.num_classes, norm_kwargs=dict(eps=.001, momentum=.01))\n    return DetBenchTrain(net, config)\n\nnet = get_net()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"run_training(df_folds,df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}