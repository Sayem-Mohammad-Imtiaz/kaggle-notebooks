{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.impute import SimpleImputer\nfrom xgboost import XGBRegressor\nfrom statistics import mean\nimport re\nfrom scipy import stats\nfrom scipy.stats import skew, norm\nfrom scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax\nfrom lightgbm import LGBMRegressor\nfrom sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error, accuracy_score\n\nimport warnings\ndef ignore_warn(*args, **kwargs):\n    pass\nwarnings.warn = ignore_warn \n\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/prohack12/train.csv')\ntest = pd.read_csv('/kaggle/input/prohack12/test.csv')\nsubmission = pd.read_csv('/kaggle/input/prohack/sample_submit.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features = train.drop(['y'], axis = 1)\ntrain_dependent = train['y']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#categorical and numerical variables\ncategorical_cols = []\nnumerical_cols = []\nfor col in train_features.columns:\n    if train_features[col].dtype in ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']:\n        numerical_cols.append(col)\n    elif train_features[col].dtype == object:\n        categorical_cols.append(col)\n\n        \nfeatures = numerical_cols + categorical_cols\ntrain_features = train[features]\ntest = test[features]\n\nprint('test', test.shape)\nprint('train_features', train_features.shape)\nprint('train_dependent', train_dependent.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features = pd.concat([train_features, test], axis = 0)\nfeatures.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Null numerical values (percentage)\nnull = features[numerical_cols].isna().sum().sort_values(ascending = False)\nnull_per = (null/4755) * 100\nnull_perc = pd.DataFrame(null_per)\nnull_perc.head(50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#percentages plot\nf, ax = plt.subplots(figsize=(25, 22))\nplt.xticks(rotation='90')\nsns.barplot(x=null_per.index, y=null_per)\nplt.xlabel('Features', fontsize=15)\nplt.ylabel('Percent of missing values', fontsize=15)\nplt.title('Percent missing data by feature', fontsize=15)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This dataset is just messed up maaaan. so sad","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#columns with null values < 60%\nfor entry, column in zip(null_perc.iloc[:, 0], null_perc.index):\n    if entry <= 60:\n        print(column)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### The latter","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* GGP - Simple Imputer based on mean\n* Population using at least basic sanitation services (%) - Simple Imputer based on mean\n* Population using at least basic drinking-water services (%) - SimpleImputer based on mean\n* Intergalactic Development Index (IDI), Rank - SimpleImputer based on most_frequent\n* Intergalactic Development Index (IDI) - SimpleImputer based on mean\n* Education Index -  SimpleImputer based on mean\n* Mean years of education (galactic years) -  SimpleImputer based on mean\n* Expected years of education (galactic years) -  SimpleImputer based on mean\n* Income Index -  SimpleImputer based on mean\n* Gross income per capita -  SimpleImputer based on mean\n* existence expectancy at birth - SimpleImputer based on mean\n* existence expectancy index-  SimpleImputer based on mean","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"features.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#imputing with column means.\ncolumns = [1,2,3,4,5,6,7,8,10,11,12]\nfor col in columns:\n    x = features.iloc[:, col].values\n    x = x.reshape(-1,1)\n    imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n    imputer = imputer.fit(x)\n    x = imputer.transform(x)\n    features.iloc[:, col] = x    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#imputing 'Intergalactic Development Index (IDI), Rank' with most_frequent.\nx = features.iloc[:, 9].values\nx = x.reshape(-1,1)\nimputer = SimpleImputer(missing_values=np.nan, strategy='mean')\nimputer = imputer.fit(x)\nx = imputer.transform(x)\nfeatures.iloc[:, 9] = x    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = features.iloc[:, 3].values\nx = x.reshape(-1,1)\nimputer = SimpleImputer(missing_values=np.nan, strategy='mean')\nimputer = imputer.fit(x)\nx = imputer.transform(x)\nfeatures.iloc[:, 3] = x    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Null numerical values (percentage)\nnull = features[numerical_cols].isna().sum().sort_values(ascending = False)\nnull_perc = (null/4755) * 100\nnull_perc = pd.DataFrame(null_perc)\nnull_perc.tail(15)         ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### The former (>60%)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"features_to_be_dropped = []\nfor feature in features.columns:\n    null_perc = null_perc\n    for entry, column in zip(null_perc.iloc[:, 0], null_perc.index):\n        if entry > 60:\n            over_60 = column\n            features_to_be_dropped.append(over_60)\n            \nprint('\\nFeatures with null values over 60%:\\n')\nprint(features_to_be_dropped)\n\nfeatures = features.drop(features_to_be_dropped, axis=1).copy()\nfeatures.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features.isna().any()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Categorical Features\n\nThere's only one categorical features, **galaxy**, which is more like **ID**. I'm gonna drop it then work with the remaining 14 numerical columns.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"features = features.drop(['galaxy'], axis = 1)\nfeatures.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature Generation\n* existence_expectancy_trend - based on 'existence expectancy index', 'existence expectancy at birth'\n* pop using at least basic needs - based on drinking water services and sanitation services\n* capital formation : per capita - based on capital formation and gross income per capita\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"features['existence_expectancy_trend '] = features['existence expectancy at birth'] / features['existence expectancy index']\nfeatures['access to basic needs'] = (features['Population using at least basic drinking-water services (%)'] + features['Population using at least basic sanitation services (%)']) / 2\nfeatures['capital formation to per capita'] = (features['Gross capital formation (% of GGP)'] * 100) / features['Gross income per capita']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Distribution test and Skewness","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Target variable\nsns.distplot(train_dependent.index , fit=norm);\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(train_dependent)\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('y distribution')\n\n#Get also the QQ-plot\nfig = plt.figure()\nres = stats.probplot(train_dependent.index, plot=plt)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Linear models love normally distributed data but the plot shows that the variable is more **uniformly** distributed than **normally**. I tried applying boxcox and log transformations to normalize it but both miu and sigma were reducing when the latter should approach 1. So I'm gonna leave it as it is and comment out the code for the transformations.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# #We use the numpy fuction log1p which  applies log(1+x) to all elements of the column\n\n#log transformation\n# train_dependent = np.log1p(train_dependent)\n\n#boxcox transformation.\n# train_dependent = boxcox1p(train_dependent, boxcox_normmax(train_dependent + 1))\n\n# #Check the new distribution \n# sns.distplot(train_dependent.index , fit=norm);\n\n# # Get the fitted parameters used by the function\n# (mu, sigma) = norm.fit(train_dependent)\n# print( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n# #Now plot the distribution\n# plt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n#             loc='best')\n# plt.ylabel('Frequency')\n# plt.title('SalePrice distribution')\n\n# #Get also the QQ-plot\n# fig = plt.figure()\n# res = stats.probplot(train_dependent.index, plot=plt)\n# plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f, axes = plt.subplots(2, 4, figsize=(67, 40), sharex=True)\nsns.distplot( features[\"galactic year\"] , color=\"skyblue\", ax=axes[0, 0])\nsns.distplot( features[\"existence expectancy index\"] , color=\"olive\", ax=axes[0, 1])\nsns.distplot( features[\"existence expectancy at birth\"] , color=\"gold\", ax=axes[0, 2])\nsns.distplot( features[\"Gross income per capita\"] , color=\"teal\", ax=axes[0, 3])\nsns.distplot( features[\"Income Index\"] , color=\"skyblue\", ax=axes[1, 0])\nsns.distplot( features[\"Expected years of education (galactic years)\"] , color=\"olive\", ax=axes[1, 1])\nsns.distplot( features[\"Mean years of education (galactic years)\"] , color=\"gold\", ax=axes[1, 2])\nsns.distplot( features[\"Intergalactic Development Index (IDI)\"] , color=\"teal\", ax=axes[1, 3])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f, axes = plt.subplots(2, 4, figsize=(57, 40), sharex=True)\nsns.distplot( features[\"Education Index\"] , color=\"skyblue\", ax=axes[0, 0])\nsns.distplot( features[\"Intergalactic Development Index (IDI), Rank\"] , color=\"olive\", ax=axes[0, 1])\nsns.distplot( features[\"Population using at least basic drinking-water services (%)\"] , color=\"gold\", ax=axes[0, 2])\nsns.distplot( features[\"Population using at least basic sanitation services (%)\"] , color=\"teal\", ax=axes[0, 3])\nsns.distplot( features[\"Gross capital formation (% of GGP)\"] , color=\"skyblue\", ax=axes[1, 0])\nsns.distplot( features[\"existence_expectancy_trend \"] , color=\"olive\", ax=axes[1, 1])\nsns.distplot( features[\"access to basic needs\"] , color=\"gold\", ax=axes[1, 2])\nsns.distplot( features[\"capital formation to per capita\"] , color=\"teal\", ax=axes[1, 3])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# numerical variables\nnorm_features = ['galactic year','Gross income per capita',\n       'Intergalactic Development Index (IDI), Rank',\n       'Population using at least basic drinking-water services (%)',\n       'Population using at least basic sanitation services (%)',\n       'Gross capital formation (% of GGP)', 'existence_expectancy_trend ',\n       'access to basic needs', 'capital formation to per capita']\nskew_features = features[norm_features].apply(lambda x: skew(x)).sort_values(ascending=False)\n\nhigh_skew = skew_features[skew_features > 0.5]\nskew_index = high_skew.index\n\nlow_skew = skew_features[skew_features < -0.5]\nlow_skew_index = low_skew.index\n\nprint(\"There are {} numerical features with Skew > 0.5 :\".format(high_skew.shape[0]))\nskewness = pd.DataFrame({'Skew' :high_skew})\n\nprint(\"There are {} numerical features with Skew < -0.5 :\".format(low_skew.shape[0]))\nskewness = pd.DataFrame({'Skew' :low_skew})\nskew_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(skew_index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Normalize skewed features with boxcox transformation\n# for i in skew_index:\n#     features[i] = boxcox1p(features[i], boxcox_normmax(features[i] + 1))\n# for j in low_skew_index:\n#     features[j] = boxcox1p(features[j], boxcox_normmax(features[j] + 1))\nfor i in skew_index:\n    log_max = np.log(features[i].max())\n    features[i] = features[i]**(1/log_max)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in low_skew_index:\n    log_max = np.log(features[i].max())\n    features[i] = features[i]**(1/log_max)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f, axes = plt.subplots(2, 4, figsize=(57, 40), sharex=True)\nsns.distplot( features[\"existence_expectancy_trend \"] , color=\"skyblue\", ax=axes[0, 0])\nsns.distplot( features[\"Gross income per capita\"] , color=\"olive\", ax=axes[0, 1])\nsns.distplot( features[\"Population using at least basic drinking-water services (%)\"] , color=\"gold\", ax=axes[0, 2])\nsns.distplot( features[\"Population using at least basic sanitation services (%)\"] , color=\"teal\", ax=axes[0, 3])\nsns.distplot( features[\"Gross capital formation (% of GGP)\"] , color=\"skyblue\", ax=axes[1, 0])\nsns.distplot( features[\"existence_expectancy_trend \"] , color=\"olive\", ax=axes[1, 1])\nsns.distplot( features[\"access to basic needs\"] , color=\"gold\", ax=axes[1, 2])\nsns.distplot( features[\"capital formation to per capita\"] , color=\"teal\", ax=axes[1, 3])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# skewness after\nnorm_features = ['galactic year','Gross income per capita',\n       'Intergalactic Development Index (IDI), Rank',\n       'Population using at least basic drinking-water services (%)',\n       'Population using at least basic sanitation services (%)',\n       'Gross capital formation (% of GGP)', 'existence_expectancy_trend ',\n       'access to basic needs', 'capital formation to per capita']\nskews = features[norm_features].apply(lambda x: skew(x)).sort_values(ascending=False)\nskews","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"good enough. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Multicolinearity","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#defining numerical features again to include the added features for the correlation plot to be plotted.\nnumerical_cols= []\nfor column in train_features.columns:\n    if train_features[column].dtype in ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']:\n        numerical_cols.append(column)\n\nnew_train_set = pd.concat([features.iloc[:len(train_dependent), :], train_dependent], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def correlation_map(f_data, f_feature, f_number):\n    f_most_correlated = f_data.corr().nlargest(f_number,f_feature)[f_feature].index\n    f_correlation = f_data[f_most_correlated].corr()\n    \n    f_mask = np.zeros_like(f_correlation)\n    f_mask[np.triu_indices_from(f_mask)] = True\n    with sns.axes_style(\"white\"):\n        f_fig, f_ax = plt.subplots(figsize=(12, 10))\n        f_ax = sns.heatmap(f_correlation, mask=f_mask, vmin=0, vmax=1, square=True,\n                           annot=True, annot_kws={\"size\": 10}, cmap=\"BuPu\")\n\n    plt.show()\n\ncorrelation_map(new_train_set, 'y', 20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"basic needs, sanitation and water: high corrrelation.\nbasic needs has a higher in relation to y, so I'm gonna drop sanitation and water.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"features = features.drop(['Population using at least basic drinking-water services (%)', 'Population using at least basic sanitation services (%)'], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Outliers","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"credits to [this kernel on outlier detection using chauvenets creterion](http://kaggle.com/nroman/detecting-outliers-with-chauvenet-s-criterion)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def chauvenet(array):\n    mean = array.mean()           # Mean of incoming array\n    stdv = array.std()            # Standard deviation\n    N = len(array)                # Lenght of incoming array\n    criterion = 1.0/(2*N)         # Chauvenet's criterion\n    d = abs(array-mean)/stdv      # Distance of a value to mean in stdv's\n    prob = erfc(d)                # Area normal dist.    \n    return prob < criterion       # Use boolean array outside this function","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features_outliers = dict()\nfor col in [col for col in features.columns if 'var_' in col]:\n    features_outliers[col] = features[chauvenet(train[col].values)].shape[0]\nfeatures_outliers = pd.Series(features_outliers)\nfeatures_outliers","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It's an empty series, meaning we're safe on the outliers side.\nWe can check that visually using scatterplots.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"f, axes = plt.subplots(3, 2, figsize=(37, 30), sharex=True)\nsns.scatterplot(x = new_train_set[\"existence_expectancy_trend \"] ,y = new_train_set['y'], color=\"skyblue\", ax=axes[0, 0])\nsns.scatterplot(x = new_train_set[\"Gross income per capita\"] ,y = new_train_set['y'], color=\"olive\", ax=axes[0, 1])\nsns.scatterplot(x = new_train_set[\"Gross capital formation (% of GGP)\"] ,y = new_train_set['y'], color=\"skyblue\", ax=axes[1, 0])\nsns.scatterplot(x = new_train_set[\"existence_expectancy_trend \"] ,y = new_train_set['y'], color=\"olive\", ax=axes[1, 1])\nsns.scatterplot(x = new_train_set[\"access to basic needs\"] ,y = new_train_set['y'], color=\"gold\", ax=axes[2, 0])\nsns.scatterplot(x = new_train_set[\"capital formation to per capita\"] ,y = new_train_set['y'], color=\"teal\", ax=axes[2, 1])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f, axes = plt.subplots(4, 2, figsize=(37, 30), sharex=True)\nsns.scatterplot(x = new_train_set[\"galactic year\"] ,y = new_train_set['y'], color=\"skyblue\", ax=axes[0, 0])\nsns.scatterplot(x = new_train_set[\"existence expectancy index\"] ,y = new_train_set['y'], color=\"olive\", ax=axes[0, 1])\nsns.scatterplot(x = new_train_set[\"existence expectancy at birth\"] ,y = new_train_set['y'], color=\"skyblue\", ax=axes[1, 0])\nsns.scatterplot(x = new_train_set[\"Gross income per capita\"] ,y = new_train_set['y'], color=\"olive\", ax=axes[1, 1])\nsns.scatterplot(x = new_train_set[\"Income Index\"] ,y = new_train_set['y'], color=\"gold\", ax=axes[2, 0])\nsns.scatterplot(x = new_train_set[\"Expected years of education (galactic years)\"] ,y = new_train_set['y'], color=\"teal\", ax=axes[2, 1])\nsns.scatterplot(x = new_train_set[\"Mean years of education (galactic years)\"] ,y = new_train_set['y'], color=\"teal\", ax=axes[3, 0])\nsns.scatterplot(x = new_train_set[\"Intergalactic Development Index (IDI)\"] ,y = new_train_set['y'], color=\"teal\", ax=axes[3, 1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Capital formation doesn't look good. Solving that using percentiles but applying to all columns.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Capping the outlier rows with Percentiles\ncols = features.columns\nfor col in cols:\n    upper_lim = features[col].quantile(.95)\n    lower_lim = features[col].quantile(.05)\n    features.loc[(features[col] > upper_lim),col] = upper_lim\n    features.loc[(features[col] < lower_lim),col] = lower_lim\n    \nprint(features.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There's no categorical features to encode, thus we skip that.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Reconstruct train and test sets","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train = features.iloc[:len(train_dependent), :]\nx_test = features.iloc[len(train_dependent):, :]\ny_train = train_dependent\ntrain_set = pd.concat([x_train, y_train], axis=1)\n\nprint('x train', x_train.shape)\nprint('y train', y_train.shape)\nprint('train set', train_set.shape)\nprint('x test', x_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Imputing forgotten nans. Not done in the most efficient order but gets the job done.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"x = x_train.iloc[:, 3].values\nx = x.reshape(-1,1)\nimputer = SimpleImputer(missing_values=np.nan, strategy='mean')\nimputer = imputer.fit(x)\nx = imputer.transform(x)\nx_train.iloc[:, 3] = x    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = x_train.iloc[:, -1].values\nx = x.reshape(-1,1)\nimputer = SimpleImputer(missing_values=np.nan, strategy='mean')\nimputer = imputer.fit(x)\nx = imputer.transform(x)\nx_train.iloc[:, -1] = x    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Modelling.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Credits to this awesome [kernel](http://kaggle.com/serigne/stacked-regressions-top-4-on-leaderboard/notebook#Modelling)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Validation function\nn_folds = 5\n\ndef rmsle_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(x_train.values)\n    rmse= np.sqrt(-cross_val_score(model, x_train.values, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#lasso regression\nlasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=0))\n\n#elastic net regression\nenet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=1))\n\n#lightgbm\nlgb = LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.05, n_estimators=720,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)\n\n#gradboost\ngdb = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =5)\n\n#kernelridge\nkr = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)\n\n#xgboost\nxgb = XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state =7, nthread = -1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model scores","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"score = rmsle_cv(lasso)\nprint(\"\\nLasso score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score = rmsle_cv(enet)\nprint(\"\\nElastic net score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score = rmsle_cv(lgb)\nprint(\"\\nlightgbm score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score = rmsle_cv(gdb)\nprint(\"\\ngradboost score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score = rmsle_cv(kr)\nprint(\"\\nkernel ridge score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score = rmsle_cv(xgb)\nprint(\"\\nxgboost score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Viewing the performances using a barplot","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"l = [0.0432, 0.0431, 0.0310, 0.0310, 0.0430, 0.0337]\nc = ['lasso', 'elasticnet', 'lgb', 'gradboost', 'kernel ridge', 'xgboost']\n# data = {'lasso': [0.0432], 'elasticnet': [0.0431], 'lgb': [0.0310], 'gradboost': [0.0310], 'kernel ridge': [0.0430], 'xgboost':  [0.0337]}\ndf = pd.DataFrame(l, index = c)\nsns.barplot(x = df.index, y = df[0])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Stacking","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, models):\n        self.models = models\n        \n    # we define clones of the original models to fit the data in\n    def fit(self, x_train, y_train):\n        self.models_ = [clone(x) for x in self.models]\n        \n        # Train cloned base models\n        for model in self.models_:\n            model.fit(x_train, y_train)\n\n        return self\n    \n    #Now we do the predictions for cloned models and average them\n    def predict(self, x_train):\n        predictions = np.column_stack([\n            model.predict(x_train) for model in self.models_\n        ])\n        return np.mean(predictions, axis=1)   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#averaging models and observing score change.\naveraged_models = AveragingModels(models = (lasso, enet, lgb, xgb, gdb))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score = rmsle_cv(averaged_models)\nprint(\" Averaged base models score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"score better than 3 of the models","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Averaging w/a meta model trained with predictions of base models under kfold","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, base_models, meta_model, n_folds=5):\n        self.base_models = base_models\n        self.meta_model = meta_model\n        self.n_folds = n_folds\n   \n    # We again fit the data on clones of the original models\n    def fit(self, X, y):\n        self.base_models_ = [list() for x in self.base_models]\n        self.meta_model_ = clone(self.meta_model)\n        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156)\n        \n        # Train cloned base models then create out-of-fold predictions\n        # that are needed to train the cloned meta-model\n        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n        for i, model in enumerate(self.base_models):\n            for train_index, holdout_index in kfold.split(X, y):\n                instance = clone(model)\n                self.base_models_[i].append(instance)\n                instance.fit(X[train_index], y[train_index])\n                y_pred = instance.predict(X[holdout_index])\n                out_of_fold_predictions[holdout_index, i] = y_pred\n                \n        # Now train the cloned  meta-model using the out-of-fold predictions as new feature\n        self.meta_model_.fit(out_of_fold_predictions, y)\n        return self\n   \n    #Do the predictions of all base models on the test data and use the averaged predictions as \n    #meta-features for the final prediction which is done by the meta-model\n    def predict(self, X):\n        meta_features = np.column_stack([\n            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n            for base_models in self.base_models_ ])\n        return self.meta_model_.predict(meta_features)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"performance","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"stacked_averaged_models = StackingAveragedModels(base_models = (xgb, gdb, averaged_models),\n                                                 meta_model = lgb)\n\n# score = rmsle_cv(stacked_averaged_models)\n# print(\"Stacking Averaged models score: {:.4f} ({:.4f})\".format(score.mean(), score.std()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Final fitting and prediction","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# evaluation\ndef rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# stacked model\nstacked_averaged_models.fit(x_train.values, y_train)\nstacked_train_pred = stacked_averaged_models.predict(x_train.values)\nstacked_pred = np.expm1(stacked_averaged_models.predict(x_test.values))\nprint(rmsle(y_train, stacked_train_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"scored better than the models and the averaged model too.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#gradboost\ngdb.fit(x_train, y_train)\ngdb_train_pred = gdb.predict(x_train)\ngdb_pred = np.expm1(gdb.predict(x_test))\nprint(rmsle(y_train, gdb_train_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# xgb\nxgb.fit(x_train, y_train)\nxgb_train_pred = xgb.predict(x_train)\nxgb_pred = np.expm1(xgb.predict(x_test))\nprint(rmsle(y_train, xgb_train_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lgb\nlgb.fit(x_train.values, y_train)\nlgb_train_pred = lgb.predict(x_train)\nlgb_pred = np.expm1(lgb.predict(x_test.values))\nprint(rmsle(y_train, lgb_train_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"l = [0.021496434619159566, 0.015215194899359909, 0.030173101327980283, 0.021534161176007767\n]\nc = ['stacked', 'gradboost', 'xgb', 'lgb']\ndf = pd.DataFrame(l, index = c)\nsns.barplot(x = df.index, y = df[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"ditch xgboost","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"'''RMSE on the entire Train data when averaging'''\n\nprint('RMSLE score on train data:')\nprint(rmsle(y_train,stacked_train_pred*0.05 +\n               gdb_train_pred*0.9 + lgb_train_pred*0.05))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ensemble = stacked_pred*0.05 + gdb_pred*0.90 + lgb_pred*0.05","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ensemble.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Submission","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = submission\nindex = list(submission.index)\npred = list(ensemble)\nfile = pd.DataFrame({'index':index, 'pred': pred})\nsubmission = file.set_index('index')\nsubmission.to_csv('sub.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Energy allocation headache","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install pulp","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### First let B  = Budget = 50,000\nThen let Ei = Energy allocation i\nThen let c = (1/1000) * (Index increase potential) ^ 2\n\nSo, now you want to maximize SUM( c x E )\n\nBy using these variable names I'm trying to clarify and separate the constants from the variable. E in this case is the optimization variable, while c is a constant basically. This is one of the reasons why it is a Linear programming problem. Otherwise, it could be convex, convex quadratic, non-convex problems, and so on.\n\nThe optimizer would assign values to E such that the maximum possible value of F = SUM(c x E) would be attained. F is known as the objective function, in other parlance; cost function or loss.\n\n\nI am not yet at the constraint, there are more or less 3 steps to an optimization process. Translate problem, define objective function, define constraints, then run solver.\n\nThe optimizer would assign values to E such that the maximum possible value of F = SUM(c x E) would be attained. F is known as the objective function, in other parlance; cost function or loss.\n\n\nFor the constraints, the first is that B = 50,000. So all the energies assigned to the galaxies (that means SUM( Ei )) has to be less than 50,000.\n\n\nFor the constraints, the first is that B = 50,000. So all the energies assigned to the galaxies (that means SUM( Ei )) has to be at most 50,000.  \n\nThe next constrain is that energy for each galaxy (that means Ei ) has to be between 0 and 100.\n\nSo constraint 2 is 0 <= Ei <= 100\n\nThe third constraint requires more nuance. One way to do it is to inject a binary variable (b). So b is where eei of galaxy i is less than 0.7. You can do it with python. It will return True and False values. Then simply add 0 (+0) to the outcome, and an automatic type coercion will change the T/F to 1/0. You then multiply this with the galaxies, to get the ones with eei < 0.7. Once this is done, the constraint becomes SUM(Ei (of b)) <= 10% x B.\n\n\nAlternatively, you could split it into two optimization problems since it is LP, One with the galaxies where eei > 0.7, and B = 45,000 and another one with galaxies eei < 0.7 and B = 5000. Then combine your predictions.\n\nYou can have a look at PuLP or Scipy.optimize\n\nThe challenge here is that your predictions y determine your objective function F. So if those are wrong, your optimization would be wrong too. It'll be quite wrong because LP will assign that maximum possible ie 100 to the galaxies and 0 to others. This is a case of uncertain data, there are many research articles on this subject, you might want to look at stochastic optimization or robust LP. Nonetheless, I don't think the challenge requires something that advanced. You can think of a way to overcome this problem. This is my trick; use different models to predict y, then for each model's prediction of y, perform the optimization. Then you take the average. This means that, you are less likely to have just 0s and 100s in your opt energies (edited) ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Using ensemble predictions","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"x_test['y_pred'] = pred\nx_test['potential for increase'] = -np.log(x_test['y_pred'] + .01) + 3\ny = x_test['potential for increase']\ny.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#creating the new column indicating <0.7 and >= in the eei\nx_test['condition_1_for_true'] = np.where(x_test['existence expectancy index'] < 0.7, 1, 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* E is the optimisation variable.(Energy allocation)\n* Ei is the energy allocation per galaxy.\n* c is a constant.\n* b is the eei condition column\n* B is the total energy available","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# b = x_test[x_test['condition_1_for_true'] == 1]\nb = x_test['condition_1_for_true']\nc = (1/1000) * (y ** 2)\nc.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(b)\nplt.ylabel('freq')\nplt.xlabel('1 for galaxies with eei < 0.7')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"b.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pulp as p \n  \n# Create a LP Minimization problem \nprob = p.LpProblem('Problem', p.LpMaximize)  \n\n# Create problem Variables \nvariables = []\nn = range(0,np.size(b),1)\nfor i in n:\n    variables.append(i)\n\n# B = p.LpVariable(\"B\", lowBound = 0)  \nE = p.LpVariable.dicts(\"E\", variables, lowBound = 0, upBound = 100)\n\n# Objective Function \nprob += p.lpSum(p.lpDot(E.values(), c))\n# prob += np.sum(c * E.values())  #objective is to maximise the sum of the column.\n\n# Constraints:   \n# prob += B == 50000\nprob += p.lpSum(E.values()) <= 50000\nprob += p.lpSum(p.lpDot(E.values(), b.values)) == 5000\n\n# # Display the problem \nprint(prob) \n\nstatus = prob.solve()   # Solver \nprint(p.LpStatus[status])   # The solution status \n\n# Printing the final solution \nprint(p.value(B), p.value(prob.objective))  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pulp import value\n\ndef print_result(problem):\n    print('Optimization status:', problem.status)\n    print('Final value of the objective:', value(problem.objective))\n    print('Final values of the variables:')\n    for var in problem.variables():\n        global x\n        x = var, '=', value(var)\n        print(x)\n        \nprint_result(prob)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Using stacked pred","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"x_test['stacked_pred'] = list(stacked_pred)\nx_test['potential for increase stacked_pred'] = -np.log(x_test['stacked_pred'] + .01) + 3\ny = x_test['potential for increase stacked_pred']\nc = (1/1000) * (y ** 2)\n\nimport pulp as p \nfrom pulp import value\n\n  \n# Create a LP Minimization problem \nprob = p.LpProblem('Problem', p.LpMaximize)  \n\n# Create problem Variables \nvariables = []\nn = range(0,np.size(b),1)\nfor i in n:\n    variables.append(i)\n\n# B = p.LpVariable(\"B\", lowBound = 0)  \nE = p.LpVariable.dicts(\"E\", variables, lowBound = 0, upBound = 100)\n\n# Objective Function \nprob += p.lpSum(p.lpDot(E.values(), c))\n# prob += np.sum(c * E.values())  #objective is to maximise the sum of the column.\n\n# Constraints: \nl = [100] * 890\n    \n# prob += B == 50000\nprob += p.lpSum(E.values()) <= 50000\nprob += p.lpSum(p.lpDot(E.values(), b.values)) == 5000\n\n# # Display the problem \nprint(prob) \n\nstatus = prob.solve()   # Solver \n\ndef print_result(problem):\n    print('Optimization status:', problem.status)\n    print('Final value of the objective:', value(problem.objective))\n    print('Final values of the variables:')\n    for var in problem.variables():\n        global y\n        y = var, '=', value(var)\n        print(y)\n        \nprint_result(prob)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Using gdb","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"x_test['gdb_pred'] = list(gdb_pred)\nx_test['potential for increase gdb'] = -np.log(x_test['gdb_pred'] + .01) + 3\ny = x_test['potential for increase gdb']\nc = (1/1000) * (y ** 2)\n\nimport pulp as p \nfrom pulp import value\n\n  \n# Create a LP Minimization problem \nprob = p.LpProblem('Problem', p.LpMaximize)  \n\n# Create problem Variables \nvariables = []\nn = range(0,np.size(b),1)\nfor i in n:\n    variables.append(i)\n\n# B = p.LpVariable(\"B\", lowBound = 0)  \nE = p.LpVariable.dicts(\"E\", variables, lowBound = 0, upBound = 100)\n\n# Objective Function \nprob += p.lpSum(p.lpDot(E.values(), c))\n# prob += np.sum(c * E.values())  #objective is to maximise the sum of the column.\n\n# Constraints: \nl = [100] * 890\n    \n# prob += B == 50000\nprob += p.lpSum(E.values()) <= 50000\nprob += p.lpSum(p.lpDot(E.values(), b.values)) == 5000\n\n# # Display the problem \nprint(prob) \n\nstatus = prob.solve()   # Solver \n\ndef print_result(problem):\n    print('Optimization status:', problem.status)\n    print('Final value of the objective:', value(problem.objective))\n    print('Final values of the variables:')\n    for var in problem.variables():\n        global z\n        z = var, '=', value(var)\n        print(z)\n        \nprint_result(prob)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Using lgb","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"x_test['lgb_pred'] = list(lgb_pred)\nx_test['potential for increase lgb'] = -np.log(x_test['lgb_pred'] + .01) + 3\ny = x_test['potential for increase lgb']\nc = (1/1000) * (y ** 2)\n\nimport pulp as p \nfrom pulp import value\n\n  \n# Create a LP Minimization problem \nprob = p.LpProblem('Problem', p.LpMaximize)  \n\n# Create problem Variables \nvariables = []\nn = range(0,np.size(b),1)\nfor i in n:\n    variables.append(i)\n\n# B = p.LpVariable(\"B\", lowBound = 0)  \nE = p.LpVariable.dicts(\"E\", variables, lowBound = 0, upBound = 100)\n\n# Objective Function \nprob += p.lpSum(p.lpDot(E.values(), c))\n# prob += np.sum(c * E.values())  #objective is to maximise the sum of the column.\n\n# Constraints: \nl = [100] * 890\n    \n# prob += B == 50000\nprob += p.lpSum(E.values()) <= 50000\nprob += p.lpSum(p.lpDot(E.values(), b.values)) == 5000\n\n# # Display the problem \nprint(prob) \n\nstatus = prob.solve()   # Solver \n\ndef print_result(problem):\n    print('Optimization status:', problem.status)\n    print('Final value of the objective:', value(problem.objective))\n    print('Final values of the variables:')\n    for var in problem.variables():\n        global w\n        w = var, '=', value(var)\n        print(w)\n        \nprint_result(prob)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. ## Energy allocation option 2.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"x_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##two\nss=pd.read_csv('sub.csv')\n\n#The index represent the y_pred \nindex = ss['pred']\npot_inc = -np.log(index+0.01)+3\np2= pot_inc**2\nss[\"p2\"] = p2\nss['opt_pred'] = 0\nss['eei'] = x_test['existence expectancy index']\n\n#Sorting using Likelyincreasing index\nss=ss.sort_values('p2',ascending=False)\n#Droping The old index \nss=ss.reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ss.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n = 340\n#Giving the max of Energy to the 340 first element (ordered using the likely Increasing Index)\nss.opt_pred[:n]=100\nss.opt_pred[n:] = 0\nc=100\nalpha = 0.62685\nfor i in range(n,374):\n  if c>=alpha: \n    c=c-alpha\n    ss.loc[i,'opt_pred'] =c\nalpha=0.067345\nfor i in range(374,455):\n  if c>=alpha: \n    c=c-alpha\n    ss.loc[i,'opt_pred'] =c\n  else:\n    ss.loc[i,'opt_pred'] = 0\nalpha = 0.03\nfor i in range(455,465):\n  if c>=alpha: \n    c=c-alpha\n    ss.loc[i,'opt_pred'] =c\nalpha=0.4339465\nfor i in range(465,890):\n  if c>=alpha: \n    c=c-alpha\n    ss.loc[i,'opt_pred'] =c\n  else:\n    ss.loc[i,'opt_pred'] = 0\n\nprint(ss.opt_pred.sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking if the sum of opt_pred in rows having eei<0.7 is >5000\nprint(\"sum\",ss.opt_pred.sum())\nprint(\"left\", (50000-ss.opt_pred.sum())) \nprint(\"eei Sum\",ss[ss.eei<0.7]['opt_pred'].sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#We aren't so sure that allocating 100 Zillion DSML to the 340 first rows is optimal  \nss.opt_pred=0\nn = 340\nss.opt_pred[:n]=100\nss.opt_pred[n:] = 0\nc=100\n#This is a simple test we could add more steps or maybe changing these one\nfor i in range(n,890):\n  if ss.pred[i]-ss.pred[i-1]<4*10**(-6):\n    alpha=0.13\n  if ss.pred[i]-ss.pred[i-1]>=4*10**(-6) and ss.pred[i]-ss.pred[i-1]<4*10**(-5):\n    alpha=0.21222895\n  if ss.pred[i]-ss.pred[i-1]>=4*10**(-5) and ss.pred[i]-ss.pred[i-1]<10**(-4):\n    alpha=0.33\n  if ss.pred[i]-ss.pred[i-1]>=10**(-4) and ss.pred[i]-ss.pred[i-1]<5*10**(-3) :\n    alpha=0.48\n  if ss.pred[i]-ss.pred[i-1]>=5*10**(-3) and ss.pred[i]-ss.pred[i-1]<10**(-3):\n    alpha=0.58\n  if ss.pred[i]-ss.pred[i-1]>=10**(-3):\n    alpha=0.73\n  c-=alpha\n  if c-alpha>0:\n    ss.loc[i,'opt_pred'] =c\n  else:\n    ss.loc[i,'opt_pred'] =0\n\nprint(ss.opt_pred.sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking if the sum of opt_pred in rows having eei<0.7 is >5000\nprint(\"sum\",ss.opt_pred.sum())\nprint(\"left\", (50000-ss.opt_pred.sum()))\nprint(\"eei\",ss[ss.eei<0.7]['opt_pred'].sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Reordering the list using the real index\nss=ss.sort_values('index',ascending=True)\nss=ss.reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ss[['index', 'pred', 'opt_pred']].to_csv('f.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}