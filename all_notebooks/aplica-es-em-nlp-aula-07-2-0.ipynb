{"cells":[{"metadata":{},"cell_type":"markdown","source":"<h1 align=\"center\"> Aplicações em Processamento de Linguagem Natural </h1>\n<h2 align=\"center\"> Aula 07 - Extração de Informação (Parte 2)</h2>\n<h3 align=\"center\"> Prof. Fernando Vieira da Silva MSc.</h3>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<h2>1. Extração de Relacionamentos</h2>\n<p>A extração de relacionamentos consiste em identificar a ligação entre diversas entidades nomeadas no texto. Isso envolve mencionar qual é o tipo da ligação entre duas entidades. Considere o exemplo de sentença abaixo.</p>\n\n<p>\"Carlos Alberto de Nogueira é o morador mais antigo da Rua Praça da Alegria.\"</p>\n\n<p>Temos as entidades:</p>\n\n* Carlos Alberto de Nogueira (PESSOA)\n* Rua Praça da Alegria (LOCALIDADE)\n\n<p>Essas mesmas entidades estão relacionadas da seguinte forma:</p>\n\n[Carlos Alberto de Nogueira (PERSON); morador mais antigo; Rua Praça da Alegria (LOCALIDADE)]\n\n\n<p>Um dos mais famosos exemplos de sistema de reconhecimento é o [Never-Ending Language Learning (NELL)](http://rtw.ml.cmu.edu/), projeto desenvolvido pela Universidade Carnigie Mellon, com participação do Google e inclusive de pesquisadores brasileiros financiados pelo CNPq. Esse projeto consiste em extrair relacionamentos de milhões de páginas da internet, criando uma gigantesca base de conhecimento.</p>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<h2>2. Métodos para identificação de relacionamentos</h2>\n\n<p>Os métodos mais comuns para identificar relacionamentos entre entidades são:</p>\n\n* **Padrões codificados manualmente**: Basta criar padrões usando expressões regulares, por exemplo, para identificar que duas entidades se relacionam. Assim como em \"X mora em Y\" pode ser um padrão para identificar o relacionamento (X, mora_em, Y) entre uma entidade X do tipo PESSOA e uma entidade Y do tipo LOCALIDADE.\n* **Métodos bootstraping**: Com poucos dados, procura por ocorrências de duas entidades em que já se conhece o relacionamento (no Google, por exemplo), e usa os modelos encontrados como modelos para o mesmo relacionamento entre outras entidades.\n* **Métodos supervisionados**: Com base num corpus anotado com relacionamentos, criar modelos que 1) detecte quando existe o relacionamento entre duas entidades e 2) classifique o tipo de relacionamento entre elas. \n\n<p>Nesta aula, vamos ver um método supervisionado para classificar o relacionamento entre entidades, usando técnicas que já utilizamos em aulas anteriores.</p>\n\n<p>Para isso, utilizaremos alguns atributos mais comuns para o problema, como:</p>\n\n* Bag of Words/LSA\n* Flags indicadores dos tipos das entidades\n* Número de palavras entre as duas entidades\n* Flag indicando se o texto de uma entidade é composto pelo texto da outra\n* POS tags\n* etc\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<h2>3. Criando um Modelo Supervisionado</h2>\n<p> Vamos utilizar o corpus [Figure Eight: Medical Sentence Summary](https://www.kaggle.com/kmader/figure-eight-medical-sentence-summary), que possui diversas sentenças extraídas do PubMed, com entidades anotadas, assim como seus tipos de relacionamento.</p>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\n\ndf_train = pd.read_csv('../input/figure-eight-medical-sentence-summary/train.csv')\ndf_test = pd.read_csv('../input/figure-eight-medical-sentence-summary/test.csv')\n\ndf_train.head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['relation'].unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p>Transformamos as sentenças e tipos de relacionamento em matrizes numpy. Também binarizamos os rótulos dos relacionamentos, para utilizarmos no nosso classificador logo mais.</p>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\n\nx_train = df_train['sentence'].as_matrix()\ny_train = df_train['relation'].as_matrix()\n\nfrom sklearn.preprocessing import label_binarize\n\ny_train = label_binarize(y_train, classes=df_train['relation'].unique())\n\nprint(x_train[:10])\nprint(y_train[:10])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p>Como não temos os tipos das entidades, mas sabemos que se trata de nomes de medicamentos e doenças na maioria dos casos, não utilizaremos o tipo das entidades como atributos, mas utilizaremos os POS tags de todas as palavras entre as entidades. Vamos criar outras matrizes com esses atributos. </p>\n\n<p>Para os POS Tags, vamos fazer algo parecido ao chunking sugerido em https://courses.cs.washington.edu/courses/cse517/13wi/slides/cse517wi13-RelationExtraction.pdf, mas ao invés de usar chunking, vamos criar 3-grams desses POS tags para simplificar.</p>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train_sub_list = []\n\nfor i, row in df_train.iterrows():\n    pos_t1 = row['sentence'].find(row['term1'])\n    pos_t2 = row['sentence'].find(row['term2'])    \n    \n    if pos_t1 < pos_t2:\n        len_t1 = len(row['term1'])\n        x_train_sub_list.append(row['sentence'][pos_t1+len_t1:pos_t2])\n    else:\n        len_t2 = len(row['term2'])\n        x_train_sub_list.append(row['sentence'][pos_t2+len_t2:pos_t1])\n        \nx_train_sub = np.array(x_train_sub_list)\n\nprint(x_train_sub[:10])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p>Agora vamos definir duas funções de tokenização: uma para tokenizar bag-of-words e outra para tokenizar os POS tags</p>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk import pos_tag\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize\nimport string\nfrom nltk.corpus import wordnet\n\ndef my_tokenizer_pos(doc):\n    words = word_tokenize(doc)\n    \n    pos_tags = pos_tag(words)\n    \n    return [pos[1] for pos in pos_tags]\n\n# testando nossa função:\n\nfor x in x_train_sub[:10]:\n    print(my_tokenizer_pos(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stopwords_list = stopwords.words('english')\n\nlemmatizer = WordNetLemmatizer()\n\ndef my_tokenizer_bow(doc):\n    words = word_tokenize(doc)\n    \n    pos_tags = pos_tag(words)\n    \n    non_stopwords = [w for w in pos_tags if not w[0].lower() in stopwords_list]\n    \n    non_punctuation = [w for w in non_stopwords if not w[0] in string.punctuation]\n    \n    lemmas = []\n    for w in non_punctuation:\n        if w[1].startswith('J'):\n            pos = wordnet.ADJ\n        elif w[1].startswith('V'):\n            pos = wordnet.VERB\n        elif w[1].startswith('N'):\n            pos = wordnet.NOUN\n        elif w[1].startswith('R'):\n            pos = wordnet.ADV\n        else:\n            pos = wordnet.NOUN\n        \n        lemmas.append(lemmatizer.lemmatize(w[0], pos))\n\n    return lemmas","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p>Vamos reaproveitar a classe para seleção de atributos usando SVD.</p>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import TruncatedSVD\n\nclass SVDDimSelect(object):\n    def fit(self, X, y=None):        \n        try:\n            self.svd_transformer = TruncatedSVD(n_components=round(X.shape[1]/2))\n            self.svd_transformer.fit(X)\n        \n            cummulative_variance = 0.0\n            k = 0\n            for var in sorted(self.svd_transformer.explained_variance_ratio_)[::-1]:\n                cummulative_variance += var\n                if cummulative_variance >= 0.5:\n                    break\n                else:\n                    k += 1\n                \n            self.svd_transformer = TruncatedSVD(n_components=k)\n        except Exception as ex:\n            print(ex)\n            \n        return self.svd_transformer.fit(X)\n    \n    def transform(self, X, Y=None):\n        return self.svd_transformer.transform(X)\n        \n    def get_params(self, deep=True):\n        return {}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p>Agora vamos criar nosso Pipeline. Em resumo, vamos usar o TFIDF Vectorizer e o nosso POS Tagger em paralelo, e depois juntar os atributos para redimensionar usando o SVD.</p>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.pipeline import FeatureUnion\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nimport scipy\n\nclf = OneVsRestClassifier(LogisticRegression(random_state=0, solver='lbfgs', \n                                             multi_class='multinomial'))\n\n\nmy_pipeline = Pipeline([\n                        ('union', FeatureUnion([('bow', TfidfVectorizer(tokenizer=my_tokenizer_bow)),\\\n                                                ('pos', Pipeline([('pos-vect', CountVectorizer(tokenizer=my_tokenizer_pos)), \\\n                                                         ('pos-tfidf', TfidfTransformer())]))\n                                               ])),\\\n                       ('svd', SVDDimSelect()), \\\n                       ('clf', clf)])\n\npar = {'clf__estimator__C' : np.logspace(-4, 4, 20)}\n\nhyperpar_selector = RandomizedSearchCV(my_pipeline, par, cv=3, scoring='f1_weighted', n_jobs=1, n_iter=20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p>Agora vamos treinar os algoritmos</p>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(x_train_sub.shape)\nprint(y_train.shape)\n\nprint(x_train_sub[:10])\nprint(y_train[:10])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hyperpar_selector.fit(X=x_train_sub[:500], y=y_train[:500])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"x_test = df_test['sentence'].as_matrix()\ny_test = df_test['relation'].as_matrix()\n\ny_test = label_binarize(y_test, classes=df_train['relation'].unique())\n\nx_test_sub_list = []\n\nfor i, row in df_test.iterrows():\n    pos_t1 = row['sentence'].find(row['term1'])\n    len_t1 = len(row['term1'])\n    \n    pos_t2 = row['sentence'].find(row['term2'])    \n    \n    \n    if pos_t1 < pos_t2:\n        len_t1 = len(row['term1'])\n        x_train_sub_list.append(row['sentence'][pos_t1+len_t1:pos_t2])\n    else:\n        len_t2 = len(row['term2'])\n        x_train_sub_list.append(row['sentence'][pos_t2+len_t2:pos_t1])\n    \n\nx_test_sub = np.array(x_test_sub_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_predicted = hyperpar_selector.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report\n\nprint(classification_report(y_test, y_predicted, target_names=df_train['relation'].unique()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p>\n</p>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<p><b>Exercício 7:</b> Treine um modelo de extração de relacionamentos em Português, utilizando o corpus extraído do DBPedia e com relacionamentos entre pares de entidades anotadas.</p>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"f = open('../input/dbpedia-with-entity-relations-in-portuguese/DBpediaRelations-PT-0.2.txt', 'r')\n\nline = f.readline()\n\ndf_dict = {'sentence':[], 'term1':[], 'term2':[], 'relation':[]}\n\nwhile line:    \n    if len(line) > 1:\n        line_vals = line.split(':')\n        if len(line_vals) >= 2:                        \n            if line_vals[0].strip() == 'SENTENCE':\n                df_dict['sentence'].append(' '.join(line_vals[1:]))\n            elif line_vals[0].strip() == 'ENTITY1':\n                df_dict['term1'].append(' '.join(line_vals[1:]))\n            elif line_vals[0].strip() == 'ENTITY2':\n                df_dict['term2'].append(' '.join(line_vals[1:]))\n            elif line_vals[0].strip() == 'REL TYPE':\n                df_dict['relation'].append(' '.join(line_vals[1:]))\n    line = f.readline()\n    \nf.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\ndf = pd.DataFrame.from_dict(df_dict)\n\ndf.head(10)\n#df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!python -m spacy download pt_core_news_sm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pt_core_news_sm\n\n\nnlp = pt_core_news_sm.load()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\nfrom nltk.stem import RSLPStemmer\n\nstopwords_list = stopwords.words('portuguese')\n\nstemmer = nltk.stem.RSLPStemmer()\nstemmer.stem('gatinho')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train = df['sentence'].as_matrix()\ny_train = df['relation'].as_matrix()\n\n\n\nfrom sklearn.preprocessing import label_binarize\n\ny_train = label_binarize(y_train, classes=df_train['relation'].unique())\n\nprint(x_train[:10])\nprint(y_train[:10])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#len(x_train)\nX_train, X_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.33, random_state=42)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(X_train))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['relation'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train_sub_list = []\n\nfor i, row in df.iterrows():\n    pos_t1 = str(row['sentence']).find(str(row['term1']).replace('\\n', ''))\n    pos_t2 = str(row['sentence']).find(str(row['term2']).replace('\\n', ''))    \n    \n    if pos_t1 < pos_t2:\n        len_t1 = len(row['term1'])\n        \n        x_train_sub_list.append(row['sentence'][pos_t1+len_t1:pos_t2])\n    else:\n        len_t2 = len(row['term2'])\n        x_train_sub_list.append(row['sentence'][pos_t2+len_t2:pos_t1])\n\nprint(row['term1'])\nprint(row['term2'])\nx_train_sub = np.array(x_train_sub_list)\n\nprint(x_train_sub[-1:])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.tokenize import sent_tokenize\n\n\nvar = sent_tokenize(str(x_train_sub[-1:]), language='portuguese')\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"var","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}