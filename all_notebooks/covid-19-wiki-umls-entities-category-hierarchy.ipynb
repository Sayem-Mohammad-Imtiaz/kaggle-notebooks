{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install scispacy\n!pip install spacy","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import csv\nimport logging\nimport json\nimport sys\nimport requests\nimport spacy\nimport scispacy\nfrom oauthlib.common import urlencode\n\n# Author: Kolja Bailly\n\n# This script use the article metadata of the COVID-19 open reasearch dataset and extend it with additional information\n# - general topics of article abstracts\n# - key statements for every article abstract in the form subject-predicate-object\n# - Named Entities from article abstracts in a controlled vocabular (the unified medical language system UMLS)\n# - a hierarchical category tree for every abstract and statement from the UMLS Ontology\n#\n# The results are written into a extended metadata csv file and can optionally exported into a semantic mediawiki pageFile format\n# The goal is to use the extended information to provide a ordered category tree within a semantic wiki frontend with all articles and statements sorted into categories in a controlled vocabular\n# A wiki containing this data can be accessed via: http://www.hesinde.org\n\n# The extended metadata result csv file is included in the data section of this notebook. Feel free to use it for your own purposes.\n# The code doesnt run within kaggle currently due to a failure in installing scispacy library.\n# Run the code locally instead, don't forget to insert your IBM Watson API Key in class 'hesinde' below before running it \n\n#the following methods are defined below:\n# 1. readMetadata():\n# - read articles metadata row by row from the file metadata.csv until given limit is reached \n# - create a unique article nr and id, matching the requirements of mediawiki pages\n# - a blacklist of articles may given as parameter to avoid processing of already annotated articles\n#\n# 2. writeExtendedMetadata()\n# - write annotated article metadada to csv\n# - option to rewrite whole file or to append to existing file \n#\n# 3. readExtendedMetadata()\n# - read in extended metadata from file (for example to add articles to blacklist for a new processing run)\n#\n# 4. initWatsonData\n# - connect to IBM Cloud Natural Language Processing API \n# - retrieve general topics for each article abstract \n# - retrieve key statements from each article abtstract in the form subject-predicate-object\n#\n# 5. InitNLPData()\n# - Use SciSpacy Library for Named Entity Recognition (NER) of article abstracts\n# - Use SciSpacy to translate recognized enities into the Unified Medical Language System (UMLS) vocabular\n# - Save semantic parent trees for each article abstract and each watson statement using method 'getSemanticCategories'\n#\n# 6. getSemanticCategories()\n# - Use SciSpacy to get a subTree of the UMLS Semantic type tree (from root to entity node)\n#  \n# 7. sanitizeMediaWikiPageName()\n# - change given string to mach requirements of mediawiki pagenames\n#\n# 8. createWikiPageFiles()\n# - use extended metadata to create a mediawiki conform page file (to import extended metadata and categories as pages into a mediawiki instance)\n#\n# ----Class Hesinde----\n# - wrapper class for running the above methods\n# - create extended metadata\n# - create mediawiki pageFiles\n#\n\n# known bugs:\n# in wikiPageFiles, statements may contain invalid characters in the article link field relatedToArticle (doesnt meet the requirements of a mediawiki pageName).\n# these names have to be sanitized as well as the article page names.\n\n##################################\n# Read in article metadata File\n##################################\ndef readMetadata(articles, skipArticleIds, limit):\n    with open('metadata.csv') as csv_file:\n    #with open('testRoutine.csv') as csv_file:\n        csv_reader = csv.reader(csv_file, delimiter=',')\n        line_count = 0\n        articles_added = 0\n        firstContentLine = 1\n        maxArticles = limit\n        colNames = []\n        for row in csv_reader:\n            if (articles_added < maxArticles or maxArticles == -1):\n                if line_count < firstContentLine:\n                    print(f'header: = {row} ')\n                    colNames = row\n                else:\n\n                    #read article data\n                    actArticle = {}\n                    for i in range(len(row)):\n                        actArticle[colNames[i]] = row[i]\n                    # create page name for wiki\n                    if ( actArticle['sha']+\"-\"+actArticle['title'] in skipArticleIds):\n                        print(f\"skip rowNr {line_count} article title: {actArticle['title']}\")\n                        line_count += 1\n                        continue\n                    actArticle['nr'] = len(skipArticleIds) + articles_added + 1  # begin at 1\n                    actArticle['id'] = actArticle['nr'].__str__() + \"-\" + actArticle['title'].replace(\"[\",\"(\").replace(\"]\",\")\").replace(\"#\",\"\").replace(\"<\",\"\").replace(\">\",\"\").replace(\"|\",\"\").replace(\"{\",\"\").replace(\"}\",\"\").replace(\"_\",\"\")\n                    # check for skip Articles\n\n                    actArticle['semanticCategories'] = []\n                    if( actArticle['abstract']  ):\n                        articles.append( actArticle )\n                        articles_added += 1\n            line_count += 1\n        print(f'Processed {line_count} lines.')\n        print(f'Articles added {articles_added} .')\n\ndef writeExtendedMetadata(articles, semanticTreeNodes, append):\n    openType = 'w'\n    if( append ):\n        openType ='a'\n    with open('metadata_ext.csv', openType, newline='') as csvfile:\n        writer = csv.writer(csvfile, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n        if( not append ):\n            writer.writerow(articles[0])\n        for article in articles:\n            actRow = [\n                article[\"sha\"],\n                article[\"source_x\"],\n                article[\"title\"],\n                article[\"doi\"],\n                article[\"pmcid\"],\n                article[\"pubmed_id\"],\n                article[\"license\"],\n                article[\"abstract\"],\n                article[\"publish_time\"],\n                article[\"authors\"],\n                article[\"journal\"],\n                article[\"Microsoft Academic Paper ID\"],\n                article[\"WHO #Covidence\"],\n                article[\"has_full_text\"],\n                article[\"full_text_file\"],\n                article[\"nr\"],\n                article[\"id\"],\n                json.dumps(article[\"semanticCategories\"]),\n                json.dumps(article[\"watsonResult\"])\n            ]\n            writer.writerow( actRow )\n\ndef readExtendedMetadata(articles, semanticTreeNodes):\n\n    try:\n        with open('metadata_ext.csv') as csv_file:\n            csv.field_size_limit(sys.maxsize)\n            csv_reader = csv.reader(csv_file, delimiter=',')\n\n            line_count = 0\n            articles_added = 0\n            firstContentLine = 1\n            maxArticles = -1\n            colNames = []\n            for row in csv_reader:\n                if articles_added < maxArticles or maxArticles == -1:\n                    if line_count < firstContentLine:\n                        print(f'header: = {row} ')\n                        colNames = row\n                    else:\n                        actArticle = {}\n                        for i in range(len(row)):\n                            if( colNames[i] == \"semanticCategories\" ):\n                                arrayStr = row[i]\n                                arrayStr = arrayStr.replace(\"[\",\"\").replace(\"]\",\"\").replace(\"'\",\"\").replace('\"','')\n                                actArticle[colNames[i]] = arrayStr.split(\", \")\n                            elif( colNames[i] == \"watsonResult\"):\n                                #parse dictionary with json\n                                actArticle[colNames[i]] = json.loads( row[i] )\n                            else:\n                                actArticle[colNames[i]] = row[i]\n                        #print(actArticle['id'])\n                        # create page name for wiki\n                        articles.append(actArticle)\n                        articles_added += 1\n                line_count += 1\n            print(f'Processed {line_count} lines.')\n            print(f'Articles added {articles_added} .')\n    except IOError:\n        print(\"File not accessible\")\n\n\n##########################################\n# connect watson service\n##########################################\ndef initWatsonData( articles, apiKey):\n\n    url = 'https://api.eu-de.natural-language-understanding.watson.cloud.ibm.com/instances/b00318ad-c96a-4fea-96e5-94f27297002f/v1/analyze'\n    headers = { 'Content-Type': 'application/json' }\n    params = {'version': '2018-11-16'}\n    auth = ('apikey', apiKey)\n    articlesToDelete = []\n    i = 0\n    for article in articles:\n        # get watson data (/concepts)\n        data = { \"features\": {\n                \"semantic_roles\": {},\n                \"concepts\":{}\n                },\n                \"text\": article['abstract']\n            }\n\n        response = requests.post(url, headers=headers, auth=auth, json=data, params=params)\n        #print(response.request.body)\n        #print(response.request.headers)\n        if response.status_code == 200:\n            print(f'watson request {i+1}/{len(articles)} received')\n            #print(response.content)\n        else:\n            print(\"error in watson request\")\n            print(response.content)\n        #convert response to json\n        jsonResult = json.loads(response.content)\n\n        #append watson response data to article object\n        article['watsonResult'] = {}\n        if( 'concepts' in jsonResult and 'semantic_roles' in jsonResult ):\n            article['watsonResult']['concepts'] = jsonResult['concepts']\n            article['watsonResult'][''] = jsonResult['semantic_roles']\n        else:\n            articlesToDelete.append(article)\n\n        i += 1\n    # remove rticles with missing data\n    for toDelete in articlesToDelete:\n        articles.remove(toDelete)\n\n    # init a list of categroytreeNodes to save all the semantic categorys and their tree structure into\n    # key=categoryName values = List of parent Category Names\n    semanticCategoryTreeNodes = {}\n\n\n####################################\n#use sciSpacy to detect entities\n####################################\ndef getSemanticCategories( doc, linker, listToAddCategories, semanticCategoryTreeNodes):\n    for entity in doc.ents:  # go through all identified medical entities\n        for umls_ent in entity._.umls_ents:  # get all matched concepts in umls ontology (normally there is only one match)\n            umlsEntity = linker.umls.cui_to_entity[umls_ent[0]]\n            listToAddCategories.append(\n                umlsEntity.canonical_name)  # add umls entity name as category to article\n\n            # now add all semantic type categories of this entity as parent categories\n            entityCategory = linker.umls.semantic_type_tree.get_node_from_id(umlsEntity.types[0])\n            semanticCategoryTreeNodes[umlsEntity.canonical_name] = entityCategory.full_name\n\n            # now go up the semantic type tree and add all nodes to the category tree\n            semanticTreeNode = entityCategory\n            for i in range(semanticTreeNode.level):\n                semanticTreeNodeParent = linker.umls.semantic_type_tree.get_parent(semanticTreeNode)\n                # save node in category tree if not exist, set his parent as value\n                if (semanticTreeNode.full_name not in semanticCategoryTreeNodes):\n                    parentCatName = semanticTreeNodeParent.full_name\n                    if (parentCatName == \"UnknownType\"):\n                        parentCatName = \"UMLS\"\n                    semanticCategoryTreeNodes[semanticTreeNode.full_name] = parentCatName\n                # set act parent as base node and go up the tree\n                semanticTreeNode = semanticTreeNodeParent\n\ndef initNLPdata( articles, semanticCategoryTreeNodes):\n    print(\"load sciSpacy NLP Libraries...\")\n\n    from scispacy.umls_linking import UmlsEntityLinker\n\n    # load NLP model\n    nlp = spacy.load(\"en_core_sci_lg\")\n    # load linker for entity recognition in unified medical language system (UMLS)\n    linker = UmlsEntityLinker(resolve_abbreviations=True)\n    nlp.add_pipe(linker)\n\n    print(\"start NLP process...\")\n    # init category tree\n    for article in articles:\n        # do NLP on abstract\n        doc = nlp( article['abstract'] )\n        # add categories to artcile\n        getSemanticCategories(doc, linker, article['semanticCategories'], semanticCategoryTreeNodes)\n\n        # add categories to  (predicates obtained by watson) of article\n        if( 'watsonResult' in article ):\n            if( '' in article['watsonResult'] ):\n                for relation in article['watsonResult']['']:\n                    if ('sentence' in relation):\n                        doc2 = nlp( relation['sentence'] )\n                        relation['semanticCategories'] = []\n                        getSemanticCategories(doc2, linker, relation['semanticCategories'],semanticCategoryTreeNodes)\n\n    print( list(semanticCategoryTreeNodes) )\n    \n\ndef sanitizeMediaWikiPageName(pageName):\n    return pageName.replace(\"[\", \"(\").replace(\"]\", \")\").replace(\"#\", \"\").replace(\"<\",\"\").replace(\">\", \"\").replace(\"|\", \"\").replace(\"{\", \"\").replace(\"}\", \"\").replace(\"_\", \"\")\n\ndef createWikiPageFiles(path, articles, semanticCategoryTreeNodes):\n    categoryPath=\"Category/\"\n    statementPath=\"Statement/\"\n    # create Category Pages\n    baseCatName = \"Generated Categories\"\n    watsonCategoryName = \"Topics\"\n    umlsCategoryName = \"UMLS\"\n\n    F = open(path + categoryPath + baseCatName, \"w\")\n    F.close()\n\n    F = open(path + categoryPath + watsonCategoryName, \"w\")\n    F.write(\"[[Category:\"+baseCatName + \"]]\")\n    F.close()\n\n    F = open(path + categoryPath + umlsCategoryName, \"w\")\n    F.write(\"[[Category:\" + baseCatName + \"]]\")\n    F.close()\n\n    articleNr = 0\n    for article in articles:\n        articleNr += 1\n        if( articleNr % 100 == 0 ):\n            print( f\"{articleNr}/{len(articles)}({round((articleNr/len(articles)),1)}%)\" )\n        fileName = article['id']\n        for concept in article['watsonResult']['concepts']:\n            categoryName = \"Topic_\" + concept['text'].replace(\"/\",\"|\") # slash is forbidden in filenames\n            actContent = \"[[Category:\" + watsonCategoryName + \"]]\"  # parent category\n            F = open(path + categoryPath + categoryName, \"w\")\n            F.write(actContent)\n            F.close()\n\n        # add UMLS concepts as categories\n        for categoryName in list(semanticCategoryTreeNodes):\n            categoryName_sanitized = sanitizeMediaWikiPageName(categoryName)\n            actContent = \"[[Category:{}]]\".format(sanitizeMediaWikiPageName(semanticCategoryTreeNodes[categoryName]).replace('\"',''))  # add parent as category\n            F = open(path + categoryPath + categoryName_sanitized, \"w\")\n            F.write(actContent)\n            F.close()\n\n        articleConceptTemplate = \"\"\"{{{{Article\n                |unique_nr={}\n                |sha={}\n                |source={}\n                |title={}\n                |doi={}\n                |pmcid={}\n                |pubmed_id={}\n                |license={}\n                |abstract={}\n                |publish_time={}\n                |authors={}\n                |journal={}\n                |microsoftAcademicPaperId={}\n                |WHO_CovidenceNr={}\n                |hasFullText={}\n                |full_text_file={}\n                }}}}\n\n                {}\"\"\"\n        # add act article to categories\n        categories = \"\"\n        for categoryName in article['semanticCategories']:\n            categoryName = sanitizeMediaWikiPageName(categoryName).replace('\"', \"\")\n            #print(\"assign umls category:\" + categoryName + \" to article:\" + article['id'])\n            categories = categories + \"[[Category:\" + categoryName + \"]]\"\n        for concept in article['watsonResult']['concepts']:\n            categoryName = \"Topic_\" + concept['text']\n            #print(\"assign watson category:\" + categoryName + \" to article:\" + article['id'])\n            categories = categories + \"[[Category:\" + categoryName + \"]]\"\n\n        articleName = article['id'].replace(\"/\",\"|\") # slash is forbidden in filenames\n        if (len(articleName) > 255):\n            articleName = articleName[0:255]\n\n        articleContent = articleConceptTemplate.format(\n            article[\"nr\"],\n            article[\"sha\"],\n            article[\"source_x\"],\n            article[\"title\"],\n            article[\"doi\"],\n            article[\"pmcid\"],\n            article[\"pubmed_id\"],\n            article[\"license\"],\n            article[\"abstract\"],\n            article[\"publish_time\"],\n            article[\"authors\"],\n            article[\"journal\"],\n            article[\"Microsoft Academic Paper ID\"],\n            article[\"WHO #Covidence\"],\n            article[\"has_full_text\"],\n            article[\"full_text_file\"],\n            categories)\n\n        F = open(path + articleName, \"w\")\n        F.write(articleContent)\n        F.close()\n\n        # insert article result statements (Natural language understanding)\n        articleResultTemplate = \"\"\"{{{{Statement\n                    |StatementSubject={}\n                    |StatementPredicate={}\n                    |StatementObject={}\n                    |StatementType={}\n                    |StatementSourceText={}\n                    |StatementRelatedTo={}\n                    |publish_time={}\n                    }}}}\n\n                    {}\"\"\"\n\n        statementNr = 1\n        for relation in article['watsonResult']['']:\n            categories = \"\"\n            if ('subject' in relation and 'action' in relation and 'object' in relation):\n                if ('text' in relation['subject'] and 'normalized' in relation['action'] and 'text' in relation['object']):\n                    if ('semanticCategories' in relation):\n                        for categoryName in relation['semanticCategories']:\n                            categoryName = sanitizeMediaWikiPageName(categoryName)\n                            categories = categories + \"[[Category:\" + categoryName + \"]]\"\n                    # insert Natural Language Understanding (relation)triples as results of articles\n                    # articleResultStatementPageName = f\"Statement:Result{statementNr}ofArticleNr{article['nr']}\"\n                    articleResultStatementPageName = f\"{relation['subject']['text']}--{relation['action']['normalized']}--{relation['object']['text']}\"\n                    if (len(articleResultStatementPageName) > 235):\n                        articleResultStatementPageName = articleResultStatementPageName[0:235]\n                    articleResultStatementPageName = sanitizeMediaWikiPageName(\n                        articleResultStatementPageName + f\"...ArticleNr{article['nr']}\")\n                    articleResultStatementPageName = articleResultStatementPageName.replace(\"/\",\"|\") # slash is forbidden in filenames\n                    actContent = articleResultTemplate.format(\n                        relation['subject']['text'],\n                        relation['action']['normalized'],\n                        relation['object']['text'],\n                        \"NaturalLanguageUnderstandingResult\",\n                        relation['sentence'],\n                        article['id'],\n                        article['publish_time'],\n                        categories)\n                    F = open(path + statementPath + articleResultStatementPageName, \"w\")\n                    F.write(actContent)\n                    F.close()\n                    statementNr += 1\n                    \nclass Hesinde:\n    logging.basicConfig(level=logging.WARNING)\n    articles = []\n    semanticCategoryTreeNodes = {}\n\n    #parameter\n    rewriteAllCategories = False\n    path=\"wikiPages/\"\n    watsonApiKey=\"<YourWatsonAPIKey>\" # see comment below how to get an api key...\n    # 1) register at ibm cloud for free account: \n    # https://cloud.ibm.com/registration?target=%2Fcatalog%2Fservices%2Fnatural-language-understanding%3FhideTours%3Dtrue%26&cm_sp=WatsonPlatform-WatsonPlatform-_-OnPageNavCTA-IBMWatson_NaturalLanguageUnderstanding-_-Watson_Developer_Website\n    # 2) create lite(free) plan for natural language understanding\n    # 3) get your api key, see: https://cloud.ibm.com/docs/iam?topic=iam-userapikey\n    \n    def __init__(self):\n       # while(True):\n        self.createExtendedMetadata() # create first 2000 extended metadata\n        #self.articles=[]\n        #self.semanticCategoryTreeNodes = {}\n        #self.readExtendedMetadataAndinsertIntoMediaWiki()\n\n    def createExtendedMetadata(self):\n        skipExistingExtendedMetadataEntries = True #when reading data for metadata extension, shall existing articles skipped instead of rewritten?\n        appendOnExistingMetadata = True # when saving extended metadata, shall new lines appended on exiting file instead of replacing them?\n\n        skipArticleIds = []\n        skipArticles = []\n        if(skipExistingExtendedMetadataEntries):\n            readExtendedMetadata( skipArticles, {} )\n            for article in skipArticles:\n                skipArticleIds.append( article['sha']+\"-\"+article['title'] )\n        readMetadata(self.articles, skipArticleIds, 20)\n        initWatsonData(self.articles)\n        initNLPdata(self.articles, self.semanticCategoryTreeNodes)\n        writeExtendedMetadata(self.articles, self.semanticCategoryTreeNodes, appendOnExistingMetadata)\n\n    def readExtendedMetadataAndinsertIntoMediaWiki(self):\n        # fill object attributes with data from file\n        readExtendedMetadata(self.articles, self.semanticCategoryTreeNodes)\n        createWikiPageFiles(self.path, self.articles, self.semanticCategoryTreeNodes)\n        #insertDataIntoMediaWiki(self.articles, self.semanticCategoryTreeNodes, self.rewriteAllCategories)\n\n\n\n####### start script\nc = Hesinde()     ","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}