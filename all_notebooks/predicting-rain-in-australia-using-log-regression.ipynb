{"cells":[{"metadata":{},"cell_type":"markdown","source":"![](https://thumbs.gfycat.com/FeistyNaughtyFinch-size_restricted.gif)\n# Predicting rain in Australia\n\nThis week, we are working with the dataset to predict the possibility of raining the next day. Studies in 2006 showed that weather information had an average value of 109 dollars per US household each year. This equates to an overall economic benefit of over 12 billion dollars per annum. Being able to forecast and plan for the future when it comes to the local climate is a major advantage when it comes to planning tourism facilities. The transport sector can also benefit, as infrastructure can be set up to measure road surface conditions to improve traffic safety."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Importing required packages\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Creating a function to print \ndef overview():\n    data =pd.read_csv('../input/weather-dataset-rattle-package/weatherAUS.csv')\n    print(\"First 5 lines of data:\\n\")\n    print(data.head())\n    print(\"\\n\\n\\n\")\n    print(\"There are {} rows and {} columns\".format(data.shape[0], data.shape[1]))\n    print(\"\\n\\n\\n\")\n    print(\"Data types:\\n\")\n    print(data.dtypes)\n    print(\"\\n\\n\\n\")\n    print(\"% of missing values per column:\\n\")\n    print(data.isnull().mean().round(2)*100)\n    print(\"Statistical summary:\\n\")\n    print(data.describe())\n    return data\n    \ndata = overview()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- It seems like we have quite alot of data cleaning to do (dealing with NaN values).\n- For columns with less than 5% missing values, we will remove the affected rows.\n- For columns with more than 5% missing values, we will fill it with median (numerical data) and mode (categorical data).\n- As stated in the dataset description, we will drop the column call \"RISK_MM\" since it will affect our forcasting."},{"metadata":{"trusted":true},"cell_type":"code","source":"data.drop(columns = \"RISK_MM\", inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Looking into distribution for RainTomorrow"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(data = data, x = \"RainTomorrow\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Looking into distribution for RainToday"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(data = data, x = \"RainToday\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Looking into min temp and max temp"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(data['MinTemp'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(data['MaxTemp'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- We can see that the min temperature is clustered around 10 while max temperature is around 20. "},{"metadata":{},"cell_type":"markdown","source":"## Pairplot"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a function to separate out numerical and categorical data \n    ## Using this function to ensure that all non-numerical in a numerical column \n    ## and non-categorical in a categorical column is annotated\ndef cat_variable(df):\n    return list(df.select_dtypes(include = ['category', 'object']))\n    \ndef num_variable(df):\n    return list(df.select_dtypes(exclude = ['category', 'object']))\n\ncategorical_variable = cat_variable(data)\nnumerical_variable = num_variable(data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(data[numerical_variable], kind='scatter', diag_kind='hist', palette='Rainbow')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dealing with outliers\n\nq = data[numerical_variable].quantile(0.99)\ndata_new = data[data[numerical_variable]< q]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Re-adjusting categorical variables to numeric variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data.dropna(subset=['Rainfall', 'WindDir3pm', 'WindSpeed9am', 'WindSpeed3pm', 'Humidity9am', 'Humidity3pm', 'Temp9am','Temp3pm','RainToday'])\ncat_variable = ['WindGustDir', 'WindDir9am']\nnum_variable = ['MinTemp', 'MaxTemp', 'Evaporation', 'Sunshine', 'WindGustSpeed', 'Pressure9am', 'Pressure3pm', 'Cloud9am', 'Cloud3pm']\nimp_cat = SimpleImputer(missing_values = np.nan, strategy='most_frequent')\ndata[cat_variable] = imp_cat.fit_transform(data[cat_variable])\nimp_num = SimpleImputer(missing_values = np.nan, strategy='median')\ndata[num_variable] = imp_num.fit_transform(data[num_variable])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"le = LabelEncoder()\n \n# Implementing LE on WindGustDir\nle.fit(data.WindGustDir.drop_duplicates()) \ndata.WindGustDir = le.transform(data.WindGustDir)\n\n# Implementing LE on WindDir9am\nle.fit(data.WindDir9am.drop_duplicates()) \ndata.WindDir9am = le.transform(data.WindDir9am)\n\n# Implementing LE on WindDir3pm\nle.fit(data.WindDir3pm.drop_duplicates()) \ndata.WindDir3pm = le.transform(data.WindDir3pm)\n\n# Implementing LE on RainToday\nle.fit(data.RainToday.drop_duplicates()) \ndata.RainToday = le.transform(data.RainToday)\n\n# Implementing LE on RainTomorrow\nle.fit(data.RainTomorrow.drop_duplicates()) \ndata.RainTomorrow = le.transform(data.RainTomorrow)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,10))\n \ncorrMatrix = data.corr()\nsns.heatmap(corrMatrix, annot=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Implementing logistic regression "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Assigning X and y\nX = data.drop(['RainTomorrow', 'Date', 'Location'], axis=1)\n\ny = data['RainTomorrow']\n\n# Implementing train and test splits\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Looking into the shape of training and test dataset\nprint(X_train.shape)\nprint(X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# instantiate the model\nlogreg = LogisticRegression(solver='liblinear', random_state=0)\n\n\n# fit the model\nlogreg.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_test = logreg.predict(X_test)\nprint('Model accuracy score: {0:0.4f}'. format(accuracy_score(y_test, y_pred_test)))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}