{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport tensorflow as tf\n","execution_count":1,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"INPUT_COLUMNS = [\"body_acc_x_\",\"body_acc_y_\",\"body_acc_z_\",\"body_gyro_x_\",\"body_gyro_y_\",\"body_gyro_z_\",\"total_acc_x_\",\"total_acc_y_\",\n                 \"total_acc_z_\"]\nLABELS = [\"WALKING\", \"WALKING_UPSTAIRS\", \"WALKING_DOWNSTAIRS\", \"SITTING\", \"STANDING\", \"LAYING\"]\n","execution_count":2,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8891b192b7afb98e6f52692b1246f1c8ef7586a8"},"cell_type":"code","source":"print(os.listdir('../input/uci-har-dataset/uci har dataset/UCI HAR Dataset/train/'))","execution_count":3,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"c9ebdf7501606725618ce64e12dd53316056d4eb"},"cell_type":"code","source":"DATA_DIR='../input/uci-har-dataset/uci har dataset/UCI HAR Dataset/'\nTRAIN='train/Inertial Signals/'\nTEST='test/Inertial Signals/'\nX_TRAIN_PATHS=[DATA_DIR+TRAIN+col+'train.txt' for col in INPUT_COLUMNS]\nX_TEST_PATHS=[DATA_DIR+TEST+col+'test.txt' for col in INPUT_COLUMNS]\n","execution_count":4,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"71d2cb176c28a0bacf7635aa614100a8993882ab","collapsed":true},"cell_type":"code","source":"X_Train = []\n\nfor path in X_TRAIN_PATHS:\n    file = open(path, 'r')\n    X_Train.append([np.array(s, dtype=np.float32) for s in [row.replace('  ', ' ').strip().split(' ') for row in file]])\n    file.close()\n\nX_Train=np.transpose(np.array(X_Train), (1, 2, 0))\n\nX_Test = []\n\nfor path in X_TEST_PATHS:\n    file = open(path, 'r')\n    X_Test.append([np.array(s, dtype=np.float32) for s in [row.replace('  ', ' ').strip().split(' ') for row in file]])\n    file.close()\n\nX_Test=np.transpose(np.array(X_Test), (1, 2, 0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f4eed1cb828d0bad502f86f320b2d334c2bd13fb","collapsed":true},"cell_type":"code","source":"Y_TRAIN_PATH=DATA_DIR+'train/y_train.txt'\nY_TEST_PATH=DATA_DIR+'test/y_test.txt'\n\ny_Train=[]\nfile = open(Y_TRAIN_PATH, 'r')\ny_ = np.array([elem for elem in [row.replace('  ', ' ').strip().split(' ') for row in file]], dtype=np.int32)\nfile.close()\ny_Train= y_ - 1\n\ny_Test=[]\nfile = open(Y_TEST_PATH, 'r')\ny_ = np.array([elem for elem in [row.replace('  ', ' ').strip().split(' ') for row in file]], dtype=np.int32)\nfile.close()\ny_Test= y_ - 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1273afa88c7f1b98a78ab261dd336e7b00c03089","collapsed":true},"cell_type":"code","source":"TRAIN_LEN = len(X_Train)  \nTEST_LEN = len(X_Test)  \nNUM_STEPS = len(X_Train[0]) \nNUM_INPUT = len(X_Train[0][0]) \nNUM_HIDDEN = 32\nNUM_CLASSES = 6\nLR = 0.0025\nLAMBDA = 0.0020\nNUM_ITERS = TRAIN_LEN * 100  \nBATCH_SIZE = 1024\nDISP_ITER = 20000  \n\nprint(X_Test.shape, y_Test.shape, X_Train.shape, X_Train.shape)","execution_count":7,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"abb3c77ac6618b14bc78166119ad5d3f243de93e"},"cell_type":"code","source":"def LSTM_RNN(_X, _weights, _biases):\n    _X = tf.transpose(_X, [1, 0, 2])\n    _X = tf.reshape(_X, [-1, NUM_INPUT]) \n    _X = tf.nn.relu(tf.matmul(_X, _weights['hidden']) + _biases['hidden'])\n    _X = tf.split(_X, NUM_STEPS, 0) \n    lstm_cell_1 = tf.contrib.rnn.BasicLSTMCell(NUM_HIDDEN, forget_bias=1.0, state_is_tuple=True)\n    lstm_cell_2 = tf.contrib.rnn.BasicLSTMCell(NUM_HIDDEN, forget_bias=1.0, state_is_tuple=True)\n    lstm_cells = tf.contrib.rnn.MultiRNNCell([lstm_cell_1, lstm_cell_2], state_is_tuple=True)\n    outputs, states = tf.contrib.rnn.static_rnn(lstm_cells, _X, dtype=tf.float32)\n    lstm_last_output = outputs[-1]\n    return tf.matmul(lstm_last_output, _weights['out']) + _biases['out']\n\n\ndef next_batch(_train, step, batch_size):\n    shape = list(_train.shape)\n    shape[0] = batch_size\n    batch_s = np.empty(shape)\n\n    for i in range(batch_size):\n        index = ((step-1)*batch_size + i) % len(_train)\n        batch_s[i] = _train[index] \n\n    return batch_s\n\n\ndef one_hot(y_):\n    y_ = y_.reshape(len(y_))\n    n_values = int(np.max(y_)) + 1\n    return np.eye(n_values)[np.array(y_, dtype=np.int32)]","execution_count":8,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"55d917cbbab44bc12ec347eb1942394d0fe7015c","collapsed":true},"cell_type":"code","source":"x = tf.placeholder(tf.float32, [None, NUM_STEPS, NUM_INPUT])\ny = tf.placeholder(tf.float32, [None, NUM_CLASSES])\nweights = {'hidden': tf.Variable(tf.random_normal([NUM_INPUT, NUM_HIDDEN])),'out': tf.Variable(tf.random_normal([NUM_HIDDEN, NUM_CLASSES], mean=1.0))}\nbiases = {'hidden': tf.Variable(tf.random_normal([NUM_HIDDEN])),'out': tf.Variable(tf.random_normal([NUM_CLASSES]))}\n\npred = LSTM_RNN(x, weights, biases)\n\nl2 = LAMBDA * sum(tf.nn.l2_loss(tf_var) for tf_var in tf.trainable_variables())\ncost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=pred)) + l2\noptimizer = tf.train.AdamOptimizer(learning_rate=LR).minimize(cost)\n\ncorrect_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\naccuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))","execution_count":9,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0d99632bca697f3421446b0c9e77bae00ad6ac78","collapsed":true},"cell_type":"code","source":"test_losses = []\ntest_accuracies = []\ntrain_losses = []\ntrain_accuracies = []\n\n\nsess = tf.InteractiveSession(config=tf.ConfigProto(log_device_placement=True))\ninit = tf.global_variables_initializer()\nsess.run(init)\nsaver = tf.train.Saver()\n\nstep = 1\nwhile step * BATCH_SIZE <= NUM_ITERS:\n    batch_xs = next_batch(X_Train, step, BATCH_SIZE)\n    batch_ys = one_hot(next_batch(y_Train, step, BATCH_SIZE))\n\n    \n    _, loss, acc = sess.run([optimizer, cost, accuracy],feed_dict={x: batch_xs, y: batch_ys})\n    train_losses.append(loss)\n    train_accuracies.append(acc)\n    \n    \n    if (step*BATCH_SIZE % DISP_ITER == 0) or (step == 1) or (step * BATCH_SIZE > NUM_ITERS):\n        print(\"Iteration \" + str(step*BATCH_SIZE) +  \":Batch Loss = \" + \"{:.3f}\".format(loss) + \", Accuracy = {}\".format(acc))\n        loss, acc = sess.run([cost, accuracy], feed_dict={x: X_Test,y: one_hot(y_Test)})\n        test_losses.append(loss)\n        test_accuracies.append(acc)\n        print(\"Test: \" + \"Batch Loss = {}\".format(loss) + \", Accuracy = {}\".format(acc))\n    step += 1\n\n\n\n\n\none_hot_predictions, accuracy, final_loss = sess.run([pred, accuracy, cost],feed_dict={x: X_Test,y: one_hot(y_Test)})\n\ntest_losses.append(final_loss)\ntest_accuracies.append(accuracy)\n\nprint(\"RESULT: \" + \"Batch Loss = {}\".format(final_loss) + \", Accuracy = {}\".format(accuracy))\n\n","execution_count":10,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dfba42750b484214e66e8c665d10754969d2f9c0","collapsed":true},"cell_type":"code","source":"saver.save(sess, \"./model.ckpt\")\nprint(\"Model saved\")\n","execution_count":11,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"3776c30634a288dc612039fa22e66ce07958210c"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}