{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install ppscore","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn import preprocessing\nimport random\nimport math\nimport ppscore as pps\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Instructions\n1. We will be conducting the entire assignment through this notebook. You will be entering your code in the cells provided, and any explanation and details asked in markdown cells. \n2. You are free to add more code and markdown cells for describing your answer, but make sure they are below the question asked and not somewhere else. \n3. The notebook needs to be submitted on LMS. You can find the submission link [here](https://lms.iiitb.ac.in/moodle/mod/assign/view.php?id=13932). \n4. The deadline for submission is **5th October, 2020 11:59PM**."},{"metadata":{},"cell_type":"markdown","source":"# Data import\nThe data required for this assignment can be downloaded from the following [link](https://www.kaggle.com/dataset/e7cff1a2c6e29e18684fe6b077d3e4c42f9a7ae6199e01463378c60fe4b4c0cc), it's hosted on kaggle. Do check directory paths on your local system.  "},{"metadata":{},"cell_type":"markdown","source":"File absolute paths\n\n/kaggle/input/iiitb-ai511ml2020-assignment-1/Assignment/ML Assignment 1.pdf\n\n/kaggle/input/iiitb-ai511ml2020-assignment-1/Assignment/alcoholism/student-mat.csv\n\n/kaggle/input/iiitb-ai511ml2020-assignment-1/Assignment/accidents/accidents_2005_to_2007.csv\n\n/kaggle/input/iiitb-ai511ml2020-assignment-1/Assignment/accidents/accidents_2012_to_2014.csv\n\n/kaggle/input/iiitb-ai511ml2020-assignment-1/Assignment/accidents/accidents_2009_to_2011.csv\n\n/kaggle/input/iiitb-ai511ml2020-assignment-1/Assignment/fifa18/data.csv"},{"metadata":{"trusted":true},"cell_type":"code","source":"def import_csv(filename): \n    df = pd.read_csv(\"/kaggle/input/iiitb-ai511ml2020-assignment-1/Assignment\" + filename)\n    print(filename + ' loaded...')\n    print(filename + ' shape: ',df.shape)\n    return df\n\n#pass df.T for better view\ndef display_all(df) :\n    with pd.option_context(\"display.max_rows\", 1000, \"display.max_columns\", 20): \n        display(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"alcdata = import_csv(\"/alcoholism/student-mat.csv\")\nfifadata = import_csv(\"/fifa18/data.csv\")\naccidata1 = import_csv(\"/accidents/accidents_2005_to_2007.csv\")\naccidata2 = import_csv(\"/accidents/accidents_2009_to_2011.csv\")\naccidata3 = import_csv(\"/accidents/accidents_2012_to_2014.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Part - 1\n## Alcohol Consumption Data\nThe following data was obtained in a survey of students' math course in secondary school. It contains a lot of interesting social, gender and study information about students. \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#common space for alcohol data\n\n#G1 and G2 are weighted as 0.25 as they are period marks. G3 is given 0.5 weight as it represents final marks.\ndef addGrades(df):\n    df['Grades'] = 0.25 * df['G1'] + 0.25 * df['G2'] + 0.5 * df['G3']\n    return df\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1. Try to visualize correlations between various features and grades and see which features have a significant impact on grades. \nTry to engineer the three grade parameters (G1, G2 and G3) as one feature for such comparisons.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#enter code/answer in this cell. You can add more code/markdown cells below for your answer.\nalcdata = addGrades(alcdata)\nalc_corr_data = alcdata.corr()['Grades']\nplt.figure(figsize=(30,10))\nplt.xticks(rotation=90)\nplt.rc('xtick', labelsize=20) \nplt.rc('ytick', labelsize=20) \nsns.lineplot(data=alc_corr_data)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is not much correlation of features with G1, G2, G3 combination.\n\nLet's look at the ppscore of the feaures as well to see any non linear relationships and more patterns.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"predictors_df = pps.predictors(alcdata, y='Grades')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(35,8))\nplt.xticks(rotation=90)\nplt.rc('xtick', labelsize=15) \nplt.rc('ytick', labelsize=15) \nsns.barplot(data=predictors_df, x=\"x\", y=\"ppscore\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Not much action here as well. We will see in below cells how grades are affected due to famrel, Pstatus etc in more detail."},{"metadata":{},"cell_type":"markdown","source":"### 2. If there is a need for encoding some of the features,  how would you go  about it? \nWould you consider combining certain encodings together ?\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#enter code/answer in this cell. You can add more code/markdown cells below for your answer. \n\nplt.figure(figsize=(20,20))\nplt.rc('xtick', labelsize=10) \nplt.rc('ytick', labelsize=10) \nsns.heatmap(alcdata.select_dtypes(include=['int64','float64']).corr(), cmap=\"YlGnBu\",cbar_kws={\"aspect\": 40}, annot=True)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Can't seem to figure out which features to combine. ALL features look like they are not related to each other and looking for ways to combine them would be more time consuming than working on something else that might help. \n\nAlso combining different features will depend a lot on what we are looking to achieve from it, what we are going to predict, what relationship we want to explore or what business need we want to solve for our product.\n\nWe can combine the G1, G2, G3 as (0.25G1 + 0.25G2 + 0.5G3) for weighted average."},{"metadata":{},"cell_type":"markdown","source":"\n### 3. Try to find out how family relation(famrel) and parents cohabitation(Pstatus) affect grades of students. \n"},{"metadata":{},"cell_type":"markdown","source":"**(a) FAMREL VS GRADES**"},{"metadata":{},"cell_type":"markdown","source":"We will convert famrel and grades to same scale"},{"metadata":{"trusted":true},"cell_type":"code","source":"#enter code/answer in this cell. You can add more code/markdown cells below for your answer. \nplt.figure(figsize=(25,7))\nplt.xticks(rotation=0)\nplt.rc('xtick', labelsize=15) \nplt.rc('ytick', labelsize=15) \nalc_grades_fam = pd.DataFrame(alcdata[['Grades', 'famrel']])\n#convert grades to same range as family relation\nalc_grades_fam['Grades'] = ((alc_grades_fam['Grades'] - alc_grades_fam['Grades'].min()) * 4 )/20 + 1\nsns.lineplot(data=alc_grades_fam)\n#could have used scatter plot here but it would make no sense as it would be too messy. Still is","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Its all over the place"},{"metadata":{"trusted":true},"cell_type":"code","source":"#randomly see variation with 1/8th sample size\nrandom.seed(10)\nplt.figure(figsize=(25,7))\nplt.xticks(rotation=0)\nplt.rc('xtick', labelsize=15) \nplt.rc('ytick', labelsize=15) \nsns.scatterplot(data=(alc_grades_fam.sample(frac=1/8)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see mean and confidence of grades with each family relation"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Mean and confidence of grades with each family relation\nplt.figure(figsize=(15,4))\nplt.xticks(rotation=0)\nplt.rc('xtick', labelsize=15) \nplt.rc('ytick', labelsize=15) \nsns.lineplot(data=alcdata, x= 'famrel', y = 'Grades')\n#Passing the entire dataset in long-form mode will aggregate over repeated values (each famrel) to show the mean and 95% confidence interval","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"General trends show grades don't vary too much across famrel"},{"metadata":{},"cell_type":"markdown","source":"**(b) PSTATUS VS GRADES**"},{"metadata":{"trusted":true},"cell_type":"code","source":"alcdata_dummy = pd.get_dummies(alcdata)\nplt.figure(figsize=(15,4))\nplt.xticks(rotation=0)\nplt.rc('xtick', labelsize=15) \nplt.rc('ytick', labelsize=15) \nsns.lineplot(data=alcdata_dummy, x= 'Pstatus_T', y = 'Grades')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,4))\nplt.xticks(rotation=0)\nplt.rc('xtick', labelsize=15) \nplt.rc('ytick', labelsize=15) \nsns.lineplot(data=alcdata_dummy, x= 'Pstatus_A', y = 'Grades')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"General trends shows the grades tend to increase very slightly if parents are apart"},{"metadata":{},"cell_type":"markdown","source":"\n### 4. Figure out which features in the data are skewed, and propose a way to remove skew from all such columns. "},{"metadata":{},"cell_type":"markdown","source":"We will remove columns which are categorical in nature at their core and try measuring skew on remaining columns.\n\nWe will use pairplot to visualize the skew and also skew values."},{"metadata":{"trusted":true},"cell_type":"code","source":"#enter code/answer in this cell. You can add more code/markdown cells below for your answer. \nnum_alcdata = alcdata.select_dtypes(include=['int64','float64'])\ndropCol = ['famrel', 'failures', 'Medu', 'Fedu', 'traveltime', 'studytime', 'freetime', 'goout', 'Dalc', 'Walc', 'health']\nnum_alcdata = num_alcdata.drop(dropCol, axis = 1)\n\nprint(\"Skew of continuous columns:\\n\\n\", num_alcdata.skew())\n\n\nsns.pairplot(num_alcdata)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"alcdata['absences'].unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"absences column has skew which needs to be corrected. This method can be used for other features as well as the need is.\n\nRight skew can be removed using log, square root, cuberoot, boxcox etc.\nLeft skew can be removed using power function"},{"metadata":{"trusted":true},"cell_type":"code","source":"num_alcdata['absences'] = np.log2(alcdata['absences']+1)\nplt.figure(figsize=(15,4))\nplt.xticks(rotation=0)\nplt.rc('xtick', labelsize=15) \nplt.rc('ytick', labelsize=15) \nsns.distplot(num_alcdata['absences'], kde_kws={'bw':0.1})\nprint(num_alcdata['absences'].skew())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Part - 2\n## FIFA 2019  Data\n"},{"metadata":{},"cell_type":"markdown","source":"We will use clean_currency function to clean features which have M, K etc in them"},{"metadata":{"trusted":true},"cell_type":"code","source":"#fifadata common space\ndef clean_currency(df, feature):\n    ans = []\n    for e in df[feature]:\n        e = e.replace('€', '')\n        if 'K' in e:\n            e = float(e.replace('K', ''))*1000\n        elif 'M' in e:\n            e = float(e.replace('M',''))*1000000\n        ans.append(float(e))\n    df[feature] = ans\n    return df\n\ndisplay_all(fifadata.T)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1. Which clubs are the most economical? How did you decide that?"},{"metadata":{"trusted":true},"cell_type":"code","source":"#enter code/answer in this cell. You can add more code/markdown cells below for your answer. \n'''first = set()\nlast = set()\nfor e in fifadata['Wage']:\n    first.add(e[0])\n    last.add(e[-1])\nprint(first)         #The first position only has € which can be removed\nprint(last)          #The last position only has {'0', 'K'} which can be removed\n'''\ndf = fifadata.copy()\ndf = clean_currency(df, 'Wage')\ndf = clean_currency(df, 'Value')\n\nplt.figure(figsize=(4,4))\nplt.rc('xtick', labelsize=10) \nplt.rc('ytick', labelsize=10) \nsns.heatmap(df[['Wage','Value']].corr(), cmap=\"YlGnBu\",cbar_kws={\"aspect\": 40}, annot=True)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Wage and Value look correlated\n\nEconomical means giving good value or return in relation to the money.\n\nLets calculate mean Value money spent per potential points and Value money spent per potential points.\n\nThe club with least mean should indicate the most economical one."},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndf['ValuePerPotential'] = df['Value'] / df['Potential']\ndf['WagePerPotential'] = df['Wage'] / df['Potential']\n\nprint(\"Most economical club in terms of Value: \", df.groupby('Club').mean()['ValuePerPotential'].idxmin())\nprint(\"Most economical club in terms of Wage: \", df.groupby('Club').mean()['WagePerPotential'].idxmin())\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Bray Wanderers is most economical having least value per potential mean = 1353.438\n\nShakhtar Donetsk is most economical having least wage per potential mean = 12.888"},{"metadata":{},"cell_type":"markdown","source":"### 2. What is the relationship between age and individual potential of the player? How does age influence the players' value? At what age does the player exhibit peak pace ?"},{"metadata":{},"cell_type":"markdown","source":"**(a) Age v/s Potential**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#enter code/answer in this cell. You can add more code/markdown cells below for your answer. \nplt.figure(figsize=(15,4))\nplt.xticks(rotation=0)\nplt.rc('xtick', labelsize=15) \nplt.rc('ytick', labelsize=15) \nsns.boxplot(data=df, x= 'Age')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Age has some outliers so just keep these in mid while viewing relationship between Age an Potential"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,4))\nplt.xticks(rotation=0)\nplt.rc('xtick', labelsize=15) \nplt.rc('ytick', labelsize=15) \nsns.boxplot(data=df, x= 'Age', y = 'Potential')\n\nplt.figure(figsize=(15,4))\nplt.xticks(rotation=0)\nplt.rc('xtick', labelsize=15) \nplt.rc('ytick', labelsize=15) \nsns.lineplot(data=df, x= 'Age', y = 'Potential')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Not much variation is observed in potential due to age.\n\nAlso above two images show the relation between boxplot and lineplot beautifully."},{"metadata":{},"cell_type":"markdown","source":"**(b) Age v/s Value**"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,4))\nplt.xticks(rotation=0)\nplt.rc('xtick', labelsize=15) \nplt.rc('ytick', labelsize=15) \nsns.lineplot(data=df, x= 'Age', y = 'Value')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Players in their prime (24-31) get good Valuation \n\n\n**At what age does the player exhibit peak pace ?**"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,4))\nplt.xticks(rotation=0)\nplt.rc('xtick', labelsize=15) \nplt.rc('ytick', labelsize=15) \nsns.lineplot(data=df, x= 'Age', y = 'SprintSpeed')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Around 25 years of age peak pace is observed in most players"},{"metadata":{},"cell_type":"markdown","source":"### 3. What skill sets are helpful in deciding a player's potential? How do the traits contribute to the players' potential? "},{"metadata":{"trusted":true},"cell_type":"code","source":"#predict for cleaned values of Value\npredictors_df = pps.predictors(df, y='Potential')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#enter code/answer in this cell. You can add more code/markdown cells below for your answer. \nprint(predictors_df)\nplt.figure(figsize=(35,8))\nplt.xticks(rotation=90)\nplt.rc('xtick', labelsize=15) \nplt.rc('ytick', labelsize=15) \nsns.barplot(data=predictors_df, x=\"x\", y=\"ppscore\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Top skills which help in deciding player Potential are **ballcontrol** and **reactions**\n\nThe PPS is an asymmetric score that can detect linear or non-linear relationships between two columns irrespective of their type"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(predictors_df.iloc[8])\nprint(\"\\n\\n\",predictors_df.iloc[9])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4. Which features directly contribute to the wages of the players?"},{"metadata":{"trusted":true},"cell_type":"code","source":"#enter code/answer in this cell. You can add more code/markdown cells below for your answer. \npredictors_df = pps.predictors(df, y='Wage')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(predictors_df)\nplt.figure(figsize=(35,8))\nplt.xticks(rotation=90)\nplt.rc('xtick', labelsize=15) \nplt.rc('ytick', labelsize=15) \nsns.barplot(data=predictors_df, x=\"x\", y=\"ppscore\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Among the given features, Overall and Value seems to contribute the most to Wage of the players"},{"metadata":{"trusted":true},"cell_type":"code","source":"predictors_df.iloc[1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5. What is the age distribution in different clubs? Which club has most players young?"},{"metadata":{"trusted":true},"cell_type":"code","source":"#enter code/answer in this cell. You can add more code/markdown cells below for your answer.\n#only plotting distribution for 10 clubs\nplt.figure(figsize=(20,8))\nplt.xticks(rotation=0)\nplt.rc('xtick', labelsize=20) \nplt.rc('ytick', labelsize=20)\ncount = 10\nfor e in df[['Club', 'Age']].groupby(by='Club'):\n    if count == 0: \n        break\n    sns.distplot(e[1]['Age'], hist=False, label=e[0])\n    count = count - 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Summary of age distribution for all clubs\n#display_all(df[['Club', 'Age']].groupby(by='Club').describe())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Considering young means less than or equal to 22 years\ndisplay_all(df[['Age','Club']].loc[df['Age'] <= 22].groupby(by='Club').count().max())\ndisplay_all(df[['Age','Club']].loc[df['Age'] <= 22].groupby(by='Club').count().idxmax())\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ajax has maximum young players (<=22 years of age) = 21 players"},{"metadata":{},"cell_type":"markdown","source":"# Part - 3\n## UK Road Accidents Data\n\n\nThe UK government amassed traffic data from 2000 and 2016, recording over 1.6 million accidents in the process and making this one of the most comprehensive traffic data sets out there. It's a huge picture of a country undergoing change."},{"metadata":{"trusted":true},"cell_type":"code","source":"#common space for accident data\ndf.dtypes\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1. The very first step should be to merge all the 3 subsets of the data."},{"metadata":{"trusted":true},"cell_type":"code","source":"#enter code/answer in this cell. You can add more code/markdown cells below for your answer. \naccidata = pd.concat([accidata1,accidata2,accidata3])\ndf = accidata.copy()\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2. What are the number of casualties in each day of the week? Sort them in descending order. "},{"metadata":{"trusted":true},"cell_type":"code","source":"#enter code/answer in this cell. You can add more code/markdown cells below for your answer. \nplt.figure(figsize=(10,4))\nplt.xticks(rotation=0)\nplt.rc('xtick', labelsize=10) \nplt.rc('ytick', labelsize=10)\nplotdata = df[['Day_of_Week', 'Number_of_Casualties']].groupby('Day_of_Week').sum().sort_values(by='Number_of_Casualties', ascending = False).reset_index();\nsns.barplot(x= 'Day_of_Week', y = 'Number_of_Casualties', data=plotdata, order=plotdata['Day_of_Week'])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here only ordering and plotting is asked. We are not asked to find out which number corresponds to which day of the week like monday, tuesday etc.\n\nBut if it is needed, it can be easily calculated using date field of one of the row and matching it against the day_of_week value and assigning correct value of like monday, tuesday etc."},{"metadata":{},"cell_type":"markdown","source":"### 3. On each day of the week, what is the maximum and minimum speed limit on the roads the accidents happened?"},{"metadata":{"trusted":true},"cell_type":"code","source":"#enter code/answer in this cell. You can add more code/markdown cells below for your answer. \nprint('Min speed limits\\n', df[['Speed_limit','Day_of_Week']].groupby(by='Day_of_Week').min().reset_index())\nprint(\"\\n\\nMax speed limits\\n\", df[['Speed_limit','Day_of_Week']].groupby(by='Day_of_Week').max().reset_index())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4. What is the importance of Light and Weather conditions in predicting accident severity? What does your intuition say and what does the data portray?"},{"metadata":{},"cell_type":"markdown","source":"Let's do some data cleaning first"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Junction_Detail null values: \", df['Junction_Detail'].isna().sum())\n#print(\"\\nDuplicate index \\n\", df.groupby('Accident_Index').count())\nprint(\"\\n2nd_Road_Class with -1 values: \", df[df['2nd_Road_Class'] == -1]['2nd_Road_Class'].count())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Junction_Detail has all null values.\n\nAccident_Index is duplicate with different values. Better drop them."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,20))\nplt.rc('xtick', labelsize=10) \nplt.rc('ytick', labelsize=10) \nsns.heatmap(df.corr(), cmap=\"YlGnBu\",cbar_kws={\"aspect\": 40}, annot=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"High correlation is observed in \n\nLocation_Easting_OSGR : Longitude\n\nLocation_Northing_OSGR : Latitude\n\nPolice_Force : Local_Authority_(District)\n\nWe can keep one among these pairs. We will also create a new epoch feature using existing features."},{"metadata":{"trusted":true},"cell_type":"code","source":"df = accidata.copy()\n#feature engineering to create epoch timestamp\ndf['epoch'] = pd.to_datetime(\n    pd.to_datetime(df['Date']).apply(str).str[:10] \n    + ' ' \n    + pd.to_datetime(df['Time']).apply(str).str[11:]).astype('int64')//1e9\n\nto_drop = ['Junction_Detail', 'Accident_Index', 'Local_Authority_(District)',\n           'Location_Easting_OSGR', 'Location_Northing_OSGR', '2nd_Road_Class', 'Time', 'Date', 'Year' ]\ndf = df.drop(to_drop,1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I am trying some more feature engineering"},{"metadata":{"trusted":true},"cell_type":"code","source":"#enter code/answer in this cell. You can add more code/markdown cells below for your answer.\ndf_t = df.copy()\ncols = ['Weather_Conditions', 'Light_Conditions', 'Number_of_Casualties']\n\n#weather preferred\nweather_p= ['Raining without high winds', 'Fine without high winds','Snowing without high winds',\n                     'Fine with high winds' ]\n#weather not preferred\nweather_np = ['Raining with high winds', 'Fog or mist','Snowing with high winds']\n#weather other\nweather_o = ['Unknown', 'Other']\n\nlight_p = ['Daylight: Street light present', 'Darkness: Street lights present and lit', ]\nlight_np = ['Darkness: Street lights present but unlit', 'Darkeness: No street lighting','Darkness: Street lighting unknown']\n\ndf_t.loc[df_t['Weather_Conditions'].isin(weather_p), 'Weather_Conditions' ] = 'p'\ndf_t.loc[df_t['Weather_Conditions'].isin(weather_np), 'Weather_Conditions' ] = 'np'\ndf_t.loc[df_t['Weather_Conditions'].isin(weather_o), 'Weather_Conditions' ] = 'o'\ndf_t.loc[df_t['Weather_Conditions'].isna(), 'Weather_Conditions' ] = 'o'\ndf_t.loc[df_t['Light_Conditions'].isin(light_p), 'Light_Conditions' ] = 'p'\ndf_t.loc[df_t['Light_Conditions'].isin(light_np), 'Light_Conditions' ] = 'np'\n\npd.get_dummies(df_t[['Weather_Conditions', 'Light_Conditions', 'Accident_Severity']]).corr()['Accident_Severity']\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The data does not show correlation of Accident_Severity with weather and light conditions.\nI am aware correlation should not be calculated after encoding for categories.\n\nIntuition tells us that it should affect.\n\nBut accident severity has only 3 unique value (practically makes it a category column), this might be the reason it is not showing any correlation."},{"metadata":{},"cell_type":"markdown","source":"### 5. To predict the severity of the accidents which columns do you think are unnecessary and should be dropped before implementing a regression model. Support your statement using relevant plots and hypotheses derived from them."},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Already done above please check. \n\nThere is not much relation being observed in order to derive conclusions for my intuitions."},{"metadata":{},"cell_type":"markdown","source":"### 6. Implement a basic Logistic Regression Model using scikit learn with cross validation = 5, where you predict the severity of the accident (Accident_Severity). Note that here your goal is not to tune appropriate hyperparameters, but to figure out what features will be best to use."},{"metadata":{"trusted":true},"cell_type":"code","source":"#enter code/answer in this cell. You can add more code/markdown cells below for your answer. \nsns.pairplot(\n            df.select_dtypes(include = ['int64', 'float64'])\n             .dropna(subset = ['Longitude', 'Latitude'])\n             .sample(1000)[['Longitude', 'Latitude','Urban_or_Rural_Area','Accident_Severity']]\n            )\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Accident_Severity\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import preprocessing\n\ncols_to_use = ['Number_of_Casualties', 'Number_of_Vehicles', 'Longitude','Weather_Conditions_Raining with high winds',\n       'Weather_Conditions_Snowing with high winds']\n\ndef preprocess(df):\n    #most preprocessing is already done in above cells\n    df = df.dropna(subset = ['Longitude', 'Latitude'])\n    return df\n\ndef get_X_y(df):\n    X = preprocess(df)\n    \n    #To use classification for one vs rest\n    y = pd.get_dummies(X['Accident_Severity'].apply(str), prefix='Severity')\n    \n    to_drop = ['Local_Authority_(Highway)', 'Accident_Severity', 'LSOA_of_Accident_Location']\n    X = pd.get_dummies(X.drop(to_drop,1))\n    \n    return X, y\n    \nX, y = get_X_y(df)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scores = []\nfor i in range(1,4):\n    column_to_predict = 'Severity_' + str(i)\n    scores.append(cross_validate(\n        LogisticRegression(),\n        preprocessing.scale(X[cols_to_use]),\n        y[column_to_predict],\n        cv=5, scoring='f1',\n        return_train_score =True))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(len(scores)):\n    print('F1 score for Accident Severity ' + str(i+1) + ': ', scores[i][\"test_score\"].mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looking at the data it looks more likely that accident severity 1 is more severe than 3."},{"metadata":{},"cell_type":"markdown","source":"# Bonus Plot of various regions of accident severity"},{"metadata":{"trusted":true},"cell_type":"code","source":"severity1 = df[ df['Accident_Severity'] == 1]\nseverity2 = df[ df['Accident_Severity'] == 2]\nseverity3 = df[ df['Accident_Severity'] == 3]\n\nfig , (a1,a2,a3) = plt.subplots(1, 3, figsize=(15,8))\nx = 'Longitude'\ny = 'Latitude'\ns= .01\na=.3\n\nseverity1.plot(kind='scatter', x=x, y =y, color='red', s=s, alpha=a, subplots=True, ax=a1)\na1.set_title(\"Accident_Severity_1\")\na1.set_facecolor('white')\n\nseverity2.plot(kind='scatter', x=x,y =y, color='red', s=s, alpha=a, subplots=True, ax=a2)\na2.set_title(\"Accident_Severity_2\")\na2.set_facecolor('white')\n\nseverity3.plot(kind='scatter', x=x,y =y, color='red', s=s, alpha=a, subplots=True, ax=a3)\na3.set_title(\"Accident_Severity_3\")\na3.set_facecolor('white')\n\nplt.rc('xtick', labelsize=15) \nplt.rc('ytick', labelsize=15) \nfig.show()\n\n#Referrence : https://www.kaggle.com/yesterdog/","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(severity1['Number_of_Casualties'].sum()/severity1.shape[0])\nprint(severity2['Number_of_Casualties'].sum()/severity2.shape[0])\nprint(severity3['Number_of_Casualties'].sum()/severity3.shape[0])","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}