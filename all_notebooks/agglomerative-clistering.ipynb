{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df=pd.read_csv('../input/wholesale-customers-data-set/Wholesale customers data.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Not a single null value in data\ndf.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* There are multiple product categories – Fresh, Milk, Grocery, etc. The values represent the number of units purchased by each client for each product. Our aim is to make clusters from this data that can segment similar clients together. We will, of course, use Hierarchical Clustering for this problem."},{"metadata":{},"cell_type":"markdown","source":"* But before applying Hierarchical Clustering, we have to normalize the data so that the scale of each variable is the same. Why is this important? Well, if the scale of the variables is not the same, the model might become biased towards the variables with a higher magnitude like Fresh or Milk (refer to the above table)."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nsc=StandardScaler()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaled_data=sc.fit_transform(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import normalize\nnorm_data=normalize(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df=pd.DataFrame(scaled_data,columns=df.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1=pd.DataFrame(norm_data,columns=df.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Here, we can see that the scale of all the variables is almost similar. Now, we are good to go. Let’s first draw the dendrogram to help us decide the number of clusters for this particular problem:"},{"metadata":{"trusted":true},"cell_type":"code","source":"import scipy.cluster.hierarchy as shc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=[10,5])\nplt.title('Dindogram')\ndend=shc.dendrogram(shc.linkage(df,method='ward'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=[10,5])\nplt.title('Dindogram')\ndend=shc.dendrogram(shc.linkage(df1,method='ward'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=[10,5])\nplt.title('Dindogram')\ndend=shc.dendrogram(shc.linkage(df,method='complete'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=[10,5])\nplt.title('Dindogram')\ndendo=shc.dendrogram(shc.linkage(df,method='single'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=[10,5])\nplt.title('Dindogram')\ndendo=shc.dendrogram(shc.linkage(df,method='average'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=[10,5])\nplt.title('Dindogram')\ndendo=shc.dendrogram(shc.linkage(df,method='weighted'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=[10,5])\nplt.title('Dindogram')\ndendo=shc.dendrogram(shc.linkage(df,method='centroid'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* from above Method='Ward' gives beteer result than other methods.\n* The x-axis contains the samples and y-axis represents the distance between these samples. The vertical line with maximum distance is the blue line and hence we can decide a threshold of 6 and 30 and cut the dendrogram:"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=[10,5])\nplt.title('Dendogram')\ndendo=shc.dendrogram(shc.linkage(df,method='ward'))\nplt.axhline(y=30,color='r',linestyle='--')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=[10,5])\nplt.title('Dendogram')\ndendo=shc.dendrogram(shc.linkage(df,method='ward'))\nplt.axhline(y=6,color='r',linestyle='--')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* We have two clusters as this line cuts the dendrogram at two points. Let’s now apply hierarchical clustering for 2 clusters:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import AgglomerativeClustering\ncluster=AgglomerativeClustering(n_clusters=2,affinity='euclidean',linkage='ward')\ncluster.fit_predict(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import AgglomerativeClustering\ncluster=AgglomerativeClustering(n_clusters=2,affinity='euclidean',linkage='ward')\ncluster.fit_predict(df1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* We can see the values of 0s and 1s in the output since we defined 2 clusters. 0 represents the points that belong to the first cluster and 1 represents points in the second cluster. Let’s now visualize the two clusters:"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=[10,5])\nplt.scatter(df.Milk,df.Grocery,c=cluster.labels_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# graph for normalize data is good than scaled\nplt.figure(figsize=[10,5])\nplt.scatter(df1.Milk,df1.Grocery,c=cluster.labels_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Happy Learning!","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":1}