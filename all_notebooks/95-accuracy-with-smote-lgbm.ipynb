{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df=pd.read_csv('/kaggle/input/hmeq-data/hmeq.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isna().sum()\ntotalmiss=pd.DataFrame(data=[df.columns,df.isna().sum()/df.shape[0]])\ntotalmiss\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The data seems okay with no much missing features. "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.impute import SimpleImputer\noe=OrdinalEncoder()\nsi=SimpleImputer(strategy='most_frequent')\ndf=pd.DataFrame(si.fit_transform(df),columns=list(df))\ndf=pd.DataFrame(oe.fit_transform(df),columns=list(df))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The problems of categorical data and nan values are solved by now."},{"metadata":{},"cell_type":"markdown","source":"# Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.corr()['BAD'].sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The correlation did not entertain much though. There is still a way!"},{"metadata":{},"cell_type":"markdown","source":" # Preparation"},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train=df.drop(columns=['BAD'],axis=1)\ny_train=df['BAD']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"q=pd.DataFrame(data=[y_train.value_counts(),y_train.value_counts()/y_train.shape[0]])\nprint(q)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The data is enormously skewed. I will be using SMOTE to balance it."},{"metadata":{"trusted":true},"cell_type":"code","source":"from imblearn.over_sampling import SMOTE\nsmt=SMOTE()\nx_train,y_train=smt.fit_resample(x_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's check the ratio now."},{"metadata":{"trusted":true},"cell_type":"code","source":"q=pd.DataFrame(data=[y_train.value_counts(),y_train.value_counts()/y_train.shape[0]])\nprint(q)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The data seems balanced! Time for a little preprocessing!"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test=train_test_split(x_train,y_train,test_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## LogisticRegression"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score,f1_score,roc_auc_score,confusion_matrix\nlr=LogisticRegression()\nlr.fit(x_train,y_train)\ny_pred=lr.predict(x_test)\n\nprint('Accuracy Score :',accuracy_score(y_test,y_pred))\nprint('F1_Score :',f1_score(y_test,y_pred))\nprint('ROC_AUC_Score',roc_auc_score(y_test,y_pred))\nprint(confusion_matrix(y_test,y_pred))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"0.74 is not a good accuracy score !!!"},{"metadata":{},"cell_type":"markdown","source":"## XGBoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost\nxgb=xgboost.XGBClassifier(max_depth=10,n_estimators=100)\nxgb.fit(x_train,y_train)\ny_pred=xgb.predict(x_test)\n\n\nprint('Accuracy Score :',accuracy_score(y_test,y_pred))\nprint('F1_Score :',f1_score(y_test,y_pred))\nprint('ROC_AUC_Score',roc_auc_score(y_test,y_pred))\nprint(confusion_matrix(y_test,y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Wow! The accuracy seemed to have skyrocketed! We can trust this figure, as the f1_score also seems pretty good!"},{"metadata":{},"cell_type":"markdown","source":"Let's try LightGBM as a last method!"},{"metadata":{},"cell_type":"markdown","source":"## LightGBM\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import lightgbm\nlgbm=lightgbm.LGBMClassifier()\nlgbm.fit(x_train,y_train)\ny_pred=lgbm.predict(x_test)\n\nprint('Accuracy Score :',accuracy_score(y_test,y_pred))\nprint('F1_Score :',f1_score(y_test,y_pred))\nprint('ROC_AUC_Score',roc_auc_score(y_test,y_pred))\nprint(confusion_matrix(y_test,y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This gave a bit low score, but we can consider this amount of precision. \n"},{"metadata":{},"cell_type":"markdown","source":"## As a sidenote:\nYou can always try the problem without SMOTE and see the changes. In my experiment, the accuracy figures before applying SMOTE to the data were around 0.92 (LightGBM) with a very disappointing f1_score of 0.80 \n"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}