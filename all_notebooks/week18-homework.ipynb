{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-02T17:30:59.375541Z","iopub.execute_input":"2021-06-02T17:30:59.375894Z","iopub.status.idle":"2021-06-02T17:30:59.387263Z","shell.execute_reply.started":"2021-06-02T17:30:59.375865Z","shell.execute_reply":"2021-06-02T17:30:59.386213Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Read the dataset as a dataframe. Create a copy of your dataframe. Solve the rest of the questions using this dataframe copy.","metadata":{}},{"cell_type":"code","source":"d18 = pd.read_csv('../input/weather-dataset-rattle-package/weatherAUS.csv')\nd18","metadata":{"execution":{"iopub.status.busy":"2021-06-02T17:30:59.403532Z","iopub.execute_input":"2021-06-02T17:30:59.40407Z","iopub.status.idle":"2021-06-02T17:30:59.99334Z","shell.execute_reply.started":"2021-06-02T17:30:59.404033Z","shell.execute_reply":"2021-06-02T17:30:59.99226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"Find out the describe and info attributes of the dataframe. Analyze these information and create a short write-up according to your findings.","metadata":{}},{"cell_type":"code","source":"print(d18.info())\nd18.describe(include=\"all\")","metadata":{"execution":{"iopub.status.busy":"2021-06-02T17:30:59.995913Z","iopub.execute_input":"2021-06-02T17:30:59.99638Z","iopub.status.idle":"2021-06-02T17:31:00.584507Z","shell.execute_reply.started":"2021-06-02T17:30:59.996333Z","shell.execute_reply":"2021-06-02T17:31:00.583293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Find out the shape and size info of the dataset.","metadata":{}},{"cell_type":"code","source":"print('The shape: ',d18.shape)\nprint('The size: ', d18.size)","metadata":{"execution":{"iopub.status.busy":"2021-06-02T17:31:00.586053Z","iopub.execute_input":"2021-06-02T17:31:00.586477Z","iopub.status.idle":"2021-06-02T17:31:00.592601Z","shell.execute_reply.started":"2021-06-02T17:31:00.586433Z","shell.execute_reply":"2021-06-02T17:31:00.591588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Find out the types values of the columns and save the result as a dataframe.","metadata":{}},{"cell_type":"code","source":"dtypes_d18 = pd.DataFrame(d18.dtypes,columns=['dtypes'])\ndtypes_d18","metadata":{"execution":{"iopub.status.busy":"2021-06-02T17:31:00.594186Z","iopub.execute_input":"2021-06-02T17:31:00.594505Z","iopub.status.idle":"2021-06-02T17:31:00.615631Z","shell.execute_reply.started":"2021-06-02T17:31:00.594477Z","shell.execute_reply":"2021-06-02T17:31:00.614296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Find out the non-null counts of the columns and save the result as a dataframe.","metadata":{}},{"cell_type":"code","source":"nonnull_d18 = pd.DataFrame(d18.notnull().sum())\n\nnonnull_d18","metadata":{"execution":{"iopub.status.busy":"2021-06-02T17:31:00.618928Z","iopub.execute_input":"2021-06-02T17:31:00.619548Z","iopub.status.idle":"2021-06-02T17:31:00.732365Z","shell.execute_reply.started":"2021-06-02T17:31:00.619508Z","shell.execute_reply":"2021-06-02T17:31:00.731239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Find out the null counts of the columns and save the result as a dataframe.","metadata":{}},{"cell_type":"code","source":"null_d18 = pd.DataFrame(d18.isnull().sum())\nnull_d18","metadata":{"execution":{"iopub.status.busy":"2021-06-02T17:31:00.734497Z","iopub.execute_input":"2021-06-02T17:31:00.734928Z","iopub.status.idle":"2021-06-02T17:31:00.841049Z","shell.execute_reply.started":"2021-06-02T17:31:00.734885Z","shell.execute_reply":"2021-06-02T17:31:00.840251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Find out the unique counts of the columns and save the result as a dataframe.","metadata":{}},{"cell_type":"code","source":"nunique_d18 = pd.DataFrame(d18.nunique())\nnunique_d18","metadata":{"execution":{"iopub.status.busy":"2021-06-02T17:31:00.842313Z","iopub.execute_input":"2021-06-02T17:31:00.842808Z","iopub.status.idle":"2021-06-02T17:31:01.059751Z","shell.execute_reply.started":"2021-06-02T17:31:00.842776Z","shell.execute_reply":"2021-06-02T17:31:01.058969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Merge the dataframes you created in questions 4-5-6-7. Expected output:","metadata":{}},{"cell_type":"code","source":"merge18 = pd.concat([dtypes_d18,null_d18,nonnull_d18, nunique_d18], axis=1)\nmerge18","metadata":{"execution":{"iopub.status.busy":"2021-06-02T17:31:01.060989Z","iopub.execute_input":"2021-06-02T17:31:01.061484Z","iopub.status.idle":"2021-06-02T17:31:01.076371Z","shell.execute_reply.started":"2021-06-02T17:31:01.061452Z","shell.execute_reply":"2021-06-02T17:31:01.075422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lowercase all column names.","metadata":{}},{"cell_type":"code","source":"d18.columns = list(map(lambda x: x.lower(), d18.columns))\nd18.sample()","metadata":{"execution":{"iopub.status.busy":"2021-06-02T17:31:01.07764Z","iopub.execute_input":"2021-06-02T17:31:01.078227Z","iopub.status.idle":"2021-06-02T17:31:01.120463Z","shell.execute_reply.started":"2021-06-02T17:31:01.078192Z","shell.execute_reply":"2021-06-02T17:31:01.11956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Change all the No values to NoRain and all the Yes values to Rain in raintoday and raintomorrow columns.","metadata":{}},{"cell_type":"code","source":"d18.raintoday.replace('no', 'noRain', inplace=True)\nd18.raintomorrow.replace('no', 'noRain', inplace=True)\n\nd18","metadata":{"execution":{"iopub.status.busy":"2021-06-02T17:31:01.121862Z","iopub.execute_input":"2021-06-02T17:31:01.122251Z","iopub.status.idle":"2021-06-02T17:31:01.245236Z","shell.execute_reply.started":"2021-06-02T17:31:01.122218Z","shell.execute_reply":"2021-06-02T17:31:01.244227Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Change the data type of \"date\" (object) column to datetime64 and reformat the date as DD/MM/YYYY.","metadata":{}},{"cell_type":"code","source":"d18.date = pd.to_datetime(d18.date)\nd18.date = d18.date.dt.strftime('%d/%m/%Y')\nd18.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-02T17:31:01.247033Z","iopub.execute_input":"2021-06-02T17:31:01.247476Z","iopub.status.idle":"2021-06-02T17:31:02.45486Z","shell.execute_reply.started":"2021-06-02T17:31:01.247432Z","shell.execute_reply":"2021-06-02T17:31:02.453633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Create a new column called \"difference\", calculate the difference between maxtemp and mintemp columns for each row, and store the value in this new column.","metadata":{}},{"cell_type":"code","source":"d18['difference'] = d18['maxtemp'] - d18['mintemp']\nd18['difference']","metadata":{"execution":{"iopub.status.busy":"2021-06-02T17:31:02.456582Z","iopub.execute_input":"2021-06-02T17:31:02.457021Z","iopub.status.idle":"2021-06-02T17:31:02.47034Z","shell.execute_reply.started":"2021-06-02T17:31:02.456983Z","shell.execute_reply":"2021-06-02T17:31:02.468797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Remove the evaporation and sunshine columns from the dataset permanently.","metadata":{}},{"cell_type":"code","source":"#d18.drop(['evaporation','sunshine'], axis=1 ,inplace=True )\nd18.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-02T17:31:02.472011Z","iopub.execute_input":"2021-06-02T17:31:02.472534Z","iopub.status.idle":"2021-06-02T17:31:02.520694Z","shell.execute_reply.started":"2021-06-02T17:31:02.472495Z","shell.execute_reply":"2021-06-02T17:31:02.519422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Find out the most rainy day for each city.","metadata":{}},{"cell_type":"code","source":"d18.groupby('location')['rainfall','date'].max()","metadata":{"execution":{"iopub.status.busy":"2021-06-02T17:31:02.522364Z","iopub.execute_input":"2021-06-02T17:31:02.522801Z","iopub.status.idle":"2021-06-02T17:31:02.623632Z","shell.execute_reply.started":"2021-06-02T17:31:02.522752Z","shell.execute_reply":"2021-06-02T17:31:02.622777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Filter out all the data for the city 'Albury' and then sort according to maxtemp column.","metadata":{}},{"cell_type":"code","source":"filter_d18 = d18[d18['location']=='Albury']\nfilter_d18.sort_values(by=['maxtemp'])","metadata":{"execution":{"iopub.status.busy":"2021-06-02T17:31:02.624849Z","iopub.execute_input":"2021-06-02T17:31:02.625118Z","iopub.status.idle":"2021-06-02T17:31:02.706261Z","shell.execute_reply.started":"2021-06-02T17:31:02.625091Z","shell.execute_reply":"2021-06-02T17:31:02.705252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Find out the NaN counts for each column.","metadata":{}},{"cell_type":"code","source":"NaN_d18 = d18.isnull().sum()\nNaN_d18","metadata":{"execution":{"iopub.status.busy":"2021-06-02T17:31:02.707557Z","iopub.execute_input":"2021-06-02T17:31:02.707857Z","iopub.status.idle":"2021-06-02T17:31:02.808563Z","shell.execute_reply.started":"2021-06-02T17:31:02.707828Z","shell.execute_reply":"2021-06-02T17:31:02.807512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Remove the rows with NaN values in \"windgustdir\" column from the dataframe permanently.","metadata":{}},{"cell_type":"code","source":"d18.dropna(subset=['windgustdir'], axis=0, inplace=True)\nd18","metadata":{"execution":{"iopub.status.busy":"2021-06-02T17:31:15.855451Z","iopub.execute_input":"2021-06-02T17:31:15.855834Z","iopub.status.idle":"2021-06-02T17:31:16.001264Z","shell.execute_reply.started":"2021-06-02T17:31:15.855803Z","shell.execute_reply":"2021-06-02T17:31:16.000347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Create a new dataframe, use \"Location\" column as the index of the dataframe, display the min, max, and median values of \"evaporation\" and \"sunshine\" columns in this dataframe.","metadata":{}},{"cell_type":"code","source":"d18a = d18.copy()\nd18_min = d18a.groupby('location')[['evaporation', 'sunshine']].min()\nd18_min","metadata":{"execution":{"iopub.status.busy":"2021-06-02T17:33:36.051747Z","iopub.execute_input":"2021-06-02T17:33:36.052155Z","iopub.status.idle":"2021-06-02T17:33:36.112408Z","shell.execute_reply.started":"2021-06-02T17:33:36.052107Z","shell.execute_reply":"2021-06-02T17:33:36.111334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nd18_max = d18a.groupby('location')[['evaporation', 'sunshine']].max()\nd18_max","metadata":{"execution":{"iopub.status.busy":"2021-06-02T17:34:49.366896Z","iopub.execute_input":"2021-06-02T17:34:49.367265Z","iopub.status.idle":"2021-06-02T17:34:49.4034Z","shell.execute_reply.started":"2021-06-02T17:34:49.367235Z","shell.execute_reply":"2021-06-02T17:34:49.402358Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"d18_median = d18a.groupby('location')[['evaporation', 'sunshine']].median()\nd18_median","metadata":{"execution":{"iopub.status.busy":"2021-06-02T17:35:29.966425Z","iopub.execute_input":"2021-06-02T17:35:29.966911Z","iopub.status.idle":"2021-06-02T17:35:30.009492Z","shell.execute_reply.started":"2021-06-02T17:35:29.966868Z","shell.execute_reply":"2021-06-02T17:35:30.008262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Find out the hottest day of \"Perth\". Example output: Timestamp('2015-01-05 00:00:00')","metadata":{}},{"cell_type":"code","source":"result=d18[d18.location=='Perth'][['location','date','maxtemp']].sort_values(by=['maxtemp'], ascending=False)\nresult.iloc[0]","metadata":{"execution":{"iopub.status.busy":"2021-06-02T17:35:37.493565Z","iopub.execute_input":"2021-06-02T17:35:37.493925Z","iopub.status.idle":"2021-06-02T17:35:37.524485Z","shell.execute_reply.started":"2021-06-02T17:35:37.493896Z","shell.execute_reply":"2021-06-02T17:35:37.522965Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Group your dataframe by location and find out the averages of all numeric values.","metadata":{}},{"cell_type":"code","source":"result1 = d18.groupby('location').mean()\nresult1","metadata":{"execution":{"iopub.status.busy":"2021-06-02T17:35:40.496171Z","iopub.execute_input":"2021-06-02T17:35:40.496527Z","iopub.status.idle":"2021-06-02T17:35:40.597058Z","shell.execute_reply.started":"2021-06-02T17:35:40.496498Z","shell.execute_reply":"2021-06-02T17:35:40.596004Z"},"trusted":true},"execution_count":null,"outputs":[]}]}