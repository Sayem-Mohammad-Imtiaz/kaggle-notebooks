{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Setup","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport re\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, confusion_matrix, roc_auc_score","metadata":{"execution":{"iopub.status.busy":"2021-08-06T08:23:41.894291Z","iopub.execute_input":"2021-08-06T08:23:41.89461Z","iopub.status.idle":"2021-08-06T08:23:41.898967Z","shell.execute_reply.started":"2021-08-06T08:23:41.89458Z","shell.execute_reply":"2021-08-06T08:23:41.898136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.models import Model, Sequential\nfrom keras.optimizers import Adam\nfrom keras.utils.vis_utils import plot_model\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences","metadata":{"execution":{"iopub.status.busy":"2021-08-06T08:44:50.909584Z","iopub.execute_input":"2021-08-06T08:44:50.909959Z","iopub.status.idle":"2021-08-06T08:44:50.916525Z","shell.execute_reply.started":"2021-08-06T08:44:50.909927Z","shell.execute_reply":"2021-08-06T08:44:50.915622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load Dataset\nTrain / Validation / Test = 7 / 1 / 2","metadata":{}},{"cell_type":"code","source":"df_train = pd.read_csv('../input/imdb-dataset/train.csv', usecols = ['review','sentiment'])\ndf_val = pd.read_csv('../input/imdb-dataset/val.csv', usecols = ['review','sentiment'])\ndf_test = pd.read_csv('../input/imdb-dataset/test.csv', usecols = ['review','sentiment'])","metadata":{"execution":{"iopub.status.busy":"2021-08-06T09:40:15.459291Z","iopub.execute_input":"2021-08-06T09:40:15.459616Z","iopub.status.idle":"2021-08-06T09:40:16.034423Z","shell.execute_reply.started":"2021-08-06T09:40:15.459582Z","shell.execute_reply":"2021-08-06T09:40:16.033593Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df_train.info())\ndf_train","metadata":{"execution":{"iopub.status.busy":"2021-08-06T09:31:50.471416Z","iopub.execute_input":"2021-08-06T09:31:50.471768Z","iopub.status.idle":"2021-08-06T09:31:50.502947Z","shell.execute_reply.started":"2021-08-06T09:31:50.471735Z","shell.execute_reply":"2021-08-06T09:31:50.501963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df_val.info())\ndf_val","metadata":{"execution":{"iopub.status.busy":"2021-08-06T09:31:54.800476Z","iopub.execute_input":"2021-08-06T09:31:54.800835Z","iopub.status.idle":"2021-08-06T09:31:54.822015Z","shell.execute_reply.started":"2021-08-06T09:31:54.800804Z","shell.execute_reply":"2021-08-06T09:31:54.82096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df_test.info())\ndf_test","metadata":{"execution":{"iopub.status.busy":"2021-08-06T09:32:04.359273Z","iopub.execute_input":"2021-08-06T09:32:04.359605Z","iopub.status.idle":"2021-08-06T09:32:04.388958Z","shell.execute_reply.started":"2021-08-06T09:32:04.359574Z","shell.execute_reply":"2021-08-06T09:32:04.387991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Metrics","metadata":{}},{"cell_type":"code","source":"def get_metrics(y_test, y_pred_proba):\n    print('ACCURACY_SCORE: ', round(accuracy_score(y_test, y_pred_proba >= 0.5), 4))\n    print('F1_SCORE: ', round(f1_score(y_test, y_pred_proba >= 0.5, average = \"macro\"), 4))\n    print('ROC_AUC_SCORE: ', round(roc_auc_score(y_test, y_pred_proba), 4))\n    print('CONFUSION_MATRIX:\\n', confusion_matrix(y_test, y_pred_proba >= 0.5),'\\n')","metadata":{"execution":{"iopub.status.busy":"2021-08-06T07:31:20.516091Z","iopub.execute_input":"2021-08-06T07:31:20.516556Z","iopub.status.idle":"2021-08-06T07:31:20.524442Z","shell.execute_reply.started":"2021-08-06T07:31:20.516515Z","shell.execute_reply":"2021-08-06T07:31:20.523235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Cleaning\nRef: https://www.kaggle.com/colearninglounge/nlp-data-preprocessing-and-cleaning\n\nRaw text gives better results than preprocessed text","metadata":{}},{"cell_type":"code","source":"#Removes Punctuations\ndef remove_punctuations(data):\n    punct_tag=re.compile(r'[^\\w\\s]')\n    data=punct_tag.sub(r'',data)\n    return data\n\n#Removes HTML syntaxes\ndef remove_html(data):\n    html_tag=re.compile(r'<.*?>')\n    data=html_tag.sub(r'',data)\n    return data\n\n#Removes URL data\ndef remove_url(data):\n    url_clean= re.compile(r\"https://\\S+|www\\.\\S+\")\n    data=url_clean.sub(r'',data)\n    return data\n\n#Removes Emojis\ndef remove_emoji(data):\n    emoji_clean= re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    data=emoji_clean.sub(r'',data)\n    url_clean= re.compile(r\"https://\\S+|www\\.\\S+\")\n    data=url_clean.sub(r'',data)\n    return data\n\ndf_train['review'] = df_train['review'].apply(lambda z: remove_punctuations(z))\ndf_train['review'] = df_train['review'].apply(lambda z: remove_html(z))\ndf_train['review'] = df_train['review'].apply(lambda z: remove_url(z))\ndf_train['review'] = df_train['review'].apply(lambda z: remove_emoji(z))\n\ndf_val['review'] = df_val['review'].apply(lambda z: remove_punctuations(z))\ndf_val['review'] = df_val['review'].apply(lambda z: remove_html(z))\ndf_val['review'] = df_val['review'].apply(lambda z: remove_url(z))\ndf_val['review'] = df_val['review'].apply(lambda z: remove_emoji(z))\n\ndf_test['review'] = df_test['review'].apply(lambda z: remove_punctuations(z))\ndf_test['review'] = df_test['review'].apply(lambda z: remove_html(z))\ndf_test['review'] = df_test['review'].apply(lambda z: remove_url(z))\ndf_test['review'] = df_test['review'].apply(lambda z: remove_emoji(z))","metadata":{"execution":{"iopub.status.busy":"2021-08-06T09:32:13.20676Z","iopub.execute_input":"2021-08-06T09:32:13.207097Z","iopub.status.idle":"2021-08-06T09:32:23.06535Z","shell.execute_reply.started":"2021-08-06T09:32:13.207068Z","shell.execute_reply":"2021-08-06T09:32:23.064508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def remove_abb(data):\n    data = re.sub(r\"he's\", \"he is\", data)\n    data = re.sub(r\"there's\", \"there is\", data)\n    data = re.sub(r\"We're\", \"We are\", data)\n    data = re.sub(r\"That's\", \"That is\", data)\n    data = re.sub(r\"won't\", \"will not\", data)\n    data = re.sub(r\"they're\", \"they are\", data)\n    data = re.sub(r\"Can't\", \"Cannot\", data)\n    data = re.sub(r\"wasn't\", \"was not\", data)\n    data = re.sub(r\"don\\x89Ûªt\", \"do not\", data)\n    data= re.sub(r\"aren't\", \"are not\", data)\n    data = re.sub(r\"isn't\", \"is not\", data)\n    data = re.sub(r\"What's\", \"What is\", data)\n    data = re.sub(r\"haven't\", \"have not\", data)\n    data = re.sub(r\"hasn't\", \"has not\", data)\n    data = re.sub(r\"There's\", \"There is\", data)\n    data = re.sub(r\"He's\", \"He is\", data)\n    data = re.sub(r\"It's\", \"It is\", data)\n    data = re.sub(r\"You're\", \"You are\", data)\n    data = re.sub(r\"I'M\", \"I am\", data)\n    data = re.sub(r\"shouldn't\", \"should not\", data)\n    data = re.sub(r\"wouldn't\", \"would not\", data)\n    data = re.sub(r\"i'm\", \"I am\", data)\n    data = re.sub(r\"I\\x89Ûªm\", \"I am\", data)\n    data = re.sub(r\"I'm\", \"I am\", data)\n    data = re.sub(r\"Isn't\", \"is not\", data)\n    data = re.sub(r\"Here's\", \"Here is\", data)\n    data = re.sub(r\"you've\", \"you have\", data)\n    data = re.sub(r\"you\\x89Ûªve\", \"you have\", data)\n    data = re.sub(r\"we're\", \"we are\", data)\n    data = re.sub(r\"what's\", \"what is\", data)\n    data = re.sub(r\"couldn't\", \"could not\", data)\n    data = re.sub(r\"we've\", \"we have\", data)\n    data = re.sub(r\"it\\x89Ûªs\", \"it is\", data)\n    data = re.sub(r\"doesn\\x89Ûªt\", \"does not\", data)\n    data = re.sub(r\"It\\x89Ûªs\", \"It is\", data)\n    data = re.sub(r\"Here\\x89Ûªs\", \"Here is\", data)\n    data = re.sub(r\"who's\", \"who is\", data)\n    data = re.sub(r\"I\\x89Ûªve\", \"I have\", data)\n    data = re.sub(r\"y'all\", \"you all\", data)\n    data = re.sub(r\"can\\x89Ûªt\", \"cannot\", data)\n    data = re.sub(r\"would've\", \"would have\", data)\n    data = re.sub(r\"it'll\", \"it will\", data)\n    data = re.sub(r\"we'll\", \"we will\", data)\n    data = re.sub(r\"wouldn\\x89Ûªt\", \"would not\", data)\n    data = re.sub(r\"We've\", \"We have\", data)\n    data = re.sub(r\"he'll\", \"he will\", data)\n    data = re.sub(r\"Y'all\", \"You all\", data)\n    data = re.sub(r\"Weren't\", \"Were not\", data)\n    data = re.sub(r\"Didn't\", \"Did not\", data)\n    data = re.sub(r\"they'll\", \"they will\", data)\n    data = re.sub(r\"they'd\", \"they would\", data)\n    data = re.sub(r\"DON'T\", \"DO NOT\", data)\n    data = re.sub(r\"That\\x89Ûªs\", \"That is\", data)\n    data = re.sub(r\"they've\", \"they have\", data)\n    data = re.sub(r\"i'd\", \"I would\", data)\n    data = re.sub(r\"should've\", \"should have\", data)\n    data = re.sub(r\"You\\x89Ûªre\", \"You are\", data)\n    data = re.sub(r\"where's\", \"where is\", data)\n    data = re.sub(r\"Don\\x89Ûªt\", \"Do not\", data)\n    data = re.sub(r\"we'd\", \"we would\", data)\n    data = re.sub(r\"i'll\", \"I will\", data)\n    data = re.sub(r\"weren't\", \"were not\", data)\n    data = re.sub(r\"They're\", \"They are\", data)\n    data = re.sub(r\"Can\\x89Ûªt\", \"Cannot\", data)\n    data = re.sub(r\"you\\x89Ûªll\", \"you will\", data)\n    data = re.sub(r\"I\\x89Ûªd\", \"I would\", data)\n    data = re.sub(r\"let's\", \"let us\", data)\n    data = re.sub(r\"it's\", \"it is\", data)\n    data = re.sub(r\"can't\", \"cannot\", data)\n    data = re.sub(r\"don't\", \"do not\", data)\n    data = re.sub(r\"you're\", \"you are\", data)\n    data = re.sub(r\"i've\", \"I have\", data)\n    data = re.sub(r\"that's\", \"that is\", data)\n    data = re.sub(r\"i'll\", \"I will\", data)\n    data = re.sub(r\"doesn't\", \"does not\",data)\n    data = re.sub(r\"i'd\", \"I would\", data)\n    data = re.sub(r\"didn't\", \"did not\", data)\n    data = re.sub(r\"ain't\", \"am not\", data)\n    data = re.sub(r\"you'll\", \"you will\", data)\n    data = re.sub(r\"I've\", \"I have\", data)\n    data = re.sub(r\"Don't\", \"do not\", data)\n    data = re.sub(r\"I'll\", \"I will\", data)\n    data = re.sub(r\"I'd\", \"I would\", data)\n    data = re.sub(r\"Let's\", \"Let us\", data)\n    data = re.sub(r\"you'd\", \"You would\", data)\n    data = re.sub(r\"It's\", \"It is\", data)\n    data = re.sub(r\"Ain't\", \"am not\", data)\n    data = re.sub(r\"Haven't\", \"Have not\", data)\n    data = re.sub(r\"Could've\", \"Could have\", data)\n    data = re.sub(r\"youve\", \"you have\", data)  \n    data = re.sub(r\"donå«t\", \"do not\", data)  \n    return data\n    \ndf_train['review'] = df_train['review'].apply(lambda z: remove_abb(z))\ndf_val['review'] = df_val['review'].apply(lambda z: remove_abb(z))\ndf_test['review'] = df_test['review'].apply(lambda z: remove_abb(z))","metadata":{"execution":{"iopub.status.busy":"2021-08-06T09:32:23.06698Z","iopub.execute_input":"2021-08-06T09:32:23.067302Z","iopub.status.idle":"2021-08-06T09:32:32.265901Z","shell.execute_reply.started":"2021-08-06T09:32:23.067274Z","shell.execute_reply":"2021-08-06T09:32:32.264947Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df_train.shape)\nprint(df_train.head(5))\nprint(df_val.shape)\nprint(df_val.head(5))\nprint(df_test.shape)\nprint(df_test.head(5))","metadata":{"execution":{"iopub.status.busy":"2021-08-06T09:32:32.2678Z","iopub.execute_input":"2021-08-06T09:32:32.268194Z","iopub.status.idle":"2021-08-06T09:32:32.281109Z","shell.execute_reply.started":"2021-08-06T09:32:32.268152Z","shell.execute_reply":"2021-08-06T09:32:32.279939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model","metadata":{}},{"cell_type":"markdown","source":"### LSTM","metadata":{}},{"cell_type":"code","source":"def LSTM_V0(embedding):\n    #...\n    return outputs","metadata":{"execution":{"iopub.status.busy":"2021-08-06T08:12:04.978683Z","iopub.execute_input":"2021-08-06T08:12:04.979015Z","iopub.status.idle":"2021-08-06T08:12:04.982859Z","shell.execute_reply.started":"2021-08-06T08:12:04.978983Z","shell.execute_reply":"2021-08-06T08:12:04.981922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### BiLSTM","metadata":{}},{"cell_type":"code","source":"def BiLSTM_V0(embedding):\n    net = Bidirectional(LSTM(units=32, return_sequences=True,))(embedding)\n    net = GlobalAveragePooling1D()(net)\n    net = Dense(20, activation='relu')(net)\n    net = Dropout(rate=0.5)(net)\n    outputs = Dense(1, activation='sigmoid', name='classifier')(net) \n    return outputs","metadata":{"execution":{"iopub.status.busy":"2021-08-06T08:12:06.502039Z","iopub.execute_input":"2021-08-06T08:12:06.502364Z","iopub.status.idle":"2021-08-06T08:12:06.507701Z","shell.execute_reply.started":"2021-08-06T08:12:06.502334Z","shell.execute_reply":"2021-08-06T08:12:06.506678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### CNN + LSTM","metadata":{}},{"cell_type":"code","source":"def CNN_LSTM_V0(embedding):\n    net = Dropout(0.3)(embedding)\n    net = Conv1D(200, 5, activation='relu')(net)\n    net = MaxPooling1D(pool_size=2)(net)\n    net = LSTM(100)(net)\n    net = Dropout(0.3)(net)\n    net = Dense(16,activation='relu')(net)\n    outputs = Dense(1, activation='sigmoid', name='classifier')(net)\n    return outputs\n\ndef CNN_LSTM_V1(embedding):\n\n    # channel 1\n    net = Conv1D(filters=128, kernel_size=3*32, activation='relu')(embedding)\n    net = MaxPooling1D(pool_size=2)(net)\n    net = Dropout(0.5)(net)\n    net = BatchNormalization()(net)\n    a = LSTM(128)(net)\n\n    # channel 2\n    net = Conv1D(filters=128, kernel_size=5*32, activation='relu')(embedding)\n    net = MaxPooling1D(pool_size=2)(net)\n    net = Dropout(0.5)(net)\n    net = BatchNormalization()(net)\n    b = LSTM(128)(net)\n\n    # channel 3\n    net = Conv1D(filters=128, kernel_size=7*32, activation='relu')(embedding)\n    net = MaxPooling1D(pool_size=2)(net)\n    net = Dropout(0.5)(net)\n    net = BatchNormalization()(net)\n    c =LSTM(128)(net)\n\n    # channel 4\n    net = Conv1D(filters=128, kernel_size=9*32, activation='relu')(embedding)\n    net = MaxPooling1D(pool_size=2)(net)\n    net = Dropout(0.5)(net)\n    net = BatchNormalization()(net)\n    d=LSTM(128)(net)\n\n    merged = concatenate([a,b,c,d])\n    dense = Dense(100, activation='relu')(merged)\n    drop = Dropout(0.2)(dense)\n    outputs = Dense(1, activation='sigmoid')(merged)\n    return outputs","metadata":{"execution":{"iopub.status.busy":"2021-08-06T08:14:18.890118Z","iopub.execute_input":"2021-08-06T08:14:18.89046Z","iopub.status.idle":"2021-08-06T08:14:18.902831Z","shell.execute_reply.started":"2021-08-06T08:14:18.890425Z","shell.execute_reply":"2021-08-06T08:14:18.901728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### LSTM + CNN","metadata":{}},{"cell_type":"code","source":"def LSTM_CNN_V0(embedding):\n    net = LSTM(256,return_sequences=True)(embedding)\n    net = Conv1D(filters=100, kernel_size=2, padding='valid', activation='relu', strides=1)(net)\n    net = GlobalMaxPooling1D()(net)\n    net = Dropout(0.2)(net)\n    net = Dense(64,activation='relu')(net)\n    outputs = Dense(1, activation='sigmoid', name='classifier')(net)\n    return outputs\n\ndef LSTM_CNN_V1(embedding):\n    net = LSTM(512, return_sequences=True,dropout=0.25, recurrent_dropout=0.1)(embedding)\n    net = Conv1D(filters=64, kernel_size=7, padding='same', activation='relu', strides=1)(net)\n    net = MaxPooling1D()(net)\n    net = Conv1D(filters=128, kernel_size=5, padding='same', activation='relu', strides=1)(net)\n    net = MaxPooling1D()(net)\n    net = Conv1D(filters=256, kernel_size=3, padding='same', activation='relu', strides=1)(net)\n    net = MaxPooling1D()(net)\n    net = Flatten()(net)\n    net = Dense(256,activation='relu')(net)\n    net = Dropout(0.2)(net)\n    outputs = Dense(1, activation='sigmoid', name='classifier')(net)\n    return outputs","metadata":{"execution":{"iopub.status.busy":"2021-08-06T08:12:10.649588Z","iopub.execute_input":"2021-08-06T08:12:10.64993Z","iopub.status.idle":"2021-08-06T08:12:10.661685Z","shell.execute_reply.started":"2021-08-06T08:12:10.649898Z","shell.execute_reply":"2021-08-06T08:12:10.660873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Choose model","metadata":{}},{"cell_type":"code","source":"def create_model(model_name, model_ver, max_seq_len, max_features, embed_size, embedding_matrix):\n\n    ## Creat dictionary\n    choose_model = {'LSTM':{},\n                    'BiLSTM':{0: BiLSTM_V0},\n                    'CNN+LSTM':{0: CNN_LSTM_V0, 1: CNN_LSTM_V1},\n                    'LSTM+CNN':{0: LSTM_CNN_V0, 1: LSTM_CNN_V1},}\n    \n    ## Embedding\n    inputs = Input(shape=(max_seq_len,))\n    embedding = Embedding(max_features,embed_size,weights=[embedding_matrix])(inputs)\n    \n    outputs = choose_model[model_name][model_ver](embedding)\n    model = keras.Model(inputs, outputs)\n        \n    return model","metadata":{"execution":{"iopub.status.busy":"2021-08-06T09:08:25.831956Z","iopub.execute_input":"2021-08-06T09:08:25.832302Z","iopub.status.idle":"2021-08-06T09:08:25.840587Z","shell.execute_reply.started":"2021-08-06T09:08:25.832269Z","shell.execute_reply":"2021-08-06T09:08:25.839754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Tokenzie","metadata":{}},{"cell_type":"code","source":"max_seq_len = 500\nmax_features = 10000\n#embed_size = 50\n\ntokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(df_train['review']))\nX_train = tokenizer.texts_to_sequences(df_train['review'])\nX_val = tokenizer.texts_to_sequences(df_val['review'])\nX_test = tokenizer.texts_to_sequences(df_test['review'])","metadata":{"execution":{"iopub.status.busy":"2021-08-06T09:52:27.378768Z","iopub.execute_input":"2021-08-06T09:52:27.379147Z","iopub.status.idle":"2021-08-06T09:52:42.214902Z","shell.execute_reply.started":"2021-08-06T09:52:27.379112Z","shell.execute_reply":"2021-08-06T09:52:42.214049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = pad_sequences(X_train, maxlen=max_seq_len)\nX_val = pad_sequences(X_val, maxlen=max_seq_len)\nX_test = pad_sequences(X_test, maxlen=max_seq_len)\ny_train = df_train['sentiment']\ny_val = df_val['sentiment']\ny_test = df_test['sentiment']","metadata":{"execution":{"iopub.status.busy":"2021-08-06T09:52:42.216438Z","iopub.execute_input":"2021-08-06T09:52:42.216775Z","iopub.status.idle":"2021-08-06T09:52:44.481438Z","shell.execute_reply.started":"2021-08-06T09:52:42.216738Z","shell.execute_reply":"2021-08-06T09:52:44.480462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Embeddings","metadata":{}},{"cell_type":"code","source":"EMBEDDING_FILE = '../input/glove-global-vectors-for-word-representation/glove.6B.200d.txt'\ndef get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\nembeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE) if len(o)>100)\n\nall_embs = np.stack(embeddings_index.values())\nemb_mean,emb_std = all_embs.mean(), all_embs.std()\nembed_size = all_embs.shape[1]\n\nword_index = tokenizer.word_index\nnb_words = min(max_features, len(word_index))\nembedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\nfor word, i in word_index.items():\n    if i >= max_features: continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None: embedding_matrix[i] = embedding_vector","metadata":{"execution":{"iopub.status.busy":"2021-08-06T09:52:44.48365Z","iopub.execute_input":"2021-08-06T09:52:44.484081Z","iopub.status.idle":"2021-08-06T09:53:17.482356Z","shell.execute_reply.started":"2021-08-06T09:52:44.484035Z","shell.execute_reply":"2021-08-06T09:53:17.48149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Creat model","metadata":{}},{"cell_type":"code","source":"model_name = \"CNN+LSTM\"\nmodel_ver = 0\nLR = 1e-3\nloss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\noptimizer = Adam(learning_rate = LR)\nmetrics = tf.metrics.BinaryAccuracy()\n\nmodel = create_model(model_name, model_ver, max_seq_len, max_features, embed_size, embedding_matrix)\nmodel.compile(loss=loss, optimizer=optimizer, metrics=metrics)\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2021-08-06T09:58:22.757284Z","iopub.execute_input":"2021-08-06T09:58:22.757609Z","iopub.status.idle":"2021-08-06T09:58:23.069801Z","shell.execute_reply.started":"2021-08-06T09:58:22.757575Z","shell.execute_reply":"2021-08-06T09:58:23.068878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot architecture model\ntf.keras.utils.plot_model(model, show_shapes=True, dpi=96) #to_file='model.png'","metadata":{"execution":{"iopub.status.busy":"2021-08-06T09:58:25.693874Z","iopub.execute_input":"2021-08-06T09:58:25.694199Z","iopub.status.idle":"2021-08-06T09:58:25.94323Z","shell.execute_reply.started":"2021-08-06T09:58:25.694168Z","shell.execute_reply":"2021-08-06T09:58:25.942153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model training","metadata":{}},{"cell_type":"code","source":"# Save model\nmodel_ckpt_path = f\"[Glove]{model_name}_V{model_ver}_{max_seq_len}_{max_features}_{embed_size}.hdf5\"\ncheckpoint = ModelCheckpoint(model_ckpt_path, monitor='val_binary_accuracy', mode='max', verbose=1, save_best_only=True, save_weights_only=True)\ncallbacks_list = [checkpoint]\n\n# Training\nprint(f\"Training model with [Glove]{model_name}_V{model_ver}_{max_seq_len}_{max_features}_{embed_size}\\n\")\ntrain_history = model.fit(X_train, y_train, validation_data=(X_val,y_val), epochs=5, batch_size=32, verbose=1, callbacks=callbacks_list)","metadata":{"execution":{"iopub.status.busy":"2021-08-06T09:58:30.524813Z","iopub.execute_input":"2021-08-06T09:58:30.525169Z","iopub.status.idle":"2021-08-06T10:02:19.061479Z","shell.execute_reply.started":"2021-08-06T09:58:30.525131Z","shell.execute_reply":"2021-08-06T10:02:19.060427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot accuracy and loss\nhistory_dict = train_history.history\nprint(history_dict.keys())\n\nacc = history_dict['binary_accuracy']\nval_acc = history_dict['val_binary_accuracy']\nloss = history_dict['loss']\nval_loss = history_dict['val_loss']\n\nepochs = range(1, len(acc) + 1)\nfig = plt.figure(figsize=(10, 6))\nfig.tight_layout()\n\nplt.subplot(2, 1, 1)\nplt.plot(epochs, loss, 'r', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.subplot(2, 1, 2)\nplt.plot(epochs, acc, 'r', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend(loc='lower right')","metadata":{"execution":{"iopub.status.busy":"2021-08-06T10:02:19.065039Z","iopub.execute_input":"2021-08-06T10:02:19.065303Z","iopub.status.idle":"2021-08-06T10:02:19.441464Z","shell.execute_reply.started":"2021-08-06T10:02:19.065274Z","shell.execute_reply":"2021-08-06T10:02:19.440075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save architecture model\nconfig = model.to_json()\nmodel_config_path = f\"[Glove]{model_name}_V{model_ver}_{max_seq_len}_{max_features}_{embed_size}.json\"\nwith open(model_config_path, \"w\") as outfile:\n    json.dump(config, outfile)","metadata":{"execution":{"iopub.status.busy":"2021-08-06T07:31:20.943118Z","iopub.status.idle":"2021-08-06T07:31:20.943711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluate","metadata":{}},{"cell_type":"code","source":"model.load_weights(model_ckpt_path)\ny_pred_proba = model.predict(X_test)\nget_metrics(y_test, y_pred_proba)","metadata":{"execution":{"iopub.status.busy":"2021-08-06T10:02:19.443278Z","iopub.execute_input":"2021-08-06T10:02:19.44362Z","iopub.status.idle":"2021-08-06T10:02:22.750457Z","shell.execute_reply.started":"2021-08-06T10:02:19.443583Z","shell.execute_reply":"2021-08-06T10:02:22.749687Z"},"trusted":true},"execution_count":null,"outputs":[]}]}