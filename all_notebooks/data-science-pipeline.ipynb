{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Lets create various Data science pipeline\n### 1.Exploratory Data Analysis \n### 2.Data visulization \n### 3.Data cleaning and Feature Engineering \n### 4.Feature Selection \n### 5.Model Selection\n","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df=pd.read_csv('/kaggle/input/factors-affecting-campus-placement/Placement_Data_Full_Class.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploratory Data Analysis","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe(include='all')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"salary have some none value it means the null value must belong to some unplaced student.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Lets plot data of male student and female.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\nplt.style.use('ggplot')\nfig,ax=plt.subplots(1,2,figsize=(15,5))\nsns.countplot(x='gender',data=df,hue='gender',ax=ax[0])\nsns.countplot(x='gender',data=df,hue='status',ax=ax[1])\nax[0].set_title('Total no of female and male students')\nax[1].set_title('Total no of female and male students who have been placed and not placed')\n\nfig.tight_layout(pad=5.0)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Lets plot countplot to degree and status\nIt will help us in analyzing which degree have higher placement ratio","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets plot \nsns.set(style=\"dark\")\nfig,ax=plt.subplots(1,3,figsize=(25,5))\nsns.countplot(x='degree_t',data=df,hue='status',palette=\"Set2\",ax=ax[0])\nsns.countplot(x='hsc_b',data=df,hue='status',ax=ax[1],palette=\"GnBu\")\nsns.countplot(x='workex',data=df,hue='status',ax=ax[2],palette=\"deep\")\n\nax[0].set_title('Degree on placement')\nax[1].set_title('Board on placement')\nax[2].set_title('Previous work experience on placement')\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plotting pair plot bw diff data\nplt.figure(figsize=(25,20))\nsns.pairplot(df[[column for column in df.columns if column not in ['sl_no']]])\nplt.tight_layout(pad=2.0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"\n# Machine learning model creation to check if student is placed or not","execution_count":null},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Feature engineering","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# first of all we will drop salary and sl_no as they are not the requi9red feature for our model creations\ndf.drop(['sl_no','salary'],axis=1,inplace=True)\ndf.head(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## There are some categorical variable so lets perform encoding in them by One hot encoding","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets seperate independent feature with dependent feature\nind_df=df.drop('status',axis=1)\ndep_df=df[['status']]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#creating dummy variable and dropping first row\nind_df=pd.get_dummies(ind_df,drop_first=True)\nind_df.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# splitting data into train part and test part\nX_train,X_test,y_train,y_test=train_test_split(ind_df,dep_df,test_size=0.2,random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#model creation SVC\nmodel=SVC()\nmodel.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# predicting score \ny_pred=model.predict(X_test)\nmodel.score(X_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Lets plot confusion matrix we get very bad accuracy*","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cm=confusion_matrix(y_test,y_pred)\n# lets plot heat map to visulize this\nsns.heatmap(cm)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### As we can see our score is very low and it does not perform better so we would try to do hyperparameter tuning","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Hyperparameter tuning for machine learning models.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"When creating a machine learning model, you'll be presented with design choices as to how to define your model architecture. Often times, we don't immediately know what the optimal model architecture should be for a given model, and thus we'd like to be able to explore a range of possibilities. In true machine learning fashion, we'll ideally ask the machine to perform this exploration and select the optimal model architecture automatically. Parameters which define the model architecture are referred to as hyperparameters and thus this process of searching for the ideal model architecture is referred to as hyperparameter tuning.\n\nThese hyperparameters might address model design questions such as:\n\nWhat degree of polynomial features should I use for my linear model?\nWhat should be the maximum depth allowed for my decision tree?\nWhat should be the minimum number of samples required at a leaf node in my decision tree?","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets import library for doing hyperparameter tuning there are two ways \n# 1. Grid search cv\n# 2. Randomized search cv\nfrom sklearn.model_selection import GridSearchCV\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets set our params\nparameter=[{'C':[50,100,150,200],'kernel':['linear']}\n          ,{'C':[100,200,300,400],'kernel':['rbf'],'gamma':[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]}]\ngrid_srch=GridSearchCV(estimator=model,param_grid=parameter,scoring='accuracy',cv=10,n_jobs=-1)\ngrid_srch.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# now check our best accuracy\ngrid_srch.best_score_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# these parameters are the best fit for our model\ngrid_srch.best_params_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## As we can see now our accuracy improves a lot after hyperparameter tuning\n# Now i am ending this kernel Right here\n# If you like my way. please do an upvote.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}