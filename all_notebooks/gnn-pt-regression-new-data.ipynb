{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"!pip -qq install torch==1.5.1+cpu torchvision==0.6.1+cpu -f https://download.pytorch.org/whl/torch_stable.html","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"! pip -qq install torch-scatter==latest+cpu -f https://pytorch-geometric.com/whl/torch-1.5.0.html\n! pip -qq install torch-sparse==latest+cpu -f https://pytorch-geometric.com/whl/torch-1.5.0.html\n! pip -qq install torch-cluster==latest+cpu -f https://pytorch-geometric.com/whl/torch-1.5.0.html\n! pip -qq install torch-spline-conv==latest+cpu -f https://pytorch-geometric.com/whl/torch-1.5.0.html\n! pip -qq install torch-geometric","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import os\nimport time\nimport random\nfrom tqdm.notebook import tqdm\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom joblib import Parallel, delayed\n\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\n\nimport torch\nimport torch.nn.functional as F\nimport torch_geometric.nn as gnn\nfrom torch_geometric.data import Dataset, Data, DataLoader\nfrom torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/cmsnewsamples/new-smaples.csv').drop(columns = 'Unnamed: 0')\ndf = df.drop(columns = [i for i in df.columns if '_1' in i])\ndf['non_hits'] = df[[i for i in df.columns if 'mask' in i]].sum(axis=1)\ndf = df[df['non_hits']==0].reset_index(drop=True)\n\ndf['1/pT'] = df['q/pt'].abs()\ndef label(a):\n    if a<=10:\n        return 0\n    if a>10 and a<=30:\n        return 1\n    if a>30 and a<=100:\n        return 2\n    if a>100:\n        return 3\n\ndf['pT'] = 1/df['1/pT']\n    \ndf['pT_classes'] = df['pT'].apply(label)\n\nfeatures = ['emtf_phi_'+str(i) for i in [0,2,3,4]] + ['emtf_theta_'+str(i) for i in [0,2,3,4]] + ['fr_'+str(i) for i in [0,2,3,4]] + ['old_emtf_phi_'+str(i) for i in [0,2,3,4]]\nlabels_1 = ['pT']\nlabels_2 = ['pT_classes']\nlabels_3 = ['vx']\n\nscaler_1 = StandardScaler()\ndf[features] = scaler_1.fit_transform(df[features])\n\nscaler_3 = MinMaxScaler()\ndf[labels_3] = scaler_3.fit_transform(df[labels_3])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shuffled_list = list(range(len(df)))\nrandom.Random(242).shuffle(shuffled_list)\nshuffled_list = np.array_split(np.array(shuffled_list), 10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"edge_index = torch.tensor([(0,1),(1,2),(2,3),(3,2),(2,1),(1,0)], dtype=torch.long).T\nX_data = df[features].to_numpy()\nY_data = df[labels_1].to_numpy()\ndef process_data(i):\n  graph = X_data[i].reshape(-1,4).T\n  y = Y_data[i]\n  data = Data(x=torch.tensor(graph, dtype=torch.float), y=torch.tensor(y, dtype=torch.float), edge_index=edge_index)\n  return data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class TriggerDataset(Dataset):\n    def __init__(self, root, indexes=list(range(len(df))), transform=None, pre_transform=None):\n        super(TriggerDataset, self).__init__(root, transform, pre_transform)\n        self.indexes = indexes\n        self.length = len(self.indexes)\n\n    @property\n    def raw_file_names(self):\n        return ['vgc']\n\n    @property\n    def processed_file_names(self):\n        return ['vghv']\n\n    def download(self):\n        return None\n\n    def process(self):\n        return None\n\n    def len(self):\n        return self.length\n\n    def get(self, idx):\n        return process_data(self.indexes[idx])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class MPNN(torch.nn.Module):\n    def __init__(self):\n      super(MPNN, self).__init__()\n      self.conv1 = gnn.SAGEConv(4,64)\n#       self.conv2 = gnn.SAGEConv(128,64)\n      self.conv3 = gnn.SAGEConv(64,32)\n      self.pool1 = gnn.TopKPooling(32, ratio=0.9)\n      self.lin1 = torch.nn.Linear(64, 128)\n      self.lin2 = torch.nn.Linear(128, 16)\n      self.lin3 = torch.nn.Linear(16, 16)\n      self.lin4 = torch.nn.Linear(16, 1)\n\n    def forward(self, data):\n      x, edge_index, batch = data.x, data.edge_index, data.batch\n      x = F.relu(self.conv1(x, edge_index))\n#       x = F.relu(self.conv2(x, edge_index))\n      x = F.relu(self.conv3(x, edge_index))\n      x, edge_index, _, batch, _, _ = self.pool1(x, edge_index, None, batch)\n      x = torch.cat([gmp(x, batch), gap(x, batch)], dim=1)\n      x = F.relu(self.lin1(x))\n      x = F.relu(self.lin2(x))\n      x = self.lin3(x)\n      x = self.lin4(x).squeeze(1)\n\n      return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 512\nepochs = 100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"val_batch, test_batch = 1, 2\ntrain_loader = DataLoader(TriggerDataset('./',np.concatenate([shuffled_list[j] for j in range(10) if j not in (val_batch, test_batch)])), batch_size=batch_size, shuffle=True, num_workers = 4) \nval_loader = DataLoader(TriggerDataset('./',shuffled_list[val_batch]), batch_size=batch_size) \ntest_loader = DataLoader(TriggerDataset('./',shuffled_list[test_batch]), batch_size=batch_size)\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = MPNN().to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\nlr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, verbose=True, patience=1, factor=0.5)\n# mse = torch.nn.MSELoss()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def mse(outputs, labels):\n    weights = torch.tensor(labels<80, dtype=torch.float)*labels + torch.tensor(labels>=80, dtype=torch.float)*torch.tensor(labels<160, dtype=torch.float)*labels*2.4 + torch.tensor(labels>=160, dtype=torch.float)*10\n    error = weights*(((outputs-labels)/labels)**2)\n    return torch.mean(error)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"m_train_loss = []\nm_val_loss = []\nm_test_loss = []\nmin_val_loss = float('inf')\nfor epoch in range(epochs):\n  train_loss = 0\n  val_loss = 0\n  pbar = tqdm(train_loader)\n  for data in pbar:\n    data = data.to(device)\n    optimizer.zero_grad()\n    outputs = model(data)\n    labels = data.y\n    loss = mse(outputs, labels)\n    loss.backward()\n    optimizer.step()\n    pbar.set_description('MSELoss: '+str(loss.cpu().detach().numpy()))\n    train_loss += loss.cpu().detach()/len(train_loader)\n\n  for data in val_loader:\n    data = data.to(device)\n    optimizer.zero_grad()\n    outputs = model(data)\n    labels = data.y\n    loss = mse(outputs, labels)\n    val_loss += loss.cpu().detach()/len(val_loader)\n  if val_loss.detach().numpy()<min_val_loss:\n    min_val_loss = val_loss.cpu().detach().numpy()\n    torch.save(model.state_dict(), 'model.pth')\n  lr_scheduler.step(val_loss)\n  print('Epoch: ', str(epoch+1)+'/'+str(epochs))\n  print('Training MSELoss: ', train_loss.numpy())\n  print('Validation MSELoss: ', val_loss.numpy())\n  m_train_loss.append(train_loss.numpy())\n  m_val_loss.append(val_loss.numpy())\n  if epoch>10 and max(m_val_loss[-11:-1])<val_loss.numpy():\n    print( max(m_val_loss[-11:-1]),val_loss.numpy())\n    break","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = MPNN().to(device)\nmodel.load_state_dict(torch.load('model.pth'))\ntest_loss = 0\ntrue = []\npreds = []\nfor data in test_loader:\n  data = data.to(device)\n  optimizer.zero_grad()\n  outputs = model(data)\n  labels = data.y\n  true += list(labels.detach().numpy())\n  preds += list(outputs.detach().numpy())\n  loss = mse(outputs, labels)\n  test_loss += loss/len(test_loader)\nprint('Test MSELoss: ', test_loss.detach().numpy())\nOOF_preds = pd.DataFrame()\nOOF_preds['true_value'] = true\nOOF_preds['preds'] = preds\nOOF_preds['row'] = shuffled_list[test_batch]\nOOF_preds.to_csv('OOF_preds.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('OOF_preds.csv').drop(columns = ['Unnamed: 0'])\ndf = df.sort_values(by = 'row').reset_index(drop = True)\ndf['True_pT'] = df['true_value']\ndf['Predicted_pT'] = df['preds']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn.metrics import mean_absolute_error as mae\n\nMAE1 = []\ndx = 0.5\nfor i in tqdm(range(int(2/dx),int(150/dx))):\n    P = df[(df['True_pT']>=(i-1)*dx)&(df['True_pT']<=(i+1)*dx)]\n    try:\n        p = mae(P['True_pT'],P['Predicted_pT'])\n    except:\n        p=0\n    MAE1.append(p)\nMAE1 = MAE1[:146]\nplt.plot([i*dx for i in range(int(75/dx))],[0]*int(int(75/dx)-len(MAE1))+MAE1,label = 'FCNN')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def pT_classes(x):\n    if x>=25:\n        return 'Above 25 GeV'\n    else:\n        return 'Below 25 GeV'\n\nprint(classification_report(df['True_pT'].apply(pT_classes), df['Predicted_pT'].apply(pT_classes)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}