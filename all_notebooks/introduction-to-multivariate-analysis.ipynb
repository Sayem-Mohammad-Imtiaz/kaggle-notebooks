{"cells":[{"metadata":{"_uuid":"eb9c8ec2e0bafda82bdd819ec7bf5e4e1a020ca3"},"cell_type":"markdown","source":"This kernel tells you how to use the Python ecosystem to carry out some simple multivariate analyses, with a focus\non principal components analysis (PCA) and linear discriminant analysis (LDA)."},{"metadata":{"_uuid":"ee4abbcd4282dd03c5d874d7c4f82b85467ac9b7"},"cell_type":"markdown","source":"### Importing the libraries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"from pydoc import help # can type in the python console `help(name of function)` to get the documentation\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import scale\nfrom sklearn.decomposition import PCA\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom scipy import stats\nfrom IPython.display import display, HTML\n# figures inline in notebook\n%matplotlib inline\nnp.set_printoptions(suppress=True)\npd.set_option('display.max_rows', 20)\nimport os\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"### Reading Data"},{"metadata":{"trusted":true,"_uuid":"e49f69516a0574e52f417537986d975be78a71e8"},"cell_type":"code","source":"data=pd.read_csv(\"../input/Wine.csv\")\ndata.columns = [\"V\"+str(i) for i in range(1, len(data.columns)+1)]\nX = data.loc[:, \"V2\":] # independent variables data\ny = data.V1 # dependednt variable data\ndata","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e99409e73c4f4de9b345816344a915d221b2c0f3"},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"22d0fdc3c177bcdc96a96b4663e33f8bf5f71aaa"},"cell_type":"code","source":"data.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"52964dff0ad5a52d8296154f3b99661175df4822"},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"43918a27251acac87e61e693d70d0421db8171cf"},"cell_type":"markdown","source":"## Plotting Multivariate Data"},{"metadata":{"_uuid":"7627281f375a95d84ff60449fbf08ce5db4256fe"},"cell_type":"markdown","source":"### Matrix Scatterplot"},{"metadata":{"trusted":true,"_uuid":"0dd6bb719f7a82e27fefb78b483d3840cfb9f7b3"},"cell_type":"code","source":"pd.plotting.scatter_matrix(data.loc[:, \"V2\":\"V14\"], diagonal=\"kde\",figsize=(20,15))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1c3c1497efa13883203b1929e702c22b05892865"},"cell_type":"markdown","source":"### Scatterplot with the Data Points Labelled by their Group"},{"metadata":{"trusted":true,"_uuid":"661dba243c1639aa12614315f41bd1714cb04675"},"cell_type":"code","source":"for i in range(2,14):\n    sns.lmplot(\"V\"+str(i), \"V\"+str(i+1), data, hue=\"V1\", fit_reg=False);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bd9eabc80003e1034e31a7249616e49f1e933b15"},"cell_type":"markdown","source":"### Profile Plot\nProfile plot, used to shows the variation in each of the variables, by plotting the value of each of the variables for each of the samples."},{"metadata":{"trusted":true,"_uuid":"87225a3db718441426dfe3dc8aeccc0bf705a3b3"},"cell_type":"code","source":"ax = data[[\"V2\",\"V3\",\"V4\",\"V5\",\"V6\",\"V7\",\"V8\",\"V9\",\"V10\",\"V11\",\"V12\",\"V13\",\"V14\"]].plot(figsize=(20,15))\nax.legend(loc='center left', bbox_to_anchor=(1, 0.5));","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"69d49b1a65ae4e3d1390a21542d95ea44f84006d"},"cell_type":"code","source":"ax = data[[\"V2\",\"V3\",\"V4\",\"V5\",\"V6\"]].plot(figsize=(20,15))\nax.legend(loc='center left', bbox_to_anchor=(1, 0.5));","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"99aae680624d5fc37fdab5f95c18e191a71260e4"},"cell_type":"code","source":"ax = data[[\"V7\",\"V8\",\"V9\",\"V10\",\"V11\"]].plot(figsize=(20,15))\nax.legend(loc='center left', bbox_to_anchor=(1, 0.5));","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b00d88dff7c4769e2ee38720194c1aa33e6c1217"},"cell_type":"code","source":"ax = data[[\"V12\",\"V13\",\"V14\"]].plot(figsize=(20,15))\nax.legend(loc='center left', bbox_to_anchor=(1, 0.5));","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7bde1336fa2aabc3859910c57406e6ac53dee684"},"cell_type":"code","source":"ax = data[[\"V12\",\"V13\"]].plot(figsize=(20,15))\nax.legend(loc='center left', bbox_to_anchor=(1, 0.5));","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9210f168dcf2fee9f831e1ec714000ebc618b248"},"cell_type":"markdown","source":"## Calculating Summary Statistics for Multivariate Data"},{"metadata":{"trusted":true,"_uuid":"0a875b130cef54df7120eff452c7a9935e8e2cc2"},"cell_type":"code","source":"X.apply(np.mean)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fdd9f154736dbb2c091537035f093f7cc18b7269"},"cell_type":"code","source":"X.apply(np.std)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7f9fa8567f4481b50d9c7ab66cb5a1d577f46c9d"},"cell_type":"code","source":"X.apply(np.max)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e352c8b4d852b86ee44b3fc7d9e09300bde6554a"},"cell_type":"code","source":"X.apply(np.min)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9b701d34301ede443f3739657c0e4c2e42782a1f"},"cell_type":"markdown","source":"### Means and Variances Per Group"},{"metadata":{"trusted":true,"_uuid":"75492a026fb995bbc135036e0a60eebd55af4650"},"cell_type":"code","source":"def printMeanAndSdByGroup(variables, groupvariable):\n    data_groupby = variables.groupby(groupvariable)\n    print(\"## Means:\")\n    display(data_groupby.apply(np.mean))\n    print(\"\\n## Standard deviations:\")\n    display(data_groupby.apply(np.std))\n    print(\"\\n## Sample sizes:\")\n    display(pd.DataFrame(data_groupby.apply(len)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2f3dd242068c345631cdfa2a8a333b863a423cfe"},"cell_type":"code","source":"printMeanAndSdByGroup(X, y)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"54f8650167bc4e46d8e078d6ea06b9aa01b9df16"},"cell_type":"markdown","source":"### Between-groups Variance and Within-groups Variance for a Variable"},{"metadata":{"trusted":true,"_uuid":"6c62eba9a52e9e28b2d71e341f5db17ea44ae5f0"},"cell_type":"code","source":"def calcWithinGroupsVariance(variable, groupvariable):\n    # find out how many values the group variable can take\n    levels = sorted(set(groupvariable))\n    numlevels = len(levels)\n    # get the mean and standard deviation for each group:\n    numtotal = 0\n    denomtotal = 0\n    for leveli in levels:\n        levelidata = variable[groupvariable==leveli]\n        levelilength = len(levelidata)\n        # get the standard deviation for group i:\n        sdi = np.std(levelidata)\n        numi = (levelilength)*sdi**2\n        denomi = levelilength\n        numtotal = numtotal + numi\n        denomtotal = denomtotal + denomi\n    # calculate the within-groups variance\n    Vw = numtotal / (denomtotal - numlevels)\n    return Vw","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f21252c9986a6cd1a2f7fd527efff175f2307a65"},"cell_type":"code","source":"calcWithinGroupsVariance(X.V2, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3b179458b33110537719d5fd297c5d4392e30227"},"cell_type":"code","source":"def calcBetweenGroupsVariance(variable, groupvariable):\n    # find out how many values the group variable can take\n    levels = sorted(set((groupvariable)))\n    numlevels = len(levels)\n    # calculate the overall grand mean:\n    grandmean = np.mean(variable)\n    # get the mean and standard deviation for each group:\n    numtotal = 0\n    denomtotal = 0\n    for leveli in levels:\n        levelidata = variable[groupvariable==leveli]\n        levelilength = len(levelidata)\n        # get the mean and standard deviation for group i:\n        meani = np.mean(levelidata)\n        sdi = np.std(levelidata)\n        numi = levelilength * ((meani - grandmean)**2)\n        denomi = levelilength\n        numtotal = numtotal + numi\n        denomtotal = denomtotal + denomi\n    # calculate the between-groups variance\n    Vb = numtotal / (numlevels - 1)\n    return(Vb)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0aab9053bb72b21ad97a3de6c9724f2d2a81d5ee"},"cell_type":"code","source":"calcBetweenGroupsVariance(X.V2, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"88a24e229b6ab89670eb5721f83758ca7a8ed319"},"cell_type":"code","source":"def calcSeparations(variables, groupvariable):\n    # calculate the separation for each variable\n    for variablename in variables:\n        variablei = variables[variablename]\n        Vw = calcWithinGroupsVariance(variablei, groupvariable)\n        Vb = calcBetweenGroupsVariance(variablei, groupvariable)\n        sep = Vb/Vw\n        print(\"variable\", variablename, \"Vw=\", Vw, \"Vb=\", Vb, \"separation=\", sep)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4009798c95a50ee7558b5c226e2322eba411f031"},"cell_type":"code","source":"calcSeparations(X, y)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f67aec38b3f0f4d721bead54850d01b9cc40f97a"},"cell_type":"markdown","source":"### Between-groups Covariance and Within-groups Covariance for Two Variables"},{"metadata":{"trusted":true,"_uuid":"4a71e914b5a2f922ad9f763cd8f0fa799f65a4e3"},"cell_type":"code","source":"def calcWithinGroupsCovariance(variable1, variable2, groupvariable):\n    levels = sorted(set(groupvariable))\n    numlevels = len(levels)\n    Covw = 0.0\n    # get the covariance of variable 1 and variable 2 for each group:\n    for leveli in levels:\n        levelidata1 = variable1[groupvariable==leveli]\n        levelidata2 = variable2[groupvariable==leveli]\n        mean1 = np.mean(levelidata1)\n        mean2 = np.mean(levelidata2)\n        levelilength = len(levelidata1)\n        # get the covariance for this group:\n        term1 = 0.0\n        for levelidata1j, levelidata2j in zip(levelidata1, levelidata2):\n            term1 += (levelidata1j - mean1)*(levelidata2j - mean2)\n        Cov_groupi = term1 # covariance for this group\n        Covw += Cov_groupi\n    totallength = len(variable1)\n    Covw /= totallength - numlevels\n    return Covw","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f15c8ac54e601a03cc14e271d1390e38ee8799b2"},"cell_type":"code","source":"calcWithinGroupsCovariance(X.V8, X.V11, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"35bd389b5cb2a67400ecafed5714466ff80d4fb7"},"cell_type":"code","source":"def calcBetweenGroupsCovariance(variable1, variable2, groupvariable):\n    # find out how many values the group variable can take\n    levels = sorted(set(groupvariable))\n    numlevels = len(levels)\n    # calculate the grand means\n    variable1mean = np.mean(variable1)\n    variable2mean = np.mean(variable2)\n    # calculate the between-groups covariance\n    Covb = 0.0\n    for leveli in levels:\n        levelidata1 = variable1[groupvariable==leveli]\n        levelidata2 = variable2[groupvariable==leveli]\n        mean1 = np.mean(levelidata1)\n        mean2 = np.mean(levelidata2)\n        levelilength = len(levelidata1)\n        term1 = (mean1 - variable1mean) * (mean2 - variable2mean) * levelilength\n        Covb += term1\n    Covb /= numlevels - 1\n    return Covb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f7c1bee02660c6b856df6fb9cfa901a35d21b781"},"cell_type":"code","source":"calcBetweenGroupsCovariance(X.V8, X.V11, y)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8198ad90e4ff60c2d46be6632bc3f5d99d5bdced"},"cell_type":"markdown","source":"### Calculating Correlations for Multivariate Data\nIt is often of interest to investigate whether any of the variables in a multivariate data set are significantly correlated.\nTo calculate the linear (Pearson) correlation coefficient for a pair of variables, you can use the pearsonr() function\nfrom scipy.stats package."},{"metadata":{"trusted":true,"_uuid":"0a789f019992bf414b87ae830a2b9007ff955b1d"},"cell_type":"code","source":"corr = stats.pearsonr(X.V2, X.V3)\nprint(\"p-value:\\t\", corr[1])\nprint(\"cor:\\t\\t\", corr[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f550529ed05b81e1b51d4555f5952cc7326141cc"},"cell_type":"code","source":"corrmat = X.corr()\ncorrmat","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"050a8365f67b1c1a58cbcfb404d25481798b8a1e"},"cell_type":"code","source":"sns.heatmap(corrmat, vmax=1., square=False).xaxis.tick_top()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"de37374c14437e944fe4612f0989508d5b75fbb0"},"cell_type":"markdown","source":"### Hinton diagram\nThe color of the boxes determines the sign of the correlation,\nin this case red for positive and blue for negative correlations; while the size of the boxes determines their\nmagnitude, the bigger the box the higher the magnitude."},{"metadata":{"trusted":true,"_uuid":"16320f3e8c310e0bbf67f64caccf042dd9fc9211"},"cell_type":"code","source":"def hinton(matrix, max_weight=None, ax=None):\n    \"\"\"Draw Hinton diagram for visualizing a weight matrix.\"\"\"\n    ax = ax if ax is not None else plt.gca()\n    if not max_weight:\n        max_weight = 2**np.ceil(np.log(np.abs(matrix).max())/np.log(2))\n    ax.patch.set_facecolor('lightgray')\n    ax.set_aspect('equal', 'box')\n    ax.xaxis.set_major_locator(plt.NullLocator())\n    ax.yaxis.set_major_locator(plt.NullLocator())\n    for (x, y), w in np.ndenumerate(matrix):\n        color = 'red' if w > 0 else 'blue'\n        size = np.sqrt(np.abs(w))\n        rect = plt.Rectangle([x - size / 2, y - size / 2], size, size,\n        facecolor=color, edgecolor=color)\n        ax.add_patch(rect)\n    nticks = matrix.shape[0]\n    ax.xaxis.tick_top()\n    ax.set_xticks(range(nticks))\n    ax.set_xticklabels(list(matrix.columns), rotation=90)\n    ax.set_yticks(range(nticks))\n    ax.set_yticklabels(matrix.columns)\n    ax.grid(False)\n    ax.autoscale_view()\n    ax.invert_yaxis()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f5895d3fd31e6b14e2ba08df3f97a95364a8a555"},"cell_type":"code","source":"hinton(corrmat)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"22b72a7623894b6d7cb2a1c80b1d6d7834256b77"},"cell_type":"code","source":"def mosthighlycorrelated(mydataframe, numtoreport):\n    # find the correlations\n    cormatrix = mydataframe.corr()\n    # set the correlations on the diagonal or lower triangle to zero,\n    # so they will not be reported as the highest ones:\n    cormatrix *= np.tri(*cormatrix.values.shape, k=-1).T\n    # find the top n correlations\n    cormatrix = cormatrix.stack()\n    cormatrix = cormatrix.reindex(cormatrix.abs().sort_values(ascending=False).index).reset_index()\n    # assign human-friendly names\n    cormatrix.columns = [\"FirstVariable\", \"SecondVariable\", \"Correlation\"]\n    return cormatrix.head(numtoreport)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ff57d254354891359bec875cce739be3b8388d41"},"cell_type":"code","source":"mosthighlycorrelated(X, 10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dfcb8b8daa25f709bdc2dc235c48ce4ca72c3827"},"cell_type":"markdown","source":"## Standardising Variables\nIf you want to compare different variables that have different units, are very different variances, it is a good idea to\nfirst standardise the variables"},{"metadata":{"trusted":true,"_uuid":"78c6b00155624e5fdcab015ec386f74214bb7b5e"},"cell_type":"code","source":"standardisedX = scale(X)\nstandardisedX = pd.DataFrame(standardisedX, index=X.index, columns=X.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dd4e051d63200d199154a3efc673b51c5c2f4cb2"},"cell_type":"code","source":"standardisedX.apply(np.mean)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"24aad5ccb1b57e01f04a6eb0138e44e61e4cac5f"},"cell_type":"code","source":"standardisedX.apply(np.std)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"60dc04c5ccaab3ce7dabbd4e4a7107403cfc71aa"},"cell_type":"markdown","source":"## Principal Component Analysis\nThe purpose of principal component analysis is to find the best low-dimensional representation of the variation in a\nmultivariate data set.\n\nTo carry out a principal component analysis (PCA) on a multivariate data set, the first step is often to standardise the\nvariables under study using the scale() function"},{"metadata":{"trusted":true,"_uuid":"ac6275300b0c2cb57a6a19af6bf7d9322eb7bdfa"},"cell_type":"code","source":"pca = PCA().fit(standardisedX)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"28fd2bbdfaf5b345bce7bc521e9f3ecabdff11e1"},"cell_type":"code","source":"def pca_summary(pca, standardised_data, out=True):\n    names = [\"PC\"+str(i) for i in range(1, len(pca.explained_variance_ratio_)+1)]\n    a = list(np.std(pca.transform(standardised_data), axis=0))\n    b = list(pca.explained_variance_ratio_)\n    c = [np.sum(pca.explained_variance_ratio_[:i]) for i in range(1, len(pca.explained_variance_ratio_)+1)]\n    columns = pd.MultiIndex.from_tuples([(\"sdev\", \"Standard deviation\"), (\"varprop\", \"Proportion of Variance\"), (\"cumprop\", \"Cumulative Proportion\")])\n    summary = pd.DataFrame(list(zip(a, b, c)), index=names, columns=columns)\n    if out:\n        print(\"Importance of components:\")\n        display(summary)\n    return summary","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cffdad047b4546a6d9873895720c92fd7c69ce04"},"cell_type":"code","source":"summary = pca_summary(pca, standardisedX)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d208ad66fc564d0def441c2ff13c89d99cc691ad"},"cell_type":"code","source":"summary.sdev","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8c4cb6d38b0fde9379dd1df82e40e86193e34a5f"},"cell_type":"code","source":"np.sum(summary.sdev**2)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"40eda03de7dcfb15a0056466b0e81beaedb408e2"},"cell_type":"markdown","source":"### How Many Principal Components to Retain"},{"metadata":{"trusted":true,"_uuid":"1fd27fc18e6518dcad0a351bdd7461d6c454af80"},"cell_type":"code","source":"def screeplot(pca, standardised_values):\n    y = np.std(pca.transform(standardised_values), axis=0)**2\n    x = np.arange(len(y)) + 1\n    plt.plot(x, y, \"o-\")\n    plt.xticks(x, [\"Comp.\"+str(i) for i in x], rotation=60)\n    plt.ylabel(\"Variance\")\n    plt.show()\nscreeplot(pca, standardisedX)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3b9a7666c7fd561ab468d8b176ed70e02ff4686f"},"cell_type":"markdown","source":"The most obvious change in slope in the scree plot occurs at component 4, which is the “elbow” of the scree plot.\nTherefore, it cound be argued based on the basis of the scree plot that the first three components should be retained.\nAnother way of deciding how many components to retain is to use **Kaiser’s criterion**: that we should only retain principal\ncomponents for which the variance is above 1 ."},{"metadata":{"trusted":true,"_uuid":"a7618baf9a2a03401babe2973c32d2b5174d8aee"},"cell_type":"code","source":"summary.sdev**2","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"121a9230c347fe4294f0cc59c27bf4eb7a147d90"},"cell_type":"markdown","source":"### Loadings for the Principal Components"},{"metadata":{"_uuid":"dfb96083ba3da1f6b97ed212f8b285c7ea565ab4"},"cell_type":"markdown","source":"The loadings for the principal components are stored in a named element components_ of the variable returned by\nPCA().fit()."},{"metadata":{"trusted":true,"_uuid":"1c6b1e4622601cc4c80122ac001f68c9366a0b32"},"cell_type":"code","source":"pca.components_[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2b5eb1e61fd4210c2068c7b87977ceecefdf94ab"},"cell_type":"code","source":"np.sum(pca.components_[0]**2)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c42e307167e6bd8e9a14124b3dd49ba816390cf8"},"cell_type":"markdown","source":"To calculate the values of the first principal component, we can define our own function to calculate a principal component given the loadings and the input variables' values:\n"},{"metadata":{"trusted":true,"_uuid":"753ba2dc921fcea1794f1007828f5d45789e7896"},"cell_type":"code","source":"def calcpc(variables, loadings):\n    # find the number of samples in the data set and the number of variables\n    numsamples, numvariables = variables.shape\n    # make a vector to store the component\n    pc = np.zeros(numsamples)\n    # calculate the value of the component for each sample\n    for i in range(numsamples):\n        valuei = 0\n        for j in range(numvariables):\n            valueij = variables.iloc[i, j]\n            loadingj = loadings[j]\n            valuei = valuei + (valueij * loadingj)\n        pc[i] = valuei\n    return pc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"533c2f300c5816f6cbfe0a9de30e13d8c2dd04d2"},"cell_type":"code","source":"calcpc(standardisedX, pca.components_[0])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0fa7a86980a671255a9ea2ea27c464c08406f735"},"cell_type":"markdown","source":"In fact, the values of the first principal component are computed with the following, so we can compare those values to the ones that we calculated, and they should agree:\n"},{"metadata":{"trusted":true,"_uuid":"0f49b67c142c3347c373786ea3e570d35397c4d6"},"cell_type":"code","source":"pca.transform(standardisedX)[:, 0]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7c82561cdc0e171137b96cb337e6db04975e4211"},"cell_type":"markdown","source":"Similarly, we can obtain the loadings for the second principal component by typing:"},{"metadata":{"trusted":true,"_uuid":"d10963b686d3bb87f7ca36ce37a074bd6ffff7c2"},"cell_type":"code","source":"pca.components_[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8b77afa9f005a10186f2206e1a04bef396958803"},"cell_type":"code","source":"np.sum(pca.components_[1]**2)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"589fd81d3b9eec230fe025e7dbae297dd75ba9da"},"cell_type":"markdown","source":"### Scatterplots of the Principal Components\nThe values of the principal components can be computed by the transform() (or fit_transform()) method of the PCA class. It returns a matrix with the principal components, where the first column in the matrix contains the first principal component, the second column the second component, and so on."},{"metadata":{"trusted":true,"_uuid":"6fc2f3211c406a2d572b9ff31338ddeeaefa90b3"},"cell_type":"code","source":"def pca_scatter(pca, standardised_values, classifs):\n    foo = pca.transform(standardised_values)\n    bar = pd.DataFrame(list(zip(foo[:, 0], foo[:, 1], classifs)), columns=[\"PC1\", \"PC2\", \"Class\"])\n    sns.lmplot(\"PC1\", \"PC2\", bar, hue=\"Class\", fit_reg=False)\n\npca_scatter(pca, standardisedX, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cc32b26a7344fec5b719ebe17886f0527394b077"},"cell_type":"code","source":"printMeanAndSdByGroup(standardisedX, y)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2fcc9e13d86489bf80051d761edd6ba85be1b1fa"},"cell_type":"markdown","source":"## Linear Discriminant Analysis\nThe purpose of principal component analysis is to find the best low-dimensional representation of the variation in a\nmultivariate data set."},{"metadata":{"_uuid":"6625d6ac705d896a0d1e7f4e4d94dae14abc15ab"},"cell_type":"markdown","source":"The purpose of linear discriminant analysis (LDA) is to find the linear combinations of the original variables that gives the best possible separation between the groups in data set. **Linear discriminant analysis** is also known as **canonical discriminant analysis**, or **simply discriminant analysis**."},{"metadata":{"trusted":true,"_uuid":"2b227dbc9a8241d8ab0ef19292ddd0af1bb34ac2"},"cell_type":"code","source":"lda = LinearDiscriminantAnalysis().fit(X, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"12bb93b5105b4615ea9c075e1f352127e64cd7ac"},"cell_type":"code","source":"def pretty_scalings(lda, X, out=False):\n    ret = pd.DataFrame(lda.scalings_, index=X.columns, columns=[\"LD\"+str(i+1) for i in range(lda.scalings_.shape[1])])\n    if out:\n        print(\"Coefficients of linear discriminants:\")\n        display(ret)\n    return ret\n\npretty_scalings_ = pretty_scalings(lda, X, out=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ac14f0450fc2d7502ea4e7865f24e4bd8fd8106c"},"cell_type":"code","source":"lda.scalings_[:, 0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cdd9014f2218ea14c14be52c009b0c396f9a456f"},"cell_type":"code","source":"pretty_scalings_.LD1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"50db6bbdf4dd05f316914963a1121c1a56253c88"},"cell_type":"code","source":"pretty_scalings_.LD2","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"428c67b077289fa13eb9ef4ae783108fae33a8ae"},"cell_type":"markdown","source":"To calculate the values of the first discriminant function, we can define our own function calclda():"},{"metadata":{"trusted":true,"_uuid":"6b81aa47ffecbd9cc2a901ee1734feb93ad71ca3"},"cell_type":"code","source":"def calclda(variables, loadings):\n    # find the number of samples in the data set and the number of variables\n    numsamples, numvariables = variables.shape\n    # make a vector to store the discriminant function\n    ld = np.zeros(numsamples)\n    # calculate the value of the discriminant function for each sample\n    for i in range(numsamples):\n        valuei = 0\n        for j in range(numvariables):\n            valueij = variables.iloc[i, j]\n            loadingj = loadings[j]\n            valuei = valuei + (valueij * loadingj)\n        ld[i] = valuei\n    # standardise the discriminant function so that its mean value is 0:\n    ld = scale(ld, with_std=False)\n    return ld","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"17bd40a986b526989643ad149474126efdda3739"},"cell_type":"code","source":"calclda(X, lda.scalings_[:, 0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"accab2d8bfc4eef34ecf52f81be324f27149c46e"},"cell_type":"code","source":"lda.fit_transform(X, y)[:, 0]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"24fdaf70bc9797551555e3e8acaf27ad442bbb8c"},"cell_type":"markdown","source":"It doesn’t matter whether the input variables for linear discriminant analysis are standardised or not, unlike for principal components analysis in which it is often necessary to standardise the input variables. However, using standardised variables in linear discriminant analysis makes it easier to interpret the loadings in a linear discriminant function."},{"metadata":{"trusted":true,"_uuid":"2d80e528f6d8f310265cc4d743326aa547343e9f"},"cell_type":"code","source":"def groupStandardise(variables, groupvariable):\n    # find the number of samples in the data set and the number of variables\n    numsamples, numvariables = variables.shape\n    # find the variable names\n    variablenames = variables.columns\n    # calculate the group-standardised version of each variable\n    variables_new = pd.DataFrame()\n    for i in range(numvariables):\n        variable_name = variablenames[i]\n        variablei = variables[variable_name]\n        variablei_Vw = calcWithinGroupsVariance(variablei, groupvariable)\n        variablei_mean = np.mean(variablei)\n        variablei_new = (variablei - variablei_mean)/(np.sqrt(variablei_Vw))\n        variables_new[variable_name] = variablei_new\n    return variables_new","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"49727b51d0835c83bea28e0a93e7c8648c20239a"},"cell_type":"code","source":"groupstandardisedX = groupStandardise(X, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b820251177246a311c56ffb0158e1cb2a325382a"},"cell_type":"code","source":"lda2 = LinearDiscriminantAnalysis().fit(groupstandardisedX, y)\npretty_scalings(lda2, groupstandardisedX)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"097f85ad6199ff110f860f1c4681bac582c6a88b"},"cell_type":"code","source":"lda.fit_transform(X, y)[:, 0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ee5ff5636fee7cbba4aa2afc0e99e989cf474bd8"},"cell_type":"code","source":"lda2.fit_transform(groupstandardisedX, y)[:, 0]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3c9960dd689c453970517b308d6438b6e5ab5477"},"cell_type":"markdown","source":"### Separation Achieved by the Discriminant Functions\n\nTo calculate the separation achieved by each discriminant function, we first need to calculate the value of each discriminant function, by substituting the values of the variables into the linear combination for the discriminant function "},{"metadata":{"trusted":true,"_uuid":"42d4e3d3a1f2115aecbba3070eb1bc5b8b3eff4c"},"cell_type":"code","source":"def rpredict(lda, X, y, out=False):\n    ret = {\"class\": lda.predict(X),\n           \"posterior\": pd.DataFrame(lda.predict_proba(X), columns=lda.classes_)}\n    ret[\"x\"] = pd.DataFrame(lda.fit_transform(X, y))\n    ret[\"x\"].columns = [\"LD\"+str(i+1) for i in range(ret[\"x\"].shape[1])]\n    if out:\n        print(\"class\")\n        print(ret[\"class\"])\n        print()\n        print(\"posterior\")\n        print(ret[\"posterior\"])\n        print()\n        print(\"x\")\n        print(ret[\"x\"])\n    return ret\n\nlda_values = rpredict(lda, standardisedX, y, True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e18dfbb40cf1c78e58d96c2fe665302fab4df8c7"},"cell_type":"markdown","source":"The returned variable has a named element x which is a matrix containing the linear discriminant functions: the first column of x contains the first discriminant function, the second column of x contains the second discriminant function, and so on "},{"metadata":{"trusted":true,"_uuid":"c170f307a2966f1b7792928a8b0b58d48756d82d"},"cell_type":"code","source":"calcSeparations(lda_values[\"x\"], y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"73746e25107394551d9ad97b02489196fc843bd0"},"cell_type":"code","source":"def proportion_of_trace(lda):\n    ret = pd.DataFrame([round(i, 4) for i in lda.explained_variance_ratio_ if round(i, 4) > 0], columns=[\"ExplainedVariance\"])\n    ret.index = [\"LD\"+str(i+1) for i in range(ret.shape[0])]\n    ret = ret.transpose()\n    print(\"Proportion of trace:\")\n    print(ret.to_string(index=False))\n    return ret\n\nproportion_of_trace(LinearDiscriminantAnalysis(solver=\"eigen\").fit(X, y))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c42ca66d0d1f4475ec56530c0deb6ce5eff24649"},"cell_type":"markdown","source":"\n### A Stacked Histogram of the LDA Values\n\nA nice way of displaying the results of a linear discriminant analysis (LDA) is to make a stacked histogram of the values of the discriminant function for the samples from different groups."},{"metadata":{"trusted":true,"_uuid":"8a957adecc94a8634c7ff2e17d3e12c5ba36e27f"},"cell_type":"code","source":"def ldahist(data, g, sep=False):\n    xmin = np.trunc(np.min(data)) - 1\n    xmax = np.trunc(np.max(data)) + 1\n    ncol = len(set(g))\n    binwidth = 0.5\n    bins=np.arange(xmin, xmax + binwidth, binwidth)\n    if sep:\n        fig, axl = plt.subplots(ncol, 1, sharey=True, sharex=True)\n    else:\n        fig, axl = plt.subplots(1, 1, sharey=True, sharex=True)\n        axl = [axl]*ncol\n    for ax, (group, gdata) in zip(axl, data.groupby(g)):\n        sns.distplot(gdata.values, bins, ax=ax, label=\"group \"+str(group))\n        ax.set_xlim([xmin, xmax])\n        if sep:\n            ax.set_xlabel(\"group\"+str(group))\n        else:\n            ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n    plt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"177720d05324b23eb762c5c89a79779d6b292522"},"cell_type":"code","source":"ldahist(lda_values[\"x\"].LD1, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"370df61d160c9a0ef18fd33ad3914ed6f2edfb83"},"cell_type":"code","source":"ldahist(lda_values[\"x\"].LD2, y)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"50eedae868dd6aa8f21939278eeca4bd2361aa10"},"cell_type":"markdown","source":"### Scatterplots of the Discriminant Functions"},{"metadata":{"trusted":true,"_uuid":"d90251dd1eca00dad53722c8d83d1f4258a56875"},"cell_type":"code","source":"sns.lmplot(\"LD1\", \"LD2\", lda_values[\"x\"].join(y), hue=\"V1\", fit_reg=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"afd9e44e8c032b215f57da5a6bf8451d69fe7e12"},"cell_type":"markdown","source":"### Allocation Rules and Misclassification Rate"},{"metadata":{"trusted":true,"_uuid":"8d4f489d486a0cb486fd97a8d5abdfea3aa14b9a"},"cell_type":"code","source":"printMeanAndSdByGroup(lda_values[\"x\"], y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9d586464b149bc38f0878ecec8e6722273a63410"},"cell_type":"code","source":"def calcAllocationRuleAccuracy(ldavalue, groupvariable, cutoffpoints):\n    # find out how many values the group variable can take\n    levels = sorted(set((groupvariable)))\n    numlevels = len(levels)\n    confusion_matrix = []\n    # calculate the number of true positives and false negatives for each group\n    for i, leveli in enumerate(levels):\n        levelidata = ldavalue[groupvariable==leveli]\n        row = []\n        # see how many of the samples from this group are classified in each group\n        for j, levelj in enumerate(levels):\n            if j == 0:\n                cutoff1 = cutoffpoints[0]\n                cutoff2 = \"NA\"\n                results = (levelidata <= cutoff1).value_counts()\n            elif j == numlevels-1:\n                cutoff1 = cutoffpoints[numlevels-2]\n                cutoff2 = \"NA\"\n                results = (levelidata > cutoff1).value_counts()\n            else:\n                cutoff1 = cutoffpoints[j-1]\n                cutoff2 = cutoffpoints[j]\n                results = ((levelidata > cutoff1) & (levelidata <= cutoff2)).value_counts()\n            try:\n                trues = results[True]\n            except KeyError:\n                trues = 0\n            print(\"Number of samples of group\", leveli, \"classified as group\", levelj, \":\", trues, \"(cutoffs:\", cutoff1, \",\", cutoff2, \")\")\n            row.append(trues)\n        confusion_matrix.append(row)\n    return confusion_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9424a92f305f08ea6e5a74381fce4f044178764e"},"cell_type":"code","source":"confusion_matrix = calcAllocationRuleAccuracy(lda_values[\"x\"].iloc[:, 0], y, [-1.751107, 2.122505])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f5466cbb0873ae81aa7da7369083ac67f0eac312"},"cell_type":"code","source":"def webprint_confusion_matrix(confusion_matrix, classes_names):\n    display(pd.DataFrame(confusion_matrix, index=[\"Is group \"+str(i) for i in classes_names], columns=[\"Allocated to group \"+str(i) for i in classes_names]))\n\nwebprint_confusion_matrix(confusion_matrix, lda.classes_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"037a01758691c631e2de912ce9e61191c1dcb8de"},"cell_type":"code","source":"import sklearn.metrics as metrics\n\ndef lda_classify(v, levels, cutoffpoints):\n    for level, cutoff in zip(reversed(levels), reversed(cutoffpoints)):\n        if v > cutoff: return level\n    return levels[0]\n    \ny_pred = lda_values[\"x\"].iloc[:, 0].apply(lda_classify, args=(lda.classes_, [-1.751107, 2.122505],)).values\ny_true = y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8cdbb7dc35287d03c1a525d7c1a070fca6d2e2de"},"cell_type":"code","source":"def plot_confusion_matrix(cm, target_names, title='Confusion matrix', cmap=plt.cm.Blues):\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(target_names))\n    plt.xticks(tick_marks, target_names, rotation=45)\n    plt.yticks(tick_marks, target_names)\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n\nprint(metrics.classification_report(y_true, y_pred))\ncm = metrics.confusion_matrix(y_true, y_pred)\nwebprint_confusion_matrix(cm, lda.classes_)\ncm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\nplot_confusion_matrix(cm_normalized, lda.classes_, title='Normalized confusion matrix')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1d6d56d843393940ec77304b5b773bdbd34168f3"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}