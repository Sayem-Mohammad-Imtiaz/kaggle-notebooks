{"cells":[{"metadata":{},"cell_type":"markdown","source":"If you like the content please upvote the kernel. Ask for anydoubt and tell if I have done anything wrong. "},{"metadata":{},"cell_type":"markdown","source":"Before starting, I would like to give an overview of how to structure any machine learning project.\n\n**Preprocess and load data**- As we have already discussed data is the key for the working of neural network and we need to process it before feeding to the neural network. In this step, we will also visualize data which will help us to gain insight into the data.\n\n**Define model**- Now we need a neural network model. This means we need to specify the number of hidden layers in the neural network and their size, the input and output size.\n\n**Loss and optimizer**- Now we need to define the loss function according to our task. We also need to specify the optimizer to use with learning rate and other hyperparameters of the optimizer.\n\n**Fit model**- This is the training step of the neural network. Here we need to define the number of epochs for which we need to train the neural network.\n\nAfter fitting model, we can test it on test data to check whether the case of overfitting. We can save the weights of the model and use it later whenever required."},{"metadata":{},"cell_type":"markdown","source":"The dataset consists of 20 features and we need to predict the price range in which phone lies. These ranges are divided into 4 classes.\n"},{"metadata":{},"cell_type":"markdown","source":"# Data preprocessing"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n#dataset import\ndataset = pd.read_csv('../input/train.csv')\ndataset.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#name of columns\ndataset.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This code as discussed in python module will make two arrays X and y.  X will contain features and y will contain classes."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Changing pandas dataframe to numpy array\nX = dataset.iloc[:,:20].values\ny = dataset.iloc[:,20:21].values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Normalization of dataset**\n\nThe next step is used to normalize the data. Normalization is a technique used to change the values of an array to a common scale, without distorting differences in the ranges of values. It is an important step and you can check the difference in accuracies on our dataset by removing this step. It is mainly required in case the dataset features vary a lot as in our case the value of battery power is in the 1000's and clock speed is less than 3. So if we feed unnormalized data to the neural network, the gradients will change differently for every column and thus the learning will oscillate. Study further from this link."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Normalizing the data\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX = sc.fit_transform(X)\nprint('Normalized data:')\nprint(X[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**One hot encoding**\n\nNext step is to one hot encode the classes. One hot encoding is a process to convert integer classes into binary values. Consider an example, let's say there are 3 classes in our dataset namely 1,2 and 3. Now we cannot directly feed this to neural network so we convert it in the form:<br>\n1- 1 0 0<br>\n2- 0 1 0<br>\n3- 0 0 1\n\nNow there is one unique binary value for the class. The new array formed will be of shape (n, number of classes), where n is the number of samples in our dataset. We can do this using simple function by sklearn:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\nohe = OneHotEncoder()\ny = ohe.fit_transform(y).toarray()\nprint('One hot encoded array:')\nprint(y[0:5])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now our dataset is processed and ready to feed in the neural network.\n\nGenerally, it is better to split data into training and testing data. Training data is the data on which we will train our neural network. Test data is used to check our trained neural network. This data is totally new for our neural network and if the neural network performs well on this dataset, it shows that there is no overfitting. Read more about this [here](https://medium.com/r/?url=https%3A%2F%2Ftowardsdatascience.com%2Ftrain-test-split-and-cross-validation-in-python-80b61beca4b6)."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This will split our dataset into training and testing. Training data will have 90% samples and test data will have 10% samples. This is specified by the test_size argument.\n\nNow we are done with the boring part and let's build a neural network."},{"metadata":{},"cell_type":"markdown","source":"# Model\nKeras is a simple tool for constructing a neural network. It is a high-level framework based on tensorflow, theano or cntk backends.\nIn our dataset, the input is of 20 values and output is of 4 values. So the input and output layer is of 20 and 4 dimensions respectively."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Dependencies\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\n\n# Neural network\nmodel = Sequential()\nmodel.add(Dense(16, input_dim=20, activation='relu'))\nmodel.add(Dense(12, activation='relu'))\nmodel.add(Dense(4, activation='softmax'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In our neural network, we are using two hidden layers of 16 and 12 dimension.\nNow I will explain the code line by line.\n\n**Sequential** specifies to keras that we are creating model sequentially and the output of each layer we add is input to the next layer we specify.\n\n**model.add** is used to add a layer to our neural network. We need to specify as an argument what type of layer we want. The **Dense** is used to specify the fully connected layer. The arguments of Dense are output dimension which is 16 in the first case, input dimension which is 20 for input dimension and the activation function to be used which is relu in this case. The second layer is similar, we dont need to specify input dimension as we have defined the model to be sequential so keras will automatically consider input dimension to be same as the output of last layer i.e 16. In the third layer(output layer) the output dimension is 4(number of classes). Now as we have discussed earlier, the output layer takes different activation functions and for the case of multiclass classification, it is softmax."},{"metadata":{},"cell_type":"markdown","source":"Now we need to specify the loss function and the optimizer. It is done using compile function in keras."},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here loss is cross entropy loss . Categorical_crossentropy specifies that we have multiple classes. The optimizer is Adam. Metrics is used to specify the way we want to judge the performance of our neural network. Here we have specified it to accuracy.\n\nNow we are done with building a neural network and we will train it."},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(X_train, y_train, epochs=100, batch_size=64)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we need to specify the input data-> X_train, labels-> y_train, number of epochs(iterations), and batch size. It returns the history of model training. History consists of model accuracy and losses after each epoch. We will visualize it later.\n\nUsually, the dataset is very big and we cannot fit complete data at once so we use batch size. This divides our data into batches each of size equal to batch_size. Now only this number of samples will be loaded into memory and processed. Once we are done with one batch it is flushed from memory and the next batch will be processed."},{"metadata":{},"cell_type":"markdown","source":"# Test model"},{"metadata":{},"cell_type":"markdown","source":"Now we can check the model's performance on test data:"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = model.predict(X_test)\n#Converting predictions to label\npred = list()\nfor i in range(len(y_pred)):\n    pred.append(np.argmax(y_pred[i]))\n#Converting one hot encoded test label to label\ntest = list()\nfor i in range(len(y_test)):\n    test.append(np.argmax(y_test[i]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This step is inverse one hot encoding process. We will get integer labels using this step. We can predict on test data using a simple method of keras, model.predict(). It will take the test data as input and will return the prediction outputs as softmax."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\na = accuracy_score(pred,test)\nprint('Accuracy is:', a*100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Validation data:"},{"metadata":{},"cell_type":"markdown","source":"We can use test data as validation data and can check the accuracies after every epoch. This will give us an insight into overfitting at the time of training only and we can take steps before the completion of all epochs. We can do this by changing fit function as:"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Dependencies\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\n#Re initialized to delete trained weights\n# Neural network\nmodel = Sequential()\nmodel.add(Dense(16, input_dim=20, activation='relu'))\nmodel.add(Dense(12, activation='relu'))\nmodel.add(Dense(4, activation='softmax'))\n\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(X_train, y_train,validation_data = (X_test,y_test), epochs=100, batch_size=64)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Visualizing training"},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(history.history['loss']) \nplt.plot(history.history['val_loss']) \nplt.title('Model loss') \nplt.ylabel('Loss') \nplt.xlabel('Epoch') \nplt.legend(['Train', 'Test'], loc='upper left') \nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}