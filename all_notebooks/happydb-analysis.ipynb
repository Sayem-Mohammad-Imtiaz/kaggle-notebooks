{"cells":[{"source":"# Introduction\nIn this notebook, I'm trying to do some preliminary analysis on the HappyDB dataset, in the meanwhile introducing some basic and useful analytical tools/libraries.\n\nThis notebook consists of three parts:\n* **Data preparation and basic analyses.** The main tasks are loading the data, doing a word count and drawing a word cloud.\n* **Entity extraction.** Here I first do a straighforward entity extraction of seasons with Python, followed by a more sophisticated entity extraction of purchased products. For the second task, I'll introduce a convenient yet powerful entity extraction system called **[Koko](http://pykoko.readthedocs.io/en/latest/)**.  \n* **Classification of genders.** I make this task a binary classification problem, and shows how to use the logistic regression model of **[scikit-learn](http://scikit-learn.org/stable/)** to finish the task.\n\nLet's get started!  \nWe first need to import a few necessary packages:","cell_type":"markdown","metadata":{"_cell_guid":"cfd25d97-656c-45ec-9af0-fb84a776a90f","_uuid":"2e9ec1b6a605e819ad76d40d22994bacb6daaa4c"}},{"outputs":[],"source":"%matplotlib inline\nimport re\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport nltk\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom wordcloud import WordCloud, ImageColorGenerator","cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"_cell_guid":"6f8b93d0-bafc-460c-b6e6-1c7a69679a4a","_uuid":"7f00e3415433ec1b3e94eb3cfd542d41c3c8416d"}},{"source":"# 1. Data preparation and basic analyses\nHappyDB consists of statement of happy moments by people around the world.  \nWe can load the data and take a quick look inside.  \n## 1.1 Load HappyDB","cell_type":"markdown","metadata":{"_cell_guid":"17648e97-711d-4782-ba70-a17b44dc0817","_uuid":"bc25a2ee265ca97eb84f48d3a041598b56ce0287"}},{"outputs":[],"source":"hm_data = pd.read_csv('../input/happydb/cleaned_hm.csv')\nhm_data.head()","cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e906f7dd-7285-41d3-95ca-5d53d6048357","_uuid":"4f6597121623d147c0b99e7a77874253d1fae56a"}},{"source":"The dataset is composed of happy moments from two reflection_period, 24 hours and 3 months.  \nLet's take a quick look at the data with reflection_period of 3 months.","cell_type":"markdown","metadata":{"_cell_guid":"de08537a-ffef-489e-9d1a-595768168827","_uuid":"0f2d9dcb45c870c8fa6a57b5d7b83be0588d2674"}},{"outputs":[],"source":"hm_data.loc[hm_data[\"reflection_period\"]=='3m'].head()","cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"65a2ebc6-ed87-455d-84da-3d8f375b74a2","_uuid":"072198bf05cdae9e15465e951264e4b54f33a942"}},{"source":"Of all the columns, \"cleaned_hm\" is of particular interest as it contains happy moments with proper cleaning (e.g., typo correction).  \nThe rest of the analysis will be mostly based on \"cleaned_hm\".  \n## 1.2 Word count\nTo get a general idea of the cleaned happy moments, we can perform a statistical analysis based on the number of words.","cell_type":"markdown","metadata":{"_cell_guid":"8efea2e0-3a7a-47aa-92b8-e709ce3837ba","_uuid":"26c0a24d093d6b388d567330a4c52778c5f81566"}},{"outputs":[],"source":"df_hm = hm_data[hm_data['cleaned_hm'].notnull()]\nlen_count = df_hm['cleaned_hm'].apply(lambda x: len(x.split()))\nlen_count.describe()","cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a9a52cbb-d90f-4e14-b6e2-af6e5877e5c5","_uuid":"22b47067e4edc3b14931ce580e5648aabc6648fa"}},{"source":"Looks like most of the happy moments are short sentences, as expected!  \nSome happy moments even have only two words.","cell_type":"markdown","metadata":{"_cell_guid":"7a6e7d4a-e7db-4398-aeeb-96c671f86ef6","_uuid":"33229e748cee05f75abaf28c7140694b0063e036"}},{"outputs":[],"source":"df_hm[df_hm['cleaned_hm'].apply(lambda x: len(x.split()))==2].head()","cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"55f4cf5b-729a-4e0c-b981-16d8bbe9b243","_uuid":"bf603cb7447133eafae903ca1265fbe1b56c84fd"}},{"source":"So what's the distribution of the word count?","cell_type":"markdown","metadata":{"_cell_guid":"358bdefe-1bd7-4d94-9d6c-6388173e0069","_uuid":"06cb40f1ac84cdd10713cec068a57de243548019"}},{"outputs":[],"source":"length_order = [\"0-4\", \"5-9\", \"10-14\", \"15-19\", \"20-24\", \"25-29\", \"30-34\", \"35-39\", \\\n                \"40-44\", \"45-49\", \">=50\"]\nlength_category = len_count.apply(lambda x: length_order[min(10, int(x/5))])\nlength_counts = pd.DataFrame(length_category.value_counts()).reset_index()\nlength_counts.columns = ['word numbers', '# of moments']\n\nsns.barplot(x='word numbers', y='# of moments', data=length_counts, order=length_order)","cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0847cd8e-3e43-429d-9a86-529389bbe2ac","_uuid":"59ee8ddd1095698eeb6576d035ab5e3766bcd746"}},{"source":"Most of the happy moments are between five words and twenty words.  \n## 1.3 Word frequency\nI'm also curious about what words people mention most in their happy moments.  \nA good tool that could help us in this case is a word cloud.","cell_type":"markdown","metadata":{"_cell_guid":"ee36cced-c6e1-4b47-a93d-de509c189847","_uuid":"0c13ab641be541664288b84a501e081af63b8671"}},{"outputs":[],"source":"text = ' '.join(df_hm['cleaned_hm'].tolist())\ntext = text.lower()\nwordcloud = WordCloud(background_color=\"white\", height=2700, width=3600).generate(text)\nplt.figure( figsize=(14,8) )\nplt.imshow(wordcloud.recolor(colormap=plt.get_cmap('Set2')), interpolation='bilinear')\nplt.axis(\"off\")","cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0f2481bf-a3af-4444-ac32-4cbcd35afec1","_uuid":"6c5a8d87e021204478499cec7621e51bac58b492"}},{"source":"It seems that some words appear more frequently, such as \"friend\", \"family\" and \"daughter\".  \nThere are also noise words that are not very informative, such as \"happy\", \"yesterday\" and \"today\".  \nLet's clean the word cloud by removing these noise.","cell_type":"markdown","metadata":{"_cell_guid":"5e7e35b9-88d6-4f6b-a04f-9d8eb8d526f8","_uuid":"562835eb9f82334aaea529faee6d1fd9dd5650c4"}},{"outputs":[],"source":"LIMIT_WORDS = ['happy', 'day', 'got', 'went', 'today', 'made', 'one', 'two', 'time', 'last', 'first', 'going', 'getting', 'took', 'found', 'lot', 'really', 'saw', 'see', 'month', 'week', 'day', 'yesterday', 'year', 'ago', 'now', 'still', 'since', 'something', 'great', 'good', 'long', 'thing', 'toi', 'without', 'yesteri', '2s', 'toand', 'ing']\n\ntext = ' '.join(df_hm['cleaned_hm'].tolist())\ntext = text.lower()\nfor w in LIMIT_WORDS:\n    text = text.replace(' ' + w, '')\n    text = text.replace(w + ' ', '')\nwordcloud = WordCloud(background_color=\"white\", height=2700, width=3600).generate(text)\nplt.figure( figsize=(14,8) )\nplt.imshow(wordcloud.recolor(colormap=plt.get_cmap('Set2')), interpolation='bilinear')\nplt.axis(\"off\")","cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"15b3de64-68ed-4f82-a911-d815636fd0c0","_uuid":"606ad0ceb8df9a5330f5e9c2cdb8668361245df0"}},{"source":"Great. Now it's quite clear that \"work\", \"friend\" and \"son\" are the most frequent words in the dataset.\n# 2. Entity Extraction on HappyDB\nNow let's try to dig deeper into the dataset by focusing on sub-domains of daily life.  \nFor example, what seasons make people happiest? What purchased products make people happiest?\n## 2.1 Happiest seasons","cell_type":"markdown","metadata":{"_cell_guid":"dae52999-8a66-4a01-a47f-ed49a57854ed","_uuid":"fd238d4299a3ab7f49e3d21553b949908ae8c09f"}},{"outputs":[],"source":"seasons = ['Spring', 'Summer', 'Fall', 'Winter']\n\n# Check each moment, and increase the count for the mentioned season\nseason_dic = dict((x,0) for x in seasons)\ntokens_hm = df_hm['cleaned_hm'].apply(lambda x: x.split())\nfor _, value in tokens_hm.iteritems():\n    for word in value:\n        if word in seasons:\n            season_dic[word] += 1\n            \nseason_dic","cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d620470d-2c6a-4a4e-96c7-88cea78f817f","_uuid":"7c1bd319a4c35be1e53147c35b345a22f1f6f76f"}},{"source":"'Spring' is clearly the winner, as it's mentioned significantly more frequently than other seasons.  \nI also tried to search for 'Autumn', but there are zero mentions. Any interested reader could try yourself as well.\n## 2.2 Products that make people happy\nIt's interesting to understand what products that people buy make them happy.  \nWe can model the task as an entity extraction problem.  \nAnd I'll use an entity extraction system called [Koko](http://pykoko.readthedocs.io/en/latest/) for this task.\n### 2.2.1 Retrieval of happy moments\nKoko takes texts as input. Let's first retrieve the cleaned happy moments (i.e., 'cleaned_hm' column) from HappyDB and put them into one text file.  \nFor efficiency, I'll only use 1/8 of the happy moments for entity extraction.","cell_type":"markdown","metadata":{"_cell_guid":"c192744f-d00c-45de-a1d1-159a61fbf99e","_uuid":"d0e6e5f065f6e269c609dd90f5b51a988664175c"}},{"outputs":[],"source":"# Read the happyDB sample file\nwith open('happydb.txt', 'w') as ofile:\n    len = int(df_hm.shape[0] / 8)\n    for i in range(0, len - 1):\n        ofile.write(df_hm['cleaned_hm'].iloc[i] + '\\n')\n        \nprint(\"Happy moments are retrieved!\")","cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"cfcc6bab-d743-452e-9d87-007655b87991","_uuid":"5efc639d5e7383819c80f417a90ef9ad4dadce25"}},{"source":"### 2.2.2 Extraction of purchased products\nNow let's write a Koko query for product extraction as follows:","cell_type":"markdown","metadata":{"_cell_guid":"c9b81eb9-bf8a-42dd-974b-1f60b3622e52","_uuid":"7f33c9e1577129e0763b5382ace3314ca75a01c5"}},{"outputs":[],"source":"with open('../input/kokosamples/purchase_v1.koko', 'r') as file:\n    print(file.read())","cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"_cell_guid":"59e2b990-17c7-46a0-8c7e-cad630ea9fd4","_uuid":"abb3cdd7f75d3f70002869879dcc03e182658693"}},{"source":"This query tells Koko to extract noun phrases 'x' from HappyDB if \"x\" is preceded by either \"buy\" or \"purchase\".  \n\nThe weight in each \"if\" condition (e.g., {0.1} for (\"buy\" x)) represents the importance of the pattern specified in the condition.  \nAny appearance of an entity in happy moments that matches the pattern is considered a piece of evidence.  \nAnd each such piece of evidence would increment the entity's score by the condition's weight.\n\nFor example, if there's a happy moment \"I buy a car\", this moment is considered as evidence for \"a car\" based on the first condition, and 0.1 is added to \"a car\"'s score.  \nIn Koko, the score of an entity is at most 1.\n\nFinally, we can specify threshold in Koko queries.  \nOnly entities scoring higher than the thresold would be returned in the results.  \nFor simplicity, I put zero as thresold here, which shows all entities that have at least one piece of evidence in happy moments.  \n\n**Let's run the Koko query now to see the results.**  \nHere I use [spaCy](https://spacy.io/) as the nlp processor for happy moments.\nKoko could leverage spaCy's APIs for entity extraction.  \nThe extracted entities could be further matched against the conditions in the Koko query to get scored, ranked and filtered.\n\nSpaCy is not the only option. We can also use Koko's default parser or [Google NLP API](https://cloud.google.com/natural-language/) as well.","cell_type":"markdown","metadata":{"_cell_guid":"3557aacf-35ae-441b-a16a-5a1db908fc6b","_uuid":"a49f7a2829112079f514a192fd98f36b4d5f8893"}},{"outputs":[],"source":"import koko\nimport spacy\n\nkoko.run('../input/kokosamples/purchase_v1.koko', doc_parser='spacy')","cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5ed66308-827b-423d-84ba-50f70cb5c1c1","_uuid":"efbd372a24adeb56bddf2ca7b4335a382a7b0c7b"}},{"source":"On one hand, the results give us useful information.  \nPeople are in general happy when they purchase a car, followed by costumes, tickets and phones etc.  \nOn the other hand, there's much noise in the results.  We have pronouns such as \"me\", \"it\" and \"my husband\" as well.  \nThis's not surprising though. We often say \"buy somebody something\" in daily life.  \n\n**Fortunately, Koko allows us to specify exclusion rules to get rid of the unwanted noise.**  \nHere's an updated query excluding pronouns.  \nAlso, to make results more concise, *I reset the thresold to 0.2 in all subsequent queries*.","cell_type":"markdown","metadata":{"_cell_guid":"83e03142-915d-44d1-889f-a842541c0edf","_uuid":"84c47f6767b7c57d0d923baa049ceebe5986f39f"}},{"outputs":[],"source":"with open('../input/kokosamples/purchase_v2.koko', 'r') as file:\n    print(file.read())","cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c6906ca7-b969-48b6-8aeb-ef9db7648d4f","_uuid":"7540ae0bd64a4976f1972b877498495c21028a68"}},{"source":"Let's run it again.","cell_type":"markdown","metadata":{"_cell_guid":"be5840b4-1c43-4655-9eb8-c2b892669c20","_uuid":"148e16975a26136efc63c27e2819e329561f14ab"}},{"outputs":[],"source":"koko.run('../input/kokosamples/purchase_v2.koko', doc_parser='spacy')","cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"bb343d9a-e016-41cd-8ca0-f29a838e842e","_uuid":"9c751c0e7ffd014b8c029300e53663f3b0819916"}},{"source":"The results look much cleaner this time.  \nIf we take a closer look at the query, we might wonder if the two conditions listed cover all the purchasing behavior?  \nThe answer may be no.","cell_type":"markdown","metadata":{"_cell_guid":"0adf33fd-87bf-46b7-8a2b-aad3e330aeb5","_uuid":"16f731df64756de25b36cdae731e03475a7c8cc7"}},{"outputs":[],"source":"# Select all happy moments used for entity extraction that contain 'purchased'.\ndf_hmee = df_hm[:int(df_hm.shape[0]/8)]\ndf_purchase = df_hmee[df_hmee['cleaned_hm'].apply(lambda x: x.find('purchased') != -1)]\n\nprint(\"Number of happy moments containing 'purchased': {}\".format(df_purchase.shape[0]))","cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"afa4e36e-b850-4816-a5bb-9c6d73afb435","_uuid":"274f4587106ac08b6bcd9eefafef1b2cb5ce60fa"}},{"source":"Well, looks like we missed quite a few happy moments that contain 'purchased'.\nHowever, enumerating all purchase-related keywords we can think of in a Koko query is quite tedious.  \n\nKoko provides a handy feature for solving this problem, which is called a descriptor.  \nTo use a descriptor, we only need to write *one* condition, and put a tilde \"~\" between the keyword in the condition and the entity we're trying to extract.  ","cell_type":"markdown","metadata":{"_cell_guid":"3fc74392-4895-4926-ab59-fec51996b18b","_uuid":"4edec826e6678e6684d3829948cb1aafc5e41633"}},{"outputs":[],"source":"with open('../input/kokosamples/purchase_v3.koko', 'r') as file:\n    print(file.read())","cell_type":"code","execution_count":null,"metadata":{"scrolled":false,"_cell_guid":"34f1bd35-f78a-493f-87b8-e8abdcf1568e","_uuid":"0ab82f41795aa10345264c2a2a361713b0291987"}},{"source":"During execution, Koko would automatically expand the keyword to all related words it can find in a word embedding file.   \nOf course, we need to supply the embedding file ourselves.","cell_type":"markdown","metadata":{"_cell_guid":"1deb8bfc-620f-4bc0-8748-90daf5ed811c","_uuid":"950093fb75bf411399f2cdaf04c1b09e1c1ed346"}},{"outputs":[],"source":"embedding_doc = \"../input/glove840b/glove.840B.300d.txt\"\nkoko.run('../input/kokosamples/purchase_v3.koko', doc_parser='spacy', embedding_file=embedding_doc)","cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"_cell_guid":"34421db4-e1b5-46a5-b971-a1783553e4d0","_uuid":"537591b28d145fbc82652d3e110e6bc10b0b80e6"}},{"source":"Now the results capture more purchasing experience in the dataset.  \nFor example, now we have 12 instances of \"a new car\" compared to 4 instances in the previous example. \n\nFrom the given results, we may conclude that purchasing of expensive products, such as \"car\", \"smartphone\" or \"bike\", tend to make people happier.\n## 3. A logistic regression classifier for gender\nThe HappyDB dataset comes with demographic info with author's information.  \nIn this part, let's try to train a classifier to identify the gender of each happy moment's author.  \nFor simplicity, I make this problem a binary classification problem. And I'll use logistic regression to approach the task.\n### 3.1 Data preparation\nFirst we load the demographic data.","cell_type":"markdown","metadata":{"_cell_guid":"3d78497c-11fb-415e-907e-3d738171dfd6","_uuid":"edd446e56df4e7c8e08190da3d52f43a3e49ad5e"}},{"outputs":[],"source":"demo_data = pd.read_csv('../input/happydb/demographic.csv')\n\ndemo_data.head()","cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1d2067c-49ed-403d-a1f3-283464e2727e","_uuid":"06c897722cdcd3c92178db06e1e9b3cbaf8aa282"}},{"source":"We then join the happy moments and the demographic data -- based on the \"wid\" column -- \nto identify the author's gender for each happy moment.","cell_type":"markdown","metadata":{"_cell_guid":"77dcce3b-ada1-478d-9812-46a63e8a6263","_uuid":"4b5dba3f9e0822b95cdc7605a096f6252229e645"}},{"outputs":[],"source":"merge_data = pd.merge(hm_data, demo_data, on='wid')\ngender_data = merge_data[['cleaned_hm', 'gender']]\n\ngender_data.head()","cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2a87efa7-7a39-4ba3-addd-0450c8652aa8","_uuid":"ed2a3acc51ac19645520386b044a17278cb012b9"}},{"source":"We can take a quick look at the frequency distribution for gender.","cell_type":"markdown","metadata":{"_cell_guid":"26e7c9fe-5a6d-4352-a4b6-e4aab39849dc","_uuid":"1458b7dacc9cf951db48fdb9e85d642545638991"}},{"outputs":[],"source":"gender_data.gender.value_counts().plot(kind='bar')","cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"76f2e0e7-a6ef-4fd7-9f84-34cec6302b64","_uuid":"a20cbce866484203e93dbb35da2c77bbb03b0fa2"}},{"source":"Since this's a binary classification problem, we only consider gender of male or female.  \nLet's clean the data set to retain happy moments whose gender is either male or female.","cell_type":"markdown","metadata":{"_cell_guid":"dc5f9f25-0c1b-49c9-aab7-773ed0c64932","_uuid":"bbb14556ccef17c744eea7fbce96753e16a41890"}},{"outputs":[],"source":"gender_bin_data = gender_data[(gender_data['gender'] == 'm') | (gender_data['gender'] == 'f')]\n\nprint(\"Happy moments written by male/female: {}\".format(gender_bin_data['cleaned_hm'].size))","cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f7562a4e-8c0f-4293-b0b6-acd1f8f4edf8","_uuid":"174cd6a5e7ff0ccb81798d5f5919bd44e9fbe02b"}},{"source":"To prepare the data for classification task, we need to convert the representation of male and female into numbers.","cell_type":"markdown","metadata":{"_cell_guid":"9d8f2ef0-5547-4a60-8b43-7e4024702ee6","_uuid":"33f9811467e6081a338fb925eb182f840262e79f"}},{"outputs":[],"source":"gender_bin_data = gender_bin_data.assign(gender_bin=(np.where(gender_bin_data['gender']=='m', 1, 0)))\n\ngender_bin_data.head()","cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b70ab433-cd9a-4d61-8178-229733cb7c58","_uuid":"a05e267fe396a9dbf9c4a3d578903e455a61da0a"}},{"source":"We use the first 70% happy moments as the training data, with the rest 30% as test data.","cell_type":"markdown","metadata":{"_cell_guid":"b9de4669-2223-48c2-b548-346c2c45401c","_uuid":"81c2ab451e2b3f7a60be18230d34f2822d3c886b"}},{"outputs":[],"source":"hm_size = gender_bin_data['cleaned_hm'].size\nnum_train_hm = int(0.7 * gender_bin_data['cleaned_hm'].size)\n\ntrain_hm = gender_bin_data.iloc[0:num_train_hm]\ntest_hm = gender_bin_data.iloc[num_train_hm:hm_size]\ntest_hm = test_hm.reset_index(drop=True)\n\ntest_hm.head()","cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"500cf31c-67cc-4025-8897-efc049e85957","_uuid":"9f9da5a59c07a83fa4bbd0c90900c1b054e71a8d"}},{"source":"We further clean up the texts, to remove numbers and punctuation.","cell_type":"markdown","metadata":{"_cell_guid":"99f5fbca-7751-4ec7-9ca5-901bc913fee5","_uuid":"05278f1b12907bff79cc19d448d2392ee74a3120"}},{"outputs":[],"source":"def clean_up_texts(hm_data):\n    prepro_hm = []\n    stops = set(stopwords.words(\"english\"))\n    for i in range(0, hm_data['cleaned_hm'].size):\n        # Remove non-english words, including punctuations and numbers\n        letters = re.sub(\"[^a-zA-Z]\", \" \", hm_data.iloc[i]['cleaned_hm'])\n\n        # Convert all words to lower case\n        lower_words = letters.lower()\n\n        # Tokenize the sentences\n        tokens = lower_words.split()\n\n        # Reconstruct the processed tokens into a string\n        prepro_string = \" \".join(tokens)\n\n        prepro_hm.append(prepro_string)\n        \n    return prepro_hm\n    \nprepro_train = clean_up_texts(train_hm)\nprepro_test = clean_up_texts(test_hm)\nprint(\"Texts cleaned up! \\n\")","cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b711cc25-5d0c-4e21-bf72-091e935d8eee","_uuid":"1658a586a9f66bf23d6edbb81e15360ebcf4b784"}},{"source":"Let's take a peek at the cleaned data:","cell_type":"markdown","metadata":{"_cell_guid":"26b6b421-08f1-4bf6-aff5-1bf730a8b239","_uuid":"097b106b8db675cca7c7002a2d55467f37c5c934"}},{"outputs":[],"source":"prepro_train[:10]","cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c0e84607-3c76-4e93-81fe-513ce2037b61","_uuid":"0defb69f561cd169b58b9a8909344b88cd3be766"}},{"source":"### 3.2 Feature selection\nThe next step is to select proper features for training and testing.  \n\nHere I start with the simpliest model: *bag-of-words model*.  \nThe bag-of-words model tries to create a dictionary based on the input strings.  \nWith the dictionary, each sentence can then be modeled as a vector representing the frequency of each word.\nI use scikit-learn here to build features for bag-of-words model.","cell_type":"markdown","metadata":{"_cell_guid":"940e6e4a-9afb-475e-84fb-1f51e67c2ea6","_uuid":"7ac968d6f2521269b4e97851989bc908b71e373f"}},{"outputs":[],"source":"vectorizer = CountVectorizer()\nfeatures_train_hm = vectorizer.fit_transform(prepro_train)\ntrain_array_hm = features_train_hm.toarray()\n\nprint(\"Dimension of the training data: {}\".format(train_array_hm.shape))","cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f88c0e02-c3de-47e8-88a9-f0437b8549b5","_uuid":"8134ccf6f493c40ccdb0ea81a55a0a88effe1622"}},{"source":"There are 20737 distinct words in the dataset!  \nA quick look at the features (i.e., words) we use:","cell_type":"markdown","metadata":{"_cell_guid":"7e4804d4-24cf-4477-9a36-26413b617c49","_uuid":"c30fcc7df527fa9c514522a1474738bd46556a0c"}},{"outputs":[],"source":"vocab = vectorizer.get_feature_names()\n\nvocab[:20]","cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"274f145d-e447-42ad-89e6-3950cf6bc76b","_uuid":"e7c884fc1ead5ba9a3d43c4188dbdad6358ad86f"}},{"source":"Now we can train a logistic regression model with the extracted features.  \n### 3.3 Training of the logistic regression classifier\nLet's train the logistic regression model:","cell_type":"markdown","metadata":{"_cell_guid":"a2d3e1e4-97b2-4ebf-a2ce-fb097a0217b8","_uuid":"9752308ab21446c4465dfcfd20737911f5bd0ee3"}},{"outputs":[],"source":"from sklearn.linear_model import LogisticRegression\n\nlogi_model = LogisticRegression()\nlogi_model.fit(train_array_hm, train_hm['gender_bin'])\n\nlogi_model.score(train_array_hm, train_hm['gender_bin'])","cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9ee48bcd-0cd3-42dc-ae03-189a0dd53112","_uuid":"5ef3c8e6cfe9318175a39df14c25237e909d07c5"}},{"source":"The training accuracy is 76%, a reasonable result.  \nWe can see which words are the most influential by looking at the coefficients of the logistic model.","cell_type":"markdown","metadata":{"_cell_guid":"721dee0d-0efb-4560-9d24-d12cb71b8387","_uuid":"0706d226abb308d617ce5bdf41de49b186dfadc6"}},{"outputs":[],"source":"feature_names = vocab\ncoefficients = logi_model.coef_.tolist()[0]\nweight_df = pd.DataFrame({'Word': feature_names,\n                          'Coeff': coefficients})\nweight_df = weight_df.sort_values(['Coeff', 'Word'], ascending=[0, 1])\nweight_df.head(n=10)","cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c71a500f-9bfb-4658-b11f-b9d2edee9e62","_uuid":"d56180d2217941503945e530c417464ca7321ab2"}},{"source":"These are the words with the biggest positive coefficients.  \nIn other words, these are the words in the dataset that strongly suggest male.  \n\nWell, I can understand that mentioning of \"wife\", \"gf\" or \"smoking\" suggests a male author.  \nBut why \"seattle\" and \"stone\"? This is definitely worth more investigation (maybe in another notebook).  \n\nLet's also take a look at the least influential words:","cell_type":"markdown","metadata":{"_cell_guid":"b13a0a54-50c1-4803-923e-74bdb9e25cec","_uuid":"84f632af3ad9eafae468e4e8f4fa17e190516306"}},{"outputs":[],"source":"weight_df.tail(n=10)","cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5a23d643-4e96-4fd3-bdd9-65e3e10fc66f","_uuid":"03fdfe1817c92989d03c1f729b0333c98f3f772e"}},{"source":"These are the words that tend to make classifier predict 0, which means female.  \nMost of the words make sense to me, but again, the appearance of \"mth\" is kind of surprising.\n### 3.4 Evaluation of the classifier\nTo evaluate the performance of the classifier, let's use the trained model to make prediction on the test data.","cell_type":"markdown","metadata":{"_cell_guid":"dc945840-fc9c-429f-9569-287b785822d7","_uuid":"1de5edf50a503f782c6420bf3375ee8e83624bb9"}},{"outputs":[],"source":"features_test_hm = vectorizer.transform(prepro_test)\ntest_array_hm = features_test_hm.toarray()\n\nprint(\"Dimension of the test data: {}\".format(test_array_hm.shape))","cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"35355a09-d438-4978-8314-562a8e6ee21b","_uuid":"8c9f33dff25a9c7881842d5dd002b391ae0c109a"}},{"outputs":[],"source":"predictions = logi_model.predict(test_array_hm)","cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"_cell_guid":"f1937a80-aca8-4bdb-92b4-c33a7bcd1cd3","_uuid":"73e3063f5cbeb61259a2d247ce93dc6b3af860de"}},{"outputs":[],"source":"from sklearn import metrics\n\nprint(metrics.accuracy_score(test_hm['gender_bin'], predictions))","cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e75483d9-5595-4ad5-881b-b5b5a3785b4b","_uuid":"45706677d591f198598b0048afd17c5e8e9cdcf3"}},{"source":"We have 64% precision on the test data. Not bad!  \nFor anyone who wants to learn more about bag-of-words model, I recommend this [tutorial](https://www.kaggle.com/c/word2vec-nlp-tutorial#part-1-for-beginners-bag-of-words).\n# 4. Conclusion\nThis notebook is just a preliminary exploration of the dataset.  I'm sure there's much more to dig there.\n\nSome of the future directions I can think of now is:\n- Extraction of locations that make people feel happy.\n- More sophisticated features with logistic regression model (e.g., ngram model).\n- More sophisticated models (e.g., a neural network).\n","cell_type":"markdown","metadata":{"_cell_guid":"97be5ea2-a3cc-438e-aea6-55bfc5129de8","_uuid":"1371fe629ff013b1d90f998004ee8f10461654dd"}},{"outputs":[],"source":"","cell_type":"code","execution_count":null,"metadata":{"collapsed":true}}],"nbformat":4,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"name":"python","file_extension":".py","version":"3.6.4","mimetype":"text/x-python","pygments_lexer":"ipython3","nbconvert_exporter":"python"}},"nbformat_minor":1}