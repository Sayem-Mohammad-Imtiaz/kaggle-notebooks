{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Intro\n\nHello everyone, in this notebook I will try to show how we can work on a multidimensional data and make it less dimensional with **Feature Selection**. First we will try to get information about the feature we have with **data visualization**, then we will try to establish the optimum model with less features with **feature selection**. I try to learn new things every day and improve myself. I may have mistakes, if you come across, please mention it in the comments. Your feedback is very important to me."},{"metadata":{},"cell_type":"markdown","source":"Notebook content is as follows:\n\n - [Base Model](#1)\n - [EDA](#2)\n - [Visualization](#3)\n - [Feature Selections and Random Forest Classification](#4)\n - [Conclusion](#5)\n "},{"metadata":{"id":"TqZd3cq-eDiI","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set()\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"id":"ofGO8RmxefNF","trusted":true},"cell_type":"code","source":"breast = pd.read_csv(\"/kaggle/input/breast-cancer-wisconsin-data/data.csv\")\n\ndf = breast.copy()","execution_count":null,"outputs":[]},{"metadata":{"id":"22dfxUKx7pGf"},"cell_type":"markdown","source":"I will drop unnecessary columns"},{"metadata":{"id":"5k9AUH9R4qdt","trusted":true},"cell_type":"code","source":"df.drop([\"Unnamed: 32\",\"id\"],axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"id":"6pXt1bwmfNdk"},"cell_type":"markdown","source":"<a id=\"1\"></a> <br>\n# **Base Model**"},{"metadata":{"id":"4rnt5iAwfLvJ","trusted":true},"cell_type":"code","source":"X = df.drop(\"diagnosis\", axis = 1)\ny = df[\"diagnosis\"]\n\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, stratify = y, random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{"id":"5_OguCXIfpX3","trusted":true},"cell_type":"code","source":"logreg = LogisticRegression().fit(X_train,y_train)\ny_pred = logreg.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"id":"mzNsx7tPf0xL","outputId":"5e3f037a-f902-45ff-aeaf-e52ac94b822e","trusted":true},"cell_type":"code","source":"print(classification_report(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"id":"GjSccVR9gOtK","outputId":"307abf7f-5dbe-499e-d2c2-b716a4e51762","trusted":true},"cell_type":"code","source":"plt.figure(figsize=(3,3))\nsns.heatmap(confusion_matrix(y_test, y_pred), annot=True, annot_kws={\"fontsize\":20}, fmt='d', cbar=False, cmap='PuBu')\nplt.xlabel('Predicted Values')\nplt.ylabel('Actual Values')\nplt.title('Base Model', color='navy', fontsize=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"IkvOfOXB7Gou"},"cell_type":"markdown","source":"The model we built using all the features,base model, gives an **accuracy score of 0.94**. Now let's try to make Feature Elimination and see if we can get better results. But first, let's visualize the data and gain an understanding of the features."},{"metadata":{"id":"FhdnrkTVKh_J"},"cell_type":"markdown","source":"<a id=\"2\"></a> <br>\n# **EDA**"},{"metadata":{"id":"dPuY075FTcdo","outputId":"855f0577-04b9-4e69-e6fc-759c853a6ab9","trusted":true},"cell_type":"code","source":"y = df.diagnosis\nx = df.drop(\"diagnosis\", axis = 1)\n\nx.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"svyy2HFuJtfL","outputId":"5b2269bc-9e03-45b9-bd41-677bd2b94e86","trusted":true},"cell_type":"code","source":"ax = sns.countplot(x = \"diagnosis\", data = df)\nplt.show()\n\nb,m = df.diagnosis.value_counts()\nprint(\"Number of Benign: \", b)\nprint(\"Number of Malignant: \", m)\n","execution_count":null,"outputs":[]},{"metadata":{"id":"GpYsNjL8LHVY","outputId":"86f507af-8202-496e-df4f-8b17729b4627","trusted":true},"cell_type":"code","source":"x.describe().T","execution_count":null,"outputs":[]},{"metadata":{"id":"mFv1e4t4URZA"},"cell_type":"markdown","source":"<a id=\"3\"></a> <br>\n# **Visualization**\n\n- Before visualization, we need to do **normalization** or **standardization**. Because differences between values of features are very high to observe on plot.\n- I plot features in 3 group and each group includes 10 features to observe better."},{"metadata":{"id":"29mMjzhBnAdJ"},"cell_type":"markdown","source":"## **Violin Plots**"},{"metadata":{"id":"dqKhjg_iOIym","outputId":"f046b407-5f00-4650-c415-bf4ddf75d3c1","trusted":true},"cell_type":"code","source":"data_dia = y\ndata = x\n\n#standardization\ndata_n2 = (data-data.mean()) / data.std()\n\ndata = pd.concat([y,data_n2.iloc[:,0:10]],axis=1)\ndata = pd.melt(data,id_vars=\"diagnosis\",\n                    var_name=\"features\",\n                    value_name='value')\n\n# plotting the violin plot\nplt.figure(figsize=(10,10))\nsns.violinplot(x=\"features\", y=\"value\", hue=\"diagnosis\", data=data,split=True, inner=\"quart\")\nplt.xticks(rotation=90)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"Ugsr5P-uY6ck"},"cell_type":"markdown","source":"In **texture_mean** feature, \n  - median of the **Malignant** and **Benign** looks like separated, so it can be good for classification.<br>\n  \nHowever, in **fractal_dimension_mean** feature, \n  - median of the **Malignant** and **Benign** does not looks like separated so it does not gives good information for classification."},{"metadata":{"id":"OZPIanAhX4DR","outputId":"74dced30-a82c-4ea9-e88d-e707e9dc4b66","trusted":true},"cell_type":"code","source":"# second ten part\ndata = pd.concat([y,data_n2.iloc[:,10:20]],axis=1)\ndata = pd.melt(data,id_vars=\"diagnosis\",\n                    var_name=\"features\",\n                    value_name='value')\n\n# plotting the violin plot\nplt.figure(figsize=(10,10))\nsns.violinplot(x=\"features\", y=\"value\", hue=\"diagnosis\", data=data,split=True, inner=\"quart\")\nplt.xticks(rotation=90)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"P70tes2_ZQZK","outputId":"0fa0ad1d-e1f9-4a39-b8f4-879e7f618890","trusted":true},"cell_type":"code","source":"# last ten part\ndata = pd.concat([y,data_n2.iloc[:,20:31]],axis=1)\ndata = pd.melt(data,id_vars=\"diagnosis\",\n                    var_name=\"features\",\n                    value_name='value')\n\n# plotting the violin plot\nplt.figure(figsize=(10,10))\nsns.violinplot(x=\"features\", y=\"value\", hue=\"diagnosis\", data=data,split=True, inner=\"quart\")\nplt.xticks(rotation=90)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"IfqBfKL2a3lg"},"cell_type":"markdown","source":"- when we looked up all graphs, some features look very similar, some features look very different. The different ones will help during the classification..\n\n- In order to compare two features deeper, lets use **joint plot** for some similar and some different features."},{"metadata":{"id":"iaI_rg2yelja"},"cell_type":"markdown","source":"## **Joint-Plot**\n### **Concavity Worst - Concave Points Worst**"},{"metadata":{"id":"nOBkQI1qe2AZ","outputId":"61b19309-9000-403d-fa02-a64fbac2db09","trusted":true},"cell_type":"code","source":"sns.jointplot(x ='concavity_worst', y = 'concave points_worst', \n              data = x, kind=\"reg\", color=\"#D81B60\");","execution_count":null,"outputs":[]},{"metadata":{"id":"mlM8NTdkfl08"},"cell_type":"markdown","source":"### **Symmetry Worst - Fractal Dimension Worst**"},{"metadata":{"id":"5WH8ZP3Ueujh","outputId":"9bafc00a-50de-4835-836b-c1629aba74b2","trusted":true},"cell_type":"code","source":"sns.jointplot(x = \"symmetry_worst\", y = \"fractal_dimension_worst\",\n              data = x, kind = \"reg\",color=\"#D81B60\" );","execution_count":null,"outputs":[]},{"metadata":{"id":"xAAo8zrdfhbV","outputId":"a931b98e-ab10-4fa8-d729-3644528ce0f2","trusted":true},"cell_type":"code","source":"sns.jointplot(x ='concavity_se', y = 'concave points_se', \n              data = x, kind=\"reg\", color=\"#D81B60\");","execution_count":null,"outputs":[]},{"metadata":{"id":"mTw6g6EkgI9o"},"cell_type":"markdown","source":"* When we looked at Violin plots, the distribution of some features was very close to each other, we looked closer to the relationship between them to better observe this. And we have seen that there is a linear relationship between them."},{"metadata":{"id":"bKpBGdMmg75C"},"cell_type":"markdown","source":"## **PairGrid**\n**Instead of looking at them individually, let's look at three or four properties in one graph.**"},{"metadata":{"id":"cd_sM-_Nf3aR","outputId":"b8139c38-4e53-4d23-b85f-12e35db8e21c","trusted":true},"cell_type":"code","source":"df = x.loc[:,['radius_worst','perimeter_worst','area_worst']]\ng = sns.PairGrid(df, diag_sharey=False)\ng.map_lower(sns.kdeplot, cmap=\"Blues_d\")\ng.map_upper(sns.scatterplot)\ng.map_diag(sns.kdeplot, lw=3)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"Aw_LMMN3hJvk","outputId":"b429d663-644c-424f-8d74-83635a95237e","trusted":true},"cell_type":"code","source":"df = x.loc[:,['perimeter_mean','area_mean','area_worst',\"concavity_mean\"]]\ng = sns.PairGrid(df, diag_sharey=False)\ng.map_lower(sns.kdeplot, cmap=\"Blues_d\")\ng.map_upper(sns.scatterplot)\ng.map_diag(sns.kdeplot, lw=3)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"hFK-XxfljC-C"},"cell_type":"markdown","source":"### **Swarm Plots**"},{"metadata":{"id":"M_I7fkKCicMf","outputId":"701b43a0-7f8e-4141-ecae-5d67f512705a","trusted":true},"cell_type":"code","source":"data = pd.concat([y,data_n2.iloc[:,0:10]],axis=1)\ndata = pd.melt(data,id_vars=\"diagnosis\",\n                    var_name=\"features\",\n                    value_name='value')\nplt.figure(figsize=(8,8))\nsns.swarmplot(x=\"features\", y=\"value\", hue=\"diagnosis\", data=data)\n\nplt.xticks(rotation=90)","execution_count":null,"outputs":[]},{"metadata":{"id":"HTA4d1rWlVTf"},"cell_type":"markdown","source":"- For example, we can easily see that **area mean** feature well separated from each other in terms of classification\n- Hovewer, **symmetry_mean** looks like malignant and benign are mixed so it is hard to classfy while using this feature."},{"metadata":{"id":"AYsKG6URjQ27","outputId":"30a7c562-30d1-4b88-dc5c-626c2a481b1d","trusted":true},"cell_type":"code","source":"data = pd.concat([y,data_n2.iloc[:,10:20]],axis=1)\ndata = pd.melt(data,id_vars=\"diagnosis\",\n                    var_name=\"features\",\n                    value_name='value')\nplt.figure(figsize=(8,8))\nsns.swarmplot(x=\"features\", y=\"value\", hue=\"diagnosis\", data=data)\n\nplt.xticks(rotation=90)","execution_count":null,"outputs":[]},{"metadata":{"id":"0ipxY_PqmT4O"},"cell_type":"markdown","source":"- in here we can make same comment as before, **area_se** almost well separated from each other\n- but for example **fractal_dimension_se** not like that."},{"metadata":{"id":"RCN733JjmOmD","outputId":"34e274be-0de1-4934-b54d-7b7bb8e39b38","trusted":true},"cell_type":"code","source":"data = pd.concat([y,data_n2.iloc[:,20:31]],axis=1)\ndata = pd.melt(data,id_vars=\"diagnosis\",\n                    var_name=\"features\",\n                    value_name='value')\nplt.figure(figsize=(8,8))\nsns.swarmplot(x=\"features\", y=\"value\", hue=\"diagnosis\", data=data)\n\nplt.xticks(rotation=90)","execution_count":null,"outputs":[]},{"metadata":{"id":"EEj8XJT1sJK7"},"cell_type":"markdown","source":"<a id=\"4\"></a> <br>\n# **Feature Selection and Random Forest Classification**\n\nIn this part we will select feature with different methods that are feature selection with,\n-  **Correlation**, \n-  **Univariate Feature Selection**, \n\nAnd we will use **Random Forest Classification** in order to train our model and predict."},{"metadata":{"id":"8lz7zJYJn2XT"},"cell_type":"markdown","source":"## **1.Correlation Matrix and Random Forest C.**"},{"metadata":{"id":"26j9zKY7mqQ2","outputId":"51a5863f-3128-4485-cc66-48827d531d4d","trusted":true},"cell_type":"code","source":"corr = x.corr()\nmask = np.triu(np.ones_like(corr, dtype=np.bool))\nf, ax = plt.subplots(figsize=(20, 15))\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\nsns.heatmap(corr, annot=True,fmt='.2f',mask=mask, cmap=cmap, ax=ax);","execution_count":null,"outputs":[]},{"metadata":{"id":"Cv3A6nMCr612"},"cell_type":"markdown","source":"I will drop features that coefficients are high from 0.9"},{"metadata":{"id":"fF5RlFVKpvvj"},"cell_type":"markdown","source":"**Highly Correlated Features**\n\n-  compactness_mean, concavity_mean and concave points_mean then I choose **concavity_mean**\n-  radius_se, perimeter_se and area_se then I choose **area_se**\n-  radius_worst, perimeter_worst and area_worst then I choose **area_worst**\n-  compactness_worst, concavity_worst and concave points_worst then I choose **concavity_worst** \n-  compactness_se, concavity_se and concave points_se then I choose **concavity_se**\n-  texture_mean and texture_worst are correlated then I choose **texture_mean** \n-  area_worst and area_mean I choose **area_mean**"},{"metadata":{"id":"33YxfU7BpaAF","outputId":"3bb47031-71b9-4c98-8f30-b6e889d01a24","trusted":true},"cell_type":"code","source":"drop_list1 = ['perimeter_mean','radius_mean','compactness_mean','concave points_mean','radius_se','perimeter_se','radius_worst','perimeter_worst','compactness_worst','concave points_worst','compactness_se','concave points_se','texture_worst','area_worst']\nx1 = x.drop(drop_list1, axis = 1 )       \nx1.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"MWV2R8eVrjBH"},"cell_type":"markdown","source":"Now lets create again correlation matrix"},{"metadata":{"id":"Wf-y9xg8rfDA","outputId":"ddb0239b-bb98-4e8a-e807-e895b27cbe5a","trusted":true},"cell_type":"code","source":"corr = x1.corr()\nmask = np.triu(np.ones_like(corr, dtype=np.bool))\nf, ax = plt.subplots(figsize=(12, 6))\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\nsns.heatmap(corr, annot=True,fmt='.2f',mask=mask, cmap=cmap, ax=ax);","execution_count":null,"outputs":[]},{"metadata":{"id":"yELLBFKss3EM"},"cell_type":"markdown","source":"### **Random Forest Model**"},{"metadata":{"id":"QCMlhpM_rsu6","outputId":"1f989539-a0eb-41bf-a46a-3e8d40dd532b","trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import f1_score,confusion_matrix\nfrom sklearn.metrics import accuracy_score\n\n\nx_train, x_test, y_train, y_test = train_test_split(x1, y, test_size=0.3, random_state=42)\n\n#n_estimators=10 (default)\nclf_rf = RandomForestClassifier(random_state=43)      \nclr_rf = clf_rf.fit(x_train,y_train)\n\nac_score = accuracy_score(y_test,clf_rf.predict(x_test))\nprint('Accuracy is: ',ac_score)\n\ncnf_m = confusion_matrix(y_test,clf_rf.predict(x_test))\n\nplt.figure(figsize=(3,3))\nsns.heatmap(cnf_m, annot=True, annot_kws={\"fontsize\":20}, fmt='d', cbar=False, cmap='PuBu')\nplt.xlabel('Predicted Values')\nplt.ylabel('Actual Values')\nplt.title('Base Model', color='navy', fontsize=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"ZaIZQ316tZ75"},"cell_type":"markdown","source":"## **2.Univariate Feature Selection and Random Forest C.**\n- In univariate feature selection, we will use **SelectKBest** that removes all but the **k highest scoring** features.\n- In this method we need to choose how many features we will use.(k) I will try model one by one with \n  - k = 4\n  - k = 5 \n  - k = 6"},{"metadata":{"id":"TJZ2GnGauYje"},"cell_type":"markdown","source":"#### **k=4**"},{"metadata":{"id":"9T9OLhNltOJW","outputId":"48a6c1e1-9142-465d-8353-916b23220fcf","trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\n\n# find best scored 4 features\nselect_feature = SelectKBest(chi2, k=4).fit(x_train, y_train)\n\nprint('Score list:', select_feature.scores_)\nprint('Feature list:', x_train.columns)","execution_count":null,"outputs":[]},{"metadata":{"id":"wRJxCUbsvWTo"},"cell_type":"markdown","source":"According to score list we will choose best 4 as follows: \n  - **texture_mean**,\n  - **area_mean**,\n  - **area_se**,\n  - **concavity_worst**\n\nand we build new model"},{"metadata":{"id":"2lv1xgnvugmp","outputId":"8e64e606-5983-403c-cc31-78d2e01ab075","trusted":true},"cell_type":"code","source":"x_train_2 = select_feature.transform(x_train)\nx_test_2 = select_feature.transform(x_test)\n\n#random forest classifier with n_estimators=10 (default)\nclf_rf_2 = RandomForestClassifier()      \nclr_rf_2 = clf_rf_2.fit(x_train_2,y_train)\n\nac_2 = accuracy_score(y_test,clf_rf_2.predict(x_test_2))\nprint('Accuracy is: ',ac_2)\n\ncm_2 = confusion_matrix(y_test,clf_rf_2.predict(x_test_2))\n\n\nplt.figure(figsize=(3,3))\nsns.heatmap(cm_2, annot=True, annot_kws={\"fontsize\":20}, fmt='d', cbar=False, cmap='PuBu')\nplt.xlabel('Predicted Values')\nplt.ylabel('Actual Values')\nplt.title('Base Model', color='navy', fontsize=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"GOYCFuLxwvG8"},"cell_type":"markdown","source":"**It gave a better result than before, predicting 1s much better.**"},{"metadata":{"id":"r3X28hvcw6I9"},"cell_type":"markdown","source":"**k = 5**"},{"metadata":{"id":"YQQKxjz5wjRX","outputId":"afb9ed47-022b-4ef7-bc3c-16d3197d3fdd","trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\n\n# find best scored 5 features\nselect_feature = SelectKBest(chi2, k=5).fit(x_train, y_train)\n\nprint('Score list:', select_feature.scores_)\nprint('Feature list:', x_train.columns)","execution_count":null,"outputs":[]},{"metadata":{"id":"fdEAkjfaxAUQ","outputId":"e3c1e8aa-5db3-4df0-dd5a-1360bf5fd77f","trusted":true},"cell_type":"code","source":"x_train_3 = select_feature.transform(x_train)\nx_test_3 = select_feature.transform(x_test)\n\n#random forest classifier with n_estimators=10 (default)\nclf_rf_2 = RandomForestClassifier()      \nclr_rf_2 = clf_rf_2.fit(x_train_3,y_train)\n\nac_3 = accuracy_score(y_test,clf_rf_2.predict(x_test_3))\nprint('Accuracy is: ',ac_2)\n\ncm_3 = confusion_matrix(y_test,clf_rf_2.predict(x_test_3))\n\nplt.figure(figsize=(3,3))\nsns.heatmap(cm_3, annot=True, annot_kws={\"fontsize\":20}, fmt='d', cbar=False, cmap='PuBu')\nplt.xlabel('Predicted Values')\nplt.ylabel('Actual Values')\nplt.title('Base Model', color='navy', fontsize=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"YSl_dcGsxVC0"},"cell_type":"markdown","source":"**k = 6**"},{"metadata":{"id":"Q5eeubYcxITz","outputId":"4ec326cb-0ce7-4668-eedb-07dae0f8603b","trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\n\n# find best scored 6 features\nselect_feature = SelectKBest(chi2, k=6).fit(x_train, y_train)\n\nprint('Score list:', select_feature.scores_)\nprint('Feature list:', x_train.columns)","execution_count":null,"outputs":[]},{"metadata":{"id":"jMxMcjhlxbWT","outputId":"a5983748-9031-4b80-abd8-950c7cd15146","trusted":true},"cell_type":"code","source":"x_train_4 = select_feature.transform(x_train)\nx_test_4 = select_feature.transform(x_test)\n\n#random forest classifier with n_estimators=10 (default)\nclf_rf_3 = RandomForestClassifier()      \nclr_rf_3 = clf_rf_3.fit(x_train_4,y_train)\n\nac_4 = accuracy_score(y_test,clf_rf_3.predict(x_test_4))\nprint('Accuracy is: ',ac_2)\n\ncm_4 = confusion_matrix(y_test,clf_rf_3.predict(x_test_4))\n\nplt.figure(figsize=(3,3))\nsns.heatmap(cm_4, annot=True, annot_kws={\"fontsize\":20}, fmt='d', cbar=False, cmap='PuBu')\nplt.xlabel('Predicted Values')\nplt.ylabel('Actual Values')\nplt.title('Base Model', color='navy', fontsize=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"On5HwTygx02I"},"cell_type":"markdown","source":"- With **k = 4**, it gave a better result (0.96) than other k values, predicting 1's much better."},{"metadata":{"id":"r5glXKMA3YRE"},"cell_type":"markdown","source":"<a id=\"5\"></a> <br>\n# **Conclusion**\nIn short, in this notebook, I tried to show **Data Visualization** and **Feature Selection** techniques. While we had 33 features in the beginning, we reduced it to 4 using statistics and some algorithms.\n"},{"metadata":{"id":"2eHN7VuV3Xh0","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}