{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1.Loading Datasets\n\nWe'll be using the Kaggle Heart Disease UCI dataset as an example. You can find it here: https://www.kaggle.com/ronitf/heart-disease-uci\n\n* Manual loading (last resort)\n* `np.loadtxt`\n* `np.genfromtxt`\n* `pd.read_csv`\n* `pd.read*`\n* `pickle`"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport pickle\n\nfilename = \"/kaggle/input/heart-disease-uci/heart.csv\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## The best method - panda's read_csv\nHandles the most edge cases, datetime and file issues best."},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(filename)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Using numpy's loadtxt and genfromtxt\n\nIf you must. Notice it fails without extra arguments - its not as smart and we have to tell it what to do. Designed for loading in data saved using `np.savetxt`, not meant to be a robust loader."},{"metadata":{"trusted":true},"cell_type":"code","source":"data = np.loadtxt(filename, delimiter=\",\", skiprows=1)\nprint(data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = np.genfromtxt(filename, delimiter=\",\", dtype=None, names=True, encoding=\"utf-8-sig\")\nprint(data)\nprint(data.dtype)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Manual Loading\nFor completely weird file structures\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_file(filename):\n    with open(filename, encoding=\"utf-8-sig\") as f:\n        data, cols = [], []\n        for i, line in enumerate(f.read().splitlines()):\n            if i == 0:\n                cols += line.split(\",\")\n            else:\n                data.append([float(x) for x in line.split(\",\")])\n#         print(cols)\n#         print(data)\n        df = pd.DataFrame(data, columns=cols)\n    \n    return df\nload_file(filename).head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Pickles!\nSome danger using pickles as encoding changes. Use an industry standard like hd5 instead if you can. Note if you're working with dataframes, dont use python's `pickle`, pandas has their own implementation - `df.to_pickle` and `df.read_pickle`. Underlying algorithm is the same, but less code for you to type, and supports compression."},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_pickle(\"../input/heartdiseaseuci/heart.pkl\")\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Recap\n\n* Use pd.read_csv 99% of the time\n* Use pd.read_* for other cases (pd.read_excel, pd.read_pickle, etc)\n* If pd cant handle it, I doubt numpy can\n* If you use a manual function, save your data to a sensible format"},{"metadata":{},"cell_type":"markdown","source":"# 2.Numpy vs Pandas\n1. Pandas has a numpy core.\n2. Extra structure and tools, but sometimes you have to strip it away"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\ndf = pd.read_csv(\"/kaggle/input/heart-disease-uci/heart.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = df.to_numpy()\n# df.values # same\ndata","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"See this link for details on why not to use `.values`: https://pandas-docs.github.io/pandas-docs-travis/whatsnew/v0.24.0.html#accessing-the-values-in-a-series-or-index (https://bit.ly/2RrecCR for convenience)"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(data.dtype, data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data[0, 0] = 100\ndf.head()\n# Notice it doesnt modify the original data frame here","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# But we can still explicitly call copy if we wanted\ndf2 = df[[\"age\", \"sex\", \"cp\"]]\ndata2 = df2.to_numpy().copy()\ndata2[0, 0] = 100\ndf2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.age.to_numpy()[0] = 100\ndf\n# Here it DOES modify the original, because to_numpy hasnt needed to make a new array\n# so our reference is to the original, underlying data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Many functions you can do in both pandas and numpy\nprint(df['age'].mean(), df['age'].to_numpy().mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Some are pandas only\nprint(df['age'].quantile(0.5))\n# print(df['age'].to_numpy().quantile(0.5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# And some are numpy only\n# print(df['age'].reshape((3, -1)))\nprint(df['age'].to_numpy().reshape((3, -1)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Most of the time, better to keep things in DataFrame format, as you can do more. For some cases, you might need to swap to numpy format, and that's fine.\n\n### Recap:\n* Work with pandas as much as you can, more functionality\n* Sometimes you need to get the actual array, and use to_numpy()"},{"metadata":{},"cell_type":"markdown","source":"# 3.Creating DataFrames\n\nMany ways to do it!"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\ndata = np.random.random(size=(5, 3))\nprint(data)\n\n# Common 2D array and columns method\ndf = pd.DataFrame(data=data, columns=[\"A\", \"B\", \"C\"])\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# A dictionary of columns\ndf = pd.DataFrame(data={\"A\": [1, 2, 3], \"B\": [\"Sam\", \"Alex\", \"John\"]})\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Or a list of rows (ie tuples) with a dtype\ndtype = [(\"A\", np.int), (\"B\", (np.str, 20))]\ndata = np.array([(1, \"Sam\"), (2, \"Alex\"), (3, \"John\")], dtype=dtype)\ndf = pd.DataFrame(data)\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Or the dictionary based version of list of rows\ndata = [{\"A\": 1, \"B\": \"Sam\"}, {\"A\": 2, \"B\": \"Alex\"}, {\"A\": 3, \"B\": \"John\"}]\ndf = pd.DataFrame(data)\ndf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4.Saving and Serialising a dataframe\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\n# Lets make a new dataframe and save it out using various formats\ndf = pd.DataFrame(np.random.random(size=(100000, 4)), columns=[\"A\", \"B\", \"C\", \"D\"])\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.to_csv(\"save.csv\", index=False, float_format=\"%0.4f\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.to_pickle(\"save.pkl\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# pip install tables\ndf.to_hdf(\"save.hdf\", key=\"data\", format=\"table\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# pip install feather-format\ndf.to_feather(\"save.fth\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# If you want to get the timings you can see in the video, you'll need this extension:\n# https://jupyter-contrib-nbextensions.readthedocs.io/en/latest/nbextensions/execute_time/readme.html","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now this is a very easy test - its only numeric data. If we add strings and categorical data things can slow down a lot! Let's try this on mixed Astronaut data from Kaggle: https://www.kaggle.com/nasa/astronaut-yearbook"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"../input/astronaut-yearbook/astronauts.csv\")\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.to_csv(\"save.csv\", index=False, float_format=\"%0.4f\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.read_csv(\"./save.csv\");","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.to_pickle(\"save.pkl\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.read_pickle(\"./save.pkl\");","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.to_hdf(\"save.hdf\", key=\"data\", format=\"table\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.read_hdf(\"./save.hdf\");","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.to_feather(\"save.fth\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.read_feather(\"./save.fth\");","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%ls","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Recap\n\nIn terms of file size, HDF5 is the largest for this example. Everything else is approximately equal. For small data sizes, often csv is the easiest as its human readable. HDF5 is great for *loading* in huge amounts of data quickly. Pickle is faster than CSV, but not human readable.\n\nLots of options, don't get hung up on any of them. csv and pickle are easy and for most cases work fine."},{"metadata":{},"cell_type":"markdown","source":"# 5.Inspecting Data\n\nAstronaut data from Kaggle: https://www.kaggle.com/nasa/astronaut-yearbook"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\n\ndf = pd.read_csv(\"../input/astronaut-yearbook/astronauts.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# First two rows of the dataframe\ndf.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Last row of the dataframe\ndf.tail(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Three random but different rows in the dataframe\n# (set replace=True to allow them to potentially double up)\ndf.sample(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The type and number of non-null values for each column\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Basic stats on all numeric columns\ndf.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Shape of the dataframe (nrows, ncols)\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Correlation between all numeric columns\ndf.corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The number of each occurance for a series\ndf[\"Year\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# And a whole host of math functions can be invoked on the dataframe as whole, like so\ndf.max()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.min()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Recap\n* head\n* tail\n* sample\n* info\n* describe"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}