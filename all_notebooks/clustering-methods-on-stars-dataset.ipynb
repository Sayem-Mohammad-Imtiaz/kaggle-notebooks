{"cells":[{"metadata":{},"cell_type":"markdown","source":"**EXPLANATION**\n\nIn this kernel, I used 2 different clustering methods on Star Cluster dataset. They are K-Means and Hierarchical Clustering .\n\n**CONTENTS**\n\n**1. EDA**\n\nI am looking data in general and delete columns like \"id\".\n\n**2. Visualization**\n\n**3. K-Means**\n\n**4. Hierarchical Clustering**\n\n**5. Conclusion**"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"../input/c_0000.csv\",sep=\",\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**1. EDA**"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We filter \"id\" columns because id has no effect on relations.\ndata = data.loc[:,data.columns != 'id']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data2 = data.iloc[:,[0,3]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data2.head()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**2. Visaualization**"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(data2.x,data2.vx,color=\"green\")\nplt.xlabel(\"X positon of stars\")\nplt.ylabel(\"Velocity in X axis of stars\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**3. K-Means**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import  KMeans\nwcss = []\n\nfor k in range(1,15):\n    kmeans = KMeans(n_clusters=k)\n    kmeans.fit(data2)\n    wcss.append(kmeans.inertia_) # inertia means that find to value of wcss\n    \nplt.plot(range(1,15),wcss)\nplt.xlabel(\"number of k (cluster) value\")\nplt.ylabel(\"wcss\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It looks that 4 has an elbow point."},{"metadata":{"trusted":true},"cell_type":"code","source":"# we can take elbow as 4\nkmean2 = KMeans(n_clusters=4)\nclusters = kmean2.fit_predict(data2)\n\ndata2[\"label\"] = clusters\n\nplt.scatter(data2.x[data2.label == 0], data2.vx[data2.label == 0], color=\"red\")\nplt.scatter(data2.x[data2.label == 1], data2.vx[data2.label == 1], color=\"blue\")\nplt.scatter(data2.x[data2.label == 2], data2.vx[data2.label == 2], color=\"green\")\nplt.scatter(data2.x[data2.label == 3], data2.vx[data2.label == 3], color=\"purple\")\n\nplt.scatter(kmean2.cluster_centers_[:,0],kmean2.cluster_centers_[:,1], color=\"orange\") # scentroidler\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# inertia\ninertia_list = np.empty(8)\nfor i in range(1,8):\n    kmeans3 = KMeans(n_clusters=i)\n    kmeans3.fit(data2)\n    inertia_list[i] = kmeans.inertia_\nplt.plot(range(0,8),inertia_list,'-o')\nplt.xlabel('Number of cluster')\nplt.ylabel('Inertia')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data3 = data2.iloc[:,data2.columns != 'label'].head(1000)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" **4. Hierarchical Clustering**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# dendrogram\nfrom scipy.cluster.hierarchy import linkage, dendrogram\n\nmerg = linkage(data3, method=\"ward\") # scipy is an algorithm of hiyerarchal clusturing\ndendrogram(merg, leaf_rotation = 90)\nplt.xlabel(\"data points\")\nplt.ylabel(\"euclidean distance\")\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# HC\nfrom sklearn.cluster import AgglomerativeClustering\n\nhiyerartical_cluster = AgglomerativeClustering(n_clusters=4, affinity=\"euclidean\",linkage=\"ward\")\ncluster = hiyerartical_cluster.fit_predict(data3)\n\ndata3[\"label\"] = cluster","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(data3.x[data3.label == 0], data3.vx[data3.label == 0], color=\"red\")\nplt.scatter(data3.x[data3.label == 1], data3.vx[data3.label == 1], color=\"blue\")\nplt.scatter(data3.x[data3.label == 2], data3.vx[data3.label == 2], color=\"green\")\nplt.scatter(data3.x[data3.label == 3], data3.vx[data3.label == 3], color=\"purple\")\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**5. Conclusion**\n\nThere is a great difference between K-Means and Hierarchical Clustering methods as you see in graphics.\n"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}