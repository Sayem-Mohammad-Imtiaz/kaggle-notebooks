{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Introduction\n해당 쥬피터파일에서는 기존의 이론에서 공부했던 방법론들에 대해 살펴보는 시간을 가져보겠습니다. \n- Apriori Algorithm \n- FP Growth \n- TF-IDF\n- Word2Vec\n- KNN Neareast Algorithm \n- SGD\n- ALS ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import mlxtend\nimport sklearn\nimport pandas as pd\nimport numpy as np\nimport gensim\nimport implicit\nimport surprise","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1. Apriori 알고리즘","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from mlxtend.preprocessing import TransactionEncoder\n\ndata = np.array([\n    ['우유', '기저귀', '쥬스'],\n    ['양상추', '기저귀', '맥주'],\n    ['우유', '양상추', '기저귀', '맥주'],\n    ['양상추', '맥주']\n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"te = TransactionEncoder()\nte_ary = te.fit(data).transform(data)\ndf = pd.DataFrame(te_ary, columns=te.columns_)\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfrom mlxtend.frequent_patterns import apriori\n\napriori(df, min_support=0.5, use_colnames=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. FP-Growth 알고리즘","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data = np.array([\n    ['우유', '기저귀', '쥬스'],\n    ['양상추', '기저귀', '맥주'],\n    ['우유', '양상추', '기저귀', '맥주'],\n    ['양상추', '맥주']\n])","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"te = TransactionEncoder()\nte_ary = te.fit(data).transform(data)\ndf = pd.DataFrame(te_ary, columns=te.columns_)\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfrom mlxtend.frequent_patterns import fpgrowth\n\nfpgrowth(df, min_support=0.5, use_colnames=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. TF-IDF 알고리즘","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"docs = [\n  '먹고 싶은 사과',\n  '먹고 싶은 바나나',\n  '길고 노란 바나나 바나나',\n  '저는 과일이 좋아요'\n] ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\n\nvect = CountVectorizer()\ncountvect = vect.fit_transform(docs)\ncountvect_df = pd.DataFrame(countvect.toarray(), columns = sorted(vect.vocabulary_))\ncountvect_df.index = ['문서1', '문서2', '문서3', '문서4']\ncountvect_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\ntfidv = TfidfVectorizer(use_idf=True, smooth_idf=False, norm=None).fit(docs)\ntfidv_df = pd.DataFrame(tfidv.transform(docs).toarray(), columns = sorted(tfidv.vocabulary_))\ntfidv_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics.pairwise import cosine_similarity\n\ncosine_similarity(tfidv_df, tfidv_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4. Word2Vec 알고리즘","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from gensim.models import Word2Vec\n\ndocs = [\n  'you say goodbye and I say hello .'\n]\n\nsentences = [list(sentence.split(' ')) for sentence in docs]\nsentences","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Word2Vec(size=3, window=1, min_count=1, sg=1)\nmodel.build_vocab(sentences)\nmodel.wv.most_similar(\"say\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5. KNN 알고리즘","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import surprise\nfrom surprise.model_selection import KFold\nfrom surprise.model_selection import cross_validate\nfrom surprise import Reader, Dataset, SVD, SVDpp, NMF, KNNBaseline\nfrom surprise.model_selection import KFold\nfrom surprise.model_selection import cross_validate\n\ndata = Dataset.load_builtin('ml-100k')\ndf = pd.DataFrame(data.raw_ratings, columns=[\"user\", \"item\", \"rate\", \"id\"])\ndf = df.astype(np.float32)\n\ndel df[\"id\"]\ndf.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time \nreader = Reader(rating_scale=(1, 5))\nknndata = Dataset.load_from_df(df[['user', 'item', 'rate']], reader)\n\nsim_options = {'name': 'cosine'}\nknn = surprise.KNNBasic(sim_options=sim_options, k=20)\nscore = cross_validate(knn, knndata, measures=['RMSE'], cv=5, verbose=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time \nuser = 196\n\nscore_dict = {}\nfor sim in knn.get_neighbors(user, k=20):\n    df_ = df[df['user'] == sim]\n    for item, rate in zip(df_['item'].values, df_['rate'].values):\n        if item not in df[df['user'] == user]['item'].values:\n            try:\n                score_dict[item] += rate\n            except:\n                score_dict[item] = rate","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 상위 10개의 영화만 추천 \ndict(sorted(score_dict.items(), key = lambda x: -x[1])[0:10]).keys()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 6. SGD 알고리즘","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"해당 코드는 https://yamalab.tistory.com/92 에 있는 Y.LAB의 블로그 글을 참고했습니다. (대부분의 코드가 같고 중간에 한 부분만 수정했습니다.)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nfrom tqdm import tqdm_notebook as tqdm\n\nimport numpy as np\n\n# Base code : https://yamalab.tistory.com/92\nclass MatrixFactorization():\n    def __init__(self, R, k, learning_rate, reg_param, epochs, verbose=False):\n        \"\"\"\n        :param R: rating matrix\n        :param k: latent parameter\n        :param learning_rate: alpha on weight update\n        :param reg_param: beta on weight update\n        :param epochs: training epochs\n        :param verbose: print status\n        \"\"\"\n\n        self._R = R\n        self._num_users, self._num_items = R.shape\n        self._k = k\n        self._learning_rate = learning_rate\n        self._reg_param = reg_param\n        self._epochs = epochs\n        self._verbose = verbose\n\n\n    def fit(self):\n        \"\"\"\n        training Matrix Factorization : Update matrix latent weight and bias\n\n        참고: self._b에 대한 설명\n        - global bias: input R에서 평가가 매겨진 rating의 평균값을 global bias로 사용\n        - 정규화 기능. 최종 rating에 음수가 들어가는 것 대신 latent feature에 음수가 포함되도록 해줌.\n\n        :return: training_process\n        \"\"\"\n\n        # init latent features\n        self._P = np.random.normal(size=(self._num_users, self._k))\n        self._Q = np.random.normal(size=(self._num_items, self._k))\n\n        # init biases\n        self._b_P = np.zeros(self._num_users)\n        self._b_Q = np.zeros(self._num_items)\n        self._b = np.mean(self._R[np.where(self._R != 0)])\n\n        # train while epochs\n        self._training_process = []\n        for epoch in range(self._epochs):\n            # rating이 존재하는 index를 기준으로 training\n            xi, yi = self._R.nonzero()\n            for i, j in zip(xi, yi):\n                self.gradient_descent(i, j, self._R[i, j])\n            cost = self.cost()\n            self._training_process.append((epoch, cost))\n\n            # print status\n            if self._verbose == True and ((epoch + 1) % 10 == 0):\n                print(\"Iteration: %d ; cost = %.4f\" % (epoch + 1, cost))\n\n\n    def cost(self):\n        \"\"\"\n        compute root mean square error\n        :return: rmse cost\n        \"\"\"\n\n        # xi, yi: R[xi, yi]는 nonzero인 value를 의미한다.\n        # 참고: http://codepractice.tistory.com/90\n        xi, yi = self._R.nonzero()\n        # predicted = self.get_complete_matrix()\n        cost = 0\n        for x, y in zip(xi, yi):\n            cost += pow(self._R[x, y] - self.get_prediction(x, y), 2)\n        return np.sqrt(cost/len(xi))\n\n\n    def gradient(self, error, i, j):\n        \"\"\"\n        gradient of latent feature for GD\n\n        :param error: rating - prediction error\n        :param i: user index\n        :param j: item index\n        :return: gradient of latent feature tuple\n        \"\"\"\n\n        dp = (error * self._Q[j, :]) - (self._reg_param * self._P[i, :])\n        dq = (error * self._P[i, :]) - (self._reg_param * self._Q[j, :])\n        return dp, dq\n\n\n    def gradient_descent(self, i, j, rating):\n        \"\"\"\n        graident descent function\n\n        :param i: user index of matrix\n        :param j: item index of matrix\n        :param rating: rating of (i,j)\n        \"\"\"\n\n        # get error\n        prediction = self.get_prediction(i, j)\n        error = rating - prediction\n\n        # update biases\n        self._b_P[i] += self._learning_rate * (error - self._reg_param * self._b_P[i])\n        self._b_Q[j] += self._learning_rate * (error - self._reg_param * self._b_Q[j])\n\n        # update latent feature\n        dp, dq = self.gradient(error, i, j)\n        self._P[i, :] += self._learning_rate * dp\n        self._Q[j, :] += self._learning_rate * dq\n\n\n    def get_prediction(self, i, j):\n        \"\"\"\n        get predicted rating: user_i, item_j\n        :return: prediction of r_ij\n        \"\"\"\n        return self._b + self._b_P[i] + self._b_Q[j] + self._P[i, :].dot(self._Q[j, :].T)\n\n\n    def get_complete_matrix(self):\n        \"\"\"\n        computer complete matrix PXQ + P.bias + Q.bias + global bias\n\n        - PXQ 행렬에 b_P[:, np.newaxis]를 더하는 것은 각 열마다 bias를 더해주는 것\n        - b_Q[np.newaxis:, ]를 더하는 것은 각 행마다 bias를 더해주는 것\n        - b를 더하는 것은 각 element마다 bias를 더해주는 것\n\n        - newaxis: 차원을 추가해줌. 1차원인 Latent들로 2차원의 R에 행/열 단위 연산을 해주기위해 차원을 추가하는 것.\n\n        :return: complete matrix R^\n        \"\"\"\n        return self._b + self._b_P[:, np.newaxis] + self._b_Q[np.newaxis:, ] + self._P.dot(self._Q.T)\n\n\n\n# run example\nif __name__ == \"__main__\":\n    # rating matrix - User X Item : (7 X 5)\n    R = np.array([\n        [1, 0, 0, 1, 3],\n        [2, 0, 3, 1, 1],\n        [1, 2, 0, 5, 0],\n        [1, 0, 0, 4, 4],\n        [2, 1, 5, 4, 0],\n        [5, 1, 5, 4, 0],\n        [0, 0, 0, 1, 0],\n    ])\n\n    # P, Q is (7 X k), (k X 5) matrix\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfactorizer = MatrixFactorization(R, k=3, learning_rate=0.01, reg_param=0.01, epochs=100, verbose=True)\nfactorizer.fit()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"factorizer.get_complete_matrix()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 7. ALS 알고리즘","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from implicit.evaluation import *\nfrom implicit.als import AlternatingLeastSquares as ALS","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Implicit data\n# 예시를 위해서 rate의 값을 1로 변경해주었습니다. \ndf['rate'] = 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"user2idx = {}\nfor i, l in enumerate(df['user'].unique()):\n    user2idx[l] = i\n    \nmovie2idx = {}\nfor i, l in enumerate(df['item'].unique()):\n    movie2idx[l] = i","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"idx2user = {i: user for user, i in user2idx.items()}\nidx2movie = {i: item for item, i in movie2idx.items()}","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"useridx = df['useridx'] = df['user'].apply(lambda x: user2idx[x]).values\nmovieidx = df['movieidx'] = df['item'].apply(lambda x: movie2idx[x]).values\nrating = df['rate'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import scipy\n\npurchase_sparse = scipy.sparse.csr_matrix((rating, (useridx, movieidx)), shape=(len(set(useridx)), len(set(movieidx))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"als_model = ALS(factors=20, regularization=0.08, iterations = 20)\nals_model.fit(purchase_sparse.T)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"als_model.recommend(0, purchase_sparse, N=150)[0:10]","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}