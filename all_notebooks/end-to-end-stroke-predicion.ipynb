{"cells":[{"metadata":{},"cell_type":"markdown","source":"## 1. Problem Definition\n\nCan we predict or classify whether the patient is susceptible for stroke or not?"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip3 install seaborn","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# EDA and plotiing libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns # seaborn gets shortened to sns\n\n%matplotlib inline\n\n# Models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Model evaluators\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nfrom sklearn.metrics import plot_roc_curve","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Exploratory Data Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load the data\ndf = pd.read_csv(\"../input/stroke-prediction-dataset/healthcare-dataset-stroke-data.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"smoking_status\"].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's see how many positives(1) and negatives(0) in our target\ndf[\"stroke\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Normalized value counts\ndf[\"stroke\"].value_counts(normalize=True)*100","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have an **unbalanced** target column, we have more samples for false and very less samples for negative"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visulazing the value counts\ndf[\"stroke\"].value_counts().plot(kind=\"bar\", color=[\"skyblue\", \"lightgreen\"]);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Getting metrics on the columns\ndf.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### `Stroke` with respect to `gender`"},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.crosstab(df.gender, df.stroke)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Female** with stroke are more than **Male**, There's an outlier with one sample in **other** gender we'll drop the sample."},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df[df[\"gender\"] != \"Other\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.crosstab(df.gender, df.stroke).plot(kind=\"bar\", color=[\"skyblue\", \"lightgreen\"]);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### `Stroke` with respect to `ever_married`"},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.crosstab(df[\"ever_married\"], df[\"stroke\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**ever_married** people have stroke more than **never_married**. (Not sure what to say about this one ðŸ˜œ)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualizing the crosstab\npd.crosstab(df[\"ever_married\"], df[\"stroke\"]).plot(kind=\"bar\", color=[\"skyblue\", \"lightgreen\"]);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### `residence_type` with respect to `stroke`"},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.crosstab(df[\"Residence_type\"], df[\"stroke\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**urban** people have a little edge than **rural** on stroke possiblity"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualizing residence_type with target\npd.crosstab(df[\"Residence_type\"], df[\"stroke\"]).plot(kind=\"bar\", color=[\"skyblue\", \"lightgreen\"]);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### `smoking_type` with respect to `stroke`"},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.crosstab(df[\"smoking_status\"], df[\"stroke\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.crosstab(df[\"smoking_status\"], df[\"stroke\"]).plot(kind=\"bar\");","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"smoking_status\"].value_counts(normalize=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**never smoked** are affected by stroke higher than other categories, but the sample size for never_smoked is 37%."},{"metadata":{},"cell_type":"markdown","source":"### `work_type` with respect to `stroke`"},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.crosstab(df[\"work_type\"], df[\"stroke\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"work_type\"].value_counts(normalize=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.crosstab(df[\"work_type\"], df[\"stroke\"]).plot(kind=\"bar\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"People in **private** are more susceptibe to stroke compared to other categories, but *private* category sample is 57% - conveying they have the option to visit the hospital or have a health check more compared to other categories"},{"metadata":{},"cell_type":"markdown","source":"### `hyper_tension` with respect to `stroke`"},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.crosstab(df[\"hypertension\"], df[\"stroke\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**hypertension** people are more susceptible to stroke"},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.crosstab(df[\"hypertension\"], df[\"stroke\"]).plot(kind=\"bar\", color=[\"skyblue\", \"lightgreen\"]);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### `age` with respect to `stroke`"},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.crosstab(df[\"age\"], df[\"stroke\"]).plot(kind=\"line\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Aged** people are more susceptible to stroke than young ones"},{"metadata":{},"cell_type":"markdown","source":"### `bmi` with respect to `stroke`"},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.crosstab(df[\"bmi\"], df[\"stroke\"]).plot(kind=\"line\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### `age` and `bmi` impact on `stroke`"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create another figure\nplt.figure(figsize=(10,6))\n\n# Start with positve examples\nplt.scatter(df.age[df.stroke==0], \n            df.bmi[df.stroke==0], \n            c=\"lightblue\") # define it as a scatter figure\n\n# Now for negative examples, we want them on the same plot, so we call plt again\nplt.scatter(df.age[df.stroke==1], \n            df.bmi[df.stroke==1], \n            c=\"salmon\") # axis always come as (x, y)\n\n# Add some helpful info\nplt.title(\"Stroke in function of Age and Hyper tension\")\nplt.xlabel(\"Age\")\nplt.legend([\"No Disease\", \"Disease\"])\nplt.ylabel(\"BMI\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Higer age and less bmi are contributing towards a possiblity in stroke"},{"metadata":{},"cell_type":"markdown","source":"### `age` and `average_glucose_level` impact on `stroke`"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create another figure\nplt.figure(figsize=(10,6))\n\n# Start with positve examples\nplt.scatter(df.age[df.stroke==0], \n            df.avg_glucose_level[df.stroke==0], \n            c=\"lightblue\") # define it as a scatter figure\n\n# Now for negative examples, we want them on the same plot, so we call plt again\nplt.scatter(df.age[df.stroke==1], \n            df.avg_glucose_level[df.stroke==1], \n            c=\"salmon\") # axis always come as (x, y)\n\n# Add some helpful info\nplt.title(\"Stroke in function of Age and Hyper tension\")\nplt.xlabel(\"Age\")\nplt.legend([\"No Disease\", \"Disease\"])\nplt.ylabel(\"Average Glucose level\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Older people with low glucose level are more susceptible to stroke"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_with_nan_bmi = df[df[\"bmi\"].isnull()]\ndf_with_nan_bmi.to_csv(\"nan-bmi-samples.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now we've saved the nan bmi samples to a scv let's drop them from the dataframe\ndf = df.dropna()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's convert the categorical features to numbers using pandas"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"gender\"].unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Getting data ready\n\n1. Let's convert `Male` to `0` and `Female` to `1` in `gender` feature"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['gender'] = np.where((df.gender == 'Male'),'0',df.gender)\ndf['gender'] = np.where((df.gender == 'Female'),'1',df.gender)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"gender\"] = df[\"gender\"].astype('int64')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"ever_married\"].unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"2. Let's convert `Yes` to `0` and `No` to `1` in `ever_married` feature"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['ever_married'] = np.where((df.ever_married == 'Yes'),'0',df.ever_married)\ndf['ever_married'] = np.where((df.ever_married == 'No'),'1',df.ever_married)\ndf[\"ever_married\"] = df[\"ever_married\"].astype('int64')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"work_type\"].unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"3. Let's convert `work_type` feature using below mapping\n* `Private`: `0`\n* `Self-employed`: `1`\n* `Govt_job`: `2`\n* `children`: `3`\n* `Never_worked`: `4`"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['work_type'] = np.where((df.work_type == 'Private'),'0',df.work_type)\ndf['work_type'] = np.where((df.work_type == 'Self-employed'),'1',df.work_type)\ndf['work_type'] = np.where((df.work_type == 'Govt_job'),'2',df.work_type)\ndf['work_type'] = np.where((df.work_type == 'children'),'3',df.work_type)\ndf['work_type'] = np.where((df.work_type == 'Never_worked'),'4',df.work_type)\ndf[\"work_type\"] = df[\"work_type\"].astype('int64')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"Residence_type\"].unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"4. Let's convert `residence_type feature`, \n* `Urban`: `0`\n* `Rural`: `1`"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Residence_type'] = np.where((df.Residence_type == 'Urban'),'0',df.Residence_type)\ndf['Residence_type'] = np.where((df.Residence_type == 'Rural'),'1',df.Residence_type)\ndf[\"Residence_type\"] = df[\"Residence_type\"].astype('int64')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"smoking_status\"].unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"4. Let's convert `smoking_status feature`, \n* `formerly smoked`: `0`\n* `never smoked`: `1`\n* `smokes`: `2`\n* `Unknown`: `3`"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['smoking_status'] = np.where((df.smoking_status == 'formerly smoked'),'0',df.smoking_status)\ndf['smoking_status'] = np.where((df.smoking_status == 'never smoked'),'1',df.smoking_status)\ndf['smoking_status'] = np.where((df.smoking_status == 'smokes'),'2',df.smoking_status)\ndf['smoking_status'] = np.where((df.smoking_status == 'Unknown'),'3',df.smoking_status)\ndf[\"smoking_status\"] = df[\"smoking_status\"].astype('int64')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"One Final EDA to check the relationship between independent variables using correlation matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_matrix = df.corr()\ncorr_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's plot the correlation matrix\nplt.figure(figsize=(30,20))\nsns.heatmap(corr_matrix,\n           annot=True,\n           linewidths=0.5,\n           fmt=\".2f\",\n           cmap=\"YlGnBu\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Much better. A higher positive value means a potential positive correlation (increase) and a higher negative value means a potential negative correlation (decrease)."},{"metadata":{},"cell_type":"markdown","source":"**Now all the categorical features are converted to numerical** Let's model yeahðŸ˜Ž"},{"metadata":{},"cell_type":"markdown","source":"## Modelling"},{"metadata":{},"cell_type":"markdown","source":"1. Let's split our features and target"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.drop(\"stroke\", axis=1)\nY = df.stroke.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Random seed for probabality\nnp.random.seed(42)\n\n# Splitting the data into train and test sets\nX_train, X_test, Y_train, Y_test = train_test_split(X,\n                                                    Y,\n                                                    test_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"len(X_train), len(Y_train), len(X_test), len(Y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Let's train the data with few models and see how it goes"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Put models in a dictionary\nmodels = {\"KNN\": KNeighborsClassifier(),\n          \"Logistic Regression\": LogisticRegression(), \n          \"Random Forest\": RandomForestClassifier()}\n\ndef fit_and_score(models, x_train, x_test, y_train, y_test):\n    \"\"\"\n    Model to fit the data to a model and score the model with test data\n    models - dictionary of models\n    x_train - training features\n    x_test - test features\n    y_train - training features\n    y_test - test_features\n    \"\"\"\n    np. random.seed(42)\n    model_scores = {}\n    for name, model in models.items():\n        model.fit(x_train, y_train)\n        model_scores[name] = model.score(x_test, y_test)\n\n    return model_scores","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"model_scores = fit_and_score(models=models,\n                             x_train=X_train,\n                             x_test=X_test,\n                             y_train=Y_train,\n                             y_test=Y_test)\nmodel_scores","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looks like our model seems to be overfitting, since it's an unbalanced dataset. It's choosing the true negative for most cases to minimize error. Let's evaluate the model using F1 score"},{"metadata":{"trusted":false},"cell_type":"code","source":"model_compare = pd.DataFrame(model_scores, index=['accuracy'])\nmodel_compare.T.plot.bar();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Hypertuning the three models"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Creating a list for train scores\ntrain_scores = []\n\n# Creating a list for test scores\ntest_scores = []\n\n# Creating a list for n neighbours\nneighbours = range(1,21)\n\nknn= KNeighborsClassifier()\n\n# Loop through different neighbours values\nfor i in neighbours:\n    # Set neighbours\n    knn.set_params(n_neighbors = i)\n    \n    # Fitting the model\n    knn.fit(X_train, Y_train)\n    \n    # Scoring the model\n    train_scores.append(knn.score(X_train, Y_train))\n    \n    # Scoring the model on test data set\n    test_scores.append(knn.score(X_test, Y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train_scores, test_scores","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.plot(neighbours, train_scores, label=\"Train score\")\nplt.plot(neighbours, test_scores, label=\"Test score\")\nplt.xticks(np.arange(1, 21, 1))\nplt.xlabel(\"Number of neighbors\")\nplt.ylabel(\"Model score\")\nplt.legend()\n\nprint(f\"Maximum KNN score on the test data: {max(test_scores)*100:.2f}%\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There seems to be **no improvement** after hypertuning the knneighbours\n\nModel seems to be working very well on train data like mentioned above, seems like overfitting. Let's perform hypertuning for other models. Let's check some more metrics like classification_report, ROC_curve"},{"metadata":{},"cell_type":"markdown","source":"### Tuning model with RandomizedSearchCV"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Different LogisticRegression hyperparameters\nlog_reg_grid = {\"C\": np.logspace(-4, 4, 20),\n                \"solver\": [\"liblinear\"]}\n\n# Different RandomForestClassifier hyperparameters\nrf_grid = {\"n_estimators\": np.arange(10, 1000, 50),\n           \"max_depth\": [None, 3, 5, 10],\n           \"min_samples_split\": np.arange(2, 20, 2),\n           \"min_samples_leaf\": np.arange(1, 20, 2)}","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Setup random seed\nnp.random.seed(42)\n\n# Setup Random hyper parameter tuning for Logistic Regression\nrs_log_reg = RandomizedSearchCV(LogisticRegression(), param_distributions=log_reg_grid,\n                               cv=5,\n                               n_iter=20,\n                               verbose=True)\n\nrs_log_reg.fit(X_train, Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Finding the best hyper parameters\nrs_log_reg.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"rs_log_score = rs_log_reg.score(X_test, Y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"model_scores[\"Logistic Regression\"] - rs_log_score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There's 0.0010183299389002753 decrease in logistic regression score after hyper tuning"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Tuning random forest classifier using randomsearchcv parameters\nrs_rf = RandomizedSearchCV(RandomForestClassifier(),\n                          param_distributions=rf_grid,\n                          cv=5,\n                          n_iter=20,\n                          verbose=True)\n\nrs_rf.fit(X_train, Y_train)\n\nrs_rf.score(X_test, Y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"rs_rf.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"rs_rf_score = rs_rf.score(X_test, Y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"model_scores[\"Random Forest\"] - rs_rf_score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The hyper tuned random classifier performs better than the model\n\nLet's find out more metrics regading the RandomSearchCV randomforestclassifier model\n\n## Model evaluation beyond accuracy\n\nWe'll use the below metrics,\n\n1. ROC and AUC curve\n2. Confusion matrix\n3. Classification report\n4. Recall score\n5. F1 score\n\nTo make comparisons and evalutions we'll need predictions, let's get em."},{"metadata":{"trusted":false},"cell_type":"code","source":"Y_preds = rs_rf.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Import roc curve from metrics module\nfrom sklearn.metrics import plot_roc_curve\n\nplot_roc_curve(rs_rf, X_test, Y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"RandomizedSearchCV random forest model does good with an auc of 0.84\n\nLet's proceed with confusion matrix"},{"metadata":{"trusted":false},"cell_type":"code","source":"print(confusion_matrix(Y_test, Y_preds))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's visualize the confusion matrix using sns heatmap\n\n#### Confusion matrix"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Import seaborn\nimport seaborn as sns\n\ndef plot_conf_matrix(y_test, y_preds):\n    \"\"\"\n    Funtion to plot confusion matrix\n    \"\"\"\n\n    fig, ax = plt.subplots(figsize=(3,3))\n    ax = sns.heatmap(confusion_matrix(y_test, y_preds),\n                    annot=True,\n                    cbar=False)\n\n    plt.xlabel(\"True label\")\n    plt.ylabel(\"Predicted label\")\n\n#plot_conf_matrix(Y_test, Y_preds)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Classification report"},{"metadata":{"trusted":false},"cell_type":"code","source":"print(classification_report(Y_test, Y_preds))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Inferences\n\nLooking at the classification report and confusion matrix, The model is unable to predict true positives at all due to the class imbalance in dataset.\n\nWe can see this more clearly in classificatio report's f1-score, 0.97 for class 0 and 0 for class 1.\n\nmacro average which will be bad when there are class imbalances and it's 0.49 which is pretty poor\n"},{"metadata":{},"cell_type":"markdown","source":"Let's give a try if the model improves with best hyper parameters and calculate the `cross_val_score` for all the metrics and visualize them"},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"rs_rf.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"clf = RandomForestClassifier(n_estimators=260,\n                            min_samples_split=8,\n                            min_samples_leaf=13,\n                            max_depth=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Cross-validated F1 score\ncv_f1 = np.mean(cross_val_score(clf,\n                                X,\n                                Y,\n                                cv=5, # 5-fold cross-validation\n                                scoring=\"f1\")) # f1 as scoring\ncv_f1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In cross validation F1 become zero meaning our model performance is ðŸ˜•ðŸ˜‘\n\nCross validation is not working."},{"metadata":{},"cell_type":"markdown","source":"## Dataset based imbalance techniques - oversampling"},{"metadata":{"trusted":false},"cell_type":"code","source":"!pip install -U imbalanced-learn","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from imblearn.under_sampling import TomekLinks, RandomUnderSampler\nfrom imblearn.over_sampling import RandomOverSampler, SMOTE, ADASYN","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Put models in a dictionary\nmodels = {\"KNN\": KNeighborsClassifier(),\n          \"LogisticRegression\": LogisticRegression(), \n          \"RandomForest\": RandomForestClassifier()}\n\nresamplers = {\n    \"ros\": RandomOverSampler(sampling_strategy='minority'),\n    \"smote\": SMOTE(sampling_strategy='minority'),\n    \"adasyn\": ADASYN(sampling_strategy='minority'),\n    \"rus\": RandomUnderSampler(sampling_strategy=\"majority\"),\n    \"tomek\": TomekLinks(sampling_strategy=\"majority\")\n}\n\ndef fit_resample_and_score(models, samplers, x, y):\n    \"\"\"\n    Model to resample data to a model and score the model with test data\n    models - dictionary of models\n    samplers - samplers to resample the data\n    x - features\n    y - labels\n    \"\"\"\n    np. random.seed(42)\n    model_scores = {}\n    for sname, sampler in samplers.items():\n        \n        # resampling the data\n        X_resampled, Y_resampled = sampler.fit_resample(x, y)\n        \n        # Splitting the data\n        X_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size=0.2)\n        \n        for mname, model in models.items():\n            #print(sname + mname)\n            model.fit(X_train, Y_train)\n            model_scores[sname+mname] = model.score(X_test, Y_test)\n\n    return model_scores","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"model_scores = fit_resample_and_score(models=models,\n                      samplers=resamplers,\n                      x=X,\n                      y=Y)\n\nmodel_scores","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We've written a function to resample and model the data ðŸ˜Ž."},{"metadata":{"trusted":false},"cell_type":"code","source":"model_compare = pd.DataFrame(model_scores, index=['accuracy'])\nmodel_compare.T.plot(kind=\"bar\", figsize=(20,10));","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(f\"The model with highest accuracy is rosKNN with {max(model_scores.values())}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see if we can improve the performace of this model"},{"metadata":{},"cell_type":"markdown","source":"# Hypertuning once again"},{"metadata":{"trusted":false},"cell_type":"code","source":"X_resampled, Y_resampled = RandomOverSampler(sampling_strategy='minority').fit_resample(X,Y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"X_train, X_test, Y_train, Y_test = train_test_split(X_resampled, Y_resampled)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train_scores = []\ntest_scores = []\nneigbors = range(1,21)\nknn = KNeighborsClassifier()\n\nfor i in neigbors:\n    \n    knn.set_params(n_neighbors = i)\n    \n    knn.fit(X_train, Y_train)\n    \n    train_scores.append(knn.score(X_train, Y_train))\n    \n    test_scores.append(knn.score(X_test, Y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.plot(neighbours, train_scores, label=\"Train score\")\nplt.plot(neighbours, test_scores, label=\"Test score\")\nplt.xticks(np.arange(1, 21, 1))\nplt.xlabel(\"Number of neighbors\")\nplt.ylabel(\"Model score\")\nplt.legend()\n\nprint(f\"Maximum KNN score on the test data: {max(test_scores)*100:.2f}%\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The model had improved a bit after hypertuning with n_neighbors=1"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Using the best hyperparameters\nclf_knn = KNeighborsClassifier(n_neighbors=1)\n\nclf_knn.fit(X_train, Y_train)\n\nclf_knn.score(X_test, Y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's find below metrics as a final destination\n1. ROC curve\n2. classificationreport\n3. confusionmatrix"},{"metadata":{"trusted":false},"cell_type":"code","source":"plot_roc_curve(clf_knn, X_test, Y_test);","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"Y_preds = clf_knn.predict(X_test)\nprint(classification_report(Y_test, Y_preds))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plot_conf_matrix(Y_test, Y_preds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"id_final = pd.Series(X_test[\"id\"])\nstroke_final = pd.Series(Y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_final = pd.DataFrame({\"id\": id_final, \"stroke\": stroke_final})","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_final.dropna(inplace=True)\ndf_final.isna().sum()\ndf_final.to_csv(\"submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is the final model KNN with RandomOverSampling"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}