{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Vectorized Linear Regression\n## The model is implemented solely over Numpy and Pandas. No other python libraries are used. \n## The goal is to reduce the Cost function by obtaining the optimal value of parameter thetas, guided by Gradient Descent. ","metadata":{}},{"cell_type":"markdown","source":"### Multivariate Linear Regression on the Boston Housing Prices dataset. No existing machine learning algorithm is used.\n### **To my surprise, I couldn't find vectorization of linear regression to this widely known dataset in particular, so hopefully, I'm making a significant contribution to the Machine Learning community.** \n### Why is vectorization important? To take complete advantage of computational power of computers, the most efficient way of implementing an algorithm is vectorizing the computations as it enables us to attain parallelized computations hence tapping into the limits of the system. It saves noteworthy time whose efficiency comes into play when dealing with Big data, where seconds in these small datasets translate to days.\n### Thus, vectorization saves us on huge amount of training time and improves our algorithm.","metadata":{}},{"cell_type":"code","source":"from sklearn import datasets\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Loadind the Boston dataset from sklearn.datasets","metadata":{}},{"cell_type":"code","source":"boston_data = datasets.load_boston()\ndf = pd.DataFrame(boston_data.data,columns=boston_data.feature_names)\ndf['target'] = pd.Series(boston_data.target)                            #Appending the target feature to the dataset\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"506 examples with 13 features and 1 target variable.","metadata":{}},{"cell_type":"code","source":"Y = df['target']\ny = Y.copy(deep=True) \ndf.drop('target', axis = 1, inplace = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 506 examples and 13 features.","metadata":{}},{"cell_type":"code","source":"df.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Breakdown of each feature :-","metadata":{}},{"cell_type":"markdown","source":"crim-\nper capita crime rate by town.\n\nzn-\nproportion of residential land zoned for lots over 25,000 sq.ft.\n\nindus-\nproportion of non-retail business acres per town.\n\nchas-\nCharles River dummy variable (= 1 if tract bounds river; 0 otherwise).\n\nnox-\nnitrogen oxides concentration (parts per 10 million).\n\nrm-\naverage number of rooms per dwelling.\n\nage-\nproportion of owner-occupied units built prior to 1940.\n\ndis-\nweighted mean of distances to five Boston employment centres.\n\nrad-\nindex of accessibility to radial highways.\n\ntax-\nfull-value property-tax rate per $10,000.\n\nptratio-\npupil-teacher ratio by town.\n\nblack-\n1000(Bk - 0.63)^2 where Bk is the proportion of african-american by town.\n\nlstat-\nlower status of the population (percent).\n\ntarget-\nmedian value of owner-occupied homes in $1000s.","metadata":{}},{"cell_type":"markdown","source":"### Since features have varying distribution we'll have to normalise them to make sure one feature does not dominate other features on deciding the target value.\n### Z-Score or mean normalisation equates the mean of feature to 0 and standard deviation to 1.","metadata":{}},{"cell_type":"code","source":"#Feature Scaling\n#Unscaled features results in dominance of a particular feature/features having higher \"weight\" \n#and thus reduces the accuracy of the model on data the model is not tested on.\n#Performing mean normalisation(Z-Score)\nfor i in df.columns:\n    if i == 'CHAS':\n        #We dont normalise a categorical data(chas only has binary values)\n        continue\n    df[i] = (df[i]-(df[i].mean()))/(df[i].std())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Dataset with normalised values\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Converting datasets into matrices to execute vectorised Linear Regression","metadata":{}},{"cell_type":"code","source":"sns.scatterplot(data = y)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## The above plot shows how the price for houses are capped at 50,000. This can prove to limit the accuracy of our model for houses whose prices are more than 50,000, both on real life and training example. This capping of the highest price is one limitation of the Boston Dataset. ","metadata":{}},{"cell_type":"markdown","source":"### Converting datasets into numpy array to implement vectorized Linear Regression model","metadata":{}},{"cell_type":"code","source":"x = df.to_numpy()\nprint(x.shape, y.shape)\ntarget = y.to_numpy()\nnumExamples = x.shape[0]\nnumFeatures = x.shape[1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = np.append(np.ones((numExamples,1)),x, axis = 1)   #Adding unit bias\nx.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"theta = np.zeros((numFeatures + 1,1)) #Initializing theta values as 0.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"theta.shape","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"iterations = [200,300,400,500,700,600,1000]\nalpha = [.001 , .003, .01, .03, .1 ,.3]\nfor i in iterations:\n    for j in alpha:\n        for iters in range(i):\n                h= x@theta                                            #Hypothesis Function\n                target = np.reshape(target, (len(target),1))\n                error = h-target\n                J = ((error**2).sum())*(1/(2*target.shape[0]))        #Cost Function \n                gradient = (x.T@error)*(j/target.shape[0])            #Gradient\n                theta = theta-gradient                                #simultaneously updating theta\n        print(\"for\",i,\"interations and alpha =\",j, \"cost is\",J.sum())\n        theta = np.zeros((numFeatures + 1,1))                         #Resetting the values of theta to zeros","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Choosing the optimal combination of number of iterations and alpha. (300,.03)\ntheta = np.zeros((numFeatures + 1,1))\niterations = 300\nalpha = 0.03\nJ_History = np.zeros((iterations, 1))\ntheta_history = np.zeros((iterations,len(theta)))\nfor i in range(iterations):\n    h= x@theta                                                    #Hypothesis Function\n    target = np.reshape(target, (len(target),1)) \n    error = h-target\n    J = ((error**2).sum())*(1/(2*target.shape[0]))                #Cost Function\n    gradient = (x.T@error)*(alpha/target.shape[0])                #Gradient\n    theta = theta-gradient\n    J_History[i] = J.sum()                                        #Appending values of Cost Function to the array\nplt.plot(range(iterations), J_History)\nplt.xlabel('Number of iterations')\nplt.ylabel('Cost function')\nplt.title('Cost function constantly decreases with increase in number of iterations')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"theta #Final values of theta obtained by our vectorized liner regression madel","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## New set of values for the 13 features can be multiplied by theta to predict the price of the house corresponding to the values of the 13 features. ","metadata":{}},{"cell_type":"markdown","source":"#### If this notebook helped you in learning, an upvote would be huge! \n#### Thank you :)","metadata":{}}]}