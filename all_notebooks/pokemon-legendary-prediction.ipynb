{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Importing Libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport warnings\nwarnings.filterwarnings('ignore')\nimport matplotlib.pyplot as plt\n%matplotlib inline ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Cleaning"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/pokemon/Pokemon.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Removing irrelevant features(# and Name) and features with Nan values(Type 2)\ndata = data.drop(['#','Type 2','Name'],axis='columns')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head(10)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"data.Legendary.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It is clear that it is imbalanced dataset. Our model will fail to predict True values and memorizes on False values. So we need to make this dataset as balanced dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"legendaryPokemon = data.loc[data['Legendary']==True]\nlegendaryPokemon = legendaryPokemon.append(legendaryPokemon.append(legendaryPokemon))\nbal_data = data.append(legendaryPokemon.append(legendaryPokemon.append(legendaryPokemon)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So, what I did was I performed oversampling of True values (multiple times) in Legendary column and appended it to our original dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"# mapping true and false to 1 and 0 respectively\nbal_data['Legendary'] = bal_data.Legendary.map({False:0,True:1})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.compose import make_column_transformer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import OneHotEncoder\ncol_trans = make_column_transformer(\n            (OneHotEncoder(),['Type 1','Generation']),\n            (StandardScaler(),['Total','HP','Attack','Defense','Sp. Atk','Sp. Def','Speed']),\n            remainder = 'passthrough')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = bal_data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train Test Split"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX = df.drop(['Legendary'], axis = 1)\ny = df['Legendary']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"col_trans.fit_transform(X_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Classification Models"},{"metadata":{},"cell_type":"markdown","source":"### 1. Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import make_pipeline\nlogreg = LogisticRegression(solver='lbfgs')\npipe = make_pipeline(col_trans,logreg)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\nprint('Accuracy score on Train data: {}'.format(cross_val_score(pipe, X_train, y_train, cv=5, scoring='accuracy').mean()*100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pipe = make_pipeline(col_trans,logreg)\npipe.fit(X_train, y_train)\ny_pred = pipe.predict(X_test)\nfrom sklearn import metrics\nprint('Accuracy score on Test data: {}'.format(metrics.accuracy_score(y_test,y_pred)*100))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2. K Nearest Neighbors Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nknn_scores = []\nfor k in range(1,31):\n    knn_classifier = KNeighborsClassifier(n_neighbors = k)\n    pipe = make_pipeline(col_trans,knn_classifier)\n    knn_scores.append(cross_val_score(pipe, X_train, y_train, cv=5, scoring='accuracy').mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,12))\nplt.plot([k for k in range(1, 31)], knn_scores, color = 'red')\nfor i in range(1,31):\n    plt.text(i, knn_scores[i-1], (i, round(knn_scores[i-1]*100,2)))\nplt.xticks([i for i in range(1, 31)])\nplt.xlabel('Number of Neighbors (K)')\nplt.ylabel('Scores')\nplt.title('K Neighbors Classifier scores for different K values')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Accuracy score on Train data: {}'.format(knn_scores[1]*100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"knn_classifier = KNeighborsClassifier(n_neighbors = 2)\npipe = make_pipeline(col_trans,knn_classifier)\npipe.fit(X_train, y_train)\ny_pred = pipe.predict(X_test)\nprint('Accuracy score on Test Data: {}'.format(metrics.accuracy_score(y_test,y_pred)*100))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3. Support Vector Classifier (SVC)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\nsvc_scores = []\nkernels = ['linear', 'poly', 'rbf', 'sigmoid']\nfor i in range(len(kernels)):\n    svc_classifier = SVC(kernel = kernels[i])\n    pipe = make_pipeline(col_trans,svc_classifier)\n    svc_scores.append(cross_val_score(pipe, X_train, y_train, cv=5, scoring='accuracy').mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from matplotlib.cm import rainbow\nimport numpy as np\ncolors = rainbow(np.linspace(0, 1, len(kernels)))\nplt.figure(figsize=(10,7))\nplt.bar(kernels, svc_scores, color = colors)\nfor i in range(len(kernels)):\n    plt.text(i, svc_scores[i], svc_scores[i])\nplt.xlabel('Kernels')\nplt.ylabel('Scores')\nplt.title('Support Vector Classifier scores for different kernels')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Accuracy score on Train data: {}'.format(svc_scores[0]*100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svc_classifier = SVC(kernel = 'linear')\npipe = make_pipeline(col_trans,svc_classifier)\npipe.fit(X_train, y_train)\ny_pred = pipe.predict(X_test)\nprint('Accuracy score on Test data: {}'.format(metrics.accuracy_score(y_test,y_pred)*100))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4. Decision Tree Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\ndt_scores = []\nfor i in range(1, len(X.columns) + 1):\n    dt_classifier = DecisionTreeClassifier(max_features = i, random_state = 0)\n    pipe = make_pipeline(col_trans,dt_classifier)\n    dt_scores.append(cross_val_score(pipe, X_train, y_train, cv=5, scoring='accuracy').mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,10))\nplt.plot([i for i in range(1, len(X.columns) + 1)], dt_scores, color = 'green')\nfor i in range(1, len(X.columns) + 1):\n    plt.text(i, dt_scores[i-1], (i, dt_scores[i-1]))\nplt.xticks([i for i in range(1, len(X.columns) + 1)])\nplt.xlabel('Max features')\nplt.ylabel('Scores')\nplt.title('Decision Tree Classifier scores for different number of maximum features')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Accuracy score on Train data: {}'.format(dt_scores[5]*100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dt_classifier = DecisionTreeClassifier(max_features = 6, random_state = 0)\npipe = make_pipeline(col_trans,dt_classifier)\npipe.fit(X_train, y_train)\ny_pred = pipe.predict(X_test)\nprint('Accuracy  score on Test data: {}'.format(metrics.accuracy_score(y_test,y_pred)*100))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5. Random Forest Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nrf_scores = []\nestimators = [10, 100, 200, 500, 1000]\nfor i in estimators:\n    rf_classifier = RandomForestClassifier(n_estimators = i, random_state = 0)\n    pipe = make_pipeline(col_trans,rf_classifier)\n    rf_scores.append(cross_val_score(pipe, X_train, y_train, cv=5, scoring='accuracy').mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,7))\ncolors = rainbow(np.linspace(0, 1, len(estimators)))\nplt.bar([i for i in range(len(estimators))], rf_scores, color = colors, width = 0.8)\nfor i in range(len(estimators)):\n    plt.text(i, rf_scores[i], round(rf_scores[i],5))\nplt.xticks(ticks = [i for i in range(len(estimators))], labels = [str(estimator) for estimator in estimators])\nplt.xlabel('Number of estimators')\nplt.ylabel('Scores')\nplt.title('Random Forest Classifier scores for different number of estimators')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Accuracy score on Train data: {}'.format(rf_scores[0]*100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_classifier = RandomForestClassifier(n_estimators = 10, random_state = 0)\npipe = make_pipeline(col_trans,rf_classifier)\npipe.fit(X_train, y_train)\ny_pred = pipe.predict(X_test)\nprint('Accuracy score on Test data: {}'.format(metrics.accuracy_score(y_test,y_pred)*100))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":1}