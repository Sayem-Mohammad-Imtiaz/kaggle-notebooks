{"cells":[{"metadata":{},"cell_type":"markdown","source":"![](https://www.pata.org/wp-content/uploads/2014/09/TripAdvisor_Logo-300x119.png)\n# Predict TripAdvisor Rating\n## В этом соревновании нам предстоит предсказать рейтинг ресторана в TripAdvisor\n**По ходу задачи:**\n* Прокачаем работу с pandas\n* Научимся работать с Kaggle Notebooks\n* Поймем как делать предобработку различных данных\n* Научимся работать с пропущенными данными (Nan)\n* Познакомимся с различными видами кодирования признаков\n* Немного попробуем [Feature Engineering](https://ru.wikipedia.org/wiki/Конструирование_признаков) (генерировать новые признаки)\n* И совсем немного затронем ML\n* И многое другое...   \n\n\n\n### И самое важное, все это вы сможете сделать самостоятельно!\n\n*Этот Ноутбук являетсся Примером/Шаблоном к этому соревнованию (Baseline) и не служит готовым решением!*   \nВы можете использовать его как основу для построения своего решения.\n\n> что такое baseline решение, зачем оно нужно и почему предоставлять baseline к соревнованию стало важным стандартом на kaggle и других площадках.   \n**baseline** создается больше как шаблон, где можно посмотреть как происходит обращение с входящими данными и что нужно получить на выходе. При этом МЛ начинка может быть достаточно простой, просто для примера. Это помогает быстрее приступить к самому МЛ, а не тратить ценное время на чисто инженерные задачи. \nТакже baseline являеться хорошей опорной точкой по метрике. Если твое решение хуже baseline - ты явно делаешь что-то не то и стоит попробовать другой путь) \n\nВ контексте нашего соревнования baseline идет с небольшими примерами того, что можно делать с данными, и с инструкцией, что делать дальше, чтобы улучшить результат.  Вообще готовым решением это сложно назвать, так как используются всего 2 самых простых признака (а остальные исключаются).","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# import","execution_count":null},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport re\nfrom datetime import datetime, timedelta\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns \n%matplotlib inline\n\n# Загружаем специальный удобный инструмент для разделения датасета:\nfrom sklearn.model_selection import train_test_split\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# всегда фиксируйте RANDOM_SEED, чтобы ваши эксперименты были воспроизводимы!\nRANDOM_SEED = 42","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# зафиксируем версию пакетов, чтобы эксперименты были воспроизводимы:\n!pip freeze > requirements.txt","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# DATA","execution_count":null},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"DATA_DIR = '/kaggle/input/sf-dst-restaurant-rating/'\ndf_train = pd.read_csv(DATA_DIR+'/main_task.csv')\ndf_test = pd.read_csv(DATA_DIR+'kaggle_task.csv')\nsample_submission = pd.read_csv(DATA_DIR+'/sample_submission.csv')\nranks = pd.read_csv(\"/kaggle/input/ta-ranks/ranks.csv\").drop(['Unnamed: 0','claimed'], axis=1).drop_duplicates(subset=['url'])\nwordcounts = pd.read_csv(\"/kaggle/input/ta-keywords/words_keys.csv\", sep=';').drop(['Unnamed: 0'],axis=1)\ncities = pd.read_csv('/kaggle/input/world-cities-datasets/worldcities.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ВАЖНО! дря корректной обработки признаков объединяем трейн и тест в один датасет\ndf_train['sample'] = 1 # помечаем где у нас трейн\ndf_test['sample'] = 0 # помечаем где у нас тест\ndf_test['Rating'] = 0 # в тесте у нас нет значения Rating, мы его должны предсказать, по этому пока просто заполняем нулями\n\ndata = df_test.append(df_train, sort=False).reset_index(drop=True) # объединяем","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Подробнее по признакам:\n* City: Город \n* Cuisine Style: Кухня\n* Ranking: Ранг ресторана относительно других ресторанов в этом городе\n* Price Range: Цены в ресторане в 3 категориях\n* Number of Reviews: Количество отзывов\n* Reviews: 2 последних отзыва и даты этих отзывов\n* URL_TA: страница ресторана на 'www.tripadvisor.com' \n* ID_TA: ID ресторана в TripAdvisor\n* Rating: Рейтинг ресторана","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Посмотрим распределение признака","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.rcParams['figure.figsize'] = (10,7)\ndf_train['Ranking'].hist(bins=100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.rcParams['figure.figsize'] = (15,10)\nsns.heatmap(data.drop(['sample'], axis=1).corr(),)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Preprocessing\nТеперь, для удобства и воспроизводимости кода, завернем всю обработку в одну большую функцию.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# на всякий случай, заново подгружаем данные\ndf_train = pd.read_csv(DATA_DIR+'/main_task.csv')\ndf_test = pd.read_csv(DATA_DIR+'/kaggle_task.csv')\n\n\ndf_train['sample'] = 1 # помечаем где у нас трейн\ndf_test['sample'] = 0 # помечаем где у нас тест\ndf_test['Rating'] = 0 # в тесте у нас нет значения Rating, мы его должны предсказать, по этому пока просто заполняем нулями\n\ndata = df_test.append(df_train, sort=False).reset_index(drop=True) # объединяем\ndata.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Additional data sets\nranks = pd.read_csv(\"/kaggle/input/ta-ranks/ranks.csv\").drop(['Unnamed: 0','claimed'], axis=1).drop_duplicates(subset=['url'])\nwordcounts = pd.read_csv(\"/kaggle/input/ta-keywords/words_keys.csv\", sep=';').drop(['Unnamed: 0'],axis=1)\ncities = pd.read_csv('/kaggle/input/world-cities-datasets/worldcities.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Конвертация уровня стоимости\ndef conv_price(x):\n    d = {\n        '$$ - $$$':2,\n        '$': 1,\n        '$$$$':3\n    }\n    if x in d.keys():\n        return d[x]\n    elif pd.isnull(x):\n        return float('NaN')\n    else:\n        return float('NaN')\n# Конвертация Cuisine Style\ndef conv_cuisine(x):\n    lx=re.compile('\\[.*\\]')\n    if type(x)==str and lx.fullmatch(x):\n        lst=eval(x)\n        return lst\n    #elif pd.isnull(x):\n    else:\n        return ['NaN']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def gen_values_set(data, colname):\n    vals=set()\n    lx=re.compile('\\[.*\\]')\n    for v in data[colname]:\n        if type(v)==str and lx.fullmatch(v):\n            lst=eval(v)\n            for x in lst:\n                vals.add(x)\n        elif type(v)==list:\n            lst=v\n            for x in lst:\n                vals.add(x)\n        else:\n            vals.add(v)\n    return vals\n# Функция создания Dummies (поддерживает списки)\ndef gen_valcols(colname, df):\n    vals=gen_values_set(df, colname)\n\n    def find_item(item):\n        if type(item)==str:\n            if v==item:\n                return 1\n            else:\n                return 0\n        elif type(item)==list:\n            if v in item:\n                return 1\n            else:\n                return 0\n            \n    for v in vals:\n        df[colname+v]=df[colname].apply(lambda x: find_item(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lrx=re.compile('\\[\\[.*\\]\\]')\ndef extr_rev(row):\n    #print(row)\n    x=row['Reviews']\n    lst=[[],[]]\n    if type(x)== str and lrx.fullmatch(x):\n        nan=''\n        lst=eval(x)\n#CORR\n#    row['rev_count'] =  len(lst[0])\n    row['rev1'] = lst[0][0] if len(lst[0])>0 else ''\n    row['rev2'] = lst[0][1] if len(lst[0])>1 else ''\n    \n    row['date1'] = pd.to_datetime(lst[1][0] if len(lst[1])>0 else '', format='%m/%d/%Y', errors='coerce')\n    row['date2'] = pd.to_datetime(lst[1][1] if len(lst[1])>1 else '')\n    \n    row['date1']= pd.to_datetime(row['date1'])\n    row['date2']=pd.to_datetime(row['date2'])\n    \n    return row","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef get_date_delta(row, date_min, date_max):\n    date1=row['date1']\n    date2=row['date2']\n    delta = date_max-date_min\n    if not pd.isnull(date1) and not pd.isnull(date2):\n        #print(date2, date1, date2-date1)\n        delta = date1 - date2 if date2<date1 else date2-date1\n    elif not pd.isnull(date1):\n        delta = date1 - date_min\n    return delta.total_seconds()\n\ndef get_last_date_delta(row, date_min, date_max):\n    date1=row['date1']\n    date2=row['date2']\n    last_date=date_min\n    if not pd.isnull(date1):\n        last_date=date1\n    if (not pd.isnull(date2)) and date2 > date1:\n        last_date=date2\n#    return (datetime.now()-last_date).total_seconds()\n    return (date_max-last_date).total_seconds()\n\ndef get_first_date_delta(row, date_min, date_max):\n    date1=row['date1']\n    date2=row['date2']\n    first_date=date_min\n    if not pd.isnull(date1):\n        first_date=date1\n    if (not pd.isnull(date2)) and date2 < date1:\n        first_date=date2\n    return (first_date - date_min).total_seconds()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"allwords=pd.Series()\ndef collectwords(x):\n    print(type(x))\n    global allwords\n    #x=map(lambda s: s.lower(), x)\n    #s=pd.Series(x)\n    allwords=allwords.append(x.str.lower())\n    \ndef get_dummies_words(df,colname, words, init=True):\n    words_re=re.compile('\\w+')\n    def find_item(row):\n        item=row[colname]\n        if type(item)==str:\n            lst=list(words_re.findall(item.lower()))\n            count = 0\n#            for v in words:\n#                if v in lst:\n#                #if v in item.lower():\n#                    \n#                    row[prefix+v]=row[prefix+v]+1\n#                    count = count + 1\n            for v in lst:\n                if v in words:\n                    row[prefix+v]=row[prefix+v]+1\n                    count = count + 1\n            row[prefix + 'COUNT']=row[prefix + 'COUNT']+len(lst)\n            row[prefix + 'NOWORDS']= row[prefix + 'NOWORDS'] + (1 if count == 0 else 0)\n        return row\n        \n    prefix='word_'\n    if init:\n        for v in words:\n            df[prefix+v]=0\n            df[prefix + 'COUNT']=0\n            df[prefix + 'NOWORDS']=0\n            \n    df=df.apply(find_item, axis=1)\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def is_closed(x):\n    if pd.isna(x['rank1']) and pd.isna(x['rank2']) and pd.isna(x['rank_main']):\n        return 1\n    else:\n        return 0\n\ndef top_rank(x):\n    top_rank = x['Ranking']\n    if not(pd.isna(x['rank1'])) and top_rank < x['rank1']:\n        top_rank = x['rank1']\n    if not(pd.isna(x['rank2'])) and top_rank < x['rank2']:\n        top_rank = x['rank2']\n    if not(pd.isna(x['rank_main'])) and top_rank < x['rank_main']:\n        top_rank = x['rank_main']\n    return float(top_rank)\n\ndef get_rank_norm(x, field, ranks_by_city):\n    maxr = ranks_by_city.loc[x['City']][( field, 'max')] / ranks_by_city.loc[x['City']][( 'population', 'max')]\n    minr = ranks_by_city.loc[x['City']][( field, 'min')] / ranks_by_city.loc[x['City']][( 'population', 'max')]\n    res=x[field]/ ranks_by_city.loc[x['City']][( 'population', 'max')]\n    return (res - minr) / (maxr - minr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def round_rating(x):\n    return (round(x*2.0)/2)\n\ndef norm(x, fieldname):\n    return (x[fieldname]-x[fieldname].min())/(x[fieldname].max()-x[fieldname].min())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def is_local_cuisine(row):\n    local_cuisine_by_country = {\n        'United Kingdom':['British','Scottish'],\n        'Spain': ['Spanish'],\n        'France': ['French','Central European'], \n        'Italy': ['Italian','Central European'],\n        'Germany': ['Dutch','German','Central European'],\n        'Portugal': ['Portuguese'],\n        'Czechia': ['Czech','Eastern European'],\n        'Poland':['Polish','Eastern European'],\n        'Austria': ['Austrian','Central European'],\n        'Netherlands':['Scandinavian'],\n        'Belgium': ['Belgian','Eastern European'],\n        'Switzerland':['Swiss','Central European'],\n        'Sweden':['Scandinavian'],\n        'Hungary':['Hungarian','Eastern European'],\n        'Ireland':['Irish'],\n        'Denmark':['Scandinavian'],\n        'Greece':['Greece'],\n        'Norway':['Scandinavian'],\n        'Finland':['Scandinavian'],\n        'Slovakia':['Eastern European'],\n        'Luxembourg':['Eastern European'],\n        'Slovenia':['Slovenian','Eastern European']\n    }\n    loc_cus = local_cuisine_by_country[row['country']]\n    for l in loc_cus:\n        if l in row['Cuisine_'] and l != '':\n            return 1\n    else:\n        return 0\n    return 0\n\ndef is_pop_cuisine(row):\n    pop_cuisine = ['Cafe','Grill','International','Mediterranean','Fast Food','Pizza','Pub','Sushi']\n    \n    for l in pop_cuisine:\n        if l in row['Cuisine_'] and l != '':\n            return 1\n    else:\n        return 0\n    return 0\n\ndef is_veget_cuisine(row):\n    veget_cuisine = ['Vegetarian Friendly', 'Healthy', 'Vegan Options', 'Gluten Free Options']\n    \n    for l in veget_cuisine:\n        if l in row['Cuisine_'] and l != '':\n            return 1\n    else:\n        return 0\n    return 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preproc_data(df_input):\n    '''includes several functions to pre-process the predictor data.'''\n    \n    df_output = df_input.copy()\n    \n    # ################### 1. Предобработка ############################################################## \n    # убираем не нужные для модели признаки\n    \n    #df_output.drop(['Restaurant_id','ID_TA',], axis = 1, inplace=True)\n    df_output['rest_id']=df_output['Restaurant_id'].apply(lambda x: x[3:])\n    df_output['id_ta']=df_output['ID_TA'].apply(lambda x:int(x[1:]))\n    rurl_id = re.compile('Review-g(\\d+)-')\n    df_output['url_']=df_output['URL_TA'].apply(lambda s: rurl_id.findall(s)[0])\n    \n    df_output['Number_of_Reviews_isNAN'] = pd.isna(df_output['Number of Reviews']).astype('uint8')\n    \n    # ################### 2. NAN ############################################################## \n    # Далее заполняем пропуски, вы можете попробовать заполнением средним или средним по городу и тд...\n    df_output['Number of Reviews'].fillna(0, inplace=True)\n    # тут ваш код по обработке NAN\n    # ....\n    df_output['nreviews_norm']=(df_output['Number of Reviews']-df_output['Number of Reviews'].min())/(df_output['Number of Reviews'].max()-df_output['Number of Reviews'].min())\n    \n#CORR\n#    df_output.drop(['Number of Reviews'], axis=1, inplace=True)\n    \n    # ################### 3. Encoding ############################################################## \n    # для One-Hot Encoding в pandas есть готовая функция - get_dummies. Особенно радует параметр dummy_na\n    #Перенесена ниже\n    #df_output = pd.get_dummies(df_output, columns=[ 'City',], dummy_na=True)\n    # тут ваш код не Encoding фитчей\n    \n    # Price Range\n    df_output['Price_']=df_output['Price Range'].apply(conv_price)\n    price_mean = df_output['Price_'].dropna().mean()\n#CORR: Уровень цен \n    price_top = df_output['Price_'].dropna().nlargest(1).iloc[0]\n    df_output['Price_'].fillna(price_top, inplace=True)\n\n    global cities, ranks, wordcounts    \n    \n    # City -> Country, capital, population\n    df_output['City'] = df_output['City'].apply(lambda x: 'Porto' if x =='Oporto' else x)\n#CORR: Убираем города    \n    gen_valcols('City', df_output)\n\n    cities = cities.drop(cities[cities.country == 'United States'].index, axis=0)\n    cities = cities.drop(cities[cities.country == 'Canada'].index, axis=0)\n    cities = cities.drop(cities[cities.country == 'Venezuela'].index, axis=0)\n    df_output=df_output.merge(cities[['city_ascii','capital','country','population', 'lat', 'lng']], how='left', left_on='City', right_on='city_ascii')\n    \n    df_output['pop_n']=norm(df_output, 'population')\n# CORR: Убираем координаты\n    df_output['lat_n']=norm(df_output, 'lat')\n    df_output['lng_n']=norm(df_output, 'lng')\n#    df_output.drop(['population','lat','lng'], axis=1, inplace=True)\n    df_output.drop(['lat','lng'], axis=1, inplace=True)\n    \n    # Cuisine Style\n    df_output['Cuisine_']=df_output['Cuisine Style'].apply(conv_cuisine)\n    df_output['Cuisine_count'] = df_output['Cuisine_'].apply(lambda x: len(x))\n    df_output['Cuisine_count_n']=norm(df_output,'Cuisine_count')\n    df_output.drop(['Cuisine_count'], axis=1, inplace=True)\n    \n    df_output['local_cuisine']=df_output.apply(is_local_cuisine, axis=1)\n    df_output['pop_cuisine']=df_output.apply(is_pop_cuisine, axis=1)\n    df_output['veget_cuisine']=df_output.apply(is_veget_cuisine, axis=1)\n    #gen_valcols('Cuisine_', df_output)\n    \n    # ################### 4. Feature Engineering ####################################################\n    # тут ваш код не генерацию новых фитчей\n    # ....\n    \n#CORR: Страна?\n#    df_output=pd.get_dummies(df_output, columns=['capital', 'country'], dummy_na=True)\n    df_output=pd.get_dummies(df_output, columns=['country'], dummy_na=True)\n    \n    \n    \n    #Rankings adds\n    df1 = df_output.merge(ranks, left_on='URL_TA', right_on='url', how='left')\n    df_output = df1\n    df_output['closed']=df_output.apply(is_closed, axis=1)\n    df_output['top_rank']=df_output.apply(top_rank, axis=1)\n    df_output.drop(['rank1', 'rank2', 'url', 'rank_main'], axis=1, inplace=True)\n    \n    ranks_by_city = df_output.groupby('City')[['Ranking', 'top_rank', 'Number of Reviews', 'population']].agg(['max', 'min'])\n    df_output['rank_norm'] = df_output[['Ranking', 'City']].apply(get_rank_norm, axis=1, field='Ranking', ranks_by_city=ranks_by_city)\n\n    df_output['rank_top_norm'] = df_output[['top_rank', 'City']].apply(get_rank_norm, axis=1, field='top_rank', ranks_by_city=ranks_by_city)\n    \n#    df_output['nreviews_norm'] = df_output[['Number of Reviews', 'City']].apply(get_rank_norm, axis=1, field='Number of Reviews', ranks_by_city=ranks_by_city)\n    \n    df_output.drop(['Ranking', 'top_rank', 'Number of Reviews', 'population'], axis=1, inplace=True)\n        \n    \n    #Reviews\n    # - Base\n    df_output=df_output.apply(extr_rev, axis=1)\n    \n    # - Dates\n    date_min = min([df_output.date1.min(),df_output.date2.min()])\n    date_max = max([df_output.date1.max(),df_output.date2.max()])\n    df_output['date_delta_sec']=df_output[['date1','date2']].apply(get_date_delta, axis=1, date_min=date_min, date_max=date_max)\n    df_output['last_date_delta']=df_output[['date1','date2']].apply(get_last_date_delta, axis=1, date_min=date_min, date_max=date_max)\n    df_output['first_date_delta']=df_output[['date1','date2']].apply(get_first_date_delta, axis=1, date_min=date_min, date_max=date_max)\n\n#CORR Даты    \n    df_output['date_delta_n']=norm(df_output,'date_delta_sec')\n    df_output['last_date_delta_n']=norm(df_output,'last_date_delta')\n    df_output['first_date_delta_n']=norm(df_output,'first_date_delta')\n\n    df_output['date_n_mult']=df_output['date_delta_n'] * df_output['first_date_delta_n']\n    df_output.drop(['date_delta_n','last_date_delta_n','first_date_delta_n'],axis=1, inplace=True)\n    \n    df_output.drop(['date_delta_sec','last_date_delta','first_date_delta'],axis=1, inplace=True)\n\n    # - Words in Review\n    df_output=get_dummies_words(df_output, 'rev1',wordcounts.word)\n    df_output=get_dummies_words(df_output, 'rev2',wordcounts.word, init=False)\n    \n    df_output['word_COUNT_n'] = norm(df_output, 'word_COUNT')\n    df_output.drop(['word_COUNT'], axis=1, inplace=True)\n    \n    #df_output = pd.get_dummies(df_output, columns=[ 'City',], dummy_na=True)\n    \n    # ################### 5. Clean #################################################### \n    # убираем признаки которые еще не успели обработать, \n    # модель на признаках с dtypes \"object\" обучаться не будет, просто выберем их и удалим\n    object_columns = [s for s in df_output.columns if df_output[s].dtypes in ['object','<m8[ns]','<M8[ns]']]\n    df_output.drop(object_columns, axis = 1, inplace=True)\n    \n    return df_output","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":">По хорошему, можно было бы перевести эту большую функцию в класс и разбить на подфункции (согласно ООП). ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Запускаем и проверяем что получилось","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_preproc = preproc_data(data)\ndf_preproc.sample(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_preproc.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Теперь выделим тестовую часть\ntrain_data = df_preproc.query('sample == 1').drop(['sample'], axis=1)\ntest_data = df_preproc.query('sample == 0').drop(['sample'], axis=1)\n\ny = train_data.Rating.values            # наш таргет\nX = train_data.drop(['Rating'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Перед тем как отправлять наши данные на обучение, разделим данные на еще один тест и трейн, для валидации. \nЭто поможет нам проверить, как хорошо наша модель работает, до отправки submissiona на kaggle.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Воспользуемся специальной функцие train_test_split для разбивки тестовых данных\n# выделим 20% данных на валидацию (параметр test_size)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_SEED)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# проверяем\ntest_data.shape, train_data.shape, X.shape, X_train.shape, X_test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model \nСам ML","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Импортируем необходимые библиотеки:\nfrom sklearn.ensemble import RandomForestRegressor # инструмент для создания и обучения модели\nfrom sklearn import metrics # инструменты для оценки точности модели","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Создаём модель (НАСТРОЙКИ НЕ ТРОГАЕМ)\nmodel = RandomForestRegressor(n_estimators=100, verbose=1, n_jobs=-1, random_state=RANDOM_SEED)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Обучаем модель на тестовом наборе данных\nmodel.fit(X_train, y_train)\n\n# Используем обученную модель для предсказания рейтинга ресторанов в тестовой выборке.\n# Предсказанные значения записываем в переменную y_pred\ny_pred = model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred=list(map(round_rating,y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Сравниваем предсказанные значения (y_pred) с реальными (y_test), и смотрим насколько они в среднем отличаются\n# Метрика называется Mean Absolute Error (MAE) и показывает среднее отклонение предсказанных значений от фактических.\nprint('MAE:', metrics.mean_absolute_error(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# в RandomForestRegressor есть возможность вывести самые важные признаки для модели\nplt.rcParams['figure.figsize'] = (10,14)\nfeat_importances = pd.Series(model.feature_importances_, index=X.columns)\nfeat_importances.nlargest(20).plot(kind='barh')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Submission\nЕсли все устраевает - готовим Submission на кагл","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data.sample(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data = test_data.drop(['Rating'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predict_submission = model.predict(test_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predict_submission=list(map(round_rating,predict_submission))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predict_submission","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission['Rating'] = predict_submission\nsample_submission.to_csv('submission.csv', index=False)\nsample_submission.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# What's next?\nИли что делать, чтоб улучшить результат:\n* Обработать оставшиеся признаки в понятный для машины формат\n* Посмотреть, что еще можно извлечь из признаков\n* Сгенерировать новые признаки\n* Подгрузить дополнительные данные, например: по населению или благосостоянию городов\n* Подобрать состав признаков\n\nВ общем, процесс творческий и весьма увлекательный! Удачи в соревновании!\n","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}