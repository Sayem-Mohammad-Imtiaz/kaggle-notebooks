{"cells":[{"metadata":{},"cell_type":"markdown","source":"# I tried to keep this as simple as possible, no preprocessing nothing, grab the data , split it and apply the algorithm.\n\n# If you have any questions you can always ask in the comment's section or you can just google it.\n\n# So Let's begin","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# this cell is just to access the csv files from Kaggle so don't worry about it\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# reading the data into the dataframe\ndf=pd.read_csv('../input/mushroom-classification/mushrooms.csv')\n# let's take a peek into our data\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's check how many records we have and how many columns / features are there in our data\n\ndf.shape\n\n# So we have 8124 records/rows and 23 columns/features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's check our target columns/feature\ndf\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's look at our target varibale\ndf.columns\n\n# Here class is our target varibale so let's start by separating it ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's move our target into y ( just a convetion for better readbality)\ny=df['class']\ny","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's define our x with all other columns except the target columns\nx=df.iloc[:,1:]\nx","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's check if there are any missing values in our  dataset\n\nx.isna().sum()\n# as we can see there are no missing values so we can start creating our model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# There are different Classification algorithms available which can be applied to this dataset , However because we want to keep things simple we will start with a Simple apporach, a model which requires minimal preprocessing  ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from catboost import CatBoostClassifier\n\nmodel=CatBoostClassifier()\n\n# we have just defined our model with default parameters \n# Cat boost Classifier is a boosting based appraoch and can work with categorical variables which spare us from the \n# gard work of converting our non numeric columns into numeric using ( LabelEncoder , get_dummies etc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# now let's make our train and test datasets\nfrom sklearn.model_selection import train_test_split\n\nx_train,x_test,y_train,y_test=train_test_split(x,y,train_size=.25,random_state=3)\n\n# we have split our x , y into x_train, x_test and y_train,y_test respectively with 25% as our test size and 75% train size ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's convert our target to numeric value \nfrom sklearn.preprocessing import LabelEncoder\ny_train=pd.get_dummies(y_train,drop_first=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's tell our model which columns are of object type\na=x_train.select_dtypes(include='object')\nli=a.columns.to_list()\n# now let's fit our model \nmodel.fit(x_train,y_train,cat_features=li)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's check the training accuracy\nmodel.score(x_train,y_train)\n\n# model gives us a 100% accuracy ( looks like the model might be overfitting)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's see how our model performs on Test Data\n\nmodel.score(x_test,y_test)\n\n# model gives us a 100% accuracy ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's look at a classification report\n\nfrom sklearn.metrics import classification_report\nclassification_report(model.predict(x_test),y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# As we can see the model performs exceptionally well and can handle categorical variable as well , and gives us a 100% accuracy on this dataset.\n\n\n# Catboost algorithm is an excellent algorithm to be used for classification problems as it can handle categorical variables as well and saves our time and effort and is extremly simple to apply","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# If you like this notebook please check out my other noteboks and if you can please upvote so that even others can see it.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}