{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport re\nimport nltk\nimport string\nimport spacy\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom wordcloud import WordCloud\nfrom numpy.random import seed\nfrom sklearn import metrics\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nfrom sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.decomposition import LatentDirichletAllocation\nfrom nltk.corpus import stopwords\nfrom xgboost import XGBClassifier\n\nnltk.download('vader_lexicon')\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\n\npd.set_option('display.max_columns', None)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python -m spacy download en_vectors_web_lg\n!python -m spacy link en_vectors_web_lg en_vectors_web_lg_link","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nlp = spacy.load('en_vectors_web_lg_link')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"true_data = pd.read_csv('../input/fake-and-real-news-dataset/True.csv')\ntrue_data['class'] = 'True'\n\nfake_data = pd.read_csv('../input/fake-and-real-news-dataset/Fake.csv')\nfake_data['class'] = 'Fake'\n\ndf = true_data.append(fake_data).sample(frac = 1, random_state = 1)\ndf.index = range(len(true_data) + len(fake_data))\ndf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Text preparation","metadata":{}},{"cell_type":"code","source":"def del_punct(text):\n    chars = []\n    for char in text:\n        if char not in string.punctuation:\n            chars.append(char)\n        else:\n            chars.append(' ')\n    return ''.join(chars)\n\ndef text_preparation(text: str) -> str:\n    text = re.sub(r'^\\w+ \\(\\w+\\) - ', '', text)\n    text = text.lower()\n    text = del_punct(text)\n    doc = nlp(text)\n    text = ' '.join([\n            token.lemma_ \n                for token in doc \n                if token.text not in nlp.Defaults.stop_words \n        ])\n    \n    text = re.sub(r'\\d+', ' somenumbers ', text)\n    text = re.sub(r'\\s+', ' ', text)\n    \n    return text\n     \ndef processing(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    df['class_'] = df['class'].apply(lambda x: {'Fake': 1, 'True':0}[x])\n    df['len title'] = df['title'].apply(len)\n    df['len text'] = df['text'].apply(len)\n    df['text cleaned'] = df['text'].apply(text_preparation)\n    df['title cleaned'] = df['title'].apply(text_preparation)\n    df['cleaned all'] = df['text cleaned'] + ' ' + df['title cleaned']\n    return df\n\ndf_d = processing(df)\ndf_d.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Bag of words","metadata":{}},{"cell_type":"code","source":"bow_transformer = CountVectorizer(max_features = 1500).fit(df_d['cleaned all'])\nbow_df = bow_transformer.transform(df_d['cleaned all'])\nbow_df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"code","source":"def print_article(line):\n    print(\n        f'\\033[1m{line[\"title\"]} \\033[0m', \n        f\"tag: {line['subject']}\", \n        f\"class: {line['class']}\", \n        line['text'], \n        sep = '\\n\\n',\n        end = '\\n\\n\\n'\n    )\n    \n\nprint_article(df.iloc[0])\nprint_article(df.iloc[1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(df['class'].value_counts())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(df[['subject', 'class']].value_counts(), columns = ['#'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Length of title and text","metadata":{}},{"cell_type":"code","source":"g = sns.FacetGrid(df_d, hue='class', height = 7, aspect = 2)\ng.map(sns.kdeplot, 'len title')\nplt.title('Length of title distribution')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"g = sns.FacetGrid(df_d, hue='class', height = 7, aspect = 2)\ng.map(sns.kdeplot, 'len text')\nplt.title('Length of text distribution')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Fake news tend to have longer title, but there is not much difference in text lenght","metadata":{}},{"cell_type":"markdown","source":"#### Name of media","metadata":{}},{"cell_type":"code","source":"def name_of_media(text):\n    text = re.findall(r'(?<= \\()\\w+(?=\\) - )', text[:100])\n    if text:\n        return text[0]\n    else:\n        return 'Not provided'\n\nname_of_media(df['text'][0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['Name of media'] = df['text'].apply(name_of_media)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(df[['Name of media', 'class']].value_counts(), columns = ['#'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### News topics discovery through wordcloud","metadata":{}},{"cell_type":"code","source":"def text_for_cloud(label):\n    text = ' '.join(df_d['text'][df_d['class'] == label].to_list())\n    text = text.replace('somenumbers', '')\n    return text\n\nfor label in ['True', 'Fake']:\n    wordcloud = WordCloud(\n        max_font_size=500,\n        max_words=1000,\n        background_color=\"white\",\n    ).generate(text_for_cloud(label))\n\n    plt.figure(figsize=(12, 8))\n    plt.imshow(wordcloud, interpolation='bilinear')\n    plt.axis(\"off\")\n    plt.title(f'{label} news wordcloud', fontsize=20)\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### News topics discovery through Latent Dirichlet Allocation","metadata":{}},{"cell_type":"markdown","source":"We'll split news dataset to 12 topics. Number was chosen randomly.","metadata":{}},{"cell_type":"code","source":"LDA = LatentDirichletAllocation(\n    n_components = 12,\n    random_state = 1,\n    n_jobs = -1\n)\nLDA.fit(bow_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for index,topic in enumerate(LDA.components_):\n    print(f'The top 10 words for topic #{index}')\n    print([bow_transformer.get_feature_names()[i] for i in topic.argsort()[-10:]])\n    print('\\n')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Groups summary (my subjective assessment):","metadata":{}},{"cell_type":"code","source":"groups_summary = {\n    0:'~ E-mail scandal',\n    1:'Federal Government',\n    2:'Election',\n    3:'International Security',\n    4:'EU/UK related',\n    5:'Clinton vs. Trump election',\n    6:'US President administration',\n    7:'Law',\n    8:'-',\n    9:'US Domestic policy',\n    10:'~ Finance',\n    11:'Ð¡rime',\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"topic_results = LDA.transform(bow_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_d['Topic N'] = topic_results.argmax(axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_d['Topic'] = df_d['Topic N'].apply(lambda x: groups_summary[x])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n = 224\nprint(f\"Topic: {df_d['Topic'][n]}\", end = '\\n\\n')\nprint_article(df.iloc[n])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(\n    df_d[['Topic', 'class']].value_counts(),\n    columns = ['#']\n).sort_values(by=['Topic'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Sentiment Analysis","metadata":{}},{"cell_type":"code","source":"sid = SentimentIntensityAnalyzer()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sid_df_title = pd.DataFrame([sid.polarity_scores(article) for article in df['title']])\nsid_df_text = pd.DataFrame([sid.polarity_scores(article) for article in df['text']])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sid_df_title['class'] = df['class']\nsid_df_text['class'] = df['class']\nsid_df_title.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"g = sns.FacetGrid(sid_df_title, hue='class', height = 7, aspect = 2)\ng.map(sns.kdeplot, 'compound')\nplt.title('Distribution of sentiment score for title')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"g = sns.FacetGrid(sid_df_text, hue='class', height = 7, aspect = 2)\ng.map(sns.kdeplot, 'compound')\nplt.title('Distribution of sentiment score for text')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see here that fake news tend to have more radical sentiment score values, especially in title. It means that fake news titles tend to have more strong emotional coloring. Also fake news text more often has negative emotion coloring.","metadata":{}},{"cell_type":"markdown","source":"# Modeling","metadata":{}},{"cell_type":"markdown","source":"#### Splits","metadata":{}},{"cell_type":"code","source":"df_else, df_validation  = train_test_split(df_d,\n                                test_size=0.25,\n                                random_state = 101)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train, df_test  = train_test_split(df_else,\n                                test_size=0.25,\n                                random_state = 101)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"datasets = {\n    'train'        : df_train, \n    'test'         : df_test, \n    'train + test' : df_else, \n    'validation'   : df_validation\n}\n\nfor dataset_name, dataset in datasets.items():\n    print('\\n' + dataset_name + ':')\n    display(pd.DataFrame(dataset['class'].value_counts()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Bag of words","metadata":{}},{"cell_type":"code","source":"bow_transformer = CountVectorizer(max_features = 1500).fit(df_train['cleaned all'])\nbow_train = bow_transformer.transform(df_train['cleaned all'])\nbow_train.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tfidf_transformer = TfidfTransformer().fit(bow_train)\ntrain_tfidf = tfidf_transformer.transform(bow_train)\n\nbow_test = bow_transformer.transform(df_test['cleaned all'])\ntest_tfidf = tfidf_transformer.transform(bow_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_tfidf = pd.DataFrame.sparse.from_spmatrix(train_tfidf)\nX_train_tfidf.columns = bow_transformer.get_feature_names()\nX_test_tfidf = pd.DataFrame.sparse.from_spmatrix(test_tfidf)\nX_test_tfidf.columns = bow_transformer.get_feature_names()\n\ny_train = df_train['class_']\ny_test = df_test['class_']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_tfidf.iloc[1:5,1:20]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Modeling on BoW","metadata":{}},{"cell_type":"code","source":"def feature_importance(model, X):\n    features = pd.DataFrame({\n                'Variable'  : X.columns,\n                'Importance': model.feature_importances_\n            })\n    features.sort_values('Importance', ascending=False, inplace=True)\n    display(features.head(20))\n    \n\ndef eval_result(model, X_test, y_test, show_feature_imp = True):\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n        pred = model.predict(X_test)\n        print(classification_report(y_test, pred, target_names = ['True', 'Fake']))\n        display(pd.DataFrame(confusion_matrix(y_test, pred), \n                         columns = ['Predicted True', 'Predicted Fake'],\n                         index = ['True', 'Fake']))\n        print(f'Accuracy: {round(accuracy_score(y_test, pred), 5)}')\n        \n        if hasattr(model, 'feature_importances_') and show_feature_imp:\n            feature_importance(model, X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lgmodel = LogisticRegression(\n    solver='lbfgs', \n    n_jobs = -1,\n    random_state = 101\n)\nlgmodel.fit(X_train_tfidf, y_train)\neval_result(lgmodel, X_test_tfidf, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nb_model = MultinomialNB()\nnb_model.fit(X_train_tfidf, y_train)\neval_result(nb_model, X_test_tfidf, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dtc = DecisionTreeClassifier(random_state = 1)\ndtc.fit(X_train_tfidf, y_train)\neval_result(dtc, X_test_tfidf, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rfc = RandomForestClassifier(n_jobs = -1, random_state = 1)\nrfc.fit(X_train_tfidf, y_train)\neval_result(rfc, X_test_tfidf, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgbr = XGBClassifier(\n    random_state = 1,\n    n_jobs = -1,\n    eval_metric = 'logloss',\n    use_label_encoder = False\n)\nxgbr.fit(X_train_tfidf, y_train)\neval_result(xgbr, X_test_tfidf, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Modeling with Sentiment data","metadata":{}},{"cell_type":"code","source":"sid = SentimentIntensityAnalyzer()\n\nsid_df_train_title = pd.DataFrame([sid.polarity_scores(article) for article in df_train['title']])\nsid_df_train_title.columns = ['sent: title_' + col for col in sid_df_train_title.columns]\nsid_df_train_text = pd.DataFrame([sid.polarity_scores(article) for article in df_train['text']])\nsid_df_train_text.columns = ['sent: text_' + col for col in sid_df_train_text.columns]\n\nsid_df_test_title = pd.DataFrame([sid.polarity_scores(article) for article in df_test['title']])\nsid_df_test_title.columns = ['sent: title_' + col for col in sid_df_test_title.columns]\nsid_df_test_text = pd.DataFrame([sid.polarity_scores(article) for article in df_test['text']])\nsid_df_test_text.columns = ['sent: text_' + col for col in sid_df_test_text.columns]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_sid = pd.concat([sid_df_train_title, sid_df_train_text], axis = 1)\nX_test_sid = pd.concat([sid_df_test_title, sid_df_test_text], axis = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_sid.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lgmodel_sid = LogisticRegression(\n    solver='lbfgs', \n    n_jobs = -1,\n    random_state = 101\n)\nlgmodel_sid.fit(X_train_sid, y_train)\neval_result(lgmodel_sid, X_test_sid, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dtc_sid = DecisionTreeClassifier(random_state = 1)\ndtc_sid.fit(X_train_sid, y_train)\neval_result(dtc_sid, X_test_sid, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rfc_sid = RandomForestClassifier(n_jobs = -1, random_state = 1)\nrfc_sid.fit(X_train_sid, y_train)\neval_result(rfc_sid, X_test_sid, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgbr_sid = XGBClassifier(\n    random_state = 1,\n    n_jobs = -1,\n    eval_metric = 'logloss',\n    use_label_encoder = False\n)\nxgbr_sid.fit(X_train_sid, y_train)\neval_result(xgbr_sid, X_test_sid, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Modeling with Topic modeling data","metadata":{}},{"cell_type":"code","source":"LDA = LatentDirichletAllocation(n_components=12,random_state=1)\nLDA.fit(bow_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for index,topic in enumerate(LDA.components_):\n    print(f'The top 10 words for topic #{index}')\n    print([bow_transformer.get_feature_names()[i] for i in topic.argsort()[-10:]])\n    print('\\n')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Groups summary (my subjective assessment):","metadata":{}},{"cell_type":"code","source":"groups_summary2 = {\n    0:'~ BLM protests',\n    1:'Clinton vs. Trump election',\n    2:'Migration Policy',\n    3:'~ family related',\n    4:'Law',\n    5:'~ E-mail scandal',\n    6:'US Election',\n    7:'Middle East wars and Terrorism',\n    8:'US Presidents related',\n    9:'US Domestic policy',\n    10:'EU Domestic policy',\n    11:'Far East security',\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_topics = pd.DataFrame(LDA.transform(bow_train))\nX_test_topics = pd.DataFrame(LDA.transform(bow_test))\nprint(f'train shape: {X_train_topics.shape}, test shape: {X_test_topics.shape}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_topics.columns = ['topic: ' + groups_summary2[x] for x in X_train_topics.columns]\nX_test_topics.columns = ['topic: ' + groups_summary2[x] for x in X_test_topics.columns]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_topics.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lgmodel_topics = LogisticRegression(\n    solver='lbfgs', \n    n_jobs = -1,\n    random_state = 101\n)\nlgmodel_topics.fit(X_train_topics, y_train)\neval_result(lgmodel_topics, X_test_topics, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dtc_topics = DecisionTreeClassifier(random_state = 1)\ndtc_topics.fit(X_train_topics, y_train)\neval_result(dtc_topics, X_test_topics, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rfc_topics = RandomForestClassifier(n_jobs = -1, random_state = 1)\nrfc_topics.fit(X_train_topics, y_train)\neval_result(rfc_topics, X_test_topics, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgbr_topics = XGBClassifier(\n    random_state = 1,\n    n_jobs = -1,\n    eval_metric = 'logloss',\n    use_label_encoder = False\n)\nxgbr_topics.fit(X_train_topics, y_train)\neval_result(xgbr_topics, X_test_topics, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Combined model: BoW + Sentiment data + Topics data (hierarchical)","metadata":{}},{"cell_type":"code","source":"class CombinedModel:\n    \n    def __init__(self, bow_model, sent_model, topic_model, gen_model):\n        self.bow_model = bow_model\n        self.sent_model = sent_model\n        self.topic_model = topic_model\n        self.gen_model = gen_model\n    \n    def __generate_data_for_gen_model(self, X_in):\n        X_tfidf, X_sid, X_topics, X_len = X_in\n        \n        bow_model_pred = pd.Series(self.bow_model.predict(X_tfidf), name =  'bow')\n        sent_model_pred = pd.Series(self.sent_model.predict(X_sid), name = 'sent')\n        topic_model_pred = pd.Series(self.topic_model.predict(X_topics), name = 'topic')\n              \n        X_general = pd.concat([\n            bow_model_pred, \n            sent_model_pred, \n            topic_model_pred, \n            X_len\n        ], axis = 1)\n        \n        return X_general\n        \n    def fit(self, X_train, y_train):\n        self.X_train_general = self.__generate_data_for_gen_model(X_train)\n        self.gen_model.fit(self.X_train_general, y_train)\n        if hasattr(self.gen_model, 'feature_importances_'):\n            self.feature_importances_ = self.gen_model.feature_importances_\n            \n    def predict(self, X_test):\n        X_test_general = self.__generate_data_for_gen_model(X_test)\n        return self.gen_model.predict(X_test_general)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_len = df_train[['len title', 'len text']]\nX_train_len.reset_index(inplace = True, drop = True)\nX_test_len = df_test[['len title', 'len text']]\nX_test_len.reset_index(inplace = True, drop = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lgmodel_gen = LogisticRegression(\n    solver='lbfgs', \n    n_jobs = -1,\n    random_state = 101\n)\n\ncomb_lg = CombinedModel(\n    bow_model = xgbr,\n    sent_model = rfc_sid,\n    topic_model = rfc_topics,\n    gen_model = lgmodel_gen\n)\n\ncomb_lg.fit(\n    X_train = (X_train_tfidf, X_train_sid, X_train_topics, X_train_len),\n    y_train = y_train\n)\n\neval_result(\n    model = comb_lg, \n    X_test = (X_test_tfidf, X_test_sid, X_test_topics,  X_test_len), \n    y_test = y_test\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dtc_gen = DecisionTreeClassifier(random_state = 1)\n\ncomb_dtc = CombinedModel(\n    bow_model = xgbr,\n    sent_model = rfc_sid,\n    topic_model = rfc_topics,\n    gen_model = dtc_gen\n)\n\ncomb_dtc.fit(\n    X_train = (X_train_tfidf, X_train_sid, X_train_topics, X_train_len),\n    y_train = y_train\n)\n\neval_result(\n    model = comb_dtc, \n    X_test = (X_test_tfidf, X_test_sid, X_test_topics,  X_test_len), \n    y_test = y_test,\n    show_feature_imp = False\n)\n\nfeature_importance(model = comb_dtc, X = comb_dtc.X_train_general)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rfc_gen = RandomForestClassifier(n_jobs = -1, random_state = 1)\n\ncomb_rfc = CombinedModel(\n    bow_model = xgbr,\n    sent_model = rfc_sid,\n    topic_model = rfc_topics,\n    gen_model = rfc_gen\n)\n\ncomb_rfc.fit(\n    X_train = (X_train_tfidf, X_train_sid, X_train_topics, X_train_len),\n    y_train = y_train\n)\n\neval_result(\n    model = comb_rfc, \n    X_test = (X_test_tfidf, X_test_sid, X_test_topics,  X_test_len), \n    y_test = y_test,\n    show_feature_imp = False\n)\n\nfeature_importance(model = comb_rfc, X = comb_rfc.X_train_general)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgbr_gen = XGBClassifier(\n    random_state = 1,\n    n_jobs = -1,\n    eval_metric = 'logloss',\n    use_label_encoder = False\n)\n\ncomb_xgbr = CombinedModel(\n    bow_model = xgbr,\n    sent_model = rfc_sid,\n    topic_model = rfc_topics,\n    gen_model = xgbr_gen\n)\n\ncomb_xgbr.fit(\n    X_train = (X_train_tfidf, X_train_sid, X_train_topics, X_train_len),\n    y_train = y_train\n)\n\neval_result(\n    model = comb_xgbr, \n    X_test = (X_test_tfidf, X_test_sid, X_test_topics,  X_test_len), \n    y_test = y_test,\n    show_feature_imp = False\n)\n\nfeature_importance(model = comb_xgbr, X = comb_xgbr.X_train_general)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Combined model: BoW + Sentiment data + Topics data (flat)","metadata":{}},{"cell_type":"code","source":"X_train_full = pd.concat([X_train_tfidf, X_train_sid, X_train_topics, X_train_len], axis = 1)\nX_test_full = pd.concat([X_test_tfidf, X_test_sid, X_test_topics,  X_test_len], axis = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_full.iloc[:5, -30:]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lgmodel_comb2 = LogisticRegression(\n    solver='lbfgs', \n    n_jobs = -1,\n    random_state = 101\n)\nlgmodel_comb2.fit(X_train_full, y_train)\neval_result(lgmodel_comb2, X_test_full, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dtc_comb2 = DecisionTreeClassifier(random_state = 1)\ndtc_comb2.fit(X_train_full, y_train)\neval_result(dtc_comb2, X_test_full, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rfc_comb2 = RandomForestClassifier(n_jobs = -1, random_state = 1)\nrfc_comb2.fit(X_train_full, y_train)\neval_result(rfc_comb2, X_test_full, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgbr_comb2 = XGBClassifier(\n    random_state = 1,\n    n_jobs = -1,\n    eval_metric = 'logloss',\n    use_label_encoder = False\n)\nxgbr_comb2.fit(X_train_full, y_train)\neval_result(xgbr_comb2, X_test_full, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Looks like the last model is the best of all - XGBClassifier on combined data","metadata":{}},{"cell_type":"markdown","source":"# Validation","metadata":{}},{"cell_type":"code","source":"#TF_IDF\nbow_val = bow_transformer.transform(df_validation['cleaned all'])\nval_tfidf = tfidf_transformer.transform(bow_val)\n\n\nX_val_tfidf = pd.DataFrame.sparse.from_spmatrix(val_tfidf)\nX_val_tfidf.columns = bow_transformer.get_feature_names()\n\n#y\ny_val = df_validation['class_']\n\n\n#SA\nsid_df_val_title = pd.DataFrame([sid.polarity_scores(article) for article in df_validation['title']])\nsid_df_val_title.columns = ['sent: title_' + col for col in sid_df_val_title.columns]\nsid_df_val_text = pd.DataFrame([sid.polarity_scores(article) for article in df_validation['text']])\nsid_df_val_text.columns = ['sent: text_' + col for col in sid_df_val_text.columns]\n\nX_val_sid = pd.concat([sid_df_val_title, sid_df_val_text], axis = 1)\n\n##topics\nX_val_topics = pd.DataFrame(LDA.transform(bow_val))\nX_val_topics.columns = ['topic: ' + groups_summary2[x] for x in X_val_topics.columns]\n\n\n#lenght\nX_val_len = df_validation[['len title', 'len text']]\nX_val_len.reset_index(inplace = True, drop = True)\n\n\n#full\nX_val_full = pd.concat([X_val_tfidf, X_val_sid, X_val_topics,  X_val_len], axis = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'X_test_full.shape: {X_test_full.shape}')\nprint(f'X_val_full.shape: {X_val_full.shape}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_columns = set(X_test_full.columns)\nval_columns = set(X_val_full.columns)\ntest_columns ^ val_columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eval_result(xgbr_comb2, X_val_full, y_val)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}