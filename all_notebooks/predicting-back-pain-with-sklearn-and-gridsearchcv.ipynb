{"cells":[{"metadata":{"_uuid":"c0aa8138e244c999da22aaf0266b38d122b2ab3e"},"cell_type":"markdown","source":"# Predicting back pain with `sklearn` and `GridSearchCV`\nI'm going to look at some medical biometric data, and then predict whether an individual is going to sugger from abnormal back pain that was seen in the patients previously using these measurements. It is quite a small data set, so I will be doing some preprocessing and model building and comparison `sklearn`; I'm hoping that this will act as a good reference point for people using `sklearn`. \n\nI'm going to focus on these main algorithms for binary classification problems:\n1. Decision trees\n2. Logisitic regression \n3. Support Vector Machines \n\nIf you are interested in seeing a kernel written in `R` on this dataset that uses `caret` then I have written one [here](https://www.kaggle.com/willcanniford/predicting-lower-back-pain-using-caret-in-r). I wrote that `R` one first, and during my Python learning, I have expanded that and decided to produce this kernel as well to transfer the findings and models into a different programming language.   \n\nUsing `GridSearchCV` here I implement cross-validation search across parameters for certain `sklearn` models and then investigate the information provided by this feedback to try and show the advantage of exploring different metrics when choosing final model parameters. "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# Import packages\nimport matplotlib.pyplot as plt\n%matplotlib inline \nimport seaborn as sns\nimport numpy as np \nimport pandas as pd ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bb367879727280517cad5d646de17840a20df999"},"cell_type":"markdown","source":"## Reading in and cleaning our data \nFirstly, we are going to read in the data and clean it so that we are able to work with it with more clarity, and to make the model building stage easier. You'll see that the last column of the data that is given to us contains the names of the other columns in the set. We can use this information to name the columns, and then drop this 'meta' column from the data frame. "},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"raw_data = pd.read_csv('../input/Dataset_spine.csv')\n\n# View the column names given in the last column\nraw_data.iloc[:, -1].head(20)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7f448a49ea4e563027e2dea676859a8e7bb5cb8a"},"cell_type":"markdown","source":"While we are cleaning the data, and renaming the columns using the last column's information, we will also split the data into feature variables and response. This is going to make it easier for us later when we are actually modelling the data. With `sklearn` it requires us to have separated the labels from the data and pass them in separately.  "},{"metadata":{"trusted":true,"_uuid":"ea60818df51516d3a71f81d0f2ff8fb6a9198e24"},"cell_type":"code","source":"# Remove the last column that only contains meta data for us\ndata = raw_data.iloc[:, :-1]\n\n# View the data prior to renaming the columns for comparison\nprint(data.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"102cb975fe3876ab8fcb8b1405f8f80a5f14972c"},"cell_type":"code","source":"# Rename the columns in place using a dictionary and the information found in the 13th column\nmeta_column_names = {\"Col1\" : \"pelvic_incidence\", \"Col2\" : \"pelvic_tilt\",\n                     \"Col3\" : \"lumbar_lordosis_angle\",\"Col4\" : \"sacral_slope\", \n                     \"Col5\" : \"pelvic_radius\",\"Col6\" : \"degree_spondylolisthesis\", \n                     \"Col7\" : \"pelvic_slope\",\"Col8\" : \"direct_tilt\", \n                     \"Col9\" : \"thoracic_slope\", \"Col10\" :\"cervical_tilt\", \n                     \"Col11\" : \"sacrum_angle\", \"Col12\" : \"scoliosis_slope\"}\n\n# Rename the columns using the above dictionary, and replace the existing data object with inplace = True\ndata.rename(columns = meta_column_names, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f3bffe10ee3d1fda213506c2aef036ec9333490"},"cell_type":"markdown","source":"Let's split the data into features (`X`) and response (`y`). Note that we have already removed the 'meta' column, so we don't have to worry about that, and can just take the final column of the dataset that represents whether the patient suffered from abnormal back pain or not (`Class_att`).  \n\nLet's also just check that the dimensions of the two variables match up. We need the features to have the same number of observations/rows as the labels that are contained with `y`. "},{"metadata":{"trusted":true,"_uuid":"396b9652ced444bc5e75ad949f1403b173b2a33d"},"cell_type":"code","source":"# Features\nX = data.iloc[:, :-1]\n# Response\ny = data.iloc[:, -1]\n\nprint(X.shape)\nprint(y.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cce09835712a3315f4bf74e6814d24ade96fa243"},"cell_type":"markdown","source":"That all looks good (we know that we have 310 observations, and 12 features); we can have a final look at the data as a whole, but just keep in mind that we are going to mainly working with the `X` and `y` variables that we have defined. "},{"metadata":{"trusted":true,"_uuid":"4d8626dbf9ae03751813499d53acf5bc4beceb20"},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c697603a9f01e59b0c83d79bcba0404736de2e82"},"cell_type":"markdown","source":"# Visualising the data \nOne of the traditional starting points of any analysis, is the visualisation of the data to try and get a feel for the values, and the interactions between some of the features. \n\n## Visualising the correlations between the feature variables with `.corr()`\nNow that we have cleaned up the column names, we can have a quick look at the diferent feature variables present in the data and their interactions, as well as try and visualise our information prior to building and comparing our different prediction models. A useful correlation visualisation is using `.corr()` on the dataset. If you have a high number of feature variables, then it is probably for the best to filter the columns prior to using the visualisation, otherwise it might become too cluttered to be informative. "},{"metadata":{"trusted":true,"_uuid":"bed9120adaddfd010a30e5acdcbfba9044e06e6c"},"cell_type":"code","source":"correlations = X.corr()\nf, ax = plt.subplots(figsize=(12, 12))\nsns.heatmap(correlations, square=True, cbar=True, annot=True, vmax=.9);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"74c8c815c18bedc7c74174b1263d5169a0aaaad0"},"cell_type":"markdown","source":"## Further visualising correlations with `seaborn`\nWe can expand on our previous plot using the `sns.pairplot()` function from `seaborn`. We can see that the first 6 columns seem to hold higher correlations between them, while the remaining variables are seemingly unrelated, Let's have a look at the pairplot of those first 6 columns. "},{"metadata":{"trusted":true,"_uuid":"c45cabcf3043290aba04ea4e22c224a755ffe9d8"},"cell_type":"code","source":"sns.pairplot(X.iloc[:, :6], height = 1.75)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fb8dfc27b4be93e5cae27d11c3d5cec0d3fdcbad"},"cell_type":"markdown","source":"## Visualising differences between groups\nWhile it is important to note the correlations between variables, our main goal is to predict whether an individual observation is likely to suffer from normal or abnormal pain. This means that is probably more informative to look through the feature variables with them split by group so that we find particular feature variables that are of immediate interest to us regarding predictive power. \n\nThere are further techniques that involve the combination of feature variables to improve the predictive power of a model; this could either be multiplicative between certain variables or additive. I may explore this at a later date to see if we can further improve the models' performance. "},{"metadata":{"trusted":true,"_uuid":"4d93374acdddab6148dd31283eb3e960d5062386","scrolled":true},"cell_type":"code","source":"fg, ax = plt.subplots(ncols=4, nrows=3, sharex=False, sharey=False, figsize=(15,12))\nfg.tight_layout()\n# Add h spacing and space at the bottom for the legend\nplt.subplots_adjust(hspace=0.25, bottom = 0.1)\n\n# If we reshape the 2D numpy array that is returned from subplots for the axes then we can loop through without worrying about dimensions\nfor index, axes in enumerate(ax.reshape(-1)):\n    normal = sns.distplot(data[data.Class_att == 'Normal'].iloc[:, index], ax=axes, color='#2980b9', label = 'Normal')\n    abnormal = sns.distplot(data[data.Class_att == 'Abnormal'].iloc[:, index], ax=axes, color='#e74c3c', label = 'Abnormal')\n    \n# For this to work you have to have named the lines AND given labels \n# Get the handles and lines from the final axes - we can do this in this case because the lines are the same for every subplot\nh, l = axes.get_legend_handles_labels()\nfg.legend(h, l, loc='lower center', ncol = 2)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7856326d90bcabe5d6b2175affdb887b48f1e828"},"cell_type":"markdown","source":"The largest disparity that we see for a single variable between 'Normal' and 'Abnormal' is 'degree_spondylolisthesis'; let's visualise that on its own using a swarm plot. "},{"metadata":{"trusted":true,"_uuid":"77644c9a35f0e3e0badcbc8d7301821ae74cb801"},"cell_type":"code","source":"sns.swarmplot(y='degree_spondylolisthesis', x='Class_att', data=data)\nplt.title('degree_spondylolisthesis difference between classes\\n')\nplt.xlabel('')\nplt.ylabel('')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"29d29f074024cc435b3ea3c29c43dfed04ee2e7f"},"cell_type":"markdown","source":"While this highlights the difference in levels between the two classes, it also makes apparent that we have more \"Abnormal\" cases than \"Normal\" in this dataset. "},{"metadata":{"_uuid":"6460550471e3d52eb11cc759d6eacc931bbaf976"},"cell_type":"markdown","source":"# Training our models\nLet's start having a look at predicting the values that we are interested in. \n\n## Scaling the data using `StandardScaler` \nI'm going to scale the feature variables below using the `StandardScaler` from `sklearn.preprocessing`. This won't necessarily change the result of effectiveness of every algorithm, but those that are distance based will be regularised so that the scale of the variable doesn't impact it's respective weight in the classification stage. i.e. those variables that are generally larger will hold more weight when the classification is made if the data isn't scaled. We want all the variables on the same scale. \n\nIf we were later to recieve more data, as you would imagine a model like this might be used in a real-world application, then you can use the same `StandardScaler` object to scale that incoming data to be used with our model. This would likely be wrapped into a single package or cleaning function that you could reuse as the data was fed into the system. "},{"metadata":{"trusted":true,"_uuid":"cedce02b2f777fa7eac77d4b31cf58a8037ac265"},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\nprint(X_scaled)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fd3ab8b278a3c10d68e794c637df6850f77e6e87"},"cell_type":"code","source":"type(X_scaled)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5da01b2d2f2d6c53fa60bf3a75d37caf4c980f9d"},"cell_type":"markdown","source":"You'll note here that a `numpy.ndarray` has been returned, rather than a data frame. We have also lost our column names. If you want to have the scaled data in a `pd.DataFrame` then you'll have to rebuild it using the column names pulled from the original `X` data frame. I've written an example below of how you might do this if you wanted to easily visualise etc. "},{"metadata":{"trusted":true,"_uuid":"1f718ca30dc39898d06aa41c4da48988004eecfe"},"cell_type":"code","source":"scaled_df = pd.DataFrame(X_scaled, columns=X.columns)\nscaled_df['Class_att'] = y\nscaled_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"65632b0d68b69839ca19e0baa5a66dc4766770c8"},"cell_type":"markdown","source":"## Train test split\nEven though this dataset is small, I think that it is still best practice to implement a training and test split of the data to try and get a better indication of out-of-sample prediction after the model has been built.\n\nNow that we can see that we have scaled the features, and encoded the response, we can create our training and test splits of the data using `sklearn.model_selection.train_test_split`.  We are going to be performing a simple 75% train, 25% test split with our data. I have used a `random_state` to ensure that the resuls are consistent throughout the kernel, and that every time it is run the results are going to be the same for everyone that views this. \n\nI am also going to quickly demonstrate the difference between using `stratify` during the class to `train_test_split` to make sure that the classes are even between both the training and test sets. It is a small point here, but may be more important with other data sets to avoid the training set being unbalanced, or the test set not containing any rare positive cases for example. "},{"metadata":{"trusted":true,"_uuid":"80d32397bb545f725bcd5012f148611cf488175f"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size = 0.25, random_state = 1000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"848411d767e8850e38ff200a42e33b6bd6e2046c"},"cell_type":"code","source":"proportion_train = round(len(y_train[y_train == 'Abnormal'])/len(y_train)*100, 2)\nproportion_test = round(len(y_test[y_test == 'Abnormal'])/len(y_test)*100, 2)\nprint('Train case proportion: {}%'.format(proportion_train))\nprint('Test case proportion:  {}%'.format(proportion_test))\nprint('Difference between test and train sets: {}%'.format(round(np.abs(proportion_train - proportion_test), 2)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"75aec93ec88352bb231fa0be5739e073cbf073e7"},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size = 0.25, random_state = 1000, stratify = y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6d88ae3511f37f71c2e428fb691971adbf4d8942"},"cell_type":"code","source":"proportion_train = round(len(y_train[y_train == 'Abnormal'])/len(y_train)*100, 2)\nproportion_test = round(len(y_test[y_test == 'Abnormal'])/len(y_test)*100, 2)\nprint('Train case proportion: {}%'.format(proportion_train))\nprint('Test case proportion:  {}%'.format(proportion_test))\nprint('Difference between test and train sets: {}%'.format(round(np.abs(proportion_train - proportion_test), 2)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bc3010f9001c7b846ab0eab92aca249373e5b85a"},"cell_type":"markdown","source":"We can see that our classes are more evenly balanced between the train and test set when we specify the response variable in the `stratify` argument. Now that we have our training and testing sets all finalised, we can get on with training and evaluating our models. "},{"metadata":{"_uuid":"e47e110efeb91750f4bc87c9f21129ee04382f53"},"cell_type":"markdown","source":"## Decision Tree model using `DecisionTreeClassifier`\nAn incredibly popular model for binomial prediction problems in the decision tree, so it seems like a good place to start. This is just a singular tree with default hyper parameters so isn't necessary reflective of the ideal model or performance that could be achieved using this particular algorithm. "},{"metadata":{"trusted":true,"_uuid":"8d910f0397dbf5de791bc454ddc177a5633817c3"},"cell_type":"code","source":"# Import decision trees and accuracy metric \nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"52f62276b8cceabc0b28322bc8b28d022c55d363"},"cell_type":"markdown","source":"We have imported our `DecisionTreeClassifier`, and now can fit the model using training data, and measure the model's effectiveness using the test data. "},{"metadata":{"trusted":true,"_uuid":"526d4b1d9da320d8d7add2c547697a7c8511bdf2"},"cell_type":"code","source":"# We are going to be doing this a lot, so better to define a function\ndef print_accuracy(accuracy_score, score_text=False):\n    \"\"\"\n    Take an accuracy score between 0 and 1 and print output to screen cleanly \n    \"\"\"\n    clean_accuracy = accuracy_score*100.0\n    if score_text:\n        clean_text = score_text.strip() + ' '\n        print('{}{:.2f}%'.format(clean_text, clean_accuracy))\n    else:\n        print('{:.2f}%'.format(clean_accuracy))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4c13d6be61991af1852a2ff48d1a984e4549d62c"},"cell_type":"code","source":"tree_model = DecisionTreeClassifier(random_state = 1000).fit(X_train, y_train)\n\n# Predict on the test set and get the accuracy using the known values \ntree_pred = tree_model.predict(X_test)\n\nprint_accuracy(accuracy_score(y_test, tree_pred), 'Decision tree accuracy:')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e0b4ef1033f2c21f598bf3e3c67275f3d620a453"},"cell_type":"markdown","source":"Note that not specifying any hyperparameters with `DecisionTreeClassifier` basically means it will keep growing until all the nodes are pure, which will likely result in huge overfitting issues. We should do some hyperparameter training to see if there is a more optimum combination of hyperparameters that will produce a better model for accuracy on our test set. \n\n### Tuning our `DecisionTreeClassifier` using `GridSearchCV` \nIn addition to our test-train split we are going to perform cross validation. This takes a number of folds from the data and trains a model on one section before testing on the other, on the training subset. The final model that is returned is a model that uses the best hyperparameters for that full testing set and is trained on the full set. This internal test from within the cross validation set is sometimes referred as the validation set; it is the performance on this set that will give us insight into the performance of the model on unseen examples outside of the training examples.  \n\nWe can then test this further because we will have already separated out a final test set for us to use; this might lead to some interesting results because the data size is so small, but I think that it is good practice so will do it regardless. "},{"metadata":{"trusted":true,"_uuid":"94927d39130a13851c7cbffad597d04d82f5331b"},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"87f4bd3f777a9943dae2f17ff3c2b4115d8bc6b1"},"cell_type":"code","source":"# Define our parameter grid that will be looped through to test our hyperparameters\nparam_grid = {'max_depth': [i for i in range(1, 11)], \n              'max_features': [i for i in range(1, 8)], \n              'min_samples_leaf': [i for i in range(1, 11)]}\n\ngrid = GridSearchCV(DecisionTreeClassifier(random_state = 1000), param_grid, cv=10, return_train_score = True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"288b4e137be53ad601a8ffa4e7317675d207d746"},"cell_type":"markdown","source":"Once the grid object has been created, we can fit it using our train data to create a model and find the best hyperparameters according to the out-of-sample tests done during the cross-validation. "},{"metadata":{"trusted":true,"_uuid":"621fb4005a5277e5b9a1f625ed879d7d013be573"},"cell_type":"code","source":"grid.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e2976ba669cda3e2fa7c86cf561c5fb6e08e07cd"},"cell_type":"markdown","source":"By accessing our `grid` object we can find out the best out-of-sample score that we obtained, as well as the parameters that provided that; the `best_score_` is defined as _mean cross-validated score of the best_estimator_, so we can see what the average score across all cross-validations for our 'optimum' parameters is. This might act as a good indication of the model's future performance on unseen data. Further to that, we can access the best estimator model through the `best_estimator_` portion of the object. I'll use that to predict our test set and see what accuracy score we get.  "},{"metadata":{"trusted":true,"_uuid":"d41415fa1854b5d10b5e69da73019653a99b4741"},"cell_type":"code","source":"print(\"Best Score: {}%\".format(round(grid.best_score_*100.0, 2)))\nprint(\"Best params: {}\".format(grid.best_params_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"10cad5677d3ec2088a6b4c1a893ae7b83ede1e73"},"cell_type":"code","source":"print_accuracy(accuracy_score(y_test, grid.best_estimator_.predict(X_test)), 'Decision Tree Classifier:')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"220cb1866ffffe762d1e0af4281adf3e65970a0e"},"cell_type":"markdown","source":"The accuracy that we saw on the test set wasn't as good as what we might have expected given the cross-validation score that we had; but it is still good, and not a bad starting point! "},{"metadata":{"_uuid":"5473c2b48529df8e06c5f02134712299fb7ae32a"},"cell_type":"markdown","source":"## Investigating the performance using `cv_results_`\nIf we wanted to know more about the performance of the model during the process, then we could take a look for ourselves into the scores that were generated during this process. Perhaps we didn't want the particular set of hyperparameters that had the best mean score, but instead wanted a set of hyperparameters that was very consistent in its predictive power so we might better be able to predict performance in our testing set. Since the training times are recorded as well, we are able to observe the tradeoff between certain parameter settings and the performance of the model. More information about this can be found in the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html). \n\nIt should also be noted that we can define a scorer of our choosing, and it might be more appropriate to use a balanced accuracy measure here to avoid scores being higher purely due to picking the majority class each time; this is something that I may implement in the next version of the kernel. \n\nIf you have set `return_train_score = True` then the results of the cross-validation will be stored as below:"},{"metadata":{"trusted":true,"_uuid":"a022e58dfaefcb006f99d494bff400cf1349b583"},"cell_type":"code","source":"keys = []\nshapes = []\nexamples = []\nfor key in list(grid.cv_results_.keys()):\n    keys.append(key)\n    shapes.append(len(grid.cv_results_.get(key)))\n    examples.append(grid.cv_results_.get(key)[0])\n\ndf_results_info = pd.DataFrame({'size': shapes, 'example': examples}, index = keys)\ndf_results_info","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c2784c462768b794357bdef8a5109d7c8bf7ca41"},"cell_type":"code","source":"plt.hist(grid.cv_results_.get('mean_fit_time'), bins = 70);plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9defc974bfc876a2ee741ee38adc82614be199cf"},"cell_type":"code","source":"mean_fit_times = grid.cv_results_.get('mean_fit_time')\nmean_test_scores = grid.cv_results_.get('mean_test_score')\nmax_score = np.argmax(mean_test_scores)\n\nplt.scatter(mean_fit_times, mean_test_scores)\nplt.xlim(min(mean_fit_times)*0.95, max(mean_fit_times*1.05))\nplt.scatter(mean_fit_times[max_score], mean_test_scores[max_score])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a36aadc8dddd7b8e47ce8317ca8dac89f037cc98"},"cell_type":"markdown","source":"Above I have highlighted the singular point that represents the highest mean test score. To continue what I was saying, it might be the case that you want to investigate the performance of a model computationally, which might be a consideration when you are training on larger models, or have a flow of data that requires retraining periodically. "},{"metadata":{"trusted":true,"_uuid":"d5470426a58a2b2c4dffc07cb8660da3ef5b5457"},"cell_type":"code","source":"max_depth = grid.cv_results_.get('param_max_depth').astype(int)\nmax_features = grid.cv_results_.get('param_max_features').astype(int)\nmin_samples_leaf = grid.cv_results_.get('param_min_samples_leaf').astype(int)\nmean_fit_times = grid.cv_results_.get('mean_fit_time')\nmean_test_scores = grid.cv_results_.get('mean_test_score')\n\nparams_df = pd.DataFrame({'max_depth' : max_depth, \n                          'max_features' : max_features, \n                          'min_samples_leaf' : min_samples_leaf,\n                          'mean_fit_time' : mean_fit_times,\n                          'mean_test_score' : mean_test_scores})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c8ce00c89a02b36366cefada6f1c9e0786b00827"},"cell_type":"code","source":"fig, ax = plt.subplots(nrows= 1, ncols = 3, figsize = (15,5), sharey=True, sharex=True)\nplt.xlim(min(mean_fit_times)*0.95, max(mean_fit_times*1.05))\n\nfeatures = ['max_depth', 'max_features', 'min_samples_leaf']\naxes = ax.flatten()\n\nfor idx in range(3):\n    sns.scatterplot(y = 'mean_test_score', x = 'mean_fit_time', hue = features[idx], data = params_df, ax = axes[idx], palette='Greens_r')\n    \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a861fe99f35eb641546946b5a75683c021215e01"},"cell_type":"markdown","source":"__Note:__ if the above graphs look to have any major outliers, then this might be due to the particular run of the grid when this notebook was rendered. \n\nLooking these graphs, sorted and coloured by the 3 parameters that were part of our grid, we can see that `max_features` seems to have a strong impact on the fitting time of a model, and that by limiting that to lower values provides using with a sizeable computational performance improvement, whereas the other two have a more spread set across the `mean_fit_time` in comparison; they still have some patterns that we can see.  \n\n`min_samples_leaf` tends to take longer the lower it is which makes sense given that the lower this parameter is, then the deeper the final model will be. `max_depth` also appears to show that the higher the depth, then the longer the model takes to train. "},{"metadata":{"_uuid":"3f39596891c3b535bb8b10a390f7f9278a2e7f91"},"cell_type":"markdown","source":"## Logistic Regression model with `LogisticRegression`\nNow we have built a model using a decision tree, we can build another model using logistic regression for comparison. "},{"metadata":{"trusted":true,"_uuid":"5a8dba9c1b527c1de90e7e05d8542ffc147135aa"},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"40434ba2d08a26075afa7dccf7008ed16f1edf03"},"cell_type":"markdown","source":" By altering the `penalty` attribute of the `LogisiticRegression` we can change the type of regularisation that the model uses. This can have an impact on the effectiveness and efficiency of the model, and could be included as one of the hyperparameters that we tune when we run the cross-validation and `gridSearch`."},{"metadata":{"trusted":true,"_uuid":"204c0b855b7cc2caa0e933c48100bb6baacc170e"},"cell_type":"code","source":"param_grid = {'penalty': ['l1', 'l2']}\ngrid = GridSearchCV(LogisticRegression(random_state = 1000, solver = 'liblinear'), param_grid, cv=10, return_train_score=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"121ac98523be11109a908a60e7d3783c7a3913f8"},"cell_type":"code","source":"grid.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d81678de1cfa2fae0ca330198935ed9887d530ed"},"cell_type":"code","source":"print(\"Best Score: {}\".format(grid.best_score_))\nprint(\"Best params: {}\".format(grid.best_params_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6cc20f4339ea3ac4f4e9b34b4f4454cfcc7837a2"},"cell_type":"code","source":"print_accuracy(accuracy_score(y_test, grid.best_estimator_.predict(X_test)), 'Best logisitic regression grid score:')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c8ced2a04b04312d15c509998c419372b161455b"},"cell_type":"markdown","source":"Just for the sake of comparison and because we are only using a small parameter grid (two levels), I will create a model using the `l1` regularisation penalty to show how it differs from `l2`. Here lies an example of the benefit of stating the `random_state` argument when creating the `LogisticRegression` model. "},{"metadata":{"trusted":true,"_uuid":"43fdf4cf985f272b6bba5cb6441a52a65ef12c06"},"cell_type":"code","source":"# Solver suppresses the warnings: more information about which solver for which problems can be found in the docs: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\nlog_reg_l1 = LogisticRegression(penalty='l1', solver = 'liblinear', random_state=1000)\nlog_model_l1 = log_reg_l1.fit(X_train, y_train)\ny_pred_l1 = log_model_l1.predict(X_test)\nprint(log_model_l1.coef_)\n\nprint_accuracy(accuracy_score(y_test, y_pred_l1), 'L1 Regularised test accuracy:')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6e0826e493f861a8e9c1745a91e07812bd36b8c1"},"cell_type":"code","source":"# Having a look at the l2 regularisation, which isn't capable of fully removing features\nlog_reg_l2 = LogisticRegression(penalty='l2', solver = 'liblinear', random_state=1000)\nlog_model_l2 = log_reg_l2.fit(X_train, y_train)\ny_pred_l2 = log_model_l2.predict(X_test)\nprint(log_model_l2.coef_)\n\nprint_accuracy(accuracy_score(y_test, y_pred_l2), 'L2 Regularised test accuracy:')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"edc0dbbdb28119f3bb83a4d1e7db308e3d775a4d"},"cell_type":"markdown","source":"Note how L1 regularisation of the model acts as feature selection as it has reduced one coefficient of our features to 0 and thus removed them from the model. This is a benefit to using the L1 regularisation as in this way it is capable of feature selection and maintaining the simplicity of the model. "},{"metadata":{"trusted":true,"_uuid":"96146f465c14646f3c9808384290b840f1033d52"},"cell_type":"code","source":"grid.best_estimator_.coef_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"de103fe3cf730c8e3e48952556180eb19e9daf53"},"cell_type":"code","source":"log_model_l2.coef_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3b44f369d47d453cf617ff86859b0e0b2749b974"},"cell_type":"code","source":"log_model_l2.coef_ == grid.best_estimator_.coef_","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1a28a6a6a54ea73cdeb666b3c5b44bf37aa1d692"},"cell_type":"markdown","source":"This is just to show to that the model returned by the grid serach is the same as that returned by the model that we built externally afterwards.  \n\nLet's take a look at the results that were captured during the grid search results and see how the models performed at every split. "},{"metadata":{"trusted":true,"_uuid":"79ac4c960d6389edbafb4d7443c9e919afdd48a0"},"cell_type":"code","source":"test_splits = [i for i in list(grid.cv_results_.keys()) if i.endswith('test_score') and i.startswith('split')]\ntest_split_results = {}\npenalty = list(grid.cv_results_.get('param_penalty'))\n\nfor split in test_splits:\n    test_split_results[split] = list(grid.cv_results_.get(split))\n    \ndf_test_splits = pd.DataFrame(test_split_results, index = penalty)\ndf_test_splits.columns = [i.replace('_test_score', '') for i in df_test_splits.columns]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e581bae623959bdba034f6e78718ee372f5dbdbc"},"cell_type":"code","source":"df_test_splits","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fddcf90371ce093d3dab23b4b08429299cbc2721"},"cell_type":"code","source":"df_test_splits.loc['l1'] >= df_test_splits.loc['l2']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1bcc2121ed3c85da99c580a81e35a93b30e5622b"},"cell_type":"markdown","source":"We can see that `L1` regularisation actually performed better or equal in 80% of the splits that we performed, and that the average performance was only marginally less overall for the test sets. "},{"metadata":{"trusted":true,"_uuid":"8566a4780b3be20f9666930102f41a8e8856a64f"},"cell_type":"code","source":"list(grid.cv_results_.get('mean_test_score'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"970e9d8f3a5e7a95986b27f95b45bef5bacda4a6"},"cell_type":"code","source":"long_test_splits = pd.DataFrame(df_test_splits.unstack().reset_index())\nlong_test_splits.columns = ['split', 'penalty', 'score']\nlong_test_splits","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"17c069254d4bb90f0d803fdd2cb43f9d23622eb6"},"cell_type":"code","source":"sns.boxplot(x='penalty', y=\"score\", data=long_test_splits)\nplt.xlabel('Penalty')\nplt.ylabel('Test score')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"14863ba01f000259ec9dbc979e105ee1bc55ab37"},"cell_type":"markdown","source":"We can see a tighter clustering of the `L2` test scores, while the `L1` results appear to be generally higher but the average brought down by two outliers; with more cross-validation passes this might have shown that `L1` was the more effective model. "},{"metadata":{"_uuid":"0cf4628c94197ca024779e0b7f09b302b221afdd"},"cell_type":"markdown","source":"## Support Vector Machine model with `SVC`\nWhen considering using support vector machines, we are going to look at the performance of two different types: Polynomial and RBF. You can see how these are defined below, as well as their performance against the test set that we have defined.  \n\nSupport vector machines can be a good method of creating interesting relationships by introducing polynomial features that allow for the creating a line that separates the two classes. RBF (radial basis function) methods are more popular, and are density based. The gamma hyperparameter is important in the battle of fighting overfitting. A higher value of gamma will make a tighter fit around the data points, but this means that it won't necessarily generalise well, whereas accuracy is likely to fall with a lower gamma value, but it might mean that performance generalises better to unseen observations.  "},{"metadata":{"trusted":true,"_uuid":"5bd3a04cea3116ebe0a5cc762c0a4418e44d3ad0"},"cell_type":"code","source":"# Import support vector machines\nfrom sklearn.svm import SVC","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0ecc47dea2e2ef8d6e10553eb0ed3fd24ed2b6fb"},"cell_type":"code","source":"poly_model = SVC(kernel = 'poly', gamma = 'auto', random_state = 1000).fit(X_train, y_train)\nrbf_model = SVC(kernel = 'rbf', gamma = 'auto', random_state = 1000).fit(X_train, y_train)\n\npoly_pred = poly_model.predict(X_test)\nprint_accuracy(accuracy_score(y_test, poly_pred), 'Polynomial kernel accuracy:')\n\nrbf_pred = rbf_model.predict(X_test)\nprint_accuracy(accuracy_score(y_test, rbf_pred), 'RBF kernel accuracy:')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e427b97fb3d1f7b533eb40ba172b7fcb935fd1bc"},"cell_type":"markdown","source":"With the default settings we can see that the RBF performed much better, let's have a look and see whether tweaking the value of gamma will make a noticeable difference to the performance on the test set. 'auto' for gamma uses a value of 1/n_features by [default](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html), so lets produce a range that is around that to see if there are any improvements to be made.   \n\nWhen we are using a grid search technique then we are selecting the best hyperparameters based on their performance on generalising outside of the data on which they have been trained, so a lower gamma makes sense as we want to avoid overfitting the data to the training set. "},{"metadata":{"trusted":true,"_uuid":"d804a16d0e9d0b703b243eee5c66e93b4a4a8bb0"},"cell_type":"code","source":"param_grid = {'gamma': np.arange(0, 0.25, 0.01)}\ngrid = GridSearchCV(SVC(random_state = 1000, kernel = 'rbf'), param_grid, cv=5, return_train_score=True)\ngrid.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4bedc10440026064e19fa738401b9bf5af0420b9"},"cell_type":"code","source":"print(\"Best Score: {}\".format(grid.best_score_))\nprint(\"Best params: {}\".format(grid.best_params_))\nprint(1/12)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3f5bb470fa75ef0f17aa53f303d53ed67f5a99f7"},"cell_type":"markdown","source":"By default the gamma would have been 1/12, or ~0.08. Using the `GridSearch` we are able to see that a lower gamma value of 0.06 might perform better on unseen data; we can see how it performs on the test that we have by manually specifying that gamma value of 0.06 and rebuilding the model. "},{"metadata":{"trusted":true,"_uuid":"65eaace63197134585f234640c1af661a98c0baa"},"cell_type":"code","source":"rbf_model_2 = SVC(kernel = 'rbf', gamma = 0.03, random_state = 1000).fit(X_train, y_train)\nrbf_pred_2 = rbf_model_2.predict(X_test)\n\nprint_accuracy(accuracy_score(y_test, rbf_pred), 'Original RBF kernel accuracy:')\nprint_accuracy(accuracy_score(y_test, rbf_pred_2), 'RBF kernel accuracy with gamma of 0.06:')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e96ba74c44bbcfe826c41d3de4e44e18486d1790"},"cell_type":"markdown","source":"Our `GridSearch` has yielded good results, and changing that gamma value has meant that the performance is slightly better on our test set, going from **76.92%** to **78.21%**. "},{"metadata":{"trusted":true,"_uuid":"c6b88e660c53a4fd135b151a90face265b23755b"},"cell_type":"code","source":"gammas = list(grid.cv_results_.get('param_gamma'))\nmean_test_scores = list(grid.cv_results_.get('mean_test_score'))\nmean_train_times = list(grid.cv_results_.get('mean_fit_time'))","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"7a091c7fd6f669846b9c6cfa0a9ad304890da733"},"cell_type":"code","source":"fig, axes = plt.subplots(nrows = 2, ncols = 1, sharex = True, sharey = False)\naxs = axes.flatten()\n\nplt.sca(axs[0])\nsns.lineplot(gammas, mean_test_scores, color = '#44bd32')\nplt.axvline(0.06, color = '#fbc531')\nplt.ylabel('Mean Test Score')\nplt.title('Mean Test Score during CV as Gamma increases')\n\nplt.sca(axs[1])\nsns.lineplot(x=gammas, y=mean_train_times, color=\"#8c7ae6\")\nplt.ylabel('Mean Training Time')\nplt.axvline(0.06, color = '#fbc531')\nplt.xlabel('Gamma value')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bc6c6ec2f437f51ee01c226b169cfa6d02b9251a"},"cell_type":"markdown","source":"It looks as though we have a good value for gamma in terms of both the test score that was achieved via the cross-validation, and the training time for the model. "},{"metadata":{"_uuid":"1f8d679173129dd10909171b09d7d13828a0bb2f"},"cell_type":"markdown","source":"# Conclusions\nWe can see that early into this investigation that both logistic regression models are performing better than our decision tree and our support vector machines; at least with the tuning that I have done. I think this is a good example of where throwing complexity at a problem doesn't always yield results. Sometimes this is important when you have large amounts of data, and have issues training computationally; this isn't a problem with our small dataset but could prove something to consider when the data size becomes larger so looking at the training information that can be provided through the `cv_results_` of a `GridSearchCV` object is useful, and I hope that I've shown some of the ways in which that might shape the final decision of choosing hyperparameters.  \n\n__Coming soon__\n- Other algorithms to explore for hyperparameter tuning - Ensemble methods\n- Binary problem accuracy methods\n\nIf you've found this useful then please consider leaving an upvote or a comment. Please let me know if you see anything that I could improve or if you have constructive feedback. \n\n\nIf you are interested in seeing a similar kernel written in`R` on this dataset that uses `caret` then I have written one with just that [here](https://www.kaggle.com/willcanniford/predicting-lower-back-pain-using-caret-in-r).\n"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}