{"cells":[{"metadata":{"_uuid":"3fb98ffb8a490b1ec4d5680e97421c75563f7781"},"cell_type":"markdown","source":"# Country Cluster Identification"},{"metadata":{"_uuid":"2833f895f98421986a8eace8c5f03dd6d9893bc6"},"cell_type":"markdown","source":"## Hierarchical Clustering and PCA"},{"metadata":{"_uuid":"d6f92b30643088292947f5cd0562b8ade392bfd6"},"cell_type":"markdown","source":"## Problem Statement\nHELP International is an international humanitarian NGO that is committed to fighting poverty and providing the people of backward countries with basic amenities and relief during the time of disasters and natural calamities. It runs a lot of operational projects from time to time along with advocacy drives to raise awareness as well as for funding purposes.\n\n \nAfter the recent project that included a lot of awareness drives and funding programmes, they have been able to raise around $ 10 million. Now the CEO of the NGO needs to decide how to use this money strategically and effectively. The significant issues that come while making this decision are mostly related to choosing the countries that are in the direst need of aid. \n\n \n\nAnd this is where you come in as a data analyst. Your job is to categorise the countries using some socio-economic and health factors that determine the overall development of the country. Then you need to suggest the countries which the CEO needs to focus on the most.  \n\n## Data\nThe datasets containing those socio-economic factors and the corresponding data dictionary are provided."},{"metadata":{"trusted":true,"_uuid":"d3e208d92cc3da35dcfda40e1a5dcdd2c6c299a5"},"cell_type":"code","source":"# Supress Warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Importing libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# visulaisation\nfrom matplotlib.pyplot import xticks\n%matplotlib inline\n\n# Data display coustomization\npd.set_option('display.max_rows', 50)\npd.set_option('display.max_columns', 50)\n\n# To perform Hierarchical clustering\nfrom scipy.cluster.hierarchy import linkage\nfrom scipy.cluster.hierarchy import dendrogram\nfrom scipy.cluster.hierarchy import cut_tree","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f581c5da02201d9ebe1a67ac88176f48d037c4aa"},"cell_type":"markdown","source":"## Data Preparation"},{"metadata":{"_uuid":"bebb7325d9cae7a7dcd01bb219178452eb4c3fb3"},"cell_type":"markdown","source":"### Data Loading"},{"metadata":{"trusted":true,"_uuid":"b7ee50954aa45b8e5d8255ed2b7ddb6d0351e404"},"cell_type":"code","source":"data = pd.DataFrame(pd.read_csv('../input/Country-data.csv'))\ndata.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"47f032c8859e0ed09b57feaad6b6d943976f40f7"},"cell_type":"code","source":"#checking duplicates\nsum(data.duplicated(subset = 'country')) == 0\n# No duplicate values","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"03b9cc64331731fdf9ab57e7063862bbd5b087a4"},"cell_type":"markdown","source":"### Data Inspection"},{"metadata":{"trusted":true,"_uuid":"7a5fce02367eb2d90704da5d6349c82a9481c7f7"},"cell_type":"code","source":"data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"98abca03092f3c97529e1fa214c3fd65e56c6701"},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3c07d6c24088d43904fc145ab71152c1bc776fc9"},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c33bc746ae86deef0abcd05025b639af622e2ef7"},"cell_type":"markdown","source":"### Data Cleaning"},{"metadata":{"trusted":true,"_uuid":"12e032bc8134f3f20193225decc93228fd30e7a3"},"cell_type":"code","source":"data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d560af7e132c26cd0cc02684d38333343ff6b41b"},"cell_type":"code","source":"# No NULL values are observed.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"69f5c2a5e27b5aa217dc9f623602a3ddd99f8511"},"cell_type":"markdown","source":"## Exploratory Data Analytics"},{"metadata":{"_uuid":"eba6aa45f1c2f011260273c73b73fdffad12d654"},"cell_type":"markdown","source":"### Univariate Analysis"},{"metadata":{"_uuid":"96394dd009190675e3a61d528dbd4129aee7c955"},"cell_type":"markdown","source":"####  We need to choose the countries that are in the direst need of aid. Hence, we need to identify those countries with  using some socio-economic and health factors that determine the overall development of the country."},{"metadata":{"trusted":true,"_uuid":"f7051be03e7052bfe69c0aa52e1cd835aed7d928"},"cell_type":"code","source":"# We will have a look on the lowest 10 countries for each factor.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a81a2a850fdca271f7a3df71478e01da4c2b5555"},"cell_type":"code","source":"fig, axs = plt.subplots(3,3,figsize = (15,15))\n\n# Child Mortality Rate : Death of children under 5 years of age per 1000 live births\n\ntop10_child_mort = data[['country','child_mort']].sort_values('child_mort', ascending = False).head(10)\nplt1 = sns.barplot(x='country', y='child_mort', data= top10_child_mort, ax = axs[0,0])\nplt1.set(xlabel = '', ylabel= 'Child Mortality Rate')\n\n# Fertility Rate: The number of children that would be born to each woman if the current age-fertility rates remain the same\ntop10_total_fer = data[['country','total_fer']].sort_values('total_fer', ascending = False).head(10)\nplt1 = sns.barplot(x='country', y='total_fer', data= top10_total_fer, ax = axs[0,1])\nplt1.set(xlabel = '', ylabel= 'Fertility Rate')\n\n# Life Expectancy: The average number of years a new born child would live if the current mortality patterns are to remain same\n\nbottom10_life_expec = data[['country','life_expec']].sort_values('life_expec', ascending = True).head(10)\nplt1 = sns.barplot(x='country', y='life_expec', data= bottom10_life_expec, ax = axs[0,2])\nplt1.set(xlabel = '', ylabel= 'Life Expectancy')\n\n# Health :Total health spending as %age of Total GDP.\n\nbottom10_health = data[['country','health']].sort_values('health', ascending = True).head(10)\nplt1 = sns.barplot(x='country', y='health', data= bottom10_health, ax = axs[1,0])\nplt1.set(xlabel = '', ylabel= 'Health')\n\n# The GDP per capita : Calculated as the Total GDP divided by the total population.\n\nbottom10_gdpp = data[['country','gdpp']].sort_values('gdpp', ascending = True).head(10)\nplt1 = sns.barplot(x='country', y='gdpp', data= bottom10_gdpp, ax = axs[1,1])\nplt1.set(xlabel = '', ylabel= 'GDP per capita')\n\n# Per capita Income : Net income per person\n\nbottom10_income = data[['country','income']].sort_values('income', ascending = True).head(10)\nplt1 = sns.barplot(x='country', y='income', data= bottom10_income, ax = axs[1,2])\nplt1.set(xlabel = '', ylabel= 'Per capita Income')\n\n\n# Inflation: The measurement of the annual growth rate of the Total GDP\n\ntop10_inflation = data[['country','inflation']].sort_values('inflation', ascending = False).head(10)\nplt1 = sns.barplot(x='country', y='inflation', data= top10_inflation, ax = axs[2,0])\nplt1.set(xlabel = '', ylabel= 'Inflation')\n\n\n# Exports: Exports of goods and services. Given as %age of the Total GDP\n\nbottom10_exports = data[['country','exports']].sort_values('exports', ascending = True).head(10)\nplt1 = sns.barplot(x='country', y='exports', data= bottom10_exports, ax = axs[2,1])\nplt1.set(xlabel = '', ylabel= 'Exports')\n\n\n# Imports: Imports of goods and services. Given as %age of the Total GDP\n\nbottom10_imports = data[['country','imports']].sort_values('imports', ascending = True).head(10)\nplt1 = sns.barplot(x='country', y='imports', data= bottom10_imports, ax = axs[2,2])\nplt1.set(xlabel = '', ylabel= 'Imports')\n\nfor ax in fig.axes:\n    plt.sca(ax)\n    plt.xticks(rotation = 90)\n    \nplt.tight_layout()\nplt.savefig('eda')\nplt.show()\n    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f3b31d7c25a90db6994563bfd08766bb0c8d52c5"},"cell_type":"code","source":"# Let's check the correlation coefficients to see which variables are highly correlated\n\nplt.figure(figsize = (16, 10))\nsns.heatmap(data.corr(), annot = True, cmap=\"YlGnBu\")\nplt.savefig('corrplot')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d4935efff0eba930fb8cb117a8e43f853fddb4e2"},"cell_type":"code","source":"# We can see there is high correlation between some variables, we will use PCA to solve this issue.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3b69050d34f0dfc1142b96f6ab5862c51ba33e0e"},"cell_type":"markdown","source":"## Outlier Analysis"},{"metadata":{"trusted":true,"_uuid":"782e0c4fa1139abda3559991106f069bab077d26"},"cell_type":"code","source":"# We will see how values in each columns are distributed using boxplot","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"e760f853a3c02d5189e6333b496a265413d64315"},"cell_type":"code","source":"fig, axs = plt.subplots(3,3, figsize = (15,7.5))\nplt1 = sns.boxplot(data['child_mort'], ax = axs[0,0])\nplt2 = sns.boxplot(data['health'], ax = axs[0,1])\nplt3 = sns.boxplot(data['life_expec'], ax = axs[0,2])\nplt4 = sns.boxplot(data['total_fer'], ax = axs[1,0])\nplt5 = sns.boxplot(data['income'], ax = axs[1,1])\nplt6 = sns.boxplot(data['inflation'], ax = axs[1,2])\nplt7 = sns.boxplot(data['gdpp'], ax = axs[2,0])\nplt8 = sns.boxplot(data['imports'], ax = axs[2,1])\nplt9 = sns.boxplot(data['exports'], ax = axs[2,2])\n\n\nplt.tight_layout()\n","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"77bbac48b91eed54b1593d2ed2d1a5dcab462ede"},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"e2b3af1cad721eedf05f1da41c4dde46d97cf397"},"cell_type":"code","source":"# Before manipulating data, we will save one copy of orignal data.\ndata_help = data.copy()\ndata_help.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4513185f9e74c52633c0ff2c163c54e653e1fa90"},"cell_type":"code","source":"# As we can see there are a number of outliers in the data.\n\n# Keeping in mind we need to identify backward countries based on socio economic and health factors.\n# We will cap the outliers to values accordingly for analysis.\n\npercentiles = data_help['child_mort'].quantile([0.05,0.95]).values\ndata_help['child_mort'][data_help['child_mort'] <= percentiles[0]] = percentiles[0]\ndata_help['child_mort'][data_help['child_mort'] >= percentiles[1]] = percentiles[1]\n\npercentiles = data_help['health'].quantile([0.05,0.95]).values\ndata_help['health'][data_help['health'] <= percentiles[0]] = percentiles[0]\ndata_help['health'][data_help['health'] >= percentiles[1]] = percentiles[1]\n\npercentiles = data_help['life_expec'].quantile([0.05,0.95]).values\ndata_help['life_expec'][data_help['life_expec'] <= percentiles[0]] = percentiles[0]\ndata_help['life_expec'][data_help['life_expec'] >= percentiles[1]] = percentiles[1]\n\npercentiles = data_help['total_fer'].quantile([0.05,0.95]).values\ndata_help['total_fer'][data_help['total_fer'] <= percentiles[0]] = percentiles[0]\ndata_help['total_fer'][data_help['total_fer'] >= percentiles[1]] = percentiles[1]\n\npercentiles = data_help['income'].quantile([0.05,0.95]).values\ndata_help['income'][data_help['income'] <= percentiles[0]] = percentiles[0]\ndata_help['income'][data_help['income'] >= percentiles[1]] = percentiles[1]\n\npercentiles = data_help['inflation'].quantile([0.05,0.95]).values\ndata_help['inflation'][data_help['inflation'] <= percentiles[0]] = percentiles[0]\ndata_help['inflation'][data_help['inflation'] >= percentiles[1]] = percentiles[1]\n\npercentiles = data_help['gdpp'].quantile([0.05,0.95]).values\ndata_help['gdpp'][data_help['gdpp'] <= percentiles[0]] = percentiles[0]\ndata_help['gdpp'][data_help['gdpp'] >= percentiles[1]] = percentiles[1]\n\npercentiles = data_help['imports'].quantile([0.05,0.95]).values\ndata_help['imports'][data_help['imports'] <= percentiles[0]] = percentiles[0]\ndata_help['imports'][data_help['imports'] >= percentiles[1]] = percentiles[1]\n\npercentiles = data_help['exports'].quantile([0.05,0.95]).values\ndata_help['exports'][data_help['exports'] <= percentiles[0]] = percentiles[0]\ndata_help['exports'][data_help['exports'] >= percentiles[1]] = percentiles[1]","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"490db28c74c6d4310b5614954ed4e2f631e7ec98"},"cell_type":"code","source":"fig, axs = plt.subplots(3,3, figsize = (15,7.5))\n\nplt1 = sns.boxplot(data_help['child_mort'], ax = axs[0,0])\nplt2 = sns.boxplot(data_help['health'], ax = axs[0,1])\nplt3 = sns.boxplot(data_help['life_expec'], ax = axs[0,2])\nplt4 = sns.boxplot(data_help['total_fer'], ax = axs[1,0])\nplt5 = sns.boxplot(data_help['income'], ax = axs[1,1])\nplt6 = sns.boxplot(data_help['inflation'], ax = axs[1,2])\nplt7 = sns.boxplot(data_help['gdpp'], ax = axs[2,0])\nplt8 = sns.boxplot(data_help['imports'], ax = axs[2,1])\nplt9 = sns.boxplot(data_help['exports'], ax = axs[2,2])\n\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"acc553c4f5b8595734658569e93e03e8dbf27fe0"},"cell_type":"markdown","source":"### Scaling the data"},{"metadata":{"trusted":true,"_uuid":"21fa541529da34ea910d3938da463cd22aa314a6"},"cell_type":"code","source":"# Import the StandardScaler()\nfrom sklearn.preprocessing import StandardScaler\n\n# Create a scaling object\nscaler = StandardScaler()\n\n# Create a list of the variables that you need to scale\nvarlist = ['child_mort', 'exports', 'health', 'imports', 'income', 'inflation', 'life_expec', 'total_fer', 'gdpp']\n# Scale these variables using 'fit_transform'\ndata_help[varlist] = scaler.fit_transform(data_help[varlist])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"65fe62359cd2bebc9775a52f447c10899fe39e34"},"cell_type":"markdown","source":"## PCA on the Data"},{"metadata":{"trusted":true,"_uuid":"54532eae3f85b6ce9b44fe21edf1cc0b9a97959e"},"cell_type":"code","source":"#Improting the PCA module\nfrom sklearn.decomposition import PCA\npca = PCA(svd_solver='randomized', random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"34350c18f448c448b526bcc1560cfdc55f3f21a5"},"cell_type":"code","source":"# Putting feature variable to X\nX = data_help.drop(['country'],axis=1)\n\n# Putting response variable to y\ny = data_help['country']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7f61c7129ca042b7651370ee9b6ecf91cee25a38"},"cell_type":"code","source":"#Doing the PCA on the train data\npca.fit(X)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"901972338245a50849b4c57af0da4db67843598a"},"cell_type":"markdown","source":"#### Let's plot the principal components and try to make sense of them. \n#### We'll plot original features on the first 2 principal components as axes"},{"metadata":{"trusted":true,"_uuid":"f85fa6763f047b9c2cbfd527168b8e5750f95058"},"cell_type":"code","source":"pca.components_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"def2b425a1df285eed3fa907d08e6db05dd2a32b"},"cell_type":"code","source":"colnames = list(X.columns)\npcs_df = pd.DataFrame({'PC1':pca.components_[0],'PC2':pca.components_[1], 'Feature':colnames})\npcs_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"88ebc387803858a2978f396d5a3e50b6ba7cc39a"},"cell_type":"code","source":"%matplotlib inline\nfig = plt.figure(figsize = (8,8))\nplt.scatter(pcs_df.PC1, pcs_df.PC2)\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')\nfor i, txt in enumerate(pcs_df.Feature):\n    plt.annotate(txt, (pcs_df.PC1[i],pcs_df.PC2[i]))\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4a965755f20bba73ca58d114f913e3508b6430ce"},"cell_type":"markdown","source":"#### Looking at the screeplot to assess the number of needed principal components\n"},{"metadata":{"trusted":true,"_uuid":"3e6ae2663fa2c473c48f0b0724585a474806cfa8"},"cell_type":"code","source":"pca.explained_variance_ratio_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4191101cb5212016c2205b4a127216b35583492b"},"cell_type":"code","source":"#Making the screeplot - plotting the cumulative variance against the number of components\n%matplotlib inline\nfig = plt.figure(figsize = (12,8))\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel('number of components')\nplt.ylabel('cumulative explained variance')\nplt.savefig('pca_no')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c77c5aa60728b893ab2b5f46948f956ea187250a"},"cell_type":"markdown","source":"#### Looks like 4 components are enough to describe 95% of the variance in the dataset\nWe'll choose 4 components for our modeling\n"},{"metadata":{"trusted":true,"_uuid":"3a20b76b3d03fb210c1afce6bfca99b14d791d68"},"cell_type":"code","source":"#Using incremental PCA for efficiency - saves a lot of time on larger datasets\nfrom sklearn.decomposition import IncrementalPCA\npca_final = IncrementalPCA(n_components=4)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"43a39d0d155704639ac04a26ffa3b182c929c6c4"},"cell_type":"markdown","source":"#### Basis transformation - getting the data onto our PCs"},{"metadata":{"trusted":true,"_uuid":"1b0fbd602bb865ee4ff36f4a7d95f51e848012f4"},"cell_type":"code","source":"df_pca = pca_final.fit_transform(X)\ndf_pca.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"537fb8946688a8c6844c8bc509e15b31ccca7155"},"cell_type":"code","source":"df_pca = pd.DataFrame(df_pca)\ndf_pca.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c9bb5f2d2404f1314f56024483dbc6c0dcb3cfc8"},"cell_type":"markdown","source":"#### Creating correlation matrix for the principal components - we expect little to no correlation"},{"metadata":{"trusted":true,"_uuid":"284d3fa0ca84f0910da9e874d35a909723ab9de6"},"cell_type":"code","source":"#creating correlation matrix for the principal components\ncorrmat = np.corrcoef(df_pca.transpose())","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"b199b9ee6db8cf1d972f74299756da1ee16b7864"},"cell_type":"code","source":"#plotting the correlation matrix\n%matplotlib inline\nplt.figure(figsize = (10,5))\nsns.heatmap(corrmat,annot = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f63a8303eeef53b15b6a4b3cfaf9cb24db2ffe64"},"cell_type":"code","source":"# To perform KMeans clustering \nfrom sklearn.cluster import KMeans","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f32aba24a0a1640c0c691259f88257a74f548ea8"},"cell_type":"markdown","source":"## Hopkins Statistics:\nThe Hopkins statistic, is a statistic which gives a value which indicates the cluster tendency, in other words: how well the data can be clustered.\n\n- If the value is between {0.01, ...,0.3}, the data is regularly spaced.\n\n- If the value is around 0.5, it is random.\n\n- If the value is between {0.7, ..., 0.99}, it has a high tendency to cluster."},{"metadata":{"trusted":true,"_uuid":"ef6c542b8fae087fc9170a8821521eed47203c42"},"cell_type":"code","source":"from sklearn.neighbors import NearestNeighbors\nfrom random import sample\nfrom numpy.random import uniform\nimport numpy as np\nfrom math import isnan\n \ndef hopkins(X):\n    d = X.shape[1]\n    #d = len(vars) # columns\n    n = len(X) # rows\n    m = int(0.1 * n) \n    nbrs = NearestNeighbors(n_neighbors=1).fit(X.values)\n \n    rand_X = sample(range(0, n, 1), m)\n \n    ujd = []\n    wjd = []\n    for j in range(0, m):\n        u_dist, _ = nbrs.kneighbors(uniform(np.amin(X,axis=0),np.amax(X,axis=0),d).reshape(1, -1), 2, return_distance=True)\n        ujd.append(u_dist[0][1])\n        w_dist, _ = nbrs.kneighbors(X.iloc[rand_X[j]].values.reshape(1, -1), 2, return_distance=True)\n        wjd.append(w_dist[0][1])\n \n    H = sum(ujd) / (sum(ujd) + sum(wjd))\n    if isnan(H):\n        print(ujd, wjd)\n        H = 0\n \n    return H","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7cddd4e7c3c80108232f040cc6dcb7940d938e13"},"cell_type":"code","source":"hopkins(df_pca)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c70fb18ded6c708c7c1ecfe7c0863966d1243f7b"},"cell_type":"code","source":"#  high tendency to cluster.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ddbbe92969fbaad061ca3b9e98ee8c99af88a965"},"cell_type":"markdown","source":"## Hierarchical clustering"},{"metadata":{"trusted":true,"_uuid":"18eb020c65d14b869e88454ee3d36452615cffb7"},"cell_type":"code","source":"mergings = linkage(df_pca, method = \"complete\", metric='euclidean')\ndendrogram(mergings)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4f30a46e355fcf3306275b82348c94432c9edffd"},"cell_type":"code","source":"# Looking at the dedrogram it is observed that cutting it at n = 5 is most optimum.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d8923517c1e75d23667dd5e3e1bc018cbc9fdadc"},"cell_type":"code","source":"clusterCut = pd.Series(cut_tree(mergings, n_clusters = 5).reshape(-1,))\ndf_pca_hc = pd.concat([df_pca, clusterCut], axis=1)\ndf_pca_hc.columns = [\"PC1\",\"PC2\",\"PC3\",\"PC4\",\"ClusterID\"]\ndf_pca_hc.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"95de15a0d4a7876c5e4672aec4c8de8deeaf67a2"},"cell_type":"code","source":"pca_cluster_hc = pd.concat([data_help['country'],df_pca_hc], axis=1, join='outer', join_axes=None, ignore_index=False, keys=None, levels=None, names=None, verify_integrity=False, sort=None, copy=True)\npca_cluster_hc.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d1595605ccb3aeab2c16eb905099b8aa376ab7ee"},"cell_type":"code","source":"clustered_data_hc = pca_cluster_hc[['country','ClusterID']].merge(data, on = 'country')\nclustered_data_hc.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ff3f01287d309770cca26f3db87b7c07430fb298"},"cell_type":"code","source":"hc_clusters_child_mort = \tpd.DataFrame(clustered_data_hc.groupby([\"ClusterID\"]).child_mort.mean())\nhc_clusters_exports = \tpd.DataFrame(clustered_data_hc.groupby([\"ClusterID\"]).exports.mean())\nhc_clusters_health = \tpd.DataFrame(clustered_data_hc.groupby([\"ClusterID\"]).health.mean())\nhc_clusters_imports = \tpd.DataFrame(clustered_data_hc.groupby([\"ClusterID\"]).imports.mean())\nhc_clusters_income = \tpd.DataFrame(clustered_data_hc.groupby([\"ClusterID\"]).income.mean())\nhc_clusters_inflation = \tpd.DataFrame(clustered_data_hc.groupby([\"ClusterID\"]).inflation.mean())\nhc_clusters_life_expec = \tpd.DataFrame(clustered_data_hc.groupby([\"ClusterID\"]).life_expec.mean())\nhc_clusters_total_fer = \tpd.DataFrame(clustered_data_hc.groupby([\"ClusterID\"]).total_fer.mean())\nhc_clusters_gdpp = \tpd.DataFrame(clustered_data_hc.groupby([\"ClusterID\"]).gdpp.mean())","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"5edbc969753afde5aeef180d304dcb9cf0213fce"},"cell_type":"code","source":"df = pd.concat([pd.Series(list(range(0,5))), hc_clusters_child_mort,hc_clusters_exports, hc_clusters_health, hc_clusters_imports,\n               hc_clusters_income, hc_clusters_inflation, hc_clusters_life_expec,hc_clusters_total_fer,hc_clusters_gdpp], axis=1)\ndf.columns = [\"ClusterID\", \"child_mort_mean\", \"exports_mean\", \"health_mean\", \"imports_mean\", \"income_mean\", \"inflation_mean\",\n               \"life_expec_mean\", \"total_fer_mean\", \"gdpp_mean\"]\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ebe6d01f0094d5a889b95f8879557be4c06fc5ec"},"cell_type":"code","source":"fig, axs = plt.subplots(3,3,figsize = (15,15))\n\nsns.barplot(x=df.ClusterID, y=df.child_mort_mean, ax = axs[0,0])\nsns.barplot(x=df.ClusterID, y=df.exports_mean, ax = axs[0,1])\nsns.barplot(x=df.ClusterID, y=df.health_mean, ax = axs[0,2])\nsns.barplot(x=df.ClusterID, y=df.imports_mean, ax = axs[1,0])\nsns.barplot(x=df.ClusterID, y=df.income_mean, ax = axs[1,1])\nsns.barplot(x=df.ClusterID, y=df.life_expec_mean, ax = axs[1,2])\nsns.barplot(x=df.ClusterID, y=df.inflation_mean, ax = axs[2,0])\nsns.barplot(x=df.ClusterID, y=df.total_fer_mean, ax = axs[2,1])\nsns.barplot(x=df.ClusterID, y=df.gdpp_mean, ax = axs[2,2])\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0fc67c8c77eca102a3baf75969905b9dbc4a99a7"},"cell_type":"code","source":"clustered_data_hc[clustered_data_hc.ClusterID == 0].country.values","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6f3edf6d5f78729c0cab8a33d3df6147ebccd623"},"cell_type":"markdown","source":"### Recommendations\n1. Cluster with ClusterID as 0, is the cluster of most backward country.\n2. Countries on which we require to focus more are\n       Afghanistan', 'Benin', 'Botswana', 'Burkina Faso', 'Burundi',\n       'Cameroon', 'Central African Republic', 'Chad', 'Comoros',\n       'Congo, Dem. Rep.', \"Cote d'Ivoire\", 'Eritrea', 'Gabon', 'Gambia',\n       'Ghana', 'Guinea', 'Guinea-Bissau', 'Haiti', 'Iraq', 'Kenya',\n       'Kiribati', 'Lao', 'Lesotho', 'Liberia', 'Madagascar', 'Malawi',\n       'Mali', 'Micronesia, Fed. Sts.', 'Mozambique', 'Namibia', 'Niger',\n       'Nigeria', 'Pakistan', 'Rwanda', 'Senegal', 'Sierra Leone',\n       'Solomon Islands', 'South Africa', 'Sudan', 'Tajikistan',\n       'Tanzania', 'Timor-Leste', 'Togo', 'Uganda', 'Yemen', 'Zambia''"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}