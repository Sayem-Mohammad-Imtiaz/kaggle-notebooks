{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from string import punctuation\nfrom os import listdir\nfrom numpy import array\nfrom numpy import asarray\nfrom numpy import zeros\nimport pandas as pd\nimport numpy as np\nfrom numpy import array\nimport keras\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense,Dropout\nfrom keras.layers import Flatten\nfrom keras.layers import Embedding\nfrom keras.layers.convolutional import Conv1D\nfrom keras.layers.convolutional import MaxPooling1D\nfrom sklearn.metrics import confusion_matrix\nimport matplotlib.pyplot as plt\nimport itertools","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-04T13:33:50.988942Z","iopub.execute_input":"2021-06-04T13:33:50.98936Z","iopub.status.idle":"2021-06-04T13:33:51.002106Z","shell.execute_reply.started":"2021-06-04T13:33:50.989322Z","shell.execute_reply":"2021-06-04T13:33:51.001232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_data(filename,encoding):\n    data = pd.read_csv(filename,encoding=encoding)\n    return data\n\ndef create_train_test_sets(data,split):\n    np.random.seed(0)\n    mask = np.random.rand(len(data)) < split\n    train_data = data[mask]\n    test_data = data[~mask]\n    return train_data,test_data\n\ndef clean_and_get_tokens(doc):\n    tokens = doc.split()\n    table = str.maketrans('','',punctuation)\n    tokens = [w.translate(table) for w in tokens]\n    tokens = [word for word in tokens if word.isalpha()]\n    tokens = [word for word in tokens if len(word)>2]\n    return tokens\n\ndef load_embedding(filename,encoding): \n    file = open(filename,'r',encoding=encoding)\n    lines = file.readlines()[1:]\n    file.close()\n    embedding = dict()\n    for line in lines:\n        parts = line.split()\n        embedding[parts[0]] = asarray(parts[1:], dtype='float32')\n    return embedding","metadata":{"execution":{"iopub.status.busy":"2021-06-04T13:06:08.76739Z","iopub.execute_input":"2021-06-04T13:06:08.767761Z","iopub.status.idle":"2021-06-04T13:06:08.78059Z","shell.execute_reply.started":"2021-06-04T13:06:08.767722Z","shell.execute_reply":"2021-06-04T13:06:08.779558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = load_data('../input/bbc-fulltext-and-category/bbc-text.csv','latin1')\nwords = set()\nvocab = {}\n\n\ntoken = data['text'][0].split()\ntable = str.maketrans('','',punctuation)\ntokens = [w.translate(table) for w in token] \n\ntokens = [word for word in tokens if word.isalpha()]\ntokens = [word for word in tokens if len(word)>2]\n\ndata['category'] = data['category'].astype('category').cat.codes","metadata":{"execution":{"iopub.status.busy":"2021-06-04T13:15:46.557067Z","iopub.execute_input":"2021-06-04T13:15:46.557402Z","iopub.status.idle":"2021-06-04T13:15:46.615535Z","shell.execute_reply.started":"2021-06-04T13:15:46.557372Z","shell.execute_reply":"2021-06-04T13:15:46.614694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-04T13:15:48.472539Z","iopub.execute_input":"2021-06-04T13:15:48.472887Z","iopub.status.idle":"2021-06-04T13:15:48.484092Z","shell.execute_reply.started":"2021-06-04T13:15:48.472855Z","shell.execute_reply":"2021-06-04T13:15:48.482281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"documents = data['text']\nfor doc in documents:\n    tokens = clean_and_get_tokens(doc)\n    for token in tokens:\n        if token in vocab:\n            vocab[token] += 1\n        else:\n            vocab[token] = 1\n\nfor word in vocab:\n    if vocab[word] > 5:\n        words.add(word)\n\n\n\ntrain_data,test_data = create_train_test_sets(data,0.8)\n\ntrain_documents = []\nfor doc in train_data['text']:\n    tokens = doc.split()\n    final_tokens = []\n    for token in tokens:\n        if token in words:\n            final_tokens.append(token)\n    final_string = ' '.join(final_tokens)\n    train_documents.append(final_string)\n\ntest_documents = []\nfor doc in test_data['text']:\n    tokens = doc.split()\n    final_tokens = []\n    for token in tokens:\n        if token in words:\n            final_tokens.append(token)\n    final_string = ' '.join(final_tokens)\n    test_documents.append(final_string)","metadata":{"execution":{"iopub.status.busy":"2021-06-04T13:15:57.745786Z","iopub.execute_input":"2021-06-04T13:15:57.746107Z","iopub.status.idle":"2021-06-04T13:15:58.75028Z","shell.execute_reply.started":"2021-06-04T13:15:57.746077Z","shell.execute_reply":"2021-06-04T13:15:58.749511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = Tokenizer()\ntokenizer.fit_on_texts(train_documents)\nencoded_docs = tokenizer.texts_to_sequences(train_documents)\n\nmax_length = max(([len(s.split()) for s in train_documents]))\nlabels = train_data['category']\ntrain_labels = labels\nXtrain = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\nytrain = keras.utils.to_categorical(labels, num_classes=5)","metadata":{"execution":{"iopub.status.busy":"2021-06-04T13:15:58.752191Z","iopub.execute_input":"2021-06-04T13:15:58.752534Z","iopub.status.idle":"2021-06-04T13:15:59.35216Z","shell.execute_reply.started":"2021-06-04T13:15:58.752499Z","shell.execute_reply":"2021-06-04T13:15:59.351109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encoded_docs = tokenizer.texts_to_sequences(test_documents)\nlabels = test_data['category']\nXtest = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\nytest = keras.utils.to_categorical(labels, num_classes=5)","metadata":{"execution":{"iopub.status.busy":"2021-06-04T13:15:59.35345Z","iopub.execute_input":"2021-06-04T13:15:59.353806Z","iopub.status.idle":"2021-06-04T13:15:59.442328Z","shell.execute_reply.started":"2021-06-04T13:15:59.353767Z","shell.execute_reply":"2021-06-04T13:15:59.441673Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vocab_size = len(tokenizer.word_index)+1\nraw_embedding = load_embedding('../input/glove-global-vectors-for-word-representation/glove.6B.100d.txt','utf8')\n\nweight_matrix = zeros((vocab_size, 100))\nfor word,i in tokenizer.word_index.items():\n    if word in raw_embedding:\n        weight_matrix[i] = raw_embedding[word]\nembedding_layer = Embedding(vocab_size, 100, weights=[weight_matrix], input_length=max_length, trainable=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-04T13:15:59.443839Z","iopub.execute_input":"2021-06-04T13:15:59.444212Z","iopub.status.idle":"2021-06-04T13:16:18.521782Z","shell.execute_reply.started":"2021-06-04T13:15:59.444171Z","shell.execute_reply":"2021-06-04T13:16:18.520962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vocab_size = len(tokenizer.word_index) + 1\n\nmodel = Sequential()\nmodel.add(Embedding(vocab_size, 100, input_length = max_length))\nmodel.add(Conv1D(filters=16, kernel_size=16, activation='relu'))\nmodel.add(MaxPooling1D(pool_size=2))\nmodel.add(Conv1D(filters=32, kernel_size=32, activation='relu'))\nmodel.add(MaxPooling1D(pool_size=2))\nmodel.add(Flatten())\nmodel.add(Dropout(0.2))\nmodel.add(Dense(5, activation='softmax'))\nprint(model.summary())","metadata":{"execution":{"iopub.status.busy":"2021-06-04T13:16:18.524343Z","iopub.execute_input":"2021-06-04T13:16:18.524871Z","iopub.status.idle":"2021-06-04T13:16:20.916761Z","shell.execute_reply.started":"2021-06-04T13:16:18.524833Z","shell.execute_reply":"2021-06-04T13:16:20.91578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.fit(Xtrain, ytrain, epochs=10, verbose=2, validation_data = (Xtest,ytest))","metadata":{"execution":{"iopub.status.busy":"2021-06-04T13:16:20.91987Z","iopub.execute_input":"2021-06-04T13:16:20.920117Z","iopub.status.idle":"2021-06-04T13:16:42.935728Z","shell.execute_reply.started":"2021-06-04T13:16:20.920091Z","shell.execute_reply":"2021-06-04T13:16:42.934831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"random_text = [\"I love business and sports\"]\nencoded_text = tokenizer.texts_to_sequences(random_text)\ntest_text = pad_sequences(encoded_text, maxlen = max_length, padding= 'post')\nmodel.predict(test_text)","metadata":{"execution":{"iopub.status.busy":"2021-06-04T13:16:42.937243Z","iopub.execute_input":"2021-06-04T13:16:42.937605Z","iopub.status.idle":"2021-06-04T13:16:43.076356Z","shell.execute_reply.started":"2021-06-04T13:16:42.937569Z","shell.execute_reply":"2021-06-04T13:16:43.075507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ypred = model.predict(Xtest)\npred_labels = []\nfor probs in ypred:\n    label = np.argmax(probs, axis=-1)\n    pred_labels.append(int(label))\nactual_labels = list(labels)","metadata":{"execution":{"iopub.status.busy":"2021-06-04T13:16:43.077689Z","iopub.execute_input":"2021-06-04T13:16:43.078027Z","iopub.status.idle":"2021-06-04T13:16:43.253942Z","shell.execute_reply.started":"2021-06-04T13:16:43.077989Z","shell.execute_reply":"2021-06-04T13:16:43.252988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cm = confusion_matrix(actual_labels, pred_labels)","metadata":{"execution":{"iopub.status.busy":"2021-06-04T13:16:43.255191Z","iopub.execute_input":"2021-06-04T13:16:43.255542Z","iopub.status.idle":"2021-06-04T13:16:43.262873Z","shell.execute_reply.started":"2021-06-04T13:16:43.255507Z","shell.execute_reply":"2021-06-04T13:16:43.261754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cmap = plt.cm.Blues\ntitle = \"Confusion Matrix\"\nclasses = 5\nnormalize = False\ntick_marks = np.arange(classes)\nplt.imshow(cm, interpolation='nearest', cmap=cmap)\nplt.title(title)\nplt.colorbar()\ntick_marks = np.arange(5)\n\nfmt = '.2f' if normalize else 'd'\nthresh = cm.max() / 2.\nfor i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n    plt.text(j, i, format(cm[i, j], fmt),\n             horizontalalignment=\"center\",\n             color=\"white\" if cm[i, j] > thresh else \"black\")\n\nplt.tight_layout()\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-04T13:16:43.264661Z","iopub.execute_input":"2021-06-04T13:16:43.265084Z","iopub.status.idle":"2021-06-04T13:16:43.591532Z","shell.execute_reply.started":"2021-06-04T13:16:43.265045Z","shell.execute_reply":"2021-06-04T13:16:43.590617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"metric = keras.metrics.CategoricalAccuracy()\nmetric.update_state(ytest, ypred)\nmetric.result().numpy()","metadata":{"execution":{"iopub.status.busy":"2021-06-04T13:33:54.682764Z","iopub.execute_input":"2021-06-04T13:33:54.68347Z","iopub.status.idle":"2021-06-04T13:33:54.713064Z","shell.execute_reply.started":"2021-06-04T13:33:54.683416Z","shell.execute_reply":"2021-06-04T13:33:54.712362Z"},"trusted":true},"execution_count":null,"outputs":[]}]}