{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Setup\n\n# common:\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport plotly.express as px\nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff\nfrom plotly import tools\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\nimport matplotlib.patches as mpatches\nfrom scipy.stats import norm\nfrom scipy import stats\nimport time\nimport folium\nimport collections\nimport eli5 # Feature importance evaluation\nimport urllib\nfrom PIL import Image\n\n# for ML:\nfrom sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, average_precision_score, roc_curve, precision_recall_curve, classification_report, confusion_matrix, mean_absolute_error, mean_squared_error\nfrom sklearn.model_selection import train_test_split, KFold, StratifiedKFold, ShuffleSplit, cross_validate, cross_val_score, cross_val_predict, RandomizedSearchCV, GridSearchCV, learning_curve\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.manifold import TSNE\nfrom sklearn.decomposition import PCA, TruncatedSVD\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler, RobustScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.gaussian_process.kernels import RBF\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, BaggingClassifier\nfrom xgboost import XGBClassifier\n\n# Imported Libraries\nfrom imblearn.pipeline import make_pipeline as imbalanced_make_pipeline\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import NearMiss\nfrom imblearn.metrics import classification_report_imbalanced\nfrom collections import Counter\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# set some display options:\nsns.set(style=\"whitegrid\")\npd.set_option(\"display.max_columns\", 36)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load data:\nfile_path = '/kaggle/input/hotel-booking-demand/hotel_bookings.csv'\ndf = pd.read_csv(file_path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.set_option(\"display.float_format\", \"{:.2f}\".format)\ndf.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.drop('company', axis=1, inplace=True)\n\n# \"meal\" contains values \"Undefined\", which is equal to SC.\ndf[\"meal\"].replace(\"Undefined\", \"SC\", inplace=True)\n\n# Some rows contain entreis with 0 adults, 0 children and 0 babies. \n# I'm dropping these entries with no guests.\nzero_guests = list(df.loc[df[\"adults\"]\n                   + df[\"children\"]\n                   + df[\"babies\"]==0].index)\ndf.drop(df.index[zero_guests], inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.drop(['country',\n            'arrival_date_year',\n            'arrival_date_week_number',\n            'arrival_date_day_of_month',\n            'stays_in_weekend_nights',\n            'stays_in_week_nights',\n            'days_in_waiting_list',\n            'required_car_parking_spaces',\n            'reservation_status'\n            ], \n            axis=1, inplace=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"categorical_features = []\nfor column in df.columns:\n    if df[column].dtype == object:\n        categorical_features.append(column)\n        print(f\"{column}\")\n        print(\"====================================\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"numerical_features = []\nfor column in df.columns:\n    if df[column].dtype != object:\n        numerical_features.append(column)\n        print(f\"{column}\")\n        print(\"====================================\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"numerical_features.remove('is_canceled')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"markdown","source":"### Cancellation rate","metadata":{}},{"cell_type":"code","source":"print('No Canceled', round(df['is_canceled'].value_counts()[0]/len(df) * 100,2), '% of the dataset')\nprint('Canceled', round(df['is_canceled'].value_counts()[1]/len(df) * 100,2), '% of the dataset')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(x='is_canceled', data=df)\nplt.title('Is_canceled Distributions \\n (0: No Canceled || 1: Canceled)', fontsize=14)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(20, 20))\n\nfor i, feature in enumerate(numerical_features, 1):\n    plt.subplot(4, 3, i)\n    df[df[\"is_canceled\"] == 0][feature].hist(bins=35, color='blue', label='Not Cancelation', alpha=0.6)\n    df[df[\"is_canceled\"] == 1][feature].hist(bins=35, color='red', label='Cancelation', alpha=0.6)\n    plt.legend()\n    plt.xlabel(feature)\n    plt.ylabel('count')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(20, 20))\n\nfor i, feature in enumerate(categorical_features, 1):\n    plt.subplot(4, 3, i)\n    df[df[\"is_canceled\"] == 0][feature].hist(bins=35, color='blue', label='Not Cancelation', alpha=0.6)\n    df[df[\"is_canceled\"] == 1][feature].hist(bins=35, color='red', label='Cancelation', alpha=0.6)\n    plt.legend()\n    plt.xlabel(feature)\n    plt.ylabel('count')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### hotel vs is_canceled","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize = (8,6))\nsns.countplot(x='hotel', data=df, hue='is_canceled')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"city_hotel_canceled_count, resort_hotel_canceled_count = df.loc[df['is_canceled'] == 1]['hotel'].value_counts()\ncity_hotel_count, resort_hotel_count = hotel_data = df['hotel'].value_counts()\npercent_city_hotel_canceled = round(city_hotel_canceled_count / city_hotel_count * 100, 2)\npercent_resort_hotel_canceled = round(resort_hotel_canceled_count / resort_hotel_count * 100, 2)\n\n# df.groupby(['hotel'])['is_canceled'].value_counts()\n# df.groupby(['hotel'])['is_canceled'].mean()\n\nprint(\nf\"\"\"\nCancelation rate on hotel type\nCity hotel: {percent_city_hotel_canceled:.2f} %\nResort hotel: {percent_resort_hotel_canceled:.2f} %\n\"\"\"    \n)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"City hotel has higher Cancellation rate than Resort Hotel.\n\nAround 30% for resort hotel and greater than 40 % for city hotel.","metadata":{}},{"cell_type":"code","source":"rh = df.loc[(df[\"hotel\"] == \"Resort Hotel\") & (df[\"is_canceled\"] == 0)]\nch = df.loc[(df[\"hotel\"] == \"City Hotel\") & (df[\"is_canceled\"] == 0)]\n\nrh[\"adr_pp\"] = rh[\"adr\"] / (rh[\"adults\"] + rh[\"children\"] + rh[\"babies\"])\nch[\"adr_pp\"] = ch[\"adr\"] / (ch[\"adults\"] + ch[\"children\"] + ch[\"babies\"])\n\nprint(\nf\"\"\"\nFrom all non-cnceled bookings, across all room types and meals, the average prices are:\nCity hotel: {ch[\"adr_pp\"].mean():.2f} € per night and person.\nResort hotel: {rh[\"adr_pp\"].mean():.2f} € per night and person.\n\"\"\"\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I think the average price is one of the most important reasons why City hotel has higher Cancellation rate than Resort Hotel.","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# normalize price per night (adr):\ndf[\"adr_pp\"] = df[\"adr\"] / (df[\"adults\"] + df[\"children\"] + df[\"babies\"])\nfull_data_guests = df.loc[df[\"is_canceled\"] == 0] # only actual gusts\nroom_prices = full_data_guests[[\"hotel\", \"reserved_room_type\", \"adr_pp\"]].sort_values(\"reserved_room_type\")\n\n# boxplot:\nplt.figure(figsize=(12, 8))\nsns.boxplot(x=\"reserved_room_type\",\n            y=\"adr_pp\",\n            hue=\"hotel\",\n            data=room_prices, \n            hue_order=[\"City Hotel\", \"Resort Hotel\"],\n            fliersize=0)\nplt.title(\"Price of room types per night and person\", fontsize=16)\nplt.xlabel(\"Room type\", fontsize=16)\nplt.ylabel(\"Price [EUR]\", fontsize=16)\nplt.legend(loc=\"upper right\")\nplt.ylim(0, 160)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This figure shows the average price per room, depending on its type and the standard deviation. \n \nNote that due to data anonymization rooms with the same type letter may not necessarily be the same across hotels.","metadata":{}},{"cell_type":"markdown","source":"### market_segment vs is_canceled","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(nrows=2, ncols=1, figsize=(10,8))\nsns.countplot(x='market_segment', data=df, ax=ax[0])\nsns.countplot(x='market_segment', data=df, hue='is_canceled', ax=ax[1])\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### customer_type vs is_canceled","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(nrows=2, ncols=1, figsize=(10,8))\nsns.countplot(x='customer_type', data=df, ax=ax[0])\nsns.countplot(x='customer_type', data=df, hue='is_canceled', ax=ax[1])\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### deposit_type vs is_canceled","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(nrows=2, ncols=1, figsize=(10,8))\nsns.countplot(x='deposit_type', data=df, ax=ax[0])\nsns.countplot(x='deposit_type', data=df, hue='is_canceled', ax=ax[1])\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### month vs is_canceled","metadata":{}},{"cell_type":"code","source":"order = ['January', 'February', 'March' ,'April', 'May', 'June','July', 'August', 'September', 'October', 'November', 'December' ]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(nrows=2, ncols=1, figsize=(10,8))\nsns.countplot(x='arrival_date_month', data=df, ax=ax[0], order=order)\nsns.countplot(x='arrival_date_month', data=df, hue='is_canceled', ax=ax[1], order=order)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### lead_time vs is_canceled","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10,8))\nsns.boxplot(x='is_canceled', y='lead_time', data=df, ax=ax[0])\nsns.violinplot(x='is_canceled', y='lead_time', data=df, hue='is_canceled', ax=ax[1])\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make sure we use the subsample in our correlation\nfig = plt.figure(figsize=(15, 12))\n\npalette = sns.diverging_palette(20, 220, n=256)\n\ncorr = df.corr(method='pearson')\nsns.heatmap(corr, cmap=palette, vmax=.3, center=0, square=True, linewidths=.5, annot_kws={\"size\":15}, cbar_kws={'shrink': .5})\nplt.title(\"Imbalanced Correlation Matrix \\n (don't use for reference)\")\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cancel_corr = df.corr()[\"is_canceled\"]\ncancel_corr.abs().sort_values(ascending=False)[1:]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ML","metadata":{}},{"cell_type":"code","source":"# Separate features and predicted value\nfeatures = numerical_features + categorical_features\nY = df['is_canceled']\nX = df.drop('is_canceled', axis=1)[features]\n\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n# preprocess numerical feats:\n# for most num cols, except the dates, 0 is the most logical choice as fill value\n# and here no dates are missing.\nnum_transformer = Pipeline(steps=[\n    (\"imputer\", SimpleImputer(strategy=\"constant\")),\n    ('scaler', StandardScaler())])\n\n# Preprocessing for categorical features:\ncat_transformer = Pipeline(steps=[\n    (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"Unknown\")),\n    (\"onehot\", OneHotEncoder(handle_unknown='ignore'))])\n\n# Bundle preprocessing for numerical and categorical features:\npreprocessor = ColumnTransformer(transformers=[(\"num\", num_transformer, numerical_features),\n                                               (\"cat\", cat_transformer, categorical_features)])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# define base_models to test:\nbase_models = {\n    'LOR_model': LogisticRegression(),\n    'KNC_model': KNeighborsClassifier(),\n    'SVM_model': SVC(),\n    'DTR_model': DecisionTreeClassifier(),\n    'RFC_model': RandomForestClassifier(),\n    'ETC_model': ExtraTreesClassifier(),\n    'BAG_model': BaggingClassifier(),\n    'MLP_model': MLPClassifier(),\n    'XGB_model': XGBClassifier(),\n}\n\nmodel_score = {}\n\n# split data into 'kfolds' parts for cross validation,\n# use shuffle to ensure random distribution of data:\nkfolds = 4 # 4 = 75% train, 25% validation\nsplit = KFold(n_splits=kfolds, shuffle=True, random_state=42)\n\n# Preprocessing, fitting, making predictions and scoring for every model:\nfor name, model in base_models.items():\n    # pack preprocessing of data and the model in a pipeline:\n    model_steps = Pipeline(steps=[\n                                    ('preprocessor', preprocessor),\n                                    ('model', model)])\n    # get cross validation score for each model:\n    cv_results = cross_val_score(model_steps, \n                                 X_train, Y_train, \n                                 cv=split,\n                                 scoring=\"accuracy\",\n                                 n_jobs=-1)\n    model_score[name] = cv_results\n    \n    # output:\n    min_score = round(min(cv_results), 4)\n    max_score = round(max(cv_results), 4)\n    mean_score = round(np.mean(cv_results), 4)\n    std_dev = round(np.std(cv_results), 4)\n    print(f\"{name} cross validation accuracy score: {mean_score} +/- {std_dev} (std) min: {min_score}, max: {max_score}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"figure = plt.figure(figsize=(15,12))\n\nmean_score = []\nlower_mean_socre = []\nupper_mean_socre = []\nmodel_name = []\nfor model, score in model_score.items():\n    mean_score.append(round(np.mean(score), 4))\n    lower_mean_socre.append(round(np.mean(score), 4) - round(np.std(score), 4))\n    upper_mean_socre.append(round(np.mean(score), 4) + round(np.std(score), 4))\n    model_name.append(model)\nplt.plot(model_name, mean_score, 'o-')\nplt.fill_between(model_name, lower_mean_socre, upper_mean_socre, alpha=0.1)\nplt.title(\"Score Curve\", fontsize=14)\nplt.xlabel('model name')\nplt.ylabel('Score')\nplt.grid(True)\nplt.legend(loc=\"best\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = RandomForestClassifier(random_state=42, n_jobs=-1,)\n\nmodel_steps = Pipeline(steps=[\n                                ('preprocessor', preprocessor),\n                                ('model', model)])\n\n# fit model(pipeline) so values can be accessed:\nmodel_steps.fit(X_train, Y_train)\n\nY_pred = model_steps.predict(X_test)\nActVPred = pd.DataFrame({'Actual': Y_test, 'Predicted': Y_pred})\nprint(ActVPred)\n\nlabels = ['No Canceled', 'Canceled']\nprint(classification_report(Y_test, Y_pred, target_names=labels))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Names of all (encoded) features are needed.\n# Get names of columns from One Hot Encoding:\nonehot_columns = list(model_steps.named_steps['preprocessor'].\n                      named_transformers_['cat'].\n                      named_steps['onehot'].\n                      get_feature_names(input_features=categorical_features))\n\n# Add num_features for full list.\n# Order must be as in definition of X, where num_features are first: \nfeat_imp_list = numerical_features + onehot_columns\n\n# show 10 most important features, provide names of features:\nfeat_imp_df = eli5.formatters.as_dataframe.explain_weights_df(\n    model_steps.named_steps['model'],\n    feature_names=feat_imp_list)\nfeat_imp_df.head(10)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Looking at the three most important features:  \n- lead_time  \n- deposit_type  \n- adr","metadata":{}},{"cell_type":"markdown","source":"### lead_time:","metadata":{}},{"cell_type":"code","source":"# group data for lead_time:\nlead_cancel_data = df.groupby(\"lead_time\")[\"is_canceled\"].describe()\n# use only lead_times wih more than 10 bookings for graph:\nlead_cancel_data_10 = lead_cancel_data.loc[lead_cancel_data[\"count\"] >= 10]\n\n#show figure:\nplt.figure(figsize=(12, 8))\nsns.regplot(x=lead_cancel_data_10.index, y=lead_cancel_data_10[\"mean\"].values * 100)\nplt.title(\"Effect of lead time on cancelation\", fontsize=16)\nplt.xlabel(\"Lead time\", fontsize=16)\nplt.ylabel(\"Cancelations [%]\", fontsize=16)\n# plt.xlim(0,365)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Bookings made a few days before the arrival date are rarely canceled, whereas bookings made over one year in advance are canceled very often. ","metadata":{}},{"cell_type":"markdown","source":"### Deposit type:","metadata":{}},{"cell_type":"code","source":"# group data for deposit_type:\ndeposit_cancel_data = df.groupby(\"deposit_type\")[\"is_canceled\"].describe()\n\n#show figure:\nplt.figure(figsize=(12, 8))\nsns.barplot(x=deposit_cancel_data.index, y=deposit_cancel_data[\"mean\"] * 100)\nplt.title(\"Effect of deposit_type on cancelation\", fontsize=16)\nplt.xlabel(\"Deposit type\", fontsize=16)\nplt.ylabel(\"Cancelations [%]\", fontsize=16)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As Susmit Vengurlekar already pointed out in the Discussion section of the dataset, the deposit_type 'Non Refund' and the 'is_canceled' column are correlated in a counter-intuitive way.  \nOver 99 % of people who paid the entire amount upfront canceled. This raises the question if there is something wrong with the data (or the description).  \nWhat else stands out for Non Refund deposits?  \nHere is a table of all mean values of the data, grouped by deposit type:","metadata":{}},{"cell_type":"code","source":"deposit_mean_data = df.groupby(\"deposit_type\").mean()\ndeposit_mean_data","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Comparing the mean values for Non refund to No Deposit shows the following:\n- Non Refund deposits are characterized by > 2x longer lead_time   \n- is_repeated_guest is ~ 1/10th  \n- previous_cancellations is 10x higher \n- previous_bookings_not_canceled is 1/15th  \n- required_car_parking_spaces is almost zero  \n- special requests are very rare \n  \nBased on these findings it seems that especially people who have not previosly visited one of the hotels book, pay and cancel repeatedly... this is strange!\n  \nTo adress this issue, I will make a model without this feature below.","metadata":{}},{"cell_type":"markdown","source":"### ADR:","metadata":{}},{"cell_type":"code","source":"# group data for adr:\nadr_cancel_data = df.groupby(\"adr\")[\"is_canceled\"].describe()\n#show figure:\nplt.figure(figsize=(12, 8))\nsns.regplot(x=adr_cancel_data.index, y=adr_cancel_data[\"mean\"].values * 100)\nplt.title(\"Effect of ADR on cancelation\", fontsize=16)\nplt.xlabel(\"ADR\", fontsize=16)\nplt.ylabel(\"Cancelations [%]\", fontsize=16)\nplt.xlim(0,400)\nplt.ylim(0,100)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Separate features and predicted value\ncategorical_features.remove('deposit_type')\nfeatures = numerical_features + categorical_features\nY = df['is_canceled']\nX = df.drop('is_canceled', axis=1)[features]\n\ncv_results = cross_val_score(model_steps, \n                             X_train, Y_train, \n                             cv=split,\n                             scoring=\"accuracy\",\n                             n_jobs=-1)\n                             \n# output:\nmin_score = round(min(cv_results), 4)\nmax_score = round(max(cv_results), 4)\nmean_score = round(np.mean(cv_results), 4)\nstd_dev = round(np.std(cv_results), 4)\nprint(f\"Enhanced RFC model cross validation accuracy score: {mean_score} +/- {std_dev} (std) min: {min_score}, max: {max_score}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fit model(pipeline) so values can be accessed:\nmodel_steps.fit(X_train, Y_train)\n\n# Names of all (encoded) features are needed.\n# Get names of columns from One Hot Encoding:\nonehot_columns = list(model_steps.named_steps['preprocessor'].\n                      named_transformers_['cat'].\n                      named_steps['onehot'].\n                      get_feature_names(input_features=categorical_features))\n\n# Add num_features for full list.\n# Order must be as in definition of X, where num_features are first: \nfeat_imp_list = numerical_features + onehot_columns\n\n# show 10 most important features, provide names of features:\nfeat_imp_df = eli5.formatters.as_dataframe.explain_weights_df(\n    model_steps.named_steps['model'],\n    feature_names=feat_imp_list)\nfeat_imp_df.head(10)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Y_pred = model_steps.predict(X_test)\nActVPred = pd.DataFrame({'Actual': Y_test, 'Predicted': Y_pred})\nprint(ActVPred)\n\nlabels = ['No Canceled', 'Canceled']\nprint(classification_report(Y_test, Y_pred, target_names=labels))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The new accuracy score of 0.8819 is almost identical to the one obtained with the deposit_type included (0.8819),  \nwhich placed significant wheight on this feature.  \nThe new model compensated this by placing increased weight on lead_time, country_PRT, adr and others. ","metadata":{}}]}