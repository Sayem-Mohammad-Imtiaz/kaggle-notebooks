{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**ROC AUC curves compare the TPR and the FPR for different classification thresholds for a classifier.\nROC AUC curves help us select the best model for a job, by evaluating how well a model distinguishes between classes.**"},{"metadata":{},"cell_type":"markdown","source":"Legend:\n\nROC = receiver operating curve\n\nAUC = area under curve\n\nTPR = true positive rate\n\nFPR = false positive rate"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"dir = '../input/sms-spam-collection-dataset/spam.csv'\nimport pandas as pd\ndf = pd.read_csv(dir, encoding='ISO-8859-1')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Convert v1 into your labels, y, and v2, into your features, X. Labels need to be integers to feed into a model so if spam set to 1, and if ham set to 0. If you don’t know, ham means it’s not spam."},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\ny = np.array([(1 if i=='spam' else 0) for i in df.v1.tolist()])\nX = np.array(df.v2.tolist())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"x is now an array of strings and y is an array of 1's and 0's\nsplit the data in test and train sets .and note the data has not been vectorized yet"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import StratifiedShuffleSplit\nsplitter = StratifiedShuffleSplit(\n    n_splits=1, test_size=0.3, random_state=0)\nfor train_index, test_index in splitter.split(X, y):\n    X_train_pre_vectorize, X_test_pre_vectorize = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Fit a vectorizer and transform the test and train sets."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nvectorizer = CountVectorizer()\nX_train = vectorizer.fit_transform(X_train_pre_vectorize)\nX_test = vectorizer.transform(X_test_pre_vectorize)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Choose a classifier and fit it on the train set. I’ve arbitrarily chosen LogisticRegression."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nclassifier = LogisticRegression()\nclassifier.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Normally this is where we’d predict classes for the test set, but since we’re just interested in building the ROC AUC curve, skip it.\nLet’s predict probabilities of classes, and convert the result to an array."},{"metadata":{"trusted":true},"cell_type":"code","source":"y_score = classifier.predict_proba(X_test)\ny_score = np.array(y_score)\nprint(y_score)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The following code can be a little confusing. Using label_binarize() with 3 (or more) classes would convert a single y value [2], into [0 0 1], or [0] into [1 0 0], but it doesn’t work the same with only 2 classes. So we call numpy’s hstack to reformat the output."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import label_binarize\ny_test_bin = label_binarize(y_test, neg_label=0, pos_label=1, classes=[0,1])\ny_test_bin = np.hstack((1 - y_test_bin, y_test_bin))\nprint(y_test_bin)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Generate the curve."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\nfpr = dict()\ntpr = dict()\nroc_auc = dict()\nfor i in [0,1]:\n    # collect labels and scores for the current index\n    labels = y_test_bin[:, i]\n    scores = y_score[:, i]\n    \n    # calculates FPR and TPR for a number of thresholds\n    fpr[i], tpr[i], thresholds = roc_curve(labels, scores)\n    \n    # given points on a curve, this calculates the area under it\n    roc_auc[i] = auc(fpr[i], tpr[i])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"At this point, we could calculate the ROC curve for the 0 and 1 class separately. But for simplicity, we’ll combine them and generate a single curve.\nDisclaimer: This makes more sense when classes are balanced or it may obscure the fact the model is doing poorly in a single class if it does well in the other. But we’ll do it here anyway to learn how.\nWe’ll use “micro-averaging” and flatten the TPR for both classes together, likewise with the FPR. .ravel() will do this for us. So for example,[[1,0],[0,1]] becomes [1,0,0,1]"},{"metadata":{"trusted":true},"cell_type":"code","source":"fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test_bin.ravel(), y_score.ravel())\nroc_auc['micro'] = auc(fpr[\"micro\"], tpr[\"micro\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure()\nlw = 2\nplt.plot(fpr[1], tpr[1], color='darkorange',\n         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc[1])\nplt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic example')\nplt.legend(loc=\"lower right\")\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The greater the area under the orange curve the better the model can distinguish between classes. Another way to look at this is the closer the curve is to the top left, the better."},{"metadata":{},"cell_type":"markdown","source":"What do we mean by “different classification thresholds”?\n\nIf you’re used to using sklearn classifiers out of the box, you’ll know that .predict outputs the predicted class. But you may not know that is based on a 50% classification threshold by default.\nRather than calling .predict(), most classifiers, like LogisticRegression also have a method called predict_proba() which predicts the probability that an example falls into each class.\nUsing this you could recalculate outputted classes using whatever threshold you specify."},{"metadata":{},"cell_type":"markdown","source":"Why ROC and AUC?\n\nThe curve is an “ROC curve” which plots the TPR and FPR at different thresholds.\nAUC is just a calculation of the area under the curve. It’s a way to quantify the model’s accuracy without looking at the curve, or for comparing curves between 2 models when eyeballing areas under the curve doesn’t give a clear winner."},{"metadata":{},"cell_type":"markdown","source":"TPR is the number of TP divided by the sum of TP and FN.\n\nExample 1: A model that predicts if an image is of a dog.\n\nTP: correctly predicted that an image of a dog is a dog.\n\nFN: incorrectly predicted that an image of a dog is a cat.\n\n\nExample 2: A model that predicts if a message is SPAM.\n\nTP: correctly predicted that a SPAM message is SPAM.\n\nFN: incorrectly predicted that a SPAM message is HAM."},{"metadata":{},"cell_type":"markdown","source":"What the ROC AUC curve does not do well\n\nROC curves are not the best choice for imbalanced datasets. They’re best when there’s an even split between classes. Otherwise, the model doing well making classifications for a specific class may hide the fact the model does poorly predicting other classes."},{"metadata":{},"cell_type":"markdown","source":"Takeaways\n\nROC AUC curve can give us insight into a model’s predictive ability in discriminating between classes.\n\nModels with higher AUC are generally more performant than models with lower AUC.\n\nROC AUC is not best for heavily imbalanced datasets."},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}