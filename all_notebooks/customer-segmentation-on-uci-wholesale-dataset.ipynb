{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_df = pd.read_csv('/kaggle/input/uci-wholesale-customers-data/Wholesale customers data.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_df.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_df.drop(['Channel','Region'],axis=1,inplace=True)\nraw_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets Explore few samples in depth"},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_df.loc[[100,200,300],:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fresh filter\nfresh_q1 = 3127.75000\nraw_df[raw_df['Fresh'] < fresh_q1].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Frozen filter\nfrozen_q1 = 742.250000\nraw_df[raw_df.Frozen < frozen_q1].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# frozen q3\nfrozen_q3 = 3554.250000\nraw_df[raw_df.Frozen > frozen_q3].head(7)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Selecting the bottom endices after filtering the quartiles \n   * 43 : low 'Fresh' highest 'Grocery'\n   * 12 : low 'Frozen' highest 'Fresh'\n   * 39 : low 'Detergents_Paper' highest 'Fresh'"},{"metadata":{"trusted":true},"cell_type":"code","source":"selected_samples = [43,12,39]\n\nsamples = pd.DataFrame(raw_df.loc[selected_samples],columns=raw_df.columns).reset_index(drop = True)\nsamples","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Comparing sample means**"},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_data = raw_df.describe().loc['mean',:]\n\nsample_bars = samples.append(mean_data)\n\nsample_bars.index = selected_samples + ['mean']\n\nsample_bars.plot(kind='bar',figsize=(15,8))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Compare percentiles**"},{"metadata":{"trusted":true},"cell_type":"code","source":"percentiles = raw_df.rank(pct=True)\n\npercentiles = 100 * percentiles.round(decimals=3)\n\npercentiles = percentiles.iloc[selected_samples]\n\nsns.heatmap(percentiles,vmin=1,vmax=99,annot=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_df.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Feature Importance**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeRegressor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"deps_vars = list(raw_df.columns)\n\nfor var in deps_vars:\n    new_data = raw_df.drop([var],axis=1)\n    \n    new_feature = pd.DataFrame(raw_df.loc[:,var])\n    \n    X_train, X_test, y_train, y_test = train_test_split(new_data, new_feature, test_size=0.25, random_state=42)\n    \n    dtr = DecisionTreeRegressor(random_state=42)\n    \n    dtr.fit(X_train,y_train)\n    \n    score = dtr.score(X_test, y_test)\n    \n    print('R2 score for {} as dependent variable: {}'.format(var, score))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Plotting Scatter graph for 6 feature**"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(data=raw_df,size=5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now lets draw a corelation matrix."},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_correlation(df,size=10):\n    corr = raw_df.corr()\n    fig, ax = plt.subplots(figsize=(size,size))\n    cax = ax.matshow(df,interpolation='nearest')\n    ax.matshow(corr)\n    fig.colorbar(cax)\n    plt.xticks(range(len(corr.columns)),corr.columns)\n    plt.yticks(range(len(corr.columns)),corr.columns)\n    \nplot_correlation(raw_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"yellow being the most correlated features that can be seen from the above plot.\nso we figure out the degree of CORRELATION between the items.\n* Grocery and Detergent Papers\n* Milk and Grocery\n* milk and Detergent Papers"},{"metadata":{},"cell_type":"markdown","source":"# Data Preprocessing "},{"metadata":{},"cell_type":"markdown","source":"we will implement the following activities on the data set:\n* Feature Scaling\n* Detecting Outliers"},{"metadata":{"trusted":true},"cell_type":"code","source":"log_data = np.log(raw_df)\nlog_sample = np.log(samples)\nsns.pairplot(log_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"log_sample","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_correlation(log_data)\nplot_correlation(log_sample)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Detecting Outliers"},{"metadata":{"trusted":true},"cell_type":"code","source":"np.percentile(raw_df.Milk,25)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import itertools\noutlier_list = []\n\nfor feature in log_data.columns:\n    Q1 = np.percentile(log_data[feature],25)\n    Q3 = np.percentile(log_data[feature],75)\n    step = 1.5 * (Q3 - Q1)\n    print(\"Data points considered outliers for the feature '{}':\".format(feature))\n    outlier_rows = log_data.loc[~((log_data[feature] >= Q1- step) & (log_data[feature] <= Q3 + step)),:]\n    outlier_list.append(list(outlier_rows.index))\noutliers = list(itertools.chain.from_iterable(outlier_list))\nuniq_outliers = list(set(outliers))\ndup_outliers = list(set([x for x in outliers if outliers.count(x) > 1]))\n\nprint('Outliers list:\\n', uniq_outliers)\nprint('Length of outliers list:\\n', len(uniq_outliers))\nprint('Duplicate list:\\n', dup_outliers)\nprint('Length of duplicates list:\\n', len(dup_outliers))\n\ngood_data = log_data.drop(log_data.index[dup_outliers]).reset_index(drop=True)\n\nprint('Original shape of data:\\n', raw_df.shape)\nprint('New shape of data:\\n', good_data.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Transformation"},{"metadata":{},"cell_type":"markdown","source":"implimentation : PCA"},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.cm as cm\nfrom sklearn.decomposition import pca\n\ndef pca_results(good_data, pca):\n\t'''\n\tCreate a DataFrame of the PCA results\n\tIncludes dimension feature weights and explained variance\n\tVisualizes the PCA results\n\t'''\n\n\t# Dimension indexing\n\tdimensions = dimensions = ['Dimension {}'.format(i) for i in range(1,len(pca.components_)+1)]\n\n\t# PCA components\n\tcomponents = pd.DataFrame(np.round(pca.components_, 4), columns = good_data.keys())\n\tcomponents.index = dimensions\n\n\t# PCA explained variance\n\tratios = pca.explained_variance_ratio_.reshape(len(pca.components_), 1)\n\tvariance_ratios = pd.DataFrame(np.round(ratios, 4), columns = ['Explained Variance'])\n\tvariance_ratios.index = dimensions\n\n\t# Create a bar plot visualization\n\tfig, ax = plt.subplots(figsize = (14,8))\n\n\t# Plot the feature weights as a function of the components\n\tcomponents.plot(ax = ax, kind = 'bar');\n\tax.set_ylabel(\"Feature Weights\")\n\tax.set_xticklabels(dimensions, rotation=0)\n\n\n\t# Display the explained variance ratios\n\tfor i, ev in enumerate(pca.explained_variance_ratio_):\n\t\tax.text(i-0.40, ax.get_ylim()[1] + 0.05, \"Explained Variance\\n          %.4f\"%(ev))\n\n\t# Return a concatenated DataFrame\n\treturn pd.concat([variance_ratios, components], axis = 1)\n\ndef cluster_results(reduced_data, preds, centers, pca_samples):\n\t'''\n\tVisualizes the PCA-reduced cluster data in two dimensions\n\tAdds cues for cluster centers and student-selected sample data\n\t'''\n\n\tpredictions = pd.DataFrame(preds, columns = ['Cluster'])\n\tplot_data = pd.concat([predictions, reduced_data], axis = 1)\n\n\t# Generate the cluster plot\n\tfig, ax = plt.subplots(figsize = (14,8))\n\n\t# Color map\n\tcmap = cm.get_cmap('gist_rainbow')\n\n\t# Color the points based on assigned cluster\n\tfor i, cluster in plot_data.groupby('Cluster'):   \n\t    cluster.plot(ax = ax, kind = 'scatter', x = 'Dimension 1', y = 'Dimension 2', \\\n\t                 color = cmap((i)*1.0/(len(centers)-1)), label = 'Cluster %i'%(i), s=30);\n\n\t# Plot centers with indicators\n\tfor i, c in enumerate(centers):\n\t    ax.scatter(x = c[0], y = c[1], color = 'white', edgecolors = 'black', \\\n\t               alpha = 1, linewidth = 2, marker = 'o', s=200);\n\t    ax.scatter(x = c[0], y = c[1], marker='$%d$'%(i), alpha = 1, s=100);\n\n\t# Plot transformed sample points \n\tax.scatter(x = pca_samples[:,0], y = pca_samples[:,1], \\\n\t           s = 150, linewidth = 4, color = 'black', marker = 'x');\n\n\t# Set plot title\n\tax.set_title(\"Cluster Learning on PCA-Reduced Data - Centroids Marked by Number\\nTransformed Sample Data Marked by Black Cross\");\n\n\ndef channel_results(reduced_data, outliers, pca_samples):\n\t'''\n\tVisualizes the PCA-reduced cluster data in two dimensions using the full dataset\n\tData is labeled by \"Channel\" and cues added for student-selected sample data\n\t'''\n\n\t# Check that the dataset is loadable\n\ttry:\n\t    full_data = pd.read_csv(\"customers.csv\")\n\texcept:\n\t    print(\"Dataset could not be loaded. Is the file missing?\")\n\t    return False\n\n\t# Create the Channel DataFrame\n\tchannel = pd.DataFrame(full_data['Channel'], columns = ['Channel'])\n\tchannel = channel.drop(channel.index[outliers]).reset_index(drop = True)\n\tlabeled = pd.concat([reduced_data, channel], axis = 1)\n\t\n\t# Generate the cluster plot\n\tfig, ax = plt.subplots(figsize = (14,8))\n\n\t# Color map\n\tcmap = cm.get_cmap('gist_rainbow')\n\n\t# Color the points based on assigned Channel\n\tlabels = ['Hotel/Restaurant/Cafe', 'Retailer']\n\tgrouped = labeled.groupby('Channel')\n\tfor i, channel in grouped:   \n\t    channel.plot(ax = ax, kind = 'scatter', x = 'Dimension 1', y = 'Dimension 2', \\\n\t                 color = cmap((i-1)*1.0/2), label = labels[i-1], s=30);\n\t    \n\t# Plot transformed sample points   \n\tfor i, sample in enumerate(pca_samples):\n\t\tax.scatter(x = sample[0], y = sample[1], \\\n\t           s = 200, linewidth = 3, color = 'black', marker = 'o', facecolors = 'none');\n\t\tax.scatter(x = sample[0]+0.25, y = sample[1]+0.3, marker='$%d$'%(i), alpha = 1, s=125);\n\n\t# Set plot title\n\tax.set_title(\"PCA-Reduced Data Labeled by 'Channel'\\nTransformed Sample Data Circled\");","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#!pip install renders\n#import renders as rs\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=6)\npca.fit(good_data)\npca_samples = pca.transform(good_data)\npca_results = pca_results(good_data, pca)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pca_results","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"type(pca_results)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pca_results['Explained Variance'].cumsum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Dimensionality Reduction"},{"metadata":{"trusted":true},"cell_type":"code","source":"pca = PCA(n_components=2)\npca.fit(good_data)\nreduced_data = pca.transform(good_data)\npca_samples = pca.transform(log_sample)\nreduced_data = pd.DataFrame(reduced_data, columns = ['Dimension 1', 'Dimension 2'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Applying Clustering to identify Customer segments in data"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Implementation two types of clustering models:\n    1. k-means\n    2. Gaussian Mix model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.mixture import GaussianMixture as GMM\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"range_n_clusters = list(range(2,11))\nprint(range_n_clusters)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for n_clusters in range_n_clusters:\n    clusterer = GMM(n_components=n_clusters).fit(reduced_data)\n    preds = clusterer.predict(reduced_data)\n    centers = clusterer.means_\n    sample_preds = clusterer.predict(pca_samples)\n    score = silhouette_score(reduced_data, preds, metric='mahalanobis')\n    print(\"For n_clusters = {}. The average silhouette_score is : {}\".format(n_clusters, score))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lowest_bic = np.infty\nbic = []\nn_components_range = range(1, 7)\ncv_types = ['spherical', 'tied', 'diag', 'full']\nfor cv_type in cv_types:\n    for n_components in n_components_range:\n        # Fit a mixture of Gaussians with EM\n        gmm = GMM(n_components=n_components, covariance_type=cv_type)\n        gmm.fit(reduced_data)\n        bic.append(gmm.bic(reduced_data))\n        if bic[-1] < lowest_bic:\n            lowest_bic = bic[-1]\n            best_gmm = gmm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for n_clusters in range_n_clusters:\n    clusterer = KMeans(n_clusters=n_clusters).fit(reduced_data)\n    preds = clusterer.predict(reduced_data)\n    centers = clusterer.cluster_centers_\n    sample_preds = clusterer.predict(pca_samples)\n    score = silhouette_score(reduced_data, preds, metric='euclidean')\n    print(\"For n_clusters = {}. The average silhouette_score is : {}\".format(n_clusters, score))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"cluster visualisation"},{"metadata":{},"cell_type":"raw","source":"clusterer = GMM(n_components=2).fit(reduced_data)\npreds = clusterer.predict(reduced_data)\ncenters = clusterer.means_\nsample_preds = clusterer.predict(pca_samples)"},{"metadata":{"trusted":true},"cell_type":"code","source":"cluster_results(reduced_data, preds, centers, pca_samples)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data recovery"},{"metadata":{"trusted":true},"cell_type":"code","source":"log_centers = pca.inverse_transform(centers)\ntrue_centers = np.exp(log_centers)\nsegments = ['Segment {}'.format(i) for i in range(0,len(centers))]\ntrue_centers = pd.DataFrame(np.round(true_centers), columns = raw_df.columns)\ntrue_centers.index = segments\ntrue_centers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"true_centers - raw_df.median()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i, pred in enumerate(sample_preds):\n    print(\"Sample point\", i, \"predicted to be in Cluster\", pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"samples","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dup_outliers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"channel_results(reduced_data, dup_outliers, pca_samples)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}