{"cells":[{"metadata":{"_uuid":"8fcf7cbaaaed671342853cbe4f65636c84da8023"},"cell_type":"markdown","source":"# Deep Learning Project Facial Expression Classification\n---\n\n![](https://i.ytimg.com/vi/wlaR5F30hiU/maxresdefault.jpg)\n\n\n\n### Outline of this Notebook\n---\n1. [**Data Preparation**](#1.-Data-Preparation)\n2. [**Model-Architecture**](#2.-Model-Architecture)\n3. [**Model Evaluation**](#3.-Model-Evaluation)\n\n\n---"},{"metadata":{"_uuid":"f155d47744eda614283d3855bd44fad1d0a48589"},"cell_type":"markdown","source":"# 1. Data Preparation"},{"metadata":{"_uuid":"e0cd506fd79fec44f7aa18aedbda4de5829c5168"},"cell_type":"markdown","source":"Here, we obtaind the dataset from the Kaggle competition \"Challenges in Representation Learning: Facial Expression Recognition Challenge\". From Kaggle open resource, we had training dataset, public test dataset (which is then used as validation dataset for our project), and further a private dataset (same size with public test dataset and will be used as data for evaluating the prediction performance). It is noteworthy that in original provided dataset (either in training dataset or in public test dataset), we have actually in total 7 categories, apart from \"Angry, Surprise, Fear, Happy, Sad and Neutral\" that has been presented in our slides, we had an additional tag of \"Disgust\". However, the main problem will be that we have a quite unbalanced distribution of Disgust label in provided data, it accounts for a unusually low percentage, the obvious unbalance in data distribution will influence further neural network training, therefore we re-classify all pictures with \"Disgust\" label into \"Angry\" label. Such re-classification is relatively subjective, completely based on our perception of those two labels. Both of our teammates believe that the muscle movements in Angry face looks highly similar to that for Disgust faces. "},{"metadata":{"trusted":true,"_uuid":"4132d06760b627904e029a79b98bc5960a7e1662"},"cell_type":"code","source":"from keras.utils.np_utils import to_categorical\nimport pandas as pd\nimport numpy as np\nimport random\nimport sys\nimport warnings \nwarnings.filterwarnings('ignore')\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport brewer2mpl\n\n\ndef emotion_count(y_train, classes):\n    \"\"\"\n    The function re-classify picture with disgust label into angry label\n    \"\"\"\n    emo_classcount = {}\n    for new_num, _class in enumerate(classes):\n        y_train.loc[(y_train == emotion[_class])] = new_num\n        class_count = sum(y_train == (new_num))\n        emo_classcount[_class] = (new_num, class_count)\n    return y_train.values, emo_classcount\n\ndef load_data(sample_split=0.3, usage='Training',classes=['Angry','Happy'], filepath='../input/fer20131.csv'):\n    \"\"\"\n    The function load provided CSV dataset and further reshape, rescale the data for feeding\n    \"\"\"\n    df = pd.read_csv(filepath)\n    df = df[df.Usage == usage]\n    frames = []\n    for _class in classes:\n        class_df = df[df['emotion'] == emotion[_class]]\n        frames.append(class_df)\n    data = pd.concat(frames, axis=0)\n    rows = random.sample(list(data.index), int(len(data)*sample_split))\n    data = data.loc[rows]\n    x = list(data[\"pixels\"])\n    X = []\n    for i in range(len(x)):\n        each_pixel = [int(num) for num in x[i].split()]\n        X.append(each_pixel)\n    ## reshape into 48*48*1 and rescale\n    X = np.array(X)\n    X = X.reshape(X.shape[0], 48, 48,1)\n    X = X.astype(\"float32\")\n    X /= 255\n    \n    y_train, new_dict = emotion_count(data.emotion, classes)\n    y_train = to_categorical(y_train)\n    return X, y_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"700e002aa256e4fb1fee170d13bcab0916bc1388"},"cell_type":"code","source":"## All three datasets are well loaded accordingly\nemotion = {'Angry': 0, 'Disgust': 1, 'Fear': 2, 'Happy': 3,\n           'Sad': 4, 'Surprise': 5, 'Neutral': 6}\nemo     = ['Angry', 'Disgust', 'Fear', 'Happy',\n           'Sad', 'Surprise', 'Neutral']\n\nX_test, y_test = load_data(sample_split=1.0,classes=emo,\nusage='PrivateTest')\n\nX_train, y_train = load_data(sample_split=1.0,classes=emo,\nusage= 'Training')\n\nX_val,y_val = load_data(sample_split=1.0,classes=emo,\nusage= 'PublicTest')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f88beeee9c255cc37032b0830e64ff80a361ab58"},"cell_type":"code","source":"## The shape of loaded data is under examination\nprint(X_train.shape)\nprint(y_train.shape)\nprint(X_test.shape)\nprint(y_test.shape)\nprint(X_val.shape)\nprint(y_val.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9ea7e94a50c677d959ac580ae362214d99384672"},"cell_type":"markdown","source":"After successfully loading the training, validation and testing dataset, we know had an input with dimension 48*48*1, one channel is used since we only have greyscale picture instead of colorful pictures stored in RGB format, for output, we had 6 labels. Fot training dataset, we had a sample size as large as 28709 while validation as well as test dataset own a sample size of 3589. Overall, we had a pretty large dataset."},{"metadata":{"trusted":true,"_uuid":"be32aab35c8ff2e79fc5fe3c33b2912647736001"},"cell_type":"code","source":"def save_data(X_test, y_test, fname=''):\n    \"\"\"\n    The function stores loaded data into numpy form for further processing\n    \"\"\"\n    np.save( 'X_test' + fname, X_test)\n    np.save( 'y_test' + fname, y_test)\nsave_data(X_test, y_test,\"_privatetest6_100pct\")\nX_fname = 'X_test_privatetest6_100pct.npy'\ny_fname = 'y_test_privatetest6_100pct.npy'\nX = np.load(X_fname)\ny = np.load(y_fname)\nprint ('Private test set')\ny_labels = [np.argmax(lst) for lst in y]\ncounts = np.bincount(y_labels)\nlabels = ['angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral']\nprint (zip(labels, counts))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"82a99399d1e6b10a78bbe443252d7fab9384f0d2"},"cell_type":"code","source":"def overview(start, end, X):\n    \"\"\"\n    The function is used to plot first several pictures for overviewing inputs format\n    \"\"\"\n    fig = plt.figure(figsize=(20,20))\n    for i in range(start, end+1):\n        input_img = X[i:(i+1),:,:,:]\n        ax = fig.add_subplot(16,12,i+1)\n        ax.imshow(input_img[0,:,:,0], cmap=plt.cm.gray)\n        plt.xticks(np.array([]))\n        plt.yticks(np.array([]))\n        plt.tight_layout()\n    plt.show()\noverview(0,191, X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8b57e4d29cfad7e1a9811484e7fc5eff2e5f6492"},"cell_type":"code","source":"## Similarly we canvisualize any input with self-defined index with following code\ninput_img = X[6:7,:,:,:] \nprint (input_img.shape)\nplt.imshow(input_img[0,:,:,0], cmap='gray')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f00aa466f3635b175b10d9271e4e6bc3eca73160"},"cell_type":"code","source":"y_train = y_train \ny_public = y_val \ny_private = y_test \ny_train_labels  = [np.argmax(lst) for lst in y_train]\ny_public_labels = [np.argmax(lst) for lst in y_public]\ny_private_labels = [np.argmax(lst) for lst in y_private]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"91732507e04a0b95af5a27854dc5b65a412a10b4"},"cell_type":"code","source":"def plot_distribution(y1, y2, data_names, ylims =[1000,1000]): \n    \"\"\"\n    The function is used to plot the distribution of the labels of provided dataset \n    \"\"\"\n    colorset = brewer2mpl.get_map('Set3', 'qualitative', 6).mpl_colors\n    fig = plt.figure(figsize=(8,4))\n    ax1 = fig.add_subplot(1,2,1)\n    ax1.bar(np.arange(1,8), np.bincount(y1), color=colorset, alpha=0.8)\n    ax1.set_xticks(np.arange(1.25,8.25,1))\n    ax1.set_xticklabels(labels, rotation=60, fontsize=14)\n    ax1.set_xlim([0, 9])\n    ax1.set_ylim([0, ylims[0]])\n    ax1.set_title(data_names[0])\n    \n    ax2 = fig.add_subplot(1,2,2)\n    ax2.bar(np.arange(1,8), np.bincount(y2), color=colorset, alpha=0.8)\n    ax2.set_xticks(np.arange(1.25,8.24,1))\n    ax2.set_xticklabels(labels, rotation=60, fontsize=14)\n    ax2.set_xlim([0, 9])\n    ax2.set_ylim([0, ylims[1]])\n    ax2.set_title(data_names[1])\n    plt.tight_layout()\n    plt.show()\n    \nplot_distribution(y_train_labels, y_public_labels, \\\n                  ['Train dataset', 'Public dataset'], \\\n                  ylims =[8000,1000]) ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"59492d5fe8e2fcb1394bdafeb6fba51b06865b9f"},"cell_type":"markdown","source":"# 2. Model Architecture"},{"metadata":{"_uuid":"a1f6d214b28145f2243ea78976d310131da27e7e"},"cell_type":"markdown","source":"Here, we first approached this project by specifying out clearly our primary purpose--classify provided greyscale pictures into one out of six labels. The picture classification task natually leads us to well-known Convolutional Neural Network (CNN). After researching around how CNN usually performs for greyscale picture, we decided to start from three sequential convolutional layers followed by a maxpooling layer, common activation function for convolutional layer \"relu\" is used, as well as the same padding pattern. Then, from the basic structure, we further added more and more convolutional layers with different features captured. The features to be captured from convolutional layer increased from 32 to 128, it is suggested that such hierarchical structure (with increasing layer nodes) performs better for deep neural network. Finally, the convolved layer is first flattened and then go through two more dense layers to reach the output layer in which softmax activation function is used for multiclass classification (six classes in total)."},{"metadata":{},"cell_type":"markdown","source":"# Я немного изменил архитектуру с целью ускорить обучение"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Final Model Architecture:\nfrom keras import layers\nfrom keras import models\nfrom keras import optimizers\n\nmodelN = models.Sequential()\nmodelN.add(layers.Conv2D(32, (3, 3), padding='same', activation='relu',\n                        input_shape=(48, 48, 1)))\nmodelN.add(layers.Conv2D(32, (1, 1), padding='same', activation='relu'))\nmodelN.add(layers.Conv2D(32, (3, 3), padding='same', activation='relu', strides=(2,2)))\n\nmodelN.add(layers.Conv2D(64, (3, 3), padding='same', activation='relu'))\nmodelN.add(layers.Conv2D(64, (1, 1), padding='same', activation='relu'))\nmodelN.add(layers.Conv2D(64, (3, 3), padding='same', activation='relu', strides=(2,2)))\n\nmodelN.add(layers.Conv2D(128, (3, 3), padding='same', activation='relu'))\nmodelN.add(layers.Conv2D(128, (1, 1), padding='same', activation='relu'))\nmodelN.add(layers.Conv2D(128, (3, 3), padding='same', activation='relu', strides=(2,2)))\n\nmodelN.add(layers.Flatten())  # this converts our 3D feature maps to 1D feature vectors\nmodelN.add(layers.Dense(32, activation='relu'))\nmodelN.add(layers.Dense(7, activation='softmax'))\n\n# optimizer:\nmodelN.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"modelN.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print ('Training....')\n\n\n#fit\nnb_epoch = 10\nbatch_size = 128\n\nmodelF = modelN.fit(X_train, y_train, nb_epoch=nb_epoch, batch_size=batch_size,\n          validation_data=(X_val, y_val), shuffle=True, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a3f9d356c7dab34e061ed59eadf0becedefb2d9c"},"cell_type":"markdown","source":"# 3. Model Evaluation"},{"metadata":{"trusted":true,"_uuid":"3cfbed47d93c3d7b122da2512f2a103c0b131896"},"cell_type":"code","source":"modelN.save('facial_1')\n\nacc = modelF.history['acc']\nval_acc = modelF.history['val_acc']\nloss = modelF.history['loss']\nval_loss = modelF.history['val_loss']\n\nepochs = range(len(acc))\n\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\n\nplt.figure()\n\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dcc8b3360ab5bfcc58afd703fd36c2301bfdbbaf"},"cell_type":"code","source":"# evaluate model on private test set\nscore = modelN.evaluate(X, y, verbose=0)\nprint (\"model %s: %.2f%%\" % (modelN.metrics_names[1], score[1]*100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2c781f97af9f9423dae0597f62c670fee1778340"},"cell_type":"code","source":"# prediction and true labels\ny_prob = modelN.predict(X, batch_size=32, verbose=0)\ny_pred = [np.argmax(prob) for prob in y_prob]\ny_true = [np.argmax(true) for true in y]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a9de75634d1b624287cb40aedf0fda29e62429e1"},"cell_type":"code","source":"def plot_subjects(start, end, y_pred, y_true, title=False):\n    \"\"\"\n    The function is used to plot the picture subjects\n    \"\"\"\n    fig = plt.figure(figsize=(12,12))\n    emotion = {0:'Angry', 1:'Disgust', 2:'Fear', 3:'Happy', 4:'Sad', 5:'Surprise', 6:'Neutral'}\n    for i in range(start, end+1):\n        input_img = X[i:(i+1),:,:,:]\n        ax = fig.add_subplot(7,7,i+1)\n        ax.imshow(input_img[0,:,:,0], cmap=matplotlib.cm.gray)\n        plt.xticks(np.array([]))\n        plt.yticks(np.array([]))\n        if y_pred[i] != y_true[i]:\n            plt.xlabel(emotion[y_true[i]], color='#53b3cb',fontsize=12)\n        else:\n            plt.xlabel(emotion[y_true[i]], fontsize=12)\n        if title:\n            plt.title(emotion[y_pred[i]], color='blue')\n        plt.tight_layout()\n    plt.show();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bfb2144f2346f9a3b3c23ec70b7c3ae65b673d0c"},"cell_type":"code","source":"import brewer2mpl\ndef plot_probs(start,end, y_prob):\n    \"\"\"\n    The function is used to plot the probability in histogram for six labels \n    \"\"\"\n    fig = plt.figure(figsize=(12,12))\n    for i in range(start, end+1):\n        input_img = X[i:(i+1),:,:,:]\n        ax = fig.add_subplot(7,7,i+1)\n        set3 = brewer2mpl.get_map('Set3', 'qualitative', 7).mpl_colors\n        ax.bar(np.arange(0,7), y_prob[i], color=set3,alpha=0.5)\n        ax.set_xticks(np.arange(0.5,7.5,1))\n        labels = ['angry', 'disgust', 'fear', 'happy', 'sad', 'surprise','neutral']\n        ax.set_xticklabels(labels, rotation=90, fontsize=10)\n        ax.set_yticks(np.arange(0.0,1.1,0.5))\n        plt.tight_layout()\n    plt.show();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"72a52168fadedbe946dda86bad1038eb11f67ff3"},"cell_type":"code","source":"def plot_subjects_with_probs(start, end, y_prob):\n    \"\"\"\n    This plotting function is used to plot the probability together with its picture\n    \"\"\"\n    iter = int((end - start)/7)\n    for i in np.arange(0,iter):\n        plot_subjects(i*7,(i+1)*7-1, y_pred, y_true, title=False)\n        plot_probs(i*7,(i+1)*7-1, y_prob)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"5af552ad658d8ea6af1f058f42d7a5906ff3b008"},"cell_type":"code","source":"plot_subjects_with_probs(0, 49, y_prob)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f2f19db1af164738c6768c5e535d2489959de012"},"cell_type":"code","source":"def plot_distribution2(y_true, y_pred):\n    \"\"\"\n    The function is used to compare the number of true labels as well as prediction results\n    \"\"\"\n    colorset = brewer2mpl.get_map('Set3', 'qualitative', 7).mpl_colors\n    ind = np.arange(1.5,8,1)  # the x locations for the groups\n    width = 0.35   \n    fig, ax = plt.subplots()\n    true = ax.bar(ind, np.bincount(y_true), width, color=colorset, alpha=1.0)\n    pred = ax.bar(ind + width, np.bincount(y_pred), width, color=colorset, alpha=0.3)\n    ax.set_xticks(np.arange(1.5,8,1))\n    ax.set_xticklabels(labels, rotation=30, fontsize=14)\n    ax.set_xlim([1.25, 8.5])\n    ax.set_ylim([0, 1000])\n    ax.set_title('True and Predicted Label Count (Private)')\n    plt.tight_layout()\n    plt.show()\n    \nplot_distribution2(y_true, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1c15d3c4bee850eea10a57712bd9278a88a0b04d"},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\n\ndef plot_confusion_matrix(y_true, y_pred, cmap=plt.cm.Blues):\n    \"\"\"\n    The function is used to construct the confusion matrix \n    \"\"\"\n    cm = confusion_matrix(y_true, y_pred)\n    fig = plt.figure(figsize=(7,7))\n    matplotlib.rcParams.update({'font.size': 16})\n    ax  = fig.add_subplot(111)\n    matrix = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n    fig.colorbar(matrix) \n    for i in range(0,7):\n        for j in range(0,7):  \n            ax.text(j,i,cm[i,j],va='center', ha='center')\n    ticks = np.arange(len(labels))\n    ax.set_xticks(ticks)\n    ax.set_xticklabels(labels, rotation=45)\n    ax.set_yticks(ticks)\n    ax.set_yticklabels(labels)\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n\nplot_confusion_matrix(y_true, y_pred, cmap=plt.cm.YlGnBu)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a0f3ff4a37ca08158ef31c795c34fdbb0321a09e"},"cell_type":"markdown","source":"From above, we have performed several visulization methods to evaluate our trained CNN. First, we plotted the training accuracy/loss versus validation accuracy/loss, from the above plot, we can easily find out that our model actually starts to overfit around 10 epoches, at which validation accuracy reaches its plateau, only oscillated around a stable line while validation loss started to increase. Therefore, we take the validation accuracy 58.62% at epoch 9 as the reported validation accuracy and the validation loss will be around 1.1736. We all know that when predicting one out of six labels randomly, we will have a baseline accuracy around 1/6=16.67%, the validation accuracy as high as 58.62% is satisfactory enough. Therefore, we decided to further examine how our model performs on test dataset.\n\nFirst, we evaluated our trained CNN on test dataset, the returned test accuracy is still as high as 55.25%. Second, we attempted to plot the input together with its corresponding true label as well as the softmax output (in which a probability will be outputed for each label) in the same plot for test dataset (first 36 pictures are choosen). When the true label is equal to the class with highest probability, we will mark the true label with Black color, otherwise Blue color indicated a wrong classification. From the first 36 outputs, we can easily find out that the softmax output can be pure (Only one class owns a probability larger than 0.9) or relatively impure (two or three classes shares the probability). Some of the pictures are even completely misclassified, for example, object 26 is labled as happy but misclassified by our neural network as angry with nearly 100% certainty. However, the picture can also be perceived easily as angry even by human. Therefore, facial expressions are indeed very difficult to classify due to complicated muscle movements. Further, we decide to display the prediction results with a plot parallelly showing true label counts and prediction label counts side by side. It is noteworthy from the plot that overall, the true counts are very similar to prediction counts. However, considering the test accuracy 55.25% we obtained, there should be some labels completely and easily classified to another label. Finally,the result can be revealed by the confusion matrix. It suggested the label which is most successfully classified is happy, that makes perfect sense given happy data accounts for the main percentage in training data. The label-pair that can cause misclassification will be neutral-sad as well as sad-fear. Based on human perception, we believe the result is pretty reasonable. "},{"metadata":{"trusted":true,"_uuid":"dbc45289233774ea5a4f9a81456d1da0e390b1af"},"cell_type":"code","source":"# Final Model Architecture:\nfrom keras import layers\nfrom keras import models\nfrom keras import optimizers\n\nmodelR = models.Sequential()\nmodelR.add(layers.Conv2D(32, (3, 3), padding='same', activation='relu',\n                        input_shape=(48, 48, 1)))\nmodelR.add(layers.Conv2D(32, (1, 1), padding='same', activation='relu'))\nmodelR.add(layers.Conv2D(32, (3, 3), padding='same', activation='relu', strides=(2,2)))\n\nmodelR.add(layers.Conv2D(64, (3, 3), padding='same', activation='relu'))\nmodelR.add(layers.Conv2D(64, (1, 1), padding='same', activation='relu'))\nmodelR.add(layers.Conv2D(64, (3, 3), padding='same', activation='relu', strides=(2,2)))\n\nmodelR.add(layers.Conv2D(128, (3, 3), padding='same', activation='relu'))\nmodelR.add(layers.Conv2D(128, (1, 1), padding='same', activation='relu'))\nmodelR.add(layers.Conv2D(128, (3, 3), padding='same', activation='relu', strides=(2,2)))\n\nmodelR.add(layers.Flatten())  # this converts our 3D feature maps to 1D feature vectors\nmodelR.add(layers.Dense(32, activation='relu'))\nmodelR.add(layers.Dense(1, activation='linear'))\n\n# optimizer:\nmodelR.compile(loss='mse', optimizer='adam', metrics=['MSE'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_reg = y_train.argmax(axis=1)\ny_val_reg = y_val.argmax(axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print ('Training....')\n\n\n#fit\nnb_epoch = 20\nbatch_size = 128\n\nmodelF = modelR.fit(X_train, y_train_reg, nb_epoch=nb_epoch, batch_size=batch_size,\n          validation_data=(X_val, y_val_reg), shuffle=True, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}