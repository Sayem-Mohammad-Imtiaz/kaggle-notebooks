{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data=pd.read_csv('../input/amazon-music-reviews/Musical_instruments_reviews.csv')\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Total ratings per rating:','\\n',data.overall.value_counts())\n\n#Number of unique instrument ids\nprint('Number of unique instruments:',len(data.asin.unique()))\nprint('Number of rows:',data.shape[0])      ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#combine text and summary columns\ndata['reviews'] = data['reviewText'] + ' ' + data['summary']\ndel data['reviewText'] \ndel data['summary']\n\n#rename overall to rating\ndata.rename(columns={'overall':'rating'},inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['reviews'].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#drop rows with missing reviews\ndata.dropna(axis=0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Visualize Data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#look at top 20 instrument reviews\n#and bottom 20 instrument reviews\n\ntop_20=data.asin.value_counts().head(20)\nbtm_20 = data.asin.value_counts().tail(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create pivot table to plot\ntop_20_df=pd.DataFrame()\ntop_20_ids=list(top_20.index)\nfor i in top_20_ids:\n    top_20_df=top_20_df.append(data[data['asin']==i],ignore_index=True)\ntable = pd.pivot_table(top_20_df, values='rating',index=top_20_df['asin'],aggfunc=np.mean)\n\n#Create Figure\nplt.figure(figsize=(10,6))\nsns.barplot(x=table.index, y='rating', data=table)\nplt.xticks(rotation=90)\nplt.xlabel('Instrument ID')\nplt.ylabel('Average Rating')\nplt.title('Instruments with the Highest Number of Ratings (Top 20)')\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plot ave rating for 20 Instruments with lower number of ratings\nbtm_20_df=pd.DataFrame()\nbtm_20_ids=list(btm_20.index)\nfor i in btm_20_ids:\n    btm_20_df=btm_20_df.append(data[data['asin']==i],ignore_index=True)\ntable_btm = pd.pivot_table(btm_20_df, values='rating',index=btm_20_df['asin'],aggfunc=np.mean)\n\nplt.figure(figsize=(10,6))\nsns.barplot(x=table_btm.index, y='rating', data=table_btm)\nplt.xticks(rotation=90)\nplt.xlabel('Instrument ID')\nplt.ylabel('Average Rating for Instrument')\nplt.title('Instruments with Fewest Number of Ratings (Bottom 20)')\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plot ratings percentages\nt=pd.DataFrame(data=data['rating'].value_counts(normalize=True)*100)\nplt.figure(figsize=(10,6))\nsns.barplot(x=t.index, y=t.rating,palette=\"Blues_d\")\nplt.xlabel('Rating',fontsize=20)\nplt.ylabel('Percent of Total Ratings',fontsize=20)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are very few low ratings. The majority of ratings are 5 stars","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Process Data for Training","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#drop columns not using for analysis\ncol_to_drop=['reviewerID','asin','reviewerName','helpful','unixReviewTime','reviewTime']\ninstrument_reviews=data.drop(columns=col_to_drop, axis=1)\ninstrument_reviews.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create sentiment column \ninstrument_reviews['sentiment'] = instrument_reviews['rating'].map({5:2,4:2,3:1,2:0,1:0})   \ninstrument_reviews.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"instrument_reviews.sentiment.value_counts(normalize=True)*100","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are 88% positive reviews and only 4.5% with negative reviews.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Data to plot\nlabels=['Positive','Neutral','Negative']\nsizes = [instrument_reviews['sentiment'].value_counts(normalize=True)]\nlabels_rating = ['5','4','3','2','1']\nsizes_rating = [instrument_reviews['rating'].value_counts(normalize=True)]\n#colors = ['olive','lightcoral']\n#colors_rating = ['blue','cyan', 'purple','gray']\ncolors_rating=sns.color_palette(\"BuGn_r\")\ncolors=sns.color_palette(\"PuRd\")\nexplode = (0.1,0.1,0.1) \nexplode_ratings = (0.1,0.1,0.1,0.1,0.1)\n\n# Plot\nplt.pie(sizes, labels=labels, colors=colors, startangle=90,frame=True,explode=explode)\nplt.pie(sizes_rating,labels=labels_rating,colors=colors_rating,radius=0.75,startangle=90,explode=explode_ratings)\ncentre_circle = plt.Circle((0,0),0.5,color='black', fc='white',linewidth=0)\nfig = plt.gcf()\nfig.gca().add_artist(centre_circle)\n \nplt.axis('equal')\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,6))\ncolors=['olive','yellow','lightcoral']\n \nplt.pie(instrument_reviews['sentiment'].value_counts(normalize=True),colors=colors,labels=['Positive','Neutral','Negative'],autopct='%1.2f%%',shadow=True)\nplt.title('Sentiment',fontsize=20)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" ### Process Data for Training and Explore Words","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.base import TransformerMixin\nfrom sklearn.pipeline import Pipeline\n\n#import models to test\nfrom sklearn.svm import LinearSVC\nfrom sklearn.linear_model import LogisticRegression\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import MultinomialNB\n\n#import metrics\nfrom sklearn.metrics import accuracy_score, classification_report\n\nimport string\nimport re\nimport spacy\nspacy.load('en')\nfrom spacy.lang.en import English\nparser = English()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Splitting into train and valid\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_valid, y_train, y_valid = train_test_split(instrument_reviews['reviews'], instrument_reviews['sentiment'], random_state=42, test_size=0.3)\nprint('Training Data Shape:', X_train.shape)\nprint('Testing Data Shape:', X_valid.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Define functions to clean and tokenize data\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\nSTOPLIST = set(stopwords.words('english') + list(ENGLISH_STOP_WORDS))\nSYMBOLS = \" \".join(string.punctuation).split(\" \") + [\"-\", \"...\", \"”\", \"”\",\".\",\"\"]\n\nclass CleanTextTransformer(TransformerMixin):\n   def transform(self, X, **transform_params):\n        return [cleanText(text) for text in X]\n   def fit(self, X, y=None, **fit_params):\n        return self\ndef get_params(self, deep=True):\n        return {}\n    \ndef cleanText(text):    \n    text = text.strip().replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n    text = text.lower()\n    return text\n\ndef tokenizeText(sample):\n    tokens = parser(sample)\n    lemmas = []\n    for tok in tokens:\n        lemmas.append(tok.lemma_.lower().strip() if tok.lemma_ != \"-PRON-\" else tok.lower_)\n    tokens = lemmas\n    tokens = [tok for tok in tokens if tok not in STOPLIST]\n    tokens = [tok for tok in tokens if tok not in SYMBOLS]\n\n    return tokens","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create string of Positive, Neutral, Negative words for Wordcloud\npos = X_train[y_train[y_train == 2].index]\nneut = X_train[y_train[y_train == 1].index]\nneg = X_train[y_train[y_train == 0].index]\n\nX_train.shape,pos.shape,neut.shape,neg.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create text for wordcloud for each sentiment\npos_words=''\nfor w in pos.apply(cleanText).apply(tokenizeText):\n     pos_words+=\" \".join(w)\nprint('There are {} positive words'.format(len(pos_words)))\n\nneut_words=''\nfor w in neut.apply(cleanText).apply(tokenizeText):\n     neut_words+=\" \".join(w)\nprint('There are {} neutral words'.format(len(neut_words)))        \n\nneg_words=''\nfor w in neg.apply(cleanText).apply(tokenizeText):\n     neg_words+=\" \".join(w)\nprint('There are {} negative words'.format(len(neg_words)))        \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Negative wordcloud\nfrom wordcloud import WordCloud\nplt.figure(figsize = (16,16)) \nwc = WordCloud(min_font_size = 3,  max_words = 3000 , width = 1600 , height = 800).generate(neg_words)\nplt.imshow(wc,interpolation = 'bilinear')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Neutral wordcloud\nplt.figure(figsize = (16,16)) \nwc = WordCloud(min_font_size = 3,  max_words = 3000 , width = 1600 , height = 800).generate(neut_words)\nplt.imshow(wc,interpolation = 'bilinear')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Positive wordcloud\nplt.figure(figsize = (16,16)) \nwc = WordCloud(min_font_size = 3,  max_words = 3000 , width = 1600 , height = 800).generate(pos_words)\nplt.imshow(wc,interpolation = 'bilinear')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Set Up Initial Models to Test","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Models to test\nclf = LinearSVC(max_iter=10000)\nxgb = XGBClassifier(n_estimators = 100, learning_rate=0.1)\nrfc = RandomForestClassifier(n_estimators=100)\nlr = LogisticRegression(max_iter=500)\nmnb = MultinomialNB()\n\nmodels = [clf, xgb, rfc, lr, mnb]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# def printNMostInformative(vectorizer, clf, N):\n#     feature_names = vectorizer.get_feature_names()\n#     coefs_with_fns = sorted(zip(clf.coef_[0], feature_names))\n#     topNeg = coefs_with_fns[:N]\n#     topPos = coefs_with_fns[:-(N + 1):-1]\n#     print(\"Negative best: \")\n#     for feat in topNeg:\n#         print(feat)\n#     print(\"Positive best: \")\n#     for feat in topPos:\n#         print(feat)\n# print(\"Top 10 features used to predict: \")        \n# printNMostInformative(vectorizer, clf, 10)\n# pipe = Pipeline([('cleanText', CleanTextTransformer()), ('vectorizer', vectorizer)])\n# transform = pipe.fit_transform(X_train, y_train)\n# vocab = vectorizer.get_feature_names()\n# for i in range(len(X_train)):\n#     s = \"\"\n#     indexIntoVocab = transform.indices[transform.indptr[i]:transform.indptr[i+1]]\n#     numOccurences = transform.data[transform.indptr[i]:transform.indptr[i+1]]\n#     for idx, num in zip(indexIntoVocab, numOccurences):\n#         s += str((vocab[idx], num))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create loop to get accuracy and classification report for models\nfor model in models:\n    \n    vectorizer = CountVectorizer(tokenizer=tokenizeText, ngram_range=(1,1))\n    pipe = Pipeline([('cleanText', CleanTextTransformer()), ('vectorizer', vectorizer), ('model', model)])\n    pipe.fit(X_train, y_train)\n    preds = pipe.predict(X_valid)\n    print('model:',model,'\\t',\"accuracy:\", accuracy_score(y_valid, preds))\n    print('Classification Report','\\n',50*'-','\\n',classification_report(y_valid, preds),'\\n',50*'-')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The initial models all gace decent accuracy.  XGBClassifier had the highest accuracy of 88.7% and SVC with the lowest at 85.5%.  Next we will optimize the model to see if accuracy can be improved.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Optimize model parameters","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model = XGBClassifier(subsample=0.8)\n\nvectorizer = CountVectorizer(tokenizer=tokenizeText, ngram_range=(1,1))\npipe = Pipeline([('cleanText', CleanTextTransformer()), ('vectorizer', vectorizer), ('model', model)])\n\nparam_grid = { \n    'model__n_estimators': range(100,1001,300),\n    'model__max_depth': [8],\n    'model__learning_rate':[0.1]}\nfrom sklearn.model_selection import GridSearchCV\nCV = GridSearchCV(pipe, param_grid, n_jobs= 1)\n                  \nCV.fit(X_train, y_train)  \nprint(CV.best_params_)    \nprint(CV.best_score_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = XGBClassifier(subsample=0.8)\n\nvectorizer = CountVectorizer(tokenizer=tokenizeText, ngram_range=(1,1))\npipe = Pipeline([('cleanText', CleanTextTransformer()), ('vectorizer', vectorizer), ('model', model)])\n\nparam_grid = { \n    'model__n_estimators': range(100,1001,300),\n    'model__max_depth': [5],\n    'model__learning_rate':[0.1]}\nfrom sklearn.model_selection import GridSearchCV\nCV = GridSearchCV(pipe, param_grid, n_jobs= 1)\n                  \nCV.fit(X_train, y_train)  \nprint(CV.best_params_)    \nprint(CV.best_score_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = XGBClassifier(subsample=0.8)\n\nvectorizer = CountVectorizer(tokenizer=tokenizeText, ngram_range=(1,1))\npipe = Pipeline([('cleanText', CleanTextTransformer()), ('vectorizer', vectorizer), ('model', model)])\n\nparam_grid = { \n    'model__n_estimators': [700],\n    'model__max_depth': [5,8],\n    'model__learning_rate':[0.1,0.2]}\nfrom sklearn.model_selection import GridSearchCV\nCV = GridSearchCV(pipe, param_grid, n_jobs= 1)\n                  \nCV.fit(X_train, y_train)  \nprint(CV.best_params_)    \nprint(CV.best_score_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = MultinomialNB()\n\nvectorizer = CountVectorizer(tokenizer=tokenizeText, ngram_range=(1,1))\npipe = Pipeline([('cleanText', CleanTextTransformer()), ('vectorizer', vectorizer), ('model', model)])\n\nparam_grid = { \n    'model__alpha': np.linspace(0.5,1.6,6),\n    'model__fit_prior': [True, False]}\nfrom sklearn.model_selection import GridSearchCV\nCV = GridSearchCV(pipe, param_grid, n_jobs= 1)\n                  \nCV.fit(X_train, y_train)  \nprint(CV.best_params_)    \nprint(CV.best_score_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = RandomForestClassifier()\n\nvectorizer = CountVectorizer(tokenizer=tokenizeText, ngram_range=(1,1))\npipe = Pipeline([('cleanText', CleanTextTransformer()), ('vectorizer', vectorizer), ('model', model)])\n\nparam_grid = { \n    'model__n_estimators': [100],\n    'model__max_features': ['auto'],\n    'model__max_depth' : [2,3,4],\n    'model__criterion' :['gini'],\n    'model__class_weight': [None]}\nfrom sklearn.model_selection import GridSearchCV\nCV = GridSearchCV(pipe, param_grid, n_jobs= 1)\n                  \nCV.fit(X_train, y_train)  \nprint(CV.best_params_)    \nprint(CV.best_score_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = LogisticRegression(max_iter=500)\nvectorizer = CountVectorizer(tokenizer=tokenizeText, ngram_range=(1,1))\npipe = Pipeline([('cleanText', CleanTextTransformer()), ('vectorizer', vectorizer), ('model', model)])\n# pipe.fit(X_train, y_train)\n# preds = pipe.predict(X_valid)\n# print(\"accuracy:\", accuracy_score(y_valid, preds))\n# print('Classification Report','\\n',50*'-','\\n',classification_report(y_valid, preds),'\\n',50*'-')\n\n#\nparam_grid = {'model__C': (0.01,0.1,1),'model__class_weight': [None,'balanced']}\nfrom sklearn.model_selection import GridSearchCV\nCV = GridSearchCV(pipe, param_grid, n_jobs= 1)\n                  \nCV.fit(X_train, y_train)  \nprint(CV.best_params_)    \nprint(CV.best_score_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Accuracy and Classification Report with Optimized Models","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We will now rerun the optimized models and create reports.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"####optimized models\nlr_opt = LogisticRegression(max_iter=500,C=1,class_weight=None)\nrf_opt = RandomForestClassifier(n_estimators=100, max_features='auto',max_depth=4,class_weight=None)\nxgb_opt = XGBClassifier(subsample=0.8, learning_rate = 0.1, n_estimators = 700, max_depth = 5 )\nmnb_opt = MultinomialNB(alpha = 0.72, fit_prior = False)\nmodels_opt = [lr_opt, rf_opt, xgb_opt, mnb_opt]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#LogisticRegressor\nmodel = lr_opt\nvectorizer = CountVectorizer(tokenizer=tokenizeText, ngram_range=(1,1))\npipe = Pipeline([('cleanText', CleanTextTransformer()), ('vectorizer', vectorizer), ('model', model)])\npipe.fit(X_train, y_train)\npreds = pipe.predict(X_valid)\nlr_opt_acc = accuracy_score(y_valid, preds)\nprint('model:',model,'\\t',\"accuracy:\",lr_opt_acc )\nprint('Classification Report','\\n',50*'-','\\n',classification_report(y_valid, preds),'\\n',50*'-')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#RandomForestClassifier\nmodel = rf_opt\nvectorizer = CountVectorizer(tokenizer=tokenizeText, ngram_range=(1,1))\npipe = Pipeline([('cleanText', CleanTextTransformer()), ('vectorizer', vectorizer), ('model', model)])\npipe.fit(X_train, y_train)\npreds = pipe.predict(X_valid)\nrf_opt_acc = accuracy_score(y_valid, preds)\nprint('model:',model,'\\t',\"accuracy:\",rf_opt_acc )\nprint('Classification Report','\\n',50*'-','\\n',classification_report(y_valid, preds),'\\n',50*'-')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#XGBClassifier\nmodel = xgb_opt\nvectorizer = CountVectorizer(tokenizer=tokenizeText, ngram_range=(1,1))\npipe = Pipeline([('cleanText', CleanTextTransformer()), ('vectorizer', vectorizer), ('model', model)])\npipe.fit(X_train, y_train)\npreds = pipe.predict(X_valid)\nxgb_opt_acc = accuracy_score(y_valid, preds)\nprint('model:',model,'\\t',\"accuracy:\",xgb_opt_acc )\nprint('Classification Report','\\n',50*'-','\\n',classification_report(y_valid, preds),'\\n',50*'-')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#MultinomialNB\nmodel = mnb_opt\nvectorizer = CountVectorizer(tokenizer=tokenizeText, ngram_range=(1,1))\npipe = Pipeline([('cleanText', CleanTextTransformer()), ('vectorizer', vectorizer), ('model', model)])\npipe.fit(X_train, y_train)\npreds = pipe.predict(X_valid)\nmnb_opt_acc = accuracy_score(y_valid, preds)\nprint('model:',model,'\\t',\"accuracy:\",mnb_opt_acc )\nprint('Classification Report','\\n',50*'-','\\n',classification_report(y_valid, preds),'\\n',50*'-')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_name = ['Logistic Regression', 'Random Forest Classifier', 'XGB Classifier', 'Multinomial NB']\naccuracy = [lr_opt_acc*100, rf_opt_acc*100,xgb_opt_acc*100,mnb_opt_acc*100]\n\nplt.figure(figsize=(10,5))\n\nplt.ylabel(\"Test Accuracy %\",fontsize=14)\nplt.xlabel(\"Machine Learning Model\",fontsize=14)\nsns.lineplot(x= model_name, y= accuracy)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"XGBClassifier remain the model with the highest accuracy and it increased from 88.7% to 89.3%","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Filter Text and Test Models\nFiltering out words that appear in most frequent list for all sentiments or words that may not be applicable to sentiment such as 's to see if it improves accuracy.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"##Find top 15 most frequent words in the text for each sentiment\nimport itertools\nimport collections\nd=pos.apply(cleanText).apply(tokenizeText)\n\n# List of all positive words \npos_list = list(itertools.chain(*d))\n\n# Create counter\ncounts_pos = collections.Counter(pos_list)\nprint('Most common positive words:')\nitems, counts = zip(*counts_pos.most_common(15))\npd.Series(counts, index=items)\npd.Series(counts, index=items).to_frame()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d=neut.apply(cleanText).apply(tokenizeText)\n# List of all neutral words\nneut_list = list(itertools.chain(*d))\n\n# Create counter\ncounts_neut = collections.Counter(neut_list)\n\nprint('Most common neutral words:')\nitems, counts = zip(*counts_neut.most_common(15))\npd.Series(counts, index=items)\npd.Series(counts, index=items).to_frame()      ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d=neg.apply(cleanText).apply(tokenizeText)\n# List of all negative words\nneg_list = list(itertools.chain(*d))\n\n# Create counter\ncounts_neg = collections.Counter(neg_list)\n\nprint('Most common negative words:')\nitems, counts = zip(*counts_neg.most_common(15))\npd.Series(counts, index=items)\npd.Series(counts, index=items).to_frame() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Define functions to clean and tokenize data adding list of words to remove\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\nSTOPLIST = set(stopwords.words('english') + list(ENGLISH_STOP_WORDS))\nSYMBOLS = \" \".join(string.punctuation).split(\" \") + [\"-\", \"...\", \"”\", \"”\",\".\",\"\"]\nREM = [\"guitar\",\"string\",\"strings\",\"amp\",\"pedal\",\"\\'s\"]\nclass CleanTextTransformer(TransformerMixin):\n   def transform(self, X, **transform_params):\n        return [cleanText(text) for text in X]\n   def fit(self, X, y=None, **fit_params):\n        return self\ndef get_params(self, deep=True):\n        return {}\n    \ndef cleanText(text):    \n    text = text.strip().replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n    text = text.lower()\n    return text\n\ndef tokenizeText(sample):\n    tokens = parser(sample)\n    lemmas = []\n    for tok in tokens:\n        lemmas.append(tok.lemma_.lower().strip() if tok.lemma_ != \"-PRON-\" else tok.lower_)\n    tokens = lemmas\n    tokens = [tok for tok in tokens if tok not in STOPLIST]\n    tokens = [tok for tok in tokens if tok not in SYMBOLS]\n    tokens = [tok for tok in tokens if tok not in REM]\n    return tokens","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Splitting into train and valid\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_valid, y_train, y_valid = train_test_split(instrument_reviews['reviews'], instrument_reviews['sentiment'], random_state=42, test_size=0.3)\nprint('Training Data Shape:', X_train.shape)\nprint('Testing Data Shape:', X_valid.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Rerunning optimized models with the common words filtered out","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#LogisticRegressor with common words removed\nmodel = lr_opt\nvectorizer = CountVectorizer(tokenizer=tokenizeText, ngram_range=(1,1))\npipe = Pipeline([('cleanText', CleanTextTransformer()), ('vectorizer', vectorizer), ('model', model)])\npipe.fit(X_train, y_train)\npreds = pipe.predict(X_valid)\nlr_opt_acc_filtered = accuracy_score(y_valid, preds)\nprint('model:',model,'\\t',\"accuracy:\",lr_opt_acc_filtered )\nprint('Classification Report','\\n',50*'-','\\n',classification_report(y_valid, preds),'\\n',50*'-')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#RandomForestClassifier\nmodel = rf_opt\nvectorizer = CountVectorizer(tokenizer=tokenizeText, ngram_range=(1,1))\npipe = Pipeline([('cleanText', CleanTextTransformer()), ('vectorizer', vectorizer), ('model', model)])\npipe.fit(X_train, y_train)\npreds = pipe.predict(X_valid)\nrf_opt_acc_filtered = accuracy_score(y_valid, preds)\nprint('model:',model,'\\t',\"accuracy:\",rf_opt_acc_filtered )\nprint('Classification Report','\\n',50*'-','\\n',classification_report(y_valid, preds),'\\n',50*'-')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#XGBClassifier\nmodel = xgb_opt\nvectorizer = CountVectorizer(tokenizer=tokenizeText, ngram_range=(1,1))\npipe = Pipeline([('cleanText', CleanTextTransformer()), ('vectorizer', vectorizer), ('model', model)])\npipe.fit(X_train, y_train)\npreds = pipe.predict(X_valid)\nxgb_opt_acc_filtered = accuracy_score(y_valid, preds)\nprint('model:',model,'\\t',\"accuracy:\",xgb_opt_acc_filtered )\nprint('Classification Report','\\n',50*'-','\\n',classification_report(y_valid, preds),'\\n',50*'-')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#MultinomialNB\nmodel = mnb_opt\nvectorizer = CountVectorizer(tokenizer=tokenizeText, ngram_range=(1,1))\npipe = Pipeline([('cleanText', CleanTextTransformer()), ('vectorizer', vectorizer), ('model', model)])\npipe.fit(X_train, y_train)\npreds = pipe.predict(X_valid)\nmnb_opt_acc_filtered = accuracy_score(y_valid, preds)\nprint('model:',model,'\\t',\"accuracy:\",mnb_opt_acc_filtered )\nprint('Classification Report','\\n',50*'-','\\n',classification_report(y_valid, preds),'\\n',50*'-')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plot of nonfiltered and filtered\nmodel_name = ['Logistic Regression', 'Random Forest Classifier', 'XGB Classifier', 'Multinomial NB']\naccuracy = [lr_opt_acc*100, rf_opt_acc*100,xgb_opt_acc*100,mnb_opt_acc*100]\naccuracy_filtered = [lr_opt_acc_filtered*100, rf_opt_acc_filtered*100,xgb_opt_acc_filtered*100,mnb_opt_acc_filtered*100]\nplt.figure(figsize=(10,5))\n\nplt.ylabel(\"Test Accuracy %\",fontsize=14)\nplt.xlabel(\"Machine Learning Model\",fontsize=14)\nsns.lineplot(x= model_name, y= accuracy,label='Not Filtered')\nsns.lineplot(x= model_name, y= accuracy_filtered,label='Filtered')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"XGB consistently performs better with the highest accuracy of 89.3%.  Filtering out words actually lowers the accuracy of the models.  ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}