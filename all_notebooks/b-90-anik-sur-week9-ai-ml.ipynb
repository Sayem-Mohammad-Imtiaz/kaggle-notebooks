{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.tree import DecisionTreeClassifier\ndata=pd.read_csv(\"../input/apndcts/apndcts.csv\")\npredictors= data.iloc[:,0:7]\ntarget=data.iloc[:,7]\npredictors_train, predictors_test, target_train, target_test=train_test_split(predictors, target, test_size=0.3, random_state=123)\ndtree_entropy = DecisionTreeClassifier (criterion=\"entropy\", random_state=100, max_depth=3, min_samples_leaf=5)\n\nmodel=dtree_entropy.fit(predictors_train, target_train)\nprediction=dtree_entropy.predict(predictors_test)\naccuracy_score(target_test, prediction, normalize=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ****FROM SCRATCH****"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"../input/apndcts/apndcts.csv\")\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def calculate_entropy(df_label):\n    classes,class_counts = np.unique(df_label,return_counts = True)\n    entropy_value = np.sum([(-class_counts[i]/np.sum(class_counts))*np.log2(class_counts[i]/np.sum(class_counts)) \n                        for i in range(len(classes))])\n    return entropy_value","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def calculate_information_gain(dataset,feature,label): \n    # Calculate the dataset entropy\n    dataset_entropy = calculate_entropy(dataset[label])   \n    values,feat_counts= np.unique(dataset[feature],return_counts=True)\n    \n    # Calculate the weighted feature entropy                                # Call the calculate_entropy function\n    weighted_feature_entropy = np.sum([(feat_counts[i]/np.sum(feat_counts))*calculate_entropy(dataset.where(dataset[feature]\n                              ==values[i]).dropna()[label]) for i in range(len(values))])    \n    feature_info_gain = dataset_entropy - weighted_feature_entropy\n    return feature_info_gain","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features = df.columns[:-1]\nlabel = 'class'\nparent=None\nfeatures","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def creat_decision_tree(dataset,df, features, label, parent):\n    datum=np.unique(df[label],return_counts=True)\n    unitque_data=np.unique(dataset[label])\n    if len(unique_data)<=1:\n        return unique_data[0]\n    elif len(dataset)==0:\n        return unique_data[no.argmax(datum[1])]\n    elif len(features)==0:\n        return parent\n    else:\n        parent=unique_data[np.argmax(datum[1])]\n        \n        item_values=[calculate_information_gain(dataset,feature, label) for feature in features]\n        \n        for value in np.unique(dataset[optimum_feature]):\n            min_data=dataset.where(dataset[optimum_feature]==value).dropna()\n            min_tree=create_decision_tree(min_data,df,features,label,parent)\n            \n            decision_tree[optimum_feature][value]=min_tree\n            \n        return(decision_tree)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}