{"cells":[{"metadata":{"id":"EEJIIIj1SO9M"},"cell_type":"markdown","source":"Link to [Colab Notebook](https://colab.research.google.com/drive/17nwWFe478Lc0-xzrMW3lVnwCTCNja5vJ?usp=sharing)\nLink to [GitHub Repo](https://github.com/vidyap-xgboost/DataScience-ML_Projects/blob/master/twitter_data_twint_sweetviz_texthero.ipynb)\n\nPlease don't forget to **Upvote** this notebook if you find it useful and learned something from this! That would really encourage me to keep writing more notebooks. Thank you!\n\n\n# 0. Scraping Data using Twint\n\nLet's collect data from twitter using [twint](https://github.com/twintproject/twint) library.\n\n**Question 1:** Why are we using **twint** instead of **Twitter's Official API**?\n\n**Ans:** Because twint requires no authentication, no API, and importantly no limits.","execution_count":null},{"metadata":{"id":"JyFJPlYIzxhL","_kg_hide-input":true},"cell_type":"markdown","source":"**For more ways to install this library, please visit the above mentioned link.**\n\n```python\n!pip3 install twint\n```","execution_count":null},{"metadata":{"id":"PiJNObAHzpIr","_kg_hide-input":true},"cell_type":"markdown","source":"```python\nimport twint\n\n# Create a function to scrape a user's account.\ndef scrape_user():\n\tprint (\"Fetching Tweets\")\n\tc = twint.Config()\n\t# choose username (optional)\n\tc.Username = input('Username: ') # I used a different account for this project. Changed the username to protect the user's privacy.\n\t# choose beginning time (narrow results)\n\tc.Since = input('Date (format: \"%Y-%m-%d %H:%M:%S\"): ')\n\t# no idea, but makes the csv format properly\n\tc.Store_csv = True\n\t# file name to be saved as\n\tc.Output = input('File name: ')\n\ttwint.run.Search(c)\n```","execution_count":null},{"metadata":{"id":"lef3yvL-zpcq","_kg_hide-input":true},"cell_type":"markdown","source":"```python\n# run the above function\n\nscrape_user()\nprint('Scraping Done!')\n```","execution_count":null},{"metadata":{"id":"E6JiFqx5mNY7"},"cell_type":"markdown","source":"# 1. Reading Data using Pandas","execution_count":null},{"metadata":{"id":"xagnq5j5mDvg","trusted":true},"cell_type":"code","source":"# pandas to read our csv file\nimport pandas as pd","execution_count":null,"outputs":[]},{"metadata":{"id":"ZW2ADo37Q2Mc","trusted":true},"cell_type":"code","source":"# save the csv file into a dataframe 'df'\ndf = pd.read_csv('../input/elon-musk-tweets-2015-to-2020/elonmusk.csv',low_memory=False, parse_dates=[['date', 'time']])","execution_count":null,"outputs":[]},{"metadata":{"id":"RrpSkY-0mcUp","trusted":true},"cell_type":"code","source":"# make a copy if you need so that the changes made in original df doesn't affect the copy\ndf_copy = df.copy(deep=True)","execution_count":null,"outputs":[]},{"metadata":{"id":"TXAbbgVumcbC","outputId":"a7c7239f-73d7-4eb8-93ef-1bef4394988a","trusted":true},"cell_type":"code","source":"# check the whole df\ndisplay(df)\n\n# check an overview of the df\ndisplay(df.info())\n\n# gives out quick analysis, notice the max retweets_count and min retweets_count and so on\ndisplay(df.describe())","execution_count":null,"outputs":[]},{"metadata":{"id":"362mnHpBmcia","trusted":true},"cell_type":"code","source":"# I don't need these columns, so dropping them. You can keep them if you want.\ndrop_list = ['id','conversation_id','created_at','name','timezone','user_id','cashtags','place','quote_url','near','geo','source','user_rt_id','user_rt','retweet_id','retweet_date','translate','trans_src','trans_dest','video','retweet']\ndf = df.drop(columns=drop_list)","execution_count":null,"outputs":[]},{"metadata":{"id":"q3UFjXUGmcp5","outputId":"e0dbe088-bba5-47a3-93a9-633a7928ddd8","_kg_hide-output":true,"trusted":true,"collapsed":true},"cell_type":"code","source":"# have a look again.\ndisplay(df.info())","execution_count":null,"outputs":[]},{"metadata":{"id":"ao5-gPbiTMHT","trusted":true},"cell_type":"code","source":"# just in case texthero cant remove URLs\ndf['tweet'] = df['tweet'].str.replace('http\\S+|www.\\S+', '',case=False)","execution_count":null,"outputs":[]},{"metadata":{"id":"Iu2jT2G_9gjr","outputId":"fea8db26-aa9d-48ba-d4ae-d8a199ffe9e3","_kg_hide-output":true,"trusted":true,"collapsed":true},"cell_type":"code","source":"df","execution_count":null,"outputs":[]},{"metadata":{"id":"c3Odv_02TGdk"},"cell_type":"markdown","source":"# 2. Install and Import TextHero\n\nHere, we will be using [TextHero](https://github.com/jbesomi/texthero), a python package to work efficiently and quickly with text data. You can think of texhero as scikit-learn for text-based dataset.\n\n**Question 2:** Why are we using TextHero instead of doing it from scratch using libraries like Gensim or other tools?\n\n**Ans:** TextHero automates the cleaning process with one method, which is pretty effective. If the text needs to be cleaned further, we can do so by manually writing code to remove those unwanted words. In the backend, it uses libraries like Spacy, Gensim, tqdm,regex, nltk. So you don't have to import all those separately when you use TextHero.","execution_count":null},{"metadata":{"id":"a00s0Px1_6w5","_kg_hide-output":true,"trusted":true,"collapsed":true},"cell_type":"code","source":"# Check the above link for other installation instructions.\n\n!pip install texthero","execution_count":null,"outputs":[]},{"metadata":{"id":"nRQTcQShmJKD","outputId":"70530efc-e10b-46d4-ad86-1b117b647623","trusted":true},"cell_type":"code","source":"# import texthero\n\nimport texthero as hero","execution_count":null,"outputs":[]},{"metadata":{"id":"bJwAEN7ax7hw"},"cell_type":"markdown","source":"# 3. TextHero for quick cleaning of **raw** text data.","execution_count":null},{"metadata":{"id":"0I6QFZNtSudX","trusted":true},"cell_type":"code","source":"# let's do text preprocessing\nfrom texthero import preprocessing\n\n# creating a custom pipeline to preprocess the raw text we have\ncustom_pipeline = [preprocessing.fillna\n                   , preprocessing.lowercase\n                  #  , preprocessing.remove_digits # you can uncomment this if you want to remove digits as well.\n                   , preprocessing.remove_punctuation\n                   , preprocessing.remove_diacritics\n                   , preprocessing.remove_stopwords\n                   , preprocessing.remove_whitespace\n                   , preprocessing.stem]\n\n# simply call clean() method to clean the raw text in 'tweet' col and pass the custom_pipeline to pipeline argument\ndf['clean_tweet'] = hero.clean(df['tweet'], pipeline = custom_pipeline)","execution_count":null,"outputs":[]},{"metadata":{"id":"BlydtIqOxWLr","outputId":"f72044e6-f99e-414a-9f53-93bc72cc116a","trusted":true},"cell_type":"code","source":"df","execution_count":null,"outputs":[]},{"metadata":{"id":"U-NcrEs2yL82"},"cell_type":"markdown","source":"# 4. EDA and basic Visualization with Sweetviz\n\n**Question 3:** Why are we using [Sweetviz](https://github.com/fbdesignpro/sweetviz) instead of matplotlib or plotly or bokeh for Exploratory Data Analysis?\n\n**Ans**: Sweetviz is an open source Python library that generates beautiful, high-density visualizations to kickstart EDA (Exploratory Data Analysis) with a single line of code. Output is a fully self-contained HTML application.\n\nThe system is built around quickly visualizing target values and comparing datasets. Its goal is to help quick analysis of target characteristics, training vs testing data, and other such data characterization tasks.\n\n[See example report generated by sweetviz from the titanic dataset HERE\n](http://cooltiming.com/SWEETVIZ_REPORT.html)\n","execution_count":null},{"metadata":{"id":"4_kVRHbiyd4o","_kg_hide-output":true,"trusted":true,"collapsed":true},"cell_type":"code","source":"# Check the above link for other installation instructions\n\n!pip3 install sweetviz","execution_count":null,"outputs":[]},{"metadata":{"id":"CogjR-WsysuN","trusted":true},"cell_type":"code","source":"# importing sweetviz\nimport sweetviz as sv","execution_count":null,"outputs":[]},{"metadata":{"id":"LutvgegN_v2-","trusted":true},"cell_type":"code","source":"# creating another dataframe df1 for further analysis.\n\ndf1 = df.drop(columns=['date_time'])","execution_count":null,"outputs":[]},{"metadata":{"id":"LedLwD5iys1g","outputId":"b201cdea-d62f-4a79-dea9-815f0aec631f","trusted":true},"cell_type":"code","source":"#to analyze the data and create a report, simply call analyze() method passing in the dataframe as argument\n\nelonmusk_report = sv.analyze(df1)","execution_count":null,"outputs":[]},{"metadata":{"id":"kcBf79MYys7l","trusted":true},"cell_type":"code","source":"#display the report as html\n\nelonmusk_report.show_html('elonmusk.html')","execution_count":null,"outputs":[]},{"metadata":{"id":"EYhy6Hu5c2Po"},"cell_type":"markdown","source":"A lot of information can be analyzed and understood from just one HTML Report before we do any further analysis.\n\nExample Screenshots:\n\n![Correlation Matrix](https://drive.google.com/uc?export=view&id=1-3_ZqGCJ6N_jrzZlt5aXxD5AGa_fyhGJ)\n\n---\n\n![Text Preview](https://drive.google.com/uc?export=view&id=1--gKvrfJ1jXPMn70VPZCk1BByLOUnN4S)\n\n","execution_count":null},{"metadata":{"id":"Ales-ssp6rCa"},"cell_type":"markdown","source":"# 5. Convert timezone UTC to IST using pytz\n\nThis step can be avoided if you wish so, however, I would like to show you how you can convert UTC timezone to your local timezone in case you're doing timeseries analysis.","execution_count":null},{"metadata":{"id":"tuHBUq4YytAH","_kg_hide-output":true,"trusted":true,"collapsed":true},"cell_type":"code","source":"!pip3 install pytz","execution_count":null,"outputs":[]},{"metadata":{"id":"ueBe__QzytDy","trusted":true},"cell_type":"code","source":"from datetime import datetime\nfrom pytz import timezone","execution_count":null,"outputs":[]},{"metadata":{"id":"XJyvTyPI69dB","trusted":true},"cell_type":"code","source":"# In place of 'UTC', replace it with whatever the current timezone is in your df.\n# In place of 'Asia/Kolkata', replace it with whatever timezone you want to convert into.\n\ndf['conv_datetime'] = df['date_time'].dt.tz_localize('UTC').dt.tz_convert('Asia/Kolkata')","execution_count":null,"outputs":[]},{"metadata":{"id":"xzgmzFO_69iD","trusted":true},"cell_type":"code","source":"# I don't need the \"+5.30\" localize information in my df.\n\ndf['datetime'] = df['conv_datetime'].dt.tz_localize(None)","execution_count":null,"outputs":[]},{"metadata":{"id":"R_BfoqXRysxy","trusted":true},"cell_type":"code","source":"# dropping the extra columns and setting the datetime as index.\n\ndf = df.drop(columns=['date_time','conv_datetime'])","execution_count":null,"outputs":[]},{"metadata":{"id":"BjGRSF_-F1pA","trusted":true},"cell_type":"code","source":"df = df.set_index('datetime')","execution_count":null,"outputs":[]},{"metadata":{"id":"lal7MxsKIc_S"},"cell_type":"markdown","source":"# 6. Visualizations using TextHero for further insights\n\nThere are some pretty cool visualizations you can explore with TextHero","execution_count":null},{"metadata":{"id":"vibam80UF1yE","outputId":"08c573f4-e838-4517-9f45-d4a578e19d56","_kg_hide-output":true,"trusted":true,"collapsed":true},"cell_type":"code","source":"df","execution_count":null,"outputs":[]},{"metadata":{"id":"xFHSI4KNZL79","trusted":true},"cell_type":"code","source":"df1 = df.drop(columns=['tweet','username','link'])","execution_count":null,"outputs":[]},{"metadata":{"id":"M4z9fS6BZo4_","outputId":"4116c294-de0f-4262-c196-b41f9b261330","trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# using top_words() method, get the top N words and make a bar plot.\nhero.top_words(df1['clean_tweet']).head(10).plot.bar(figsize=(15,10))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"I27l3QrzZ1tU","trusted":true},"cell_type":"code","source":"# Want to add more stop words to your list? No problem. Follow the below steps.\n\nfrom texthero import stopwords\ndefault_stopwords = stopwords.DEFAULT\n#add a list of stopwords to the stopwords\nstop_w = [\"twitter\",\"pic\",\"com\",\"yes\",\"like\",\"year\",\"need\",\"ok\",\"exact\",\"come soon\",\"yeah\",\n          \"yup\",\"would\",\"much\",\"use\"]\ncustom_stopwords = default_stopwords.union(set(stop_w))\n#Call remove_stopwords and pass the custom_stopwords list\ndf1['clean_tweet'] = hero.remove_stopwords(df1['clean_tweet'], custom_stopwords)","execution_count":null,"outputs":[]},{"metadata":{"id":"AvaWXQM4W_u1","outputId":"f8177767-b1eb-45a8-c4ac-8aa067e3a1e8","trusted":true},"cell_type":"code","source":"# Let's visualize again.\n\nhero.top_words(df1['clean_tweet']).head(10).plot.bar(figsize=(15,10))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"-w-PqD_RbSUz","outputId":"707f4c0e-5be3-40ad-8b64-5cd54d350354","trusted":true},"cell_type":"code","source":"# just checking for any null values\ndf1.clean_tweet.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"id":"nuWr9Y_6TpdF","outputId":"79fd9b27-4f1b-4d57-b647-7e7d6de762f1","trusted":true},"cell_type":"code","source":"# WordCloud with single line of code.\n\nhero.visualization.wordcloud(df1['clean_tweet'],width = 400, height= 400,background_color='White')","execution_count":null,"outputs":[]},{"metadata":{"id":"Plo0brZQdr0j","outputId":"416096f4-4814-4e12-fb98-c74bccec6783","trusted":true},"cell_type":"code","source":"#Add pca value to dataframe to use as visualization coordinates\ndf1['pca'] = (\n            df1['clean_tweet']\n            .pipe(hero.tfidf)\n            .pipe(hero.pca)\n   )\n#Add k-means cluster to dataframe \ndf1['kmeans'] = (\n            df1['clean_tweet']\n            .pipe(hero.tfidf)\n            .pipe(hero.kmeans, n_clusters=5)\n   )\ndf1.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"l1eC-aJEe27h","outputId":"36187098-4e06-41f7-ce96-86248b6e2534","trusted":true},"cell_type":"code","source":"# Generate scatter plot for pca and kmeans. Cool isn't it?\nhero.scatterplot(df1, 'pca', color = 'kmeans', hover_data=['clean_tweet'] )","execution_count":null,"outputs":[]},{"metadata":{"id":"_e1qAnyrTk4z"},"cell_type":"markdown","source":"# 7. Other Visualizations for further analysis","execution_count":null},{"metadata":{"id":"wLf9aPnZVs_w","outputId":"69cc2eae-8a66-4f7a-f7b1-c52c8f832350","_kg_hide-output":true,"trusted":true,"collapsed":true},"cell_type":"code","source":"!pip3 install chart-studio","execution_count":null,"outputs":[]},{"metadata":{"id":"UyuLXsr9UOOz","outputId":"de149a4e-e1a1-4de1-d00d-6441104a8c6b","trusted":true},"cell_type":"code","source":"import seaborn as sns # visualization library\nimport chart_studio.plotly as py # visualization library\nfrom plotly.offline import init_notebook_mode, iplot # plotly offline mode\ninit_notebook_mode(connected=True) \nimport plotly.graph_objs as go # plotly graphical object","execution_count":null,"outputs":[]},{"metadata":{"id":"v39hAZoRWEuU","trusted":true},"cell_type":"code","source":"df2 = df.drop(columns=['username','tweet','link'])","execution_count":null,"outputs":[]},{"metadata":{"id":"Xr4tuH4GWE3C","outputId":"33bb0c5b-15cf-4a70-e197-76d9f83a36c1","trusted":true},"cell_type":"code","source":"df2.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"O3aIWLLyWFCd","outputId":"139c5233-3432-49ec-fc21-78c342cfded9","trusted":true},"cell_type":"code","source":"plt.figure(figsize=(17,10))\nsns.lineplot(data=df2['retweets_count'], dashes=False)\nplt.title(\"Retweets over time\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"XmjXNbUdWFFz","outputId":"fb1533e3-13d7-4856-f850-d939e86361f9","trusted":true},"cell_type":"code","source":"plt.figure(figsize=(17,10))\nsns.lineplot(data=df2['replies_count'], dashes=False)\nplt.title(\"Replies over time\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"h0AWNAgjb3w7","outputId":"7e08467d-c21e-499a-dcb2-b8f278e9545a","trusted":true},"cell_type":"code","source":"plt.figure(figsize=(17,10))\nsns.lineplot(data=df2['likes_count'], dashes=False)\nplt.title(\"Likes over time\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"Y_8JbS-oYlXK"},"cell_type":"markdown","source":"# What else?\n\nYou can check other libraries like [huggingface](https://github.com/huggingface) for NLP, [pendulum](https://github.com/sdispater/pendulum) if you're dealing with dates & time, and [Vaex](https://github.com/vaexio/vaex) if you're dealing with large datasets.","execution_count":null},{"metadata":{"id":"AApFTpmVZwUZ"},"cell_type":"markdown","source":"# Next Steps\n\n- Build a Topic Model and check if you can categorize @elonmusk's tweets into different categories.\n- Sentiment Analysis on his tweets\n- How the sentiment is changing over time\n- Take recent stocks data on TSLA and check if his tweets are influencing the TSLA stock or other stocks.\n\nPlease give a star for this repository if it helped you and raise issues if you find any. Thank you!","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Link to [Colab Notebook](https://colab.research.google.com/drive/17nwWFe478Lc0-xzrMW3lVnwCTCNja5vJ?usp=sharing)\nLink to [GitHub Repo](https://github.com/vidyap-xgboost/DataScience-ML_Projects/blob/master/twitter_data_twint_sweetviz_texthero.ipynb)\n\nPlease don't forget to **Upvote** this notebook if you find it useful and learned something from this! That would really encourage me to keep writing more notebooks. Thank you!","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}