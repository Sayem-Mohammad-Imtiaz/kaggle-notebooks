{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#imports\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport re\nfrom nltk.tokenize import RegexpTokenizer\nimport string \nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.preprocessing import LabelEncoder, RobustScaler\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.model_selection import train_test_split\nfrom collections import Counter\nimport wordcloud\npd.set_option('max_rows',1000000)\npd.set_option('max_columns',10000)\nsns.set(rc={'figure.figsize':(15,10)})\nimport spacy","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/mbti-type/mbti_1.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EDA"},{"metadata":{"trusted":true},"cell_type":"code","source":"gr = df.groupby('type').count()\ngr.sort_values(\"posts\", ascending=False, inplace=True)\ngr","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"lets plot the count."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,10))\ngr['posts'].plot(kind='bar',title=\"Number of Posts per Personality type\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(x=gr.index,y='posts',data=gr,palette='rocket')\nplt.title('Number of Posts per Personality type',fontsize=15)\nplt.xticks(fontsize=15)\nplt.yticks(fontsize=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"lets see the distribution of length of all posts..."},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"LenP\"] = df[\"posts\"].apply(len)\nsns.distplot(df[\"LenP\"]).set_title(\"Distribution of Lengths of all 50 Posts\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"plotting the number of posts per user."},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"NumPosts\"] = df[\"posts\"].apply(lambda x: len(x.split(\"|||\")))\n\nsns.distplot(df[\"NumPosts\"], kde=False).set_title(\"Number of Posts per User\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Split to posts\ndef extract(posts, new_posts):\n    for post in posts[1].split(\"|||\"):\n        new_posts.append((posts[0], post))\n\nposts = []\ndf.apply(lambda x: extract(x, posts), axis=1)\nprint(\"Number of users\", len(df))\nprint(\"Number of posts\", len(posts))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"posts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_df = pd.DataFrame(posts, columns=[\"type\", \"posts\"])\nnew_df.head(100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finding the most common words in all posts."},{"metadata":{"trusted":true},"cell_type":"code","source":"words = list(new_df[\"posts\"].apply(lambda x: x.split()))\nwords = [x for y in words for x in y]\nCounter(words).most_common(40)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"plotting the most common words with WordCloud."},{"metadata":{"trusted":true},"cell_type":"code","source":"wc = wordcloud.WordCloud(width=1200, height=500, \n                         collocations=False, background_color=\"white\", \n                         colormap=\"tab20b\").generate(\" \".join(words))\nplt.figure(figsize=(25,10))\nplt.imshow(wc, interpolation='bilinear')\n_ = plt.axis(\"off\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess_text(df, remove_special=True):\n    #Remove links \n    df[\"posts\"] = df[\"posts\"].apply(lambda x: re.sub(r'https?:\\/\\/.*?[\\s+]', '', x.replace(\"|\",\" \") + \" \"))\n    \n    #Keep EOS\n    df[\"posts\"] = df[\"posts\"].apply(lambda x: re.sub(r'\\.', ' EOSTokenDot ', x + \" \"))\n    df[\"posts\"] = df[\"posts\"].apply(lambda x: re.sub(r'\\?', ' EOSTokenQuest ', x + \" \"))\n    df[\"posts\"] = df[\"posts\"].apply(lambda x: re.sub(r'!', ' EOSTokenExs ', x + \" \"))\n    \n    #Strip Punctation\n    df[\"posts\"] = df[\"posts\"].apply(lambda x: re.sub(r'[^\\w\\s]','',x))\n\n    #Remove Non-words\n    df[\"posts\"] = df[\"posts\"].apply(lambda x: re.sub(r'[^a-zA-Z\\s]','',x))\n\n    #To lower\n    df[\"posts\"] = df[\"posts\"].apply(lambda x: x.lower())\n\n    #Remove multiple letter repating words\n    df[\"posts\"] = df[\"posts\"].apply(lambda x: re.sub(r'([a-z])\\1{2,}[\\s|\\w]*','',x)) \n\n    #Remove short/long words\n    df[\"posts\"] = df[\"posts\"].apply(lambda x: re.sub(r'(\\b\\w{0,3})?\\b','',x)) \n    df[\"posts\"] = df[\"posts\"].apply(lambda x: re.sub(r'(\\b\\w{30,1000})?\\b','',x))\n\n    #Remove Personality Types Words\n    #This is crutial in order to get valid model accuracy estimation for unseen data. \n    if remove_special:\n        pers_types = ['INFP' ,'INFJ', 'INTP', 'INTJ', 'ENTP', 'ENFP', 'ISTP' ,'ISFP' ,'ENTJ', 'ISTJ','ENFJ', 'ISFJ' ,'ESTP', 'ESFP' ,'ESFJ' ,'ESTJ']\n        pers_types = [p.lower() for p in pers_types]\n        p = re.compile(\"(\" + \"|\".join(pers_types) + \")\")\n\n    df[\"posts\"] = df[\"posts\"].apply(lambda x: p.sub(' PTypeToken ',x))\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Preprocess Text\n#new_df = preprocess_text(new_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# MODELLING"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Remove posts with less than X words\nmin_words = 15\nprint(\"Number of posts\", len(new_df)) \nnew_df[\"nw\"] = new_df[\"posts\"].apply(lambda x: len(re.findall(r'\\w+', x)))\nnew_df = new_df[new_df[\"nw\"] >= min_words]\nprint(\"Number of posts\", len(new_df))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"encoding the personality types,"},{"metadata":{"trusted":true},"cell_type":"code","source":"enc = LabelEncoder()\nnew_df['type_enc'] = enc.fit_transform(new_df['type'])\ntarget = new_df['type_enc']\ntarget.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_df.head(100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nlp = spacy.load('en_core_web_lg')\n#train_vector = np.array([nlp(text).vector for text in new_df.posts])\n#print(train_vector.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"vectorizing the posts for the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"vect = CountVectorizer(stop_words='english') \ntrain =  vect.fit_transform(new_df[\"posts\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Splitting the data into train and test sets."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(train, target, test_size=0.1, stratify=target, random_state=42)\nprint(X_train.shape, X_test.shape, y_train.shape, y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Training the model."},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb = XGBClassifier()\nxgb.fit(X_train,y_train)\naccuracy_score(y_test,xgb.predict(X_test))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}