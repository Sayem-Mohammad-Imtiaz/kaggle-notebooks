{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a class=\"anchor\" id=\"0\"></a>\n# **Rainfall in Australia** \n\n\n<img src=https://thumbs.dreamstime.com/b/trees-whatipu-point-huia-bay-auckland-new-zealand-march-two-tall-green-windswept-shoreline-under-heavy-cloudy-sky-91689726.jpg> \n\n\n\n\n","metadata":{}},{"cell_type":"markdown","source":"<a class=\"anchor\" id=\"0.1\"></a>\n# **Table of Contents** \n\n1. [Background](#1)\n2. [The Data](#2)\n3. [Data Preprocessing](#3)\n4. [Feature Engineering](#4)\n5. [Training the Model](#5)\n6. [Evaluating the Model](#6)\n7. [Dealing with Class Imbalance](#7)\n8. [Tuning Hyperparameters](#8)\n9. [Conclusion](#9)\n\n\n\n\n\n","metadata":{}},{"cell_type":"markdown","source":"# **1. Background** <a class=\"anchor\" id=\"1\"></a>\n\n[Table of Contents](#0.1)\n\nOur objective is to predict whether or not rain will fall the next day in Australia. This knowledge might be relevant for several reasons, and some of them are listed below:\n\n- To help decide if you should head out with your umbrella or not\n- To know what kind of clothes would be suitable\n- To know if additional preparations are needed to ensure an outdoor date or event goes smoothly\n\nWhatever the case may be, we would try to make sense of the data we have to inform our predictions.\n\nHere we go.\n\n","metadata":{}},{"cell_type":"markdown","source":"<a class = \"anchor\" id=\"2\"></a>\n# 2. **The Data**\n\n[Table of Contents](#0.1)\n\nLet's take a look at our data and get working\n","metadata":{}},{"cell_type":"code","source":"# importing relevant libraries\nimport pandas as pd\nimport numpy as np","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# loading the dataset\nrain_data = pd.read_csv('../input/weather-dataset-rattle-package/weatherAUS.csv')\nprint('The dataset has {} rows and {} columns'.format(rain_data.shape[0],rain_data.shape[1]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rain_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A cursory look at the first five rows of our data reveals that some columns have missing values. We will try to resolve this later, but let's do some more inspection on our dataset","metadata":{}},{"cell_type":"code","source":"rain_data.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# printing out the column names\nprint(rain_data.columns)","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's see the categorical and numeriacal columns we have in our data","metadata":{}},{"cell_type":"code","source":"# Checking the categorical and non-categorical datasets\ncat = rain_data.dtypes=='object'\nnum = rain_data.dtypes=='float64'\ncat_columns = list(cat[cat].index)\nnum_columns = list(num[num].index)\nprint(\"Categorical variables are:\",cat_columns)\n\nprint(\"Numerical variables are:\",num_columns)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We see that there are 7 categorical variables while the rest are numerical. Great.\n\nNow let's do some preprocessing of our data, and try to clean it up a bit","metadata":{}},{"cell_type":"markdown","source":"<a class=\"anchor\" id=\"3\"></a>\n# 3. **Data Preprocessing** \n\n[Table of Contents](#0.1)","metadata":{}},{"cell_type":"markdown","source":"### Missing Values\n\nLet's examine our missing value problem more squarely","metadata":{}},{"cell_type":"code","source":"# checking the number of missing values per column\nrain_data.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Some visualization might be useful, let's see","metadata":{}},{"cell_type":"code","source":"# visualizing missing data\nimport missingno as msno\nmsno.matrix(rain_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plotting the number of rows with entries per column\nmsno.bar(rain_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"msno.heatmap(rain_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the plot above and also the data printed out, there are columns with missing values. \"Sunshine\", \"Evaporation\", \"Cloud9am\", and \"Cloud3am\" in particular have a significant number of missing values. According to the definition of these columns, they seem to be important features.\n\nYou would observe that the target column (RainTomorrow) contains some missing values. We will drop row entries without targets. You would also observe that the correlation between RainToday and Rainfall is high, and that they both even have the same number of missing values. I'll keep both (no pressure), but drop rows with missing values. Later on, the missing values in other columns will be replaced.","metadata":{}},{"cell_type":"code","source":"# drop rows without targets, raintoday, and rainfall entries\nrain_data_clean = rain_data.dropna(axis=0,how='any',subset=[\"RainTomorrow\", \"Rainfall\", \"RainToday\"])\n","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# checking the number of missing values we now have;\nrain_data_clean.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Splitting the data into training and split sets\n\nLest we  fall victim to the silent killer called data leakage, let's split our data into training and test sets","metadata":{}},{"cell_type":"code","source":"# separating the target variables from the features\nX = rain_data_clean.drop(columns = \"RainTomorrow\")\ny = rain_data_clean.loc[:,\"RainTomorrow\"]\nprint (\"The size of X is {}\".format(X.shape))\nprint (\"The size of y is {}\".format(y.shape))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# importing train_test_split\nfrom sklearn.model_selection import train_test_split","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# splitting the dataset, and using the \"stratify\" argument to preserve the class ratio in the train and test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42, stratify=y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a class=\"anchor\" id=\"4\"></a>\n# 4. **Feature Engineering**\n\n[Table of Contents](#0.1)","metadata":{}},{"cell_type":"markdown","source":"### Replacing Dates with Seasons","metadata":{}},{"cell_type":"markdown","source":"According to [TripSavvy](https://www.tripsavvy.com/australian-seasons-1462601#:~:text=To%20break%20things%20down%20for%20you%2C%20each%20of,to%20August%2C%20and%20spring%20from%20September%20to%20November), Australia has four seasons categorised into months as follows:\n<ol>\n <li> Summer : December - February </li>\n <li> Autumn : March - May </li>\n <li> Winter : June - August </li>\n <li> Spring : September - November </li>\n</ol>\n \nAs such, we will replace the entries in the Date column with the corresponding season. This might help us get some insight as rainfall tends to be seasonal.","metadata":{}},{"cell_type":"code","source":"def season_replace(df):\n    import datetime as dt\n#     initialize empty list of monthsmonth \n    month = []\n    for num in df['Date']:\n#         get the year, month, day per entry\n        date_obj = dt.datetime.strptime(num,\"%Y-%m-%d\")\n#         get the month only\n        date_mon = date_obj.month\n#     add month to the series of months\n        month.append(date_mon)\n#     initialise the seasons and let their indexes correspond with the month of the year\n#     i.e. Jan, Feb, Mar correspond to index 0, 1, 2 which are Summer, Summer, Autumn based on seasons\n    season_options = ['Summer', 'Summer', 'Autumn', 'Autumn', 'Autumn', 'Winter', 'Winter', 'Winter', 'Spring', 'Spring', 'Spring','Summer']\n#     intialize empty list of seasons\n    seasons= []\n    for i in month:\n#         add the season for each date entry to the seasons list\n        seasons.append(season_options[i-1])\n#     Drop the date column (it is the first column, index is 0)\n    n = df.columns[0]\n    df.drop(n, axis = 1, inplace = True)\n#     add seasons to the dataframe\n    df['Seasons'] = seasons\n#     re-order the dataframe to start with the seasons column\n    df = df[['Seasons'] +  [col for col in df.columns if col != 'Seasons']]\n    return df","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# replacing with the corresponding season\nX_train = season_replace(X_train)","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next, we will encode the data using One Hot Encoder. First, we create a list of the categorical variables to encode, and numerical variables to standardize.","metadata":{}},{"cell_type":"code","source":"features = X_train.columns\nfeatures_to_encode = X_train.select_dtypes(include=['object', 'bool']).columns\nfeatures_to_scale = X_train.select_dtypes(include=['int64', 'float64']).columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next, we will create a transformer object through which we will pass the encoder, and scaler","metadata":{}},{"cell_type":"code","source":"# importing relevant packages\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.pipeline import make_pipeline\n\nfrom sklearn.preprocessing import OneHotEncoder\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import RobustScaler\n\nfrom sklearn.impute import SimpleImputer","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# instantiate one hot encoder to use\nencoder = OneHotEncoder(handle_unknown='error', drop='first', sparse='True')\n# setting up categorical pipeline\ncat_transformer = Pipeline(steps=[('onehot', encoder)])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# instantiate imputer and scalar for numeric variables\nimputer = SimpleImputer(missing_values = np.nan, strategy=\"median\")\nscaler = RobustScaler()\n# setting up the numerical pipeline\nnum_transformer = Pipeline(steps = [\n    ('imputer', imputer),\n    ('scaler', scaler)\n])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# combining both the numerical and categorical pipeline into a ColumnTransformer instance\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', num_transformer, features_to_scale),\n        ('cat', cat_transformer, features_to_encode)\n    ],remainder='passthrough')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a class=\"anchor\" id=\"5\"></a>\n# 5. **Training the Model** \n\n[Table of Contents](#0.1)","metadata":{}},{"cell_type":"markdown","source":"We will be using a Random Forest Classifier model for this probem","metadata":{}},{"cell_type":"code","source":"# importing Random Forest Classifie\nfrom sklearn.ensemble import RandomForestClassifier\n\n# instantiating the classifier\nrf_classifier = RandomForestClassifier(\n                      min_samples_leaf=50,\n                      n_estimators=150,\n                      bootstrap=True,\n                      oob_score=True,\n                      n_jobs=-1,\n                      random_state=42,\n                      max_features='auto')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next we label encode the target variable as it is currently a catrgorical data type","metadata":{}},{"cell_type":"code","source":"# Encoding the dependent variable\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ny_train = le.fit_transform(y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next, we train the model using the training dataset. The pipe object makes it easy to pass data through a series of processes that happene one after the other. Remember that the preprocessor object was defined for the imputing of missing values and standardization of out data while the rf_classifier is our chosen model.","metadata":{}},{"cell_type":"code","source":"pipe = make_pipeline(preprocessor, rf_classifier)\npipe.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a class=\"anchor\" id=\"6\"></a>\n# 6. **Evaluating the Model**\n\n[Table of Contents](#0.1)","metadata":{}},{"cell_type":"markdown","source":"Let's see how well our does with predicting the target class for out test dataset.\n\nRemember, that we are to take the test data through all the preprocessing and feature engineering processes our training set went through.","metadata":{}},{"cell_type":"markdown","source":"## Preprocessing the test data\nFirst, we replace the dates with their corresponding season","metadata":{}},{"cell_type":"code","source":"# Replace Dates with season in test data\nX_test = season_replace(X_test)\nX_test.head()","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next, we label the target test variable accordingly.","metadata":{}},{"cell_type":"code","source":"# Label encode y_test\ny_test = le.transform(y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next, we predict our target classses","metadata":{}},{"cell_type":"code","source":"y_pred = pipe.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluating the Classifier","metadata":{"tags":[]}},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, roc_auc_score, roc_curve, f1_score\n\nimport matplotlib.pyplot as plt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"acc = accuracy_score(y_test, y_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"The accuracy of the model is {}%\".format(round(acc * 100,3)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Probability Predictions","metadata":{}},{"cell_type":"markdown","source":"Next, let's see our ROC and AUC performance","metadata":{}},{"cell_type":"code","source":"train_probs = pipe.predict_proba(X_train)[:, 1]\ntest_probs = pipe.predict_proba(X_test)[:, 1]\ntrain_pred = pipe.predict(X_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Train ROC AUC Score: {}\".format(roc_auc_score(y_train,train_probs)))\nprint(\"Test ROC AUC Score: {}\".format(roc_auc_score(y_test,test_probs)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To plot the ROC curve, let's define a function that takes in all the necessary arguments and returns the ROC Curve as well as the precision and recall metrics","metadata":{}},{"cell_type":"code","source":"def evaluate_model(y_pred, test_probs, train_pred, train_probs, y_train):\n    \n    baseline = {}\n    baseline['recall'] = recall_score(y_test,\n                        [1 for _ in range(len(y_test))])\n    baseline['precision'] = precision_score(y_test,\n                            [1 for _ in range(len(y_test))])\n    baseline['roc'] = 0.5\n    \n    results = {}\n    results['recall'] = recall_score(y_test, y_pred)\n    results['precision'] = precision_score(y_test, y_pred)\n    results['roc'] = roc_auc_score(y_test, test_probs)\n    \n    train_results = {}\n    train_results[\"recall\"] = recall_score(y_train, train_pred)\n    train_results['precision'] = precision_score(y_train, train_pred)\n    train_results['roc'] = roc_auc_score(y_train, train_probs)\n\n    for metric in ['recall', 'precision', 'roc']:\n        print('{} \\n Baseline: {} \\n Test: {} \\n Train: {}'.format(metric.capitalize(),round(baseline[metric], 2),round(results[metric], 2),round(train_results[metric], 2)))\n              \n#     calculate the  FPR and TPR\n    base_fpr, base_tpr, _ = roc_curve(y_test, [1 for _ in range(len(y_test))])\n    model_fpr, model_tpr, _ = roc_curve(y_test, test_probs)\n              \n    plt.figure(figsize = (8,6))\n    plt.rcParams['font.size'] = 16\n    \n#     Plot both curves\n    plt.plot(base_fpr, base_tpr, 'b', label='baseline')\n    plt.plot(model_fpr, model_tpr, 'r', label='model')\n    plt.legend();\n    \n    plt.xlabel('False Positive Rate');\n    plt.ylabel('True Positive Rate');\n    plt.title('ROC Curves');\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluate_model(y_pred, test_probs, train_pred, train_probs, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that the recall, precision, and auc score for the train and test sets are pretty close to each other. This suggests it is unlikely that our model is being overfitted","metadata":{}},{"cell_type":"markdown","source":"#### Confusion Matrix\n\nNext, let's plot a pretty confusion matrix for some more insight into our model performance","metadata":{}},{"cell_type":"code","source":"import itertools\n\ndef plot_confusion_matrix (cm, classes, normalize=False, title='Confusion Matrix', cmap = plt.cm.Blues):\n    \n    plt.figure(figsize = (10, 10))\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title, size = 24)\n    plt.colorbar (aspect = 4)\n    \n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45, size=14)\n    plt.yticks(tick_marks, classes, size = 14)\n    \n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    \n#     Label the plot\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                fontsize=20,\n                horizontalalignment='center',\n                color='white' if cm[i, j] > thresh else \"black\")\n        \n        plt.grid(None)\n        plt.tight_layout()\n        plt.ylabel ('True label', size = 18)\n        plt.xlabel ('Predicted label', size = 18)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cm = confusion_matrix(y_test, y_pred)\nplot_confusion_matrix(cm, classes=['0 - No Rain', '1 - Rain'],\n                     title = 'Rainfall Confusion Matrix')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From this, we can see that the model is not doing so well to correctly predict that it would rain the next day. In fact you can infer this from the low recall score we had (0.44 and 0.43 for the test and training sets respectively). The precision score, on the other hand was pretty high (0.78 and 0.81 for the test and training sets respectively.\n\nThis means that when our model predicts rainfall, it is more likely to rain than otherwise. However, we would also run issues, because there are a good number of cases where it predicts an absense of rainfall, and it actually rains. Having a low recall score in this case or a high number of false negatives is not desirable.\n\nOne reason why this is the case could be that the dataset is imbalanced, i.e. there are way more instances of the \"No rain\" class than the \"Rain\" class.\n\nLet's attempt to use SMOTE to sample the dataset and improve our model's predictive performance","metadata":{}},{"cell_type":"markdown","source":"<a class=\"anchor\" id=\"7\"></a>\n# 7. **Dealing with Class Imbalance**\n\n[Table of Contents](#0.1)","metadata":{}},{"cell_type":"markdown","source":"Using SMOTE to remedy the class imbalance","metadata":{}},{"cell_type":"code","source":"from imblearn.over_sampling import SMOTE\nsm = SMOTE(random_state = 42)\n# pipe = make_pipeline(preprocessor, rf_classifier)\n# first we fit and transform the training data using the preprocessor transformer instance\n# this ensures that the categorical variables are encoded before the sampling takes place\nX_train_new = preprocessor.fit_transform(X_train)\nX_train_new, y_train_new = sm.fit_resample(X_train_new, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Instantiating a new pipeline to train the resampled dataset","metadata":{}},{"cell_type":"code","source":"pipe_smote = make_pipeline(rf_classifier)\npipe_smote.fit(X_train_new, y_train_new)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next, we prepare the test dataset for prediction","metadata":{}},{"cell_type":"code","source":"X_test_new = preprocessor.fit_transform(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next, we make predictions using the new model","metadata":{}},{"cell_type":"code","source":"y_pred_new = pipe_smote.predict(X_test_new)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's see our model acccuracy","metadata":{}},{"cell_type":"code","source":"acc_smote = accuracy_score(y_test, y_pred_new)\nprint(\"The accuracy of the smote_model is {}%\".format(round(acc_smote * 100,3)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next, let's evaluate our model's recall, precison, and roc_auc_score","metadata":{}},{"cell_type":"code","source":"train_probs_new = pipe_smote.predict_proba(X_train_new)[:, 1]\ntest_probs_new = pipe_smote.predict_proba(X_test_new)[:, 1]\ntrain_pred_new = pipe_smote.predict(X_train_new)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluate_model(y_pred_new, test_probs_new, train_pred_new, train_probs_new, y_train_new)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next, let's see what our confusion matrix looks like","metadata":{}},{"cell_type":"code","source":"cm_smote = confusion_matrix(y_test, y_pred_new)\nplot_confusion_matrix(cm_smote, classes=['0 - No Rain', '1 - Rain'],\n                     title = 'Rainfall Confusion Matrix [Smote]')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can observe some improvements to our model. The number of false negatives have reduced (so we can expect our recall score to improve). However, the number of false positives have also increased (so our precision has dropped). But that's fine. It is much better to be wrong about rain falling than about rain not falling. ","metadata":{}},{"cell_type":"markdown","source":"<a class=\"anchor\" id=\"8\"></a>\n# **8. Tuning Hyperparameters**\n\n[Table of Contents](#0.1)","metadata":{}},{"cell_type":"code","source":"# this package prints out data in a pretty format\nfrom pprint import pprint\n\n# let's see the current parameters in use\nprint('Parameters currently in useL\\n')\npprint(rf_classifier.get_params())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next, create a grid of parameters for the model to randomly pick and train","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV\n\nn_estimators = [int (x) for x in np.linspace(start=100, stop=700,num=50)]\n\n# number of features to consider at every split\nmax_features = ['auto', 'log2'] \n\n# maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n\n# include None in max_depth\nmax_depth.append(None)\n\n# minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n\n# minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 4, 10]\n\n# method of selecting samples for training each tree\nbootstrap = [True]\n\nmax_leaf_nodes = [None] + list(np.linspace(10, 50, 500).astype(int))\n\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'max_leaf_nodes': max_leaf_nodes,\n               'bootstrap': bootstrap}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rf = RandomForestClassifier(oob_score=True, n_jobs=-1)\n\n# creating a grid of hyperparameters\nrf_random = RandomizedSearchCV(\n                estimator = rf,\n                param_distributions = random_grid,\n                n_iter = 5, cv = 3,\n                verbose=1, random_state=42,\n                scoring='roc_auc')\n\n# next, we define a pipeline instance that takes fits each model gotten from the grid search\n# onto the training data\npipe_random = make_pipeline(rf_random)\npipe_random.fit(X_train_new, y_train_new)\n\n# return the hyperparameters of the best model\nrf_random.best_params_","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next, we checj the average number of nodes and maximum depths in our best random forest classifier","metadata":{}},{"cell_type":"code","source":"best_model = rf_random.best_estimator_\nn_nodes = []\nmax_depths = []\n\nfor ind_tree in best_model.estimators_:\n    n_nodes.append(ind_tree.tree_.node_count)\n    max_depths.append(ind_tree.tree_.max_depth)\n    \nprint ('Average number of nodes: {}'.format(int(np.mean(n_nodes))))\nprint ('Average maximum depth: {}'.format(int(np.mean(max_depths))))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next, we evaluate the best model","metadata":{}},{"cell_type":"code","source":"pipe_best = make_pipeline(best_model)\npipe_best.fit(X_train_new, y_train_new)\ny_pred_best = pipe_best.predict(X_test_new)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_rf_probs_best = pipe_best.predict_proba(X_train_new)[:, 1]\ntest_rf_probs_best = pipe_best.predict_proba(X_test_new)[:, 1]\ntrain_rf_pred_best = pipe_best.predict(X_train_new)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"acc_best = accuracy_score(y_test, y_pred_best)\nprint(\"The accuracy of the smote_model is {}%\".format(round(acc_best * 100,3)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluate_model(y_pred_best, test_rf_probs_best, train_rf_pred_best, train_rf_probs_best, y_train_new)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And of course, the confusion matrix","metadata":{}},{"cell_type":"code","source":"cm_best_model = confusion_matrix(y_test, y_pred_best)\nplot_confusion_matrix(cm_best_model, classes=['0 - No Rain', '1 - Rain'],\n                     title = 'Rainfall Confusion Matrix')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a class=\"anchor\" id=\"8\"></a>\n# **9. Conclusion** \n\n[Table of Contents](#0.1)","metadata":{}},{"cell_type":"markdown","source":"We have been able to make predictions on whether rain will fall in Australia the next day with an accuracy of 79%. Our recall score shows was optimized over the precision score because we'd rather have a situation where we were wrong to predict rainfall than one where we were wrong to predict no rainfall. \n\nThank you.","metadata":{}},{"cell_type":"markdown","source":"Kindly upvote if you found it interesting or helpful. Also, I'd very much appreciate any comments and feedback!","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}