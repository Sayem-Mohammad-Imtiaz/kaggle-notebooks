{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#importing necessary libs\nimport numpy as np\nimport pandas as pd \nimport torch\nimport torch.nn as nn\nimport cv2\nimport os\nimport time\nfrom torch.autograd import Variable\nimport matplotlib.pyplot as plt","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-06-17T15:44:36.055685Z","iopub.execute_input":"2021-06-17T15:44:36.055929Z","iopub.status.idle":"2021-06-17T15:44:38.209526Z","shell.execute_reply.started":"2021-06-17T15:44:36.055903Z","shell.execute_reply":"2021-06-17T15:44:38.208876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#for using BERT\n!pip install sentence_transformers","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-06-17T15:44:38.213303Z","iopub.execute_input":"2021-06-17T15:44:38.213521Z","iopub.status.idle":"2021-06-17T15:47:02.512996Z","shell.execute_reply.started":"2021-06-17T15:44:38.213498Z","shell.execute_reply":"2021-06-17T15:47:02.512251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **YOLO MODEL**","metadata":{}},{"cell_type":"code","source":"#getting weights from pretrained model\nweightsfile = '/kaggle/input/data-for-yolo-v3-kernel/yolov3.weights'\nclassfile = '/kaggle/input/data-for-yolo-v3-kernel/coco.names'\ncfgfile = '/kaggle/working/yolov3.cfg'\nsample_img1 = '/kaggle/working/input/insta20881.jpg'\ninput_dir = '/kaggle/working/input'\noutput_dir = '/kaggle/working/output'\nnms_thesh = 0.5","metadata":{"execution":{"iopub.status.busy":"2021-06-17T15:47:02.516961Z","iopub.execute_input":"2021-06-17T15:47:02.517185Z","iopub.status.idle":"2021-06-17T15:47:02.521537Z","shell.execute_reply.started":"2021-06-17T15:47:02.517157Z","shell.execute_reply":"2021-06-17T15:47:02.520854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if not os.path.exists(input_dir):\n    os.mkdir(input_dir)","metadata":{"execution":{"iopub.status.busy":"2021-06-17T15:47:02.522783Z","iopub.execute_input":"2021-06-17T15:47:02.52334Z","iopub.status.idle":"2021-06-17T15:47:02.531258Z","shell.execute_reply.started":"2021-06-17T15:47:02.523292Z","shell.execute_reply":"2021-06-17T15:47:02.530503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"! wget https://raw.githubusercontent.com/pjreddie/darknet/master/cfg/yolov3.cfg\n#! wget wget https://github.com/ayooshkathuria/pytorch-yolo-v3/raw/master/dog-cycle-car.png\n! cp /kaggle/input/instagram-images-with-captions/instagram_data2/img2/insta20881.jpg /kaggle/working/input\n#! cp /kaggle/input/data-for-yolo-v3-kernel/office.jpg /kaggle/working/input","metadata":{"execution":{"iopub.status.busy":"2021-06-17T15:47:02.533926Z","iopub.execute_input":"2021-06-17T15:47:02.534194Z","iopub.status.idle":"2021-06-17T15:47:04.034361Z","shell.execute_reply.started":"2021-06-17T15:47:02.534161Z","shell.execute_reply":"2021-06-17T15:47:04.033447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will create the model using the configuration file.First we will parse it and then feed to our model","metadata":{}},{"cell_type":"code","source":"# Reading the file line by line (splitting on the ‘/n’ newline signal) \n# we store the parameters in a more readabale format within a dictionary, \n# and then transfer those to a list, we retturn that list as final_list.  \ndef parse_cfg(config_file):\n    file = open(config_file,'r')\n    file = file.read().split('\\n')\n    file =  [line for line in file if len(line)>0 and line[0] != '#']\n    file = [line.lstrip().rstrip() for line in file]\n\n    final_list = []\n    element_dict = {}\n    for line in file:\n\n        if line[0] == '[':\n            if len(element_dict) != 0:     # appending the dict stored on previous iteration\n                    final_list.append(element_dict)\n                    element_dict = {} # again emtying dict\n            element_dict['type'] = ''.join([i for i in line if i != '[' and i != ']'])\n            \n        else:\n            val = line.split('=')\n            element_dict[val[0].rstrip()] = val[1].lstrip()  #removing spaces on left and right side\n        \n    final_list.append(element_dict) # appending the values stored for last set\n    return final_list","metadata":{"execution":{"iopub.status.busy":"2021-06-17T15:47:04.038299Z","iopub.execute_input":"2021-06-17T15:47:04.038549Z","iopub.status.idle":"2021-06-17T15:47:04.048316Z","shell.execute_reply.started":"2021-06-17T15:47:04.038517Z","shell.execute_reply":"2021-06-17T15:47:04.047592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DummyLayer(nn.Module):\n    def __init__(self):\n        super(DummyLayer, self).__init__()\n        \n\n        \nclass DetectionLayer(nn.Module):\n    def __init__(self, anchors):\n        super(DetectionLayer, self).__init__()\n        self.anchors = anchors\n        \n        \n\ndef create_model(blocks):\n#     blocks = parse_cfg(cfgfile)\n    darknet_details = blocks[0]\n    channels = 3 \n    output_filters = []\n    modulelist = nn.ModuleList()\n    # we load he module list from pyorch’s nn.ModuleList and go on to enumerate through the parameter blocks. \n    # Of course, before any of that we declare a Sequential model. \n    # For each block, we check he block’s [‘type’] and if the block’s [type] is convolutional, upsample, root, yolo, as such \n    # then we intialize modules from nn.[Conv2d]/[Upsample] or, for type[‘route’] and type[‘shortcut’] both \n    # we respecitvely declare DummyLayer objects, all to be added to the sequential neural network module. \n    # For type[‘yolo’] layers we declare a DetectionLayer instance instead. \n    # The filters that need to be defined are also taken from the block parameters when needed within each type of module \n    #and finally we return  darknet_details and moduleList variables as output.\n    \n    for i,block in enumerate(blocks[1:]):\n        seq = nn.Sequential()\n        if (block[\"type\"] == \"convolutional\"):\n            activation = block[\"activation\"]\n            filters = int(block[\"filters\"])\n            kernel_size = int(block[\"size\"])\n            strides = int(block[\"stride\"])\n            use_bias= False if (\"batch_normalize\" in block) else True\n            pad = (kernel_size - 1) // 2\n            \n            conv = nn.Conv2d(in_channels=channels, out_channels=filters, kernel_size=kernel_size, \n                             stride=strides, padding=pad, bias = use_bias)\n            seq.add_module(\"conv_{0}\".format(i), conv)\n            \n            if \"batch_normalize\" in block:\n                bn = nn.BatchNorm2d(filters)\n                seq.add_module(\"batch_norm_{0}\".format(i), bn)\n\n            if activation == \"leaky\":\n                activn = nn.LeakyReLU(0.1, inplace = True)\n                seq.add_module(\"leaky_{0}\".format(i), activn)\n            \n        elif (block[\"type\"] == \"upsample\"):\n            upsample = nn.Upsample(scale_factor = 2, mode = \"bilinear\")\n            seq.add_module(\"upsample_{}\".format(i), upsample)\n        \n        elif (block[\"type\"] == 'route'):\n            # start and end is given in format (eg:-1 36 so we will find layer number from it.\n            # we will find layer number in negative format\n            # so that we can get the number of filters in that layer\n            block['layers'] = block['layers'].split(',')\n            block['layers'][0] = int(block['layers'][0])\n            start = block['layers'][0]\n            if len(block['layers']) == 1:               \n                filters = output_filters[i + start]\n                       \n            \n            elif len(block['layers']) > 1:\n                block['layers'][1] = int(block['layers'][1]) - i \n                end = block['layers'][1]\n                filters = output_filters[i + start] + output_filters[i + end]\n                  \n            \n            route = DummyLayer()\n            seq.add_module(\"route_{0}\".format(i),route)\n            # The route layer has an attribute ’layers’ which can have either one or two values. \n            # If the layers attribute has only one value, it outputs the feature maps of the layer indexed by its value. \n            # If the layers atribute has two values, it returns the concatenated feature maps of the layers indexed by its values, \n            # in which case we use torch.cat function wih he argument ‘1’ in order to concatenate the feature maps along their ‘depth’. \n      \n      \n        elif block[\"type\"] == \"shortcut\":\n            from_ = int(block[\"from\"])\n            shortcut = DummyLayer()\n            seq.add_module(\"shortcut_{0}\".format(i),shortcut)\n            \n            \n        elif block[\"type\"] == \"yolo\":\n            mask = block[\"mask\"].split(\",\")\n            mask = [int(m) for m in mask]\n            anchors = block[\"anchors\"].split(\",\")\n            anchors = [(int(anchors[i]), int(anchors[i + 1])) for i in range(0, len(anchors), 2)]\n            anchors = [anchors[i] for i in mask]\n            block[\"anchors\"] = anchors\n            \n            detectorLayer = DetectionLayer(anchors)\n            seq.add_module(\"Detection_{0}\".format(i),detectorLayer)\n                \n        modulelist.append(seq)\n        output_filters.append(filters)  \n        channels = filters\n    \n    return darknet_details, modulelist\n    ","metadata":{"execution":{"iopub.status.busy":"2021-06-17T15:47:04.049972Z","iopub.execute_input":"2021-06-17T15:47:04.050643Z","iopub.status.idle":"2021-06-17T15:47:04.074984Z","shell.execute_reply.started":"2021-06-17T15:47:04.050602Z","shell.execute_reply":"2021-06-17T15:47:04.074296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Creating Yolo Model","metadata":{}},{"cell_type":"code","source":"def prediction(x,inp_dim,anchors,num_classes,CUDA=False):\n    # x --> 4D feature map\n    batch_size = x.size(0)\n    grid_size = x.size(2)\n    stride =  inp_dim // x.size(2)   # factor by which current feature map reduced from input\n    #grid_size = inp_dim // stride\n    \n    bbox_attrs = 5 + num_classes\n    num_anchors = len(anchors)\n    # \n    prediction = x.view(batch_size, bbox_attrs*num_anchors, grid_size*grid_size)\n    prediction = prediction.transpose(1,2).contiguous()\n    prediction = prediction.view(batch_size, grid_size*grid_size*num_anchors, bbox_attrs)\n    \n    # the dimension of anchors is wrt original image.We will make it corresponding to feature map\n    anchors = [(a[0]/stride, a[1]/stride) for a in anchors]\n    #Sigmoid the  centre_X, centre_Y. and object confidencce\n    prediction[:,:,0] = torch.sigmoid(prediction[:,:,0])\n    prediction[:,:,1] = torch.sigmoid(prediction[:,:,1])\n    prediction[:,:,4] = torch.sigmoid(prediction[:,:,4])\n    #Add the center offsets\n    grid = np.arange(grid_size)\n    a,b = np.meshgrid(grid, grid)\n\n    x_offset = torch.FloatTensor(a).view(-1,1) #(1,gridsize*gridsize,1)\n    y_offset = torch.FloatTensor(b).view(-1,1)\n\n    if CUDA:\n        x_offset = x_offset.cuda()\n        y_offset = y_offset.cuda()\n\n    x_y_offset = torch.cat((x_offset, y_offset), 1).repeat(1,num_anchors).view(-1,2).unsqueeze(0)\n    \n\n    prediction[:,:,:2] += x_y_offset\n\n    #log space transform height and the width\n    anchors = torch.FloatTensor(anchors)\n\n    if CUDA:\n        anchors = anchors.cuda()\n\n    anchors = anchors.repeat(grid_size*grid_size, 1).unsqueeze(0)\n    prediction[:,:,2:4] = torch.exp(prediction[:,:,2:4])*anchors #width and height\n    prediction[:,:,5: 5 + num_classes] = torch.sigmoid((prediction[:,:, 5 : 5 + num_classes]))    \n    prediction[:,:,:4] *= stride    \n    return prediction\n    ","metadata":{"execution":{"iopub.status.busy":"2021-06-17T16:01:35.364047Z","iopub.execute_input":"2021-06-17T16:01:35.364348Z","iopub.status.idle":"2021-06-17T16:01:35.380205Z","shell.execute_reply.started":"2021-06-17T16:01:35.364318Z","shell.execute_reply":"2021-06-17T16:01:35.379338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Darknet(nn.Module):\n    def __init__(self, cfgfile):\n        super(Darknet, self).__init__()\n        self.blocks = parse_cfg(cfgfile)\n        self.net_info, self.module_list = create_model(self.blocks)\n        \n    def forward(self, x, CUDA=False):\n        '''\n        We start by defining the modules, outputs and write variables. \n        When defining modules, we iterate over self.blocks[1:] rather than self.blocks since the first element of self.blocks is a ‘net’ block \n        which isn't strictly a component of a forward pass. \n        Then we iterate over the modules in the nework using modüle_list, which holds their order identical to the one in the configuration file. \n        So, simplified as everything is, we just run the input through the layers as they come. \n        The convolutional or upsample modules are as straightforward as that, \n        meanwhile route layer instances need to check for two conditions. \n        '''\n        modules = self.blocks[1:]\n        outputs = {}  \n        # The outputs dictionary will be used to cache the outputs maps from previous layers as the route and shortcu layers need them. \n        # The key value pairs are respectively made up of the indices of the layers and the values are the corresponding feature maps. \n        \n        write = 0     #This is explained \n        #As for the yolo modüle that declares detection layers, its ouput is predictably a convolutional feature map \n        # that has the bounding box attributes along the depth of the feature map. \n        # We transform our outputs tensors in order to have them share spatial dimensions; \n        # then we would need to concatenate the three detection maps into one tensor. \n        #We don’t initialize a concatenation tensor yet and check whether it has been or not with the ‘write’ flag: \n        #If i’s equal o zero then it hasn’t been initialized yet and if it is 1 then the first detection has happened already \n        # and so we have a tensor we can concatenate the rest of the maps to. \n        # This rounabout way o create a tensor to hold the maps together is \n        # the way i is because of our inability to initialize an empty tensor to be populated later.\n        \n        for i, module in enumerate(modules):        \n            module_type = (module[\"type\"])\n            if module_type == \"convolutional\" or module_type == \"upsample\":\n                x = self.module_list[i](x)\n                outputs[i] = x\n                \n            elif module_type == \"route\":\n                layers = module[\"layers\"]\n                layers = [int(a) for a in layers]\n                if len(layers) == 1:\n                    x = outputs[i + layers[0]]\n                if len(layers) > 1:\n                    map1 = outputs[i + layers[0]]\n                    map2 = outputs[i + layers[1]]\n                    x = torch.cat((map1,map2),1)\n       \n                outputs[i] = x\n                \n            elif  module_type == \"shortcut\":\n                from_ = int(module[\"from\"])\n\n                # just adding outputs for residual network\n                x = outputs[i-1] + outputs[i+from_]  \n                outputs[i] = x\n                \n            elif module_type == 'yolo':\n                anchors = self.module_list[i][0].anchors\n                \n                #Get the input dimensions\n                inp_dim = int(self.net_info[\"height\"])\n                #Get the number of classes\n                num_classes = int(module[\"classes\"])\n            \n                #Transform \n                x = x.data   # get the data at that point\n                x = prediction(x,inp_dim,anchors,num_classes)\n                \n                if not write:              #if no collector has been intialised. \n                    detections = x\n                    write = 1\n                else:       \n                    detections = torch.cat((detections, x), 1)\n\n                outputs[i] = outputs[i-1]\n                \n        try:\n            return detections   #return detections if present\n        except:\n            return 0\n    #The factors to consider here are firstly, that the weights can only belong to eiher a batch norm or a convolutional type layer,\n    #secondly, that those weights are stored in the same order of the layers in the configuration file, and \n    # thirdly that when a batch norm layer appears within a ‘convolutional’ block there aren’t any biases. \n    # Conversely, if there are no batch layers in that block then there will be biases. \n    # We read the header and the actual weights separately according to the file’s structure and move on to extracing the weights \n    # then loading them into their lawful layers. \n    # The variable ‘ptr’ which is basically a pointer points out where we are in the weights array. \n    # It bears mentioning that according to whether there is a batch norm layer ina convolutional block or not \n    # we read the weights in different ways.\n    def load_weights(self, weightfile):\n        \n        #Open the weights file\n        fp = open(weightfile, \"rb\")\n\n        #The first 4 values are header information \n        # 1. Major version number\n        # 2. Minor Version Number\n        # 3. Subversion number \n        # 4. IMages seen \n        header = np.fromfile(fp, dtype = np.int32, count = 5)\n        self.header = torch.from_numpy(header)\n        self.seen = self.header[3]\n        \n        #The rest of the values are the weights\n        # Let's load them up\n        weights = np.fromfile(fp, dtype = np.float32)\n        \n        ptr = 0\n        for i in range(len(self.module_list)):\n            module_type = self.blocks[i + 1][\"type\"]\n            \n            if module_type == \"convolutional\":\n                model = self.module_list[i]\n                try:\n                    batch_normalize = int(self.blocks[i+1][\"batch_normalize\"])\n                except:\n                    batch_normalize = 0\n                \n                conv = model[0]\n                \n                if (batch_normalize):\n                    bn = model[1]\n                    \n                    #Get the number of weights of Batch Norm Layer\n                    num_bn_biases = bn.bias.numel()\n                    \n                    #Load the weights\n                    bn_biases = torch.from_numpy(weights[ptr:ptr + num_bn_biases])\n                    ptr += num_bn_biases\n                    \n                    bn_weights = torch.from_numpy(weights[ptr: ptr + num_bn_biases])\n                    ptr  += num_bn_biases\n                    \n                    bn_running_mean = torch.from_numpy(weights[ptr: ptr + num_bn_biases])\n                    ptr  += num_bn_biases\n                    \n                    bn_running_var = torch.from_numpy(weights[ptr: ptr + num_bn_biases])\n                    ptr  += num_bn_biases\n                    \n                    #Cast the loaded weights into dims of model weights. \n                    bn_biases = bn_biases.view_as(bn.bias.data)\n                    bn_weights = bn_weights.view_as(bn.weight.data)\n                    bn_running_mean = bn_running_mean.view_as(bn.running_mean)\n                    bn_running_var = bn_running_var.view_as(bn.running_var)\n\n                    #Copy the data to model\n                    bn.bias.data.copy_(bn_biases)\n                    bn.weight.data.copy_(bn_weights)\n                    bn.running_mean.copy_(bn_running_mean)\n                    bn.running_var.copy_(bn_running_var)\n                \n                else:\n                    #Number of biases\n                    num_biases = conv.bias.numel()\n                \n                    #Load the weights\n                    conv_biases = torch.from_numpy(weights[ptr: ptr + num_biases])\n                    ptr = ptr + num_biases\n                    \n                    #reshape the loaded weights according to the dims of the model weights\n                    conv_biases = conv_biases.view_as(conv.bias.data)\n                    \n                    #Finally copy the data\n                    conv.bias.data.copy_(conv_biases)\n                    \n                    \n                #Let us load the weights for the Convolutional layers\n                num_weights = conv.weight.numel()\n                \n                #Do the same as above for weights\n                conv_weights = torch.from_numpy(weights[ptr:ptr+num_weights])\n                ptr = ptr + num_weights\n\n                conv_weights = conv_weights.view_as(conv.weight.data)\n                conv.weight.data.copy_(conv_weights)\n                # Note: we dont have bias for conv when batch normalization is there","metadata":{"execution":{"iopub.status.busy":"2021-06-17T16:01:59.65581Z","iopub.execute_input":"2021-06-17T16:01:59.656087Z","iopub.status.idle":"2021-06-17T16:01:59.687921Z","shell.execute_reply.started":"2021-06-17T16:01:59.656059Z","shell.execute_reply":"2021-06-17T16:01:59.686904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# YOLO Predictions ","metadata":{}},{"cell_type":"code","source":"def bbox_iou(box1, box2):\n    \"\"\"\n  \n    Bbox_iou is one that deals wih bounding box Intersection over Union calculation. \n    Basically, if we have two bounding boxes of the same class that have IoU larger than a certain threshold value \n    then the one with the lower classificaion confidence is discarded.  \n    In the loop within the function body if any of all the bounding boxes with a larger index than ‘i’ \n    has an ‘iou’ larger than ‘nms_thresh’ (our threshold) then that box is discarded.  \n    \n  \n    \n    \"\"\"\n    #Get the coordinates of bounding boxes\n    b1_x1, b1_y1, b1_x2, b1_y2 = box1[:,0], box1[:,1], box1[:,2], box1[:,3]\n    b2_x1, b2_y1, b2_x2, b2_y2 = box2[:,0], box2[:,1], box2[:,2], box2[:,3]\n    \n    #get the corrdinates of the intersection rectangle\n    inter_rect_x1 =  torch.max(b1_x1, b2_x1)\n    inter_rect_y1 =  torch.max(b1_y1, b2_y1)\n    inter_rect_x2 =  torch.min(b1_x2, b2_x2)\n    inter_rect_y2 =  torch.min(b1_y2, b2_y2)\n    \n    #Intersection area\n    inter_area = torch.clamp(inter_rect_x2 - inter_rect_x1 + 1, min=0) * torch.clamp(inter_rect_y2 - inter_rect_y1 + 1, min=0)\n \n    #Union Area\n    b1_area = (b1_x2 - b1_x1 + 1)*(b1_y2 - b1_y1 + 1)\n    b2_area = (b2_x2 - b2_x1 + 1)*(b2_y2 - b2_y1 + 1)\n    \n    iou = inter_area / (b1_area + b2_area - inter_area)\n    \n    return iou\n\n\ndef unique(tensor):\n    '''\n    ’unique’ lets us count the overall unique class values when muliple ‘true’ detecions of the same class is present in some image. \n    This is accomplished through NMS. \n    '''\n    tensor_np = tensor.cpu().numpy()\n    unique_np = np.unique(tensor_np)\n    unique_tensor = torch.from_numpy(unique_np)\n    \n    tensor_res = tensor.new(unique_tensor.shape)\n    tensor_res.copy_(unique_tensor)\n    return tensor_res\n\n#TBelow, objectness confidence thresholding, sets a threshold so that we can discard the bounding boxes \n# that aren’t made in confident classification. \n# The bounding boxes we are left with are still desribed by their center coordinates plus their height and width \n# but for IoU calculation we are better off with these boxes’ diagonals. \n# Thus we tranform box_corner as necessary. \n# And we can have several instances ‘true’ for a single class within any given image so \n# this confidence thresholding and NMS have to be done for an image at once, \n# the operations involved in the process unable to be vectorised.   \n\n#As an aside, It’s worth noting here that within this function body the write flag represents the presence of the ‘output’ tensor. \ndef write_results(prediction, confidence, num_classes, nms_conf = 0.4):\n    # taking only values above a particular threshold and set rest everything to zero\n    conf_mask = (prediction[:,:,4] > confidence).float().unsqueeze(2)\n    prediction = prediction*conf_mask\n    \n    \n    #(center x, center y, height, width) attributes of our boxes, \n    #to (top-left corner x, top-left corner y, right-bottom corner x, right-bottom corner y)\n    box_corner = prediction.new(prediction.shape)\n    box_corner[:,:,0] = (prediction[:,:,0] - prediction[:,:,2]/2)\n    box_corner[:,:,1] = (prediction[:,:,1] - prediction[:,:,3]/2)\n    box_corner[:,:,2] = (prediction[:,:,0] + prediction[:,:,2]/2) \n    box_corner[:,:,3] = (prediction[:,:,1] + prediction[:,:,3]/2)\n    prediction[:,:,:4] = box_corner[:,:,:4]\n    \n    batch_size = prediction.size(0)\n    write = False\n    \n    # we can do non max suppression only on individual images so we will loop through images\n    for ind in range(batch_size):  \n        image_pred = prediction[ind] \n        # we will take only those rows with maximm class probability\n        # and corresponding index\n        max_conf, max_conf_score = torch.max(image_pred[:,5:5+ num_classes], 1)\n        max_conf = max_conf.float().unsqueeze(1)\n        max_conf_score = max_conf_score.float().unsqueeze(1)\n        seq = (image_pred[:,:5], max_conf, max_conf_score)\n        # concatinating index values and max probability with box cordinates as columns\n        image_pred = torch.cat(seq, 1) \n        #Remember we had set the bounding box rows having a object confidence\n        # less than the threshold to zero? Let's get rid of them.\n        non_zero_ind =  (torch.nonzero(image_pred[:,4])) # non_zero_ind will give the indexes \n        image_pred_ = image_pred[non_zero_ind.squeeze(),:].view(-1,7)\n        try:\n            #Get the various classes detected in the image\n            img_classes = unique(image_pred_[:,-1]) # -1 index holds the class index\n        except:\n             continue\n       \n        # As each box has 85 attributes, 80 of which are class scores, \n    # we are mostly trying to ensure that we are dealing with the maximum value out of all those 80 as the box’s class. \n    # What we do here is that we remove those 80 attribues we won’t be needing but \n    # add the index of the class that has the maximum value alongside its actual confidence score. \n\n    \n\n        #Having fetched the unique classes as ‘cls’ in an image, we perform NMS for each of those classes. \n        for cls in img_classes:\n            #perform NMS\n            #get the detections with one particular class\n            cls_mask = image_pred_*(image_pred_[:,-1] == cls).float().unsqueeze(1)\n            class_mask_ind = torch.nonzero(cls_mask[:,-2]).squeeze()\n            image_pred_class = image_pred_[class_mask_ind].view(-1,7)\n            \n            # sort them based on probability\n            conf_sort_index = torch.sort(image_pred_class[:,4], descending = True )[1]#getting index\n            image_pred_class = image_pred_class[conf_sort_index]\n            idx = image_pred_class.size(0)\n            \n            for i in range(idx):\n                #Get the IOUs of all boxes that come after the one we are looking at \n                 #in the loop\n                try:\n                    ious = bbox_iou(image_pred_class[i].unsqueeze(0), image_pred_class[i+1:])\n                except ValueError:\n                    break\n                except IndexError:\n                    break\n                \n                #Zero out all the detections that have IoU > treshhold\n                iou_mask = (ious < nms_conf).float().unsqueeze(1)\n                image_pred_class[i+1:] *= iou_mask\n                \n                #Remove the non-zero entries\n                non_zero_ind = torch.nonzero(image_pred_class[:,4]).squeeze()\n                image_pred_class = image_pred_class[non_zero_ind].view(-1,7)\n          \n            #Concatenate the batch_id of the image to the detection\n            #this helps us identify which image does the detection correspond to \n            #We use a linear straucture to hold ALL the detections from the batch\n            #the batch_dim is flattened\n            #batch is identified by extra batch column\n            \n            #creating a row with index of images\n            batch_ind = image_pred_class.new(image_pred_class.size(0), 1).fill_(ind)\n            seq = batch_ind, image_pred_class\n            if not write:\n                output = torch.cat(seq,1)\n                write = True\n            else:\n                out = torch.cat(seq,1)\n                output = torch.cat((output,out))\n    \n    return output\n            ","metadata":{"execution":{"iopub.status.busy":"2021-06-17T16:02:02.555508Z","iopub.execute_input":"2021-06-17T16:02:02.555782Z","iopub.status.idle":"2021-06-17T16:02:02.587385Z","shell.execute_reply.started":"2021-06-17T16:02:02.555752Z","shell.execute_reply":"2021-06-17T16:02:02.586595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Utility functions\n\n# function to load the classes\ndef load_classes(namesfile):\n    '''outputs a dicionary wthat has its keys as class indices and their names as strings as values. '''\n    fp = open(namesfile, \"r\")\n    names = fp.read().split(\"\\n\")[:-1]\n    return names\n\n# function converting images from opencv format to torch format\ndef prep_image(img, inp_dim):\n\n    \"\"\"\n    we move on to define the prep_image function to transform our image \n    (that’s loaded with opencv wih BGR as its order of channels) \n    into PyTorch’s image input format, which is BatchesxChannelsxHeightxWidth.\n    \"\"\"\n\n    orig_im = cv2.imread(img)\n    dim = orig_im.shape[1], orig_im.shape[0]\n    img = (letterbox_image(orig_im, (inp_dim, inp_dim)))\n    img_ = img[:,:,::-1].transpose((2,0,1)).copy()\n    img_ = torch.from_numpy(img_).float().div(255.0).unsqueeze(0)\n    return img_, orig_im, dim\n\n#function letterbox_image that resizes our image, keeping the \n# aspect ratio consistent, and padding the left out areas with the color (128,128,128)\ndef letterbox_image(img, inp_dim):\n    '''Right after we also the letterbox_image function which resizes our image while no touching the  aspect ratio \n    and padding the left out areas with the color ‘grey’, which is also (128, 128, 128). '''\n    img_w, img_h = img.shape[1], img.shape[0]\n    w, h = inp_dim\n    new_w = int(img_w * min(w/img_w, h/img_h))\n    new_h = int(img_h * min(w/img_w, h/img_h))\n    resized_image = cv2.resize(img, (new_w,new_h), interpolation = cv2.INTER_CUBIC)\n    \n    canvas = np.full((inp_dim[1], inp_dim[0], 3), 128)\n\n    canvas[(h-new_h)//2:(h-new_h)//2 + new_h,(w-new_w)//2:(w-new_w)//2 + new_w,:] = resized_image\n    \n    return canvas","metadata":{"execution":{"iopub.status.busy":"2021-06-17T16:02:23.224259Z","iopub.execute_input":"2021-06-17T16:02:23.224525Z","iopub.status.idle":"2021-06-17T16:02:23.235886Z","shell.execute_reply.started":"2021-06-17T16:02:23.224498Z","shell.execute_reply":"2021-06-17T16:02:23.235163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CUDA = False\nbatch_size = 2\n#Set up the neural network\nprint(\"Loading network.....\")\nmodel = Darknet(cfgfile)\nmodel.load_weights(weightsfile)\nprint(\"Network successfully loaded\")\nclasses = load_classes(classfile)\nprint('Classes loaded')\ninp_dim = int(model.net_info[\"height\"])\nassert inp_dim % 32 == 0 \nassert inp_dim > 32\nquery=''\n\n#If there's a GPU availible, put the model on GPU\nif CUDA:\n    model.cuda()\n\n#Set the model in evaluation mode\nmodel.eval()\n\n\n# read images from folder 'images' or direcly  image\nread_dir = time.time()\n#Detection phase\ntry:\n    imlist = [os.path.join(os.path.realpath('.'), input_dir, img) for img in os.listdir(input_dir)]\nexcept NotADirectoryError:\n    imlist = []\n    imlist.append(os.path.join(os.path.realpath('.'), input_dir))\nexcept FileNotFoundError:\n    print (\"No file or directory with the name {}\".format(input_dir))\n    exit()\n    \nif not os.path.exists(output_dir):\n    os.makedirs(output_dir)\n    \nload_batch = time.time()\n\n# preparing list of loaded images\n# [[image,original_image,dim[0],dim[1]]]\nbatches = list(map(prep_image, imlist, [inp_dim for x in range(len(imlist))]))\nim_batches = [x[0] for x in batches] # list of resized images\norig_ims = [x[1] for x in batches] # list of original images\nim_dim_list = [x[2] for x in batches] # dimension list\nim_dim_list = torch.FloatTensor(im_dim_list).repeat(1,2) #repeating twice\n    \n    \nif CUDA:\n    im_dim_list = im_dim_list.cuda()\n\n    \n# converting image to batches    \nreminder = 0\nif (len(im_dim_list) % batch_size): #if reminder is there, reminder = 1\n    reminder = 1\n\nif batch_size != 1:\n    num_batches = len(imlist) // batch_size + reminder            \n    im_batches = [torch.cat((im_batches[i*batch_size : min((i +  1)*batch_size,len(im_batches))])) \n                 for i in range(num_batches)] \n    \n    \ni = 0\nwrite = False\n    \nobjs = {}    \n\n#In the loop concerned wih detection below we iterate over the batches, \n# generate a prediction and concatenate the prediction tensors of all the images into the tensor ‘output’. \n# For a batch, we time the detection duration. \n# In the outputs of write_prediction was also the index of an image within the batch, we take that index and transform it with \n# ‘prediction[:,0] += i*batch_size’ so that i represents the index of an image in imlist instead.  \n\nfor batch in im_batches:\n        #load the image \n        start = time.time()\n        if CUDA:\n            batch = batch.cuda()       \n        #Apply offsets to the result predictions\n        #Tranform the predictions as described in the YOLO paper\n        #flatten the prediction vector \n        # B x (bbox cord x no. of anchors) x grid_w x grid_h --> B x bbox x (all the boxes) \n        # Put every proposed box as a row.\n        with torch.no_grad():\n            prediction = model(Variable(batch), CUDA)\n        \n        prediction = write_results(prediction, confidence=0.5, num_classes=80, nms_conf = nms_thesh)\n        \n        if type(prediction) == int:\n            i += 1\n            continue\n\n\n        prediction[:,0] += i*batch_size\n                  \n        if not write:\n            output = prediction\n            write = 1\n        else:\n            output = torch.cat((output,prediction))  # concating predictions from each batch\n        i += 1\n        \n        if CUDA:\n            torch.cuda.synchronize()\n    \ntry:\n    output\nexcept NameError:\n    print(\"No detections were made\")\n    exit()\n\n    \n#Before we draw the bounding boxes, the predictions contained in our output tensor \n#are predictions on the padded image, and not the original image. Merely, re-scaling them \n#to the dimensions of the input image won't work here. We first need to transform the\n#co-ordinates of the boxes to be measured with respect to boundaries of the area on the\n#padded image that contains the original image\n\n\nim_dim_list = torch.index_select(im_dim_list, 0, output[:,0].long())\nscaling_factor = torch.min(inp_dim/im_dim_list,1)[0].view(-1,1)\noutput[:,[1,3]] -= (inp_dim - scaling_factor*im_dim_list[:,0].view(-1,1))/2\noutput[:,[2,4]] -= (inp_dim - scaling_factor*im_dim_list[:,1].view(-1,1))/2\noutput[:,1:5] /= scaling_factor\n    \nfor i in range(output.shape[0]):\n    output[i, [1,3]] = torch.clamp(output[i, [1,3]], 0.0, im_dim_list[i,0])\n    output[i, [2,4]] = torch.clamp(output[i, [2,4]], 0.0, im_dim_list[i,1])\n    \ndef getLabels(labels):\n    global query\n    query=labels\n    \n\ndef write(x, batches, results):\n    c1 = tuple(x[1:3].int())\n    c2 = tuple(x[3:5].int())\n    img = results[int(x[0])]\n    cls = int(x[-1])\n    label = \"{0}\".format(classes[cls])\n    color = (0,0,255)\n    cv2.rectangle(img, c1, c2,color, 2)\n    t_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_PLAIN, 1 , 1)[0]\n    c2 = c1[0] + t_size[0] + 3, c1[1] + t_size[1] + 4\n    cv2.rectangle(img, c1, c2,color, -1)\n    cv2.putText(img, label, (c1[0], c1[1] + t_size[1] + 4), cv2.FONT_HERSHEY_PLAIN, 1, [225,255,255], 1)\n    getLabels(label)\n    return img\n    \n            \nlist(map(lambda x: write(x, im_batches, orig_ims), output))\n      \ndet_names = pd.Series(imlist).apply(lambda x: \"{}/det_{}\".format(output_dir,x.split(\"/\")[-1]))\n    \nlist(map(cv2.imwrite, det_names, orig_ims))\n\n    \n\ntorch.cuda.empty_cache()\n    ","metadata":{"execution":{"iopub.status.busy":"2021-06-17T16:02:28.776823Z","iopub.execute_input":"2021-06-17T16:02:28.777092Z","iopub.status.idle":"2021-06-17T16:02:36.147246Z","shell.execute_reply.started":"2021-06-17T16:02:28.777066Z","shell.execute_reply":"2021-06-17T16:02:36.146557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img = cv2.imread('/kaggle/working/output/det_insta20881.jpg') \nimg = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\nplt.figure(figsize=(20,10))\nplt.imshow(img)","metadata":{"execution":{"iopub.status.busy":"2021-06-17T16:02:42.368252Z","iopub.execute_input":"2021-06-17T16:02:42.368522Z","iopub.status.idle":"2021-06-17T16:02:42.753292Z","shell.execute_reply.started":"2021-06-17T16:02:42.368493Z","shell.execute_reply":"2021-06-17T16:02:42.752323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **BERT MODEL**","metadata":{}},{"cell_type":"code","source":"from sentence_transformers import *","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-06-17T16:02:49.575278Z","iopub.execute_input":"2021-06-17T16:02:49.57554Z","iopub.status.idle":"2021-06-17T16:02:55.941277Z","shell.execute_reply.started":"2021-06-17T16:02:49.575515Z","shell.execute_reply":"2021-06-17T16:02:55.94056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#getting the model\nmodel=SentenceTransformer('bert-large-nli-mean-tokens')","metadata":{"execution":{"iopub.status.busy":"2021-06-17T16:02:55.94292Z","iopub.execute_input":"2021-06-17T16:02:55.943181Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#getting the dataset for fine tuning\ntrain=pd.read_csv('../input/instagram-images-with-captions/instagram_data/captions_csv.csv',delimiter=',')\ntrain['Caption']=train['Caption'].astype(str)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#creating embeddings from the model word corpus\nembeddings = model.encode(train['Caption'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#printing the label generated from yolo\nprint(query)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#encoding the query \nquery_embedding = model.encode(query)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(query_embedding)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# RESULT\n> Printing the caption SUGGESTIONS","metadata":{}},{"cell_type":"code","source":"top_k=5\ncos_scores = util.pytorch_cos_sim(query_embedding, embeddings)[0]\ncos_scores = cos_scores.cpu()\n\n#We use torch.topk to find the highest 5 scores for similiarty\ntop_results = torch.topk(cos_scores, k=top_k)\n\nprint(\"\\nTop 5 most similar sentences in corpus:\")\nimage_id=[]\nfor score, idx in zip(top_results[0], top_results[1]):\n    print(train['Caption'].values[idx])","metadata":{"execution":{"iopub.status.busy":"2021-06-17T13:26:40.859409Z","iopub.execute_input":"2021-06-17T13:26:40.859704Z","iopub.status.idle":"2021-06-17T13:26:41.133839Z","shell.execute_reply.started":"2021-06-17T13:26:40.859655Z","shell.execute_reply":"2021-06-17T13:26:41.132872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n### Reference:\n\nhttps://arxiv.org/pdf/1804.02767.pdf <br>\nhttps://github.com/ayooshkathuria/pytorch-yolo-v3/blob/master/util.py <br>\n\n","metadata":{}}]}