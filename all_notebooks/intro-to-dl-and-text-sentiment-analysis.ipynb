{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 1. Reading the Dataset","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n# Selecting a subset of data to be faster in demonstration\ntrain_df = pd.read_csv('../input/imdb-dataset-sentiment-analysis-in-csv-format/Train.csv').head(4000)\nvalid_df = pd.read_csv('../input/imdb-dataset-sentiment-analysis-in-csv-format/Valid.csv').head(500)\ntest_df = pd.read_csv('../input/imdb-dataset-sentiment-analysis-in-csv-format/Test.csv').head(500)\nprint('Train: '+ str(len(train_df)))\nprint('Valid: '+ str(len(valid_df)))\nprint('Test: '+ str(len(test_df)))\ntrain_df.head(10)","metadata":{"execution":{"iopub.status.busy":"2021-07-16T14:50:05.197832Z","iopub.execute_input":"2021-07-16T14:50:05.198445Z","iopub.status.idle":"2021-07-16T14:50:06.338381Z","shell.execute_reply.started":"2021-07-16T14:50:05.19833Z","shell.execute_reply":"2021-07-16T14:50:06.337258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Some text pre-processing","metadata":{}},{"cell_type":"code","source":"# Turnig all text to lowercase\ntrain_df['text'] = train_df['text'].str.lower()\nvalid_df['text'] = valid_df['text'].str.lower()\ntest_df['text'] = test_df['text'].str.lower()\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-16T14:50:06.343338Z","iopub.execute_input":"2021-07-16T14:50:06.343783Z","iopub.status.idle":"2021-07-16T14:50:06.383619Z","shell.execute_reply.started":"2021-07-16T14:50:06.343747Z","shell.execute_reply":"2021-07-16T14:50:06.3826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Removing ponctuation\nimport string\n\nexclude = set(string.punctuation) \n\ndef remove_punctuation(x): \n    try: \n        x = ''.join(ch for ch in x if ch not in exclude) \n    except: \n        pass \n    return x \n\ntrain_df['text'] = train_df['text'].apply(remove_punctuation)\nvalid_df['text'] = valid_df['text'].apply(remove_punctuation)\ntest_df['text'] = test_df['text'].apply(remove_punctuation)\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-16T14:50:06.387913Z","iopub.execute_input":"2021-07-16T14:50:06.388266Z","iopub.status.idle":"2021-07-16T14:50:07.229345Z","shell.execute_reply.started":"2021-07-16T14:50:06.388231Z","shell.execute_reply":"2021-07-16T14:50:07.225845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Removing stopwords\nfrom nltk.corpus import stopwords\n\nstop = stopwords.words('english')\n\ntrain_df['text'] = train_df['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\nvalid_df['text'] = valid_df['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\ntest_df['text'] = test_df['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-16T14:50:07.233207Z","iopub.execute_input":"2021-07-16T14:50:07.233545Z","iopub.status.idle":"2021-07-16T14:50:10.337902Z","shell.execute_reply.started":"2021-07-16T14:50:07.23351Z","shell.execute_reply":"2021-07-16T14:50:10.337213Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Sentences as Bag of Words","metadata":{}},{"cell_type":"markdown","source":"## Classical Model with TF-IDF and SVM","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\n# Create feature vectors for every sentence\nvectorizer = TfidfVectorizer(#min_df = 5,\n                             #max_df = 0.8,\n                             max_features = 20000,\n                             sublinear_tf = True,\n                             use_idf = True)#, stop_words='english')#vocabulary = list(embeddings_index.keys()\n\ntrain_vectors = vectorizer.fit_transform(train_df['text'])\nvalid_vectors = vectorizer.transform(valid_df['text'])\ntest_vectors = vectorizer.transform(test_df['text'])","metadata":{"execution":{"iopub.status.busy":"2021-07-16T14:50:10.341452Z","iopub.execute_input":"2021-07-16T14:50:10.341726Z","iopub.status.idle":"2021-07-16T14:50:11.109339Z","shell.execute_reply.started":"2021-07-16T14:50:10.341698Z","shell.execute_reply":"2021-07-16T14:50:11.10834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn import svm\n# SVM\nclassifier_linear = svm.SVC(kernel='linear')\n#Train\nclassifier_linear.fit(train_vectors, train_df['label'])","metadata":{"execution":{"iopub.status.busy":"2021-07-16T14:50:11.111778Z","iopub.execute_input":"2021-07-16T14:50:11.112039Z","iopub.status.idle":"2021-07-16T14:50:20.846582Z","shell.execute_reply.started":"2021-07-16T14:50:11.112014Z","shell.execute_reply":"2021-07-16T14:50:20.845749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report\n\npredictions = classifier_linear.predict(test_vectors)\n# results\nreport = classification_report(test_df['label'], predictions)\nprint(report)","metadata":{"execution":{"iopub.status.busy":"2021-07-16T14:50:20.847918Z","iopub.execute_input":"2021-07-16T14:50:20.848259Z","iopub.status.idle":"2021-07-16T14:50:21.955738Z","shell.execute_reply.started":"2021-07-16T14:50:20.848223Z","shell.execute_reply":"2021-07-16T14:50:21.954828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Changing the classifier by a NN model","metadata":{}},{"cell_type":"code","source":"import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Dropout\n\n# Defining the NN model\nmodel = Sequential()\nmodel.add(Dense(20, input_shape=(train_vectors.shape[1],), activation='relu'))\nmodel.add(Dropout(0.3))\n#model.add(Dense(5, activation='relu'))\n#model.add(Dropout(0.3))\nmodel.add(Dense(1, activation='sigmoid'))\n# compile network\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2021-07-16T14:50:21.957358Z","iopub.execute_input":"2021-07-16T14:50:21.957754Z","iopub.status.idle":"2021-07-16T14:50:29.138778Z","shell.execute_reply.started":"2021-07-16T14:50:21.957703Z","shell.execute_reply":"2021-07-16T14:50:29.138038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train\n#Salvar o melhor modelo\ncallbacks_list = [\n    keras.callbacks.ModelCheckpoint(\n        filepath='model.h5',\n        monitor='val_loss', save_best_only=True, verbose=1),\n    keras.callbacks.EarlyStopping(monitor='val_loss', patience=10,verbose=1)\n]\n\nhistory = model.fit(\n    train_vectors.toarray(), train_df['label'], \n    epochs=20, \n    verbose=1,\n    callbacks = callbacks_list,\n    validation_data=(valid_vectors.toarray(), valid_df['label']))","metadata":{"execution":{"iopub.status.busy":"2021-07-16T14:50:29.140103Z","iopub.execute_input":"2021-07-16T14:50:29.140445Z","iopub.status.idle":"2021-07-16T14:50:38.535708Z","shell.execute_reply.started":"2021-07-16T14:50:29.140409Z","shell.execute_reply":"2021-07-16T14:50:38.534753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfig, ax = plt.subplots(1,2, figsize=(16,8))\nax[0].plot(history.history['loss'], color='b', label=\"Training loss\")\nax[0].plot(history.history['val_loss'], color='r', label=\"validation loss\",axes =ax[0])\nlegend = ax[0].legend(loc='best', shadow=True)\n\nax[1].plot(history.history['accuracy'], color='b', label=\"Training accuracy\")\nax[1].plot(history.history['val_accuracy'], color='r',label=\"Validation accuracy\")\nlegend = ax[1].legend(loc='best', shadow=True)","metadata":{"execution":{"iopub.status.busy":"2021-07-16T14:50:38.53724Z","iopub.execute_input":"2021-07-16T14:50:38.537526Z","iopub.status.idle":"2021-07-16T14:50:38.850717Z","shell.execute_reply.started":"2021-07-16T14:50:38.537498Z","shell.execute_reply":"2021-07-16T14:50:38.849912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.models import load_model\n# Load the best saved model\nmodel = load_model('model.h5')\n\ny_pred = model.predict_classes(valid_vectors.toarray())\nprint(classification_report(valid_df['label'], y_pred, target_names=['0','1']))","metadata":{"execution":{"iopub.status.busy":"2021-07-16T14:50:38.851993Z","iopub.execute_input":"2021-07-16T14:50:38.852467Z","iopub.status.idle":"2021-07-16T14:50:39.313597Z","shell.execute_reply.started":"2021-07-16T14:50:38.852428Z","shell.execute_reply":"2021-07-16T14:50:39.312511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Sentences as stream of words","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nfrom tensorflow.keras.layers.experimental.preprocessing import TextVectorization\nimport string\n\n# Model constants.\nmax_features = 20000\nembedding_dim = 100\nsequence_length = 500\n\n# function to stardardize texts\ndef custom_standardization(input_data):\n    lowercase = tf.strings.lower(input_data)\n    #stripped_html = tf.strings.regex_replace(lowercase, \"<br />\", \" \")\n    #return tf.strings.regex_replace(\n    #    stripped_html, \"[%s]\" % re.escape(string.punctuation), \"\"\n    #)\n    return lowercase\n\n# normalize, split, and map strings to integers\nvectorize_layer = TextVectorization(\n    #standardize=custom_standardization,\n    max_tokens=max_features,\n    output_mode=\"int\",\n    output_sequence_length=sequence_length,\n)\n\n# Creating the vocabulary\nvectorize_layer.adapt(train_df['text'].values)","metadata":{"execution":{"iopub.status.busy":"2021-07-16T14:50:39.31821Z","iopub.execute_input":"2021-07-16T14:50:39.318735Z","iopub.status.idle":"2021-07-16T14:50:40.309534Z","shell.execute_reply.started":"2021-07-16T14:50:39.318694Z","shell.execute_reply":"2021-07-16T14:50:40.308465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Vetorizing all the texts\ndef vectorize_text(text):\n    text = tf.expand_dims(text, -1)\n    return vectorize_layer(text)\n\n\n# Vectorize the data.\ntrain_ds = vectorize_text(train_df['text'])\nvalid_ds = vectorize_text(valid_df['text'])\ntest_ds = vectorize_text(test_df[ 'text'])\n","metadata":{"execution":{"iopub.status.busy":"2021-07-16T14:50:40.311178Z","iopub.execute_input":"2021-07-16T14:50:40.311552Z","iopub.status.idle":"2021-07-16T14:50:40.484864Z","shell.execute_reply.started":"2021-07-16T14:50:40.311511Z","shell.execute_reply":"2021-07-16T14:50:40.483822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_df['text'][0])\nprint(train_ds[0])\nprint(vectorize_text(['beautiful pretty']))","metadata":{"execution":{"iopub.status.busy":"2021-07-16T14:50:40.486395Z","iopub.execute_input":"2021-07-16T14:50:40.48675Z","iopub.status.idle":"2021-07-16T14:50:40.509491Z","shell.execute_reply.started":"2021-07-16T14:50:40.486704Z","shell.execute_reply":"2021-07-16T14:50:40.508556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## A Convolutional Model","metadata":{}},{"cell_type":"code","source":"from keras import layers\n\nmodel = Sequential()\n# A integer input for vocab indices.\nmodel.add(layers.Input(shape=(None,), dtype=\"int64\"))\n# Layer to map those vocab indices into a space of dimensionality 'embedding_dim'.\nmodel.add(layers.Embedding(max_features, embedding_dim))\n# Conv1D + global max pooling\nmodel.add(layers.Conv1D(50, 7, padding=\"valid\", activation=\"relu\", strides=3))#200\n#model.add(layers.Conv1D(100, 7, padding=\"valid\", activation=\"relu\", strides=3))\nmodel.add(layers.GlobalMaxPooling1D())\n\n# Common hidden layer for final classification\nmodel.add(layers.Dense(10, activation=\"relu\"))#100\nmodel.add(layers.Dropout(0.5))\n# Single unit output layer with sigmoid activation\nmodel.add(layers.Dense(1, activation=\"sigmoid\", name=\"predictions\"))\n\n# Compile the model\nmodel.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])","metadata":{"execution":{"iopub.status.busy":"2021-07-16T14:50:40.511411Z","iopub.execute_input":"2021-07-16T14:50:40.511811Z","iopub.status.idle":"2021-07-16T14:50:40.581711Z","shell.execute_reply.started":"2021-07-16T14:50:40.511768Z","shell.execute_reply":"2021-07-16T14:50:40.581035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train\n#Salvar o melhor modelo\ncallbacks_list = [\n    keras.callbacks.ModelCheckpoint(\n        filepath='model.h5',\n        monitor='val_loss', save_best_only=True, verbose=1),\n    keras.callbacks.EarlyStopping(monitor='val_loss', patience=5,verbose=1)\n]\n\nhistory = model.fit(\n    train_ds, train_df['label'], \n    epochs=20, \n    verbose=1,\n    callbacks = callbacks_list,\n    validation_data=(valid_ds, valid_df['label']))","metadata":{"execution":{"iopub.status.busy":"2021-07-16T14:50:40.582912Z","iopub.execute_input":"2021-07-16T14:50:40.583265Z","iopub.status.idle":"2021-07-16T14:51:06.033222Z","shell.execute_reply.started":"2021-07-16T14:50:40.58323Z","shell.execute_reply":"2021-07-16T14:51:06.032335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfig, ax = plt.subplots(1,2, figsize=(16,8))\nax[0].plot(history.history['loss'], color='b', label=\"Training loss\")\nax[0].plot(history.history['val_loss'], color='r', label=\"validation loss\",axes =ax[0])\nlegend = ax[0].legend(loc='best', shadow=True)\n\nax[1].plot(history.history['accuracy'], color='b', label=\"Training accuracy\")\nax[1].plot(history.history['val_accuracy'], color='r',label=\"Validation accuracy\")\nlegend = ax[1].legend(loc='best', shadow=True)","metadata":{"execution":{"iopub.status.busy":"2021-07-16T14:51:06.034884Z","iopub.execute_input":"2021-07-16T14:51:06.035291Z","iopub.status.idle":"2021-07-16T14:51:06.338282Z","shell.execute_reply.started":"2021-07-16T14:51:06.035251Z","shell.execute_reply":"2021-07-16T14:51:06.337567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = load_model('model.h5')\n\ny_pred = model.predict_classes(test_ds)\nprint(classification_report(test_df['label'], y_pred, target_names=['0','1']))","metadata":{"execution":{"iopub.status.busy":"2021-07-16T14:51:06.340847Z","iopub.execute_input":"2021-07-16T14:51:06.34114Z","iopub.status.idle":"2021-07-16T14:51:06.564218Z","shell.execute_reply.started":"2021-07-16T14:51:06.341112Z","shell.execute_reply":"2021-07-16T14:51:06.561421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## A LSTM Recursive Model","metadata":{}},{"cell_type":"code","source":"model = Sequential()\n# Input for variable-length sequences of integers\nmodel.add(keras.Input(shape=(None,), dtype=\"int64\"))\n# Embed each integer in a embedding_dim vector\nmodel.add(layers.Embedding(max_features, embedding_dim))\n# Add 2 bidirectional LSTMs\n#model.add(layers.Bidirectional(layers.LSTM(32, return_sequences=True)))\nmodel.add(layers.Bidirectional(layers.LSTM(32)))\n# Add a classifier\nmodel.add(layers.Dense(1, activation=\"sigmoid\"))\n\n# Compile the model\nmodel.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])","metadata":{"execution":{"iopub.status.busy":"2021-07-16T14:51:06.565427Z","iopub.execute_input":"2021-07-16T14:51:06.565749Z","iopub.status.idle":"2021-07-16T14:51:07.015592Z","shell.execute_reply.started":"2021-07-16T14:51:06.565722Z","shell.execute_reply":"2021-07-16T14:51:07.014786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train\n#Salvar o melhor modelo\ncallbacks_list = [\n    keras.callbacks.ModelCheckpoint(\n        filepath='model.h5',\n        monitor='val_loss', save_best_only=True, verbose=1),\n    keras.callbacks.EarlyStopping(monitor='val_loss', patience=5,verbose=1)\n]\n\nhistory = model.fit(\n    train_ds, train_df['label'], \n    epochs=20, \n    verbose=1,\n    callbacks = callbacks_list,\n    validation_data=(valid_ds, valid_df['label']))","metadata":{"execution":{"iopub.status.busy":"2021-07-16T14:51:07.016881Z","iopub.execute_input":"2021-07-16T14:51:07.017212Z","iopub.status.idle":"2021-07-16T14:51:56.988423Z","shell.execute_reply.started":"2021-07-16T14:51:07.017181Z","shell.execute_reply":"2021-07-16T14:51:56.987632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfig, ax = plt.subplots(1,2, figsize=(16,8))\nax[0].plot(history.history['loss'], color='b', label=\"Training loss\")\nax[0].plot(history.history['val_loss'], color='r', label=\"validation loss\",axes =ax[0])\nlegend = ax[0].legend(loc='best', shadow=True)\n\nax[1].plot(history.history['accuracy'], color='b', label=\"Training accuracy\")\nax[1].plot(history.history['val_accuracy'], color='r',label=\"Validation accuracy\")\nlegend = ax[1].legend(loc='best', shadow=True)","metadata":{"execution":{"iopub.status.busy":"2021-07-16T14:51:56.991942Z","iopub.execute_input":"2021-07-16T14:51:56.992258Z","iopub.status.idle":"2021-07-16T14:51:57.324074Z","shell.execute_reply.started":"2021-07-16T14:51:56.99222Z","shell.execute_reply":"2021-07-16T14:51:57.323162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = load_model('model.h5')\n\ny_pred = model.predict_classes(test_ds)\nprint(classification_report(test_df['label'], y_pred, target_names=['0','1']))","metadata":{"execution":{"iopub.status.busy":"2021-07-16T14:51:57.325511Z","iopub.execute_input":"2021-07-16T14:51:57.325907Z","iopub.status.idle":"2021-07-16T14:51:58.616388Z","shell.execute_reply.started":"2021-07-16T14:51:57.325869Z","shell.execute_reply":"2021-07-16T14:51:58.615549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Using Word Embeddings","metadata":{}},{"cell_type":"code","source":"voc = vectorize_layer.get_vocabulary()\nword_index = dict(zip(voc, range(len(voc))))","metadata":{"execution":{"iopub.status.busy":"2021-07-16T14:51:58.619642Z","iopub.execute_input":"2021-07-16T14:51:58.619925Z","iopub.status.idle":"2021-07-16T14:51:58.674127Z","shell.execute_reply.started":"2021-07-16T14:51:58.619897Z","shell.execute_reply":"2021-07-16T14:51:58.673468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"word_index","metadata":{"execution":{"iopub.status.busy":"2021-07-16T14:51:58.67538Z","iopub.execute_input":"2021-07-16T14:51:58.675686Z","iopub.status.idle":"2021-07-16T14:51:58.715451Z","shell.execute_reply.started":"2021-07-16T14:51:58.67566Z","shell.execute_reply":"2021-07-16T14:51:58.714511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = [\"pretty\", \"cat\", \"sat\", \"near\", \"yellow\", \"cat\"]\n[word_index[w] for w in test]","metadata":{"execution":{"iopub.status.busy":"2021-07-16T14:51:58.717205Z","iopub.execute_input":"2021-07-16T14:51:58.717604Z","iopub.status.idle":"2021-07-16T14:51:58.724502Z","shell.execute_reply.started":"2021-07-16T14:51:58.717564Z","shell.execute_reply":"2021-07-16T14:51:58.723362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Glove Embeddings","metadata":{}},{"cell_type":"code","source":"!wget http://nlp.stanford.edu/data/glove.6B.zip\n!unzip -q glove.6B.zip","metadata":{"execution":{"iopub.status.busy":"2021-07-16T14:51:58.726209Z","iopub.execute_input":"2021-07-16T14:51:58.726933Z","iopub.status.idle":"2021-07-16T14:55:02.1183Z","shell.execute_reply.started":"2021-07-16T14:51:58.726864Z","shell.execute_reply":"2021-07-16T14:55:02.117035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls","metadata":{"execution":{"iopub.status.busy":"2021-07-16T14:55:02.122601Z","iopub.execute_input":"2021-07-16T14:55:02.12293Z","iopub.status.idle":"2021-07-16T14:55:02.888694Z","shell.execute_reply.started":"2021-07-16T14:55:02.122897Z","shell.execute_reply":"2021-07-16T14:55:02.887853Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embeddings_index = {}\nwith open(\"glove.6B.100d.txt\") as f:\n    for line in f:\n        word, coefs = line.split(maxsplit=1)\n        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n        embeddings_index[word] = coefs\n\nprint(\"Found %s word vectors.\" % len(embeddings_index))","metadata":{"execution":{"iopub.status.busy":"2021-07-16T14:55:02.892155Z","iopub.execute_input":"2021-07-16T14:55:02.89243Z","iopub.status.idle":"2021-07-16T14:55:17.891263Z","shell.execute_reply.started":"2021-07-16T14:55:02.892403Z","shell.execute_reply":"2021-07-16T14:55:17.890149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embeddings_index[\"cat\"]","metadata":{"execution":{"iopub.status.busy":"2021-07-16T14:55:17.892853Z","iopub.execute_input":"2021-07-16T14:55:17.89341Z","iopub.status.idle":"2021-07-16T14:55:17.903156Z","shell.execute_reply.started":"2021-07-16T14:55:17.893368Z","shell.execute_reply":"2021-07-16T14:55:17.901944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_tokens = len(voc) + 2\n#embedding_dim = 100\nhits = 0\nmisses = 0\n\n# Prepare embedding matrix to be used in a Embedding layer\n# matrix where entry at index i is the pre-trained vector for the word of index i in our vectorizer's vocabulary\nembedding_matrix = np.zeros((num_tokens, embedding_dim))\nfor word, i in word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        # Words not found in embedding index will be all-zeros.\n        # This includes the representation for \"padding\" and \"OOV\"\n        embedding_matrix[i] = embedding_vector\n        hits += 1\n    else:\n        misses += 1\nprint(\"Converted %d words (%d misses)\" % (hits, misses))","metadata":{"execution":{"iopub.status.busy":"2021-07-16T14:55:17.90525Z","iopub.execute_input":"2021-07-16T14:55:17.905697Z","iopub.status.idle":"2021-07-16T14:55:17.962505Z","shell.execute_reply.started":"2021-07-16T14:55:17.905652Z","shell.execute_reply":"2021-07-16T14:55:17.961354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embeddings_index[\"movie\"]","metadata":{"execution":{"iopub.status.busy":"2021-07-16T14:55:17.964057Z","iopub.execute_input":"2021-07-16T14:55:17.96439Z","iopub.status.idle":"2021-07-16T14:55:17.97395Z","shell.execute_reply.started":"2021-07-16T14:55:17.964355Z","shell.execute_reply":"2021-07-16T14:55:17.973019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"word_index[\"movie\"]","metadata":{"execution":{"iopub.status.busy":"2021-07-16T14:55:17.981073Z","iopub.execute_input":"2021-07-16T14:55:17.981328Z","iopub.status.idle":"2021-07-16T14:55:17.987279Z","shell.execute_reply.started":"2021-07-16T14:55:17.981304Z","shell.execute_reply":"2021-07-16T14:55:17.98585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embedding_matrix[3]","metadata":{"execution":{"iopub.status.busy":"2021-07-16T14:55:17.988808Z","iopub.execute_input":"2021-07-16T14:55:17.989286Z","iopub.status.idle":"2021-07-16T14:55:18.000919Z","shell.execute_reply.started":"2021-07-16T14:55:17.989189Z","shell.execute_reply":"2021-07-16T14:55:17.999758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualizing with T-SNE\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n\nmax_w = 1000\n# Creates and TSNE model and plots it\nlabels = []\ntokens = []\nfor word, i in word_index.items():\n    if i == max_w:\n        break\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        tokens.append(embedding_vector)\n        labels.append(word)\ntsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2500, random_state=23)\nnew_values = tsne_model.fit_transform(tokens)\nx = []\ny = []\nfor value in new_values:\n    x.append(value[0])\n    y.append(value[1])\nplt.figure(figsize=(16, 16))\nfor i in range(len(x)):\n    plt.scatter(x[i],y[i])\n    plt.annotate(labels[i],\n        xy=(x[i], y[i]),\n        xytext=(5, 2),\n        textcoords='offset points',\n        ha='right',\n        va='bottom')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2021-07-16T14:55:18.002904Z","iopub.execute_input":"2021-07-16T14:55:18.003351Z","iopub.status.idle":"2021-07-16T14:56:00.024893Z","shell.execute_reply.started":"2021-07-16T14:55:18.003309Z","shell.execute_reply":"2021-07-16T14:56:00.02401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load the pre-trained word embeddings matrix into an Embedding layer. trainable = False\nembedding_layer = layers.Embedding(\n    num_tokens,\n    embedding_dim,\n    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n    trainable=False,\n)","metadata":{"execution":{"iopub.status.busy":"2021-07-16T14:56:00.026013Z","iopub.execute_input":"2021-07-16T14:56:00.026321Z","iopub.status.idle":"2021-07-16T14:56:00.033711Z","shell.execute_reply.started":"2021-07-16T14:56:00.02629Z","shell.execute_reply":"2021-07-16T14:56:00.032951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Sequential()\n# Input for variable-length sequences of integers\nmodel.add(keras.Input(shape=(None,), dtype=\"int64\"))\n# load the pre-trained word embeddings matrix into an Embedding layer. trainable = False\nmodel.add(layers.Embedding(num_tokens,embedding_dim, embeddings_initializer=keras.initializers.Constant(embedding_matrix), trainable=False))\n# Add 2 bidirectional LSTMs\nmodel.add(layers.Bidirectional(layers.LSTM(32, return_sequences=True)))\nmodel.add(layers.Bidirectional(layers.LSTM(32)))\n# Add a classifier\nmodel.add(layers.Dense(1, activation=\"sigmoid\"))\n\n# Compile the model\nmodel.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])","metadata":{"execution":{"iopub.status.busy":"2021-07-16T14:56:00.035216Z","iopub.execute_input":"2021-07-16T14:56:00.035822Z","iopub.status.idle":"2021-07-16T14:56:00.910767Z","shell.execute_reply.started":"2021-07-16T14:56:00.035782Z","shell.execute_reply":"2021-07-16T14:56:00.909785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train\n#Salvar o melhor modelo\ncallbacks_list = [\n    keras.callbacks.ModelCheckpoint(\n        filepath='model.h5',\n        monitor='val_loss', save_best_only=True, verbose=1),\n    keras.callbacks.EarlyStopping(monitor='val_loss', patience=5,verbose=1)\n]\n\nhistory = model.fit(\n    train_ds, train_df['label'], \n    epochs=20, \n    verbose=1,\n    callbacks = callbacks_list,\n    validation_data=(valid_ds, valid_df['label']))","metadata":{"execution":{"iopub.status.busy":"2021-07-16T14:56:00.912536Z","iopub.execute_input":"2021-07-16T14:56:00.912951Z","iopub.status.idle":"2021-07-16T14:59:22.62356Z","shell.execute_reply.started":"2021-07-16T14:56:00.912908Z","shell.execute_reply":"2021-07-16T14:59:22.62285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfig, ax = plt.subplots(1,2, figsize=(16,8))\nax[0].plot(history.history['loss'], color='b', label=\"Training loss\")\nax[0].plot(history.history['val_loss'], color='r', label=\"validation loss\",axes =ax[0])\nlegend = ax[0].legend(loc='best', shadow=True)\n\nax[1].plot(history.history['accuracy'], color='b', label=\"Training accuracy\")\nax[1].plot(history.history['val_accuracy'], color='r',label=\"Validation accuracy\")\nlegend = ax[1].legend(loc='best', shadow=True)","metadata":{"execution":{"iopub.status.busy":"2021-07-16T14:59:22.626716Z","iopub.execute_input":"2021-07-16T14:59:22.626997Z","iopub.status.idle":"2021-07-16T14:59:22.950125Z","shell.execute_reply.started":"2021-07-16T14:59:22.626958Z","shell.execute_reply":"2021-07-16T14:59:22.949398Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = load_model('model.h5')\n\ny_pred = model.predict(test_ds)\ny_pred = [1 if x >=0.5 else 0 for x in y_pred]\nprint(classification_report(test_df['label'], y_pred, target_names=['0','1']))","metadata":{"execution":{"iopub.status.busy":"2021-07-16T14:59:22.951442Z","iopub.execute_input":"2021-07-16T14:59:22.951855Z","iopub.status.idle":"2021-07-16T14:59:26.904046Z","shell.execute_reply.started":"2021-07-16T14:59:22.951815Z","shell.execute_reply":"2021-07-16T14:59:26.903288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Using a Transformer","metadata":{}},{"cell_type":"code","source":"class TransformerBlock(layers.Layer):\n    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1, name=None, **kwargs):\n        super(TransformerBlock, self).__init__(name=name)\n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        self.ff_dim = ff_dim\n        self.rate = rate\n        super(TransformerBlock, self).__init__(**kwargs)\n        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n        self.ffn = keras.Sequential(\n            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n        )\n        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n        self.dropout1 = layers.Dropout(rate)\n        self.dropout2 = layers.Dropout(rate)\n\n    def call(self, inputs, training):\n        attn_output = self.att(inputs, inputs)\n        attn_output = self.dropout1(attn_output, training=training)\n        out1 = self.layernorm1(inputs + attn_output)\n        ffn_output = self.ffn(out1)\n        ffn_output = self.dropout2(ffn_output, training=training)\n        return self.layernorm2(out1 + ffn_output)\n    \n    def get_config(self):\n        config = super().get_config().copy()\n        config.update({\n            'embed_dim': self.embed_dim,\n            'num_heads': self.num_heads,\n            'ff_dim': self.ff_dim,\n            'rate': self.rate,\n        })\n        return config","metadata":{"execution":{"iopub.status.busy":"2021-07-16T15:07:46.364301Z","iopub.execute_input":"2021-07-16T15:07:46.364627Z","iopub.status.idle":"2021-07-16T15:07:46.375313Z","shell.execute_reply.started":"2021-07-16T15:07:46.364597Z","shell.execute_reply":"2021-07-16T15:07:46.374476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Two seperate embedding layers, one for tokens, one for token index (positions).\nclass TokenAndPositionEmbedding(layers.Layer):\n    def __init__(self, maxlen, vocab_size, embed_dim, name=None, **kwargs):\n        super(TokenAndPositionEmbedding, self).__init__(name=name)\n        self.maxlen = maxlen\n        self.vocab_size = vocab_size\n        self.embed_dim = embed_dim\n        super(TokenAndPositionEmbedding, self).__init__(**kwargs)\n        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n\n    def call(self, x):\n        maxlen = tf.shape(x)[-1]\n        positions = tf.range(start=0, limit=maxlen, delta=1)\n        positions = self.pos_emb(positions)\n        x = self.token_emb(x)\n        return x + positions\n    \n    def get_config(self):\n        config = super().get_config().copy()\n        config.update({\n            'maxlen': self.maxlen,\n            'vocab_size': self.vocab_size,\n            'embed_dim': self.embed_dim\n        })\n        return config","metadata":{"execution":{"iopub.status.busy":"2021-07-16T15:07:46.377518Z","iopub.execute_input":"2021-07-16T15:07:46.378028Z","iopub.status.idle":"2021-07-16T15:07:46.396243Z","shell.execute_reply.started":"2021-07-16T15:07:46.377992Z","shell.execute_reply":"2021-07-16T15:07:46.395408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#max_features = 20000\n#embedding_dim = 100\n#sequence_length = 500\n\n#embed_dim = 32  # Embedding size for each token\nnum_heads = 2  # Number of attention heads\nff_dim = 32  # Hidden layer size in feed forward network inside transformer\n\ninputs = layers.Input(shape=(sequence_length,))\nembedding_layer = TokenAndPositionEmbedding(sequence_length, max_features, embedding_dim)\nx = embedding_layer(inputs)\ntransformer_block = TransformerBlock(embedding_dim, num_heads, ff_dim)\nx = transformer_block(x)\nx = layers.GlobalAveragePooling1D()(x)\nx = layers.Dropout(0.1)(x)\nx = layers.Dense(20, activation=\"relu\")(x)\nx = layers.Dropout(0.1)(x)\noutputs = layers.Dense(1, activation=\"sigmoid\")(x)\n\nmodel = keras.Model(inputs=inputs, outputs=outputs)\n#model.compile(\"adam\", \"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\nmodel.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])","metadata":{"execution":{"iopub.status.busy":"2021-07-16T15:07:46.406946Z","iopub.execute_input":"2021-07-16T15:07:46.407307Z","iopub.status.idle":"2021-07-16T15:07:46.713454Z","shell.execute_reply.started":"2021-07-16T15:07:46.407282Z","shell.execute_reply":"2021-07-16T15:07:46.712694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train\n#Salvar o melhor modelo\ncallbacks_list = [\n    keras.callbacks.ModelCheckpoint(\n        filepath='model.h5',\n        monitor='val_loss', save_best_only=True, verbose=1),\n    keras.callbacks.EarlyStopping(monitor='val_loss', patience=5,verbose=1)\n]\n\nhistory = model.fit(\n    train_ds, train_df['label'], \n    epochs=10, \n    verbose=1,\n    callbacks = callbacks_list,\n    validation_data=(valid_ds, valid_df['label'])\n)","metadata":{"execution":{"iopub.status.busy":"2021-07-16T15:07:46.715075Z","iopub.execute_input":"2021-07-16T15:07:46.715385Z","iopub.status.idle":"2021-07-16T15:08:25.864161Z","shell.execute_reply.started":"2021-07-16T15:07:46.715349Z","shell.execute_reply":"2021-07-16T15:08:25.863101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfig, ax = plt.subplots(1,2, figsize=(16,8))\nax[0].plot(history.history['loss'], color='b', label=\"Training loss\")\nax[0].plot(history.history['val_loss'], color='r', label=\"validation loss\",axes =ax[0])\nlegend = ax[0].legend(loc='best', shadow=True)\n\nax[1].plot(history.history['accuracy'], color='b', label=\"Training accuracy\")\nax[1].plot(history.history['val_accuracy'], color='r',label=\"Validation accuracy\")\nlegend = ax[1].legend(loc='best', shadow=True)","metadata":{"execution":{"iopub.status.busy":"2021-07-16T15:08:25.868143Z","iopub.execute_input":"2021-07-16T15:08:25.869957Z","iopub.status.idle":"2021-07-16T15:08:26.39412Z","shell.execute_reply.started":"2021-07-16T15:08:25.869909Z","shell.execute_reply":"2021-07-16T15:08:26.393224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = load_model('model.h5', custom_objects={'TokenAndPositionEmbedding': TokenAndPositionEmbedding, 'TransformerBlock': TransformerBlock })\n\ny_pred = model.predict(test_ds)\ny_pred = [1 if x >=0.5 else 0 for x in y_pred]\nprint(classification_report(test_df['label'], y_pred, target_names=['0','1']))","metadata":{"execution":{"iopub.status.busy":"2021-07-16T15:08:26.406073Z","iopub.execute_input":"2021-07-16T15:08:26.408137Z","iopub.status.idle":"2021-07-16T15:08:27.384748Z","shell.execute_reply.started":"2021-07-16T15:08:26.408095Z","shell.execute_reply":"2021-07-16T15:08:27.383816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}