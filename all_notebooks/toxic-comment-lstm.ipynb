{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from keras.preprocessing.sequence import pad_sequences\nfrom keras.preprocessing.text import Tokenizer\nimport matplotlib.pyplot as plt\nfrom keras.models import Sequential\nfrom keras.layers import Dense,Dropout,Flatten\nfrom keras.optimizers import Adam\nfrom keras.regularizers import l2\nfrom keras.layers import LSTM\nfrom keras.layers.embeddings import Embedding\nfrom sklearn.model_selection import cross_val_score, train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn import model_selection\nimport nltk\nfrom nltk.stem.snowball import SnowballStemmer\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv('../input/russian-language-toxic-comments/labeled.csv')\ndata.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nltk.download('averaged_perceptron_tagger_ru')\ntext = np.array(data.comment.values)\ntarget = data.toxic.astype(int).values\ndef upperCaseRate(string):\n    \"Returns percentage of uppercase letters in the string\"\n    return np.array(list(map(str.isupper, string))).mean()\nupcaseRate = list(map(upperCaseRate, data.comment.values))\ndef cleanText(string):\n    \"\"\"This function deletes all symbols except Cyrilic and Base Latin alphabet,\n    stopwords, functional parts of speech. Returns string of words stem.\"\"\"\n    # Common cleaning\n    string = string.lower()\n    string = re.sub(r\"http\\S+\", \"\", string)\n    string = str.replace(string,'Ё','е')\n    string = str.replace(string,'ё','е')\n    prog = re.compile('[А-Яа-яA-Za-z]+')\n    words = prog.findall(string.lower())\n    \n    # Word Cleaning\n    ## Stop Words\n    stopwords = nltk.corpus.stopwords.words('russian')\n    words = [w for w in words if w not in stopwords]\n    ## Cleaning functional POS (Parts of Speech)\n    functionalPos = {'CONJ', 'PRCL'}\n    words = [w for w, pos in nltk.pos_tag(words, lang='rus') if pos not in functionalPos]\n    ## Stemming\n    stemmer = SnowballStemmer('russian')\n    return ' '.join(list(map(stemmer.stem, words)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text = list(map(cleanText, text))\nfrom collections import Counter\ndef counter_word(t):\n    count=Counter()\n    for i in t:\n        for word in i.split():\n            count[word]+=1\n    return count\nlen(counter_word(text))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(text, target, test_size=0.2, stratify=target, shuffle = True, random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dictionary_size = len(counter_word(text))\ntokenizer = Tokenizer(num_words=dictionary_size)\n\ntokenizer.fit_on_texts(X_train)\n\nX_train_tokenized_lst = tokenizer.texts_to_sequences(X_train)\nX_test_tokenized_lst  = tokenizer.texts_to_sequences(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_comment_length = 300\nX_trained = pad_sequences(X_train_tokenized_lst, maxlen=max_comment_length)\nX_tested =  pad_sequences(X_test_tokenized_lst, maxlen=max_comment_length )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_features =len(counter_word(text))\nembedding_dim =16\nsequence_length = 300\n\nmodel = Sequential()\nmodel.add(Embedding(max_features, embedding_dim, input_length=sequence_length,\\\n                                    embeddings_regularizer = l2(0.005))) ####обьявление dim\nmodel.add(Dropout(0.4))\nmodel.add(LSTM(embedding_dim,dropout=0.2, recurrent_dropout=0.2,return_sequences=True,\\\n                                                             kernel_regularizer=l2(0.005),\\\n                                                             bias_regularizer=l2(0.005)))\nmodel.add(Flatten())\nmodel.add(Dense(512, activation='relu',\\\n                                kernel_regularizer=l2(0.001),\\\n                                bias_regularizer=l2(0.001),))\nmodel.add(Dropout(0.4))\n\nmodel.add(Dense(8, activation='relu',\\\n                                kernel_regularizer=l2(0.001),\\\n                                bias_regularizer=l2(0.001),))\nmodel.add(Dropout(0.4))\n\n\nmodel.add(Dense(1,activation='sigmoid'))\n                               \n\n\n\nmodel.summary()\nmodel.compile(loss='binary_crossentropy',optimizer=Adam(1e-3),metrics=['accuracy'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs = 100\n\nhistory = model.fit(X_trained, y_train, epochs=epochs,validation_data=(X_tested,y_test), batch_size=2048)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = model.predict(X_tested,verbose=1)\nprint(predictions[:10])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test[:10]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'Total accuracy  {np.sum((np.round(predictions).flatten()))/sum(y_test)*100}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}