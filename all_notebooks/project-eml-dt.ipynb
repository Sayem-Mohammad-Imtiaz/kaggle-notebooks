{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\") \nimport pandas as pd\nimport calendar\nimport numpy as np\n\nimport random\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import LabelEncoder\nfrom deap import creator, base, tools, algorithms\nimport sys","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def week2month(hep):\n    hep['LastDayWeek'] = pd.to_datetime((hep['week']-1).astype(str) + \"6\", format=\"%Y%U%w\")\n    hep['MonthMax'] = pd.DatetimeIndex(hep['LastDayWeek']).month\n    hep['Year'] = pd.DatetimeIndex(hep['LastDayWeek']).year\n    hep['MonthName'] = [calendar.month_name[i] for i in hep.MonthMax]\n    return hep","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_input_transform(file_):\n    return week2month(pd.read_csv(file_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# load the dataset from Kaggle\nhep = get_input_transform('../input/contagious-diseases/hepatitis.csv')\nmea = get_input_transform('../input/contagious-diseases/measles.csv')\nmum = get_input_transform('../input/contagious-diseases/mumps.csv')\nper = get_input_transform('../input/contagious-diseases/pertussis.csv')\npol = get_input_transform('../input/contagious-diseases/polio.csv')\nrub = get_input_transform('../input/contagious-diseases/rubella.csv')\nsma = get_input_transform('../input/contagious-diseases/smallpox.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for our exploratory purpose we examine data for 1960 through to 2011 for US states\n# combining all the disease datasets is shown below\n\ntrain_data = hep\nfor i in [mea,mum,per,pol,rub,sma]:\n    train_data = train_data.append(i)\ntrain_data = train_data.loc[(train_data['Year'] >= 1960) & (train_data['Year'] <=2011)]\n\n# examine the dataset \ntrain_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# some data discrepancies must be resolved (issues like \\\\N )\ntrain_data_bad = train_data[train_data.cases==train_data.cases.astype(str).max()]\nprint(train_data_bad.head(10))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ind = list(train_data_bad.index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = train_data.drop(train_data.index[ind])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# will be using visulisation.csv for our visualization purposes later\n\ntrain_data.to_csv('for_visulisation.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# examining the characteristics of the dataset\ntrain_data.describe()\ntrain_data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# converting non-numerical data to lowercase (to keep consistencies over all future datasets included)\n\ntrain_data.state_name = [i.lower() for i in train_data.state_name]\ntrain_data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"join_tavg = pd.read_csv('../input/temperature/tavg_data.csv')\njoin_tavg.state_name = [i.lower() for i in join_tavg.state_name]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"join_tavg.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result = pd.merge(train_data, join_tavg, on=['week', 'state_name'])\ntrain_data = result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"join_pcp = pd.read_csv(\"../input/precipitate/pcp.csv\")\njoin_pcp.state_name = [i.lower() for i in join_pcp.state_name]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result = pd.merge(train_data, join_pcp, on=['week', 'state_name'])\ntrain_data = result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler as mm\n\nscaler = mm()\n\n# avg temp deviations\nscaler.fit(train_data['tavg_anomaly'].values.reshape(-1,1))\ntrain_data['tavg_anomaly'] = scaler.transform(train_data['tavg_anomaly'].values.reshape(-1,1))\n\n# avg temp\nscaler.fit(train_data['t_avg'].values.reshape(-1,1))\ntrain_data['t_avg'] = scaler.transform(train_data['t_avg'].values.reshape(-1,1))\n\n# precipitation deviations\npcp_anomaly = scaler.fit_transform(train_data['pcp_anomaly'].values.reshape(-1,1))\ntrain_data['pcp_anomaly'] = pcp_anomaly\n\n# precipitation\nscaler.fit(train_data['precipitate'].values.reshape(-1,1))\ntrain_data['precipitate'] = scaler.transform(train_data['precipitate'].values.reshape(-1,1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.to_csv('result.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.duplicated(subset=None, keep='first').sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# remove the useless attributes from the dataset \n\n# store unique state_name and diseases for label encoding (dont drop them!)\nstates = np.asarray(train_data.state_name.unique())\n#dis = np.unique(train_data['disease'].values)\n\nweek       = train_data.pop('week')\nLOW        = train_data.pop('LastDayWeek')\nmonthN     = train_data.pop('MonthName')\nstate     = train_data.pop('state')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = train_data.pop('disease')\nX = train_data\nprint(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check dimensions\n\nprint(X.shape)\nprint(y.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# label encoding of useful non-numerical attributes\n\nfrom sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\nle.fit(y)\n\ny = le.transform(y)\n\nle2 = LabelEncoder()\nle2.fit(states)\n\nX['state_name'] = le2.transform(X.state_name)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Genetic Algorithm for feature selection"},{"metadata":{"trusted":true},"cell_type":"code","source":"def avg(l):\n    \"\"\"\n    Returns the average between list elements\n    \"\"\"\n    return (sum(l)/float(len(l)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def getFitness(individual, X, y):\n    \"\"\"\n    Feature subset fitness function\n    \"\"\"\n\n    if(individual.count(0) != len(individual)):\n        # get index with value 0\n        cols = [index for index in range(\n            len(individual)) if individual[index] == 0]\n\n        # get features subset\n        X_parsed = X.drop(X.columns[cols], axis=1)\n        X_subset = pd.get_dummies(X_parsed)\n\n        # apply classification algorithm\n        clf = DecisionTreeClassifier(max_depth=10)\n        #clf = SVC()\n\n        return (avg(cross_val_score(clf, X_subset, y, cv=5)),)\n    else:\n        return(0,)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def geneticAlgorithm(X, y, n_population, n_generation):\n    \"\"\"\n    Deap global variables\n    Initialize variables to use eaSimple\n    \"\"\"\n    # create individual\n    creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n    creator.create(\"Individual\", list, fitness=creator.FitnessMax)\n\n    # create toolbox\n    toolbox = base.Toolbox()\n    toolbox.register(\"attr_bool\", random.randint, 0, 1)\n    toolbox.register(\"individual\", tools.initRepeat,\n                     creator.Individual, toolbox.attr_bool, len(X.columns))\n    toolbox.register(\"population\", tools.initRepeat, list,\n                     toolbox.individual)\n    toolbox.register(\"evaluate\", getFitness, X=X, y=y)\n    toolbox.register(\"mate\", tools.cxOnePoint)\n    toolbox.register(\"mutate\", tools.mutFlipBit, indpb=0.05)\n    toolbox.register(\"select\", tools.selTournament, tournsize=3)\n\n    # initialize parameters\n    pop = toolbox.population(n=n_population)\n    hof = tools.HallOfFame(n_population * n_generation)\n    stats = tools.Statistics(lambda ind: ind.fitness.values)\n    stats.register(\"avg\", np.mean)\n    stats.register(\"min\", np.min)\n    stats.register(\"max\", np.max)\n\n    # genetic algorithm\n    pop, log = algorithms.eaSimple(pop, toolbox, cxpb=0.5, mutpb=0.2,\n                                   ngen=n_generation, stats=stats, halloffame=hof,\n                                   verbose=True)\n\n    # return hall of fame\n    return hof","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def bestIndividual(hof, X, y):\n    \"\"\"\n    Get the best individual\n    \"\"\"\n    maxAccurcy = 0.0\n    for individual in hof:\n        ind = individual.fitness.values\n        if(ind[0] > maxAccurcy):\n            maxAccurcy = ind[0]\n            _individual = individual\n\n    _individualHeader = [list(X)[i] for i in range(\n        len(_individual)) if _individual[i] == 1]\n    return _individual.fitness.values, _individual, _individualHeader","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def getArguments():\n    \"\"\"\n    Get argumments from command-line\n    If pass only dataframe path, pop and gen will be default\n    \"\"\"\n    dfPath = sys.argv[1]\n    if(len(sys.argv) == 4):\n        pop = int(sys.argv[2])\n        gen = int(sys.argv[3])\n    else:\n        pop = 10\n        gen = 2\n    return dfPath, pop, gen\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if __name__ == '__main__':\n    # get dataframe path, population number and generation number from command-line argument \n    n_pop = 10\n    n_gen = 15\n    # read dataframe from csv\n    #df = pd.read_csv(dataframePath, sep=',')\n\n    # encode labels column to numbers\n    #le = LabelEncoder()\n    #le.fit(df.iloc[:, -1])\n    #y = le.transform(df.iloc[:, -1])\n    #X = df.iloc[:, :-1]\n\n    # get accuracy with all features\n    individual = [1 for i in range(len(X.columns))]\n    print(\"Accuracy with all features: \\t\" +\n          str(getFitness(individual, X, y)) + \"\\n\")\n\n    # apply genetic algorithm\n    hof = geneticAlgorithm(X, y, n_pop, n_gen)\n\n    # select the best individual\n    accuracy, individual, header = bestIndividual(hof, X, y)\n    print('Best Accuracy: \\t' + str(accuracy))\n    print('Number of Features in Subset: \\t' + str(individual.count(1)))\n    print('Individual: \\t\\t' + str(individual))\n    print('Feature Subset\\t: ' + str(header))\n\n    print('\\n\\ncreating a new classifier with the result')\n\n    # read dataframe from csv one more time\n    #df = pd.read_csv(dataframePath, sep=',')\n\n    # with feature subset\n    X = X[header]\n\n    clf = DecisionTreeClassifier(max_depth=10)\n    #clf = SVC()\n\n    scores = cross_val_score(clf, X, y, cv=5)\n    print(\"Accuracy with Feature Subset: \\t\" + str(avg(scores)) + \"\\n\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pip install chart_studio","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# using plotly for the beautiful plots \n\nimport chart_studio\nimport pandas as pd\n\n# login api for plotly (dont forget to sign up to plotly)\nchart_studio.tools.set_credentials_file(username= 'ab-bh', api_key ='KeUFpD51Wy55BOfM9Czx')\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd \nimport numpy as np\nimport plotly.offline as py\n\ndef get_viz(the_yr_data, yr):\n    py.init_notebook_mode(connected=True)\n\n\n    for col in the_yr_data.columns:\n        the_yr_data[col] = the_yr_data[col].astype(str)\n\n    scl = [[0.0, 'rgb(242,240,247)'],[0.2, 'rgb(218,218,235)'],[0.4, 'rgb(188,189,220)'],\\\n                [0.6, 'rgb(158,154,200)'],[0.8, 'rgb(117,107,177)'],[1.0, 'rgb(84,39,143)']]\n    scl = [\n            # Let first 10% (0.1) of the values have color rgb(0, 0, 0)\n            [0, 'rgb(0, 0, 0)'],\n            [0.1, 'rgb(0, 0, 0)'],\n\n            # Let values between 10-20% of the min and max of z\n            # have color rgb(20, 20, 20)\n            [0.1, 'rgb(20, 20, 20)'],\n            [0.2, 'rgb(20, 20, 20)'],\n\n            # Values between 20-30% of the min and max of z\n            # have color rgb(40, 40, 40)\n            [0.2, 'rgb(40, 40, 40)'],\n            [0.3, 'rgb(40, 40, 40)'],\n\n            [0.3, 'rgb(60, 60, 60)'],\n            [0.4, 'rgb(60, 60, 60)'],\n\n            [0.4, 'rgb(80, 80, 80)'],\n            [0.5, 'rgb(80, 80, 80)'],\n\n            [0.5, 'rgb(100, 100, 100)'],\n            [0.6, 'rgb(100, 100, 100)'],\n\n            [0.6, 'rgb(120, 120, 120)'],\n            [0.7, 'rgb(120, 120, 120)'],\n\n            [0.7, 'rgb(140, 140, 140)'],\n            [0.8, 'rgb(140, 140, 140)'],\n\n            [0.8, 'rgb(160, 160, 160)'],\n            [0.9, 'rgb(160, 160, 160)'],\n\n            [0.9, 'rgb(180, 180, 180)'],\n            [1.0, 'rgb(180, 180, 180)']\n        ]\n    data = [ dict(\n            type='choropleth',\n            colorscale = scl,\n            autocolorscale = True,\n            locations = the_yr_data['state'],\n            z = the_yr_data['cases'].astype(float),\n            zmin=0,\n            zmax=500,\n            locationmode = 'USA-states',\n            text = the_yr_data['text'],\n            marker = dict(\n                line = dict (\n                    color = 'rgb(255,255,255)',\n                    width = 2\n                )\n            ),\n            colorbar = dict(\n                title = \"Disease outbreak - cases in %d\" %(yr)\n            )\n        ) ]\n\n    layout = dict(\n        title = '%d US Diseases Cases Found by State<br>(Hover for breakdown)' %(yr),\n        geo = dict(\n            scope='usa',\n            projection=dict( type='albers usa' ),\n            showlakes = True,\n            lakecolor = 'rgb(255, 255, 255)',\n            ),\n    )\n\n    fig = dict( data=data, layout=layout )\n\n    url = py.iplot( fig, validate=False)\n\ndef get_1yr_viz(yr):\n    data = pd.read_csv('for_visulisation.csv')\n    the_yr_data = data.loc[data['Year'] ==yr]\n    from collections import defaultdict\n    har = defaultdict(set)\n    a = list(the_yr_data.state_name)\n    b = list(the_yr_data.disease)\n    #print len(a),len(b)\n    for i in range(len(a)):\n        har[a[i]].add(b[i])\n    \n    the_yr_data['disease_all'] = [' '.join(list(har[i])) for i in the_yr_data.state_name]\n    \n    the_yr_data['text'] = the_yr_data['state_name'] + '<br>' +\\\n    'Disease '+the_yr_data['disease_all']\n    \n    tf = the_yr_data.filter(['state_name','state','cases'], axis=1)\n    tf.cases = tf.cases.astype(int)\n    the_yr_data_2 = tf.groupby(['state_name','state']).sum().reset_index()\n    the_yr_data_2['disease_all'] = [' '.join(list(har[i])) for i in the_yr_data_2.state_name]\n    the_yr_data_2['text'] = the_yr_data_2['state_name'] + '<br>' +\\\n    'Disease '+the_yr_data_2['disease_all']\n    \n    get_viz(the_yr_data_2, yr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_1yr_viz(2011)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_1yr_viz(1970)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}