{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"* > # EE226 - Coding 2\n## Streaming algorithm & Locality Sensitive Hashing","metadata":{}},{"cell_type":"markdown","source":"### Streaming: DGIM","metadata":{}},{"cell_type":"markdown","source":"DGIM is an efficient algorithm in processing large streams. When it's infeasible to store the flowing binary stream, DGIM can estimate the number of 1-bits in the window. In this coding, you're given the *stream_data.txt* (binary stream), and you need to implement the DGIM algorithm to count the number of 1-bits. Write code and ask the problems below.","metadata":{}},{"cell_type":"markdown","source":"### Your task","metadata":{}},{"cell_type":"markdown","source":"1. Set the window size to 1000, and count the number of 1-bits in the current window.","metadata":{}},{"cell_type":"code","source":"# let current time be 1e4, starting from 1\nimport time\n \nbucket_n = [] #bucket list\nn_max_bucket = 3 # or any other int\nsize_window = 1000\ntime_location = 3000\n\ndef Is_due(time_now):\n    if len(bucket_n)>0 and time_now-size_window==bucket_n[0]['timestamp']:\n        del bucket_n[0]\n        \ndef Merge():\n    for i in range(len(bucket_n)-1,n_max_bucket-1,-1):\n        if bucket_n[i]['bit_sum']==bucket_n[i-n_max_bucket]['bit_sum']:\n            bucket_n[i-n_max_bucket]['bit_sum']+=bucket_n[i-n_max_bucket+1]['bit_sum']\n            bucket_n[i-n_max_bucket]['timestamp']=bucket_n[i-n_max_bucket+1]['timestamp']\n            del bucket_n[i-n_max_bucket+1]\n            \ndef Count_bit():\n    bit_sum=0\n    flag_half=1\n    start_time=time.time()\n    with open('../input/coding2/stream_data.txt', 'r') as f:\n        for i in range(time_location):\n            temp = f.read(2)[0]\n            if temp:\n                Is_due(i+1)\n                if temp == '1':\n                    bucket={\"timestamp\":i+1,\"bit_sum\":1}\n                    bucket_n.append(bucket)\n                    Merge()\n    for i in range(len(bucket_n)):\n        bit_sum+=bucket_n[i]['bit_sum']\n    bit_sum-=bucket_n[0]['bit_sum']/2\n    return bit_sum if len(bucket_n)>0 else 0,time.time()-start_time\n\nbit_sum,bit_time = Count_bit()\nprint(int(bit_sum),bit_time)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"2. Write a function that accurately counts the number of 1-bits in the current window, and compare the difference between its running time and space and the DGIM algorithm.","metadata":{}},{"cell_type":"code","source":"def Count_act():\n    bit_sum=0\n    start_time=time.time()\n    with open('../input/coding2/stream_data.txt', 'r') as f:\n        f.seek(0 if time_location<=size_window else 2*(time_location-size_window))\n        for i in range(time_location if time_location<=size_window else size_window):\n            temp=f.read(2)[0]\n            if temp and temp == '1':\n                bit_sum+=1\n    return bit_sum,time.time()-start_time\n\nbit_act_sum,bit_act_time=Count_act()\nprint(bit_act_sum,bit_act_time)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Locality Sensitive Hashing","metadata":{}},{"cell_type":"markdown","source":"The locality sensitive hashing (LSH) algorithm is efficient in near-duplicate document detection. In this coding, you're given the *docs_for_lsh.csv*, where the documents are processed into set of k-shingles (k = 8, 9, 10). *docs_for_lsh.csv* contains 201 columns, where column 'doc_id' represents the unique id of each document, and from column '0' to column '199', each column represents a unique shingle. If a document contains a shingle ordered with **i**, then the corresponding row will have value 1 in column **'i'**, otherwise it's 0. You need to implement the LSH algorithm and ask the problems below.","metadata":{}},{"cell_type":"markdown","source":"### Your task","metadata":{}},{"cell_type":"markdown","source":"Use minhash algoirthm to create signature of each document, and find 'the most similar' documents under Jaccard similarity. \nParameters you need to determine:\n1) Length of signature (number of distinct minhash functions) *n*. Recommanded value: n > 20.\n\n2) Number of bands that divide the signature matrix *b*. Recommanded value: b > n // 10.","metadata":{}},{"cell_type":"code","source":"\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\n\ndata = pd.read_csv('../input/coding2/docs_for_lsh.csv')\ndata = np.array(data)\ndata = data[:,1:]\n\ndef signature(data):\n    n = 70\n    b = 80\n\n    row = data.shape[0]\n    col = data.shape[1]\n    #print(row,col)\n    seq = np.arange(col)\n    persig = np.zeros((b,col))\n    for i in range(b):\n        persig[i,:] = np.random.permutation(seq)\n        \n    sig = np.ones((b,row))\n    sig = -1*sig\n\n    for i in tqdm(range(b)):\n        h = persig[i,:]\n        for j in range(row):\n            data_j = data[j,:]\n            min_index = min(h[data_j==1])\n            sig[i,j] = min_index\n    \n    return sig\n\nsig = signature(data)\n#print(sig.shape())\nprint(sig)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import random\nimport hashlib\n\ndef minHash(sigMatrix, b, r):\n\n    hashBuckets = {}\n    \n    begin, end = 0, r\n    \n    count = 0\n    row = sigMatrix.shape[1]\n    while end <= sigMatrix.shape[0]:\n        for colNum in range(row):\n\n            hashObj = hashlib.md5()\n\n            band = str(sigMatrix[begin: begin + r, colNum])\n            hashObj.update(band.encode())\n\n            tag = hashObj.hexdigest()\n\n            if tag not in hashBuckets:\n                hashBuckets[tag] = [colNum]\n            elif colNum not in hashBuckets[tag]:\n                hashBuckets[tag].append(colNum)\n        begin += r\n        end += r\n\n    return hashBuckets\n\nhashBucket = minHash(sig,14,5)\nprint(len(hashBucket))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Problem: For document 0 (the one with id '0'), list the **30** most similar document ids (except document 0 itself). You can valid your results with the [sklearn.metrics.jaccard_score()](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.jaccard_score.html) function.\n\nTips: You can adjust your parameters to hash the documents with similarity *s > 0.8* into the same bucket.","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import jaccard_score\n\nrow = sig.shape[1]\ncnt = np.zeros(sig.shape[1])\nqueryCol = 0\nfor key in hashBucket:\n    if queryCol in hashBucket[key]:\n        for i in hashBucket[key]:\n            cnt[i] += 1\n\nsorted_cnt = np.argsort(cnt)\n\ntop30 = []\n\nfor i in range(30):\n    temp = sorted_cnt[row-i-1]\n    top30.append(temp)\n\nprint('top 30 with jaccard:')\nfor idx in top30:\n    print(idx,jaccard_score(data.T[:,0],data.T[:,idx]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}