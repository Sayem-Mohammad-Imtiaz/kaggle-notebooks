{"cells":[{"metadata":{},"cell_type":"markdown","source":"# <span style=\"color:green\">Predicting House Prices in King County, USA.</span>"},{"metadata":{},"cell_type":"markdown","source":"The task of this chalange is to predict of prices based on other features for beginners (**like me**) in field of machine learning. At the end, different regression models are compared with together and evaluated. This is my first machine learning project :)\n\nThis dataset contains 20 columns as features and one column (price) as target.\n\nThe steps are generally as follows::\n\n1. - **Preprocessing:**\n    * Impot libraries\n    * DataFrame preparation\n    * Data wrangling:\n         * Descriptive statistic\n         * Visualizations\n         * Drop unnecessary features\n         * Handling with outliers\n         \n            \n2. - **Model selection:**\n    * Train-Test Split:\n        * 80% as TrainSet and 20% as TestSet\n    * Scaling:\n        * Using StandardScaler\n    * Regressor selection:\n        * Linear model:\n            * LinearRegression \n            * Ridge\n            * Lasso\n            * ElasticNet\n        * Tree:\n            * DecisionTreeRegressor\n        * Ensemlbe:\n            * RandomForestRegressor\n            * AdaBoostRegressor\n            * GradientBoostingRegressor\n        * Boost:\n            * ExtremeGradientBoostingRegressor (XGBRegressor)\n    * Cross validation\n    * Prediction\n    * Visualization\n    \n3. - **Evaluation:**\n   * Scores comparison\n   * Visualization\n   * Conclusion"},{"metadata":{},"cell_type":"markdown","source":"## <span style=\"color:darkblue\">1-Preprocessing</span>"},{"metadata":{},"cell_type":"markdown","source":"### Importing libraries"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"## Import libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nimport statsmodels.api as sm\n%matplotlib inline\n\n## Ignoring of warnings\ndef warn(*args, **kwargs):\n    pass\nimport warnings\nwarnings.warn = warn\n\n## Setting of print Data\n#np.set_printoptions(formatter={'all':lambda x:'%.3f'%x})\n\n\n## Definition of font size\nSMALL_SIZE = 8\nMEDIUM_SIZE = 10\nBIG_SIZE = 12\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Importing data"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/housesalesprediction/kc_house_data.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Information about formats of the features "},{"metadata":{"trusted":true},"cell_type":"code","source":"## Information about DataSet\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Searching for zero-values"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Zero-Values\ndf.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Visualization"},{"metadata":{},"cell_type":"markdown","source":"### Find out about correlation between features with help oh heatmap. So we can make a decision, which features could be dropping out. The negative crellation score should be droped from DataFrame."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(15, 11))\nsns.heatmap(df.corr(),annot=True, fmt=\".2f\", linewidths=.5, ax=ax,cmap='RdYlGn')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Cleaning of DataFrame form unnecessary features correlation coefficient and scatterplots"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax= plt.subplots(figsize=(27,30), ncols=3, nrows=6)\nsns.scatterplot(x=\"bedrooms\", y=\"price\",data=df, ax=ax[0][0])\nsns.scatterplot(x=\"bathrooms\", y=\"price\",data=df, ax=ax[0][1])\nsns.scatterplot(x=\"sqft_living\", y=\"price\",data=df, ax=ax[0][2])\nsns.scatterplot(x=\"sqft_lot\", y=\"price\",data=df, ax=ax[1][0])\nsns.scatterplot(x=\"floors\", y=\"price\",data=df, ax=ax[1][1])\nsns.scatterplot(x=\"waterfront\", y=\"price\",data=df, ax=ax[1][2])\nsns.scatterplot(x=\"view\", y=\"price\",data=df, ax=ax[2][0])\nsns.scatterplot(x=\"condition\", y=\"price\",data=df, ax=ax[2][1])\nsns.scatterplot(x=\"grade\", y=\"price\",data=df, ax=ax[2][2])\nsns.scatterplot(x=\"sqft_above\", y=\"price\",data=df, ax=ax[3][0])\nsns.scatterplot(x=\"sqft_basement\", y=\"price\",data=df, ax=ax[3][1])\nsns.scatterplot(x=\"yr_built\", y=\"price\",data=df, ax=ax[3][2])\nsns.scatterplot(x=\"yr_renovated\", y=\"price\",data=df, ax=ax[4][0])\nsns.scatterplot(x=\"zipcode\", y=\"price\",data=df, ax=ax[4][1])\nsns.scatterplot(x=\"lat\", y=\"price\",data=df, ax=ax[4][2])\nsns.scatterplot(x=\"long\", y=\"price\",data=df, ax=ax[5][0])\nsns.scatterplot(x=\"sqft_living15\", y=\"price\",data=df, ax=ax[5][1])\nsns.scatterplot(x=\"sqft_lot15\", y=\"price\",data=df, ax=ax[5][2])\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Dropping out the unnecessary features regarding to Correlation coefficient "},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop(['id','date','zipcode','condition','long','sqft_lot15','yr_built','sqft_lot','view','waterfront','yr_renovated'],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **Residual graphs** can be used to analyze whether a linear regression model is appropriate for the data."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Adjustment\nfig, axs = plt.subplots(figsize=(25,55),nrows=5,ncols=2)\n# Features \nsns.residplot(x=df[\"bedrooms\"],y=df['price'],data=df,label='Residual',lowess=True,line_kws={'color': 'red'},ax=axs[0][0])\nsns.residplot(x=df[\"bathrooms\"],y=df['price'],data=df,label='Residual',lowess=True,line_kws={'color': 'red'},ax=axs[0][1])\nsns.residplot(x=df[\"sqft_living\"],y=df['price'],data=df,label='Residual',lowess=True,line_kws={'color': 'red'},ax=axs[1][0])\nsns.residplot(x=df[\"floors\"],y=df['price'],data=df,label='Residual',lowess=True,line_kws={'color': 'red'},ax=axs[1][1])\nsns.residplot(x=df[\"grade\"],y=df['price'],data=df,label='Residual',lowess=True,line_kws={'color': 'red'},ax=axs[2][0])\nsns.residplot(x=df[\"sqft_above\"],y=df['price'],data=df,label='Residual',lowess=True,line_kws={'color': 'red'},ax=axs[2][1])\nsns.residplot(x=df[\"sqft_basement\"],y=df['price'],data=df,label='Residual',lowess=True,line_kws={'color': 'red'},ax=axs[3][0])\nsns.residplot(x=df[\"sqft_above\"],y=df['price'],data=df,label='Residual',lowess=True,line_kws={'color': 'red'},ax=axs[3][1])\nsns.residplot(x=df[\"lat\"],y=df['price'],data=df,label='Residual',lowess=True,line_kws={'color': 'red'},ax=axs[4][0])\nsns.residplot(x=df[\"sqft_living15\"],y=df['price'],data=df,label='Residual',lowess=True,line_kws={'color': 'red'},ax=axs[4][1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Visualization of outliers"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Adjustments\nf,ax=plt.subplots(figsize=(15, 20),nrows=5,ncols=2)\nplt.subplots_adjust(hspace=1)\n## Features\nsns.boxplot(df['price'],data=df,ax=ax[0][0])\nsns.boxplot(df['bedrooms'],data=df,ax=ax[0][1])\nsns.boxplot(df['bathrooms'],data=df,ax=ax[1][0])\nsns.boxplot(df['sqft_living'],data=df,ax=ax[1][1])\nsns.boxplot(df['floors'],data=df,ax=ax[2][0])\nsns.boxplot(df['grade'],data=df,ax=ax[2][1])\nsns.boxplot(df['sqft_above'],data=df,ax=ax[3][0])\nsns.boxplot(df['sqft_basement'],data=df,ax=ax[3][1])\nsns.boxplot(df['lat'],data=df,ax=ax[4][0])\nsns.boxplot(df['sqft_living15'],data=df,ax=ax[4][1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Handling with outliers using IQR.\n\n#### IQR or interquartile range is a measurement of variability built on dividing the dataset into different counties. Counties are broken down in Q1, Q2, and Q3, where Q1is the middle value of the first half of the dataset. Q2 is the median value, and Q3 is the middle value of the second half of the dataset. \n\n#### IQR = Q3-Q1\n\n#### Q1 = df.column.quantile(0.25)\n\n#### Q3 = df.column.quantile(0.75)\n\n\n#### After computing IQR, we calculate the lower limit and upper limit and then simply discard all the values that are less or above the limit and replace them with lower and upper limit consequently."},{"metadata":{"trusted":true},"cell_type":"code","source":"#price\n\n#Q1 & Q2 defination\nQ1 = df['price'].quantile(0.25)\nQ3 = df['price'].quantile(0.75)\nprint('Q1:',Q1)\nprint('Q3: ',Q3)\n\nIQR = Q3-Q1\nprint('IQR: ',IQR)\n\nlower_limit = Q1 - 1.5*IQR\nupper_limit = Q3 + 1.5*IQR\nprint('Lower limit: ',lower_limit)\nprint('Upper limit: ',upper_limit)\n\ndf['price'] = np.where(df['price']>upper_limit,upper_limit,df['price'])\ndf['price'] = np.where(df['price']<lower_limit,lower_limit,df['price'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#bedrooms\n\n#Q1 & Q2 defination\nQ1 = df['bedrooms'].quantile(0.25)\nQ3 = df['bedrooms'].quantile(0.75)\nprint('Q1:',Q1)\nprint('Q3: ',Q3)\n\nIQR = Q3-Q1\nprint('IQR: ',IQR)\n\nlower_limit = Q1 - 1.5*IQR\nupper_limit = Q3 + 1.5*IQR\nprint('Lower limit: ',lower_limit)\nprint('Upper limit: ',upper_limit)\n\ndf['bedrooms'] = np.where(df['bedrooms']>upper_limit,upper_limit,df['bedrooms'])\ndf['bedrooms'] = np.where(df['bedrooms']<lower_limit,lower_limit,df['bedrooms'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#bathrooms\n\n#Q1 & Q2 defination\nQ1 = df['bathrooms'].quantile(0.25)\nQ3 = df['bathrooms'].quantile(0.75)\nprint('Q1:',Q1)\nprint('Q3: ',Q3)\n\nIQR = Q3-Q1\nprint('IQR: ',IQR)\n\nlower_limit = Q1 - 1.5*IQR\nupper_limit = Q3 + 1.5*IQR\nprint('Lower limit: ',lower_limit)\nprint('Upper limit: ',upper_limit)\n\ndf['bathrooms'] = np.where(df['bathrooms']>upper_limit,upper_limit,df['bathrooms'])\ndf['bathrooms'] = np.where(df['bathrooms']<lower_limit,lower_limit,df['bathrooms'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#sqft_living\n\n#Q1 & Q2 defination\nQ1 = df['sqft_living'].quantile(0.25)\nQ3 = df['sqft_living'].quantile(0.75)\nprint('Q1:',Q1)\nprint('Q3: ',Q3)\n\nIQR = Q3-Q1\nprint('IQR: ',IQR)\n\nlower_limit = Q1 - 1.5*IQR\nupper_limit = Q3 + 1.5*IQR\nprint('Lower limit: ',lower_limit)\nprint('Upper limit: ',upper_limit)\n\ndf['sqft_living'] = np.where(df['sqft_living']>upper_limit,upper_limit,df['sqft_living'])\ndf['sqft_living'] = np.where(df['sqft_living']<lower_limit,lower_limit,df['sqft_living'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#floors\n\n#Q1 & Q2 defination\nQ1 = df['floors'].quantile(0.25)\nQ3 = df['floors'].quantile(0.75)\nprint('Q1:',Q1)\nprint('Q3: ',Q3)\n\nIQR = Q3-Q1\nprint('IQR: ',IQR)\n\nlower_limit = Q1 - 1.5*IQR\nupper_limit = Q3 + 1.5*IQR\nprint('Lower limit: ',lower_limit)\nprint('Upper limit: ',upper_limit)\n\ndf['floors'] = np.where(df['floors']>upper_limit,upper_limit,df['floors'])\ndf['floors'] = np.where(df['floors']<lower_limit,lower_limit,df['floors'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#grade\n\n#Q1 & Q2 defination\nQ1 = df['grade'].quantile(0.25)\nQ3 = df['grade'].quantile(0.75)\nprint('Q1:',Q1)\nprint('Q3: ',Q3)\n\nIQR = Q3-Q1\nprint('IQR: ',IQR)\n\nlower_limit = Q1 - 1.5*IQR\nupper_limit = Q3 + 1.5*IQR\nprint('Lower limit: ',lower_limit)\nprint('Upper limit: ',upper_limit)\n\ndf['grade'] = np.where(df['grade']>upper_limit,upper_limit,df['grade'])\ndf['grade'] = np.where(df['grade']<lower_limit,lower_limit,df['grade'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#sqft_above\n\n#Q1 & Q2 defination\nQ1 = df['sqft_above'].quantile(0.25)\nQ3 = df['sqft_above'].quantile(0.75)\nprint('Q1:',Q1)\nprint('Q3: ',Q3)\n\nIQR = Q3-Q1\nprint('IQR: ',IQR)\n\nlower_limit = Q1 - 1.5*IQR\nupper_limit = Q3 + 1.5*IQR\nprint('Lower limit: ',lower_limit)\nprint('Upper limit: ',upper_limit)\n\ndf['sqft_above'] = np.where(df['sqft_above']>upper_limit,upper_limit,df['sqft_above'])\ndf['sqft_above'] = np.where(df['sqft_above']<lower_limit,lower_limit,df['sqft_above'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#sqft_basement\n\n#Q1 & Q2 defination\nQ1 = df['sqft_basement'].quantile(0.25)\nQ3 = df['sqft_basement'].quantile(0.75)\nprint('Q1:',Q1)\nprint('Q3: ',Q3)\n\nIQR = Q3-Q1\nprint('IQR: ',IQR)\n\nlower_limit = Q1 - 1.5*IQR\nupper_limit = Q3 + 1.5*IQR\nprint('Lower limit: ',lower_limit)\nprint('Upper limit: ',upper_limit)\n\ndf['sqft_basement'] = np.where(df['sqft_basement']>upper_limit,upper_limit,df['sqft_basement'])\ndf['sqft_basement'] = np.where(df['sqft_basement']<lower_limit,lower_limit,df['sqft_basement'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#lat\n\n#Q1 & Q2 defination\nQ1 = df['lat'].quantile(0.25)\nQ3 = df['lat'].quantile(0.75)\nprint('Q1:',Q1)\nprint('Q3: ',Q3)\n\nIQR = Q3-Q1\nprint('IQR: ',IQR)\n\nlower_limit = Q1 - 1.5*IQR\nupper_limit = Q3 + 1.5*IQR\nprint('Lower limit: ',lower_limit)\nprint('Upper limit: ',upper_limit)\n\ndf['lat'] = np.where(df['lat']>upper_limit,upper_limit,df['lat'])\ndf['lat'] = np.where(df['lat']<lower_limit,lower_limit,df['lat'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#sqft_living15\n\n#Q1 & Q2 defination\nQ1 = df['sqft_living15'].quantile(0.25)\nQ3 = df['sqft_living15'].quantile(0.75)\nprint('Q1:',Q1)\nprint('Q3: ',Q3)\n\nIQR = Q3-Q1\nprint('IQR: ',IQR)\n\nlower_limit = Q1 - 1.5*IQR\nupper_limit = Q3 + 1.5*IQR\nprint('Lower limit: ',lower_limit)\nprint('Upper limit: ',upper_limit)\n\ndf['sqft_living15'] = np.where(df['sqft_living15']>upper_limit,upper_limit,df['sqft_living15'])\ndf['sqft_living15'] = np.where(df['sqft_living15']<lower_limit,lower_limit,df['sqft_living15'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Adjustments\nf,ax=plt.subplots(figsize=(20, 20),nrows=5,ncols=2)\nplt.subplots_adjust(hspace=1)\n## Features\nsns.boxplot(df['price'],data=df,ax=ax[0][0])\nsns.boxplot(df['bedrooms'],data=df,ax=ax[0][1])\n\nsns.boxplot(df['bathrooms'],data=df,ax=ax[1][0])\nsns.boxplot(df['sqft_living'],data=df,ax=ax[1][1])\n\nsns.boxplot(df['floors'],data=df,ax=ax[2][0])\nsns.boxplot(df['grade'],data=df,ax=ax[2][1])\n\nsns.boxplot(df['sqft_above'],data=df,ax=ax[3][0])\nsns.boxplot(df['sqft_basement'],data=df,ax=ax[3][1])\n\nsns.boxplot(df['lat'],data=df,ax=ax[4][0])\nsns.boxplot(df['sqft_living15'],data=df,ax=ax[4][1])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <span style=\"color:darkblue\">2-Model selection</span>"},{"metadata":{},"cell_type":"markdown","source":"### Different regressors are selected for testing."},{"metadata":{"trusted":true},"cell_type":"code","source":"model = []\nscore = []\n\nmodel_pred = []\nscore_pred = []\n\nx=df.drop(['price'],axis=1)\ny=df['price']\n\n\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor,AdaBoostRegressor,GradientBoostingRegressor\nfrom xgboost import XGBRegressor\n\n\n#Spliting Train/Test --> Generally, we need to split our DataSet into 3 groups: Train, Test and Evaluation!\nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=42)\n\n#Scores\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\nscaler=StandardScaler()\nx_train=scaler.fit_transform(x_train,y_train)\nx_test=scaler.transform(x_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Before the prediction, we first test the models with train-data by cross validation. This way, the overfitting or underfitting can be better observed. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# LinearRegression\n\nLinReg=LinearRegression()\nLinReg.fit(x_train,y_train)\n\nfrom sklearn.model_selection import  cross_val_score,cross_val_predict\ncv_LinReg=cross_val_score(LinReg,x_train,y_train,cv=10)\ncv_LinReg_pred=cross_val_predict(LinReg,x_train,y_train,cv=10)\n\nmodel.append(\"Linear Regression\")\nscore.append(r2_score(y_train,cv_LinReg_pred))\n\n#Visualisation\nfig,ax=plt.subplots(figsize=(10, 5))\nplt.rc('font', size=BIG_SIZE)   \nax=sns.distplot(y_train,hist=False,label='Y-Train',color='r')\nsns.distplot(cv_LinReg_pred,hist=False,label='Pred-CV-Value',color='black',ax=ax)\nplt.title(\"CV-prediction with LinearRegression\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Ridge\nRge=Ridge(alpha=0.1)\nRge.fit(x_train,y_train)\n\nfrom sklearn.model_selection import  cross_val_score,cross_val_predict\ncv_Rge=cross_val_score(Rge,x_train,y_train,cv=10)\ncv_Rge_pred=cross_val_predict(Rge,x_train,y_train,cv=10)\n\nmodel.append(\"Ridge\")\nscore.append(r2_score(y_train,cv_Rge_pred))\n\n#Visualisation\nfig,ax=plt.subplots(figsize=(10, 5))\nplt.rc('font', size=BIG_SIZE)   \nax=sns.distplot(y_train,hist=False,label='Y-Train',color='r')\nsns.distplot(cv_Rge_pred,hist=False,label='Pred-CV-Value',color='black',ax=ax)\nplt.title(\"CV-prediction with Ridge\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##Lasso\n\nLSO=Lasso()\nLSO.fit(x_train,y_train)\n\nfrom sklearn.model_selection import  cross_val_score,cross_val_predict\ncv_LSO=cross_val_score(LSO,x_train,y_train,cv=10)\ncv_LSO_pred=cross_val_predict(LSO,x_train,y_train,cv=10)\n\n#print(\"R2-Score: \",r2_score(y_train,cv_LSO_pred))\nmodel.append(\"Lasso\")\nscore.append(r2_score(y_train,cv_LSO_pred))\n\n#Visualisation\nfig,ax=plt.subplots(figsize=(10, 5))\nplt.rc('font', size=BIG_SIZE)   \nax=sns.distplot(y_train,hist=False,label='Y-Train',color='r')\nsns.distplot(cv_LSO_pred,hist=False,label='Pred-CV-Value',color='black',ax=ax)\nplt.title(\"CV-prediction with LinearRegression\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##ElasticNet\n\nELN=ElasticNet(random_state=42)\nELN.fit(x_train,y_train)\n\nfrom sklearn.model_selection import  cross_val_score,cross_val_predict\ncv_ELN=cross_val_score(ELN,x_train,y_train,cv=10)\ncv_ELN_pred=cross_val_predict(ELN,x_train,y_train,cv=10)\n\nmodel.append(\"ElasticNet\")\nscore.append(r2_score(y_train,cv_ELN_pred)) \n\n#Visalisation\nfig,ax=plt.subplots(figsize=(10, 5))\nplt.rc('font', size=BIG_SIZE)   \nax=sns.distplot(y_train,hist=False,label='Y-Train',color='r')\nsns.distplot(cv_ELN_pred,hist=False,label='Pred-CV-Value',color='black',ax=ax)\nplt.title(\"CV-prediction with ElasticNet\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## DecisionTreeRegressor\n\nDTR=DecisionTreeRegressor()\nDTR.fit(x_train,y_train)\n\nfrom sklearn.model_selection import  cross_val_score,cross_val_predict\ncv_DTR=cross_val_score(DTR,x_train,y_train,cv=10)\ncv_DTR_pred=cross_val_predict(DTR,x_train,y_train,cv=10)\n\nmodel.append(\"DecisionTreeRegressor\")\nscore.append(r2_score(y_train,cv_DTR_pred)) \n\n#Visualisation\nfig,ax=plt.subplots(figsize=(10, 5))\nplt.rc('font', size=BIG_SIZE)   \nax=sns.distplot(y_train,hist=False,label='Y-Train',color='r')\nsns.distplot(cv_DTR_pred,hist=False,label='Pred-CV-Value',color='black',ax=ax)\nplt.title(\"CV-prediction with DecisionTreeRegressor\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## RandomForestRegressor\n\nRFR=RandomForestRegressor(n_estimators=100, random_state = 42)\nRFR.fit(x_train,y_train)\n\nfrom sklearn.model_selection import  cross_val_score,cross_val_predict\ncv_RFR=cross_val_score(RFR,x_train,y_train,cv=10)\ncv_RFR_pred=cross_val_predict(RFR,x_train,y_train,cv=10)\n\nmodel.append(\"RandomForestRegressor\")\nscore.append(r2_score(y_train,cv_RFR_pred)) \n\n#Visualisation\nfig,ax=plt.subplots(figsize=(10, 5))\nplt.rc('font', size=BIG_SIZE)   \nax=sns.distplot(y_train,hist=False,label='Y-Train',color='r')\nsns.distplot(cv_RFR_pred,hist=False,label='Pred-CV-Value',color='black',ax=ax)\nplt.title(\"CV-prediction with RandomForestRegressor\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#AdaBoostRegressor\n\nABR=AdaBoostRegressor(n_estimators=6,random_state=42)\nABR.fit(x_train,y_train)\n\nfrom sklearn.model_selection import  cross_val_score,cross_val_predict\ncv_ABR=cross_val_score(ABR,x_train,y_train,cv=5)\ncv_ABR_pred=cross_val_predict(ABR,x_train,y_train,cv=5)\n\nmodel.append(\"AdaBoostRegressor\")\nscore.append(r2_score(y_train,cv_ABR_pred)) \n\n#Visaulisation\nfig,ax=plt.subplots(figsize=(10, 5))\nplt.rc('font', size=BIG_SIZE)   \nax=sns.distplot(y_train,hist=False,label='Y-Train',color='r')\nsns.distplot(cv_ABR_pred,hist=False,label='Pred-CV-Value',color='black',ax=ax)\nplt.title(\"CV-prediction with AdaBoostRegressor\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#GradientBoostingRegressor\n\nGBR=GradientBoostingRegressor()\nGBR.fit(x_train,y_train)\n\nfrom sklearn.model_selection import  cross_val_score,cross_val_predict\ncv_GBR=cross_val_score(GBR,x_train,y_train,cv=10)\ncv_GBR_pred=cross_val_predict(GBR,x_train,y_train,cv=10)\n\nmodel.append(\"AdaBoostRegressor\")\nscore.append(r2_score(y_train,cv_GBR_pred)) \n\n#Visaulisation\nf,ax3=plt.subplots(figsize=(10, 5))\nax3=sns.distplot(y_train,hist=False,label='Y-Train',color='r')\nsns.distplot(cv_GBR_pred,hist=False,label='Pred-CV-Value',ax=ax3,color='black')\nplt.title(\"CV-prediction with AdaBoostRegressor\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#XGBRegressor\n\nXGBR=XGBRegressor()\nXGBR.fit(x_train,y_train)\n\nfrom sklearn.model_selection import  cross_val_score,cross_val_predict\ncv_XGBR=cross_val_score(XGBR,x_train,y_train,cv=10)\ncv_XGBR_pred=cross_val_predict(XGBR,x_train,y_train,cv=10)\n\nmodel.append(\"XGBRegressor\")\nscore.append(r2_score(y_train,cv_XGBR_pred)) \n\n#Visaulisation\nfig,ax=plt.subplots(figsize=(10, 5))\nplt.rc('font', size=BIG_SIZE)   \nax=sns.distplot(y_train,hist=False,label='Y-Train',color='r')\nsns.distplot(cv_XGBR_pred,hist=False,label='Pred-CV-Value',color='black',ax=ax)\nplt.title(\"CV-prediction with XGBRegressor\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Prediction with test data"},{"metadata":{"trusted":true},"cell_type":"code","source":"#LinearRegression\nLinReg.fit(x_train,y_train)\nLinReg_pred=LinReg.predict(x_test)\n\nmodel_pred.append(\"Linear Regression\")\nscore_pred.append(r2_score(y_test,LinReg_pred))\n\n#Visualisation\nf,ax=plt.subplots(figsize=(10, 5))\nax=sns.distplot(y_test,hist=False,label='Test data',color='r')\nsns.distplot(LinReg_pred,hist=False,label='Pred-Value',color='g',ax=ax)\nplt.title(\"Prediction with test features in LinearRegression\") ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Ridge\n\nRge.fit(x_train,y_train)\nRge_pred=Rge.predict(x_test)\n\nmodel_pred.append(\"Ridge\")\nscore_pred.append(r2_score(y_test,Rge_pred))\n\n#Visualisation\nf,ax3=plt.subplots(figsize=(10, 5))\nax3=sns.distplot(y_test,hist=False,label='Test data',color='r')\nsns.distplot(Rge_pred,hist=False,label='Pred-Value',ax=ax3,color='g')\nplt.title(\"Prediction with test features in Ridge\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Lasso\n\nLSO.fit(x_train,y_train)\nLSO_pred=LSO.predict(x_test)\n\nmodel_pred.append(\"Lasso\")\nscore_pred.append(r2_score(y_test,LSO_pred))\n\n#Visualisation\nf,ax=plt.subplots(figsize=(10, 5))\nax=sns.distplot(y_test,hist=False,label='Test data',color='r')\nsns.distplot(LSO_pred,hist=False,label='Pred-Value',ax=ax,color='g')\nplt.title(\"Prediction with test features in Lasso\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#ElasticNet\n\nELN.fit(x_train,y_train)\nELN_pred=ELN.predict(x_test)\n\nmodel_pred.append(\"ElasticNet\")\nscore_pred.append(r2_score(y_test,ELN_pred))\n\n#Visualisation\nf,ax=plt.subplots(figsize=(10, 5))\nax=sns.distplot(y_test,hist=False,label='Test data',color='r')\nsns.distplot(ELN_pred,hist=False,label='Pred-Value',ax=ax,color='g')\nplt.title(\"Prediction with test features in ElasticNet\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#DecisionTreeRegressor\n\nDTR.fit(x_train,y_train)\nDTR_pred=DTR.predict(x_test)\n\nmodel_pred.append(\"DecisionTreeRegressor\")\nscore_pred.append(r2_score(y_test,DTR_pred))\n\n#Visualisation\nf,ax=plt.subplots(figsize=(10, 5))\nax=sns.distplot(y_test,hist=False,label='Test data',color='r', axlabel='prediction via Lasso in compare with y-test')\nsns.distplot(DTR_pred,hist=False,label='Pred-Value',ax=ax,color='g')\nplt.title(\"Prediction with test features via DecisionTreeRegressor\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#RandomForestRegressor\n\nRFR.fit(x_train,y_train)\nRFR_pred=RFR.predict(x_test)\n\nmodel_pred.append(\"RandomForestRegressor\")\nscore_pred.append(r2_score(y_test,RFR_pred))\n\n#Visualisation\nf,ax=plt.subplots(figsize=(10, 5))\nax=sns.distplot(y_test,hist=False,label='Test data',color='r')\nsns.distplot(RFR_pred,hist=False,label='Pred-Value',ax=ax,color='g')\nplt.title(\"Prediction with test features via RandomForestRegressor\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#AdaBoostRegressor\nABR.fit(x_train,y_train)\nABR_pred=ABR.predict(x_test)\n\nmodel_pred.append(\"AdaBoostRegressor\")\nscore_pred.append(r2_score(y_test,ABR_pred))\n\n#Visualisation\nf,ax=plt.subplots(figsize=(10, 5))\nax=sns.distplot(y_test,hist=False,label='Test data',color='r')\nsns.distplot(ABR_pred,hist=False,label='Pred-Value',ax=ax,color='g')\nplt.title(\"Prediction with test features via AdaBoostRegressor\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#GradientBoostingRegressor\n\nGBR.fit(x_train,y_train)\nGBR_pred=GBR.predict(x_test)\n\nmodel_pred.append(\"GradientBoostingRegressor\")\nscore_pred.append(r2_score(y_test,GBR_pred))\n\n#Visualisation\nf,ax=plt.subplots(figsize=(10, 5))\nax=sns.distplot(y_test,hist=False,label='Test data',color='r', axlabel='prediction via Lasso in compare with y-test')\nsns.distplot(GBR_pred,hist=False,label='Pred-Value',ax=ax,color='g')\nplt.title(\"Prediction with test features via GradientBoostingRegressor\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#XGBRegressor\n\nXGBR.fit(x_train,y_train)\nXGBR_pred=GBR.predict(x_test)\n\nmodel_pred.append(\"XGBRegressor\")\nscore_pred.append(r2_score(y_test,XGBR_pred))\n\n#Visualisation\nf,ax=plt.subplots(figsize=(10, 5))\nplt.rc('font', size=BIG_SIZE)  \nax=sns.distplot(y_test,hist=False,label='Test data',color='r', axlabel='prediction via Lasso in compare with y-test')\nsns.distplot(XGBR_pred,hist=False,label='Pred-Value',ax=ax,color='g')\nplt.title(\"Prediction with test features via XGBRegressor\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <span style=\"color:darkblue\">3-Evaluation</span>"},{"metadata":{},"cell_type":"markdown","source":"### It will now compare the scores of CV and Prediction with aader to be selected the best regressor."},{"metadata":{"trusted":true},"cell_type":"code","source":"## R2-Score in CV and MSE in test comparision\nprint('The comparision of R2-Score between of predictions in CV with train data and predictions with test data:')\nprint('')\n\nprint('R2-Score of CV:    \\t\\t\\t\\t R2-Score with test data:')\nprint('LinearRegression: %8.3f \\t\\t\\t LinearRegression: %8.3f'%(r2_score(y_train,cv_LinReg_pred),r2_score(y_test,LinReg_pred)))       \nprint('RidgeRegression:  %8.3f \\t\\t\\t RidgeRegression:  %8.3f'%(r2_score(y_train,cv_Rge_pred),r2_score(y_test,Rge_pred)))  \nprint('LassoRegression:  %8.3f \\t\\t\\t LassoRegression:  %8.3f'%(r2_score(y_train,cv_LSO_pred),r2_score(y_test,LSO_pred)))\nprint('ElasticNet:       %8.3f \\t\\t\\t ElasticNet:       %8.3f'%(r2_score(y_train,cv_ELN_pred),r2_score(y_test,ELN_pred)))\nprint('DecisionTree:     %8.3f \\t\\t\\t DecisionTree:     %8.3f'%(r2_score(y_train,cv_DTR_pred),r2_score(y_test,DTR_pred))) \nprint('RandomForest:     %8.3f \\t\\t\\t RandomForest:     %8.3f'%(r2_score(y_train,cv_RFR_pred),r2_score(y_test,RFR_pred))) \nprint('AdaBoost:         %8.3f \\t\\t\\t AdaBoost:         %8.3f'%(r2_score(y_train,cv_ABR_pred),r2_score(y_test,ABR_pred))) \nprint('GradientBoosting: %8.3f \\t\\t\\t GradientBoosting: %8.3f'%(r2_score(y_train,cv_ABR_pred),r2_score(y_test,ABR_pred))) \nprint('XGBRegressorg:    %8.3f \\t\\t\\t XGBRegressor:     %8.3f'%(r2_score(y_train,cv_XGBR_pred),r2_score(y_test,XGBR_pred))) \n\n# Predictions of Cross Validation\nplt.subplots(figsize=(15, 5))\nplt.rc('font', size=BIG_SIZE)  \nsns.barplot(x=score,y=model,palette = sns.cubehelix_palette(len(score),rot=21))\nplt.xlabel(\"R2-Score\")\nplt.ylabel(\"Regression\")\nplt.title('R2- Score in cross validation')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualisation of predictions in CV with 10 fold\nplt.subplots(figsize=(11, 3))\nsns.barplot(x=score_pred,y=model_pred,palette = sns.cubehelix_palette(len(score_pred),rot=0.9))\nplt.xlabel(\"Score\")\nplt.ylabel(\"Regression\")\nplt.title('R2-Score with test values')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#MSE in CV and MSE in test comparision\nprint('The comparision of MSE between of predictions in CV with train data and predictions with test data:')\nprint('')\n\nprint('Predictions via CV:   \\t\\t\\t         Predictions with test data:')\nprint('LinearRegression: %d \\t\\t\\t LinearRegression: %d'%(mean_squared_error(y_train,cv_LinReg_pred),mean_squared_error(y_test,LinReg_pred)))       \nprint('RidgeRegression:  %d \\t\\t\\t RidgeRegression:  %d'%(mean_squared_error(y_train,cv_Rge_pred),mean_squared_error(y_test,Rge_pred)))  \nprint('LassoRegression:  %d \\t\\t\\t LassoRegression:  %d'%(mean_squared_error(y_train,cv_LSO_pred),mean_squared_error(y_test,LSO_pred)))\nprint('ElasticNet:       %d \\t\\t\\t ElasticNet:       %d'%(mean_squared_error(y_train,cv_ELN_pred),mean_squared_error(y_test,ELN_pred)))\nprint('DecisionTree:     %d \\t\\t\\t DecisionTree:     %d'%(mean_squared_error(y_train,cv_DTR_pred),mean_squared_error(y_test,DTR_pred))) \nprint('RandomForest:     %d \\t\\t\\t RandomForest:     %d'%(mean_squared_error(y_train,cv_RFR_pred),mean_squared_error(y_test,RFR_pred))) \nprint('AdaBoost:         %d \\t\\t\\t AdaBoost:         %d'%(mean_squared_error(y_train,cv_ABR_pred),mean_squared_error(y_test,ABR_pred))) \nprint('GradientBoosting: %d \\t\\t\\t GradientBoosting: %d'%(mean_squared_error(y_train,cv_ABR_pred),mean_squared_error(y_test,ABR_pred))) \nprint('XGBRegressorg:    %d \\t\\t\\t XGBRegressor:     %d'%(mean_squared_error(y_train,cv_XGBR_pred),mean_squared_error(y_test,XGBR_pred))) ","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"MSE=[]\nRegressor=[]\n\nMSE.append(mean_squared_error(y_test,LinReg_pred)),Regressor.append('LinearRegression')\nMSE.append(mean_squared_error(y_test,Rge_pred)),Regressor.append('Ridge')\nMSE.append(mean_squared_error(y_test,LSO_pred)),Regressor.append('Lasso')\nMSE.append(mean_squared_error(y_test,ELN_pred)),Regressor.append('ElasticNet')\nMSE.append(mean_squared_error(y_test,DTR_pred)),Regressor.append('DecisionTreeRegressor')\nMSE.append(mean_squared_error(y_test,RFR_pred)),Regressor.append('RandomForestRegressor')\nMSE.append(mean_squared_error(y_test,ABR_pred)),Regressor.append('AdaBoostRegressor')\nMSE.append(mean_squared_error(y_test,GBR_pred)),Regressor.append('GradientBoostingRegressor')\nMSE.append(mean_squared_error(y_test,XGBR_pred)),Regressor.append('XGBRegressor')\n\nplt.subplots(figsize=(15, 5))\nplt.rc('font', size=BIG_SIZE)  \nsns.barplot(x=MSE,y=Regressor,palette = sns.cubehelix_palette(len(MSE),rot=2))\nplt.xlabel(\"MSE\")\nplt.ylabel(\"Regression\")\nplt.title('MSE of diffrenet prediction models')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <span style=\"color:darkred\">Conclusion</span>"},{"metadata":{},"cell_type":"markdown","source":"- According to result, The best model can set up RANDOMFOREST regression algorithm. \n- RANDOMFOREST algorithm achieved 82% in cross validation and prediction successful, although the score of XGBRegressor in cross validation is a little bit better.\n- Elastic Net,which is regulaziton regression algorithm, aren't convenient to the dataset. \n- no **feature engineering** and **GridSearch of Scikit-Learn** was used."},{"metadata":{},"cell_type":"markdown","source":"Please upvote my work if you like it :-)"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}