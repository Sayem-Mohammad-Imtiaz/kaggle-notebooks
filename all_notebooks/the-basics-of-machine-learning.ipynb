{"cells":[{"metadata":{"_uuid":"b9c7c30a1c60094b1a0b05c623ece57a7b1147e6"},"cell_type":"markdown","source":"# Is This Mushroom Poisonous? - The basics of Machine Learning\n[Index](https://www.kaggle.com/veleon/in-depth-look-at-machine-learning-models)\n\nEveryone has to start somewhere, usually the beginning. It is no different when you're learning Machine Learning. This Kernel is my attempt at showing the basics of Machine Learning.\n\nWe'll be using the Mushroom Classification dataset to determine if a mushroom we found is poisonous or edible.\n\n<img src=\"https://cdn-images-1.medium.com/max/1600/1*_QGyIwpgq831xI54cIe_GQ.jpeg\" alt=\"ML Process\" width=\"600\"/>\n## Index\n1. Importing Libraries & Data\n2. Data Analysis & Cleaning\n3. Training a Model\n4. Visualize \n\n# 1. Importing Libraries & Data"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns # data visualisation\nsns.set(style=\"darkgrid\")\nimport matplotlib.pyplot as plt # data plotting\n\nimport warnings\nwarnings.simplefilter(\"ignore\") # Remove certain warnings from Machine Learning Models\n\ndata = pd.read_csv('../input/mushrooms.csv')\ndata.head(2)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c8b4e925d16e219fe06be1794d353d6a8dd97c8f"},"cell_type":"markdown","source":"# 2. Data Analysis & Cleaning\nBefore we can train a model to help us determine if a mushroom is poisonous we have to look at what kind of data we have. After that we'll clean the data so it can be used by a model."},{"metadata":{"trusted":true,"_uuid":"bc4ae7563269ec6204a64d4381df3d4538f28f43"},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ea8a92ef30c8ba26d872094334ce10846a93af25"},"cell_type":"markdown","source":"We have a lot of different columns. We'll try to get some insight into what effect they have on the class of a mushroom by making some graphs.\n\nLet's start by looking at the distribution of edible and poisonous mushrooms in our data. As you can see the distribution is almost 50/50, which is very nice when you are going to let a machine learning model use your data. This means it won't have to struggle to find correlations."},{"metadata":{"trusted":true,"_uuid":"802fe6bef541d199dbd77dc20af3ca5bd70ad12f","_kg_hide-input":true},"cell_type":"code","source":"sns.countplot(data['class'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"50f416ff4224c37bb4d08a2687a9e37e9a40f6ad"},"cell_type":"markdown","source":"Most of the cap related columns seem fairly balanced between edible and poisonous. Notable are cap-shape k, cap-surface f and cap-color w for not having a fairly equal distribution."},{"metadata":{"trusted":true,"_uuid":"81718cb6fb6824e94bb8fb84c1e3a2ea3b15bb64","_kg_hide-input":true},"cell_type":"code","source":"fig, ax =plt.subplots(1,3, figsize=(15,5))\nsns.countplot(x=\"cap-shape\", hue='class', data=data, ax=ax[0])\nsns.countplot(x=\"cap-surface\", hue='class', data=data, ax=ax[1])\nsns.countplot(x=\"cap-color\", hue='class', data=data, ax=ax[2])\nfig.tight_layout()\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"99ac05ea871192e85d0a6b05306cca842117d69d"},"cell_type":"markdown","source":"The bruises are a much better indication if a mushroom is poisonous or not. You can clearly see the distribution in the plot. The odors are even more clear, only odor n has both poisonous and edible. Even then it's not really a fair distribution with edible being multiple times larger. These are both good columns to use for determining the class of a mushroom."},{"metadata":{"trusted":true,"_uuid":"30f3e181a3ee3f4749b1fdd00b7990c8d0ac05b8","_kg_hide-input":true},"cell_type":"code","source":"fig, ax =plt.subplots(1,2, figsize=(15,5))\nsns.countplot(x=\"bruises\", hue='class', data=data, ax=ax[0])\nsns.countplot(x=\"odor\", hue='class', data=data, ax=ax[1])\nfig.tight_layout()\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4d2c716b0402625375ebe2f554a8293e0dfad09a"},"cell_type":"markdown","source":"Gills are a bit more evenly distributed again though there are some outliers in spacing, size and color"},{"metadata":{"trusted":true,"_uuid":"e7ef3ab42439e4d0765c12202637fd246d856d00","_kg_hide-input":true},"cell_type":"code","source":"fig, ax =plt.subplots(1,4, figsize=(20,5))\nsns.countplot(x=\"gill-attachment\", hue='class', data=data, ax=ax[0])\nsns.countplot(x=\"gill-spacing\", hue='class', data=data, ax=ax[1])\nsns.countplot(x=\"gill-size\", hue='class', data=data, ax=ax[2])\nsns.countplot(x=\"gill-color\", hue='class', data=data, ax=ax[3])\nfig.tight_layout()\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"79f01825b595b665451f7c191fa98d6922eb0983"},"cell_type":"markdown","source":"Again, the stalk columns are fairly evenly distributed. There are some outliers in surface and color that could be usefull for classification."},{"metadata":{"trusted":true,"_uuid":"6274bef6f8aa2d3d2887aae2bc436a57a6c0d7c5","_kg_hide-input":true},"cell_type":"code","source":"fig, ax =plt.subplots(2,3, figsize=(20,10))\nsns.countplot(x=\"stalk-shape\", hue='class', data=data, ax=ax[0,0])\nsns.countplot(x=\"stalk-root\", hue='class', data=data, ax=ax[0,1])\nsns.countplot(x=\"stalk-surface-above-ring\", hue='class', data=data, ax=ax[0,2])\nsns.countplot(x=\"stalk-surface-below-ring\", hue='class', data=data, ax=ax[1,0])\nsns.countplot(x=\"stalk-color-above-ring\", hue='class', data=data, ax=ax[1,1])\nsns.countplot(x=\"stalk-color-below-ring\", hue='class', data=data, ax=ax[1,2])\nfig.tight_layout()\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f4a62149431da267ef45ac02d22c49e5122531dd"},"cell_type":"markdown","source":"Most of these columns are not very interesting, but ring-type has some useful information."},{"metadata":{"trusted":true,"_uuid":"2ecf11c4c8d022a653bfeefbf7d0a9c14d17a780","_kg_hide-input":true},"cell_type":"code","source":"fig, ax =plt.subplots(2,2, figsize=(15,10))\nsns.countplot(x=\"veil-type\", hue='class', data=data, ax=ax[0,0])\nsns.countplot(x=\"veil-color\", hue='class', data=data, ax=ax[0,1])\nsns.countplot(x=\"ring-number\", hue='class', data=data, ax=ax[1,0])\nsns.countplot(x=\"ring-type\", hue='class', data=data, ax=ax[1,1])\nfig.tight_layout()\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dea7fbd9a9b0276da4c65e0eb90f4312cb5b05cb"},"cell_type":"markdown","source":"These last columns have some stronger outliers we can use for prediction. especially spore print color."},{"metadata":{"trusted":true,"_uuid":"e0dcf04d13e778778e7b2ff92223b52061adf2bc","_kg_hide-input":true},"cell_type":"code","source":"fig, ax =plt.subplots(1,3, figsize=(20,5))\nsns.countplot(x=\"spore-print-color\", hue='class', data=data, ax=ax[0])\nsns.countplot(x=\"population\", hue='class', data=data, ax=ax[1])\nsns.countplot(x=\"habitat\", hue='class', data=data, ax=ax[2])\nfig.tight_layout()\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3e1ac910017f6c5b0ed21a6456b390d9b7cd279e"},"cell_type":"markdown","source":""},{"metadata":{"_uuid":"a96c93b37644e4392d0d8138ed8f89060c72df48"},"cell_type":"markdown","source":"Now that we know what our data looks like we can clean it. Let's start by turning the columns with only 2 different values into Booleans."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# Make column class True/False for isPoisonous\ndata['class'].replace('p', 1, inplace = True)\ndata['class'].replace('e', 0, inplace = True)\n\n# Bruises: t = True / f = False\ndata['bruises'].replace('t', 1, inplace = True)\ndata['bruises'].replace('f', 0, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3f4b25916b3887b518590f7d992e4c3fbbb5e33d"},"cell_type":"markdown","source":"Our machine learning models can't read characters (only integers and floats). So we'll have to make a column for every unique value. Pandas has a function for this, named get_dummies. Our DataFrame looks a bit different now."},{"metadata":{"trusted":true,"_uuid":"12d03244bd68a6af94430cc604aab1f0ce11949e"},"cell_type":"code","source":"# Encode the rest of the string data\ndata = pd.get_dummies(data)\n\npd.set_option(\"display.max_columns\",200)\ndata.head(5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3e4c7b5bbc592979431378fdb743985c0dbc0b2e"},"cell_type":"markdown","source":"Let's make lists with the columns so we can make some correlation heatmaps."},{"metadata":{"trusted":true,"_uuid":"9adf66a023267992bcffcb67d746ddf7ca4428cf"},"cell_type":"code","source":"Target = ['class']\nbruisesColumn = ['bruises']\ncapColumns = list(data.columns[2:22])\nodorColumns = list(data.columns[22:31])\ngillColumns = list(data.columns[31:49])\nstalkColumns = list(data.columns[49:82])\nveilColumns = list(data.columns[82:87])\nringColumns = list(data.columns[87:95])\nsporeColumns = list(data.columns[95:104])\npopulationColumns = list(data.columns[104: 110])\nhabitatColumns = list(data.columns[110:117])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"87a118891a79f532ee5064ee90e568c6ef32ec7a","_kg_hide-input":true},"cell_type":"code","source":"plt.subplots(figsize=(10,10))\nsns.heatmap(data[Target+odorColumns].corr(), annot=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d6a02bfa1760a6c4296f5638eee2fb2f451d5fcc","_kg_hide-input":true},"cell_type":"code","source":"plt.subplots(figsize=(10,10))\nsns.heatmap(data[Target+populationColumns].corr(), annot=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d1e8416df905c1ab959620bae5562c7eb936d73e"},"cell_type":"markdown","source":"# 3. Training a Model\nNow that we have usable data we can start training our data. We'll be using the Decision Tree model since it's the easiest to visualise and understand.\n\nFirst we have to create X and y DataFrames. Y DataFrames contain the data we predict, X DataFrames contain the data the model uses to predict y."},{"metadata":{"trusted":true,"_uuid":"7b4a851349e5d19b4031513ff2aec20cf86b020b"},"cell_type":"code","source":"#Create X & y\nX = data.iloc[:, 1:]\ny = data['class']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2e23bb03f59371dcf0e0bbb45dc8412b67db35c1"},"cell_type":"markdown","source":"To test our model we split the data into training and testing data. This ensures that the model doesn't just memorize the data instead of finding correlations."},{"metadata":{"trusted":true,"_uuid":"21ba01788cc1ae6c693375ad09d674259061be8f"},"cell_type":"code","source":"#Create Testing and Training Data\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ffac1b68c7177a06435e5ac215e83a2508fbb3f3"},"cell_type":"markdown","source":"Finally we can import and test our Decision Tree. We are using a Classifier version which gives us a Boolean variable ( 0 or 1 ). If we'd used a Regressor we would have gotten a number between 0 and 1.\n\nTo keep it simple we will limit the tree to a maximum depth of 5. Then we'll fit the training data. Fitting prepares the model for the real work, in this case the identification of X_test."},{"metadata":{"trusted":true,"_uuid":"f515a8794b52c484c2faabdb4687d65f7a18b4e3"},"cell_type":"code","source":"from sklearn import tree\n\ndtc = tree.DecisionTreeClassifier(max_depth=2, random_state=0)\ndtc.fit(X_train, y_train)\n\ndtc.score(X_test, y_test)\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"493c91f4f5a99f1f6d8590718a53461cd084bfc4"},"cell_type":"markdown","source":"Our final score is 0.954 which is really good! This means we are correct 95.4% of the time. To increase this we could remove the max_depth limit so the tree can grow bigger.\n\nNow let's see what our Decision Tree looks like!\nThe tree is built out of leaves and these leaves contain information about our model:\n* Condition of the leaf\n* Gini (or chance of incorrect measurement of a random training sample at that point)\n* The number of samples that passed during fitting\n* Class (or prediction) of the sample at that point"},{"metadata":{"trusted":true,"_uuid":"6bcd4a8ce0aee20691c44d32c158f9a7382308ea","_kg_hide-input":true},"cell_type":"code","source":"import graphviz\ndot_data = tree.export_graphviz(dtc, feature_names=X.columns.values, class_names=['Edible', 'Poisonous'], filled=True )\ngraphviz.Source(dot_data) ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9b784e508ca880915772b28ed4af2d7145c85e98"},"cell_type":"markdown","source":"# Conclusion \nNow that you know how to start a Machine Learning project we'll go some more in depth into different Machine Learning Models in the next Kernels.\n### Next Kernel\n[How Does Linear Regression Work?](https://www.kaggle.com/veleon/how-does-linear-regression-work)\n### Back to Index\n[Index](https://www.kaggle.com/veleon/in-depth-look-at-machine-learning-models)"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}