{"cells":[{"metadata":{},"cell_type":"markdown","source":"The Internet has not only made information accessible to the masses but also has become a hotspot of misinformation and fake news. Fake news can lead to more harm if not correctly identified and tagged. The severity of of the effects of misinformation can be judged from the fact that there have been riots and killings attributed to fake news.\n\nFake news can even sway people's opinions and affiliations - a fact that political parties have used (and still use) to make people vote in their favour.\n\nAs such, it has become necessary to segregate the real from the fake news. But this is not feasible manually thanks to the huge amount of information that is churned out every minute on the internet.\n\nTo overcome this problem, machine learnng and natural languaging processing can be to automatically classify the fake from the real news."},{"metadata":{},"cell_type":"markdown","source":"# Overview\nIn this project, I've tried to classify the given news as fake or real, by using Naive Bayes classifier and Passive Aggressive Classifier. I tried using Naive Bayes with hyperparameter tuning. \n\nThe best result was given by Passive Aggressive Classifier, with an accuracy of over 99.5%"},{"metadata":{},"cell_type":"markdown","source":"# Importing necessary Libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn import metrics\nimport itertools\nimport nltk\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import plot_confusion_matrix\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Dataset\nThe [dataset used here](https://www.kaggle.com/pnkjgpt/fake-news-dataset) consists of a train and a test file. The test file can be ignored as it doesn't contain the labels (as I'm doing this project as a part of a competition). We will work only on the train data.\n\nThe train dataset contains 7 columns - **'index', 'title', 'text', 'subject', 'date', 'class', 'Unnamed: 6'**"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/fake-news-dataset/train.csv')\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Checking for null values**"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Checking whether the dataset is balanced or imbalanced**"},{"metadata":{"trusted":true},"cell_type":"code","source":"train['class'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The dataset is fairly balanced with the number of Real and Fake classes almost equal. \n\nTheir seems to be another class with the name **'February 5, 2017'** consisting of only one data point. On further inspection, I find that the features for this data point has been shifted one column ahead for all the features. Since it is just one point, it can be removed or the features shifted in the reverse direction. \n\nI chose to shift the columns in the right places."},{"metadata":{"trusted":true},"cell_type":"code","source":"train[train['class'] == 'February 5, 2017']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#shifting the column values in the respective places\ntrain.iloc[504, 2] = train.iloc[504, 3]\ntrain.iloc[504, 3] = train.iloc[504, 4]\ntrain.iloc[504, 4] = train.iloc[504, 5]\ntrain.iloc[504, 5] = train.iloc[504, 6]\ntrain.iloc[504, 6] = np.nan","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.iloc[[504]]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The **index** and **Unnamed: 6** columns can now be removed as these are redundant and don't convey any information."},{"metadata":{"trusted":true},"cell_type":"code","source":"train.drop(['index', 'Unnamed: 6'], axis = 1, inplace=True)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data description"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.describe(include = 'all').T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above table, it can be seen that out of 40,000 titles and texts, 35,075 and 34,965 are unique, respectively.\n\nSo the remaining non-unique titles and texts must be removed. I removed the **text**, as it has more non-unique values comapred to **title**."},{"metadata":{"trusted":true},"cell_type":"code","source":"train.drop_duplicates(subset = ['text'], inplace=True)\ntrain.reset_index(drop = True, inplace = True)\ntrain.describe(include = 'all').T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As can be seen, out of 34,965 texts, 34,653 are unique. The remaining 312 (34965-34653) are non-unique. Since it's a small number, I let it as it is."},{"metadata":{},"cell_type":"markdown","source":"# Pie chart showing the type of articles"},{"metadata":{"trusted":true},"cell_type":"code","source":"train['subject'].value_counts().plot.pie(figsize = (7, 7));","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Removing stopwords"},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\nfrom nltk.corpus import stopwords\n\ndef stopwordsRemover(document):\n    corpus = []\n    for i in range(len(train)):\n        temp = re.sub('[^a-zA-Z]', ' ', document[i])\n        temp = temp.lower()\n        temp = temp.split()\n\n        temp = [word for word in temp if not word in stopwords.words('english')]\n        temp = ' '.join(temp)\n        corpus.append(temp)\n    return(corpus)\nnoStopWordTitle = stopwordsRemover(train['title'])\nnoStopWordText = stopwordsRemover(train['text'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#first 10 titles\nnoStopWordTitle[:10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I removed the **title** and **text** and inserted the **noStopWordTitle** and **noStopWordText** in the dataframe"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.insert(0, 'noStopWordTitle', noStopWordTitle, True)\ntrain.insert(1, 'noStopWordText', noStopWordText, True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.drop(['title', 'text'], axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Top unigrams, bigrams and trigrams used in the title"},{"metadata":{},"cell_type":"markdown","source":"Separating the Fake and Real titles"},{"metadata":{"trusted":true},"cell_type":"code","source":"fakeTitles = train.noStopWordTitle[train['class'] == 'Fake']\nrealTitles = train.noStopWordTitle[train['class'] == 'Real']\n\nmergedFake = ' '.join(fakeTitles)\nmergedReal = ' '.join(realTitles)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk import word_tokenize\nfrom nltk.util import ngrams\nfrom collections import Counter\n\ndef ngramFunct(corpus, n):\n    token = nltk.word_tokenize(corpus)\n    ans = ngrams(token,n)\n    return(Counter(ans))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Unigrams"},{"metadata":{"trusted":true},"cell_type":"code","source":"unigramReal = ngramFunct(mergedReal, 1)\nunigramFake = ngramFunct(mergedFake, 1)\nufreqReal = (nltk.FreqDist(unigramReal))\nufreqFake = (nltk.FreqDist(unigramFake))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.title('Top 20 Unigrams in Real News')\nufreqReal.plot(20, cumulative=False, color = 'b');\n\nplt.title('Top 20 Unigrams in Fake News')\nufreqFake.plot(20, cumulative=False, color = 'r');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Bigrams"},{"metadata":{"trusted":true},"cell_type":"code","source":"bigramReal = ngramFunct(mergedReal, 2)\nbigramFake = ngramFunct(mergedFake, 2)\nbfreqReal = (nltk.FreqDist(bigramReal))\nbfreqFake = (nltk.FreqDist(bigramFake))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.title('Top 20 Bigrams in Real News')\nbfreqReal.plot(20, cumulative=False, color = 'b');\n\nplt.title('Top 20 Bigrams in Fake News')\nbfreqFake.plot(20, cumulative=False, color = 'r');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Trigrams"},{"metadata":{"trusted":true},"cell_type":"code","source":"trigramReal = ngramFunct(mergedReal, 3)\ntrigramFake = ngramFunct(mergedFake, 3)\ntfreqReal = (nltk.FreqDist(trigramReal))\ntfreqFake = (nltk.FreqDist(trigramFake))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.title('Top 20 Trigrams in Real News')\ntfreqReal.plot(20, cumulative=False, color = 'b');\n\nplt.title('Top 20 Trigrams in Fake News')\ntfreqFake.plot(20, cumulative=False, color = 'r');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Building"},{"metadata":{},"cell_type":"markdown","source":"I first tried to use the models on the titles only and then the text only and then merged the titles and text."},{"metadata":{},"cell_type":"markdown","source":"### Stemming the words"},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.stem.snowball import SnowballStemmer\n\ndef stem(data):    \n    stemmer = SnowballStemmer('english')\n    stemmed = []\n    for i in range(len(data)):\n        temp = data[i]\n        temp = [stemmer.stem(word) for word in temp]\n        temp = ''.join(temp)\n        stemmed.append(temp)\n    return(stemmed)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Applying models to the Titles"},{"metadata":{"trusted":true},"cell_type":"code","source":"titleCorpus = stem(train.noStopWordTitle)\ntitleCorpus[:10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### TF-IDF"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\ntf = TfidfVectorizer(max_features=5000, ngram_range=(1, 3))\nX = tf.fit_transform(titleCorpus).toarray()\ny = train['class']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.33)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Multinomial Naive Bayes"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\nclassifierMNB = MultinomialNB()\nclassifierMNB.fit(X_train, y_train) \npred = classifierMNB.predict(X_val)\nscore = metrics.accuracy_score(y_val, pred)\nprint('Accuracy : %0.3f' %score)\n\ncm = plot_confusion_matrix(classifierMNB, X_val, y_val, cmap = 'coolwarm')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Passive Aggressive Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import PassiveAggressiveClassifier\nclassifierPAC = PassiveAggressiveClassifier(n_iter_no_change=50)\nclassifierPAC.fit(X_train, y_train)\npred = classifierPAC.predict(X_val)\nscore = metrics.accuracy_score(y_val, pred)\nprint('Accuracy : %0.3f'%score)\ncm = plot_confusion_matrix(classifierPAC, X_val, y_val, cmap = 'coolwarm')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Applying models to the Text"},{"metadata":{"trusted":true},"cell_type":"code","source":"textCorpus = stem(train.noStopWordText)\ntextCorpus[:10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## TF-IDF"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\ntf = TfidfVectorizer(max_features=5000, ngram_range=(1, 3))\nX = tf.fit_transform(textCorpus).toarray()\ny = train['class']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.33)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Multinomial Naive Bayes"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\nclassifierMNB = MultinomialNB()\nclassifierMNB.fit(X_train, y_train) \npred = classifierMNB.predict(X_val)\nscore = metrics.accuracy_score(y_val, pred)\nprint('Accuracy : %0.3f' %score)\n\ncm = plot_confusion_matrix(classifierMNB, X_val, y_val, cmap = 'coolwarm')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Passive Aggressive Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import PassiveAggressiveClassifier\nclassifierPAC = PassiveAggressiveClassifier(n_iter_no_change=50)\nclassifierPAC.fit(X_train, y_train)\npred = classifierPAC.predict(X_val)\nscore = metrics.accuracy_score(y_val, pred)\nprint('Accuracy : %0.3f'%score)\ncm = plot_confusion_matrix(classifierPAC, X_val, y_val, cmap = 'coolwarm')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Combining the Title and Text and applying Models"},{"metadata":{"trusted":true},"cell_type":"code","source":"train['Title and Text'] = train[['noStopWordTitle', 'noStopWordText']].apply(' '.join, axis=1)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"titleTextCorpus = stem(train['Title and Text'])\ntitleTextCorpus[:5]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## TD-IDF"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\ntf = TfidfVectorizer(max_features=5000, ngram_range=(1, 3))\nX = tf.fit_transform(titleTextCorpus).toarray()\ny = train['class']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.33)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Multinomial Naive Bayes"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\nclassifierMNB = MultinomialNB()\nclassifierMNB.fit(X_train, y_train) \npred = classifierMNB.predict(X_val)\nscore = metrics.accuracy_score(y_val, pred)\nprint('Accuracy : %0.3f' %score)\n\ncm = plot_confusion_matrix(classifierMNB, X_val, y_val, cmap = 'coolwarm')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Passive Aggressive Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import PassiveAggressiveClassifier\nclassifierPAC = PassiveAggressiveClassifier(n_iter_no_change=50)\nclassifierPAC.fit(X_train, y_train)\npred = classifierPAC.predict(X_val)\nscore = metrics.accuracy_score(y_val, pred)\nprint('Accuracy : %0.3f'%score)\ncm = plot_confusion_matrix(classifierPAC, X_val, y_val, cmap = 'coolwarm')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As it can be seen, the classification accuracy when the models are applied to the **titles** only is the least followed by the **text**.\n\nThe accuracy is highest when the **title** and **text** both are combined together. \n\nCompared with Naive Bayes, Passive Aggressive Classifier gives the best accuracy."},{"metadata":{},"cell_type":"markdown","source":"**Do upvote if this notebook helped you in learning something new.**\n\n**Suggestions and discussions are welcome**"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}