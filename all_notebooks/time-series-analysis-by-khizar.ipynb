{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Welcome Dear !!!\n### This Jupyter Notebook will run on Google Colab :)"},{"metadata":{"id":"PzsLOoDELwZ2","colab_type":"text"},"cell_type":"markdown","source":"## This project will analyse the time series power consumption data (2 million rows) using deep learning \n### The aim is just to show how to build the simplest Long Short-Term Memory (LSTM) recurrent neural network for the data.\n\n#### lets do!"},{"metadata":{"id":"owYHZUo6NhfH","colab_type":"text"},"cell_type":"markdown","source":"The description of data can be found here:\nhttp://archive.ics.uci.edu/ml/datasets/Individual+household+electric+power+consumption\n\nAttribute Information:\n1.date: Date in format dd/mm/yyyy\n\n2.time: time in format hh:mm:ss\n\n3.global_active_power (output label): household global minute-averaged active power (in kilowatt)\n\n4.global_reactive_power: household global minute-averaged reactive power (in kilowatt)\n\n5.voltage: minute-averaged voltage (in volt)\n\n6.global_intensity: household global minute-averaged current intensity (in ampere)\n\n7.sub_metering_1: energy sub-metering No. 1 (in watt-hour of active energy). It corresponds to the kitchen, containing mainly a dishwasher, an oven and a microwave (hot plates are not electric but gas powered).\n\n8.sub_metering_2: energy sub-metering No. 2 (in watt-hour of active energy). It corresponds to the laundry room, containing a washing-machine, a tumble-drier, a refrigerator and a light.\n\n9.sub_metering_3: energy sub-metering No. 3 (in watt-hour of active energy). It corresponds to an electric water-heater and an air-conditioner."},{"metadata":{"id":"4K-oDdm4HtHs","colab_type":"code","colab":{},"trusted":false},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":0,"outputs":[]},{"metadata":{"id":"wpoufdgLIk43","colab_type":"code","colab":{},"trusted":false},"cell_type":"code","source":"# data = https://drive.google.com/open?id=1Qm1L8izAJ-8NAt2ZROmtAVSf1CNEPyGH\nid = \"1Qm1L8izAJ-8NAt2ZROmtAVSf1CNEPyGH\"","execution_count":0,"outputs":[]},{"metadata":{"id":"Hz2Jvfp6ITm7","colab_type":"code","colab":{},"trusted":false},"cell_type":"code","source":"from pydrive.auth import GoogleAuth\nfrom pydrive.drive import GoogleDrive\nfrom google.colab import auth\nfrom oauth2client.client import GoogleCredentials ","execution_count":0,"outputs":[]},{"metadata":{"id":"lWkYYDV6JS8T","colab_type":"code","colab":{},"trusted":false},"cell_type":"code","source":"auth.authenticate_user()\ngauth = GoogleAuth()\ngauth.credentials = GoogleCredentials.get_application_default()\ndrive = GoogleDrive(gauth)","execution_count":0,"outputs":[]},{"metadata":{"id":"yfPazBFiJxxA","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"8836ce22-4a48-4eee-ea53-64e178033bed","trusted":false},"cell_type":"code","source":"download = drive.CreateFile({'id':id})\ndownload.GetContentFile('household_power_consumption.txt')\nprint(f\"data has been download to google colab\")","execution_count":null,"outputs":[]},{"metadata":{"id":"gxpgAjuIKbO7","colab_type":"code","colab":{},"trusted":false},"cell_type":"code","source":"df = pd.read_csv('household_power_consumption.txt', sep = ';', \n                 parse_dates={'datetime':['Date','Time']},\n                 na_values=['nan','?'],\n                 index_col = 'datetime'\n                 )","execution_count":0,"outputs":[]},{"metadata":{"id":"Zo-aBmljLFaz","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":431},"outputId":"fe3ffba5-1a7c-4d47-a397-a7448d2124d2","trusted":false},"cell_type":"code","source":"df.head(100)","execution_count":null,"outputs":[]},{"metadata":{"id":"QERsKx50MSs1","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"9e1c4874-1311-435b-82db-ce7d8fa4b20c","trusted":false},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"id":"5RpM4iYeMVWU","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":284},"outputId":"d89c6fed-c512-4bbd-8b57-bb17868eb560","trusted":false},"cell_type":"code","source":"df.describe(include='all')","execution_count":null,"outputs":[]},{"metadata":{"id":"3-q4K7h0M6Tf","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":218},"outputId":"2d0aca11-3f45-4c1a-ee6f-d835ba32dd70","trusted":false},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"id":"BfPSX9hAM8Gp","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":151},"outputId":"f263e29b-8f77-4715-a0cc-49eb47f783a9","trusted":false},"cell_type":"code","source":"# remove null values\ndf.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"id":"JLqlcutQSWkz","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":283},"outputId":"d01ccd49-396d-46be-b4ba-8b0ee4457352","trusted":false},"cell_type":"code","source":"# bearable outliers\ndf.Global_active_power.plot(kind='box') ","execution_count":null,"outputs":[]},{"metadata":{"id":"kgGlNMMTN1-d","colab_type":"code","colab":{},"trusted":false},"cell_type":"code","source":"# df.fillna({\n#     'Global_active_power':np.mean()\n# })\nfrom sklearn.preprocessing import Imputer\nfrom sklearn.pipeline import Pipeline","execution_count":0,"outputs":[]},{"metadata":{"id":"7K040KtROvbq","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":70},"outputId":"26c98e65-6b66-4110-ebcd-299c2b0cae28","trusted":false},"cell_type":"code","source":"cat_pipe = Pipeline([\n       ('imputer', Imputer(strategy='median'))              \n])\ncleaned_data = cat_pipe.fit_transform(df)","execution_count":null,"outputs":[]},{"metadata":{"id":"-ytDKj_pR6sE","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":151},"outputId":"ce04a6fb-f058-4a07-97df-0eea768eb9fa","trusted":false},"cell_type":"code","source":"clean_df = pd.DataFrame(cleaned_data,columns=df.columns)\nclean_df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"id":"HLB_nrvKSKqJ","colab_type":"code","colab":{},"trusted":false},"cell_type":"code","source":"clean_df.set_index(df.index, inplace = True)","execution_count":0,"outputs":[]},{"metadata":{"id":"J4bek2aRT8-I","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":577},"outputId":"b788a66b-6649-491f-f8f9-599d47ae3f38","trusted":false},"cell_type":"code","source":"# now explore the monthly wise gloabl active power\nmonthly_resampled_data_mean = clean_df.Global_active_power.resample('M').mean()\nmonthly_resampled_data_sum = clean_df.Global_active_power.resample('M').sum()\n\nmonthly_resampled_data_mean.plot(title = 'Global_active_power resampled over month for mean')\nplt.tight_layout()\nplt.show() \n\nmonthly_resampled_data_sum.plot(title = 'Global_active_power resampled over month for sum', color = 'red')\nplt.tight_layout()\nplt.show() \n","execution_count":null,"outputs":[]},{"metadata":{"id":"g6gFnPGJV42M","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":298},"outputId":"ae160cdb-8472-493c-9859-c3a767a6b476","trusted":false},"cell_type":"code","source":"r2 = clean_df.Global_reactive_power.resample('M').agg(['mean', 'std'])\nr2.plot(subplots = True, title='Global_reactive_power resampled over day', color='red')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"sRaOxF1DV60G","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":298},"outputId":"1ee9b85d-8a72-43f3-84d6-1f0d12fb8675","trusted":false},"cell_type":"code","source":"r2 = clean_df.Voltage.resample('M').agg(['mean', 'std'])\nr2.plot(subplots = True, title='Voltage resampled over month', color='red')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"InBLSNFvWc0-","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"67acec9a-1f13-4302-a0ba-0d5c606f630b","trusted":false},"cell_type":"code","source":"# sns.pairplot(clean_df, kind = 'reg')\nsns.pairplot(clean_df)\nplt.show()\n# KDE Plot described as Kernel Density Estimate is used for visualizing the Probability Density of a continuous variable. \n# It depicts the probability density at different values in a continuous variable. \n# We can also plot a single graph for multiple samples which helps in more efficient data visualization","execution_count":null,"outputs":[]},{"metadata":{"id":"caNkQOpkYcDP","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":279},"outputId":"6feb88b1-9d7f-4ad0-ea98-c590623d2ba8","trusted":false},"cell_type":"code","source":"# global active power and gloabl density are directly proportional to each other\nclean_df.Global_reactive_power.resample('W').mean().plot(color='y', legend=True)\nclean_df.Global_active_power.resample('W').mean().plot(color='r', legend=True)\nclean_df.Sub_metering_1.resample('W').mean().plot(color='b', legend=True)\nclean_df.Global_intensity.resample('W').mean().plot(color='g', legend=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"1ZUARlmUbzgX","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":265},"outputId":"6e0f1b89-9882-4d2c-9caa-f70eddbbfcb3","trusted":false},"cell_type":"code","source":"clean_df.Global_reactive_power.resample('W').mean().plot(kind = 'hist', color='y', legend=True)\nclean_df.Global_active_power.resample('W').mean().plot(kind = 'hist', color='r', legend=True)\nclean_df.Sub_metering_1.resample('W').mean().plot(kind = 'hist', color='b', legend=True)\nclean_df.Global_intensity.resample('W').mean().plot(kind = 'hist',color='g', legend=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"oCmbfqATcUNr","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":431},"outputId":"df120773-3eb6-4d55-eddf-90923ec1a83f","trusted":false},"cell_type":"code","source":"# find the percentage change with the previous row \ndata_returns = clean_df.pct_change()\ndata_returns","execution_count":null,"outputs":[]},{"metadata":{"id":"pS1EsFcjczQT","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":441},"outputId":"12a61348-9816-4093-d2bf-db7ed79c1f34","trusted":false},"cell_type":"code","source":"sns.jointplot(x='Voltage', y='Global_active_power', data=data_returns)  \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"2Xil6rr-dWAa","colab_type":"text"},"cell_type":"markdown","source":"# Machine-Leaning: LSTM Data Preparation and feature engineering\n### * I will apply recurrent nueral network (LSTM) which is best suited for time-seriers and sequential problem. This approach is the best if we have large data.  \n\n### * ***Its time to convert Time Series data  into a Supervised Learning Problem***\n\nhttps://machinelearningmastery.com/convert-time-series-supervised-learning-problem-python/"},{"metadata":{"id":"pZxUsfo6B4OI","colab_type":"code","colab":{},"trusted":false},"cell_type":"code","source":"\n\ndef series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n\tn_vars = 1 if type(data) is list else data.shape[1]\n\tdff = pd.DataFrame(data)\n\tcols, names = list(), list()\n\t# input sequence (t-n, ... t-1)\n\tfor i in range(n_in, 0, -1):\n\t\tcols.append(dff.shift(i))\n\t\tnames += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n\t# forecast sequence (t, t+1, ... t+n)\n\tfor i in range(0, n_out):\n\t\tcols.append(dff.shift(-i))\n\t\tif i == 0:\n\t\t\tnames += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n\t\telse:\n\t\t\tnames += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n\t# put it all together\n\tagg = pd.concat(cols, axis=1)\n\tagg.columns = names\n\t# drop rows with NaN values\n\tif dropnan:\n\t\tagg.dropna(inplace=True)\n\treturn agg\n ","execution_count":0,"outputs":[]},{"metadata":{"id":"Z2TdhuiBGwkj","colab_type":"text"},"cell_type":"markdown","source":"### * In order to reduce the computation time, and also get a quick result to test the model.  One can resmaple the data over hour (the original data are given in minutes). This will reduce the size of data from 2075259 to 34589 but keep the overall strucure of data as shown in the above.  "},{"metadata":{"id":"Xn756l-ICjou","colab_type":"code","colab":{},"trusted":false},"cell_type":"code","source":"resamble_data_hours = clean_df.resample('h').mean() ","execution_count":0,"outputs":[]},{"metadata":{"id":"GnTO2vDdG--Y","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"c949c933-6f58-4298-d83b-dcff0d75f317","trusted":false},"cell_type":"code","source":"resamble_data_hours.shape","execution_count":null,"outputs":[]},{"metadata":{"id":"2fKUCBhoG_8U","colab_type":"code","colab":{},"trusted":false},"cell_type":"code","source":"# its time to normalize the data\nfrom sklearn.preprocessing import MinMaxScaler","execution_count":0,"outputs":[]},{"metadata":{"id":"Ds9WmgApHSzR","colab_type":"code","colab":{},"trusted":false},"cell_type":"code","source":"scaler = MinMaxScaler(feature_range=(0,1))\nscaled = scaler.fit_transform(resamble_data_hours)","execution_count":0,"outputs":[]},{"metadata":{"id":"W8PxkKWNHVl5","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":235},"outputId":"9c55e2c3-7f06-4093-a64c-7d4f887dd5c5","trusted":false},"cell_type":"code","source":"scaled # normalized data","execution_count":null,"outputs":[]},{"metadata":{"id":"vKBOnb2dHqKe","colab_type":"code","colab":{},"trusted":false},"cell_type":"code","source":"# frame as supervised learning\nreframed = series_to_supervised(scaled, 1, 1)","execution_count":0,"outputs":[]},{"metadata":{"id":"jvhmlgonHzoh","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":402},"outputId":"18ad8766-0a57-4c00-fbdb-e6b2dfd9bc04","trusted":false},"cell_type":"code","source":"reframed","execution_count":null,"outputs":[]},{"metadata":{"id":"y9iAbWhwIfMF","colab_type":"text"},"cell_type":"markdown","source":"output variables var1(t)(Global Active Power)\tvar2(t)\tvar3(t)\tvar4(t)\tvar5(t)\tvar6(t)\tvar7(t)\nwe only need var1(t) (Global Active Power) Output variabel so, we should delete other(var2(t)\tvar3(t)\tvar4(t)\tvar5(t)\tvar6(t)\tvar7(t)) output varibles"},{"metadata":{"id":"I1WeUjW9H0qt","colab_type":"code","colab":{},"trusted":false},"cell_type":"code","source":"# we only need var1(t) (Global Active Power) Output variabel so,\n# we should delete other(var2(t)\tvar3(t)\tvar4(t)\tvar5(t)\tvar6(t)\tvar7(t)) output varibles\n\nreframed.drop(reframed.columns[[8,9,10,11,12,13]], inplace=True, axis=1)","execution_count":0,"outputs":[]},{"metadata":{"id":"XrIUhndNJFA7","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":343},"outputId":"56ab03c0-ca4b-4422-da5d-55231a2c34b7","trusted":false},"cell_type":"code","source":"reframed.head(10)","execution_count":null,"outputs":[]},{"metadata":{"id":"ZRBF9aktKXq9","colab_type":"text"},"cell_type":"markdown","source":"### Now Its time to split the data into training and validation\n### it is time series data so we cant split randomly\n\n### * First, I split the prepared dataset into train and test sets. To speed up the training of the model (for the sake of the demonstration), we will only train the model on the first year of data, then evaluate it on the next 3 years of data."},{"metadata":{"id":"ujnHWAP5JGnn","colab_type":"code","colab":{},"trusted":false},"cell_type":"code","source":"# from sklearn.model_selection import train_test_split\n","execution_count":0,"outputs":[]},{"metadata":{"id":"py5dQ1YGMS70","colab_type":"text"},"cell_type":"markdown","source":"#### We reshaped the input into the 3D format as expected by LSTMs, namely [samples, timesteps, features].\n\n<ol> \n<li> Samples. One sequence is one sample. A batch is comprised of one or more samples. </li>\n\n<li>Time Steps. One time step is one point of observation in the sample.</li>\n\n<li>\nFeatures. One feature is one observation at a time step.</li>\n</ol>"},{"metadata":{"id":"0-KAvGZzKkoi","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"e044a66b-fc32-4352-c007-1a362f18a2b4","trusted":false},"cell_type":"code","source":"# split into train and test sets\nvalues = reframed.values\n\nn_train_time = 365*24\ntrain = values[:n_train_time, :]\ntest = values[n_train_time:, :]\n##test = values[n_train_time:n_test_time, :]\n# split into input and outputs\n\ntrain_X, train_y = train[:, :-1], train[:, -1]\ntest_X, test_y = test[:, :-1], test[:, -1]\n\n# reshape input to be 3D [samples, timesteps, features]\ntrain_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\ntest_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))\nprint(train_X.shape, train_y.shape, test_X.shape, test_y.shape) \n# We reshaped the input into the 3D format as expected by LSTMs, namely [samples, timesteps, features].","execution_count":null,"outputs":[]},{"metadata":{"id":"iMdXNN3PPRWd","colab_type":"text"},"cell_type":"markdown","source":"# Model architecture\n\n### 1)  LSTM with 100 neurons in the first visible layer \n### 3) dropout 20%\n### 4) 1 neuron in the output layer for predicting Global_active_power. \n### 5) The input shape will be 1 time step with 7 features.\n\n### 6) I use the Mean Absolute Error (MAE) loss function and the efficient Adam version of stochastic gradient descent.\n### 7) The model will be fit for 20 training epochs with a batch size of 70.\n"},{"metadata":{"id":"E7xx3jFkLjnP","colab_type":"code","colab":{},"trusted":false},"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense,Dropout,LSTM,Conv1D,MaxPool1D\nfrom tensorflow.keras.optimizers import SGD,Adam","execution_count":0,"outputs":[]},{"metadata":{"id":"WyajVjhKTVNb","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"80521359-e394-40e4-b8d9-4c752cc423f7","trusted":false},"cell_type":"code","source":"X_train.shape[2]","execution_count":null,"outputs":[]},{"metadata":{"id":"5edm98kyPitM","colab_type":"code","colab":{},"trusted":false},"cell_type":"code","source":"model = Sequential()\nmodel.add(LSTM(100, input_shape = (X_train.shape[1],X_train.shape[2])))\n\nmodel.add(Dropout(0.2))\n# model.add(LSTM(80))\n# model.add(Dropout(0.3))\n\nmodel.add(Dense(1))\nmodel.compile(optimizer='adam',loss='mse')","execution_count":0,"outputs":[]},{"metadata":{"id":"VavWnbDEjD-G","colab_type":"code","colab":{},"trusted":false},"cell_type":"code","source":"","execution_count":0,"outputs":[]},{"metadata":{"id":"K9E1Dh7SQexh","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"7f9408d8-61c0-49e0-87da-ca56b34f1fc5","trusted":false},"cell_type":"code","source":"# now fit the model\nhistory = None\nhistory = model.fit(x=X_train,y=Y_train,batch_size=70,epochs=50,verbose=2,validation_data=(X_test,Y_test),shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{"id":"OqnmzBvYUb56","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":295},"outputId":"316f20d9-7f01-4c91-80e9-3a7776245a24","trusted":false},"cell_type":"code","source":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title(\"Loss of Training and Validation\")\nplt.xlabel(\"Epoches\")\nplt.ylabel(\"Loss\")\nplt.legend(['Train','Test'], loc = 'upper right')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"4f9TsiSiTwvB","colab_type":"code","colab":{},"trusted":false},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error","execution_count":0,"outputs":[]},{"metadata":{"id":"mCqK0xeMZZuq","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"ce5f462d-f4c0-4cd2-af59-3a29a3156361","trusted":false},"cell_type":"code","source":"# invert predictions\n# make a prediction\nyhat = model.predict(X_test)\n\ntest_X = X_test.reshape((X_test.shape[0], 7))\n\n# # invert scaling for forecast\n# test_X[:,-6:] mean only features\n\ninv_yhat = np.concatenate((yhat, test_X[:, -6:]), axis=1)\ninv_yhat = scaler.inverse_transform(inv_yhat)\ninv_yhat = inv_yhat[:,0] #out put variable\n\n# # invert scaling for actual\n\ntest_y = Y_test.values.reshape((len(Y_test), 1))\ninv_y = np.concatenate((test_y, test_X[:, -6:]), axis=1)\ninv_y = scaler.inverse_transform(inv_y)\ninv_y = inv_y[:,0]\n# calculate RMSE\n\nrmse = np.sqrt(mean_squared_error(inv_y, inv_yhat))\nprint('Test RMSE: %.3f' % rmse)","execution_count":null,"outputs":[]},{"metadata":{"id":"1RbAnwHXWZO1","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"7ac45aef-03ec-4dc1-be6f-5e3a154f7002","trusted":false},"cell_type":"code","source":"# without inverse, its is normlize\ny_predict = model.predict(X_test)\nmse = np.sqrt(mean_squared_error(Y_test,y_predict))\nprint(f\"The Mean Squarred error is: {mse}\")","execution_count":null,"outputs":[]},{"metadata":{"id":"sHAOqq2QWrkP","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":359},"outputId":"731189a5-a752-4218-d523-68cf50c518fa","trusted":false},"cell_type":"code","source":"sample = list(range(200))\nplt.figure(figsize=(10,5))\nplt.plot(sample,inv_y[:200],marker = '.', label = 'Actual')\n\nplt.plot(sample,inv_yhat[:200],marker = '.', label = 'Prediction')\nplt.ylabel('Global_active_power', size=15)\nplt.xlabel('Time step', size=20)\nplt.legend(fontsize=15)\nplt.title(\"This is not Overfitting the data\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"TyKz0V2Jjb_t","colab_type":"text"},"cell_type":"markdown","source":"### * Here I have used the LSTM neural network which is now the state-of-the-art for sequencial problems. \n\n### * In order to reduce the computation time, and get some results quickly, I took the first year of data (resampled over hour) to train the model and the rest of data to test the model.  \n\n### * I put together a very simple LSTM neural-network to show that one can obtain reasonable predictions. However numbers of rows is too high and as a result the computation is very time-consuming (even for the simple model in the above it took few mins to be run on  2.8 GHz Intel Core i7).  The Best is to write the last part of code using Spark (MLlib) running on GPU.  \n\n### * Moreover, the neural-network architecture that I have designed is a toy model. It can be easily improved by adding CNN  and dropout layers.  The CNN is useful here since there are correlations in data (CNN layer is a good way to probe the local structure of data).  "},{"metadata":{"id":"gyxuLPJMjmdc","colab_type":"text"},"cell_type":"markdown","source":"if you have any query related to RNN, Plz feel free to ask at khizersultan007@gmail.com"},{"metadata":{"id":"meKhKCYTjzrc","colab_type":"code","colab":{},"trusted":false},"cell_type":"code","source":"","execution_count":0,"outputs":[]}],"metadata":{"colab":{"name":"power_consumption_time_series_analysis_using_deep_learning_by_Khizar.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":1}