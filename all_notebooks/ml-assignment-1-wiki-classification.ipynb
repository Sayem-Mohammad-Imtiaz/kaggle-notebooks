{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import svm\nfrom sklearn import tree\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\n\nfrom sklearn.model_selection import validation_curve, learning_curve\n\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"promotional_articles = pd.read_csv(\"/kaggle/input/wikipedia-promotional-articles/promotional.csv\")\ngood_articles = pd.read_csv(\"/kaggle/input/wikipedia-promotional-articles/good.csv\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"promotional_articles.shape, good_articles.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"SAMPLING_FRACTION = 0.1\ncorpus = promotional_articles.sample(frac=SAMPLING_FRACTION)[\"text\"]\nlabels = np.ones(corpus.shape[0])\n\ngood_sample = good_articles.sample(frac=SAMPLING_FRACTION)[\"text\"]\ncorpus = corpus.append(good_sample)\nlabels = np.append(labels, np.zeros(good_sample.shape[0]))\n\ncorpus.shape, labels.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature extraction"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# use natural language toolkit\nimport nltk\nfrom nltk.stem.lancaster import LancasterStemmer\nstemmer = LancasterStemmer()\n\nlemmatized_corpus = []\ni = 0\nfor sentence in corpus:\n    lemmatized_corpus.append(\" \".join([stemmer.stem(word.lower()) for word in nltk.word_tokenize(sentence)]))\n    i += 1\n    if i%1000 == 0:\n        print (f\"Reached loop : {i}\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nfrom collections import defaultdict\n\ncounter = defaultdict(int)\nvocab = set()\n\nfor s in lemmatized_corpus:\n    for w in s.split(\" \"):\n        counter[w] += 1 \n        if counter[w] > 20:\n            vocab.add(w)\n\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.pipeline import Pipeline\n\npipe = Pipeline([('count', CountVectorizer(vocabulary=vocab)),\n                 ('tfid', TfidfTransformer())]).fit(lemmatized_corpus)\n\nfeatures = pipe['count'].transform(lemmatized_corpus).toarray()\ntfidf_features = pipe.transform(lemmatized_corpus)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features.shape, tfidf_features.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_samples(sample_size=None):\n    if sample_size is None:\n        sample_size = labels.shape[0]\n\n    choices = np.random.choice(np.arange(labels.shape[0]), sample_size, replace=False)\n    return tfidf_features[choices], labels[choices]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Tuning Decision tree hyper-parameter.****"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\ndef plot_decision_tree_validations(sample_size, depths, cv=5):\n    features, labels = get_samples(sample_size)\n    \n    train_scores, valid_scores = validation_curve(\n        tree.DecisionTreeClassifier(), features, labels, param_name=\"max_depth\", param_range=depths, cv=cv)\n\n    print(f\"depths : {depths}\")\n    print(f\"train_scores : {train_scores.mean(axis=1)}\")\n    print(f\"valid_scores : {valid_scores.mean(axis=1)}\")\n    print(\"--\" * 10)\n    \n    plt.clf()\n    plt.plot(depths, valid_scores.mean(axis=1), 'ro-', label=\"Validation Score\")\n    plt.plot(depths, train_scores.mean(axis=1), 'go-', label=\"Training Score\")\n    plt.xlabel(\"Max depth hyperparameter\")\n    plt.ylabel(\"Training/Validation Score\")\n    plt.title(f\"Validation curve for Decision Tree (Sample size: {sample_size})\")\n    plt.legend()\n    plt.show()\n    \nplot_decision_tree_validations(sample_size=1000, depths=np.arange(10, 200, 10))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Tuning Boosting hyperparameter - number of weak learners ****"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\ndef plot_boosting_validations(sample_size, num_weak_learners_range, cv=5):\n    features, labels = get_samples(sample_size)\n    \n    train_scores, valid_scores = validation_curve(\n        AdaBoostClassifier(), features, labels, param_name=\"n_estimators\", param_range=num_weak_learners_range, cv=cv)\n\n    print(f\"num_weak_learners : {num_weak_learners_range}\")\n    print(f\"train_scores : {train_scores.mean(axis=1)}\")\n    print(f\"valid_scores : {valid_scores.mean(axis=1)}\")\n    print(\"--\" * 10)\n\n    plt.clf()\n    plt.plot(num_weak_learners_range, valid_scores.mean(axis=1), 'ro-', label=\"Validation Score\")\n    plt.plot(num_weak_learners_range, train_scores.mean(axis=1), 'go-', label=\"Training Score\")\n    plt.xlabel(\"Hyperparameter (Number of weak learners)\")\n    plt.ylabel(\"Training/Validation Score\")\n    plt.title(f\"Validation curve for Boosting (Sample size: {sample_size})\")\n    plt.legend()\n    plt.show()\n    \nplot_boosting_validations(sample_size=1000, num_weak_learners_range=np.arange(10, 100, 10))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Tuning KNN algorithm"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\ndef plot_knn_validations(sample_size, k_range, cv=5):\n    features, labels = get_samples(sample_size)\n\n    train_scores, valid_scores = validation_curve(\n        KNeighborsClassifier(), features, labels, param_name=\"n_neighbors\", param_range=k_range, cv=cv)\n\n    print(f\"k_range : {k_range}\")\n    print(f\"train_scores : {train_scores.mean(axis=1)}\")\n    print(f\"valid_scores : {valid_scores.mean(axis=1)}\")\n    print(\"--\" * 10)\n    \n    plt.clf()\n    plt.plot(k_range, valid_scores.mean(axis=1), 'ro-', label=\"Validation Score\")\n    plt.plot(k_range, train_scores.mean(axis=1), 'go-', label=\"Training Score\")\n    plt.xlabel(f\"Hyperparameter (Number of neighbours (K))\")\n    plt.ylabel(\"Training/Validation Score\")\n    plt.title(f\"Validation curve for KNN (Sample size: {sample_size})\")\n    plt.legend()\n    plt.show()\n    \nplot_knn_validations(sample_size=1000, k_range=np.arange(1, 20, 2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Tuning NN hidden layers"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\ndef plot_nn_validations(sample_size, hidden_layers, layer_width=10, cv=5):\n    features, labels = get_samples(sample_size)\n    \n    hidden_layers_range = [(i, layer_width) for i in hidden_layers]\n    \n    clf = MLPClassifier(solver='lbfgs', alpha=1e-5, random_state=1, max_iter=4000)\n    train_scores, valid_scores = validation_curve(\n        clf, features, labels, param_name=\"hidden_layer_sizes\", param_range=hidden_layers, cv=cv)\n\n    print(f\"hidden_layers : {hidden_layers}\")\n    print(f\"train_scores : {train_scores.mean(axis=1)}\")\n    print(f\"valid_scores : {valid_scores.mean(axis=1)}\")\n    print(\"--\" * 10)\n    \n    plt.clf()\n    plt.plot(hidden_layers, valid_scores.mean(axis=1), 'ro-', label=\"Validation Score\")\n    plt.plot(hidden_layers, train_scores.mean(axis=1), 'go-', label=\"Training Score\")\n    plt.xlabel(f\"Hyperparameter (Number of hidden layers with width={layer_width})\")\n    plt.ylabel(\"Training/Validation Score\")\n    plt.title(f\"Validation curve for NN (Sample size: {sample_size})\")\n    plt.legend()\n    plt.show()\n    \nplot_nn_validations(sample_size=1000, hidden_layers=np.arange(1, 200, 20), layer_width=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training curves"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plotLearningCurve(clf, name, sample_size, train_sizes, cv=5):\n    features, labels = get_samples(sample_size)\n\n    train_sizes_used, train_scores, valid_scores = learning_curve(\n         clf, features, labels, train_sizes=train_sizes, cv=cv)\n\n    print(f\"Learning curve for {name}\")\n    print(f\"train_sizes : {train_sizes_used}\")\n    print(f\"train_scores : {train_scores.mean(axis=1)}\")\n    print(f\"valid_scores : {valid_scores.mean(axis=1)}\")\n    print(\"--\" * 10)\n\n    plt.clf()\n    plt.plot(train_sizes_used, valid_scores.mean(axis=1), 'ro-', label=\"Validation Score\")\n    plt.plot(train_sizes_used, train_scores.mean(axis=1), 'go-', label=\"Training Score\")\n    plt.xlabel(\"Training sample size\")\n    plt.ylabel(\"Training/Validation Error\")\n    plt.title(f\"Learning curve for {name} classifier (Sample size: {sample_size})\")\n    plt.legend()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Tuned classifiers.\n\nnn_clf = MLPClassifier(solver='lbfgs', alpha=1e-5,\n                    hidden_layer_sizes=(50, 10), random_state=1, max_iter=4000)\nboosting_clf = AdaBoostClassifier(n_estimators=100)\ndt_clf = tree.DecisionTreeClassifier(max_depth=25)\nknn_clf = KNeighborsClassifier(n_neighbors=1)\nsvm_clf = svm.SVC()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Sample size = 1000"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nsample_size = 1000\ntrain_sizes = np.arange(100, 800, 50)\n\nplotLearningCurve(dt_clf, \"Decision Tree\", sample_size=sample_size, train_sizes=train_sizes)\nplotLearningCurve(knn_clf, \"KNN\", sample_size=sample_size, train_sizes=train_sizes)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nplotLearningCurve(svm.SVC(), \"SVM\", sample_size=sample_size, train_sizes=train_sizes)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time \n\nplotLearningCurve(boosting_clf, \"Boosting\", sample_size=sample_size, train_sizes=train_sizes)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nplotLearningCurve(nn_clf, \"Neural nets\", sample_size=sample_size, train_sizes=train_sizes)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Sample size = 5000"},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_size = 5000\ntrain_sizes = np.arange(400, 4000, 400)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nplotLearningCurve(dt_clf, \"Decision Tree\", sample_size=sample_size, train_sizes=train_sizes)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nplotLearningCurve(knn_clf, \"KNN\", sample_size=sample_size, train_sizes=train_sizes)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nplotLearningCurve(boosting_clf, \"Boosting\", sample_size=sample_size, train_sizes=train_sizes)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nplotLearningCurve(svm.SVC(), \"SVM\", sample_size=sample_size, train_sizes=train_sizes)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nplotLearningCurve(nn_clf, \"Neural nets\", sample_size=sample_size, train_sizes=train_sizes)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Testing & Performance**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from datetime import datetime\nfrom sklearn.metrics import accuracy_score\n\nfeatures, labels = get_samples(5000)\ntrain_features, train_labels = features[:4500], labels[:4500]\ntest_features, test_labels = features[4500:], labels[4500:]\n\ndef train(clf):\n    t_start = datetime.now()\n    t_clf = clf.fit(train_features, train_labels)\n    t_end = datetime.now()\n    print (f\"Trained in {t_end - t_start} time\")\n    return t_clf\n    \ndef test(clf):\n    t_start = datetime.now()\n    predicted_labels = clf.predict(test_features)\n    acc_score = accuracy_score(test_labels, predicted_labels)\n    t_end = datetime.now()\n    print (f\"Predicted in {t_end - t_start} time with accuracy score: {acc_score}\")\n    return acc_score\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time \nx = train(dt_clf)\ntest(x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nprint (\"Testing and performance of Boosting\")\ntest(train(boosting_clf))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nprint (\"Testing and performance of KNN\")\ntest(train(knn_clf))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nprint (\"Testing and performance of NN\")\ntest(train(nn_clf))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nprint (\"Testing and performance of SVM\")\ntest(train(svm_clf))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}