{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"\n\nimport numpy as np \nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\ndata = pd.read_csv(\"/kaggle/input/factors-affecting-campus-placement/Placement_Data_Full_Class.csv\")\nprint(data.head())\n\n\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This problem is a  classification problem \nVarious classification alogrithms are:\n1. Logistic regression\n2. Support vector machines(SVM)\n3. Clustering\n4. K-mean alogrithm\netc\nIn the data we are provided with label dataset hence it is a case of supervised\nLogistic regression and SVM could be used to find out the required classes"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"y = data['status'].tolist()\nctr1 = 0\nctr2 = 0\nY = []\nfor i in range(0,len(y)):\n    if y[i]=='Placed':\n        ctr1 = ctr1 + 1\n        Y.append(1)\n    else:\n        ctr2 = ctr2 + 1\n        Y.append(0)\nctr_1 = [ctr1, ctr2]\nlabels1 = ['placed', 'not placed']\nypos1 = np.arange(len(labels1))\nplt.xticks(ypos1, labels1)\nplt.ylabel('Number')\nplt.bar(ypos1, ctr_1)\nplt.show()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ctr3 = 0\nctr4 = 0\ngen = data['gender'].tolist()\nfor i in range(0,len(gen)):\n    if gen[i] == 'M':\n            if Y[i] == 1:\n                ctr3 = ctr3 + 1\n    elif gen[i] == 'F':\n        if Y[i] == 1:\n               ctr4 = ctr4 + 1\nctr_2 = [ctr3 ,ctr4]\nlabels2 = ['Male', 'Female']\nypos2 = np.arange(len(labels2))\nplt.xticks(ypos2, labels2)\nplt.ylabel('Placed number')\nplt.bar(ypos2, ctr_2, color = ['red', 'green'])\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ctr5 = 0\nctr6 = 0\nctr7 = 0\nst = data['hsc_s'].tolist()\nfor i in range(0,len(st)):\n    if (st[i].lower()) == 'commerce':\n        if Y[i] == 1:\n            ctr5 = ctr5 + 1\n    elif (st[i].lower()) == 'science':\n        if Y[i] == 1:\n            ctr6 = ctr6 + 1\n    elif (st[i].lower()) == 'arts':\n        if Y[i] == 1:\n            ctr7 = ctr7 + 1\nctr_3 = [ctr5, ctr6, ctr7]\nlabels3 = ['Commerce', 'Science', 'Arts']\nypos3 = np.arange(len(labels3))\nplt.xticks(ypos3, labels3)\nplt.ylabel('Placed number')\nplt.bar(ypos3, ctr_3, color = ['red', 'green', 'blue'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"deg = data['degree_t'].tolist()\nctr7 = 0\nctr8 = 0\nfor i in range(0,len(deg)):\n    if (deg[i].lower()) == 'sci&tech':\n        if Y[i] == 1:\n            ctr7 = ctr7 + 1\n    elif (deg[i].lower()) == 'comm&mgmt':\n        if Y[i] == 1:\n            ctr8 = ctr8 + 1\nctr_4 = [ctr7, ctr8]\nlabels4 = ['Sci&tech', 'Comm&mgmt']\nypos4 = np.arange(0,len(labels4))\nplt.xticks(ypos4, labels4)\nplt.ylabel(\"Placed number\")\nplt.bar(ypos4, ctr_4, color = ['red', 'green'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The data consist of both categorical and numerical and thus must be cleaned before predictiona"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Numberical data\n\nprint(data.head())\nX1 = data[['gender', 'ssc_b','hsc_b','hsc_s','degree_t', 'workex', 'specialisation']]\nX1 = pd.get_dummies(X1)\nprint(X1.shape)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the above code I have done one-hot encoding so as to deal with categorical data\nOne-hot encoding is a necessary task because:\n1. Models can only take numerical values\n2. We have non-ordinal data\n\nIn the next block of code I will extract numberical data and then concatenate the datas"},{"metadata":{"trusted":true},"cell_type":"code","source":"X2 = data[['ssc_p','hsc_p', 'degree_p','etest_p', 'mba_p']]\n\n#concat the data\n\nX = pd.concat([X1, X2], axis=1, sort=False)\n\ny = pd.DataFrame(Y) # 1 represents placed and 0 represents not placed\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the next block I will try to fit the logistic regression and will carry out feature selection"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics, preprocessing\nfrom sklearn.feature_selection import SelectFromModel\n\nX_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2)  # Spliting the data\nX_train = preprocessing.StandardScaler().fit(X_train).transform(X_train)   # Preprocessing the data\nX_test = preprocessing.StandardScaler().fit(X_test).transform(X_test)\nLR = LogisticRegression()\nLR.fit(X_train, y_train)            # Fiting the logistic regression\nyhat = LR.predict(X_test)\nprint(\"Logistic regression accuracy:\", metrics.accuracy_score(y_test, yhat)) #Finding out the accuracy\nprint(X_train)\n#Feature selection using L1 regularization\n\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2)\nsel = SelectFromModel(LogisticRegression())\nsel.fit(X_train, y_train)\nselected_feat = X_train.columns[(sel.get_support())]\nprint(\"Optimum number of features from L1 regularisation:\", len(selected_feat))\nX_train_lasso = sel.fit_transform(X_train, y_train)\nX_test_lasso = sel.transform(X_test)\nmdl_lasso = LogisticRegression()\nmdl_lasso.fit(X_train_lasso, y_train)\nscore_lasso = mdl_lasso.score(X_test_lasso, y_test)\nprint(\"Score with L1 regularisation:\",score_lasso)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above code it was clear that the model didn't required any feature selection \nLogistic regression provied reasonable accuracy\n\nIn the next block I will try to fit SVM"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\nX_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2)  # Spliting the data\nX_train = preprocessing.StandardScaler().fit(X_train).transform(X_train)   # Preprocessing the data\nX_test = preprocessing.StandardScaler().fit(X_test).transform(X_test)\nmdl = SVC(gamma='auto')\nmdl.fit(X_train, y_train)\nyhat_svm = mdl.predict(X_test)\nprint(\"Support vector machine accuracy:\", metrics.accuracy_score(yhat_svm, y_test))\nsvc_accuracy = metrics.accuracy_score(yhat_svm, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As visible from the above code blocks both Support vector machine and logistic regression models showed reasonable accuracy\nHence any of the model can be used while doing classification in this case."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}