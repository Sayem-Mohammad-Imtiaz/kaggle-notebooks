{"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3","language":"python"},"language_info":{"version":"3.6.3","nbconvert_exporter":"python","file_extension":".py","pygments_lexer":"ipython3","mimetype":"text/x-python","name":"python","codemirror_mode":{"version":3,"name":"ipython"}}},"nbformat_minor":1,"cells":[{"cell_type":"markdown","metadata":{},"source":"I'm going to apply 19 machine learning algorithm. The objective of this kernel is to  get the performance comparison between them."},{"cell_type":"code","metadata":{},"execution_count":null,"source":"#sklearn\nfrom sklearn.cross_validation import train_test_split, cross_val_score\nfrom sklearn.metrics import mean_squared_error,confusion_matrix, precision_score, recall_score, auc,roc_curve\nfrom sklearn import ensemble, linear_model, neighbors, svm, tree, neural_network\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.linear_model import Ridge\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\n\n#load package\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n#from math import sqrt\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings('ignore')","outputs":[]},{"cell_type":"code","metadata":{},"execution_count":null,"source":"#upload the data\ndf=pd.read_csv('../input/mushrooms.csv',encoding='ISO-8859-1')\ndf.head()","outputs":[]},{"cell_type":"code","metadata":{},"execution_count":null,"source":"#check missing value\ndf.isnull().sum()","outputs":[]},{"cell_type":"code","metadata":{},"execution_count":null,"source":"df.select_dtypes(include=['object']).head()","outputs":[]},{"cell_type":"code","metadata":{},"execution_count":null,"source":"#Check the data type\ndf.dtypes","outputs":[]},{"cell_type":"code","metadata":{},"execution_count":null,"source":"#it is shown in previous section that all of the columns type are Object (string), thus i has to be encode to\n#integer type\n\nfrom sklearn.preprocessing import LabelEncoder\nenc=LabelEncoder()\nfor col in df.columns:\n    df[col] = enc.fit_transform(df[col])\n \ndf.head()","outputs":[]},{"cell_type":"code","metadata":{},"execution_count":null,"source":"df.dtypes","outputs":[]},{"cell_type":"code","metadata":{"collapsed":true},"execution_count":null,"source":"#spliting the data\nx = df.drop(\"class\", axis=1)\ny = df[\"class\"]","outputs":[]},{"cell_type":"code","metadata":{"collapsed":true},"execution_count":null,"source":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size=.25,random_state=1)","outputs":[]},{"cell_type":"code","metadata":{"collapsed":true},"execution_count":null,"source":"MLA = [\n    #Ensemble Methods\n    ensemble.AdaBoostClassifier(),\n    ensemble.BaggingClassifier(),\n    ensemble.ExtraTreesClassifier(),\n    ensemble.GradientBoostingClassifier(),\n    ensemble.RandomForestClassifier(),\n\n    #Gaussian Processes\n    gaussian_process.GaussianProcessClassifier(),\n    \n    #GLM\n    linear_model.LogisticRegressionCV(),\n    linear_model.PassiveAggressiveClassifier(),\n    linear_model. RidgeClassifierCV(),\n    linear_model.SGDClassifier(),\n    linear_model.Perceptron(),\n    \n    #Navies Bayes\n    naive_bayes.BernoulliNB(),\n    naive_bayes.GaussianNB(),\n    \n    #Nearest Neighbor\n    neighbors.KNeighborsClassifier(),\n    \n    #SVM\n    svm.SVC(probability=True),\n    svm.NuSVC(probability=True),\n    svm.LinearSVC(),\n    \n    #Trees    \n    tree.DecisionTreeClassifier(),\n    tree.ExtraTreeClassifier(),\n    \n    ]\n","outputs":[]},{"cell_type":"code","metadata":{},"execution_count":null,"source":"MLA_columns = []\nMLA_compare = pd.DataFrame(columns = MLA_columns)\n\n\nrow_index = 0\nfor alg in MLA:\n    \n    \n    predicted = alg.fit(x_train, y_train).predict(x_test)\n    fp, tp, th = roc_curve(y_test, predicted)\n    #roc_auc_rf = auc(fp, tp)\n    MLA_name = alg.__class__.__name__\n    MLA_compare.loc[row_index,'MLA Name'] = MLA_name\n    #MLA_compare.loc[row_index, 'Square root mean error'] = sqrt(mean_squared_error(y_test,predicted))\n    MLA_compare.loc[row_index, 'MLA Accuracy'] = round(alg.score(x_train, y_train), 4)\n    MLA_compare.loc[row_index, 'MLA Precission'] = precision_score(y_test, predicted)\n    MLA_compare.loc[row_index, 'MLA Recall'] = recall_score(y_test, predicted)\n    MLA_compare.loc[row_index, 'MLA AUC'] = auc(fp, tp)\n\n\n\n\n\n    row_index+=1\n    \nMLA_compare.sort_values(by = ['MLA Accuracy'], ascending = False, inplace = True)    \nMLA_compare","outputs":[]},{"cell_type":"code","metadata":{},"execution_count":null,"source":"index = 1\nfor alg in MLA:\n    \n    \n    predicted = alg.fit(x_train, y_train).predict(x_test)\n    fp, tp, th = roc_curve(y_test, predicted)\n    roc_auc_mla = auc(fp, tp)\n    MLA_name = alg.__class__.__name__\n    plt.plot(fp, tp, lw=2, alpha=0.3, label='ROC %s (AUC = %0.2f)'  % (MLA_name, roc_auc_mla))\n   \n    index+=1\n\nplt.title('ROC Curve')\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\nplt.plot([0,1],[0,1],'r--')\nplt.xlim([0,1])\nplt.ylim([0,1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')    \nplt.show()\n","outputs":[]},{"cell_type":"code","metadata":{"collapsed":true},"execution_count":null,"source":"","outputs":[]}],"nbformat":4}