{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import sys\nsys.path.append('../input/wav2vec-asr-utils')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -U datasets -q\n!pip install fsspec==2021.4.0 -q","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom datasets import load_dataset, load_metric,Dataset\nfrom tqdm import tqdm\nimport torch\nimport soundfile as sf\nimport torchaudio\nfrom transformers import Wav2Vec2ForCTC\nfrom transformers import Wav2Vec2Processor\nfrom transformers import Wav2Vec2FeatureExtractor\nfrom transformers import Wav2Vec2CTCTokenizer\n\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val =pd.read_csv(\"../input/automatic-speech-recognition-in-wolof/Test.csv\")\nval[\"path\"] = \"../input/automatic-speech-recognition-in-wolof/Noise Removed/tmp/WOLOF_ASR_dataset/noise_remove/\"+val[\"ID\"]+\".wav\"\nval.rename(columns = {'transcription':'sentence'}, inplace = True)\ncommon_voice_val = Dataset.from_pandas(val)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Wav2Vec2ForCTC.from_pretrained(\"../input/wav2vec2largexlsrwolof-model/Full-Base-wav2vec2-large-xlsr-WOLOF/wav2vec2-large-xlsr-WOLOF\").to(\"cuda\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"processor = Wav2Vec2Processor.from_pretrained(\"../input/wav2vec2largexlsrwolof-model/Full-Base-wav2vec2-large-xlsr-WOLOF/wav2vec2-large-xlsr-WOLOF\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# common_voice_val = Dataset.load_from_disk(\"../input/automatic-speech-recognition-in-wolof/perproc_data/Submission\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def speech_file_to_array_fn_test(batch):\n    speech_array, sampling_rate = sf.read(batch[\"path\"])\n    batch[\"speech\"] = speech_array\n    batch[\"sampling_rate\"] = sampling_rate\n#     batch[\"target_text\"] = batch[\"sentence\"]\n    return batch","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prepare_dataset_test(batch):\n    # check that all files have the correct sampling rate\n    assert (\n        len(set(batch[\"sampling_rate\"])) == 1\n    ), f\"Make sure all inputs have the same sampling rate of {processor.feature_extractor.sampling_rate}.\"\n\n    batch[\"input_values\"] = processor(batch[\"speech\"], padding=True,sampling_rate=batch[\"sampling_rate\"][0]).input_values\n    \n#     with processor.as_target_processor():\n#         batch[\"labels\"] = processor(batch[\"target_text\"]).input_ids\n    return batch","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"common_voice_val = common_voice_val.remove_columns([ \"ID\",\"age\",  \"down_votes\", \"gender\",  \"up_votes\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"common_voice_val = common_voice_val.map(speech_file_to_array_fn_test, remove_columns=common_voice_val.column_names)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"common_voice_val = common_voice_val.map(prepare_dataset_test, remove_columns=common_voice_val.column_names, batch_size=8, num_proc=4, batched=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_pred = []\nfor i in tqdm(range(common_voice_val.shape[0])):    \n    input_dict = processor(common_voice_val[i][\"input_values\"], return_tensors=\"pt\", padding=True)\n\n    logits = model(input_dict.input_values.to(\"cuda\")).logits\n\n    pred_ids = torch.argmax(logits, dim=-1)[0]\n    prediction = processor.decode(pred_ids)\n    final_pred.append(prediction)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val[\"transcription\"] = final_pred\nval[\"transcription\"] = val[\"transcription\"].str.capitalize()\nval.iloc[1390,6] = \"Mosqu√©e de\"\n\n\nval[[\"ID\",\"transcription\"]].to_csv(\"submission16.csv\",index=False)\nval.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}