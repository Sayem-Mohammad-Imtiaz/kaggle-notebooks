{"cells":[{"metadata":{"_uuid":"877ac6828a6873a9ee04203fb1e1752894e1259b"},"cell_type":"markdown","source":"# How long does an outage last?"},{"metadata":{"_uuid":"ea20eb51af9c159889020630b9f0ac045033221a"},"cell_type":"markdown","source":"* Title: 15 Years Of Power Outages\n\n* Objective: Predict attrition of your valuable employees\n\n* Kaggle link: https://www.kaggle.com/autunno/15-years-of-power-outages\n\n* Inspired by: __[my EDA on this dataset](https://www.kaggle.com/autunno/eda-15-years-of-power-outage)__\n\nThis notebook aims to make an EDA (Exploratory Data Analysis) on 15 years of outage to find out the features' relations and to prepare the ground for a Machine Learning model to predict the cause."},{"metadata":{"ExecuteTime":{"end_time":"2018-04-20T16:15:12.671714Z","start_time":"2018-04-20T16:15:12.670714Z"},"_uuid":"6b287f445ccf2fdd0b9cd31b7bf4efac10405f7c"},"cell_type":"markdown","source":"# How  this notebook is organized"},{"metadata":{"ExecuteTime":{"end_time":"2018-04-20T16:15:29.991573Z","start_time":"2018-04-20T16:15:29.980573Z"},"_uuid":"22aff1be40104f01fd4697919e41146c1cfa3644"},"cell_type":"markdown","source":"1. [Data pre-processing](#1.-Data-pre-processing)\n2. [Feature engineering and selection](#2.-Feature-engineering-and-selection)\n3. [Data analysis](#3.-Data-analysis)\n4. [Data preparation](#4.-Data-preparation)\n5. [First model](#5. First-model)\n6. [More models!](#6. More-models!)\n7. [Back to the drawing board](#7. Back-to-the-drawing-board)"},{"metadata":{"_uuid":"a213f5513b744a69583c3575a09506227f723ebd"},"cell_type":"markdown","source":"# 1. Data pre-processing"},{"metadata":{"_uuid":"23916351743f8bccd5753f2b45f378ea51737b18"},"cell_type":"markdown","source":"We start by importing all the libraries we're going to use:"},{"metadata":{"ExecuteTime":{"end_time":"2018-05-16T13:54:46.890047Z","start_time":"2018-05-16T13:54:46.877043Z"},"collapsed":true,"trusted":true,"_uuid":"f777bb34e5d294474a4421c3a510442674303562"},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score, confusion_matrix, r2_score, mean_squared_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.svm import SVR\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.linear_model import BayesianRidge\nfrom sklearn.neighbors import KNeighborsRegressor","execution_count":4,"outputs":[]},{"metadata":{"_uuid":"8d090194c97f9044a250fff2c038445736040ac3"},"cell_type":"markdown","source":"We now need to import our data:"},{"metadata":{"ExecuteTime":{"end_time":"2018-05-16T12:46:33.784551Z","start_time":"2018-05-16T12:46:33.754544Z"},"trusted":true,"_uuid":"4a4a0fbc291c2d6d7980e4784912c91f3e7e0422"},"cell_type":"code","source":"# Allow us to see all columns when printing\npd.set_option('display.max_columns', 1000)\n\n# Disable chained assignment warning message\npd.options.mode.chained_assignment = None \n\n# Read the dataset and print the first 5 rows\ndataset = pd.read_csv('../input/Grid_Disruption_00_14_standardized - Grid_Disruption_00_14_standardized.csv')\ndataset.head()","execution_count":5,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-05-16T12:46:33.793554Z","start_time":"2018-05-16T12:46:33.785552Z"},"trusted":true,"_uuid":"2f845dda6fc3497f866684e1eae849e421ee70a0"},"cell_type":"code","source":"print(\"Number of entries: \" + str(len(dataset.index)))","execution_count":6,"outputs":[]},{"metadata":{"_uuid":"5f08ff6f9eb12f7ce3940b685d45be80a2e00c74"},"cell_type":"markdown","source":"A quick peek at the data shows us that empty values are defined as \"Unknown\", which means we should treat them as NULL. To decide what to do with each value, we must first analyze how many empty values each column has:"},{"metadata":{"ExecuteTime":{"end_time":"2018-04-20T16:21:44.295848Z","start_time":"2018-04-20T16:21:44.294848Z"},"_uuid":"7996dbe765165fa44fb8b660ddbd99c3701e06e8"},"cell_type":"markdown","source":"### Analyzing the numeric values"},{"metadata":{"_uuid":"720fbb5f07f758d3d45042c536418d60e4e95cc7"},"cell_type":"markdown","source":"We can see many columns have \"Unknown\", which needs to be cleaned. We need to have special care with our numerical columns. Year is pretty likely pretty clean, I expect that \"Number of Customers Affected\" is more troublesome:"},{"metadata":{"ExecuteTime":{"end_time":"2018-05-16T12:46:33.813559Z","start_time":"2018-05-16T12:46:33.794554Z"},"trusted":true,"_uuid":"bb2b3c1c578518e557d5cc137dce1d1ecb242b8f"},"cell_type":"code","source":"len(pd.to_numeric(dataset['Year'], 'coerce').dropna().astype(int))","execution_count":7,"outputs":[]},{"metadata":{"_uuid":"d3a0ed1694f4a3c5fa873b26cb5ef3843b82f0e9"},"cell_type":"markdown","source":"\"Year\" seems to be perfectly filled, we don't need to worry about it."},{"metadata":{"ExecuteTime":{"end_time":"2018-05-16T12:46:33.832564Z","start_time":"2018-05-16T12:46:33.814559Z"},"trusted":true,"_uuid":"f8e5bb39af0e3b5a91e4a59d4c352bf0c795a811"},"cell_type":"code","source":"len(pd.to_numeric(dataset['Demand Loss (MW)'], 'coerce').dropna().astype(int))","execution_count":8,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-04-20T19:30:06.227006Z","start_time":"2018-04-20T19:30:06.215005Z"},"_uuid":"1087899e2f7a1372618991d0878c362fe4431dcc"},"cell_type":"markdown","source":"Over 700 rows are not numeric on 'Demand Loss (MW)'. It's quite a lot of missing values (almost 50%), we'll have to decide if we want to keep it or not."},{"metadata":{"ExecuteTime":{"end_time":"2018-05-16T12:46:33.85257Z","start_time":"2018-05-16T12:46:33.833564Z"},"trusted":true,"_uuid":"4a4e42937518a3c16486f3cd6458c96dcf6ebd5f"},"cell_type":"code","source":"len(pd.to_numeric(dataset['Number of Customers Affected'], 'coerce').dropna().astype(int))","execution_count":9,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-04-20T18:19:12.590609Z","start_time":"2018-04-20T18:19:12.581609Z"},"_uuid":"b3801e29ce09bfc74001795a02910a3302de814f"},"cell_type":"markdown","source":"As we can see above, we have too many non numerical rows for this column (only 222 are correctly filled), it may be best to simply drop it. Let's take a quick look at 'Demand Loss (MW)' first. I've already analyzed these columns on __[my EDA](https://www.kaggle.com/autunno/eda-15-years-of-power-outage)__, which proved that this data is not in good shape to be used. Besides the usual culprits (\"NaN\", \"Unknown\", \"None\"), we also have some strange choices, such as using \"Approx. \" and \" - \" to indicate a possible range of values"},{"metadata":{"_uuid":"1f7869500c7933758984b51e03c5e2d547288f5a"},"cell_type":"markdown","source":"### Analyzing the date values"},{"metadata":{"_uuid":"b97f6623ff7b5188e5c79eb94b5935061d03bd61"},"cell_type":"markdown","source":"We should also take a look at our date columns, to check if they are all in good shape:"},{"metadata":{"ExecuteTime":{"end_time":"2018-05-16T12:46:34.057622Z","start_time":"2018-05-16T12:46:33.85357Z"},"trusted":true,"_uuid":"ce138cc0db8a78c50dba93390bae1cc7768d0e54"},"cell_type":"code","source":"len(dataset[pd.isnull(pd.to_datetime(dataset['Date Event Began'], 'coerce'))])","execution_count":10,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-05-16T12:46:34.237681Z","start_time":"2018-05-16T12:46:34.058623Z"},"trusted":true,"_uuid":"24e35cf17278a44815b8379ac17bb469fecfefe0"},"cell_type":"code","source":"len(dataset[pd.isnull(pd.to_datetime(dataset['Date of Restoration'], 'coerce'))])","execution_count":11,"outputs":[]},{"metadata":{"_uuid":"458dd0cccf53d4d6856345cf1ccb6c0dfd215af3"},"cell_type":"markdown","source":"We can see from the above snippets that 'Date of Restoration' has quite a few invalid dates (events described as ongoing, unknown or simply wrongly inputed). We'll have to clean it up alongside with the other data. We should do the same for the hour attributes:"},{"metadata":{"ExecuteTime":{"end_time":"2018-05-16T12:46:34.377717Z","start_time":"2018-05-16T12:46:34.238669Z"},"scrolled":true,"trusted":true,"_uuid":"5c3f2fde4f124f5afef75fc3353b019e46d39c0d"},"cell_type":"code","source":"len(dataset[pd.isnull(pd.to_datetime(dataset['Time Event Began'], 'coerce'))])","execution_count":12,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-05-16T12:46:34.526743Z","start_time":"2018-05-16T12:46:34.379723Z"},"trusted":true,"_uuid":"2c5a7004cce43c895b4798fe35de70282ecdf074"},"cell_type":"code","source":"len(dataset[pd.isnull(pd.to_datetime(dataset['Time of Restoration'], 'coerce'))])","execution_count":13,"outputs":[]},{"metadata":{"_uuid":"b63f0bd6f266026aa457b52f8eb57ffa722e8f14"},"cell_type":"markdown","source":"There are several wrongly inputted time variables (both on start and restoration). These wrong values should all be replaced with np.nan (not a number) so that the rows can be dropped with dropna()."},{"metadata":{"_uuid":"325ccabe7c332afd5e95e40226e6807799ea3cea"},"cell_type":"markdown","source":"### So, what now?"},{"metadata":{"_uuid":"db11d5d05d676ce83db5dacd93937618e88d825f"},"cell_type":"markdown","source":"We have two very badly formed numeric columns: number of customers affected and demand loss. We can't possibly keep demand loss, as that would leave us with a too small dataset. However, we could keep demand loss if it makes sense for our model. It all depends on what we want from the data. We have a few options:\n * Build a regression model to predict demand loss.\n * Build a regression to predict the duration of an outage. \n * Build a classifier to predict what part of the day (e.g. morning, afternoon, night, etc) or month of the year outages are more likely to occur.\n * Find out which respondents solve outages faster.\n * Only use data which has number of customers affected known, and find the relation between this data and the other features.\n \nIn this kernel, I've chosen to build a regressor to predict the duration of an outage, which means that demand loss is actually undesirable. The reason for this is that this is something that is only known after the fact, so we'll never be able to use this in order to predict the duration of a new outage. The only things we'll need to know is: Duration,  event description, NERC Region, Geographic Areas and Respondent."},{"metadata":{"_uuid":"f9d7067e3ee8cb8c64dedacb527dec49e3faf67b"},"cell_type":"markdown","source":"### Removing undesirable attributes"},{"metadata":{"_uuid":"ff40370bd2d256697f1e08c4c04d55f492166ee9"},"cell_type":"markdown","source":"Having decided what path to take, we can remove unnecessary attributes. Even though we won't use Year on our model, let's keep it for now so that we can use it on our data analysis further on:"},{"metadata":{"ExecuteTime":{"end_time":"2018-05-16T12:46:34.533752Z","start_time":"2018-05-16T12:46:34.528759Z"},"collapsed":true,"trusted":true,"_uuid":"ed72f149a8162af5ebd98d42988ff41d19af82c6"},"cell_type":"code","source":"dataset = dataset[dataset.columns.difference(['Demand Loss (MW)', 'Number of Customers Affected', 'Event Description'])]","execution_count":14,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-05-08T16:22:42.543088Z","start_time":"2018-05-08T16:22:42.542088Z"},"_uuid":"340a0d4872b7a249befa8c159ef288f08679cc4a"},"cell_type":"markdown","source":"### Removing empty and noisy values"},{"metadata":{"_uuid":"5e580257ea4badbac84b7357c2ef092fd3cf8de0"},"cell_type":"markdown","source":"With that in mind, we can continue with our data pre-processing and replace 'Unknown' with None on all other columns, so that we can have a better idea of how many empty values we have:"},{"metadata":{"ExecuteTime":{"end_time":"2018-05-16T12:46:34.561769Z","start_time":"2018-05-16T12:46:34.535746Z"},"collapsed":true,"trusted":true,"_uuid":"381abc03a9eb42ceed5f34cfb60aeed62695356a"},"cell_type":"code","source":"dataset.replace('Unknown', np.nan, inplace=True)\ndataset.replace('Ongoing', np.nan, inplace=True)","execution_count":15,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-05-16T12:46:34.583776Z","start_time":"2018-05-16T12:46:34.563753Z"},"trusted":true,"_uuid":"0418ca1e30e028e4d3289b4c569422ecb65913c6"},"cell_type":"code","source":"dataset.isnull().any()","execution_count":16,"outputs":[]},{"metadata":{"_uuid":"6b8339a96b17ffffeea250d88fda76f41990e44f"},"cell_type":"markdown","source":"Many columns have empty values, lets now check how bad it is:"},{"metadata":{"ExecuteTime":{"end_time":"2018-05-16T12:46:34.604781Z","start_time":"2018-05-16T12:46:34.585758Z"},"trusted":true,"_uuid":"3ebc401d0a3b5c88f65b0eabaf63569e3a47b336"},"cell_type":"code","source":"print(\"Total number of rows: \" + str(len(dataset.index)))\nprint(\"Number of empty values:\")\nfor column in dataset.columns:\n    print(\" * \" + column + \": \" + str(dataset[column].isnull().sum()))","execution_count":17,"outputs":[]},{"metadata":{"_uuid":"fc67ddabbb071801d4320207d6f37906fad86a51"},"cell_type":"markdown","source":"We now have very few columns left with 'None' values, we can just remove these rows."},{"metadata":{"ExecuteTime":{"end_time":"2018-05-16T12:46:34.622768Z","start_time":"2018-05-16T12:46:34.606764Z"},"collapsed":true,"trusted":true,"_uuid":"1705df46f1ecf344b1ebb1316d97cadfd0a8e377"},"cell_type":"code","source":"dataset = dataset.dropna()","execution_count":18,"outputs":[]},{"metadata":{"_uuid":"b7c43987b3ba390fd86fde21f56c5b7cdfbdaa3a"},"cell_type":"markdown","source":"We can now check if our data is properly cleaned:"},{"metadata":{"ExecuteTime":{"end_time":"2018-05-16T12:46:34.644774Z","start_time":"2018-05-16T12:46:34.624769Z"},"trusted":true,"_uuid":"8b34ce8dca22fdf730094eaf26ebc285cbec0d51"},"cell_type":"code","source":"print(\"Total number of rows: \" + str(len(dataset.index)))\nprint(\"Number of empty values:\")\nfor column in dataset.columns:\n    print(\" * \" + column + \": \" + str(dataset[column].isnull().sum()))","execution_count":19,"outputs":[]},{"metadata":{"_uuid":"67a781d8d5c465da49161f97437c928b6ada3aaa"},"cell_type":"markdown","source":"It apparently is. However, there may still be some cases of wrong dates which were not dropped, but are not necessarily well-formed. Let's take a last look at it:"},{"metadata":{"ExecuteTime":{"end_time":"2018-05-16T12:46:34.830821Z","start_time":"2018-05-16T12:46:34.646774Z"},"trusted":true,"_uuid":"d352f7b5f5cfbee650e490a4fd996b6c2ce52fb7"},"cell_type":"code","source":"wrong_date_began = dataset[pd.isnull(pd.to_datetime(dataset['Date Event Began'], 'coerce'))].index\nprint(wrong_date_began)","execution_count":20,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-05-16T12:46:35.006867Z","start_time":"2018-05-16T12:46:34.831821Z"},"trusted":true,"_uuid":"0c4491b7b1f82b0261e43845309c944244f16181"},"cell_type":"code","source":"wrong_date_restoration = dataset[pd.isnull(pd.to_datetime(dataset['Date of Restoration'], 'coerce'))].index\nprint(wrong_date_restoration)","execution_count":21,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-05-16T12:46:35.139914Z","start_time":"2018-05-16T12:46:35.008869Z"},"trusted":true,"_uuid":"05ceb51fd8dc2528347587a8214a3033358f8ce9"},"cell_type":"code","source":"wrong_time_began = dataset[pd.isnull(pd.to_datetime(dataset['Time Event Began'], 'coerce'))].index\nprint(wrong_time_began)","execution_count":22,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-05-16T12:46:35.268947Z","start_time":"2018-05-16T12:46:35.141902Z"},"trusted":true,"_uuid":"e855f227ff60cfb2ab64fcccc57f922bc97343a5"},"cell_type":"code","source":"wrong_time_restoration = dataset[pd.isnull(pd.to_datetime(dataset['Time of Restoration'], 'coerce'))].index\nprint(wrong_time_restoration)","execution_count":23,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-05-08T17:08:53.504651Z","start_time":"2018-05-08T17:08:53.503651Z"},"_uuid":"c5db35ebc88fdffb2b298ad61b514bef12fa610f"},"cell_type":"markdown","source":"There's still a few wrong dates and we can easily deal with them by directly removing the indexes:"},{"metadata":{"ExecuteTime":{"end_time":"2018-05-16T12:46:35.277955Z","start_time":"2018-05-16T12:46:35.269936Z"},"trusted":true,"_uuid":"c50382b89cbd67125ffaa66789af651c6bd63169"},"cell_type":"code","source":"# append all wrong dates into a single array, turn into a set to remove duplicated indexes and drop them from the feature map\nwrong_dates = set(wrong_date_began.append(wrong_date_restoration).append(wrong_time_began).append(wrong_time_restoration))\ndataset = dataset.drop(wrong_dates)\nprint(len(dataset))","execution_count":24,"outputs":[]},{"metadata":{"_uuid":"44af5270843c01569d8defa6419d2461f860feb8"},"cell_type":"markdown","source":"The final size of our dataset is 1593; we didn't lose much data in our cleaning process, which is nice. Let's take a last look at our data before proceeding:"},{"metadata":{"ExecuteTime":{"end_time":"2018-05-16T12:46:35.303961Z","start_time":"2018-05-16T12:46:35.279938Z"},"trusted":true,"_uuid":"684349d3e074b7c2ae2b1221ba9b4906828935f8"},"cell_type":"code","source":"dataset.reset_index()\ndataset.head()","execution_count":25,"outputs":[]},{"metadata":{"_uuid":"82df4d9f4cc4a57969c97ba12d95a59dada6cd62"},"cell_type":"markdown","source":"# 2. Feature engineering and selection"},{"metadata":{"_uuid":"f2e91801a918af0ea274f48094748a2850a780fc"},"cell_type":"markdown","source":"We have two main issues to tackle: \n * Create a duration attribute, from Date Event Began, Date of Restoration, Time Event Began and Time of Restoration.\n * Create a one hot encoded set of attributes to represent the event description, since many columns have more than a single value.\n * Either deal with the location (which varies between state, city, county, region), or drop it."},{"metadata":{"_uuid":"d31081b8a22fa3cfed8fc29a3faf05cdfed7fad7"},"cell_type":"markdown","source":"### Getting the duration (in minutes) of an outage"},{"metadata":{"_uuid":"a3fdfd570ba39695867de92d4d55497615d6e8d5"},"cell_type":"markdown","source":"As mentioned on the previous section, I've chosen to build a regressor to predict the duration of an outage, so we'll convert our time-stamps into a single attribute containing the duration in minutes. We'll start by getting our dates and time together:"},{"metadata":{"ExecuteTime":{"end_time":"2018-05-16T12:46:35.976119Z","start_time":"2018-05-16T12:46:35.304944Z"},"collapsed":true,"trusted":true,"_uuid":"0ac9d259e8949630904f521ce74bb0a546c36c35"},"cell_type":"code","source":"date_start = pd.to_datetime(dataset['Date Event Began'] + ' ' + dataset['Time Event Began'])\ndate_end = pd.to_datetime(dataset['Date of Restoration'] + ' ' + dataset['Time of Restoration'])","execution_count":26,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-05-16T12:46:35.98512Z","start_time":"2018-05-16T12:46:35.97812Z"},"trusted":true,"_uuid":"fc2a43136386bca8c9778e8b44fc33b1ccb28962"},"cell_type":"code","source":"date_end.head()","execution_count":27,"outputs":[]},{"metadata":{"_uuid":"88a898e9582fabbbe35f84d6b55c05b706485938"},"cell_type":"markdown","source":"With that done, we can now calculate the difference in minutes, and then add this new attribute to our dataset, removing the old time columns (let's keep the dates for now, we may want to use it on data analysis):"},{"metadata":{"ExecuteTime":{"end_time":"2018-05-16T12:46:36.012128Z","start_time":"2018-05-16T12:46:35.986121Z"},"trusted":true,"_uuid":"d9414f3adfa597ebd41231e0775b787dfa440d3d"},"cell_type":"code","source":"# Create the attribute'Duration in Minutes', which is the difference between the end and start date of the event.\ndataset['Duration in Minutes'] = (date_end - date_start).dt.total_seconds() / 60\ndataset = dataset[dataset.columns.difference(['Time Event Began', 'Time of Restoration'])]\n\ndataset.head()","execution_count":28,"outputs":[]},{"metadata":{"_uuid":"ad6a7488f2e442d397263bcdd963f7b20e553abb"},"cell_type":"markdown","source":"Before proceeding, we should remove any negative durations, since that most likely means input error:"},{"metadata":{"ExecuteTime":{"end_time":"2018-05-16T12:46:36.020129Z","start_time":"2018-05-16T12:46:36.014128Z"},"collapsed":true,"trusted":true,"_uuid":"cfa71fa0ffbd22ddb7eab29d7f3a739deac0b1a8"},"cell_type":"code","source":"dataset = dataset[(dataset['Duration in Minutes'] > 0)]","execution_count":29,"outputs":[]},{"metadata":{"_uuid":"ea56cfc4da050b74ff94180cf0fa355ddc137185"},"cell_type":"markdown","source":"A few more entries were removed due to bad duration. Technically, this operation belongs in the data pre-processing section, but it couldn't be done until we had merged the columns. We now also need to take a look at our max, to make sure we don't have any crazy values:"},{"metadata":{"ExecuteTime":{"end_time":"2018-05-16T12:46:36.040134Z","start_time":"2018-05-16T12:46:36.02113Z"},"trusted":true,"_uuid":"517b88823fe2c1a0388ef6291652285682c321b0"},"cell_type":"code","source":"dataset.loc[dataset['Duration in Minutes'].idxmax()]","execution_count":30,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-05-15T13:13:21.071405Z","start_time":"2018-05-15T13:13:21.068404Z"},"_uuid":"839f35e52afc53240b86e3d7c1ffb7290731bf37"},"cell_type":"markdown","source":"The maximum duration we had is of 188978 minutes, which means 131 days! That is obviously a mistake (otherwise, poor souls). Before proceeding, it would be wise to calculate how many long outages we have; let's define 4320 minutes as the limit, which rounds up to 3 days:"},{"metadata":{"ExecuteTime":{"end_time":"2018-05-16T12:46:36.05814Z","start_time":"2018-05-16T12:46:36.041135Z"},"trusted":true,"_uuid":"92c0ca9d7dc1ffdf2696f7d4ad14313f4e4f30ee"},"cell_type":"code","source":"print('Number of outages that lasted over 3 days: ' + str(len(dataset[(dataset['Duration in Minutes'] > 4320)])))","execution_count":31,"outputs":[]},{"metadata":{"_uuid":"bcd26d6eb4375d447dfeb33bbfb21d2d4b7b298e"},"cell_type":"markdown","source":"There are 276 occurrences, which is a fairly good hit to our dataset. However, it's for the best, as it's a tall task to deal with these outliers while scoring a good prediction for lower duration outages."},{"metadata":{"ExecuteTime":{"end_time":"2018-05-16T12:46:36.075143Z","start_time":"2018-05-16T12:46:36.05914Z"},"collapsed":true,"trusted":true,"_uuid":"e2a378b2fa4844c37f85989770ddbffb9732b51f"},"cell_type":"code","source":"long_outages = dataset[(dataset['Duration in Minutes'] >= 4320)]\ndataset = dataset[(dataset['Duration in Minutes'] < 4320)]","execution_count":32,"outputs":[]},{"metadata":{"_uuid":"5bb6acbd5269bebff981c7b51b1440a611377a98"},"cell_type":"markdown","source":"We also kept two additional dataframes called long_outages, which contains really lasting outages (above 3 days). We most likely won't use this data on a model, but it may provide us with interesting analysis."},{"metadata":{"ExecuteTime":{"end_time":"2018-05-08T19:01:54.597532Z","start_time":"2018-05-08T19:01:54.596532Z"},"_uuid":"f4072de9a9b18264cdf2633b80120f5968f52243"},"cell_type":"markdown","source":"### Splitting the events into several attributes"},{"metadata":{"_uuid":"f178f935309786b9ffbc74a0a8cf525d1bb8dc30"},"cell_type":"markdown","source":"The main issue with the 'Tags' feature is that it may contain several causes into a single entry (separated by commas), as seen below:"},{"metadata":{"ExecuteTime":{"end_time":"2018-05-16T12:46:36.092148Z","start_time":"2018-05-16T12:46:36.076144Z"},"trusted":true,"_uuid":"df6c9737b863027a8e01ad37a9f74dfcc74f5cf6"},"cell_type":"code","source":"dataset.iloc[:, 6].head()","execution_count":33,"outputs":[]},{"metadata":{"_uuid":"45312beaedbdb890c3c157349e78d73f21d3e77f"},"cell_type":"markdown","source":"There are a few ways to deal with this:\n 1. Accept this attribute as is, considering that the combination of these events is an unique categorical value.\n 2. Rank the most important events, and replace the full string with it if they are present\n 3. Build a set list with all event types, and create a binary column for each to map all reasons for an outage\n \nOn this kernel, we're going to explore the mysteries behind door number 3, as it's not a good idea at this point to assume which events are more important (2) and the first option is just lazy.\n\nIf we always had a fixed number of tags, this could be simply be addressed by the following code:"},{"metadata":{"ExecuteTime":{"end_time":"2018-05-16T12:46:36.119155Z","start_time":"2018-05-16T12:46:36.093149Z"},"trusted":true,"_uuid":"aa0f90608494fd07481b6a451f81e24a4388dab0"},"cell_type":"code","source":"test_split = dataset['Tags'].str.split(',', expand=True)\ntest_split.head()","execution_count":34,"outputs":[]},{"metadata":{"_uuid":"c9b0e303432caee01ee0f0f6d6109a61f18fc9d4"},"cell_type":"markdown","source":"Since it's not the case, we're better off creating a unique set with all occurences and manually creating our one-hot-encoded attributes:"},{"metadata":{"ExecuteTime":{"end_time":"2018-05-16T12:46:36.13716Z","start_time":"2018-05-16T12:46:36.120155Z"},"trusted":true,"_uuid":"1e60e86689329a2b1576afecd863a50c1e9049aa"},"cell_type":"code","source":"tags = dataset.Tags.str.split(',').tolist()\nunique_events = set(x.lstrip() for lst in tags for x in lst)\nprint(unique_events)","execution_count":35,"outputs":[]},{"metadata":{"_uuid":"38af2133e2a36ad171ee9b404bc00393d7794134"},"cell_type":"markdown","source":"Note that .lspstrip() was used in the conversion to trim leading whitespace. One interesting thing to note here is that we can remove unknown not because it's a bad feature to have; on the contrary, it may actually be relevant as we don't always know the reason for an outage when it happens; we can remove it because of the dummy variable trap:"},{"metadata":{"ExecuteTime":{"end_time":"2018-05-16T12:46:36.228183Z","start_time":"2018-05-16T12:46:36.140161Z"},"trusted":true,"_uuid":"bfbc973cc2dda81635e8a8a1120d7f2a35e7ad50"},"cell_type":"code","source":"tags = dataset['Tags']\nunique_events.remove('unknown')\nlabelencoder = LabelEncoder()\n\n# Create the new features and encode them\nfor event in unique_events:\n    dataset[event] = tags.str.contains(event)\n    dataset[event] = labelencoder.fit_transform(dataset[event])\n    \ndataset = dataset[dataset.columns.difference(['Tags'])]   \ndataset.head()","execution_count":36,"outputs":[]},{"metadata":{"_uuid":"5d1a6b81f59eeeb0950707b02130785c6ab7cdbf"},"cell_type":"markdown","source":"### Geographic Areas"},{"metadata":{"_uuid":"f00fe62b0b1fd3fb6ca1339c2f4a9112c8bf6ac3"},"cell_type":"markdown","source":"Before deciding how to deal with this attribute, we must first take a look at all the unique values it has:"},{"metadata":{"ExecuteTime":{"end_time":"2018-05-16T12:46:36.235185Z","start_time":"2018-05-16T12:46:36.230184Z"},"trusted":true,"_uuid":"1e11c23a5353b1663b0d914f5b5d836288c9eed2"},"cell_type":"code","source":"areas = dataset['Geographic Areas'].unique()\nprint('Ammount of unique areas: ', len(areas))","execution_count":37,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-05-16T12:46:36.25219Z","start_time":"2018-05-16T12:46:36.236185Z"},"trusted":true,"_uuid":"5ef46826dd90e75f5a1c71eab3c998d078ba1283"},"cell_type":"code","source":"print('Unique areas: ' + str(len(areas)))","execution_count":38,"outputs":[]},{"metadata":{"_uuid":"c188ae41cf31d0195451e2d9ee4c16e3ee179f33"},"cell_type":"markdown","source":"That is too much variety to handle for such a small dataset. Taking a look at it, we can see that the data has no pattern or rule. A few disparity examples:\n  * North Part of the Island (What Island??)\n  * Entergy System (Not a city/state)\n  * Georgia and Alabama (Two states)\n  * Southern, Central and Northern New Jersey (Portions of a single state)\n  * City of Los Angeles (A single city)\n  * Primary Dade County Florida (A single county)\n  \nThat leaves us with two options:\n  1. Replace these values with State only\n  2. Ignore this column altogether, and use the NERC region to \n  \nGoing with option (1) is a tough road to take, mainly because not all entries have the state information and a lot of data would be lost for no good reason, since the NERC region should be enough information for us to build an interesting model. That being said, we're going to remove this column:"},{"metadata":{"ExecuteTime":{"end_time":"2018-05-16T12:46:36.272194Z","start_time":"2018-05-16T12:46:36.2542Z"},"collapsed":true,"trusted":true,"_uuid":"6afe4ee00aa5ad733d4ef24359f5124744f57afe"},"cell_type":"code","source":"dataset = dataset[dataset.columns.difference(['Geographic Areas'])]","execution_count":39,"outputs":[]},{"metadata":{"_uuid":"09cea749d9fef51a6328935383e6666138749306"},"cell_type":"markdown","source":"### Respondent & NERC Region"},{"metadata":{"_uuid":"6fab3af00ce56588f954a6d0aa72c04cf98d30ea"},"cell_type":"markdown","source":"These columns are pretty simple and can be simply dealt with by applying OneHotEncoder to it:"},{"metadata":{"ExecuteTime":{"end_time":"2018-05-16T12:46:36.2902Z","start_time":"2018-05-16T12:46:36.274196Z"},"trusted":true,"_uuid":"47dfe49ecd75c47ce6d3361fc5dc241b848ec645"},"cell_type":"code","source":"nerc_regions = dataset['NERC Region'].unique()\nprint('Unique NERC Regions (' + str(len(nerc_regions)) + '):')\nprint(nerc_regions)","execution_count":40,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-05-16T12:46:36.306203Z","start_time":"2018-05-16T12:46:36.2912Z"},"trusted":true,"_uuid":"a9f13594cef828f7c2208f2c6b16b15bacac92be"},"cell_type":"code","source":"respondent = dataset['Respondent'].unique()\nprint('Unique Respondents: ' + str(len(respondent)) )","execution_count":41,"outputs":[]},{"metadata":{"_uuid":"65ca13cf65ba3662d106cbc8f7795b0e7744e929"},"cell_type":"markdown","source":"The respondent column seems to be a bit messy. There doesn't seem to be a pattern, and the same value is represented differently in many places. It may be too hard to properly clean it up through code, it's better to simply disregard it. \n\nNERC Region on the other hand seems fine and can be used *almost* as is. There are a few cases of two values clustered in a single value separated by comma (e.g. RFC, SERC), which we want to deal with after we create our one hot encoded feature map:"},{"metadata":{"ExecuteTime":{"end_time":"2018-05-16T12:46:36.32921Z","start_time":"2018-05-16T12:46:36.307204Z"},"trusted":true,"_uuid":"991bd310500163aec51aae9907e6819f68bcefc8"},"cell_type":"code","source":"# Apply one hot encoding to NERC Region\nnerc_region = pd.get_dummies(dataset['NERC Region'], drop_first=True)\nprint(nerc_region.columns)\n\n# Append the NERC Region OneHotEncoded attributes to the feature map\ndataset = pd.concat([dataset, nerc_region], axis=1)\n\n# Remove the original attributes\ndataset = dataset[dataset.columns.difference(['Respondent'])]","execution_count":42,"outputs":[]},{"metadata":{"_uuid":"e5f91db84d66fb4d4fa55b52361e75da242f15b4"},"cell_type":"markdown","source":"With our feature map created, we can now deal with the comma separated values:"},{"metadata":{"ExecuteTime":{"end_time":"2018-05-16T12:46:36.380223Z","start_time":"2018-05-16T12:46:36.33021Z"},"trusted":true,"_uuid":"238d1467d567fb944b6c1dfb3a3ae5d85a589449"},"cell_type":"code","source":"# Deal with 'RFC, SERC' attribute\ndataset['RFC'] = np.where(dataset['RFC, SERC'] + dataset['RFC'] > 0, 1, 0)\ndataset['SERC'] = np.where(dataset['RFC, SERC'] + dataset['SERC'] > 0, 1, 0)\n\n# Deal with 'NPCC, RFC' attribute\ndataset['NPCC'] = np.where(dataset['NPCC, RFC'] + dataset['NPCC'] > 0, 1, 0)\ndataset['RFC'] = np.where(dataset['NPCC, RFC'] + dataset['RFC'] > 0, 1, 0)\n\n# Deal with 'FRCC, SERC' attribute\ndataset['FRCC'] = np.where(dataset['FRCC, SERC'] + dataset['FRCC'] > 0, 1, 0)\ndataset['SERC'] = np.where(dataset['FRCC, SERC'] + dataset['SERC'] > 0, 1, 0)\n\n# Remove the old (and now unneeded) attributes\ndataset = dataset[dataset.columns.difference(['RFC, SERC', 'NPCC, RFC', 'FRCC, SERC'])]\ndataset.head()","execution_count":43,"outputs":[]},{"metadata":{"_uuid":"e090713577913adce67344d8b35ee6adec76f365"},"cell_type":"markdown","source":"And that's it, we're done! For reference, the two following images shows the new and old NERC regions respectively, which we need to keep in mind since we're dealing with a dataset that has both (it changed after 2010):\n\nNew NERC Regions            |  Old NERC Regions\n:-------------------------:|:-------------------------:\n![](https://www.eia.gov/electricity/data/eia411/images/nerc_2010.jpg)  |  ![](https://www.eia.gov/electricity/data/eia411/images/nerc_new.jpg)"},{"metadata":{"_uuid":"b8356bd2937824f255be345ac359486174cbb1e7"},"cell_type":"markdown","source":"Ideally, we would either separate it in two datsets (one before 2010, and other one after), but we could end up with too little data that way for the newer outages.\n\nThe data is now ready for analysis and modeling! We ended up only keeping the OneHotEncoded NERC Regions and the cause of the events. That should be fine for what we're trying to achieve, since the other data would not be promptly available to help us determine how long of an outage we're looking at."},{"metadata":{"ExecuteTime":{"end_time":"2018-05-08T18:27:31.523532Z","start_time":"2018-05-08T18:27:31.521532Z"},"_uuid":"af1d9a3e321f181a293709fb4a1e67054d782c54"},"cell_type":"markdown","source":"# 3. Data analysis"},{"metadata":{"_uuid":"58d36fdb39b9cdfd448258083a5a3b036baa1664"},"cell_type":"markdown","source":"Data analysis is usually done before feature engineering, since it helps you identify what you want to use, what needs to change and give you a better taste of the problem. However, sine my previous __[kernel](https://www.kaggle.com/autunno/eda-15-years-of-power-outage)__ already covered some basic EDA, we already jumped into this problem with a good idea of what we can do with this dataset and what we want to achieve. Also, our data was a bit too untame to extract useful information from it, and doing some feature engineering helps with that.\n\nWith our dataset properly cleaned, we can now take a look and see how it's distributed (and how the columns relate to each other). A few interesting plots comes to mind:"},{"metadata":{"ExecuteTime":{"end_time":"2018-05-15T11:44:26.920378Z","start_time":"2018-05-15T11:44:26.916376Z"},"_uuid":"a00dfc7f9849216ca8dc1da8ce182daa83b7fa16"},"cell_type":"markdown","source":"### NERC Regions"},{"metadata":{"ExecuteTime":{"end_time":"2018-05-16T12:46:36.604293Z","start_time":"2018-05-16T12:46:36.382224Z"},"trusted":true,"_uuid":"cd39eb1e9e843a5d3f83c16d2c45c528978d5d0d"},"cell_type":"code","source":"dim = (15, 10)\nfig, ax = plt.subplots(figsize=dim)\ntag_plot = sns.countplot(x=\"NERC Region\", ax=ax, data=dataset)\n\nfor item in tag_plot.get_xticklabels():\n    item.set_rotation(45)","execution_count":44,"outputs":[]},{"metadata":{"_uuid":"b73e16e41fe141f65274a6039f6398a391756c7c"},"cell_type":"markdown","source":"### Outage duration per year"},{"metadata":{"ExecuteTime":{"end_time":"2018-05-16T12:46:36.897335Z","start_time":"2018-05-16T12:46:36.605281Z"},"trusted":true,"_uuid":"f391ef0d690fe72ca613e78cfb0bbaed9a8b9ee2"},"cell_type":"code","source":"dim = (15, 10)\nfig, ax = plt.subplots(figsize=dim)\ndemand_plot = sns.boxplot(x=\"Year\", y=\"Duration in Minutes\", ax=ax, data=dataset)\n\nfor item in demand_plot.get_xticklabels():\n    item.set_rotation(45)","execution_count":45,"outputs":[]},{"metadata":{"_uuid":"115f496dde73c3c335dee25b6d4eb6617e845ab5"},"cell_type":"markdown","source":"### Outage duration per NERC region"},{"metadata":{"ExecuteTime":{"end_time":"2018-05-16T12:46:37.30144Z","start_time":"2018-05-16T12:46:36.899354Z"},"trusted":true,"_uuid":"d9e4d18c0a771a94074c9217e1d89603af8466b2"},"cell_type":"code","source":"dim = (15, 10)\nfig, ax = plt.subplots(figsize=dim)\ndemand_plot = sns.boxplot(x=\"NERC Region\", y=\"Duration in Minutes\", ax=ax, data=dataset)\n\nfor item in demand_plot.get_xticklabels():\n    item.set_rotation(45)","execution_count":46,"outputs":[]},{"metadata":{"_uuid":"d428bed47f2fb8763ace31fed66ddbda38ac2126"},"cell_type":"markdown","source":"### Events per year"},{"metadata":{"ExecuteTime":{"end_time":"2018-05-16T12:46:37.479499Z","start_time":"2018-05-16T12:46:37.30244Z"},"trusted":true,"_uuid":"e2fc44547d7ca944e15b1a3b05b499e959530312"},"cell_type":"code","source":"dim = (15, 10)\nfig, ax = plt.subplots(figsize=dim)\nsns.countplot(x=\"Year\", ax=ax, data=dataset)","execution_count":47,"outputs":[]},{"metadata":{"_uuid":"4265a5dd0cbae1953676189e69728fc3f2daa56f"},"cell_type":"markdown","source":"### Long lasting outages"},{"metadata":{"_uuid":"1ff9ce0ee7234b7a39d00f570fc86ad67f348ee2"},"cell_type":"markdown","source":"On the previous section, we separated our dataset in two: one with outages that last longer than 15 days, another with less. We only had 20 such outages, some of them might even just be typing errors, but it's worth taking a look at what this data brings:"},{"metadata":{"ExecuteTime":{"end_time":"2018-05-16T12:46:37.772562Z","start_time":"2018-05-16T12:46:37.481487Z"},"trusted":true,"_uuid":"b86a1337ce1d6d6269b0a7c74119f118542691ec"},"cell_type":"code","source":"dim = (15, 10)\nfig, ax = plt.subplots(figsize=dim)\ndemand_plot = sns.boxplot(x=\"Year\", y=\"Duration in Minutes\", ax=ax, data=long_outages)\n\nfor item in demand_plot.get_xticklabels():\n    item.set_rotation(45)","execution_count":48,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-05-16T12:46:37.975614Z","start_time":"2018-05-16T12:46:37.773562Z"},"trusted":true,"_uuid":"e88dbccd5087feca44626a15a85b55312edbe78a"},"cell_type":"code","source":"dim = (20, 10)\nfig, ax = plt.subplots(figsize=dim)\ntag_plot = sns.countplot(x=\"NERC Region\", ax=ax, data=long_outages)\n\nfor item in tag_plot.get_xticklabels():\n    item.set_rotation(45)","execution_count":49,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-05-16T12:46:38.328705Z","start_time":"2018-05-16T12:46:37.976615Z"},"trusted":true,"_uuid":"45930a6a2c8896c4ae48e408c2f2b2f32d8c82d9"},"cell_type":"code","source":"dim = (20, 10)\nfig, ax = plt.subplots(figsize=dim)\ntag_plot = sns.countplot(x=\"Tags\", ax=ax, data=long_outages)\n\nfor item in tag_plot.get_xticklabels():\n    item.set_rotation(45)","execution_count":50,"outputs":[]},{"metadata":{"_uuid":"e5180f9d15abd9270bdc6f284c4455b7df5335c3"},"cell_type":"markdown","source":"### Feature correlation heatmap"},{"metadata":{"_uuid":"bfa7b9967033504a5847eb599d321012f10aa690"},"cell_type":"markdown","source":"Before implementing the correlation heatmap, we better remove the columns we won't be using anymore:"},{"metadata":{"ExecuteTime":{"end_time":"2018-05-16T12:46:39.166355Z","start_time":"2018-05-16T12:46:38.329705Z"},"trusted":true,"_uuid":"4f0a575a8e345204d7aa1d1dd63afc69e5d46ca8"},"cell_type":"code","source":"#correlation matrix\ncorrmat = dataset.corr()\nf, ax = plt.subplots(figsize=(15, 13))\nsns.heatmap(corrmat, vmax=.8, square=True);","execution_count":51,"outputs":[]},{"metadata":{"_uuid":"e4817d205aea9b89c5be5fa5fdf16d9ee21b2e99"},"cell_type":"markdown","source":"### Analysis"},{"metadata":{"_uuid":"5e2561101550a52787019f928ec61d1e8b3c7e0a"},"cell_type":"markdown","source":"Short duration outages (less than 3 days):\n  * NERC Regions: We can see that the most of the outage occurrences are on the RFC, WECC and SERC regions, which are the part of the old NERC model. This is understandable, since they are big regions which are now described in many smaller pieces (WECC is now other 9 regions). Unfortunately, the only way to transform the old regions into the new ones would be to use the geographic location, which is in terrible shape and won't be of much help. Half of the NERC regions are barely affected by outages\n  * Duration per year: There are some outliers in 2009, 2013 and 2014, however, they are not that absurd and can be kept. This plot show us a trend that, in general, things have been improving after they got worst in 2002 (2000 and 2001 were very good years, apparently). It's interesting to note that there's not always a strong relation between number of outages and duration, since 2011 had a low total duration but the most cases in the last 15 years.\n  * The feature heatmap gave us a few interesting nuggets of wisdom:\n    * PR NERC region is often affected by voltage reduction\n    * Physical events are strongly related to vandalism (who would have thought!)\n    * Severe weather are usually related to storms\n    * Earthquakes are faily common in HECO NERC region "},{"metadata":{"_uuid":"0a940ca02e23f49281564cc074e2a1d28509b1b7"},"cell_type":"markdown","source":"As for the long lasting outages:\n  * 2013 is the biggest outlier in the whole dataset. According to the tags column, it's due to cyber vandalism, which should not take 131 days to recover from (otherwise, this would be highly troubling)\n  * Even though 2014 was a good year in general, it was pretty bad from a long lasting outages perspective.\n  * RFC is the most troublesome regions in this regard, not much different from our analysis on the main dataset.\n  * The biggest reasons for long lasting outages are severe weather, fuel supply and vandalism."},{"metadata":{"_uuid":"ea065e6c3cc5f802dd0543bc44244e4dbfb75ab3"},"cell_type":"markdown","source":"# 4. Data preparation"},{"metadata":{"_uuid":"ebb370d44b3021e1b812f01f701459c6d80ae46a"},"cell_type":"markdown","source":"With all that said and done, we now need to get our feature map and output array:"},{"metadata":{"ExecuteTime":{"end_time":"2018-05-16T12:46:39.173357Z","start_time":"2018-05-16T12:46:39.167344Z"},"collapsed":true,"trusted":true,"_uuid":"e033df448560c8a6843018975d9280cb77fe6619"},"cell_type":"code","source":"output = dataset.iloc[:, 2]\nfeatures = dataset[dataset.columns.difference(['Duration in Minutes','Date Event Began', 'Date of Restoration', 'Year', 'NERC Region'])]","execution_count":52,"outputs":[]},{"metadata":{"_uuid":"b4d8bd0f4487bdb1d5703d085d65ed5ce3f203d3"},"cell_type":"markdown","source":"Since we identified that storm and severe weather are too alike, we can combine it:"},{"metadata":{"ExecuteTime":{"end_time":"2018-05-16T12:46:39.19535Z","start_time":"2018-05-16T12:46:39.175351Z"},"collapsed":true,"trusted":true,"_uuid":"6ca2e6fbb11c233fd6f02034d456aaeb9013b535"},"cell_type":"code","source":"features['severe weather'] = np.where(features['storm'] + features['severe weather'] > 0, 1, 0)\nfeatures = features[features.columns.difference(['storm'])]","execution_count":53,"outputs":[]},{"metadata":{"_uuid":"216dca308cd8ae304e5854b31afeb9dcc161204a"},"cell_type":"markdown","source":"The same goes for vandalism and physical events:"},{"metadata":{"ExecuteTime":{"end_time":"2018-05-16T12:46:39.214368Z","start_time":"2018-05-16T12:46:39.197351Z"},"collapsed":true,"trusted":true,"_uuid":"c0d9e097f41e1fcce7da301c0bd27353590b5617"},"cell_type":"code","source":"features['physical'] = np.where(features['vandalism'] + features['physical'] > 0, 1, 0)\nfeatures = features[features.columns.difference(['physical'])]","execution_count":54,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-05-15T13:01:58.973847Z","start_time":"2018-05-15T13:01:58.964832Z"},"_uuid":"66f32fc3b1d4b25070d0666eb3de794e93d0c70d"},"cell_type":"markdown","source":"Let's take a quick look to make sure we did everything correctly:"},{"metadata":{"ExecuteTime":{"end_time":"2018-05-16T12:46:39.274371Z","start_time":"2018-05-16T12:46:39.215357Z"},"trusted":true,"_uuid":"e8e34cfbb57b3fe2dd06f4bfaf81ecacc0ea0bb8"},"cell_type":"code","source":"features.head()","execution_count":55,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-05-16T12:46:39.297377Z","start_time":"2018-05-16T12:46:39.276372Z"},"trusted":true,"_uuid":"47553dbd8d69bcf228fcf38715dfd0f9a2a498c9"},"cell_type":"code","source":"output.head()","execution_count":56,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-05-16T12:46:39.315382Z","start_time":"2018-05-16T12:46:39.298377Z"},"trusted":true,"_uuid":"cfa633d27dc58db43f2239e1273be24c803fec97"},"cell_type":"code","source":"print('Features size: ' + str(len(features)) + ' - Output size: ' + str(len(output)))","execution_count":57,"outputs":[]},{"metadata":{"_uuid":"5638a6baaeaecc913824b6e15c4f3487fba8b89a"},"cell_type":"markdown","source":"Finally, let's split the dataset into train and test:"},{"metadata":{"ExecuteTime":{"end_time":"2018-05-16T12:46:39.335387Z","start_time":"2018-05-16T12:46:39.316382Z"},"collapsed":true,"trusted":true,"_uuid":"80e293bbd208c67bc8e1f0811c55582743762d54"},"cell_type":"code","source":"features_train, features_test, duration_train, duration_test = train_test_split(features, output, test_size = 0.3, random_state = 0)","execution_count":58,"outputs":[]},{"metadata":{"_uuid":"8e82ca3eeddb5c8a06c346f9c5e72588bc3a20dd"},"cell_type":"markdown","source":"And that's it, we're done. Let's build some models!"},{"metadata":{"_uuid":"4e9f37429d8a600bf69fbe74f84670d93ffa66ab"},"cell_type":"markdown","source":"# 5. First model"},{"metadata":{"_uuid":"bf690b47e0f697bfd5eb1141fc0c8788e8a9c549"},"cell_type":"markdown","source":"We came, we saw and now we need to conquer. Let's start by running a grid search on a Kernel SVR regressor, this will hopefully tell us two things: \n 1. Is the problem linearly separable?\n 2. Is there any hope of finding a good relationship between NERC Region and event type with outage duration? In other words, should we even bother with building models for this dataset?"},{"metadata":{"ExecuteTime":{"end_time":"2018-05-15T23:12:14.093211Z","start_time":"2018-05-15T23:12:14.087202Z"},"collapsed":true,"_uuid":"2fd60e1ccd5a50ae6ef7fa4814a79459a681dd02"},"cell_type":"markdown","source":"Normally we would need to normalize the data in order to properly run Kernel SVM, but since all our features are *onehotencoded*, we can skip this step. We start by defining a function to build our grid search models:"},{"metadata":{"ExecuteTime":{"end_time":"2018-05-16T12:46:39.35039Z","start_time":"2018-05-16T12:46:39.337387Z"},"collapsed":true,"trusted":true,"_uuid":"e5d6d6c40d031e3e0d5d28d604e7130d8561f3d7"},"cell_type":"code","source":"# Run grid search, get the prediction array and print the accuracy and best combination\ndef fit_and_pred_grid_classifier(regressor, param_grid, X_train, X_test, y_train, y_test, folds=5):\n    # Apply grid search with F1 Score to help balance the results (avoid bias on \"no attrition\")\n    grid_search = GridSearchCV(estimator = regressor, param_grid = param_grid, cv = folds, n_jobs = -1, verbose = 0)\n    grid_search.fit(X_train, y_train)\n    best_accuracy = grid_search.best_score_\n    best_parameters = grid_search.best_params_\n\n    # Get the prediction array\n    grid_search_pred = grid_search.predict(X_test)\n\n    # Print the MSE, R2 score and best parameter combination\n    print(\"MSE: \" + str(mean_squared_error(y_test, grid_search_pred))) \n    print(\"R2: \" + str(r2_score(y_test, grid_search_pred))) \n    print(\"Best parameter combination: \" + str(best_parameters)) \n    \n    return grid_search_pred","execution_count":59,"outputs":[]},{"metadata":{"_uuid":"186e4053903267ef564f4a391e3e37fa74e29845"},"cell_type":"markdown","source":"And finally, we can build our first regressor:"},{"metadata":{"ExecuteTime":{"end_time":"2018-05-16T12:56:01.531435Z","start_time":"2018-05-16T12:55:43.570847Z"},"trusted":true,"_uuid":"ba64b134e2dfd7a5a1d977c7b8bfd5935bcb32bf"},"cell_type":"code","source":"regressor = SVR(kernel='rbf', C=10, gamma=0.1)\nparam_grid = [\n    {\n        'C': [400, 450, 500, 550, 600], \n        'kernel': ['linear'],\n        'epsilon': [550, 600, 650, 700, 750],\n    }, \n    {\n        'C': [12, 13, 14, 15, 20], \n        'kernel': ['rbf', 'sigmoid'], \n        'gamma': [0.3, 0.4, 0.5, 1],\n        'epsilon': [10, 100, 500, 750, 1000],\n    },\n]\n# Build and fit the grid search SVR model\npred = fit_and_pred_grid_classifier(regressor, param_grid, features_train, features_test, duration_train, duration_test)","execution_count":60,"outputs":[]},{"metadata":{"_uuid":"917e73667214f18b78b9360f625febd530766353"},"cell_type":"markdown","source":"YIKES! Pretty bad MSE and R2 numbers after doing so much feature engineering and analysis is worrisome. Let's take a look at what we're getting:"},{"metadata":{"ExecuteTime":{"end_time":"2018-05-16T12:57:29.548997Z","start_time":"2018-05-16T12:57:26.848361Z"},"trusted":true,"_uuid":"355dca2bf638afb5a446331eebdcf5d3f8b5f4f6"},"cell_type":"code","source":"X_grid = np.arange(1, 367)\n\ndim = (20, 10)\nfig, ax = plt.subplots(figsize=dim)\nax.set_xticks([])\nplot_1 = sns.swarmplot(x=X_grid, y=duration_test, color='green')\nplot_2 = sns.swarmplot(x=X_grid, y=pred, color='red')\nsns.set_context(\"notebook\", font_scale=1.5, rc={\"lines.linewidth\": 2.5})\n\nfor item in plot_1.get_xticklabels():\n    item.set_visible(False)\n    \nfor item in plot_2.get_xticklabels():\n    item.set_visible(False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a1e616547e378cba3c7963b7490f89569edb03c0"},"cell_type":"markdown","source":"We can see that our model is often not predicting well, there were very few cases where it got it *almost* right. So, what now? Now we make a few more sad attempts of modeling this dataset."},{"metadata":{"_uuid":"e21f592d2eb25779b51cf00897c4b6381c3b434b"},"cell_type":"markdown","source":"# 6. More models!"},{"metadata":{"_uuid":"24ced02f37f32f41bba8fa7c83976fbfb924d974"},"cell_type":"markdown","source":"As we've seen from the past section, things are rather dire. As it stands, it seems that our data is not fit to predict the duration of an outage only based on NERC region and event type. It could be that, after all, outages are unpredictable. However, we'll not give up without a fight, and so let's try a few more models before hitting the shower. Will we succeed? Will we fail? We will probably fail, but have some fun while doing so:"},{"metadata":{"ExecuteTime":{"end_time":"2018-05-16T13:01:13.124698Z","start_time":"2018-05-16T13:01:13.122697Z"},"_uuid":"799bd61416d2905975b62ca0efbaa52edf469582"},"cell_type":"markdown","source":"### Random forest"},{"metadata":{"ExecuteTime":{"end_time":"2018-05-16T14:04:29.905033Z","start_time":"2018-05-16T14:04:15.810532Z"},"trusted":true,"_uuid":"47b0654bf94877b714f08ac30e2dcc716080ff47"},"cell_type":"code","source":"regressor = DecisionTreeRegressor()\nparam_grid = {\n    'max_depth': [ 50, 75, 85, 90, 95, 100, 105, 110],\n    'max_features': [10, 13, 14, 15, 16, 17, 18, 19, 20],\n    'min_samples_leaf': [10, 11, 12, 13, 14, 15],\n    'min_samples_split': [5, 6, 7, 8, 9, 10]\n}\npred = fit_and_pred_grid_classifier(regressor, param_grid, features_train, features_test, duration_train, duration_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6299585369b4855d0297ce1a75ea7b102b602657"},"cell_type":"markdown","source":"### Bayesian ridge"},{"metadata":{"ExecuteTime":{"end_time":"2018-05-16T13:53:33.350139Z","start_time":"2018-05-16T13:53:15.753933Z"},"trusted":true,"collapsed":true,"_uuid":"805d9342a8009d8903b6ad89f56babd199f75f87"},"cell_type":"code","source":"regressor = BayesianRidge()\nparam_grid = [\n    {\n        'alpha_1': [125, 150, 175, 200, 225, 250],\n        'alpha_2': [0.00000001, 0.0000001, 0.000001],\n        'lambda_1': [0.00000001, 0.0000001, 0.000001],\n        'lambda_2': [125, 150, 175, 200, 225, 250]\n    }, \n]\npred = fit_and_pred_grid_classifier(regressor, param_grid, features_train, features_test, duration_train, duration_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"28b7cb7ddfd18a86f871624c73d68dce47868178"},"cell_type":"markdown","source":"### KNN"},{"metadata":{"ExecuteTime":{"end_time":"2018-05-16T13:55:49.637288Z","start_time":"2018-05-16T13:55:38.073199Z"},"trusted":true,"collapsed":true,"_uuid":"def345bcf5c3915c23d245bafad6aa371629f662"},"cell_type":"code","source":"regressor = KNeighborsRegressor()\nparam_grid = {\n    'n_neighbors': [1,2,4,5,10,20],\n    'weights': ['distance', 'uniform'],\n    'algorithm': ['ball_tree', 'kd_tree', 'brute'],\n    'metric': ['minkowski','euclidean','manhattan'], \n    'p': [1, 2]\n}\npred = fit_and_pred_grid_classifier(regressor, param_grid, features_train, features_test, duration_train, duration_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f5360adfb6b1f78bb100eee53f92c67156074796"},"cell_type":"markdown","source":"# 7. Back to the drawing board"},{"metadata":{"_uuid":"9ddff631eb44f14cc4b1a669da037c8da6d50d58"},"cell_type":"markdown","source":"Alas, it was not meant to be. We could probably build some complex ensembles and try advanced techniques, but it would not help much. When a simple model can't even achieve above 30% R2 it's a data issue, not a modeling one.\n\nThat being said, our exploratory data analysis was quite successful as we were able to extract good information out of the data. It could be improved further, however, by splitting the outage duration in more windows (e.g. from 3 days to a week, from one week to two weeks, from two weeks to a month, etc), separating the data by year (or, at least, by NERC epochs), etc.\n\nThe last question remaining is: can we get any meaningful models out of this dataset? Probably. there are a few things that could be done in order to help create a meaningful model:\n 1. Fix the geographic location so that it becomes usable (much more useful than a NERC region), but it should take quite a lot of effort.\n 2. Simplify the feature map by cutting down the event columns, keeping only the most common ones.\n 3. Split the dataset by year (as we've seen on EDA, every year is very different from another, there's no consistency)\n 4. Build a model for outliers only, since long lasting outages are more dangerous and thus more interesting to prepare for."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false}},"nbformat":4,"nbformat_minor":1}