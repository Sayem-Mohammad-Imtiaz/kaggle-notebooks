{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"\n## Overview\n\nThis script performs EDA and then fine-tunes BERT to perform sentiment analysis on a dataset of plain-text IMDB movie reviews. In addition to training a model, it preprocesses text into an appropriate format.\n","metadata":{}},{"cell_type":"markdown","source":"## NOTE : \n\nVersion 1 of the script uses the dataset [imdb-dataset-sentiment-analysis-in-csv-format](https://www.kaggle.com/columbine/imdb-dataset-sentiment-analysis-in-csv-format) whereas Version 2 uses the dataset [imdb-dataset-of-50k-movie-reviews](https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews)\n\nPlease select appropriate version according to the dataset analyzed","metadata":{}},{"cell_type":"markdown","source":"\n## Installing and importing dependencies\n\nFirst let us import all the modules and packages that will be required.\n","metadata":{}},{"cell_type":"code","source":"# A dependency of the preprocessing for BERT inputs\n!pip data.shapeinstall -q tensorflow-text","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -q tf-models-official","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport tensorflow as tf\nimport tensorflow_hub as hub\nimport tensorflow_text as text\nfrom official.nlp import optimization  # to create AdamW optimizer\n\n#for visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ntf.get_logger().setLevel('ERROR')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Loading and Exploring dataset\n\n\n\nThere is one file provided to us in this dataset:\n\n * `IMDB-Dataset.csv`: The CSV file containing all the reviews and their polarity.\n \nLet's read the file.\n","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv(r'../input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Thus there are 50000 rows and 2 columns in the `data` dataframe ","metadata":{}},{"cell_type":"code","source":"data.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see the `sentiment` column has 2 values:\n\n   * Negative Sentiment\n   * Postive Sentiment\n  ","metadata":{}},{"cell_type":"code","source":"data.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we encode the positive sentiment to 1 and negative sentiment to 0","metadata":{}},{"cell_type":"code","source":"labeling = {\n    'positive':1, \n    'negative':0\n}\n\ndata['sentiment'] = data['sentiment'].apply(lambda x : labeling[x])\n# Output first ten rows\ndata.head(10)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Thus, there are no missing values in any of the columns of the dataset.","metadata":{}},{"cell_type":"markdown","source":"## EDA","metadata":{}},{"cell_type":"code","source":"# The distribution of sentiments\ndata.groupby('sentiment').count().plot(kind='bar')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This means that the no. of positive reviews is equal to the no. of negative reviews in the dataset. This is a good thing since it means our dataset is not skewed.","metadata":{}},{"cell_type":"code","source":"# Calculate review lengths\nreview_len = pd.Series([len(review.split()) for review in data['review']])\n\n# The distribution of review text lengths\nreview_len.plot(kind='box')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, let us visualize how long our sentences are in the training data.","metadata":{}},{"cell_type":"code","source":"sns.set_theme(\n    context='notebook',\n    style='darkgrid',\n    palette='deep',\n    font='sans-serif',\n    font_scale=1,\n    color_codes=True,\n    rc=None,\n)\n\nplt.figure(figsize = (10,12))\nsns.histplot(review_len)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize=(14,7))\ndata['length'] = data.review.str.split().apply(len)\nax1 = fig.add_subplot(122)\nsns.histplot(data[data['sentiment']==1]['length'], ax=ax1,color='green')\ndescribe = data.length[data.sentiment==1].describe().to_frame().round(2)\n\nax2 = fig.add_subplot(121)\nax2.axis('off')\nfont_size = 14\nbbox = [0, 0, 1, 1]\ntable = ax2.table(cellText = describe.values, rowLabels = describe.index, bbox=bbox, colLabels=describe.columns)\ntable.set_fontsize(font_size)\nfig.suptitle('Distribution of text length for positive sentiment reviews.', fontsize=16)\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize=(14,7))\nax1 = fig.add_subplot(122)\nsns.histplot(data[data['sentiment']==0]['length'], ax=ax1,color='red')\ndescribe = data.length[data.sentiment==0].describe().to_frame().round(2)\n\nax2 = fig.add_subplot(121)\nax2.axis('off')\nfont_size = 14\nbbox = [0, 0, 1, 1]\ntable = ax2.table(cellText = describe.values, rowLabels = describe.index, bbox=bbox, colLabels=describe.columns)\ntable.set_fontsize(font_size)\nfig.suptitle('Distribution of text length for Negative sentiment reviews.', fontsize=16)\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### WORDCLOUD FOR NEGATIVE TEXT (LABEL - 0)","metadata":{}},{"cell_type":"code","source":"from wordcloud import WordCloud\nplt.figure(figsize = (20,20)) # Negative Review Text\nwc = WordCloud(max_words = 2000 , width = 1600 , height = 800).generate(\" \".join(data[data.sentiment == 0].review))\nplt.imshow(wc , interpolation = 'bilinear')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### WORDCLOUD FOR POSITIVE TEXT (LABEL - 1)","metadata":{}},{"cell_type":"code","source":"from wordcloud import WordCloud\nplt.figure(figsize = (20,20)) # Positive Review Text\nwc = WordCloud(max_words = 2000 , width = 1600 , height = 800).generate(\" \".join(data[data.sentiment == 1].review))\nplt.imshow(wc , interpolation = 'bilinear')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## BERT\n\nBERT (Bidirectional Encoder Representations from Transformers) is a recent paper published by researchers at Google AI Language. It has caused a stir in the Machine Learning community by presenting state-of-the-art results in a wide variety of NLP tasks, including Question Answering (SQuAD v1.1), Natural Language Inference (MNLI), and others.\n\nBERT’s key technical innovation is applying the bidirectional training of Transformer, a popular attention model, to language modelling. This is in contrast to previous efforts which looked at a text sequence either from left to right or combined left-to-right and right-to-left training. The paper’s results show that a language model which is bidirectionally trained can have a deeper sense of language context and flow than single-direction language models. In the paper, the researchers detail a novel technique named Masked LM (MLM) which allows bidirectional training in models in which it was previously impossible.\n\nPaper: [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)","metadata":{}},{"cell_type":"markdown","source":"## How to use BERT (Fine-tuning)\n\nUsing BERT for a specific task is relatively straightforward:\n\nBERT can be used for a wide variety of language tasks, while only adding a small layer to the core model:\n\n * Classification tasks such as sentiment analysis are done similarly to Next Sentence classification, by adding a classification layer on top of the Transformer output for the [CLS] token.\n * In Question Answering tasks (e.g. SQuAD v1.1), the software receives a question regarding a text sequence and is required to mark the answer in the sequence. Using BERT, a Q&A model can be trained by learning two extra vectors that mark the beginning and the end of the answer.\n * In Named Entity Recognition (NER), the software receives a text sequence and is required to mark the various types of entities (Person, Organization, Date, etc) that appear in the text. Using BERT, a NER model can be trained by feeding the output vector of each token into a classification layer that predicts the NER label.\n \n \nRefs: https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270","metadata":{}},{"cell_type":"markdown","source":"## Loading model from TensorFlow Hub\n\nHere you can choose which BERT model you will load from TensorFlow Hub and fine-tune. There are multiple BERT models available.\n\n  - [BERT-Base](https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3), [Uncased](https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3) and [seven more models](https://tfhub.dev/google/collections/bert/1) with trained weights released by the original BERT authors.\n  - [Small BERTs](https://tfhub.dev/google/collections/bert/1) have the same general architecture but fewer and/or smaller Transformer blocks, which lets you explore tradeoffs between speed, size and quality.\n  - [ALBERT](https://tfhub.dev/google/collections/albert/1): four different sizes of \"A Lite BERT\" that reduces model size (but not computation time) by sharing parameters between layers.\n  - [BERT Experts](https://tfhub.dev/google/collections/experts/bert/1): eight models that all have the BERT-base architecture but offer a choice between different pre-training domains, to align more closely with the target task.\n  - [Electra](https://tfhub.dev/google/collections/electra/1) has the same architecture as BERT (in three different sizes), but gets pre-trained as a discriminator in a set-up that resembles a Generative Adversarial Network (GAN).\n  - BERT with Talking-Heads Attention and Gated GELU [[base](https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_base/1), [large](https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_large/1)] has two improvements to the core of the Transformer architecture.\n\nThe model documentation on TensorFlow Hub has more details and references to the\nresearch literature. Follow the links above, or click on the [`tfhub.dev`](http://tfhub.dev) URL\nprinted after the next cell execution.\n\nHere, we have used a Small BERT (with fewer parameters) since they are faster to fine-tune. If you like a small model but with higher accuracy, ALBERT might be your next option. If you want even better accuracy, choose\none of the classic BERT sizes or their recent refinements like Electra, Talking Heads, or a BERT Expert.\n","metadata":{}},{"cell_type":"code","source":"bert_model_name = 'small_bert/bert_en_uncased_L-4_H-512_A-8'  \n\ntfhub_handle_encoder = 'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1'\ntfhub_handle_preprocess = 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'\n\nprint(f'BERT model selected           : {tfhub_handle_encoder}')\nprint(f'Preprocess model auto-selected: {tfhub_handle_preprocess}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## The preprocessing model\n\nText inputs need to be transformed to numeric token ids and arranged in several Tensors before being input to BERT. TensorFlow Hub provides a matching preprocessing model for each of the BERT models discussed above, which implements this transformation using TF ops from the TF.text library. It is not necessary to run pure Python code outside your TensorFlow model to preprocess text.\n\nThe preprocessing model must be the one referenced by the documentation of the BERT model. \n\nNote: You will load the preprocessing model into a [hub.KerasLayer](https://www.tensorflow.org/hub/api_docs/python/hub/KerasLayer) to compose your fine-tuned model. This is the preferred API to load a TF2-style SavedModel from TF Hub into a Keras model.","metadata":{}},{"cell_type":"code","source":"bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's try the preprocessing model on sample text and see the output:","metadata":{}},{"cell_type":"code","source":"text_test = ['this is such an amazing movie!']\ntext_preprocessed = bert_preprocess_model(text_test)\n\nprint(f'Keys       : {list(text_preprocessed.keys())}')\nprint(f'Shape      : {text_preprocessed[\"input_word_ids\"].shape}')\nprint(f'Word Ids   : {text_preprocessed[\"input_word_ids\"][0, :12]}')\nprint(f'Input Mask : {text_preprocessed[\"input_mask\"][0, :12]}')\nprint(f'Type Ids   : {text_preprocessed[\"input_type_ids\"][0, :12]}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As you can see, now you have the 3 outputs from the preprocessing that a BERT model would use (`input_words_id`, `input_mask` and `input_type_ids`).\n\nSome other important points:\n- The input is truncated to 128 tokens. \n- The `input_type_ids` only have one value (0) because this is a single sentence input. For a multiple sentence input, it would have one number for each input.\n\nSince this text preprocessor is a TensorFlow model, It can be included in your model directly.","metadata":{}},{"cell_type":"code","source":"bert_model = hub.KerasLayer(tfhub_handle_encoder)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Defining model\n\nWe have created a very simple fine-tuned model, with the preprocessing model, the selected BERT model, one Dense and a Dropout layer.","metadata":{}},{"cell_type":"code","source":"def build_classifier_model():\n  text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n  preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n  encoder_inputs = preprocessing_layer(text_input)\n  encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n  outputs = encoder(encoder_inputs)\n  net = outputs['pooled_output']\n  net = tf.keras.layers.Dropout(0.1)(net)\n  net = tf.keras.layers.Dense(1, activation=None, name='classifier')(net)\n  return tf.keras.Model(text_input, net)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For the fine-tuning I have used the`pooled_output` array which represents each input sequence as a whole. The shape is [batch_size, H]. You can think of this as an embedding for the entire movie review.","metadata":{}},{"cell_type":"code","source":"classifier_model = build_classifier_model()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.keras.utils.plot_model(classifier_model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model training\n\nNext we train a model since we have the preprocessing module, BERT encoder, data, and classifier.","metadata":{}},{"cell_type":"markdown","source":"### Loss function\n\nSince this is a binary classification problem and the model outputs a probability (a single-unit layer), we'll use `losses.BinaryCrossentropy` loss function.","metadata":{}},{"cell_type":"code","source":"loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\nmetrics = tf.metrics.BinaryAccuracy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Optimizer\n\nFor fine-tuning, let's use the same optimizer that BERT was originally trained with: the \"Adaptive Moments\" (Adam). This optimizer minimizes the prediction loss and does regularization by weight decay (not using moments), which is also known as [AdamW](https://arxiv.org/abs/1711.05101).","metadata":{}},{"cell_type":"code","source":"epochs = 5\nsteps_per_epoch = 625 #tf.data.experimental.cardinality(train_data).numpy()\nnum_train_steps = steps_per_epoch * epochs\nnum_warmup_steps = int(0.1*num_train_steps)\n\ninit_lr = 3e-5\noptimizer = optimization.create_optimizer(init_lr=init_lr,\n                                          num_train_steps=num_train_steps,\n                                          num_warmup_steps=num_warmup_steps,\n                                          optimizer_type='adamw')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next we compile the model with the loss, metric and optimizer.","metadata":{}},{"cell_type":"code","source":"classifier_model.compile(optimizer=optimizer,\n                         loss=loss,\n                         metrics=metrics)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, we define the test and validation variables to input in the model for evaluation and prediction.","metadata":{}},{"cell_type":"code","source":"X_train = train_data['text']\ny_train = train_data['label']\nX_valid = valid_data['text']\ny_valid = valid_data['label']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = data['review']\nfrom sklearn.model_selection import train_test_split\n#X_train,X_test,y_train,y_test = train_test_split(data.review,data.sentiment,random_state = 0 , stratify = data.sentiment)\n\n#valid_size=5000\n#X_valid, y_valid = X_train[-valid_size:], y_train[-valid_size:]\n#X_test, y_test = X_train[:-valid_size], y_train[:-valid_size]\nX_train, X_test, y_train, y_test = train_test_split(data.review,data.sentiment, test_size=0.2, random_state=1)\n\nX_train, X_val, y_train, y_val  = train_test_split(X_train, y_train, test_size=0.16, random_state=1) # 0.25 x 0.8 = 0.2\n#print('Train Set ->', X_train.shape, y_train.shape)\n#print('Validation Set ->', X_valid.shape, y_valid.shape)\n#print('Test Set ->', X_test.shape, y_test.shape)\n\nprint('Number of reviews in the total set : {}'.format(len(X)))\nprint('Number of reviews in the training set : {}'.format(len(X_train)))\nprint('Number of reviews in the validation set : {}'.format(len(X_valid)))\nprint('Number of reviews in the testing set : {}'.format(len(X_test)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'Training model with {tfhub_handle_encoder}')\nhistory = classifier_model.fit(X_train, y_train,\n                               validation_data=(X_valid, y_valid),\n                               epochs=epochs)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluate the model\n\nLet's see how the model performs. Two values will be returned. Loss (a number which represents the error, lower values are better), and accuracy.","metadata":{}},{"cell_type":"code","source":"#X_test = test_data['text']\n#y_test = test_data['label']\nloss, accuracy = classifier_model.evaluate(X_test, y_test)\n\nprint(f'Loss: {loss}')\nprint(f'Accuracy: {accuracy}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history_dict = history.history\nprint(history_dict.keys())\n\nacc = history_dict['binary_accuracy']\nval_acc = history_dict['val_binary_accuracy']\nloss = history_dict['loss']\nval_loss = history_dict['val_loss']\n\nepochs = range(1, len(acc) + 1)\nfig = plt.figure(figsize=(10, 6))\nfig.tight_layout()\n\nplt.subplot(2, 1, 1)\n# \"bo\" is for \"blue dot\"\nplt.plot(epochs, loss, 'r', label='Training loss')\n# b is for \"solid blue line\"\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\n# plt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.subplot(2, 1, 2)\nplt.plot(epochs, acc, 'r', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend(loc='lower right')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Saving Model\n\nSaving our fine-tuned model for later use.","metadata":{}},{"cell_type":"code","source":"dataset_name = 'imdb_2'\nsaved_model_path = './{}_bert'.format(dataset_name.replace('/', '_'))\n\nclassifier_model.save(saved_model_path, include_optimizer=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Reloading saved model","metadata":{}},{"cell_type":"code","source":"reloaded_model = tf.saved_model.load(saved_model_path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" Testing our model on any sentence we want, by adding to the `examples` variable below.","metadata":{}},{"cell_type":"code","source":"def print_my_examples(inputs, results):\n  result_for_printing = \\\n    [f'input: {inputs[i]:<30} : score: {results[i][0]:.6f}'\n                         for i in range(len(inputs))]\n  print(*result_for_printing, sep='\\n')\n  print()\n\n\nexamples = [\n    'this is such an amazing movie!',  # this is the same sentence tried earlier\n    'The movie was great!',\n    'The movie was meh.',\n    'The movie was idiotic.',\n    'The movie was terrible...'\n]\n\nreloaded_results = tf.sigmoid(reloaded_model(tf.constant(examples)))\noriginal_results = tf.sigmoid(classifier_model(tf.constant(examples)))\n\nprint('Results from the saved model:')\nprint_my_examples(examples, reloaded_results)\nprint('Results from the model in memory:')\nprint_my_examples(examples, original_results)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}