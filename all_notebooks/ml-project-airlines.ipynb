{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Exploratory Data Analysis (EDA)"},{"metadata":{},"cell_type":"markdown","source":"# Intro\nThis data shows whether a customer is satisfied with the airlines or not after travelling with them. There are several other measurement or to say feedback taken from the customers as well as their demographic data is also recorded.\n\nData set URL : https://www.kaggle.com/teejmahal20/airline-passenger-satisfaction\n"},{"metadata":{},"cell_type":"markdown","source":"# STEP #1: Import Libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport random as rnd\n\nfrom collections import Counter\n\n# From Sklearn, sub-library model_selection, train_test_split so I can, well, split to training and test sets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import make_scorer, accuracy_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import plot_roc_curve\nfrom sklearn.model_selection import learning_curve, validation_curve\nfrom sklearn.metrics import roc_curve, roc_auc_score\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.svm import SVC\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\n\n%matplotlib inline\nplt.style.use(\"seaborn-whitegrid\")\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# STEP #2: IMPORT DATASET"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = pd.read_csv(\"../input/airlines-customer-satisfaction/Invistico_Airline.csv\")\ndata = dataset.sample(frac=.05)\ndata.sample(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# STEP #3: Explore /Visualze Data set"},{"metadata":{},"cell_type":"markdown","source":"## STEP #3.1: Check the count / data type for each feature in train and test data files"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"as shown above, there are missing values in Arrival Delay in Minutes feature in dataset"},{"metadata":{},"cell_type":"markdown","source":"## STEP #3.2: Features analysis"},{"metadata":{},"cell_type":"markdown","source":"By checking the features, there are two types of features:\n   * Categorical Features: satisfaction, Gender, Customer Type, Type of Travel, Class.\n   * Numerical Features: Age, Flight Distance, Seat comfort, Departure/Arrival time convenient, Food and drink, Gate location, Inflight wifi service, Inflight entertainment, Online support, Ease of Online booking, On-board service, Leg room service, Baggage handling, Checkin service, Cleanliness, Online boarding, Departure Delay in Minutes, Arrival Delay in Minutes."},{"metadata":{},"cell_type":"markdown","source":"## STEP #3.3: Categorical Analysis:\n\ncheck the specific values for each of categorical features"},{"metadata":{"trusted":true},"cell_type":"code","source":"category = [\"satisfaction\", \"Gender\", \"Customer Type\", \"Type of Travel\", \"Class\"]\nfor c in category:\n    print (\"{} \\n\".format(data[c].value_counts()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"so, we conclude that all the categorical features have specific values"},{"metadata":{},"cell_type":"markdown","source":"## STEP #3.4: Mapping"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Mapping satisfied and dissatisfied in number \nsatisfaction_mapping = {\"satisfied\": 1,\"dissatisfied\": 0 }\ndata['satisfaction']  = data['satisfaction'].map(satisfaction_mapping)\n\n#Mapping Male and Female in number \nGender_mapping = {\"Male\": 1,\"Female\": 2 }\ndata['Gender']  = data['Gender'].map(Gender_mapping)\n\n#Mapping Loyal and disloyal in number \nCustomer_Type_mapping = {\"Loyal Customer\": 1,\"disloyal Customer\": 0 }\ndata['Customer Type']  = data['Customer Type'].map(Customer_Type_mapping)\n\n#Mapping Business travel and Business travel in number \nType_of_Travel_mapping = {\"Business travel\": 1,\"Personal Travel\": 2 }\ndata['Type of Travel']  = data['Type of Travel'].map(Type_of_Travel_mapping)\n\n#Mapping Business and Eco and Eco plus in number \nClass_mapping = {\"Business\": 1,\"Eco\": 3, \"Eco Plus\": 2 }\ndata['Class']  = data['Class'].map(Class_mapping)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## STEP #3.5: Numerical Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15, 5))\nplt.subplot(121)\nsns.countplot(x='Class',data=data)\nplt.subplot(122)\nsns.countplot(x='Class',hue='satisfaction',data=data)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"numericVar = [\"Age\", \"Flight Distance\", \"Seat comfort\", \"Departure/Arrival time convenient\", \"Food and drink\", \"Gate location\", \"Inflight wifi service\", \"Inflight entertainment\", \"Online support\", \"Ease of Online booking\", \"On-board service\", \"Leg room service\", \"Baggage handling\", \"Checkin service\", \"Cleanliness\", \"Online boarding\", \"Departure Delay in Minutes\", \"Arrival Delay in Minutes\"]\n\nfig, axs = plt.subplots(nrows=9, ncols=2,figsize=(20,25))\n\nrow = 0\ncol = 0\nfor n in numericVar:\n    if(col==2):\n        row+=1\n        col=0\n    axs[row,col].hist(data[n], bins = 50)\n    axs[row,col].set_xlabel(n)\n    axs[row,col].set_ylabel('Frequency')\n    axs[row,col].set_title(\"{} distribution with hist\".format(n))\n    \n    col+=1\n    \nfig.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(80,30))\nsns.countplot(x='Age',hue='satisfaction',data=data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## STEP #3.6: Prepare the Data for Training / Data Cleaning\nfind the best way to fill the missing values in each feature\n### Find the missing values in train and test data files\nCount the null values in each column to decide to drop these rows or replace the values\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# import missingno\n# # Plot graphic of missing values\n# missingno.matrix(data, figsize = (30,20))\n\n\nsns.heatmap(data.isnull(),cmap='Blues')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Fill Missing Values\ndata['Arrival Delay in Minutes']=data['Arrival Delay in Minutes'].fillna(data['Arrival Delay in Minutes'].mean()) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(data.isnull(),cmap='Blues')\n# sns.heatmap(dataset.isnull(),yticklabels=False,cbar=False,cmap='YlGnBu')","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,10))\nsns.boxplot(x='Gender',y='Age',data=data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"from shown above in dataset, missing values for Arrival Delay in Minutes column are not that much (didnot get more than 90%), so we are going to keep them"},{"metadata":{},"cell_type":"markdown","source":"==================================================================================================================\n\n## STEP #3.7: Visualization"},{"metadata":{},"cell_type":"markdown","source":"### 7.1- Correlation Between numeric values (Satisfaction, Gender, Customer Type, Age, Type of Travel, Class, Flight Distance, Seat comfort, Departure/Arrival time convenient, Food and drink, Gate location, Inflight wifi service, Inflight entertainment, Online support, Ease of Online booking, On-board service, Leg room service, Baggage handling, Checkin service, Cleanliness, Online boarding, Departure Delay in Minutes, Arrival Delay in Minutes)"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"list1 =[\"satisfaction\", \"Gender\", \"Customer Type\", \"Age\" , \"Type of Travel\", \"Class\" , \"Flight Distance\" , \"Seat comfort\" ,\"Departure/Arrival time convenient\" ,\"Food and drink\"\n, \"Gate location\" ,\"Inflight wifi service\",\"Inflight entertainment\",\"Online support\",\"Ease of Online booking\"\n,\"On-board service\",\"Leg room service\",\"Baggage handling\",\"Checkin service\",\"Cleanliness\",\"Online boarding\"\n,\"Departure Delay in Minutes\", \"Arrival Delay in Minutes\"]\n\nplt.subplots(figsize=(15,15)) \nsns.heatmap(data[list1].corr(), annot = True, fmt = \".2f\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Departure delay in Minutes and Arrival Delay in Minutes features have the weakest correlation with all other features, so these twi feature are going to be dropped.\n* Food and drink feature has the strongest correlation with seat comfort feature (0.72).\n* Ease of Online booking, Online boarding, Online support, Cleanless, Baggage handling, and inflight wifi service features are considered to be have the secong strong correclation between all of them (~0.6)\n* Food drink, Online suppor, Cleanless, Baggage handling, Gate location, Onboard service, inflight wifi service features are considered to be have the secong strong correclation between all of them (~0.5)\n* satisfaction feature has a strong correlation with inflight entertainment and Ease of Online booking\n* Now, visualize each of these features with the satisfaction feature"},{"metadata":{"trusted":true},"cell_type":"code","source":"#define a function, so that we can make bar chart for every feature. \ndef barchart(feature):\n    g = sns.barplot(x=feature,y=\"satisfaction\",data=data)\n    g = g.set_ylabel(\"Satisfaction Probability\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## STEP #3.8: Check features with satisfaction"},{"metadata":{"trusted":true},"cell_type":"code","source":"# For Gender feature.\nbarchart('Gender')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# For Customer Type feature.\nbarchart('Customer Type')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# For Class feature.\nbarchart('Class')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# For Type of Travel feature.\nbarchart('Type of Travel')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# For Age feature.\ng = sns.FacetGrid(data, col = \"satisfaction\")\ng.map(sns.distplot, \"Age\", bins = 25)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For Ages between 20 and 40 have high dissatisfaction, while the Ages more than 40 have high satisfaction."},{"metadata":{"trusted":true},"cell_type":"code","source":"# For Flight distance feature.\ng = sns.FacetGrid(data, col = \"satisfaction\")\ng.map(sns.distplot, \"Flight Distance\", bins = 25)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(x=\"Customer Type\", y=\"satisfaction\", hue=\"Gender\", data=data);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(x=\"Class\", y=\"satisfaction\", hue=\"Gender\", data=data);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(x=\"Type of Travel\", y=\"satisfaction\", hue=\"Gender\", data=data);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"===========================================================================================\n\n## STEP #3.9: Feature Engineering"},{"metadata":{},"cell_type":"markdown","source":"### 8.1- Arrival Delay in Minutes Feature"},{"metadata":{},"cell_type":"markdown","source":"Feature Arrival Delay in Minutes has no correlation with any other feature, so this feature is going to be dropped"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data.drop(['Arrival Delay in Minutes'], axis=1)\ndata.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 8.2- Departure Delay in Minutes Feature"},{"metadata":{},"cell_type":"markdown","source":"Feature Departure Delay in Minutes has no correlation with any other feature, so this feature is going to be dropped"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data.drop(['Departure Delay in Minutes'], axis=1)\ndata.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# STEP #4: Split dataset to train / test data"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = data.drop('satisfaction',axis=1).values \ny = data['satisfaction'].values\n\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2, random_state = 100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# y = data.satisfaction\n# data = data.drop('satisfaction',axis=1)\n\n# # split train and test data\n# x_train,x_test,y_train,y_test = train_test_split(data,y,test_size=0.2)\n\nprint (X_train.shape, y_train.shape)\nprint (X_test.shape, y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# STEP #5: Fitting and Tuning an Algorithm"},{"metadata":{},"cell_type":"markdown","source":"## Define a general function for showing the Learning curve for any classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plotLearningCurves(X_train, y_train, classifier, title):\n    train_sizes, train_scores, test_scores = learning_curve(\n            classifier, X_train, y_train, cv=5, scoring=\"accuracy\")\n    \n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n\n    plt.plot(train_sizes, train_scores_mean, label=\"Training Error\")\n    plt.plot(train_sizes, test_scores_mean, label=\"Cross Validation Error\")\n    \n    plt.legend()\n    plt.grid()\n    plt.title(title, fontsize = 18, y = 1.03)\n    plt.xlabel('Training Error', fontsize = 14)\n    plt.ylabel('Cross Validation Error', fontsize = 14)\n    plt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plotValidationCurves(X_train, y_train, classifier, param_name, param_range, title):\n    train_scores, test_scores = validation_curve(\n        classifier, X_train, y_train, param_name = param_name, param_range = param_range,\n        cv=5, scoring=\"accuracy\")\n\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n\n    plt.plot(param_range, train_scores_mean, label=\"Training Error\")\n    plt.plot(param_range, test_scores_mean, label=\"Cross Validation Error\")\n\n    plt.legend()\n    plt.grid()\n    plt.title(title, fontsize = 18, y = 1.03)\n    plt.xlabel('Training Error', fontsize = 14)\n    plt.ylabel('Cross Validation Error', fontsize = 14)\n    plt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## STEP #5.1: Logistic Regression Calssifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"# # LogisticRegression\n# from sklearn.linear_model import LogisticRegression\n# LR = LogisticRegression()\n# LR.fit(X_train,y_train)\n\n# # making predictions on the testing set \n# y_perdict_test = LR.predict(X_test)\n\n# # from sklearn.metrics import confusion_matrix,classification_report\n# # cm = confusion_matrix(y_test,y_perdict_test)\n# # sns.heatmap(cm,annot=True)\n\n  \n# # comparing actual response values (y_test) with predicted response values (y_pred) \n# from sklearn.metrics import accuracy_score\n\n# LR_accuracy = accuracy_score(y_test,y_perdict_test)\n# print(\"Logistic Regression model accuracy:\", LR_accuracy) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## STEP #5.2: Random Forest Calssifier"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# Choose some initial parameters combinations to try\nrondomForestClf = RandomForestClassifier(n_estimators=9, # no of sample trees\n                             # max_features=['log2', 'sqrt','auto'], \n                             # criterion=['entropy', 'gini'],\n                             max_depth=2, # max depth of each ensamble tree\n                             min_samples_split=2, # min no of samples in each node\n                             min_samples_leaf=1 # min no of samples in each leaf\n                            )\n\n# Fit the best algorithm to the data. \nrondomForestClf.fit(X_train, y_train)\nrondomForestPredictions1 = rondomForestClf.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# call general function to fit the classifier and draw the learning curve\nplt.figure(figsize = (16,5))\ntitle = 'Random Forest Learning Curve'\nplotLearningCurves(X_train, y_train, rondomForestClf, title)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# call general function to fit the classifier and draw the validation curve\ntitle = 'Random Forest Validation Curve'\nparam_name = 'n_estimators'\nparam_range = [4, 6, 9]\nplt.figure(figsize = (16,5))\nplotValidationCurves(X_train, y_train, rondomForestClf, param_name, param_range, title)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"print(accuracy_score(y_test, rondomForestPredictions1))\nprint(confusion_matrix(y_test, rondomForestPredictions1))\nprint(classification_report(y_test, rondomForestPredictions1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rondomForestClf_disp = plot_roc_curve(rondomForestClf, X_test, y_test)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* ### keeping the tree depth at 2 and add more points in split / leaf"},{"metadata":{"trusted":true},"cell_type":"code","source":"# change some parameters combinations to increase the accuracy\n# increase the number of samples split / leaf numbers\nrondomForestClf = RandomForestClassifier(n_estimators=9, # no of sample trees\n                             # max_features=['log2', 'sqrt','auto'], \n                             # criterion=['entropy', 'gini'],\n                             max_depth=2, # max depth of each ensamble tree\n                             min_samples_split=5, # min no of samples in each node\n                             min_samples_leaf=3 # min no of samples in each leaf\n                            )\n\n# Fit the best algorithm to the data. \nrondomForestClf.fit(X_train, y_train)\nrondomForestPredictions2 = rondomForestClf.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# call general function to fit the classifier and draw the learning curve\nplt.figure(figsize = (16,5))\ntitle = 'Random Forest Learning Curve'\nplotLearningCurves(X_train, y_train, rondomForestClf, title)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# call general function to fit the classifier and draw the validation curve\ntitle = 'Random Forest Validation Curve'\nparam_name = 'n_estimators'\nparam_range = [4, 6, 9]\nplt.figure(figsize = (16,5))\nplotValidationCurves(X_train, y_train, rondomForestClf, param_name, param_range, title)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(accuracy_score(y_test, rondomForestPredictions2))\nprint(confusion_matrix(y_test, rondomForestPredictions2))\nprint(classification_report(y_test, rondomForestPredictions2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### So changing the sample split / leaf numbers only without changing the tree depth decrease the accuracy"},{"metadata":{"trusted":true},"cell_type":"code","source":"# change some parameters combinations to increase the accuracy\n# change the tree depth with preserving the other parameters\nrondomForestClf = RandomForestClassifier(n_estimators=9, # no of sample trees\n                             # max_features=['log2', 'sqrt','auto'], \n                             # criterion=['entropy', 'gini'],\n                             max_depth=3, # max depth of each ensamble tree\n                             min_samples_split=5, # min no of samples in each node\n                             min_samples_leaf=1 # min no of samples in each leaf\n                            )\n\n# Fit the best algorithm to the data. \nrondomForestClf.fit(X_train, y_train)\n\nrondomForestPredictions3 = rondomForestClf.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# call general function to fit the classifier and draw the learning curve\nplt.figure(figsize = (16,5))\ntitle = 'Random Forest Learning Curve'\nplotLearningCurves(X_train, y_train, rondomForestClf, title)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# call general function to fit the classifier and draw the validation curve\ntitle = 'Random Forest Validation Curve'\nparam_name = 'n_estimators'\nparam_range = [4, 6, 9]\nplt.figure(figsize = (16,5))\nplotValidationCurves(X_train, y_train, rondomForestClf, param_name, param_range, title)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(accuracy_score(y_test, rondomForestPredictions3))\nprint(confusion_matrix(y_test, rondomForestPredictions3))\nprint(classification_report(y_test, rondomForestPredictions3))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rondomForestClf_disp = plot_roc_curve(rondomForestClf, X_test, y_test)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### as shown above, increasing the depth with the same number of samples for split / leaf increase the accuracy slightly, so next we will preserve the depth and try to increase the samples split / leaf numbers"},{"metadata":{"trusted":true},"cell_type":"code","source":"# change some parameters combinations to increase the accuracy\n# change the samples split / leaf numbers with preserving the other parameters\nrondomForestClf = RandomForestClassifier(n_estimators=9, # no of sample trees\n                             # max_features=['log2', 'sqrt','auto'], \n                             # criterion=['entropy', 'gini'],\n                             max_depth=3, # max depth of each ensamble tree\n                             min_samples_split=8, # min no of samples in each node\n                             min_samples_leaf=3 # min no of samples in each leaf\n                            )\n\n# Fit the best algorithm to the data. \nrondomForestClf.fit(X_train, y_train)\n\nrondomForestPredictions4 = rondomForestClf.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# call general function to fit the classifier and draw the learning curve\nplt.figure(figsize = (16,5))\ntitle = 'Random Forest Learning Curve'\nplotLearningCurves(X_train, y_train, rondomForestClf, title)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# call general function to fit the classifier and draw the validation curve\ntitle = 'Random Forest Validation Curve'\nparam_name = 'n_estimators'\nparam_range = [4, 6, 9]\nplt.figure(figsize = (16,5))\nplotValidationCurves(X_train, y_train, rondomForestClf, param_name, param_range, title)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(accuracy_score(y_test, rondomForestPredictions4))\nprint(confusion_matrix(y_test, rondomForestPredictions4))\nprint(classification_report(y_test, rondomForestPredictions4))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### accuracy increased after increasing the number of samples split / leaf, so next expirement will preserve the samples split / leaf number and try to increase the tree depth"},{"metadata":{"trusted":true},"cell_type":"code","source":"# change some parameters combinations to increase the accuracy\n# change the tree depth with preserving the other parameters\nrondomForestClf = RandomForestClassifier(n_estimators=9, # no of sample trees\n                             # max_features=['log2', 'sqrt','auto'], \n                             # criterion=['entropy', 'gini'],\n                             max_depth=5, # max depth of each ensamble tree\n                             min_samples_split=8, # min no of samples in each node\n                             min_samples_leaf=5 # min no of samples in each leaf\n                            )\n\n# Fit the best algorithm to the data. \nrondomForestClf.fit(X_train, y_train)\n\nrondomForestPredictions5 = rondomForestClf.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# call general function to fit the classifier and draw the learning curve\nplt.figure(figsize = (16,5))\ntitle = 'Random Forest Learning Curve'\nplotLearningCurves(X_train, y_train, rondomForestClf, title)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# call general function to fit the classifier and draw the validation curve\ntitle = 'Random Forest Validation Curve'\nparam_name = 'n_estimators'\nparam_range = [4, 6, 9]\nplt.figure(figsize = (16,5))\nplotValidationCurves(X_train, y_train, rondomForestClf, param_name, param_range, title)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(accuracy_score(y_test, rondomForestPredictions5))\nprint(confusion_matrix(y_test, rondomForestPredictions5))\nprint(classification_report(y_test, rondomForestPredictions5))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### as shown above, the accuracy increased significantly when the tree depth increased to 5, so the tree depth parameter has a great impact on the accuracy, let's increase it to see if the accuracy will increase or not"},{"metadata":{"trusted":true},"cell_type":"code","source":"# change some parameters combinations to increase the accuracy\n# change the tree depth with preserving the other parameters\nrondomForestClf = RandomForestClassifier(n_estimators=9, # no of sample trees\n                             # max_features=['log2', 'sqrt','auto'], \n                             # criterion=['entropy', 'gini'],\n                             max_depth=8, # max depth of each ensamble tree\n                             min_samples_split=8, # min no of samples in each node\n                             min_samples_leaf=5 # min no of samples in each leaf\n                            )\n\n# Fit the best algorithm to the data. \nrondomForestClf.fit(X_train, y_train)\n\nrondomForestPredictions6 = rondomForestClf.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# call general function to fit the classifier and draw the learning curve\nplt.figure(figsize = (16,5))\ntitle = 'Random Forest Learning Curve'\nplotLearningCurves(X_train, y_train, rondomForestClf, title)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# call general function to fit the classifier and draw the validation curve\ntitle = 'Random Forest Validation Curve'\nparam_name = 'n_estimators'\nparam_range = [4, 6, 9]\nplt.figure(figsize = (16,5))\nplotValidationCurves(X_train, y_train, rondomForestClf, param_name, param_range, title)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(accuracy_score(y_test, rondomForestPredictions6))\nprint(confusion_matrix(y_test, rondomForestPredictions6))\nprint(classification_report(y_test, rondomForestPredictions6))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"so the best parameters will be: max_depth=3, min_samples_leaf=1, min_samples_split=5 for perdiction rondomForestPredictions3"},{"metadata":{"trusted":true},"cell_type":"code","source":"rondomForestClf = RandomForestClassifier()\n\nparameters = {'n_estimators': [4, 6, 9], # no of sample trees\n              'max_features': ['log2', 'sqrt','auto'], \n              'criterion': ['entropy', 'gini'],\n              'max_depth': [2, 3, 5, 8], # max depth of each ensamble tree\n              'min_samples_split': [2, 5, 8], # min no of samples in each node\n              'min_samples_leaf': [1, 3, 5] # min no of samples in each leaf\n             }\n\n# Type of scoring used to compare parameter combinations\nacc_scorer = make_scorer(accuracy_score)\n\n# Run the grid search\ngrid_obj = GridSearchCV(rondomForestClf, parameters, scoring=acc_scorer)\ngrid_obj = grid_obj.fit(X_train, y_train)\n\n# Set the clf to the best combination of parameters\nrondomForestClf = grid_obj.best_estimator_\n\n# Fit the best algorithm to the data. \nrondomForestClf.fit(X_train, y_train)\n\npredictions = rondomForestClf.predict(X_test)\n\nprint(grid_obj.best_estimator_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# call general function to fit the classifier and draw the learning curve\nplt.figure(figsize = (16,5))\ntitle = 'Random Forest Learning Curve'\nplotLearningCurves(X_train, y_train, rondomForestClf, title)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# call general function to fit the classifier and draw the validation curve\ntitle = 'Random Forest Validation Curve'\nparam_name = 'n_estimators'\nparam_range = [4, 6, 9]\nplt.figure(figsize = (16,5))\nplotValidationCurves(X_train, y_train, rondomForestClf, param_name, param_range, title)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## STEP #5.3: K Nearest Neighbors Calssifier"},{"metadata":{},"cell_type":"markdown","source":"### STEP #5.3.1: Results for neighbors = 1"},{"metadata":{"trusted":true},"cell_type":"code","source":"knn = KNeighborsClassifier(1)\nknn.fit(X_train, y_train)\nknnPredictions1 = knn.predict(X_test)\nprint(accuracy_score(y_test, knnPredictions1))\nprint(confusion_matrix(y_test, knnPredictions1))\nprint(classification_report(y_test, knnPredictions1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"knn_disp = plot_roc_curve(knn, X_test, y_test)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (16,5))\ntitle = 'KNN k=1 Learning Curve'\nplotLearningCurves(X_train, y_train, knn, title)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### STEP #5.3.2: Results for neighbors = 3"},{"metadata":{"trusted":true},"cell_type":"code","source":"knn = KNeighborsClassifier(3)\nknn.fit(X_train, y_train)\nknnPredictions3 = knn.predict(X_test)\nprint(accuracy_score(y_test, knnPredictions3))\nprint(confusion_matrix(y_test, knnPredictions3))\nprint(classification_report(y_test, knnPredictions3))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"knn_disp = plot_roc_curve(knn, X_test, y_test)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (16,5))\ntitle = 'KNN k=3 Learning Curve'\nplotLearningCurves(X_train, y_train, knn, title)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### STEP #5.3.3: Results for neighbors = 5"},{"metadata":{"trusted":true},"cell_type":"code","source":"knn = KNeighborsClassifier(5)\nknn.fit(X_train, y_train)\nknnPredictions5 = knn.predict(X_test)\nprint(accuracy_score(y_test, knnPredictions5))\nprint(confusion_matrix(y_test, knnPredictions5))\nprint(classification_report(y_test, knnPredictions5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"knn_disp = plot_roc_curve(knn, X_test, y_test)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (16,5))\ntitle = 'KNN k=5 Learning Curve'\nplotLearningCurves(X_train, y_train, knn, title)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### STEP #5.3.4: Results for neighbors = 7"},{"metadata":{"trusted":true},"cell_type":"code","source":"knn = KNeighborsClassifier(7)\nknn.fit(X_train, y_train)\nknnPredictions7 = knn.predict(X_test)\nprint(accuracy_score(y_test, knnPredictions7))\nprint(confusion_matrix(y_test, knnPredictions7))\nprint(classification_report(y_test, knnPredictions7))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"knn_disp = plot_roc_curve(knn, X_test, y_test)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (16,5))\ntitle = 'KNN k=7 Learning Curve'\nplotLearningCurves(X_train, y_train, knn, title)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### as shown above, the accuracy increased significantly when the neighbors increased to 5\n### Validation Curve\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# call general function to fit the classifier and draw the validation curve\nknn = KNeighborsClassifier()\ntitle = 'KNN Validation Curve'\nparam_name = 'n_neighbors'\nparam_range = np.arange(1,9,2)\nplt.figure(figsize = (16,5))\nplotValidationCurves(X_train, y_train, knn, param_name, param_range, title)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{},"cell_type":"markdown","source":"## STEP #5.4: Support Vector Machine Calssifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train a SVC model using different kernal\nsvclassifier = SVC(C = 0.1 , gamma =1 ,kernel='sigmoid')\nsvclassifier.fit(X_train, y_train)\n# Make prediction\nSvcPredictions1 = svclassifier.predict(X_test)\n# Evaluate our model\nprint(classification_report(y_test,SvcPredictions1))","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# call general function to fit the classifier and draw the learning curve\nplt.figure(figsize = (16,5))\ntitle = 'Support Vector Machine Learning Curve'\nplotLearningCurves(X_train, y_train, svclassifier, title)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# call general function to fit the classifier and draw the validation curve\ntitle = 'Support Vector Machine Validation Curve'\nparam_name = 'C'\nparam_range = [0.1,1, 10]\nplt.figure(figsize = (16,5))\nplotValidationCurves(X_train, y_train, svclassifier, param_name, param_range, title)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Initialy tried with C=0.1, and gamma=1, and kernal = segmoid, accuracy = 0.9, then change the C parameter to 1"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train a SVC model using different kernal\nsvclassifier = SVC(C = 1 , gamma =1 ,kernel='sigmoid')\nsvclassifier.fit(X_train, y_train)\n# Make prediction\nSvcPredictions2 = svclassifier.predict(X_test)\n# Evaluate our model\nprint(classification_report(y_test,SvcPredictions2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# call general function to fit the classifier and draw the learning curve\nplt.figure(figsize = (16,5))\ntitle = 'Support Vector Machine Learning Curve'\nplotLearningCurves(X_train, y_train, svclassifier, title)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# call general function to fit the classifier and draw the validation curve\ntitle = 'Support Vector Machine Validation Curve'\nparam_name = 'C'\nparam_range = [0.1,1, 10]\nplt.figure(figsize = (16,5))\nplotValidationCurves(X_train, y_train, svclassifier, param_name, param_range, title)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"now C=1, and gamma=1, and kernal = segmoid, accuracy = 0.9 (no change), then change the C parameter to 10"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train a SVC model using different kernal\nsvclassifier = SVC(C = 10 , gamma =1 ,kernel='sigmoid')\nsvclassifier.fit(X_train, y_train)\n# Make prediction\nSvcPredictions3 = svclassifier.predict(X_test)\n# Evaluate our model\nprint(classification_report(y_test,SvcPredictions3))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# call general function to fit the classifier and draw the learning curve\nplt.figure(figsize = (16,5))\ntitle = 'Support Vector Machine Learning Curve'\nplotLearningCurves(X_train, y_train, svclassifier, title)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# call general function to fit the classifier and draw the validation curve\ntitle = 'Support Vector Machine Validation Curve'\nparam_name = 'C'\nparam_range = [0.1,1, 10]\nplt.figure(figsize = (16,5))\nplotValidationCurves(X_train, y_train, svclassifier, param_name, param_range, title)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"now C=10, and gamma=1, and kernal = segmoid, accuracy = 0.9 (no change), then change the gamma parameter to 0.01"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train a SVC model using different kernal\nsvclassifier = SVC(C = 10 , gamma =0.01 ,kernel='sigmoid')\nsvclassifier.fit(X_train, y_train)\n# Make prediction\nSvcPredictions4 = svclassifier.predict(X_test)\n# Evaluate our model\nprint(classification_report(y_test,SvcPredictions4))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# call general function to fit the classifier and draw the learning curve\nplt.figure(figsize = (16,5))\ntitle = 'Support Vector Machine Learning Curve'\nplotLearningCurves(X_train, y_train, svclassifier, title)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# call general function to fit the classifier and draw the validation curve\ntitle = 'Support Vector Machine Validation Curve'\nparam_name = 'C'\nparam_range = [0.1,1, 10]\nplt.figure(figsize = (16,5))\nplotValidationCurves(X_train, y_train, svclassifier, param_name, param_range, title)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"for C and gamma parameters, the effect is slightly noticable, now change the kernal parameter to rbf"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train a SVC model using different kernal\nsvclassifier = SVC(C = 10 , gamma =0.01 ,kernel='rbf')\nsvclassifier.fit(X_train, y_train)\n# Make prediction\nSvcPredictions5 = svclassifier.predict(X_test)\n# Evaluate our model\nprint(classification_report(y_test,SvcPredictions5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# call general function to fit the classifier and draw the learning curve\nplt.figure(figsize = (16,5))\ntitle = 'Support Vector Machine Learning Curve'\nplotLearningCurves(X_train, y_train, svclassifier, title)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# call general function to fit the classifier and draw the validation curve\ntitle = 'Support Vector Machine Validation Curve'\nparam_name = 'C'\nparam_range = [0.1,1, 10]\nplt.figure(figsize = (16,5))\nplotValidationCurves(X_train, y_train, svclassifier, param_name, param_range, title)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"rbf generates a high variance model as the gab increased between the training error and the cross validation error, now change the gamma parameter to 1 and check"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train a SVC model using different kernal\nsvclassifier = SVC(C = 10 , gamma =1 ,kernel='rbf')\nsvclassifier.fit(X_train, y_train)\n# Make prediction\nSvcPredictions6 = svclassifier.predict(X_test)\n# Evaluate our model\nprint(classification_report(y_test,SvcPredictions6))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# call general function to fit the classifier and draw the learning curve\nplt.figure(figsize = (16,5))\ntitle = 'Support Vector Machine Learning Curve'\nplotLearningCurves(X_train, y_train, svclassifier, title)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# call general function to fit the classifier and draw the validation curve\ntitle = 'Support Vector Machine Validation Curve'\nparam_name = 'C'\nparam_range = [0.1,1, 10]\nplt.figure(figsize = (16,5))\nplotValidationCurves(X_train, y_train, svclassifier, param_name, param_range, title)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"changing the gamma parameter to 1,increased the gab (higher variance),so the best parameters are: C= 10, gamma = 0.01, kernal = segmoid for prediction: SvcPredictions4"},{"metadata":{"trusted":true},"cell_type":"code","source":"param_grid = {'C': [0.1,1, 10], 'gamma': [1,0.1,0.01],'kernel': ['sigmoid']}\ngrid = GridSearchCV(SVC(),param_grid,refit=True,verbose=2)\nsvclassifier7 = grid.fit(X_train,y_train)\nSvcPredictions7 = svclassifier7.predict(X_test)\nprint(grid.best_estimator_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(accuracy_score(y_test, SvcPredictions7))\nprint(confusion_matrix(y_test, SvcPredictions7))\nprint(classification_report(y_test, SvcPredictions7))","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"svm_disp = plot_roc_curve(svclassifier, X_test, y_test)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## STEP #5.5: Neural Network Calssifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"def neural_network_learning_curve(classifier):\n    # call general function to fit the classifier and draw the learning curve\n    title = 'Neural Network Learning Curve'\n    plt.figure(figsize = (16,5))\n    plotLearningCurves(X_train, y_train, classifier, title)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" def neural_network_validation_curve(classifier):   \n    # call general function to fit the classifier and draw the validation curve\n    title = 'Neural Network Validation Curve'\n    param_name=\"alpha\"\n    param_range = np.logspace(-6, -1, 5)\n    plt.figure(figsize = (16,5))\n    plotValidationCurves(X_train, y_train, classifier, param_name, param_range, title)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nnclf1 = MLPClassifier(hidden_layer_sizes=(2, 1), activation='logistic', solver='adam', max_iter=500, \n                    alpha=1e-5, random_state=1)\n\nnnclf1.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"NeuralNetworkPredictions1 = nnclf1.predict(X_test)\nprint(confusion_matrix(y_test,NeuralNetworkPredictions1))\nprint(classification_report(y_test,NeuralNetworkPredictions1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"neural_network_learning_curve(nnclf1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"neural_network_validation_curve(nnclf1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ### as shown we built an neural classifier with 2 hiden layer wich contint of 3 nodes and we got .84 accuracy"},{"metadata":{"trusted":true},"cell_type":"code","source":"nnclf2 = MLPClassifier(hidden_layer_sizes=(2, 2), activation='logistic', solver='adam', max_iter=500, \n                    alpha=1e-5, random_state=1)\n\nnnclf2.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"NeuralNetworkPredictions2 = nnclf1.predict(X_test)\nprint(confusion_matrix(y_test,NeuralNetworkPredictions2))\nprint(classification_report(y_test,NeuralNetworkPredictions2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"neural_network_learning_curve(nnclf2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"neural_network_validation_curve(nnclf2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### here we added another node to the second hidin layer so we have .85 accuracy"},{"metadata":{"trusted":true},"cell_type":"code","source":"nnclf3 = MLPClassifier(hidden_layer_sizes=(2, 3), activation='logistic', solver='adam', max_iter=500, \n                    alpha=1e-5, random_state=1)\n\nnnclf3.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"NeuralNetworkPredictions3 = nnclf1.predict(X_test)\nprint(confusion_matrix(y_test,NeuralNetworkPredictions3))\nprint(classification_report(y_test,NeuralNetworkPredictions3))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"neural_network_learning_curve(nnclf3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"neural_network_validation_curve(nnclf3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### then we added another node to the second hidin layer and we still recieved the same accuracy .85"},{"metadata":{"trusted":true},"cell_type":"code","source":"nnclf4 = MLPClassifier(hidden_layer_sizes=(3, 2), activation='logistic', solver='adam', max_iter=500, \n                    alpha=1e-5, random_state=1)\n\nnnclf4.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"NeuralNetworkPredictions4 = nnclf1.predict(X_test)\nprint(confusion_matrix(y_test,NeuralNetworkPredictions4))\nprint(classification_report(y_test,NeuralNetworkPredictions4))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"neural_network_learning_curve(nnclf4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"neural_network_validation_curve(nnclf4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### here we tryed to swap the number of nodes in our 2 hidin layer instead of (2, 3) we made it (3, 2), so we recieved .88 accuracy"},{"metadata":{"trusted":true},"cell_type":"code","source":"nnclf5 = MLPClassifier(hidden_layer_sizes=(3, 3), activation='logistic', solver='adam', max_iter=500, \n                    alpha=1e-5, random_state=1)\n\nnnclf5.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"NeuralNetworkPredictions5 = nnclf1.predict(X_test)\nprint(confusion_matrix(y_test,NeuralNetworkPredictions5))\nprint(classification_report(y_test,NeuralNetworkPredictions5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"neural_network_learning_curve(nnclf5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"neural_network_validation_curve(nnclf5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### then we added another node in the second hidin layer and we recieved .89 accuracy"},{"metadata":{"trusted":true},"cell_type":"code","source":"nnclf6 = MLPClassifier(hidden_layer_sizes=(5, 3), activation='logistic', solver='adam', max_iter=500, \n                    alpha=1e-5, random_state=1)\n\nnnclf6.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"NeuralNetworkPredictions6 = nnclf1.predict(X_test)\nprint(confusion_matrix(y_test,NeuralNetworkPredictions6))\nprint(classification_report(y_test,NeuralNetworkPredictions6))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"neural_network_learning_curve(nnclf6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"neural_network_validation_curve(nnclf6)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### finally we have the best accuracy of .90 when we tryed to make 2 hidin layers with 8 nodes for prediction: NeuralNetworkPredictions6"},{"metadata":{},"cell_type":"markdown","source":"# STEP #6: Comparison between all classifier performance(AUC curve) "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import the classifiers\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import roc_curve, roc_auc_score\n\n \n\n# Instantiate the classfiers and make a list\nclassifiers = [LogisticRegression(random_state=1234), \n               SVC(),\n               KNeighborsClassifier(), \n               RandomForestClassifier(random_state=1234),\n               MLPClassifier()]\n\nresult_table = pd.DataFrame(columns=['classifiers', 'fpr','tpr','auc'])\n \n# print('auc =', auc)\nlr_fpr1, lr_tpr1, _ = roc_curve(y_test, rondomForestPredictions3)\nlr_fpr2, lr_tpr2, _ = roc_curve(y_test, knnPredictions5)\nlr_fpr3, lr_tpr3, _ = roc_curve(y_test, SvcPredictions4)\nlr_fpr4, lr_tpr4, _ = roc_curve(y_test, NeuralNetworkPredictions2)\n# fpr , tpr, _= roc_curve(X_test, predict6_test)\nauc1 = roc_auc_score(y_test, rondomForestPredictions3)\nauc2 = roc_auc_score(y_test, knnPredictions5)\nauc3 = roc_auc_score(y_test, SvcPredictions5)\nauc4 = roc_auc_score(y_test, NeuralNetworkPredictions2)\n\n\n\n\nresult_table = result_table.append({'classifiers':RandomForestClassifier.__class__.__name__,\n                                     'fpr':lr_fpr1, \n                                     'tpr':lr_tpr1, \n                                     'auc':auc1}, ignore_index=True)\n\nresult_table = result_table.append({'classifiers':KNeighborsClassifier.__class__.__name__,\n                                     'fpr':lr_fpr2, \n                                     'tpr':lr_tpr2, \n                                     'auc':auc2}, ignore_index=True)\n\nresult_table = result_table.append({'classifiers':SVC.__class__.__name__,\n                                     'fpr':lr_fpr3, \n                                     'tpr':lr_tpr3, \n                                     'auc':auc3}, ignore_index=True)\n\nresult_table = result_table.append({'classifiers':MLPClassifier.__class__.__name__,\n                                     'fpr':lr_fpr4, \n                                     'tpr':lr_tpr4, \n                                     'auc':auc4}, ignore_index=True)\n\n \nfig = plt.figure(figsize=(8,6))\n\n# for i in result_table.index:\n#     plt.plot(result_table.loc[i]['fpr'], \n#              result_table.loc[i]['tpr'], \n#              label=\"{}, AUC={:.3f}\".format(i, result_table.loc[i]['auc']))\n\n\nplt.plot(result_table.loc[0]['fpr'], \n         result_table.loc[0]['tpr'], \n         label=\"RandomForestClassifier, AUC={:.3f}\".format( result_table.loc[0]['auc']))\n\nplt.plot(result_table.loc[1]['fpr'], \n         result_table.loc[1]['tpr'], \n         label=\"KNeighborsClassifier, AUC={:.3f}\".format( result_table.loc[1]['auc']))\n\nplt.plot(result_table.loc[2]['fpr'], \n         result_table.loc[2]['tpr'], \n         label=\"SVM, AUC={:.3f}\".format( result_table.loc[2]['auc']))\n\nplt.plot(result_table.loc[3]['fpr'], \n         result_table.loc[3]['tpr'], \n         label=\"MLPClassifier, AUC={:.3f}\".format( result_table.loc[3]['auc']))\n    \n# plt.plot([0,1], [0,1], color='orange', linestyle='--')\n\nplt.xticks(np.arange(0.0, 1.1, step=0.1))\nplt.xlabel(\"Flase Positive Rate\", fontsize=15)\n\nplt.yticks(np.arange(0.0, 1.1, step=0.1))\nplt.ylabel(\"True Positive Rate\", fontsize=15)\n\nplt.title('ROC Curve Analysis', fontweight='bold', fontsize=15)\nplt.legend(prop={'size':13}, loc='lower right')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As shown in the above curve, there are variations between the four curves that shows the randomforest classifier has the biggest area under curve, and SVM covers he lowest area under curve, while neural network has better performance than K nearest neighbors."},{"metadata":{},"cell_type":"markdown","source":"# STEP #7: Apply AutoML (e.g. auto sklearn )and compare its performance to your best model"},{"metadata":{},"cell_type":"markdown","source":"### Install autosklearn on Kaggle\n!apt-get remove swig\n!apt-get install swig3.0 build-essential -y\n!ln -s /usr/bin/swig3.0 /usr/bin/swig\n!apt-get install build-essential\n!pip install --upgrade setuptools\n!pip install auto-sklearn"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Regression AutoML\n\nimport sklearn.model_selection\nimport sklearn.datasets\nimport sklearn.metrics\nimport os  \nimport autosklearn.regression\n\ntmp_folder='/tmp/autosklearn_regression_example_tmp'\noutput_folder='/tmp/autosklearn_regression_example_out'\n    \n\nautoml = autosklearn.regression.AutoSklearnRegressor(\n    time_left_for_this_task=120,\n    per_run_time_limit=30,\n    tmp_folder = tmp_folder,\n    output_folder = output_folder,\n)\nautoml.fit(X_train, y_train, dataset_name='Airlines' )\n\nprint(automl.show_models())\npredictions = automl.predict(X_test)\nprint(\"R2 score:\", sklearn.metrics.r2_score(y_test, predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Cross-Validation AutoML\n\nimport sklearn.model_selection\nimport sklearn.datasets\nimport sklearn.metrics\n\nimport autosklearn.classification\n\n\nautoml = autosklearn.classification.AutoSklearnClassifier(\n    time_left_for_this_task=120,\n    per_run_time_limit=30,\n    tmp_folder='/tmp/autosklearn_cv_example_tmp1',\n    output_folder='/tmp/autosklearn_cv_example_out1',\n    delete_tmp_folder_after_terminate=False,\n    resampling_strategy='cv',\n    resampling_strategy_arguments={'folds': 5},\n)\n\n# fit() changes the data in place, but refit needs the original data. We\n# therefore copy the data. In practice, one should reload the data\nautoml.fit(X_train.copy(), y_train.copy(), dataset_name='AirLines')\n# During fit(), models are fit on individual cross-validation folds. To use\n# all available data, we call refit() which trains all models in the\n# final ensemble on the whole dataset.\nautoml.refit(X_train.copy(), y_train.copy())\n\nprint(automl.show_models())\n\npredictions = automl.predict(X_test)\nprint(\"Accuracy score\", sklearn.metrics.accuracy_score(y_test, predictions))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# STEP #8: Conclusion\n\nWith this notebook we learned the basics of EDA with Pandas and Matplotlib as well as the foundations\nfor applying the classification models of the scikit learn library.\nBy EDA we found a strong impact of features like Gender and Age on Satisfaction.\n\nWe then built a simple baseline model with Pandas, using only these features.\nAgain, using Pandas, we also created a dataset that can be used by the scikit learn classifiers for prediction.\n\nWe applied Random Forest, k-nearest neighbors, Support vector machine (SVM) and Multilayer Perceptron (MLP).\n\n\nDeciding by Auto ML, the best ML models for this task and set of features was: Random Forest with accuracy 85.7%\n"},{"metadata":{},"cell_type":"markdown","source":"# STEP #9: # Reference\n\nThis kernel would have been imposible to make if not this amazing tutorials:\nMost basic matplotlib: https://towardsdatascience.com/plt-xxx-or-ax-xxx-that-is-the-question-in-matplotlib-8580acf42f44\n\nUnderstand the difference between all the methods (add_subplot, add_subplots, add_axes ...): https://towardsdatascience.com/the-many-ways-to-call-axes-in-matplotlib-2667a7b06e06\n\nMatplotlib grid documentation: https://matplotlib.org/tutorials/intermediate/gridspec.html\n\nGreat tutorial fore beginners: https://github.com/rougier/matplotlib-tutorial\n\n50 beautiful plots using matplotlib: https://www.machinelearningplus.com/plots/top-50-matplotlib-visualizations-the-master-plots-python/\n\nPandas plotting capabilities: https://pandas.pydata.org/pandas-docs/stable/user_guide/visualization.html"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.2"}},"nbformat":4,"nbformat_minor":4}