{"cells":[{"metadata":{},"cell_type":"markdown","source":"This is my starter code for the Kaggle competition. Will discuss this in greater detail during the Zoom meeting on Monday. This could may evolve a bit. This is a basic implementation that should get around 2.3 RMSE on the leaderboard. Next, we build a dataframe that contains the filenames and the clip_count. You will need to modify the paths below to match where you put train.csv and master.csv."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\n\ndf = pd.read_csv(\n    \"/kaggle/input/count-the-paperclips/train.csv\", \n    na_values=['NA', '?'])\n\ndf['filename']=\"clips-\"+df[\"id\"].astype(str)+\".png\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Separate into a training and validation (for early stopping)"},{"metadata":{"trusted":true},"cell_type":"code","source":"TRAIN_PCT = 0.9\nTRAIN_CUT = int(len(df) * TRAIN_PCT)\n\ndf_train = df[0:TRAIN_CUT]\ndf_validate = df[TRAIN_CUT:]\n\nprint(f\"Training size: {len(df_train)}\")\nprint(f\"Validate size: {len(df_validate)}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Create DataGenerators for training and validation. This is what links the .csv with the images. This part might take a few minutes to run."},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nimport keras_preprocessing\nfrom keras_preprocessing import image\nfrom keras_preprocessing.image import ImageDataGenerator\n\nIMAGES_DIR = \"/kaggle/input/count-the-paperclips/clips-data-2020/clips\"\n\ntraining_datagen = ImageDataGenerator(\n  rescale = 1./255,\n  horizontal_flip=True,\n  vertical_flip=True,\n  fill_mode='nearest')\n\ntrain_generator = training_datagen.flow_from_dataframe(\n        dataframe=df_train,\n        directory=IMAGES_DIR,\n        x_col=\"filename\",\n        y_col=\"clip_count\",\n        target_size=(256, 256),\n        batch_size=32,\n        class_mode='other')\n\nvalidation_datagen = ImageDataGenerator(rescale = 1./255)\n\nval_generator = validation_datagen.flow_from_dataframe(\n        dataframe=df_validate,\n        directory=IMAGES_DIR,\n        x_col=\"filename\",\n        y_col=\"clip_count\",\n        target_size=(256, 256),\n        class_mode='other')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next we construct the neural network."},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.callbacks import EarlyStopping\n\nmodel = tf.keras.models.Sequential([\n    # Note the input shape is the desired size of the image 150x150 with 3 bytes color\n    # This is the first convolution\n    tf.keras.layers.Conv2D(64, (3,3), activation='relu', input_shape=(256, 256, 3)),\n    tf.keras.layers.MaxPooling2D(2, 2),\n    # The second convolution\n    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n    tf.keras.layers.MaxPooling2D(2,2),\n    tf.keras.layers.Flatten(),\n    # 512 neuron hidden layer\n    tf.keras.layers.Dense(512, activation='relu'),\n    tf.keras.layers.Dense(1, activation='linear')\n])\n\n\nmodel.summary()\nepoch_steps = 250 # needed for 2.2\nvalidation_steps = len(df_validate)\nmodel.compile(loss = 'mean_squared_error', optimizer='adam')\nmonitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=1, mode='auto',\n        restore_best_weights=True)\nhistory = model.fit(train_generator,  \n  verbose = 1, \n  validation_data=val_generator, callbacks=[monitor], epochs=25)\n#  steps_per_epoch=epoch_steps, validation_steps=validation_steps, # needed for 2.2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Generate Submission File"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test = pd.read_csv(\n    \"/kaggle/input/count-the-paperclips/test.csv\", \n    na_values=['NA', '?'])\n\ndf_test['filename']=\"clips-\"+df_test[\"id\"].astype(str)+\".png\"\n\ntest_datagen = ImageDataGenerator(rescale = 1./255)\n\ntest_generator = validation_datagen.flow_from_dataframe(\n        dataframe=df_test,\n        directory=IMAGES_DIR,\n        x_col=\"filename\",\n        batch_size=1,\n        shuffle=False,\n        target_size=(256, 256),\n        class_mode=None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_generator.reset()\npred = model.predict(test_generator,steps=len(df_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_submit = pd.DataFrame({'id':df_test['id'],'clip_count':pred.flatten()})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_submit.to_csv(\"/kaggle/working/submit.csv\",index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls /kaggle/input/count-the-paperclips/clips-data-2020","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}