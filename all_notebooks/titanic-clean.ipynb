{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Imports\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn import ensemble\nfrom sklearn import svm\nimport time\nimport random\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import StandardScaler\nimport math\nimport matplotlib.ticker as plticker\nimport matplotlib.patches as mpatches\nimport matplotlib.lines as mlines\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import metrics \nfrom sklearn.ensemble import AdaBoostClassifier\nimport matplotlib.cm as cm\nfrom matplotlib.colors import Normalize\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"# Read in data\nbig_df = pd.read_csv(\"../input/titanic-cleaned-data/train_clean.csv\")\nbig_df.info()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# util functions\n\ndef eval_for_conclusion(model_id, clf, test_x, test_y):\n    y_pred = clf.predict(test_x)\n    print(classification_report(test_y, y_pred))\n    print(confusion_matrix(test_y, y_pred))\n    accuracy = metrics.accuracy_score(test_y, y_pred)\n    precision = metrics.precision_score(test_y, y_pred)\n    recall = metrics.recall_score(test_y, y_pred)\n    print(\"Final {0} model accuracy:\".format(model_id), accuracy)\n    print(\"Final {0} model precision:\".format(model_id), precision) \n    print(\"Final {0} model recall:\".format(model_id), recall) \n    return {\"model\":model_id, \"recall\":recall, \"accuracy\":accuracy, \"precision\":precision}\n\ndef drop_columns(df, columns_to_drop):\n    for col in columns_to_drop:\n        del df[col]   \ndef split_test_train(train_size, all_data):\n    msk = np.random.rand(len(all_data)) < train_size\n    train_df = all_data[msk]\n    test_df = all_data[~msk]\n    train_y = train_df[\"Survived\"]\n    train_x = train_df.drop(\"Survived\", axis=1)\n    test_y = test_df[\"Survived\"]\n    test_x  = test_df.drop(\"Survived\", axis=1)\n    return (train_x, train_y, test_x, test_y)\n\ndef cross_validate(all_data, model):\n    depth = []\n    all_y = all_data[\"Survived\"]\n    all_x  = all_data.drop(\"Survived\", axis=1)\n    # Perform k-fold cross validation \n    scores = cross_val_score(estimator=model, X=all_x, y=all_y, cv=5, n_jobs=4)\n    depth.append((i,scores.mean()))\n    return depth\n    \ndef train_and_test(all_data, model):\n    test_scores = []\n    train_scores = []\n    times = []\n    for i in range(1,10):\n        (train_x, train_y, test_x, test_y) = split_test_train(0.1 * i, big_df)\n        #print(\"len test: \", len(test_x), \", len train: \", len(train_x))\n        start = time.time()\n        #TODO iterations\n        model.fit(train_x, train_y)\n        end = time.time()\n        times.append(end - start)\n        pred_test_y = model.predict(test_x) # TODO add wallclock time\n        test_score = round(model.score(test_x, test_y) * 100, 2)\n        pred_train_y = model.predict(train_x)\n        train_score = round(model.score(train_x, train_y) * 100, 2)\n        test_scores.append(test_score)\n        train_scores.append(train_score)\n    return (test_scores, train_scores, times)\n\ndef plot_data(x_vars, x_label, all_y_vars, y_var_labels, y_label, title, y_bounds=None):\n    plt.rcParams[\"figure.figsize\"] = (4,3)\n    colors = ['red','orange','black','green','blue','violet']\n    i = 0\n    for y_var in all_y_vars:\n#         if i == 2: # don't plot when i = 1 for cv\n#             x_vars = x_vars[1:]\n        plt.plot(x_vars, y_var, 'o-', color=colors[i % 6], label=y_var_labels[i])\n        i += 1\n    plt.xlabel(x_label)\n    plt.ylabel(y_label)\n    plt.title(title)\n    if y_bounds != None:\n        plt.ylim(y_bounds)\n    leg = plt.legend();\n    plt.show()\n\ndef evaluate_model(all_data, model, model_id):\n    (test_scores, train_scores, times) = train_and_test(all_data, model)\n    print(\"{0} train timings (seconds): {1}\".format(model_id, times))\n    print(\"{0} test set scores: {1} \".format(model_id, test_scores))\n    print(\"{0} train set scores: {1}\".format(model_id, train_scores))\n    plot_data([x * 10 for x in range(1,10)], \"Percentage of data in training set\", [test_scores, train_scores],\\\n              [\"test_scores\", \"train_scores\"], \"Accuracy\", \"{0} Accuracy Over Train/Test Split\".format(model_id), (50,105))\n    plot_data([x * 10 for x in range(1,10)], \"Percentage of data in training set\", [times],\n             [\"times\"], \"Train time in Seconds\", \"{0} Time Spent Training Over Train/Test Split\".format(model_id))\n    return (test_scores, train_scores, times)\n\ndef plot_grid_search(grid_results, plotting_func, title, x_label, y_label, grid_size, model_handles):\n    means = grid_results.cv_results_['mean_test_score']\n    stds = grid_results.cv_results_['std_test_score']\n    params = grid_results.cv_results_['params']\n    plt.rcParams[\"figure.figsize\"] = grid_size\n    plt.xlabel(x_label)\n    plt.ylabel(y_label)\n    plt.title(title)\n    plt.subplots\n    ax = plt.subplot()\n    \n    for mean, std, params in zip(means, stds, params):\n        #print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params))\n        plotting_func(mean, params, plt, ax)\n    if model_handles: plt.legend(handles=model_handles)\n    plt.show()\n\n\n#def grid_search(model, params, x_train, y_train, x_test, y_test):\n    \n\n#TODO come up with graphing function that takes in two arrays of test and train and plots them","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"columns_to_drop = [\"Cabin\", \"Name\", \"Ticket\", \"Parch\", \"Embarked\", \"Title\", \"PassengerId\"]  # TODO include reasoning for dropping these\ndrop_columns(big_df, columns_to_drop)\nis_male = {\"male\": 1, \"female\": 0}\nbig_df[\"Sex\"].replace(is_male, inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Decision Tree\ndt_model = DecisionTreeClassifier()\n(dt_test_scores, dt_train_scores, dt_times) = evaluate_model(big_df, dt_model, \"Titanic Decision Tree\")\n# choose best test split and k fold value\noptimal_test_split = dt_test_scores.index(max(dt_test_scores)) * 0.1\nprint(\"max index for means was: \", optimal_test_split * 10)\n(dt_grid_train_x, dt_grid_train_y, dt_grid_test_x, dt_grid_test_y) = split_test_train(optimal_test_split, big_df)\ndt_param_grid = {\"criterion\":[\"gini\",\"entropy\"], \"max_depth\":[3,4,5,6,7,8,9,10], \"min_samples_split\":[3,5,7]}  #\"splitter\":[\"best\", \"random\"], \ndt_grid_results = GridSearchCV(dt_model, dt_param_grid, cv=5).fit(dt_grid_train_x, dt_grid_train_y)\ndt_means = dt_grid_results.cv_results_['mean_test_score']\ndt_stds = dt_grid_results.cv_results_['std_test_score']\ndt_params = dt_grid_results.cv_results_['params']\n\nmax_depths = []\naccuracies = []\nplt.xlabel(\"Max Depth of Decision Tree\")\nplt.ylabel(\"Accuracy\")\nplt.title(\"Titanic Decision Tree Grid Search, Accuracy as a Function of Max_depth\")\nfor mean, std, params in zip(dt_means, dt_stds, dt_params):\n    print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params))\n    accuracies.append(mean)\n    max_depths.append(params[\"max_depth\"])\nplt.plot(max_depths, accuracies, 'o', color=\"red\")\nplt.show()\nprint(\"best params \", dt_grid_results.best_params_)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plotting_func_dt(mean, params, plt, ax):\n    x_var = \"max_depth\"\n    #print(params[\"hidden_layer_sizes\"])\n    color_map = {\"gini\":\"b\", \"entropy\":\"red\"}\n    ax.plot(params[x_var], mean, \"o\", color=color_map[params[\"criterion\"]])\n\nblue_patch = mpatches.Patch(color='blue', label='gini')\nred_patch = mpatches.Patch(color='red', label='entropy')\nhandles = [blue_patch, red_patch]\nplot_grid_search(dt_grid_results, plotting_func_dt, \"Titanic Decision Tree Training Accuracy as a Function of Max_depth\", \"max_depth\", \"accuracy\",(6,4), handles)\n\neval_for_conclusion(\"Titanic Decision Tree\", dt_grid_results, dt_grid_test_x, dt_grid_test_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"knn_classifier = KNeighborsClassifier()\nevaluate_model(big_df, knn_classifier, \"Titanic knn baseline\")\nknn_param_grid = {\"n_neighbors\":[i for i in range(2,21)]+[k*10 for k in range(3,11)]}\n(knn_grid_train_x, knn_grid_train_y, knn_grid_test_x, knn_grid_test_y) = split_test_train(0.8, big_df)\nknn_grid_results = GridSearchCV(KNeighborsClassifier(), knn_param_grid, cv=5).fit(knn_grid_train_x, knn_grid_train_y)\n# All results\n\nknn_means = knn_grid_results.cv_results_['mean_test_score']\nknn_stds = knn_grid_results.cv_results_['std_test_score']\nknn_params = knn_grid_results.cv_results_['params']\n\nfor mean, std, params in zip(knn_means, knn_stds, knn_params):\n    print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params))\nprint('Best parameters found:\\n', knn_grid_results.best_params_, \"with score of: \", max(knn_grid_results.cv_results_['mean_test_score']))\n\ndef plotting_func_knn(mean, params, plt, ax):\n    x_var = \"n_neighbors\"\n    #print(params[\"hidden_layer_sizes\"])\n    ax.plot(params[x_var], mean, \"o\", color=\"r\")\n    \nplot_grid_search(knn_grid_results, plotting_func_knn, \"Titanic KNN Training Accuracy as a Function of Number of Neigbors\", \"n_neighbors\", \"accuracy\",(6,4), [])\n\neval_for_conclusion(\"Titanic KNN\", knn_grid_results, knn_grid_test_x, knn_grid_test_y)\n\n\n# (knn_test_scores, knn_train_scores) = train_and_test(big_df, knn_classifier)\n# knn_cv_scores = cross_validate(big_df, knn_classifier)\n\n# print(\"knn test set scores: \", knn_test_scores)\n# print(\"knn train set scores: \", knn_train_scores) \n# print(\"knn cross validation set scores: \", knn_cv_scores) \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"neural_net_classifier = MLPClassifier(max_iter=10000) #, alpha=0.01, hidden_layer_sizes=(6, 3), random_state=1)\n# tried with6,3 and works great. Other dimensions are horrible\nevaluate_model(big_df, neural_net_classifier, \"Titanic NeuralNet baseline\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# neural_net_classifier = MLPClassifier(max_iter=10000, alpha=0.01, hidden_layer_sizes=(6, 3), random_state=1)\n# # tried with6,3 and works great. Other dimensions are horrible\n# (nn_test_scores, nn_train_scores, nn_cv_scores) = evaluate_model(big_df, neural_net_classifier, \"NeuralNet baseline model\")\n# optimal_test_split = nn_test_scores.index(max(nn_test_scores)) * 0.1\n# optimal_test_split = max(optimal_test_split, 0.7)\n\n# print(\"max index for means was: \", optimal_test_split * 10)\n\n\nmlp = MLPClassifier(max_iter=10000)\n#     'hidden_layer_sizes': [(50,50,50), (50,100,50), (100,)],\n#     'activation': ['tanh', 'relu'],\n#     'solver': ['sgd', 'adam'],\n#     'learning_rate': ['constant','adaptive'],\nparameter_space = {\n    'activation': ['tanh', 'relu'],\n    'alpha': [0.0001, 0.0005, 0.01, 0.05, 0.1],\n    \"hidden_layer_sizes\": [(3,), (5,), (7,)]\n    \n}\n\nnn_grid_clf = GridSearchCV(mlp, parameter_space, n_jobs=-1, cv=3)\n(nn_train_x, nn_train_y, nn_test_x, nn_test_y) = split_test_train(0.8, big_df)\nnn_grid_clf.fit(nn_train_x, nn_train_y)\n# Best paramete set\nprint('Best parameters found:\\n', nn_grid_clf.best_params_)\n\n# All results\nnn_means = nn_grid_clf.cv_results_['mean_test_score']\nnn_stds = nn_grid_clf.cv_results_['std_test_score']\nnn_params = nn_grid_clf.cv_results_['params']\n\nfor mean, std, params in zip(nn_means, nn_stds, nn_params):\n    print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params))\nprint(\"best params \", nn_grid_clf.best_params_)\nnn_grid_score = nn_grid_clf.score(nn_test_x,nn_test_y)\nprint(\"nn grid search model score: \", nn_grid_score)\n\n\n# mlp = MLPClassifier(max_iter=10000, hidden_layer_sizes=(6, 3), alpha=clf.best_params_[\"alpha\"], activation=clf.best_params_[\"activation\"])\n# mlp.fit(nn_train_x, nn_train_y)\n# nn_test_score = round(mlp.score(nn_test_x, nn_test_y) * 100, 2)\n# print(\"nn grid search model score: \", nn_test_score)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# All results\n# nn_means = nn_grid_clf.cv_results_['mean_test_score']\n# nn_stds = nn_grid_clf.cv_results_['std_test_score']\n# nn_params = nn_grid_clf.cv_results_['params']\n\n# for mean, std, params in zip(nn_means, nn_stds, nn_params):\n#     print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params))\n# print(\"best params \", nn_grid_clf.best_params_)\n\n\ndef plotting_func_nn(mean, params, plt, ax):\n    x_var = \"alpha\"\n    tick_spacing = 0.1\n    layer_colors = {\"(3,)\":\"orange\", \"(5,)\":\"red\", \"(7,)\":\"black\"}\n    activation_labels = {\"tanh\": \"o\", \"relu\":\"s\"}\n    #print(params[\"hidden_layer_sizes\"])\n    layer_color_idx = str(params[\"hidden_layer_sizes\"])\n    activation_idx = params[\"activation\"]\n    ax.plot(math.log(params[x_var],10), mean, activation_labels[activation_idx], color=layer_colors[layer_color_idx])\n    x_loc = plticker.MultipleLocator(base=1.0) # this locator puts ticks at regular intervals\n    ax.xaxis.set_major_locator(x_loc)\n    y_loc = plticker.MultipleLocator(base=0.005) # this locator puts ticks at regular intervals\n    ax.yaxis.set_major_locator(y_loc)\nred_patch = mpatches.Patch(color='orange', label='(3,) layer')\norange_patch = mpatches.Patch(color='red', label='(5,) layer')\nblack_patch = mpatches.Patch(color='black', label='(7,) layer')\ntanh = mlines.Line2D([], [],marker='o',\n                         label='tanh')\nrelu = mlines.Line2D([], [],marker='s',\n                         label='relu')\nhandles = [tanh, relu, red_patch, orange_patch, black_patch]\n\nplot_grid_search(nn_grid_clf, plotting_func_nn, \"Titanic NN accuracy as a function of log(alpha Param)\", \"log(alpha)\", \"accuracy\",(5,7), handles)\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"boost_classifier = AdaBoostClassifier()\nevaluate_model(big_df, boost_classifier, \"Titanic Boosting_classifier\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"boost_parameter_space = {\n    'n_estimators': [i*10 for i in range(5,11)],\n    'learning_rate': [ float(i) / 100 for i in range (1, 150, 10)]\n}\nboost_grid_clf = GridSearchCV(boost_classifier, boost_parameter_space, n_jobs=-1, cv=3)\n(boost_train_x, boost_train_y, boost_test_x, boost_test_y) = split_test_train(0.1 * 8, big_df)\nscaler = StandardScaler()\nscaler.fit(boost_train_x)\nboost_train_x = scaler.transform(boost_train_x)\nboost_test_x = scaler.transform(boost_test_x)\nboost_grid_clf.fit(boost_train_x, boost_train_y)\n\nprint(\"best Boost params \", boost_grid_clf.best_params_)\nboost_grid_score = boost_grid_clf.score(boost_test_x,boost_test_y)\nprint(\"Bost grid search model test set score: \", boost_grid_score)\nprint('Best Boost parameters found through cv:\\n', boost_grid_clf.best_params_, \"with score of: \", max(boost_grid_clf.cv_results_['mean_test_score']))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plotting_func_boost(mean, params, plt, ax):\n    x_var = \"learning_rate\"\n    cmap = cm.hot\n    norm = Normalize(vmin=-110, vmax=-20)\n    ax.plot(params[x_var], mean,\"o\", color=cmap(norm(-1*params[\"n_estimators\"]))) #, markeredgecolor = \"black\")\n    plt.ylim(.76, .835)\n    plt.xlim(0, 1.5)\n\ncmap = cm.hot\nnorm = Normalize(vmin=-110, vmax=-20)\nyellow_patch = mpatches.Patch(color=cmap(norm(-50)), label='n_estimators=50')\nred_patch = mpatches.Patch(color=cmap(norm(-80)), label='n_estimators=80')\nblack_patch = mpatches.Patch(color=cmap(norm(-110)), label='n_estimators=110')\nhandles = [yellow_patch, red_patch, black_patch] #[linear, rbf]#, red_patch, orange_patch, black_patch]\n\n\nplot_grid_search(boost_grid_clf, plotting_func_boost, \"Titanic Boosting accuracy as a function of learning rate\", \"learning rate\", \"accuracy\",(5,8), handles)\n\neval_for_conclusion(\"Titanic Boosting\", boost_grid_clf, boost_test_x, boost_test_y)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svm_classifier = svm.SVC()\nevaluate_model(big_df, svm_classifier, \"Titanic svm_classifier_baseline\")\n\n# (svm_test_scores, svm_train_scores) = train_and_test(big_df, svm_classifier)\n# svm_cv_scores = cross_validate(big_df, svm_classifier)\n\n# print(\"svm test set scores: \", svm_test_scores)\n# print(\"svm train set scores: \", svm_train_scores) \n# print(\"svm cross validation set scores: \", svm_cv_scores) \n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svm_parameter_space = {\n    'kernel': ['poly', 'rbf'],\n    'C': [ float(i) / 100 for i in range (1, 130, 5)]+[0.0001, 0.000001, 3,4,5,6,7],    \n}\nsvm_grid_clf = GridSearchCV(svm_classifier, svm_parameter_space, n_jobs=-1, cv=3)\n(svm_train_x, svm_train_y, svm_test_x, svm_test_y) = split_test_train(0.1 * 8, big_df)\nscaler = StandardScaler()\nscaler.fit(svm_train_x)\nsvm_train_x = scaler.transform(svm_train_x)\nsvm_test_x = scaler.transform(svm_test_x)\nsvm_grid_clf.fit(svm_train_x, svm_train_y)\n\nprint(\"best params \", svm_grid_clf.best_params_)\nsvm_grid_score = svm_grid_clf.score(svm_test_x,svm_test_y)\nprint(\"Titanic SVM grid search model test set score: \", svm_grid_score)\nprint('Titanic Best SVM parameters found through cv:\\n', svm_grid_clf.best_params_, \"with score of: \", max(svm_grid_clf.cv_results_['mean_test_score']))\n\n\n\n# svm_classifier.fit(nn_train_x, nn_train_y)\n# svm_svc = svm.SVC(kernel='rbf', gamma=0.013, C=1)\n# evaluate_model(big_df, rbf_svc, \"svm_classifier__kernel='rbf'__gamma=0.013__C=1\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plotting_func_svm(mean, params, plt, ax):\n    x_var = \"C\"\n    tick_spacing = 0.1\n    #layer_colors = {\"(3,)\":\"orange\", \"(5,)\":\"red\", \"(7,)\":\"black\"}\n    kernel_labels = {\"poly\": \"o\", \"rbf\":\"o\"}\n    kernel_colors = {\"poly\": \"red\", \"rbf\":\"black\"}\n    #print(params[\"hidden_layer_sizes\"])\n    #layer_color_idx = str(params[\"hidden_layer_sizes\"])\n    kernel_idx = params[\"kernel\"]\n    ax.plot(params[x_var], mean, kernel_labels[kernel_idx], color=kernel_colors[kernel_idx])\n    x_loc = plticker.MultipleLocator(base=0.5) # this locator puts ticks at regular intervals\n    ax.xaxis.set_major_locator(x_loc)\n    y_loc = plticker.MultipleLocator(base=0.005) # this locator puts ticks at regular intervals\n    ax.yaxis.set_major_locator(y_loc)\n    plt.ylim(.77, .83)\n\nlinear = mlines.Line2D([], [],marker='o',\n                         label='poly', color=\"r\")\nrbf = mlines.Line2D([], [],marker='o',\n                         label='rbf', color=\"black\")\nhandles = [linear, rbf]#, red_patch, orange_patch, black_patch]\n\nplot_grid_search(svm_grid_clf, plotting_func_svm, \"Titanic SVM accuracy as a function of C\", \"C\", \"accuracy\",(5,7), handles)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"eval_df = pd.DataFrame(columns=[\"model\", \"recall\", \"accuracy\", \"precision\"])\neval_df = eval_df.append(eval_for_conclusion(\"Titanic DT\", dt_grid_results, dt_grid_test_x, dt_grid_test_y), ignore_index=True)\neval_df = eval_df.append(eval_for_conclusion(\"Titanic Boosting\", boost_grid_clf, boost_test_x, boost_test_y), ignore_index=True)\neval_df = eval_df.append(eval_for_conclusion(\"Titanic SVM\", svm_grid_clf, svm_test_x, svm_test_y), ignore_index=True)\neval_df = eval_df.append(eval_for_conclusion(\"Titanic KNN\", knn_grid_results, knn_grid_test_x, knn_grid_test_y), ignore_index=True)\neval_df = eval_df.append(eval_for_conclusion(\"Titanic Neural Network\", nn_grid_clf, nn_test_x, nn_test_y), ignore_index=True)\neval_df.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}