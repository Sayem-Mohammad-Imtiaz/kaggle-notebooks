{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Columns\n\nage: age of primary beneficiary\n\nsex: insurance contractor gender, female, male\n\nbmi: Body mass index, providing an understanding of body, weights that are relatively high or low relative to height,\nobjective index of body weight (kg / m ^ 2) using the ratio of height to weight, ideally 18.5 to 24.9\n\nchildren: Number of children covered by health insurance / Number of dependents\n\nsmoker: Smoking\n\nregion: the beneficiary's residential area in the US, northeast, southeast, southwest, northwest.\n\ncharges: Individual medical costs billed by health insurance","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df=pd.read_csv('/kaggle/input/insurance/insurance.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Bivariate Analysis**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Age vs charges","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.scatterplot(df.age,df.charges)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we can see as age is increasing, insurance charges are also increasing . Another important insight here would be 3 different groups can be seen here. This could be reflective of 3 different insurance packages with different benefits","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Sex vs charges","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.groupby('sex')['charges'].mean().plot.bar()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see mean insurance charges for males are higher in comparison to females","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"BMI vs charges","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.scatterplot(df.bmi,df.charges)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There does not seem to be no significant correlation between the bmi and charges","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Children vs charges","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(y='charges',x='children',data=df)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Insurance cost is high if children are 2 or 3 but if children are 3+ then maybe insurance company is providing discounts and thus leading to decrease in insurance charges","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Smoker vs Charges","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.groupby('smoker')['charges'].mean().plot.bar()\nplt.show","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Insurance charges are high if the customer is a smoker as there is a high chance he/she is at high risk of other diseases  ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Region vs charges","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x='region',y='charges',data=df)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Median Insurance cost is highest in southeast US whereas its similar in other parts ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Converting categorical columns into numerical \n\nColumns- region ,smoker,sex","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.groupby('region')['charges'].median().sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Replacing region categories with labels as per the median values\n#Region with highest median will get the highest numerical values\ndf.region=df.region.map({'northeast':4,'southeast':3,'northwest':2,'southwest':1})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating dummies for smoker with drop_first=True\ndf=pd.get_dummies(df,columns=['sex','smoker'],drop_first=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **Base Model**\n\nBuilding linear regression through OLS method","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import statsmodels.api as  sm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X=df.drop(columns='charges')\nY=df.charges","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=.3,random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_const=sm.add_constant(X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model=sm.OLS(Y_train,X_train_const).fit()\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Calcualting SSE (Sum of squared error) & SSR(Sum of Squared Regression)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test.shape,X_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test_const=sm.add_constant(X_test)\ny_pred=model.predict(X_test_const)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"SSE=np.sum((Y_test-y_pred)**2)\nSSR=np.sum((y_pred-Y_test.mean())**2)\nSST=SSE+SSR","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"R2=SSR/SST\nR2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"N=len(X_test)# test data size\np=len(X_test.columns)\nAdj_R2=1-(((1-R2)*(N-1))/(N-p-1))\nAdj_R2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Calculating RMSE(Root Mean Squared Error)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"rmse=np.sqrt(np.sum((y_pred-Y_test)**2)/N)\nrmse","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Building model through sklearn (Sklearn uses Gradient Descent Approach)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr=LinearRegression()\nlr.fit(X_train,Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_lr=lr.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr.score(X_test,Y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\nrmse_lr=np.sqrt(mean_squared_error(Y_test,y_pred_lr))\nrmse_lr","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\nRMSE is coming out to be same from both the methods OLS and Gradient Descent(Sklearn lib)\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Checking Assumptions of Linear Regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## No Auto Correlation - \nIn Context of linear regression, auto correlation refers to correlation between consecutive residuals\n\nTest needed : Durbin- Watson Test.\n\n    It's value ranges from 0-4. If the value of Durbin- Watson is Between 0-2, it's known as Positive Autocorrelation.\n    \n    If the value ranges from 2-4, it is known as Negative autocorrelation.\n    \n    If the value is exactly 2, it means No Autocorrelation.\n    \nFor a good linear model, it should have low or no autocorrelation.\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We can see from summary table Durbin Watson is almost close to 2 so there is no auto correlation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import statsmodels.tsa.api as smt\nacf = smt.graphics.plot_acf(model.resid, lags=40 , alpha=0.05)\nacf.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Normality of residuals\n1. The second assumption is the Normality of Residuals. \nFor this we prefer the Jarque Bera test. For a good model, the residuals should be normally distributed.\nThe higher the value of Jarque Bera test , the lesser the residuals are normally distributed.\nWe generally prefer a lower value of jarque bera test.\n\n  The Jarque–Bera test is a goodness-of-fit test of whether sample data \n  have the skewness and kurtosis matching a normal distribution.\n\n    \n  The jarque bera test tests whether the sample data has the skewness and kurtosis matching a normal distribution.\n  Note that this test generally works good for large enough number of data samples(>2000) as the test statistics asymptotically has a chi       squared distribution with degrees 2 of freedom.\n\n2. We can go for shapiro test here as our dataset is small.\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.stats import shapiro\nshapiro(model.resid)\n#Since p value is almost 0 so we reject null hypothesis and residual distribution is not normal","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Here we can see visually also that residuals are not normally distributed and there is high skewness\nsns.distplot(model.resid)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Linearity of residuals\nHere we have 2 options. Either we can plot the observed values Vs predicted values and plot the Residual Vs predicted values and see the linearity of residuals.\n\nOR\n\nWe can go for rainbow test. Let's look both of them one by one.\n\nOR \n\nWe can also check if mean of residual is 0 or not","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline\n%config InlineBackend.figure_format ='retina'\nimport seaborn as sns \nimport matplotlib.pyplot as plt\nimport statsmodels.stats.api as sms\nsns.set_style('darkgrid')\nsns.mpl.rcParams['figure.figsize'] = (15.0, 9.0)\n\ndef linearity_test(model, y):\n    '''\n    Function for visually inspecting the assumption of linearity in a linear regression model.\n    It plots observed vs. predicted values and residuals vs. predicted values.\n    \n    Args:\n    * model - fitted OLS model from statsmodels\n    * y - observed values\n    '''\n    fitted_vals = y_pred\n    resids = y_pred-Y_test\n\n    fig, ax = plt.subplots(1,2)\n    \n    sns.regplot(x=fitted_vals, y=y, lowess=True, ax=ax[0], line_kws={'color': 'red'})\n    ax[0].set_title('Observed vs. Predicted Values', fontsize=16)\n    ax[0].set(xlabel='Predicted', ylabel='Observed')\n\n    sns.regplot(x=fitted_vals, y=resids, lowess=True, ax=ax[1], line_kws={'color': 'red'})\n    ax[1].set_title('Residuals vs. Predicted Values', fontsize=16)\n    ax[1].set(xlabel='Predicted', ylabel='Residuals')\n    \nlinearity_test(model, Y_test)  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To detect nonlinearity one can inspect plots of observed vs. predicted values or residuals vs. predicted values. \nThe desired outcome is that points are symmetrically distributed around a diagonal line in the former plot or \naround horizontal line in the latter one. \nIn both cases with a low linearity of residuals can be seen. Less verify with rainbow test","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import statsmodels.api as sm\nsm.stats.diagnostic.linear_rainbow(res=model, frac=0.5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"P value is  more than .05 so we fail to reject null hypothesis and there is linearity in residuals","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"np.mean(model.resid)\n# mean is close to 0 so residual are linear","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Homoscedasticity_test(using goldfeld test) OR (Beusch-Wagon Test)\nHomoscedacity :: If the residuals are symmetrically distributed across the trend , then it is called as homoscedacious.\n\nHeteroscedacity :: If the residuals are not symmetric across the trend, then it is called as heteroscedacious. In this the residuals can form an arrow shape or any other non symmetrical shape.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"This test is based on the hytpothesis testing where null and alternate hypothesis are:\n\nH0 = constant variance among residuals. (Homoscedacity)\n\nHa = Heteroscedacity.\n\nThe residuals should be homoscedacious.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.compat import lzip\nfrom statsmodels.compat import lzip\n%matplotlib inline\n%config InlineBackend.figure_format ='retina'\nimport statsmodels.stats.api as sms\nsns.set_style('darkgrid')\nsns.mpl.rcParams['figure.figsize'] = (15.0, 9.0)\n\nmodel = model\nfitted_vals = model.predict()\nresids = model.resid\nresids_standardized = model.get_influence().resid_studentized_internal\nfig, ax = plt.subplots(1,2)\n\nsns.regplot(x=fitted_vals, y=resids, lowess=True, ax=ax[0], line_kws={'color': 'red'})\nax[0].set_title('Residuals vs Fitted', fontsize=16)\nax[0].set(xlabel='Fitted Values', ylabel='Residuals')\nsns.regplot(x=fitted_vals, y=np.sqrt(np.abs(resids_standardized)), lowess=True, ax=ax[1], line_kws={'color': 'red'})\nax[1].set_title('Scale-Location', fontsize=16)\nax[1].set(xlabel='Fitted Values', ylabel='sqrt(abs(Residuals))')\n\nname = ['F statistic', 'p-value']\ntest = sms.het_goldfeldquandt(model.resid, model.model.exog)\nlzip(name, test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" We can also use two statistical tests: Breusch-Pagan and Goldfeld-Quandt. In both of them the null hypothesis assumes \nhomoscedasticity and a p-value below a certain level (like 0.05)\nindicates we should reject the null in favor of heteroscedasticity.\n\nHere, p value is less than 0.05 so, it is homoscedasticity distribution.\n\nH0 = constant variance (Homoscedacity)\n\nHa = Heteroscedacity","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## MultiCollinearity\n\nWe use VIF method to check for multicollinearity.\nIn VIF Method we only deal with independent variables or features. VIF value for each feature is calculated by making that feature as target and rest of the features as independent variables and regression is done and VIF is calculated as 1/(1-R^2).\n\nSo for a feature exhibiting multicollinearity, R score would be high so denomminator will be low and VIF value would be high. We take feature with VIF factor >5 to have multicollinearity","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.stats.outliers_influence import variance_inflation_factor\n\nvif = [variance_inflation_factor(X_train_const.values, i) for i in range(X_train_const.shape[1])]\npd.DataFrame({'vif': vif}, index=X_train_const.columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since VIF factor for all features is less than 5 so no multicollinearity exists","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}