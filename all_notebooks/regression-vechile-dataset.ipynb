{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Car price prediction"},{"metadata":{},"cell_type":"markdown","source":"<img src=\"https://s1.1zoom.ru/b5050/215/BMW_E46_M3_silver_450821_1366x768.jpg\" alt=\"Drawing\" style=\"width: 900px;\">\n\n"},{"metadata":{},"cell_type":"markdown","source":"# Table of contents"},{"metadata":{},"cell_type":"markdown","source":"- [Imports](#imports)\n- [Read the data](#read)\n- [EDA](#eda)\n  - [Overview](#eda.overview)\n  - [Data transformation. Stage 1](#eda.dt1)\n  - [Let's take a closer look at the data](#eda.closer)\n  - [Data transformation. Stage 2](#eda.dt2)\n  - [Deal with NA](#eda.na)\n  - [Data transformation. Stage 3](#eda.dt3)\n  - [And final pairplot...](#eda.fpp)\n  - [Conclusion](#eda.c)\n- [Linear Regression](#lr)\n  - [Dataset](#lr.ds)\n  - [Regression analysis](#lr.ra)\n  - [Conclusion](#lr.c)\n  - [Ridge regression](#lr.rr)\n  - [Conclusion](#lr.rrc)\n- [XGBoost](#xgb)\n  - [Dataset](#xgb.ds)\n  - [Model](#xgb.m)\n- [Conclusion](#conclusion)"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"imports\"></a>\n# Imports"},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport re\nimport random\nimport warnings\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n\nfrom sklearn.linear_model import LinearRegression, Lasso, Ridge\nimport xgboost as xgb\n\nfrom sklearn.metrics import mean_squared_error, r2_score\n\nimport statsmodels.api as sm\nimport statsmodels.stats.diagnostic as smd\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nfrom scipy.stats import shapiro, boxcox, kstest, probplot\nfrom hyperopt import fmin, tpe, hp, STATUS_OK, Trials","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"random_state=10\n#warnings.filterwarnings(\"error\")\npd.set_option('display.max_rows', 100)\npd.set_option('display.max_columns', 100)\npd.set_option('display.width', None)\npd.set_option('display.max_colwidth', None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def validate(y_true, y_pred):\n    resid = y_true - y_pred\n    \n    mse = mean_squared_error(y_true, y_pred)\n    r2 = r2_score(y_true, y_pred)\n    print(\"MSE: %s\" % mse)\n    print(\"R^2: %s\" % r2)\n    print(\"Residuals mean: {0}\".format(np.mean(resid)))\n    \n    fig, ax = plt.subplots(figsize=(19,4), ncols=4)\n    ax[0] = sns.scatterplot(x=y_true, y=resid, ax=ax[0])\n    ax[1] = sns.scatterplot(x=y_true, y=y_pred, ax=ax[1])\n    ax[2] = sns.histplot(resid, ax=ax[2])\n    probplot(resid, dist=\"norm\",  plot=ax[3])\n    \n    statistic, p_value = kstest(resid, 'norm')\n    if p_value>0.05:\n        print(\"Distribution is normal. Statistic: {0:.3}, p-value: {1:.4}\".format(statistic, p_value))\n    else:\n        print(\"Distribution is not normal. Statistic: {0:.3}, p-value: {1:.4}\".format(statistic, p_value))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"read\"></a>\n# Read the data "},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/input/vehicle-dataset-from-cardekho/Car details v3.csv\")\ndisplay(data.head(3))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"eda\"></a>\n# EDA"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"eda.overview\"></a>\n### Overview"},{"metadata":{"trusted":false},"cell_type":"code","source":"display(data.info())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see missing values and some data type mismatches.\n\nThe most important features are filled in completely, so i'll deal with missing data later."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"eda.dt1\"></a>\n### Data transformation. Stage 1"},{"metadata":{},"cell_type":"markdown","source":"#### Data types"},{"metadata":{"trusted":false},"cell_type":"code","source":"data[\"mileage\"] = data[\"mileage\"].str.replace(\" kmpl\", \"\")\ndata[\"mileage\"] = data[\"mileage\"].str.replace(\" km/kg\", \"\")\ndata[\"mileage\"] = data[\"mileage\"].astype(float)\n\ndata[\"engine\"] = data[\"engine\"].str.replace(\" CC\", \"\")\ndata[\"engine\"] = data[\"engine\"].astype(float, errors=\"ignore\")\n\ndata[\"max_power\"] = data[\"max_power\"].str.replace(\" bhp\", \"\")\ndata.loc[data[\"max_power\"]=='', \"max_power\"]=np.NaN\ndata[\"max_power\"] = data[\"max_power\"].astype(float, errors=\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### owner"},{"metadata":{"trusted":false},"cell_type":"code","source":"remapped = {'First Owner': 1, 'Second Owner': 2, 'Third Owner': 3, 'Fourth & Above Owner': 4, 'Test Drive Car': 0}\ndata = data.replace({\"owner\": remapped})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### date"},{"metadata":{"trusted":false},"cell_type":"code","source":"max_date = max(data[\"year\"])\ndata[\"year2\"] = data[\"year\"].apply(lambda x: max_date - x)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### torque"},{"metadata":{"trusted":false},"cell_type":"code","source":"def torque_parser(x):\n    try:\n        try:\n            parsed = re.findall(r\"([\\d]+).*(nm|kgm)\", x, re.IGNORECASE)[0]\n        except Exception as e:\n            parsed = [re.findall(r\"[\\d]+\", x)[0], \"nm\"]\n        finally:\n            if parsed[1].lower() == \"nm\":\n                torque = float(parsed[0])\n            else:\n                kgm = float(parsed[0])\n                if kgm < 100:\n                    torque = float(parsed[0])/0.10197\n                else:\n                    torque = float(parsed[0])\n    except Exception as e:\n        torque = np.NaN\n    return torque \n\ndata[\"torque2\"] = data[\"torque\"].apply(torque_parser)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"data = data.drop([\"year\", \"torque\"], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"eda.closer\"></a>\n### Let's take a closer look at the data"},{"metadata":{"trusted":false},"cell_type":"code","source":"display(data.describe())\ndisplay(data.describe(include=object))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"ax = sns.pairplot(data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Not all predictors have a linear relationship with the target variable\n- I assume that the brand and model will affect the value\n- Zero mileage and zero max_power looks bad\n- km_driver more than 300k km looks like outliars.\n- mileage more than 35 looks like outliars.\n- 789nm looks like a bug"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"eda.dt2\"></a>\n### Data transformation. Stage 2"},{"metadata":{},"cell_type":"markdown","source":"#### Brand and model"},{"metadata":{"trusted":false},"cell_type":"code","source":"def brand_parser(x):\n    try:\n        parsed = re.findall(r\"^(\\S*)\\s(\\S*)\", x, re.IGNORECASE)[0]\n    except Exception as e:\n        parsed = [\"unparsed\", \"value\"]\n    finally:\n        return parsed[0] + \" \" + parsed[1]\n    \ndata[\"brand_model\"] = data[\"name\"].apply(brand_parser)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(16,3))\n\nvals, cnts = np.unique(data[\"brand_model\"], return_counts=True)\nidxs = np.argsort(-cnts)\n\nmodels = np.random.choice(vals, 40)\ndf = data[data[\"brand_model\"].isin(models)]\nax = sns.boxplot(x=df[\"brand_model\"], y=df[\"selling_price\"], ax=ax)\n\nfor tick in ax.get_xticklabels():\n    tick.set_rotation(90)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"eda.na\"></a>\n### Deal with NA"},{"metadata":{},"cell_type":"markdown","source":"Fill in the missing values with the average for each brand_model"},{"metadata":{"trusted":false},"cell_type":"code","source":"data[\"mileage\"] = data.groupby(\"brand_model\").transform(lambda x: x.fillna(x.mean()))[\"mileage\"]\ndata[\"engine\"] = data.groupby(\"brand_model\").transform(lambda x: x.fillna(x.mean()))[\"engine\"]\ndata[\"max_power\"] = data.groupby(\"brand_model\").transform(lambda x: x.fillna(x.mean()))[\"max_power\"]\ndata[\"seats\"] = data.groupby(\"brand_model\").transform(lambda x: x.fillna(np.round(x.mean())))[\"seats\"]\ndata[\"torque2\"] = data.groupby(\"brand_model\").transform(lambda x: x.fillna(x.mean()))[\"torque2\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"na_count = data.isna().any(axis=1).sum()\nprint(\"Records with NA values: %s\" % na_count)\ndata = data.dropna()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"eda.dt3\"></a>\n### Data transformation. Stage 3"},{"metadata":{},"cell_type":"markdown","source":"#### Drop bad values"},{"metadata":{"trusted":false},"cell_type":"code","source":"data = data[data[\"mileage\"]>0]\ndata = data[data[\"max_power\"]>0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Target variable"},{"metadata":{"trusted":false},"cell_type":"code","source":"data[\"selling_price2\"] = np.log(data[\"selling_price\"])\ncols = data.columns.tolist()\ncols = cols[-1:] + cols[:-1]\ndata = data[cols]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### km_driver"},{"metadata":{"trusted":false},"cell_type":"code","source":"# km_driver more than 300k km looks like outliars. От греха подальше...\ndata = data[data[\"km_driven\"]<300000]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### mileage"},{"metadata":{"trusted":false},"cell_type":"code","source":"# mileage more than 35 looks like outliars. Туда же.\ndata = data[data[\"mileage\"]<35]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Torque  values"},{"metadata":{"trusted":false},"cell_type":"code","source":"# \"Maruti Zen D\" torque looks like a mistake. It isn't 789nm, but 78nm. Хотел бы я 790 Нм, но нет.\ndata.loc[data[\"name\"]==\"Maruti Zen D\", \"torque2\"] = 78\n# this will make the relationship between torque2 and target variable more linear \ndata[\"torque2\"] = np.log(data[\"torque2\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Other mistakes"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Fix some mistakes\ndata.loc[data[\"brand_model\"]==\"Honda BRV\", \"brand_model\"] = \"Honda BR-V\"\ndata.loc[data[\"brand_model\"]==\"Ford Ecosport\", \"brand_model\"] = \"Ford EcoSport\"\ndata.loc[data[\"brand_model\"]==\"Ambassador CLASSIC\", \"brand_model\"] = \"Ambassador Classic\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"eda.fpp\"></a>\n### And final pairplot..."},{"metadata":{"trusted":false},"cell_type":"code","source":"ax = sns.pairplot(data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(15,5))\ncorr = data.corr()\nax = sns.heatmap(corr, annot=True, ax=ax, cmap=\"YlGnBu\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"eda.c\"></a>\n## Conclusion \n1. The distribution of the target variable appears to be normal. This does not linear regression assume, but in this case it improves the result.\n2. Removed explicit outliers and corrected data errors\n3. The dependence of predictors with target variable appears to be linear\n4. Correlation matrix does not show strong linear relationship between predictors"},{"metadata":{"trusted":false},"cell_type":"code","source":"data = data.drop([\"selling_price\"], axis=1)\ndata_cleared = data.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"data = data_cleared.copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"lr\"></a>\n# Linear regression model"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"lr.ds\"></a>\n## Dataset"},{"metadata":{"trusted":false},"cell_type":"code","source":"data_lr = data.copy()\n\ny = data_lr[\"selling_price2\"]\nX = data_lr.drop([\"name\", \"selling_price2\"], axis=1)\n\nX = pd.get_dummies(X, columns=[\"fuel\", \"seller_type\", \"transmission\", \"owner\", \"seats\", \"brand_model\"])\n\ndisplay(X.shape)\ndisplay(X.head(2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"lr.ra\"></a>\n## Regression analysis"},{"metadata":{"trusted":false},"cell_type":"code","source":"X_ = sm.add_constant(X)\nmodel_ols = sm.OLS(y, X_).fit()\nprint(model_ols.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"y_pred = model_ols.predict(X_)\nvalidate(y, y_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This model explains 94.6% of the variation in the dependent variable, while the MSE was 0.038.\n\nWhen diagnosing the model, 2 problems were identified:\n - Abnormal distribution of residuals\n - Signs of heteroscedasticity\n \nViolating the linear regression assumptions can result in the trained model not being optimal for a given dataset.\nAlso, if the assumption about the random distribution of residuals is violated, we cannot reliably use statistical tests to determine the significance of the predictor.\n\nViolations of linear regression assumptions may be due to outliers, non-linear relationships, or the absence of a predictor.\n\nLooking ahead, I will say that the transformation of predictors did not lead to an increase in the accuracy of the model.\n\nLet's try to identify and remove outliers."},{"metadata":{"trusted":false},"cell_type":"code","source":"influence = model_ols.get_influence()\n(c, p) = influence.cooks_distance\n    \ndistances = pd.DataFrame(c, index=X.index)\ndistances = distances.fillna(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"n_max = 30\nmax_values = distances.nlargest(n_max, columns=0)[0]\n\nfig, ax = plt.subplots(figsize=(16, 4))\nax.set_yscale(\"log\")\nax = sns.barplot(y=max_values, x=np.arange(n_max), ax=ax, palette=\"Blues_r\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"X = X.drop(max_values.index)\ny = y.drop(max_values.index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"X_ = sm.add_constant(X)\nmodel_ols2 = sm.OLS(y, X_).fit()\nprint(model_ols2.summary())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"y_pred = model_ols.predict(X_)\nvalidate(y, y_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"lr.c\"></a>\n### Conclusion"},{"metadata":{},"cell_type":"markdown","source":"Deleting points with great influence allowed to slightly improve performance, but the model has not changed fundamentally."},{"metadata":{},"cell_type":"markdown","source":"Let's build a regression model taking into account the identified problems.\n\nI will assume that hetetoscedasticity may be due to the absence of a predictor. For example, the equipment of the car, which affects the cost."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"lr.rr\"></a>\n## Ridge regression"},{"metadata":{"trusted":false},"cell_type":"code","source":"scaler = StandardScaler()\nX_sc = pd.DataFrame(scaler.fit_transform(X), index=X.index)\n\nX_train, X_test, y_train, y_test = train_test_split(X_sc, y, test_size=0.33, random_state=random_state)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def hyperopt(X, y, params):\n    try:\n        model = Ridge(**params, normalize=False)\n        score = cross_val_score(model, X, y, cv=5, n_jobs=-1, scoring='neg_mean_squared_error')\n        return -score.mean()\n    \n    except Exception as ex :\n        print(ex)\n        return np.inf\n\ndef f_model(params):\n    global best\n    global best_params\n    acc = hyperopt(X_train, y_train, params)\n    if (acc < best):\n        best = acc\n        best_params = params\n        print(\"new best: {0:.7} {1}\".format(best, params))\n    return {'loss': acc, 'status': STATUS_OK}\n\n\ndef model_tune(space, random_state=random_state, iters=10):\n    global best\n    global best_params\n    best, best_params = np.inf, None \n    res = fmin(f_model, space, algo=tpe.suggest, max_evals=iters, rstate=np.random.RandomState(random_state))\n    model = Ridge(random_state=random_state, **best_params, normalize=False)\n    print(\"\\nBest_params: \\n\", best_params)\n    return model\n\n\nspace_l = {\n    'alpha': hp.uniform('alpha', 0.00001, 2),\n    'tol': hp.uniform('tol', 0.000001, 0.5),\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"model_reg = model_tune(space_l, iters=30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"model_reg = model_reg.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"y_pred = model_reg.predict(X_test)\nvalidate(y_test, y_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"lr.rrc\"></a>\n### Conclusion"},{"metadata":{},"cell_type":"markdown","source":"The model is built, the previously mentioned problems are observed - the non-normality of the distribution of the residuals and the signs of heteroscedasticity.\n\nThe MSE value is 0.042. But the average of the errors is close to zero."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"xgb\"></a>\n# XGBOOST"},{"metadata":{},"cell_type":"markdown","source":"For comparison, let's build a XGBoost model."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"xgb.ds\"></a>\n## Dataset"},{"metadata":{"trusted":false},"cell_type":"code","source":"data_xgb = data.copy()\n\ny = data_xgb[\"selling_price2\"].copy()\nX = data_xgb.drop([\"name\", \"selling_price2\"], axis=1).copy()\n\nX = pd.get_dummies(X, columns=[\"fuel\", \"seller_type\", \"seats\", \"transmission\", \"brand_model\"], drop_first=True)\n\nscaler = MinMaxScaler()\nX = pd.DataFrame(scaler.fit_transform(X), index=X.index)\n\ndisplay(X.head(3))\ndisplay(y.head(3))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"xgb.m\"></a>\n## Model"},{"metadata":{"trusted":false},"cell_type":"code","source":"xg_reg = xgb.XGBRegressor(objective ='reg:squarederror', \n                          colsample_bytree = 0.3, \n                          learning_rate = 0.1, \n                          max_depth = 10, \n                          alpha = 1, \n                          n_estimators = 250)\n\nxg_reg = xg_reg.fit(X_train,y_train)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"y_pred = xg_reg.predict(X_test)\nvalidate(y_test, y_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"conclusion\"></a>\n# Conclusion"},{"metadata":{},"cell_type":"markdown","source":"In this solution the following steps were taken\n\n\n1. Data understanding and preparing\n    - Removed outliers and erroneous values\n    - Parsed text values\n    - Filled missing values\n    - Features are transformed\n2. Performing regression analysis\n    - Evaluated the fulfillment of the linear regression assumptions\n    - Removed outliers based on Cook's distance\n3. Fitted a linear regression model. Optimal parameters are configured via HyperOpt.\n4. Fitted a comparative model based on XGBoost.\n\nThe linear regression model showed 2 problems - the residuals are not normally distributed, and heteroscedasticity is also observed.\nI will assume that the reasons lie in the absence of an important predictor. When using such a model, it should be borne in mind that it may not be optimal for the given task/dataset.\n\nIn addition, I note that the accuracy of linear regression almost coincided with the accuracy of the model based on XGBoost. It seems that it is difficult to achieve a better result on the current data.\n\nThanks for attention!"},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}