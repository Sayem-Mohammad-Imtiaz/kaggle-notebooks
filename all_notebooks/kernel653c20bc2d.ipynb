{"cells":[{"metadata":{},"cell_type":"markdown","source":"### Business Problem Overview\n\nIn the telecom industry, customers are able to choose from multiple service providers and actively switch from one operator to another. In this highly competitive market, the telecommunications industry experiences an average of 15-25% annual churn rate. Given the fact that it costs 5-10 times more to acquire a new customer than to retain an existing one, customer retention has now become even more important than customer acquisition.\n\nFor many incumbent operators, retaining high profitable customers is the number one business goal.\nTo reduce customer churn, telecom companies need to predict which customers are at high risk of churn.\n\n### Business Ojective\n\nThe business objective is to predict the churn in the last (i.e. the ninth) month using the data (features) from the first three months. To do this task well, understanding the typical customer behaviour during churn will be helpful.\n\nThus, churn prediction is usually more critical (and non-trivial) for prepaid customers, and the term ‘churn’ should be defined carefully. Prepaid is also the most common model in India and southeast Asia, while postpaid is more common in Europe in North America.\n\n \n\nThis project is based on the Indian and Southeast Asian market.\n\n### Solution Overview\n\nThe project starts with understanding the data set,  dropping unnecessary features, deriving features and performing exploratory data analysis. Where possible and required IterativeImputer from SKLearn is used to fill in gaps of numerical data by regression\nThe project is limited to High Value customers and there is a set definition to drive High Value Customers based on the data set. This definition is applied. The resultant data set is around 30K entries. \n\nFor the south asian market it is enough if a good enough Churn prediction is performed on the high value customers\nThe below algorithms have been applied on the data set and various models created and compared.\n\n**Principal Component Analysis  (PCA) with Logistic Regression** \n\n**Logistic Regression with Recursive Feature Elimination**\n\n**Random Forest algorithm based model with Hyper Parameter tuning**\n\n**Gradient Boosting - XGBoost algorithm**\n\nAfter this the results are compared based on the Confusion Matrix and Accuracy and a good model chosen.\nThen based on the model chosen the main features that affect Churn are identified so that some business recommendations can be made\n\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import required libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline    \npd.options.display.float_format = '{:.2f}'.format\nfrom sklearn.preprocessing import LabelEncoder\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Preparation & Understanding "},{"metadata":{"trusted":true},"cell_type":"code","source":"#Load the data into a dataframe\ntelecom = pd.read_csv(\"../input/telecom-churn-data-set-for-the-south-asian-market/telecom_churn_data.csv\", low_memory=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"telecom.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"telecom.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"telecom.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Check unique values"},{"metadata":{"trusted":true},"cell_type":"code","source":"#mobile_number is unique\nprint(telecom.mobile_number.is_unique)\ntelecom.mobile_number.nunique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Columns with 70% missing data"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# Columns with more than 70% missing values\ncolmns_missing_data = round(100*(telecom.isnull().sum()/len(telecom.index)), 2)\ncolmns_missing_data[colmns_missing_data >= 40]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"telecom.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Filter High Value Customers\n#### Define high-value customers as follows: \n- Those who have recharged with an amount more than or equal to X, where X is the 70th percentile of the average recharge amount in the first two months (the good phase)."},{"metadata":{},"cell_type":"markdown","source":"- There are lot of missing values for the Data and the Amt_Data Columns indicating that no recharge was done on that month\n- The NaN values should be replaced by 0"},{"metadata":{"trusted":true},"cell_type":"code","source":"telecom.total_rech_data_6.fillna(value=0, inplace=True)\ntelecom.total_rech_data_7.fillna(value=0, inplace=True)\ntelecom.total_rech_data_8.fillna(value=0, inplace=True)\ntelecom.total_rech_data_9.fillna(value=0, inplace=True)#\ntelecom.av_rech_amt_data_6.fillna(value=0, inplace=True)\ntelecom.av_rech_amt_data_7.fillna(value=0, inplace=True)\ntelecom.av_rech_amt_data_8.fillna(value=0, inplace=True)\ntelecom.av_rech_amt_data_9.fillna(value=0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"#Total recharge amounts for months 6 and 7\n#Total recharge amount logic = Total data recharge + Total recharge Amount. \n#if any of the data recharge columns are 0 then retain the total recharge amt column as is\n\ntelecom['total_rech_amt_6'] = np.where((telecom['total_rech_data_6'] != 0) & (telecom['av_rech_amt_data_6'] != 0),\n                                            telecom['total_rech_data_6']*telecom['av_rech_amt_data_6']+telecom['total_rech_amt_6'],\n                                            telecom['total_rech_amt_6'])\n\ntelecom['total_rech_amt_7'] = np.where((telecom['total_rech_data_7'] != 0) & (telecom['av_rech_amt_data_7'] != 0),\n                                            telecom['total_rech_data_7']*telecom['av_rech_amt_data_7']+telecom['total_rech_amt_7'],\n                                            telecom['total_rech_amt_7'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Filter high-value customers\ntelecom['av_rech_amt'] = (telecom[\"total_rech_amt_6\"] + \n                          telecom[\"total_rech_amt_7\"]) / 2.0\ncutoff = telecom.av_rech_amt.quantile(.70)\nprint('70 percentile of first two months avg recharge amount: ', cutoff)\ntelecom_hv = telecom[telecom['av_rech_amt'] >= cutoff]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"telecom_hv.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We can drop total_rech_data_* and av_rech_amt_data_*\ndrop_data_columns = [\"total_rech_data_6\", \"total_rech_data_7\", \"total_rech_data_8\", \"total_rech_data_9\", \n                'av_rech_amt_data_6', 'av_rech_amt_data_7', 'av_rech_amt_data_8', 'av_rech_amt_data_9']\ntelecom_hv.drop(drop_data_columns, axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### The above columns can be dropped as they have not meaningful either for high value customers or for Churn Labelling "},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.set_option('display.max_rows', telecom_hv.shape[0]+1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### From a total of 99999 records 30001 Records satisfy the High Value Customers\n### The churn prediction will be executed on the high value customers"},{"metadata":{},"cell_type":"markdown","source":"###  Derive Churn Label using the 'Churn Phase' data \n#### Those who have not made any calls (either incoming or outgoing) AND have not used mobile internet even once in the churn phase\n- 1 Churn\n- 0 No Churn"},{"metadata":{"trusted":true},"cell_type":"code","source":"def conditions(s):\n    if ((s['total_ic_mou_9'] <= 0) & (s['total_og_mou_9'] <= 0) & (s['vol_2g_mb_9'] <= 0) & (s['vol_3g_mb_9'] <= 0)):\n        return 1\n    else:\n        return 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"telecom_hv['Churn'] = telecom_hv.apply(conditions, axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Drop the Churn Phase Data set after Label Derivation as per Problem Instruction"},{"metadata":{"trusted":true},"cell_type":"code","source":"telecom_hv = telecom_hv.loc[:,~telecom_hv.columns.str.endswith('_9')]\ntelecom_hv = telecom_hv.loc[:,~telecom_hv.columns.str.startswith('sep')]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"telecom_hv.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n#### Understand the Churn Rate & Imbalance"},{"metadata":{"trusted":true},"cell_type":"code","source":"churn_rate = (sum(telecom_hv['Churn'])/len(telecom_hv['Churn'].index))*100\nchurn_rate","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"imbalance = (sum(telecom_hv['Churn'] != 0)/sum(telecom_hv['Churn'] == 0))*100\nimbalance","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Churn Rate: We have 8.14% Churn rate\n- Imbalance ratio of 8.66 %"},{"metadata":{},"cell_type":"markdown","source":"## Data Prep, Exploratory Data Analysis & Feature Generation"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"#Study the dataset\ntelecom_hv.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Few columns have a unique value in all the rows\n#### They cannot be uesd to predict any variance between the data set\n#### it makes intuitive sense to drop these columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"nunique = telecom_hv.apply(pd.Series.nunique)\ncols_to_drop = nunique[nunique == 1].index\ncols_to_drop","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"telecom_hv.drop(cols_to_drop,axis=1,inplace=True)\ntelecom_hv.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Rows with more than 55% of data missing"},{"metadata":{"trusted":true},"cell_type":"code","source":"# sum it up to check how many rows have all missing values\nprint(\"All null values:\", telecom_hv.isnull().all(axis=1).sum())\n# drop rows with 55% of missing data\ntelecom_hv = telecom_hv[(telecom_hv.isnull().sum(axis=1)/telecom_hv.shape[1])*100 < 55]\nprint(\"Record Count after Row/Column Data deletion:\", telecom_hv.shape[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Date Format Alignment"},{"metadata":{},"cell_type":"markdown","source":"### Box Plots, Bar Plots, Scatter plots and Correlation Matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create Bar Plot\nplt.figure(figsize=(20, 12))\nplt.subplot(2,3,1)\nsns.barplot(x = 'Churn', y = 'arpu_6', data = telecom_hv)\nplt.subplot(2,3,2)\nplt.ylabel('Av Rev. Month 7')\nsns.barplot(x = 'Churn', y = 'arpu_7', data = telecom_hv)\nplt.subplot(2,3,3)\nplt.ylabel('Av Rev. Month 8')\nsns.barplot(x = 'Churn', y = 'arpu_8', data = telecom_hv)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### The Average Revenue per user metric Drop in month 8 indicates Churn"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create Bar Plot\nplt.figure(figsize=(20, 12))\nplt.subplot(2,3,1)\nsns.barplot(x = 'Churn', y = 'total_og_mou_6', data = telecom_hv)\nplt.subplot(2,3,2)\nsns.barplot(x = 'Churn', y = 'total_og_mou_7', data = telecom_hv)\nplt.subplot(2,3,3)\nsns.barplot(x = 'Churn', y = 'total_og_mou_8', data = telecom_hv)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### The Outgoing minutes Drop in month 8 indicates Churn"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create Bar Plot\nplt.figure(figsize=(20, 12))\nplt.subplot(2,3,1)\nsns.barplot(x = 'Churn', y = 'total_ic_mou_6', data = telecom_hv)\nplt.subplot(2,3,2)\nsns.barplot(x = 'Churn', y = 'total_ic_mou_7', data = telecom_hv)\nplt.subplot(2,3,3)\nsns.barplot(x = 'Churn', y = 'total_ic_mou_8', data = telecom_hv)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### The Incoming minutes usage Drop in month 8 indicates Churn"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create Bar Plot\nplt.figure(figsize=(20, 12))\nplt.subplot(2,3,1)\nsns.barplot(x = 'Churn', y = 'onnet_mou_6', data = telecom_hv)\nplt.subplot(2,3,2)\nsns.barplot(x = 'Churn', y = 'onnet_mou_7', data = telecom_hv)\nplt.subplot(2,3,3)\nsns.barplot(x = 'Churn', y = 'onnet_mou_8', data = telecom_hv)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### The Same Operator/network Calls Drop in month 8 indicates Churn"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create Bar Plot\nplt.figure(figsize=(20, 12))\nplt.subplot(2,3,1)\nsns.barplot(x = 'Churn', y = 'offnet_mou_6', data = telecom_hv)\nplt.subplot(2,3,2)\nsns.barplot(x = 'Churn', y = 'offnet_mou_7', data = telecom_hv)\nplt.subplot(2,3,3)\nsns.barplot(x = 'Churn', y = 'offnet_mou_8', data = telecom_hv)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### The Different operator/network Calls Drop in month 8 indicates Churn\n#### It would benefit if we Merge the months 6 and 7 into an average number indicating Good Phase"},{"metadata":{"trusted":true},"cell_type":"code","source":"telecom_hv.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Extract all other columns separately for Plotting the correlation and observing highly correlated variables\n#### Different metrics include Totals, Amounts, Minutes of Usage, OFFNET & ONNNET, 2g and 3g Data sets"},{"metadata":{"trusted":true},"cell_type":"code","source":"rech_data = telecom_hv.loc[:,telecom_hv.columns.str.contains('rech')]\ntot_data = telecom_hv.loc[:,telecom_hv.columns.str.contains('tot')]\namt_data = telecom_hv.loc[:,telecom_hv.columns.str.contains('amt')]\nic_mou_data = telecom_hv.loc[:,(telecom_hv.columns.str.contains('ic') & telecom_hv.columns.str.contains('mou'))]\nog_mou_data = telecom_hv.loc[:,(telecom_hv.columns.str.contains('og') & telecom_hv.columns.str.contains('mou'))]\nnet_mou_data = telecom_hv.loc[:,telecom_hv.columns.str.contains('net_mou')]\ndata3g = telecom_hv.loc[:,(telecom_hv.columns.str.contains('3g'))]\ndata2g = telecom_hv.loc[:,(telecom_hv.columns.str.contains('2g'))]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rech_data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plotting the correlation matrix\n%matplotlib inline\nplt.figure(figsize = (25,25))\nsns.heatmap(rech_data.corr(),annot = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Observations\n- High correlation between Average Recharge Amount and Rechage amounts for 6 and 7\n- This is expected as the recharge amount is calculated for purpose of filtering high value customers\n- There is high correlation 80% between data recharge for month 7 and recharge for month 8. \n- Any factor that has correlation with month 8 is probably correlated to the churn prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"tot_data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plotting the correlation matrix\n%matplotlib inline\nplt.figure(figsize = (12,12))\nsns.heatmap(tot_data.corr(),annot = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Observations\n- There is greater than 70% and some cases 82% correlation between months 7 and 8 regarding Incoming & Outgoing minutes of usage\n- This is probabaly due to the fact that if there is heavy usage in month 7 then subsequently in month 8 there is also heavy usage - The cusotmer will not churn if there is heavy usage and vice versa"},{"metadata":{"trusted":true},"cell_type":"code","source":"amt_data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plotting the correlation matrix\n%matplotlib inline\nplt.figure(figsize = (10,10))\nsns.heatmap(amt_data.corr(),annot = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Observations\n- Some of this correlation is the same as the First Recharge Amount correlation\n- There is also higher correlation between the Max Recharge Amount in month 8 (Bad Phase) and the Last Day Recharge Amount\n- This could indicate that if a customer is not going to Churn then they Recharge for a higher amount in month 8 "},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create scatter plot to understand distribution of amounts\nplt.figure(figsize=(25, 10))\nplt.subplot(2,3,1)\nsns.scatterplot(x = 'total_rech_amt_6', y = 'total_rech_amt_8', data = telecom_hv, hue = 'Churn')\nplt.subplot(2,3,2)\nsns.scatterplot(x = 'total_rech_amt_7', y = 'total_rech_amt_8', data = telecom_hv, hue = 'Churn')\nplt.subplot(2,3,3)\nsns.scatterplot(x = 'av_rech_amt', y = 'total_rech_amt_8', data = telecom_hv, hue = 'Churn')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ic_mou_data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plotting the correlation matrix\n%matplotlib inline\nplt.figure(figsize = (36,36))\nsns.heatmap(ic_mou_data.corr(),annot = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Observations\n\n- Total Incoming minutes of usage is almost entirely explained by the LOCAL call usage and not a lot by the STD calls\n- Total Incoming minutes of usage of month 8 is also correlated to the month 7. Indicating that if a customer has High MOU in       month 7 then they will continue to have High MOU in month 8\n- The STD Incoming MOU is fully explained by the T2M Minutes of Usage\n- High Correlation between Incoming T2T Usage for Months 6 and 7 and Months 7 and 8"},{"metadata":{"trusted":true},"cell_type":"code","source":"og_mou_data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plotting the correlation matrix\n%matplotlib inline\nplt.figure(figsize = (40,40))\nsns.heatmap(og_mou_data.corr(),annot = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Observations\n\n- Total outgoing minutes of usage is almost entirely explained by the Std calls usage and not a lot by the Local calls\n- Total Outgoing minutes of usage of month 8 is also correlated to the month 7. Indicating that if a customer has High MOU in       month 7 then they will continue to have High MOU in month 8\n- The STD Outgoing MOU is highly correlated to the T2T Minutes of Usage\n- High Correlation between OutGoing T2T Usage for Months 6 and 7 and Months 7 and 8"},{"metadata":{"trusted":true},"cell_type":"code","source":"net_mou_data.shape\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plotting the correlation matrix\n%matplotlib inline\nplt.figure(figsize = (6,6))\nsns.heatmap(net_mou_data.corr(),annot = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Observations\n\n- No Correlation between ONNET and OFFNET Minutes of usage\n- High correlation between months 7 and 8 both for ONNET and OFFNET usage"},{"metadata":{"trusted":true},"cell_type":"code","source":"data3g.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plotting the correlation matrix\n%matplotlib inline\nplt.figure(figsize = (18,18))\nsns.heatmap(data3g.corr(),annot = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Observations\n\n- 70% correlation between Average revenue per user and the 3G Volume of data usage for all Months"},{"metadata":{"trusted":true},"cell_type":"code","source":"data2g.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plotting the correlation matrix\n%matplotlib inline\nplt.figure(figsize = (15,15))\nsns.heatmap(data2g.corr(),annot = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Observations\n\n- Very High correlation between the Recharge Sachets and the Count of Recharges for all months"},{"metadata":{},"cell_type":"markdown","source":"#### Consider dropping columns where most of the values are 0 i.e. Greater than 75% "},{"metadata":{"trusted":true},"cell_type":"code","source":"check_cols = (telecom_hv[telecom_hv == 0].count(axis=0)/len(telecom_hv.index)*100)\ncheck_cols = check_cols[check_cols > 75].index\ncheck_cols","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"check_cols = check_cols[check_cols != 'Churn']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Numeric Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"telecom_n = telecom_hv.select_dtypes(include=np.number)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"telecom_n.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"telecom_n.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### If there are any NAN values then fill them with 0"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Columns with more than 70% missing values\ncolmns_missing_data = round(100*(telecom_n.isnull().sum()/len(telecom_n.index)), 2)\ncols = colmns_missing_data[colmns_missing_data>1]","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"cols","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"telecom_cat = pd.DataFrame(telecom_n,columns = ['mobile_number','night_pck_user_6','night_pck_user_7','night_pck_user_8','fb_user_6','fb_user_7','fb_user_8'])\ntelecom_n.drop(['night_pck_user_6','night_pck_user_7','night_pck_user_8','fb_user_6','fb_user_7','fb_user_8'],axis=1,inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"telecom_cat.shape\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\nimp = IterativeImputer(max_iter=10, verbose=0)\nimp.fit(telecom_n)\nimputed_df = imp.transform(telecom_n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Columns with more than 70% missing values\nnew_df = pd.DataFrame(imputed_df)\nnew_df.columns = telecom_n.columns\nnew_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"telecom_n = pd.merge(new_df, telecom_cat, on='mobile_number', how='inner')\ntelecom_n.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Non Numeric Variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"telecom_nn = telecom_hv.select_dtypes(exclude=telecom_n.dtypes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"telecom_nn.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Since we are not predicting any churn based on the above Date variables - is it safe to drop them? "},{"metadata":{},"cell_type":"markdown","source":"### Derived Features based on the Exploratory Data Analysis performed in the previous step\n- 1. Average ARPU during months 6 and 7\n- 2. Average OG minutes of usage during months 6 and 7\n- 3. Average IC Minutes of usage during months 6 and 7\n- 4. Average OFFNET and ONNET minutes of usage for months 6 and 7"},{"metadata":{"trusted":true},"cell_type":"code","source":"telecom_n['gp_avg_arpu'] = (telecom_n['arpu_6'] + telecom_n['arpu_7'])/2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- If a Customer is only joining in the Bad Phase assign the same valus to the Good Phase"},{"metadata":{"trusted":true},"cell_type":"code","source":"telecom_n['gp_avg_arpu'] = np.where((telecom_n['arpu_8'] > 0) & (telecom_n['gp_avg_arpu'] == 0),telecom_n['arpu_8'],telecom_n['gp_avg_arpu'])                              ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Drop the individual month ARPU data as it's redundant"},{"metadata":{"trusted":true},"cell_type":"code","source":"telecom_n.drop(['arpu_6','arpu_7'],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"telecom_n['total_og_mou_gp'] = (telecom_n['total_og_mou_6'] + telecom_n['total_og_mou_7'])/2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"telecom_n['total_og_mou_gp'] = np.where((telecom_n['total_og_mou_8'] > 0) & (telecom_n['total_og_mou_gp'] == 0),telecom_n['total_ic_mou_8'],telecom_n['total_og_mou_gp'])                              ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Drop the individual month Outgoing usage data as it's redundant"},{"metadata":{"trusted":true},"cell_type":"code","source":"telecom_n.drop(['total_og_mou_6','total_og_mou_7'],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"telecom_n['total_ic_mou_gp'] = (telecom_n['total_ic_mou_6'] + telecom_n['total_ic_mou_7'])/2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- If a Customer is only joining in the Bad Phase assign the same valus to the Good Phase"},{"metadata":{"trusted":true},"cell_type":"code","source":"telecom_n['total_ic_mou_gp'] = np.where((telecom_n['total_ic_mou_8'] > 0) & (telecom_n['total_ic_mou_gp'] == 0),telecom_n['total_ic_mou_8'],telecom_n['total_ic_mou_gp'])                              ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Drop the individual incoming usage month data as it's redundant"},{"metadata":{"trusted":true},"cell_type":"code","source":"telecom_n.drop(['total_ic_mou_6','total_ic_mou_7'],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"telecom_n['onnet_mou_gp'] = (telecom_n['onnet_mou_6'] + telecom_n['onnet_mou_7'])/2\ntelecom_n['offnet_mou_gp'] = (telecom_n['offnet_mou_6'] + telecom_n['offnet_mou_7'])/2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Drop the individual ONNET and OFFNET usage month data as it's redundant"},{"metadata":{"trusted":true},"cell_type":"code","source":"telecom_n.drop(['onnet_mou_6','onnet_mou_7','offnet_mou_6','offnet_mou_7'],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"telecom_n.fillna(0,inplace=True)\ntelecom_n.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Feature Generation - Introduce new Feature called Retain Factor\n- Retain Factor is calculated as the ratio of the Bad phase Average Revenue / Good Phase Average revenue\n- And the Ratio of the number of recharges in month 8 Vs Month 7"},{"metadata":{"trusted":true},"cell_type":"code","source":"#telecom_n.dtypes\ntelecom_n['retain_factor_arpu'] = round(telecom_n['arpu_8'] / telecom_n['gp_avg_arpu'],2)\ntelecom_n['retain_factor_rech'] = round(telecom_n['total_rech_num_8'] / telecom_n['total_rech_num_7'],2)\ntelecom_n['retain_factor_rech'] = np.where(telecom_n['retain_factor_rech'] > 1,1,telecom_n['retain_factor_rech'])\ntelecom_n['retain_factor_arpu'] = np.where(telecom_n['retain_factor_arpu'] > 1,1,telecom_n['retain_factor_arpu'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Deduce a factor for retaining the customer\ntelecom_n['retain_factor'] = np.where((telecom_n['retain_factor_arpu'] > 0.5) & (telecom_n['retain_factor_rech'] > 0.5),0,1)\ntelecom_n.drop(columns = ['retain_factor_rech','retain_factor_arpu'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Perform PCA and Predict Churn"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n# Assign feature variable to X\nX = telecom_n.drop(['Churn','mobile_number'],axis=1)\n# Assign response variable to y\ny = telecom_n['Churn']\ny.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature Standardisation"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"from sklearn import preprocessing\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = preprocessing.StandardScaler().fit(X)\nXScale = scaler.transform(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Splitting the data into train and test\nX_train, X_test, y_train, y_test = train_test_split(XScale,y, train_size=0.7,test_size=0.3,random_state=100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Understand the data imbalance"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_imb = (y_train != 0).sum()/(y_train == 0).sum()\ny_test_imb = (y_test != 0).sum()/(y_test == 0).sum()\nprint(\"Imbalance in Train Data:\", y_train_imb)\nprint(\"Imbalance in Test Data:\", y_test_imb)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"count_class = pd.value_counts(telecom_n['Churn'], sort=True)\ncount_class.plot(kind='bar',rot = 0)\nplt.title('Churn Distribution')\nplt.xlabel('Churn')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Handle data imbalance by Performing SMOTE oversampling on the data set\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"### Other Sampling Techniques just for playing around\n#from imblearn.combine import SMOTETomek\n#from imblearn.under_sampling import NearMiss\n#smk = SMOTETomek(random_state = 42)\n#X_trainb,y_trainb = smk.fit_sample(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Other Sampling Techniques just for playing around\n#from imblearn.over_sampling import RandomOverSampler\n#os = RandomOverSampler(sampling_strategy=1)\n#X_trainb,y_trainb = os.fit_sample(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from imblearn.over_sampling import SMOTE","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"smt = SMOTE(random_state = 2) \nX_trainb,y_trainb = smt.fit_sample(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_trainb.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_trainb.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Improting the PCA module\nfrom sklearn.decomposition import PCA\npca = PCA(svd_solver='randomized', random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Doing the PCA on the train data\npca.fit(X_trainb)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pca.components_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"colnames = list(X.columns)\npcs_df = pd.DataFrame({'PC1':pca.components_[0],'PC2':pca.components_[1], 'PC3':pca.components_[2],'Feature':colnames})\npcs_df.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline\nfig = plt.figure(figsize = (20,20))\nplt.scatter(pcs_df.PC1, pcs_df.PC2)\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')\nfor i, txt in enumerate(pcs_df.Feature):\n    plt.annotate(txt, (pcs_df.PC1[i],pcs_df.PC2[i]))\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Making the screeplot - plotting the cumulative variance against the number of components\n%matplotlib inline\nfig = plt.figure(figsize = (12,9))\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel('number of components')\nplt.ylabel('cumulative explained variance')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Looks like approx. 50 components are enough to describe 90% of the variance in the dataset\n- We'll choose 50 components for our modeling"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Using incremental PCA for efficiency - saves a lot of time on larger datasets\nfrom sklearn.decomposition import IncrementalPCA\npca_final = IncrementalPCA(n_components=50)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Basis transformation - getting the data onto our PCs\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_pca = pca_final.fit_transform(X_trainb)\ndf_train_pca.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Creating correlation matrix for the principal components - we expect little to no correlation"},{"metadata":{"trusted":true},"cell_type":"code","source":"#creating correlation matrix for the principal components\ncorrmat = np.corrcoef(df_train_pca.transpose())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plotting the correlation matrix\n%matplotlib inline\nplt.figure(figsize = (50,30))\nsns.heatmap(corrmat,annot = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 1s -> 0s in diagonals\ncorrmat_nodiag = corrmat - np.diagflat(corrmat.diagonal())\nprint(\"max corr:\",corrmat_nodiag.max(), \", min corr: \", corrmat_nodiag.min(),)\n# we see that correlations are indeed very close to 0","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### There is no correlation between any two components! \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Applying selected components to the test data - 45 components\ntelecom_test_pca = pca_final.transform(X_test)\ntelecom_test_pca.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Multiple Logistic Regression Models with the Principal Components\n#### Model 1 - Use the No. of Principal Components determined by PCA"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Training the model on the train data\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\n\nlearner_pca = LogisticRegression()\nmodel_pca = learner_pca.fit(df_train_pca,y_trainb)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Making prediction on the test data\npred_probs_test = model_pca.predict_proba(telecom_test_pca)[:,1]\n\"{:2.2}\".format(metrics.roc_auc_score(y_test, pred_probs_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predict Results from PCA Model\nypred_pca = model_pca.predict(telecom_test_pca)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Confusion matrix \nconfusion_PCA = metrics.confusion_matrix(y_test, ypred_pca)\nprint(confusion_PCA)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report\nprint(classification_report(y_test, ypred_pca))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Model 2 - Let PCA Choose the number of Principal Components explaining 90% of Variance"},{"metadata":{"trusted":true},"cell_type":"code","source":"pca_again = PCA(0.90)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_pca2 = pca_again.fit_transform(X_trainb)\ndf_train_pca2.shape\n# we see that PCA selected 38 components","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#training the regression model\nlearner_pca2 = LogisticRegression()\nmodel_pca2 = learner_pca2.fit(df_train_pca2,y_trainb)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test_pca2 = pca_again.transform(X_test)\ndf_test_pca2.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Making prediction on the test data\npred_probs_test2 = model_pca2.predict_proba(df_test_pca2)[:,1]\n\"{:2.2f}\".format(metrics.roc_auc_score(y_test, pred_probs_test2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predict Results from PCA Model\nypred_pca2 = model_pca2.predict(df_test_pca2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Confusion matrix \nconfusion_PCA = metrics.confusion_matrix(y_test, ypred_pca2)\nprint(confusion_PCA)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report\nprint(classification_report(y_test, ypred_pca2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Model 3 - Let PCA Choose the number of Principal Components explaining 95% of Variance"},{"metadata":{"trusted":true},"cell_type":"code","source":"pca_again = PCA(0.95)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_pca3 = pca_again.fit_transform(X_trainb)\ndf_train_pca3.shape\n# we see that PCA selected 51 components","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#training the regression model\nlearner_pca3 = LogisticRegression()\nmodel_pca3 = learner_pca3.fit(df_train_pca3,y_trainb)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test_pca3 = pca_again.transform(X_test)\ndf_test_pca3.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Making prediction on the test data\npred_probs_test3 = model_pca3.predict_proba(df_test_pca3)[:,1]\n\"{:2.2f}\".format(metrics.roc_auc_score(y_test, pred_probs_test3))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predict Results from PCA Model\nypred_pca3 = model_pca3.predict(df_test_pca3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Confusion matrix \nconfusion_PCA = metrics.confusion_matrix(y_test, ypred_pca3)\nprint(confusion_PCA)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report\nprint(classification_report(y_test, ypred_pca3))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## PCA Model with Logistic Regression Conclusions\n- PCA way of selecting variables is simple and easy\n- We choose Model1 using 50 components as the best fit model due to the Confusion Matrix and the Accuracy Scores\n- The False Positives are still quite High\n\n- **Confusion Matrix for Model 1**\n\n   6818     1445\n\n   137      571\n   \n   \n- **Model Accuracy**\n\n    88%\n \n- **Classification Report for Model 1**\n\n               precision    recall  f1-score   support\n\n           0       0.98      0.83      0.90      8263\n           1       0.28      0.81      0.42       708\n\n   \n "},{"metadata":{},"cell_type":"markdown","source":"#### Visualize the data to see if we can spot any patterns"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Function to map the colors as a list from the input list of x variables\ndef pltcolor(lst):\n    cols=[]\n    for l in lst:\n        if l==0:\n            cols.append('red')\n        elif l==1:\n            cols.append('blue')\n        else:\n            cols.append('green')\n    return cols\n# Create the colors list using the function above\ncols=pltcolor(y_trainb)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n%matplotlib inline\nfig = plt.figure(figsize = (12,10))\nplt.scatter(df_train_pca[:,0], df_train_pca[:,1], s=200,c = cols)\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')\nplt.gray()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### PCA with 10 Principal Components just to illustrate the difference\n- Model 4"},{"metadata":{"trusted":true},"cell_type":"code","source":"pca_last = PCA(n_components=10)\ndf_train_pca4 = pca_last.fit_transform(X_trainb)\ndf_test_pca4 = pca_last.transform(X_test)\ndf_test_pca4.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#training the regression model\nlearner_pca4 = LogisticRegression()\nmodel_pca4 = learner_pca4.fit(df_train_pca4,y_trainb)\n#Making prediction on the test data\npred_probs_test4 = model_pca4.predict_proba(df_test_pca4)[:,1]\n\"{:2.2f}\".format(metrics.roc_auc_score(y_test, pred_probs_test4))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### We get good results with the Chosen Principal components and almost close to it with just 10 Principal Components."},{"metadata":{},"cell_type":"markdown","source":"### Logistic Regression Algorithm with RFE"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a copy\ntelecom_LR_wPCA = telecom_n.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"telecom_LR_wPCA.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"telecom_LR_wPCA['Churn'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,4))\ntelecom_LR_wPCA['Churn'].value_counts().plot(kind = 'bar')\nplt.ylabel('Count')\nplt.xlabel('Churn status')\nplt.title('Churn status Distribution',fontsize=14)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create correlation matrix and check correlation greater than 0.95 adn drop those columns\ncorr_matrix = telecom_LR_wPCA.corr().abs()\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\nto_drop = [column for column in upper.columns if any(upper[column] > 0.95)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(to_drop)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop high correlated features\ntelecom_LR_wPCA.drop(telecom_LR_wPCA[to_drop], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"telecom_LR_wPCA.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Feature Generation - Introduce new Feature called Retain Factor\n- Retain Factor is calculated as the ratio of the Bad phase Average Revenue / Good Phase Average revenue\n- And the Ratio of the number of recharges in month 8 Vs Month 7"},{"metadata":{"trusted":true},"cell_type":"code","source":"#telecom_LR_wPCA.dtypes\ntelecom_LR_wPCA['retain_factor_arpu'] = round(telecom_LR_wPCA['arpu_8'] / telecom_LR_wPCA['gp_avg_arpu'],2)\ntelecom_LR_wPCA['retain_factor_rech'] = round(telecom_LR_wPCA['total_rech_num_8'] / telecom_LR_wPCA['total_rech_num_7'],2)\ntelecom_LR_wPCA['retain_factor_rech'] = np.where(telecom_LR_wPCA['retain_factor_rech'] > 1,1,telecom_LR_wPCA['retain_factor_rech'])\ntelecom_LR_wPCA['retain_factor_arpu'] = np.where(telecom_LR_wPCA['retain_factor_arpu'] > 1,1,telecom_LR_wPCA['retain_factor_arpu'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- If the Ratio between the ARPU for the bad phase and the good phase is > than 0.6\n  and if the Ratio of the Number of recharges is > 0.6 \n- Then the consideration is that if the customer retention ratio is High then the user is likely not to Churn"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Deduce a factor for retaining the customer\ntelecom_LR_wPCA['retain_factor'] = np.where((telecom_LR_wPCA['retain_factor_arpu'] > 0.6) & (telecom_LR_wPCA['retain_factor_rech'] > 0.6),0,1)\ntelecom_LR_wPCA.drop(columns = ['retain_factor_rech','retain_factor_arpu'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"telecom_LR_wPCA.retain_factor.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Assign feature variable to X\nX = telecom_LR_wPCA.drop(['Churn','mobile_number'],axis=1)\nX.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Assign the response variable to y\ny_LR = telecom_LR_wPCA[['Churn']]\ny_LR.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Splitting the data into train and test\nX_train_LR, X_test_LR, y_train_LR, y_test_LR = train_test_split(X, y_LR, train_size=0.7, test_size=0.3, random_state=100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_LR.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Balance data set by oversampling"},{"metadata":{"trusted":true},"cell_type":"code","source":"smt = SMOTE(random_state = 2) \nX_train_LR,y_train_LR = smt.fit_sample(X_train_LR,y_train_LR)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_LR.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_imbalance = (y_train_LR != 0).sum()/(y_train_LR == 0).sum()\nprint(\"Imbalance in Train Data: {}\".format(data_imbalance))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#X_train_LR.head()\ncolumns = X.columns\nX_train_LR = pd.DataFrame(X_train_LR)\nX_train_LR.columns = columns\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ycolumns = y_LR.columns\ny_train_LR = pd.DataFrame(y_train_LR)\ny_train_LR.columns = ycolumns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_LR.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import preprocessing\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_train_LR[columns] = scaler.fit_transform(X_train_LR[columns])\nX_train_LR.retain_factor.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_LR.retain_factor.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import statsmodels.api as sm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Feature Selection Using RFE"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import RFE\nrfe = RFE(logreg, 45)             # running RFE with 38 variables as output\nrfe = rfe.fit(X_train_LR, y_train_LR)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rfe.support_","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"list(zip(X_train_LR.columns, rfe.support_, rfe.ranking_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"col = X_train_LR.columns[rfe.support_]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Assessing the model with StatsModels"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"X_train_sm = sm.add_constant(X_train_LR[col])\nlogm2 = sm.GLM(y_train_LR,X_train_sm, family = sm.families.Binomial())\nres = logm2.fit()\nres.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Getting the predicted values on the train set\ny_train_pred = res.predict(X_train_sm)\ny_train_pred[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_pred = y_train_pred.values.reshape(-1)\ny_train_pred[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"###Creating a dataframe with the actual churn flag and the predicted probabilities\ny_train_pred_final = pd.DataFrame({'Churn':y_train_LR.Churn, 'Churn_Prob':y_train_pred})\ny_train_pred_final['MobileNumber'] = y_train_LR.index\ny_train_pred_final.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Creating new column 'predicted' with 1 if Churn_Prob > 0.5 else 0"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_pred_final['predicted'] = y_train_pred_final.Churn_Prob.map(lambda x: 1 if x > 0.5 else 0)\n\n# Let's see the head\ny_train_pred_final.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Confusion matrix \nconfusion = metrics.confusion_matrix(y_train_pred_final.Churn, y_train_pred_final.predicted )\nprint(confusion)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's check the overall accuracy.\nprint(metrics.accuracy_score(y_train_pred_final.Churn, y_train_pred_final.predicted))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Training accuracy using RFE is Approx. 0.853"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check for the VIF values of the feature variables. \nfrom statsmodels.stats.outliers_influence import variance_inflation_factor","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train_LR[col].columns\nvif['VIF'] = [variance_inflation_factor(X_train_LR[col].values, i) for i in range(X_train_LR[col].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#### There are a some variables with very high VIF. It's best to drop these variables as they aren't helping much with prediction and unnecessarily making the model complex.\n#### Lets drop all variables that have very high VIF i.e. above 9\ncol = vif[vif['VIF'] < 9]\ncol = col.Features","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# Let's re-run the model using the selected variables\nX_train_sm = sm.add_constant(X_train_LR[col])\nlogm3 = sm.GLM(y_train_LR,X_train_sm, family = sm.families.Binomial())\nres = logm3.fit()\nres.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_pred = res.predict(X_train_sm).values.reshape(-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_pred[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_pred_final['Churn_Prob'] = y_train_pred","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# Creating new column 'predicted' with 1 if Churn_Prob > 0.5 else 0\ny_train_pred_final['predicted'] = y_train_pred_final.Churn_Prob.map(lambda x: 1 if x > 0.5 else 0)\ny_train_pred_final.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's check the overall accuracy.\nprint(metrics.accuracy_score(y_train_pred_final.Churn, y_train_pred_final.predicted))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"##### Let's check the VIFs again"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"vif = pd.DataFrame()\nvif['Features'] = X_train_LR[col].columns\nvif['VIF'] = [variance_inflation_factor(X_train_LR[col].values, i) for i in range(X_train_LR[col].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# Let's re-run the model using the selected variables\nX_train_sm = sm.add_constant(X_train_LR[col])\nlogm4 = sm.GLM(y_train_LR,X_train_sm, family = sm.families.Binomial())\nres = logm4.fit()\nres.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_pred = res.predict(X_train_sm).values.reshape(-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_pred[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_pred_final['Churn_Prob'] = y_train_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating new column 'predicted' with 1 if Churn_Prob > 0.5 else 0\ny_train_pred_final['predicted'] = y_train_pred_final.Churn_Prob.map(lambda x: 1 if x > 0.5 else 0)\ny_train_pred_final.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's check the overall accuracy.\nprint(metrics.accuracy_score(y_train_pred_final.Churn, y_train_pred_final.predicted))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report\nprint(classification_report(y_train_pred_final.Churn, y_train_pred_final.predicted))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Making predictions on the Test Data set"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test_LR = X_test_LR[col]\nX_test_LR.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test_sm = sm.add_constant(X_test_LR)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test_LR.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Making predictions on the test set\ny_test_pred = res.predict(X_test_sm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test_pred.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test_pred[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Converting y_pred to a dataframe which is an array\ny_pred_1 = pd.DataFrame(y_test_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Converting y_test to dataframe\ny_test_df = pd.DataFrame(y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Assigning CustID to index\ny_test_df['MobileNumber'] = y_test_df.index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Removing index for both dataframes to append them side by side \ny_pred_1.reset_index(drop=True, inplace=True)\ny_test_df.reset_index(drop=True, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Appending y_test_df and y_pred_1\ny_pred_final = pd.concat([y_test_df, y_pred_1],axis=1)\ny_pred_final.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Renaming the column \ny_pred_final= y_pred_final.rename(columns={ 0 : 'Churn_Prob'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Rearranging the columns\n#y_pred_final = y_pred_final.reindex_axis(['CustID','Churn','Churn_Prob'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_final['final_predicted'] = y_pred_final.Churn_Prob.map(lambda x: 1 if x > 0.5 else 0)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"y_pred_final.describe(percentiles=[.25, .5, .75, .90, .95, .99])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's check the overall accuracy.\nmetrics.accuracy_score(y_pred_final.Churn, y_pred_final.final_predicted)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_final.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"confusion2 = metrics.confusion_matrix(y_pred_final.Churn, y_pred_final.final_predicted )\nconfusion2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report\nprint(classification_report(y_pred_final.Churn, y_pred_final.final_predicted))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Conclusions of RFE & Logistic Regression Algorithm\n\n- The Model Accuracy Score that the accuracy on the Test Data Set is High i.e. 0.913\n- The confusion matrix is better than the PCA Algorithm\n- There were other models created different Hyperparameter VIF settings for e.g. VIF<5 but the accuracy and the precision/recall was not very good\n- Hence a Model with a VIF < 9 and and set of 25 variables is chosen here\n- The precision and the recall for Churn Probability is still lower than optimal as there are a few False Positives and False   Negatives\n\n#### The main predictor variables for Telecom Churn are\n\n- total_ic_mou_8\n- onnet_mou_8\n- std_og_t2m_mou_8\n- arpu_2g_8\n- total_rech_num_8\n- loc_ic_t2m_mou_7\n- max_rech_data_8\n- loc_ic_t2m_mou_6\n- total_rech_num_7\n- std_ic_t2t_mou_8\n- count_rech_3g_8\n- retain_factor\n- std_ic_t2t_mou_7\n- gp_avg_arpu\n- loc_ic_t2t_mou_7\n- aug_vbc_3g\n- count_rech_2g_8\n- last_day_rch_amt_8\n- vol_2g_mb_8\n- aon\n"},{"metadata":{},"cell_type":"markdown","source":"## Alternate Model with Random Forest Algorithm"},{"metadata":{"trusted":true},"cell_type":"code","source":"# create a copy first\ntelecom_wPCA_RF = telecom_LR_wPCA.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Assign feature variable to X\nX_RF = telecom_wPCA_RF.drop(['Churn','mobile_number'],axis=1)\nX_RF.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Assign response variable to y\ny_RF = telecom_wPCA_RF['Churn']\ny_RF.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Splitting the data into train and test\nX_train_RF, X_test_RF, y_train_RF, y_test_RF = train_test_split(X_RF, y_RF, train_size=0.7, test_size=0.3, random_state=100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"smt = SMOTE(random_state = 2) \nX_train_RF,y_train_RF = smt.fit_sample(X_train_RF,y_train_RF)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_RF.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_RF = pd.DataFrame(X_train_RF)\nX_train_RF.columns = X_RF.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_RF.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Parameter reduction using L1 LinearVector Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import LinearSVC\nfrom sklearn.feature_selection import SelectFromModel","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"numerics = ['int16','int32','int64','float16','float32','float64']\nnumerical_vars = list(X_train_RF.select_dtypes(include=numerics).columns)\nX_train_RF = X_train_RF[numerical_vars]\nX_train_RF.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Perform L1 Regularisation using LinearSVC Algorithm to do dimensionality reduction\n#### This is a feature available as part of the SciKit Learn Llibrary\n- https://scikit-learn.org/stable/modules/feature_selection.html"},{"metadata":{"trusted":true},"cell_type":"code","source":"Linear_SVC = LinearSVC(C=0.1, penalty=\"l1\", dual=False).fit(X_train_RF, y_train_RF)\nlasso_model = SelectFromModel(Linear_SVC, prefit=False)\nlasso_model.fit(scaler.transform(X_train_RF.fillna(0)), y_train_RF)\nlasso_model.get_support()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.sum(lasso_model.estimator_.coef_ == 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"deleted_vars = X_train_RF.columns[(lasso_model.estimator_.coef_ == 0).ravel().tolist()]\ndeleted_vars","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#perform the same operation in the Test Data set for matching the columns\nX_train_RF.drop(columns = deleted_vars,inplace=True,axis=1)\nX_test_RF.drop(columns = deleted_vars,inplace=True,axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_RF.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Default Hyperparameters - Fit the Random Forest with default hyperparameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing random forest classifier from sklearn library\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Running the random forest with default parameters.\nrfc_d = RandomForestClassifier()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fit\nrfc_d.fit(X_train_RF,y_train_RF)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Making predictions\npredictions = rfc_d.predict(X_test_RF)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing classification report and confusion matrix from sklearn metrics\nfrom sklearn.metrics import classification_report,confusion_matrix, accuracy_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's check the report of our default model\nprint(classification_report(y_test_RF,predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Printing confusion matrix\nprint(confusion_matrix(y_test_RF,predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(accuracy_score(y_test_RF,predictions))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Get List of Important Features from the Decision Tree Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"importances = list(rfc_d.feature_importances_)\n# List of tuples with variable and importance\nfeature_importances = [(feature, round(importance, 2)) for feature, importance in zip(X_train_RF.columns, importances)]\n# Sort the feature importances by most important first\nfeature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n# Print out the feature and importances \n[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances];","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Now lets look at the list of hyperparameters which we can tune to improve model performance."},{"metadata":{},"cell_type":"markdown","source":"#### Tuning max_depth\nLet's try to find the optimum values for max_depth"},{"metadata":{"trusted":true},"cell_type":"code","source":"# GridSearchCV to find optimal min_samples_leaf\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# GridSearchCV to find optimal max_depth\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\n\n# specify number of folds for k-fold CV\nn_folds = 5\n\n# parameters to build the model on\nparameters = {'max_depth': range(4, 10, 2)}\n\n# instantiate the model\nrf = RandomForestClassifier()\n\n\n# fit tree on training data\nrf = GridSearchCV(rf, parameters, \n                    cv=n_folds, \n                   scoring=\"accuracy\", return_train_score=True)\nrf.fit(X_train_RF, y_train_RF)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# scores of GridSearch CV\nscores = rf.cv_results_\npd.DataFrame(scores).head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plotting accuracies with max_depth\nplt.figure()\nplt.plot(scores[\"param_max_depth\"], \n         scores[\"mean_train_score\"], \n         label=\"training accuracy\")\nplt.plot(scores[\"param_max_depth\"], \n         scores[\"mean_test_score\"], \n         label=\"test accuracy\")\nplt.xlabel(\"max_depth\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Tuning n_estimators"},{"metadata":{"trusted":true},"cell_type":"code","source":"# GridSearchCV to find optimal n_estimators\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\n\n# specify number of folds for k-fold CV\nn_folds = 5\n\n# parameters to build the model on\nparameters = {'n_estimators': range(50, 200, 50)}\n\n# instantiate the model (note we are specifying a max_depth)\nrf = RandomForestClassifier(max_depth=6)\n\n# fit tree on training data\nrf = GridSearchCV(rf, parameters, \n                    cv=n_folds, \n                   scoring=\"precision\", return_train_score=True)\nrf.fit(X_train_RF, y_train_RF)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# scores of GridSearch CV\nscores = rf.cv_results_\npd.DataFrame(scores).head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plotting accuracies with n_estimators\nplt.figure()\nplt.plot(scores[\"param_n_estimators\"], \n         scores[\"mean_train_score\"], \n         label=\"training accuracy\")\nplt.plot(scores[\"param_n_estimators\"], \n         scores[\"mean_test_score\"], \n         label=\"test accuracy\")\nplt.xlabel(\"n_estimators\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Tuning max_features"},{"metadata":{"trusted":true},"cell_type":"code","source":"# GridSearchCV to find optimal max_features\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\n\n# specify number of folds for k-fold CV\nn_folds = 5\n\n# parameters to build the model on\nparameters = {'max_features': [ 8, 12, 16, 20, 24]}\n\n# instantiate the model\nrf = RandomForestClassifier(max_depth = 6)\n\n# fit tree on training data\nrf = GridSearchCV(rf, parameters, \n                    cv=n_folds, \n                    \n                   scoring=\"accuracy\", return_train_score=True)\nrf.fit(X_train_RF, y_train_RF)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# scores of GridSearch CV\nscores = rf.cv_results_\npd.DataFrame(scores).head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plotting accuracies with max_features\nplt.figure()\nplt.plot(scores[\"param_max_features\"], \n         scores[\"mean_train_score\"], \n         label=\"training accuracy\")\nplt.plot(scores[\"param_max_features\"], \n         scores[\"mean_test_score\"], \n         label=\"test accuracy\")\nplt.xlabel(\"max_features\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Tuning min_samples_leaf"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\n# specify number of folds for k-fold CV\nn_folds = 5\n\n# parameters to build the model on\nparameters = {'min_samples_leaf': range(30, 200, 50)}\n\n# instantiate the model\nrf = RandomForestClassifier()\n\n# fit tree on training data\nrf = GridSearchCV(rf, parameters, \n                    cv=n_folds,                   \n                   scoring=\"accuracy\", return_train_score=True)\nrf.fit(X_train_RF, y_train_RF)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# scores of GridSearch CV\nscores = rf.cv_results_\npd.DataFrame(scores).head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plotting accuracies with min_samples_leaf\nplt.figure()\nplt.plot(scores[\"param_min_samples_leaf\"], \n         scores[\"mean_train_score\"], \n         label=\"training accuracy\")\nplt.plot(scores[\"param_min_samples_leaf\"], \n         scores[\"mean_test_score\"], \n         label=\"test accuracy\")\nplt.xlabel(\"min_samples_leaf\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Fitting the final model with the best parameters obtained from grid search.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# model with the best hyperparameters\nfrom sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier(bootstrap=True,\n                             max_depth=10,\n                             min_samples_leaf=50, \n                             min_samples_split=200,\n                             max_features=22,\n                             n_estimators=100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fit\nrfc.fit(X_train_RF,y_train_RF)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# predict\npredictions = rfc.predict(X_test_RF)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# evaluation metrics\nfrom sklearn.metrics import classification_report,confusion_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test_RF,predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(confusion_matrix(y_test_RF,predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(accuracy_score(y_test_RF,predictions))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Get numerical feature importances"},{"metadata":{"trusted":true},"cell_type":"code","source":"importances = list(rfc.feature_importances_)\n# List of tuples with variable and importance\nfeature_importances = [(feature, round(importance, 2)) for feature, importance in zip(X_train_RF.columns, importances)]\n# Sort the feature importances by most important first\nfeature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n# Print out the feature and importances \n[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances];","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Conclusions of Random Forest Algorithm\n\n- The Model Accuracy Score that the accuracy on the Test Data Set is very High i.e. 0.93\n- The model was tuned with a number of Hyperparameters for the yield\n\n#### The main predictor variables for Telecom Churn are\n\n- loc_ic_mou_8\n- total_ic_mou_8\n- loc_ic_t2m_mou_8\n- last_day_rch_amt_8\n- max_rech_data_8\n- loc_ic_t2t_mou_8\n- max_rech_amt_8\n- count_rech_2g_8\n- total_og_mou_8\n- loc_og_t2t_mou_8\n- total_rech_num_8\n\n\n"},{"metadata":{},"cell_type":"markdown","source":"## XGBoost - Queen Bee Algorithm!"},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost as xgb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_xgboost, y_xgboost = telecom_n.drop(['Churn'],axis=1),telecom_n[['Churn']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create a matrix for identifying important predictors\ndata_dmatrix = xgb.DMatrix(data=x_xgboost,label=y_xgboost)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#separate the data into train and test\nX_train_xg, X_test_xg, y_train_xg, y_test_xg = train_test_split(x_xgboost, y_xgboost, test_size=0.3, random_state=123)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Crate XGBoost classifer model\nxg_class = xgb.XGBClassifier(objective ='reg:logistic', colsample_bytree = 0.3, learning_rate = 0.1,\n                max_depth = 5, alpha = 10, n_estimators = 10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Train and Predict based on the model\nxg_class.fit(X_train_xg,y_train_xg)\n\npreds = xg_class.predict(X_test_xg)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(accuracy_score(y_test_xg,preds))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(confusion_matrix(y_test_xg,preds))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {\"objective\":\"reg:logistic\",'colsample_bytree': 0.3,'learning_rate': 0.1,\n                'max_depth': 5, 'alpha': 10}\n\ncv_results = xgb.cv(dtrain=data_dmatrix, params=params, nfold=5,\n                    num_boost_round=50,early_stopping_rounds=10,metrics=\"auc\", as_pandas=True, seed=123)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cv_results.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Perform KFold cross validation to obtain a better meausre of Accuracy\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nkfold = KFold(n_splits=10, random_state=7)\nresults = cross_val_score(xg_class, x_xgboost, y_xgboost, cv=kfold)\nprint(\"Accuracy: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test_xg,preds))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xg_class1 = xgb.train(params=params, dtrain=data_dmatrix, num_boost_round=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nxgb.plot_tree(xg_class1,num_trees=0)\nplt.figure(figsize=(50,50))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"xgb.plot_importance(xg_class1)\nplt.rcParams['figure.figsize'] = [100, 100]\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#####                                                                                      **Fin**"},{"metadata":{},"cell_type":"markdown","source":"- The machine Learning model with **XGBoosting** Algorithm has been chosen as the best\n- The reason is that the Accuracy scores and the precision / recall scores are the highest of all the algorithms\n- The performance of the algorithm is also better than most other algorithms"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":1}