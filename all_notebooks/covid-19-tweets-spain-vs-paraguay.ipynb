{"cells":[{"metadata":{},"cell_type":"markdown","source":"# COVID-19 tweets: Spain vs. Paraguay\n\nTweets about COVID-19 written in Spanish on April 2020 in Spain and Paraguay."},{"metadata":{},"cell_type":"markdown","source":"## Load libraries"},{"metadata":{},"cell_type":"markdown","source":"### Install missing modules"},{"metadata":{"trusted":true},"cell_type":"code","source":"!python -m spacy download es_core_news_md\n!python -m spacy link es_core_news_md es_md\n!pip install spacy_spanish_lemmatizer stopwordsiso stop_words tweet-preprocessor\n!python -m spacy_spanish_lemmatizer download wiki","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Load modules"},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport spacy\nfrom spacy_spanish_lemmatizer import SpacyCustomLemmatizer\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom stop_words import get_stop_words\nimport stopwordsiso as stopwordsiso\nfrom wordcloud import WordCloud, ImageColorGenerator\nfrom PIL import Image\nimport requests\nfrom io import BytesIO\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport preprocessor as p\nimport re\nimport csv","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"nlp = spacy.load('es_md', disable=['ner']) # disabling Named Entity Recognition for speed\nlemmatizer = SpacyCustomLemmatizer() \nnlp.add_pipe(lemmatizer, name=\"lemmatizer\", after=\"tagger\")\n#\np.set_options(p.OPT.URL, p.OPT.EMOJI, p.OPT.MENTION, p.OPT.RESERVED, p.OPT.SMILEY)\n# Stopword removal\nstop_words = set(stopwords.words('spanish'))\nstop_words_en = set(stopwords.words('english'))\nstop_words_iso = set(stopwordsiso.stopwords([\"es\", \"en\"]))\nreserved_words = [\"rt\", \"fav\", \"españa\", \"paraguay\", \"vía\", \"nofollow\", \"twitter\", \"true\", \"href\", \"rel\"]\nkey_words = ['coronavirus', 'coronavirusoutbreak', 'coronavirusPandemic', 'covid19', 'covid_19', 'epitwitter', 'ihavecorona', 'StayHomeStaySafe', 'TestTraceIsolate'] # twitter search keys\nstop_words_es = set(get_stop_words('es'))\nstop_words_en_ = set(get_stop_words('en'))\nstop_words.update(stop_words_es)\nstop_words.update(stop_words_en)\nstop_words.update(stop_words_en_)\nstop_words.update(stop_words_iso)\nstop_words.update(reserved_words)\nstop_words.update(key_words)\n#\nfile_name = 'covid19-tweets-early-late-april'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Dataset\n\nThe dataset have about 14M of tweets written in April 2020 (around the world) about COVID-19 with the keywords: #coronavirus, #coronavirusoutbreak, #coronavirusPandemic, #covid19, #covid_19, #epitwitter, #ihavecorona, #StayHomeStaySafe, #TestTraceIsolate"},{"metadata":{},"cell_type":"markdown","source":"### Concat tweets files\n\nOnly if language is Spanish"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"li = []\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        df = pd.read_csv(os.path.join(dirname, filename), index_col=None, header=0)\n        df = df[df['lang']=='es']\n        li.append(df)\n\ntweets_es = pd.concat(li, axis=0, ignore_index=True)\ndel li # free memory","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Random sample of tweets"},{"metadata":{"trusted":true},"cell_type":"code","source":"tweets_es.text.sample(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Export tweets"},{"metadata":{},"cell_type":"markdown","source":"Language: Spanish"},{"metadata":{"trusted":true},"cell_type":"code","source":"file_name += '_lang_es'\ntweets_es.info()\ntweets_es.to_csv(file_name+'_lang_es.csv',encoding='utf8', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Country: Spain"},{"metadata":{"trusted":true},"cell_type":"code","source":"tweets_es_ES = tweets_es[tweets_es['country_code']=='ES']\ntweets_es_ES.info()\nfile_name += '_country'\ntweets_es_ES.to_csv(file_name+'_ES.csv',encoding='utf8', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Country: Paraguay"},{"metadata":{"trusted":true},"cell_type":"code","source":"tweets_es_PY = tweets_es[tweets_es['country_code']=='PY']\ntweets_es_PY.info()\ntweets_es_PY.to_csv(file_name+'_PY.csv',encoding='utf8', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Getting uni-grams - wordcloud for Spain and Paraguay! \n\n1. remove string special characters\n1. remove stop-words\n1. get lemmas (only content words)\n1. calculate word frequencies\n1. plot wordcloud over flags of Spain/Paraguay\n1. write a csv with word frequencies"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Preprocessing\ndef remove_string_special_characters(s):\n    stripped = str(s)\n\n    # Python regex, keep alphanumeric but remove numeric\n    stripped = re.sub(r'\\b[0-9]+\\b', '', stripped)\n\n    # Change any white space to one space\n    stripped = re.sub('\\s+', ' ', stripped)\n\n    # Remove urls\n    stripped = re.sub(r\"http\\S+\",'', stripped)\n    \n    # check again\n    #stripped = p.clean(stripped) be careful... also deletes ñ and accents\n    \n    # # to ''\n    stripped = stripped.replace('#','')\n\n    # Remove start and end white spaces\n    stripped = stripped.strip()\n    if len(stripped) >= 3:#stripped != '':\n        return stripped.lower()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# only lemmas of content words           \ndef lemmatizer(text):        \n    sent = []\n    doc = nlp(text)\n    for word in doc:\n        if (word.pos_ not in ['VERB','ADV','ADJ','NOUN','PROPN']):\n            continue\n        sent.append(word.lemma_)\n    return \" \".join(sent)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# wordcloud\nmaxWords=200\ndef show_wordcloud_mask(data, mask, stopwords, fileName=\"wordcloud.png\", title = None, maxWords=maxWords):\n    wordcloud = WordCloud(\n        background_color = 'white',\n        max_words = maxWords,\n        max_font_size = 180,\n        scale = 3,\n        random_state = 42,\n        stopwords = stopwords,\n        mask=mask,\n        collocations=False,\n    ).generate_from_frequencies(frequencies=data)\n\n    # create coloring from image\n    image_colors = ImageColorGenerator(mask)\n    fig = plt.figure(figsize=[14,14])\n    plt.imshow(wordcloud.recolor(color_func=image_colors), interpolation=\"bilinear\")\n    plt.axis(\"off\")\n    if title:\n        fig.suptitle(title, fontsize = 20)\n        fig.subplots_adjust(top = 2.3)\n    plt.savefig(fileName, facecolor='k', bbox_inches='tight')\n    return","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def unigrams_freq_wordcloud(data,retweets=False,country='ES'):\n    \n    print('country', country, \"=\"*50)\n\n    data = data[data['is_retweet']==retweets]\n    data[\"text\"]=data[\"text\"].astype(str)\n    data.info()\n    print(data['text'].sample(5))\n    \n    print(\"remove_string_special_characters ...\")\n    data[\"text\"]= data[\"text\"].apply(remove_string_special_characters)\n    data.dropna(subset = [\"text\"], inplace=True)\n    print(data['text'].sample(5))\n\n    print(\"remove_stopwords ...\")\n    data[\"text\"] = data[\"text\"].apply(lambda y: ' '.join([x for x in nltk.word_tokenize(y) if ( x not in set(stop_words) and len(x)>2 and \"covid\" not in x) ]))\n    data = data[~data['text'].isnull()]\n    data.dropna(subset = [\"text\"], inplace=True)\n    print(data['text'].sample(5))\n    \n    print(\"lemmatizer ...\")\n    data[\"text_lemmatize\"] = data.apply(lambda x: lemmatizer(x['text']), axis=1)\n    data['text'] = data['text_lemmatize'].str.replace('-PRON-', '')\n    data.dropna(subset = [\"text\"], inplace=True)\n    print(data['text'].sample(5))\n\n    print(\"n-grams ...\")\n    gram = 1\n    vectorizer = CountVectorizer(ngram_range = (gram,gram),stop_words=stop_words).fit(data.text.values.astype('U'))\n    bag_of_words = vectorizer.transform(data[\"text\"].values.astype('U'))\n    sum_words = bag_of_words.sum(axis=0)\n    words_freq = [(word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items() if len(word)>2]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n\n    print(\"plot ...\")\n    mask_url = \"https://live.staticflickr.com/7102/7378035790_231462ff2e_b.jpg\"\n    if country == 'PY':\n        mask_url = \"https://live.staticflickr.com/8161/7383472952_4f08c69c6c_b.jpg\"\n    response = requests.get(mask_url)\n    mask = np.array(Image.open(BytesIO(response.content)))\n    show_wordcloud_mask(dict(words_freq), mask, stop_words, \"retweets\"+str(retweets)+\"country\"+country+\"-1gram.png\")\n    \n\n    print(\"to_file ...\")\n    with open(\"word_freq-\"+str(gram)+\"gram_\"+country+\".csv\", \"w\") as f:\n        w = csv.writer(f)\n        w.writerows(words_freq)\n    f.close()\n    \n    return","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Give an idea about the conversation on Twitter..."},{"metadata":{"trusted":true},"cell_type":"code","source":"unigrams_freq_wordcloud(tweets_es_ES,retweets=False,country='ES')\nunigrams_freq_wordcloud(tweets_es_PY,retweets=False,country='PY')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}