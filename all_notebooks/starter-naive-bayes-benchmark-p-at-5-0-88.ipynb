{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\n\nprint(os.listdir(\"../input\"))\n\n# running our benchmark code in this kernel lead to memory errors, so \n# we do a slightly less memory intensive procedure if this is True, \n# set this as False if you are running on a computer with a lot of RAM\n# it should be possible to use less memory in this kernel using generators\n# rather than storing everything in RAM, but we won't explore that here\nRUNNING_KAGGLE_KERNEL = True ","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"rspct_df = pd.read_csv('../input/rspct.tsv', sep='\\t')\ninfo_df  = pd.read_csv('../input/subreddit_info.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7416711cd382370e78ba5079f465ff1d4c6eac3a"},"cell_type":"markdown","source":"## Basic data analysis"},{"metadata":{"trusted":true,"_uuid":"b9cd6088f16683e066378ea9d35785f6c89af44c"},"cell_type":"code","source":"rspct_df.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d2cfcbdd2f7c470c70c73c4bbddb37adc5427a0b"},"cell_type":"code","source":"# note that info_df has information on subreddits that are not in data, \n# we filter them out here\n\ninfo_df = info_df[info_df.in_data].reset_index()\ninfo_df.head(5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cc2ff266075c990c3df99de54dc84cd4566b8e6a"},"cell_type":"markdown","source":"## Naive Bayes benchmark"},{"metadata":{"trusted":true,"_uuid":"c486638ded3da5e05e16482214250bc2dec49748"},"cell_type":"code","source":"# we join the title and selftext into one field\n\ndef join_text(row):\n    if RUNNING_KAGGLE_KERNEL:\n        return row['title'][:100] + \" \" + row['selftext'][:512]\n    else:\n        return row['title'] + \" \" + row['selftext']\n\nrspct_df['text'] = rspct_df[['title', 'selftext']].apply(join_text, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"43c34a5b2ceb8b2f13cc66eb94e170a49ad30433"},"cell_type":"code","source":"# take the last 20% as a test set - N.B data is already randomly shuffled,\n# and last 20% is a stratified split (equal proportions of subreddits)\n\ntrain_split_index = int(len(rspct_df) * 0.8)\n\ntrain_df, test_df = rspct_df[:train_split_index], rspct_df[train_split_index:]\nX_train , X_test  = train_df.text, test_df.text\ny_train, y_test   = train_df.subreddit, test_df.subreddit","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f04a86c4ed629ef2ff1180dd7a4191c130fbb3b2"},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\n# label encode y\n\nle = LabelEncoder()\nle.fit(y_train)\ny_train = le.transform(y_train)\ny_test  = le.transform(y_test)\n\ny_train[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"27ddcca6c2af541e94133222961a83c11f54208e"},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\n# extract features from text using bag-of-words (single words + bigrams)\n# use tfidf weighting (helps a little for Naive Bayes in general)\n# note : you can do better than this by extracting more features, then \n# doing feature selection, but not enough memory on this kernel!\n\nprint('this cell will take about 10 minutes to run')\n\nNUM_FEATURES = 30000 if RUNNING_KAGGLE_KERNEL else 100000\n\ntf_idf_vectorizer = TfidfVectorizer(max_features = NUM_FEATURES,\n                                min_df=5,\n                                ngram_range=(1,2),\n                                stop_words=None,\n                                token_pattern='(?u)\\\\b\\\\w+\\\\b',\n                            )\n\nX_train = tf_idf_vectorizer.fit_transform(X_train)\nX_test  = tf_idf_vectorizer.transform(X_test)\n\nfrom sklearn.feature_selection import chi2, SelectKBest\n\n# if we have more memory, select top 100000 features and select good features\nif not RUNNING_KAGGLE_KERNEL:\n    chi2_selector = SelectKBest(chi2, 30000)\n\n    chi2_selector.fit(X_train, y_train) \n\n    X_train = chi2_selector.transform(X_train)\n    X_test  = chi2_selector.transform(X_test)\n\nX_train.shape, X_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6d317a155229fa60c6241e7b8d2355fb1cba9d43"},"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\n\n# train a naive bayes model, get predictions\n\nnb_model = MultinomialNB(alpha=0.1)\nnb_model.fit(X_train, y_train)\n\ny_pred_proba = nb_model.predict_proba(X_test)\ny_pred = np.argmax(y_pred_proba, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8d31738daa5d0382761477f16e79d1800ac6f730"},"cell_type":"code","source":"# we use precision-at-k metrics to evaluate performance\n# (https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)#Precision_at_K)\n\ndef precision_at_k(y_true, y_pred, k=5):\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    y_pred = np.argsort(y_pred, axis=1)\n    y_pred = y_pred[:, ::-1][:, :k]\n    arr = [y in s for y, s in zip(y_true, y_pred)]\n    return np.mean(arr)\n\nprint('precision@1 =', np.mean(y_test == y_pred))\nprint('precision@3 =', precision_at_k(y_test, y_pred_proba, 3))\nprint('precision@5 =', precision_at_k(y_test, y_pred_proba, 5))\n\n# RUNNING_KAGGLE_KERNEL == True\n# precision@1 = 0.610528134254689\n# precision@3 = 0.7573692003948668\n# precision@5 = 0.8067670286278381\n\n# RUNNING_KAGGLE_KERNEL == False\n# precision@1 = 0.7292102665350444\n# precision@3 = 0.8512240868706812\n# precision@5 = 0.8861500493583415","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"755686e8e4421d8f4a61819bc3c4afa958294779"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}