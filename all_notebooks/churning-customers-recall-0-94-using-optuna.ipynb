{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Predict Churning customers"},{"metadata":{},"cell_type":"markdown","source":"The goal of this project is to predict the customers who want to cancel a credit card program, such that actions can be taken to prevent the event from happening.\n\nThe top priority is to identify customers who are getting churned. Even if we predict non-churning customers as churned, it wonâ€™t harm our business. But predicting churning customers as non-churning will do. So recall (True positives/(True positives + False negatives) must be high.\n\nThe dataset is strongly un-balanced: only 16% of customers churned.\n\nThe notebook is organized as follow:\n\n+ In Section 1, the dataset is explored, checking if null values are present.\n\n+ In Section 2, feature engineering is performed as follow:\n\n    + The categorical target feature (the Attrition_Flag) is converted to numerical.\n    + Other categorical features are one-hot encoded.\n    + The dataset is divided into train and test, using stratified sampling.\n    + The outliers in the training dataset are identified.\n    + A data preprocessing pipeline is built, removing the outliers identified in the previous step and standardizing each feature.\n    + Highly correlated features are removed, identifing highly correleted feature pairs\n    + Highly multicollinear features are removed, estimating the variance inflation factor\n\n+ In Section 3, XGBoost is used as a model for predicting churned/not churned customers. Model hyperparameters are searched as follow:\n\n    + An objective function is defined. The objective function computes the average value of the cross-validation score on the training dataset, using the negative log loss as a scoring metric\n    + The maximum value of the objective function is searched using the Bayesian framework Optuna [https://optuna.org/]\n    + On the test dataset, the recall value is 0.927"},{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Common imports"},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport warnings\ndef ignore_warn(*args, **kwargs):\n    pass\nwarnings.warn = ignore_warn #ignore annoying warning (from sklearn and seaborn)\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1. Explore dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"bankchurners = pd.read_csv(\"/kaggle/input/credit-card-customers/BankChurners.csv\")\nbankchurners.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bankchurners.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Ignore the last 2 columns (as suggested by the data description section)"},{"metadata":{"trusted":true},"cell_type":"code","source":"bankchurners.drop(['Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_1',\n       'Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_2'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## How many NaNs values are present?"},{"metadata":{"trusted":true},"cell_type":"code","source":"bankchurners.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bankchurners.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Feature Engineering"},{"metadata":{},"cell_type":"markdown","source":"## CLIENTNUM will not have predictive power, it is an id"},{"metadata":{"trusted":true},"cell_type":"code","source":"bankchurners.drop(['CLIENTNUM'],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Which columns are numerical and which categorical?"},{"metadata":{"trusted":true},"cell_type":"code","source":"numerical_features = bankchurners._get_numeric_data().columns\nnumerical_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Categorical Columns\ncategorical_features = bankchurners.select_dtypes(include='object').columns\ncategorical_features","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Convert the income into ordinal features, it makes sense for this categorical variable"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot('Income_Category', data=bankchurners)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bankchurners['Income_Category'] = bankchurners['Income_Category'].replace({\n    'Unknown':0,\n    'Less than $40K':1,\n    '$40K - $60K':2,\n    '$60K - $80K':3,\n    '$80K - $120K':4,\n    '$120K +':5\n})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Convert attrition_flag to numerical"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot('Attrition_Flag', data=bankchurners)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bankchurners['Attrition_Flag'] = bankchurners['Attrition_Flag'].replace({\n    'Existing Customer':0,\n    'Attrited Customer':1,\n})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## For the other categorical variables, use dummies (OneHot encoding)"},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical = bankchurners.select_dtypes(include='object').columns\ncategorical","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bankchurners = pd.get_dummies(bankchurners, columns = categorical, drop_first=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bankchurners.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Raname columns to remove white spaces and other charaters incompatible with patsy (used in the next section)"},{"metadata":{"trusted":true},"cell_type":"code","source":"bankchurners.rename(columns={\n    \"Education_Level_High School\": \"Education_Level_High_School\",\n    \"Education_Level_Post-Graduate\": \"Education_Level_Post_Graduate\",\n},inplace=True)\nbankchurners.columns.values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Split the dataset in train and dev, before any other analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ny = bankchurners['Attrition_Flag']\nX = bankchurners.drop(['Attrition_Flag'], axis=1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42,stratify =y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.concat([X_train, y_train],axis=1)\ntest = pd.concat([X_test, y_test],axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Check outliers on numerical features"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(font_scale=1.5)\ndef box_plot(key):\n    fig = plt.figure(figsize=(30, 20));\n    sns.boxplot(x='Attrition_Flag', y=key, data=train[['Attrition_Flag', key]])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"box_plot('Customer_Age')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"box_plot('Dependent_count')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"box_plot('Months_on_book')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"box_plot('Total_Relationship_Count')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"box_plot('Months_Inactive_12_mon')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"box_plot('Contacts_Count_12_mon')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"box_plot('Credit_Limit')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"box_plot('Total_Revolving_Bal')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"box_plot('Avg_Open_To_Buy')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"box_plot('Total_Amt_Chng_Q4_Q1')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"box_plot('Total_Trans_Amt')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"box_plot('Total_Trans_Ct')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"box_plot('Total_Ct_Chng_Q4_Q1')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"box_plot('Avg_Utilization_Ratio')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Define a pre-processing pipeline"},{"metadata":{},"cell_type":"markdown","source":"### Write a custom transform to remove outliers"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin\nclass RemoveOutliers(BaseEstimator, TransformerMixin):\n    \n    def __init__(self, feature, min_value, max_value):\n        self.feature = feature\n        self.min_value = min_value\n        self.max_value = max_value\n        \n    def fit(self, X, y=None):\n        return self # nothing else to do\n    \n    def transform(self, X, y=None):\n        \n        feature_values = X[self.feature]\n        feature_values[feature_values < self.min_value ] = self.min_value\n        feature_values[feature_values > self.max_value ] = self.max_value\n        \n        X[self.feature] = feature_values\n        \n        return X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.feature_selection import VarianceThreshold\n\nnumerical_pipeline = Pipeline([\n    ('age_outliers',RemoveOutliers('Customer_Age',0,70)),\n    ('varianceThreshold', VarianceThreshold()),\n    ('std_scaler', StandardScaler())])\n\nfull_pipeline = ColumnTransformer([(\"num\", numerical_pipeline, numerical_features)])\n\nX_train_values = full_pipeline.fit_transform(X_train)\nX_test_values = full_pipeline.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train[numerical_features] = X_train_values\nX_test[numerical_features] = X_test_values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.concat([X_train, y_train],axis=1)\ntest = pd.concat([X_test, y_test],axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Function to remove columns in both train and test set"},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_columns(df, columns):\n    for c in columns:\n        if c in df.columns:\n            df.drop(c, axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Remove highly correlated columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.stats import pearsonr\ndef highly_correleted_columns(df, columns_to_preserve, threshold):\n    corr_columns=[]\n    for c in df.columns:\n        # column to preserve\n        if c in corr_columns:\n            continue\n        # correlation with pval\n        for cc in df.columns:\n            if cc == c:\n                continue\n            if cc in columns_to_preserve:\n                continue\n            if cc in corr_columns:\n                continue\n            corrtest = pearsonr(df[c], df[cc])\n            corr = corrtest[0]\n            pval = corrtest[1]\n            if abs(corr) > threshold and pval < 0.05:\n                corr_columns.append(cc)\n    return corr_columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"columns_to_remove = highly_correleted_columns(train,['Attrition_Flag'], 0.70)\ncolumns_to_remove","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"remove_columns(train, columns_to_remove)\nremove_columns(test, columns_to_remove)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Remove highly collinear features"},{"metadata":{"trusted":true},"cell_type":"code","source":"from patsy import dmatrices\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\ndef compute_variance_inflation_factor(df, column_to_predict):\n\n    feature_columns = list(df.columns.values)\n    # always remove the column to predict\n    feature_columns.remove(column_to_predict)\n    features = \"+\".join(feature_columns)\n\n    # get y and X dataframes based on this regression:\n    y, X = dmatrices(column_to_predict + '~' + features, data=df, return_type='dataframe')\n\n    # Calculate VIF Factors, for each X, calculate VIF and save in dataframe\n    vif = pd.DataFrame()\n    vif[\"VIF_Factor\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n    vif[\"features\"] = X.columns\n\n    # Inspect VIF Factors\n    print(vif.sort_values('VIF_Factor'))\n    return vif","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vif = compute_variance_inflation_factor(train, 'Attrition_Flag')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nans_columns = vif[vif.isin([np.nan, np.inf, -np.inf]).any(1)].features.values\nremove_columns(train, nans_columns)\nremove_columns(test, nans_columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"highly_collinear = vif.loc[vif.VIF_Factor > 5.0].features.values\nremove_columns(train, highly_collinear)\nremove_columns(test, highly_collinear)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Modelling"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import confusion_matrix, recall_score\nimport optuna","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Plot confusion matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"import itertools\ndef plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## XGBoost classifier"},{"metadata":{},"cell_type":"markdown","source":"## Use SMOTE and RandomUnderSampler to reduce imbalance"},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost as xgb\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom imblearn.over_sampling import BorderlineSMOTE\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.pipeline import Pipeline as imblearn_pipeline\n\nover = BorderlineSMOTE(sampling_strategy=0.3)\nunder = RandomUnderSampler(sampling_strategy=0.6)\nsteps = [('oversampling', over), ('undersampling', under)]\n\ndef objectiveXGBoost(trial):\n    \n    over = BorderlineSMOTE(sampling_strategy=0.3)\n    under = RandomUnderSampler(sampling_strategy=0.6)\n    \n    gamma_int = trial.suggest_float('gamma', 0.01, 10,log=True)\n    max_depth = trial.suggest_int('max_depth', 1, 5)\n    clf = xgb.XGBClassifier(n_jobs=3,seed=42,gamma=gamma_int,max_depth=max_depth)\n    \n    # by using a pipeline the metric is computed on the original, not balanced dataset\n    full_pipeline = imblearn_pipeline( [('oversampling', over), ('undersampling', under),('model',clf)])\n    \n    # stratified k-fold cross-validation \n    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=42)\n    \n    return cross_val_score(full_pipeline, X_train,y_train, n_jobs = 3, cv=cv, scoring='neg_log_loss').mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import optuna\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objectiveXGBoost, n_trials=30)\ntrial = study.best_trial\nprint(trial.params)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## The dataset used for fitting should be re-balanced"},{"metadata":{"trusted":true},"cell_type":"code","source":"over = BorderlineSMOTE(sampling_strategy=0.3)\nunder = RandomUnderSampler(sampling_strategy=0.6)\nsteps = [('oversampling', over), ('undersampling', under)]\nsampling_pipeline = imblearn_pipeline(steps=steps)\nX_train_fit, y_train_fit = sampling_pipeline.fit_resample(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = xgb.XGBClassifier()\nclf.set_params(**study.best_trial.params)\nclf.fit(X_train_fit,y_train_fit)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"yhat = clf.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cnf_matrix = confusion_matrix(y_test, yhat, labels=[0,1])\nplot_confusion_matrix(cnf_matrix, classes=['Existing Customer','Attrited Customer'],normalize=True,  title='Confusion matrix')\nprint('Recall is ', recall_score(y_test, yhat, average='macro'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}