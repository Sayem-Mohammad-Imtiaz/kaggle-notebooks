{"cells":[{"metadata":{"_uuid":"3c1cf36e0c9eabe6ca1f3786b6b1a38539bd7f9d"},"cell_type":"markdown","source":"Hello everyone\n\nIn this kernel we will learn K-means clustering and Hierarchical clustering.\n\nLet's begin"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport matplotlib.pyplot as plt\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"\n# UNSUPERVISED LEARNING\n* Unsupervised learning: It uses data that has unlabeled and uncover hidden patterns from unlabeled data. Example, there are orthopedic patients data that do not have labels. You do not know which orthopedic patient is normal or abnormal.\n* As you know orthopedic patients data is labeled (supervised) data. It has target variables. In order to work on unsupervised learning, lets drop target variables and to visualize just consider pelvic_radius and degree_spondylolisthesis"},{"metadata":{"_uuid":"cbb150acd60ca2034342660164dbc186e3cf7834"},"cell_type":"markdown","source":"\n# KMEANS\n* Lets try our first unsupervised method that is KMeans Cluster\n* KMeans Cluster: The algorithm works iteratively to assign each data point to one of K groups based on the features that are provided. Data points are clustered based on feature similarity\n* KMeans(n_clusters = 2): n_clusters = 2 means that create 2 cluster"},{"metadata":{"trusted":true,"_uuid":"8ae31bb1438a28d8b68852c761d8aae015054881"},"cell_type":"code","source":"# As you can see there is no labels in data\ndata = pd.read_csv('../input/column_2C_weka.csv')\nplt.scatter(data['pelvic_radius'],data['degree_spondylolisthesis'])\nplt.xlabel('pelvic_radius')\nplt.ylabel('degree_spondylolisthesis')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6e05709c5c2bf30f8615907c56904bdb0be70594"},"cell_type":"code","source":"# KMeans Clustering\ndata2 = data.loc[:,['degree_spondylolisthesis','pelvic_radius']]\nfrom sklearn.cluster import KMeans\nkmeans = KMeans(n_clusters = 2)\nkmeans.fit(data2)\nlabels = kmeans.predict(data2)\nplt.scatter(data['pelvic_radius'],data['degree_spondylolisthesis'],c = labels)\nplt.xlabel('pelvic_radius')\nplt.xlabel('degree_spondylolisthesis')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"41928696537175ca6759cc7536c79cd0d6b4daf0"},"cell_type":"markdown","source":"# EVALUATING OF CLUSTERING\nWe cluster data in two groups. Okey well is that correct clustering? In order to evaluate clustering we will use cross tabulation table.\n\n* There are two clusters that are 0 and 1\n* First class 0 includes 138 abnormal and 100 normal patients\n* Second class 1 includes 72 abnormal and 0 normal patiens *The majority of two clusters are abnormal patients."},{"metadata":{"trusted":true,"_uuid":"b3566eeca4187a2c0926c9ebf6c4cc180edfc836"},"cell_type":"code","source":"# cross tabulation table\ndf = pd.DataFrame({'labels':labels,\"class\":data['class']})\nct = pd.crosstab(df['labels'],df['class'])\nprint(ct)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ab2c7867c66d00ac3659cf878db6081b355c57ba"},"cell_type":"markdown","source":"The new question is that we know how many class data includes, but what if number of class is unknow in data. This is kind of like hyperparameter in KNN or regressions.\n\n* inertia: how spread out the clusters are distance from each sample\n* lower inertia means more clusters\n* What is the best number of clusters ? *There are low inertia and not too many cluster trade off so we can choose elbow"},{"metadata":{"trusted":true,"_uuid":"a58ab80a50a111d07c7d021024b7345bff21585a"},"cell_type":"code","source":"# inertia\ninertia_list = np.empty(8)\nfor i in range(1,8):\n    kmeans = KMeans(n_clusters=i)\n    kmeans.fit(data2)\n    inertia_list[i] = kmeans.inertia_\nplt.plot(range(0,8),inertia_list,'-o')\nplt.xlabel('Number of cluster')\nplt.ylabel('Inertia')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"728a105f446437e386d75e84e532157880ff4c1d"},"cell_type":"markdown","source":"# STANDARDIZATION\n* Standardizaton is important for both supervised and unsupervised learning\n* Do not forget standardization as pre-processing\n* As we already have visualized data so you got the idea. Now we can use all features for clustering.\n* We can use pipeline like supervised learning."},{"metadata":{"trusted":true,"_uuid":"5c40ce4adb4a96f333f170e075f4313314d03d82"},"cell_type":"code","source":"data = pd.read_csv('../input/column_2C_weka.csv')\ndata3 = data.drop('class',axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6412293203db3f541aed3e49185815f2ccba972f"},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\nscalar = StandardScaler()\nkmeans = KMeans(n_clusters = 2)\npipe = make_pipeline(scalar,kmeans)\npipe.fit(data3)\nlabels = pipe.predict(data3)\ndf = pd.DataFrame({'labels':labels,\"class\":data['class']})\nct = pd.crosstab(df['labels'],df['class'])\nprint(ct)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"585c950548d97ffedd8fa1f5b3bfef3c13227b8c"},"cell_type":"markdown","source":"# HIERARCHY\n* vertical lines are clusters\n* height on dendogram: distance between merging cluster\n* method= 'single' : closest points of clusters"},{"metadata":{"trusted":true,"_uuid":"95711d5c56d6640ed9c8baf87f7277787aab2ca3"},"cell_type":"code","source":"from scipy.cluster.hierarchy import linkage,dendrogram\n\nmerg = linkage(data3.iloc[200:220,:],method = 'single')\ndendrogram(merg, leaf_rotation = 90, leaf_font_size = 6)\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}