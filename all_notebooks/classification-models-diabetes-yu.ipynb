{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5"},"cell_type":"markdown","source":"\n\n# DIABETES\n\n### The unprocessed dataset was acquired from UCI Machine Learning organisation. This dataset is preprocessed by me, originally from the National Institute of Diabetes and Digestive and Kidney Diseases. The objective of the dataset is to accurately predict whether or not, a patient has diabetes, based on multiple features included in the dataset. \n\n\n### Number of Instances: 768\n### Number of Attributes: 8 plus class\n### For Each Attribute: (all numeric-valued)\n\n***Pregnancies**:                Number of times pregnant \n\n***Glucose**    :                Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n\n***BloodPressure**:              Diastolic blood pressure (mm Hg)\n\n***SkinThickness**:              Triceps skin fold thickness (mm)\n\n***Insulin**:                    2-Hour serum insulin (mu U/ml)\n\n***BMI**:                        Body mass index (weight in kg/(height in m)^2)\n\n***DiabetesPedigreeFunction**:   Diabetes pedigree function\n\n***Age**:                        Age (years)\n\n***Outcome**:                    Class variable (0 or 1)\n\n***Missing Attribute Values**: Yes\n\n### Class Distribution: (class value 1 is interpreted as \"tested positive for diabetes\")\n\n\n### Attributes Normal Value Range:\n\n***Glucose: Glucose (< 140) = Normal, Glucose (140-200) = Pre-Diabetic, Glucose (> 200) = Diabetic\n\n\n***BloodPressure: B.P (< 60) = Below Normal, B.P (60-80) = Normal, B.P (80-90) = Stage 1 Hypertension, B.P (90-120) = Stage 2 Hypertension, B.P (> 120) = Hypertensive Crisis\n\n\n***SkinThickness: SkinThickness (< 10) = Below Normal, SkinThickness (10-30) = Normal, SkinThickness (> 30) = Above Normal\n\n\n***Insulin: Insulin (< 200) = Normal, Insulin (> 200) = Above Normal\n\n\n***BMI: BMI (< 18.5) = Underweight, BMI (18.5-25) = Normal, BMI (25-30) = Overweight, BMI (> 30) = Obese*\n\n\n\n\n### Class Value Number of instances\n*0 : 500\n\n*1 : 268","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Load libraries\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nimport statsmodels.api as sm\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import scale, StandardScaler, RobustScaler, MinMaxScaler\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\nfrom sklearn.metrics import plot_confusion_matrix, confusion_matrix, accuracy_score, mean_squared_error, r2_score, roc_auc_score, roc_curve, classification_report, precision_recall_curve, auc\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nimport missingno as msno\n\nimport warnings\nwarnings.simplefilter(action = 'ignore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Read data","execution_count":null},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"diabetes = pd.read_csv('../input/pima-indians-diabetes-database/diabetes.csv')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Overview","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"diabetes.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"diabetes.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"diabetes.describe([0.10, 0.25, 0.40, 0.50,0.70, 0.90,0.95, 0.99]).T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = diabetes.copy()\ndf[\"Outcome\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Correlation:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr = df.corr()\ncorr.style.background_gradient(cmap='coolwarm')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.groupby(\"Outcome\").agg({\"Pregnancies\":\"mean\"})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.groupby(\"Outcome\").agg({\"Age\":\"mean\"})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.groupby(\"Outcome\").agg({\"Age\":\"max\"})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.groupby(\"Outcome\").agg({\"Insulin\": \"mean\"})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.groupby(\"Outcome\").agg({\"Insulin\": \"max\"})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.groupby(\"Outcome\").agg({\"Glucose\": \"mean\"})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.groupby(\"Outcome\").agg({\"Glucose\": \"max\"})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.groupby(\"Outcome\").agg({\"BMI\": \"mean\"})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Missing Values","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df[['Glucose','BloodPressure','SkinThickness','Insulin','BMI']] = df[['Glucose','BloodPressure','SkinThickness','Insulin','BMI']].replace(0,np.NaN)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###  Visualization of the Missing Values","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"msno.bar(df);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The missing data are assigned the median values of the variable in which they are located.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def median_target(sfy):   \n    temp = df[df[sfy].notnull()]\n    temp = temp[[sfy, 'Outcome']].groupby(['Outcome'])[[sfy]].median().reset_index()\n    return temp\n\ncolumns = df.columns\ncolumns = columns.drop(\"Outcome\")\nfor i in columns:\n    median_target(i)\n    df.loc[(df['Outcome'] == 0 ) & (df[i].isnull()), i] = median_target(i)[i][0]\n    df.loc[(df['Outcome'] == 1 ) & (df[i].isnull()), i] = median_target(i)[i][1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Visualization of outliers in all columns with boxplot.","execution_count":null},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"sns.set(font_scale=0.7) \nfig, axes = plt.subplots(nrows=int(len(df.columns)/2), ncols=2,figsize=(7,12))\nfig.tight_layout()\nfor ax,col in zip(axes.flatten(),df.columns):\n    sns.boxplot(x=df[col],ax=ax)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for feature in df:\n    \n    Q1 = df[feature].quantile(0.05)\n    Q3 = df[feature].quantile(0.95)\n    IQR = Q3-Q1\n    upper = Q3 + 1.5*IQR\n    \n    if df[(df[feature] > upper)].any(axis=None):\n        print(feature,\"yes\")\n    else:\n        print(feature, \"no\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###  As a result of our analysis, it was seen that there were outliers in two variables. These values are filled with threshold values.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Q1 = df.Insulin.quantile(0.25)\nQ3 = df.Insulin.quantile(0.75)\nIQR = Q3-Q1\nlower = Q1 - 1.5*IQR\nupper = Q3 + 1.5*IQR\ndf.loc[df[\"Insulin\"] > upper,\"Insulin\"] = upper","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Q1 = df.SkinThickness.quantile(0.25)\nQ3 = df.SkinThickness.quantile(0.75)\nIQR = Q3-Q1\nlower = Q1 - 1.5*IQR\nupper = Q3 + 1.5*IQR\ndf.loc[df[\"SkinThickness\"] > upper,\"SkinThickness\"] = upper","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Local Outlier Factor (LOF)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import LocalOutlierFactor\nlof = LocalOutlierFactor(n_neighbors = 20, contamination = 0.1)\nlof.fit_predict(df)\ndf_scores = lof.negative_outlier_factor_\ndf_scores = pd.DataFrame(np.sort(df_scores))\ndf_scores.plot(stacked=True, xlim=[0,60], style='.-'); # first 20 rows\n    \ndf_scores[0:20]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_scores.iloc[4,:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"threshold = np.sort(df_scores)[4]\nnew_df = df[np.array(df_scores > threshold)]\nnew_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = new_df\ndf.describe().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##  Feature Engineering\n\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"The BMI variable is divided into groups according to general standards and a new categorical variable named NewBMI is created.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"NewBMI = pd.Series([\"Underweight\", \"Normal\", \"Overweight\", \"Obesity 1\", \"Obesity 2\", \"Obesity 3\"], dtype = \"category\")\ndf[\"NewBMI\"] = NewBMI\ndf.loc[df[\"BMI\"] < 18.5, \"NewBMI\"] = NewBMI[0]\ndf.loc[(df[\"BMI\"] > 18.5) & (df[\"BMI\"] <= 24.9), \"NewBMI\"] = NewBMI[1]\ndf.loc[(df[\"BMI\"] > 24.9) & (df[\"BMI\"] <= 29.9), \"NewBMI\"] = NewBMI[2]\ndf.loc[(df[\"BMI\"] > 29.9) & (df[\"BMI\"] <= 34.9), \"NewBMI\"] = NewBMI[3]\ndf.loc[(df[\"BMI\"] > 34.9) & (df[\"BMI\"] <= 39.9), \"NewBMI\"] = NewBMI[4]\ndf.loc[df[\"BMI\"] > 39.9 ,\"NewBMI\"] = NewBMI[5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The data in the insulin variable was divided into normal and abnormal groups and a new variable called NewInsulinScore was created.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def set_insulin(row):\n    if row[\"Insulin\"] >= 16 and row[\"Insulin\"] <= 166:\n        return \"Normal\"\n    else:\n        return \"Abnormal\"\n    \ndf = df.assign(NewInsulinScore=df.apply(set_insulin, axis=1))\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The data in the glucose variable were divided into groups according to general standards and a new variable named NewGlucose was defined.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"NewGlucose = pd.Series([\"Low\", \"Normal\", \"Overweight\", \"Secret\", \"High\"], dtype = \"category\")\ndf[\"NewGlucose\"] = NewGlucose\ndf.loc[df[\"Glucose\"] <= 70, \"NewGlucose\"] = NewGlucose[0]\ndf.loc[(df[\"Glucose\"] > 70) & (df[\"Glucose\"] <= 99), \"NewGlucose\"] = NewGlucose[1]\ndf.loc[(df[\"Glucose\"] > 99) & (df[\"Glucose\"] <= 126), \"NewGlucose\"] = NewGlucose[2]\ndf.loc[df[\"Glucose\"] > 126 ,\"NewGlucose\"] = NewGlucose[3]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# One Hot Encoding\n\nWith the One Hot Encoding method, the values in categorical variables have been converted into numerical expressions.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.get_dummies(df, columns =[\"NewBMI\",\"NewInsulinScore\", \"NewGlucose\"], drop_first = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_df = df[['NewBMI_Obesity 1','NewBMI_Obesity 2', 'NewBMI_Obesity 3', 'NewBMI_Overweight','NewBMI_Underweight',\n                     'NewInsulinScore_Normal','NewGlucose_Low','NewGlucose_Normal', 'NewGlucose_Overweight', 'NewGlucose_Secret']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_ = df.drop(['NewBMI_Obesity 1','NewBMI_Obesity 2', 'NewBMI_Obesity 3', 'NewBMI_Overweight','NewBMI_Underweight',\n                     'NewInsulinScore_Normal','NewGlucose_Low','NewGlucose_Normal', 'NewGlucose_Overweight', 'NewGlucose_Secret'], axis = 1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.concat([X_,categorical_df], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Splitting Dataset\n### Splitting the target variable in y and all the other features in X","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y = df['Outcome']\nX = df.drop('Outcome', axis = 1)\nX.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Standardization:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"MinMax = MinMaxScaler(feature_range = (0, 1)).fit(X)\nX_st = MinMax.transform(X)\nX_st = pd.DataFrame(X_st, columns = X.columns)\nX_st.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sdf = pd.concat([X_st, y], axis = 1)\nsdf.describe().T\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Machine Learning:\n\n\n### We will train out data on different machine learning models and use different techniques on each model and then compare our finding at the end to determine which model is working best for out data.\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"\n\n\n## -----   Model Performance and Comparison   -----\n\n### To measure the performance of a model, we need several elements\n\n**Confusion matrix** : also known as the error matrix, allows visualization of the performance of an algorithm\n\n    True Positive (TP) : Diabetic correctly identified as diabetic\n    True Negative (TN) : Healthy correctly identified as healthy\n    False Positive (FP) : Healthy incorrectly identified as diabetic\n    False Negative (FN) : Diabetic incorrectly identified as healthy\n\n**Metrics**\n\n    Accuracy : (TP + TN) / (TP + TN + FP +FN)\n    Precision : TP / (TP + FP)\n    Recall : TP / (TP + FN)\n    F1 score : 2 x ((Precision x Recall) / (Precision + Recall))\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Defining variables to store the outputs.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"avg_accuracies={}\naccuracies={}\nroc_auc={}\npr_auc={}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Defining function to calculate the Cross-Validation score.\n\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def cal_score(name,model,folds):\n    scores = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']\n    avg_result = []\n    for sc in scores:\n        scores = cross_val_score(model, X_st, y, cv = folds, scoring = sc)\n        avg_result.append(np.average(scores))\n    df_avg_score = pd.DataFrame(avg_result)\n    df_avg_score = df_avg_score.rename(index={0: 'Accuracy',\n                                             1:'Precision',\n                                             2:'Recall',\n                                             3:'F1 score',\n                                             4:'Roc auc'}, columns = {0: 'Average'})\n    avg_accuracies[name] = np.round(df_avg_score.loc['Accuracy'] * 100, 2)\n    values = [np.round(df_avg_score.loc['Accuracy'] * 100, 2),\n            np.round(df_avg_score.loc['Precision'] * 100, 2),\n            np.round(df_avg_score.loc['Recall'] * 100, 2),\n            np.round(df_avg_score.loc['F1 score'] * 100, 2),\n            np.round(df_avg_score.loc['Roc auc'] * 100, 2)]\n    plt.figure(figsize = (15, 8))\n    sns.set_palette('mako')\n    ax = sns.barplot(x = ['Accuracy', 'Precision', 'Recall', 'F1 score', 'Roc auc'], y = values)\n    plt.yticks(np.arange(0, 100, 10))\n    plt.ylabel('Percentage %', labelpad = 10)\n    plt.xlabel('Scoring Parameters', labelpad = 10)\n    plt.title('Cross Validation ' + str(folds) + '-Folds Average Scores', pad = 20)\n    for p in ax.patches:\n        ax.annotate(str(p.get_height()), (p.get_x(), p.get_height()), xytext = (p.get_x() + 0.3, p.get_height() + 1.02))\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Defining function to create Confusion Matrix.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def conf_matrix(ytest, pred):\n    plt.figure(figsize = (15, 8))\n    global cm1\n    cm1 = confusion_matrix(ytest, pred)\n    ax = sns.heatmap(cm1, annot = True, cmap = 'Blues')\n    plt.title('Confusion Matrix', pad = 30)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Defining function to calculate the Metrics Scores.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def metrics_score(cm):\n    total = sum(sum(cm))\n    accuracy = (cm[0, 0] + cm[1, 1]) / total\n    precision = cm[1, 1] / (cm[0, 1] + cm[1, 1])\n    sensitivity = cm[1, 1] / (cm[1, 0] + cm[1, 1])\n    f1 = 2 * (precision * sensitivity) / (precision + sensitivity)\n    specificity = cm[0,0] / (cm[0, 1] + cm[0, 0])\n    values = [np.round(accuracy * 100, 2),\n            np.round(precision * 100, 2),\n            np.round(sensitivity * 100, 2),\n            np.round(f1 * 100, 2),\n            np.round(specificity * 100, 2)]\n    plt.figure(figsize = (15, 8))\n    sns.set_palette('magma')\n    ax = sns.barplot(x = ['Accuracy', 'Precision', 'Recall', 'F1 score', 'Specificity'], y = values)\n    plt.yticks(np.arange(0, 100, 10))\n    plt.ylabel('Percentage %', labelpad = 10)\n    plt.xlabel('Scoring Parameter', labelpad = 10)\n    plt.title('Metrics Scores', pad = 20)\n    for p in ax.patches:\n        ax.annotate(str(p.get_height()), (p.get_x(), p.get_height()), xytext = (p.get_x() + 0.3, p.get_height() + 1.02))\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Defining function to plot ROC Curve.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_roc_curve(fpr, tpr):\n    plt.figure(figsize = (8, 6))\n    plt.plot(fpr, tpr, color = 'Orange', label = 'ROC')\n    plt.plot([0, 1], [0, 1], color = 'black', linestyle = '--')\n    plt.ylabel('True Positive Rate', labelpad = 10)\n    plt.xlabel('False Positive Rate', labelpad = 10)\n    plt.title('Receiver Operating Characteristic (ROC) Curve', pad = 20)\n    plt.legend()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Defining function to plot Precision-Recall Curve.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_precision_recall_curve(recall, precision):\n    plt.figure(figsize=(8,6))\n    plt.plot(recall, precision, color='orange', label='PRC')\n    plt.ylabel('Precision',labelpad=10)\n    plt.xlabel('Recall',labelpad=10)\n    plt.title('Precision Recall Curve',pad=20)\n    plt.legend()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1. Logistic Regression Classifier:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X_st, y, test_size = 0.20, random_state = 5)\nlog_model = LogisticRegression()\nlog_model.fit(X_train, y_train)\nprediction1 = log_model.predict(X_test)\naccuracy1 = log_model.score(X_test, y_test) \nprint ('Model Accuracy:',accuracy1 * 100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Storing model accuracy to plot for comparison with other Machine Learning models.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracies['Linear Regression'] = np.round(accuracy1 * 100, 2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. Plotting Confusion Matrix to describe the performance of Random Forest Classifier on a set of test data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"conf_matrix(y_test, prediction1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plotting different metrics scores for the Linear Regression Classifier for evaluation.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"metrics_score(cm1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Plotting the average of different metrics scores for further evaluation.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cal_score('Linear Regression', log_model, 5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plotting Receiver Operating Characteristic (ROC) Curve, to illustrate the diagnostic ability of Linear Regression Classifier as its discrimination threshold is varied and showing the Area under the ROC Curve (AUC) value which will tell us how much our model is capable of distinguishing between healthy and diabetic patients.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"probs = log_model.predict_proba(X_test)\nprobs = probs[:, 1]\nauc1 = roc_auc_score(y_test, probs)\nroc_auc['Linear Regression'] = np.round(auc1, 2)\nprint('Area under the ROC Curve (AUC): %.2f' % auc1)\nfpr1, tpr1, _ = roc_curve(y_test, probs)\nplot_roc_curve(fpr1, tpr1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plotting Precision-Recall Curve for different thresholds of precision and recall much like the ROC Curve and showing the Area under the Precision-Recall Curve (AUCPR), it gives the number summary of the information in the Precision-Recall Curve.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"precision1, recall1, _ = precision_recall_curve(y_test, probs)\nauc_score1 = auc(recall1, precision1)\npr_auc['Linear Regression'] = np.round(auc_score1, 2)\nprint('Area under the PR Curve (AUCPR): %.2f' % auc_score1)\nplot_precision_recall_curve(recall1, precision1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. KNNeighbors Classifier:","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"KNN is a non-parametric, lazy learning algorithm. Its purpose is to use a database in which the data points are separated into several classes to predict the classification of a new sample point.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X_st, y, test_size = 0.20, random_state = 5)\nKNN_model = KNeighborsClassifier()\nKNN_model.fit(X_train, y_train)\nprediction2 = KNN_model.predict(X_test)\naccuracy2 = KNN_model.score(X_test, y_test) \nprint ('Model Accuracy:',accuracy2 * 100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Storing model accuracy to plot for comparison with other Machine Learning models.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracies['KNeighbors Classifier'] = np.round(accuracy2 * 100, 2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. Plotting Confusion Matrix to describe the performance of KNN Classifier on a set of test data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"conf_matrix(y_test, prediction2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plotting different metrics scores for the KNN Classifier for evaluation.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"metrics_score(cm1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Plotting the average of different metrics scores for further evaluation.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cal_score('KNeighbors Classifier', KNN_model, 5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plotting Receiver Operating Characteristic (ROC) Curve, to illustrate the diagnostic ability of KNN Classifier as its discrimination threshold is varied and showing the Area under the ROC Curve (AUC) value which will tell us how much our model is capable of distinguishing between healthy and diabetic patients.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"probs = KNN_model.predict_proba(X_test)\nprobs = probs[:, 1]\nauc2 = roc_auc_score(y_test, probs)\nroc_auc['KNeighbors Classifier'] = np.round(auc2, 2)\nprint('Area under the ROC Curve (AUC): %.2f' % auc2)\nfpr2, tpr2, _ = roc_curve(y_test, probs)\nplot_roc_curve(fpr2, tpr2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plotting Precision-Recall Curve for different thresholds of precision and recall much like the ROC Curve and showing the Area under the Precision-Recall Curve (AUCPR), it gives the number summary of the information in the Precision-Recall Curve.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"precision2, recall2, _ = precision_recall_curve(y_test, probs)\nauc_score2 = auc(recall2, precision2)\npr_auc['KNeighbors Classifier'] = np.round(auc_score2, 2)\nprint('Area under the PR Curve (AUCPR): %.2f' % auc_score2)\nplot_precision_recall_curve(recall2, precision2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. Support Vector Machine Classifier:\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X_st, y, test_size = 0.20, random_state = 5)\nSVC_model = SVC(probability = True)\nSVC_model.fit(X_train, y_train)\nprediction3 = SVC_model.predict(X_test)\naccuracy3 = SVC_model.score(X_test, y_test) \nprint ('Model Accuracy:',accuracy3 * 100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Storing model accuracy to plot for comparison with other Machine Learning models.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracies['Support Vector Machine Classifier'] = np.round(accuracy3 * 100, 2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. Plotting Confusion Matrix to describe the performance of SVM Classifier on a set of test data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"conf_matrix(y_test, prediction3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plotting different metrics scores for the SVM Classifier for evaluation.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"metrics_score(cm1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Plotting the average of different metrics scores for further evaluation.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cal_score('Support Vector Machine Classifier', SVC_model, 5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plotting Receiver Operating Characteristic (ROC) Curve, to illustrate the diagnostic ability of SVM Classifier as its discrimination threshold is varied and showing the Area under the ROC Curve (AUC) value which will tell us how much our model is capable of distinguishing between healthy and diabetic patients.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"probs = SVC_model.predict_proba(X_test)\nprobs = probs[:, 1]\nauc3 = roc_auc_score(y_test, probs)\nroc_auc['Support Vector Machine Classifier'] = np.round(auc3, 2)\nprint('Area under the ROC Curve (AUC): %.2f' % auc3)\nfpr3, tpr3, _ = roc_curve(y_test, probs)\nplot_roc_curve(fpr3, tpr3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plotting Precision-Recall Curve for different thresholds of precision and recall much like the ROC Curve and showing the Area under the Precision-Recall Curve (AUCPR), it gives the number summary of the information in the Precision-Recall Curve.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"precision3, recall3, _ = precision_recall_curve(y_test, probs)\nauc_score3 = auc(recall3, precision3)\npr_auc['Support Vector Machine Classifier'] = np.round(auc_score3, 2)\nprint('Area under the PR Curve (AUCPR): %.2f' % auc_score3)\nplot_precision_recall_curve(recall3, precision3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4. Classification and Regression Tree:\n\n\nDecision Trees are a non-parametric supervised learning method used for classification and regression. The goal is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X_st, y, test_size = 0.20, random_state = 5)\nCART_model = DecisionTreeClassifier(max_depth = 10, min_samples_split = 50)\nCART_model.fit(X_train, y_train)\nprediction4 = CART_model.predict(X_test)\naccuracy4 = CART_model.score(X_test, y_test) \nprint ('Model Accuracy:',accuracy4 * 100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Storing model accuracy to plot for comparison with other Machine Learning models.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracies['Classification and Regression Tree'] = np.round(accuracy4 * 100, 2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. Plotting Confusion Matrix to describe the performance of CART Classifier on a set of test data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"conf_matrix(y_test, prediction4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plotting different metrics scores for the CART Classifier for evaluation.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"metrics_score(cm1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Plotting the average of different metrics scores for further evaluation.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cal_score('Classification and Regression Tree', CART_model, 5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plotting Receiver Operating Characteristic (ROC) Curve, to illustrate the diagnostic ability of CART Classifier as its discrimination threshold is varied and showing the Area under the ROC Curve (AUC) value which will tell us how much our model is capable of distinguishing between healthy and diabetic patients.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"probs = CART_model.predict_proba(X_test)\nprobs = probs[:, 1]\nauc4 = roc_auc_score(y_test, probs)\nroc_auc['Desicion Tree Classifier']=np.round(auc4, 2)\nprint('Area under the ROC Curve (AUC): %.2f' % auc4)\nfpr4, tpr4, _ = roc_curve(y_test, probs)\nplot_roc_curve(fpr4, tpr4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plotting Precision-Recall Curve for different thresholds of precision and recall much like the ROC Curve and showing the Area under the Precision-Recall Curve (AUCPR), it gives the number summary of the information in the Precision-Recall Curve.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"precision4, recall4, _ = precision_recall_curve(y_test, probs)\nauc_score4 = auc(recall4, precision4)\npr_auc['Desicion Tree Classifier'] = np.round(auc_score4, 2)\nprint('Area under the PR Curve (AUCPR): %.2f' % auc_score4)\nplot_precision_recall_curve(recall4, precision4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5. Random Forests:\n\n\nA Random Forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X_st, y, test_size = 0.20, random_state = 5)\nrf_model = RandomForestClassifier(max_features = 8, min_samples_split = 12, n_estimators = 120)\nrf_model.fit(X_train, y_train)\nprediction5 = rf_model.predict(X_test)\naccuracy5 = rf_model.score(X_test, y_test) \nprint ('Model Accuracy:',accuracy5 * 100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Storing model accuracy to plot for comparison with other Machine Learning models.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracies['Random Forests'] = np.round(accuracy5 * 100, 2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. Plotting Confusion Matrix to describe the performance of Random Forest Classifier on a set of test data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"conf_matrix(y_test, prediction5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plotting different metrics scores for the Random Forest Classifier for evaluation.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"metrics_score(cm1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Plotting the average of different metrics scores for further evaluation.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cal_score('Random Forests', rf_model, 5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plotting Receiver Operating Characteristic (ROC) Curve, to illustrate the diagnostic ability of Random Forest Classifier as its discrimination threshold is varied and showing the Area under the ROC Curve (AUC) value which will tell us how much our model is capable of distinguishing between healthy and diabetic patients.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"probs = rf_model.predict_proba(X_test)\nprobs = probs[:, 1]\nauc5 = roc_auc_score(y_test, probs)\nroc_auc['Random Forests Classifier']=np.round(auc5, 2)\nprint('Area under the ROC Curve (AUC): %.2f' % auc5)\nfpr5, tpr5, _ = roc_curve(y_test, probs)\nplot_roc_curve(fpr5, tpr5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plotting Precision-Recall Curve for different thresholds of precision and recall much like the ROC Curve and showing the Area under the Precision-Recall Curve (AUCPR), it gives the number summary of the information in the Precision-Recall Curve.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"precision5, recall5, _ = precision_recall_curve(y_test, probs)\nauc_score5 = auc(recall5, precision5)\npr_auc['Random Forests'] = np.round(auc_score5,3)\nprint('Area under the PR Curve (AUCPR): %.2f' % auc_score5)\nplot_precision_recall_curve(recall5, precision5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Importance:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_imp = pd.Series(rf_model.feature_importances_,\n                        index = X_train.columns).sort_values(ascending = False)\n\nsns.barplot(x = feature_imp, y = feature_imp.index)\nplt.xlabel('Feature Important Scores')\nplt.ylabel('Features')\nplt.title(\"Feature Important Range\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 6. Gradient Boosting Machines","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X_st, y, test_size = 0.20, random_state = 5)\ngbm_model = GradientBoostingClassifier(learning_rate = 0.01, max_depth = 2, n_estimators = 500)\ngbm_model.fit(X_train, y_train)\nprediction6 = gbm_model.predict(X_test)\naccuracy6 = gbm_model.score(X_test, y_test) \nprint ('Model Accuracy:',accuracy6 * 100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Storing model accuracy to plot for comparison with other Machine Learning models.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracies['Gradient Boosting Machines'] = np.round(accuracy6 * 100, 2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. Plotting Confusion Matrix to describe the performance of GBM Classifier on a set of test data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"conf_matrix(y_test, prediction6)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plotting different metrics scores for the GBM Classifier for evaluation.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"metrics_score(cm1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Plotting the average of different metrics scores for further evaluation.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cal_score('Gradient Boosting Machines', gbm_model, 5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plotting Receiver Operating Characteristic (ROC) Curve, to illustrate the diagnostic ability of GBM Classifier as its discrimination threshold is varied and showing the Area under the ROC Curve (AUC) value which will tell us how much our model is capable of distinguishing between healthy and diabetic patients.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"probs = gbm_model.predict_proba(X_test)\nprobs = probs[:, 1]\nauc6 = roc_auc_score(y_test, probs)\nroc_auc['Gradient Boosting Machine Classifier'] = np.round(auc6, 2)\nprint('Area under the ROC Curve (AUC): %.2f' % auc6)\nfpr6, tpr6, _ = roc_curve(y_test, probs)\nplot_roc_curve(fpr6, tpr6)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plotting Precision-Recall Curve for different thresholds of precision and recall much like the ROC Curve and showing the Area under the Precision-Recall Curve (AUCPR), it gives the number summary of the information in the Precision-Recall Curve.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"precision6, recall6, _ = precision_recall_curve(y_test, probs)\nauc_score6 = auc(recall6, precision6)\npr_auc['Gradient Boosting Machine Classifier'] = np.round(auc_score6, 2)\nprint('Area under the PR Curve (AUCPR): %.2f' % auc_score6)\nplot_precision_recall_curve(recall6, precision6)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Importance:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_imp = pd.Series(gbm_model.feature_importances_,\n                        index = X_train.columns).sort_values(ascending = False)\n\nsns.barplot(x = feature_imp, y = feature_imp.index)\nplt.xlabel('Feature Important Scores')\nplt.ylabel('Features')\nplt.title(\"Feature Important Range\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 7. XGBoost:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X_st, y, test_size = 0.20, random_state = 5)\nxgb_model = XGBClassifier(learning_rate = 0.01,max_depth = 3, n_estimators = 500, subsample = 1 )\nxgb_model.fit(X_train, y_train)\nprediction7 = xgb_model.predict(X_test)\naccuracy7 = xgb_model.score(X_test, y_test) \nprint ('Model Accuracy:',accuracy7 * 100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Storing model accuracy to plot for comparison with other Machine Learning models.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracies['XGBoost Classifier'] = np.round(accuracy7 * 100, 2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. Plotting Confusion Matrix to describe the performance of XGBM Classifier on a set of test data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"conf_matrix(y_test, prediction7)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plotting different metrics scores for the XGBM Classifier for evaluation.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"metrics_score(cm1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Plotting the average of different metrics scores for further evaluation.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cal_score('XGBoost Classifier', xgb_model, 5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plotting Receiver Operating Characteristic (ROC) Curve, to illustrate the diagnostic ability of XGBM Classifier as its discrimination threshold is varied and showing the Area under the ROC Curve (AUC) value which will tell us how much our model is capable of distinguishing between healthy and diabetic patients.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"probs = xgb_model.predict_proba(X_test)\nprobs = probs[:, 1]\nauc7 = roc_auc_score(y_test, probs)\nroc_auc['XGB Machine Classifier']=np.round(auc7, 2)\nprint('Area under the ROC Curve (AUC): %.2f' % auc7)\nfpr7, tpr7, _ = roc_curve(y_test, probs)\nplot_roc_curve(fpr7, tpr7)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plotting Precision-Recall Curve for different thresholds of precision and recall much like the ROC Curve and showing the Area under the Precision-Recall Curve (AUCPR), it gives the number summary of the information in the Precision-Recall Curve.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"precision7, recall7, _ = precision_recall_curve(y_test, probs)\nauc_score7 = auc(recall7, precision7)\npr_auc['XGB Machine Classifier'] = np.round(auc_score7, 2)\nprint('Area under the PR Curve (AUCPR): %.2f' % auc_score7)\nplot_precision_recall_curve(recall7, precision7)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Importance:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_imp = pd.Series(gbm_model.feature_importances_,\n                        index = X_train.columns).sort_values(ascending = False)\n\nsns.barplot(x = feature_imp, y = feature_imp.index)\nplt.xlabel('Feature Important Scores')\nplt.ylabel('Features')\nplt.title(\"Feature Important Range\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 8. Light GBM","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X_st, y, test_size = 0.20, random_state = 5)\nlgbm_model = LGBMClassifier(learning_rate = 0.1, max_depth = 1, n_estimators = 200)\nlgbm_model.fit(X_train, y_train)\nprediction8 = lgbm_model.predict(X_test)\naccuracy8 = lgbm_model.score(X_test, y_test) \nprint ('Model Accuracy:',accuracy8 * 100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Storing model accuracy to plot for comparison with other Machine Learning models.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracies['LightGBM Classifier'] = np.round(accuracy8 * 100, 2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. Plotting Confusion Matrix to describe the performance of LGBM Classifier on a set of test data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"conf_matrix(y_test, prediction8)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plotting different metrics scores for the LGBM Classifier for evaluation.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"metrics_score(cm1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Plotting the average of different metrics scores for further evaluation.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cal_score('LightGBM Classifier', lgbm_model, 5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plotting Receiver Operating Characteristic (ROC) Curve, to illustrate the diagnostic ability of LGBM Classifier as its discrimination threshold is varied and showing the Area under the ROC Curve (AUC) value which will tell us how much our model is capable of distinguishing between healthy and diabetic patients.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"probs = lgbm_model.predict_proba(X_test)\nprobs = probs[:, 1]\nauc8 = roc_auc_score(y_test, probs)\nroc_auc['LightGBM Classifier'] = np.round(auc8, 2)\nprint('Area under the ROC Curve (AUC): %.2f' % auc8)\nfpr8, tpr8, _ = roc_curve(y_test, probs)\nplot_roc_curve(fpr8, tpr8)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plotting Precision-Recall Curve for different thresholds of precision and recall much like the ROC Curve and showing the Area under the Precision-Recall Curve (AUCPR), it gives the number summary of the information in the Precision-Recall Curve.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"precision8, recall8, _ = precision_recall_curve(y_test, probs)\nauc_score8 = auc(recall8, precision8)\npr_auc['LightGBM Classifier'] = np.round(auc_score8, 2)\nprint('Area under the PR Curve (AUCPR): %.2f' % auc_score8)\nplot_precision_recall_curve(recall8, precision8)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Importance:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_imp = pd.Series(gbm_model.feature_importances_,\n                        index = X_train.columns).sort_values(ascending = False)\n\nsns.barplot(x = feature_imp, y = feature_imp.index)\nplt.xlabel('Feature Important Scores')\nplt.ylabel('Features')\nplt.title(\"Feature Important Range\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 9. CatBoost","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X_st, y, test_size = 0.20, random_state = 5)\ncatboost_model = CatBoostClassifier(depth = 8, iterations = 100, learning_rate = 0.1)\ncatboost_model.fit(X_train, y_train)\nprediction9 = catboost_model.predict(X_test)\naccuracy9 = catboost_model.score(X_test, y_test) \nprint ('Model Accuracy:', accuracy9 * 100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Storing model accuracy to plot for comparison with other Machine Learning models.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracies['CatBoost Classifier'] = np.round(accuracy9 * 100, 2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. Plotting Confusion Matrix to describe the performance of CatBoost Classifier on a set of test data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"conf_matrix(y_test, prediction9)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plotting different metrics scores for the CatBoost Classifier for evaluation.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"metrics_score(cm1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Plotting the average of different metrics scores for further evaluation.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cal_score('CatBoost Classifier', catboost_model, 5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plotting Receiver Operating Characteristic (ROC) Curve, to illustrate the diagnostic ability of CatBoost Classifier as its discrimination threshold is varied and showing the Area under the ROC Curve (AUC) value which will tell us how much our model is capable of distinguishing between healthy and diabetic patients.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"probs = catboost_model.predict_proba(X_test)\nprobs = probs[:, 1]\nauc9 = roc_auc_score(y_test, probs)\nroc_auc['CatBoost Classifier']=np.round(auc9, 2)\nprint('Area under the ROC Curve (AUC): %.2f' % auc9)\nfpr9, tpr9, _ = roc_curve(y_test, probs)\nplot_roc_curve(fpr9, tpr9)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plotting Precision-Recall Curve for different thresholds of precision and recall much like the ROC Curve and showing the Area under the Precision-Recall Curve (AUCPR), it gives the number summary of the information in the Precision-Recall Curve.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"precision9, recall9, _ = precision_recall_curve(y_test, probs)\nauc_score9 = auc(recall9, precision9)\npr_auc['CatBoost Classifier'] = np.round(auc_score9, 2)\nprint('Area under the PR Curve (AUCPR): %.2f' % auc_score9)\nplot_precision_recall_curve(recall9, precision9)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Importance:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_imp = pd.Series(gbm_model.feature_importances_,\n                        index = X_train.columns).sort_values(ascending = False)\n\nsns.barplot(x = feature_imp, y = feature_imp.index)\nplt.xlabel('Feature Important Scores')\nplt.ylabel('Features')\nplt.title(\"Feature Important Range\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Performance Comparison\n\nPlotting the accuracy metric score of the machine learning models for comparison.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"models_tuned = [\n    log_model,\n    KNN_model,\n    SVC_model,\n    CART_model,\n    rf_model,\n    gbm_model,\n    catboost_model,\n    lgbm_model,\n    xgb_model]\n\nresult = []\nresults = pd.DataFrame(columns = [\"Models\",\"Accuracy\"])\n\nfor model in models_tuned:\n    names = model.__class__.__name__\n    y_pred = model.predict(X_test)\n    acc = accuracy_score(y_test, y_pred)\n    result = pd.DataFrame([[names, acc * 100]], columns = [\"Models\", \"Accuracy\"])\n    results = results.append(result)\nresults","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (15, 8))\nsns.set_palette('cividis')\nax = sns.barplot(x = list(accuracies.keys()), y = list(accuracies.values()))\nplt.yticks(np.arange(0, 100, 10))\nplt.ylabel('Percentage %', labelpad = 10)\nplt.xlabel('Algorithms', labelpad = 10)\nplt.title('Accuracy Scores Comparison', pad = 20)\nfor p in ax.patches:\n    ax.annotate(str(p.get_height()), (p.get_x(), p.get_height()), xytext = (p.get_x() + 0.3, p.get_height() + 1.02))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\nPlotting the average accuracy metric score of the machine learning models for comparison.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (15, 8))\nsns.set_palette('viridis')\nax=sns.barplot(x = list(avg_accuracies.keys()), y = list(avg_accuracies.values()))\nplt.yticks(np.arange(0, 100, 10))\nplt.ylabel('Percentage %', labelpad = 10)\nplt.xlabel('Algorithms', labelpad = 10)\nplt.title('Average Accuracy Scores Comparison', pad = 20)\nfor p in ax.patches:\n    ax.annotate(str(p.get_height()), (p.get_x(), p.get_height()),xytext=(p.get_x() + 0.3, p.get_height() + 1.02))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\nPlotting the ROC Curve of the machine learning models for comparison.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (8, 6))\nsns.set_palette('Set1')\nplt.plot(fpr1, tpr1, label = 'Linear Regression')\nplt.plot(fpr2, tpr2, label = 'KNeiihbors Classifier')\nplt.plot(fpr3, tpr3, label = 'SVM')\nplt.plot(fpr4, tpr4, label = 'Decision Tree')\nplt.plot(fpr5, tpr5, label = 'Random Forests')\nplt.plot(fpr6, tpr6, label = 'Gradient Boosting MachineC')\nplt.plot(fpr7, tpr7, label = 'XGBoost')\nplt.plot(fpr8, tpr8, label = 'LightGBM')\nplt.plot(fpr9, tpr9, label = 'CatBosst')\nplt.plot([0, 1], [0, 1], linestyle = '--')\nplt.ylabel('True Positive Rate', labelpad = 10)\nplt.xlabel('False Positive Rate', labelpad = 10)\nplt.title('Receiver Operating Characteristic (ROC) Curves', pad = 20)\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plotting the AUC values of ROC Curve of the machine learning models for comparison.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (15, 8))\nsns.set_palette('magma')\nax = sns.barplot(x = list(roc_auc.keys()), y = list(roc_auc.values()))\n#plt.yticks(np.arange(0,100,10))\nplt.ylabel('Score', labelpad = 10)\nplt.xlabel('Algorithms', labelpad = 10)\nplt.title('Area under the ROC Curves (AUC)', pad = 20)\nfor p in ax.patches:\n    ax.annotate(str(p.get_height()), (p.get_x(), p.get_height()), xytext = (p.get_x() + 0.3, p.get_height() + 0.01))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\nPlotting the PR Curve of the machine learning models for comparison.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (8, 6))\nsns.set_palette('Set1')\nplt.plot(recall1, precision1, label = 'Linear Regression PRC')\nplt.plot(recall2, precision2, label = 'KNN PRC')\nplt.plot(recall3, precision3, label = 'SVM PRC')\nplt.plot(recall4, precision4, label = 'CART PRC')\nplt.plot(recall5, precision5, label = 'Random Forests PRC')\nplt.plot(recall6, precision6, label = 'GBM PRC')\nplt.plot(recall7, precision7, label = 'XGB PRC')\nplt.plot(recall5, precision5, label = 'LGBM PRC')\nplt.plot(recall6, precision6, label = 'CatBoost PRC')\nplt.ylabel('Precision', labelpad = 10)\nplt.xlabel('Recall', labelpad = 10)\nplt.title('Precision Recall Curves', pad = 20)\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plotting the AUC values of PR Curve of the machine learning models for comparison.","execution_count":null},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (15, 8))\nsns.set_palette('mako')\nax = sns.barplot(x = list(pr_auc.keys()), y = list(pr_auc.values()))\nplt.ylabel('Score', labelpad = 10)\nplt.xlabel('Algorithms', labelpad = 10)\nplt.title('Area under the PR Curves (AUCPR)', pad = 20)\nfor p in ax.patches:\n    ax.annotate(str(p.get_height()), (p.get_x(), p.get_height()), xytext = (p.get_x() + 0.3, p.get_height() + 0.01))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Final Model:","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Random Forests:\n\n\nA Random Forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X_st, y, test_size = 0.20, random_state = 5)\nrf_model = RandomForestClassifier(max_features = 8, min_samples_split = 12, n_estimators = 120)\nrf_model.fit(X_train, y_train)\nprediction5 = rf_model.predict(X_test)\naccuracy5 = rf_model.score(X_test, y_test) \nprint ('Model Accuracy:',accuracy5 * 100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. Plotting Confusion Matrix to describe the performance of Random Forest Classifier on a set of test data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"conf_matrix(y_test, prediction5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plotting different metrics scores for the Random Forest Classifier for evaluation.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"metrics_score(cm1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Plotting the average of different metrics scores for further evaluation.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cal_score('Random Forests', rf_model, 10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# - - - -  REPORTING  - - - -\n\n\n\n### Our aim in this study was to estimate the probability of diabetes disease by using different classification models on the 'diabetes' data set.\n\n### First the data set read and displayed.\n\n###  Missing values were filled with the median values of the variables in which they were found.\n\n###  Then outliers were detected and suppressed.\n\n### Then, the values in the variables were divided into groups according to general health standards and new variables were created.\n\n### Values in all variables are standardized from 0 to 1.\n\n### Then results were with 9 different classification models predicted.\n\n### Predictions were evaluated with different metrics. Alle results were visualisated.\n\n### Finally, an prediction of over 91% was achieved with the random forests model.\nl","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}