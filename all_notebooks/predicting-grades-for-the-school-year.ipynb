{"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3","language":"python"},"language_info":{"file_extension":".py","name":"python","codemirror_mode":{"version":3,"name":"ipython"},"version":"3.6.3","mimetype":"text/x-python","nbconvert_exporter":"python","pygments_lexer":"ipython3"}},"nbformat":4,"nbformat_minor":1,"cells":[{"metadata":{"_uuid":"bc0944c5f79f6eb5876672c2d1d2cc846676515d","_cell_guid":"68c5949d-499f-4e00-ae16-e32f47011844"},"source":"<h1 style=\"text-align:center\"> Predicting Grades for the School Year (Math Subject) </h1>","cell_type":"markdown"},{"metadata":{"_uuid":"513192e96c251da88fab0f9dcdcc7085f6f34d99","_cell_guid":"8c8faf4d-8e5f-4522-8fbb-c5d2b72fc80a"},"source":"# Introduction: \nIn this small project I implement a simple algorithmic model that predicts the score of an individual students at he end of the year. \"G3\" or the final grade will be our label (output) and the rest of the columns will be our features (inputs). First, I will explore the data to see if we can get a grasp of what is the story behind the data. I'm not looking for to just implementing a linear regression or random forest regression algorithm just to get the score. My aim is to understand what the data is telling us through visualizations (plotly, matplotlib, seaborn). One last thing, Im new to plotly so please excuse me for my simple graphs, I'm in the process of learning this new visualization tool. Have fun and I'm open to constructive criticisms that will make this project more effective and interesting. ","cell_type":"markdown"},{"metadata":{"_uuid":"cc2d6c62858c9ae5f28503c9286128dc4f3f2173","_cell_guid":"e195bbdf-7d3b-489e-966f-6ef228111d32"},"source":"# Outline of the Project: <br>\n1) **Extract the Data and Gather General Information of the Dataset** <br>\n2)** Visualize the three outputs [\"G1\", \"G2\" and \"G3]** <br>\na) We will see how the data is distributed.<br>\nb) Gain some insights about the three grades. (Distribution Plots)<br>\nc) We will finally use \"G3\" as our output for our linear regression algorithm since it is the final and most important grade.<br>\n3) **Data Structuring:** <br>\na) Drop the G1 and G2 columns. <br>\nb) Transform some of the columns into binary columns for future analysis. <br>\nc) Split the data (Training and Testing sets.)<br>\nd) Implement StartiefiedShuffleSplit to the two most important features in terms of correlation with G3. <br>\ne) Create a binary column and a criteria determining what score is a Failing and what score is a Passing Grade. <br>\n4) **Data Analysis and Visualization** <br>\na) Students that passed and failed the course (Using Plotly) (%). <br>\nb) How did students perform by gender (Using Plotly). (%) <br>\nc) Correlation Analysis.  <br>\nd) Number of Absences throughout the Course (Using Plotly) <br>\ne) Further Analysis. <br>\n5) ** Data Cleaning (Preparing the Data for our Algorithm) ** <br>\na) Split the data into Numeric and Categorical Values.<br>\nb) Scale the Numeric and Categorical columns using StandardScaler in order to fit the data into our algorithms. <br>\nc) Select the best algorithm (Better score and accuracy) Make sure its not overfitting! <br>\nd) Select the best hyperparameters by using GridSearchCV <br>\n6) **Conclusion**","cell_type":"markdown"},{"execution_count":null,"metadata":{"_uuid":"229144f12b0f091777bc54ac98b371c4a4d81738","_cell_guid":"be047970-3092-4ca3-a323-046510c8dc88","collapsed":true},"source":"import pandas as pd\nimport numpy as np\nimport plotly\nfrom plotly import tools\n\n\ndf = pd.read_csv(\"../input/student-mat.csv\")\ndf.head()\n","outputs":[],"cell_type":"code"},{"metadata":{"_uuid":"9bc0ac9ee8c5c2b02c951c9b9997a326a1c3a0b3","_cell_guid":"cbb7ec11-128e-4672-9939-fcae9a6eee4b"},"source":"# Information about the Variables: \nAttribute Information:\n\n# Attributes for both student-mat.csv (Math course) and student-por.csv (Portuguese language course) datasets: \n\n1 school - student's school (binary: 'GP' - Gabriel Pereira or 'MS' - Mousinho da Silveira) <br>\n2 sex - student's sex (binary: 'F' - female or 'M' - male) <br>\n3 age - student's age (numeric: from 15 to 22) <br>\n4 address - student's home address type (binary: 'U' - urban or 'R' - rural)<br> \n5 famsize - family size (binary: 'LE3' - less or equal to 3 or 'GT3' - greater than 3) <br>\n6 Pstatus - parent's cohabitation status (binary: 'T' - living together or 'A' - apart) <br>\n7 Medu - mother's education (numeric: 0 - none, 1 - primary education (4th grade), 2 â€“ 5th to 9th grade, 3 â€“ secondary education or 4 â€“ higher education) <br>\n8 Fedu - father's education (numeric: 0 - none, 1 - primary education (4th grade), 2 â€“ 5th to 9th grade, 3 â€“ secondary education or 4 â€“ higher education)<br> \n9 Mjob - mother's job (nominal: 'teacher', 'health' care related, civil 'services' (e.g. administrative or police), 'at_home' or 'other') <br>\n10 Fjob - father's job (nominal: 'teacher', 'health' care related, civil 'services' (e.g. administrative or police), 'at_home' or 'other') <br>\n11 reason - reason to choose this school (nominal: close to 'home', school 'reputation', 'course' preference or 'other') <br>\n12 guardian - student's guardian (nominal: 'mother', 'father' or 'other')<br> \n13 traveltime - home to school travel time (numeric: 1 - <15 min., 2 - 15 to 30 min., 3 - 30 min. to 1 hour, or 4 - >1 hour) <br>\n14 studytime - weekly study time (numeric: 1 - <2 hours, 2 - 2 to 5 hours, 3 - 5 to 10 hours, or 4 - >10 hours) <br>\n15 failures - number of past class failures (numeric: n if 1<=n<3, else 4) <br>\n16 schoolsup - extra educational support (binary: yes or no) <br>\n17 famsup - family educational support (binary: yes or no) <br>\n18 paid - extra paid classes within the course subject (Math or Portuguese) (binary: yes or no) <br>\n19 activities - extra-curricular activities (binary: yes or no) <br>\n20 nursery - attended nursery school (binary: yes or no) <br>\n21 higher - wants to take higher education (binary: yes or no) <br>\n22 internet - Internet access at home (binary: yes or no) <br>\n23 romantic - with a romantic relationship (binary: yes or no) <br>\n24 famrel - quality of family relationships (numeric: from 1 - very bad to 5 - excellent) <br>\n25 freetime - free time after school (numeric: from 1 - very low to 5 - very high) <br>\n26 goout - going out with friends (numeric: from 1 - very low to 5 - very high) <br>\n27 Dalc - workday alcohol consumption (numeric: from 1 - very low to 5 - very high) <br>\n28 Walc - weekend alcohol consumption (numeric: from 1 - very low to 5 - very high) <br>\n29 health - current health status (numeric: from 1 - very bad to 5 - very good) <br>\n30 absences - number of school absences (numeric: from 0 to 93) <br>\n\n# These grades are related with the course subject, Math or Portuguese: <br>\n31 G1 - first period grade (numeric: from 0 to 20) <br>\n31 G2 - second period grade (numeric: from 0 to 20) <br>\n32 G3 - final grade (numeric: from 0 to 20, output target)<br>","cell_type":"markdown"},{"execution_count":null,"metadata":{"_uuid":"11633f31c505c1acd6b459f61a8cc9ca0648d75d","_cell_guid":"e2c7b809-0c47-4d18-88b1-efb65a008a5f","collapsed":true},"source":"%%javascript\nIPython.OutputArea.prototype._should_scroll = function(lines) {\n    return false;\n}","outputs":[],"cell_type":"code"},{"metadata":{"_uuid":"b50b29eab9d473d25bb7450b0a71ebd5ff9b05b6","_cell_guid":"5e1c444d-e04e-45a3-87f2-4a48b1f6d0eb"},"source":"","cell_type":"markdown"},{"metadata":{"_uuid":"14551657c923c0e382eba29d0d22f253fe7f4e58","_cell_guid":"468b696f-9d33-41df-99b4-521c2ac8b594"},"source":"# Exploring the Information of the DataFrame\n## Why Explore the DataFrame Information?\n1) To see if we have any null values in the dataset. As you can see there are 395 rows in each of the columns so that means there are no null values. <br>\n2) It is important to know if we have any null values because in case we do have we have to find a way to fill the null values normally we do this with the (\"mean\", \"median\", \"mode\" or we just drop the rows that contain null values.) Nomally I do this by using sklearn Imputer.","cell_type":"markdown"},{"metadata":{"_uuid":"86d1288bf8d8857f8c4183ca8ea7dcbb04d4b9f3","_cell_guid":"b70d28c3-a97f-424d-8c4b-a7675797cf40"},"source":"<img src=\"https://media.giphy.com/media/1pF0wNxRjHbk4/giphy.gif\">","cell_type":"markdown"},{"execution_count":null,"metadata":{"_uuid":"b6d368b6174547b25c4a175dc712347e1f796d5f","_cell_guid":"230b571c-e821-46be-88c9-c65020c2cc6b","collapsed":true},"source":"df.info()","outputs":[],"cell_type":"code"},{"execution_count":null,"metadata":{"_uuid":"11909e877f4fc776725d8d919830dacec4c92ea6","_cell_guid":"0bb5b2c9-31b4-48db-beab-b509d64c3cc5","collapsed":true},"source":"# Descriptive Data\ndf.describe()","outputs":[],"cell_type":"code"},{"metadata":{"_uuid":"dc3cb2ce1288eca4701bdae5b2c26a25746246a6","_cell_guid":"fd1d063a-34ea-465b-9823-0a130eca6b32"},"source":"<h1 style=\"text-align:center\">Let's Explore some of Our Main Data:</h1><br>\nNow we will just see some of our main data and see what data insights can we get from our exploration. However, it is important to state that for our linear regression algorithm we will drop G1 and G2 since they have a high impact in what the result of G3 will be. We want our model to discover what other features have a high impact in determining that a student will either pass or fail.","cell_type":"markdown"},{"metadata":{"_uuid":"ab1c2ffbcb202a283823393ba791359fac9b5aaf","_cell_guid":"0d5db57a-2d0e-496c-a41d-288c8f71fc03"},"source":"<img src=\"https://media.giphy.com/media/l8LTENNory6PK/giphy.gif\">","cell_type":"markdown"},{"metadata":{"_uuid":"7afbf8e6c2828b4973c00e3ad2d0645301186b16","_cell_guid":"43961182-0a43-4549-a84a-9ba8f26837bc"},"source":"# Let's See How all our Data is Distributed:","cell_type":"markdown"},{"execution_count":null,"metadata":{"_uuid":"f2ab692c58e669f26c96b99459a396871ee6b973","_cell_guid":"480ebfa9-9dde-47aa-acc6-ddd4240c51dd","collapsed":true},"source":"import matplotlib.pyplot as plt\n\ndf.hist(bins=50, figsize=(20,15), color='r')\nplt.show()","outputs":[],"cell_type":"code"},{"metadata":{"_uuid":"46fe70bc9194f1c9da945a3a9898951df7453d28","_cell_guid":"4f69d400-8329-48c5-9be8-7b9926560844"},"source":"## Distribution Plots with Plotly: \nThe distribution plots is a great visual way to see how many students passed in G1 [First Period], G2 [Second Period], and <br>\nG3 [Last Period]. \n\n## Failing Grade: \nIn our case we are assuming that everything lower than 12 is a \"Failing Grade\". If the grade is greater or equal to 12 then this will be a \"Passing\" grade. \n\n## What can we gather from this Data?\n1) Did students that failed in the first period improved in the second and third period? <br>\n2) How many students got each grade and what grade did most of the students got.<br>\n3) In which of the periods did students failed the most.","cell_type":"markdown"},{"execution_count":null,"metadata":{"_uuid":"92ce3fb5ffd35862fe4c3e43fc3ffe3351676ed7","_cell_guid":"e3c0697b-7654-4bd3-9a49-eb193a99b12d","collapsed":true},"source":"# import cufflinks as cf\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, iplot\nfrom plotly.graph_objs import *\ninit_notebook_mode()\n\n# cf.set_config_file(offline=False, world_readable=True, theme='pearl')\n\nx0 = df[\"G1\"]\nx1 = df[\"G2\"]\nx2 = df[\"G3\"]\n\nFirst_Period = Histogram(\n    x=x0,\n    name=\"First Semester\",\n    text=\"Grades\",\n    marker= dict(\n        color='#F79F81',\n    )\n)\n\nSecond_Period = Histogram(\n    x=x1,\n    name=\"Second Semester\",\n    text=\"Grades\",\n    marker= dict(\n        color='#9FF781',\n    )\n)\n\nThird_Period = Histogram(\n    x=x2,\n    name=\"Third Semester\",\n    text=\"Grades\",\n    marker= dict(\n        color='#CED8F6',\n\n    )\n)\n\ndata = [First_Period, Second_Period, Third_Period]\nlayout = Layout(barmode='stack',\n                  title=\"Distribution of Student's Grades\",\n                   font=dict(size=16),\n                  xaxis=dict(\n                  title=\"Grades\"\n                  ),\n                  yaxis=dict(\n                  title=\"Number of Students\"))\n\nfig = dict(data=data, layout=layout)\niplot(fig)\n\n","outputs":[],"cell_type":"code"},{"execution_count":null,"metadata":{"_uuid":"6e28fe2bec2e76e59bbbaf1aaec473fad0c2d522","_cell_guid":"f7fdc16a-ceba-4891-98bf-bd0c84955302","collapsed":true},"source":"import plotly.plotly as py\nimport plotly.figure_factory as ff\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, iplot\nfrom plotly.graph_objs import *\nimport numpy as np\ninit_notebook_mode()\n\n\n\n# Add histogram data\nx1 = df['G1'].values.tolist() \nx2 = df['G2'].values.tolist()  \nx3 = df['G3'].values.tolist()    \n\n\n# Group data together\nhist_data = [x1, x2, x3]\n\ngroup_labels = ['First Semester', 'Second Semester', 'Third Semester']\n\ncolors = [\"#F79F81\", \"#9FF781\", \"#2E64FE\"]\n\n# # Create distplot with custom bin_size\nfig = ff.create_distplot(hist_data, group_labels, bin_size=3, curve_type='normal', colors=colors)\nfig['layout'].update(title='Distplot with Normal Distribution', font=dict(size=16))\n\n# # Plot!\niplot(fig)","outputs":[],"cell_type":"code"},{"metadata":{"_uuid":"1762d8738091c54cea396b5ce3c7cbe9272cf231","_cell_guid":"601a6890-b4fb-4e4c-9345-c2394368d6de"},"source":"<h1 style=\"text-align:center\">Data Cleaning</h1><br>\n<img src=\"https://media.giphy.com/media/ffd0F6WNcRJMQ/giphy.gif\">","cell_type":"markdown"},{"execution_count":null,"metadata":{"_uuid":"b7c66a45c47b43145c66d9af31c2cc21f4579171","_cell_guid":"64eee28c-6519-44ed-8403-1db0cffb050e","collapsed":true},"source":"# we will drop those columns to make it more hard to predict and see which attributes impact the most on the final grade.\ndf.drop(['G1', 'G2'], axis=1, inplace=True)","outputs":[],"cell_type":"code"},{"execution_count":null,"metadata":{"_uuid":"643668f1e0255a0c05f6d91d9e97d9e55d4e1b53","_cell_guid":"fa95a7d8-b5c9-4acd-8c5d-5f59d5022a99","collapsed":true},"source":"# Let's not forget there is also a famsup column that is in our dataframe. It tells us if the student need extra educational support.\ndf.columns\ndf['famsup'].head()","outputs":[],"cell_type":"code"},{"metadata":{"_uuid":"ffb6b8725ea3fd743bfbefdbb904b362657c9835","_cell_guid":"eca93819-3f72-4002-85c4-44cc027e3796"},"source":"# Dropping Columns that are not highly correlated <br>\n<font size='4'>\n<b>1)</b> Here we will find out the columns that are highly correlated (either positively or negatively) in order to leave it in our dataframe. <br><br>\n<b>2)</b> We should have a criteria in order to retain the columns. Let's say if the column is correlated more than 8% to G3 we retain the column for further analysis.<br><br>\n<b>3)</b> Some columns that are not highly correlated might stay to see if we could develop new features by combining 2 or more columns.<br><br>\n<b>4)</b> We will drop the columns G1 and G2 since we already know this is highly indicative of what G3 will be. We want to find out other features that could have a positive impact <br><br>\n<b>5)</b> The columns that are in binary format we will convert it into numeric in a new column to see if they have a high correlation with G3. </font>","cell_type":"markdown"},{"execution_count":null,"metadata":{"_uuid":"5bf84241ceb97b905bcd78a50070ed61969b67f1","_cell_guid":"63505e8a-2c6f-4ab9-9027-3981f43c901f","collapsed":true},"source":"# 0 stands for U and 1 stands for R. [U=Urban, R=Rural]\n# Here we will convert all the binary columns to integers.\ndf['b_address'] = df['address'].apply(lambda x: 0 if x == 'U' else 1)\ndf['b_address'].value_counts()\n# Interestingly there are more students in families that are greater than 3.\n# Could it be possible that all family members are in the same school? This might be a reason why it is higher.\n# LE3 = Less than 3. [0], GE3 = Greater than 3.[1]\ndf['b_famsize'] = df['famsize'].apply(lambda x: 0 if x == 'LE3' else 1)\ndf['b_famsize'].value_counts()\n# T = Parents are living together [0], A = Parents living apart. [1]\ndf['b_Pstatus'] = df['Pstatus'].apply(lambda x: 0 if x == 'T' else 1)\ndf['b_Pstatus'].value_counts()\n# 0 = no and 1 = yes\ndf['b_famsup'] = df['famsup'].apply(lambda x: 0 if x == 'no' else 1)\ndf['b_famsup'].value_counts()\n# 0 = no and 1 = yes\n# This is an interesting column when it comes to having a positive effect on G3.\ndf['b_paidxtraclasses'] = df['paid'].apply(lambda x: 0 if x == 'no' else 1)\ndf['b_paidxtraclasses'].value_counts()\n# 0 = no and 1 = yes\ndf['b_xtraactivities'] = df['activities'].apply(lambda x: 0 if x == 'no' else 1)\ndf['b_xtraactivities'].value_counts()\n# 0 = no and 1 = yes\n# It has a high correlation however, we only have 20 students that are not interested in having a high education and \n# thus this column should not be taken into consideration.\ndf['b_higher_education'] = df['higher'].apply(lambda x: 0 if x == 'no' else 1)\n# continue with the analisis.\ndf['b_internet'] = df['internet'].apply(lambda x: 0 if x == 'no' else 1)\n# Interestingly when people are not in a romantic relationship they tend to get better grades.\ndf['b_romantic'] = df['romantic'].apply(lambda x: 0 if x == 'no' else 1)\n\ndf['b_nursery'] = df['nursery'].apply(lambda x: 0 if x == 'no' else 1)\n\ndf['b_guardian'] = df['guardian'].apply(lambda x: 0 if x == 'mother' else (1 if x=='father' else 2))\n# Does not have any effect on G3. Low correlation.\ndf['b_reason'] = df['reason'].apply(lambda x: 0 if x == 'home' else (1 if x=='reputation' else (3 if x=='course' else 4)))\n# Does not have any effect on G3. Low correlation.\ndf['b_school'] = df['school'].apply(lambda x: 0 if x == 'GP' else 1)\n\ndf['b_schoolsup'] = df['schoolsup'].apply(lambda x: 0 if x == 'no' else 1)","outputs":[],"cell_type":"code"},{"metadata":{"_uuid":"94552a3f631b07655732c2543f314a50ae5e984c","_cell_guid":"c6b35ce1-a62e-46ed-905d-6e5207a10ec1"},"source":"# What binary columns influenced the G3 grade:<br>\n<font size='4'> \n<b>1)</b> Higher education [higher]: Students that were aiming in having a higher education tended to get better results in the final grade. However, we have to take into consideration that only 20 students were not aiming for higher education. Nevertheless, this feature can give us a hint as to whether a student will get higher grades or not. <br><br>\n<b>2)</b> Paid extra Classes [paid]: These are the extra paid classes within the course sibject. It has a positive correlation whichmeans the more the students took the extra classes the more likely they were to pass. <br><br>\n<b>3)</b> People in a relationship [romantic]: The people that were not in a romantic relationship tended to get higher grades than the ones that were in a relationship.</font>\n","cell_type":"markdown"},{"execution_count":null,"metadata":{"_uuid":"95fcef86b2ca55550eaec66f467b3070a29ea93f","_cell_guid":"2b1a0f0b-fad4-4c5c-bde6-8b1342431c8e","collapsed":true},"source":"new_corr = df.corr()\nnew_corr['G3'].sort_values(ascending=False)","outputs":[],"cell_type":"code"},{"metadata":{"_uuid":"fc4b1ac91829777ac39d4a3fd0c02a5cee051081","_cell_guid":"c8a3632d-c7c0-4bdf-b343-869a89c79686"},"source":"# Now let's drop the Columns that we don't need or that dont have a high correlation with G3. \n## Note: I didn't drop absences from our column because I believe it has a huge impact on G3. The more absences the more likely you are to fail. The other columns I dropped because they tend to be redundant since there are other columns that are somehow similar. ","cell_type":"markdown"},{"execution_count":null,"metadata":{"_uuid":"f54bc8905b8d82c970a01296f248adea128b3100","_cell_guid":"03162213-553b-4422-a675-d4a83b552bac","collapsed":true},"source":"df = df.drop(['school', 'b_school', 'address', 'famsize', 'b_famsize', 'Pstatus', 'reason', 'b_reason',\n        'guardian', 'b_guardian', 'famsup', 'b_famsup', 'paid', 'activities', 'b_xtraactivities', 'nursery', 'b_nursery',\n        'higher', 'internet', 'romantic', 'famrel', 'freetime'], axis=1)","outputs":[],"cell_type":"code"},{"execution_count":null,"metadata":{"_uuid":"90226457d460cf9980e3c61625029d00cc76694d","_cell_guid":"d4b39a33-46d6-4967-bdc7-dd38eabb9124","collapsed":true},"source":"#We have reduced the amount of columns from 33 to 22. Making our Dataframe much simpler and hopefully we will avoid overfitting.\ndf.shape","outputs":[],"cell_type":"code"},{"execution_count":null,"metadata":{"_uuid":"248eb478fee1bfd00b530718cf53a93b511603cc","_cell_guid":"cbc38c63-87b8-4ec7-8466-aacc6cef1390","collapsed":true},"source":"# Rename the columns\ndf = df.rename(columns={'b_address': 'address', 'b_paidxtraclasses': 'paid_classes', 'b_higher_education': 'higher_education',\n          'b_internet': 'internet_availability', 'b_romantic': 'relationship', 'b_schoolsup': 'educational_support'})\n\ndf.columns","outputs":[],"cell_type":"code"},{"metadata":{"_uuid":"aacdb2a44fb2f9848b668c4e873f655439510fe7","_cell_guid":"2a49ca54-ae34-48a4-b210-8c5a72652e42"},"source":"<h1 style=\"text-align:center\"> Split the Data: </h1>\n<img src=\"https://media.giphy.com/media/xTiTnxpQ3ghPiB2Hp6/giphy.gif\">[](http://)","cell_type":"markdown"},{"execution_count":null,"metadata":{"_uuid":"af0ca8300eec9fcd6361e1d81442ffcd1f8a846d","_cell_guid":"bf09d650-86da-4389-86f2-ef38051eb885","collapsed":true},"source":"# Simple way to split our data into Training and testing.\n# We use random_state so that everytime we run the data we get the exact same split of the data. We don't want the training and\n# testing everytime we run the data.\nfrom sklearn.model_selection import train_test_split\n\ntrain_set, test_set = train_test_split(df, test_size=0.2, random_state=42)\n\nprint(len(train_set), 'Train', len(test_set))","outputs":[],"cell_type":"code"},{"execution_count":null,"metadata":{"_uuid":"2ffac717d70e390ceac3a85589fb67a5f7fcb8bb","_cell_guid":"8042fa77-9dc4-49c4-8f35-7126f0964043","collapsed":true},"source":"correlations = df.corr()\ncorrelations['G3'].sort_values(ascending=False)","outputs":[],"cell_type":"code"},{"metadata":{"_uuid":"5d2c0cf8b312b9c30de7b8d3986997d33f053158","_cell_guid":"fcad98d4-bd1f-4fa1-acf6-243ca5a07520"},"source":"# StratifiedShuffleSplit:<br>\n<font size='4'> We will use this sklearn function to have our two variables that impact the most the \"G3\" label be equally distributed in our train and test sets. </font><br>\nHere we learn that the highest correlations or the attributes that impact the most the final grade (G3) are \"Medu\" (Education of the Mother) [Positive Correlation] and \"failures[negative correlation]\". ","cell_type":"markdown"},{"execution_count":null,"metadata":{"_uuid":"84fb00d9a37dd1055a7cd7c6d84eba5e07534481","_cell_guid":"8f6ccca1-5d1a-4bde-b000-68803984b3ec","collapsed":true},"source":"# Code from HandsOn Machine Learning with ScikitLearn and Tensorflow by Aurélien Geron.\nfrom sklearn.model_selection import StratifiedShuffleSplit\n\nsplit = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\nfor train_index, test_index in split.split(df, df[\"Medu\"]):\n    strat_train_set = df.loc[train_index]\n    strat_test_set = df.loc[test_index] \n\nfor train_index, test_index in split.split(df, df[\"failures\"]):\n    strat_train_set = df.loc[train_index]\n    strat_test_set = df.loc[test_index]  ","outputs":[],"cell_type":"code"},{"execution_count":null,"metadata":{"_uuid":"e05ad52f67046fd36daa93d344ea694e7e9aef63","_cell_guid":"e1492022-9f80-412f-96df-9f6d81ed67b5","collapsed":true},"source":"# We want to equally distribute in our training and test set the failures values since it is the highest correlated with G3.\n# It is more convenient to use StratifiedShuffleSplit on columns that are highly correlated and equally distributed it tends to \n# vary less.\nprint(df['Medu'].value_counts()/len(df))\nprint(df['failures'].value_counts()/len(df))","outputs":[],"cell_type":"code"},{"metadata":{"_uuid":"635065fc68a62325acdaa81f2e3f1ddaf34e6b6d","_cell_guid":"cf14371b-af1f-4a69-8f8c-6a89dbd19af1"},"source":"# Let's Create a Column that tells if the Student Passed or Fail \n## Our criteria is x < 12 = Failed , x > 12 = Passed","cell_type":"markdown"},{"metadata":{"_uuid":"f6c3338773fc79b768198811fc05f77fab64175b","_cell_guid":"98ddb84b-51c8-4b77-ac2f-41cdab87e397"},"source":"","cell_type":"markdown"},{"execution_count":null,"metadata":{"_uuid":"1f02f189656c2ae5318e14d9bb96c7ec138093c5","_cell_guid":"35c63f86-7f96-4427-9ae3-385429a85b0b","collapsed":true},"source":"df['grade_status'] = df['G3'].apply(lambda x: 'Fail' if x < 12 else 'Pass')\ndf['grade_status'].value_counts()","outputs":[],"cell_type":"code"},{"metadata":{"_uuid":"9099ef92209bf89f683a55c64859177315171914","_cell_guid":"7b78205b-e266-4667-ab51-6fc5bdc556c4"},"source":"<h1 style=\"text-align:center\">Now we will Discover and Visualize Data </h1>\n<img src=\"https://media.giphy.com/media/rsii9v51Eq6eQ/giphy.gif\">","cell_type":"markdown"},{"execution_count":null,"metadata":{"_uuid":"de053e74f376d29ef5f4bb91313e26d1b34e1a7a","_cell_guid":"58242bfb-6811-49af-bc36-5c5025c54f16","collapsed":true},"source":"# A criteria to determine if the student failed or passed.\ndf['grade_status'] = df['G3'].apply(lambda x: 'Fail' if x < 12 else 'Pass')\ndf['grade_status'].value_counts()\n\n# Can we combine any further attributes in order to add it to our dataframe?\n# Move the grade_status and G3 columns to the 1st and 2nd columns just for visualization.\nlst_c = df.columns.tolist()\n\ndf = df[\n['G3',\n 'grade_status',\n 'sex',\n 'age',\n 'Medu',\n 'Fedu',\n 'Mjob',\n 'Fjob',\n 'traveltime',\n 'studytime',\n 'failures',\n 'schoolsup',\n 'goout',\n 'Dalc',\n 'Walc',\n 'health',\n 'absences',\n 'address',\n 'paid_classes',\n 'higher_education',\n 'internet_availability',\n 'b_Pstatus',\n 'relationship',\n 'educational_support']\n]","outputs":[],"cell_type":"code"},{"metadata":{"_uuid":"544a00efd25a30ecc36fe068582897d07f4f9c63","_cell_guid":"4b60fb14-10f5-4a68-849b-bd38e7f39427"},"source":"# What percentage of students failed and passed?\n\n## Why would we want to know how many students passed or fail?\n1) We want to know if there needs an improvement in the method of education at both Portuguese schools. <br>\n2) Is the gap between students that passed and failed close enough?","cell_type":"markdown"},{"execution_count":null,"metadata":{"_uuid":"d8401137f8fa41731bb4120370e7d29121ac1e82","_cell_guid":"04dbe4c5-f33b-49f0-8dde-c774d97096d1","collapsed":true},"source":"from plotly.offline import download_plotlyjs, init_notebook_mode, iplot\nfrom plotly.graph_objs import *\ninit_notebook_mode()\n\nlabels = ['Failed','Passed']\nF = df['grade_status'].value_counts()[0]\nP = df['grade_status'].value_counts()[1]\nvalues = [F, P]\ncolors = ['#FA5858', '#01DF3A']\n\ntrace = Pie(labels=labels, values=values,\n               hoverinfo='label+percent', textinfo='value', \n               textfont=dict(size=20),\n               marker=dict(colors=colors, \n                           line=dict(color='#282828', width=3)))\n\ndata = [trace]\nlayout = Layout(\n    title='Students that Failed and Passed the Course',\n    font=dict(size=20)\n)\n\n\nfig = dict(data=data, layout=layout)\niplot(fig)","outputs":[],"cell_type":"code"},{"execution_count":null,"metadata":{"_uuid":"01cafac03953c81dd7200346f3f29a975728b867","_cell_guid":"b15a7aea-d859-461c-a32b-9d770a52febb","collapsed":true},"source":"cross_sex = pd.crosstab(df['sex'], df['grade_status']).apply(lambda x: x/x.sum() * 100, axis=1)\ncross_sex.iloc[1][0]","outputs":[],"cell_type":"code"},{"execution_count":null,"metadata":{"_uuid":"cdfa6b7ad7d01a4a158226c2187f746527e95281","_cell_guid":"e4fc3d06-b2a6-45ae-8aba-bd3c44b5b12f","collapsed":true},"source":"from plotly.offline import download_plotlyjs, init_notebook_mode, iplot\nfrom plotly.graph_objs import *\ninit_notebook_mode()\n\nF_Failed = cross_sex.iloc[0][0]\nF_Passed = cross_sex.iloc[0][1]\nM_Failed = cross_sex.iloc[1][0]\nM_Passed = cross_sex.iloc[1][1]\n\ncolors = [\"#FA5858\", \"#81F781\"]\n\nfig = {\n  \"data\": [\n    {\n      \"values\": [F_Failed, F_Passed],\n      \"labels\": [\n          \"Failed\",\n          \"Passed\"\n      ],\n      \"domain\": {\"x\": [0, .48]},\n      \"name\": \"Females\",\n        \"marker\": {\"colors\": colors},\n      \"hoverinfo\":\"label+percent+name\",\n      \"hole\": .4,\n      \"type\": \"pie\"\n    },     \n    {\n      \"values\": [M_Failed, M_Passed],\n      \"labels\": [\n          \"Failed\",\n          \"Passed\"\n      ],\n      \"text\":\"Males\",\n      \"textposition\":\"inside\",\n      \"domain\": {\"x\": [.52, 1]},\n      \"name\": \"Males\",\n        \"marker\": {\"colors\": colors},\n      \"hoverinfo\":\"label+percent+name\",\n      \"hole\": .4,\n      \"type\": \"pie\"\n    }],\n  \"layout\": {\n        \"title\":\"Passing School by Gender\",\n      \"font\": dict(size=20),\n        \"annotations\": [\n            {\n                \"font\": {\n                    \"size\": 16\n                },\n                \"showarrow\": False,\n                \"text\": \"Females\",\n                \"x\": 0.185,\n                \"y\": 0.5\n            },\n            {\n                \"font\": {\n                    \"size\": 16\n                },\n                \"showarrow\": False,\n                \"text\": \"Males\",\n                \"x\": 0.8,\n                \"y\": 0.5\n            }\n        ]\n    }\n}\n\niplot(fig)","outputs":[],"cell_type":"code"},{"metadata":{"_uuid":"f8782d9f19cb86df6b834bbc72a0943e1948ee62","_cell_guid":"b0c71d25-eb47-4f89-be4c-005aafe30a12"},"source":"# What can we learn from both Charts?:\n1) There are more students that failed rather than passed.<br>\n2) Females had a wider gap in failing the Math program than did Males.<br>\n\n# What to do Next?\n1) Maybe we should think about what possible features are making the girls to fail. Could it be that girl students are in a relationship and maybe that relationship is somehow negaively impacting the performance of Females?","cell_type":"markdown"},{"execution_count":null,"metadata":{"_uuid":"9b7f09c9c1921e8f11fea1664aac9e03eb61ce1b","_cell_guid":"496cc145-67e9-43a3-b782-b992ea50457d","collapsed":true},"source":"# 166 students are female.\ndf['sex'].value_counts()\n\n# Number of female students in a relationship\n# 67 female students are in a relationship\n# Roughly 40% of the total female students are in a relationship.\ndf.loc[(df['sex'] == 'F') & df['relationship'] == 1]\n\ndf.head()","outputs":[],"cell_type":"code"},{"execution_count":null,"metadata":{"_uuid":"0bfdd9b897944148ecf6ba215189343a1f2833de","_cell_guid":"b8fbf670-59d2-437e-bd66-f5cd3579c9ca","collapsed":true},"source":"from plotly.offline import download_plotlyjs, init_notebook_mode, iplot\nfrom plotly.graph_objs import *\ninit_notebook_mode()\n\nrelationship_females = df.loc[(df['sex'] =='F') & (df['relationship'] == 1)]\nrelationship_males = df.loc[(df['sex'] =='M') & (df['relationship'] == 1)]\n\nclass Relationship:\n        \n    def __init__(self, relationship_females):\n        self.relationship_females = relationship_females\n        self.relationship_males = relationship_males\n    \n    def females_failed(self):\n        cross_grades_f = pd.crosstab(self.relationship_females['sex'], self.relationship_females['grade_status'])\n        girls_in_relationship = cross_grades_f.iloc[0][0] + cross_grades_f.iloc[0][1] \n        girls_failed = (cross_grades_f.iloc[0][0]/ girls_in_relationship) * 100 \n        return girls_failed\n    \n    def females_passed(self): \n        cross_grades_f = pd.crosstab(self.relationship_females['sex'], self.relationship_females['grade_status'])\n        girls_in_relationship = cross_grades_f.iloc[0][0] + cross_grades_f.iloc[0][1] \n        girls_passed = (cross_grades_f.iloc[0][1]/ girls_in_relationship) * 100\n        return girls_passed\n    \n    def males_failed(self):\n        cross_grades_m = pd.crosstab(self.relationship_males['sex'], self.relationship_males['grade_status'])\n        boys_in_relationship =  cross_grades_m.iloc[0][0] + cross_grades_m.iloc[0][1]\n        boys_failed = (cross_grades_m.iloc[0][0]/boys_in_relationship) * 100\n        boys_passed = (cross_grades_m.iloc[0][1]/boys_in_relationship) * 100\n        return boys_failed\n    \n    def males_passed(self):\n        cross_grades_m = pd.crosstab(self.relationship_males['sex'], self.relationship_males['grade_status'])\n        boys_in_relationship =  cross_grades_m.iloc[0][0] + cross_grades_m.iloc[0][1]\n        boys_passed = (cross_grades_m.iloc[0][1]/boys_in_relationship) * 100\n        return boys_passed\n        \n    \nrel = Relationship(relationship_females)\nrel.females_failed()\n\nfemales_passed = rel.females_passed()\nmales_passed = rel.males_passed()\nfemales_failed = rel.females_failed()\nmales_failed = rel.males_failed()\n\n# Let's Start graphing in plotly.\ntable_data = [['Gender','% Passed', '% Failed'],\n             ['Males in<br>relationships', males_passed, males_failed],\n             ['Females in<br>relationships', females_passed, females_failed]]\n\n\nfigure = ff.create_table(table_data, height_constant=60)\n\ngenders = ['Male', 'Female']\npassed = [males_passed, females_passed]\nfailed = [males_failed, females_failed]\ntrace1 = Bar(x=genders, y=passed, xaxis='x2', yaxis='y2',\n                marker=dict(color='#819FF7'),\n                text='%',\n                name='Passed')\ntrace2 = Bar(x=genders, y=failed, xaxis='x2', yaxis='y2',\n                marker=dict(color='#FA5882'),\n                text='%',\n                name='Failed')\n# Add trace data to the figure.\nfigure['data'].extend(Data([trace1, trace2]))\n\n# Edit layout for subplots\nfigure.layout.yaxis.update({'domain': [0, .45]})\nfigure.layout.yaxis2.update({'domain': [.6, 1]})\n# The graph's yaxis2 MUST BE anchored to the graph's xaxis2 and vice versa\nfigure.layout.yaxis2.update({'anchor': 'x2'})\nfigure.layout.xaxis2.update({'anchor': 'y2'})\nfigure.layout.yaxis2.update({'title': '% Result'})\n# Update the margins to add a title and see graph x-labels. \nfigure.layout.margin.update({'t':75, 'l':50})\nfigure.layout.update({'title': 'Results of Students that are in a Relationship by Gender'})\n# Update the height because adding a graph vertically will interact with\n# the plot height calculated for the table\nfigure.layout.update({'height':800})\n\niplot(figure)\n","outputs":[],"cell_type":"code"},{"metadata":{"_uuid":"3e8eaa9f097c5349d253870821f9ecb0f69ab434","_cell_guid":"39d42bc7-e819-4292-8922-8a3cb78f87c6"},"source":"# What are our conclusions from this Analysis:\n1) Of course relationships is not the only attribute that could influence female students to perform badly in the math course. However, such a gap between females that failed and females that passed could be indicative that relationships could be a huge factor on how woman perform in the future.<br>\n# Let's ask more new questions to ourselves:\n1) What other factors could influence on female students in having a higher probability than male students of failing the class?<br>\n2) What other variables could be somehow associated with the relationship factor?\n\n","cell_type":"markdown"},{"execution_count":null,"metadata":{"_uuid":"daba3ce8370e6ab59b91425af83158e8b3669caa","_cell_guid":"a96f3962-7a3b-4a72-a065-e2338e8c2a56","collapsed":true},"source":"corr_matrix = df.corr()\ncorr_matrix['G3'].sort_values(ascending=False)","outputs":[],"cell_type":"code"},{"execution_count":null,"metadata":{"_uuid":"681d5b4bc665498a53ede294de7999ace35aa360","_cell_guid":"8511e5ae-a4ba-4215-8845-783fb8b1c00a","collapsed":true},"source":"import seaborn as sns\n\nfig, ax = plt.subplots(figsize=(12,6))\nsns.heatmap(corr_matrix,\n           xticklabels=corr_matrix.columns.values,\n           yticklabels = corr_matrix.columns.values,\n            ax = ax,\n           ).set_title(\"Correlation Between Columns\")\n\n\nplt.show()","outputs":[],"cell_type":"code"},{"metadata":{"_uuid":"aacc816b847785d8a4468fe34ae578adfac7d327","_cell_guid":"f37d9753-a957-4ec4-95dc-12136450af3e"},"source":"# What insights can we gain from the Heatmap:\n1) Medu (Mother's Education) is the highest positive correlation with regards to G3 and hence this is why it is the \"reddest\" square in the G3 column. <br>\n2) Let's see if \"Medu\" was a significant factor in why Females failed the course.\n\n## Positive Correlations:\n1) Here we can see some interesting insights: Medu (Mothers education) seems to be the feature that impacts the most on a student to get better grades. Fedu (Fathers education) is the second feature that impacts the most on a student's grade.<br>\n2) Studytime also influences positively the outcome of G3. <br>\n\n\n## Negative Correlations: \n1) There is some negative correlation with G3 (Final Grades) when Walc (Weeknd alcohol consumption) and Dalc (Workday alcohol consumption).<br>\n2) There is also some negative correlation the more the students go out with the  final grade.<br>\n3) The highest negative correlation is with \"failures\" or how many past classes they fail. This is indicative that the the higher the amount of past classes failures the more likely it is for the student to get a lower grade and thus fail.","cell_type":"markdown"},{"execution_count":null,"metadata":{"_uuid":"87aa06da03652571f462e9b33f9b46b1eebc9a07","_cell_guid":"b9afd430-9e9b-4bfc-a737-ee738b22a232","collapsed":true},"source":"cross_medu = pd.crosstab(df['Medu'], df['sex']).apply(lambda x: x/x.sum() * 100)\ncross_medu","outputs":[],"cell_type":"code"},{"execution_count":null,"metadata":{"_uuid":"afe849e7af77206f00659c11bda6acf46b15959d","_cell_guid":"f52d94d2-d7d1-4eb6-af6b-eef2b7d85338","collapsed":true},"source":"colors = [\"#FF4000\", \"#2E64FE\"]\nct = pd.crosstab(df.Medu, df.grade_status).apply(lambda x: x/x.sum() * 100)\n\n# ct.plot.bar(stacked=True, color=colors)\n# fig, ax =  plt.subplots(figsize=(14,10))\nax = ct.plot.bar(stacked=True, color=colors, figsize=(12,6))\nax.set(xlabel=\"Mom's Education (from lowest to highest)\", ylabel='% of Passes and Fails', title=\"How Mom's Education influenced Performance\")\nplt.show()","outputs":[],"cell_type":"code"},{"execution_count":null,"metadata":{"_uuid":"b35a9ab8eb4176a67d0376cda4ffbf6d4d443abd","_cell_guid":"795b928a-57d4-4e1f-a5ed-96e2a5c46a8d","collapsed":true},"source":"cross_fedu = pd.crosstab(df['Fedu'], df['grade_status']).apply(lambda x: x/x.sum() * 100)\ncross_fedu","outputs":[],"cell_type":"code"},{"execution_count":null,"metadata":{"_uuid":"1396d35977e1c580ae3c040dabe9d0c1e682d7f2","_cell_guid":"e1216cf7-1ba5-4292-b1ff-6e8d8faa2fef","collapsed":true},"source":"colors = [\"#DF0101\",\"#585858\"]\ncf = pd.crosstab(df.Fedu, df.grade_status).apply(lambda x: x/x.sum() * 100)\n\n# ct.plot.bar(stacked=True, color=colors)\n# fig, ax =  plt.subplots(figsize=(14,10))\nax = cf.plot.bar(stacked=True, color=colors, figsize=(12,6))\nax.set(xlabel=\"Dad's Education (from lowest to highest)\", ylabel='% of Passes and Fails', title=\"How Dad's Education influenced Performance\")\nplt.show()","outputs":[],"cell_type":"code"},{"execution_count":null,"metadata":{"_uuid":"fa5fac3d81c8ee1149be98cb81e13c57df01c267","_cell_guid":"d44003c2-55bb-4293-8c47-477f3e1a500a","collapsed":true},"source":"# Let's see how the students that receive educaional support performed did they passed or fail?\ncross_edus = pd.crosstab(df['educational_support'], df['sex']).apply(lambda x: x/x.sum() * 100)\ncross_edus","outputs":[],"cell_type":"code"},{"execution_count":null,"metadata":{"_uuid":"3f51ab35ddffd364988364f62014067ff34e4cb1","_cell_guid":"e2f41a99-364f-4056-8993-71071babc048","collapsed":true},"source":"# Let's see how effective are the paid classes.\n# The same % of people who dont take paid classes Fail as the people who pay for classes the paid classes are not effective.\ncross_paid = pd.crosstab(df['grade_status'], df['paid_classes']).apply(lambda x: x/x.sum() * 100)\ncross_paid","outputs":[],"cell_type":"code"},{"execution_count":null,"metadata":{"_uuid":"eddb35aa76732ea3c253e3014e3c16be5379102c","_cell_guid":"f01efc65-9bf3-4b0e-afab-9cdd344fb874","collapsed":true},"source":"# More Females pay for classes and still more males pass the class the paid classes are definitely not helping in students \n# getting better grades.\ncross_paids = pd.crosstab(df['sex'], df['paid_classes']).apply(lambda x: x/x.sum() * 100)\ncross_paids","outputs":[],"cell_type":"code"},{"metadata":{"_uuid":"e89980376409f2913195558bd136c268b04efb90","_cell_guid":"572011e4-b1cd-4ed9-b714-5ed9c7cf235d"},"source":"## Analysis of the Crosstab: \n1) Let's say Medu's located in the 3 and 4 areas means the Mother's education is high while the others are located in the low area.<br>\n2)  Around 53% of the Female's students mothers have a high education while the remaining 47% have a poor education. <br>\n3) Around 64% of the Male's students mothers have a high education while the remaining 36% have a poor education.<br>\n4) 59% of Females pay for classes yet, 65.7% of female students fail the course. \n5) 59% of students overall that pay for classes fail the course. While only 41% of the students that pay pass the course. \n\n\n\n## Possible Solutions: \n1) Provide a higher educational support to the Female students that have parents with a low education. This will help the students improve their grades at school.<br>\n2) We can also see that a low % of students are having educational support. 81% of Females and 92% of Males don't receive any sort of educational support! <br>\n3) The educational support program of the school is not effective at all. There has to be some sort of improvement with the program perhaps, change the program totally. <br>\n4) If the paid classes are provided by the school there has to be changes to the way the classes are given to the students. There is a higher percentage of people that fail the course even if they take the paid classes than people that pass the course.\n","cell_type":"markdown"},{"execution_count":null,"metadata":{"_uuid":"82ed8b43df8ba63a02fe374e0fee302e24a6126f","_cell_guid":"86229ff8-2c71-47ba-894a-02541154f463","collapsed":true},"source":"failures_matrix = df.corr()\nfailures_matrix['failures'].sort_values(ascending=False)","outputs":[],"cell_type":"code"},{"execution_count":null,"metadata":{"_uuid":"e1a2a82fc0c2abe3e167d40ea5cd12956e00ca4e","_cell_guid":"b25da476-0d4d-4f6f-b069-b5b27aa4e5ce","collapsed":true},"source":"df['failures'].value_counts()","outputs":[],"cell_type":"code"},{"execution_count":null,"metadata":{"_uuid":"5be2c0c5d93a7a9db73c20569d10fff3e8d47f0c","_cell_guid":"8d5078ba-d629-4caa-8c98-891e170a4561","collapsed":true},"source":"# We will do Further analysis and we will visualize the data we consider will be useful to have in a graph.\n# 65% of Females fail and 34% of Females passed.\n\n# The higher the education of the mom the more likely students will pass. [We might visualize this data.]\ncross_Medu = pd.crosstab(df['Medu'], df['grade_status']).apply(lambda x: x/x.sum() * 100, axis=1)\ncross_Medu\n# Interestingly the amount of failures tells us that a student is more likely to fail.\n# failures is the number of past class failures.\ncross_failures = pd.crosstab(df['failures'], df['grade_status']).apply(lambda x: x/x.sum() * 100, axis=1)\ncross_walc = pd.crosstab(df['Walc'], df['grade_status']).apply(lambda x: x/x.sum() * 100, axis=1)\ncross_walc\n","outputs":[],"cell_type":"code"},{"metadata":{"_uuid":"ad4253926f04686c9933fb88fe1b18f1b152e4fc","_cell_guid":"247046c9-482c-4bb7-a737-ca473a0e8632"},"source":"# Let's analyze if the amount of Failures Determines will Fail the course.","cell_type":"markdown"},{"execution_count":null,"metadata":{"_uuid":"f478cd5a0179fbac4c408a79a28f1c681ff87d54","_cell_guid":"b7a031ef-c2de-484a-914e-62bd3b542d8e","collapsed":true},"source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots(figsize=(12,6))\ncolors = [\"#DF0101\", \"#088A08\"]\ng = sns.countplot(x=\"failures\", hue=\"grade_status\", palette=colors, data=df)\nsns.set(style=\"darkgrid\")\ng.axes.set_title(\"Students with Previous Failed Classes [Pass vs Fail]\",fontsize=24)\ng.set_xlabel(\"# of Failures\",fontsize=16)\ng.set_ylabel(\"# of Students\",fontsize=16)\nsns.set(font_scale=3)\nplt.show()","outputs":[],"cell_type":"code"},{"execution_count":null,"metadata":{"_uuid":"6caa16d30dc0165aa656c48eebbe08d861f461d5","_cell_guid":"b15ac5ba-5636-4e8c-a886-248c461dd60b","collapsed":true},"source":"# Now we will have the results percent wise for each of the failures. \n# By looking at the graph we can tell that people who have failed previous classes before tend to fail more this class.\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, iplot\nfrom plotly.graph_objs import *\ninit_notebook_mode()\n# Let's see % wise how many people studied 1,2,3 hours.\n\nnof_f = cross_failures.iloc[0][0]\nnof_p = cross_failures.iloc[0][1]\nonef_f = cross_failures.iloc[1][0]\nonef_p = cross_failures.iloc[1][1]\ntwof_f = cross_failures.iloc[2][0]\ntwof_p = cross_failures.iloc[2][1]\nthreef_f = cross_failures.iloc[3][0]\nthreef_p = cross_failures.iloc[3][1]\n\ntrace1 = Bar(\n    y=['No Failures', 'One Failure', 'Two Failures', 'Three Failures'],\n    x=[nof_f, onef_f, twof_f, threef_f],\n    text='Percent Failed (%)',\n    name='Failed',\n    orientation='h',\n    marker= dict(\n        color='#FA5858',\n        line = dict(color = '#DF0101',\n            width = 3)\n    )\n)\n\ntrace2 = Bar(\n    y=['No Failures', 'One Failure', 'Two Failures', 'Three Failures'],\n    x=[nof_p, onef_p, twof_p, threef_p],\n    text='Percent Passed (%)',\n    name='Passed',\n    orientation = 'h',\n    marker= dict(\n        color='#BEF781',\n        line = dict(color = '#0B610B',\n            width = 3)\n    )\n)\n\ndata = [trace1, trace2]\nlayout = Layout(\n    title = '% of People with Failures who passed the Course ',\n    barmode='stack',\n)\n\nfig = dict(data=data, layout=layout)\niplot(fig)\n","outputs":[],"cell_type":"code"},{"execution_count":null,"metadata":{"_uuid":"a1acb8e7e1f7edd6a368220fb6197dbef373dae7","_cell_guid":"efb36c7b-7357-405d-9de4-377965a8bd93","collapsed":true},"source":"df.columns","outputs":[],"cell_type":"code"},{"execution_count":null,"metadata":{"_uuid":"2d30d6f4e7c372dbc7c4c56d5dedb9835569ee18","_cell_guid":"c8139cbe-e293-402e-9b3f-fdd0d5b1914b","collapsed":true},"source":"# color_name = grade_status\n# y = G3\n# x = age\nimport plotly.figure_factory as ff\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, iplot\nfrom plotly.graph_objs import *\ninit_notebook_mode()\n\nfig = ff.create_facet_grid(\n    df,\n    x='absences',\n    y='G3',\n    color_name='grade_status',\n    show_boxes=False,\n    marker={'size': 8, 'opacity': 0.75},\n    colormap={'Pass': '#0B610B', 'Fail': '#DF0101'},\n)\n\nfig.layout.update({'title': 'Absences of Students'})\nfig.layout.update({'autosize': 'False', 'width': 800, 'height': 600})\n\niplot(fig)","outputs":[],"cell_type":"code"},{"metadata":{"_uuid":"00b097457dd7e62064e7294a6271316dff5d7002","_cell_guid":"419f6e18-2f79-455d-97d2-e8b0d6607ecc"},"source":"The more absences the more likely the student was going to fail the class. (Logically).","cell_type":"markdown"},{"execution_count":null,"metadata":{"_uuid":"bfc7846d7d4eecf73c41bc3b2fbe4f846f03d9dc","_cell_guid":"2d622dc1-362b-4757-bcde-5411260067fb","collapsed":true},"source":"# Mo\nct = pd.crosstab(df.Medu, df.grade_status).apply(lambda x: x/x.sum() * 100)\nct","outputs":[],"cell_type":"code"},{"metadata":{"_uuid":"de294b6899142e371b3f16eebd22ce18a8fae136","_cell_guid":"7b17aebe-3127-4609-b1da-dc600796e3ed"},"source":"## Conclusion: \n1) We can determine that the Mother's education have a significant impact on whether a Female student is likely to pass or fail the course. <br>","cell_type":"markdown"},{"execution_count":null,"metadata":{"_uuid":"427fb2ca9a03747ac151fe03b458f3f923cf9673","_cell_guid":"d083eccb-581f-446c-814d-12877ff52712","collapsed":true},"source":"# Check if we have any null values.\n# It looks like we dont have any null values.\ndf_rows_with_null = df[df.isnull().any(axis=1)].head()\ndf_rows_with_null","outputs":[],"cell_type":"code"},{"metadata":{"_uuid":"57137e130f1eb0ca78b818cf2bdff1c2dc9c0e0a","_cell_guid":"c6bd2706-b554-4d68-b907-6dd36edf50e8"},"source":"<img src=\"https://media.giphy.com/media/VG7gGGBzhBSP6/giphy.gif\">","cell_type":"markdown"},{"metadata":{"_uuid":"baafd135b8768f7449689ca7392010dd1b5c57bd","_cell_guid":"fcefe399-e485-403b-b713-c938919a6da1"},"source":"# Let's Start with the Algorithms:\n1) We don't have any Null values so we dont need to fill for missing values.","cell_type":"markdown"},{"execution_count":null,"metadata":{"_uuid":"9083a3cb19059d9874f639d90ab2ec62f18d1d09","_cell_guid":"6ea20101-0025-4f36-96d7-92cad5ba7b8a","collapsed":true},"source":"grades = df.drop([\"G3\", \"grade_status\"], axis=1) #we drop the labels for the training set.\ngrades_labels = df[\"G3\"].copy()\n\nprint(grades.shape, grades_labels.shape)","outputs":[],"cell_type":"code"},{"execution_count":null,"metadata":{"_uuid":"96a862767152d09df49f5a0f5e708ed4ed8ac7d4","_cell_guid":"3663d096-4649-4d47-b4dc-54d627dd371c","collapsed":true},"source":"# We need to preprocess the categorical values which are:\n# We have grade_status, sex, Mjob, Fjob, schoolsup are objects so we need to preprocess those columns.\ngrades.dtypes","outputs":[],"cell_type":"code"},{"execution_count":null,"metadata":{"_uuid":"63344bc8119a177156bebeb65582471fdfd7b467","_cell_guid":"899184c0-eb0b-41cf-b7bd-ac777caedba9","collapsed":true},"source":"# Let's split the numerical and categorical values to fit it into our Linear Regression Model.\n\ngrades_num = grades.drop([\"sex\", \"Mjob\", \"Fjob\", \"schoolsup\"], axis=1)\ngrades_cat = grades[['sex', 'Mjob', 'Fjob', 'schoolsup']]","outputs":[],"cell_type":"code"},{"metadata":{"_uuid":"d7be64afa0ff0582abc4915acc974db32433e1f4","_cell_guid":"f2a96fd9-8159-451d-bde1-9a47d6547043"},"source":"# Reference to the Class Below:\nThe class below is used to encode categorical values in the feature columns. <br>\nThe source of this class is found in the following website: https://github.com/scikit-learn/scikit-learn/pull/9151\nwhich I copied from Pull Request #9151","cell_type":"markdown"},{"execution_count":null,"metadata":{"_uuid":"96168538f7fc28ee176a64dadbc31b681d0399c9","_cell_guid":"38460bea-316d-4d41-950a-10c129cd1d9c","collapsed":true},"source":"from sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.utils import check_array\nfrom sklearn.preprocessing import LabelEncoder\nfrom scipy import sparse\n\nclass CategoricalEncoder(BaseEstimator, TransformerMixin):\n    \"\"\"Encode categorical features as a numeric array.\n    The input to this transformer should be a matrix of integers or strings,\n    denoting the values taken on by categorical (discrete) features.\n    The features can be encoded using a one-hot aka one-of-K scheme\n    (``encoding='onehot'``, the default) or converted to ordinal integers\n    (``encoding='ordinal'``).\n    This encoding is needed for feeding categorical data to many scikit-learn\n    estimators, notably linear models and SVMs with the standard kernels.\n    Read more in the :ref:`User Guide <preprocessing_categorical_features>`.\n    Parameters\n    ----------\n    encoding : str, 'onehot', 'onehot-dense' or 'ordinal'\n        The type of encoding to use (default is 'onehot'):\n        - 'onehot': encode the features using a one-hot aka one-of-K scheme\n          (or also called 'dummy' encoding). This creates a binary column for\n          each category and returns a sparse matrix.\n        - 'onehot-dense': the same as 'onehot' but returns a dense array\n          instead of a sparse matrix.\n        - 'ordinal': encode the features as ordinal integers. This results in\n          a single column of integers (0 to n_categories - 1) per feature.\n    categories : 'auto' or a list of lists/arrays of values.\n        Categories (unique values) per feature:\n        - 'auto' : Determine categories automatically from the training data.\n        - list : ``categories[i]`` holds the categories expected in the ith\n          column. The passed categories are sorted before encoding the data\n          (used categories can be found in the ``categories_`` attribute).\n    dtype : number type, default np.float64\n        Desired dtype of output.\n    handle_unknown : 'error' (default) or 'ignore'\n        Whether to raise an error or ignore if a unknown categorical feature is\n        present during transform (default is to raise). When this is parameter\n        is set to 'ignore' and an unknown category is encountered during\n        transform, the resulting one-hot encoded columns for this feature\n        will be all zeros.\n        Ignoring unknown categories is not supported for\n        ``encoding='ordinal'``.\n    Attributes\n    ----------\n    categories_ : list of arrays\n        The categories of each feature determined during fitting. When\n        categories were specified manually, this holds the sorted categories\n        (in order corresponding with output of `transform`).\n    Examples\n    --------\n    Given a dataset with three features and two samples, we let the encoder\n    find the maximum value per feature and transform the data to a binary\n    one-hot encoding.\n    >>> from sklearn.preprocessing import CategoricalEncoder\n    >>> enc = CategoricalEncoder(handle_unknown='ignore')\n    >>> enc.fit([[0, 0, 3], [1, 1, 0], [0, 2, 1], [1, 0, 2]])\n    ... # doctest: +ELLIPSIS\n    CategoricalEncoder(categories='auto', dtype=<... 'numpy.float64'>,\n              encoding='onehot', handle_unknown='ignore')\n    >>> enc.transform([[0, 1, 1], [1, 0, 4]]).toarray()\n    array([[ 1.,  0.,  0.,  1.,  0.,  0.,  1.,  0.,  0.],\n           [ 0.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.]])\n    See also\n    --------\n    sklearn.preprocessing.OneHotEncoder : performs a one-hot encoding of\n      integer ordinal features. The ``OneHotEncoder assumes`` that input\n      features take on values in the range ``[0, max(feature)]`` instead of\n      using the unique values.\n    sklearn.feature_extraction.DictVectorizer : performs a one-hot encoding of\n      dictionary items (also handles string-valued features).\n    sklearn.feature_extraction.FeatureHasher : performs an approximate one-hot\n      encoding of dictionary items or strings.\n    \"\"\"\n\n    def __init__(self, encoding='onehot', categories='auto', dtype=np.float64,\n                 handle_unknown='error'):\n        self.encoding = encoding\n        self.categories = categories\n        self.dtype = dtype\n        self.handle_unknown = handle_unknown\n\n    def fit(self, X, y=None):\n        \"\"\"Fit the CategoricalEncoder to X.\n        Parameters\n        ----------\n        X : array-like, shape [n_samples, n_feature]\n            The data to determine the categories of each feature.\n        Returns\n        -------\n        self\n        \"\"\"\n\n        if self.encoding not in ['onehot', 'onehot-dense', 'ordinal']:\n            template = (\"encoding should be either 'onehot', 'onehot-dense' \"\n                        \"or 'ordinal', got %s\")\n            raise ValueError(template % self.handle_unknown)\n\n        if self.handle_unknown not in ['error', 'ignore']:\n            template = (\"handle_unknown should be either 'error' or \"\n                        \"'ignore', got %s\")\n            raise ValueError(template % self.handle_unknown)\n\n        if self.encoding == 'ordinal' and self.handle_unknown == 'ignore':\n            raise ValueError(\"handle_unknown='ignore' is not supported for\"\n                             \" encoding='ordinal'\")\n\n        X = check_array(X, dtype=np.object, accept_sparse='csc', copy=True)\n        n_samples, n_features = X.shape\n\n        self._label_encoders_ = [LabelEncoder() for _ in range(n_features)]\n\n        for i in range(n_features):\n            le = self._label_encoders_[i]\n            Xi = X[:, i]\n            if self.categories == 'auto':\n                le.fit(Xi)\n            else:\n                valid_mask = np.in1d(Xi, self.categories[i])\n                if not np.all(valid_mask):\n                    if self.handle_unknown == 'error':\n                        diff = np.unique(Xi[~valid_mask])\n                        msg = (\"Found unknown categories {0} in column {1}\"\n                               \" during fit\".format(diff, i))\n                        raise ValueError(msg)\n                le.classes_ = np.array(np.sort(self.categories[i]))\n\n        self.categories_ = [le.classes_ for le in self._label_encoders_]\n\n        return self\n\n    def transform(self, X):\n        \"\"\"Transform X using one-hot encoding.\n        Parameters\n        ----------\n        X : array-like, shape [n_samples, n_features]\n            The data to encode.\n        Returns\n        -------\n        X_out : sparse matrix or a 2-d array\n            Transformed input.\n        \"\"\"\n        X = check_array(X, accept_sparse='csc', dtype=np.object, copy=True)\n        n_samples, n_features = X.shape\n        X_int = np.zeros_like(X, dtype=np.int)\n        X_mask = np.ones_like(X, dtype=np.bool)\n\n        for i in range(n_features):\n            valid_mask = np.in1d(X[:, i], self.categories_[i])\n\n            if not np.all(valid_mask):\n                if self.handle_unknown == 'error':\n                    diff = np.unique(X[~valid_mask, i])\n                    msg = (\"Found unknown categories {0} in column {1}\"\n                           \" during transform\".format(diff, i))\n                    raise ValueError(msg)\n                else:\n                    # Set the problematic rows to an acceptable value and\n                    # continue `The rows are marked `X_mask` and will be\n                    # removed later.\n                    X_mask[:, i] = valid_mask\n                    X[:, i][~valid_mask] = self.categories_[i][0]\n            X_int[:, i] = self._label_encoders_[i].transform(X[:, i])\n\n        if self.encoding == 'ordinal':\n            return X_int.astype(self.dtype, copy=False)\n\n        mask = X_mask.ravel()\n        n_values = [cats.shape[0] for cats in self.categories_]\n        n_values = np.array([0] + n_values)\n        indices = np.cumsum(n_values)\n\n        column_indices = (X_int + indices[:-1]).ravel()[mask]\n        row_indices = np.repeat(np.arange(n_samples, dtype=np.int32),\n                                n_features)[mask]\n        data = np.ones(n_samples * n_features)[mask]\n\n        out = sparse.csc_matrix((data, (row_indices, column_indices)),\n                                shape=(n_samples, indices[-1]),\n                                dtype=self.dtype).tocsr()\n        if self.encoding == 'onehot-dense':\n            return out.toarray()\n        else:\n            return out","outputs":[],"cell_type":"code"},{"metadata":{"_uuid":"3d58239a9df8a44e64709d791e2508ccfa1bb1f0","_cell_guid":"4a95177b-f614-4cc2-bf07-7609abd4ada4"},"source":"# Feature Scaling and Transformation Pipelines:","cell_type":"markdown"},{"execution_count":null,"metadata":{"_uuid":"78b52d9e2f7ee26205b04601c25db4b371d429c9","_cell_guid":"036a55e5-74b3-477e-87fd-7b6aae8146ae","collapsed":true},"source":"# Transform our numeric values into a StandardScaler form for better performance of the linear regressoin model.\n# We use pipeline to automize the steps for the linear regression model.\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\nnumeric_pipeline = Pipeline([\n    ('standard_scaler', StandardScaler())\n])\n\n# fit and transorm into standard_scaler form the numeric values.\ngrading_numeric_tr = numeric_pipeline.fit_transform(grades_num)\ngrading_numeric_tr","outputs":[],"cell_type":"code"},{"metadata":{"_uuid":"6b5f37528c496c67a64484d7a17df540c47be914","_cell_guid":"e16c4151-9658-44c8-8275-878a830a9584"},"source":"# Referencing the Code Below: \n## Hands on Machine Learning with Scikit-Learn & Tensorflow by Aurélien Geron <br>\n1) I will recommend to anyone who wants to learn more about machine learning to buy this book totally worth it ! I haven't finished it yet but I can tell you everyday I am learning more and more from this book. The insights are super interesting! <br><br>\n2) The main purpose of the DataFrameSelector class is to feed into our Pipeline our numerical and cateogrical attributes in a DataFrame format as simple as that! Then we Scale the data and after that we just make a union of the categorical_features and numerical_features.\n","cell_type":"markdown"},{"execution_count":null,"metadata":{"_uuid":"7abeccda9989353e5f21430b8a65e1e75fd3dcd5","_cell_guid":"308f9672-e050-4a63-8cfd-9c17f65e0826","collapsed":true},"source":"from sklearn.base import BaseEstimator, TransformerMixin\n# Create a class to select numerical or categorical columns.\n# since Scikit-learn dosen't handle DataFrames yet.\n# BaseEstimator is to avoid kargs and args which will help us later for hyperparameter tuning.\n# TranformerMixin helps us with the fit_transform function which we don't have to implement here since we have TransformerMixin.\nclass DataFrameSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, attribute_names):\n        self.attribute_names = attribute_names\n    def fit (self, X, y=None):\n        return self\n    def transform(self, X):\n        return X[self.attribute_names].values","outputs":[],"cell_type":"code"},{"execution_count":null,"metadata":{"_uuid":"50caac3db0b64ddef91f4146c6fea7f515835f73","_cell_guid":"4c22f754-1161-43a8-83ef-976d702e52ae","collapsed":true},"source":"numerical_features = list(grades_num)\ncategorical_features = ['sex', 'Mjob', 'Fjob', 'schoolsup']\n\nnumeric_pipeline = Pipeline([\n    ('selector', DataFrameSelector(numerical_features)),\n    ('standard_scaler', StandardScaler())\n])\n\ncategorical_pipeline = Pipeline([\n    ('selector', DataFrameSelector(categorical_features)),\n    ('cat_encoder', CategoricalEncoder(encoding=\"onehot-dense\"))\n])\n\n# Now lets join both pipelines into one.","outputs":[],"cell_type":"code"},{"execution_count":null,"metadata":{"_uuid":"4709577da1635ee1a943832c3b041a9ab97d5a66","_cell_guid":"cfd6f8f4-1bd5-4869-aaa7-cddd6932890d","collapsed":true},"source":"from sklearn.pipeline import FeatureUnion\n\nfull_pipeline = FeatureUnion(transformer_list=[\n    (\"numeric_pipeline\", numeric_pipeline),\n    (\"categorical_pipeline\", categorical_pipeline),\n])\n\n# Now let's run the full pipeline and all our data is scaled! Ready for applying the scaled data to the different algorthims.\ngrades_scaled = full_pipeline.fit_transform(grades)\ngrades_scaled","outputs":[],"cell_type":"code"},{"execution_count":null,"metadata":{"_uuid":"1592cd07662d7f9c063249c889a065f6e14444c4","_cell_guid":"0e076316-f7aa-49d1-8b72-5544bf0e06fa","collapsed":true},"source":"grades_scaled.shape","outputs":[],"cell_type":"code"},{"execution_count":null,"metadata":{"_uuid":"f4dbc1b624a35022decfa3b1554ab95afac09db3","_cell_guid":"c5b28c6f-43b4-48d2-a36f-8658b5c0cdf4","collapsed":true},"source":"from sklearn.linear_model import LinearRegression\n\nlin_reg = LinearRegression()\nlin_reg.fit(grades_scaled, grades_labels)","outputs":[],"cell_type":"code"},{"execution_count":null,"metadata":{"_uuid":"fd2d5cbaf31c650ce27fb5d48443acb10e9193d2","_cell_guid":"5a144b47-ee32-466b-b2dc-8035a2be2e9f","collapsed":true},"source":"top_data = grades.iloc[:5]\ntop_labels = grades_labels[:5]\ntop_data_ready = full_pipeline.transform(top_data)\nprint(\"Predictions:\", lin_reg.predict(top_data_ready))","outputs":[],"cell_type":"code"},{"execution_count":null,"metadata":{"_uuid":"ee68a6d04367ea1d14850b0e99ee2854bf221021","_cell_guid":"20be9577-7c56-4b54-8040-c239e798d0d5","collapsed":true},"source":"# We got 1/5 correct.\nprint(\"Labels\", list(top_labels))","outputs":[],"cell_type":"code"},{"execution_count":null,"metadata":{"_uuid":"3e557e4f52a7bffc8928bc3817fb0212e8695aef","_cell_guid":"b3ffbe06-5faf-4649-8212-1790ca03c0d2","collapsed":true},"source":"top_data_ready","outputs":[],"cell_type":"code"},{"execution_count":null,"metadata":{"_uuid":"36f14b29022841f85ddfdee19114ce30b4a5331f","_cell_guid":"49ecd139-163b-4891-93a0-4b3adbf9df47","collapsed":true},"source":"from sklearn.metrics import mean_squared_error\n# Let's see what is the prediction error of our model.\n# We have an error of 3.96 points.\n\ngrades_predictions = lin_reg.predict(grades_scaled)\nlin_mse = mean_squared_error(grades_labels, grades_predictions)\nlin_rmse = np.sqrt(lin_mse)\nlin_rmse","outputs":[],"cell_type":"code"},{"execution_count":null,"metadata":{"_uuid":"3c60dc23d74c221cb86790466cf75384c82212d7","_cell_guid":"666e3a48-2635-4d1a-be74-4c3b9fcdcc70","collapsed":true},"source":"# Let's see how accurate is our model.\nfrom sklearn import metrics\naccuracy = metrics.r2_score(grades_labels, grades_predictions)\naccuracy\n# 24% accurate.","outputs":[],"cell_type":"code"},{"execution_count":null,"metadata":{"_uuid":"77edd06c1a68233c3585310e32ff0565db9fb429","_cell_guid":"20dfa648-cb26-4bcb-b7bb-30770349c690","collapsed":true},"source":"from sklearn.metrics import mean_absolute_error\n# We have a lower error of 3.033 points.\nlin_mae = mean_absolute_error(grades_labels, grades_predictions)\nlin_mae","outputs":[],"cell_type":"code"},{"execution_count":null,"metadata":{"_uuid":"6060e4b458e4a8e893d3d0546dc166821e988785","_cell_guid":"694cfb27-4be6-4798-8f0b-a5089e8b96a6","collapsed":true},"source":"from sklearn.tree import DecisionTreeRegressor\n\ntree_reg = DecisionTreeRegressor(random_state=42)\ntree_reg.fit(grades_scaled, grades_labels)","outputs":[],"cell_type":"code"},{"metadata":{"_uuid":"7a31f8008d57c78c343611db37236fcf7f599222","_cell_guid":"257f7f97-07ba-4398-9889-39f79a11010d"},"source":"## The reason why we use (-) in tree_rmse_scores:\nWe use a negative sign because the method of cross_validation considers that the best model unlike linear regression in which the lower is the error rate the better will perform our model. [Cost Function]<br>\n\n## Avoiding overfitting: \nWe have to use cross validation in the training set in order to avoid overfitting. Overfitting basically means that our model fits well to our training set since it memorized it but will not perform well when new data is analyzed.","cell_type":"markdown"},{"execution_count":null,"metadata":{"_uuid":"24704c614db5c2e7eded117dc240596052bc0161","_cell_guid":"63d9668b-044e-4a62-9f6d-16c1d7e2dd7c","collapsed":true},"source":"# Let's implement the Decision Tree Regressor \nfrom sklearn.model_selection import cross_val_score \nscores = cross_val_score(tree_reg, grades_scaled, grades_labels,\n                        scoring=\"neg_mean_squared_error\", cv=10)\n\ntree_rmse_scores = np.sqrt(-scores)\n# What this does is that we will get 10 different scores in order to avoid overfitting to see how our model behaves with\n# newly introduced data.","outputs":[],"cell_type":"code"},{"execution_count":null,"metadata":{"_uuid":"950c21e48e09caf76752c69b7689696cec9ceccb","_cell_guid":"a84e07a5-b345-49a7-8633-63b5189cd353","collapsed":true},"source":"def show_result(scores):\n    print(\"Scores:\", scores)\n    print(\"Mean: \", scores.mean())\n    \nshow_result(tree_rmse_scores)\n# The model gives us worse results than the linearregression results.\n","outputs":[],"cell_type":"code"},{"execution_count":null,"metadata":{"_uuid":"b01155c2c3744dd3dcfacf2fd7787b6fdfe72c8f","_cell_guid":"9d9068b7-d6df-49f7-8482-90e8aab72b4d","collapsed":true},"source":"# It slightly improves when we use the linea regression model.\nlin_scores = cross_val_score(lin_reg, grades_scaled, grades_labels,\n                             scoring=\"neg_mean_squared_error\", cv=10)\n\nlin_rmse_scores = np.sqrt(-lin_scores)\n\nshow_result(lin_rmse_scores)\n","outputs":[],"cell_type":"code"},{"execution_count":null,"metadata":{"_uuid":"4051570280334f96f7971f4f00997ce23b92dfb8","_cell_guid":"c900a5f6-ef4c-48c7-81c0-61351ec88963","collapsed":true},"source":"# Lets try with RandomForestRegressor\nfrom sklearn.ensemble import RandomForestRegressor\n\nforest_reg = RandomForestRegressor()\nforest_reg.fit(grades_scaled, grades_labels)","outputs":[],"cell_type":"code"},{"execution_count":null,"metadata":{"_uuid":"7079884c728405b63035bd61e1175da80b0c6d28","_cell_guid":"44a5a7bc-4d85-4578-883c-ba7c71fd0ff8","collapsed":true},"source":"grades_predictions = forest_reg.predict(grades_scaled)\nforest_mse = mean_squared_error(grades_labels, grades_predictions)\nforest_rmse = np.sqrt(forest_mse)\nforest_rmse\n# 1.69 points error by far the lowest error from all the algorithms we have tried out.","outputs":[],"cell_type":"code"},{"execution_count":null,"metadata":{"_uuid":"8f29a424b95d60a076466f6c5c251e6d50b7670d","_cell_guid":"0b3597d7-ee7e-41e2-91b7-8c08fc9b23f6","collapsed":true},"source":"# Accuracy for random_forest_regressor\naccuracy = metrics.r2_score(grades_labels, grades_predictions)\naccuracy\n# 86% accuracy.","outputs":[],"cell_type":"code"},{"execution_count":null,"metadata":{"_uuid":"8edfc3b6ad00a89f548fcf40fd716553011aa692","_cell_guid":"4794834b-ecac-43ad-aa94-a237ce2bc75a","collapsed":true},"source":"from sklearn.model_selection import cross_val_score\n# Best score so far.\nforest_scores = cross_val_score(forest_reg, grades_scaled, grades_labels,\n                               scoring=\"neg_mean_squared_error\", cv=10)\n\nforest_rmse_scores = np.sqrt(-forest_scores)\nshow_result(forest_rmse_scores)","outputs":[],"cell_type":"code"},{"execution_count":null,"metadata":{"_uuid":"057cb4b898c89056595cf5547c6465708e0d7c16","_cell_guid":"310e86ba-720f-494d-87ae-9ca7348b295a","collapsed":true},"source":"# Implement Support Vector Machines.\n# 4.09 points error this does not look good.\nfrom sklearn.svm import SVR\nsvm_reg = SVR(kernel=\"linear\")\nsvm_reg.fit(grades_scaled, grades_labels)\ngrades_predictions = svm_reg.predict(grades_scaled)\nsvm_mse = mean_squared_error(grades_labels, grades_predictions)\nsvm_rmse = np.sqrt(svm_mse)\nsvm_rmse","outputs":[],"cell_type":"code"},{"execution_count":null,"metadata":{"_uuid":"2afdff442230d584325631cc86eeed2e13e9e369","_cell_guid":"2d26d561-65b8-4013-9bd6-d374ff6df028","collapsed":true},"source":"# Accuracy for SVR = 20% accuracy.\naccuracy = metrics.r2_score(grades_labels, grades_predictions)\naccuracy","outputs":[],"cell_type":"code"},{"execution_count":null,"metadata":{"_uuid":"ad04cb93dd7a9868f303fa6eed479e941a0337bb","_cell_guid":"60d83dc4-906c-471e-84a0-9326666e2eae","collapsed":true},"source":"# Now we will implement GridSearchCV which tells us what are our best hyperparameters to \n# have the lowest error rate.\nfrom sklearn.model_selection import GridSearchCV\n\nparam_grid = [\n    # 3 x 4 combinations of hyperparameters.\n    {'n_estimators': [3, 10, 30, 50], 'max_features': [2, 4, 6, 8, 10]},\n    # then we try (2x3) combinations with bootstrap set as False.\n    {'bootstrap': [False], 'n_estimators': [3, 10, 30], 'max_features': [2,3,4,5]}\n]\n\nforest_reg = RandomForestRegressor(random_state=42)\n# Let's train acrros 5 folds, thats a total of (12 + 6) * 5 = 90 rounds of training.\ngrid_search = GridSearchCV(forest_reg, param_grid, cv=5,\n                          scoring='neg_mean_squared_error')\ngrid_search.fit(grades_scaled, grades_labels)","outputs":[],"cell_type":"code"},{"execution_count":null,"metadata":{"_uuid":"96bc82f12e6b4007102665b4f0049e5c08f6c017","_cell_guid":"303b7a58-6e0e-4620-b314-4e4a8b0725aa","collapsed":true},"source":"# Best parameter to obtain a better score.\n# When you get the highest parameters you can tune the hyperparameters and slightly increase the parameters\n# In order to see by how much the error rate decreases.\ngrid_search.best_params_","outputs":[],"cell_type":"code"},{"execution_count":null,"metadata":{"_uuid":"302b1ec58f1ebbbbf60c427c74c380eaf9b6f928","_cell_guid":"2ee07265-852a-4177-b45d-69e55a565299","collapsed":true},"source":"cvres = grid_search.cv_results_\n\nfor mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n    print(np.sqrt(-mean_score), params)","outputs":[],"cell_type":"code"},{"execution_count":null,"metadata":{"_uuid":"35a9b02942a25802709eac32d5ad8dd40ff0dfba","_cell_guid":"a75eec7c-6077-4671-9042-452a89c76173","collapsed":true},"source":"feature_importances = grid_search.best_estimator_.feature_importances_\nfeature_importances","outputs":[],"cell_type":"code"},{"execution_count":null,"metadata":{"_uuid":"424423224a3c4465fa5daddc03d2a090e1199358","_cell_guid":"8d28bb21-b6a8-41ec-ae96-aa21b7091dc2","collapsed":true},"source":"cat_encoder = categorical_pipeline.named_steps[\"cat_encoder\"]\ncat_one_hot_attribs = list(cat_encoder.categories_[0])\nqualities = numerical_features + cat_one_hot_attribs\nsorted(zip(feature_importances, qualities), reverse=True)\n# This shows the relative importance of each attribute for making accurate prediction.\n# Absences gives you a guidance on what score the student will obtain in G3. It is the feature that is the most important","outputs":[],"cell_type":"code"},{"execution_count":null,"metadata":{"_uuid":"7e4e5d79b1b88cdd57607cae9b262c980bcb0d8c","_cell_guid":"c9204639-36eb-422a-9416-484c30e1eda2","collapsed":true},"source":"# Create a list without \"grade_status\" we already know this will be the highest indicator on whether a student will pass or fail\n# the course.\n\ndf = df[\n['G3',\n 'sex',\n 'age',\n 'Medu',\n 'Fedu',\n 'Mjob',\n 'Fjob',\n 'traveltime',\n 'studytime',\n 'failures',\n 'schoolsup',\n 'goout',\n 'Dalc',\n 'Walc',\n 'health',\n 'absences',\n 'address',\n 'paid_classes',\n 'higher_education',\n 'internet_availability',\n 'b_Pstatus',\n 'relationship',\n 'educational_support']\n]\n\nlst_y = df.columns.tolist()\nlst_y","outputs":[],"cell_type":"code"},{"execution_count":null,"metadata":{"_uuid":"8e7746ee07202392a510e52e953a88bd0178cddd","_cell_guid":"e628a597-67ea-4201-b93f-df81ac4b742b","collapsed":true},"source":"# Time to test the model hopefully it gets a good score into new seen data.\ndf.head()\nstrat_test_set = df[lst_y]\nstrat_test_set.head()","outputs":[],"cell_type":"code"},{"execution_count":null,"metadata":{"_uuid":"2b9edac97bff44032f9f4169ef39116f6a00b6b7","_cell_guid":"2afb1e0e-269c-425b-a97e-01da1072d45d","collapsed":true},"source":"# Let's do the final test.\n# We have an error of 1.50 points.\nfinal_model = grid_search.best_estimator_\n\nX_test = strat_test_set.drop(\"G3\", axis=1)\ny_test = strat_test_set[\"G3\"].copy()\n\nX_test_prepared = full_pipeline.transform(X_test)\nfinal_predictions = final_model.predict(X_test_prepared)\nfinal_mse = mean_squared_error(y_test, final_predictions)\nfinal_rmse = np.sqrt(final_mse)\nfinal_rmse","outputs":[],"cell_type":"code"},{"execution_count":null,"metadata":{"_uuid":"d149d28140f778a300ba70374d92c7581b875a64","_cell_guid":"1549ea44-776c-4e8c-8de6-0b4ace313b68","collapsed":true},"source":"from sklearn import metrics\n# Our model is 89% accurate. Not bad!\naccuracy = metrics.r2_score(y_test, final_predictions)\naccuracy","outputs":[],"cell_type":"code"},{"execution_count":null,"metadata":{"_uuid":"80125c7009f75567ad89d5d60c381e0412745a19","_cell_guid":"1a3221c7-9170-457c-a3cd-4ca2acd2a6f5","collapsed":true},"source":"# Finally let's create a full pipeline with both the preparation phase and prediction.\nfull_pipeline_with_predictor = Pipeline([\n    (\"preparation\", full_pipeline),\n    (\"linear\",  RandomForestRegressor())\n])\n\nfull_pipeline_with_predictor.fit(grades, grades_labels)\nfull_pipeline_with_predictor.predict(top_data)\n","outputs":[],"cell_type":"code"},{"execution_count":null,"metadata":{"_uuid":"4a51509f51d7a4078a3c6ab1b4839d089cf6691c","_cell_guid":"e54e6bd6-7396-426b-8133-d121544fff83","collapsed":true},"source":"# Comparing our predictions with our labels now we can say that our Random Forest Regressor Model is\n# really accurate in predicting math scores!\nprint(\"Labels\", list(top_labels))","outputs":[],"cell_type":"code"},{"metadata":{"_uuid":"4f74ebc51ead09c08f7af05d78dbbe30d1685dbb","_cell_guid":"7d08ab0a-2493-4408-ba5d-c102db59c3ac"},"source":"<img src=\"https://media.giphy.com/media/LfNYfVwk0ICbu/giphy.gif\">","cell_type":"markdown"},{"metadata":{"_uuid":"0d6deb1e5d355ba7adb8508396ed9e02645ca220","_cell_guid":"5cadbaf2-b263-4c5b-ad6b-4e232541ff2a"},"source":"# Conclusion:\nIn this project we did a deep analysis of what could be possible factor on whether a student is likely to get a high score or a low score.The data does not contain that much information but still we were able to predict a pretty precise RandomForest Regressor algorithm that predicts what score a student will get in the foreseen feature by analyzing the features. This is my first Kernel so I am open to constructive criticisms. \n\n## Reference:\nI will like to reference the book Hands On Machine Learning with Scikit Learn and Tensorflow by Aurélien Géron. It really helped me gather a general concept of how Regression models work. \n\n## For what would we use a Linear Regression Model: \nIt is to my understanding that the linear regression model is used to predict values with a given amount of features. Here we learned how to tuned Hyperparameters in a more automatic way in order for our model to have better predictions when new features of students will come in and will let us know as data comes by if a student is most likely to pass the class or not. ","cell_type":"markdown"},{"execution_count":null,"metadata":{"_uuid":"565393527abf32e013f857b33b20ddf28b31152c","_cell_guid":"c2cb5b4d-74cc-4aaf-b3ce-671a52eb7aa2","collapsed":true},"source":"","outputs":[],"cell_type":"code"}]}