{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport scipy.sparse as sp\nimport torch\n\n\ndef encode_onehot(labels):\n    classes = set(labels)\n    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in\n                    enumerate(classes)}\n    labels_onehot = np.array(list(map(classes_dict.get, labels)),\n                             dtype=np.int32)\n    return labels_onehot\n\n\ndef load_data(path=\"../input/coraembeding\", dataset=\"cora\"):\n    \"\"\"Load citation network dataset (cora only for now)\"\"\"\n    print('Loading {} dataset...'.format(dataset))\n\n    idx_features_labels = np.genfromtxt(\"../input/coraembeding/cora.content\".format(path, dataset),\n                                        dtype=np.dtype(str))\n    features = sp.csr_matrix(idx_features_labels[:, 1:-1], dtype=np.float32)\n    labels = encode_onehot(idx_features_labels[:, -1])\n\n    # build graph\n    idx = np.array(idx_features_labels[:, 0], dtype=np.int32)\n    idx_map = {j: i for i, j in enumerate(idx)}\n    edges_unordered = np.genfromtxt(\"../input/coracites/cora.cites\".format(path, dataset),\n                                    dtype=np.int32)\n    edges = np.array(list(map(idx_map.get, edges_unordered.flatten())),\n                     dtype=np.int32).reshape(edges_unordered.shape)\n    adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])),\n                        shape=(labels.shape[0], labels.shape[0]),\n                        dtype=np.float32)\n\n    # build symmetric adjacency matrix\n    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n\n    features = normalize(features)\n    adj = normalize(adj + sp.eye(adj.shape[0]))\n\n    idx_train = range(140)\n    idx_val = range(200, 500)\n    idx_test = range(500, 1500)\n\n    features = torch.FloatTensor(np.array(features.todense()))\n    labels = torch.LongTensor(np.where(labels)[1])\n    adj = sparse_mx_to_torch_sparse_tensor(adj)\n\n    idx_train = torch.LongTensor(idx_train)\n    idx_val = torch.LongTensor(idx_val)\n    idx_test = torch.LongTensor(idx_test)\n\n    return adj, features, labels, idx_train, idx_val, idx_test\n\n\ndef normalize(mx):\n    \"\"\"Row-normalize sparse matrix\"\"\"\n    rowsum = np.array(mx.sum(1))\n    r_inv = np.power(rowsum, -1).flatten()\n    r_inv[np.isinf(r_inv)] = 0.\n    r_mat_inv = sp.diags(r_inv)\n    mx = r_mat_inv.dot(mx)\n    return mx\n\n\ndef accuracy(output, labels):\n    preds = output.max(1)[1].type_as(labels)\n    correct = preds.eq(labels).double()\n    correct = correct.sum()\n    return correct / len(labels)\n\n\ndef sparse_mx_to_torch_sparse_tensor(sparse_mx):\n    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n    indices = torch.from_numpy(\n        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n    values = torch.from_numpy(sparse_mx.data)\n    shape = torch.Size(sparse_mx.shape)\n    return torch.sparse.FloatTensor(indices, values, shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import math\n\nimport torch\n\nfrom torch.nn.parameter import Parameter\nfrom torch.nn.modules.module import Module\n\n\nclass GraphConvolution(Module):\n    \"\"\"\n    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n    \"\"\"\n\n    def __init__(self, in_features, out_features, bias=True):\n        super(GraphConvolution, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n        if bias:\n            self.bias = Parameter(torch.FloatTensor(out_features))\n        else:\n            self.register_parameter('bias', None)\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        stdv = 1. / math.sqrt(self.weight.size(1))\n        self.weight.data.uniform_(-stdv, stdv)\n        if self.bias is not None:\n            self.bias.data.uniform_(-stdv, stdv)\n\n    def forward(self, input, adj):\n        support = torch.mm(input, self.weight)\n        output = torch.spmm(adj, support)\n        if self.bias is not None:\n            return output + self.bias\n        else:\n            return output\n\n    def __repr__(self):\n        return self.__class__.__name__ + ' (' \\\n               + str(self.in_features) + ' -> ' \\\n               + str(self.out_features) + ')'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch.nn as nn\nimport torch.nn.functional as F\n\n\n\nclass GCN(nn.Module):\n    def __init__(self, nfeat, nhid, nclass, dropout):\n        super(GCN, self).__init__()\n\n        self.gc1 = GraphConvolution(nfeat, nhid)\n        self.gc2 = GraphConvolution(nhid, nclass)\n        self.dropout = dropout\n\n    def forward(self, x, adj):\n        x = F.relu(self.gc1(x, adj))\n        x = F.dropout(x, self.dropout, training=self.training)\n        x = self.gc2(x, adj)\n        return F.log_softmax(x, dim=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from __future__ import division\nfrom __future__ import print_function\n\nimport time\nimport argparse\nimport numpy as np\n\nimport torch\nimport torch.nn.functional as F\nimport torch.optim as optim\n\n\n# Load data\nadj, features, labels, idx_train, idx_val, idx_test = load_data()\n\n# Model and optimizer\nmodel = GCN(nfeat=features.shape[1],\n            nhid=64,\n            nclass=labels.max().item() + 1,\n            dropout=0.5)\noptimizer = optim.Adam(model.parameters(),\n                       lr=0.01, weight_decay=0.0005)\n\nif torch.cuda.is_available():\n    model.cuda()\n    features = features.cuda()\n    adj = adj.cuda()\n    labels = labels.cuda()\n    idx_train = idx_train.cuda()\n    idx_val = idx_val.cuda()\n    idx_test = idx_test.cuda()\n\n\ndef train(epoch):\n    t = time.time()\n    model.train()\n    optimizer.zero_grad()\n    output = model(features, adj)\n    loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n    acc_train = accuracy(output[idx_train], labels[idx_train])\n    loss_train.backward()\n    optimizer.step()\n\n    if not False:\n        # Evaluate validation set performance separately,\n        # deactivates dropout during validation run.\n        model.eval()\n        output = model(features, adj)\n\n    loss_val = F.nll_loss(output[idx_val], labels[idx_val])\n    acc_val = accuracy(output[idx_val], labels[idx_val])\n    print('Epoch: {:04d}'.format(epoch+1),\n          'loss_train: {:.4f}'.format(loss_train.item()),\n          'acc_train: {:.4f}'.format(acc_train.item()),\n          'loss_val: {:.4f}'.format(loss_val.item()),\n          'acc_val: {:.4f}'.format(acc_val.item()),\n          'time: {:.4f}s'.format(time.time() - t))\n\n\ndef test():\n    model.eval()\n    output = model(features, adj)\n    loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n    acc_test = accuracy(output[idx_test], labels[idx_test])\n    print(\"Test set results:\",\n          \"loss= {:.4f}\".format(loss_test.item()),\n          \"accuracy= {:.4f}\".format(acc_test.item()))\n\n\n# Train model\nt_total = time.time()\nfor epoch in range(200):\n    train(epoch)\nprint(\"Optimization Finished!\")\nprint(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n\n# Testing\ntest()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}