{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"Every day, the average human heart beats around 100,000 times, pumping 2,000 gallons of blood through the body. Inside your body there are 60,000 miles of blood vessels.\nThe signs of a woman having a heart attack are much less noticeable than the signs of a male. In women, heart attacks may feel uncomfortable squeezing, pressure, fullness, or pain in the center of the chest. It may also cause pain in one or both arms, the back, neck, jaw or stomach, shortness of breath, nausea and other symptoms. Men experience typical symptoms of heart attack, such as chest pain , discomfort, and stress. They may also experience pain in other areas, such as arms, neck , back, and jaw, and shortness of breath, sweating, and discomfort that mimics heartburn.\nItâ€™s a lot of work for an organ which is just like a large fist and weighs between 8 and 12 ounces.\n\nsource: healthblog.uofmhealth"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"# Part 1: EDA\n## Dataset Columns: \n* id (Unique id for each patient)\n* age (Age of the patient in years)\n* origin (place of study)\n* sex (Male/Female)\n* cp chest pain type ([typical angina, atypical angina, non-anginal, asymptomatic])\n* trestbps resting blood pressure (resting blood pressure (in mm Hg on admission to the hospital))\n* chol (serum cholesterol in mg/dl)\n* fbs (if fasting blood sugar > 120 mg/dl)\n* restecg (resting electrocardiographic results)\n* Values: [normal, stt abnormality, lv hypertrophy]\n* thalach: maximum heart rate achieved\n* exang: exercise-induced angina (True/ False)\n* oldpeak: ST depression induced by exercise relative to rest\n* slope: the slope of the peak exercise ST segment\n* ca: number of major vessels (0-3) colored by fluoroscopy\n* thal: [normal; fixed defect; reversible defect]\n* num: the predicted attribute"},{"metadata":{},"cell_type":"markdown","source":"## 1.1 Loading the modules and dataset\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport plotly.figure_factory as ff\nimport plotly.graph_objects as go\n\n\n%matplotlib inline\ndata = pd.read_csv('../input/heart-disease-data/heart_disease_uci.csv')\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1.2 Handling Missing Data:\nlet's have a look at the original dataset and see if there is some missing values. If there is any missing values in some patient's record, we will drop the record altogether. Now we can simply drop the rows with missing values and we have a good amount of data. Now we will shuffle the dataset in order to have a uniform distribution when we split them into train and test dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"data.dropna(inplace = True)\nfrom sklearn.utils import shuffle\ndata = shuffle(data)\ndata.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we are now done with handling the missing values, we can simply do a data visualization before we start with machine learning model model.\n\n### 1.3 Age Distribution: "},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = go.Figure()\nfig.add_trace(go.Box(y=data['age'].values , name='Age', marker_color = 'green',boxmean=True))\nfig.add_trace(go.Box(y=data[data['sex']=='Male']['age'].values, name ='Male only', marker_color = 'blue', boxmean = True))\nfig.add_trace(go.Box(y=data[data['sex']=='Female']['age'].values, name ='Female only', marker_color = 'red', boxmean = True))\nfig.update_layout(title = 'Age Distribution(all)', yaxis_title = 'Age', title_x = 0.5)\nfig.update_xaxes(showline=True, linewidth=2, linecolor='black', mirror=True)\nfig.update_yaxes(showline=True, linewidth=2, linecolor='black', mirror=True)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see the histogram of the ages. "},{"metadata":{"trusted":true},"cell_type":"code","source":"group_labels = ['Age Distribution'] # name of the dataset\nfig = ff.create_distplot([data.age], group_labels)\nfig.update_layout(title = 'Age Distribution(all)', yaxis_title = 'propotion', xaxis_title = 'Age', title_x = 0.5)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1.4 Male and Female Propotion"},{"metadata":{"trusted":true},"cell_type":"code","source":"df=data['sex'].value_counts().reset_index().rename(columns={'index':'sex','sex':'count'})\nfig = go.Figure([go.Pie(labels=['Male', 'Female'],values=df['count'], hole = 0.5)])\nfig.update_traces(hoverinfo='label+percent', textinfo='value+percent', textfont_size=15,insidetextorientation='radial')\nfig.update_layout(title=\"Male to Female ratio in the study\",title_x=0.5)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1.5 Dataset Contributors:\nAs we already know that the whole dataset is actually an amalgamation of four different independent studies. However, after cleaning, let's have a look at he propotion of the data of different contributors."},{"metadata":{"trusted":true},"cell_type":"code","source":"df=data['dataset'].value_counts().reset_index().rename(columns={'index':'dataset','dataset':'count'})\nfig = go.Figure([go.Pie(labels=df['dataset'],values=df['count'], hole = 0.5)])\nfig.update_traces(hoverinfo='label+percent', textinfo='value+percent', textfont_size=15,insidetextorientation='radial')\nfig.update_layout(title=\"Dataset Contributors\",title_x=0.5)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1.6 CP(Chest Pain Type) Propotions\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"df=data['cp'].value_counts().reset_index().rename(columns={'index':'cp','cp':'count'})\nfig = go.Figure([go.Pie(labels=df['cp'],values=df['count'], hole = 0.5)])\nfig.update_traces(hoverinfo='label+percent', textinfo='value+percent', textfont_size=15,insidetextorientation='radial')\nfig.update_layout(title=\"Chest Pain Conditions\",title_x=0.5)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1.7 Resting Blood Pressure vs Gender: "},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = go.Figure()\nfig.add_trace(go.Box(y=data['trestbps'].values , name='BP at Rest for all', marker_color = 'green',boxmean=True))\nfig.add_trace(go.Box(y=data[data['sex']=='Male']['trestbps'].values, name ='Male only', marker_color = 'blue', boxmean = True))\nfig.add_trace(go.Box(y=data[data['sex']=='Female']['trestbps'].values, name ='Female only', marker_color = 'red', boxmean = True))\nfig.update_layout(title = 'BP Distribution', yaxis_title = 'Blood Pressure (mm/Hg)', title_x = 0.5)\nfig.update_xaxes(showline=True, linewidth=2, linecolor='black', mirror=True)\nfig.update_yaxes(showline=True, linewidth=2, linecolor='black', mirror=True)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1.8 Resting Blood Prssure vs Disease"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = go.Figure()\nfig.add_trace(go.Box(y=data['trestbps'].values , name='BP at Rest for all', marker_color = 'green',boxmean=True))\nfig.add_trace(go.Box(y=data[data['num']== 0]['trestbps'].values, name ='No Disease', marker_color = 'blue', boxmean = True))\nfig.add_trace(go.Box(y=data[data['num'] !=0]['trestbps'].values, name ='Heart Disease', marker_color = 'red', boxmean = True))\nfig.update_layout(title = 'BP Distribution (at rest)', yaxis_title = 'Blood Pressure (mm/Hg)', title_x = 0.5)\nfig.update_xaxes(showline=True, linewidth=2, linecolor='black', mirror=True)\nfig.update_yaxes(showline=True, linewidth=2, linecolor='black', mirror=True)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1.9 Cholesterol Level Distribution"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = go.Figure()\nfig.add_trace(go.Violin(y=data['chol'].values , name='All Patient', marker_color = 'green'))\nfig.add_trace(go.Violin(y=data[data['num']== 0]['chol'].values, name ='No Disease', marker_color = 'blue'))\nfig.add_trace(go.Violin(y=data[data['num'] ==4]['chol'].values, name ='Heart Disease', marker_color = 'red'))\nfig.update_layout(title = 'Cholesterol Level Distribution', yaxis_title = 'Cholesterol Level', title_x = 0.5)\nfig.update_xaxes(showline=True, linewidth=2, linecolor='black', mirror=True)\nfig.update_yaxes(showline=True, linewidth=2, linecolor='black', mirror=True)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Part 2: Classification "},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head(30)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.1 Pre-Processing"},{"metadata":{},"cell_type":"markdown","source":"If we just look at the data, we will see some of the features have categorical values. So we have to do one hot encoding for them. Also the original dataset contains the target as 0, 1, 2, 3, 4. But for identifying simply the presence of disease, we will take binary classification. With that view in mind, we will covert all the target features in the `num` column into 1/0. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# In some of the features, there is space will will create problem later on. \n# So we rename those attributes to handle problems in the future.\n\n# data[\"restecg\"].replace({\"lv hypertrophy\": \"lv_hypertrophy\",\"st-t abnormality\": \"stt_abnormality\" }, inplace=True)\ndata['thal'].replace({'fixed defect':'fixed_defect' , 'reversable defect': 'reversable_defect' }, inplace =True)\ndata['cp'].replace({'typical angina':'typical_angina', 'atypical angina': 'atypical_angina' }, inplace =True)\n\n\ndata_tmp = data[['age','sex','cp', 'trestbps', 'chol', 'fbs',  'thalch', 'exang', 'oldpeak', 'slope', 'ca', 'thal']].copy()\ndata_tmp['target'] = ((data['num'] > 0)*1).copy()\ndata_tmp['sex'] = (data['sex'] == 'Male')*1\ndata_tmp['fbs'] = (data['fbs'])*1\ndata_tmp['exang'] = (data['exang'])*1\n\ndata_tmp.columns = ['age', 'sex', 'chest_pain_type', 'resting_blood_pressure', \n              'cholesterol', 'fasting_blood_sugar',\n              'max_heart_rate_achieved', 'exercise_induced_angina', \n              'st_depression', 'st_slope_type', 'num_major_vessels', \n              'thalassemia_type', 'target']\ndata_tmp.head(15)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.2 One-hot Encoding"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.get_dummies(data_tmp, drop_first=False)\ndata.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.3 Logistic Regression: \n### 2.3.1 Gathering Data\nlet us separate the input and labels for the dataset and thus we will be able to put them in the training models. \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ny = data['target']\nX = data.drop('target', axis = 1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\nprint(f'Shape of X_train: {X_train.shape}')\nprint(f'Shape of y_train: {y_train.shape}')\nprint(f'Shape of X_test: {X_test.shape}')\nprint(f'Shape of y_test: {y_test.shape}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.3.3. Normalization\nMin-Max Normalization method is used to Normalize the data. This method scales the data range to [0,1]. Standardization is also used on a feature-wise basis in most cases.  Normalization is done by the following formula. \n$$ x_{scaled} = \\frac{(x - x_{min})}{(x_{max}-x_{min})}$$"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train=(X_train-np.min(X_train))/(np.max(X_train)-np.min(X_train)).values\nX_test=(X_test-np.min(X_test))/(np.max(X_test)-np.min(X_test)).values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.3.4 Fitting Into the Regression Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlogre = LogisticRegression()\nlogre.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.3.5 Prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = logre.predict(X_test)\nactual = []\npredcition = []\nfor i,j in zip(y_test,y_pred):\n    actual.append(i)\n    predcition.append(j)\n    \ndic = {'Actual':actual,\n       'Prediction':predcition }\n\nresult  = pd.DataFrame(dic)\nimport plotly.graph_objects as go\n \nfig = go.Figure()\nfig.add_trace(go.Scatter(x=np.arange(0,len(y_test)), y=y_test,    mode='markers',  name='Test'))\nfig.add_trace(go.Scatter(x=np.arange(0,len(y_test)), y=y_pred,  mode='markers',  name='Pred'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the above figure, the red dots represent the predicted values that is either 0 or 1 and the blue line & and dot represents the actual value of that particular patient. In the places where the red dot and blue dot do not overlap are the wrong predictions and where the both dots overlap those are the right predicted values."},{"metadata":{},"cell_type":"markdown","source":"### 2.3.6 Model Evaluation\n#### 2.3.6.1 Accuracy"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nprint('The Accuracy Score is: ', accuracy_score(y_test,y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 2.3.6.2 Precision, Recall, F1-Score, Support"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report\nprint(classification_report(y_test,y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 2.3.6.3 Confusion Matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nprint(confusion_matrix(y_test,y_pred))\nsns.heatmap(confusion_matrix(y_test,y_pred),annot=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"![](https://miro.medium.com/max/356/1*gdIoF8dsWv3dbKSeHLZy_A.png)"},{"metadata":{},"cell_type":"markdown","source":"#### 2.3.6.4 Area under ROC and ROC Curve"},{"metadata":{"trusted":true},"cell_type":"code","source":"import sklearn\nprint('Area Under ROC-Curve: ', sklearn.metrics.roc_auc_score(y_test,y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_curve\nfpr, tpr, thresholds = roc_curve(y_test, y_pred, drop_intermediate = False)\nplt.plot(fpr,tpr)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.title('ROC curve for Heart disease classifier')\nplt.xlabel('False positive rate (1-Specificity)')\nplt.ylabel('True positive rate (Sensitivity)')\nplt.grid(True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Part 3: Analysis\n## 3.1 Co-efficents: \nLinear Regression actually calculates the total outcome by summing up the weighted sum of the different features. Let's have a look at those weights. "},{"metadata":{"trusted":true},"cell_type":"code","source":"print(logre.intercept_)\nplt.figure(figsize=(10,12))\ncoeffecients = pd.DataFrame(logre.coef_.ravel(),X.columns)\ncoeffecients.columns = ['Coeffecient']\ncoeffecients.sort_values(by=['Coeffecient'],inplace=True,ascending=False)\ncoeffecients","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. Conclusion\n* The Area under the ROC curve is 87.09% which is somewhat satisfactory.\n* The model predicted with 86.88% accuracy. The model is more specific than sensitive.\n* According to this model the major features contributing in precision of predicting model are shown in the heatmap in Ascending order.\n"},{"metadata":{},"cell_type":"markdown","source":"plt.figure(figsize=(10,12))\ncoeffecients = pd.DataFrame(logre.coef_.ravel(),X.columns)\ncoeffecients.columns = ['Coeffecient']\ncoeffecients.sort_values(by=['Coeffecient'],inplace=True,ascending=False)\nsns.heatmap(coeffecients,annot=True,fmt='.2f',cmap='Set2',linewidths=0.5)"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}