{"cells":[{"metadata":{},"cell_type":"markdown","source":"# ML Project 1\n## Introduction\nThe objective of this project is to train several classification models, and practice model tuning (bias/varience) tradeoff. \n\n## Agenda\n1. Data Set Selection \n1. EDA\n1. Models\n1. AutoML\n\n## Team members\n1. Eden Zere\n1. Essey Abraham Tezare\n1. Hussien Mohamed Bayoumy Mohamed Elgabry\n1. Mario Arismendi Matos\n1. Moustafa Ahmed Galal Bahnasawy\n1. Youssef Samy Mounir\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 1. Data Set Selection\n\nObjective is to understand the influence of various factors like economic, personal and social on the depression.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 1.1 Import libraries","execution_count":null},{"metadata":{"_cell_guid":"cfdaacbc-23a3-423d-8d4d-120939ac7383","trusted":true},"cell_type":"code","source":"# Imports\n\n# pandas\nimport pandas as pd\nfrom pandas import Series,DataFrame\n\n# numpy, matplotlib, seaborn\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport missingno as missing\nimport seaborn as sns\nsns.set_style('whitegrid')\n%matplotlib inline\n\nimport random\nfrom sklearn import linear_model\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import learning_curve, validation_curve\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn import metrics\nfrom sklearn.metrics import make_scorer, accuracy_score\nfrom sklearn.metrics import roc_curve, roc_auc_score ,auc, plot_roc_curve\nfrom sklearn import svm\nimport sklearn.metrics\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.preprocessing import OneHotEncoder\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import cross_validate\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1.2 Reading the data","execution_count":null},{"metadata":{"_cell_guid":"3ab4c525-a5cb-4183-9468-c1dd005c4c78","trusted":true},"cell_type":"code","source":"\ndf = pd.read_csv(\"../input/b_depressed.csv\")\n\n# preview the data\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1.3 Data Dictionary\n1. * Survey_id\n1. * Ville_id\n1. * sex\n1. * Age\n1. * Married\n1. * Number_children\n1. * education_level\n1. * total_members (in the family)\n1. * gained_asset\n1. * durable_asset\n1. * save_asset\n1. * living_expenses\n1. * other_expenses\n1. * incoming_salary\n1. * incoming_own_farm\n1. * incoming_business\n1. * incoming_no_business\n1. * incoming_agricultural\n1. * farm_expenses\n1. * labor_primary\n1. * lasting_investment\n1. * no_lasting_investmen\n1. * depressed: [ Zero: No depressed]  or [One: depressed] (Binary for target class)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 1.4 Training Data Info","execution_count":null},{"metadata":{"_cell_guid":"86179af8-3cb4-4661-84ea-addd2c7679d4","trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Our data seems to be clean of missing values.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 1.5 Data description","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1.6 Distict values","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets check the no. of unique items present in the categorical column\n\ndf.select_dtypes('object').nunique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. EDA","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 2.1 Checking for Skewness","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(25,6))\nplt.subplot(1, 4, 1)\nsns.distplot(df['Age'])\n\nplt.subplot(1, 4, 2)\nsns.distplot(df['Number_children'])\n\nplt.subplot(1, 4, 3)\nsns.distplot(df['education_level'])\n\nplt.subplot(1, 4, 4)\nsns.distplot(df['no_lasting_investmen'])\n\nplt.suptitle('Checking for Skewness', fontsize = 15)\nplt.show()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(25,6))\nplt.subplot(1, 4, 1)\nsns.distplot(df['farm_expenses'])\n\nplt.subplot(1, 4, 2)\nsns.distplot(df['incoming_agricultural'])\n\nplt.subplot(1, 4, 3)\nsns.distplot(df['gained_asset'])\n\nplt.subplot(1, 4, 4)\nsns.distplot(df['durable_asset'])\n\nplt.suptitle('Checking for Skewness', fontsize = 15)\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> level of education is almost normally distributed, as for age, children and no lasting investment all are positively distributed meaning that most of the data is less than the medium same also goes for farm expense, incoming agriculture, gained and durable assets","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 2.2 Relation between features and Depression","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 2.2.1 Personal Info VS depressed","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,15))\n\nplt.subplot(3,3,1)\nsns.countplot(x='sex', hue='depressed', data=df)\nplt.subplot(3,3,2)\nsns.countplot(x='Married', hue='depressed', data=df)\nplt.subplot(3,3,3)\nsns.countplot(x='Number_children', hue='depressed', data=df)\n\nplt.subplot(3,3,4)\nsns.countplot(x='education_level', hue='depressed', data=df)\nplt.subplot(3,3,5)\nsns.countplot(x='total_members', hue='depressed', data=df)\nplt.subplot(3,3,6)\nsns.countplot(x='incoming_no_business', hue='depressed', data=df)\n\nplt.subplot(3,3,7)\nsns.countplot(x='incoming_salary', hue='depressed', data=df)\nplt.subplot(3,3,8)\nsns.countplot(x='incoming_own_farm', hue='depressed', data=df)\nplt.subplot(3,3,9)\nsns.countplot(x='incoming_business', hue='depressed', data=df)\n\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dfPairplot = df.drop(['Survey_id' , 'Married' , 'Ville_id' , 'sex'  , 'gained_asset' , 'durable_asset' , 'save_asset' , 'living_expenses' , 'other_expenses' , 'incoming_salary' , 'incoming_own_farm' , 'incoming_business' , 'incoming_no_business' , 'incoming_agricultural' , 'farm_expenses' , 'labor_primary' , 'lasting_investment' , 'no_lasting_investmen'], axis=1)\ndfPairplot.head()\nplt.figure(figsize=(25,6))\nsns.pairplot(data=dfPairplot,hue='depressed',plot_kws={'alpha':0.2})\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":">in the observation in this graph the older you and the higher education you the less likely you will get depressed yet children and family members doesn't have the much of an effect","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dfPairplot = df.drop(['save_asset','Survey_id' , 'Ville_id' , 'sex' , 'Age' , 'Married' , 'Number_children' , 'education_level' , 'total_members' , 'living_expenses' , 'other_expenses' , 'incoming_salary' , 'incoming_own_farm' , 'incoming_business' , 'incoming_no_business' , 'incoming_agricultural' , 'farm_expenses' , 'labor_primary' , 'lasting_investment' , 'no_lasting_investmen'], axis=1)\ndfPairplot.head()\nplt.figure(figsize=(25,6))\nsns.pairplot(data=dfPairplot,hue='depressed',plot_kws={'alpha':0.2})\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"gaining assets is not as much effective as durable assets which consistently bring  money to its owner we realize that durable assets increase cause depression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dfPairplot = df.drop(['Survey_id' , 'Ville_id' , 'sex' , 'Age' , 'Married' , 'Number_children' , 'education_level' , 'total_members' , 'gained_asset' , 'durable_asset' , 'save_asset' , 'living_expenses' , 'other_expenses' , 'incoming_salary' , 'incoming_own_farm' , 'incoming_business' , 'incoming_no_business'  , 'labor_primary'     , 'no_lasting_investmen'], axis=1)\ndfPairplot.head()\nplt.figure(figsize=(25,6))\nsns.pairplot(data=dfPairplot,hue='depressed',plot_kws={'alpha':0.2})\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> most of the dataset is not depressed, yet you can notice in some spots depression is fairly distributed in all, yet we can find more males, more married, more with no income","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 2.2.2 Age VS depressed","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"facet = sns.FacetGrid(df,hue=\"depressed\", aspect=4)\nfacet.map(sns.kdeplot, 'Age', shade=True)\nfacet.set(xlim=(0,df['Age'].max()))\nfacet.add_legend()\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"in the age from 17,18 to age 35,36 its less likely to get depressed than older than 36 and younger than 17","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 2.2.3 Gain Asset VS depressed","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"facet = sns.FacetGrid(df,hue=\"depressed\", aspect=4)\nfacet.map(sns.kdeplot, 'gained_asset', shade=True)\nfacet.set(xlim=(0,df['gained_asset'].max()))\nfacet.add_legend()\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"as we discussed before gained asset is almost averagely distributed and not effective","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 2.2.4 Durable Asset VS depressed","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"facet = sns.FacetGrid(df,hue=\"depressed\", aspect=4)\nfacet.map(sns.kdeplot, 'durable_asset', shade=True)\nfacet.set(xlim=(0,df['durable_asset'].max()))\nfacet.add_legend()\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"however people who have from two to 3 durable assets is in favor of being depressed","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 2.2.5 Incoming Agricultural VS depressed","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"facet = sns.FacetGrid(df,hue=\"depressed\", aspect=4)\nfacet.map(sns.kdeplot, 'incoming_agricultural', shade=True)\nfacet.set(xlim=(0,df['incoming_agricultural'].max()))\nfacet.add_legend()\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> from 2.5 to 3.5 incoming agriculture means more not depressed","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 2.2.6 Famr expenses VS depressed","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"facet = sns.FacetGrid(df,hue=\"depressed\", aspect=4)\nfacet.map(sns.kdeplot, 'farm_expenses', shade=True)\nfacet.set(xlim=(0,df['farm_expenses'].max()))\nfacet.add_legend()\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> from 2.5 to 3.5 incoming Farm expenses means more not depressed\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 2.2.7 Lasting investment VS depressed","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"facet = sns.FacetGrid(df,hue=\"depressed\", aspect=4)\nfacet.map(sns.kdeplot, 'lasting_investment', shade=True)\nfacet.set(xlim=(0,df['lasting_investment'].max()))\nfacet.add_legend()\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> from 2.5 to 3.5 incoming Lasting investment means more not depressed  ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 2.2.8 Correclation Matrix between features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dfCorr = df.drop(['no_lasting_investmen'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.subplots(figsize=(20,10)) \nsns.heatmap(dfCorr.corr(), annot = True, fmt = \".2f\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> It is observed that depression is most correclated with age with 0.11 (positive correclation), and -0.1 with education (negative correclation).\n\n> There is a string correlation between number of family and number of children, this is very logical.\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 2.2.9 Drop unneccessary columns\nNow we can drop math score, reading score and writing score, as we will use the pass column instead.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dfDrop = df.drop(['no_lasting_investmen', 'Survey_id', 'Ville_id', 'gained_asset', 'durable_asset', 'save_asset', 'farm_expenses', 'labor_primary', 'Number_children','lasting_investment','incoming_agricultural'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. Models","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### This function is for drawing the learning curve.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Learning curve is a measurement to check how well the model learns. This is measured by taking a reading of the accuracy of the algorithm as it trains and also while it is testing. This are plotting to see the convergence.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def plotLearningCurves(X_train, y_train, classifier, title):\n    train_sizes, train_scores, test_scores = learning_curve(\n            classifier, X_train, y_train, cv=5, scoring=\"accuracy\")\n    \n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    \n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"b\" ,label=\"Training Error\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"r\" ,label=\"Cross Validation Error\")\n    \n    plt.legend()\n    plt.grid()\n    plt.title(title, fontsize = 18, y = 1.03)\n    plt.xlabel('Data Size', fontsize = 14)\n    plt.ylabel('Error', fontsize = 14)\n    plt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### This function is for drawing the validation curve.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Cross validation is a measure of how well our model can generalize from what it learns. How well will it perform with data it has neven seen before. This is done by saving part of the data to later predict and measure the accuracy. The training data is split with differing testing folds to be used. Default in this case is k=5 folds.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def plotValidationCurves(X_train, y_train, classifier, param_name, param_range, title):\n    train_scores, test_scores = validation_curve(\n        classifier, X_train, y_train, param_name = param_name, param_range = param_range,\n        cv=5, scoring=\"accuracy\")\n\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n\n    plt.plot(param_range, train_scores_mean, 'o-', color=\"b\" ,label=\"Training Error\")\n    plt.plot(param_range, test_scores_mean, 'o-', color=\"r\" ,label=\"Cross Validation Error\")\n\n    plt.legend()\n    plt.grid()\n    plt.title(title, fontsize = 18, y = 1.03)\n    plt.xlabel('Complexity', fontsize = 14)\n    plt.ylabel('Error', fontsize = 14)\n    plt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### This function is for printing the confusion matrix","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"The confusion matrix shows the frequency for True Positives, True Negatives, False Positives, and False Negative. Also a summary of the different properties can be presented here, along with the accuracy for predicted values.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def printConfusionMatrix(y_train, pred):\n    print(\"Confusion Matrix:\")\n    print(confusion_matrix(y_test, pred))\n    print(\"Classification Report:\",)\n    print (classification_report(y_test, pred))\n    print(\"Accuracy:\", accuracy_score(y_test, pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3.1 Random Forest ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X = dfDrop.iloc[:, :-1].values\ny = dfDrop.iloc[:, -1].values\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf = RandomForestClassifier(n_estimators = 9,\n                                    max_depth=3,\n                                    min_samples_split=9,\n                                    min_samples_leaf=5\n                                   )\nrf.fit(X_train, y_train)\nrf_pred1 = rf.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (16,5))\ntitle = 'Random Forest Learning Curve 1'\nplotLearningCurves(X_train, y_train, rf, title)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"title = 'Random Forest Validation Curve 1'\nparam_name = 'n_estimators'\nparam_range = [4, 6, 9]\nplt.figure(figsize = (16,5))\nplotValidationCurves(X_train, y_train, rf, param_name, param_range, title)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"printConfusionMatrix(y_test, rf_pred1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_roc_curve(rf, X_test, y_test)\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Using Entropy instead of default (gini)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"rf = RandomForestClassifier(n_estimators = 9,\n                                    max_depth=3,\n                                    criterion='entropy',\n                                    min_samples_split=9,\n                                    min_samples_leaf=5\n                                   )\nrf.fit(X_train, y_train)\nrf_pred2 = rf.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (16,5))\ntitle = 'Random Forest Learning Curve 2'\nplotLearningCurves(X_train, y_train, rf, title)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (16,5))\ntitle = 'Random Forest Validation Curve 2'\nparam_name = 'n_estimators'\nparam_range = [4, 6, 9]\nplotValidationCurves(X_train, y_train, rf, param_name, param_range, title)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"printConfusionMatrix(y_test, rf_pred2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_roc_curve(rf, X_test, y_test)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf = RandomForestClassifier(n_estimators = 9,\n                                    max_depth=3,\n                                    criterion='entropy',\n                                    min_samples_split=10,\n                                    min_samples_leaf=5\n                                   )\nrf.fit(X_train, y_train)\nrf_pred3 = rf.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (16,5))\ntitle = 'Random Forest Learning Curve 3'\nplotLearningCurves(X_train, y_train, rf, title)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"title = 'Random Forest Validation Curve 3'\nparam_name = 'n_estimators'\nparam_range = [4, 6, 9]\nplt.figure(figsize = (16,5))\nplotValidationCurves(X_train, y_train, rf, param_name, param_range, title)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"printConfusionMatrix(y_test, rf_pred3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_roc_curve(rf, X_test, y_test)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf = RandomForestClassifier(n_estimators = 9,\n                                    max_depth=5,\n                                    criterion='entropy',\n                                    min_samples_split=9,\n                                    min_samples_leaf=10\n                                   )\nrf.fit(X_train, y_train)\nrf_pred4 = rf.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (16,5))\ntitle = 'Random Forest Learning Curve 4'\nplotLearningCurves(X_train, y_train, rf, title)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"title = 'Random Forest Validation Curve 4'\nparam_name = 'n_estimators'\nparam_range = [4, 6, 9]\nplt.figure(figsize = (16,5))\nplotValidationCurves(X_train, y_train, rf, param_name, param_range, title)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"printConfusionMatrix(y_test, rf_pred4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_roc_curve(rf, X_test, y_test)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf = RandomForestClassifier(n_estimators = 9,\n                                    max_depth=5,\n                                    criterion='entropy',\n                                    max_features='sqrt',\n                                    min_samples_split=9,\n                                    min_samples_leaf=5\n                                   )\nrf.fit(X_train, y_train)\nrf_pred5 = rf.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (16,5))\ntitle = 'Random Forest Learning Curve 5'\nplotLearningCurves(X_train, y_train, rf, title)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"title = 'Random Forest Validation Curve 5'\nparam_name = 'n_estimators'\nparam_range = [4, 6, 9]\nplt.figure(figsize = (16,5))\nplotValidationCurves(X_train, y_train, rf, param_name, param_range, title)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nprintConfusionMatrix(y_test, rf_pred5)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_roc_curve(rf, X_test, y_test)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### GridSearch Results","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Classifier = RandomForestClassifier()\ngrid_obj = GridSearchCV(Classifier,\n                        {'n_estimators': [4, 6, 9],\n                         'max_features': ['log2', 'sqrt','auto'],\n                         'criterion': ['entropy', 'gini'],\n                         'max_depth': [2, 3, 5, 8],\n                         'min_samples_split': [2, 5, 8, 10],\n                         'min_samples_leaf': [1, 3, 5]\n                        },\n                        scoring=make_scorer(accuracy_score))\ngrid_obj = grid_obj.fit(X_train, y_train)\n\n# Set the clf to the best combination of parameters\nClassifier = grid_obj.best_estimator_\n\n# Fit the best algorithm to the data. \nClassifier.fit(X_train, y_train)\n\npredictions = Classifier.predict(X_test)\n\nprint(\"Best Params: \" , grid_obj.best_estimator_)\nprint(\"Best Score: \" , grid_obj.best_score_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3.2 KNN","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X = dfDrop.iloc[:, :-1].values\ny = dfDrop.iloc[:, -1].values\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create KNN classifier\nknn=KNeighborsClassifier(n_neighbors=3)\n# Fit the classifier to the data\nknn.fit(X_train,y_train)\n#show first 5 model predictions on the test data\nknn_pred1=knn.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,5))\ntitle='KNN Learning Curve 1'\nplotLearningCurves(X_train,y_train,knn,title)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"title = 'KNN Validation Curve 1' \nparam_name = 'n_neighbors'\nparam_range = np.arange(1,9,2)\nplt.figure(figsize = (16,5))\nplotValidationCurves(X_train, y_train, knn, param_name, param_range, title)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"printConfusionMatrix(y_test, knn_pred1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_roc_curve(knn, X_test, y_test)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create KNN classifier\nknn=KNeighborsClassifier(n_neighbors=7)\n# Fit the classifier to the data\nknn.fit(X_train,y_train)\n#show first 5 model predictions on the test data\nknn_pred2=knn.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,5))\ntitle='KNN Learning Curve 2'\nplotLearningCurves(X_train,y_train,knn,title)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"title = 'KNN Validation Curve 2' \nparam_name = 'n_neighbors'\nparam_range = np.arange(1,9,2)\nplt.figure(figsize = (16,5))\nplotValidationCurves(X_train, y_train, knn, param_name, param_range, title)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"printConfusionMatrix(y_test, knn_pred2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_roc_curve(knn, X_test, y_test)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create KNN classifier\nknn=KNeighborsClassifier(n_neighbors=10)\n# Fit the classifier to the data\nknn.fit(X_train,y_train)\n#show first 5 model predictions on the test data\nknn_pred3=knn.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,5))\ntitle='KNN Learning Curve 3'\nplotLearningCurves(X_train,y_train,knn,title)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"title = 'KNN Validation Curve 3' \nparam_name = 'n_neighbors'\nparam_range = np.arange(1,9,2)\nplt.figure(figsize = (16,5))\nplotValidationCurves(X_train, y_train, knn, param_name, param_range, title)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"printConfusionMatrix(y_test, knn_pred3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_roc_curve(knn, X_test, y_test)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create KNN classifier\nknn=KNeighborsClassifier(n_neighbors=20)\n# Fit the classifier to the data\nknn.fit(X_train,y_train)\n#show first 5 model predictions on the test data\nknn_pred4=knn.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,5))\ntitle='KNN Learning Curve 4'\nplotLearningCurves(X_train,y_train,knn,title)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"title = 'KNN Validation Curve 4' \nparam_name = 'n_neighbors'\nparam_range = np.arange(1,9,2)\nplt.figure(figsize = (16,5))\nplotValidationCurves(X_train, y_train, knn, param_name, param_range, title)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"printConfusionMatrix(y_test, knn_pred4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_roc_curve(knn, X_test, y_test)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create KNN classifier\nknn=KNeighborsClassifier(n_neighbors=17)\n# Fit the classifier to the data\nknn.fit(X_train,y_train)\n#show first 5 model predictions on the test data\nknn_pred5=knn.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,5))\ntitle='KNN Learning Curve 5'\nplotLearningCurves(X_train,y_train,knn,title)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"title = 'KNN Validation Curve 5' \nparam_name = 'n_neighbors'\nparam_range = np.arange(1,9,2)\nplt.figure(figsize = (16,5))\nplotValidationCurves(X_train, y_train, knn, param_name, param_range, title)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"printConfusionMatrix(y_test, knn_pred5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_roc_curve(knn, X_test, y_test)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Grid Search Results","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#create new a knn model\nknn2=KNeighborsClassifier()\n#create a dictionary of all values we want to test for n_neighbors\nparam_grid= {'n_neighbors': np.arange(1, 20)}\n#use gridsearch to test all values for n_neighbors\nknn_gscv=GridSearchCV(knn2, param_grid, cv=5)\n#fit model to data\nknn_gscv.fit(X, y)\n\nprint(\"Best Params: \" , knn_gscv.best_estimator_)\nprint(\"Best Score: \" , knn_gscv.best_score_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. Best Model (Over All AUC) and AutoML","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 4.1 AUC curve over all models","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# Instantiate the classfiers and make a list\nclassifiers = [RandomForestClassifier(),\n               KNeighborsClassifier()]\n\nresult_table = pd.DataFrame(columns=['classifiers', 'fpr','tpr','auc'])\n\n\n# print('auc =', auc)\nlr_fpr1, lr_tpr1, _ = roc_curve(y_test, rf_pred5)\nlr_fpr2, lr_tpr2, _ = roc_curve(y_test,  knn_pred2)\n\n# fpr , tpr, _= roc_curve(X_test, predict6_test)\nauc1 = roc_auc_score(y_test, rf_pred5)\nauc2 = roc_auc_score(y_test,  knn_pred2)\n \n    \nresult_table = result_table.append({'classifiers':RandomForestClassifier.__class__.__name__,\n                                     'fpr':lr_fpr1, \n                                     'tpr':lr_tpr1, \n                                     'auc':auc1}, ignore_index=True)\n\nresult_table = result_table.append({'classifiers':KNeighborsClassifier.__class__.__name__,\n                                     'fpr':lr_fpr2, \n                                     'tpr':lr_tpr2, \n                                     'auc':auc2}, ignore_index=True)\n\n\nfig = plt.figure(figsize=(8,6))\n\nplt.plot(result_table.loc[0]['fpr'], \n         result_table.loc[0]['tpr'], \n         label=\"RandomForestClassifier, AUC={:.3f}\".format( result_table.loc[0]['auc']))\n\nplt.plot(result_table.loc[1]['fpr'], \n         result_table.loc[1]['tpr'], \n         label=\"KNeighborsClassifier, AUC={:.3f}\".format( result_table.loc[1]['auc']))\n\n\nplt.xticks(np.arange(0.0, 1.1, step=0.1))\nplt.xlabel(\"Flase Positive Rate\", fontsize=15)\n\nplt.yticks(np.arange(0.0, 1.1, step=0.1))\nplt.ylabel(\"True Positive Rate\", fontsize=15)\n\nplt.title('ROC Curve Analysis', fontweight='bold', fontsize=15)\nplt.legend(prop={'size':13}, loc='lower right')\n\nplt.show()\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4.2 AutoML","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"!apt-get remove swig \n!apt-get install swig3.0 build-essential -y\n!ln -s /usr/bin/swig3.0 /usr/bin/swig\n!apt-get install build-essential\n!pip install --upgrade setuptools\n!pip install auto-sklearn","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = dfDrop.iloc[:, :-1].values\ny = dfDrop.iloc[:, -1].values\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import autosklearn.classification\nimport sklearn.model_selection\nimport sklearn.datasets\nimport sklearn.metrics\nimport os  \nimport autosklearn.regression\n\n\nautoml = autosklearn.classification.AutoSklearnClassifier(\n    time_left_for_this_task=120,\n    per_run_time_limit=30,\n    tmp_folder='/tmp/autosklearn_cv_example_tmp3',\n    output_folder='/tmp/autosklearn_cv_example_out3',\n    delete_tmp_folder_after_terminate=False,\n    resampling_strategy='cv',\n    resampling_strategy_arguments={'folds': 5},\n)\n\n# fit() changes the data in place, but refit needs the original data. We\n# therefore copy the data. In practice, one should reload the data\nautoml.fit(X_train.copy(), y_train.copy(), dataset_name='Students')\n# During fit(), models are fit on individual cross-validation folds. To use\n# all available data, we call refit() which trains all models in the\n# final ensemble on the whole dataset.\nautoml.refit(X_train.copy(), y_train.copy())\n\nprint(automl.show_models())\n\npredictions = automl.predict(X_test)\nprint(\"Accuracy as per AutoML: \", sklearn.metrics.accuracy_score(y_test, predictions))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# References: \n\n* https://www.kaggle.com/jeffd23/scikit-learn-ml-from-start-to-finish\n* https://www.kaggle.com/roshansharma/student-performance-analysis\n* https://www.kaggle.com/spscientist/student-performance-in-exams\n* https://www.kaggle.com/nitindatta/eda-in-depth\n* http://scikit-learn.sourceforge.net/stable/auto_examples/model_selection/plot_validation_curve.html#example-model-selection-plot-validation-curve-py\n* https://chrisalbon.com/machine_learning/model_evaluation/plot_the_validation_curve/\n* https://datascience.stackexchange.com/questions/76304/gridsearchcv-with-random-forest-classifier\n* https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#:~:text=A%20random%20forest%20classifier.%20A%20random%20forest%20is,to%20improve%20the%20predictive%20accuracy%20and%20control%20over-fitting.\n* https://scikit-learn.org/stable/modules/multiclass.html#multioutput-regression\n* https://www.datacamp.com/community/tutorials/random-forests-classifier-python\n* https://www.tutorialspoint.com/machine_learning_with_python/machine_learning_with_python_classification_algorithms_random_forest.htm\n* https://florianhartl.com/thoughts-on-machine-learning-dealing-with-skewed-classes.html\n* https://www.kaggle.com/ahmedengu/lanl-master-s-features-autosklearn","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}