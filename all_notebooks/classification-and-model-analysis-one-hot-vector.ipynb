{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Read Mushroom Dataset\nfile = \"/kaggle/input/mushroom-classification/mushrooms.csv\"\ndf = pd.read_csv(file)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check for NA \ndf.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check for available classes for classification\ndf['class'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Label encoder to convert the dataset\nfrom sklearn import preprocessing\nle = preprocessing.LabelEncoder()\nfor col in df.columns:\n    df[col] = le.fit_transform(df[col])\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualizing outliers if any\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.figure(figsize=(60, 6))\nsns.boxplot(x=\"variable\", y=\"value\", data=pd.melt(df))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting correlation matrix which depicts that, there is good amount of correlation among the features.\nplt.figure(figsize=(25, 15))\nsns.heatmap(df.corr(),\n            vmin=-1,\n            cmap='coolwarm',\n            annot=True);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# As our dataset has nominal values, we will be performing the one hot encoding operation and making it more sparse.\nfile = \"/kaggle/input/mushroom-classification/mushrooms.csv\"\ndf = pd.read_csv(file)\ndf_one_hot = pd.concat([df.iloc[:,0], pd.get_dummies(df.iloc[:,1:23])], axis=1)\nfeatures = df_one_hot.iloc[:,1:118]\nlabel = df_one_hot.iloc[:,0] \nlabel = le.fit_transform(label)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\npca = PCA(svd_solver='randomized', random_state=123) #instantiate.\npca.fit(features)\nfig = plt.figure(figsize = (20,5))\nax = plt.subplot(121)\nplt.plot(pca.explained_variance_ratio_)\nplt.xlabel('principal components')\nplt.ylabel('explained variance')\n\nax2 = plt.subplot(122)\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel('number of components')\nplt.ylabel('cumulative explained variance')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# what percentage of variance in data can be explained by first 2,3 and 4 principal components respectively?\npca.explained_variance_ratio_[0:50].sum().round(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_pca = pca.transform(features)# our data transformed with new features as principal components.\ndf_pca = df_pca[:, 0:2] # Since we require first two principal components only.\ndf_s = scaler.fit_transform(df_pca) # s in df_s stands for scaled.\nsns.pairplot(pd.DataFrame(df_s))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checks the possibility in the dataset for clustering and values depicts that the dataset is indeed highly clusterable\n\nfrom sklearn.neighbors import NearestNeighbors\nfrom random import sample\nfrom numpy.random import uniform\nimport numpy as np\nfrom math import isnan\n \ndef hopkins(X):\n    d = X.shape[1]\n    #d = len(vars) # columns\n    n = len(X) # rows\n    m = int(0.1 * n) \n    nbrs = NearestNeighbors(n_neighbors=1).fit(X.values)\n \n    rand_X = sample(range(0, n, 1), m)\n \n    ujd = []\n    wjd = []\n    for j in range(0, m):\n        u_dist, _ = nbrs.kneighbors(uniform(np.amin(X,axis=0),np.amax(X,axis=0),d).reshape(1, -1), 2, return_distance=True)\n        ujd.append(u_dist[0][1])\n        w_dist, _ = nbrs.kneighbors(X.iloc[rand_X[j]].values.reshape(1, -1), 2, return_distance=True)\n        wjd.append(w_dist[0][1])\n \n    H = sum(ujd) / (sum(ujd) + sum(wjd))\n    if isnan(H):\n        print(ujd, wjd)\n        H = 0\n \n    return H\n\nhopkins(pd.DataFrame(df_s))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import KMeans # import.\n\n# silhouette scores to choose number of clusters.\nfrom sklearn.metrics import silhouette_score\ndef sil_score(df):\n    sse_ = []\n    for k in range(2, 15):\n        kmeans = KMeans(n_clusters=k, random_state=123).fit(df_s) # fit.\n        sse_.append([k, silhouette_score(df, kmeans.labels_)])\n    plt.plot(pd.DataFrame(sse_)[0], pd.DataFrame(sse_)[1])\n\nsil_score(df_s)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Elbow method to find possible clusters.\ndef plot_ssd(df):\n    ssd = []\n    for num_clusters in list(range(1,19)):\n        model_clus = KMeans(n_clusters = num_clusters, max_iter=50, random_state=123)\n        model_clus.fit(df)\n        ssd.append(model_clus.inertia_)\n    plt.plot(ssd)\n\nplot_ssd(df_s)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# K-means with K=3.\nkm2c = KMeans(n_clusters=3, max_iter=50, random_state=93)\nkm2c.fit(df_s)\ndf_dummy = pd.DataFrame.copy(df)\ndfkm2c = pd.concat([df_dummy, pd.Series(km2c.labels_)], axis=1)\ndfkm2c.rename(columns={0:'Cluster ID'}, inplace=True)\ndf_dummy = pd.DataFrame.copy(pd.DataFrame(df_s))\ndfpcakm2c = pd.concat([df_dummy, pd.Series(km2c.labels_)], axis=1)\ndfpcakm2c.columns = ['PC1', 'PC2', 'Cluster ID']\nsns.pairplot(data=dfpcakm2c, vars=['PC1', 'PC2'], hue='Cluster ID')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# K-means with K=5.\nkm2c = KMeans(n_clusters=5, max_iter=50, random_state=93)\nkm2c.fit(df_s)\ndf_dummy = pd.DataFrame.copy(df)\ndfkm2c = pd.concat([df_dummy, pd.Series(km2c.labels_)], axis=1)\ndfkm2c.rename(columns={0:'Cluster ID'}, inplace=True)\ndf_dummy = pd.DataFrame.copy(pd.DataFrame(df_s))\ndfpcakm2c = pd.concat([df_dummy, pd.Series(km2c.labels_)], axis=1)\ndfpcakm2c.columns = ['PC1', 'PC2', 'Cluster ID']\nsns.pairplot(data=dfpcakm2c, vars=['PC1', 'PC2'], hue='Cluster ID')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(features,label,test_size=0.2,random_state=4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import roc_curve, auc, classification_report\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nimport xgboost\nfrom sklearn import svm, tree\nfrom sklearn import metrics\n\nclassifiers = []\nnb_model = GaussianNB()\nclassifiers.append((\"Gausian Naive Bayes Classifier\",nb_model))\nlr_model= LogisticRegression()\nclassifiers.append((\"Logistic Regression Classifier\",lr_model))\n# sv_model = svm.SVC()\n# classifiers.append(sv_model)\ndt_model = tree.DecisionTreeClassifier()\nclassifiers.append((\"Decision Tree Classifier\",dt_model))\nrf_model = RandomForestClassifier()\nclassifiers.append((\"Random Forest Classifier\",rf_model))\nxgb_model = xgboost.XGBClassifier()\nclassifiers.append((\"XG Boost Classifier\",xgb_model))\nlda_model = LinearDiscriminantAnalysis()\nclassifiers.append((\"Linear Discriminant Analysis\", lda_model))\ngp_model =  GaussianProcessClassifier()\nclassifiers.append((\"Gaussian Process Classifier\", gp_model))\nab_model =  AdaBoostClassifier()\nclassifiers.append((\"AdaBoost Classifier\", ab_model))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cv_scores = []\nnames = []\nfor name, clf in classifiers:\n    print(name)\n    clf.fit(X_train, y_train)\n    y_prob = clf.predict_proba(X_test)[:,1] # This will give you positive class prediction probabilities  \n    y_pred = np.where(y_prob > 0.5, 1, 0) # This will threshold the probabilities to give class predictions.\n    print(\"Model Score : \",clf.score(X_test, y_pred))\n    print(\"Number of mislabeled points from %d points : %d\"% (X_test.shape[0],(y_test!= y_pred).sum()))\n    scores = cross_val_score(clf, features, label, cv=10, scoring='accuracy')\n    cv_scores.append(scores)\n    names.append(name)\n    print(\"Cross validation scores : \",scores.mean())\n    confusion_matrix=metrics.confusion_matrix(y_test,y_pred)\n    print(\"Confusion Matrix \\n\",confusion_matrix)\n    classification_report = metrics.classification_report(y_test,y_pred)\n    print(\"Classification Report \\n\",classification_report)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Model Comparison\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfig = plt.figure(figsize=(25, 10))\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(111)\nplt.boxplot(cv_scores)\nax.set_xticklabels(names)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}