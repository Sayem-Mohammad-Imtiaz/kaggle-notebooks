{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"!pip install tez\n\nimport tez\nimport torch\nimport torch.nn as nn\nimport transformers\nfrom transformers import AdamW, get_linear_schedule_with_warmup\nfrom sklearn.metrics import accuracy_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class BertDataset:\n    def __init__(self, texts, targets, max_len = 64):\n        self.texts = texts\n        self.targets = targets\n        self.tokenizer = transformers.BertTokenizer.from_pretrained(\n            \"bert-base-uncased\",\n            do_lower_case = False\n            )\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        text = str(self.texts[idx])\n        inputs = self.tokenizer.encode_plus(\n            text,\n            None,\n            add_special_tokens = True,\n            max_length = self.max_len,\n            padding = 'max_length',\n            truncation = True\n        )\n        resp = {\n            'ids': torch.tensor(inputs['input_ids'], dtype = torch.long),\n            'mask': torch.tensor(inputs['attention_mask'], dtype = torch.long),\n            'token_type_ids': torch.tensor(inputs['token_type_ids'], dtype = torch.long),\n            'targets': torch.tensor(self.targets[idx], dtype = torch.long),\n        }\n        return resp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class TextModel(tez.Model):\n    def __init__(self, num_classes, num_train_steps):\n        super().__init__()\n        self.bert = transformers.BertModel.from_pretrained(\n            'bert-base-uncased', return_dict = False)\n        self.bert_drop = nn.Dropout(0.3)\n        self.out = nn.Linear(768, num_classes)\n        self.num_train_steps = num_train_steps\n        self.step_scheduler_after = 'batch'\n        \n    def fetch_optimizer(self):\n        opt = AdamW(self.parameters(), lr = 3e-5)\n        return opt\n\n    def fetch_scheduler(self):\n        sch = get_linear_schedule_with_warmup(\n            self.optimizer,\n            num_warmup_steps = 0,\n            num_training_steps = self.num_train_steps)\n        return sch\n\n    def loss(self, outputs, targets):\n        return nn.CrossEntropyLoss()(outputs, targets)\n\n    def monitor_metrics(self, outputs, targets):\n        out = torch.argmax(outputs, axis = 1).cpu().detach().numpy()\n        tag = targets.cpu().detach().numpy()\n\n        return {'accuracy' : accuracy_score(out, tag)}\n\n    def forward(self, ids, mask, token_type_ids, targets = None):\n        _, x = self.bert(ids, attention_mask = mask, token_type_ids = token_type_ids)\n        x = self.bert_drop(x)\n        x = self.out(x)\n        if targets is not None:\n            loss = self.loss(x, targets)\n            met = self.monitor_metrics(x, targets)\n            return x, loss, met\n        return x, 0, {}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_model(fold, df, train_idx, val_idx):\n    df_train = df.iloc[train_idx, :].reset_index(drop = True)\n    df_val = df.iloc[val_idx, :].reset_index(drop = True)\n\n    train_dataset = BertDataset(df_train.Review.values, df_train.Rating.values)\n    val_dataset = BertDataset(df_val.Review.values, df_val.Rating.values)\n    whole_dataset = BertDataset(df.Review.values, df.Rating.values)\n    \n    n_train_steps = int(len(df_train) / TRAIN_BS * EPOCHS)\n    model = TextModel(num_classes = 5,\n                      num_train_steps= n_train_steps)\n\n    es = tez.callbacks.EarlyStopping(monitor = 'valid_loss', patience = 1, model_path=\"model.bin\")\n    model.fit(\n        train_dataset,\n        valid_dataset = val_dataset,\n        device = DEVICE,\n        epochs = 200,\n        train_bs = 32,\n        callbacks = [es])\n    model.load('model.bin', device = DEVICE)\n    pred = model.predict(whole_dataset, device = DEVICE)\n    return pd.Series([p for p in pred])\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/trip-advisor-hotel-reviews/tripadvisor_hotel_reviews.csv')\ndf.Rating -= 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ntrain, test = train_test_split(df, train_size = 0.9, test_size = 0.1, random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TRAIN_BS = 32\nEPOCHS = 200\nfrom sklearn.model_selection import StratifiedKFold\nskf = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['pred'] = 0\npred_folds = {}\nfor i, (train_idx, val_idx) in enumerate(skf.split(X = train, y = train.Rating)):\n    pred = train_model(fold = i, df = df, train_idx = train_idx, val_idx = val_idx)\n    pred_folds[i] = pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prob_preds = np.zeros((len(df), 5))\nfor i in range(5):\n    for j in range(1281):\n        for k in range(16):\n            try:\n                prob_preds[16 * j + k] += pred_folds[i][j][k]\n            except:\n                break\npreds = pd.DataFrame(np.argmax(prob_preds, axis = 1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Train CV accuracy:', accuracy_score(train.Rating, preds.iloc[train.index, :]))\nprint('Test accuracy:', accuracy_score(test.Rating, preds.iloc[test.index, :]))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}