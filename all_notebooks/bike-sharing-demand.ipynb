{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n#https://towardsdatascience.com/end-to-end-case-study-bike-sharing-demand-dataset-53201926c8db\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\ndf = pd.read_csv('/kaggle/input/bikesfortutorial/Bikes.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.rename(columns = {'hum':'humidity','weathersit':'weather','cnt':'count'},inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from datetime import datetime\ntime = []\nmonth = []\nfor i in df['datetime']:\n    dt_object2 = datetime.strptime(i, \"%Y-%m-%d %H:%M:%S\")\n    time.append(dt_object2.hour)\n    month.append(dt_object2.month)\ndf['time'] = pd.DataFrame(time)\ndf['time'] = df['time'].astype(float)\ndf['month'] = pd.DataFrame(month)\ndf['month'] = df['month'].astype(float)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop('datetime',axis=1,inplace=True) \ndf.drop('holiday',axis=1,inplace=True) \ndf = df.drop('atemp',axis=1)\ndf = pd.get_dummies(df,columns=['season','weather'],drop_first=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in df.columns:\n    df[i].fillna(value = df[i].median())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" work_day = df[df['workingday']==1] \nnon_work_day = df[df['workingday']==0]\n    # Model for registered\nX = work_day.drop(['casual','registered','count'],axis = 1)\ny = work_day.registered\n    # Dividing the data into train and test\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(X,y,test_size = 0.30 ,random_state = 2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nregressor = LinearRegression()\nregressor.fit(x_train,y_train)\ny_pred = regressor.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\nmse=mean_squared_error(y_pred,y_test)\nrmse=np.sqrt(mse)\nprint('RMLSE for the data:',rmse)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Finding best parameters for decision tree\nfrom sklearn.tree import DecisionTreeRegressor\ndt = DecisionTreeRegressor(random_state=0)\ndt_params = {'max_depth':np.arange(1,50,2),'min_samples_leaf':np.arange(2,15)}\n    \nfrom sklearn.model_selection import GridSearchCV\ngs_dt = GridSearchCV(dt,dt_params,cv=3)\ngs_dt.fit(x_train,y_train)\na = gs_dt.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training with best parameters\nfrom sklearn.tree import DecisionTreeRegressor\ndtr=DecisionTreeRegressor(max_depth=a['max_depth'],min_samples_leaf= a['min_samples_leaf'])\nmodel = dtr.fit(x_train,y_train)\ny_pred = model.predict(x_test)\n\nfrom sklearn.metrics import mean_squared_log_error\nmsle=mean_squared_log_error(y_pred,y_test)\nrmsle=np.sqrt(msle)\nprint('RMLSE for the data:',rmsle) # For decision tree\n\nfrom sklearn.metrics import mean_squared_error\nmse=mean_squared_error(y_pred,y_test)\nrmse=np.sqrt(mse)\nprint('RMLSE for the data:',rmse)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting important features\nimport matplotlib.pyplot as plt\nimportances = dtr.feature_importances_\nplt.title('Registered Feature Importances')\nplt.barh(range(len(importances)), importances, color='g', align='center')\nplt.yticks(range(len(importances)), X.columns)\nplt.xlabel('Relative Importance')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Finding best parameters for RandomForestRegressor\nfrom sklearn.ensemble import RandomForestRegressor\n\nrf = RandomForestRegressor(random_state=0)\nrf_params = {'n_estimators':np.arange(25,150,25),'max_depth':np.arange(1,11,2),'min_samples_leaf':np.arange(2,15,3)}\n\nfrom sklearn.model_selection import GridSearchCV\ngs_rf = GridSearchCV(rf,rf_params,cv=3)\ngs_rf.fit(x_train,y_train)\nb = gs_rf.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fitting the model with best params\nRF = RandomForestRegressor(n_estimators=b['n_estimators'],max_depth=b['max_depth'],min_samples_leaf=b['min_samples_leaf'],random_state=0)\nmodel = RF.fit(x_train,y_train)\ny_pred = model.predict(x_test)\n\nfrom sklearn.metrics import mean_squared_log_error\nmsle=mean_squared_log_error(y_pred,y_test)\nrmsle=np.sqrt(msle)\nprint('RMLSE for the data:',rmsle) # For random forest","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting important features\nimport matplotlib.pyplot as plt\nimportances = RF.feature_importances_\nplt.title('Registered Feature Importances')\nplt.barh(range(len(importances)), importances, color='g', align='center')\nplt.yticks(range(len(importances)), X.columns)\nplt.xlabel('Relative Importance')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = non_work_day.drop(['casual','registered','count'],axis = 1)\ny = non_work_day.casual\n    # Dividing the data into train and test\nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.30 ,random_state = 2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"from sklearn.ensemble import AdaBoostRegressor\nar = AdaBoostRegressor(base_estimator=RF,random_state=0)\nar_params = {'n_estimators':np.arange(25,200,25)}\nfrom sklearn.model_selection import GridSearchCV\ngs_ar = GridSearchCV(ar,ar_params,cv=3)\ngs_ar.fit(x_train,y_train)\nc = gs_ar.best_params_","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"ab_rf = AdaBoostRegressor(base_estimator=RF,n_estimators=c['n_estimators'],random_state=0)\nmodel = ab_rf.fit(x_train,y_train)\ny_pred = model.predict(x_test)\n\nfrom sklearn.metrics import mean_squared_log_error\nmsle=mean_squared_log_error(y_pred,y_test)\nrmsle=np.sqrt(msle)\nprint('RMLSE for the data:',rmsle) # For Ada-Boost\n","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}