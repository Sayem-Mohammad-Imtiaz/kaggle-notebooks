{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Downloading nltk packages\nimport nltk\nnltk.download('all')","metadata":{"execution":{"iopub.status.busy":"2021-06-02T06:08:10.51578Z","iopub.execute_input":"2021-06-02T06:08:10.516143Z","iopub.status.idle":"2021-06-02T06:08:30.332029Z","shell.execute_reply.started":"2021-06-02T06:08:10.516052Z","shell.execute_reply":"2021-06-02T06:08:30.331344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# importing Dataset\nimport pandas as pd\ndf=pd.read_csv('./../input/sms-spam-collection-dataset/spam.csv',usecols=['v1','v2'], encoding='latin-1')\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-02T06:08:30.333248Z","iopub.execute_input":"2021-06-02T06:08:30.333708Z","iopub.status.idle":"2021-06-02T06:08:30.389873Z","shell.execute_reply.started":"2021-06-02T06:08:30.333678Z","shell.execute_reply":"2021-06-02T06:08:30.388811Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nltk\nfrom nltk.stem import PorterStemmer\nfrom nltk.corpus import stopwords\nimport re\ncorpus=[]\n\n# Get all the sentences from the 2nd column\nsentences=df.v2\n\n# I am using stemming for classification\nstemmer=PorterStemmer()\n\n# Loop over all the sentences\nfor i in range(len(sentences)):\n  # Remove non - alphabetical characters from the sentence\n  sent=re.sub('[^A-Za-z]',' ',sentences[i])\n  # Lower the sentence\n  sent=sent.lower()\n  # Convert the sentence to a list of words\n  words=nltk.word_tokenize(sent)\n  # Stem each word which is not in stopwords\n  words=[stemmer.stem(word) for word in words if word not in stopwords.words('english')]\n  # rejoin the sentence\n  sent=' '.join(words)\n  # add the sentence to the corpus\n  corpus.append(sent)\n# One hot encoding the target column i.e,1st column\ny=pd.get_dummies(df.v1,drop_first=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-02T06:08:30.392621Z","iopub.execute_input":"2021-06-02T06:08:30.392942Z","iopub.status.idle":"2021-06-02T06:08:43.54978Z","shell.execute_reply.started":"2021-06-02T06:08:30.392914Z","shell.execute_reply":"2021-06-02T06:08:43.547111Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom collections import Counter\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# The dataset is imbalanced\n# Random oversampling duplicates examples from the minority class in the training dataset\nfrom imblearn.over_sampling import RandomOverSampler\nos=RandomOverSampler(0.93)\nX_train_ns,y_train_ns=os.fit_resample(np.array(corpus).reshape((-1,1)),(y.values))\n\n# PLotting the original Spam : Non Spam distribution\nprint(\"original Spam : Non Spam distribution 'Imbalanced'\",Counter((y.values).reshape((1,-1))[0]))\nplt.hist(((y.values).reshape((1,-1))[0]),bins=3)\nplt.title(\"original Spam : Non Spam distribution 'Imbalanced'\")\nplt.show()\n\n# PLotting the Randomly Oversampled Spam : Non Spam distribution\nprint(\"Randomly Oversampled Spam : Non Spam distribution 'Balanced'\",Counter((y_train_ns).reshape((1,-1))[0]))\nplt.hist((y_train_ns).reshape((1,-1))[0],bins=3)\nplt.title(\"Randomly Oversampled Spam : Non Spam distribution 'Balanced'\")\nplt.show()\n\nfrom sklearn.feature_extraction.text import CountVectorizer\ncv=CountVectorizer(ngram_range=(1,2),max_features=5000)\nX_train_ns=cv.fit_transform(np.array(X_train_ns).ravel()).toarray()","metadata":{"execution":{"iopub.status.busy":"2021-06-02T06:25:30.158948Z","iopub.execute_input":"2021-06-02T06:25:30.159406Z","iopub.status.idle":"2021-06-02T06:25:30.85758Z","shell.execute_reply.started":"2021-06-02T06:25:30.159362Z","shell.execute_reply":"2021-06-02T06:25:30.856607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Making a trian test split\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(X_train_ns,y_train_ns,test_size=0.2,random_state=42)\n\n# Using Naive bayes Calssifier\nfrom sklearn.naive_bayes import GaussianNB\ng=GaussianNB()\ng.fit(X_train_ns,y_train_ns)\ny_pred=g.predict(X_test)\n\nfrom sklearn.metrics import accuracy_score,classification_report,f1_score,confusion_matrix\nprint(\"Accuracy of the model ==>\",accuracy_score(y_test,y_pred))\nprint(\"classification_report ==>\\n\",classification_report(y_test,y_pred))\nprint(\"f1_score==>\",f1_score(y_test,y_pred))\nprint(\"Confusion Matrix ==>\\n\",confusion_matrix(y_test,y_pred),)\nsns.heatmap(confusion_matrix(y_test,y_pred),annot=True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-02T06:25:30.859256Z","iopub.execute_input":"2021-06-02T06:25:30.859573Z","iopub.status.idle":"2021-06-02T06:25:32.102797Z","shell.execute_reply.started":"2021-06-02T06:25:30.859543Z","shell.execute_reply":"2021-06-02T06:25:32.102068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Lets gooo ðŸš€.... We got a model with testing accuracy 99.7%","metadata":{}},{"cell_type":"markdown","source":"# Upvote if Like the Work ðŸ˜‹","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}