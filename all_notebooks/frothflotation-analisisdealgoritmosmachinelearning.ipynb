{"cells":[{"metadata":{"id":"bTpHVmIhMkAw"},"cell_type":"markdown","source":"# Tarea 2 IA\n\n> Mauricio Montanares\n\n> Sebastian Mendoza"},{"metadata":{"id":"PULWma5BlSkT"},"cell_type":"markdown","source":"Este dataset contiene datos de una planta procesadora de minerales. \n\nEn la industria minera es de vital importancia conocer el nivel de impurezas de la pulpa final de la etapa de flotación. Los métodos tradicionales consisten en realizar pruebas químicas en base a muestras de está pulpa al final del proceso de flotación, no hay métodos que permitan una estimación previa de los niveles de contaminantes.\n\nConocer el % de contaminación de la pulpa puede ayudar a reducir pérdidas en la calidad del producto final y posibles pérdidas de dinero.\n\nEn este trabajo se implementan modelos Min Max Scaler, Random Forest Regressor, Support Vector Machines, Classic NN y LSTM Implementation.\n\nSe recalca que este es un problema de regresión, ya que se busca predecir valores continuos, y no clasificar.\n\n"},{"metadata":{"id":"V6v7QnDVrqbC","trusted":false},"cell_type":"code","source":"#Imports\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sn","execution_count":null,"outputs":[]},{"metadata":{"id":"6cSLFEkvrxud","trusted":false},"cell_type":"code","source":"#data_clean = pd.read_csv(\"drive/My Drive/Colab Notebooks/CpcProyect/data/data_cleaned.csv\")\ndata_mining = pd.read_csv('MiningProcess_Flotation_Plant_Database.csv',decimal=\",\",parse_dates=[\"date\"],infer_datetime_format=True).drop_duplicates()\n#data_mining = pd.read_csv('drive/My Drive/Colab Notebooks/CpcProyect/data/Copia de MiningProcess_Flotation_Plant_Database.csv')","execution_count":null,"outputs":[]},{"metadata":{"id":"5suz1e69F34i","outputId":"f7119373-ee8a-418f-b675-f7ef898f6b1a","trusted":false},"cell_type":"code","source":"data_mining.head().transpose() #transpose the data","execution_count":null,"outputs":[]},{"metadata":{"id":"Zi-k0OvGlHzH","trusted":false},"cell_type":"code","source":"#change the order of columns (the last column) for do the job more easy\n\ndata_mining = data_mining[['date', '% Iron Feed', '% Silica Feed', 'Starch Flow', 'Amina Flow',\n       'Ore Pulp Flow', 'Ore Pulp pH', 'Ore Pulp Density',\n       'Flotation Column 01 Air Flow', 'Flotation Column 02 Air Flow',\n       'Flotation Column 03 Air Flow', 'Flotation Column 04 Air Flow',\n       'Flotation Column 05 Air Flow', 'Flotation Column 06 Air Flow',\n       'Flotation Column 07 Air Flow', 'Flotation Column 01 Level',\n       'Flotation Column 02 Level', 'Flotation Column 03 Level',\n       'Flotation Column 04 Level', 'Flotation Column 05 Level',\n       'Flotation Column 06 Level', 'Flotation Column 07 Level', \n        '% Silica Concentrate','% Iron Concentrate']]","execution_count":null,"outputs":[]},{"metadata":{"id":"PbJaWNRNlmFi"},"cell_type":"markdown","source":"# First view of the data"},{"metadata":{"id":"cSi4ZfAGVmoh","outputId":"c840bd4f-e2a0-4efd-c9a0-6d77e165626d","trusted":false},"cell_type":"code","source":"data_mining.shape","execution_count":null,"outputs":[]},{"metadata":{"id":"egf28vsg2GMi","outputId":"7206c8c3-6134-4c5c-d0a4-11650979a346","trusted":false},"cell_type":"code","source":"#create corrmatrix\nplt.figure(figsize=(30, 25))\ncorr_matrix = data_mining.corr()\n\nsn.heatmap(corr_matrix , annot=True)\nplt.savefig(\"corr_matrix.eps\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"xx_fRlgblHzQ"},"cell_type":"markdown","source":"> No observamos ninguna correlacion evidente ademas de la correlacion inversa entre la concentracion de silice y la de hierro. Esto ultimo coincide con la relacion inversamente proporcional de la cual la fuente del dataset hacia mencion."},{"metadata":{"id":"MbUmlQYx3qYe","outputId":"62bb399c-e33a-4203-f928-1a74f1c92692","trusted":false},"cell_type":"code","source":"df_corr = pd.DataFrame(corr_matrix)\n\n\ncorr_values = np.array(df_corr.iloc[22]) #drop IRON\ncorr_values = corr_values[0:len(corr_values)-1]\n\ncorr_names = np.array(df_corr.columns)\ncorr_names = corr_names[0:len(corr_names)-1] #drop iron\n#corr_names = corr_names + corr_names[len(corr_names)] #add silica \n\nplt.figure(figsize=(55,25))\nplt.barh(corr_names, corr_values, color=\"salmon\")\nplt.title(\"Correlation Values for % Iron Concentrate\", fontsize=45)\nplt.xticks(fontsize=30,rotation=90)\nplt.yticks(fontsize=30)\n#plt.savefig(\"corr_silicia.eps\")\n#plt.savefig(\"corr_silicia.svg\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"F3UZuNGTlHzX"},"cell_type":"markdown","source":"# Resampling.\n\n> La fuente del dataset especifica que existen variables medidas cada 1 minuto y otras (silica y concentracion de hierro) que son medidas cada 1 hora aproximadamente.\n\n> Nuestro resampling considera un promedio de las muestras tomadas cada 1 minuto para generar un nuevo dato, este dato es almacenado en nuevo dataset."},{"metadata":{"id":"EfVj0FJNqXDE","trusted":false},"cell_type":"code","source":"#resampling with mean \n\nnames = ['% Iron Feed', '% Silica Feed', 'Starch Flow', 'Amina Flow',\n       'Ore Pulp Flow', 'Ore Pulp pH', 'Ore Pulp Density',\n       'Flotation Column 01 Air Flow', 'Flotation Column 02 Air Flow',\n       'Flotation Column 03 Air Flow', 'Flotation Column 04 Air Flow',\n       'Flotation Column 05 Air Flow', 'Flotation Column 06 Air Flow',\n       'Flotation Column 07 Air Flow', 'Flotation Column 01 Level',\n       'Flotation Column 02 Level', 'Flotation Column 03 Level',\n       'Flotation Column 04 Level', 'Flotation Column 05 Level',\n       'Flotation Column 06 Level', 'Flotation Column 07 Level',\n       '% Iron Concentrate', '% Silica Concentrate']\n \n \nts = 175\nfor x in range(1,data_mining.shape[1]):\n  data = [] #clear data \n  n = 0 #set counter\n \n  while n*ts <= round(data_mining.shape[0]):\n    sample_mean_value = data_mining.iloc[(ts*n):(ts*(n+1)),x].mean()  #sampling data and save mean of n*ts elements\n    data += [sample_mean_value]   #save sampling into a list\n    data_array = np.array(data)   #list to array\n    data_array= np.transpose(data_array)  #transpose\n    df = pd.DataFrame(data_array)   #array to dataframe\n    n += 1  #counter up \n \n  if x == 1:\n    df_first = df\n  if x == 2:\n    new_df = pd.concat([df_first, df], axis = 1)\n  if x > 2: \n    new_df = pd.concat([new_df, df], axis=1) \n \nnew_df.columns = names","execution_count":null,"outputs":[]},{"metadata":{"id":"MwqI3EzJ831a","trusted":false},"cell_type":"code","source":"new_df.to_csv('resampling_data.csv')","execution_count":null,"outputs":[]},{"metadata":{"id":"dXJJmFN5lHzf"},"cell_type":"markdown","source":"> *We will work with this new dataset!*"},{"metadata":{"id":"VeUWoXZ0Cce8","trusted":false},"cell_type":"code","source":"#remove output %Silica concentrate for post ML analysis \nnew_df = new_df.drop(labels=[ '% Silica Concentrate'], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"id":"UqapfSJAlHzh"},"cell_type":"markdown","source":"# MinMaxScaler"},{"metadata":{"id":"Vs9eGORDlHzl","trusted":false},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler","execution_count":null,"outputs":[]},{"metadata":{"id":"9D9u62CnlHzn","trusted":false},"cell_type":"code","source":"names = new_df.columns #store the names","execution_count":null,"outputs":[]},{"metadata":{"id":"HyF5UDBflHzp","trusted":false},"cell_type":"code","source":"scaler = MinMaxScaler()\nscaled_df = scaler.fit_transform(new_df)\nscaled_df = pd.DataFrame(scaled_df, columns=names)","execution_count":null,"outputs":[]},{"metadata":{"id":"7peDsPIClHzq","outputId":"f9fc5d3c-f669-460d-9054-74949cbc8a5b","trusted":false},"cell_type":"code","source":"scaled_df","execution_count":null,"outputs":[]},{"metadata":{"id":"6FGTC5rElHzv"},"cell_type":"markdown","source":"> Now data it's normalized"},{"metadata":{"id":"c7pOWHKolHzw"},"cell_type":"markdown","source":"# Random Forest Regressor"},{"metadata":{"id":"VhqD8CIplHzw","trusted":false},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import classification_report\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"id":"uoCyqzaNlHzx","trusted":false},"cell_type":"code","source":"data_mining = scaled_df  #change the name... \nX = data_mining.drop(['% Iron Concentrate'], axis=1) #drop target \ny = data_mining['% Iron Concentrate']  #select target \n\n#make split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"id":"_8qpe0QclHzz","outputId":"e186d05f-6cbe-4409-eb6d-2f49715033fe","trusted":false},"cell_type":"code","source":"# Set the parameters by cross-validation and GridSearch\npara_grids = {\n            \"n_estimators\" : [10,50,100,200],\n            \"max_features\" : [\"auto\", \"log2\", \"sqrt\"],\n            \"bootstrap\"    : [True, False]\n        }\n\nscores = ['precision', 'recall']\n\nfor score in scores:\n    print(\"# Tuning hyper-parameters for %s\" % score)\n    print()\n    estimator = RandomForestRegressor()\n    clf = GridSearchCV(estimator, para_grids)\n    clf.fit(X_train, y_train)\n\n    print(\"Best parameters set found on development set:\")\n    print()\n    print(clf.best_params_)\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"id":"fULLhZp8lHz2","trusted":false},"cell_type":"code","source":"model = RandomForestRegressor(n_estimators=200, max_features='sqrt', bootstrap = False) #utilizamos los parametros anteriors","execution_count":null,"outputs":[]},{"metadata":{"id":"qb4rFNeElHz6","outputId":"850b2ed9-f023-40fe-9189-1f13b3ef1cfe","trusted":false},"cell_type":"code","source":"model.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"id":"nRDTjNSvlHz9","outputId":"333542a9-57c5-4fc2-f791-646666308fab","trusted":false},"cell_type":"code","source":"model.score(X_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{"id":"2DqMim2flHz-"},"cell_type":"markdown","source":"> SCORE 0.48"},{"metadata":{"id":"7k-Eu_tIlHz_","outputId":"e4950ca8-1860-47a4-aaba-560b90562e6f","trusted":false},"cell_type":"code","source":"#PLOT PREDICT VALUES VS REAL VALUES\npred = model.predict(X_test)\nplt.figure(figsize=(20, 10))\nplt.plot(y_test.values[1:600]) #JUST SOME DATA\nplt.plot(pred[1:600])\nplt.savefig(\"regressionRF.pdf\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"2-dCuErvlH0B"},"cell_type":"markdown","source":"# Support Vector Machines"},{"metadata":{"id":"YANLTo_elH0C","trusted":false},"cell_type":"code","source":"from sklearn.svm import SVR","execution_count":null,"outputs":[]},{"metadata":{"id":"B3On4llnlH0F","outputId":"48f6708c-c2c8-4de3-8266-cdf3f3818afc","trusted":false},"cell_type":"code","source":"#WARNING: be careful with this code. Take  A LOT OF TIME in finish!\n#param = { ,\n\n#model = SVR()\n\n#grids = grid_search = GridSearchCV(estimator = model, param_grid = param, \n                    #  cv = 3, n_jobs = 4, verbose = 2)\n\n#grids.fit(X_train, y_train)\n# Tuning hyper-parameters for precision\n\n\n#print(\"Best parameters set found on development set:\")\n#print()\n#print(grid_search.best_params_)\n\n","execution_count":null,"outputs":[]},{"metadata":{"id":"ryRCPmsWlH0I","outputId":"8b2c3119-bc87-4370-a748-fabbe6add5a7","trusted":false},"cell_type":"code","source":"regressor = SVR(kernel = 'poly', C = 50, degree=8, coef0=0.5, gamma='auto')\nregressor.fit(X_train, y_train)\nregressor.score(X_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{"id":"FY4f9g0JlH0M"},"cell_type":"markdown","source":"> SCORE 0.3217"},{"metadata":{"id":"7xItC_HflH0N","outputId":"9cce617b-047b-4dd4-faef-0bada3329582","trusted":false},"cell_type":"code","source":"pred = model.predict(X_test)\nplt.figure(figsize=(20, 10))\nplt.plot(y_test.values[1:600])\nplt.plot(pred[1:600])\nplt.savefig(\"regressionSVRF.pdf\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"7fmru0NzDxAW"},"cell_type":"markdown","source":"# CLASSIC NN"},{"metadata":{"id":"d6bGK4fbCcfS","trusted":false},"cell_type":"code","source":"#define x's and y \nx = data_mining.drop(labels='% Iron Concentrate', axis=1)\ny = data_mining['% Iron Concentrate']","execution_count":null,"outputs":[]},{"metadata":{"id":"53N8p_7PCcfm","trusted":false},"cell_type":"code","source":"# Create train/test\nfrom sklearn.model_selection import train_test_split\n \nx_train, x_test, y_train, y_test = train_test_split(    \n    x, y, test_size=0.3)","execution_count":null,"outputs":[]},{"metadata":{"id":"SUDFiUlYCcf0","outputId":"4089490a-ba56-4409-e5f1-cb1d625403a7","trusted":false},"cell_type":"code","source":"#Import TensorFlow and keras\n \ntry:\n    %tensorflow_version 2.x\n    COLAB = True\n    print(\"Note: using Google CoLab\")\nexcept:\n    print(\"Note: not using Google CoLab\")\n    COLAB = False\n    \nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Activation\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom scipy.stats import zscore","execution_count":null,"outputs":[]},{"metadata":{"id":"65L_Yu_4lH0T","trusted":false},"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Activation\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom scipy.stats import zscore","execution_count":null,"outputs":[]},{"metadata":{"id":"AgdzBKD6vmQI","trusted":false},"cell_type":"code","source":"monitor = EarlyStopping(monitor='val_loss', min_delta=1e-4, \n                        patience=5, verbose=1, mode='auto', \n                        restore_best_weights=True)","execution_count":null,"outputs":[]},{"metadata":{"id":"MoChieFWCcf_","outputId":"709ace6a-9a2d-4290-a9e1-1fdb723f84a3","trusted":false},"cell_type":"code","source":" \n#Build a NN\n \nmodel = Sequential()\nmodel.add(Dense(22, input_dim=x.shape[1], activation='relu')) # Hidden 1\nmodel.add(Dense(18, activation='relu')) # Hidden 2\nmodel.add(Dense(1)) # Output\nmodel.compile(loss='mae', optimizer='adam', metrics=['accuracy'])\n\nhistory = model.fit(x_train,y_train,batch_size = 10,validation_data=(x_test,y_test),\n          callbacks=[monitor],verbose=2,epochs=100)","execution_count":null,"outputs":[]},{"metadata":{"id":"wJqeMJKE-HHB","outputId":"c1b702f5-4ca3-4cc0-9426-a8617b3c65a4","trusted":false},"cell_type":"code","source":"model.evaluate(x_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{"id":"E1SGmq6QCcgY","outputId":"ec259bc3-5d1c-40e0-e63c-440f976b3683","trusted":false},"cell_type":"code","source":"plt.figure(figsize=(10, 4))\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"8FNhkL6DlH0d"},"cell_type":"markdown","source":"> Not so good..."},{"metadata":{"id":"a2b1KoPTCcgl","outputId":"7e863ed5-a4a3-4be5-e534-87ca26999186","trusted":false},"cell_type":"code","source":"# summarize history for loss\nplt.figure(figsize=(10, 4))\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"lANefiXxlH0g"},"cell_type":"markdown","source":"> Se observa una disminucion pero no es rapida ni constante"},{"metadata":{"id":"r7szqUFzCcgx","outputId":"84fb5d44-f5d1-4973-c64a-604d00275556","trusted":false},"cell_type":"code","source":"pred = model.predict(x_test)\nplt.figure(figsize=(20, 10))\nplt.plot(y_test.values[1:600])\nplt.plot(pred[1:600])\nplt.savefig(\"regression.pdf\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"Xci3NW40Ccg9","outputId":"a2851074-5b9e-4a08-b519-b87f129191de","trusted":false},"cell_type":"code","source":"from sklearn import metrics\n \n# Predict\npred = model.predict(x_test)\n \n# Measure MSE error.  \nscore = metrics.mean_squared_error(pred,y_test)\nprint(\"Final score (MSE): {}\".format(score))","execution_count":null,"outputs":[]},{"metadata":{"id":"lgeCxqlFCchH","outputId":"66ab217a-81ed-4420-b6c3-123b9e78d6ef","trusted":false},"cell_type":"code","source":" \nimport numpy as np\n \n# Measure RMSE error.  RMSE is common for regression.\nscore = np.sqrt(metrics.mean_squared_error(pred,y_test))\nprint(\"Final score (RMSE): {}\".format(score))\n\n","execution_count":null,"outputs":[]},{"metadata":{"id":"kp4rZuMiNPNY"},"cell_type":"markdown","source":"# LSTM IMPLEMENTATION with mean()\n\n> Al ser un dataset del tipo temporal(dependencia temporal) los datos en t, t+1, dependen de t-n\n\n> Existen arquitecturas especiales para estos tipos de problemas, una de ellas es la LSTM (Long short-term memory)\n\n"},{"metadata":{"id":"-HAui_DeNVFN","outputId":"4a3e2b40-f605-4e9e-eb7f-c0c44c403544","trusted":false},"cell_type":"code","source":"\nfrom pandas import DataFrame\nfrom pandas import concat\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import mean_squared_error\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\n \n# convert series to supervised learning \ndef series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n\tn_vars = 1 if type(data) is list else data.shape[1]\n\tdf = DataFrame(data)\n\tcols, names = list(), list()\n\t# input sequence (t-n, ... t-1)\n\tfor i in range(n_in, 0, -1):\n\t\tcols.append(df.shift(i))\n\t\tnames += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n\t# forecast sequence (t, t+1, ... t+n)\n\tfor i in range(0, n_out):\n\t\tcols.append(df.shift(-i))\n\t\tif i == 0:\n\t\t\tnames += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n\t\telse:\n\t\t\tnames += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n\t# put it all together\n\tagg = concat(cols, axis=1)\n\tagg.columns = names\n\t# drop rows with NaN values\n\tif dropnan:\n\t\tagg.dropna(inplace=True)\n\treturn agg\n\n\n\nvalues = new_df.values\n\n# normalize features\nscaler = MinMaxScaler(feature_range=(0, 1))\nscaled = scaler.fit_transform(values)\n# frame as supervised learning\nreframed = series_to_supervised(scaled, 1, 1)\n\n# drop columns we don't want to predict\nreframed.drop(reframed.columns[(reframed.shape[1]-21):(reframed.shape[1]-1)], axis=1, inplace=True)\n\n# split into train and test sets\nvalues = reframed.values\nn_train_hours = round(reframed.shape[0] * 0.8) #70% of data for training\ntrain = values[:n_train_hours, :]\ntest = values[n_train_hours:, :]\n# split into input and outputs\ntrain_X, train_y = train[:, :-1], train[:, -1]\ntest_X, test_y = test[:, :-1], test[:, -1]\n# reshape input to be 3D [samples, timesteps, features]\ntrain_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\ntest_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))\nprint(train_X.shape, train_y.shape, test_X.shape, test_y.shape)\n\n\nfrom tensorflow.keras.callbacks import EarlyStopping\n\n\n\n# design network\nmodel = Sequential()\nmodel.add(LSTM(5, activation='sigmoid', input_shape=(train_X.shape[1], train_X.shape[2])))\nmodel.add(Dense(1))\nmodel.compile(loss='mae', optimizer='adam')\nmonitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, \n                        patience=5, verbose=1, mode='auto', \n                        restore_best_weights=True)\n# fit network\nhistory = model.fit(train_X, train_y, epochs=100, batch_size=20, validation_data=(test_X, test_y), verbose=2, shuffle=False)\n\n ","execution_count":null,"outputs":[]},{"metadata":{"id":"QUFiWVWoD-gY"},"cell_type":"markdown","source":"\n\n\n# Test Model and some plots "},{"metadata":{"id":"8ZFt-MQTWiUM","outputId":"aa162f25-629c-40a3-dac9-a99544a1813b","trusted":false},"cell_type":"code","source":"# plot history\nplt.figure(figsize=(8, 5))\nplt.plot(history.history['loss'], label='train')\nplt.plot(history.history['val_loss'], label='test')\nplt.legend(fontsize=15)\nplt.xlabel('epochs', fontsize = 15)\nplt.ylabel('Loss value', fontsize = 15)\n\n\n\nplt.savefig(\"lossLSTM.svg\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"QMbYSo9OP8e3","outputId":"bea4777b-5df0-4423-bd90-f5c3feac031b","trusted":false},"cell_type":"code","source":"pred = model.predict(test_X)\nplt.figure(figsize=(15, 8))\n\n#subplots\n\nplt.subplot(211)\n\n\nplt.plot(pred[1:int(818/2),0], label='predict', color='black')\nplt.plot(test_y[:int(818/2)], label='test', color = 'salmon')\nplt.title('LSTM. Predicted values and test values', fontsize=20)\nplt.ylabel('%Iron Concentrate', fontsize = 15)\nplt.legend(fontsize = 15)\n\n\nplt.subplot(212)\nplt.plot(np.arange(int(820/2),841),pred[int(820/2):,0], label='predict', color='black')\nplt.plot(np.arange(int(820/2),842),test_y[int(818/2):], label='test', color = 'salmon')\n\n\nplt.xlabel('Data[Hours]', fontsize = 15)\nplt.ylabel('%Iron Concentrate', fontsize = 15)\nplt.legend(fontsize = 15)\n\nplt.savefig(\"regressionLSTM.svg\")\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"WMcJK7Thav-C","outputId":"76bd2032-9fe6-4e57-9c20-a4de12a2e0e4","trusted":false},"cell_type":"code","source":"#error plot \n\npred = model.predict(test_X)\nplt.figure(figsize=(11, 8))\n\nerror = (test_y[:818]-pred[1:819,0]) * 100\n\n#error_percent = error*100/test_y.max()\n\nplt.plot(range(error.shape[0]) ,error, color = 'red')\n\nplt.title('LSTM. Error predicted values vs test values', fontsize=20)\nplt.xlabel('Data[Hours]', fontsize = 15)\nplt.ylabel('%Error', fontsize = 15)\n#plt.legend(fontsize = 15)\n\nplt.grid(b=True, which='major', color='#999999', linestyle='-', alpha=0.2)\nplt.minorticks_on()\nplt.grid(b=True, which='minor', color='#999999', linestyle='-', alpha=0.2)\n\nplt.savefig(\"LSTM_error.pdf\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"eLscyuYEeX5X","outputId":"f6eccd5d-5143-41d1-b982-1a4732dc42ef","trusted":false},"cell_type":"code","source":"error.max()","execution_count":null,"outputs":[]},{"metadata":{"id":"E7X5jJQCbcjO"},"cell_type":"markdown","source":"# METRICS LSTM"},{"metadata":{"id":"lEAdTp_BfyGW","outputId":"78e6e490-b58a-4e21-9206-e47f23659928","trusted":false},"cell_type":"code","source":"from sklearn import metrics\n\n# Predict\npred = model.predict(test_X)\n\n# Measure MSE error.  \nscore = metrics.mean_squared_error(pred,test_y)\nprint(\"Final score (MSE): {}\".format(score))","execution_count":null,"outputs":[]},{"metadata":{"id":"EI_wwmpNgK-f","outputId":"a01628d2-9648-4763-e718-4e1e1db8fe8a","trusted":false},"cell_type":"code","source":"import numpy as np\n\n# Measure RMSE error.  RMSE is common for regression.\nscore = np.sqrt(metrics.mean_squared_error(pred,test_y))\nprint(\"Final score (RMSE): {}\".format(score))","execution_count":null,"outputs":[]},{"metadata":{"id":"pvcTeRLSr1_o","outputId":"74505009-28a3-44aa-bf82-823182216112","trusted":false},"cell_type":"code","source":"#mae\nfrom sklearn.metrics import mean_absolute_error\npred = model.predict(test_X)\nscore = mean_absolute_error(test_y, pred, multioutput='raw_values')\nprint(\"Final score (MAE): {}\".format(score))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}