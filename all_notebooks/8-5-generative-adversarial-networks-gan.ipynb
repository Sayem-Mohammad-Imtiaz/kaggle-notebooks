{"nbformat":4,"metadata":{"language_info":{"version":"3.6.4","file_extension":".py","mimetype":"text/x-python","codemirror_mode":{"version":3,"name":"ipython"},"nbconvert_exporter":"python","name":"python","pygments_lexer":"ipython3"},"kernelspec":{"name":"python3","language":"python","display_name":"Python 3"}},"nbformat_minor":1,"cells":[{"cell_type":"markdown","metadata":{},"source":"This is the example from the book \"Deep Learning with Python\" by FranÃ§ois Chollet.\n\n[https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/8.5-introduction-to-gans.ipynb](https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/8.5-introduction-to-gans.ipynb)"},{"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"cell_type":"code","metadata":{"_cell_guid":"95354eb3-5c69-400b-a0c7-fdfdd90fd157","_uuid":"c6c5fa329c9636436808b1c0a1a92828cd11522a"},"outputs":[]},{"cell_type":"markdown","metadata":{},"source":"## Decompress and move to Keras cache the \"CIFAR-10 Python\" dataset"},{"source":"from os import listdir, makedirs\nfrom os.path import join, exists, expanduser\n\ncache_dir = expanduser(join('~', '.keras'))\nif not exists(cache_dir):\n    makedirs(cache_dir)\ndatasets_dir = join(cache_dir, 'datasets') # /cifar-10-batches-py\nif not exists(datasets_dir):\n    makedirs(datasets_dir)\n\n# If you have multiple input datasets, change the below cp command accordingly, typically:\n# !cp ../input/cifar10-python/cifar-10-python.tar.gz ~/.keras/datasets/\n!cp ../input/cifar-10-python.tar.gz ~/.keras/datasets/\n!ln -s  ~/.keras/datasets/cifar-10-python.tar.gz ~/.keras/datasets/cifar-10-batches-py.tar.gz\n!tar xzvf ~/.keras/datasets/cifar-10-python.tar.gz -C ~/.keras/datasets/","execution_count":null,"cell_type":"code","metadata":{"_cell_guid":"fb78c3bd-1dd7-4af8-9af7-63c49e9ebad7","_uuid":"1f93e2d40be744458d0c4b061216fabc4019d4d0"},"outputs":[]},{"source":"import keras\n\nkeras.__version__","execution_count":null,"cell_type":"code","metadata":{"_cell_guid":"8b6fb377-581d-4c31-a3b6-02796919f8e4","_uuid":"69016e4bf664ebe6a80836a06d120593e51d4a1d"},"outputs":[]},{"cell_type":"markdown","metadata":{},"source":"## Setup generator model"},{"source":"import keras\nfrom keras import layers\nimport numpy as np\n\nlatent_dim = 32\nheight = 32\nwidth = 32\nchannels = 3\n\ngenerator_input = keras.Input(shape=(latent_dim,))\n\n# First, transform the input into a 16x16 128-channels feature map\nx = layers.Dense(128 * 16 * 16)(generator_input)\nx = layers.LeakyReLU()(x)\nx = layers.Reshape((16, 16, 128))(x)\n\n# Then, add a convolution layer\nx = layers.Conv2D(256, 5, padding='same')(x)\nx = layers.LeakyReLU()(x)\n\n# Upsample to 32x32\nx = layers.Conv2DTranspose(256, 4, strides=2, padding='same')(x)\nx = layers.LeakyReLU()(x)\n\n# Few more conv layers\nx = layers.Conv2D(256, 5, padding='same')(x)\nx = layers.LeakyReLU()(x)\nx = layers.Conv2D(256, 5, padding='same')(x)\nx = layers.LeakyReLU()(x)\n\n# Produce a 32x32 1-channel feature map\nx = layers.Conv2D(channels, 7, activation='tanh', padding='same')(x)\ngenerator = keras.models.Model(generator_input, x)\ngenerator.summary()","execution_count":null,"cell_type":"code","metadata":{"_cell_guid":"d2994f18-12a8-438d-af9e-144d0e31c1f5","_uuid":"00d134308c5e07fb6b69bd061cf91b29c6bf717f"},"outputs":[]},{"cell_type":"markdown","metadata":{},"source":"## Setup discriminator model"},{"source":"discriminator_input = layers.Input(shape=(height, width, channels))\nx = layers.Conv2D(128, 3)(discriminator_input)\nx = layers.LeakyReLU()(x)\nx = layers.Conv2D(128, 4, strides=2)(x)\nx = layers.LeakyReLU()(x)\nx = layers.Conv2D(128, 4, strides=2)(x)\nx = layers.LeakyReLU()(x)\nx = layers.Conv2D(128, 4, strides=2)(x)\nx = layers.LeakyReLU()(x)\nx = layers.Flatten()(x)\n\n# One dropout layer - important trick!\nx = layers.Dropout(0.4)(x)\n\n# Classification layer\nx = layers.Dense(1, activation='sigmoid')(x)\n\ndiscriminator = keras.models.Model(discriminator_input, x)\ndiscriminator.summary()\n\n# To stabilize training, we use learning rate decay\n# and gradient clipping (by value) in the optimizer.\ndiscriminator_optimizer = keras.optimizers.RMSprop(lr=0.0008, clipvalue=1.0, decay=1e-8)\ndiscriminator.compile(optimizer=discriminator_optimizer, loss='binary_crossentropy')","execution_count":null,"cell_type":"code","metadata":{"_cell_guid":"3435e550-df16-4907-bdca-f3ff90965d86","_uuid":"5d30bbb29782caa47c6a54193420dc39127f9230"},"outputs":[]},{"cell_type":"markdown","metadata":{},"source":"## Setup adversarial network"},{"source":"# Set discriminator weights to non-trainable\n# (will only apply to the `gan` model)\ndiscriminator.trainable = False\n\ngan_input = keras.Input(shape=(latent_dim,))\ngan_output = discriminator(generator(gan_input))\ngan = keras.models.Model(gan_input, gan_output)\n\ngan_optimizer = keras.optimizers.RMSprop(lr=0.0004, clipvalue=1.0, decay=1e-8)\ngan.compile(optimizer=gan_optimizer, loss='binary_crossentropy')\ngan.summary()","execution_count":null,"cell_type":"code","metadata":{"_cell_guid":"f920848a-49a6-4b11-8f5c-79784997f5fc","_uuid":"3a3c02d073ddf57d3250c4de65beaaf6a26bf1f1"},"outputs":[]},{"cell_type":"markdown","metadata":{},"source":"## Load data and start training.\n**WARNING ! In Keras kernel it's going to take eternally long!**"},{"source":"import os\nfrom keras.preprocessing import image\n\n# Load CIFAR10 data\n(x_train, y_train), (_, _) = keras.datasets.cifar10.load_data()\n\n# Select frog images (class 6)\nx_train = x_train[y_train.flatten() == 6]\n\n# Normalize data\nx_train = x_train.reshape(\n    (x_train.shape[0],) + (height, width, channels)).astype('float32') / 255.\n\niterations = 10000\nbatch_size = 20\nsave_dir = './output/gan_images/'\nif not os.path.exists(save_dir):\n    os.makedirs(save_dir)\n\n# Start training loop\nstart = 0\nfor step in range(iterations):\n    # Sample random points in the latent space\n    random_latent_vectors = np.random.normal(size=(batch_size, latent_dim))\n\n    # Decode them to fake images\n    generated_images = generator.predict(random_latent_vectors)\n\n    # Combine them with real images\n    stop = start + batch_size\n    real_images = x_train[start: stop]\n    combined_images = np.concatenate([generated_images, real_images])\n\n    # Assemble labels discriminating real from fake images\n    labels = np.concatenate([np.ones((batch_size, 1)),\n                             np.zeros((batch_size, 1))])\n    # Add random noise to the labels - important trick!\n    labels += 0.05 * np.random.random(labels.shape)\n\n    # Train the discriminator\n    d_loss = discriminator.train_on_batch(combined_images, labels)\n\n    # sample random points in the latent space\n    random_latent_vectors = np.random.normal(size=(batch_size, latent_dim))\n\n    # Assemble labels that say \"all real images\"\n    misleading_targets = np.zeros((batch_size, 1))\n\n    # Train the generator (via the gan model,\n    # where the discriminator weights are frozen)\n    a_loss = gan.train_on_batch(random_latent_vectors, misleading_targets)\n    \n    start += batch_size\n    if start > len(x_train) - batch_size:\n      start = 0\n\n    # Occasionally save / plot\n    if step % 100 == 0:\n        # Save model weights\n        gan.save_weights('./output/gan.h5')\n\n        # Print metrics\n        print('discriminator loss at step %s: %s' % (step, d_loss))\n        print('adversarial loss at step %s: %s' % (step, a_loss))\n\n        # Save one generated image\n        img = image.array_to_img(generated_images[0] * 255., scale=False)\n        img.save(os.path.join(save_dir, 'generated_frog' + str(step) + '.png'))\n\n        # Save one real image, for comparison\n        img = image.array_to_img(real_images[0] * 255., scale=False)\n        img.save(os.path.join(save_dir, 'real_frog' + str(step) + '.png'))","execution_count":null,"cell_type":"code","metadata":{"_cell_guid":"8d5939bf-befd-4254-9e7c-fe15cd734d38","_uuid":"60d3a82e0575973a19b96a9f31bb5f6d075c25af"},"outputs":[]},{"source":"import matplotlib.pyplot as plt\n\n# Sample random points in the latent space\nrandom_latent_vectors = np.random.normal(size=(10, latent_dim))\n\n# Decode them to fake images\ngenerated_images = generator.predict(random_latent_vectors)\n\nfor i in range(generated_images.shape[0]):\n    img = image.array_to_img(generated_images[i] * 255., scale=False)\n    plt.figure()\n    plt.imshow(img)\n    \nplt.show()","execution_count":null,"cell_type":"code","metadata":{"_cell_guid":"74463b12-575c-4bfc-8e62-831fd4c8432a","_uuid":"a1ff2e1f2ee3ad373299a5d730e25e11d7d6be43","collapsed":true},"outputs":[]}]}