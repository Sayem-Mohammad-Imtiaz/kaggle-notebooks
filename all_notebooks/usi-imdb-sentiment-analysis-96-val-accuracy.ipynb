{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\ndata = pd.read_csv('../input/usinlppracticum/imdb_train.csv',delimiter = \",\",encoding=\"latin-1\")\ndata.head() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_test = pd.read_csv(\"../input/usinlppracticum/imdb_test.csv\", delimiter=\",\",header=0,encoding=\"latin-1\")\ndata_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_mas = pd.read_csv('../input/imdb-review-dataset/imdb_master.csv',encoding=\"latin-1\")\ndata_mas.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_mas = data_mas.drop(['Unnamed: 0','type','file'],axis=1)\ndata_mas.columns = [\"review\",\"sentiment\"]\ndata_mas.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_mas = data_mas[data_mas.sentiment != 'unsup']\ndata_mas['sentiment'] = data_mas['sentiment'].map({'pos': 1, 'neg': 0})\ndata_mas.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['sentiment'] = data['sentiment'].map({'positive': 1, 'negative': 0}) \ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.concat([data, data_mas]).reset_index(drop=True)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Data PreProcessing:**\nCheck for any special character in the review column"},{"metadata":{"trusted":true},"cell_type":"code","source":"import string\nalphabet = string.ascii_letters+string.punctuation\ndata.review.str.strip(alphabet).astype(bool).any()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Remove special characters to clean data."},{"metadata":{},"cell_type":"markdown","source":"First we will remove html tags"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.review = data.review.str.replace('<br />', ' ')\ndata_test.review = data_test.review.str.replace('<br />', ' ')\ndata.head(17)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.review = data.review.str.replace(r\"[^a-zA-Z\\s]+\", \"\") \ndata.head()\n\ndata_test.review = data_test.review.str.replace(r\"[^a-zA-Z\\s]+\", \"\") \ndata_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['review'] = data['review'].str.lower()\ndata.head()\n\ndata_test['review'] = data_test['review'].str.lower()\ndata_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\n\nstop_words = set(stopwords.words(\"english\"))\nlemmatizer = WordNetLemmatizer()\n\ndef clean_text(text): \n    text = re.sub(r'[^\\w\\s]','',text, re.UNICODE)\n    text = text.lower()\n    text = [lemmatizer.lemmatize(token) for token in text.split(\" \")]\n    text = [lemmatizer.lemmatize(token, \"v\") for token in text]\n    text = [word for word in text if not word in stop_words]\n    text = \" \".join(text)\n    return text\n\ndata['c_review'] = data.review.apply(lambda x: clean_text(x))\ndata.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import keras.preprocessing\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, LSTM, Embedding, Dropout, Activation, GRU,Flatten\nfrom keras.layers import Bidirectional, GlobalMaxPool1D\nfrom keras.models import Model, Sequential\nfrom keras.layers import Convolution1D\nfrom keras import initializers, regularizers, constraints, optimizers, layers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_features = 8800\ntokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(data['c_review'])\nlist_tokenized_train = tokenizer.texts_to_sequences(data['c_review'])\n\nmaxlen = 130\nX_t = pad_sequences(list_tokenized_train, maxlen=maxlen)\ny = data['sentiment']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embed_size = 128\nmodel = Sequential()\nmodel.add(Embedding(max_features, embed_size))\nmodel.add(Bidirectional(LSTM(32, return_sequences = True)))\nmodel.add(Dropout(0.3))\nmodel.add(GlobalMaxPool1D())\nmodel.add(Dense(20,activation = \"relu\")) \nmodel.add(Dropout(0.3))\nmodel.add(Dense(1, activation = \"sigmoid\"))\nmodel.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n\nmodel.fit(X_t, y,batch_size=500,epochs = 10, validation_split=0.1 )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_test.head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_test['review']=data_test.review.apply(lambda x: clean_text(x)) \ndata_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list_sentences_test = data_test[\"review\"]\nlist_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\nX_te = pad_sequences(list_tokenized_test, maxlen=maxlen)\nprediction = model.predict(X_te)\ny_pred = (prediction > 0.5)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = pd.DataFrame(y_pred.flatten())\ndata_pred= pd.merge(data_test, pred, left_index=True, right_index=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_pred.columns = ['id','review','sentiment']\ndata_pred['sentiment'] = data_pred['sentiment'].map({True: 'positive', False: 'negative'})\ndata_pred_s = data_pred[['id','sentiment']]\ndata_pred_s.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_pred_s.to_csv (r'submissions_v4.csv', index = None, header=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}