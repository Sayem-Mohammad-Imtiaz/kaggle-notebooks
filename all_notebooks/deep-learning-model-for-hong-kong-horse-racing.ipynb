{"cells":[{"metadata":{},"cell_type":"markdown","source":"This is an improved kernel on [previous one](https://www.kaggle.com/cullensun/deep-learning-model-for-horse-racing). In previous kernel, I tried to predict on every single horse run. However, I found it makes more sense to predict winner horse for every race because winning is **relative** to other horses performance. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Import packages\nHere, we import common packages for deep learning. \n- pandas: for data reading and preprocessing\n- tensorflow: for neural network construction \n- sklearn.preprocessing: for data encoding\n- sklearn.model_selection: it has convenient method for training/test data spliting \n- matplotlib.pyplot: to plot performance of the training process.","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport tensorflow as tf\nimport sklearn.preprocessing as preprocessing\nimport sklearn.model_selection as model_selection\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Prepare races data from races.csv\nOnly select several columns that make sense for this kernel. Then, use different encoders for different types of attribute.","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"races_df = pd.read_csv(r\"../input/hkracing/races.csv\", delimiter=\",\", header=0, index_col='race_id')\nraces_df = races_df[['venue', 'config', 'surface', 'distance', 'going', 'race_class']]\n\n# check to see if we have NaN, then drop NaN\nprint(races_df[races_df.isnull().any(axis=1)])\nraces_df = races_df.dropna()\n\n# encode ordinal columns: config, going, \nconfig_encoder = preprocessing.OrdinalEncoder()\nraces_df['config'] = config_encoder.fit_transform(races_df['config'].values.reshape(-1, 1))\ngoing_encoder = preprocessing.OrdinalEncoder()\nraces_df['going'] = going_encoder.fit_transform(races_df['going'].values.reshape(-1, 1))\n\n# encode nominal column: venue\nvenue_encoder = preprocessing.LabelEncoder()\nraces_df['venue'] = venue_encoder.fit_transform(races_df['venue'])\n\nprint(races_df.dtypes)\nprint(races_df.shape)\nprint(races_df.head())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Prepare races data from runs.csv\nSimilar to races data, only select columns that are relevant to the model. \n\n### Data cleaning\n- two rows that includes NaN, so just drop them.\n- strange data for 'draw', e.g. 15. As we only deal with standard 14 horses racing, so let's drop it.\n\n### Encoding \nThen, use label encoders for 'horse_country' and 'horse_type'.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"runs_df = pd.read_csv(r\"../input/hkracing/runs.csv\", delimiter=\",\", header=0)\nruns_df = runs_df[['race_id', 'draw', \n                   'horse_age', 'horse_country', 'horse_type', 'horse_rating', 'declared_weight', 'actual_weight', 'win_odds', \n                   'result']] \n\n# check to see if we have NaN, then drop NaN\nprint(runs_df[runs_df.isnull().any(axis=1)])\nruns_df = runs_df.dropna()\n\n# not sure why, but we got some strange draw in the dataset. Maximum shall be 14\nstrange_draw_index = runs_df[runs_df['draw'] > 14].index\n# delete these row indexes from dataFrame\nruns_df = runs_df.drop(strange_draw_index)\n\n# encode nominal columns: horse_country, horse_type\nhorse_country_encoder = preprocessing.LabelEncoder()\nruns_df['horse_country'] = horse_country_encoder.fit_transform(runs_df['horse_country'])\nhorse_type_encoder = preprocessing.LabelEncoder()\nruns_df['horse_type'] = horse_type_encoder.fit_transform(runs_df['horse_type'])\n\nprint(runs_df.dtypes)\nprint(runs_df.shape)\nprint(runs_df.head())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Further preprocessing for runs data\nWe are targeting to put all the 14 horses' features into the one input, but it expands into multiple rows now. Luckily, pandas has a nice method called `pivot`. `pivot` aggregates horses data from multiple rows, which belongs to a single race, into one row. \n\nAfter `pivot`, some races may not have 14 horses, so let's fill NaN with 0.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def group_horse_and_result(element):\n    if element[0] == 'result':\n        return 100 + element[1] # to make sure results are put near the end\n    else:\n        return element[1]   \n\nruns_df = runs_df.pivot(index='race_id', columns='draw', values=runs_df.columns[2:])\nrearranged_columns = sorted(list(runs_df.columns.values), key=group_horse_and_result)\nruns_df = runs_df[rearranged_columns]\nprint(runs_df.head())\n\n# quite some NaNs appreared in the dataframe, reason is some races didnt have full 14 horses participating\n# fill with 0\nruns_df = runs_df.fillna(0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Prepare training/test data \nHere, we combine races data and runs data by `join` two data frames above. \n\n### Standardization\nIf you look at the data closely, if will find that features are not in the same scale, e.g. weight can go to 1000+. Standardize the data for to make training easier. \n\n### Select right columns for X, y\n- Select all the data except last 28 columns, because last 28 columns is about 'result' and 'won'\n- Select last 14 columns for y_won. Each row shall have one '1.0' and rest are 0. \n- Select second last 14 columns for y_top3. It used to the the column 'result', e.g. 1~14, which is horses' final positions when the race finishes. Apply a function to convert it to 1.0 if the horse is in top 3, else 0. \n\n### Split data into train/test sets\n\nsklearn comes with such a handy method `train_test_split`. We split the data as following:\n- 80% for training\n- 20% for testing(validation)\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data = races_df.join(runs_df, on='race_id', how='right')\nX = data[data.columns[:-14]] \nss = preprocessing.StandardScaler()\nX = pd.DataFrame(ss.fit_transform(X),columns = X.columns)\n\ny_won = data[data.columns[-14:]].applymap(lambda x: 1.0 if 0.5 < x < 1.5 else 0.0) \n\nprint(X.shape)\nprint(y_won.shape)\n\n# split data into train and test sets\nX_train, X_test, y_train, y_test = model_selection.train_test_split(X, y_won, train_size=0.8, test_size=0.2, random_state=1)\nprint('X_train', X_train.shape)\nprint('y_train', y_train.shape)\nprint('X_test', X_test.shape)\nprint('y_test', y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Build the model\n\nUse keras to build the model with easy-to-use api `Sequential`. \n\nHave to mention that input layer has 104 inputs. The calculation is following:\n- 6 features from races dataframe: 'venue', 'config', 'surface', 'distance', 'going', 'race_class'\n- 14 horses per races, and each horse has 7 features; 'horse_age', 'horse_country', 'horse_type', 'horse_rating', 'declared_weight', 'actual_weight', 'win_odds'\n- so total 104 features = 6 + 14 x 7\n\n\nOutput layer has 14 nodes, as each node stands for each horse's result.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model = tf.keras.Sequential([\n    tf.keras.layers.Dense(96, activation='relu', input_shape=(104,)),\n    tf.keras.layers.Dense(14, activation='softmax')\n])\nmodel.compile(optimizer=tf.keras.optimizers.Adam(5e-04),\n              loss=tf.keras.losses.CategoricalCrossentropy(),\n              metrics=[tf.keras.metrics.Precision(name='precision')])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train the model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = tf.data.Dataset.from_tensor_slices((X_train.values, y_train.values))\ntrain_dataset = dataset.shuffle(len(X_train)).batch(500)\ndataset = tf.data.Dataset.from_tensor_slices((X_test.values, y_test.values))\nvalidation_dataset = dataset.shuffle(len(X_test)).batch(500)\n\nprint(\"Start training..\\n\")\nhistory = model.fit(train_dataset, epochs=200, validation_data=validation_dataset)\nprint(\"Done.\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Plot the result\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"precision = history.history['precision']\nval_precision = history.history['val_precision']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(1, len(precision) + 1)\n\nplt.plot(epochs, precision, 'b', label='Training precision')\nplt.plot(epochs, val_precision, 'r', label='Validation precision')\nplt.title('Training and validation precision')\nplt.legend()\nplt.figure()\n\nplt.plot(epochs, loss, 'b', label='Training loss')\nplt.plot(epochs, val_loss, 'r', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Conclusion\n\nWith the 2 layer nerual network, we reached 0.92 precision on the the training dataset. However, best precision on the testing dataset was about 0.3, which happened around epoch 70~80. Then overfitting happened. \n\nprecision = 0.3, means If we bet 'Win' 10 times based on the model's prediction, only 3 times is correct.  ","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}