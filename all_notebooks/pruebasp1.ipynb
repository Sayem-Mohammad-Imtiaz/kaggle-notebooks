{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Third party\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import KBinsDiscretizer\nfrom sklearn.tree import DecisionTreeClassifier\nimport plotly.express as px\nimport ipywidgets as widgets\nimport seaborn as sns\n\n# Local application\nimport miner_a_de_datos_an_lisis_exploratorio_utilidad as utils","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"seed = 17102","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1. Acceso y almacenamiento de datos"},{"metadata":{},"cell_type":"markdown","source":"El conjunto de datos a tratar ahora es `wisconsin`. En este conjunto se detalla una bbdd en la que se examinan características de las células de un cáncer benigno o maligno. Encontramos 569 casos donde la variable objetivo, que denomina el tipo de cáncer`diagnosis`, puede ser:\n\n* `M = malignant`: Maligno\n* `B = benign`: Benigno\n\nPara determinar el tipo de cáncer se han tomado varias de las características que hacen referencia a la media, el error estándar y \"peor\" o mayor (media de los tres valores más grandes) para cada imagen de las células, resultando en 30 variables predictoras, todas ellas continuas:\n\n* `radius`: radio, distancia media desde el centro hasta los puntos del perímetro.\n* `texture`: textura, desviación estándar de los valores de la escala de grises.\n* `perimeter`: perímetro, suma de las longitudes del contorno de las figuras/formas.\n* `area`: concepto métrico que puede permitir asignar una medida a la extensión de una superficie.\n* `smoothness`: suavidad, variación local en longitudes de radio.\n* `compactness`: compacidad, (perímetro^2 / area - 1.0)\n* `concavity`: concavidad, severidad de las porciones cóncavas del contorno\n* `concave points`: puntos cóncavos, número de puntos cóncavos del contorno\n* `symmetry`: simetría\n* `fractal dimension`: dimensión fractal, (\"aproximación de la costa\" - 1)\n\nEl objetivo sería clasificar nuevos casos como malignos o benignos en función de sus propiedades."},{"metadata":{},"cell_type":"markdown","source":"Cargamos el conjunto de datos `wisconsin`"},{"metadata":{"trusted":true},"cell_type":"code","source":"filepath = \"../input/breast-cancer-wisconsin-data/data.csv\"\n\nindex = 'id'\ntarget = 'diagnosis'\n\ndata = utils.load_data(filepath, index, target)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ahora comprobamos que la base de datos se ha cargado correctamente cargando los 5 ejemplos aleatorios"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.sample(5, random_state=seed)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Podemos observar que por error se nos genera una última columna que, comprobando la descripción de nuestra base `wisconsin`,se debe a que en la última variable introduce una coma al final, lo que acaba generando una especie de \"variable vacía\" que eliminaremos"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data.drop(columns=['Unnamed: 32'])\ndata.sample(5, random_state=seed)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ahora separaemos nuestro conjunto de datos en 2, uno con las variables predictoras (X) y otro con la variable objetivo `diagnosis` (y)."},{"metadata":{"trusted":true},"cell_type":"code","source":"(X, y) = utils.divide_dataset(data, target=\"diagnosis\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"De nuevo, comprobamos que todo esté correcto"},{"metadata":{"trusted":true},"cell_type":"code","source":"X.sample(5, random_state=seed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y.sample(5, random_state=seed)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Para el análisis exploratorio hemos dividido nuestro conjunto de datos en 2 con los siguientes porcentajes:\n\n* Una muestra de entrenamiento (típicamente, 70%)\n* Una muestra de prueba (típicamente, 30%)\n"},{"metadata":{},"cell_type":"markdown","source":"Ahora aleatorizamos nuestros datos e iniciamos el proceso de holdout"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_size = 0.7\n\n(X_train, X_test, y_train, y_test) = train_test_split(X, y,\n                                                      stratify=y,\n                                                      random_state=seed,\n                                                      train_size=train_size)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"De nuevo, comprobamos que esta división se ha llevado a cabo correctamente"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.sample(5, random_state=seed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test.sample(5, random_state=seed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train.sample(5, random_state=seed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test.sample(5, random_state=seed)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Análisis exploratorio de datos"},{"metadata":{},"cell_type":"markdown","source":"Antes de cualquier operación es indispensable identificar:\n* Número de casos\n* Número de variables\n    * Tipo de las variables: Continuas (t.c.c. numéricas) o discretas (t.c.c. categóricas)\n    "},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"(número de casos, númerero de variables): \",data.shape,'\\n') \nprint(\"información de las varibles:\\n\")\nprint(data.info(memory_usage=False))\nprint(\"\\nvalores de la variable objetivo: \\n\",y.cat.categories)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Como ya hemos comentado, tenemos **569 casos** y **31 variables**, 30 discretas (`float64`) y 1 categórica (`category`) que es nuestra variable objetivo cuyos valores pueden ser `B`o `M`\n\nA parte también observamos que no existen nulos ya que la cuenta de todas las variables es el total, 569"},{"metadata":{},"cell_type":"markdown","source":"### Visualización de las variables"},{"metadata":{},"cell_type":"markdown","source":"Para empezar con la visualización, vamos a observar la diferencia de casos que hay en la partición de entrenamientos referidas a la variable `diagnosis`"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creamos una variable auxiliar con todos los datos para las gráficas, como están ordenadas de la misma manera simplifica el resultado\nX_y_train = X_train[0:]\nX_y_train['diagnosis'] = y_train\nutils.plot_barplot(X_y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Podemos observar gracias a esto que abundan más casos donde el cáncer acaba siendo benigno (63% = 250) que maligno (37% = 148)"},{"metadata":{},"cell_type":"markdown","source":"Como ya hemos mencionado, esta bbdd tiene 30 variables, que realmente son 10 pero resultan en 30 debido a que se calculan en relación a la media, el error y la media de los 3 valores mas grandes. Luego para verlo mejor, dividiremos esta sección en 10 graficas para que de esta forma puedan ser más visuales."},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = list(X_train.columns)\n\nX_train1 = X_train[[cols[0]] + [cols[10]]+ [cols[20]]]\nutils.plot_histogram(X_train1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train2 = X_train[[cols[1]] + [cols[11]] + [cols[21]]]\nutils.plot_histogram(X_train2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train3 = X_train[[cols[2]] + [cols[12]] + [cols[22]]]\nutils.plot_histogram(X_train3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train4 = X_train[[cols[3]] + [cols[13]]+ [cols[23]]]\nutils.plot_histogram(X_train4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train5 = X_train[[cols[4]] + [cols[14]]+ [cols[24]]]\nutils.plot_histogram(X_train5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train6 = X_train[[cols[5]] + [cols[15]]+ [cols[25]]]\nutils.plot_histogram(X_train6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train7 = X_train[[cols[6]] + [cols[16]]+ [cols[26]]]\nutils.plot_histogram(X_train7)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train8 = X_train[[cols[7]] + [cols[17]]+ [cols[27]]]\nutils.plot_histogram(X_train8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train9 = X_train[[cols[8]] + [cols[18]]+ [cols[28]]]\nutils.plot_histogram(X_train9)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train10 = X_train[[cols[9]] + [cols[19]]+ [cols[29]]]\nutils.plot_histogram(X_train10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Las conclusiones que podemos sacar de estas gráficas son varias, comenzando con que todas las variables siguen más o menos (en algunos casos necesitamos ampliar para verlo) que las distribuciones tienen una tendencia en forma de campana, a parte también podemos ver que las gráficas `mean` son semejantes entre si, lo mismo ocurre con `se` y `worst`.\n\nTambién podemos observar que los mayores outliers se encuentran en las gráficas de `mean` y `worst`"},{"metadata":{},"cell_type":"markdown","source":"Antes de echar un vistazo a las gráficas de discretización, vamos a inspeccionar todas variables para ver cómo evoluciona el cáncer dependiendo de los valores de dichas variables."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.describe(include=\"number\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def _plot_barplot2(variable, color):\n    fig = px.histogram(X_y_train, x=variable, color=color)\n    fig.show()\n\ncategorical_data = utils._filter_numerical_data(X_y_train)\nvar = categorical_data.columns\ndata = widgets.fixed(categorical_data)\n\nwidgets.interact(_plot_barplot2, variable=var,color=\"diagnosis\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lo que primeramente podemos observar de estas gráficas es que los casos de beningno alcanzan valores `count` más altos en general que los de maligno, esto se debe a lo ya analizado anteriormente y es que tenemos más casos benignos que malignos luego es algo norlmal.\n\nRespecto a los valores hay algo que siguen la totalidad de las variables, esto es que a valores bajos siempre predominan los casos de benigno, mientras que a valores altos tenemos 2 opciones bastante reñidas, una es que más o menos estén en igualdad de casos o que el caso maligno predomine (hay algunas excepciones donde en valores altos predomina los casos benignos estos son `fractal_dimension_mean`, `fractal_dimension_se`), esto podría influir entonces a la hora de discretizar.\n\nOtra cosa bastante observable es que las gráficas describen figuras en forma de campana y en muchos casos también podemos darnos cuenta de como ambos casos, beningno y maligno, coinciden a la hora de crecer, el punto más alto y la disminución, como ocurre en `smoothness_mean`"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_y_train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"En las próximas gráficas podemos ver que las diagonales se corresponden con las anteriores luego ya llevamos algo de adelanto para la hora de discretizar, nos quedaría el resto, luego:\n\n**¿Cuáles serían buenas variables para discretizar?**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#MEAN\ncols = list(X_y_train.columns)\nX_y_train11 = X_y_train[[cols[-1]] + cols[0:10]]\nutils.plot_pairplot(X_y_train11, target=\"diagnosis\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"En el caso de `mean` podríamos discretizar por`radius_mean`, `perimeter_mean`, `area_mean` y `concave pointes_mean` (eje x) ya que al combinarlas con el resto de variables, estas 4 provocan que a mayor número de ellas, el cáncer sea maligno y a menor benigno, luego podriamos discretizar bastante bien con ellas. Además de que prácticamente no hay muchos outliers en estos casos, cosa que por ejemplo si ocurre en bastantes combinaciones de la variable `concavity_mean` (eje x)."},{"metadata":{"trusted":true},"cell_type":"code","source":"#SE\nX_y_train12 = X_y_train[[cols[-1]] + cols[10:20]]\nutils.plot_pairplot(X_y_train12, target=\"diagnosis\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Respecto a los casos `se` podemos observar que de principio existen bastantes outliers en la mayoría de las gráficas, y que básicamente las discretizaciones podrían salir de`perimeter_se` y `area_se` (eje x) junto con todas sus combinaciones. Además algunas discretizaciones adicionales también podrían ser algunas combinaciones de `radius_se` (aunque nos puedan empeorar algo los outliers)."},{"metadata":{"trusted":true},"cell_type":"code","source":"#WORST\nX_y_train13 = X_y_train[cols[20:]]\nutils.plot_pairplot(X_y_train13, target=\"diagnosis\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Por último los `worst`, existen batantes casos por los que se podría realizar una buena discretización, por ejemplo todas las combinaciones de las variables `radius_worst`, `perimeter_worst`, `area_worst`, `compactness_worst`, `concave points_worst`.\n\nA parte, ya no las voy a nombrar porque son muchísimas, el resto también tiene combinaciones de variables muy buenas para discretizar como es el caso de por ejemplo la combinación (x,y) (`texture_worst`,`area_worst`)."},{"metadata":{},"cell_type":"markdown","source":"# 4. Preprocesamiento de datos"},{"metadata":{},"cell_type":"markdown","source":"Dentro del preprocesamiento de datos, podemos destacar las siguientes tareas:\n\n* Limpieza de datos (imputación de valores perdidos, suavizado del ruido, etc.)\n* Integración de datos (a partir de múltiples fuentes)\n* Transformación de datos (normalización, construcción, etc.)\n* Reducción de datos (discretización de variables numéricas, selección de variables, selección de instancias, etc.)\n\nSin embargo siguiendo las directrices de la práctica, solo nos centraremos en la discretización de variables numéricas"},{"metadata":{},"cell_type":"markdown","source":"### Discretización"},{"metadata":{},"cell_type":"markdown","source":"Observando las gráficas, una discretización por anchura puede que no sea una buena manera, luego probaremos con frecuencia y a parte divideremos en más intervalos ya que viendo las gráficas al divirlas en más podremos dejar casos más aislados ya que la mezcla de variables, menigno y maligno, se encuentra más o menos cerca del centro y en los principios y finales de las gráficas suele predominar una variabble"},{"metadata":{"trusted":true},"cell_type":"code","source":"discretizer = KBinsDiscretizer(n_bins=4, strategy=\"quantile\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5. Algoritmos de clasificación"},{"metadata":{},"cell_type":"markdown","source":"## Zero-R"},{"metadata":{"trusted":true},"cell_type":"code","source":"zero_r_model = DummyClassifier(strategy=\"most_frequent\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Árbol de decisión"},{"metadata":{"trusted":true},"cell_type":"code","source":"tree_model = DecisionTreeClassifier(random_state=seed)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### *Pipeline*"},{"metadata":{"trusted":true},"cell_type":"code","source":"discretize_tree_model = make_pipeline(discretizer, tree_model)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 6. Evaluación de modelos"},{"metadata":{},"cell_type":"markdown","source":"## Zero-R"},{"metadata":{"trusted":true},"cell_type":"code","source":"utils.evaluate(zero_r_model,\n               X_train, X_test,\n               y_train, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Árbol de decisión sin descretización\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"utils.evaluate(tree_model,\n               X_train, X_test,\n               y_train, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Árbol de decisión con descretización"},{"metadata":{"trusted":true},"cell_type":"code","source":"utils.evaluate(discretize_tree_model,\n               X_train, X_test,\n               y_train, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Realizando varias pruebas, cambiendo entre el número de `n_bins` y probando también la estrategia de anchura, nos damos cuenta de que zero-R y el árbol sin discretización no cambian su `Accuracy` solo cambia el árbol con discretización que básicamente la búsqueda actual parece ser si no la mejor de las mejores, aún así este método de validación `Accuracy` no parece el más apropiado para esto.\n\nEstamos diagnosticando una enfermedad, encima muy grave, no sería lo mismo fallar diagnosticando un cáncer benigno que maligno, luego un método de validación sensible al coste donde se penalizara más el equivocarnos al diagnosticar un cáncer que debe ser maligno, tendría más sentido que utilizar el `Accuracy`."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}