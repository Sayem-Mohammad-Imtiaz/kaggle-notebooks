{"cells":[{"metadata":{},"cell_type":"markdown","source":"Its very important to know the type of Data to decide which Machine learning algorthim you have to use.There are three types of Data:\n\n* Categorical Data\n* Numerical Data\n* Ordinal Data\n\nIn these notebook we will perfom following task:\n* First we will look through data and explore\n* We will see if data contains any missing value and try to fix it.\n* Insights of data are always usefull hence we will explore through seaborn , matplotlib libraries.\n* We will deploy through Logistic Regression Model."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input/loan-predication'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/loan-predication/train_u6lujuX_CVtuZ9i (1).csv')\ntest = pd.read_csv('../input/av-test/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**First we deal with the missing values** "},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nsns.set(rc={'figure.figsize':(10,8)})\nsns.countplot('Loan_Status',hue='Gender',data=train\n             )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['self'] = 'train'\ntest['self'] = 'test'\nraw_data = pd.concat([train,test])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Most probably after looking through the graph we are going to fill with mode of the data i.e 'Male'*"},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_data['Gender'] = raw_data['Gender'].fillna('Male')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot('Loan_Status',hue='Married',data=raw_data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can infer from this that those who were married were most probably to get the loan."},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_data['Married'] = raw_data['Married'].fillna('Yes')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_data['Dependents'] = raw_data['Dependents'].fillna(-99)\nfor i in range(len(raw_data)):\n    if(raw_data['CoapplicantIncome'].iloc[i]!=0 and raw_data['Dependents'].iloc[i]==-99 ):\n        raw_data['Dependents'].iloc[i]= '1'\n    elif(raw_data['CoapplicantIncome'].iloc[i]==0 and raw_data['Dependents'].iloc[i]==-99 ):\n        raw_data['Dependents'].iloc[i]=='0'\n    else:\n        continue\n        \n        \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_data['more_than_one_dependents'] = raw_data['Dependents'].apply(lambda x : 1 if x!='0' else 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_data['no_dependent'] = raw_data['Dependents'].apply(lambda x : 1 if x=='0' else 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since most of the dependents are 0 we are gonna fill with it with 0"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot('Loan_Status',hue='Self_Employed',data=raw_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_data['Self_Employed'] = raw_data['Self_Employed'].fillna('No')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(raw_data['LoanAmount'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.scatterplot(raw_data['LoanAmount'],y=np.arange(0,981),hue=raw_data['Loan_Status'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see outliers as data seems to right skewed , so we will not fill values by mean of this data, we gonna take the mean without containing outliers."},{"metadata":{"trusted":true},"cell_type":"code","source":"mean=raw_data[raw_data['LoanAmount']<=200]['LoanAmount'].mean()\nraw_data['LoanAmount']=raw_data['LoanAmount'].fillna(mean)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.scatterplot(raw_data['Loan_Amount_Term'],y=np.arange(0,981),hue=raw_data['Loan_Status'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For the missing value we are going to use mode."},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_data['Loan_Amount_Term'].fillna(raw_data['Loan_Amount_Term'].value_counts().idxmax(), inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_data['Credit_History'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot('Loan_Status',hue='Credit_History',data=raw_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.scatterplot(raw_data['Credit_History'],y=np.arange(0,981),hue=raw_data['Loan_Status'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Again we are going to use mode*"},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_data['Credit_History'].fillna(raw_data['Credit_History'].value_counts().idxmax(), inplace=True)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_data.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_data['total_income'] = raw_data['ApplicantIncome'] + raw_data['CoapplicantIncome']\nraw_data['EMI'] = (raw_data['LoanAmount']*0.09*(1.09**raw_data['Loan_Amount_Term']))/(1.09**(raw_data['Loan_Amount_Term']-1))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_data['Loan_Amount_Term'].value_counts()\nraw_data['common_loan_term'] = raw_data['Loan_Amount_Term'].apply(lambda x : 1 if x==360.0 else 0)\nraw_data['uncommon_loan_term'] = raw_data['Loan_Amount_Term'].apply(lambda x: 1 if x!=360.0 else 0)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.figure(figsize=(12,12))\nsns.heatmap(raw_data.corr(),annot=True,cbar=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_data = raw_data.drop(['Loan_ID','ApplicantIncome','CoapplicantIncome','Dependents','Loan_Amount_Term'],1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Part 2\nWe are first going to one hot encode every Categorcial data then we are going to deploy it in our model"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = raw_data[raw_data['self']=='train']\ntrain = train.drop(['self'],1)\ntest = raw_data[raw_data['self']=='test']\ntest = test.drop(['self','Loan_Status'],1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nlb = LabelEncoder()\nfrom sklearn.metrics import roc_auc_score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us check if the category doesnt contain any duplicates such as yes, YES ..."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train['Gender'].unique())\nprint(train['Married'].unique())\nprint(train['Education'].unique())\nprint(train['Self_Employed'].unique())\nprint(train['Property_Area'].unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['Gender'] = lb.fit_transform(train['Gender'])\ntrain['Married'] = lb.fit_transform(train['Married'])\ntrain['Education'] = lb.fit_transform(train['Education'])\ntrain['Self_Employed'] = lb.fit_transform(train['Self_Employed'])\ntrain['Property_Area'] = lb.fit_transform(train['Property_Area'])\ntrain['Loan_Status'] = lb.fit_transform(train['Loan_Status'])\n\n\ntest['Gender'] = lb.fit_transform(test['Gender'])\ntest['Married'] = lb.fit_transform(test['Married'])\ntest['Education'] = lb.fit_transform(test['Education'])\ntest['Self_Employed'] = lb.fit_transform(test['Self_Employed'])\ntest['Property_Area'] = lb.fit_transform(test['Property_Area'])\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets see our result"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train['Gender'].unique())\nprint(train['Married'].unique())\nprint(train['Education'].unique())\nprint(train['Self_Employed'].unique())\nprint(train['Property_Area'].unique())\nprint(train['Loan_Status'].unique())\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Part 3\nDeploying it in the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler,RobustScaler\nfrom sklearn.model_selection import train_test_split\nrb = RobustScaler()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"col = ['LoanAmount','total_income','EMI']\nfor i in col:\n    train[[i]] = rb.fit_transform(train[[i]])\n    test[[i]] = rb.transform(test[[i]])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train.drop(['Loan_Status'],axis=1)\ny = train['Loan_Status']\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42,stratify=y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def training(model,X,y):\n    from sklearn.model_selection import train_test_split\n    X_train,X_test,y_train,y_test = train_test_split(X,y,test_size =0.25,random_state=42,stratify=y)\n    model.fit(X_train,y_train)\n    preds_train = model.predict(X_train)\n    preds_test = model.predict(X_test)\n    print(\"ROC score for train:\",roc_auc_score(y_train,preds_train))\n    print(\"ROC score for test:\",roc_auc_score(y_test,preds_test))\n    print('\\n')\n    print(\"Confusion Matrix for train :\\n\",confusion_matrix(y_train,preds_train))\n    print(\"Confusion Matrix for test :\\n\",confusion_matrix(y_test,preds_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report,confusion_matrix\nlgt = LogisticRegression()\ntraining(lgt,X,y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgt_preds = lgt.predict(test)\nsub = pd.read_csv('../input/sample-sub/sample_submission_49d68Cx.csv')\nsub['Loan_Status'] = lgt_preds\nsub['Loan_Status'] = sub['Loan_Status'].replace({1:'Y',0:'N'})\nsub.to_csv('lgt_prediction_2.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Hence from Logistic regression we get an accuracy of about 85%**<br>\nJust as starting our Logisitic Regression performed well."},{"metadata":{},"cell_type":"markdown","source":"LightGBM"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold\nfrom lightgbm import LGBMClassifier\n\noof_pred               = np.zeros((len(train),))\ny_pred_final           = np.zeros((len(test),))\nnum_models             = 3\n\nn_splits               = 20\nerror                  = []\n\nkf=StratifiedKFold(n_splits=n_splits,shuffle=True,random_state=294)\n    \nfor i,(train_idx,val_idx) in enumerate(kf.split(X,y)):    \n    \n    wghts                     = [0]*num_models\n    test_roc_score            = []\n    \n    \n    X_train, y_train = X.iloc[train_idx,:], y.iloc[train_idx]\n\n    X_val, y_val = X.iloc[val_idx, :], y.iloc[val_idx]\n    \n\n    print('\\nFold: {}\\n'.format(i+1))\n\n    model1 = LGBMClassifier(boosting_type='gbdt',n_estimators=500,depth=-1,learning_rate=0.03,scale_pos_weight=7,objective='binary',metric='auc',\n                 colsample_bytree=0.5,random_state=294,n_jobs=-1)\n    model1.fit(X_train,y_train)\n    testpred1 = model1.predict_proba(X_val)[:,1]\n    test_roc_score.append(roc_auc_score(y_val, testpred1))\n    print(\"Test ROC AUC for model 1: %.4f\"%(roc_auc_score(y_val, testpred1)))\n    \n    model2 = LGBMClassifier(boosting_type='gbdt',n_estimators=300,depth=-1,learning_rate=0.03,scale_pos_weight=7,objective='binary',metric='auc',\n                 colsample_bytree=0.3,reg_alpha=2,random_state=294,n_jobs=-1)\n    model2.fit(X_train,y_train)\n    testpred2 = model2.predict_proba(X_val)[:,1]\n    test_roc_score.append(roc_auc_score(y_val, testpred2))\n    print(\"Test ROC AUC for model 2: %.4f\"%(roc_auc_score(y_val, testpred2)))\n    \n    model3 = LGBMClassifier(boosting_type='gbdt',n_estimators=400,depth=-1,learning_rate=0.03,scale_pos_weight=7,objective='binary',metric='auc',\n                 colsample_bytree=0.4,reg_alpha=2,random_state=294,n_jobs=-1)\n    model3.fit(X_train,y_train)\n    testpred3 = model3.predict_proba(X_val)[:,1]\n    test_roc_score.append(roc_auc_score(y_val, testpred3))\n    print(\"Test ROC AUC for model 3: %.4f\"%(roc_auc_score(y_val, testpred3)))\n    \n    wghts              = np.exp(-1000*np.array(test_roc_score/sum(test_roc_score)))\n    wghts              = wghts/sum(wghts)\n    \n    val_pred           = wghts[0]*testpred1+wghts[1]*testpred2 +wghts[2]*testpred3\n    print('validation roc_auc_score fold-',i+1,': ',roc_auc_score(y_val, val_pred))\n    \n    oof_pred[val_idx]  = val_pred\n    y_pred_final += (wghts[0]*model1.predict_proba(test)[:,1]+wghts[1]*model2.predict_proba(test)[:,1]+wghts[2]*model3.predict_proba(test)[:,1])/(n_splits)\n    \n    print('\\n')\n    \nprint('OOF ROC_AUC_Score:- ',(roc_auc_score(y,oof_pred)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"CATBOOST"},{"metadata":{"trusted":true},"cell_type":"code","source":"from catboost import CatBoostClassifier\noof_pred               = np.zeros((len(train),))\ny_pred_final           = np.zeros((len(test),))\nnum_models             = 3\n\nn_splits               = 20\nerror                  = []\n\nkf=StratifiedKFold(n_splits=n_splits,shuffle=True,random_state=294)\n    \nfor i,(train_idx,val_idx) in enumerate(kf.split(X,y)):    \n    \n    wghts                     = [0]*num_models\n    test_roc_score            = []\n    \n    \n    X_train, y_train = X.iloc[train_idx,:], y.iloc[train_idx]\n\n    X_val, y_val = X.iloc[val_idx, :], y.iloc[val_idx]\n    \n\n    print('\\nFold: {}\\n'.format(i+1))\n\n    model1 = CatBoostClassifier(learning_rate = 0.03,random_state=42,scale_pos_weight=7, custom_metric=['AUC'])\n    model1.fit(X_train,y_train,eval_set=(X_val, y_val),early_stopping_rounds=30,verbose=100)\n    testpred1 = model1.predict_proba(X_val)[:,1]\n    test_roc_score.append(roc_auc_score(y_val, testpred1))\n    print(\"Test ROC AUC for model 1: %.4f\"%(roc_auc_score(y_val, testpred1)))\n    \n    model2 = CatBoostClassifier(learning_rate = 0.04,random_state=42,scale_pos_weight=7, custom_metric=['AUC'])\n    model2.fit(X_train,y_train,eval_set=(X_val, y_val),early_stopping_rounds=40,verbose=100)\n    testpred2 = model2.predict_proba(X_val)[:,1]\n    test_roc_score.append(roc_auc_score(y_val, testpred2))\n    print(\"Test ROC AUC for model 2: %.4f\"%(roc_auc_score(y_val, testpred2)))\n    \n    model3 = CatBoostClassifier(learning_rate = 0.05,random_state=42,scale_pos_weight=7, custom_metric=['AUC'])\n    model3.fit(X_train,y_train,eval_set=(X_val, y_val),early_stopping_rounds=20,verbose=100)\n    testpred3 = model3.predict_proba(X_val)[:,1]\n    test_roc_score.append(roc_auc_score(y_val, testpred3))\n    print(\"Test ROC AUC for model 3: %.4f\"%(roc_auc_score(y_val, testpred3)))\n    \n    wghts              = np.exp(-1000*np.array(test_roc_score/sum(test_roc_score)))\n    wghts              = wghts/sum(wghts)\n    \n    val_pred           = wghts[0]*testpred1+wghts[1]*testpred2 +wghts[2]*testpred3\n    print('validation roc_auc_score fold-',i+1,': ',roc_auc_score(y_val, val_pred))\n    \n    oof_pred[val_idx]  = val_pred\n    y_pred_final += (wghts[0]*model1.predict_proba(test)[:,1]+wghts[1]*model2.predict_proba(test)[:,1]+wghts[2]*model3.predict_proba(test)[:,1])/(n_splits)\n    \n    print('\\n')\n    \nprint('OOF ROC_AUC_Score:- ',(roc_auc_score(y,oof_pred)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cb_preds = cb.predict(test)\nsub = pd.read_csv('../input/sample-sub/sample_submission_49d68Cx.csv')\nsub['Loan_Status'] = cb_preds\nsub['Loan_Status'] = sub['Loan_Status'].replace({1:'Y',0:'N'})\nsub.to_csv('cb_prediction_2.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import AdaBoostClassifier\nada = AdaBoostClassifier(base_estimator=lgt)\nada.fit(X_train,y_train)\nada.score(X_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using SVM"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import svm\nfrom sklearn.metrics import accuracy_score\nclf = svm.SVC(kernel='linear')\nclf.fit(X_train,y_train)\npreds = clf.predict(X_test)\nprint(accuracy_score(y_test,preds))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import lightgbm as lgb\nmodel = lgb.LGBMClassifier(objective='mse', seed=8798, num_threads=1)\nmodel.fit(X_train, y_train, eval_set=[(X_test, y_test), (X_train, y_train)], verbose=10)\nlgb.plot_metric(model)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.score(X_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_preds = model.predict(test)\nsub = pd.read_csv('../input/sample-sub/sample_submission_49d68Cx.csv')\nsub['Loan_Status'] = model_preds\nsub['Loan_Status'] = sub['Loan_Status'].replace({1:'Y',0:'N'})\nsub.to_csv('model_lgb_prediction.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**THERE IS AN OVERFITTING**<br>\nWe need to tune parameters to prevent the overfitting."},{"metadata":{},"cell_type":"markdown","source":"**Consider this as just starting model, what more could be done:<br>**\n\n1) Hyperparamater tunings of best model.<br>\n2) Adding features using best feature engineering.<br>\n3) I haven't tried neural network but you can use it.<br>\n4) More over we can achieve atleast accuracy of 0.82-0.85, using proper hyperparameter tuning, and <br>\n   using additional features."},{"metadata":{},"cell_type":"markdown","source":"# Feel free to comment any doubt , if you got more better result or thought of any other feature engineering you can comment, as sharing make us stronger.\n** Thanks if you liked my work dont forget to upvote. **"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}