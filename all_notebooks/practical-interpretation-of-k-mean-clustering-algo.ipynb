{"cells":[{"metadata":{},"cell_type":"markdown","source":"<center>\n    \n<img src=\"https://1.bp.blogspot.com/-fHdsJ8Q5TFU/WjqTHcKqZ-I/AAAAAAAAAic/_tVg-_c5XjcU96uWkMlzvkJ-yY3kyx2JgCLcBGAs/s1600/K-Means-Clustering-In-Machine-Learning.jpg\" height=500 width=500/>\n\n<br>\n<img src=\"https://media.giphy.com/media/12vVAGkaqHUqCQ/giphy.gif\"/>\n\n</center>"},{"metadata":{},"cell_type":"markdown","source":"## Table of Contents\n\n1. [Problem Statement](#section1)<br>\n \n2. [Importing packages](#section2)<br>\n    \n3. [Data Loading and Description](#section3)<br>\n    - 3.1 [Description of the Dataset](#section301)<br>\n    - 3.2 [Pandas Profiling before Data Preprocessing](#section302)<br>\n4. [Preprocessing](#section4)<br>\n    - 4.1 [Droping the Highly correlated column](#section401)<br>\n    - 4.2 [Encode the categorical feature](#section402)<br>\n    - 4.3 [Pandas Profiling after Data Preprocessing](#section403)<br>\n    \n5. [Data Exploration](#section5)<br>\n    - 5.1 [Scatterplot of Grad.Rate vs Room.Board](#section501)\n    - 5.2 [Scatterplot of P.Undergrad vs Outstate](#section502)\n    \n6. [K-means Clustering Tree](#section6)<br>\n    - 6.1 [k-means Clustering Use case](#section601)<br>\n    - 6.2 [Overview : What is Clustering?](#section602)<br>\n        - 6.2.1 [Types of Clustering](#section60201)\n    - 6.3 [Types of Clustering Algorithms](#section603)<br>\n    - 6.4 [Introduction to K-means Algorithm](#section604)<br>\n    - 6.5 [Business Cases](#section605)<br>\n    - 6.6 [Algorithm](#section606)<br>\n        - 6.6.1 [Cluster Assignment](#section60601)<br>\n        - 6.6.2 [Move Centroid](#section60601)<br>\n    - 6.7 [Choosing K](#section607)<br>\n    - 6.8 [How good is K-means?](#section608)<br>\n    - 6.9 [Introduction to Heirarchial Clustering](#section609)<br>\n        - 6.9.1 [How to measure closeness of points?](#section60901)<br>\n        - 6.9.2 [How to calculate distance between two clusters?](#section60902)<br>\n        - 6.9.3 [Algorithm Explained](#section60903)<br>\n        - 6.9.4 [How many clusters to form?](#section60904)<br>\n        - 6.9.5 [Good Cluster Analysis](#section60905)<br>\n7. [K-means Cluster Creation](#section7)<br>\n    - 7.1 [Normalize data](#section701)<br>\n    - 7.2 [Evaluation](#section702)<br>\n    - 7.3 [Create a confusion matrix](#section703)<br>\n    - 7.4 [Question: What about the K Value?](#section704)<br>\n    - 7.5 [Choosing appropriate value of k](#section705)<br>\n    - 7.6 [Clustering Visualization](#section706)<br>\n        - 7.6.1 [Dimensinal Reduction using PCA](#section70601)\n    - 7.7 [Centroid Visualization](#section707)<br>\n    \n8. [Conclusion](#section8)<br>"},{"metadata":{},"cell_type":"markdown","source":"<a id=section1></a>\n## 1. Problem Statement"},{"metadata":{},"cell_type":"markdown","source":"The __U.S News and World Report’s College Data__ about the different __colleges or university__ and using the given information our task is to __predict__ wether the College | University is __Private University__ or __Public University__.\n\n__Imp Note__: When using the Kmeans algorithm under normal circumstances, it is because you don't have __labels__. In this case we will use the labels to try to get an idea of how well the algorithm performed(So we can practically get the real intution of K-means Clustering), but you won't usually do this for Kmeans."},{"metadata":{},"cell_type":"markdown","source":"<img src=\"http://ssd6.org/files/2016/06/high-school-graduation-traditions.jpg\"/>"},{"metadata":{},"cell_type":"markdown","source":"<a id=section2></a>\n## 2. Importing packages                                          "},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np                                   # Implemennts milti-dimensional array and matrices\nnp.set_printoptions(precision=4)                     # To display values only upto four decimal places. \n\nimport pandas as pd                                  # For data manipulation and analysis\npd.set_option('mode.chained_assignment', None)       # To suppress pandas warnings.\npd.set_option('display.max_colwidth', -1)            # To display all the data in the columns.\npd.options.display.max_columns = 40                  # To display all the columns.\n\nimport pandas_profiling\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.style.use('seaborn-whitegrid')    # To apply seaborn whitegrid style to the plots.\nplt.rc('figure', figsize=(10, 8))     # Set the default figure size of plots.\n%matplotlib inline\n\n\nimport warnings\nwarnings.filterwarnings('ignore')     # To suppress all the warnings in the notebook.\n\nfrom sklearn.metrics import classification_report, confusion_matrix","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=section3></a>\n## 3. Data Loading and Description"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing the Dataset\ncollege_data = pd.read_csv(\"/kaggle/input/us-news-and-world-reports-college-data/College.csv\",index_col=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"college_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=section301></a>\n### 3.1 Description of the Dataset"},{"metadata":{},"cell_type":"markdown","source":"| Column Name                                                                                      | Description                                                                               |\n| ------------------------------- |:-----------------------------------------------------------------------------------------:| \n| Private                             | A factor with levels No and Yes indicating private or public university                                |  \n| Apps        | Number of applications received                                                                                    | \n| Accept           | Number of applications accepted                                                                        |   \n| Enroll           | Number of new students enrolled                                                                                        |\n| Top10perc           | Pct. new students from top 10% of H.S. class                                                                                |\n| Top25perc         | Pct. new students from top 25% of H.S. class                                                                                        |\n| F.Undergrad         | Number of fulltime undergraduates                                               |\n| P.Undergrad        | Number of parttime undergraduates                                                                            |\n| Outstate         | Out-of-state tuition                                                                                     |\n| Room.Board         | Room and board costs                                                       |\n|Books | Estimated book costs                                                                                          |\n|Personal| Estimated personal spending                                                                     |\n|PhD|    Pct. of faculty with Ph.D.’s                                                                  |\n|Terminal |Pct. of faculty with terminal degree                                                                         |\n|S.F.Ratio|Student/faculty ratio                                                                                 |\n|perc.alumni  |Pct. alumni who donate                                                                                       |\n|Expend | Instructional expenditure per student                                                                           |\n|Grad.Rate|Graduation rate                                                                          |\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"college_data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- ```info``` function gives us the following insights into the data:\n  - There are a total of 777 samples (rows) and 18 columns in the dataset.\n  - One colums is __Object__ dtype\n  - Remaining columns are __Int__ and __float__ dtype\n  - None of the feature have missing value "},{"metadata":{"trusted":true},"cell_type":"code","source":"college_data.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- ```describe``` function gives us the following insights into the data:\n  - Number of unique values in each feature\n  - skewness of feature basesd __Mean__ and __Median__ value. i.e if __Mean__ is less compare to __Median__ feature is right skewed and vice versa."},{"metadata":{},"cell_type":"markdown","source":"<a id=section302></a>\n### 3.2 Pandas Profiling before Data Preprocessing\n\n<img src=\"https://raw.githubusercontent.com/insaid2018/Term-2/master/images/Pandas%20profiling.png\" height=\"500\" width=\"500\"/>"},{"metadata":{},"cell_type":"markdown","source":"- Here, we will perform **Pandas Profiling before preprocessing** our dataset, so we will name the **output file** as __data_before_preprocessing.html__. \n\n\n- The file will be stored in the directory of your notebook. Open it using the jupyter notebook file explorer and take a look at it and see what insights you can develop from it. \n\n\n- Or you can **output the profiling report** in the **current jupyter notebook** as well as shown in the code below. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Performing pandas profiling before data preparation\n# Saving the output as data_before_preprocessing.html\n\n# To output pandas profiling report to an external html file.\n'''\nprofile = college.profile_report(title='Pandas Profiling before Data Preprocessing')\nprofile.to_file(output_file=\"data_before_preprocessing.html\")\n'''\n\n# To output the pandas profiling report on the notebook.\n\ncollege_data.profile_report(title='Pandas Profiling before Data Preprocessing', style={'full_width':True})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Observations from Pandas Pre-Profiling before Data Processing** <br><br>\n__Dataset info__:\n- Number of variables: 19\n    - __Note__: Here you observe one more columns it's because pandas_profiling consider the index columns as a variable.\n    \n- Variable Type\n    - __14__ Numberic\n    - __1__ Text\n    - __1__ Boolean\n    - __3__ Rejected ( High correlation)\n    \n- Rejected variable\n\n    - __Apps__ is highly correlated with __Accept__ (ρ = 0.943450572)\t\n    - __Enroll__ is highly correlated with __Accept__ (ρ = 0.9116366634)\t\n    - __F.Undergrad__ is highly correlated with __Enroll__ (ρ = 0.964639652)\n    \n\n    \n    \n- Number of observations: 777\n- Missing cells: 0 (0.0%)\n\n- __Accept__ Feature:\n    - Right Skewed \n    - High value of kurtosis suggests __long tail__ and possibly the presence of outliers in the feature.\n<center><h3>Ouliers Detection Forumla</h3></center>\n<center><img src=\"https://raw.githubusercontent.com/insaid2018/Term-3/master/Images/outliers.png\"/>\n    \n- __Expend__ Feature:\n    - Percentage of unique values : 95.8%.\n    - High value of kurtosis suggests __long tail__ and possibly the presence of outliers in the feature.\n    - Right Skewed\n- __Grad.Rate__:\n    - Skewness value : -0.11, Which explains data near around being symmetric.\n    - Kurtosis vlaue : -0.2, Which explains data have light tail, Which mean the of data is of  __platykurtic distribution__\n    \n- __Outstate__ feature is near around symmetricity.\n\n- __P.Undergrad__:\n    - Skewness value is near to 6 mean right skewed.\n    - High value of kurtosis suggests __long tail__ and possibly the presence of outliers in the feature.\n    \n"},{"metadata":{},"cell_type":"markdown","source":"<a id=section4></a>\n## 4. Preprocessing the data"},{"metadata":{},"cell_type":"markdown","source":"<a id=section401></a>\n\n### 4.1 Droping the Highly correlated column\ni.e \n- __Apps__\n- __Enroll__\n- __F.Undergrad__"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Droping the Highly correlated features \ncollege_data.drop(['Apps','Enroll','F.Undergrad'],inplace=True,axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"college_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=section402></a>\n### 4.2 Encode the categorical feature"},{"metadata":{"trusted":true},"cell_type":"code","source":"college_data['Private'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"__Yes__: Private University\n<br>\n__No__: Public University"},{"metadata":{"trusted":true},"cell_type":"code","source":"# map the feature to integer integer value.\ncollege_data['Private']=college_data['Private'].map({'Yes':1,\"No\":0})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"college_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=section403></a>\n### 4.3 Pandas Profiling after Data Preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"# To output the pandas profiling report on the notebook.\n\ncollege_data.profile_report(title='Pandas Profiling after Data Preprocessing', style={'full_width':True})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Observations from Pandas Pre-Profiling before Data Processing** <br><br>\n__Dataset info__:\n- Number of variables: 16\n    - __Note__: Here you observe one more columns it's because pandas_profiling consider the index columns as a variable.\n    \n- Variable Type\n    - __14__ Numberic\n    - __1__ Text\n    - __1__ Boolean"},{"metadata":{},"cell_type":"markdown","source":"<a id=section5></a>\n## 5. Data Exploration\n"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section501\"></a>\n### 5.1 Scatterplot of Grad.Rate vs Room.Board"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set_style('whitegrid')\nsns.lmplot('Room.Board','Grad.Rate',data=college_data, hue='Private',\n           palette='coolwarm',size=6,aspect=1,fit_reg=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"```From``` the above ```scatter plot``` we can see:\n- As the __Room.Board__ cost increases the __Graduation Rate__ starts decreasing And this is the most common in Private Universities.\n- In the scatter plot we can see most of the data have max ```Grad.Rate``` near around __100__ but there is a point which have ```Grad.Rate``` near to __120__ could be possible __outlier__. \n    - But if you see __pandas-profiling__ and __outlier formula__ then you found the data-point is  just near to outlier But here we already have limited data so let's not drop it. Consider only extrement ouliers. i.e __Q3+3*IQR__ or __Q1-3*IQR__.\n    "},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section502\"></a>\n### 5.2 Scatterplot of P.Undergrad vs Outstate  "},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set_style('whitegrid')\nsns.lmplot('Outstate','P.Undergrad',data=college_data, hue='Private',\n           palette='coolwarm',size=6,aspect=1,fit_reg=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"```From``` the above ```scatter plot``` we can see:\n- If you see the number of P.Undergrad(part time undergrad) found their is a outliers beyond value `20000`.Refer to Pandas_profiling and find it out using outliers formula."},{"metadata":{},"cell_type":"markdown","source":"#### Drop the P.Undergrad outliers"},{"metadata":{"trusted":true},"cell_type":"code","source":"c=college_data[college_data['P.Undergrad']>10000].index.values\nc","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"college_data.drop(index=['Northeastern University', 'University of Minnesota Twin Cities',\n       'University of South Florida'],inplace=True)"},{"metadata":{"trusted":true},"cell_type":"code","source":"college_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"college_data.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=section6></a>\n## 6. K-means Clustering Tree"},{"metadata":{},"cell_type":"markdown","source":"<a id=section601></a>\n### 6.1 k-means Clustering Use case"},{"metadata":{},"cell_type":"markdown","source":"Have you come across a situation when a Chief Marketing Officer of a company tells you – __“Help me understand our customers better so that we can market our products to them in a better manner!\"__<br>\n- If the person would have asked me to calculate Life Time Value (LTV) or propensity of Cross-sell, I wouldn’t have blinked. But this question looked very broad to me!<br>\n- You are _not looking for __specific insights__ for a phenomena_, but what you are looking for are __structures with in data _with out them being tied down_ to a specific outcome__.<br>\n- The method of __identifying similar groups of data__ in a data set is called __clustering__. Entities in each group are __comparatively more similar__ to entities of that group than those of the other groups. In this article, I will be taking you through the __types of clustering__, __different clustering algorithms__ and a _comparison between two of the __most commonly used__ cluster methods_.\n\n![image.png](https://raw.githubusercontent.com/insaid2018/Term-3/master/Images/clustering%201.png)"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section602\"></a>\n### 6.2 Overview : What is Clustering?"},{"metadata":{},"cell_type":"markdown","source":"Clustering is the task of __dividing the population__ or __data points__ into _a number of groups_ such that _data points in the same groups are more similar to other data points in the same group_ than those in other groups. \n> __In simple words, the aim is to segregate groups with similar traits and assign them into clusters__. \n\n![image.png](https://raw.githubusercontent.com/insaid2018/Term-3/master/Images/clustering%202.png)\n\nLet’s understand this with an example.<br>\n1. Suppose, you are the __head of a rental store__ and wish to __understand preferences__ of your costumers to __scale up your business__. \n2. Is it possible for you to look at _details of each costumer and devise a unique business strategy_ for each one of them? __Definitely not__. \n3. But, what you can do is to __cluster__ all of your _costumers into say 10 groups based on their purchasing habits_ and use a __separate strategy__ for _costumers in each of these 10 groups_. \n4. This is what we call __clustering__."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section60201\"></a>\n#### 6.2.1 Types of Clustering"},{"metadata":{},"cell_type":"markdown","source":"Clustering can be divided into two subgroups :\n\n1. __Hard Clustering__: In hard clustering, each data point __either belongs to a cluster completely or not__.<br> _For example_, in the above example each customer is put into one group out of the 10 groups.<br><br>\n\n2. __Soft Clustering__: In soft clustering, instead of putting each data point into a separate cluster, a __probability or likelihood__ of that data point to be in those clusters is assigned.<br>_For example_, from the above scenario each costumer is assigned a probability to be in either of 10 clusters of the retail store.\n\n![image.png](https://raw.githubusercontent.com/insaid2018/Term-3/master/Images/Types%20of%20clustering.png)\n"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section603\"></a>\n### 6.3 Types of Clustering Algorithms"},{"metadata":{},"cell_type":"markdown","source":"Since the task of clustering is subjective, this means that can be used for achieving this goal are plenty. Every methodology follows a different set of rules for defining the ‘similarity’ among data points. In fact, there are more than 100 clustering algorithms known. But few of the algorithms are used popularly, let’s look at them in detail:\n- __Connectivity models__: These models are based on the notion that the data points _closer in data space __exhibit more similarity__ to each other __than__ the data points __lying farther__ away_. These models can follow two approaches.<br> Examples of these models are __hierarchical clustering algorithm__ and its variants.<br><br>\n- __Centroid models__: These are __iterative clustering algorithms__ in which the notion of similarity is derived by the __closeness of a data point to the centroid of the clusters__. K-Means clustering algorithm is a popular algorithm that falls into this category. These models run iteratively to find the __local optima__.<br><br>\n- __Distribution models__: These clustering models are based on the notion of __how probable is it that all data points in the cluster belong to the same distribution__ (For example: Normal, Gaussian). These models often suffer from __overfitting__. A popular example of these models is __Expectation-maximization algorithm__ which uses multivariate normal distributions.<br><br>\n- __Density Models__: These models search the data space for __areas of varied density of data points__ in the data space. It isolates various different density regions and assign the data points within these regions in the same cluster. Popular examples of density models are __DBSCAN and OPTICS__.\n"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section604\"></a>\n### 6.4 Introduction to K-means Algorithm"},{"metadata":{},"cell_type":"markdown","source":"K-means clustering is a type of __unsupervised learning__, which is used when you have __unlabeled data (i.e., data without defined categories or groups)__. \n> __The goal of this algorithm is to find groups in the data, with the number of groups represented by the variable K__. <br>\n\n- The algorithm works iteratively to assign each data point to one of K groups based on the features that are provided. <br>\n- Data points are clustered based on feature similarity.<br>\n\nThe results of the __K-means clustering algorithm__ are:\n1. The centroids of the K clusters, which can be used to label new data\n2. Labels for the training data (each data point is assigned to a single cluster)\n\n![image.png](https://raw.githubusercontent.com/insaid2018/Term-3/master/Images/clustering%203.gif)\n"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section605\"></a>\n### 6.5 Business Cases"},{"metadata":{},"cell_type":"markdown","source":"The K-means clustering algorithm is used to __find groups__ which have not been __explicitly labeled__ in the data. This can be used to confirm business assumptions about what types of groups exist or to identify unknown groups in complex data sets.<br>\nThis is a versatile algorithm that can be used for any type of grouping. Some examples of __use cases__ are:\n- Behavioral Segmentation\n- Inventory Categorization\n- Sorting Sensor measurements\n- Detecting bots and anomalies\n- Computer Vision\n- Astronomy"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section606\"></a>\n### 6.6 Algorithm"},{"metadata":{},"cell_type":"markdown","source":"To start with k-means algorithm, you first have to randomly initialize points called the cluster centroids (K).<br>\nK-means is an __iterative algorithm__ and it does __two__ steps:<br>\n"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section60601\"></a>\n#### 6.6.1 Cluster Assignment\nThe algorithm goes through each of the data points and depending on which cluster is closer, It assigns the data points to one of the three cluster centroids.![image.png](https://raw.githubusercontent.com/insaid2018/Term-3/master/Images/clustering%204.png)"},{"metadata":{},"cell_type":"markdown","source":"![image.png](https://cdn-images-1.medium.com/max/800/1*4LOxZL6bFl3rXlr2uCiKlQ.gif)"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section60602\"></a>\n#### 6.6.2 Move Centroid\nHere, K-means moves the centroids to the average of the points in a cluster. In other words, the algorithm calculates the average of all the points in a cluster and moves the centroid to that average location.\n\nThis process is repeated until there is no change in the clusters (or possibly until some other stopping condition is met). K is chosen randomly or by giving specific initial starting points by the user.\n\n![image.png](https://raw.githubusercontent.com/insaid2018/Term-3/master/Images/clustering%205.png)"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section607\"></a>\n### 6.7 Choosing K"},{"metadata":{},"cell_type":"markdown","source":"One of the metrics that is commonly used to compare results across different values of 'K' is the __mean distance between data points and their cluster centroid__. \n- Since _increasing the number of clusters will always reduce the distance to data points, increasing K will always decrease this metric, to the extreme of reaching zero when K is the same as the number of data points_. \nThus, this metric cannot be used as the sole target. \nInstead, mean distance to the centroid as a function of K is plotted and the __\"elbow point,\"__ where the __rate of decrease sharply shifts__, can be used to roughly __determine K__.\n\nA number of other techniques exist for validating K, including __cross-validation__, __information criteria__, the __information theoretic jump method__, the __silhouette method__, and the __G-means algorithm__. In addition, monitoring the distribution of data points across groups provides __insight__ into how the _algorithm is splitting the data for each K_.![image.png](https://raw.githubusercontent.com/insaid2018/Term-3/master/Images/elbow.png)"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section7\"></a>\n## 7.  K-means Cluster Creation\n\n- Now it is time to create the Cluster labels!"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Storing the target variable\ny=college_data['Private']\n\ncollege_data.drop(['Private'],inplace=True,axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"college_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import KMeans from SciKit Learn.\nfrom sklearn.cluster import KMeans\n\n# Create an instance of a K Means model with 2 clusters\nkmeans=KMeans(n_clusters=2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section701\"></a>\n### 7.1 Normalize data\n\n- Normalize the data with MinMax scaling provided by sklearn"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import preprocessing\n# Scaling the data\nminmax_processed = preprocessing.MinMaxScaler().fit_transform(college_data)\ncollege_data.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_numeric_scaled = pd.DataFrame(minmax_processed, index=college_data.index, columns=college_data.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_numeric_scaled.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fit the model to all the data except for the Private label.\nkmeans.fit(df_numeric_scaled)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# What are the cluster center vectors?\nkmeans.cluster_centers_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section702\"></a>\n### 7.2 Evaluation"},{"metadata":{},"cell_type":"markdown","source":"There is no perfect way to evaluate clustering if you don't have the labels, however since this is just an exercise, we do __have the labels__, so we take advantage of this to __evaluate our clusters__, keep in mind, you usually won't have this luxury in the real world."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section703\"></a>\n### 7.3 Create a confusion matrix \n\nCreate a confusion matrix and classification report to see how well the Kmeans clustering worked without being given any labels.\n\nA confusion matrix is a summary of prediction results on a classification problem.\n\nThe number of correct and incorrect predictions are summarized with count values and broken down by each class.\nBelow is a diagram showing a general confusion matrix."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(confusion_matrix(y,kmeans.labels_))\n# print(classification_report(y,kmeans.labels_))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section704\"></a>\n### 7.4 Question: What about the K Value?\n\nWhile creating cluster we already know about the number of labels in the target variable so we go for __k=2__. But what method to follow for unlabled data which is the practical use case for this K-means Clustering Algorithm."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section705\"></a>\n### 7.5 Choosing appropriate value of k"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's fit cluster size 1 to 20 on our data and take a look at the corresponding score value.\nNc = range(1, 20)\nkmeans = [KMeans(n_clusters=i) for i in Nc]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score = [kmeans[i].fit(df_numeric_scaled).score(df_numeric_scaled) for i in range(len(kmeans))]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- These score values signify how far our observations are from the cluster center. We want to keep this score value around 0. A large positive or a large negative value would indicate that the cluster center is far from the observations.\n\n- Based on these scores value, we plot an Elbow curve to decide which cluster size is optimal. Note that we are dealing with tradeoff between cluster size(hence the computation required) and the relative accuracy."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(Nc,score)\nplt.xlabel('Number of Clusters')\nplt.ylabel('Score')\nplt.title('Elbow Curve')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our Elbow point is around cluster size of 2. Which we already know but here we can practically see how to choose appropriate value of K."},{"metadata":{},"cell_type":"markdown","source":"![](https://raw.githubusercontent.com/insaid2018/Term-3/master/Images/elbow%202.png)"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section706\"></a>\n### 7.6  Clustering Visualization\n\nLet's try to understand the clustering concept using visualization of data points.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section70601\"></a>\n#### 7.6.1 Dimension Reduction using PCA"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Here we compress the data to two dimension\npca=PCA(n_components=2)\nprincipalComponents = pca.fit_transform(df_numeric_scaled)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"principalComponents","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot the explained variances\n# features = range(pca.n_components_)\n# plt.bar(features, pca.explained_variance_ratio_, color='black')\n# plt.xlabel('PCA features')\n# plt.ylabel('variance %')\n# plt.xticks(features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Save components to a DataFrame\nPCA_components = pd.DataFrame(principalComponents)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"PCA_components.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"k_means2=KMeans(n_clusters=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Computer cluster centers and predict cluster indices \nX_clustered=k_means2.fit_predict(PCA_components)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define your own color map\nLabel_color_map={0:'r',1:'g'}\nlabel_color=[Label_color_map[i] for i in X_clustered]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot the scatter diagram\nplt.figure(figsize=(7,5))\nplt.scatter(principalComponents[:,0],principalComponents[:,1],c=label_color,alpha=0.5)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"```Plot Observation```\n- From the plot you can directly observe the data set in two different color. Each color represent the clusters.\n\n<img src=\"https://raw.githubusercontent.com/insaid2018/Term-3/master/Images/cluster2.png\" height=500 width=500/>"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section707\"></a>\n### 7.7 Centroid Visualization\n\nAs the centroid for clustering is movable but one we get the final clusters, we also get our final centroid for each clusters.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot the centroid\ncenter=k_means2.cluster_centers_\n\nplt.scatter(principalComponents[:,0],principalComponents[:,1],c=label_color,alpha=0.5)\nplt.scatter(center[:, 0], center[:, 1], c='blue', s=300, alpha=0.9,label = 'Centroids')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The ```Big-Blue-Dots``` represents the centroid of each clusters."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section8\"></a>\n## 8. Conclusion"},{"metadata":{},"cell_type":"markdown","source":"- From the K-means clustering we observe how to cluster our data points and here we already have labels for out data points so we can practically observe the clustering of data points.\n- The result of clustering is not much cherishing because here we are dealing with small data set. If the data is large enough then possibly the result improve.\n- But to improve your ML model further you need to try out with different machine learning algorithms like \n  - **LogisticRegression**\n  -  **SVM**\n  - **Random Forest**\n  - **Ensemble Learning Algorithms** \n  - **Artificial Neural Networks(ANNs)**\n  \n  - etc..\n  \n"},{"metadata":{},"cell_type":"markdown","source":"I am super excited to share my kernel with the Kaggle community. I prepared this kernal to made easy to understand the concept of clustering for every beginner and will continue to explain other algorithms in a very naive ways as I go on in this journey and learn new topics, I will incorporate them with each new updates. So, check for them and please leave a comment if you have any suggestions to make them better!! \n\n\n<br>\n<br>\n\n\n\n<center><h3>If you learn something out of it.. Leave your appreciation by simply hit upvote..</h3></center>"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}