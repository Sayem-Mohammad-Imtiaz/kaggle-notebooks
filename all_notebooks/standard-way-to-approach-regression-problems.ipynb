{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('../input/concrete-compressive-strength/Concrete Compressive Strength.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Looks like theres n null values present in the dataset , now to check for outliers","metadata":{}},{"cell_type":"code","source":"for column in df.columns:\n    plt.figure()\n    sns.distplot(df[column])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for column in df.columns:\n    plt.figure()\n    sns.boxplot(df[column])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There looks like theres a few outliers in the dataset ,however there's no need to worry about them as DecisionTreeRegressor is not influenced by outliers.","metadata":{}},{"cell_type":"code","source":"import plotly.graph_objects as go\ncorr = df.corr()\ngraph = go.Figure()\ngraph.add_trace(go.Heatmap(z=corr.values, x=corr.index.values, y=corr.columns.values))\ngraph.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now to check for imbalence in the dataset\n1. https://pypi.org/project/smogn/","metadata":{}},{"cell_type":"code","source":"!pip install smogn","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import smogn\nconcrete_smogn = smogn.smoter(\n    data = df,       \n    y = 'Concrete compressive strength(MPa, megapascals) '  \n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.kdeplot(df['Concrete compressive strength(MPa, megapascals) '], label = \"Original\")\nsns.kdeplot(concrete_smogn['Concrete compressive strength(MPa, megapascals) '], label = \"Modified\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Above step is not compulsory for regression , however the accuracy of classification problems increases by using SMOTE","metadata":{}},{"cell_type":"code","source":"#splitting the data\nX = df.iloc[:, :-1]\ny = df.iloc[:, -1]\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since we are using a decision tree model , we don't need to use StandardScaler","metadata":{}},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor\ndtr = DecisionTreeRegressor()\ndtr.fit(X_train, y_train)\ndtr.score(X_test, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dtr1 = DecisionTreeRegressor()\ndtr1.fit(X_train, y_train)\ndtr1.score(X_test, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see there's no major improvement in the performance of the model, now to go for hyperparameter optimisation","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\ngrid_params = {\n    'criterion' : [ 'mae' ,'mse', 'friedman_mse','poisson'],\n    'splitter' : ['best', 'random'],\n    'max_depth' : [3, 5, 7, 9, 10,12,],\n    'min_samples_split' : [ 2, 3, 4, 5,7],\n    'min_samples_leaf' : [ 2, 3, 4, 5,7]\n}\n\ngrid_search = GridSearchCV(dtr1, grid_params, cv = 5, n_jobs = -1, verbose = 1)\ngrid_search.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(grid_search.best_params_)\nprint(grid_search.best_score_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"An alternative approach to select a base model is to use the lazyregressor from lazypredict \n1. https://pypi.org/project/lazypredict/","metadata":{}},{"cell_type":"code","source":"!pip install lazypredict","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After installing lazypredict you need to restart your kernel otherwise it will not work","metadata":{}},{"cell_type":"code","source":"from lazypredict.Supervised import LazyRegressor\nreg = LazyRegressor(ignore_warnings=False, custom_metric=None)\nmodels, predictions = reg.fit(X_train, X_test, y_train, y_test)\nmodels","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As the XGB regressor has the highest Adjusted R-Squared and R - Squared metric","metadata":{}},{"cell_type":"code","source":"from xgboost import XGBRegressor\nxgbr = XGBRegressor(verbosity=0) \nxgbr.fit(X_train,y_train)\n#bellow is the Adjusted R-Squared for the model\n1 - (1-xgbr.score(X_test, y_test))*(len(y_test)-1)/(len(y_test)-X_test.shape[1]-1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here is a basic approach to regression problems","metadata":{}}]}