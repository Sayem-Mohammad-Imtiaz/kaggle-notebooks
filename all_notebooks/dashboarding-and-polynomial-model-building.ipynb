{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Library and Data Imports","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats.kde import gaussian_kde\nfrom scipy.stats import norm\nfrom scipy.stats import spearmanr\nfrom scipy.stats import kendalltau\nfrom scipy.stats import pearsonr\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\n\n\ndf = pd.read_csv('/kaggle/input/the-economic-freedom-index/economic_freedom_index2019_data.csv')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Glance at Data and Preprocessing","metadata":{}},{"cell_type":"code","source":"# glance at data\nprint(df.head())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df.info())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It seems like there is a minor problem with null values. After doing some research on Economic Freedom Index Website, it seems like some countries all nearly completely null--i.e. Iraq, Syria. This is confirmed by visualizing the pattern of null values with Seaborn. ","metadata":{}},{"cell_type":"code","source":" sns.heatmap(df.isnull(), cbar=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Because of the clustering of the null values, a intuitive and straightforward approach is to simply delete all the rows that contain nulls. ","metadata":{}},{"cell_type":"code","source":"df = df.dropna(axis=0)\nsns.heatmap(df.isnull(), cbar=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# it looks like some columns that should be floats have object dtype\n# also some columns have extraneous symbols like $ signs and commans\n# this code removes those symbols and converts to floats\n\ncolumnsToChange = ['FDI Inflow (Millions)', 'GDP per Capita (PPP)', 'GDP (Billions, PPP)', 'Unemployment (%)', 'Population (Millions)']\nfor column in columnsToChange:\n  data = df[column]\n  edited = []\n  for row in data:\n    noComma = row.replace(',', '')\n    noDollar = noComma.replace('$', '')\n    edited.append(noDollar)\n  df[column] = [x for x in edited]\n  df[column] = df[column].astype(float)\n    \nprint(df.info())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Some of the columns have spelling errors or are hard to access. This code renames these columns. ","metadata":{}},{"cell_type":"code","source":"df.rename(columns={'Country Name': 'CountryName', \n                   'Judical Effectiveness': 'Judicial Effectiveness', \n                   'Gov\\'t Spending': 'Gov Spending', \n                   'Gov\\'t Expenditure % of GDP ': 'Gov Expenditure % of GDP',\n                    'Investment Freedom ': 'Investment Freedom'}, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After visting the Economic Freedom Index website again, I learned more about the structure of the dataset. Each country's '2019 Score' is simply the average of 12 component columns, each scored between 0 and 100. These components or aspects of freedom include Judicial Effectiveness, Fiscal Health, and Monetary Freedom, as well as others. Each country is also classified as follows: \"Free\" if total score is between 80 and 100, \"Mostly Free\" if the total score is between 70 and 79, \"Moderately Free\" if the total score is between 60 and 69, \"Mostly Unfree\" if total score is between 50 and 59, and \"Repressed\" if total score is less than 49.\n\nThe dataset also includes other economic statistics about each country, like GDP per Capita, Populaiton, and Unemployment. \n\nThis code helps organize the dataset by storing similar groups of columns into lists for later access. ","metadata":{}},{"cell_type":"code","source":"# the twelve components of economic freedom\nRANKS = ['Property Rights', 'Judicial Effectiveness', 'Government Integrity', 'Tax Burden', \n              'Gov Spending', 'Fiscal Health', 'Business Freedom', \n              'Labor Freedom', 'Monetary Freedom', 'Trade Freedom', \n              'Investment Freedom', 'Financial Freedom']\n\n# earlier list plus 2019 score column\nRANKS_PLUS_TOTAL = ['2019 Score'] + RANKS\n\n# columns with other statistics for each country calculated as a percentage\nPERCENTAGE_STATS = ['Tariff Rate (%)', 'Income Tax Rate (%)', 'Corporate Tax Rate (%)', \n                     'Tax Burden % of GDP', 'Gov Expenditure % of GDP', 'GDP Growth Rate (%)', '5 Year GDP Growth Rate (%)',\n                     'Unemployment (%)', 'Inflation (%)', 'Public Debt (% of GDP)', 'GDP per Capita (PPP)']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"markdown","source":"Since this dataset is documented at length at the Economic Freedom Index website, there is not a lot of EDA needed. However, here are some visualizations that I found insightful. ","metadata":{}},{"cell_type":"code","source":"# returns color that matches color code found on website for scores\ndef classifier(item):\n  if item > 80:\n    return \"darkgreen\"\n  elif item > 70:\n    return 'limegreen'\n  elif item > 60:\n    return \"yellow\"\n  elif item > 50:\n    return 'orange'\n  else:\n    return 'red'\n\n# function for making boxplots\ndef boxPlots(df):\n  fig, ax = plt.subplots(1, 1, figsize=(14, 8))\n  lst = [df[column] for column in df.columns]\n  medianPropDict = dict(color='black', linewidth=1.5)\n  bplot = ax.boxplot(lst, vert=True, showfliers=False, positions=range(1, len(df.columns)+1), patch_artist=True, medianprops=medianPropDict)\n  ax.set_xticks(range(1, len(df.columns)+1))\n  ax.set_xticklabels(df.columns, rotation=45)\n  ax.set_xlabel(\"Categories\")\n  ax.set_ylabel(\"Score\")\n  ax.set_title('World Scores by Category')\n  \n  colors = []\n  for column in df.columns:\n    data = df[column]\n    median = data.median()\n    colors.append(classifier(median))\n  \n  for patch, color in zip(bplot['boxes'], colors):\n    patch.set_facecolor(color)\n    \n# calls function\nboxPlots(df[RANKS_PLUS_TOTAL])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This boxplot figure provides an excellent visualization of the spread and central tendancy for the total score column and the 12 component columns. Note that the colors are aligned with the theme of the website--dark green for \"Free\" or over 80, light green for \"Mostly Free\" and over 70, and so forth. The median value for each boxplot is what determines its color classification. \n\nAs an additional note, it is worth pointing out that the total score boxplot is generally less spread out because it contains averages of the other columns. ","metadata":{}},{"cell_type":"code","source":"# produces correlation matrix heatmap with only half included--omits redundant symmetrical half and diagnol 1 values\ndef correlationMatrix(df):\n  corr = df.corr()\n  mask = np.triu(np.ones_like(corr, dtype=bool))\n  f, ax = plt.subplots(figsize=(11, 9))\n  cmap = sns.diverging_palette(20, 230, as_cmap=True)\n  sns.heatmap(corr, mask=mask, cmap=cmap, vmax=1, vmin=-1, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n  plt.title(\"Correlation Matrix\")\n\n\n# calls function with 12 component columns\ncorrelationMatrix(df[RANKS])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It is no surprise that most of the 12 component columns are positively correlated. It makes intuitive sense that a country with high Government Integrity also has high Judicial Effectiveness, for instance. \n\nWhat is noteworthy, though, are the two columns that seem to be slightly negatively correlated with the rest--Tax Burden and Government Spending. This makes sense too, though, when you think about it. Many \"left leaning\" countries are ecnomically free in many respects but also have larger governments and higher taxes.","metadata":{}},{"cell_type":"code","source":"# calls correlation matrix with percentage stats and 2019 score\nlst = PERCENTAGE_STATS + ['2019 Score']\ncorrelationMatrix(df[lst])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The correlations are generally much more mild, of course, but there are still some interesting points to note. \n\nFirst, a couple of the strong positive correlations aren't that interesting. For example, GDP Growth Rate and 5 Year GDP Growth Rate. Of course these are positively correlated, so not much to see there. Also, the strong positive correlation between Tax Burden and Gov Expenditure isn't that interesting, either. \n\nThe two points that are most noteworthy about this graph is the strong negative correlation between 2019 score and Tariff Rate as well as the strong positive correlation between 2019 Score and GDP per Capita. \n\nBoth of these correlations are interesting because of how dramatic they are. For the negative correlation between 2019 Score and Tariffs, this correlation is more unexpected. Maybe this is because American Conservatives, traditionally proponents of ecnomic freedom, have started supporting Tariffs, but I assumed before analyzing this data that Tariffs have a relatively smaller impact on economic freedom. That is clearly not the case, however. \n\nAs for the positive correlation between 2019 Score and GDP per Capita, that is less surprising. In the report on the website, the Heritage Foundation discusses at length the connection between the total score and GDP per Capita. We will discuss this relationship more later. But for now, it's interesting to see the connection showing up already. ","metadata":{}},{"cell_type":"markdown","source":"# Dashboarding","metadata":{}},{"cell_type":"markdown","source":"To help visualize any given country's Econnomic Freedom status, I built a small dashboard-like arrangement of bar plots. Again, the colors follow from the color code in the economic freedom index website. I quite like how it turned out. ","metadata":{}},{"cell_type":"code","source":"def countryDashboard(countryName):\n  countryData = df.loc[df.CountryName == countryName]\n  \n\n  fig = plt.figure(constrained_layout=True, figsize=(15, 9))\n  gs = fig.add_gridspec(ncols=9, nrows=3)\n\n  f_ax1 = fig.add_subplot(gs[:-1, 1:])\n  string = countryName + ' Average and Category Scores'\n  f_ax1.set_title(string, fontsize=16)\n  data = countryData[RANKS]\n  for idx, column in enumerate(data.columns):\n    point = data[column].iloc[0]\n    color = classifier(point)\n    plt.bar(idx, height=point, color=color, width=0.7)\n    plt.text(idx-0.25, point+2,point, fontsize=15)\n  numColumns = range(len(data.columns))\n\n\n  f_ax1.set_yticks(range(0, 110, 10))\n  f_ax1.set_yticklabels([])\n  f_ax1.set_xticks(numColumns)\n  f_ax1.set_xticklabels(data.columns, rotation=45)\n  \n\n  f_ax2 = fig.add_subplot(gs[:-1, 0])\n  totalScore = countryData['2019 Score'].iloc[0]\n  color = classifier(totalScore)\n  f_ax2.set_ylabel(\"Score\")\n  f_ax2.bar(1, height=totalScore, color=color, width=1.5)\n  f_ax2.text(1-0.25, totalScore+2, totalScore, fontsize=15)\n  f_ax2.set_xticks([1])\n  f_ax2.set_xticklabels([\"Average Score\"], rotation=45)\n  f_ax2.set_yticks(range(0, 110, 10))\n  f_ax2.set_yticklabels(range(0, 110, 10))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# calls country dashboard function for United States\ncountryDashboard(\"United States\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Not surprisingly, the worst scores of the U.S. were Gov Spending and Fiscal Health. And the fact that the best score for the U.S. is Labor Freedom doesn't surprise me either--I remember reading about how easy it is for companies in the U.S. to fire employees for my intro ECON class last semester. ","metadata":{}},{"cell_type":"code","source":"# dashboard for Venezuela\ncountryDashboard(\"Venezuela\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There's not much to say here. The obviously disastrous situation in Venezuela is clearly reflected in these scores. ","metadata":{}},{"cell_type":"code","source":"# dashboard for China\ncountryDashboard(\"China\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#dashboard for Australia \ncountryDashboard(\"Australia\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#dashboard for France\ncountryDashboard(\"France\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Notice the disastrous Tax Burden and Government Spending--and how they are paired with excellent scores in many of the other categories and a decent overall score. This kind of score structure is why Tax Burden and Gov Spending are negatively correlated with many of the categories, as revealed by the Correlation Matrix. ","metadata":{}},{"cell_type":"markdown","source":"# Model Building and Verification","metadata":{}},{"cell_type":"markdown","source":"The report for the 2019 Economic Freedom Index claims that the total score and GDP per Capita have a correlation coefficient of 0.64. Furthermore, it also claims that the best fitting trendline is an exponential one. In the following sections, I'll test these claims by conducting my own correlation calculations and fitting my own model. ","metadata":{}},{"cell_type":"code","source":"# correlation coefficients for various correlation calculations\n\nx = df['2019 Score']\ny = df['GDP per Capita (PPP)']\n\npearson, _ = pearsonr(x, y)\nspearman, _ = spearmanr(x, y)\nkendall, _ = kendalltau(x, y)\n\nprint(\"Pearson: \", round(pearson, 3))\nprint(\"Spearman: \", round(spearman, 3))\nprint(\"Kendall: \", round(kendall, 3))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"These results seem consistent with the website's claims. Pearson's correlation coefficient is the closest to the claim of 0.64, but it also measures linear correlation--and if the relationship is exponential rather than linear, this score might not be accurate. Spearman's coefficient and Kendall's coefficient supposedly hold for non-linear relationships, but I don't know enough about how these coefficients are calculated to opine on their relative merits.","metadata":{}},{"cell_type":"code","source":"# calls the sub functions\ndef scatter_plot_trend(x, y, degrees):\n    # scatter plot\n    plt.figure(figsize=(9, 7))\n    plt.scatter(x, y)\n\n    # calls polyFit function to get fitted terms\n    terms = getFittedTerms(x, y, degrees)\n\n    # smooth, continuous predictions for graphing purposes\n    smoothInput = range(0, 100, 1)\n    smoothPredictions = predict(smoothInput, terms)\n\n    #plots predictions for polynomial\n    label = \"Fitted \" + str(degrees) + ' degree Polynomial'\n    poly = plt.plot(smoothInput, smoothPredictions, color='black', label=label)\n    plt.legend()\n    plt.xlim(20, 100)\n    plt.ylim(-10000, 140000)\n    plt.xlabel('Score')\n    plt.ylabel(\"GDP per Capita\")\n    plt.title(\"Score vs GDP per Capita w/ Fitted Polynomial\")\n\n    predictions = predict(df['2019 Score'], terms)\n\n\n    RSQ = coefficientOfDetermination(np.array(df['GDP per Capita (PPP)']), np.array(predictions))\n    print(\"R-Squared Score: \", round(RSQ, 3))\n\n\n\n# produces predicted values for input x values\n# abstract enough to work for any degree polynomial\ndef predict(x, terms):\n    numberOfTerms = len(terms)\n    degrees = numberOfTerms - 1\n    predictions = []\n\n    for point in x:\n      prediction = 0\n      for i in range(len(terms)):\n        exponent = numberOfTerms - i - 1\n        if i < degrees:\n          prediction += terms[i] * (point ** exponent)\n        else:\n          prediction += terms[-1]\n          predictions.append(prediction)\n\n    return predictions\n\n  \n# extracts fitted coefficients and constant for any degree polynomial\ndef getFittedTerms(x, y, degrees):\n    x = np.array(x)\n    y = np.array(y)\n\n    fitted = np.polyfit(x, y, degrees)\n    terms = list(fitted)\n\n    return terms\n\n# calculates and returns R-Squared score for given lists of actual and predictions\ndef coefficientOfDetermination(actual, predictions):\n    squaredError = [np.square(actual[i] - predictions[i]) for i in range(len(actual))]\n    explainedVariance = np.sum(squaredError) / len(actual)\n\n    mean = np.mean(actual)\n    squaredError = [np.square(actual[i] - mean) for i in range(len(actual))]\n    totalVariance = np.sum(squaredError) / len(actual)\n\n    return 1 - (explainedVariance / totalVariance) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's now try fitting the 2019 Score and GDP per Capita with different degree polynomials and see which scores best.","metadata":{}},{"cell_type":"code","source":"# produces fitted 1 degree polynomail--i.e. a fitted linear line\nscatter_plot_trend(df['2019 Score'], df['GDP per Capita (PPP)'], 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# produces fitted 2 degree polynomail--a quadratic or exponential line\nscatter_plot_trend(df['2019 Score'], df['GDP per Capita (PPP)'], 2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Note that the R-Squared score improved with the 2 degree polynomial. This might lead us to conclude that that line is the better fit. This would be an interesting insight--it would mean that the GDP per Capita for a country is expected to increase exponentially with increases in econonomic freedom, not merely linearly. However, we should not be jump to this conclusion just yet. To illustrate my point, let's fit other degree polynomials to the points.  ","metadata":{}},{"cell_type":"code","source":"# fits 3 degree poly to points\nscatter_plot_trend(df['2019 Score'], df['GDP per Capita (PPP)'], 3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Oh wow, the R-Score improved again? Does that mean that this is a better model for the point? No. Check out what happens when I fit a 40 degree polynomial to the points. ","metadata":{}},{"cell_type":"code","source":"# fits 40 degree polynomial to the points\nscatter_plot_trend(df['2019 Score'], df['GDP per Capita (PPP)'], 40)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The R-Square score improved, but intuitively speaking, it does not make sense for the trend to be modelled by such a curve. Even ignoring the extreme fluctuations away from the main points, it is still implausible. Why the high R-Square score, then? What is happening is that the model is overfitting the data. This model might have a high score, but it wouldn't be very good at predicting additional points. It has tuned itself too narrower to these specific points to accurately predict other ones. \n\nNormally, this would be handled by splitting the data into a training set and a test set. The training set would be used to train the model, while the model would be scored from the test set--from points it hasn't seen yet. Let's now try that and see which model comes out ahead. ","metadata":{}},{"cell_type":"code","source":"# re-writing function to split data into training and test sets\ndef scatter_plot_trend(x, y, degrees):\n    # scatter plot\n    plt.figure(figsize=(9, 7))\n    plt.scatter(x, y)\n\n    # splits data into training and test sets\n    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=3)\n\n\n    # calls polyFit function to get fitted terms\n    # uses training data to fit model\n    terms = getFittedTerms(x_train, y_train, degrees)\n\n    # smooth, continuous predictions for graphing purposes\n    smoothInput = range(0, 100, 1)\n    smoothPredictions = predict(smoothInput, terms)\n\n    #plots predictions for polynomial\n    label = \"Fitted \" + str(degrees) + ' degree Polynomial'\n    poly = plt.plot(smoothInput, smoothPredictions, color='black', label=label)\n    plt.legend()\n    plt.xlim(20, 100)\n    plt.ylim(0, 140000)\n    plt.xlabel('Score')\n    plt.ylabel(\"GDP per Capita\")\n    plt.title(\"Score vs GDP per Capita w/ Fitted Polynomial\")\n    \n    # makes predictions from x_test\n    predictions = predict(x_test, terms)\n\n    # scores from y_test\n    RSQ = coefficientOfDetermination(np.array(y_test), np.array(predictions))\n    print(\"R-Squared Score: \", round(RSQ, 3))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scatter_plot_trend(df['2019 Score'], df['GDP per Capita (PPP)'], 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scatter_plot_trend(df['2019 Score'], df['GDP per Capita (PPP)'], 2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scatter_plot_trend(df['2019 Score'], df['GDP per Capita (PPP)'], 3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scatter_plot_trend(df['2019 Score'], df['GDP per Capita (PPP)'], 40)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I am not actually sure how the R-Squared Score is ending up beyond negative 1. \n\nIn any case, it looks like the 2nd degree polynomial is the best. Even though I have controlled the randomness of train test split with a random seed so the splitting is consistent for all polys, the random splitting might be better for some models over others. In order to test that the 2nd degree polynomial is better, I'll take the average R-Square score after calculating it many times with different random states using a while loop and compare it to the averages with other degree polys. ","metadata":{}},{"cell_type":"code","source":"# condensing scatter_plot_trend function to not plot anything\ndef score(x, y, degrees):\n    # splits data\n    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n    \n    # calls polyFit function to get fitted terms\n    # uses training data to fit model\n    terms = getFittedTerms(x_train, y_train, degrees)\n    \n    # makes predictions on x test\n    predictions = predict(x_test, terms)\n\n    # scores from y_test, returns\n    return coefficientOfDetermination(np.array(y_test), np.array(predictions))\n\nx = df['2019 Score']\ny = df['GDP per Capita (PPP)']\n\n# lists to store scores for each iteration\ndegree1 = []\ndegree2 = []\ndegree3 = []\n\n# sets up while loop\nnum_iterations = 1000\ncounter = 0\nwhile counter < num_iterations:\n    counter += 1\n    \n    # appends score for each deg poly to respective list\n    degree1.append(score(x, y, 1))\n    degree2.append(score(x, y, 2))\n    degree3.append(score(x, y, 3))\n    \n\n# finds averages\ndegree1AVG = np.sum(degree1) / len(degree1)\ndegree2AVG = np.sum(degree2) / len(degree2)\ndegree3AVG = np.sum(degree3) / len(degree3)\n\n# ouputs results in formatted way\nprint(\"1-Degree Model AVG: \", round(degree1AVG, 3))\nprint(\"2-Degree Model AVG: \", round(degree2AVG, 3))\nprint(\"3-Degree Model AVG: \", round(degree3AVG, 3))\n\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This seems like fairly conclusive evidence that the 2-degree model, after all, is the best. If we wanted to be even more certain, we could ramp up the number of iterations on the while loop, add more degree polynomials, or even conduct an hypothesis test on the comparison between these two sample means. I'll end my calculations here, though. ","metadata":{}},{"cell_type":"markdown","source":"# Concluding Thoughts","metadata":{}},{"cell_type":"markdown","source":"I'll conclude this notebook by going over the motivation behind these calculations again. In their report, the Economic Freedom Index website claimed that the correlation between GDP per Capita and their Economic Freedom Score is 0.64. This seems fairly accurate. Additionally, they claimed that the trend between these two variables can be modeled with an exponential function. Because the 2-Degree polynomial seems like the best fit from my calculations, I can agree with that claim as well. \n\nThis fact is significant because of the exponentiality here. It suggests that countries with increases in their economic freedom score can expect exponentially higher increases in their GDP per Capita. We must be careful, though--none of this has proven casaulity. My guess is that it is highly likely that confounding variables are at play. The website hasn't exactly been correct about this point--they suggest that economic freedom is causing rises in GDP per Capita, as well as other things, like social progress and environmental health. From an intuitive point of view, I would suggest that social progress is more likely causing economic freedom. ","metadata":{}},{"cell_type":"markdown","source":"Thanks for reading my notebook!","metadata":{}}]}