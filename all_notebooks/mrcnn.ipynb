{"cells":[{"metadata":{"_uuid":"47dc5fe9-8bc8-4d5c-b779-72ddff20b2b8","_cell_guid":"383ecbe4-b450-4d3a-80b4-41314ccb60dc","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"98abc4be-b130-434b-9f1f-8fde131d34dd","_cell_guid":"5833dce0-66af-4029-8e6f-95c6cb26c8ec","trusted":true},"cell_type":"code","source":"!pip uninstall -y tensorflow","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"71699216-26a9-440c-a201-f12d65eb1833","_cell_guid":"d613de33-fba7-40d4-a0c7-d2f695015a97","trusted":true},"cell_type":"code","source":"!pip install tensorflow-gpu==1.14","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d9fd9261-8736-4c0e-bfd6-db789fe07e6f","_cell_guid":"b0ae4e79-3f22-4317-a181-dc69b4ac32e3","trusted":true},"cell_type":"code","source":"import tensorflow as tf","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d7399b6e-96ab-41a4-a1c5-f55d0dd56646","_cell_guid":"74100c70-e984-4ec8-a30f-2512f20fc47d","trusted":true},"cell_type":"code","source":"tf.test.is_gpu_available()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9a216011-c7ea-4a63-ab6c-82ddb9e465b7","_cell_guid":"cb308345-8828-4cd4-84a1-e409889154a7","trusted":true},"cell_type":"code","source":"print(tf.__version__)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"56ba12c6-cbbb-446a-a9ee-f6a7ace1450d","_cell_guid":"11657426-2f07-4116-988d-065e022aef0a","trusted":true},"cell_type":"code","source":"\n\nimport os, cv2, re, random\nimport numpy as np\nimport pandas as pd\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.preprocessing.image import img_to_array, load_img\nfrom keras import layers, models, optimizers\nfrom keras import backend as K\nfrom sklearn.model_selection import train_test_split\nfrom keras.utils.np_utils import to_categorical\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, AvgPool2D, BatchNormalization, Reshape\nfrom keras.callbacks import LearningRateScheduler\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"beae637f-ea2b-4f49-a39f-f05a04e30575","_cell_guid":"1fa7c8b0-4fda-4fc6-a405-4c70937d65b0","trusted":true},"cell_type":"code","source":"tf.test.is_gpu_available()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0aa1c556-8736-4af0-aa19-9443449c12ab","_cell_guid":"d8a02eaf-7662-4430-98e1-df73cc9a2d0a","trusted":true},"cell_type":"code","source":"train=pd.read_csv('/kaggle/input/face-mask-detection-dataset/train.csv')\nprint(train.head)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e7240c8a-4174-4445-9951-b2908ba268aa","_cell_guid":"af60b0b6-930f-4f58-9142-34898c49fd23","trusted":true},"cell_type":"code","source":"train.sort_values(by='name', ascending=True)\ntrain.isnull().any().value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"966ff813-afa7-43b1-bf7d-b7deb7a72a88","_cell_guid":"13a55679-a64d-4a45-a28b-1c6a8359b265","trusted":true},"cell_type":"code","source":"DIR='/kaggle/input/face-mask-detection-dataset/Medical mask/Medical mask/Medical Mask/images/'\nimages = [DIR+i for i in os.listdir(DIR)]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3dbdbdaa-56a0-4aef-a8ef-09c5bee137b1","_cell_guid":"13b20e2f-e93a-4414-bb3e-f68e0b432d09","trusted":true},"cell_type":"code","source":"def atoi(text):\n    return int(text) if text.isdigit() else text\n\ndef natural_keys(text):\n    return [ atoi(c) for c in re.split('(\\d+)', text) ]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9a5e4509-60d0-4f17-b6c8-128e9f210ad1","_cell_guid":"d7c1388b-8763-430b-b68b-20c0c1af64a4","trusted":true},"cell_type":"code","source":"images.sort(key=natural_keys)\ntrain_images = images[1698:] \ntest_images= images[0:1698]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b2d5bb74-ac60-413a-b790-de07dc387721","_cell_guid":"11195071-c069-4f1c-a4f6-292f06026519","trusted":true},"cell_type":"code","source":"print(train_images)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"929b00e1-1404-4f88-9a6f-f966f03a86ce","_cell_guid":"287f9a2a-9666-481f-88ea-b65c49eed351","trusted":true},"cell_type":"code","source":"from matplotlib.image import imread\n\nfolder = '/kaggle/input/face-mask-detection-dataset/Medical mask/Medical mask/Medical Mask/images/'\n# plot first few images\nfor i in range(2):\n\t# define subplot\n\tplt.subplot(330 + 1 + i)\n\t# define filename\n\tfilename = folder +'200' + str(i) + '.jpg'\n\t#print(filename)\n\timage = imread(filename)\n\tprint(image.shape)\n\tplt.imshow(image)\n# show the figure\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d74a6380-b590-433d-99be-5358f566f4fd","_cell_guid":"f2e0f121-4741-400c-8a7f-ee286da1ce28","trusted":true},"cell_type":"code","source":"!git clone https://github.com/matterport/Mask_RCNN.git\nos.chdir('Mask_RCNN')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"53a4374e-20f9-4492-b0d2-da4bda70dd67","_cell_guid":"3d50f6b3-cae9-4e0f-ac6d-512847301d6c","trusted":true},"cell_type":"code","source":"import mrcnn\nfrom mrcnn.utils import Dataset\nfrom mrcnn.model import MaskRCNN\nfrom mrcnn.config import Config\nfrom mrcnn import utils\nimport mrcnn.model as modellib\nfrom mrcnn import visualize\nfrom mrcnn.model import log\nfrom mrcnn.visualize import display_instances\nfrom mrcnn.utils import extract_bboxes\n\nimport numpy as np\nfrom numpy import zeros\nfrom numpy import asarray\nimport random\nimport cv2\nimport os\nimport time\nfrom matplotlib import pyplot\nfrom matplotlib.patches import Rectangle\nfrom keras.models import load_model\n%matplotlib inline\nfrom os import listdir","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"29b3abd6-64d1-4ce3-883a-5b56c8c85661","_cell_guid":"5b157344-b958-408a-97e1-84b0dc97bce0","trusted":true},"cell_type":"code","source":"import json\n\nwith open('/kaggle/input/face-mask-detection-dataset/Medical mask/Medical mask/meta.json') as f:\n    data_classes = json.load(f)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cc4041dd-e2f8-4981-8a3b-447494eabf42","_cell_guid":"133d837a-e6ff-44b8-b3d7-61db9466d6c0","trusted":true},"cell_type":"code","source":"class DetectorConfig(Config):\n    \"\"\"Configuration for training pneumonia detection on the RSNA pneumonia dataset.\n    Overrides values in the base Config class.\n    \"\"\"\n    \n    # Give the configuration a recognizable name  \n    NAME = 'face_amsk'\n    \n    # Train on 1 GPU and 8 images per GPU. We can put multiple images on each\n    # GPU because the images are small. Batch size is 8 (GPUs * images/GPU).\n    GPU_COUNT = 1\n    IMAGES_PER_GPU = 1\n    \n    BACKBONE = 'resnet50'\n    LEARNING_RATE=0.01\n    NUM_CLASSES = 21\n    DETECTION_MIN_CONFIDENCE = 0.9\n\n    STEPS_PER_EPOCH = 100\n    \nconfig = DetectorConfig()\nconfig.display()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"461ae4a5-a17a-4166-9e08-7a11d8f10888","_cell_guid":"ba6906c1-02fc-4416-aa43-72178d1f0174","trusted":true},"cell_type":"code","source":"class Face_mask_Dataset(Dataset):\n    # load the dataset definitions\n    def load_dataset(self, dataset_dir, is_train=True):\n        \n        # Add classes. We have only one class to add.\n        for i in range(20):\n            self.add_class('dataset', i+1, data_classes[\"classes\"][i][\"title\"])\n        \n        # define data locations for images and annotations\n        images_dir = dataset_dir + '/images/'\n        annotations_dir = dataset_dir + '/annotations/'\n        \n        # Iterate through all files in the folder to \n        #add class, images and annotaions\n        for filename in listdir(images_dir):\n            \n            # extract image id\n            image_id = filename[:-4]\n            if(image_id[-1]=='.'):\n                image_id = image_id[:-1]\n            # skip all images before 1801 if we are building the train set\n            if is_train and int(image_id) <= 2800:\n                continue\n            # skip all images after 1800 if we are building the test set\n            if  int(image_id) < 1800 :\n                continue\n            if  not is_train and int(image_id) > 2800:\n                continue\n            # setting image file\n            img_path = images_dir + filename\n            \n            # setting annotations file\n            ann_path = annotations_dir + filename + '.json'\n            \n            # adding images and annotations to dataset\n            self.add_image('dataset', image_id=image_id, path=img_path, annotation=ann_path)\n# extract bounding boxes from an annotation file\n    def extract_boxes(self, filename):\n        \n        with open(filename) as f:\n            x = json.load(f)\n        boxes = list()\n        box_class = list()\n        for i in range(x[\"NumOfAnno\"]):\n            boxes.append(x[\"Annotations\"][i][\"BoundingBox\"])\n            box_class.append(x[\"Annotations\"][i][\"classname\"])\n        return boxes,box_class\n    \"\"\"Generate instance masks for an image.\n       Returns:\n        masks: A bool array of shape [height, width, instance count] with\n            one mask per instance.\n        class_ids: a 1D array of class IDs of the instance masks.\n     \"\"\"\n    def load_mask(self, image_id):\n        # get details of image\n        info = self.image_info[image_id]\n        \n        # define anntation  file location\n        path = info['annotation']\n        filename= info['path']\n        \n        boxes,boxes_class= self.extract_boxes(path)\n        image = imread(filename)\n        w=image.shape[1]\n        h=image.shape[0]\n        # create one array for all masks, each on a different channel\n        masks = zeros([h, w, len(boxes)], dtype='uint8')\n        \n        # create masks\n        class_ids = list()\n        for i in range(len(boxes)):\n            box = boxes[i]\n            row_s, row_e = box[1], box[3]\n            col_s, col_e = box[0], box[2]\n            masks[row_s:row_e, col_s:col_e, i] = 1\n            class_ids.append(self.class_names.index(boxes_class[i]))\n        return masks, asarray(class_ids, dtype='int32')\n    def image_reference(self, image_id):\n        info = self.image_info[image_id]\n        print(info)\n        return info['path']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0857ab80-0a7b-4f98-aa0c-eb5005da0fc7","_cell_guid":"0890473f-0835-4792-8f4a-3e714d2d56af","trusted":true},"cell_type":"code","source":"# prepare train set\ntrain_set = Face_mask_Dataset()\ntrain_set.load_dataset('/kaggle/input/face-mask-detection-dataset/Medical mask/Medical mask/Medical Mask', is_train=True)\ntrain_set.prepare()\nprint('Train: %d' % len(train_set.image_ids))\n# prepare test/val set\nval_set = Face_mask_Dataset()\nval_set.load_dataset('/kaggle/input/face-mask-detection-dataset/Medical mask/Medical mask/Medical Mask', is_train=False)\nval_set.prepare()\nprint('Test: %d' % len(val_set.image_ids))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2896bc8e-4b05-4672-858b-ff49adea6c36","_cell_guid":"cffa1b1f-140f-4c9f-b8e6-8ebfa2b2ede2","trusted":true},"cell_type":"code","source":"model = modellib.MaskRCNN(mode=\"training\", config=config, model_dir='./')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c6fbc3c4-609e-4f17-a0e9-ac72ec4516e3","_cell_guid":"10eca6f6-8611-45eb-ae44-505b5e351e0b","trusted":true},"cell_type":"code","source":"from imgaug import augmenters as iaa\naugmentation = iaa.SomeOf((0, 1), [\n    iaa.Fliplr(0.5),\n    iaa.Affine(\n        scale={\"x\": (0.8, 1.2), \"y\": (0.8, 1.2)},\n        translate_percent={\"x\": (-0.2, 0.2), \"y\": (-0.2, 0.2)},\n        rotate=(-25, 25),\n        shear=(-8, 8)\n    ),\n    iaa.Multiply((0.9, 1.1))\n])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5c00f5b1-0210-4041-a270-714b74e35449","_cell_guid":"d1a5e9a9-281a-473a-9ee0-762252df67c2","trusted":true},"cell_type":"code","source":"import warnings \nwarnings.filterwarnings(\"ignore\")\nmodel.keras_model.metrics_tensors = []\nmodel.train(train_set, val_set, \n            learning_rate=config.LEARNING_RATE, \n            epochs=5, \n            layers='all',\n            augmentation=augmentation)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5457928e-3a83-48ae-84fb-c60770425515","_cell_guid":"96467e71-0b26-44e0-99d4-667330a20814","trusted":true},"cell_type":"code","source":"from keras.preprocessing.image import load_img\nfrom keras.preprocessing.image import img_to_array","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"32855d42-0f75-43bd-a20f-7b0431c4ac69","_cell_guid":"833000b0-7e84-4a27-b0e1-12bff74273d8","trusted":true},"cell_type":"code","source":"import time\nmodel_path = '../kaggle/working/mask_rcnn_'  + '.' + str(time.time()) + '.h5'\nmodel.keras_model.save_weights(model_path)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}