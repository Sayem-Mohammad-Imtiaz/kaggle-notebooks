{"cells":[{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"# NSU Distributional Semantics 2019 Course. Seminar 3"},{"metadata":{"_uuid":"4f108fcd7e78fa0c90ec7bfa1e3b9e7e13532a4a"},"cell_type":"markdown","source":"Our course repo: https://github.com/disemantics/course2019\n\n![](http://imgur.com/S8WgwBp.png)\n\nIn this seminal, we will learn how to implement and use topic modeling algorithms. We will consider **LSA** (latent semantic analysis), **PLSA** (probabilistic latent semantic analysis) and **LDA** (latent Dirichlet allocation)."},{"metadata":{"_uuid":"875299ffc8fbe124c16bd9b74971bfce365c0923"},"cell_type":"markdown","source":"## Reading data\nAt first, we need to open load a dataset and select text colomns from it."},{"metadata":{"trusted":true,"_uuid":"3e91cad5987fa8148772df8eabf9c57f7ff1a9a2"},"cell_type":"code","source":"import pandas as pd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d346265bbe8bd17d57c8fd1d8967fba3c862ce83"},"cell_type":"code","source":"data_path = \"../input/Emails.csv\"\ndata = pd.read_csv(data_path)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e41599b26459545cfa2c264745af77d83d555500"},"cell_type":"code","source":"data.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"53acb27459d906b6614a477f99e60ff99b96804d"},"cell_type":"code","source":"print(f\"Number of Emails: {data.shape[0]}\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"95ff21bee80d430776f4667ec7980f1892773190"},"cell_type":"markdown","source":"## Preprocessing data"},{"metadata":{"_uuid":"7b76d221133af70a4bc0a027d2b818f2df3b6cca"},"cell_type":"markdown","source":"We select only the main text column (ExtractedBodyText) witiout NaNs (empty emails)."},{"metadata":{"trusted":true,"_uuid":"29080f709db703bec01b9b799064f467f65ee9cb"},"cell_type":"code","source":"data = data[pd.notnull(data['ExtractedBodyText'])]\nprint(data.sample(5)['ExtractedBodyText'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"99181eeaa87c1710a42448a63fdd013210115879","_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"print(f\"Number of Emails: {data.shape[0]}\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"07e92c0c020fbc4f3a1df01bef3ddbe19f377889"},"cell_type":"markdown","source":"On the next step, we need to clear our data from punctuation and stopwords."},{"metadata":{"trusted":true,"_uuid":"78e7696cc3e07cc2814d14dea6f8d404bab80b7b"},"cell_type":"code","source":"from nltk import RegexpTokenizer\nfrom nltk.corpus import stopwords\nstop_words = set(stopwords.words('english'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c68ccfa8362d84b6d25540037489828486d6cf38"},"cell_type":"code","source":"tokenizer = RegexpTokenizer(r'\\w+')\ntexts = [tokenizer.tokenize(email.lower()) for email in data['ExtractedBodyText']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"60d7726278aa3b340a02a99ab88a41b35ccffb55","_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"print(texts[5070])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ddb0f829f032d0eb3605f66667927853eddb05ca"},"cell_type":"code","source":"def delete_stopwords(tokenized_sentence: list):\n    return list(filter(lambda x: x not in stop_words, tokenized_sentence))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0126e409a17dc8c71a8187018560be475c345b3a"},"cell_type":"code","source":"texts = list(filter(lambda x: len(x) > 5, [delete_stopwords(text) for text in texts]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"04450ead407217ee88f80a11f95fea1ea5861de5"},"cell_type":"code","source":"print(f\"Number of Emails: {len(texts)}\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5d741ee40237b60e2bbb42fd13f1179b5307eaef"},"cell_type":"markdown","source":"Now it's time to convert texts to bag-of-words format."},{"metadata":{"trusted":true,"_uuid":"d601c98493043873325aa42ffbb94f8137cb14fe"},"cell_type":"code","source":"from gensim import corpora","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8237b4cc2a32876ff9a37f3750cba219d95a6efe"},"cell_type":"code","source":"corpora_dict = corpora.Dictionary(texts)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"29b0ab020048d8cf1ae425809ec8c34ecce703ef","_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"print(list(corpora_dict.token2id.items())[::500])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0467276cd94e04b44c8ffca7a43061f8c0c19a1c","_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"corpora_dict[500]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"14be222a1976cff15c5bfdbfaa41ba2a034d38d0","_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"corpora_dict.id2token[500]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f9a5ec4331288accb9b6f33fce35442a1c15edba"},"cell_type":"code","source":"corpus = [corpora_dict.doc2bow(text) for text in texts]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dde312d3412334fbaff71777d951d5c28b7b0c14","_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"print(corpus[0])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dfbe1e8e8a46eb1f15c88ed57ac7c02fc60b43b8"},"cell_type":"markdown","source":"## LSI (LSA)"},{"metadata":{"trusted":true,"_uuid":"5b55131feab129ca452be8e43c1577495f9ba0da"},"cell_type":"code","source":"from gensim.models import LsiModel","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cce5e8698535e85a7e3a77841b614e39e833142b"},"cell_type":"code","source":"model_lsi = LsiModel(corpus, id2word=corpora_dict.id2token, num_topics=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b9e51f95b5fd962ac3011ccf874827981413d4bc"},"cell_type":"code","source":"str_topics = [topic_w for topic_number, topic_w in model_lsi.print_topics()]\nstr_topics_split = list(map(lambda x: x.split(\"+\"), str_topics))\nstr_topics_split = [list(map(lambda x: x.split(\"*\")[1].strip()[1:-1], elem)) for elem in str_topics_split]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b42f9b51853efcdfe153d1dff089f602fd0998cf"},"cell_type":"code","source":"for topic in str_topics_split:\n    print(topic)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e3e3fa8c76f584360562dd018d7e8b11af43a024"},"cell_type":"markdown","source":"## LDA"},{"metadata":{"trusted":true,"_uuid":"5a418974a9dbe3dd525cefaa20a0a5b57bce3d47"},"cell_type":"code","source":"from gensim import matutils\nfrom gensim.models.ldamodel import LdaModel","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6f2baa247a875787ea2ae04ec15b0eedbb4edf77"},"cell_type":"code","source":"model_lda = LdaModel(corpus, passes=20, num_topics=10, id2word=corpora_dict.id2token)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7b97b8291440c64711fe1bf223d51b6861bcf736"},"cell_type":"code","source":"str_topics = [topic_w for topic_number, topic_w in model_lda.print_topics()]\nstr_topics_split = list(map(lambda x: x.split(\"+\"), str_topics))\nstr_topics_split = [list(map(lambda x: x.split(\"*\")[1].strip()[1:-1], elem)) for elem in str_topics_split]\n\nfor topic in str_topics_split:\n    print(topic)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5d20f76d262a630de2a0f093c64f74828372a25d"},"cell_type":"code","source":"import pyLDAvis","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2e137e6ad8462c8fdd8a166f6bf859404057d66b"},"cell_type":"code","source":"import pyLDAvis.gensim\n\ndata_lda = pyLDAvis.gensim.prepare(model_lda, corpus, corpora_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"121b1678ee832ce81b4e2266ec1cc9d5470934df"},"cell_type":"code","source":"pyLDAvis.enable_notebook()\npyLDAvis.display(data_lda)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ddedbec170feaabf7bc4accae61eaa9e0f637f92"},"cell_type":"markdown","source":"# Homework (10 points)\n## PLSA\nImplement a PLSA model with gensim-like interface."},{"metadata":{"trusted":true,"_uuid":"73a8ba1ec92ba058c262ff1f6849794b5346bae1"},"cell_type":"code","source":"from gensim.matutils import corpus2dense\nimport numpy as np\nnp.random.seed(42)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c7df8b77fee1cfa0236ad7cedd1af2f973a6e233"},"cell_type":"markdown","source":"This pseudocode of EM-algorithm is taken from here (http://www.machinelearning.ru/wiki/images/8/88/Voron-iip9-talk.pdf) (p. 10):\n\n1. initialize $\\mathbf{\\Phi}$ and $\\mathbf{\\Theta}$ so that $\\forall z \\in \\mathcal{Z}\\,\\,\\, \\sum \\limits_{w \\in \\mathcal{W}} \\Phi_{w, z} = 1,\\,\\Phi_{w, z} \\geq 0,\\,\\, \\forall d \\in \\mathcal{D} \n    \\sum \\limits_{z \\in \\mathcal{Z}} \\Theta_{z, d} = 1,\\, \\Theta_{z, d} \\geq 0.$ \n2. $passes$ times repeat\n3. $\\quad \\forall w \\in \\mathcal{W}, z \\in \\mathcal{Z}\\,n_{wz} := 0,\\,n_{zd} := 0,\\,n_z := 0$.\n4. $\\quad \\forall d \\in \\mathcal{D},\\,w \\in d$:\n5. $\\quad \\quad Z_w = \\sum\\limits_{z \\in \\mathcal{Z}} \\Phi_{w,z} \\Theta_{z,d}$\n6. $\\quad \\quad \\forall z \\in \\mathcal{Z}$ such that  $\\Phi_{w,z} \\Theta_{z,d} > 0$\n7. $\\quad \\quad \\quad$ add $\\frac{n_{wd}}{Z_w} \\Phi_{w,z} \\Theta_{z,d}$ to $\\,n_{wz},\\,n_{zd} ,\\,n_z$\n8. $\\quad \\forall w \\in \\mathcal{W}, z \\in \\mathcal{Z}\\,\\, \\Phi_{w,z} := n_{wz} / n_{z} $\n9. $\\quad \\forall d \\in \\mathcal{D}, z \\in \\mathcal{Z}\\,\\, \\Theta_{z,d} := n_{zd} / n_{d} $"},{"metadata":{"trusted":true,"_uuid":"af87733cdb62fa863f68db2dd9048d2a14415d24"},"cell_type":"code","source":"class PlsaModel:\n    def __init__(self, corpus=None, id2word=None, num_topics=10, passes=30):\n        self.passes = passes\n        \n        self.num_topics = num_topics\n        self.num_documents = len(corpus)\n        self.num_words = len(id2word)\n\n        self.id2word = id2word\n        \n        self.n_wd = corpus2dense(corpus, num_terms=self.num_words)  # [word][document]\n        self.n_d = np.sum(self.n_wd, axis=0)\n        self.n = np.sum(self.n_d)\n\n        self.phi = np.random.random_sample(size=(self.num_words, self.num_topics))\n        self.phi /= np.sum(self.phi, axis=0)\n        self.theta_t = np.random.random_sample(size=(self.num_documents, self.num_topics))\n        self.theta_t /= np.sum(self.theta_t, axis=1)[:, None]\n        \n        for i in range(self.passes):\n            self._fit()\n\n    def _fit(self):\n        # n_zd = # YOUR CODE HERE\n        # n_wz = # YOUR CODE HERE\n        # n_z = # YOUR CODE HERE\n        for d in range(self.num_documents):\n            # YOUR CODE HERE\n            pass\n\n        # YOUR CODE HERE\n        \n    def print_topics(self, top_n=10):\n        res = []\n        for t in range(self.num_topics):\n            top_inds = self.phi[:, t].argsort()[-top_n:][::-1]\n            top_words = [self.id2word[x] for x in top_inds]\n            res.append(top_words)\n        return res","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c5caf7f6d7700f4215ce355f57760711dd865d41"},"cell_type":"code","source":"model_plsa = PlsaModel(corpus, passes=10, num_topics=10, id2word=corpora_dict.id2token)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d702d85c861448a6e97068125de2cc62d0efc495"},"cell_type":"code","source":"for topic in model_plsa.print_topics():\n    print(topic)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d2c118423950f950c7a8ac03f193d514a3bf9993"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"22fcb5f1b7803b787c08d2e897f704c8baa5107c"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"986655d3190a7881f17d217829e23ac69e3530ad"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ea39857b83354df571ead690d1c7754c486b94b7"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9b0768436e4f0e1678b17f77feaab9198f9345b8"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"171491028fac6bf428858c794d7764591d8395f9"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6166b26db46bc1d6bd23daa9d53011bae595fb35"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"598ae950aded743573aa56b58788a4969286a8c1"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}