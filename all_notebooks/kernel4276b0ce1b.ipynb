{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Task \"What do we know about COVID-19 risk factors?\""},{"metadata":{},"cell_type":"markdown","source":"**Subtask \"Transmission dynamics of the virus, including the basic reproductive number, incubation period, serial interval, modes of transmission and environmental factors\"**\n\n*A model of generic document crawler.*"},{"metadata":{},"cell_type":"markdown","source":"*NOTE: This approach may be employed for any other subtask (i.e. bullet point) in the task. This notebook showcases the issue of transmission dynamics. Moreover, this approach is fully customizable meaning that the user of this notebook may fine-tune parameters to achieve better results.*"},{"metadata":{},"cell_type":"markdown","source":"Special credits to:\n- https://www.kaggle.com/acmiyaguchi for providing an excellent notebook with pyspark data import \n- https://www.merriam-webster.com for providing a free API and outstanding word base I used to seek for synonyms\n- https://wordassociations.net/ for providing a free API and outstanding word base I used to seek for associations"},{"metadata":{},"cell_type":"markdown","source":"## Workflow"},{"metadata":{},"cell_type":"markdown","source":"The diagram below presents the workflow of browsing the documents."},{"metadata":{},"cell_type":"markdown","source":"![](https://www.lucidchart.com/publicSegments/view/f95a3e22-8511-49e6-b412-f5a8c1ef1f39)"},{"metadata":{},"cell_type":"markdown","source":"*NOTE: To print out intermediate results and control values, uncomment print() commands.*"},{"metadata":{},"cell_type":"markdown","source":"## Notebook"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install pyspark\n!pip install pyarrow","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport os\nimport pyspark\nfrom pyspark.sql.functions import lit\nfrom pyspark.sql.types import (\n    ArrayType,\n    IntegerType,\n    MapType,\n    StringType,\n    StructField,\n    StructType,\n)\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql import functions as F\nfrom pyspark.sql import Window\nfrom nltk.corpus import stopwords\nimport requests\nimport json\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import PorterStemmer\nfrom pandas.core.common import flatten","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Import Data"},{"metadata":{},"cell_type":"markdown","source":"Credits: https://www.kaggle.com/acmiyaguchi"},{"metadata":{"trusted":true},"cell_type":"code","source":"from pyspark.sql.functions import lit\nfrom pyspark.sql.types import (\n    ArrayType,\n    IntegerType,\n    MapType,\n    StringType,\n    StructField,\n    StructType,\n)\n\n\ndef generate_cord19_schema():\n    \"\"\"Generate a Spark schema based on the semi-textual description of CORD-19 Dataset.\n\n    This captures most of the structure from the crawled documents, and has been\n    tested with the 2020-03-13 dump provided by the CORD-19 Kaggle competition.\n    The schema is available at [1], and is also provided in a copy of the\n    challenge dataset.\n\n    One improvement that could be made to the original schema is to write it as\n    JSON schema, which could be used to validate the structure of the dumps. I\n    also noticed that the schema incorrectly nests fields that appear after the\n    `metadata` section e.g. `abstract`.\n    \n    [1] https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/2020-03-13/json_schema.txt\n    \"\"\"\n\n    # shared by `metadata.authors` and `bib_entries.[].authors`\n    author_fields = [\n        StructField(\"first\", StringType()),\n        StructField(\"middle\", ArrayType(StringType())),\n        StructField(\"last\", StringType()),\n        StructField(\"suffix\", StringType()),\n    ]\n\n    authors_schema = ArrayType(\n        StructType(\n            author_fields\n            + [\n                # Uncomment to cast field into a JSON string. This field is not\n                # well-specified in the source.\n                StructField(\n                    \"affiliation\",\n                    StructType(\n                        [\n                            StructField(\"laboratory\", StringType()),\n                            StructField(\"institution\", StringType()),\n                            StructField(\n                                \"location\",\n                                StructType(\n                                    [\n                                        StructField(\"settlement\", StringType()),\n                                        StructField(\"country\", StringType()),\n                                    ]\n                                ),\n                            ),\n                        ]\n                    ),\n                ),\n                StructField(\"email\", StringType()),\n            ]\n        )\n    )\n\n    # used in `section_schema` for citations, references, and equations\n    spans_schema = ArrayType(\n        StructType(\n            [\n                # character indices of inline citations\n                StructField(\"start\", IntegerType()),\n                StructField(\"end\", IntegerType()),\n                StructField(\"text\", StringType()),\n                StructField(\"ref_id\", StringType()),\n            ]\n        )\n    )\n\n    # A section of the paper, which includes the abstract, body, and back matter.\n    section_schema = ArrayType(\n        StructType(\n            [\n                StructField(\"text\", StringType()),\n                StructField(\"cite_spans\", spans_schema),\n                StructField(\"ref_spans\", spans_schema),\n                # While equations don't appear in the abstract, but appear here\n                # for consistency\n                StructField(\"eq_spans\", spans_schema),\n                StructField(\"section\", StringType()),\n            ]\n        )\n    )\n\n    bib_schema = MapType(\n        StringType(),\n        StructType(\n            [\n                StructField(\"ref_id\", StringType()),\n                StructField(\"title\", StringType()),\n                StructField(\"authors\", ArrayType(StructType(author_fields))),\n                StructField(\"year\", IntegerType()),\n                StructField(\"venue\", StringType()),\n                StructField(\"volume\", StringType()),\n                StructField(\"issn\", StringType()),\n                StructField(\"pages\", StringType()),\n                StructField(\n                    \"other_ids\",\n                    StructType([StructField(\"DOI\", ArrayType(StringType()))]),\n                ),\n            ]\n        ),\n        True,\n    )\n\n    # Can be one of table or figure captions\n    ref_schema = MapType(\n        StringType(),\n        StructType(\n            [\n                StructField(\"text\", StringType()),\n                # Likely equation spans, not included in source schema, but\n                # appears in JSON\n                StructField(\"latex\", StringType()),\n                StructField(\"type\", StringType()),\n            ]\n        ),\n    )\n\n    return StructType(\n        [\n            StructField(\"paper_id\", StringType()),\n            StructField(\n                \"metadata\",\n                StructType(\n                    [\n                        StructField(\"title\", StringType()),\n                        StructField(\"authors\", authors_schema),\n                    ]\n                ),\n                True,\n            ),\n            StructField(\"abstract\", section_schema),\n            StructField(\"body_text\", section_schema),\n            StructField(\"bib_entries\", bib_schema),\n            StructField(\"ref_entries\", ref_schema),\n            StructField(\"back_matter\", section_schema),\n        ]\n    )\n\n\ndef extract_dataframe_kaggle(spark):\n    \"\"\"Extract a structured DataFrame from the semi-structured document dump.\n\n    It should be fairly straightforward to modify this once there are new\n    documents available. The date of availability (`crawl_date`) and `source`\n    are available as metadata.\n    \"\"\"\n    base = \"/kaggle/input/CORD-19-research-challenge\"\n    crawled_date = \"2020-03-13\"\n    sources = [\n        \"noncomm_use_subset\",\n        \"comm_use_subset\",\n        \"biorxiv_medrxiv\",\n        \"custom_license\",\n    ]\n\n    dataframe = None\n    for source in sources:\n        #path = f\"{base}/{crawled_date}/{source}/{source}\"\n        path = f\"{base}/{source}/{source}\"\n        df = (\n            spark.read.json(path, schema=generate_cord19_schema(), multiLine=True)\n            .withColumn(\"crawled_date\", lit(crawled_date))\n            .withColumn(\"source\", lit(source))\n        )\n        if not dataframe:\n            dataframe = df\n        else:\n            dataframe = dataframe.union(df)\n    return dataframe","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"spark = SparkSession.builder.getOrCreate()\ndf = extract_dataframe_kaggle(spark)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# df.printSchema()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.createOrReplaceTempView(\"cord19\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Extracting authors (in case they were needed later to human-assess the result), abstracts (so that human can faster reject inrellevant ones in case they were reported as relevant) and texts."},{"metadata":{"trusted":true},"cell_type":"code","source":"text = (\n    df.select(\"paper_id\", F.posexplode(\"body_text\").alias(\"pos\", \"value\"))\n    .select(\"paper_id\", \"pos\", \"value.text\")\n    .withColumn(\"ordered_text\", F.collect_list(\"text\").over(Window.partitionBy(\"paper_id\").orderBy(\"pos\")))\n    .groupBy(\"paper_id\")\n    .agg(F.max(\"ordered_text\").alias(\"sentences\"))\n    .select(\"paper_id\", F.array_join(\"sentences\", \" \").alias(\"text\"))\n    .withColumn(\"words\", F.size(F.split(\"text\", \"\\s+\")))\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# text.show(n=5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# if you want to limit the number of rows sent for processing, uncomment this line\n source_texts = text.limit(20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Retrieving Subtask Words"},{"metadata":{},"cell_type":"markdown","source":"On the basis of the bullet point in the task (starting with \"Specifically, we want to know what the literature reports about: ...\") I build a list of words I am going to look for in the text. I clean the list from stopwords, duplicates and get it lowercased.\nAs mentioned before I focus on transmission dynamics, but this workflow may be used for any question in the task and other tasks as well."},{"metadata":{"trusted":true},"cell_type":"code","source":"task_specs = ('Transmission dynamics of the virus, including the basic reproductive number, incubation period, serial interval, modes of transmission and environmental factors'.replace(',', '')).lower()\n\ntransmission_dynamics_words = task_specs.split(' ')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#print(transmission_dynamics_words)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Remove stopwords."},{"metadata":{"trusted":true},"cell_type":"code","source":"for stopword in stopwords.words('english'):\n    if transmission_dynamics_words.__contains__(stopword):\n        transmission_dynamics_words.remove(stopword)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print(transmission_dynamics_words)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Discarding duplicate words."},{"metadata":{"trusted":true},"cell_type":"code","source":"transmission_dynamics_words = list(set(transmission_dynamics_words)) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print(transmission_dynamics_words)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lowercase words."},{"metadata":{"trusted":true},"cell_type":"code","source":"for word in transmission_dynamics_words: \n    word.lower()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Synonyms"},{"metadata":{},"cell_type":"markdown","source":"As presented on the diagram this step covers looking for synonyms to words in transmission_dynamics_words. It queries Merriam Webster via API to give me a JSON with the words with free Merriam Webster subscription and then performs initial data pre-processing."},{"metadata":{"trusted":true},"cell_type":"code","source":"synonyms_ragged = [] #temporary structure\n\nfor word in transmission_dynamics_words:\n    url = 'https://www.dictionaryapi.com/api/v3/references/thesaurus/json/'+ word +'?key=26e5f7f5-056c-42e3-85ba-956a86beef64'\n    synonyms_work = requests.get(url)\n    synonyms_dictionary = synonyms_work.json()\n    for i in (0, len(synonyms_dictionary)-1):\n        try:\n            dic = synonyms_dictionary[i]\n            meta = dic['meta']\n            syns = meta['syns']\n            synonyms_ragged.append(syns)\n        except:\n            continue\n\n\nsynonyms = list(flatten(synonyms_ragged))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print(synonyms)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for word in synonyms: \n    word.lower()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Associations"},{"metadata":{},"cell_type":"markdown","source":"Calling Word Associations to seek for accociations for each word in transmission_dynamics_words."},{"metadata":{"trusted":true},"cell_type":"code","source":"associations = pd.DataFrame(['item', 'weight', 'pos'])\ndf_associations = pd.DataFrame()\n\nfor word in transmission_dynamics_words:\n    url = 'https://api.wordassociations.net/associations/v1.0/json/search?apikey=786154c9-f2dd-483c-b5f1-6eb934c67275&text=' + word + '&lang=en'\n    associations_work = requests.get(url)\n    associations_as_dictionary = associations_work.json()\n    response_body = associations_as_dictionary['response']\n    # get the content of the response that is saved as list with one element \n    # clean response until it can be a dataframe\n    response_content = response_body[0]\n    response_content_str = response_content['items']\n    response_content_str = response_content_str[1 : len(response_content_str)]\n    for association in response_content_str:\n        list_to_merge = [association['item'], association['weight'], association['pos']]\n        df_to_merge = pd.DataFrame(list_to_merge)\n        df_associations = pd.concat([df_associations, df_to_merge], axis=1)\n        associations_chunk = df_associations.transpose()\n        pd.concat([associations, associations_chunk], axis=0)\n\n\nassociations = df_associations.transpose()\n    \nassociations.columns = ['item', 'weight', 'pos']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print(associations.info())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"associations.reset_index(inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def lower_case_item(item):\n    return item.lower()\n\n\nassociations['item'] = associations['item'].apply(lambda item : lower_case_item(item))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print(associations.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# remove duplicates that may occur within associations\nassociations.drop_duplicates(subset='item', keep='first', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Tokenizing, Removing Stopwords, Stemming"},{"metadata":{},"cell_type":"markdown","source":"Preparing texts and lists for analysis. "},{"metadata":{},"cell_type":"markdown","source":"First full texts - will be tokenized, cleaned from stopwords and stemmed."},{"metadata":{"trusted":true},"cell_type":"code","source":"spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# convert Spark format to Pandas\nsource_texts_df = source_texts.limit(10).toPandas()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"source_texts_df['text'] = source_texts_df['text'].apply(lambda item : lower_case_item(item))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def tokenize_remove_stopwords(text):\n    tokenized_text = word_tokenize(text)\n    for stopword in stopwords.words('english'):\n        if tokenized_text.__contains__(stopword):\n            tokenized_text.remove(stopword)\n    tokenized_text=[word.lower() for word in tokenized_text if word.isalpha()]\n    return tokenized_text\n\n\nsource_texts_df['text_tokens'] = source_texts_df['text'].apply(lambda text: tokenize_remove_stopwords(text))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print(source_texts_df['text_tokens'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ps = PorterStemmer()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def stem_words(text):\n    stemmed_text = []\n    for word in text:\n        stemmed_text.append(ps.stem(word))\n\n    stemmed_text_string = ' '.join([str(elem) for elem in stemmed_text])\n\n    return stemmed_text_string\n\n\nsource_texts_df['text_stemmed'] = source_texts_df['text_tokens'].apply(lambda text: stem_words(text))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print(source_texts_df['text_stemmed'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Stem transmission_dynamics_words, synonyms and associations."},{"metadata":{"trusted":true},"cell_type":"code","source":"transmission_dynamics_words_stemmed = []\n\nfor word in transmission_dynamics_words:\n    transmission_dynamics_words_stemmed.append(ps.stem(word))\n\n\nsynonyms_stemmed = []\n\nfor word in synonyms:\n    synonyms_stemmed.append(ps.stem(word))\n    \n\nassociations['item_stemmed'] = associations['item'].apply(lambda item: ps.stem(item))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Browsing texts for words"},{"metadata":{},"cell_type":"markdown","source":"Now that we've got:\n- texts \n- list of words from the task - transmission_dynamics_words_stemmed\n- list of associations with words on transmission_dynamics_words - associations_stemmed\n- list of synonyms - synonyms_stemmed\n\nwe can proceed to sweeping texts for these words (let's call them 'keywords' from now on).\n\nWe do it in 3 steps:\n1. Browse for words from task. Score texts: for every word found 1 point (no matter how many occurences). Pick 1000 best matches.\n2. Browse for synonyms. Pick top scoring 700 texts from step 1 (or any other number you like, this is a parameter). Score texts: for every close synonym give 0.75 point, for every far synonym give 0.25 point. Add points to points from step 1.\n3. Browse for associations. Pick top scoring texts from step 2. Score texts: every association has weight assigned up-front by Word Associations. Use this exact value for scoring. Total points. Pick 100 winners."},{"metadata":{"trusted":true},"cell_type":"code","source":"# define cutoff - number of texts to keep after browsing texts for key words\ncutoff = 1000\nper_word = 2\nper_synonym = 1\n# per association are defined in their respective df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# add columns to keep text scores to full_table with all the texts\nsource_texts_df['score_task_words'] = 0.00\nsource_texts_df['score_synonyms'] = 0.00\nsource_texts_df['score_associations'] = 0.00\nsource_texts_df['score_total'] = 0.00","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# source_texts_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# browse by words from task\nfor index, row in source_texts_df.iterrows():\n    row_score = 0\n    for word in transmission_dynamics_words_stemmed:\n        if str(row['text_stemmed']).find(word) != -1:\n            row_score += per_word\n            source_texts_df.at[index, 'score_task_words'] = row_score\n            source_texts_df['score_total'] = source_texts_df['score_task_words'] + source_texts_df['score_synonyms'] + source_texts_df['score_associations']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print(source_texts_df.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# browse by words from synonyms\nfor index, row in source_texts_df.iterrows():\n    row_score = 0\n    for word in synonyms_stemmed:\n        if str(row['text_stemmed']).find(word) != -1:\n            row_score += per_word\n            source_texts_df.at[index, 'score_synonyms'] = row_score\n            source_texts_df['score_total'] = source_texts_df['score_task_words'] + source_texts_df['score_synonyms'] + source_texts_df['score_associations']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print(source_texts_df.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# browse by words from associations\nfor index, row in source_texts_df.iterrows():\n    row_score = 0.00\n    for word in associations['item_stemmed']:\n        if str(row['text_stemmed']).find(word) != -1:\n            row_score += float((associations.at[index, 'weight'])/100)\n            source_texts_df.at[index, 'score_associations'] = row_score\n            source_texts_df['score_total'] = source_texts_df['score_task_words'] + source_texts_df['score_synonyms'] + source_texts_df['score_associations']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print(source_texts_df.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get top texts (by cutoff parameter)\ntop_texts = source_texts_df.nlargest(cutoff, 'score_total', keep=\"all\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" # print(top_texts['score_total'].head(20))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# export to .csv file\ntop_texts.to_csv('top_texts.csv')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":4}