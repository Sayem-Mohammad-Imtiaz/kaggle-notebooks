{"metadata":{"language_info":{"version":"3.6.0","mimetype":"text/x-python","name":"python","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"}},"nbformat":4,"cells":[{"metadata":{},"cell_type":"markdown","source":"# fake2vec\n\n_You shall know a word by the company it keeps_\n\n~John Firth\n\n\nWe do some exploratory analysis of the fake news articles provided by kaggle.\n\nWe then transform words to vectors using Gensims word2vec to understand the semmantics of the words used in the articles. We use the semmantic relationships to conclude the new articles are in fact very biased.\n\n**I do not endorse and am not expressing any political affiliation or intent expressed in the articles in this dataset.**\n\nWe start by tokenizing the articles into sentences and then words; so the final result is each article is a list of lists, the inner list is a list of tokens with no punctuation.\n\nWe find the most frequent words used, and plot a histogram of document lengths.\n\nAn overview of the word2vec algorithm is given and we use the gensim package to implement word2vec. We explore what the model has discovered to be semmantically similar words visually through the TSNE algorithm."},{"metadata":{"collapsed":true},"cell_type":"code","outputs":[],"source":"# import dependencies\n%matplotlib inline\nimport pandas as pd\nimport numpy as np\nimport multiprocessing\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk import FreqDist\nfrom collections import Counter\nimport random\nimport re\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport gensim.models.word2vec as w2v\nimport sklearn.manifold\nimport time\nsns.set_style(\"darkgrid\")","execution_count":51},{"metadata":{},"cell_type":"markdown","source":"Read in data; only keep essential columns and English language articles"},{"metadata":{"scrolled":false},"cell_type":"code","outputs":[],"source":"df = pd.read_csv('fake.csv', usecols = ['uuid','author','title','text','language','site_url','country'])\ndf = df[df.language == 'english']\ndf['title'].fillna(value=\"\", inplace=True)\ndf.dropna(axis=0, inplace=True, subset=['text'])\ndf = df.sample(frac=1.0) # shuffle the data\ndf.reset_index(drop=True,inplace=True)\ndf.head()","execution_count":52},{"metadata":{},"cell_type":"markdown","source":"### Tokenizing\n\nWe will use the nltk sent_tokenizer to tokenize our text into by sentene; this is a trained model and can distinguihs between complex sentences that have period \".\" in the middle\n\ne.g."},{"metadata":{},"cell_type":"code","outputs":[],"source":"example_text = \"Hi there! Good morning Mr. Smith. You should check out www.example.com, its a great website\"\nnltk.sent_tokenize(example_text)","execution_count":54},{"metadata":{},"cell_type":"markdown","source":"The above was tokenized correctly into 3 sentences.\n\nNext, we want to\n- Tokenize all articles by sentence\n- Transform all text to lower case so that the model doesnt distinguish between cases\n- Remove all websites and non alphabetical characters (punctuation, numbers, etc.)"},{"metadata":{"collapsed":true},"cell_type":"code","outputs":[],"source":"def sent_tokenizer(text):\n    \"\"\"\n    Function to tokenize sentences\n    \"\"\"\n    text = nltk.sent_tokenize(text)\n    return text\n\ndef sentence_cleaner(text):\n    \"\"\"\n    Function to lower case remove all websites, emails and non alphabetical characters\n    \"\"\"\n    new_text = []\n    for sentence in text:\n        sentence = sentence.lower()\n        sentence = re.sub(\"((\\S+)?(http(s)?)(\\S+))|((\\S+)?(www)(\\S+))|((\\S+)?(\\@)(\\S+)?)\", \" \", sentence)\n        sentence = re.sub(\"[^a-z ]\", \"\", sentence)\n        sentence = nltk.word_tokenize(sentence)\n        sentence = [word for word in sentence if len(word)>1] # exclude 1 letter words\n        new_text.append(sentence)\n    return new_text\n\ndef apply_all(text):\n    return sentence_cleaner(sent_tokenizer(text))","execution_count":55},{"metadata":{"scrolled":false},"cell_type":"code","outputs":[],"source":"t1 = time.time()\ndf['sent_tokenized_text'] = df['text'].apply(apply_all)\nt2 = time.time()\nprint(\"Time to clean and tokenize\", len(df), \"articles:\", (t2-t1)/60, \"min\")","execution_count":56},{"metadata":{},"cell_type":"code","outputs":[],"source":"df.head()","execution_count":57},{"metadata":{},"cell_type":"markdown","source":"Now we want to create a list of all words to be able to generate a word frequency count. With this we can plot the a histogram of document lengths and decide the minimum number of times for a word to appear in the corpus to contribute to our model."},{"metadata":{"collapsed":true},"cell_type":"code","outputs":[],"source":"# create a list of all words using list comprehension\nall_words = [word for item in list(df['sent_tokenized_text']) for word in item]\nall_words = [subitem for item in all_words for subitem in item]","execution_count":58},{"metadata":{},"cell_type":"code","outputs":[],"source":"fdist = FreqDist(all_words)\nlen(fdist) # number of unique words","execution_count":59},{"metadata":{},"cell_type":"markdown","source":"The most common words, are of course, stop words; words like \"the\",\"to\",\"a\",\"is\" which fill sentences to make them grammatically correct, but dont describe the actual context of a sentence.\n\nWe will leave these here in our analysis, but remove them in the lda_news notebook when we apply lda to the articles."},{"metadata":{"scrolled":true},"cell_type":"code","outputs":[],"source":"# show the most common words and their counts corpus wide\nfdist.most_common(20)","execution_count":60},{"metadata":{"scrolled":true},"cell_type":"code","outputs":[],"source":"# choose k and visually inspect the bottom 10 words of the top k\nk = 50000\ntop_k_words = fdist.most_common(k)\ntop_k_words[-10:]","execution_count":61},{"metadata":{},"cell_type":"code","outputs":[],"source":"# choose k and visually inspect the bottom 10 words of the top k\nk = 30000\ntop_k_words = fdist.most_common(k)\ntop_k_words[-10:]","execution_count":62},{"metadata":{},"cell_type":"markdown","source":"k = 50,000 is too high, as the bottom words aren't even real words and are very rarely used (3 times in entire corpus)\n\nk = 30,000 seems much more reasonable as these have been used at least 7 times in the corpus"},{"metadata":{"collapsed":true},"cell_type":"code","outputs":[],"source":"# get document lengths to plot histogram\ndef doc_length(text):\n    return len([word for sent in text for word in sent])","execution_count":63},{"metadata":{},"cell_type":"code","outputs":[],"source":"# document length\ndf['doc_len'] = df['sent_tokenized_text'].apply(doc_length)\ndoc_lengths = list(df['doc_len'])\ndf.drop(labels='doc_len', axis=1, inplace=True)","execution_count":64},{"metadata":{},"cell_type":"code","outputs":[],"source":"print(\"length of list:\",len(doc_lengths),\n      \"\\naverage document length\", np.average(doc_lengths),\n      \"\\nmaximum document length\", max(doc_lengths))","execution_count":65},{"metadata":{},"cell_type":"code","outputs":[],"source":"# plot a histogram of document length\nnum_bins = 1000\nfig, ax = plt.subplots(figsize=(12,6));\n# the histogram of the data\nn, bins, patches = ax.hist(doc_lengths, num_bins, normed=1)\nax.set_xlabel('Document Length (tokens)', fontsize=15)\nax.set_ylabel('Normed Frequency', fontsize=15)\nax.grid()\nax.set_xticks(np.logspace(start=np.log10(250),stop=np.log10(4000),num=7, base=10.0))\nplt.xlim(0,4000)\nax.plot([np.average(doc_lengths) for i in np.linspace(0.0,0.0022,100)], np.linspace(0.0,0.0022,100), '-',\n        label='average doc length')\nax.legend()\nax.grid()\nfig.tight_layout()\nplt.show()","execution_count":66},{"metadata":{},"cell_type":"markdown","source":"Now create a list of lists of all sentences.\n\nThe word2vec model will be trained on the all_sentences variable"},{"metadata":{"scrolled":true},"cell_type":"code","outputs":[],"source":"all_sentences = list(df['sent_tokenized_text'])\nall_sentences = [subitem for item in all_sentences for subitem in item]\nall_sentences[:2] # print first 5 sentences","execution_count":67},{"metadata":{},"cell_type":"code","outputs":[],"source":"token_count = sum([len(sentence) for sentence in all_sentences])\nprint(\"The corpus contains {0:,} tokens\".format(token_count)) # total words in corpus","execution_count":68},{"metadata":{},"cell_type":"markdown","source":"## Word2vec\n\nAt a high level, word2vec turns all words from a corpus into vectors in some medium dimensional (100-300) vector space. This vector space can be thought of as a \"semmantic\" or \"contextual\" vector space. In this vector space, words that are close to each other, in a euclidean distance sense, are semmantically simmilar. For example, the words \"car\" and \"engine\" would be semmantically close to each other, or the words \"machine\" and \"learnig\" would also be semmantically close to each other. The word2vec does this in a completely unsupervised way, simply by calculating distributions for words and a surrounding window of context words.\n\nThis representation of words, more formally known as word embeddings, is extremely useful for understanding language and sentiment in a corpus. One thing I want to stress here, is that word2vec is a completely unsupervised algorithm; this means we do not need to label or classify any data - simply feed the algorithm the raw sentence tokenized text and it will derive the word embeddings itself. Given it is unsupervised, when you see the results, you will be amazed!\n\nWord2vec uses a 2 layer neural network to learn word embeddings. It does so in the following way\n\n1) Weight matrices are randomly initialised\n\n2) OHE word vectors are multiplied by the input weight matrix\n\n3) The output of 2) is passed through a softmax function to transform the values into probabilities\n\n4) The output of 3) is multiplied by an output weight matrix\n\n5) An error is computed using the negative log likelihood of the softmax function\n\n6) This error is **backpropagated** through the network to update the weight matrices\n\n7) This process is reapeted using gradient descent until the error is minimised\n\nAs it would be very expensive to update all word vectors at every iteration, optimised methods of reducing the number of updates such as negative sampling or hierarchical softmax are used to improve efficiency."},{"metadata":{},"cell_type":"markdown","source":"### Hyperparameters\n\nSome last and very quick comments on word2vec hyperparamaters:\n\n- num_features is the number of dimensions we want to embed our words in (the length of the final word vectors)\n- min_word_count, if the count of a words use is below this threshold, corpus wide, ignore the word\n- context_size is the window size left and right of the target word to consider words"},{"metadata":{"collapsed":true},"cell_type":"code","outputs":[],"source":"num_features = 300 # number of dimensions\n# if any words appear less than min_word_count amount of times, disregard it\n# recall we saw that the bottom 10 of the top 30,000 words appear only 7 times in the corpus, so lets choose 10 here\nmin_word_count = 10\nnum_workers = multiprocessing.cpu_count()\ncontext_size = 7 # window size around target word to analyse\ndownsampling = 1e-3 # downsample frequent words\nseed = 1 # seed for RNG","execution_count":69},{"metadata":{"collapsed":true},"cell_type":"code","outputs":[],"source":"# setting up model with parameters above\nfake2vec = w2v.Word2Vec(\n    sg=1,\n    seed=seed,\n    workers=num_workers,\n    size=num_features,\n    min_count=min_word_count,\n    window=context_size,\n    sample=downsampling\n)","execution_count":70},{"metadata":{},"cell_type":"code","outputs":[],"source":"fake2vec.build_vocab(all_sentences)","execution_count":71},{"metadata":{},"cell_type":"code","outputs":[],"source":"print(\"Word2Vec vocabulary length:\", len(fake2vec.wv.vocab))","execution_count":72},{"metadata":{},"cell_type":"code","outputs":[],"source":"# number of sentences\nfake2vec.corpus_count","execution_count":73},{"metadata":{},"cell_type":"code","outputs":[],"source":"# train word2vec - this may take a minute...\nfake2vec.train(all_sentences, total_examples=fake2vec.corpus_count, epochs=fake2vec.iter)","execution_count":74},{"metadata":{},"cell_type":"code","outputs":[],"source":"# dense 2D matrix of word vectors\nall_word_vectors_matrix = fake2vec.wv.syn0","execution_count":75},{"metadata":{},"cell_type":"code","outputs":[],"source":"all_word_vectors_matrix.shape # .shape[0] are the top words we are considering in training word2vec","execution_count":76},{"metadata":{},"cell_type":"markdown","source":"### TSNE\n\nT Stochastic Neighbour Embedding is a dimensionality reduction method, specifically for manifold leanring and visualisation.\n\nEssentially tsne computes euclidean distances between points in a high dimensional space, maps these two a lower dimensional space (usually 2 or 3) while preserving local structure. It does this by centering a gaussian on every point and scaling the euclidean distance between points by that gaussian. It then minimizes the distance difference between the high dimensional space and the low dimensional space via gradient descent on the Kullback-Leiblier divergence of probabilities between the high dimensional space and low dimensional space.\n\n_The visual output of TSNE is a 2D map where we arbitrarily call the axes 'X' and 'Y'; these axes dont actually represent the original data in any way (unlike a linear combination from PCA), so we do not need to interpret their meaning. Each point on the map is a unique word in the corpus, and points that are close to each other, in a euclidean sense, are words that word2vec has calculated to be contextually similar._\n\nMore sophistciated and accurate explanations can be found in videos and documentation, but I thought I'd give a quick overview here before I started using the method."},{"metadata":{},"cell_type":"code","outputs":[],"source":"# train tsne model for visualisation\ntsne = sklearn.manifold.TSNE(n_components=2, random_state=0)\nt1 = time.time()\nall_word_vectors_matrix_2d = tsne.fit_transform(all_word_vectors_matrix)\nt2 = time.time()\nprint(\"time to train TSNE on\", all_word_vectors_matrix.shape[0], \"word vectors:\", (t2-t1)/60, \"min\")","execution_count":77},{"metadata":{},"cell_type":"code","outputs":[],"source":"# create a dataframe *points* to store the 2D embeddings of all words\npoints = pd.DataFrame(\n    [\n        (word, coords[0], coords[1])\n        for word, coords in [\n            (word, all_word_vectors_matrix_2d[fake2vec.wv.vocab[word].index])\n            for word in fake2vec.wv.vocab\n        ]\n    ],\n    columns=[\"word\", \"x\", \"y\"]\n)","execution_count":78},{"metadata":{"collapsed":true},"cell_type":"code","outputs":[],"source":"def plot_region(x_bounds, y_bounds):\n    \"\"\"\n    This function defines regions of the tsne map\n    in which to zoom in on\n    \"\"\"\n    slice = points[\n        (x_bounds[0] <= points.x) &\n        (points.x <= x_bounds[1]) & \n        (y_bounds[0] <= points.y) &\n        (points.y <= y_bounds[1])\n    ]\n    \n    ax = slice.plot.scatter(\"x\", \"y\", s=35, figsize=(10, 8))\n    for i, point in slice.iterrows():\n        ax.text(point.x + 0.005, point.y + 0.005, point.word, fontsize=11)","execution_count":79},{"metadata":{},"cell_type":"code","outputs":[],"source":"points.head(10)","execution_count":80},{"metadata":{"collapsed":true},"cell_type":"code","outputs":[],"source":"sns.set_context(\"poster\")","execution_count":81},{"metadata":{},"cell_type":"code","outputs":[],"source":"points.plot.scatter(\"x\", \"y\", s=10, figsize=(20, 12), title=\"TSNE Map of word2vec embeddings\")","execution_count":105},{"metadata":{},"cell_type":"markdown","source":"The points above may look like random unorganised noise, but if we zoom into any dense region we see that all words nearby are related."},{"metadata":{},"cell_type":"markdown","source":"#### Numbers"},{"metadata":{},"cell_type":"code","outputs":[],"source":"plot_region((3,5), (-53,-50))","execution_count":84},{"metadata":{},"cell_type":"markdown","source":"#### Medical Issues / Anatomy"},{"metadata":{},"cell_type":"code","outputs":[],"source":"plot_region((-36,-31), (-32,-27))","execution_count":87},{"metadata":{},"cell_type":"markdown","source":"#### Military Equipment"},{"metadata":{},"cell_type":"code","outputs":[],"source":"plot_region((44,47), (3,6))","execution_count":90},{"metadata":{},"cell_type":"markdown","source":"#### Names"},{"metadata":{},"cell_type":"code","outputs":[],"source":"plot_region((8,11), (38,42))","execution_count":92},{"metadata":{},"cell_type":"markdown","source":"#### CAUTION: Biases ahead\n\nGensim's word2vec has a method **.most_similar**, which takes the cosine similarity between the input word vector and all other word vectors and returns the most similar words i.e. words with the smallest angle between there vectors."},{"metadata":{"collapsed":true},"cell_type":"code","outputs":[],"source":"# similar word relations\ndef nearest_similarity_cosmul(start1, end1, end2):\n    similarities = fake2vec.most_similar_cosmul(\n        positive=[end2, start1],\n        negative=[end1]\n    )\n    start2 = similarities[0][0]\n    print(\"{start1} is related to {end1}, as {start2} is related to {end2}\".format(**locals()))\n    return start2","execution_count":93},{"metadata":{},"cell_type":"code","outputs":[],"source":"fake2vec.most_similar('muslim')","execution_count":94},{"metadata":{},"cell_type":"code","outputs":[],"source":"fake2vec.most_similar('trump')","execution_count":95},{"metadata":{},"cell_type":"code","outputs":[],"source":"fake2vec.most_similar('clinton')","execution_count":96},{"metadata":{},"cell_type":"code","outputs":[],"source":"nearest_similarity_cosmul(\"trump\", \"presidentelect\", \"clinton\") # makes sense","execution_count":97},{"metadata":{},"cell_type":"code","outputs":[],"source":"nearest_similarity_cosmul(\"cancer\", \"body\", \"trump\") # what?","execution_count":103},{"metadata":{},"cell_type":"markdown","source":"## Conclusion\n\n- The corpus of fake news was visually inspected, primarily through understanding the distribution in document length\n- The corpus was tokenized by sentence so as to be used for training by word2vec\n- We visually inspected the bottom words of the top 50,000 and top 30,000 words by count to get an understanding of how often these words are used. This lead us to use 10 as the minimum word count in word2vec\n- The word2vec model was trained\n- A TSNE model was trained on the output of word2vec to visually inspect the data\n- We saw that, by zooming into the TSNE map, close clusters of words were in fact semmantically related. We gave several visual examples\n- There are clearly extreme biases in the articles if the closest words to \"Muslim\" are \"Brotherhood\" and \"Freeloaders\"; another example of closest words are \"Clinton\" and \"Crooked\"... clearly biased. This further shows that these news articles are fake\n- We now have a basis on which to build out LDA model in the next notebook, as we know the documents lengths are long enough for LDA and that the articles, albeit fake, have some structure and classification so that such a model could be trained on it"}],"nbformat_minor":1}