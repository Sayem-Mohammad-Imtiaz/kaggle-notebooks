{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Morgan Dally - 1313361 - Ensembles"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport itertools\nimport os\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# read in input data\ntrain = pd.read_csv('../input/mnist_train.csv', dtype=int)\nX_train = train.drop('label', axis=1)\ny_train = train['label']\n\ntest = pd.read_csv('../input/mnist_test.csv', dtype=int)\nX_test = test.drop('label', axis=1)\ny_test = test['label']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# CONSTS\nRANDOM_STATE = 1313361\n\n# mapping consts\nCLASSIFIER = 'classifier'\nOUT_OF_BAG = 'oob_score'\nDEPTH = 'depth'\nTEST_PRED = 'test_pred'\nPRED_PROB = 'prediction_probability'\nDEC_FUNC = 'decision_function'\n\nNAME = 'name'\n\ndepth_list = [10, 20, 30, 40, 50, 60]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.metrics import accuracy_score\n\ndef find_best_max_depth(depth_list, classifier_generator, X_train, X_test, y_train, y_test):\n    '''\n    Runs through each depth in depth_list, creates a new classifier using\n    classifier_generator. classifier_generator should be a function which\n    takes as input a depth and returns a classifier. It should also set\n    any parameters needed to generate an oob_score.\n    i.e. bootstrap &/or oob_score.\n    '''\n    top_classifier = {\n        CLASSIFIER: None,\n        OUT_OF_BAG: -1,\n        DEPTH: -1,\n        TEST_PRED: None,\n        DEC_FUNC: None,\n        PRED_PROB: None,\n    }\n    for depth in depth_list:\n        CLF = classifier_generator(depth=depth)\n        CLF.fit(X_train, y_train)\n        oob = CLF.oob_score_\n\n        if oob > top_classifier[OUT_OF_BAG]:\n            top_classifier[OUT_OF_BAG] = oob\n            top_classifier[CLASSIFIER] = CLF\n            top_classifier[DEPTH] = depth\n\n    assert top_classifier[CLASSIFIER] is not None\n    assert top_classifier[OUT_OF_BAG] != -1\n    assert top_classifier[DEPTH] != -1\n\n    # predict, get accuracy, decision function and prediction probability\n    y_pred = top_classifier[CLASSIFIER].predict(X_test)\n    top_classifier[TEST_PRED] = accuracy_score(y_test, y_pred)\n    top_classifier[DEC_FUNC] = top_classifier[CLASSIFIER].oob_decision_function_\n    top_classifier[PRED_PROB] = top_classifier[CLASSIFIER].predict_proba(X_test)\n\n    print(\"Best model parameter: %d, oob_score: %f, actual prediction: %f\" % (\n        top_classifier[DEPTH], top_classifier[OUT_OF_BAG], top_classifier[TEST_PRED]\n    ))\n    return top_classifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import AdaBoostClassifier, BaggingClassifier\n\ndef single_extra_tree(depth=None, seed=RANDOM_STATE):\n    '''Creates a single ExtraTreesClassifier with only one tree.'''\n    return ExtraTreesClassifier(\n        max_depth=depth, n_estimators=1,\n        bootstrap=True, random_state=seed,\n        oob_score=True,\n    )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import AdaBoostClassifier, BaggingClassifier\nfrom sklearn.decomposition import PCA\nfrom sklearn.pipeline import Pipeline\n\n# create bagged pca first, otherwise kernel runs out of memory\nbest_ada_depth = 20 # best_bagged_clf[DEPTH] # \ndef create_bagged_pca_pipeline(depth=None, seed=RANDOM_STATE, best_depth=best_ada_depth):\n    '''\n    depth is actually used as num_components.\n    '''\n    pca = PCA(n_components=depth, svd_solver='randomized', random_state=seed)\n    pipe = Pipeline(\n        [('pca', pca),\n        ('ada_xt',\n            AdaBoostClassifier(\n                base_estimator=single_extra_tree(depth=best_depth),\n                n_estimators=10,\n                random_state=seed))\n        ])\n    return BaggingClassifier(base_estimator=pipe, n_estimators=30, bootstrap=True, oob_score=True, n_jobs=-1, random_state=seed)\n\nnum_components_list = [20, 40, 60]\n# 20\nbest_pipeline_clf = find_best_max_depth(\n    num_components_list, create_bagged_pca_pipeline,\n    X_train, X_test, y_train, y_test\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_extra_trees(depth=None, seed=RANDOM_STATE):\n    '''Creates an ExtraTreesClassifier'''\n    return ExtraTreesClassifier(\n        n_estimators=300,\n        max_depth=depth,\n        n_jobs=-1,\n        bootstrap=True,\n        oob_score=True,\n        random_state=seed,\n    )\n\n# 50\nbest_etc = find_best_max_depth(\n    depth_list, create_extra_trees,\n    X_train, X_test, y_train, y_test\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_bagging_predictor(depth=None, seed=RANDOM_STATE):\n    boost = AdaBoostClassifier(base_estimator=single_extra_tree(depth=depth), n_estimators=10, random_state=seed)\n    return BaggingClassifier(base_estimator=boost, n_estimators=30, bootstrap=True, oob_score=True, n_jobs=-1, random_state=seed)\n\n# 20\nbest_bagged_clf = find_best_max_depth(\n    depth_list, create_bagging_predictor,\n    X_train, X_test, y_train, y_test\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check overall prediction\nbest_etc[NAME] = 'extra_trees'\nbest_bagged_clf[NAME] = 'ada_bagged'\nbest_pipeline_clf[NAME] = 'ada_bagged_pipeline'\n\ntrained_models = [best_etc, best_bagged_clf, best_pipeline_clf]\npred_prob_list = [clf[DEC_FUNC] for clf in trained_models]\nclf_names = [clf[NAME] for clf in trained_models]\n\ndef average_oob_scores(pred_prob_list):\n    '''Averages a list of oob_decision_function_.'''\n    return sum(pred_prob_list) / len(pred_prob_list)\n\ndef get_best_model_pp_avg(pred_prob_list, subset_names, y_train):\n    '''\n    Goes through every subset of a list consisting of\n    oob_decision_function_ values. Uses subset_names\n    to name each entry to the list of oob_decision_function_.\n    Prints the concatted models accuracy using y_train.\n    '''\n    if not pred_prob_list:\n        return None\n    if len(pred_prob_list) != len(subset_names):\n        raise RuntimeError(\n            'Pred probailities needs to have same amount of entries as subset names'\n        )\n    best_subset_score = -1\n    best_subset = None\n    # for every possible subset combination\n    for num_entries in range(1, len(pred_prob_list) + 1):\n        subsets = itertools.combinations(pred_prob_list, num_entries)\n        named_subsets = itertools.combinations(subset_names, num_entries)\n\n        # for every subset, check how it should perform\n        for clf_subset, subset in zip(named_subsets, subsets):\n            avg_decsion_funcs = average_oob_scores(subset)\n            avg_predictions = np.argmax(avg_decsion_funcs, axis=1)\n            accuracy = accuracy_score(y_train, avg_predictions)\n            print(clf_subset, accuracy)\n\nbest_model = get_best_model_pp_avg(pred_prob_list, clf_names, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import VotingClassifier\n\n# not actually the best because the best was only a single model.\n# I wanted to run voting with more than one model\nbest_subset = [best_etc, best_bagged_clf]\nestimators = [(clf_entries[NAME], clf_entries[CLASSIFIER]) for clf_entries in best_subset]\n\nvoting_clf = VotingClassifier(\n    estimators=estimators,\n    voting='soft'\n)\nvoting_clf.fit(X_train, y_train)\nvoted_y_pred = voting_clf.predict(X_test)\nprint('extra_trees + ada_bagged acheived %.2f%% accuracy' % (accuracy_score(y_test, voted_y_pred) * 100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfrom sklearn.linear_model import LogisticRegression\n\n# get the x meta train set using the decision function from every model\nX_train_meta = np.concatenate([clf[DEC_FUNC] for clf in trained_models], axis=1)\n\n# get the x meta test set using the prediction probability from every model\nX_test_meta = np.concatenate([clf[PRED_PROB] for clf in trained_models], axis=1)\n\n# train and predict a logistic regressor with the meta sets and y train/test data\nlog_regressor = LogisticRegression(C=50, random_state=RANDOM_STATE)\nlog_regressor.fit(X_train_meta, y_train)\nlog_reg_pred = log_regressor.predict(X_test_meta)\n\n# print the results\nprint('logistic regressor with meta sets acheived %.2f%% accuracy' % (accuracy_score(y_test, log_reg_pred) * 100))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# What is the best test set accuracy?\n* Logistic Regressor trained with the X meta training set.\n* Acheived 97.16% accuracy.\n* Test set accuracy was slightly better than the best voting accuracy (96.72%)"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_digit_missclassifications(y_test, y_pred):\n    '''\n    '''\n    digit_range = range(0, 10)\n    # [[], [], [], [], [], [], [], [], []]\n    digit_mapping = [[0 for digit in digit_range] for _ in digit_range]\n    for expected, prediction in  zip(y_test, y_pred):\n        digit_mapping[expected][prediction] += 1\n\n    def make_missclassified_dict(missclassifications):\n        return { digit: missclassification for digit, missclassification in enumerate(missclassifications) }\n\n    digit_dict = {}\n    for digit, missclassifications in enumerate(digit_mapping):\n        digit_dict[digit] = make_missclassified_dict(missclassifications)\n    return digit_dict\n\ndef plot_digit(X_test, digit_index, missclassifications, img_title, bar_title):\n    '''\n    Prints the digit at digit_index in X_test\n    with the title set.\n    '''\n    pixels = pd.DataFrame(X_test.iloc[digit_index,:])\n    pixels = pixels.values.reshape((28,28))\n    # plt.subplot(1, 2, 1)\n    plt.suptitle(img_title)\n    plt.imshow(pixels, cmap='gray')\n    plt.show()\n\n    mclf_x_labels = missclassifications.keys()\n    mclf_counts = missclassifications.values()\n    # plt.subplot(1, 2, 2)\n    plt.suptitle(bar_title)\n    plt.bar(mclf_x_labels, mclf_counts, align='center', alpha=1)\n\n    plt.xlabel('Digit')\n    plt.ylabel('Misclassification counts')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# finds missclassified predictions\ndef plot_missclassifications(y_test, y_pred, X_test):\n    '''\n    Goes through y_test and y_pred and plots\n    one digit from each digit class.\n    '''\n    missclassifications = get_digit_missclassifications(y_test, log_reg_pred)\n    found = []\n    missclassified_as = []\n\n    # go through the expected predictions\n    idx = -1\n    for expected, prediction in zip(y_test, y_pred):\n        idx += 1\n\n        # if prediction was valid or we've already found a missclassifcation\n        if (expected == prediction\n            or expected in set(found)\n            or prediction in set(missclassified_as)):\n            continue\n        \n\n        found.append(expected)\n        missclassified_as.append(prediction)\n\n        # print the missclassification information\n        title = '%d missclassified as %d ' % (y_test[idx], y_pred[idx])\n        bar_title = 'missclassification counts for \"%d\"' % expected\n\n        # setting axis labels sucks\n        missclassifications[expected][expected] = -1\n        plot_digit(X_test, idx, missclassifications[expected], title, bar_title)\n\nplot_missclassifications(y_test, log_reg_pred, X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"CORRECT = 'correct'\nTOTAL = 'total'\n\ndef get_digit_prediction_accuracies(y_test, y_pred):\n    '''\n    Gets the prediction accuracy for each digit in the dataset\n    '''\n    digit_range = range(0, 10)\n    digit_mapping = [{ CORRECT: 0, TOTAL: 0 } for _ in digit_range]\n    for actual, prediction in zip(y_test, y_pred):\n        assert actual in set(digit_range)\n        if actual == prediction:\n            digit_mapping[actual][CORRECT] += 1\n        digit_mapping[actual][TOTAL] += 1\n\n    def get_pred(predictions):\n        '''Gets the prediction accuracy.'''\n        return (predictions[CORRECT] / predictions[TOTAL]) # * 100\n\n    return [get_pred(predictions) for predictions in digit_mapping]      \n\ndigit_accuracies = get_digit_prediction_accuracies(y_test, log_reg_pred)\n\n# print the accuracy\nfor digit, accuracy in enumerate(digit_accuracies):\n    print('%d: %.2f%% accuracy' % (digit, accuracy))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# def plot_digit_prediction_accuracies(digit_accuracies):\n#     '''\n#     Plots digit prediction accuracy using the output\n#     from get_digit_prediction_accuracies.\n#     '''\n#     # generate 0, 1, ... 9\n#     digits = [str(digit) for digit in range(0, len(digit_accuracies))]\n\n#     # plot the figure\n#     plt.figure(figsize=(10, 7))\n#     plt.bar(digits, digit_accuracies, align='center', alpha=1)\n#     plt.title('Digit Prediction Accuracies')\n#     plt.xlabel('Digit')\n#     plt.ylabel('Model Prediction Accuracy')\n#     plt.show()\n\n# plot_digit_prediction_accuracies(digit_accuracies)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}