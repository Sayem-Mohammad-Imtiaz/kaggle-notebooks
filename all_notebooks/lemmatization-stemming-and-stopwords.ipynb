{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Lemmatization, Stemming and StopWords for sarcasm detection"},{"metadata":{},"cell_type":"markdown","source":"The goal of this notebook is to try the LogisticRegression model for detecting sarcasm comments of Reddit users. We will try different methods of preprocessing for text features (such as \"stop words\", stemming and lemmatization) and compare results.\nThe notebook consists of the following parts:\n* Loading and preparing data\n* EDA\n* LogisticRegression model\n*    Bag of Words\n*    IfIdf\n*    Stop words\n*    Stemming\n*    Lemmatization\n* Adding non-text features\n* Conclusion"},{"metadata":{},"cell_type":"markdown","source":"## Loadind data"},{"metadata":{},"cell_type":"markdown","source":"First we import the libraries, we will use. Most of them are well-known, and [eli5](https://eli5.readthedocs.io/en/latest/index.html) library is for inspecting model weights."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport eli5\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nimport seaborn as sns\nfrom scipy.sparse import csr_matrix, hstack\nfrom datetime import datetime\nimport string\nfrom nltk.stem import PorterStemmer, WordNetLemmatizer","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"# Load the data. There is a test dataset in the folder, but we cannot use it because we have no answers. Therefore we create train and test sets from the training data\nsarcasm_df = pd.read_csv(\"../input/sarcasm/train-balanced-sarcasm.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Take the look at the data. Here we have not so many features:\n* label - our target feature, we will separate it\n* comment - our main text feature\n* autor - this field should be treaten as categorial\n* subreddit - the theme, where the comment was published\n* score, ups, downs - the number of votes for/against the comment. Accordint to the Reddit faq ups and downs are fuzzed, so we will use only the score field\n* date and created_uts - contains the same information, we will use created_utc\n* parent_commet - we should inspect this field, because it's not clear, if we will use it"},{"metadata":{"trusted":true},"cell_type":"code","source":"sarcasm_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Here we inspect data for missing values\nsarcasm_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We can see, that comment row has missing values: comment 1010773 non-null and it should be 1010825. We will delete these rows completely.\n# We also drop columns 'ups', 'downs', 'date' and convert string 'created_utc' to datetime format\nsarcasm_df.dropna(subset=['comment'], inplace=True)\nsarcasm_df.drop(['ups', 'downs', 'date'], axis=1)\nsarcasm_df['created_utc'] = sarcasm_df['created_utc'].apply(lambda x: datetime.strptime(x, '%Y-%m-%d %H:%M:%S'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now we compare the number of instances for each class (1 - sarcasm, 0 - not). We can see, that the dataset is balanced and classes have almost the same size\nsarcasm_df['label'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Here we split our data for the train and test sets\ntrain_df, test_df, train_y, test_y = train_test_split(sarcasm_df, sarcasm_df['label'], test_size=0.33, random_state=17)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## EDA part"},{"metadata":{},"cell_type":"markdown","source":"Now we will make the dataframe for EDA, we will add the *label* field again. Later we also will add some new features to it, which can help us to explore the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"eda_data = train_df[['comment', 'author', 'subreddit','created_utc', 'score','parent_comment']].copy()\neda_data['label'] = train_y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"eda_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Fist we will look at additional features. Let's display the most sarcastic topics. We filter the data first to save only topics with more than 1000 comments. This may not help to build models, but allow to \"feel\" the data before diving into the modeling. And it's fun :)"},{"metadata":{"trusted":true},"cell_type":"code","source":"filtered = eda_data.groupby(['subreddit']).filter(lambda x: x['comment'].count()>1000)\nfiltered.groupby(['subreddit']).agg({\n    'comment':'count',\n    'label': 'mean'}).sort_values(by='label', ascending=False).iloc[:10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And now the most sarcastic authors with more than 20 comments"},{"metadata":{"trusted":true},"cell_type":"code","source":"filtered_authors = eda_data.groupby(['author']).filter(lambda x: x['comment'].count()>20)\nfiltered_authors.groupby(['author']).agg({\n    'comment':'count',\n    'label': 'mean'}).sort_values(by='label', ascending=False).iloc[:10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's inspect whether there are any relations between the length of the comment and its sarcastic tone. Maybe sarcastic comments tend to be short and laconic. We need to create a new feature for it. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Here we add the new field -  comment_len\neda_data['comment_len'] = eda_data['comment'].apply(len)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here is a function for printing percentiles for the selected feature. For most datasets it's better and more informative to build a boxplot, but in our case it shows us nothing because of outliers. Therefore we will look at percentiles instead"},{"metadata":{"trusted":true},"cell_type":"code","source":"def percentile_print(data, feature, percentile_list = [25, 50, 75]):\n    for percentile in percentile_list:\n        print (\"Percentile\",percentile,\n               \"Sarcasm\", np.percentile(data[data['label']==1][feature], percentile),\n               \"Not\", np.percentile(data[data['label']==0][feature], percentile))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# So we see, that there no difference in comment length for both classes. That'a a bit dissapointing, but we will move forward\npercentile_print(eda_data, 'comment_len')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let's look at she score field. Maybe sarcastic comments are more popular that non, we shall look at the *score* feature to check it."},{"metadata":{"trusted":true},"cell_type":"code","source":"# But unfortunatelly, the situation is the same for scores too: no visible difference.\npercentile_print(eda_data, 'score')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we add two more features: whether the comment was been written on the working day or weekend and was it day or night. Again we're trying to spot any difference, maybe people at night are more sacrastic"},{"metadata":{"trusted":true},"cell_type":"code","source":"eda_data['weekend'] = eda_data['created_utc'].apply(lambda x: x.dayofweek==1 or x.dayofweek==6).astype(int)\neda_data['day']= eda_data['created_utc'].apply(lambda x: x.hour>7 and x.hour<20).astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# So what have we here? Working days don't make users more sarcastic.\nsns.countplot(x='weekend', hue='label', data=eda_data )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# And finally here we can see the difference! Day - that's the time for the sarcasm =)\nsns.countplot(x='day', hue='label', data=eda_data )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now it's time to explore the text features. First let's look at the most popular words for sarcastic comments. We will throw away most common words, like 'a', 'the', 'and' etc. and will concider unigramms and bigramms separately"},{"metadata":{"trusted":true},"cell_type":"code","source":"vectorizer_1 = CountVectorizer(stop_words='english', ngram_range=(1, 1))\nvectorizer_2 = CountVectorizer(stop_words='english', ngram_range=(2, 2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will write a small function, because we don't want to repeat the same code"},{"metadata":{"trusted":true},"cell_type":"code","source":"def freq_words(vectorizer, data):\n    X = vectorizer.fit_transform(data)\n    freqs = zip(vectorizer.get_feature_names(), np.asarray(X.sum(axis=0)).ravel())\n    return sorted(freqs, key = lambda x: x[1], reverse=True)[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# First column - sarcastic comments, second - not\nl = [freq_words(vectorizer_1, eda_data[eda_data['label']==1]['comment']),\n     freq_words(vectorizer_1, eda_data[eda_data['label']==0]['comment'])]\nlist(map(list, zip(*l)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So we can see, that most popular words for both classes are almost the same. The bigramms vectorizer shows us better (more specific) result as you can see below."},{"metadata":{"trusted":true},"cell_type":"code","source":"# First column - sarcastic comments, second - not\nl = [freq_words(vectorizer_2, eda_data[eda_data['label']==1]['comment']),\n     freq_words(vectorizer_2, eda_data[eda_data['label']==0]['comment'])]\nlist(map(list, zip(*l)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now it's time to look at the *parent_comment* feature. It's not clear how to use this information and whether we should use it at all. My suggestion is to find intersection of words for *comment* and *parent_comment* and hope it will show us something =) Maybe sarcastic comments tend to repeat part of a phrase or some words of parent comment? "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Again a small function for finding the intersection. This one does the following:\n# 1) set characters in the string to lowercase, delete punctuation and split for the words\n# 2) the same for the parent comment\n# 3) find words in the comment, that are also in the parent comment\n# 4) returns the rate of intersection length to the length of all words in the comment \n\ndef find_intersection(comment, parent):\n    comment_words = [x.strip(string.punctuation) for x in comment.lower().split()]\n    parent_words = [x.strip(string.punctuation) for x in parent.lower().split()]\n    intersection_words = [x for x in comment_words if x in parent_words]\n    return len(intersection_words)/len(comment_words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now we add this intersection feature to our dataframe and will look at it\neda_data['intersection'] = [find_intersection(x,y) for x,y in zip(eda_data['comment'], eda_data['parent_comment'])]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x = eda_data[eda_data['label']==1]['intersection'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x = eda_data[eda_data['label']==0]['intersection'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"percentile_print(eda_data, 'intersection')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The difference is really small. Anyway we have tried. Now it's time to build our models"},{"metadata":{},"cell_type":"markdown","source":"## Logistic Regression"},{"metadata":{},"cell_type":"markdown","source":"Here we define a LogisticRegression model. First we will compare two methods of converting text features to vectors: CountVectorizer (\"Bag of Words\") and TfIdfVectorizer"},{"metadata":{"trusted":true},"cell_type":"code","source":"log_reg = LogisticRegression(random_state=17, solver='lbfgs')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Function for printing metrics for our predicted result\ndef print_report(model, x_test, y_test):\n    y_pred = model.predict(x_test)\n    report = metrics.classification_report(y_test, y_pred)\n    print(report)\n    print(\"accuracy: {:0.3f}\".format(metrics.accuracy_score(y_test, y_pred)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# This function helps us not to repeat the same lines of code many times. Here we:\n# 1) make pipeline\n# 2) train it\n# 3) print metrics\n# 4) return our trained regression and feature_names, so we will be able to look at the weights\ndef model_cycle(vectorizer, train_x=train_df['comment'], test_x=test_df['comment']):\n    train_vect = vectorizer.fit_transform(train_x)\n    test_vect = vectorizer.transform(test_x)\n    log_reg.fit(train_vect,train_y)\n    print_report(log_reg, test_vect , test_y)\n    return (log_reg, vectorizer.get_feature_names())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### CountVectorizer"},{"metadata":{"trusted":true},"cell_type":"code","source":"# This code takes some time to run, be patient\n(model, features) = model_cycle(CountVectorizer(ngram_range=(1, 3), max_features=100000))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The accuracy is not bad, but but it's just the start. Let's look at the weights of the model to see, which features have greatest weights"},{"metadata":{"trusted":true},"cell_type":"code","source":"eli5.show_weights(model,\n                  feature_names=features,\n                  target_names = ['0','1'],\n                  )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It looks plausible so far. Let's try one more vectorizer TfIdf to extract features."},{"metadata":{},"cell_type":"markdown","source":"### TfIdf"},{"metadata":{"trusted":true},"cell_type":"code","source":"# This code takes some time to run, be patient\n(model, features) = model_cycle(TfidfVectorizer(ngram_range=(1, 3), max_features=100000))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The results of this vectorizer are better. Let's look at the weights"},{"metadata":{"trusted":true},"cell_type":"code","source":"eli5.show_weights(model,\n                  feature_names=features,\n                  target_names = ['0','1'],\n                  )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The most important features here differ from the CountVectorizer method. If we look more careful  features and 'yes because' and 'yeah because' logicaly should be the same feature. Maybe stemming or lemmization can help here, we will check it further"},{"metadata":{},"cell_type":"markdown","source":"### Stop Words"},{"metadata":{},"cell_type":"markdown","source":"There is the build-in vocabulary for english most commom words in sklearn library (stop_words='english'), let's try it"},{"metadata":{"trusted":true},"cell_type":"code","source":"(model, features) = model_cycle(CountVectorizer(ngram_range=(1, 3), stop_words='english', max_features=100000))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"eli5.show_weights(model,\n                  feature_names=features,\n                  target_names = ['0','1'],\n                  )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(model, features) = model_cycle(TfidfVectorizer(ngram_range=(1, 3), stop_words='english', max_features=100000))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"eli5.show_weights(model,\n                  feature_names=features,\n                  target_names = ['0','1'],\n                  )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After applying 'stop words' we can see that accuracy is worse for both vectorizing methods. Maybe the reason lies in the relatively small length of comments and meager vocabulary, therefore when we throw away common words we throw away important information too."},{"metadata":{},"cell_type":"markdown","source":"### Stemming"},{"metadata":{},"cell_type":"markdown","source":"Now it's time to check stemming. This method cuts the ending of the word like this \"house, houses, house’s, houses’ => house\". To test it is not a simple task, because the sklearn library hasn't such build-in method. We will use nltk library instead and write our own CountVectorizer (or we could preprocess our data)."},{"metadata":{"trusted":true},"cell_type":"code","source":"stemmer = PorterStemmer()\nclass StemmedCountVectorizer(CountVectorizer):\n    def build_analyzer(self):\n        analyzer = super(StemmedCountVectorizer, self).build_analyzer()\n        return lambda doc: ([stemmer.stem(w) for w in analyzer(doc)])\n\nvectorizer_s = StemmedCountVectorizer(analyzer=\"word\", ngram_range=(1, 3), max_features=100000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# This code runs realy for a long time. If you want to re-run it be patient.\n(model, features) = model_cycle(vectorizer_s)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The precission here is slightly worse than without stemmer, but the recall for sarcastic comments is better (0.68 instead of 0.67)"},{"metadata":{"trusted":true},"cell_type":"code","source":"eli5.show_weights(model,\n                  feature_names=features,\n                  target_names = ['0','1'],\n                  )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The same for TfIdf"},{"metadata":{"trusted":true},"cell_type":"code","source":"class StemmedTfidfVectorizer(TfidfVectorizer):\n    def build_analyzer(self):\n        analyzer = super(StemmedTfidfVectorizer, self).build_analyzer()\n        return lambda doc: ([stemmer.stem(w) for w in analyzer(doc)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# This code also takes a lot of time. You can make a tea and talk with friends a bit\n(model, features) = model_cycle(StemmedTfidfVectorizer(analyzer=\"word\", ngram_range=(1, 3), max_features=100000))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"eli5.show_weights(model,\n                  feature_names=features,\n                  target_names = ['0','1'],\n                  )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Well. The accuracy of the model with and without stemming is almost the same, and again the recall for sarcastic comments is better (0.69 instead of 0.68). But the time of the calculation takes more time. And we can see, that our problem with doubled features is still here. Now let's try lemmatization"},{"metadata":{},"cell_type":"markdown","source":"### Lemmatization"},{"metadata":{},"cell_type":"markdown","source":"The idea of this method is to bring a wird to it's base form, for example: \"seen => see\", \"drove => drive\". The situation here is the same as with stemming,even more tricky. We need to write tokenizer here and strip the punctuation manually, but finally it works."},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk import word_tokenize\nfrom nltk.corpus import wordnet \n\nclass LemmaTokenizer(object):\n    def __init__(self):\n        self.wnl = WordNetLemmatizer()\n    def __call__(self, articles):\n        return [self.wnl.lemmatize(t,wordnet.VERB) for t in word_tokenize(articles)]\n\nvectorizer = CountVectorizer(tokenizer=LemmaTokenizer(),\n                             strip_accents = 'unicode',\n                             lowercase = True,\n                             ngram_range=(1, 3), max_features=100000)\n\n#stripping punctuation here\ntrain_stripped_comment = train_df['comment'].str.replace('[^\\w\\s]', '')\ntest_stripped_comment = test_df['comment'].str.replace('[^\\w\\s]', '')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(model, features) = model_cycle(vectorizer,train_stripped_comment, test_stripped_comment)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see a small improvement in precission here and rather strange features. Maybe we should remove digits as well? I'll leave it for next the commit. Now we move to the next vectorizer"},{"metadata":{"trusted":true},"cell_type":"code","source":"eli5.show_weights(model,\n                  feature_names=features,\n                  target_names = ['0','1'],\n                  )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tf_vectorizer = TfidfVectorizer(tokenizer=LemmaTokenizer(),\n                                strip_accents = 'unicode',\n                                lowercase = True,\n                                ngram_range=(1, 3), max_features=100000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(model, features) = model_cycle(tf_vectorizer,train_stripped_comment, test_stripped_comment)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"eli5.show_weights(model,\n                  feature_names=features,\n                  target_names = ['0','1'],\n                  )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For now this id the best result: average accuracy 0.725, recall for sarcastic comments 0.69"},{"metadata":{},"cell_type":"markdown","source":"### Non-text features"},{"metadata":{},"cell_type":"markdown","source":"Now let'l try an add other features, maybe we can improve our result. We will not make a pipe here, but prepare our features one by one. Taking into account our previous tests, we will use TfidfVectorizer here with lemma tokenizer."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define vectorizer and convert our text features\ntrain_comment = tf_vectorizer.fit_transform(train_stripped_comment)\ntest_comment = tf_vectorizer.transform(test_stripped_comment)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Subreddit feature should be codded with OneHotEncoder, after it every unique subreddit value will be the separate feature\nenc_sub = OneHotEncoder(handle_unknown='ignore')\ntrain_subreddit = enc_sub.fit_transform(train_df['subreddit'].values.reshape(-1,1))\ntest_subreddit = enc_sub.transform(test_df['subreddit'].values.reshape(-1,1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The same for the author field\nenc_aut = OneHotEncoder(handle_unknown='ignore')\ntrain_author = enc_aut.fit_transform(train_df['author'].values.reshape(-1,1))\ntest_author = enc_aut.transform(test_df['author'].values.reshape(-1,1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We will scale our real-valued features\nscaler = StandardScaler()\ntrain_scores = scaler.fit_transform(train_df['score'].values.reshape(-1,1))\ntest_scores = scaler.transform(test_df['score'].values.reshape(-1,1))\ntrain_len = scaler.fit_transform((train_df['comment'].apply(len)).values.reshape(-1,1))\ntest_len = scaler.transform((test_df['comment'].apply(len)).values.reshape(-1,1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# And finally here we append day, weekend and intersection features\n\ntrain_df['day'] = train_df['created_utc'].apply(lambda x: x.hour>7 and x.hour<20).astype(int)\ntest_df['day'] = test_df['created_utc'].apply(lambda x: x.hour>7 and x.hour<20).astype(int)\n\ntrain_df['intersection'] = [find_intersection(x,y) for x,y in zip(train_df['comment'], train_df['parent_comment'])]\ntest_df['intersection'] = [find_intersection(x,y) for x,y in zip(test_df['comment'], test_df['parent_comment'])]\n\ntrain_df['weekend'] = train_df['created_utc'].apply(lambda x: x.dayofweek==1 or x.dayofweek==6).astype(int)\ntest_df['weekend'] = test_df['created_utc'].apply(lambda x: x.dayofweek==1 or x.dayofweek==6).astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Here we combine all our features in the big sparse matrix\ntrain_sparse = hstack([train_comment, train_subreddit, train_author, train_scores, train_len,\n                       train_df['day'].values.reshape(-1,1), train_df['intersection'].values.reshape(-1,1),\n                       train_df['weekend'].values.reshape(-1,1)]).tocsr()\ntest_sparse = hstack([test_comment, test_subreddit, test_author, test_scores, test_len,\n                      test_df['day'].values.reshape(-1,1), test_df['intersection'].values.reshape(-1,1),\n                      test_df['weekend'].values.reshape(-1,1)]).tocsr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fit the model and print report\nlog_reg.fit(train_sparse, train_y)\nprint_report(log_reg, test_sparse, test_y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"That's disappinting, we haven't improved the best score. But anyway we've tried =)"},{"metadata":{},"cell_type":"markdown","source":"## Conclusion"},{"metadata":{},"cell_type":"markdown","source":"Well, guys, a lot of calculation here and most of them without any improvement in the model's results. Now let's drive the conclusions:\n* The EDA part is not a vital one here, but I still recommend it to \"feel\" the data\n* Using StopWords seems to be not suitable for relatively short text fields, like our comments. In our case it decreased accuracy approximatelly by 0.05\n* TfidfVectorizer with Lemmatization showed the best result here and took slightly less time comparing to the CountVectorizer\n* Non-text features gave us no improvement, but still it was worth trying\n* The model tuning section is missed here, because the notebook is already huge and with many long-time calculatins. I left it aside, but you can do it by yourself you just need to uncomment the code bellow (and spoiler: the best value for C will be 1 - the default one) \n\nThanks for reading!"},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"This is the commented part with parameters tuning\nIn our LogisticRegression model we have only one parameter C\nand we need to make the pipeline so the train and validation data will not blend.\nThis code runs a long time, be ready.\n\"\"\"\n\"\"\"\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import GridSearchCV\n\npipe_logit = make_pipeline(tf_vectorizer, log_reg)\nparam_grid_logit = {'logisticregression__C': np.logspace(-3, 1, 5)}\n\ngrid_logit = GridSearchCV(pipe_logit, \n                          param_grid_logit, \n                          return_train_score=True, \n                          cv=3, n_jobs=-1)\n\ngrid_logit.fit(train_stripped_comment, train_y)\n\ngrid_logit.best_params_, grid_logit.best_score_\n\ngrid_logit.score(test_stripped_comment,test_y)\n\"\"\"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}