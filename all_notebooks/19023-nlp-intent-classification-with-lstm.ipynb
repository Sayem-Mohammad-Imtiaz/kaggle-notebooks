{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"scrolled":true},"cell_type":"code","source":"import pandas as pd\ntrain_data= pd.read_csv('/kaggle/input/atis-airlinetravelinformationsystem/atis_intents_train.csv',\n                       names= [\"target\", \"text\"])\n\ntest_data= pd.read_csv('/kaggle/input/atis-airlinetravelinformationsystem/atis_intents_test.csv',\n                       names= [\"target\", \"text\"])\n\ntrain_data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Data Preprocessing**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.groupby(\"target\").count()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Resample training data**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data= train_data.append(train_data.loc[train_data.target.isin([\"atis_flight_time\", \"atis_quantity\"]), :])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **Target One Hot Encoding**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder as OHE\n\ny_encoder= OHE().fit(np.array(train_data.target).reshape(-1,1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ytr_encoded= y_encoder.transform(np.array(train_data.target).reshape(-1,1)).toarray()\nyts_encoded= y_encoder.transform(np.array(test_data.target).reshape(-1,1)).toarray()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **Text Preprocessing With NLTK and Tensorflow**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Convert text to lowercase**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data[\"lower_text\"]= train_data.text.map(lambda x: x.lower())\ntest_data[\"lower_text\"]= test_data.text.map(lambda x: x.lower())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Word Tokenize**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk import word_tokenize\n\ntrain_data[\"tokenized\"]= train_data.lower_text.map(word_tokenize)\ntest_data[\"tokenized\"]= test_data.lower_text.map(word_tokenize)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Remove Stop Words**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.corpus import stopwords\nfrom string import punctuation\n\ndef remove_stop(strings, stop_list):\n    classed= [s for s in strings if s not in stop_list]\n    return classed\n\nstop= stopwords.words(\"english\")\nstop_punc= list(set(punctuation))+ stop\n\ntrain_data[\"selected\"]= train_data.tokenized.map(lambda df: remove_stop(df, stop_punc))\ntest_data[\"selected\"]= test_data.tokenized.map(lambda df: remove_stop(df, stop_punc))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Stemming**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.stem import PorterStemmer\n\ndef normalize(text):\n    return \" \".join(text)\n\nstemmer= PorterStemmer()\n\ntrain_data[\"stemmed\"]= train_data.selected.map(lambda xs: [stemmer.stem(x) for x in xs])\ntrain_data[\"normalized\"]= train_data.stemmed.apply(normalize)\n\ntest_data[\"stemmed\"]= test_data.selected.map(lambda xs: [stemmer.stem(x) for x in xs])\ntest_data[\"normalized\"]= test_data.stemmed.apply(normalize)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Tokenize with tensorflow**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.preprocessing.text import Tokenizer\n\ntokenizer= Tokenizer(num_words= 10000)\ntokenizer.fit_on_texts(train_data.normalized)\n\ntokenized_train= tokenizer.texts_to_sequences(train_data.normalized)\ntokenized_test= tokenizer.texts_to_sequences(test_data.normalized)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer.word_index.keys().__len__()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Pad Text**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.preprocessing.sequence import pad_sequences\n\ntrain_padded= pad_sequences(tokenized_train, maxlen= 20, padding= \"pre\")\ntest_padded= pad_sequences(tokenized_test, maxlen= 20, padding= \"pre\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_padded.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Create X Matrix (samples, steps, wordlist)**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#this function transform final processed text (columns padded) into 3D matrix (samples, steps, unique_words)\n#matrix contents one hot encoded words. Encoding was done for each step and based on unique words\n\ndef transform_x(data, tokenizer):\n    output_shape= [data.shape[0],\n                  data.shape[1],\n                  tokenizer.word_index.keys().__len__()]\n    results= np.zeros(output_shape)\n    \n    for i in range(data.shape[0]):\n        for ii in range(data.shape[1]):\n            results[i, ii, data[i,ii]-1]= 1\n    return results\n\nxtr_transformed= transform_x(train_padded, tokenizer)\nxts_transformed= transform_x(test_padded, tokenizer)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **LSTM Modelling**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.layers import Dense, LSTM, BatchNormalization, Dropout, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.losses import CategoricalCrossentropy as CC\nfrom tensorflow.keras.activations import relu, softmax\nfrom tensorflow.keras.initializers import he_uniform, glorot_uniform\nfrom tensorflow.keras.metrics import AUC\nfrom tensorflow.keras import Model\nfrom tensorflow.keras.regularizers import l2\n\n\nclass LSTMModel(object):\n    \n    def build_model(self, input_dim, output_shape, steps, dropout_rate, kernel_regularizer, bias_regularizer):\n        input_layer= Input(shape= (steps, input_dim))\n        \n        #make lstm_layer\n        lstm= LSTM(units= steps)(input_layer)\n        dense_1= Dense(output_shape, kernel_initializer= he_uniform(),\n                       bias_initializer= \"zeros\", \n                       kernel_regularizer= l2(l= kernel_regularizer),\n                       bias_regularizer= l2(l= bias_regularizer))(lstm)\n        x= BatchNormalization()(dense_1)\n        x= relu(x)\n        x= Dropout(rate= dropout_rate)(x)\n        o= Dense(output_shape, kernel_initializer= glorot_uniform(),\n                 bias_initializer= \"zeros\", \n                 kernel_regularizer= l2(l= kernel_regularizer), \n                 bias_regularizer= l2(l= bias_regularizer))(dense_1)\n        o= BatchNormalization()(o)\n        output= softmax(o, axis= 1)\n        \n        loss= CC()\n        metrics= AUC()\n        optimizer= Adam()\n        self.model= Model(inputs= [input_layer], outputs= [output])\n        self.model.compile(optimizer= optimizer, loss= loss, metrics= [metrics])\n        \n        \n    def train(self, x, y, validation_split, epochs):\n        self.model.fit(x, y, validation_split= validation_split, epochs= epochs)\n        \n    def predict(self, x):\n        return self.model.predict(x)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Build Model**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"steps= xtr_transformed.shape[1]\ndim= xtr_transformed.shape[2]\noutput_shape= ytr_encoded.shape[1]\n\nmodel= LSTMModel()\nmodel.build_model(input_dim= dim,\n                  output_shape= output_shape,\n                  steps= steps, \n                  dropout_rate= 0.5, \n                  bias_regularizer= 0.3, \n                  kernel_regularizer= 0.3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.train(xtr_transformed, ytr_encoded,\n           0.2, 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Evaluation**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Train","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report\n\nprediction= y_encoder.inverse_transform(model.predict(xtr_transformed))\nprint(classification_report(train_data.target, prediction))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Test","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report\n\nprediction_test= y_encoder.inverse_transform(model.predict(xts_transformed))\nprint(classification_report(test_data.target, prediction_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## case 2","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model.train(xtr_transformed, ytr_encoded,\n           0.2,1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report\n\nprediction= y_encoder.inverse_transform(model.predict(xtr_transformed))\nprint(classification_report(train_data.target, prediction))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report\n\nprediction_test= y_encoder.inverse_transform(model.predict(xts_transformed))\nprint(classification_report(test_data.target, prediction_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Case 3\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model.train(xtr_transformed, ytr_encoded,\n           0.1,0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report\n\nprediction= y_encoder.inverse_transform(model.predict(xtr_transformed))\nprint(classification_report(train_data.target, prediction))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report\n\nprediction_test= y_encoder.inverse_transform(model.predict(xts_transformed))\nprint(classification_report(test_data.target, prediction_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.train(xtr_transformed, ytr_encoded,\n           0.5,0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report\n\nprediction= y_encoder.inverse_transform(model.predict(xtr_transformed))\nprint(classification_report(train_data.target, prediction))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report\n\nprediction_test= y_encoder.inverse_transform(model.predict(xts_transformed))\nprint(classification_report(test_data.target, prediction_test))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}