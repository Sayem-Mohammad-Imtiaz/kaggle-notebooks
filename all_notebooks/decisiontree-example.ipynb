{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Classificazione con alberi di decisione\n\nNel seguente notebook verrà creato un albero di decisione per eseguire una classificazione tra pazienti diabetici e non.\n\n## Dataset\nIl dataset utilizzato è Pima Indians Diabetes Database, contiene dati relativi a 768 pazienti, per ognuno dei quali sono presenti 8 diversi attributi:\n- Pregnancies\t\n- Glucose\t\n- BloodPressure\t\n- SkinThickness\t\n- Insulin\t\n- BMI\t\n- DiabetesPedigreeFunction\n- Age\n- Outcome: la classificazione, 1 per i pazienti diabetici 0 altrimenti\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\n\ndataset = pd.read_csv(\"/kaggle/input/pima-indians-diabetes-database/diabetes.csv\")\ndataset.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# divido i dati relativi agli features (attributi) dalle classi (Outcome)\nfeature_cols = ['Pregnancies', 'Insulin', 'BMI', 'Age','Glucose','BloodPressure','DiabetesPedigreeFunction']\nx = dataset[feature_cols] # features\ny = dataset.Outcome # classi\n# divido il dataset in training set e test set\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=1) # 70% training and 30% test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# creo il classificatore DecisionTreeClassifier\ndtc = DecisionTreeClassifier()\n# training del classificatore\ndtc = dtc.fit(x_train,y_train)\n# eseguo la predizione dei dati del test set\ny_pred = dtc.predict(x_test)\n# calcolo l'accuracy\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Visualizzazione dell'albero ottenuto\nIl seguente codice si occupa di mostrare l'albero di decisione creato. La funzione export_graphviz si occupa di traformare il classificatore (dtc) in un file dot che succesivamente viene convertito in un immagine grazie al package pydotplus."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import export_graphviz\nfrom sklearn.externals.six import StringIO  \nfrom IPython.display import Image \n!pip install pydotplus\nimport pydotplus\n\ndot_data = StringIO()\nexport_graphviz(dtc, out_file=dot_data,  \n                filled=True, rounded=True,\n                special_characters=True,feature_names = feature_cols,class_names=['0','1'])\ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \ngraph.write_png('diabetes.png')\nImage(graph.create_png())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Possiamo notare come l'albero ottenuto sia molto esteso e di difficile comprensione, ora procederemo ad ottimizzare le performance del classificatore."},{"metadata":{},"cell_type":"markdown","source":"## Ottimizzazione\nPossiamo incrementare le performance del classificatore cambiando alcuni parametri, analizzando la [documentazione](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html), troviamo i seguneti parametri per il classificatore:\n\n- criterion : consente di utilizzare un criterio differente per la selezione degli attributi. Sono disponibili i valori “gini” per scegliere gli attributi in base al Gini index e “entropy” per scegliere gli attributi in base all'information gain.\n\n- splitter : consente di scegliere la strategia di divisione (split strategy). I valori disponibili sono “best” per scegliere una strategia ottima o “random” per scegliere una strategia random.\n\n- max_depth : Questo paramtro permette di impostare un limite alla profondità massima dell'albero. Se viene settato a None, allora i nodi vengono espansi fino a trovare foglie pure. Utilizzando valori alti, quindi sviluppando l'abero in profondità rischiamo l'overfitting, mentre con valori troppo bassi rischiamo di ottenere un classificatore sottospecializzato (underfitting)."},{"metadata":{"trusted":true},"cell_type":"code","source":"# creo il classificatore DecisionTreeClassifier\ndtc_optimized = DecisionTreeClassifier(criterion=\"entropy\", max_depth=3)\n# training del classificatore\ndtc_optimized = dtc_optimized.fit(x_train,y_train)\n# eseguo la predizione dei dati del test set\ny_pred = dtc_optimized.predict(x_test)\n# calcolo l'accuracy per il classificatore ottimizzato\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndot_data = StringIO()\nexport_graphviz(dtc_optimized, out_file=dot_data,  \n                filled=True, rounded=True,\n                special_characters=True, feature_names = feature_cols,class_names=['0','1'])\ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \ngraph.write_png('diabetes.png')\nImage(graph.create_png())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Possiamo notare come sia migliorata la situazione, potando l'albero abbiamo incrementato l'accuracy circa del 10%. "}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}