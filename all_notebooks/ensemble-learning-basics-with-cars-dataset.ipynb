{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n        \n# https://www.kaggle.com/hoonkaiwei/ensemble-learning-basics-with-cars-dataset\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <span style=\"color:red;\">This is only a draft, Final to be exported in a folder with dataset</span>"},{"metadata":{},"cell_type":"markdown","source":"# Week 4 Co-Learning: Ensemble Methods with Cars Dataset \n\n___\n\nSo after covering the theory behind ensemble methods, let us dive into the coding segments of today's co-learning \n\n___ \n\nWe'll first have a quick recap on how to implement decision trees in Python. After that we'll move on to some data cleaning and preparation, followed by implementations of various ensemble methods and tweaking of their parameters. \n\n___\n\nNow we'll first import all the necessary modules and packages. \n\nWe'll be using Scikit-Learn as our main library for all the various classifier models we'll work with today "},{"metadata":{"trusted":true},"cell_type":"code","source":"# General modules\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib\nfrom matplotlib.legend_handler import HandlerLine2D \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom math import ceil\n\n# Required Classifiers from SKLearn \n\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nfrom sklearn.linear_model import LogisticRegression \nfrom sklearn.tree import DecisionTreeClassifier, export_graphviz, plot_tree\nfrom sklearn.ensemble import BaggingClassifier, RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, VotingClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score, mean_squared_error\nfrom sklearn.model_selection import train_test_split \n\n# Configuring Matplotlib \n%matplotlib inline ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Let's have a quick recap of Descision Trees\n\nWe will use the iris dataset for a quick recap. \n\nThe iris dataset contains various features, such as petal length, petal width and sepal width, with the target being classifying a given iris flower under its correct species. \n\nWe would use petal length and petal width as features to predict the species of iris flower for our recap on decision trees. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing the dataset \n\nfrom sklearn.datasets import load_iris","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Loading the dataset\n\niris = load_iris()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Selecting only the desired features of petal width and petal length for this recap\n# Experiment with a only two levels in the tree (not including root node)\n\nX = iris.data[:,2:]\ny = iris.target \n\niris_tree = DecisionTreeClassifier(max_depth=2)\niris_tree.fit(X, y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So actually creating and fitting a decision tree is relatively simple. \n\nEarlier we mentioned that at each non-leaf node, the decision tree makes a decision that splits the samples into corresponding children nodes. If we want to know what decision each node made, we can actually visualize it. "},{"metadata":{},"cell_type":"markdown","source":"## Visualizing the decision tree\n\n___ \n\nLet's display the decision tree in a more visually accessible format. \n\n___ \n\nWe can do this in 2 ways, but let's first use the method the textbook advocates."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Method 1 (textbook): Using export_graphviz\n# Using export_graphviz to visualize the decision tree\n\nexport_graphviz(iris_tree, out_file='iris_tree.dot',feature_names=iris.feature_names[2:], class_names=iris.target_names, rounded=True, filled=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!dot -Tpng iris_tree.dot -o iris_tree.png","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Loading the image of the visualized decision tree to display it using Matplotlib\n\nfig, axes = plt.subplots(figsize = (20, 8))\niris_img = img_to_array(load_img('./iris_tree.png'))\nplt.suptitle(\"Decision tree structure\")\nplt.imshow(iris_img.astype('uint8')) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Method 2 (by Scikit-Learn): Using plot_tree()\n\nfig, axes = plt.subplots(figsize = (20, 8))\nplot_tree(iris_tree, filled=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From this image, we can see that Sklearn's Decision Tree Algorithm, CART, is a greedy algorithm that tries to place as many of one class in one children node as possible at each decision node split"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot the decision boundary\n\nfig, axes = plt.subplots(figsize = (20, 8))\n\n# Determine the axes' units and ranges\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),np.arange(y_min, y_max, 0.02))\nplt.tight_layout(h_pad=1.0, w_pad=1.0, pad=2.5)\n\n# Plotting the decision boundaries\nZ = iris_tree.predict(np.c_[xx.ravel(), yy.ravel()])\nZ = Z.reshape(xx.shape)\ncs = plt.contourf(xx, yy, Z, cmap=plt.cm.RdYlBu)\n\n# Determining the axes labels\nplt.xlabel(iris.feature_names[2])\nplt.ylabel(iris.feature_names[3])\n\n# Plotting the training points in the graph\nfor i, color in zip(range(3), 'ryb'):\n    idx = np.where(y == i)\n    plt.scatter(X[idx, 0], X[idx, 1], c=color, label=iris.target_names[i], cmap=plt.cm.RdYlBu, edgecolor='black', s=15)\n\nplt.suptitle(\"Decision surface of a decision tree\")\nplt.legend(loc='lower right', borderpad=0, handletextpad=0)\nplt.axis(\"tight\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In this sense of being able to see how the model decides, decision trees are \\_\\_\\_\\_\\_\\_\\_\\_ models unlike Neural Networks"},{"metadata":{},"cell_type":"markdown","source":"## Adding more layers\n\nInstead of having 2 layers, excluding the root node, let's try a decision tree with 3 layers. We'll then visualize what differences more layers create in our model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fitting again with 3 layers instead \n# ICE \n\niris_tree = DecisionTreeClassifier(max_depth=3)\niris_tree.fit(X, y)\n\nfig, axes = plt.subplots(figsize = (20, 8))\nplot_tree(iris_tree, filled=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting the new decision boundaries\n# ICE\n\nfig, axes = plt.subplots(figsize = (20, 8))\n\n# Determine the axes' units and ranges\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),np.arange(y_min, y_max, 0.02))\nplt.tight_layout(h_pad=1.0, w_pad=1.0, pad=2.5)\n\n# Plotting the decision boundaries\nZ = iris_tree.predict(np.c_[xx.ravel(), yy.ravel()])\nZ = Z.reshape(xx.shape)\ncs = plt.contourf(xx, yy, Z, cmap=plt.cm.RdYlBu)\n\n# Determining the axes labels\nplt.xlabel(iris.feature_names[2])\nplt.ylabel(iris.feature_names[3])\n\n# Plotting the training points in the graph\nfor i, color in zip(range(3), 'ryb'):\n    idx = np.where(y == i)\n    plt.scatter(X[idx, 0], X[idx, 1], c=color, label=iris.target_names[i], cmap=plt.cm.RdYlBu, edgecolor='black', s=15)\n\nplt.suptitle(\"Decision surface of a decision tree\")\nplt.legend(loc='lower right', borderpad=0, handletextpad=0)\nplt.axis(\"tight\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The model becomes slightly more precise but this can easily become a case of \\_\\_\\_\\_\\_\\_\\_\\_ which leads to \\_\\_\\_\\_\\_\\_\\_\\_ "},{"metadata":{},"cell_type":"markdown","source":"## Tweaking other hyper parameters \n\nWhat are the other hyperparameters we can tweak? "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Tweaking the other hyperparameters\n# ICE \n\niris_tree = DecisionTreeClassifier(max_depth=3)\niris_tree.fit(X, y)\n\nfig, axes = plt.subplots(figsize = (20, 8))\nplot_tree(iris_tree, filled=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting the new decision boundaries\n# ICE\n\nfig, axes = plt.subplots(figsize = (20, 8))\n\n# Determine the axes' units and ranges\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),np.arange(y_min, y_max, 0.02))\nplt.tight_layout(h_pad=1.0, w_pad=1.0, pad=2.5)\n\n# Plotting the decision boundaries\nZ = iris_tree.predict(np.c_[xx.ravel(), yy.ravel()])\nZ = Z.reshape(xx.shape)\ncs = plt.contourf(xx, yy, Z, cmap=plt.cm.RdYlBu)\n\n# Determining the axes labels\nplt.xlabel(iris.feature_names[2])\nplt.ylabel(iris.feature_names[3])\n\n# Plotting the training points in the graph\nfor i, color in zip(range(3), 'ryb'):\n    idx = np.where(y == i)\n    plt.scatter(X[idx, 0], X[idx, 1], c=color, label=iris.target_names[i], cmap=plt.cm.RdYlBu, edgecolor='black', s=15)\n\nplt.suptitle(\"Decision surface of a decision tree\")\nplt.legend(loc='lower right', borderpad=0, handletextpad=0)\nplt.axis(\"tight\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Gini index vs. Entropy\n\n___\n\nPreviously Jason mentioned that there are 2 ways that a decision tree can decide to split its node by: \n  1. Gini index\n  2. Entropy \n\nNow let's see what difference using each makes. We'll comparing between to models of 3 layers"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Gini-using model\n# ICE\n\nfig, axes = plt.subplots(figsize = (20, 8))\nplot_tree(iris_tree, filled=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Entropy-using model\n# ICE\n\niris_tree = DecisionTreeClassifier(criterion='entropy', max_depth=3)\niris_tree.fit(X, y)\n\nfig, axes = plt.subplots(figsize = (20, 8))\nplot_tree(iris_tree, filled=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As can be seen from this example, there is only minor differences between the models --- the result is the same in this case but the decision criterias are somewhat different \n\n___\n\nThe models we have built so far are used for classification problems, due to the nature of the dataset, but decision trees can also be used for regression problems. In that case, the __DecisionTreeRegressor__ class is used, the loss measured is \\_\\_\\_\\_\\_\\_\\_\\_"},{"metadata":{},"cell_type":"markdown","source":"## Now, we'll move on to Ensemble Learning methods proper"},{"metadata":{},"cell_type":"markdown","source":"# Ensemble Learning \n\n### We'll go through the implementation of the various ensemble methods that were introduced in the following part, namely: \n  1. Voting Classifier\n  2. Bagging Classifier\n  3. Random Forests\n  4. Adaboost\n  5. Gradient Boosting \n\n___\n\nBut as always, before we start fitting the models, we have to clean and prepare the data, as well as, conduct some basic Exploratory Data Analysis (EDA). "},{"metadata":{},"cell_type":"markdown","source":"## Data Cleaning, EDA and Data Preparation\n\nData Cleaning: \n  1. Reviewing cars data and data types\n  2. Removing null values\n\nEDA: \n  1. Looking at the statistical summary of our data\n  2. Observing correlation in our data\n  3. Observing spread of each feature in our data\n  4. Observing the distribution of our target class\n\nData Preparation: \n  1. Split into training and testing datasets"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Loading our data from storage \n\ndf = pd.read_csv('../input/carsdata/cars.csv', skipinitialspace=True, na_values=' ')\n# Skipinitialspace remove whitespaces before titles and na_values determines what are considered null values ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Reviewing Cars_data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Initial look at data\n\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Statistical summary of our data\n\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seems our dataset contains quite a few null values, let's take a look at the null values and determine whether to fill or drop them "},{"metadata":{},"cell_type":"markdown","source":"### Dropping null values "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check null values in 'cubicinches' column\n# ICE\n\ndf['cubicinches'][pd.isna(df['cubicinches'])]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check null values in 'weightlbs' column\n# ICE\n\ndf['weightlbs'][pd.isna(df['weightlbs'])]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Given the small number of null records compared to the whole dataset and the difficulty in giving a suitable fill value for these null values, we should proceed to drop them. "},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.dropna(how='any')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us have a quick look at the dataset again"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Observing the correlation of the data, the distribution of features and target classes"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting correlation map\n\nfig, axes = plt.subplots(figsize = (12, 6))\n\ncorrelation = df.corr()\n\ncorr_m = sns.heatmap(round(correlation, 2), annot=True, cmap='Blues', ax=axes, fmt='.2f')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Correlation of data can be used to help reduce the dimensionality of models in some cases by removing features that are redundant (of very high correlation)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Function to help plot the distribution of our features\n\ndef features_hist(df, features, fig_size, xsize=8, ysize=8):\n    df[features].hist(bins=20, xlabelsize=xsize, ylabelsize=ysize, grid=False, figsize=fig_size,color='blue')\n\n    plt.tight_layout(rect=(0,0,1.2,1.2))\n\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Separating the dataframe into features and labels\n# ICE \n\nX = df.iloc[:,:7] \n\nX","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = df['brand']\n\ny","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Features of our dataset\n\nfeatures = list(df.columns[:7])\n\nfeatures","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features_hist(df, features, (20,8))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features_hist(df, 'brand', (20, 8), xsize=20, ysize=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['brand'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Now let us split the cars dataset into training and test datasets"},{"metadata":{"trusted":true},"cell_type":"code","source":"# train_perc = 0.85 \n\n# train_index = ceil(len(df) * 0.85)\n# print(train_index)\n\n# X_train = X.iloc[:train_index] \n# X_test = X.iloc[train_index:]\n\n# y_train = y[:train_index]\n# y_test = y[train_index:]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"decisiontree_results = []","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cars_tree = DecisionTreeClassifier(min_samples_leaf=7)\n\ncars_tree.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"decisiontree_results.append(cars_tree.score(X_test, y_test))\n\nprint(decisiontree_results)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Voting Classifier\n\nBefore we delve into specific ensemble techniques, let us construct a basic ensemble method model from scratch, using a voting classifier, as covered by Jiaying earlier. We'll be using a hard voting classifier. \n\n___\n\nAs mentioned earlier, a hard-voting classifier is made up of a collection of \\_\\_\\_\\_\\_\\_\\_\\_, with the final decision being decided by \\_\\_\\_\\_\\_\\_\\_\\_. \n\n___\n\nWe'll be using a logistic regression model, a decision tree classifier and a k-nearest neighbour classifier in our Voting classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Forming the hard-voting classifier \n\nvoter = VotingClassifier(estimators=[('lr', LogisticRegression()), ('dt', DecisionTreeClassifier()), ('svc', SVC())], voting='hard', n_jobs=-1)\n\n# Fitting the VotingClassifier \n\nresults = [] \n\nvoter.fit(X_train, y_train)\n\n# VotingClassifier accuracy\n\nresults.append(f\"VotingClassifier Accuracy: {accuracy_score(y_test, voter.predict(X_test))}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fitting individual classifiers\n\nlog_reg = LogisticRegression()\ndec_tree = DecisionTreeClassifier(max_depth=3)\nsvc = SVC() \n\nlog_reg.fit(X_train, y_train)\ndec_tree.fit(X_train, y_train)\nsvc.fit(X_train, y_train)\n\n# Checking accuracy of different classifiers\n\nresults.append(f\"LogisticRegression Accuracy: {accuracy_score(y_test, log_reg.predict(X_test))}\")\nresults.append(f\"DecisionTreeClassifier Accuracy: {accuracy_score(y_test, dec_tree.predict(X_test))}\")\nresults.append(f\"Support Vector Machine Accuracy: {accuracy_score(y_test, svc.predict(X_test))}\")\n\nfor result in results: \n    print(result)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"If we compare the accuracy of the Voting Classifier, we can see the difference an ensemble of weak learners can make. \n\n--- \n\nIf we used soft-voting instead, which means \\_\\_\\_\\_\\_\\_\\_\\_, the accuracy would potentially be even higher. "},{"metadata":{},"cell_type":"markdown","source":"## Bagging Classifier\n\nIn using a Bagging Classifier, we use random sampling to train various weak learners, forming an ensemble. The randomness of the sample each learner fits to help make better and more accurate predictions. \n\n___ \n\nThe BaggingClassifier class can be used for both bagging and pasting, we just need to change the parameter \\_\\_\\_\\_\\_\\_\\_\\_"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fitting a BaggingClassifier\n# ICE \n\nbag = BaggingClassifier(DecisionTreeClassifier(), n_estimators=100, max_samples=100, bootstrap=True, n_jobs=-1)\n\nbag.fit(X_train, y_train)\n\n# ICE \n\nprint(f\"Accuracy: {accuracy_score(y_test, bag.predict(X_test))}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"If we're just using a bagging classifier with decision trees as its weak learners, we can just dirctly use the RandomForestClassifier, which is essentially an ensemble method working in a similar way. \n\n___\n\nBagging classifiers can, of course, use different weak learner models. "},{"metadata":{},"cell_type":"markdown","source":"## Bagging - Random Forests\n\nNow let's have a look at Bagging ensemble methods using Random Forests"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fitting a Random Forest model\n\nrf = RandomForestClassifier()\nrf.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Random Forest accuracy\n\ny_pred = rf.predict(X_test)\nprint(f\"accuracy score: {accuracy_score(y_test, y_pred)}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can actually access the importance of a feature across all the estimators trained in a random forest using sklearn's feature_importances_ variable"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Print feature importance\n\nfor name, importance in zip(features, rf.feature_importances_): \n    print(name, importance)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's tweak some hyperparameters to see the differences in performance. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Results for different hyperparameters\n# ICE \n\nn_estimators = [5, 10, 50, 100, 150, 300]\ntrain_results = []\ntest_results = []\n\nfor estimator in n_estimators: \n    rf = RandomForestClassifier(n_estimators=estimator, n_jobs=-1)\n    rf.fit(X_train, y_train)\n    \n    train_pred = rf.predict(X_train)\n    train_results.append(accuracy_score(y_train, train_pred))\n    test_pred = rf.predict(X_test)\n    test_results.append(accuracy_score(y_test, test_pred)) \n\nprint('Train\\t\\t\\tTest')\nfor i in range(len(n_estimators)):\n    print(f\"{i+1}: {round(train_results[i], 5):.5f}\\t\\t{round(test_results[i],5):.5f}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Graphical representation\n\nline1 = plt.plot(n_estimators, train_results, 'b', label='Training')\nline2 = plt.plot(n_estimators, test_results, 'r', label='Testing') \nplt.legend(handler_map={'line1':HandlerLine2D(numpoints=2)})\nplt.xlabel('n_estimators')\nplt.ylabel('accuracy')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Boosting - AdaBoost \n\nFor boosting techniques, we have 2 common kinds, the first will be AdaBoost. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fitting an AdaBoost model\n\nam = AdaBoostClassifier(n_estimators=100, learning_rate=1)\nadaboostmodel = am.fit(X_train, y_train)\n\ny_pred = adaboostmodel.predict(X_test)\nprint(f\"accuracy score: {accuracy_score(y_test, y_pred)}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Tweaking hyperparameters\n# ICE \n\nn_estimators = [5, 10, 50, 100, 150, 300]\ntrain_results = []\ntest_results = []\n\nfor estimator in n_estimators: \n    am = AdaBoostClassifier(n_estimators=estimator, learning_rate=1)\n    adamodel = am.fit(X_train, y_train)\n    \n    train_pred = adamodel.predict(X_train)\n    train_results.append(accuracy_score(y_train, train_pred))\n    test_pred = adamodel.predict(X_test)\n    test_results.append(accuracy_score(y_test, test_pred)) \n\nprint('Train\\t\\t\\tTest')\nfor i in range(len(n_estimators)):\n    print(f\"{i+1}: {round(train_results[i], 5):.5f}\\t\\t{round(test_results[i],5):.5f}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"line1 = plt.plot(n_estimators, train_results, 'b', label='Training')\nline2 = plt.plot(n_estimators, test_results, 'r', label='Testing') \nplt.legend(handler_map={'line1':HandlerLine2D(numpoints=2)})\nplt.xlabel('n_estimators')\nplt.ylabel('accuracy')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"How about tweaking the learning rate? "},{"metadata":{"trusted":true},"cell_type":"code","source":"# ICE\n\nn_lr = [0.4, 0.8, 1.0, 1.4, 1.8, 2.0]\ntrain_results = []\ntest_results = []\n\nfor lr in n_lr: \n    am = AdaBoostClassifier(n_estimators=300, learning_rate=lr)\n    adamodel = am.fit(X_train, y_train)\n    \n    train_pred = adamodel.predict(X_train)\n    train_results.append(accuracy_score(y_train, train_pred))\n    test_pred = adamodel.predict(X_test)\n    test_results.append(accuracy_score(y_test, test_pred)) \n\nprint('Train\\t\\t\\tTest')\nfor i in range(len(n_lr)):\n    print(f\"{i+1}: {round(train_results[i], 5):.5f}\\t\\t{round(test_results[i],5):.5f}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"line1 = plt.plot(n_lr, train_results, 'b', label='Training')\nline2 = plt.plot(n_lr, test_results, 'r', label='Testing') \nplt.legend(handler_map={'line1':HandlerLine2D(numpoints=2)})\nplt.xlabel('n_lr')\nplt.ylabel('accuracy')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Boosting - Gradient Boosting\n\nThe other boosting technique is Gradient Boosting. \n\n___\n\nWe'll be using a Classifier instead of a Regressor due to the nature of our dataset, but both regressors and classifiers can use the gradient boosting method."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fitting the Gradient Boosting model\n\ngm = GradientBoostingClassifier(max_depth=2, n_estimators=10, learning_rate=1)\ngradboostmodel = gm.fit(X_train, y_train)\n\ny_pred = gradboostmodel.predict(X_test)\nprint(accuracy_score(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Tweaking the hyperparameters\n# ICE\n\nn_estimators = [5, 10, 50, 100, 150, 300]\ntrain_results = []\ntest_results = []\n\nfor estimator in n_estimators: \n    gm = GradientBoostingClassifier(max_depth = 2, n_estimators=estimator, learning_rate=1)\n    gradmodel = gm.fit(X_train, y_train)\n    \n    train_pred = gradmodel.predict(X_train)\n    train_results.append(accuracy_score(y_train, train_pred))\n    test_pred = gradmodel.predict(X_test)\n    test_results.append(accuracy_score(y_test, test_pred)) \n\nprint('Train\\t\\t\\tTest')\nfor i in range(len(n_estimators)):\n    print(f\"{i+1}: {round(train_results[i], 5):.5f}\\t\\t{round(test_results[i],5):.5f}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"line1 = plt.plot(n_estimators, train_results, 'b', label='Training')\nline2 = plt.plot(n_estimators, test_results, 'r', label='Testing') \nplt.legend(handler_map={'line1':HandlerLine2D(numpoints=2)})\nplt.xlabel('n_estimators')\nplt.ylabel('accuracy')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Tweaking the learning rate\n# ICE\n\nn_lr = [0.4, 0.8, 1.0, 1.4, 1.8, 2.0]\ntrain_results = []\ntest_results = []\n\nfor lr in n_lr: \n    gm = GradientBoostingClassifier(max_depth = 2, n_estimators=100, learning_rate=lr)\n    gradmodel = gm.fit(X_train, y_train)\n    \n    train_pred = gradmodel.predict(X_train)\n    train_results.append(accuracy_score(y_train, train_pred))\n    test_pred = gradmodel.predict(X_test)\n    test_results.append(accuracy_score(y_test, test_pred)) \n\nprint('Train\\t\\t\\tTest')\nfor i in range(len(n_lr)):\n    print(f\"{i+1}: {round(train_results[i], 5):.5f}\\t\\t{round(test_results[i],5):.5f}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"line1 = plt.plot(n_lr, train_results, 'b', label='Training')\nline2 = plt.plot(n_lr, test_results, 'r', label='Testing') \nplt.legend(handler_map={'line1':HandlerLine2D(numpoints=2)})\nplt.xlabel('n_lr')\nplt.ylabel('accuracy')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# That's all for today's Co-Learning session and hope this session was useful for you all!"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}