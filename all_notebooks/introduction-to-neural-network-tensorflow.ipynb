{"nbformat_minor":1,"metadata":{"language_info":{"codemirror_mode":{"version":3,"name":"ipython"},"name":"python","pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.1","file_extension":".py","mimetype":"text/x-python"},"kernelspec":{"display_name":"Python 3","name":"python3","language":"python"}},"cells":[{"outputs":[],"metadata":{"collapsed":true,"_uuid":"7d1937a1280c3dcb1a06ec5f62a0487a0129e627","_cell_guid":"cb5554e6-96c9-46fc-a300-3ee68d52aba6","trusted":true,"_execution_state":"idle"},"cell_type":"code","execution_count":null,"source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import metrics\nfrom sklearn.decomposition import PCA"},{"outputs":[],"metadata":{"collapsed":true,"trusted":true,"_uuid":"fccc431f8c8d1e624f3d2cebcd43870fbf8d15f3","_execution_state":"idle"},"cell_type":"code","execution_count":null,"source":"dataset = pd.read_csv('../input/data.csv')\ndataset = dataset.iloc[:, 1:-1]\ndataset.head()"},{"outputs":[],"metadata":{"_uuid":"b46a5a55c9e3c89fef69c0912d685b7d61f499ce"},"cell_type":"markdown","execution_count":null,"source":"Do we need to normalize it?"},{"outputs":[],"metadata":{"collapsed":true,"trusted":true,"_uuid":"9964c6e50f520f73b492df8861b522b9ce9c41ab","_execution_state":"idle"},"cell_type":"code","execution_count":null,"source":"from sklearn.preprocessing import LabelEncoder\n\ndiagnosis_unique, diagnosis_count = np.unique(dataset['diagnosis'].values, return_counts = True)\n\nfor i in range(diagnosis_unique.shape[0]):\n    print (diagnosis_unique[i], ': ', diagnosis_count[0])"},{"outputs":[],"metadata":{"collapsed":true,"trusted":true,"_uuid":"97966f969a259d63b953fc1b120d369ef592aba1","_execution_state":"idle"},"cell_type":"code","execution_count":null,"source":"dataset['diagnosis'] = LabelEncoder().fit_transform(dataset['diagnosis'])\ncorrelation = dataset.corr()\nplt.figure(figsize = (20, 20))\nsns.heatmap(correlation, vmax = 1, square = True, annot = False)\nplt.show()"},{"outputs":[],"metadata":{"_uuid":"64e5a6cb8d321c3b2ab7e4e743885dc543329fe3"},"cell_type":"markdown","execution_count":null,"source":"Now I want to analyse the component of our dataset"},{"outputs":[],"metadata":{"collapsed":true,"trusted":true,"_uuid":"6cb9fa1528720daf19b92dab88612ba0598a6c44","_execution_state":"idle"},"cell_type":"code","execution_count":null,"source":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import Normalizer\n\n# change into numpy form for neural network use later    \ndataset_matrix = dataset.values\ndataset_matrix[:, 0] = LabelEncoder().fit_transform(dataset_matrix[:, 0])\nlabel_matrix = dataset_matrix[:, 0]\ndataset_matrix = dataset_matrix[:, 1:]\n\nnormalize_dataset_matrix = Normalizer().fit_transform(dataset_matrix)\nstd_normalize_dataset = StandardScaler().fit_transform(normalize_dataset_matrix)\n\nmean_vec = np.mean(std_normalize_dataset, axis = 0)\ncov_mat = (std_normalize_dataset - mean_vec).T.dot((std_normalize_dataset - mean_vec)) / (std_normalize_dataset.shape[0] - 1)\neig_vals, eig_vecs = np.linalg.eig(cov_mat)"},{"outputs":[],"metadata":{"collapsed":true,"trusted":true,"_uuid":"9db7caab9709630000e88316eeea42b409be845f","_execution_state":"idle"},"cell_type":"code","execution_count":null,"source":"eig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:,i]) for i in range(len(eig_vals))]\neig_pairs.sort(key = lambda x: x[0], reverse=True)\ntot = sum(eig_vals)\nvar_exp = [(i / tot) * 100 for i in sorted(eig_vals, reverse = True)]\ncum_var_exp = np.cumsum(var_exp)\nplt.figure(figsize = (10, 5))\nplt.bar(range(len(eig_pairs)), var_exp, alpha = 0.5, align = 'center', label = 'individual explained variance')\nplt.step(range(len(eig_pairs)), cum_var_exp, where = 'mid', label = 'cumulative explained variance')\nplt.ylabel('Explained variance ratio')\nplt.xlabel('Principal components')\nplt.legend(loc = 'best')\nplt.tight_layout()\nplt.show()"},{"outputs":[],"metadata":{"_uuid":"61b5d3bcae80c20763df56923899ca8887cd393b"},"cell_type":"markdown","execution_count":null,"source":"I will take 5 components, contain atleast 90% of dataset"},{"outputs":[],"metadata":{"collapsed":true,"trusted":true,"_uuid":"c8902825d59c87b2d6e02ae55ca895374e4da4d1","_execution_state":"idle"},"cell_type":"code","execution_count":null,"source":"x_5d = PCA(n_components = 5).fit_transform(std_normalize_dataset)\ncolors = ['b', 'r']\nfor n, i in enumerate(np.unique(label_matrix)):\n    plt.scatter(x_5d[:,0][label_matrix == i], x_5d[:,1][label_matrix == i], c = colors[n], label = diagnosis_unique[n], alpha = 0.7)\nplt.legend()\nplt.show()"},{"outputs":[],"metadata":{"_uuid":"7c4216efb5813c2ed7559a57f9c3237eaa668226"},"cell_type":"markdown","execution_count":null,"source":"Below I will create 3 types of feed-forward Neural Network:\n\nAll learning rate = 0.001, beta_l2 = 0.00005, beta1 = default, beta2 = default, batch_size = 32\n\ninput layer = [batch_size, x_shape]\n\nfirst hidden layer = [x_shape, 512]\n\nsecond hidden layer = [512, 256]\n\nthird hidden layer = [256, 128]\n\noutput layer = [128, y_shape]\n\n1. 3 Hidden layers, all RELU as activation functions except softmax for last one, cross entropy for cost function, adaptive plus square root optimization stochastic gradient descent\n2. 3 Hidden layers, dropout with 0.5, l2 norm, all RELU as activation functions except softmax for last one, cross entropy for cost function, adaptive plus square root optimization stochastic gradient descent\n3. 3 Hidden layers, dropout with 0.5, l2 norm, batch normalization, all RELU as activation functions except softmax for last one, cross entropy for cost function, adaptive plus square root optimization stochastic gradient descent"},{"outputs":[],"metadata":{"_uuid":"a79454cdd545c24d6bea3ba1fce59bb0144888db"},"cell_type":"markdown","execution_count":null,"source":"But why I chose RELU as activation function? There is no upper boundary in RELU, a very simple function f(x) = max(x, 0)\n\n![](http://cs231n.github.io/assets/nn1/relu.jpeg)\n\nbecause it can go up until infinity, derivative of it is 1"},{"outputs":[],"metadata":{"collapsed":true,"trusted":true,"_uuid":"a0f95d7665f986c70a5b49fa445e37d0b94a4bb0","_execution_state":"idle"},"cell_type":"code","execution_count":null,"source":"class first_network:\n    def __init__(self, learning_rate, x_shape, y_shape):\n        self.X = tf.placeholder(\"float\", [None, x_shape])\n        self.Y = tf.placeholder(\"float\", [None, y_shape])\n        \n        hidden1 = tf.Variable(tf.random_normal([x_shape, 512]))\n        hidden2 = tf.Variable(tf.random_normal([512, 256]))\n        hidden3 = tf.Variable(tf.random_normal([256, 128]))\n        output = tf.Variable(tf.random_normal([128, y_shape]))\n\n        hidden_bias1 = tf.Variable(tf.random_normal([512], stddev = 0.1))\n        hidden_bias2 = tf.Variable(tf.random_normal([256], stddev = 0.1))\n        hidden_bias3 = tf.Variable(tf.random_normal([128], stddev = 0.1))\n        output_bias = tf.Variable(tf.random_normal([y_shape], stddev = 0.1))\n        \n        feedforward1 = tf.nn.relu(tf.matmul(self.X, hidden1) + hidden_bias1)\n        feedforward2 = tf.nn.relu(tf.matmul(feedforward1, hidden2) + hidden_bias2)\n        feedforward3 = tf.nn.relu(tf.matmul(feedforward2, hidden3) + hidden_bias3)\n        \n        self.logits = tf.matmul(feedforward3, output) + output_bias\n        self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = self.Y, logits = self.logits))\n        self.optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(self.cost)\n        \n        correct_pred = tf.equal(tf.argmax(self.logits, 1), tf.argmax(self.Y, 1))\n        self.accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n\nclass second_network:\n    def __init__(self, learning_rate, x_shape, y_shape, beta = 0.00005):\n        self.X = tf.placeholder(\"float\", [None, x_shape])\n        self.Y = tf.placeholder(\"float\", [None, y_shape])\n        \n        hidden1 = tf.Variable(tf.random_normal([x_shape, 512]))\n        hidden2 = tf.Variable(tf.random_normal([512, 256]))\n        hidden3 = tf.Variable(tf.random_normal([256, 128]))\n        output = tf.Variable(tf.random_normal([128, y_shape]))\n\n        hidden_bias1 = tf.Variable(tf.random_normal([512], stddev = 0.1))\n        hidden_bias2 = tf.Variable(tf.random_normal([256], stddev = 0.1))\n        hidden_bias3 = tf.Variable(tf.random_normal([128], stddev = 0.1))\n        output_bias = tf.Variable(tf.random_normal([y_shape], stddev = 0.1))\n        \n        feedforward1 = tf.nn.dropout(tf.nn.relu(tf.matmul(self.X, hidden1) + hidden_bias1), 0.5)\n        feedforward2 = tf.nn.dropout(tf.nn.relu(tf.matmul(feedforward1, hidden2) + hidden_bias2), 0.5)\n        feedforward3 = tf.nn.dropout(tf.nn.relu(tf.matmul(feedforward2, hidden3) + hidden_bias3), 0.5)\n        \n        self.logits = tf.matmul(feedforward3, output) + output_bias\n        self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = self.Y, logits = self.logits))\n        self.cost += tf.nn.l2_loss(hidden1) * beta + tf.nn.l2_loss(hidden2) * beta + tf.nn.l2_loss(hidden3) * beta + tf.nn.l2_loss(output) * beta\n        self.optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(self.cost)\n        \n        correct_pred = tf.equal(tf.argmax(self.logits, 1), tf.argmax(self.Y, 1))\n        self.accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n        \nclass third_network:\n    def __init__(self, learning_rate, x_shape, y_shape, beta = 0.00005):\n        self.X = tf.placeholder(\"float\", [None, x_shape])\n        self.Y = tf.placeholder(\"float\", [None, y_shape])\n        \n        hidden1 = tf.Variable(tf.random_normal([x_shape, 512]))\n        hidden2 = tf.Variable(tf.random_normal([512, 256]))\n        hidden3 = tf.Variable(tf.random_normal([256, 128]))\n        output = tf.Variable(tf.random_normal([128, y_shape]))\n\n        hidden_bias1 = tf.Variable(tf.random_normal([512], stddev = 0.1))\n        hidden_bias2 = tf.Variable(tf.random_normal([256], stddev = 0.1))\n        hidden_bias3 = tf.Variable(tf.random_normal([128], stddev = 0.1))\n        output_bias = tf.Variable(tf.random_normal([y_shape], stddev = 0.1))\n        \n        feedforward1 = tf.nn.relu(tf.matmul(self.X, hidden1) + hidden_bias1)\n        feedforward1 = tf.nn.dropout(tf.layers.batch_normalization(feedforward1), 0.5)\n        feedforward2 = tf.nn.relu(tf.matmul(feedforward1, hidden2) + hidden_bias2)\n        feedforward2 = tf.nn.dropout(tf.layers.batch_normalization(feedforward2), 0.5)\n        feedforward3 = tf.nn.relu(tf.matmul(feedforward2, hidden3) + hidden_bias3)\n        feedforward3 = tf.nn.dropout(tf.layers.batch_normalization(feedforward3), 0.5)\n        \n        self.logits = tf.matmul(feedforward3, output) + output_bias\n        self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = self.Y, logits = self.logits))\n        self.cost += tf.nn.l2_loss(hidden1) * beta + tf.nn.l2_loss(hidden2) * beta + tf.nn.l2_loss(hidden3) * beta + tf.nn.l2_loss(output) * beta\n        self.optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(self.cost)\n        \n        correct_pred = tf.equal(tf.argmax(self.logits, 1), tf.argmax(self.Y, 1))\n        self.accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"},{"outputs":[],"metadata":{"collapsed":true,"trusted":true,"_uuid":"1fd017e5af2e50caa089e16222ec651a58efbb65","_execution_state":"idle"},"cell_type":"code","execution_count":null,"source":"def train(model, x_train, y_train, x_test, y_test, epoch, batch):\n    LOST, ACC_TRAIN, ACC_TEST = [], [], []\n    for i in range(epoch):\n        loss, acc_train = 0, 0\n        for n in range(0, (x_train.shape[0] // batch) * batch, batch):\n            onehot = np.zeros((batch, np.unique(y_train).shape[0]))\n            \n            # change to one-hot for cross entropy\n            for k in range(batch):\n                onehot[k, int(y_train[n + k])] = 1.0\n            \n            cost, _ = sess.run([model.cost, model.optimizer], feed_dict = {model.X : x_train[n: n + batch, :], model.Y : onehot})\n            acc_train += sess.run(model.accuracy, feed_dict = {model.X : x_train[n: n + batch, :], model.Y : onehot})\n            loss += cost\n            \n        loss /= (x_train.shape[0] // batch)\n        acc_train /= (x_train.shape[0] // batch)\n        LOST.append(loss); ACC_TRAIN.append(acc_train)\n        \n        print ('epoch: ', i + 1, ', loss: ', loss, ', accuracy: ', acc_train)\n        \n        onehot = np.zeros((y_test.shape[0], np.unique(y_test).shape[0]))\n        \n        # change to one-hot for cross entropy\n        for k in range(y_test.shape[0]):\n            onehot[k, int(y_test[k])] = 1.0\n            \n        testing_acc, logits = sess.run([model.accuracy, tf.cast(tf.argmax(model.logits, 1), tf.int32)], feed_dict = {model.X : x_test, model.Y : onehot})\n        \n        print ('testing accuracy: ', testing_acc)\n        print (metrics.classification_report(y_test, logits, target_names = diagnosis_unique))\n        \n        ACC_TEST.append(testing_acc)\n        \n    plt.subplot(1, 2, 1)\n    x_component = [i for i in range(len(LOST))]\n    plt.plot(x_component, LOST)\n    plt.xlabel('epoch')\n    plt.ylabel('loss')\n    plt.subplot(1, 2, 2)\n    plt.plot(x_component, ACC_TRAIN, label = 'train accuracy')\n    plt.plot(x_component, ACC_TEST, label = 'test accuracy')\n    plt.legend()\n    plt.xlabel('epoch')\n    plt.ylabel('loss')\n    plt.show()"},{"outputs":[],"metadata":{"_uuid":"3c4396fc1db9206931e0445dc11518bf1202f98e"},"cell_type":"markdown","execution_count":null,"source":"We will try to train the model with default dataset, no normalization applied"},{"outputs":[],"metadata":{"collapsed":true,"trusted":true,"_uuid":"78076239dafcce2135ed0555837204fcc5b2e706","_execution_state":"idle"},"cell_type":"code","execution_count":null,"source":"EPOCH = 10\nBATCH = 32\nLEARNING_RATE = 0.001\n\nfrom sklearn.cross_validation import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(dataset_matrix, label_matrix, test_size = 0.2)"},{"outputs":[],"metadata":{"_uuid":"2f5f1c322e60d18c698934bb3bf2536a1d66b9d6"},"cell_type":"markdown","execution_count":null,"source":"First neural model trained with un-normalized dataset"},{"outputs":[],"metadata":{"collapsed":true,"trusted":true,"_uuid":"78e9f86c63c96109c3619f84341d8f60a9f3763c","_execution_state":"idle"},"cell_type":"code","execution_count":null,"source":"tf.reset_default_graph()\nsess = tf.InteractiveSession()\nmodel = first_network(LEARNING_RATE, X_train.shape[1], diagnosis_unique.shape[0])\nsess.run(tf.global_variables_initializer())\ntrain(model, X_train, Y_train, X_test, Y_test, EPOCH, BATCH)"},{"outputs":[],"metadata":{"_uuid":"ee44b22cf722d381ef8ab9a27020d209025b649f"},"cell_type":"markdown","execution_count":null,"source":"This is good! validation also good!"},{"outputs":[],"metadata":{"collapsed":true,"trusted":true,"_uuid":"b851a6de034cd85ce6954b341dc75eb389ab601d","_execution_state":"idle"},"cell_type":"code","execution_count":null,"source":"tf.reset_default_graph()\nsess = tf.InteractiveSession()\nmodel = second_network(LEARNING_RATE, X_train.shape[1], diagnosis_unique.shape[0])\nsess.run(tf.global_variables_initializer())\ntrain(model, X_train, Y_train, X_test, Y_test, EPOCH, BATCH)"},{"outputs":[],"metadata":{"_uuid":"3f8aa38ebfb8dfd0ebc556d356f8fac7850bea16"},"cell_type":"markdown","execution_count":null,"source":"Oh no! what happen! This is caused by the dropout and penalty L2 square root. It preventing the neural network become over-fitted and high variance for the dataset, but the accuracy during validation also sucks!"},{"outputs":[],"metadata":{"collapsed":true,"trusted":true,"_uuid":"b93fd968a3b6e7942151f60b25adb3810df6d629","_execution_state":"idle"},"cell_type":"code","execution_count":null,"source":"tf.reset_default_graph()\nsess = tf.InteractiveSession()\nmodel = third_network(LEARNING_RATE, X_train.shape[1], diagnosis_unique.shape[0])\nsess.run(tf.global_variables_initializer())\ntrain(model, X_train, Y_train, X_test, Y_test, EPOCH, BATCH)"},{"outputs":[],"metadata":{"_uuid":"c3454834e7d86fcd192f13fff237098935f49b51"},"cell_type":"markdown","execution_count":null,"source":"dropping out some nodes connection totally not a good idea for this dataset!"},{"outputs":[],"metadata":{"_uuid":"12425939f90349f8b968d7ea3deec19a9b1bc3c3"},"cell_type":"markdown","execution_count":null,"source":"How about we train on normalized dataset?"},{"outputs":[],"metadata":{"collapsed":true,"trusted":true,"_uuid":"4841e94ac78d7516804b72eae7d450813d2bcc6a","_execution_state":"idle"},"cell_type":"code","execution_count":null,"source":"X_train, X_test, Y_train, Y_test = train_test_split(normalize_dataset_matrix, label_matrix, test_size = 0.2)"},{"outputs":[],"metadata":{"collapsed":true,"trusted":true,"_uuid":"f73bae0e0f8aa4071fff4878b9885dbb632eef53","_execution_state":"idle"},"cell_type":"code","execution_count":null,"source":"tf.reset_default_graph()\nsess = tf.InteractiveSession()\nmodel = first_network(LEARNING_RATE, X_train.shape[1], diagnosis_unique.shape[0])\nsess.run(tf.global_variables_initializer())\ntrain(model, X_train, Y_train, X_test, Y_test, EPOCH, BATCH)"},{"outputs":[],"metadata":{"_uuid":"762d30f0dcd7494621eaadd9f4cda56850890c82"},"cell_type":"markdown","execution_count":null,"source":"Our first model is very good enough!"},{"outputs":[],"metadata":{"collapsed":true,"trusted":true,"_uuid":"7b5e30d44051cc87d7e059855214f523dac5c4b1","_execution_state":"idle"},"cell_type":"code","execution_count":null,"source":"tf.reset_default_graph()\nsess = tf.InteractiveSession()\nmodel = second_network(LEARNING_RATE, X_train.shape[1], diagnosis_unique.shape[0])\nsess.run(tf.global_variables_initializer())\ntrain(model, X_train, Y_train, X_test, Y_test, EPOCH, BATCH)"},{"outputs":[],"metadata":{"_uuid":"843dbfb375e5ee22e8417b3e7cc311aafee4849e"},"cell_type":"markdown","execution_count":null,"source":"Bad as un-normalized dataset. So let's continue with third model"},{"outputs":[],"metadata":{"collapsed":true,"trusted":true,"_uuid":"55e9ff6d11d9c140916c35164817754c2d082dee","_execution_state":"idle"},"cell_type":"code","execution_count":null,"source":"tf.reset_default_graph()\nsess = tf.InteractiveSession()\nmodel = third_network(LEARNING_RATE, X_train.shape[1], diagnosis_unique.shape[0])\nsess.run(tf.global_variables_initializer())\ntrain(model, X_train, Y_train, X_test, Y_test, EPOCH, BATCH)"},{"outputs":[],"metadata":{"_uuid":"857490d743000242473d04b6646a69bce68ee2cd"},"cell_type":"markdown","execution_count":null,"source":"Nope, nope, totally a nope. We reject our second and third model? But why it happens like that for second and third model? I thought dropout and l2 square root normalization is to help us to prevent overfitted? Even our first model is very good trading between bias and variance, validation also got good accuracy."},{"outputs":[],"metadata":{"_uuid":"27eefb56023afacfa68731c03241611889a8a45a"},"cell_type":"markdown","execution_count":null,"source":"Now let's visualize how our neural network regressed the decision"},{"outputs":[],"metadata":{"collapsed":true,"trusted":true,"_uuid":"277c8c43fe3eb533d08991b6ed15766cc1217d23","_execution_state":"idle"},"cell_type":"code","execution_count":null,"source":"x_2d = x_5d[:, :2]\nX_train, X_test, Y_train, Y_test = train_test_split(x_2d, label_matrix, test_size = 0.2)\n\ntf.reset_default_graph()\nsess = tf.InteractiveSession()\nmodel = first_network(LEARNING_RATE, X_train.shape[1], diagnosis_unique.shape[0])\nsess.run(tf.global_variables_initializer())\ntrain(model, X_train, Y_train, X_test, Y_test, 20, BATCH)"},{"outputs":[],"metadata":{"collapsed":true,"trusted":true,"_uuid":"9f5b77cea79a3c282388137123056ee0fd8070ea","_execution_state":"idle"},"cell_type":"code","execution_count":null,"source":"plt.figure(figsize = (30, 10))\nx_min, x_max = x_2d[:, 0].min() - 0.5, x_2d[:, 0].max() + 0.5\ny_min, y_max = x_2d[:, 1].min() - 0.5, x_2d[:, 1].max() + 0.5\nxx, yy = np.meshgrid(np.arange(x_min, x_max, 0.2), np.arange(y_min, y_max, 0.2))\n\nax = plt.subplot(1, 2, 1)\nax.set_title('Input data')\nax.scatter(X_train[:, 0], X_train[:, 1], c = Y_train, cmap = plt.cm.Set1, label = diagnosis_unique)\nax.scatter(X_test[:, 0], X_test[:, 1], c = Y_test, cmap = plt.cm.Set1, alpha = 0.6)\nax.set_xlim(xx.min(), xx.max())\nax.set_ylim(yy.min(), yy.max())\nax.set_xticks(())\nax.set_yticks(())\n\n\nax = plt.subplot(1, 2, 2)\ncontour = np.c_[xx.ravel(), yy.ravel()].astype(np.float32)\nZ = sess.run(tf.nn.softmax(model.logits), feed_dict = {model.X: contour})\ntemp_answer = []\nfor q in range(Z.shape[0]):\n    temp_answer.append(np.argmax(Z[q]))\nZ = np.array(temp_answer)\nZ = Z.reshape(xx.shape)\nax.contourf(xx, yy, Z, cmap = plt.cm.Set1, alpha = 0.4)\nax.scatter(X_train[:, 0], X_train[:, 1], c = Y_train, cmap = plt.cm.Set1, label = diagnosis_unique)\nax.scatter(X_test[:, 0], X_test[:, 1], c = Y_test, cmap = plt.cm.Set1, alpha = 0.6)\nax.set_xlim(xx.min(), xx.max())\nax.set_ylim(yy.min(), yy.max())\nax.set_xticks(())\nax.set_yticks(())\nax.set_title('hypothesis space')\nplt.tight_layout()\nplt.show()"}],"nbformat":4}