{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **PROJECT SCOPE**\nThis notebook covers the process of using XGBoost to predict the outlet sales on Big Mart sales prediction data. \n\n# The dataset\nThe dataset contains information about the stores, products and historical sales. We will predict the sales of the products in the stores.\n\n# XGBoost\nThis is a Machine Learning algorithm that deals with structured data, and uses the gradient boosting (GBM) framework at its core.\n\nBoosting is a sequential technique which works on the principle of an ensemble. It combines a set of weak learners and delivers improved prediction accuracy. At any instant t, the model outcomes are weighted based on the outcomes of previous instant t-1. The oucomes predicted correctly are given a lower weight and the ones miss-classified are weighted higher.\n\nThus the basic idea behind boosting algorithms is building a weak model, making conclusions about the various feature importance and parameters, and then using those conclusions to build a new, stronger model and capitalize on the misclassification error of the previous model and try to reduce it. \n\nThe default base learners of XGBoost are tree ensembles. The tree ensemble model is a set of classification and regression trees (CART). Trees are grown one after another, and attempts to reduce the misclassification rate are made in subsequent iterations. \n\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"#Importing libraries\nimport os\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2021-05-30T20:21:21.968221Z","iopub.execute_input":"2021-05-30T20:21:21.968652Z","iopub.status.idle":"2021-05-30T20:21:21.977105Z","shell.execute_reply.started":"2021-05-30T20:21:21.968566Z","shell.execute_reply":"2021-05-30T20:21:21.976126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Exploration and Preprocessing","metadata":{}},{"cell_type":"code","source":"#loading data\ntrain = pd.read_csv(\"../input/big-mart-sales-prediction/Train.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-05-30T20:21:21.978877Z","iopub.execute_input":"2021-05-30T20:21:21.979612Z","iopub.status.idle":"2021-05-30T20:21:22.024991Z","shell.execute_reply.started":"2021-05-30T20:21:21.979573Z","shell.execute_reply":"2021-05-30T20:21:22.024363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.shape","metadata":{"execution":{"iopub.status.busy":"2021-05-30T20:21:22.029522Z","iopub.execute_input":"2021-05-30T20:21:22.030207Z","iopub.status.idle":"2021-05-30T20:21:22.038719Z","shell.execute_reply.started":"2021-05-30T20:21:22.030171Z","shell.execute_reply":"2021-05-30T20:21:22.037818Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Our data contains 8523 rows of data with 12 columns.","metadata":{}},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2021-05-30T20:21:22.0405Z","iopub.execute_input":"2021-05-30T20:21:22.041018Z","iopub.status.idle":"2021-05-30T20:21:22.067001Z","shell.execute_reply.started":"2021-05-30T20:21:22.040981Z","shell.execute_reply":"2021-05-30T20:21:22.066206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.info()","metadata":{"execution":{"iopub.status.busy":"2021-05-30T20:21:22.068697Z","iopub.execute_input":"2021-05-30T20:21:22.069051Z","iopub.status.idle":"2021-05-30T20:21:22.088917Z","shell.execute_reply.started":"2021-05-30T20:21:22.069018Z","shell.execute_reply":"2021-05-30T20:21:22.088112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#check for missing values\ntrain.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2021-05-30T20:21:22.090037Z","iopub.execute_input":"2021-05-30T20:21:22.090536Z","iopub.status.idle":"2021-05-30T20:21:22.105011Z","shell.execute_reply.started":"2021-05-30T20:21:22.090501Z","shell.execute_reply":"2021-05-30T20:21:22.104033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Only Item_Weight and Outlet_Size have missing values.\n\nItem_Weight is a continuous variable. We can use either mean or median to impute the missing values, but here we will use mean.\n\nOutlet_Size is a categorical variable so will use mode to impute the missing values in the column.","metadata":{}},{"cell_type":"code","source":"#impute missing values in Item_Weight using mean\ntrain.Item_Weight.fillna(train.Item_Weight.mean(), inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-05-30T20:21:22.107153Z","iopub.execute_input":"2021-05-30T20:21:22.107653Z","iopub.status.idle":"2021-05-30T20:21:22.112333Z","shell.execute_reply.started":"2021-05-30T20:21:22.107618Z","shell.execute_reply":"2021-05-30T20:21:22.111416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.Item_Weight.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2021-05-30T20:21:22.113928Z","iopub.execute_input":"2021-05-30T20:21:22.114467Z","iopub.status.idle":"2021-05-30T20:21:22.123991Z","shell.execute_reply.started":"2021-05-30T20:21:22.114433Z","shell.execute_reply":"2021-05-30T20:21:22.122904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#impute missing values in Outlet_Size using mode\ntrain.Outlet_Size.fillna(train.Outlet_Size.mode()[0], inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-05-30T20:21:22.125461Z","iopub.execute_input":"2021-05-30T20:21:22.126011Z","iopub.status.idle":"2021-05-30T20:21:22.13347Z","shell.execute_reply.started":"2021-05-30T20:21:22.125976Z","shell.execute_reply":"2021-05-30T20:21:22.132621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.Outlet_Size.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2021-05-30T20:21:22.135323Z","iopub.execute_input":"2021-05-30T20:21:22.13595Z","iopub.status.idle":"2021-05-30T20:21:22.146251Z","shell.execute_reply.started":"2021-05-30T20:21:22.135915Z","shell.execute_reply":"2021-05-30T20:21:22.145152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Machine learning models cannot work with categorical(string) data. We will convert the categorical variables into numeric types.","metadata":{}},{"cell_type":"code","source":"#checking categorical variables in the data\ntrain.dtypes","metadata":{"execution":{"iopub.status.busy":"2021-05-30T20:21:22.147906Z","iopub.execute_input":"2021-05-30T20:21:22.148568Z","iopub.status.idle":"2021-05-30T20:21:22.156056Z","shell.execute_reply.started":"2021-05-30T20:21:22.148529Z","shell.execute_reply":"2021-05-30T20:21:22.154805Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Our data has the following categorical variables\n\n* Item_Identifier\n* Item_Fat_Content\n* Item_Type\n* Outlet_Identifier\n* Outlet_Size\n* Outlet_Type\n* Outlet_Location_Type\n\nWe will use target encording to convert these variables variables. ","metadata":{}},{"cell_type":"code","source":"#target encorders\nItem_Fat_Content_mean = train.groupby('Item_Fat_Content')['Item_Outlet_Sales'].mean()\ntrain['Item_Fat_Content'] = train['Item_Fat_Content'].map(Item_Fat_Content_mean)\nItem_Type_mean = train.groupby('Item_Type')['Item_Outlet_Sales'].mean()\ntrain['Item_Type'] = train['Item_Type'].map(Item_Type_mean)\nOutlet_Identifier_mean = train.groupby('Outlet_Identifier')['Item_Outlet_Sales'].mean()\ntrain['Outlet_Identifier'] = train['Outlet_Identifier'].map(Outlet_Identifier_mean)\nOutlet_Size_mean = train.groupby('Outlet_Size')['Item_Outlet_Sales'].mean()\ntrain['Outlet_Size'] = train['Outlet_Size'].map(Outlet_Size_mean)\nOutlet_Location_Type_mean = train.groupby('Outlet_Location_Type')['Item_Outlet_Sales'].mean()\ntrain['Outlet_Location_Type'] = train['Outlet_Location_Type'].map(Outlet_Location_Type_mean)\nOutlet_Type_mean = train.groupby('Outlet_Type')['Item_Outlet_Sales'].mean()\ntrain['Outlet_Type'] = train['Outlet_Type'].map(Outlet_Type_mean)","metadata":{"execution":{"iopub.status.busy":"2021-05-30T20:21:22.287753Z","iopub.execute_input":"2021-05-30T20:21:22.288018Z","iopub.status.idle":"2021-05-30T20:21:22.312834Z","shell.execute_reply.started":"2021-05-30T20:21:22.287993Z","shell.execute_reply":"2021-05-30T20:21:22.312198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2021-05-30T20:21:22.314361Z","iopub.execute_input":"2021-05-30T20:21:22.3147Z","iopub.status.idle":"2021-05-30T20:21:22.330447Z","shell.execute_reply.started":"2021-05-30T20:21:22.314667Z","shell.execute_reply":"2021-05-30T20:21:22.329514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.shape","metadata":{"execution":{"iopub.status.busy":"2021-05-30T20:21:22.332357Z","iopub.execute_input":"2021-05-30T20:21:22.332888Z","iopub.status.idle":"2021-05-30T20:21:22.339713Z","shell.execute_reply.started":"2021-05-30T20:21:22.33285Z","shell.execute_reply":"2021-05-30T20:21:22.338724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now that we have taken care of our categorical variables, we move on to the continous variables. We will nnormalize the data in such a way that the range of all variables is almost similar. We will use the StandardScaler function to do this.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n#create an object of the StandardScaler\nscaler = StandardScaler()\n\n#fit with the Item_MRP\nscaler.fit(np.array(train.Item_MRP).reshape(-1,1))\n\n#transform the data\ntrain.Item_MRP = scaler.transform(np.array(train.Item_MRP).reshape(-1,1))","metadata":{"execution":{"iopub.status.busy":"2021-05-30T20:21:22.341363Z","iopub.execute_input":"2021-05-30T20:21:22.341994Z","iopub.status.idle":"2021-05-30T20:21:23.154522Z","shell.execute_reply.started":"2021-05-30T20:21:22.341954Z","shell.execute_reply":"2021-05-30T20:21:23.153735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# The model\nWe will build the model using Trees as base learners using XGBoost's scikit-learn compatible API.","metadata":{}},{"cell_type":"code","source":"#importing libraries\nimport xgboost as xgb\nfrom sklearn.metrics import mean_squared_error","metadata":{"execution":{"iopub.status.busy":"2021-05-30T20:21:23.156297Z","iopub.execute_input":"2021-05-30T20:21:23.156644Z","iopub.status.idle":"2021-05-30T20:21:26.561054Z","shell.execute_reply.started":"2021-05-30T20:21:23.156617Z","shell.execute_reply":"2021-05-30T20:21:26.560059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Separate the target variable and rest of the variables using .iloc to subset the data.","metadata":{}},{"cell_type":"code","source":"X, y = train.iloc[:,:-1],train.iloc[:,-1]","metadata":{"execution":{"iopub.status.busy":"2021-05-30T20:21:26.567598Z","iopub.execute_input":"2021-05-30T20:21:26.567947Z","iopub.status.idle":"2021-05-30T20:21:26.579445Z","shell.execute_reply.started":"2021-05-30T20:21:26.567909Z","shell.execute_reply":"2021-05-30T20:21:26.57836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X","metadata":{"execution":{"iopub.status.busy":"2021-05-30T20:21:26.588608Z","iopub.execute_input":"2021-05-30T20:21:26.589284Z","iopub.status.idle":"2021-05-30T20:21:26.631619Z","shell.execute_reply.started":"2021-05-30T20:21:26.589218Z","shell.execute_reply":"2021-05-30T20:21:26.630326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#drop Item Identifier column\nX = X.drop(columns=['Item_Identifier']) ","metadata":{"execution":{"iopub.status.busy":"2021-05-30T20:21:26.637218Z","iopub.execute_input":"2021-05-30T20:21:26.641473Z","iopub.status.idle":"2021-05-30T20:21:26.652772Z","shell.execute_reply.started":"2021-05-30T20:21:26.641411Z","shell.execute_reply":"2021-05-30T20:21:26.651404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X","metadata":{"execution":{"iopub.status.busy":"2021-05-30T20:21:26.658474Z","iopub.execute_input":"2021-05-30T20:21:26.659443Z","iopub.status.idle":"2021-05-30T20:21:26.693534Z","shell.execute_reply.started":"2021-05-30T20:21:26.659407Z","shell.execute_reply":"2021-05-30T20:21:26.692427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y","metadata":{"execution":{"iopub.status.busy":"2021-05-30T20:21:26.698185Z","iopub.execute_input":"2021-05-30T20:21:26.701435Z","iopub.status.idle":"2021-05-30T20:21:26.71489Z","shell.execute_reply.started":"2021-05-30T20:21:26.701364Z","shell.execute_reply":"2021-05-30T20:21:26.713314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we will convert the dataset into an optimized data structure called Dmatrix that XGBoost supports and gives it acclaimed performance and efficiency gains. ","metadata":{}},{"cell_type":"code","source":"data_dmatrix = xgb.DMatrix(data=X,label=y)","metadata":{"execution":{"iopub.status.busy":"2021-05-30T20:21:26.717745Z","iopub.execute_input":"2021-05-30T20:21:26.718513Z","iopub.status.idle":"2021-05-30T20:21:26.7387Z","shell.execute_reply.started":"2021-05-30T20:21:26.718478Z","shell.execute_reply":"2021-05-30T20:21:26.738104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, we will create the train and test set for cross-validation of the results using the train_test_split function from sklearn's model_selection module with test_size size equal to 20% of the data. Also, to maintain reproducibility of the results, a random_state is also assigned.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)","metadata":{"execution":{"iopub.status.busy":"2021-05-30T20:21:26.741979Z","iopub.execute_input":"2021-05-30T20:21:26.743812Z","iopub.status.idle":"2021-05-30T20:21:26.755901Z","shell.execute_reply.started":"2021-05-30T20:21:26.743772Z","shell.execute_reply":"2021-05-30T20:21:26.755007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The next step is to instantiate an XGBoost regressor object by calling the XGBRegressor() class from the XGBoost library with the hyper-parameters passed as arguments.","metadata":{}},{"cell_type":"code","source":"xg_reg = xgb.XGBRegressor(objective ='reg:squarederror', colsample_bytree = 0.3, learning_rate = 0.1,\n                max_depth = 5, alpha = 10, n_estimators = 10)","metadata":{"execution":{"iopub.status.busy":"2021-05-30T20:21:26.759789Z","iopub.execute_input":"2021-05-30T20:21:26.761445Z","iopub.status.idle":"2021-05-30T20:21:26.767556Z","shell.execute_reply.started":"2021-05-30T20:21:26.761414Z","shell.execute_reply":"2021-05-30T20:21:26.766516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Fit the regressor to the training set and make predictions on the test set using the familiar .fit() and .predict() methods.","metadata":{}},{"cell_type":"code","source":"xg_reg.fit(X_train,y_train)\n\npreds = xg_reg.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2021-05-30T20:21:26.773291Z","iopub.execute_input":"2021-05-30T20:21:26.775149Z","iopub.status.idle":"2021-05-30T20:21:27.001216Z","shell.execute_reply.started":"2021-05-30T20:21:26.77511Z","shell.execute_reply":"2021-05-30T20:21:27.000429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Compute the rmse by invoking the mean_sqaured_error function from sklearn's metrics module.","metadata":{}},{"cell_type":"code","source":"rmse = np.sqrt(mean_squared_error(y_test, preds))\nprint(\"RMSE: %f\" % (rmse))","metadata":{"execution":{"iopub.status.busy":"2021-05-30T20:21:27.005603Z","iopub.execute_input":"2021-05-30T20:21:27.008035Z","iopub.status.idle":"2021-05-30T20:21:27.019265Z","shell.execute_reply.started":"2021-05-30T20:21:27.007999Z","shell.execute_reply":"2021-05-30T20:21:27.018225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The RMSE for the outlet sales prediction came out to be around 1661.80","metadata":{}},{"cell_type":"markdown","source":"# k-fold Cross Validation using XGBoost\n\nIn order to build more robust models, we will do a k-fold cross validation where all the entries in the original training dataset are used for both training as well as validation. Also, each entry is used for validation just once. \n\nWe will create a hyper-parameter dictionary params which holds all the hyper-parameters and their values as key-value pairs but will exclude the n_estimators from the hyper-parameter dictionary because we will use num_boost_rounds (denotes the number of tress we build, analogous to n_estimators) instead.\n\nYou will use these parameters to build a 3-fold cross validation model by invoking XGBoost's cv() method and store the results in a cv_results DataFrame. We are using the Dmatrix object we created before.","metadata":{}},{"cell_type":"code","source":"params = {\"objective\":\"reg:squarederror\",'colsample_bytree': 0.3,'learning_rate': 0.1,\n                'max_depth': 5, 'alpha': 10}\n\ncv_results = xgb.cv(dtrain=data_dmatrix, params=params, nfold=3,\n                    num_boost_round=50,early_stopping_rounds=10,metrics=\"rmse\", as_pandas=True, seed=123)","metadata":{"execution":{"iopub.status.busy":"2021-05-30T20:21:27.023483Z","iopub.execute_input":"2021-05-30T20:21:27.023808Z","iopub.status.idle":"2021-05-30T20:21:27.34198Z","shell.execute_reply.started":"2021-05-30T20:21:27.02378Z","shell.execute_reply":"2021-05-30T20:21:27.341059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"cv_results contains train and test RMSE metrics for each boosting round.","metadata":{}},{"cell_type":"code","source":"cv_results.head()","metadata":{"execution":{"iopub.status.busy":"2021-05-30T20:21:27.343189Z","iopub.execute_input":"2021-05-30T20:21:27.343535Z","iopub.status.idle":"2021-05-30T20:21:27.356786Z","shell.execute_reply.started":"2021-05-30T20:21:27.3435Z","shell.execute_reply":"2021-05-30T20:21:27.355822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Extract and print the final boosting round metric.","metadata":{}},{"cell_type":"code","source":"print((cv_results[\"test-rmse-mean\"]).tail(1))","metadata":{"execution":{"iopub.status.busy":"2021-05-30T20:21:27.358202Z","iopub.execute_input":"2021-05-30T20:21:27.358774Z","iopub.status.idle":"2021-05-30T20:21:27.371193Z","shell.execute_reply.started":"2021-05-30T20:21:27.35873Z","shell.execute_reply":"2021-05-30T20:21:27.370119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that your RMSE for the outlet sales prediction has reduced as compared to last time and came out to be around 1120.77.","metadata":{}},{"cell_type":"markdown","source":"# Visualize Boosting Trees and Feature Importance\n\nWe will now visualize individual trees from the fully boosted model that XGBoost creates using the entire dataset. XGBoost has a plot_tree() function that makes this type of visualization easy. Once we train a model using the XGBoost learning API, we can pass it to the plot_tree() function along with the number of trees we want to plot using the num_trees argument.","metadata":{}},{"cell_type":"code","source":"xg_reg = xgb.train(params=params, dtrain=data_dmatrix, num_boost_round=10)","metadata":{"execution":{"iopub.status.busy":"2021-05-30T20:21:27.373228Z","iopub.execute_input":"2021-05-30T20:21:27.373784Z","iopub.status.idle":"2021-05-30T20:21:27.419237Z","shell.execute_reply.started":"2021-05-30T20:21:27.373745Z","shell.execute_reply":"2021-05-30T20:21:27.418557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will visualize our XGBoost models by examining the importance of each feature column in the original dataset within the model.\n\nThis involves counting the number of times each feature is split on across all boosting rounds (trees) in the model, and then visualizing the result as a bar graph, with the features ordered according to how many times they appear. XGBoost plot_importance() function allows us to do exactly this.","metadata":{}},{"cell_type":"code","source":"xgb.plot_importance(xg_reg)\nplt.rcParams['figure.figsize'] = [8, 6]\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-30T20:21:27.420351Z","iopub.execute_input":"2021-05-30T20:21:27.42084Z","iopub.status.idle":"2021-05-30T20:21:27.628808Z","shell.execute_reply.started":"2021-05-30T20:21:27.420802Z","shell.execute_reply":"2021-05-30T20:21:27.628155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see, the feature Item_Weight has been given the highest importance score among all the features.","metadata":{}}]}