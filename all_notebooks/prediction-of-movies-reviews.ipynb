{"cells":[{"metadata":{"id":"GAM51_LgD6H3"},"cell_type":"markdown","source":"* Import google drive","execution_count":null},{"metadata":{"id":"m-lieIhaF9N6","outputId":"a3fd2731-7177-4c45-ba62-2385a44ac267","trusted":false},"cell_type":"code","source":"from google.colab import drive\ndrive.mount('/gdrive')","execution_count":null,"outputs":[]},{"metadata":{"id":"pCdpQ1bqEAe_"},"cell_type":"markdown","source":"* Import movies  folder from my drive","execution_count":null},{"metadata":{"id":"abX6GlRRJmk0","trusted":false},"cell_type":"code","source":"\nimport os\nos.chdir(\"/gdrive/My Drive/movies\")","execution_count":null,"outputs":[]},{"metadata":{"id":"WAD_A32cEfrE"},"cell_type":"markdown","source":"* install NTLK library","execution_count":null},{"metadata":{"id":"tHYPRlMcO4nc","outputId":"a5fe32ba-268b-49c8-edd7-bf68aa8ffb0f","trusted":false},"cell_type":"code","source":"!pip install nltk","execution_count":null,"outputs":[]},{"metadata":{"id":"GtRbdZXDJmqs","trusted":false},"cell_type":"code","source":"\nimport pandas as pd \nimport numpy as np\n\nreviews=pd.read_csv(\"IMDB Dataset.csv\")","execution_count":null,"outputs":[]},{"metadata":{"id":"E3nSNWRfEsw0"},"cell_type":"markdown","source":"# import re\n* re ( Regular expressions) is sequence of character that forms a search pattern which can be used to chekc over text.\n* They can be assist an filtering data","execution_count":null},{"metadata":{"id":"_JuVyVKRFcAK"},"cell_type":"markdown","source":"# import nltk\n* nltk ( Natural Language Toolkit) is a powerful Python package that provides a set of diverse natural languages algorithms. \n* nltk is a leading platform for building Python programs to work with human language data.\n","execution_count":null},{"metadata":{"id":"P5Y12grcHR45"},"cell_type":"markdown","source":"# Stopwords\n* In NLTK for removing stopwords, you need to create a list of stopwords and filter out your list of tokens from these words.","execution_count":null},{"metadata":{"id":"aG53xY8vJmxG","outputId":"f70dfb1d-3493-4649-9633-a34f23f76f39","trusted":false},"cell_type":"code","source":"\nimport re\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nstops = stopwords.words('english')\nprint(stopwords.words('english'))\n\n","execution_count":null,"outputs":[]},{"metadata":{"id":"XnnTGCpcHg_J"},"cell_type":"markdown","source":"# PorterStemmer\n* Stemming is a process of cutting the last alphabetg letters of a word and achives the root word.","execution_count":null},{"metadata":{"id":"YPU_lVC2Jm2U","trusted":false},"cell_type":"code","source":"\nfrom nltk.stem.porter import PorterStemmer\ncorpus = []\nfor i in range(0, 50000):\n    review = re.sub('[^a-zA-Z]', ' ', reviews['review'][i])\n    review = review.lower()\n    review = review.split()\n    ps = PorterStemmer()\n    review = [ps.stem(word) for word in review if not word in set(stops)]\n    review = ' '.join(review)\n    corpus.append(review)","execution_count":null,"outputs":[]},{"metadata":{"id":"Jm3LdZrbIeeX"},"cell_type":"markdown","source":"# Word Cloud\n* Wordcloud is the pictorial representation of the most frequently repeated words representing the size of the word.\n* Many times we  might have seen a cloud filled with lots of words in different sizes, which represent the frequency or the importance of each word. This is called Tag Cloud or WordCloud.\n# corpus\n* Corpora is a group presenting multiple collections of text documents.\n* A single collection is called corpus.","execution_count":null},{"metadata":{"id":"1oXfeuKVJmuU","outputId":"94e116b6-7d4e-4c6e-9aad-754f1603d24d","trusted":false},"cell_type":"code","source":"\ncorp_str = str(corpus)\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\nwordcloud = WordCloud(relative_scaling=1.0).generate(corp_str)\nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"zhe34KW3Le7b"},"cell_type":"markdown","source":"* To customize the stopwords ","execution_count":null},{"metadata":{"id":"qiIX7Ua2Jmns","outputId":"f1aa13a1-564c-4616-c4f6-066e1106145e","trusted":false},"cell_type":"code","source":"from wordcloud import STOPWORDS\nmystopwrds = set(STOPWORDS)\nmystopwrds.add(\"br\")\nwc = WordCloud(stopwords=mystopwrds,relative_scaling=1.0,background_color=\"white\")\nwordcloud = wc.generate(corp_str)\nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"mthqAPM3WhL4"},"cell_type":"markdown","source":"# COUNTVECTORIZOR \n*  It is used to convert a collection of text documents to a vector of term/token counts. It also enables the â€‹pre-processing of text data prior to generating the vector representation. This functionality makes it a highly flexible feature representation module for text.\n* Creating the Bag of Words model","execution_count":null},{"metadata":{"id":"T4tG177qJme2","trusted":false},"cell_type":"code","source":"\nfrom sklearn.feature_extraction.text import CountVectorizer\ncv = CountVectorizer(max_features = 100)\nX = cv.fit_transform(corpus).toarray()\ny = reviews.iloc[:, 1]\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 2020)\n","execution_count":null,"outputs":[]},{"metadata":{"id":"t6curOycX99W"},"cell_type":"markdown","source":"# Gaussian Naives Bayes\n\n* Describe that probability of an event based on prior knowledge of conditions be related of conditions to the event.\n\n","execution_count":null},{"metadata":{"id":"uQFw_HCJKHS7","outputId":"26b9672c-c710-4200-8b7d-7f5652eaca4f","trusted":false},"cell_type":"code","source":"\nfrom sklearn.naive_bayes import GaussianNB\nclassifier = GaussianNB()\nclassifier.fit(X_train, y_train)\n\ny_pred = classifier.predict(X_test)\n\nfrom sklearn.metrics import confusion_matrix, classification_report,accuracy_score\nconfusion_matrix(y_test, y_pred)\n\nprint(classification_report(y_test, y_pred))\n\nprint(accuracy_score(y_test, y_pred))\n","execution_count":null,"outputs":[]},{"metadata":{"id":"7l1zoKi5YSUb"},"cell_type":"markdown","source":"# TF-IDF VECTORIZER (Term Frequency Inverse Document Frequency)\n* This is a technique to quantify a word in documents, we generally compute a weight to each word which signifies the importance of the word in the document and corpus. \n* TF-IDF is word frequency score that try to highlight words that are more interesting.","execution_count":null},{"metadata":{"id":"1dLlMLyKKH3U","trusted":false},"cell_type":"code","source":"\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\ncv = TfidfVectorizer(max_features = 100)\nX = cv.fit_transform(corpus).toarray()\ny =reviews.iloc[:, 1]\n\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 2020)\n","execution_count":null,"outputs":[]},{"metadata":{"id":"o5ZF5i28j74L"},"cell_type":"markdown","source":"# Gaussian Naives Bayes","execution_count":null},{"metadata":{"id":"Tez8jPj3KH8k","outputId":"6c55a073-2a0e-415a-f0fd-3f7f5db70910","trusted":false},"cell_type":"code","source":"\n\nfrom sklearn.naive_bayes import GaussianNB\nclassifier = GaussianNB()\nclassifier.fit(X_train, y_train)\n\ny_pred = classifier.predict(X_test)\n\nfrom sklearn.metrics import confusion_matrix, classification_report,accuracy_score\nconfusion_matrix(y_test, y_pred)\n\nprint(classification_report(y_test, y_pred))\n\nprint(accuracy_score(y_test, y_pred))\n","execution_count":null,"outputs":[]},{"metadata":{"id":"7nH4x2zHkJrw"},"cell_type":"markdown","source":"# RANDOM FOREST USING PORTERSTEMMER \n* Replacing Apostrophe to Word","execution_count":null},{"metadata":{"id":"2jfP4IJgKICV","trusted":false},"cell_type":"code","source":"\ndef decontracted(phrase):\n    # specific\n    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n    phrase = re.sub(r\"don\\'t\", \"do not\", phrase)\n    # general\n    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n    return phrase\n\n","execution_count":null,"outputs":[]},{"metadata":{"id":"9MMRXioOkW8o"},"cell_type":"markdown","source":"# import PorterStemmer","execution_count":null},{"metadata":{"id":"EcBtkdmQKIHE","trusted":false},"cell_type":"code","source":"\n\nfrom nltk.stem.porter import PorterStemmer\ncorpus = []\nfor i in range(0, 50000):\n    review = reviews['review'][i]\n    review = decontracted(review)    \n    review = re.sub('[^a-zA-Z]', ' ', review)\n    review = review.lower()\n#    ps = PorterStemmer()\n    review = review.split()\n#    review = [word for word in review if not word in set(stops)]\n    #review = ' '.join(review)\n    #review = [review]\n    corpus.append(review)\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"id":"7clBh7i3keHQ"},"cell_type":"markdown","source":"# import  Word2Vec\n* word2vector using gensim\n * It is algorithm that takes corpora as a input and output it in the form of vectors.\n * Word2Vec finds relation (Semantic or Syntactic) between the words which was not possible by our Tradional TF-IDF or Frequency based approach.\n* Gensim is fairly easy to use module which inherits CBOW and Skip-gram.\n * Continuous Bag of Words (CBOW) in  neighboring words are provided as Input to predict the Target. In other words, A context is Provided as input to predict the Target.\n * Skip-gram is opposite / inverse of CBOW, wherein a target word is provided as output in order to predict the Contextual / Neighboring words.\n","execution_count":null},{"metadata":{"id":"jgoW2FwrkdDp","trusted":false},"cell_type":"code","source":"from gensim.models import  Word2Vec","execution_count":null,"outputs":[]},{"metadata":{"id":"J0XPAUmqnVsN"},"cell_type":"markdown","source":"* sg = 0 for CBOW  \n* sg = 1 for skip-gram","execution_count":null},{"metadata":{"id":"_DaSRxlAKIE8","trusted":false},"cell_type":"code","source":"\n \n\nmodel_r =  Word2Vec(corpus, min_count=1,sg=0)\n","execution_count":null,"outputs":[]},{"metadata":{"id":"VPO8fATypLFR"},"cell_type":"markdown","source":"# Using pandas ","execution_count":null},{"metadata":{"id":"ihwRZKWfKH_j","outputId":"bc82a06e-7cb5-4249-fbd7-270e870bc612","trusted":false},"cell_type":"code","source":"\nmeans = pd.DataFrame()\nfor i in corpus :\n    row_means = np.mean(model_r[i],axis=0)\n    row_means = pd.Series(row_means)\n    means = pd.concat([means, row_means],axis=1)\n    \n\nX = means.T\ny = reviews.iloc[:, 1]\n\n\n\nfrom sklearn.model_selection import train_test_split \nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report, accuracy_score\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y,test_size = 0.3, \n                                                    random_state=2020,\n                                                    stratify=y)","execution_count":null,"outputs":[]},{"metadata":{"id":"j4v3VDTunyL7"},"cell_type":"markdown","source":"# RandomForestClassifier","execution_count":null},{"metadata":{"id":"IBjNvswbKH6I","outputId":"5ad5a603-78c7-47e6-ca2d-ede458dc8d9d","trusted":false},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nmodel_rf = RandomForestClassifier(random_state=2020,\n                                  n_estimators=10,oob_score=True)\nmodel_rf.fit( X , y )\n\n\ny_pred = model_rf.predict(X_test)\n\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))\nprint(accuracy_score(y_test, y_pred))\n","execution_count":null,"outputs":[]},{"metadata":{"id":"M6VDsjeyn26c"},"cell_type":"markdown","source":" * ROC_CURVE,ROC_AUC_SCORE","execution_count":null},{"metadata":{"id":"hwv1fn81KH0j","outputId":"637e4835-9e03-4107-f70d-24ae05249986","trusted":false},"cell_type":"code","source":"\nfrom sklearn.metrics import roc_curve, roc_auc_score\ny_test = pd.get_dummies(y_test, drop_first=True)\ny_pred_prob = model_rf.predict_proba(X_test)[:,1]\n\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n\n\nimport matplotlib.pyplot as plt\nplt.plot([0, 1], [0, 1], 'k--')\nplt.plot(fpr, tpr)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.show()\nroc_auc_score(y_test, y_pred_prob)\n\n\n# summarize the loaded model\nprint(model_r)\n\n# summarize vocabulary\nwords = list(model_r.wv.vocab)\n","execution_count":null,"outputs":[]},{"metadata":{"id":"Y9CoROjkoLk0"},"cell_type":"markdown","source":"* Multiple  ROC_CURVE in one plot","execution_count":null},{"metadata":{"id":"O0nRmN3jaAia","trusted":false},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\nclassifiers=[RandomForestClassifier(random_state=2020),GaussianNB(),KNeighborsClassifier(),\n            DecisionTreeClassifier(random_state=2020)]","execution_count":null,"outputs":[]},{"metadata":{"id":"-jo9OUMSoq2i"},"cell_type":"markdown","source":"","execution_count":null},{"metadata":{"id":"fh9Q6Gj_ofiK"},"cell_type":"markdown","source":"* Define a result table as a DataFrame\n\n* Train the models and record the results\n* Set name of the classifiers as index labels","execution_count":null},{"metadata":{"id":"FUe9uJ24oqGx"},"cell_type":"markdown","source":"","execution_count":null},{"metadata":{"id":"j9LRPJBKKHQM","trusted":false},"cell_type":"code","source":"\nresult_table = pd.DataFrame(columns=['classifiers', 'fpr','tpr','auc'])\n\nfor cls in classifiers:\n    model = cls.fit(X_train, y_train)\n    yproba = model.predict_proba(X_test)[::,1]\n\n    \n    fpr, tpr, _ = roc_curve(y_test,  yproba)\n    auc = roc_auc_score(y_test, yproba)\n\n    \n    result_table = result_table.append({'classifiers':cls.__class__.__name__,\n                                        'fpr':fpr, \n                                        'tpr':tpr, \n                                        'auc':auc}, ignore_index=True)\n\n\nresult_table.set_index('classifiers', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"id":"rj7Dp13Qo49A"},"cell_type":"markdown","source":"* Plot the figure","execution_count":null},{"metadata":{"id":"CF8dsCb4bYEo","outputId":"de1448b4-ce46-44f6-9464-48652719d9b7","trusted":false},"cell_type":"code","source":"\nfig = plt.figure(figsize=(8,6))\n\nfor i in result_table.index:\n    plt.plot(result_table.loc[i]['fpr'], \n             result_table.loc[i]['tpr'], \n             label=\"{}, AUC={:.3f}\".format(i, result_table.loc[i]['auc']))\n    \nplt.plot([0,1], [0,1], color='orange', linestyle='--')\n\nplt.xticks(np.arange(0.0, 1.1, step=0.1))\nplt.xlabel(\"Flase Positive Rate\", fontsize=15)\n\nplt.yticks(np.arange(0.0, 1.1, step=0.1))\nplt.ylabel(\"True Positive Rate\", fontsize=15)\n\nplt.title('ROC Curve Analysis', fontweight='bold', fontsize=15)\nplt.legend(prop={'size':13}, loc='lower right')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"u6Qf2nSMpA3i"},"cell_type":"markdown","source":"* From above graph here we found Randomforest is the best model","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}