{"cells":[{"metadata":{},"cell_type":"markdown","source":"<b> Obejective</b> : Observe the impact of changing parameter on Accuracy. We would use a Small dataset of Dimension (7043, 31)"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"<strong style=\"color:Tomato;\">Import Libraries </strong>"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Data Handling\nfrom pandas import read_csv\nimport os\nimport pandas as pd\nimport numpy as np\nimport time\n\n# Modeling\nimport xgboost\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import LabelEncoder\n\n# Visualization\nimport matplotlib\nfrom matplotlib import pyplot\n\n## Hyperparameter optimization using RandomizedSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<strong style=\"color:Tomato;\">Load Data Set </strong>"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_basedata_train_0 = pd.read_csv('/kaggle/input/telco-customer-churn/WA_Fn-UseC_-Telco-Customer-Churn.csv')\ndf_basedata_train_0.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<strong style=\"color:Tomato;\">Data Preparation </strong>"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_basedata_train_0.info()\n# Converting Total Charges to a numerical data type.\ndf_basedata_train_0.TotalCharges = pd.to_numeric(df_basedata_train_0.TotalCharges, errors='coerce')\n\n# Find Categorical Variables\ns = (df_basedata_train_0.dtypes == 'object')\nobject_cols = list(s[s].index)\nprint(\"Categorical variables:\")\nprint(object_cols)\n# Encode Categorical Variables\ndf_1 = pd.get_dummies(df_basedata_train_0, columns=[\"gender\",\"Partner\",\"Dependents\", \"PhoneService\", \"MultipleLines\", \"InternetService\",\"OnlineSecurity\",\"OnlineBackup\",\"DeviceProtection\", \"TechSupport\",\"StreamingTV\",\"StreamingMovies\",\"Contract\",\"PaperlessBilling\",\"PaymentMethod\", \"Churn\"   ],drop_first=True)\n\n# Rename Target feature\ndf_1 = df_1.rename(columns={'Churn_Yes': 'target'})\n\n## Drop ID Features\ndf_2=df_1.drop(['customerID'],axis=1)\ndf_2.head()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<strong style=\"color:Tomato;\">Split the Dataset in Predictor and Target </strong>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# split data into X and y\nX = df_2.drop(\"target\", axis=1)\ny = df_2[\"target\"]\n\n\n# encode string class values as integers\nlabel_encoded_y = LabelEncoder().fit_transform(y)\nnp.unique(label_encoded_y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<strong style=\"color:Tomato;\">Define Hyper-Parameters </strong>"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Hyper Parameter Optimization\n\nlearning_rate = [0.01, 0.03, 0.05, 0.10, 0.15, 0.20, 0.25, 0.3] \nmax_depth = [ 3, 4, 5, 6, 8, 10, 15, 20]\nmin_child_weight = [ 1, 3, 5, 7 ]\ngamma = [ 0.0, 0.1,  0.4,   0.9, 1 , 2, 5 , 10, 15, 20 ]\ncolsample_bytree = [ 0.3, 0.4, 0.5 , 0.7 ]\nn_estimators = [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1 style=\"color:DodgerBlue;\">Tune the Number of Decision Trees in XGBoost</h1>"},{"metadata":{},"cell_type":"markdown","source":"Using scikit-learn we can perform a grid search of the n_estimators model parameter, evaluating a series of values from <b>100 to 1000 </b>with a step size of 100 (100, 200, 300, 400, 500, 600, 700, 800, 900, 1000).\nWe can perform this grid search on the Otto dataset, using 10-fold cross validation, requiring 100 models to be trained (10 configurations * 10 folds)."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create Model Object\nclassifier=xgboost.XGBClassifier()\n\n# Initiate start time\nstart_time = time.time()\n\n# Parameters\n#n_estimators = [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000] \nparam_grid = dict(n_estimators=n_estimators)\n\n#Cross Validation \nkfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=7)\n#Model creation with GridSearchCV\ngrid_search = GridSearchCV(classifier, param_grid, scoring=\"accuracy\", n_jobs=-1, cv=kfold, verbose = 1)\n#Fit the Model\ngrid_result = grid_search.fit(X, label_encoded_y)\n\n# Check Execution time\nend_time = time.time()\nexecution_time = (end_time - start_time)\nprint(\"Successfully executed in\", round(execution_time/60,1), \"mins\")\n\n# summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n\tprint(\"%f (%f) with: %r\" % (mean, stdev, param))\n\n# plot the results\npyplot.plot(n_estimators, means)\npyplot.title(\"XGBoost n_estimators vs Accuracy\")\npyplot.xlabel('n_estimators')\npyplot.ylabel('Accuracy')\npyplot.show()    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The best number of trees was <b>n_estimators=100</b> resulting in a <b>Accuracy of 78.67%</b>. we can see that accuracy dropped with more number of trees but it stablizes after that. it means we did not get any significant advantage by adding further number of trees. Successfully executed in <b>1.6 mins</b>"},{"metadata":{},"cell_type":"markdown","source":"<h1 style=\"color:DodgerBlue;\">Tune the Size of Decision Trees in XGBoost </h1>\n"},{"metadata":{},"cell_type":"markdown","source":"\nWe can tune this hyperparameter of XGBoost using the grid search infrastructure in scikit-learn on the Otto dataset. \n<ul>Below we evaluate odd values for<b> max_depth between 3 and 20</b> (3, 4, 5, 6, 8, 10, 15, 20).</ul>\n\nEach of the 8 configurations is evaluated using 10-fold cross validation, resulting in <b>80 models</b> being constructed. The full code listing is provided below for completeness.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Experiment 2\n\n# Create Model Object\nclassifier=xgboost.XGBClassifier(n_estimators=100)\n\n# Initiate start time\nstart_time = time.time()\n\n# Parameters\n#max_depth = [2, 4, 6, 8, 10, 15, 20]\nparam_grid = dict(max_depth=max_depth)\n\n#Cross Validation \nkfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=7)\n#Model creation with GridSearchCV\ngrid_search = GridSearchCV(classifier, param_grid, scoring=\"accuracy\", n_jobs=-1, cv=kfold, verbose = 1)\n#Fit the Model\ngrid_result = grid_search.fit(X, label_encoded_y)\n\n# Check Execution time\nend_time = time.time()\nexecution_time = (end_time - start_time)\nprint(\"Successfully executed in\", round(execution_time/60,1), \"mins\")\n\n# summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n\tprint(\"%f (%f) with: %r\" % (mean, stdev, param))\n\n# plot\npyplot.plot(max_depth, means)\npyplot.title(\"XGBoost Max Depth vs Accuracy\")\npyplot.xlabel('Tree Size')\npyplot.ylabel('Accuracy')\npyplot.show()    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The optimal configuration was <b>max_depth=3</b> resulting in <b>accuracy of 0.799947</b>. Successfully executed in 0.3 mins\n\nReviewing the plot of Accuracy scores, we can see a drop from max_depth=3 to max_depth=10 then pretty even and trending down performance for the rest the values of max_depth.\n\nAlthough the best score was observed for max_depth=3, it is interesting to note that there was <b>practically little difference between using max_depth=8 or max_depth=20.</b>\n\nThis suggests a point of diminishing returns in max_depth on a problem that you can tease out using grid search. <b>Using tree with Depth 3 would solve the purpose.</b>"},{"metadata":{},"cell_type":"markdown","source":"<h1 style=\"color:DodgerBlue;\">Tune The Number of Trees and Max Depth in XGBoost</h1>"},{"metadata":{},"cell_type":"markdown","source":"There is a relationship between the number of trees in the model and the depth of each tree.\n\nWe would expect that deeper trees would result in fewer trees being required in the model, and the inverse where simpler trees (such as decision stumps) require many more trees to achieve similar results.\n\nWe can investigate this relationship by evaluating a grid of n_estimators and max_depth configuration values. To avoid the evaluation taking too long, we will limit the total number of configuration values evaluated. Parameters were chosen to tease out the relationship rather than optimize the model.\n\nWe will create a grid of 10 different n_estimators values (100-1000) and 8 different max_depth values (3-20) and each combination will be evaluated using 5-fold cross validation. <ul>A total of 10 X 8 X 5 or <b>400 models</b> will be trained and evaluated.</ul>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Experiment 3\n# No. of Trees & Max_depth \n\n\n# Create Model Object\nclassifier=xgboost.XGBClassifier()\n\n#Initiate start time\nstart_time = time.time()\n\n# Parameters\n#n_estimators = [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000]\n#max_depth = [2, 4, 6, 8, 10]\nparam_grid = dict(max_depth=max_depth, n_estimators=n_estimators)\n\n#Cross Validation \nkfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=7)\n#Model creation with GridSearchCV\ngrid_search = GridSearchCV(classifier, param_grid, scoring=\"accuracy\", n_jobs=-1, cv=kfold, verbose = 1)\n#Fit the Model\ngrid_result = grid_search.fit(X, label_encoded_y)\n\n\n# Check Execution time\nend_time = time.time()\nexecution_time = (end_time - start_time)\nprint(\"Successfully executed in\", round(execution_time/60,1), \"mins\")\n\n\n# summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n\tprint(\"%f (%f) with: %r\" % (mean, stdev, param))\n\n# plot results\nscores = np.array(means).reshape(len(max_depth), len(n_estimators))\nfor i, value in enumerate(max_depth):\n    pyplot.plot(n_estimators, scores[i], label='depth: ' + str(value))\n\n    pyplot.title(\"XGBoost n_estimators vs Accuracy\")\n    pyplot.xlabel('n_estimators')\n    pyplot.ylabel('Accuracy')\n    pyplot.legend()\n\n    \n# Plot results in Heat map\ndf_scores = pd.DataFrame(scores, columns = (n_estimators), index = (max_depth))\ndf_scores\n\n#import for visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nf,ax = plt.subplots(figsize=(10, 10))\n\n\nsns.heatmap(df_scores, annot = True, cbar = False, linewidths = 0.2, cmap=\"YlGnBu\",\n           xticklabels=n_estimators, yticklabels=max_depth, fmt='.5f')\n\nplt.title('Comparative summary of Accuracy Trend', fontsize = 15) # title with fontsize 20\nplt.xlabel('Trees-Numbers', fontsize = 15) # x-axis label with fontsize 15\nplt.ylabel('Trees-Size', fontsize = 15) # y-axis label with fontsize 15\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here are trends of accuracy for various Tree sizes accross number of trees."},{"metadata":{},"cell_type":"markdown","source":"We can see that the best result was achieved with a <b>n_estimators=100 and max_depth=4</b>. We must look into other scores as well. <b> Depth 20 </b> is a big tree and creating <b> 1000 trees </b> is also time consuming. It is important to see other trends as well.\n<ul>\n  <li>Accuracy reduces as Tree size increases</li>\n  <li>Accuracy reduces as number of trees increase </li>\n  <li>Accuracy reduces as with both increasing together</li>\n</ul>"},{"metadata":{},"cell_type":"markdown","source":"<b>Conclusion:</b> Max_Depth of 4-5 and #Trees till 400 will give us accuracy more than what we have achieved from max_depth 20 and 1000 Trees. this would save resources and time.\n<ul><b>Both MaxDepth and N_estimators should be considered for better accuracy </b></ul>\n\n<b>Future consideration:</b> Try multiple models with changing Learning Rate, reg_lambda, Gamma and reg_alpha."},{"metadata":{},"cell_type":"markdown","source":"<h1 style=\"color:DodgerBlue;\">Tune The Learning Rate in XGBoost</h1>"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Experiment 4\n# Create Model Object\nclassifier=xgboost.XGBClassifier(max_depth=4, n_estimators=100)\n\n# Initiate start time\nstart_time = time.time()\n\n# Parameters\n#learning_rate = [0.01, 0.03, 0.05, 0.10, 0.15, 0.20, 0.25, 0.3] \nparam_grid = dict(learning_rate=learning_rate)\n\n#Cross Validation \nkfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=7)\n#Model creation with GridSearchCV\ngrid_search = GridSearchCV(classifier, param_grid, scoring=\"accuracy\", n_jobs=-1, cv=kfold, verbose = 1)\n#Fit the Model\ngrid_result = grid_search.fit(X, label_encoded_y)\n\n# Check Execution time\nend_time = time.time()\nexecution_time = (end_time - start_time)\nprint(\"Successfully executed in\", round(execution_time/60,1), \"mins\")\n\n# summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n\tprint(\"%f (%f) with: %r\" % (mean, stdev, param))\n\n# plot the results\npyplot.plot(learning_rate, means)\npyplot.title(\"XGBoost learning_rate vs Accuracy\")\npyplot.xlabel('learning_rate')\npyplot.ylabel('Accuracy')\npyplot.show()   ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Successfully executed in 0.2 mins\n<ul>\n    Accuracy peaks at <b>0.05</b> and continuously declines after that."},{"metadata":{},"cell_type":"markdown","source":"<h1 style=\"color:DodgerBlue;\">Tune The Gamma in XGBoost</h1>"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Experiment 5\n# Create Model Object\nclassifier=xgboost.XGBClassifier(max_depth=4, n_estimators=100, learning_rate= 0.05)\n\n# Initiate start time\nstart_time = time.time()\n\n# Parameters\n#gamma = [ 0.0, 0.1, 0.2 , 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9 ]\nparam_grid = dict(gamma=gamma)\n\n#Cross Validation \nkfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=7)\n#Model creation with GridSearchCV\ngrid_search = GridSearchCV(classifier, param_grid, scoring=\"accuracy\", n_jobs=-1, cv=kfold, verbose = 1)\n#Fit the Model\ngrid_result = grid_search.fit(X, label_encoded_y)\n\n# Check Execution time\nend_time = time.time()\nexecution_time = (end_time - start_time)\nprint(\"Successfully executed in\", round(execution_time/60,1), \"mins\")\n\n# summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n\tprint(\"%f (%f) with: %r\" % (mean, stdev, param))\n\n# plot the results\npyplot.plot(gamma, means)\npyplot.title(\"XGBoost Gamma vs Accuracy\")\npyplot.xlabel('Gamma')\npyplot.ylabel('Accuracy')\npyplot.show()   ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Successfully executed in <b>0.2 mins</b>\n<ul>\nAccuracy flatuates Gamma below 1. <b>Improves above 1 till 10</b> followed by decline."},{"metadata":{},"cell_type":"markdown","source":"<h1 style=\"color:DodgerBlue;\">Tune The Learning Rate & Gamma in XGBoost</h1>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Experiment 6\n# Learning Rate & Gamma\n\n\n# Create Model Object\nclassifier=xgboost.XGBClassifier(max_depth=4, n_estimators=100)\n\n#Initiate start time\nstart_time = time.time()\n\n# Parameters\n# learning_rate = [0.01, 0.03, 0.05, 0.10, 0.15, 0.20, 0.25, 0.3] \n# gamma = [ 0.0, 0.1,  0.4,   0.9, 1 , 2, 5 , 10, 15, 20 ]\nparam_grid = dict(learning_rate=learning_rate, gamma=gamma)\n\n#Cross Validation \nkfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=7)\n#Model creation with GridSearchCV\ngrid_search = GridSearchCV(classifier, param_grid, scoring=\"accuracy\", n_jobs=-1, cv=kfold, verbose = 1)\n#Fit the Model\ngrid_result = grid_search.fit(X, label_encoded_y)\n\n\n# Check Execution time\nend_time = time.time()\nexecution_time = (end_time - start_time)\nprint(\"Successfully executed in\", round(execution_time/60,1), \"mins\")\n\n\n# summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n\tprint(\"%f (%f) with: %r\" % (mean, stdev, param))\n\n# plot results\nscores = np.array(means).reshape(len(gamma), len(learning_rate))\nfor i, value in enumerate(gamma):\n    pyplot.plot(learning_rate, scores[i], label='gamma: ' + str(value))\n\n    pyplot.title(\"XGBoost learning_rate vs Accuracy\")\n    pyplot.xlabel('learning_rate')\n    pyplot.ylabel('Accuracy')\n    pyplot.legend()\n\n# Plot results in Heat map\ndf_scores = pd.DataFrame(scores, columns = (learning_rate), index = (gamma))\ndf_scores\n\n#import for visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nf,ax = plt.subplots(figsize=(10, 10))\n\n\nsns.heatmap(df_scores, annot = True, cbar = False, linewidths = 0.2, cmap=\"YlGnBu\",\n           xticklabels=learning_rate, yticklabels=gamma, fmt='.5f')\n\nplt.title('Comparative summary of Accuracy Trend', fontsize = 15) # title with fontsize 20\nplt.xlabel('learning_rate', fontsize = 15) # x-axis label with fontsize 15\nplt.ylabel('gamma', fontsize = 15) # y-axis label with fontsize 15\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Successfully executed in <b>0.7 mins</b>\n<ul>\nAccuracy improves as learning rate increase from 0.05 till 0.15. <b>Accuracy peaks learning rate 0.05 to 0.1. gamma 5 & 10</b> followed by slow decline."},{"metadata":{},"cell_type":"markdown","source":"<h1 style=\"color:DodgerBlue;\">Tune with best parameters found till now</h1>"},{"metadata":{},"cell_type":"markdown","source":"<li>gamma=10, </li>\n<li>learning_rate=0.05,  </li>\n<li>max_depth=4, </li>\n<li>n_estimators=500,  </li>\n<li>verbosity=1 </li>"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Experiment 7\nclassifier=xgboost.XGBClassifier(gamma=10, learning_rate=0.05, max_depth=4,\n              n_estimators=600, verbosity=1)\n\nfrom sklearn.model_selection import cross_val_score\nkfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=7)\nscore=cross_val_score(classifier,X,label_encoded_y,cv=kfold)\nscore.mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1 style=\"color:DodgerBlue;\">Find Best Parameters in XGBoost with Random Search</h1>"},{"metadata":{},"cell_type":"markdown","source":"Find Best Parameters with Random Search."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Experiment 8\n# Create Model Object\nclassifier=xgboost.XGBClassifier()\n\n# Initiate start time\nstart_time = time.time()\n\n# Parameters\nparams = dict(learning_rate = learning_rate, max_depth = max_depth, min_child_weight = min_child_weight, gamma = gamma, colsample_bytree = colsample_bytree, n_estimators = n_estimators )\n\n# Check Best parameters\nrandom_search=RandomizedSearchCV(classifier,param_distributions=params,n_iter=5,scoring='accuracy',n_jobs=-1,cv=5,verbose=1)\nrandom_result = random_search.fit(X,label_encoded_y)\n\n# Check Execution time\nend_time = time.time()\nexecution_time = (end_time - start_time)\nprint(\"Successfully executed in\", round(execution_time/60,1), \"mins\")\n\nrandom_result.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier=xgboost.XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n              colsample_bynode=1, colsample_bytree=0.3, gamma=5, gpu_id=-1,\n              importance_type='gain', interaction_constraints='',\n              learning_rate=0.03, max_delta_step=0, max_depth=4,\n              min_child_weight=5,  monotone_constraints='()',\n              n_estimators=1000, n_jobs=0, num_parallel_tree=1, random_state=0,\n              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n              tree_method='exact', validate_parameters=1, verbosity=1)\n\nfrom sklearn.model_selection import cross_val_score\nkfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=7)\nscore=cross_val_score(classifier,X,y,cv=kfold)\nscore.mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b>Conclusion:</b> We achieved Accuracy close to parameters found by best estimators with Random Search CV\n<ul><b>Both MaxDepth, N_estimators, Learning Rate & Gamma are key parameters for better accuracy </b></ul>\n"},{"metadata":{},"cell_type":"markdown","source":"<h1 style=\"color:DodgerBlue;\">Find Best Parameters in XGBoost with Optuna</h1>"},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nOptuna example that optimizes a classifier configuration for Telco Churn dataset\nusing XGBoost.\nIn this example, we optimize the validation accuracy of churn detection\nusing XGBoost. We optimize both the choice of booster model and their hyper\nparameters.\n\"\"\"\n\nimport numpy as np\nimport sklearn.datasets\nimport sklearn.metrics\nfrom sklearn.model_selection import train_test_split\nimport xgboost as xgb\n\nimport optuna\n\n\n# FYI: Objective functions can take additional arguments\n# (https://optuna.readthedocs.io/en/stable/faq.html#objective-func-additional-args).\n\ndef objective(trial):\n    (data, target) = (X,y)\n\n    train_x, valid_x, train_y, valid_y = train_test_split(data, target, test_size=0.25)\n    dtrain = xgb.DMatrix(train_x, label=train_y)\n    dvalid = xgb.DMatrix(valid_x, label=valid_y)\n\n    param = {\n        \"verbosity\": 0,\n        \"objective\": \"binary:logistic\",\n        \"booster\": trial.suggest_categorical(\"booster\", [\"gbtree\", \"gblinear\", \"dart\"]),\n        \"lambda\": trial.suggest_float(\"lambda\", 1e-8, 1.0, log=True),\n        \"alpha\": trial.suggest_float(\"alpha\", 1e-8, 1.0, log=True),\n    }\n\n    if param[\"booster\"] == \"gbtree\" or param[\"booster\"] == \"dart\":\n        param[\"max_depth\"] = trial.suggest_int(\"max_depth\", 1, 9)\n        param[\"eta\"] = trial.suggest_float(\"eta\", 1e-8, 1.0, log=True)\n        param[\"gamma\"] = trial.suggest_float(\"gamma\", 1e-8, 1.0, log=True)\n        param[\"grow_policy\"] = trial.suggest_categorical(\"grow_policy\", [\"depthwise\", \"lossguide\"])\n    if param[\"booster\"] == \"dart\":\n        param[\"sample_type\"] = trial.suggest_categorical(\"sample_type\", [\"uniform\", \"weighted\"])\n        param[\"normalize_type\"] = trial.suggest_categorical(\"normalize_type\", [\"tree\", \"forest\"])\n        param[\"rate_drop\"] = trial.suggest_float(\"rate_drop\", 1e-8, 1.0, log=True)\n        param[\"skip_drop\"] = trial.suggest_float(\"skip_drop\", 1e-8, 1.0, log=True)\n\n    bst = xgb.train(param, dtrain)\n    preds = bst.predict(dvalid)\n    pred_labels = np.rint(preds)\n    accuracy = sklearn.metrics.accuracy_score(valid_y, pred_labels)\n    return accuracy\n\n\nif __name__ == \"__main__\":\n    study = optuna.create_study(direction=\"maximize\")\n    study.optimize(objective, n_trials=100, timeout=600)\n\n    print(\"Number of finished trials: \", len(study.trials))\n    print(\"Best trial:\")\n    trial = study.best_trial\n\n    print(\"  Value: {}\".format(trial.value))\n    print(\"  Params: \")\n    for key, value in trial.params.items():\n        print(\"    {}: {}\".format(key, value))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}