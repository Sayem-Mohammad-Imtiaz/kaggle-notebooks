{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"!pip install cdqa ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"'''Import all necessary modules'''\n!pip install --upgrade pandas\nimport pandas as pd\nimport numpy as np\nimport datetime\nimport matplotlib.pyplot as plt\nimport re\nimport os\nimport json\nimport nltk \nfrom math import log, sqrt\nfrom collections import defaultdict\nfrom copy import deepcopy\nimport glob","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"''' Load csv to dataframe'''\nmy_df= pd.DataFrame([])\n\n'''Source file path ( change if needed )'''\nfile_path = \"../input/CORD-19-research-challenge/\"\n\n'''Some computers lag So we load it chunk wise'''\nfor df in pd.read_csv(file_path+'metadata.csv', iterator=True, chunksize=10000):  \n    my_df = my_df.append(pd.DataFrame(df))\n\n# '''Else remove comment and run the code mentioned below'''\n# my_df = pd.read_csv(file_path+\"metadata.csv\")#,dtype={'cord_uid': str,'sha':str,'source_x':str,'title':str,'doi':str,'pmcid':str,'license':str,'abstract':str,'publish_time':str,'authors':str,'journal':str,'mag_id':str,'arxiv':str,'pdf_json_files':str,'pmc_json_files':str,'url':str,'s2_id':str})\n\nmy_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"my_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''Count the valid entries for each column'''\ndef display_barchart(my_df):\n    column = my_df.columns.tolist()\n    # https://datatofish.com/convert-pandas-dataframe-to-list/\n    valid_cnt = list(my_df.count())\n    # Visualize the valid entries with bar chart\n    plt.bar(column,valid_cnt,align = \"center\",width = 0.5,alpha = 1)\n    plt.xticks(rotation=90)\n\n    \ndisplay_barchart(my_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"my_df.drop_duplicates(subset=['title'],keep='first')\nmy_df.drop_duplicates(subset=['abstract'],keep='first')\nmy_df.drop_duplicates(subset=['doi'],keep='first')\nmy_df.count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"''' Abstract plays an important role in finding related journals because it contains the objective or             overview     of the literature\n    We visualize it with bar chart to get insights'''\nvalid_cnt = list(my_df.count())\nwith_abstract = valid_cnt[8]\ntotal = valid_cnt[0]\nwithout_abstract  = total - with_abstract\n\ntot = plt.bar(1,total,color='green',width = 1)\nplt.annotate(str(total), xy=(1,total), ha='center', va='bottom')\n\nabstract = plt.bar(2,with_abstract,color='yellow',width = 1)\nplt.annotate(str(with_abstract), xy=(2,with_abstract), ha='center', va='bottom')\n\nlost = plt.bar(3,without_abstract,color='red',width = 1)\nplt.annotate(str(without_abstract), xy=(3,without_abstract), ha='center', va='bottom')\n\nplt.legend((tot, abstract,lost), ('Total literature', 'literatures with abstract','literatures without abstract'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"''' Remove empty rows'''\nmy_df.dropna(how='all')\n\n''' We have 2 options right now:\n    1. Remove rows without abstract\n    2. Add abstract from  the first paragraph of the literature'''\n\n'''Remove rows without abstract'''\nmy_df.dropna(subset=['abstract'],inplace=True)\nmy_df.count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"''' Covid-19 is found in 2019\n    So there is no need to keep literatures that was published before 2019'''\nmy_df = my_df[pd.DatetimeIndex(my_df.publish_time).year>2018]\n# https://www.interviewqs.com/ddi_code_snippets/extract_month_year_pandas\n# https://stackoverflow.com/questions/13851535/delete-rows-from-a-pandas-dataframe-based-on-a-conditional-expression-involving\nmy_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"''' Our task is related to incubation, transmission and environmental effects, \n    So we collect words from literature survey which are related to our topic \n    and store it in 'key_words'. '''\n''' To be updated '''\ncovid_terms =['covid', 'coronavirus disease 19', 'sars cov 2', '2019 ncov', '2019ncov', '2019 n cov', '2019n cov',\n              'ncov 2019', 'n cov 2019', 'coronavirus 2019', 'wuhan pneumonia', 'wuhan virus', 'wuhan coronavirus',\n              'coronavirus 2', 'covid-19', 'SARS-CoV-2', '2019-nCov']\ncovid_terms = [elem.lower() for elem in covid_terms]\ncovid_terms = re.compile('|'.join(covid_terms))\n\ndef checkYear(date):\n    return int(date[0:4])\n\ndef checkCovid(row, covid_terms):\n    return bool(covid_terms.search(row['abstract'].lower())) and checkYear(row['publish_time']) > 2019","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"my_df['is_covid'] = my_df.apply(checkCovid, axis=1, covid_terms=covid_terms)\nmy_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_covid_only = my_df[my_df['is_covid']==True]\ndf_covid_only = df_covid_only.reset_index(drop=True)\ndf_covid_only.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"key_words = ['transmission','transmitted','long','symptomatic','asymptomatic','infected','infection','range', 'incubation', 'periods', 'surfaces', 'prevent','protective','SARS-CoV-2','infectious','reported','respiratory', 'secretions', 'saliva', 'droplets','short', 'time', 'fomites','sanitation']\npattern = '|'.join(key_words)\ndf_covid_only = df_covid_only.loc[df_covid_only['abstract'].str.contains(pattern, case=False)]\ndf_covid_only.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''Check if the result is correct'''\npd.options.display.max_colwidth = 100000\ndf_covid_only.head().abstract\ndisplay_barchart(df_covid_only)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''Take only those files which contain the json_file linked within them because these are the json files which contain the original info of the article'''\ndf_covid_only.dropna(subset=['pdf_json_files'],inplace=True)\ndf_covid_only.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"base_path = '../input/CORD-19-research-challenge/'\nall_selected_json = df_covid_only['pdf_json_files']\n# print(all_selected_json)\nprint(base_path+all_selected_json[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# This piece of code was adopted from the original source at:\n# https://www.kaggle.com/xhlulu/cord-19-eda-parse-json-and-generate-clean-csv/notebook \n\ndef format_name(author):\n    middle_name = \" \".join(author['middle'])\n    if author['middle']:\n        return \" \".join([author['first'], middle_name, author['last']])\n    else:\n        return \" \".join([author['first'], author['last']])\n\ndef format_affiliation(affiliation):\n    text = []\n    location = affiliation.get('location')\n    if location:\n        text.extend(list(affiliation['location'].values()))\n    institution = affiliation.get('institution')\n    if institution:\n        text = [institution] + text\n    return \", \".join(text)\n\ndef format_authors(authors, with_affiliation=False):\n    name_ls = []\n    for author in authors:\n        name = format_name(author)\n        if with_affiliation:\n            affiliation = format_affiliation(author['affiliation'])\n            if affiliation:\n                name_ls.append(f\"{name} ({affiliation})\")\n            else:\n                name_ls.append(name)\n        else:# First, for each query the system arranges all the scientific papers within the corpus in the relevant order.\n# Second, the system analize texts of top N the mosr relevant papers to answer to the query in the best way.\n            name_ls.append(name)\n    return \", \".join(name_ls)\n\ndef format_body(body_text):\n    texts = [(di['section'], di['text']) for di in body_text]\n    texts_di = {di['section']: \"\" for di in body_text}\n    for section, text in texts:\n        texts_di[section] += text\n    body = \"\"\n    for section, text in texts_di.items():\n        body += section\n        body += \"\\n\\n\"\n        body += text\n        body += \"\\n\\n\"\n    return body\n\ndef format_bib(bibs):\n    if type(bibs) == dict:\n        bibs = list(bibs.values())\n    bibs = deepcopy(bibs)\n    formatted = []\n    for bib in bibs:\n        bib['authors'] = format_authors(\n            bib['authors'], \n            with_affiliation=False\n        )\n        formatted_ls = [str(bib[k]) for k in ['title', 'authors', 'venue', 'year']]\n        formatted.append(\", \".join(formatted_ls))\n    return \"; \".join(formatted)\n\ndef load_files(file_count,filenames_selected = all_selected_json):\n#     filenames = os.listdir(dirname)\n\n    raw_files = []\n#     if filename:\n#         filename = dirname + filename\n#         raw_files = [json.load(open(filename, 'rb'))]\n#     else:\n#         #for filename in tqdm(filenames):\n    i = 0\n    for filename in filenames_selected:\n        if i == file_count:\n            break\n        \n        try:\n            file = json.load(open(filename, 'rb'))\n            raw_files.append(file)   \n            i = i+1\n        except:\n            try:\n                filename = base_path + filename\n                file = json.load(open(filename, 'rb'))\n                raw_files.append(file)\n                i = i+1\n            except:\n                x = 1\n                #print(filename)\n    return raw_files\n    \n\ndef generate_clean_df(all_files):\n    cleaned_files = []\n    #for file in tqdm(all_files):\n#     i = 0\n    for file in all_files:\n#         if i == 500:\n#             break\n# #         print(i)\n#         i  = i + 1\n        features = [\n            file['paper_id'],\n            file['metadata']['title'],\n            format_authors(file['metadata']['authors']),\n            format_authors(file['metadata']['authors'], \n                           with_affiliation=True),\n            format_body(file['abstract']),\n            format_body(file['body_text']),\n            format_bib(file['bib_entries']),\n            file['metadata']['authors'],\n            file['bib_entries']\n        ]\n        cleaned_files.append(features)\n    col_names = ['paper_id', 'title', 'authors',\n                 'affiliations', 'abstract', 'text', \n                 'bibliography','raw_authors','raw_bibliography']\n    clean_df = pd.DataFrame(cleaned_files, columns=col_names)\n    clean_df = clean_df.drop(columns=['authors','affiliations','bibliography',\n                                      'raw_authors','raw_bibliography'])\n    return clean_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_corpus():\n    num_of_papers = {}\n    corpus = pd.DataFrame(columns=['paper_id','title','abstract','text'])\n    \n    file_count = 40928\n    \n    print('Reading ', file_count, 'json files')\n#     num_of_papers[folder_names[i]] = len(filenames)\n    print('Loading......')\n    files = load_files(file_count)\n    print('Generating clean dataframe')\n    df = generate_clean_df(files)\n    print('Generated......')\n    print('Forming Corpus.......')\n    corpus = pd.concat([corpus, df], ignore_index=True, sort=False)\n    print('4')\n    \n    print('Corpus includes {0} scientific articles.'.format(len(corpus)))\n    return corpus, num_of_papers\n\ncorpus, num_of_papers = get_corpus()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# This processing algorithm can originaly be found at:\n# https://github.com/nilayjain/text-search-engine\n\ninverted_index = defaultdict(list)\nnum_of_documents = len(corpus)\nvects_for_docs = []  # we will need nos of docs number of vectors, each vector is a dictionary\ndocument_freq_vect = {}  # sort of equivalent to initializing the number of unique words to 0\n\n# It updates the vects_for_docs variable with vectors of all the documents.\ndef iterate_over_all_docs():\n    print('Processing corpus...')\n    for i in range(num_of_documents):\n        if np.mod(i, 1000) == 0:\n            print('{0} of {1}'.format(str(i).zfill(len(str(num_of_documents))),num_of_documents))\n        doc_text = corpus['title'][i] + ' ' + corpus['abstract'][i] + ' ' + corpus['text'][i]\n        token_list = get_tokenized_and_normalized_list(doc_text)\n        vect = create_vector(token_list)\n        vects_for_docs.append(vect)\n    print('{0} of {1}'.format(num_of_documents, num_of_documents))\n\ndef create_vector_from_query(l1):\n    vect = {}\n    for token in l1:\n        if token in vect:\n            vect[token] += 1.0\n        else:\n            vect[token] = 1.0\n    return vect\n\ndef generate_inverted_index():\n    count1 = 0\n    for vector in vects_for_docs:\n        for word1 in vector:\n            inverted_index[word1].append(count1)\n        count1 += 1\n\ndef create_tf_idf_vector():\n    vect_length = 0.0\n    for vect in vects_for_docs:\n        for word1 in vect:\n            word_freq = vect[word1]\n            temp = calc_tf_idf(word1, word_freq)\n            vect[word1] = temp\n            vect_length += temp ** 2\n        vect_length = sqrt(vect_length)\n        for word1 in vect:\n            vect[word1] /= vect_length\n\ndef get_tf_idf_from_query_vect(query_vector1):\n    vect_length = 0.0\n    for word1 in query_vector1:\n        word_freq = query_vector1[word1]\n        if word1 in document_freq_vect:\n            query_vector1[word1] = calc_tf_idf(word1, word_freq)\n        else:\n            query_vector1[word1] = log(1 + word_freq) * log(\n                num_of_documents)\n        vect_length += query_vector1[word1] ** 2\n    vect_length = sqrt(vect_length)\n    if vect_length != 0:\n        for word1 in query_vector1:\n            query_vector1[word1] /= vect_length\n\ndef calc_tf_idf(word1, word_freq):\n    return log(1 + word_freq) * log(num_of_documents / document_freq_vect[word1])\n\ndef get_dot_product(vector1, vector2):\n    if len(vector1) > len(vector2):\n        temp = vector1\n        vector1 = vector2\n        vector2 = temp\n    keys1 = vector1.keys()\n    keys2 = vector2.keys()\n    sum = 0\n    for i in keys1:\n        if i in keys2:\n            sum += vector1[i] * vector2[i]\n    return sum\n\ndef get_tokenized_and_normalized_list(doc_text):\n    tokens = nltk.word_tokenize(doc_text)\n    ps = nltk.stem.PorterStemmer()\n    stemmed = []\n    for words in tokens:\n        stemmed.append(ps.stem(words))\n    return stemmed\n\ndef create_vector(l1):\n    vect = {}  # this is a dictionary\n    global document_freq_vect\n    for token in l1:\n        if token in vect:\n            vect[token] += 1\n        else:\n            vect[token] = 1\n            if token in document_freq_vect:\n                document_freq_vect[token] += 1\n            else:\n                document_freq_vect[token] = 1\n    return vect\n\ndef get_result_from_query_vect(query_vector1):\n    parsed_list = []\n    for i in range(num_of_documents - 0):\n        dot_prod = get_dot_product(query_vector1, vects_for_docs[i])\n        parsed_list.append((i, dot_prod))\n        parsed_list = sorted(parsed_list, key=lambda x: x[1])\n    return parsed_list\n\niterate_over_all_docs()\ngenerate_inverted_index()\ncreate_tf_idf_vector()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The End-To-End Closed Domain Question Answering System is used here.\n# It is available at: https://pypi.org/project/cdqa/\n\nfrom cdqa.utils.filters import filter_paragraphs\nfrom cdqa.utils.download import download_model, download_bnpp_data\nfrom cdqa.pipeline.cdqa_sklearn import QAPipeline\n\ndownload_bnpp_data(dir='./data/bnpp_newsroom_v1.1/')\ndownload_model(model='bert-squad_1.1', dir='./models')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def find_relevant_articles(query=None, top_n_papers=20, min_n_papers=3):\n    if query == None:\n        query = input('Please enter your query...')\n    print('\\n\\n'+'*'*34+' PROCESSING NEW QUERY '+'*'*34+'\\n')   \n    query_list = get_tokenized_and_normalized_list(query)\n    query_vector = create_vector_from_query(query_list)\n    get_tf_idf_from_query_vect(query_vector)\n    result_set = get_result_from_query_vect(query_vector)\n    papers_info = {'query':query, 'query list':query_list, 'query vector':query_vector,\n                   'id':[], 'title':[], 'abstract':[], 'text':[], 'weight':[], 'index':[]}\n    for i in range(1, top_n_papers+1):\n        tup = result_set[-i]\n        papers_info['id'].append(corpus['paper_id'][tup[0]])\n        papers_info['title'].append(corpus['title'][tup[0]])\n        papers_info['abstract'].append(corpus['abstract'][tup[0]])\n        papers_info['text'].append(corpus['text'][tup[0]])\n        papers_info['weight'].append(tup[1])\n        papers_info['index'].append(tup[0])\n    colms = ['date', 'title', 'category', 'link', 'abstract', 'paragraphs']\n    df = pd.DataFrame(columns=colms)\n    for i in range(len(papers_info['text'])):\n        papers_info['text'][i] = papers_info['text'][i].replace('\\n\\n', ' ')\n        CurrentText = papers_info['text'][i]\n        CurrentText = CurrentText.split('. ')\n        #CurrentList = [\"None\", papers_info['title'][i], \"None\", \"None\", \"None\", CurrentText]\n        CurrentList = [\"None\", papers_info['title'][i], \"None\", \"None\", papers_info['abstract'][i], CurrentText]\n        CurrentList = np.array(CurrentList)\n        CurrentList = CurrentList.reshape(1, CurrentList.shape[0])\n        CurrentList = pd.DataFrame(data = CurrentList, columns=colms)\n        df = pd.concat([df, CurrentList], ignore_index=True)\n    df = filter_paragraphs(df)\n    # Loading QAPipeline with CPU version of BERT Reader pretrained on SQuAD 1.1\n    cdqa_pipeline = QAPipeline(reader='models/bert_qa.joblib')\n    # Fitting the retriever to the list of documents in the dataframe\n    cdqa_pipeline.fit_retriever(df=df)\n    # Sending a question to the pipeline and getting prediction\n    query = papers_info['query']\n    prediction = cdqa_pipeline.predict(query=query)\n    for i in range(top_n_papers):\n        if papers_info['title'][i] == prediction[1]:\n            pid = papers_info['id'][i]\n    response = {query:{'id':pid,'title':prediction[1],'answer':prediction[0],'summary':prediction[2],\n                       'important papers':{'id':papers_info['id'],'title':papers_info['title']}}}\n    print('QUERY: {0}\\n'.format(query))\n    print('ANSWER MINED FROM PAPER: {0}\\n'.format(prediction[0]))\n    print('PAPER TITLE: {0}\\n'.format(prediction[1]))\n    print('PARAGRAPH IN PAPER: {0}\\n'.format(prediction[2]))\n    show_paper = np.min([min_n_papers, top_n_papers])\n    print('\\nTOP {0} MOST RELEVANT PAPERS RELATED TO THE QUERY:\\n'.format(show_paper))\n    for i in range(show_paper):\n        print('PAPER #{0}. \\nID: {1} \\nTITLE: {2}\\n'.format(i+1, papers_info['id'][i], papers_info['title'][i]))\n    return response, papers_info, prediction, result_set, df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#List of queries\nqueries = ['What is range of incubation period for coronavirus SARS-CoV-2 COVID-19 in humans',\n           'What is optimal quarantine period for coronavirus COVID-19',\n           'What is effective quarantine period for coronavirus COVID-19',\n           'What is percentage of death cases for coronavirus SARS-CoV-2 COVID-19',\n           'What is death rate for coronavirus COVID-19 and air pollution',\n           'At which temperature coronavirus COVID-19 can survive',\n           'How long coronavirus SARS-CoV-2 can survive on plastic surface',\n           'What are risk factors for coronavirus COVID-19',\n           'What is origin of coronavirus COVID-19',\n           'At which temperature coronavirus cannot survive'\n           'What is the range of incubation periods for coronavirus SARS-CoV-2 COVID-19 in humans (and how this varies across age and health status) and how long individuals are contagious, even after recovery',\n           'What are the prevalence of asymptomatic shedding and transmission of coronavirus COVID-19(e.g., particularly children)',\n           'Mention the seasonality COVID-19 transmission',\n           'Explain the physical science of the coronavirus (e.g., charge distribution, adhesion to hydrophilic/phobic surfaces, environmental survival to inform decontamination efforts for affected areas and provide information about viral shedding)',\n           'What is the period of persistence and stability of coronavirus on a multitude of substrates and sources (e.g., nasal discharge, sputum, urine, fecal matter, blood)',\n           'How long does coronavirus persists on surfaces of different materials (e,g., copper, stainless steel, plastic)',\n           'Explain the natural history of the coronavirus COVID-19 and shedding of it from an infected person',\n           'What is the process of implementation of diagnostics and products to improve clinical processes',\n           'What are the disease models for coronavirus COVID-19,including animal models for infection, disease and transmission',\n           'Mention the tools and studies to monitor phenotypic change and potential adaptation of the coronavirus COVID-19',\n           'What is the immune response and immunity of a person affected by coronavirus COVID-19',\n           'What are the effective measures to be taken to prevent COVID-19 secondary transmission in health care and community settings',\n           'What is the effectiveness of personal protective equipment (PPE) and its usefulness to reduce risk of transmission of coronavirus COVID-19 in health care and community settings',\n           'What is the role of the environment in COVID-19 transmission'] ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" for query in queries:\n     response, papers_info, prediction, result_set, df = find_relevant_articles(query, top_n_papers=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"query = 'What is the incubation period for covid19 ?'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"find_relevant_articles(query=query, top_n_papers=5, min_n_papers=5);","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}