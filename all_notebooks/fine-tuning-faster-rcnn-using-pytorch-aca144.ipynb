{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **FINE TUNING FASTER RCNN USING PYTORCH**\n\nHello Everyone!\n\nIn this Notebook I will show you how we can fine tune a Faster RCNN on the fruits images dataset. If you want to brush up about what is Faster RCNN, [here's](https://medium.com/@whatdhack/a-deeper-look-at-how-faster-rcnn-works-84081284e1cd) an awesome medium article on the same.\n\nThe code is inspired by the Pytorch docs tutorial [here](https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html)\n","metadata":{}},{"cell_type":"markdown","source":"## Installs and Imports","metadata":{}},{"cell_type":"markdown","source":"Since a lot of code for object detection is same and has to be rewritten by everyone, torchvision contributers have provided us with helper codes for training, evaluation and transformations.\n\nLet's clone the repo and copy the libraries into working directory","metadata":{}},{"cell_type":"code","source":"# Download TorchVision repo to use some files from\n# references/detection\n!pip install pycocotools --quiet\n!git clone https://github.com/pytorch/vision.git\n!git checkout v0.3.0\n\n!cp vision/references/detection/utils.py ./\n!cp vision/references/detection/transforms.py ./\n!cp vision/references/detection/coco_eval.py ./\n!cp vision/references/detection/engine.py ./\n!cp vision/references/detection/coco_utils.py ./","metadata":{"execution":{"iopub.status.busy":"2021-06-21T21:16:52.938887Z","iopub.execute_input":"2021-06-21T21:16:52.939282Z","iopub.status.idle":"2021-06-21T21:17:03.539586Z","shell.execute_reply.started":"2021-06-21T21:16:52.939247Z","shell.execute_reply":"2021-06-21T21:17:03.538389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lets import the libraries","metadata":{}},{"cell_type":"code","source":"# Basic python and ML Libraries\nimport os\nimport random\nimport numpy as np\nimport pandas as pd\n# for ignoring warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# We will be reading images using OpenCV\nimport cv2\n\n# xml library for parsing xml files\nfrom xml.etree import ElementTree as et\n\n# matplotlib for visualization\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\n\n# torchvision libraries\nimport torch\nimport torchvision\nfrom torchvision import transforms as torchtrans  \nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n\n# these are the helper libraries imported.\nfrom engine import train_one_epoch, evaluate\nimport utils\nimport transforms as T\n\n# for image augmentations\nimport albumentations as A\nfrom albumentations.pytorch.transforms import ToTensorV2\n","metadata":{"execution":{"iopub.status.busy":"2021-06-21T21:17:03.543571Z","iopub.execute_input":"2021-06-21T21:17:03.543876Z","iopub.status.idle":"2021-06-21T21:17:03.551278Z","shell.execute_reply.started":"2021-06-21T21:17:03.543845Z","shell.execute_reply":"2021-06-21T21:17:03.550421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dataset ","metadata":{}},{"cell_type":"markdown","source":"Lets build the fruits images dataset!","metadata":{}},{"cell_type":"code","source":"# defining the files directory and testing directory\nfiles_dir = '../input/singapore-maritime/Singapore_maritime_dataset_1/train'\ntest_dir = '../input/singapore-maritime/Singapore_maritime_dataset_1/test'\n\n\nclass FruitImagesDataset(torch.utils.data.Dataset):\n\n    def __init__(self, files_dir, width, height, transforms=None):\n        self.transforms = transforms\n        self.files_dir = files_dir\n        self.height = height\n        self.width = width\n        \n        # sorting the images for consistency\n        # To get images, the extension of the filename is checked to be jpg\n        self.imgs = [image for image in sorted(os.listdir(files_dir))\n                        if image[-4:]=='.jpg']\n        \n        \n        # classes: 0 index is reserved for background\n        self.classes = ['Ferry','Buoy','Vessel/ship','Speed boat','Boat', 'Kayak', 'Sail boat', 'Swimming person', 'Flying bird/plane', 'Other']\n        \n    def __getitem__(self, idx): \n\n        img_name = self.imgs[idx]\n        image_path = os.path.join(self.files_dir, img_name)\n\n        # reading the images and converting them to correct size and color    \n        img = cv2.imread(image_path)\n        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB).astype(np.float32)\n        img_res = cv2.resize(img_rgb, (self.width, self.height), cv2.INTER_AREA)\n        # diving by 255\n        img_res /= 255.0\n        \n        # annotation file\n        annot_filename = img_name[:-4] + '.xml'\n        annot_file_path = os.path.join(self.files_dir, annot_filename)\n        \n        boxes = []\n        labels = []\n        tree = et.parse(annot_file_path)\n        root = tree.getroot()\n        \n        # cv2 image gives size as height x width\n        wt = img.shape[1]\n        ht = img.shape[0]\n        \n        # box coordinates for xml files are extracted and corrected for image size given\n        for member in root.findall('object'):\n            labels.append(self.classes.index(member.find('name').text))\n            \n            # bounding box\n            xmin = float(member.find('bndbox').find('xmin').text)\n            xmin=abs(xmin)\n            if(xmin>1):\n                xmin=0.5\n            xmax = float(member.find('bndbox').find('xmax').text)\n            xmax=abs(xmax)\n            if(xmax>1):\n                xmax=0.8\n            ymin = float(member.find('bndbox').find('ymin').text)\n            ymin=abs(ymin)\n            if(ymin>1):\n                ymin=0.4\n            ymax = float(member.find('bndbox').find('ymax').text)\n            ymax=abs(ymax)\n            if(ymax>1):\n                ymax=0.8\n            \n            \n            xmin_corr = (xmin/wt)*self.width\n            xmax_corr = (xmax/wt)*self.width\n            ymin_corr = (ymin/ht)*self.height\n            ymax_corr = (ymax/ht)*self.height\n            \n            boxes.append([xmin_corr, ymin_corr, xmax_corr, ymax_corr])\n        \n        # convert boxes into a torch.Tensor\n        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n        \n        # getting the areas of the boxes\n        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n\n        # suppose all instances are not crowd\n        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)\n        \n        labels = torch.as_tensor(labels, dtype=torch.int64)\n\n\n        target = {}\n        target[\"boxes\"] = boxes\n        target[\"labels\"] = labels\n        #target[\"area\"] = area\n        target[\"iscrowd\"] = iscrowd\n        # image_id\n        image_id = torch.tensor([idx])\n        target[\"image_id\"] = image_id\n\n\n        if self.transforms:\n            \n            sample = self.transforms(image = img_res,\n                                     bboxes = target['boxes'],\n                                     labels = labels)\n            \n            img_res = sample['image']\n            target['boxes'] = torch.Tensor(sample['bboxes'])\n            \n            \n            \n        return img_res, target\n\n    def __len__(self):\n        return len(self.imgs)\n\n\n# check dataset\ndataset = FruitImagesDataset(files_dir, 480, 480)\nprint('length of dataset = ', len(dataset), '\\n')\n\n# getting the image and target for a test index.  Feel free to change the index.\nimg, target = dataset[1]\nprint(img.shape, '\\n',target)","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","execution":{"iopub.status.busy":"2021-06-21T21:17:03.552625Z","iopub.execute_input":"2021-06-21T21:17:03.553274Z","iopub.status.idle":"2021-06-21T21:17:03.644545Z","shell.execute_reply.started":"2021-06-21T21:17:03.553185Z","shell.execute_reply":"2021-06-21T21:17:03.643613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset[12]","metadata":{"execution":{"iopub.status.busy":"2021-06-21T21:17:03.646304Z","iopub.execute_input":"2021-06-21T21:17:03.646684Z","iopub.status.idle":"2021-06-21T21:17:03.694645Z","shell.execute_reply.started":"2021-06-21T21:17:03.646644Z","shell.execute_reply":"2021-06-21T21:17:03.693931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Points to be noted -\n1. The dataset returns a tuple. The first element is the image shape and the second element is a dictionary.\n2. The image is of the size, we provided while defining the dataset and the color mode is RGB.\n3. There are four bounding boxes in the image which is evident from four lists in boxes and length of labels.","metadata":{}},{"cell_type":"markdown","source":"And its done! \n\nDataset building is one of the hardest things in the notebook. If you got till here while understand all of the above, you are doing pretty good!\n\nLet's now see, what our data looks like. The function is inspired from [here](https://www.kaggle.com/kiwifairy/visualize-x-ray-image-with-bounding-boxes)","metadata":{}},{"cell_type":"markdown","source":"# Visualization","metadata":{}},{"cell_type":"code","source":"# Function to visualize bounding boxes in the image\n\ndef plot_img_bbox(img, target):\n    # plot the image and bboxes\n    # Bounding boxes are defined as follows: x-min y-min width height\n    fig, a = plt.subplots(1,1)\n    fig.set_size_inches(20,40)\n    a.imshow(img)\n    for box in (target['boxes']):\n        x, y, width, height  = box[0], box[1], box[2]-box[0], box[3]-box[1]\n        rect = patches.Rectangle((x, y),\n                                 width, height,\n                                 linewidth = 2,\n                                 edgecolor = 'r',\n                                 facecolor = 'none')\n\n        # Draw the bounding box on top of the image\n        a.add_patch(rect)\n    plt.show()\n    \n# plotting the image with bboxes. Feel free to change the index\nimg, target = dataset[1010]\nplot_img_bbox(img, target)","metadata":{"execution":{"iopub.status.busy":"2021-06-21T21:17:03.698996Z","iopub.execute_input":"2021-06-21T21:17:03.699279Z","iopub.status.idle":"2021-06-21T21:17:04.772443Z","shell.execute_reply.started":"2021-06-21T21:17:03.699249Z","shell.execute_reply":"2021-06-21T21:17:04.771622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"You can see that we are doing great till now, as the bbox is correctly placed. \n\nOne thing to note is that, the dataset wants us to predict only the full apple as \"apple\" but not the half cut one. This will be a challenge to overcome.\n\nLets build the model then!","metadata":{}},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"markdown","source":"We will define a function for loading the model. We will call it later","metadata":{}},{"cell_type":"code","source":"\ndef get_object_detection_model(num_classes):\n\n    # load a model pre-trained pre-trained on COCO\n    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n    \n    # get number of input features for the classifier\n    in_features = model.roi_heads.box_predictor.cls_score.in_features\n    # replace the pre-trained head with a new one\n    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes) \n\n    return model","metadata":{"execution":{"iopub.status.busy":"2021-06-21T21:17:04.773895Z","iopub.execute_input":"2021-06-21T21:17:04.774322Z","iopub.status.idle":"2021-06-21T21:17:04.779974Z","shell.execute_reply.started":"2021-06-21T21:17:04.774289Z","shell.execute_reply":"2021-06-21T21:17:04.778968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"You can clearly see, how easy it is to load and prepare the model using pytorch","metadata":{}},{"cell_type":"markdown","source":"# Augmentations","metadata":{}},{"cell_type":"markdown","source":"This is where we can apply augmentations to the image. \n\nThe augmentations to object detection vary from normal augmentations becuase here we need to ensure that, bbox still aligns with the object correctly after transforming.\n\nHere I have added random flip transform, feel free to customize it as you feel\n\n","metadata":{}},{"cell_type":"code","source":"# Send train=True fro training transforms and False for val/test transforms\ndef get_transform(train):\n    \n    if train:\n        return A.Compose([\n                            A.HorizontalFlip(0.5),\n                     # ToTensorV2 converts image to pytorch tensor without div by 255\n                            ToTensorV2(p=1.0) \n                        ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n    else:\n        return A.Compose([\n                            ToTensorV2(p=1.0)\n                        ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})","metadata":{"execution":{"iopub.status.busy":"2021-06-21T21:17:04.781331Z","iopub.execute_input":"2021-06-21T21:17:04.781828Z","iopub.status.idle":"2021-06-21T21:17:04.791991Z","shell.execute_reply.started":"2021-06-21T21:17:04.78174Z","shell.execute_reply":"2021-06-21T21:17:04.791017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preparing dataset","metadata":{}},{"cell_type":"markdown","source":"Now lets prepare datasets and dataloaders for training and testing.","metadata":{}},{"cell_type":"code","source":"# use our dataset and defined transformations\ndataset = FruitImagesDataset(files_dir, 480, 480, transforms= get_transform(train=True))\ndataset_test = FruitImagesDataset(files_dir, 480, 480, transforms= get_transform(train=False))\n\n# split the dataset in train and test set\ntorch.manual_seed(1)\nindices = torch.randperm(len(dataset)).tolist()\n\n# train test split\ntest_split = 0.2\ntsize = int(len(dataset)*test_split)\ndataset = torch.utils.data.Subset(dataset, indices[:-tsize])\ndataset_test = torch.utils.data.Subset(dataset_test, indices[-tsize:])\n\n# define training and validation data loaders\ndata_loader = torch.utils.data.DataLoader(\n    dataset, batch_size=10, shuffle=True, num_workers=4,\n    collate_fn=utils.collate_fn)\n\ndata_loader_test = torch.utils.data.DataLoader(\n    dataset_test, batch_size=10, shuffle=False, num_workers=4,\n    collate_fn=utils.collate_fn)","metadata":{"execution":{"iopub.status.busy":"2021-06-21T21:17:04.793238Z","iopub.execute_input":"2021-06-21T21:17:04.793795Z","iopub.status.idle":"2021-06-21T21:17:04.824952Z","shell.execute_reply.started":"2021-06-21T21:17:04.793747Z","shell.execute_reply":"2021-06-21T21:17:04.823978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#dataset.xmin[0]","metadata":{"execution":{"iopub.status.busy":"2021-06-21T21:17:04.826242Z","iopub.execute_input":"2021-06-21T21:17:04.826735Z","iopub.status.idle":"2021-06-21T21:17:04.830671Z","shell.execute_reply.started":"2021-06-21T21:17:04.826696Z","shell.execute_reply":"2021-06-21T21:17:04.829735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset[81]","metadata":{"execution":{"iopub.status.busy":"2021-06-21T21:17:04.83224Z","iopub.execute_input":"2021-06-21T21:17:04.832803Z","iopub.status.idle":"2021-06-21T21:17:04.88061Z","shell.execute_reply.started":"2021-06-21T21:17:04.83275Z","shell.execute_reply":"2021-06-21T21:17:04.879715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"markdown","source":"Let's prepare the model for training","metadata":{}},{"cell_type":"code","source":"# to train on gpu if selected.\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n\n\nnum_classes = 10\n\n# get the model using our helper function\nmodel = get_object_detection_model(num_classes)\n\n# move model to the right device\nmodel.to(device)\n\n# construct an optimizer\nparams = [p for p in model.parameters() if p.requires_grad]\noptimizer = torch.optim.SGD(params, lr=0.005,\n                            momentum=0.9, weight_decay=0.0005)\n\n# and a learning rate scheduler which decreases the learning rate by\n# 10x every 3 epochs\nlr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n                                               step_size=3,\n                                               gamma=0.1)","metadata":{"execution":{"iopub.status.busy":"2021-06-21T21:17:04.881955Z","iopub.execute_input":"2021-06-21T21:17:04.882297Z","iopub.status.idle":"2021-06-21T21:17:05.654583Z","shell.execute_reply.started":"2021-06-21T21:17:04.882259Z","shell.execute_reply":"2021-06-21T21:17:05.653839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let the training begin!","metadata":{}},{"cell_type":"code","source":"# training for 10 epochs\nnum_epochs = 1\ntry:\n    for epoch in range(num_epochs):\n        # training for one epoch\n        train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n        # update the learning rate\n        lr_scheduler.step()\n        # evaluate on the test dataset\n        evaluate(model, data_loader_test, device=device)\nexcept:\n    pass","metadata":{"execution":{"iopub.status.busy":"2021-06-21T21:21:10.617086Z","iopub.execute_input":"2021-06-21T21:21:10.617433Z","iopub.status.idle":"2021-06-21T21:21:14.416553Z","shell.execute_reply.started":"2021-06-21T21:21:10.6174Z","shell.execute_reply":"2021-06-21T21:21:14.415617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"An AP of 0.78-0.80 is not bad but perhaps we can make it even better with more augmentations, I will leave that to you.","metadata":{}},{"cell_type":"markdown","source":"# Decode predictions","metadata":{}},{"cell_type":"markdown","source":"Our model predicts a lot of bounding boxes per image, to take out the overlapping ones, We will use **Non Max Suppression** if you want to brush up on that, check [this](https://towardsdatascience.com/non-maximum-suppression-nms-93ce178e177c) out.\n\nTorchvision provides us a utility to apply nms to our predictions, lets build a function `apply_nms` using that.","metadata":{}},{"cell_type":"code","source":"# the function takes the original prediction and the iou threshold.\n\ndef apply_nms(orig_prediction, iou_thresh=0.3):\n    \n    # torchvision returns the indices of the bboxes to keep\n    keep = torchvision.ops.nms(orig_prediction['boxes'], orig_prediction['scores'], iou_thresh)\n    \n    final_prediction = orig_prediction\n    final_prediction['boxes'] = final_prediction['boxes'][keep]\n    final_prediction['scores'] = final_prediction['scores'][keep]\n    final_prediction['labels'] = final_prediction['labels'][keep]\n    \n    return final_prediction\n\n# function to convert a torchtensor back to PIL image\ndef torch_to_pil(img):\n    return torchtrans.ToPILImage()(img).convert('RGB')","metadata":{"execution":{"iopub.status.busy":"2021-06-21T21:17:09.363153Z","iopub.status.idle":"2021-06-21T21:17:09.363827Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Testing our Model","metadata":{}},{"cell_type":"markdown","source":"Lets take an image from our test dataset and see, how our model does.\n\nWe will first see, how many bounding boxes does our model predict compared to actual","metadata":{}},{"cell_type":"code","source":"# pick one image from the test set\nimg, target = dataset_test[5]\n# put the model in evaluation mode\nmodel.eval()\nwith torch.no_grad():\n    prediction = model([img.to(device)])[0]\n    \nprint('predicted #boxes: ', len(prediction['labels']))\nprint('real #boxes: ', len(target['labels']))","metadata":{"execution":{"iopub.status.busy":"2021-06-21T21:17:09.36489Z","iopub.status.idle":"2021-06-21T21:17:09.365447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Whoa! Thats a lot of bboxes. Lets plot them and check what did it predict","metadata":{}},{"cell_type":"code","source":"print('EXPECTED OUTPUT')\nplot_img_bbox(torch_to_pil(img), target)","metadata":{"execution":{"iopub.status.busy":"2021-06-21T21:17:09.366405Z","iopub.status.idle":"2021-06-21T21:17:09.367233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('MODEL OUTPUT')\nplot_img_bbox(torch_to_pil(img), prediction)","metadata":{"execution":{"iopub.status.busy":"2021-06-21T21:17:09.368378Z","iopub.status.idle":"2021-06-21T21:17:09.369065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"You can see that our model predicts a lot of bounding boxes for every apple. Lets apply nms to it and see the final output","metadata":{}},{"cell_type":"code","source":"nms_prediction = apply_nms(prediction, iou_thresh=0.2)\nprint('NMS APPLIED MODEL OUTPUT')\nplot_img_bbox(torch_to_pil(img), nms_prediction)","metadata":{"execution":{"iopub.status.busy":"2021-06-21T21:17:09.370232Z","iopub.status.idle":"2021-06-21T21:17:09.370925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now lets take an image from the test set and try to predict on it","metadata":{}},{"cell_type":"code","source":"test_dataset = FruitImagesDataset(test_dir, 480, 480, transforms= get_transform(train=True))\n# pick one image from the test set\nimg, target = test_dataset[10]\n# put the model in evaluation mode\nmodel.eval()\nwith torch.no_grad():\n    prediction = model([img.to(device)])[0]\n    \nprint('EXPECTED OUTPUT\\n')\nplot_img_bbox(torch_to_pil(img), target)\nprint('MODEL OUTPUT\\n')\nnms_prediction = apply_nms(prediction, iou_thresh=0.01)\n\nplot_img_bbox(torch_to_pil(img), nms_prediction)","metadata":{"execution":{"iopub.status.busy":"2021-06-21T21:17:09.372265Z","iopub.status.idle":"2021-06-21T21:17:09.372993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The model does well on single object images.\n\nYou can see that our model predicts the slices too and that means a failure ☹️ . But fear not, this is just a base line model here are some ideas we can improve it - \n1. Use a better model. \n   We have the option of changing the backbone of our model which at present is `resnet 50` and the fine tune it.\n   \n2. We can change the training configurations like size of the images, optimizers and learning rate schedule.\n3. We can add more augmentations.\n   We have used the Albumentations library which has an extensive library of data augmentation functions. Feel free to explore and try them out. ","metadata":{"trusted":true}},{"cell_type":"markdown","source":"# Fin.\n\nThat's it for the notebook. \n\nPlease tell me if you have any suggestions to improve this kernel or if you find any errors. I will be glad to hear them.\n\nIf you find the notebook useful, Consider upvoting this kernel :)","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}