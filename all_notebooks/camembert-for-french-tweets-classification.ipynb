{"cells":[{"metadata":{},"cell_type":"markdown","source":"# CamemBERT for text classification task\n\nCamemBERT is a state-of-the-art language model for French based on the RoBERTa model.\n\nIt is now available on Hugging Face in 6 different versions with varying number of parameters, amount of pretraining data and pretraining data source domains.\nFor more information here's the huggingface website: https://huggingface.co/camembert-base\n\nIn this notebook I would like to finetune a camembert model for classifying french tweets based on the french-twitter-sentiment-analysis dataset: https://www.kaggle.com/hbaflast/french-twitter-sentiment-analysis"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# In the beginning let's install transformers library\n!pip install transformers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing standard libraries for every machine/deep learning pipeline\nimport pandas as pd\nimport torch\nfrom tqdm import tqdm, trange\nimport numpy as np\n\n\n# Importing specific libraries for data prerpcessing, model archtecture choice, training and evaluation\nfrom sklearn.model_selection import train_test_split\nfrom keras.preprocessing.sequence import pad_sequences\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\nfrom transformers import CamembertTokenizer, CamembertForSequenceClassification\nfrom transformers import AdamW\n# import torch.optim as optim\n# from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n# import seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Defining constants\nepochs = 5\nMAX_LEN = 128\nbatch_size = 16\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load the dataset, I selected only 5000 sample because of memory limitation\ndf = pd.read_csv('../input/french-twitter-sentiment-analysis/french_tweets.csv').sample(5000).reset_index(drop=True)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Spliting Training and validation data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Initialize CamemBERT tokenizer\ntokenizer = CamembertTokenizer.from_pretrained('camembert-base',do_lower_case=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creates list of texts and labels\ntext = df['text'].to_list()\nlabels = df['label'].to_list()\n\n#user tokenizer to convert sentences into tokenizer\ninput_ids  = [tokenizer.encode(sent,add_special_tokens=True,max_length=MAX_LEN) for sent in text]\n\n# Pad our input tokens\ninput_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n\n# Create attention masks\nattention_masks = []\n# Create a mask of 1s for each token followed by 0s for padding\nfor seq in input_ids:\n    seq_mask = [float(i>0) for i in seq]  \n    attention_masks.append(seq_mask)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use train_test_split to split our data into train and validation sets for training\ntrain_inputs, validation_inputs, train_labels, validation_labels, train_masks, validation_masks = train_test_split(input_ids, labels, attention_masks,\n                                                            random_state=42, test_size=0.1)\n\n# Convert all of our data into torch tensors, the required datatype for our model\ntrain_inputs = torch.tensor(train_inputs)\nvalidation_inputs = torch.tensor(validation_inputs)\ntrain_labels = torch.tensor(train_labels)\nvalidation_labels = torch.tensor(validation_labels)\ntrain_masks = torch.tensor(train_masks)\nvalidation_masks = torch.tensor(validation_masks)\n\n# Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop, \n# with an iterator the entire dataset does not need to be loaded into memory\n\ntrain_data = TensorDataset(train_inputs, train_masks, train_labels)\ntrain_sampler = RandomSampler(train_data)\ntrain_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n\nvalidation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\nvalidation_sampler = SequentialSampler(validation_data)\nvalidation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# CamemBERT model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load BertForSequenceClassification, the pretrained BERT model with a single linear classification layer on top. \nmodel = CamembertForSequenceClassification.from_pretrained(\"camembert-base\", num_labels=2)\nmodel.to(device)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Defining the parameters and metrics to optimize"},{"metadata":{"trusted":true},"cell_type":"code","source":"param_optimizer = list(model.named_parameters())\nno_decay = ['bias', 'gamma', 'beta']\noptimizer_grouped_parameters = [\n    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n     'weight_decay_rate': 0.01},\n    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n     'weight_decay_rate': 0.0}\n]\n\noptimizer = AdamW(optimizer_grouped_parameters, lr=2e-5, eps=10e-8)\n\n# Function to calculate the accuracy of our predictions vs labels\ndef flat_accuracy(preds, labels):\n    pred_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n    return np.sum(pred_flat == labels_flat) / len(labels_flat)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training and evaluating our model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Store our loss and accuracy for plotting if we want to visualize training evolution per epochs after the training process\ntrain_loss_set = []\n\n# trange is a tqdm wrapper around the normal python range\nfor _ in trange(epochs, desc=\"Epoch\"):  \n    # Tracking variables for training\n    tr_loss = 0\n    nb_tr_examples, nb_tr_steps = 0, 0\n  \n    # Train the model\n    model.train()\n    for step, batch in enumerate(train_dataloader):\n        # Add batch to device CPU or GPU\n        batch = tuple(t.to(device) for t in batch)\n        # Unpack the inputs from our dataloader\n        b_input_ids, b_input_mask, b_labels = batch\n        # Clear out the gradients (by default they accumulate)\n        optimizer.zero_grad()\n        # Forward pass\n        outputs = model(b_input_ids,token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n        # Get loss value\n        loss = outputs[0]\n        # Add it to train loss list\n        train_loss_set.append(loss.item())    \n        # Backward pass\n        loss.backward()\n        # Update parameters and take a step using the computed gradient\n        optimizer.step()\n    \n        # Update tracking variables\n        tr_loss += loss.item()\n        nb_tr_examples += b_input_ids.size(0)\n        nb_tr_steps += 1\n\n    print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n    \n    \n\n\n    # Tracking variables for validation\n    eval_loss, eval_accuracy = 0, 0\n    nb_eval_steps, nb_eval_examples = 0, 0\n    # Validation of the model\n    model.eval()\n    # Evaluate data for one epoch\n    for batch in validation_dataloader:\n        # Add batch to device CPU or GPU\n        batch = tuple(t.to(device) for t in batch)\n        # Unpack the inputs from our dataloader\n        b_input_ids, b_input_mask, b_labels = batch\n        # Telling the model not to compute or store gradients, saving memory and speeding up validation\n        with torch.no_grad():\n            # Forward pass, calculate logit predictions\n            outputs =  model(b_input_ids,token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n            loss, logits = outputs[:2]\n    \n        # Move logits and labels to CPU if GPU is used\n        logits = logits.detach().cpu().numpy()\n        label_ids = b_labels.to('cpu').numpy()\n\n        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n    \n        eval_accuracy += tmp_eval_accuracy\n        nb_eval_steps += 1\n\n    print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Test the model on an unseen texts"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Test the model on a comment\ncomments = [\"La vie est très mauvaise, Je la déteste\", \"Quelle belle voiture c'est très magnifique\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Encode the comments\ntokenized_comments_ids = [tokenizer.encode(comment,add_special_tokens=True,max_length=MAX_LEN) for comment in comments]\n# Pad the resulted encoded comments\ntokenized_comments_ids = pad_sequences(tokenized_comments_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n\n# Create attention masks \nattention_masks = []\nfor seq in tokenized_comments_ids:\n  seq_mask = [float(i>0) for i in seq]\n  attention_masks.append(seq_mask)\n\nprediction_inputs = torch.tensor(tokenized_comments_ids)\nprediction_masks = torch.tensor(attention_masks)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Apply the finetuned model (Camembert)\nflat_pred = []\nwith torch.no_grad():\n    # Forward pass, calculate logit predictions\n    outputs =  model(prediction_inputs.to(device),token_type_ids=None, attention_mask=prediction_masks.to(device))\n    logits = outputs[0]\n    logits = logits.detach().cpu().numpy() \n    flat_pred.extend(np.argmax(logits, axis=1).flatten())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(len(flat_pred)):\n    print('Comment: ', comments[i])\n    print('Label', flat_pred[i])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I hope you enjoyed this tutorial, I tried to make it as simple as possible "}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}