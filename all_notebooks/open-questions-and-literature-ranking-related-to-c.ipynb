{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Open questions and literature ranking related to COVID-19 ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Goal\nGiven the large number of literature and the rapid spread of COVID-19, it is difficult for health professionals to keep up with new information on the virus. Can \nclustering similar research articles together to simplify the search for related publications? How can the content of the clusters be qualified? Is clustering only \nsufficient to get the related papers to the question/s? or we need to use a classification algorithm to solve this problem.  \n\nBy using unsupervised clustering algorithm (K-means clustering) for labelling in combination with dimensionality reduction for visualization, the collection of \nliterature can be represented by a scatter plot. On this plot, publications of highly similar topic will share a label and will be plotted near each other.\n\nIn order to answer the question/s with the most similar papers, we used the supervised classification algorithm k-nearest neighbors (KNN) as it takes the data \nlabels from the clustering stage.\n\nThis is a difficult time in which health care workers, sanitation staff, and many other essential personnel are out there keeping the world afloat. While adhering \nto quarantine protocol, the Kaggle CORD-19 competition has given us an opportunity to help in the best way we can as computer science students. It should be noted, \nhowever, that we are not epidemiologists, and it is not our place to gauge the importance of these papers. This tool was created to help make it easier for trained \nprofessionals to sift through many, many publications related to the virus, and find their own determinations.\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Approach:\n\n- Parse the text from the body of each document using Natural Language Processing (NLP).\n- Turn each document instance $d_i$ into a feature vector $X_i$ using Term Frequencyâ€“inverse Document Frequency (TF-IDF).\n- Apply Dimensionality Reduction to each feature vector $X_i$ using t-Distributed Stochastic Neighbor Embedding (t-SNE) to cluster similar research articles in the \ntwo dimensional plane $X$ embedding $Y_1$.\n- Use Principal Component Analysis (PCA) to project down the dimensions of $X$ to a number of dimensions that will keep .95 variance while removing noise and \noutliers in embedding $Y_2$.\n- Apply k-means clustering on $Y_2$, where $k$ is 28, to label each cluster on $Y_1$ (the suitable k was determined by the Elbow method).\n- Apply classification on the given question/s to find the highest 10 related papers among the given dataset using KNN with the aid of clustering labels. \n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Pros:\n- Training a generalized model to responed faster to questions related to kaggle tasks.\n\n- Using unsubervised clustering technique to have labelled data that are used to train knn supervised classification technique which is simple and efficient.\n\n\n# Cons:\n- The results do not depend on the meaning of the question but search for the related papers according to words matching.\n- choosing abstarct data to train the model leads to dropping of miltiple documents which don't have an abstarct \n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Table of Contents\n1. Loading the data\n2. Pre-processing\n3. Vectorization\n4. PCA  & Clustering\n5. Dimensionality Reduction with t-SNE\n6. Classification with KNN\n7. How to Use the Plot?\n8. Conclusion","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Table of Contents\n1. Loading the data\n2. Pre-processing\n3. Vectorization\n4. PCA  & k_means Clustering\n5. Dimensionality Reduction with t-SNE\n6. Visualization Plot for clustering\n7. Classify using KNN\n8. Conclusion\n9. Citation/Sources","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 1) Loading the Data\nLoad the data following the notebook by Ivan Ega Pratama, from Kaggle.\n#### Cite: [Dataset Parsing Code | Kaggle, COVID EDA: Initial Exploration Tool](https://www.kaggle.com/ivanegapratama/covid-eda-initial-exploration-tool)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Loading Metadata","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport glob\nimport json\nfrom IPython.display import Image\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's load the metadata of the dateset. 'title' and 'journal' attributes may be useful later when we cluster the articles to see what kinds of articles cluster together.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"root_path = '/kaggle/input/CORD-19-research-challenge/'\nmetadata_path = f'{root_path}/metadata.csv'\nmeta_df = pd.read_csv(metadata_path, dtype={\n    'pubmed_id': str,\n    'Microsoft Academic Paper ID': str, \n    'doi': str\n})\nmeta_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"meta_df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Fetch All of JSON File Path","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Get path to all JSON files:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"all_json = glob.glob(f'{root_path}/**/*.json', recursive=True)\nlen(all_json)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Helper Functions","execution_count":null},{"metadata":{},"cell_type":"markdown","source":" File Reader Class","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class FileReader:\n    def __init__(self, file_path):\n        with open(file_path) as file:\n            content = json.load(file)\n            self.paper_id = content['paper_id']\n            self.abstract = []\n            self.body_text = []\n            # Abstract\n            if not 'abstract' in content.keys():\n                self.abstract = ''\n                self.body_text= ''\n                return\n            for entry in content['abstract']:\n                self.abstract.append(entry['text'])\n            # Body text\n            for entry in content['body_text']:\n                self.body_text.append(entry['text'])\n            self.abstract = '\\n'.join(self.abstract)\n            self.body_text = '\\n'.join(self.body_text)\n    def __repr__(self):\n        return f'{self.paper_id}: {self.abstract[:200]}... {self.body_text[:200]}...'\nfirst_row = FileReader(all_json[0])\nprint(first_row)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Helper function adds break after every words when character length reach to certain amount. This is for the interactive plot so that hover tool fits the screen.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_breaks(content, length):\n    data = \"\"\n    words = content.split(' ')\n    total_chars = 0\n\n    # add break every length characters\n    for i in range(len(words)):\n        total_chars += len(words[i])\n        if total_chars > length:\n            data = data + \"<br>\" + words[i]\n            total_chars = 0\n        else:\n            data = data + \" \" + words[i]\n    return data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Load the Data into DataFrame","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Using the helper functions, let's read in the articles into a DataFrame that can be used easily:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dict_ = {'paper_id': [], 'doi':[], 'abstract': [], 'body_text': [], 'authors': [], 'title': [], 'journal': [], 'abstract_summary': []}\nfor idx, entry in enumerate(all_json):\n    if idx % (len(all_json) // 10) == 0:\n        print(f'Processing index: {idx} of {len(all_json)}')\n    \n    try:\n        content = FileReader(entry)\n    except Exception as e:\n        continue  # invalid paper format, skip\n    \n    # get metadata information\n    meta_data = meta_df.loc[meta_df['sha'] == content.paper_id]\n    # no metadata, skip this paper\n    if len(meta_data) == 0:\n        continue\n    \n    dict_['abstract'].append(content.abstract)\n    dict_['paper_id'].append(content.paper_id)\n    dict_['body_text'].append(content.body_text)\n    \n    # also create a column for the summary of abstract to be used in a plot\n    if len(content.abstract) == 0: \n        # no abstract provided\n        dict_['abstract_summary'].append(\"Not provided.\")\n    elif len(content.abstract.split(' ')) > 100:\n        # abstract provided is too long for plot, take first 100 words append with ...\n        info = content.abstract.split(' ')[:100]\n        summary = get_breaks(' '.join(info), 40)\n        dict_['abstract_summary'].append(summary + \"...\")\n    else:\n        # abstract is short enough\n        summary = get_breaks(content.abstract, 40)\n        dict_['abstract_summary'].append(summary)\n        \n    # get metadata information\n    meta_data = meta_df.loc[meta_df['sha'] == content.paper_id]\n    \n    try:\n        # if more than one author\n        authors = meta_data['authors'].values[0].split(';')\n        if len(authors) > 2:\n            # if more than 2 authors, take them all with html tag breaks in between\n            dict_['authors'].append(get_breaks('. '.join(authors), 40))\n        else:\n            # authors will fit in plot\n            dict_['authors'].append(\". \".join(authors))\n    except Exception as e:\n        # if only one author - or Null valie\n        dict_['authors'].append(meta_data['authors'].values[0])\n    \n    # add the title information, add breaks when needed\n    try:\n        title = get_breaks(meta_data['title'].values[0], 40)\n        dict_['title'].append(title)\n    # if title was not provided\n    except Exception as e:\n        dict_['title'].append(meta_data['title'].values[0])\n    \n    # add the journal information\n    dict_['journal'].append(meta_data['journal'].values[0])\n    \n    # add doi\n    dict_['doi'].append(meta_data['doi'].values[0])\n    \ndf_covid = pd.DataFrame(dict_, columns=['paper_id', 'doi', 'abstract', 'body_text', 'authors', 'title', 'journal', 'abstract_summary'])\ndf_covid.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_covid","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Some feature engineering\nAdding word count columns for both abstract and body_text can be useful parameters later:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_covid['abstract_word_count'] = df_covid['abstract'].apply(lambda x: len(x.strip().split()))  # word count in abstract\ndf_covid['body_word_count'] = df_covid['body_text'].apply(lambda x: len(x.strip().split()))  # word count in body\ndf_covid['body_unique_words']=df_covid['body_text'].apply(lambda x:len(set(str(x).split())))  # number of unique words in body\ndf_covid.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_covid.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_covid['abstract'].describe(include='all')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Handle Possible Duplicates","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"When we look at the unique values above, we can see that there are duplicates. It may have caused because of author submiting the article to multiple journals. Let's remove the duplicats from our dataset:\n\n(Thank you Desmond Yeoh for recommending the below approach on Kaggle)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndf_covid.drop_duplicates(['abstract', 'body_text'], inplace=True)\ndf_covid['abstract'].describe(include='all')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_covid['body_text'].describe(include='all')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It looks like we didn't have duplicates. Instead, it was articles without Abstracts.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Take a Look at the Data:","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"####  we need to clean-up the text to improve any clustering or classification efforts. First, let's drop Null vales","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_covid.dropna(inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_covid.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_covid.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_covid.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Pre-processing","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df_covid","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Handling multiple languages\nWe going to determine the language of each paper in the dataframe. Not all of the sources are English and the language needs to be identified so that we know how handle these instances","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm import tqdm\nfrom langdetect import detect\nfrom langdetect import DetectorFactory\n\n# set seed\nDetectorFactory.seed = 0\n\n# hold label - language\nlanguages = []\n\n# go through each text\nfor ii in tqdm(range(0,len(df))):\n    # split by space into list, take the first x intex, join with space\n    text = df.iloc[ii]['body_text'].split(\" \")\n    \n    lang = \"en\"\n    try:\n        if len(text) > 50:\n            lang = detect(\" \".join(text[:50]))\n        elif len(text) > 0:\n            lang = detect(\" \".join(text[:len(text)]))\n    # ught... beginning of the document was not in a good format\n    except Exception as e:\n        all_words = set(text)\n        try:\n            lang = detect(\" \".join(all_words))\n        # what!! :( let's see if we can find any text in abstract...\n        except Exception as e:\n            \n            try:\n                # let's try to label it through the abstract then\n                lang = detect(df.iloc[ii]['abstract_summary'])\n            except Exception as e:\n                lang = \"unknown\"\n                pass\n    \n    # get the language    \n    languages.append(lang)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pprint import pprint\n\nlanguages_dict = {}\nfor lang in set(languages):\n    languages_dict[lang] = languages.count(lang)\n    \nprint(\"Total: {}\\n\".format(len(languages)))\npprint(languages_dict)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets take a look at the language distribution in the dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['language'] = languages\nplt.bar(range(len(languages_dict)), list(languages_dict.values()), align='center')\nplt.xticks(range(len(languages_dict)), list(languages_dict.keys()))\nplt.title(\"Distribution of Languages in Dataset\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\nWe will be dropping any language that is not English. Attempting to translate foreign texts gave the following problems:\n\n1. API calls were limited\n\n2. Translating the language may not carry over the true semantic meaning of the text\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df[df['language'] == 'en'] \ndf.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.4/en_core_sci_lg-0.2.4.tar.gz\nimport en_core_sci_lg","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#NLP \nfrom IPython.utils import io\nimport spacy\nfrom spacy.lang.en.stop_words import STOP_WORDS\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Stopwords\n\nPart of the preprocessing will be finding and removing stopwords (common words that will act as noise in the clustering step).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import string\n\npunctuations = string.punctuation\nstopwords = list(STOP_WORDS)\nstopwords[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"custom_stop_words = [\n    'doi', 'preprint', 'copyright', 'peer', 'reviewed', 'org', 'https', 'et', 'al', 'author', 'figure', \n    'rights', 'reserved', 'permission', 'used', 'using', 'biorxiv', 'medrxiv', 'license', 'fig', 'fig.', \n    'al.', 'Elsevier', 'PMC', 'CZI', 'www'\n]\n\nfor w in custom_stop_words:\n    if w not in stopwords:\n        stopwords.append(w)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Next lets create a function that will process the text data for us. \nFor this purpose we will be using the spacy library. This function will convert text to lower case, remove punctuation, and find and remove stopwords. For the parser, we will use en_core_sci_lg. This is a model for processing biomedical, scientific or clinical text.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\n# Parser\nparser = en_core_sci_lg.load(disable=[\"tagger\", \"ner\"])\nparser.max_length = 7000000\n\ndef spacy_tokenizer(sentence):\n    mytokens = parser(sentence)\n    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n    mytokens = [ word for word in mytokens if word not in stopwords and word not in punctuations ]\n    mytokens = [ re.sub('[0-9%]','',word) for word in mytokens ]\n    mytokens = \" \".join([i for i in mytokens])\n    return mytokens","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### To focus on features using main needed words related to the topic , we chose to work with processed text of the abstract column\n       ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm import tqdm\nimport numpy as np\nimport math","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tqdm.pandas()\ndf[\"processed_text\"] = df[\"abstract\"].progress_apply(spacy_tokenizer)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Remove all samples with emtpy abstract","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['abstract'].replace('', np.nan, inplace=True)\ndf.dropna(inplace=True)\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### visualizing processed text","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['processed_word_count'] = df[\"processed_text\"].apply(lambda x: len(x.strip().split()))\nsns.distplot(df['processed_word_count'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['abstract_word_count'].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Vectorization\n\nNow that we have pre-processed the data, it is time to convert it into a format that can be handled by our algorithms. For this purpose we will be using tf-idf. This will convert our string formatted data into a measure of how important each word is to the instance out of the literature as a whole.\n\nVectorize our data. We will be clustering based off the content of the processed text. The maximum number of features will be limited to 4096 features , eseentially acting as a noise filter. Additionally, more features cause painfully long runtimes.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\ntext = df['processed_text'].values\nvectorizer = TfidfVectorizer(max_features=4096)\nX = vectorizer.fit_transform(text) \nterms = vectorizer.get_feature_names()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### terms indicate the chosen features from the vectorizer , let's take a look on some of them","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"terms[0:20]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# PCA  & Clustering\n\nLet's see how much we can reduce the dimensions while still keeping 95% variance. We will apply Principle Component Analysis (PCA) to our vectorized data. The reason for this is that by keeping a large number of dimensions with PCA, you donâ€™t destroy much of the information, but hopefully will remove some noise/outliers from the data, and make the clustering problem easier for k-means. Note that X_reduced will only be used for k-means, t-SNE will still use the original feature vector X that was generated through tf-idf on the NLP processed text.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\n\npca = PCA(n_components=0.95, random_state=42)\nX_reduced= pca.fit_transform(X.toarray())\nX_reduced.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To separate the literature, k-means will be run on the vectorized text. Given the number of clusters, k, k-means will categorize each vector by taking the mean distance to a randomly initialized centroid. The centroids are updated iteratively.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import KMeans","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Image(filename='/kaggle/input/kaggle-resources/kmeans.PNG', width=800, height=800)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"[source](https://en.wikipedia.org/wiki/K-means_clustering)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### How many clusters? \n\nTo find the best k value for k-means we'll look at the distortion at different k values. Distortion computes the sum of squared distances from each point to its assigned center. When distortion is plotted against k there will be a k value after which decreases in distortion are minimal. This is the desired number of clusters.\n\n##### Uncomment those cells to watch the elbow , it takes some time","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# from sklearn import metrics\n# from scipy.spatial.distance import cdist\n# %matplotlib inline\n# from matplotlib import pyplot as plt\n\n# # run kmeans with many different k\n# distortions = []\n# K = range(10, 35)\n# for k in tqdm(K):\n#     k_means = KMeans(n_clusters=k, random_state=42).fit(X_reduced)\n#     k_means.fit(X_reduced)\n#     distortions.append(sum(np.min(cdist(X_reduced, k_means.cluster_centers_, 'euclidean'), axis=1)) / X.shape[0])\n#     #print('Found distortion for {} clusters'.format(k))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# X_line = [K[0], K[-1]]\n# Y_line = [distortions[0], distortions[-1]]\n\n# # Plot the elbow\n# plt.plot(K, distortions, 'b-')\n# plt.plot(X_line, Y_line, 'r')\n# plt.xlabel('k')\n# plt.ylabel('Distortion')\n# plt.title('The Elbow Method showing the optimal k')\n# plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In this plot we can see that the better k values are between 18-25. After that, the decrease in distortion is not as significant. For simplicity, we will use k=20","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Run k-means","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Now that we have an appropriate k value, we can run k-means on the PCA-processed feature vector (X_reduced). ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"k = 20\nkmeans = KMeans(n_clusters=k, random_state=42)\ny_pred = kmeans.fit_predict(X_reduced)\ndf['y'] = y_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"order_centroids = kmeans.cluster_centers_.argsort()[:, ::-1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### let's see each cluster with its features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(20):\n    print('Cluster %d:' % i),\n    for ind in order_centroids[i, :20]:\n        print('%s' % terms[ind])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Dimensionality Reduction with t-SNE","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Dimensionality Reduction with t-SNE\nUsing t-SNE we can reduce our high dimensional features vector to 2 dimensions. By using the 2 dimensions as x,y coordinates, the processed_text can be plotted.\n\nt-Distributed Stochastic Neighbor Embedding (t-SNE) reduces dimensionality while trying to keep similar instances close and dissimilar instances apart. It is mostly used for visualization, in particular to visualize clusters of instances in high-dimensional space","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# from sklearn.manifold import TSNE\n\n# tsne = TSNE(verbose=1, perplexity=100, random_state=42)\n# X_embedded = tsne.fit_transform(X.toarray())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So that step took a while! Let's take a look at what our data looks like when compressed to 2 dimensions.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# # sns settings\n# sns.set(rc={'figure.figsize':(15,15)})\n\n# # colors\n# palette = sns.color_palette(\"bright\", 1)\n\n# # plot\n# sns.scatterplot(X_embedded[:,0], X_embedded[:,1], palette=palette)\n# plt.title('t-SNE with no Labels')\n# plt.savefig(\"t-sne_covid19.png\")\n# plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This looks pretty bland. There are some clusters we can immediately detect, but the many instances closer to the center are harder to separate. t-SNE did a good job at reducing the dimensionality, but now we need some labels. Let's use the clusters found by k-means as labels. This will help visually separate different concentrations of topics.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# %matplotlib inline\n# from matplotlib import pyplot as plt\n# import seaborn as sns\n\n# # sns settings\n# sns.set(rc={'figure.figsize':(15,15)})\n\n# # colors\n# palette = sns.hls_palette(20, l=.4, s=.9)\n\n# # plot\n# sns.scatterplot(X_embedded[:,0], X_embedded[:,1], hue=y_pred, legend='full', palette=palette)\n# plt.title('t-SNE with Kmeans Labels')\n# plt.savefig(\"improved_cluster_tsne.png\")\n# plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Using Knn to get the nearest  neighbours to the question","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"knn_model = KNeighborsClassifier(n_neighbors=10)\nknn_model.fit(X_reduced,df['y'].values )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict_nearest_neighbour(model,sentence):\n    print(\"Prediction\")\n    sentence = spacy_tokenizer(sentence)\n    print (sentence)\n    X = vectorizer.transform([sentence])\n    X = pca.transform(X.toarray())\n    predicted = model.kneighbors(X, 100)\n    return predicted","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Use knn to get the nearest neighbours to different questions","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sentence = \"What has been published about ethical and social science considerations?\"\nsentence_1 = 'What do we know about virus genetics, origin and evolution ?'\nsentence_2 = 'What is known about transmission, incubation, and environmental stability?'\nsentence_3 = 'Create summary tables that address risk factors related to COVID-19'\nsentence_4 = 'What do we know about COVID-19 risk factors?'\nsentence_5 = 'What has been published about medical care?'\nsentence_6 = 'What do we know about diagnostics and surveillance?'\nsentence_7 = 'What do we know about vaccines and therapeutics?'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### For question 1 ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"res = predict_nearest_neighbour(knn_model, sentence_1)\nprint (res)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"inds =[]\nfor i,dist in enumerate(res[0][0]):\n    if dist > 1:\n        inds.append(res[1][0][i])\nprint (inds)\n        \n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Ten nearest douments related to the question","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(10):\n    index = inds[i]\n    print (df.iloc[index]['abstract'])\n    print (\"virus appeared\", df.iloc[index]['abstract'].count('virus'), 'times')\n    print (\"genetic appeared\", df.iloc[index]['abstract'].count('genetic'), 'times')\n    print (\"origin appeared\", df.iloc[index]['abstract'].count('origin'), 'times')\n    print (\"evolution appeared\", df.iloc[index]['abstract'].count('evolution'), 'times')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### The documents' summary table","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(10):\n    index = inds[i]\n    print (df.iloc[index])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Trying question 4","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"res = predict_nearest_neighbour(knn_model, sentence_4)\nprint (res)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"inds =[]\nfor i,dist in enumerate(res[0][0]):\n    if dist > 1:\n        inds.append(res[1][0][i])\nprint (inds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(10):\n    index = inds[i]\n    print (df.iloc[index]['abstract'])\n    print (\"risk appeared\", df.iloc[index]['abstract'].count('risk'), 'times')\n    print (\"factor appeared\", df.iloc[index]['abstract'].count('factor'), 'times')\n    print (\"COVID-19\", df.iloc[index]['abstract'].count('covid-19'), 'times')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(10):\n    index = inds[i]\n    print (df.iloc[index])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Deploy the model to pickle file","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import pickle\npickle.dump(kmeans, open(\"k-means_abstract.pkl\", \"wb\"))\npickle.dump(knn_model, open(\"knn_model_abstract.pkl\", \"wb\"))\n\n#kmeans = pickle.load(open(\"k-means_model.pkl\", \"rb\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusion","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"After loading the data we used **langdetect** to detect documents' languages to remove non english documents ,\nafter that we used spacy_tokenizer in our data pre-processing where it lemmitize the words , make them all small alphapetically and we removed numbers and null values  from them .\nWe used TF_IDF vectorizer for features abstraction , and used PCA for dimensionality reduction for more efficiency and  too many features take more running time .\n\nWe used k-means clustering  as an unsupervised approach as we don't have labels , after clustering the documents we used k-nearest neighbours to get the closest documents to our question.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Future thoughts to consider","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"1) Topic modelling to discover the topic of each cluster ,  so after applying KNN algorithm we can vote for the major \n    label to indicate the topic related to this question so that results will be more accurate\n\n2) Using of alternative approach such as neural networks LSTM to classify based on words meanining instead of our\n    approach in TF-IDF that rely on the word count and its frequency inside the whole documents .","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}