{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# IMPORT LIBRARIES\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import RandomizedSearchCV\n\nfrom keras.models import Sequential\nfrom keras import layers\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.wrappers.scikit_learn import KerasClassifier\n\nimport os\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1. Loading a Dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/sms-spam-collection-dataset/spam.csv',delimiter=',',encoding='latin-1')\ndf = df.drop(['Unnamed: 2','Unnamed: 3','Unnamed: 4'],axis=1)\ndf = df.rename(columns = {'v1':'spam','v2':'text'})\ndf['spam'].replace(['ham','spam'],[0,1],inplace=True)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Defining a Baseline Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(df['text'], df['spam'], test_size=0.1, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now use the CountVectorizer provided by the scikit-learn library to vectorize sentences. It takes the words of each sentence and creates a vocabulary of all the unique words in the sentences. This vocabulary can then be used to create a feature vector of the count of the words:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\n\nvectorizer = CountVectorizer()\nvectorizer.fit(X_train)\n\nX_train_cv = vectorizer.transform(X_train)\nX_test_cv  = vectorizer.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\n\nlogreg = LogisticRegression()\nlogreg.fit(X_train_cv, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = logreg.predict(X_test_cv)\nprint('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg.score(X_test_cv, y_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nconfusion_matrix = confusion_matrix(y_test, y_pred)\nprint(confusion_matrix)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report\nprint(classification_report(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\nlogit_roc_auc = roc_auc_score(y_test, y_pred)\nfpr, tpr, thresholds = roc_curve(y_test, logreg.predict_proba(X_test_cv)[:,1])\nplt.figure()\nplt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.legend(loc=\"lower right\")\nplt.savefig('Log_ROC')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. Keras Model\n### 3.1 Tokenizing and Padding"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\n\ntokenizer = Tokenizer(num_words=5000)\ntokenizer.fit_on_texts(X_train)\n\nX_train_emb = tokenizer.texts_to_sequences(X_train)\nX_test_emb = tokenizer.texts_to_sequences(X_test)\n\nvocab_size = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index\n\nprint(X_train[0])\nprint(X_train_emb[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All the messages should have the same lenght. For that we use pad_sequences"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.sequence import pad_sequences\n\nmaxlen = 100\n\nX_train = pad_sequences(X_train_emb, padding='post', maxlen=maxlen)\nX_test = pad_sequences(X_test_emb, padding='post', maxlen=maxlen)\n\nprint(X_train[0, :])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3.2 Build the model\n\nThe layers are stacked sequentially to build the classifier:\n\n- The first layer is an **Embedding layer**. This layer takes the integer-encoded vocabulary and looks up the embedding vector for each word-index. These vectors are learned as the model trains. The vectors add a dimension to the output array. The resulting dimensions are: (batch, sequence, embedding).\n- Next, a **GlobalAveragePooling1D** layer returns a fixed-length output vector for each example by averaging over the sequence dimension. This allows the model to handle input of variable length, in the simplest way possible.\n- This fixed-length output vector is piped through a fully-connected **(Dense)** layer with 16 hidden units.\n- The last layer is densely connected with a **single output node**. Using the **sigmoid activation** function, this value is a float between 0 and 1, representing a probability, or confidence level."},{"metadata":{"trusted":true},"cell_type":"code","source":"import keras\nimport tensorflow as tf\n\nembedding_dim = 50\nmodel = keras.Sequential()\nmodel.add(keras.layers.Embedding(vocab_size, embedding_dim))\nmodel.add(keras.layers.GlobalMaxPool1D())\nmodel.add(keras.layers.Dense(16, activation=tf.nn.relu))\nmodel.add(keras.layers.Dense(1, activation=tf.nn.sigmoid))\nmodel.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['acc'])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_history(history):\n    acc = history.history['acc']\n    val_acc = history.history['val_acc']\n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n    x = range(1, len(acc) + 1)\n\n    plt.figure(figsize=(12, 5))\n    plt.subplot(1, 2, 1)\n    plt.plot(x, acc, 'b', label='Training acc')\n    plt.plot(x, val_acc, 'r', label='Validation acc')\n    plt.title('Training and validation accuracy')\n    plt.legend()\n    plt.subplot(1, 2, 2)\n    plt.plot(x, loss, 'b', label='Training loss')\n    plt.plot(x, val_loss, 'r', label='Validation loss')\n    plt.title('Training and validation loss')\n    plt.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(X_train, y_train,\n                    epochs=20,\n                    verbose=False,\n                    validation_data=(X_test, y_test),\n                    batch_size=16)\nloss, accuracy = model.evaluate(X_train, y_train, verbose=False)\nprint(\"Training Accuracy: {:.4f}\".format(accuracy))\nloss, accuracy = model.evaluate(X_test, y_test, verbose=False)\nprint(\"Testing Accuracy:  {:.4f}\".format(accuracy))\nplot_history(history)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3.3 Using Pretrained Word Embeddings"},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_embedding_matrix(filepath, word_index, embedding_dim):\n    vocab_size = len(word_index) + 1  # Adding again 1 because of reserved 0 index\n    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n\n    with open(filepath) as f:\n        for line in f:\n            word, *vector = line.split()\n            if word in word_index:\n                idx = word_index[word] \n                embedding_matrix[idx] = np.array(\n                    vector, dtype=np.float32)[:embedding_dim]\n\n    return embedding_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_dim = 50\nembedding_matrix = create_embedding_matrix('../input/glove6b50dtxt/glove.6B.50d.txt', tokenizer.word_index, embedding_dim)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nonzero_elements = np.count_nonzero(np.count_nonzero(embedding_matrix, axis=1))\nnonzero_elements / vocab_size","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\nmodel.add(layers.Embedding(vocab_size, embedding_dim, \n                           weights=[embedding_matrix], \n                           input_length=maxlen, \n                           trainable=False))\nmodel.add(layers.GlobalMaxPool1D())\nmodel.add(layers.Dense(10, activation='relu'))\nmodel.add(layers.Dense(1, activation='sigmoid'))\nmodel.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(X_train, y_train,\n                    epochs=20,\n                    verbose=False,\n                    validation_data=(X_test, y_test),\n                    batch_size=16)\nloss, accuracy = model.evaluate(X_train, y_train, verbose=False)\nprint(\"Training Accuracy: {:.4f}\".format(accuracy))\nloss, accuracy = model.evaluate(X_test, y_test, verbose=False)\nprint(\"Testing Accuracy:  {:.4f}\".format(accuracy))\nplot_history(history)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# add trainable\nmodel = Sequential()\nmodel.add(layers.Embedding(vocab_size, embedding_dim, \n                           weights=[embedding_matrix], \n                           input_length=maxlen, \n                           trainable=True))\nmodel.add(layers.GlobalMaxPool1D())\nmodel.add(layers.Dense(10, activation='relu'))\nmodel.add(layers.Dense(1, activation='sigmoid'))\nmodel.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(X_train, y_train,\n                    epochs=20,\n                    verbose=False,\n                    validation_data=(X_test, y_test),\n                    batch_size=16)\nloss, accuracy = model.evaluate(X_train, y_train, verbose=False)\nprint(\"Training Accuracy: {:.4f}\".format(accuracy))\nloss, accuracy = model.evaluate(X_test, y_test, verbose=False)\nprint(\"Testing Accuracy:  {:.4f}\".format(accuracy))\nplot_history(history)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"You can see that it is most effective to allow the embeddings to be trained. When dealing with large training sets it can boost the training process to be much faster than without."},{"metadata":{},"cell_type":"markdown","source":"### 3.4 CNN"},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_dim = 50\n\nmodel = Sequential()\nmodel.add(layers.Embedding(vocab_size, embedding_dim, input_length=maxlen))\nmodel.add(layers.Conv1D(128, 5, activation='relu'))\nmodel.add(layers.GlobalMaxPooling1D())\nmodel.add(layers.Dense(10, activation='relu'))\nmodel.add(layers.Dense(1, activation='sigmoid'))\nmodel.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(X_train, y_train,\n                    epochs=20,\n                    verbose=False,\n                    validation_data=(X_test, y_test),\n                    batch_size=16)\nloss, accuracy = model.evaluate(X_train, y_train, verbose=False)\nprint(\"Training Accuracy: {:.4f}\".format(accuracy))\nloss, accuracy = model.evaluate(X_test, y_test, verbose=False)\nprint(\"Testing Accuracy:  {:.4f}\".format(accuracy))\nplot_history(history)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3.5 Hyperparameters optimization"},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_model(num_filters, kernel_size, vocab_size, embedding_dim, maxlen):\n    model = Sequential()\n    model.add(layers.Embedding(vocab_size, embedding_dim, input_length=maxlen))\n    model.add(layers.Conv1D(num_filters, kernel_size, activation='relu'))\n    model.add(layers.GlobalMaxPooling1D())\n    model.add(layers.Dense(10, activation='relu'))\n    model.add(layers.Dense(1, activation='sigmoid'))\n    model.compile(optimizer='adam',\n                  loss='binary_crossentropy',\n                  metrics=['accuracy'])\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"param_grid = dict(num_filters=[32, 64, 128],\n                  kernel_size=[3, 5, 7],\n                  vocab_size=[5000], \n                  embedding_dim=[50],\n                  maxlen=[100])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\n\n# Main settings\nepochs = 20\nembedding_dim = 50\nmaxlen = 100\ndf = pd.read_csv('../input/sms-spam-collection-dataset/spam.csv',delimiter=',',encoding='latin-1')\ndf = df.drop(['Unnamed: 2','Unnamed: 3','Unnamed: 4'],axis=1)\ndf = df.rename(columns = {'v1':'spam','v2':'text'})\ndf['spam'].replace(['ham','spam'],[0,1],inplace=True)\n\n# Run grid search for each source (yelp, amazon, imdb)\n\nsentences = df['text'].values\ny = df['spam'].values\n\n# Train-test split\nsentences_train, sentences_test, y_train, y_test = train_test_split(\n    sentences, y, test_size=0.20, random_state=1000)\n\n# Tokenize words\ntokenizer = Tokenizer(num_words=5000)\ntokenizer.fit_on_texts(sentences_train)\nX_train = tokenizer.texts_to_sequences(sentences_train)\nX_test = tokenizer.texts_to_sequences(sentences_test)\n\n# Adding 1 because of reserved 0 index\nvocab_size = len(tokenizer.word_index) + 1\n\n# Pad sequences with zeros\nX_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\nX_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n\n# Parameter grid for grid search\nparam_grid = dict(num_filters=[32, 64, 128],\n                  kernel_size=[3, 5, 7],\n                  vocab_size=[vocab_size],\n                  embedding_dim=[embedding_dim],\n                  maxlen=[maxlen])\nmodel = KerasClassifier(build_fn=create_model,\n                        epochs=epochs, batch_size=10,\n                        verbose=False)\ngrid = RandomizedSearchCV(estimator=model, param_distributions=param_grid,\n                          cv=4, verbose=1, n_iter=5)\ngrid_result = grid.fit(X_train, y_train)\n\n# Evaluate testing set\ntest_accuracy = grid.score(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(test_accuracy)\nprint(grid.best_score_)\nprint(grid.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}