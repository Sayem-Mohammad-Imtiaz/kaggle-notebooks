{"cells":[{"metadata":{"_uuid":"d0078177c2c04d5d42f2c6a6ae54816ac05c46b6"},"cell_type":"markdown","source":"# **Using dataset on WNS (Holdings) Limited provided by Analytics Vidyha WNS Analytics Hackathon 2018, I would try to predict promotion of employees.**"},{"metadata":{"_uuid":"8e49ed247e3dc020230a8b236f814a2cfb574a77"},"cell_type":"markdown","source":"# **1. Import packages and dataset**"},{"metadata":{"trusted":true,"_uuid":"22f35af9ee1005cc47bf833921e191a818ff4e3d"},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport seaborn as sns\nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport cufflinks as cf\nfrom plotly import tools\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.grid_search import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.linear_model import LinearRegression\nimport statsmodels.formula.api as sm\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nimport xgboost as xgb\nimport scikitplot as skplt\nfrom sklearn.metrics import average_precision_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import f1_score\nfrom sklearn import preprocessing\nimport warnings\n#plt.style.use(['dark_background'])\n%matplotlib inline\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6056fa7851aca7d6430a304a44b897b61a33abcd"},"cell_type":"code","source":"direc = \"../input/\"\ntrain = pd.read_csv(direc + \"train_LZdllcl.csv\")\ntest = pd.read_csv(direc + \"test_2umaH9m.csv\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3cab94107dcd2ff2f72361628f2fb0d1809e34ba"},"cell_type":"markdown","source":"# **2. Exploratory Analysis**"},{"metadata":{"trusted":true,"_uuid":"70e653d2f42440ae7cfd0176b9b075669286d9fd"},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bb915ec23268d6adc99a349f3d660cdbdefc53f4"},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f02bfe582c5d1353b41f732b7e57128189abfa74"},"cell_type":"code","source":"train.describe()  #5-number summary for numerical columns","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5e807525ad48914460038591a43c395176bc8c25"},"cell_type":"markdown","source":"**Dealing with Missing Values**"},{"metadata":{"_uuid":"2512c75e123b814abc1ed335b5b3082c31c7e6ef"},"cell_type":"markdown","source":"**Checking for missing values**"},{"metadata":{"trusted":true,"_uuid":"ec80cd2416498565f49c6a52408d2456f6c97def"},"cell_type":"code","source":"for i in train.columns:\n    print (i + \": \"+str(sum(train[i].isnull()))+\" missing values\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fff7d135ab442815042cb2eaf2cc38bdd66496f8"},"cell_type":"markdown","source":"\nOnly education and previous year rating have missing values. We will remove the missing values since the number of rows that will be removed is small in comparison with the amount of rows we have."},{"metadata":{"trusted":true,"_uuid":"155beaf9f300121e2c42ba17f54398f3208e4272"},"cell_type":"code","source":"train = train.dropna()\ntrain.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"71e8b38ac76d73408d37c536191bd5b724334b1d"},"cell_type":"code","source":"train[\"high_prev_rating\"] = np.where(train[\"previous_year_rating\"]>=3,1,0)\ntrain[\"low_prev_rating\"] = np.where(train[\"previous_year_rating\"]<3,1,0)\ntrain = train.drop([\"previous_year_rating\"],axis=1)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0b96afc38772b6c4cf7dc6eedb8c30a83d4c6f0f"},"cell_type":"markdown","source":"**2.1 Let's start by visualising the distribution of each columns.**"},{"metadata":{"_uuid":"80d8bf5b1488f9b2058197415822279b2a3bcbc0"},"cell_type":"markdown","source":"**2.1.1 Distribution of categorical features**"},{"metadata":{"trusted":true,"_uuid":"16699fdf86f6b95891f0b76016f4210bb6dbb50f"},"cell_type":"code","source":"trace1 = go.Bar(\n            x=['Not Promoted','Promoted'],\n            y=[sum(train[\"is_promoted\"]==0),sum(train[\"is_promoted\"]==1)],\n            marker=dict(color=[\"red\",\"red\"]),\n            name=\"Promotion Rate\",\n    )\n\ntrace2 = go.Bar(\n            x=['Males','Females'],\n            y=[sum(train[\"gender\"]=='m'),sum(train[\"gender\"]=='f')],\n            marker=dict(color=[\"yellow\",\"yellow\"]),\n            name='Gender'\n    )\n\n\ntrace3 = go.Bar(\n            x=['Did Not Win','Won Award'],\n            y=[sum(train[\"awards_won?\"]==0),sum(train[\"awards_won?\"]==1)],\n            marker=dict(color=[\"green\",\"green\"]),\n            name='Award won'\n    \n    )\n\ntrace4 = go.Bar(\n            x=['Did not meet KPI','Met KPI'],\n            y=[sum(train[\"KPIs_met >80%\"]==0),sum(train[\"KPIs_met >80%\"]==1)],\n            marker=dict(color=[\"blue\",\"blue\"]),\n            name='Met KPI'\n    )\n\ntrace5 = go.Bar(\n            x=['Other','Sourcing','Referred'],\n            y=[sum(train[\"recruitment_channel\"]=='other'),sum(train[\"recruitment_channel\"]=='sourcing'),sum(train['recruitment_channel']=='referred')],\n            marker=dict(color=[\"lime\",\"lime\",\"lime\"]),\n            name='Recruitment Channels'\n    )\n\ntrace6 = go.Bar(\n            x=list(train[\"education\"].unique()),\n            y=[sum(train[\"education\"]==i) for i in list(train[\"education\"].unique())],\n            marker=dict(color=[\"purple\",\"purple\",\"purple\"]),\n            name=\"Education\"\n    )\ntrace7 = go.Histogram(x=train['age'],name=\"Distribution of Age\")\n\ntrace8= go.Histogram(x=train['length_of_service'],name=\"Length of Service\")\n\ntrace9=go.Histogram(x=train['avg_training_score'],name=\"Distribution of average training score\")\n\nfig = tools.make_subplots(rows=3, cols=3,\n                          subplot_titles=[\"Promotion rate (Training Set): \" + str(round(100*(sum(train[\"is_promoted\"]==1)/train.shape[0]),2)) +\"%\",\n                                         \"Male: \" + str(round(100*(sum(train[\"gender\"]==\"m\")/train.shape[0]),2)) + \"%, Female: \" + str(round(100-100*(sum(train[\"gender\"]==\"m\")/train.shape[0]),2))+\"%\",\n                                         \"Award Winning rate: \" + str(round(100*(sum(train[\"awards_won?\"]==1)/train.shape[0]),2)) +\"%\",\n                                         \"Percent of KPI>80% Rate: \" + str(round(100*(sum(train[\"KPIs_met >80%\"]==1)/train.shape[0]),2)) +\"%\",\n                                         \"Recruitment Channels\",\n                                         \"Distribution of Education\",\n                                          \"Distribution of Age\",\n                                          \"Length of Service\",\n                                          \"Average Training Score\"\n                                         ])\nfor i in fig['layout']['annotations']:\n    i['font'] = dict(size=10,color='black')\nfig.append_trace(trace1, 1,1)\nfig.append_trace(trace2, 1, 2)\nfig.append_trace(trace3, 1, 3)\nfig.append_trace(trace4, 2, 1)\nfig.append_trace(trace5, 2, 2)\nfig.append_trace(trace6, 2, 3)\nfig.append_trace(trace7, 3, 1)\nfig.append_trace(trace8, 3, 2)\nfig.append_trace(trace9, 3, 3)\nfig['layout'].update(height=900, width=900, title=\"<b>Distribution of Features<b>\")\npy.iplot(fig)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"136d971f7d821452e8022c1a6acff90676cb0bf5"},"cell_type":"markdown","source":"We can see that the distribution of age is slightly skewed to the right. The mean age is around 34.8 while the median age is 33 based on the 5-number summary. "},{"metadata":{"_uuid":"214df150e8e357f3cbc8cf7ceba9688e519a9f2d"},"cell_type":"markdown","source":"The distribution of the length of service is also right-skewed, with the mean 5.8 years and the median 5 years."},{"metadata":{"_uuid":"089f2f89c56010bf5f1033186e8bd61ab1ebe060"},"cell_type":"markdown","source":"**2.1.2 Departments**"},{"metadata":{"trusted":true,"_uuid":"c081687da00b0cf1dc82e8767771b82f5d1f99f8"},"cell_type":"code","source":"labels=list(train[\"department\"].unique())\nsizes=[sum(train[\"department\"]==x) for x in labels]\n\ntrace = go.Pie(labels=labels, values=sizes,textfont=dict(size=13,color=\"black\"))\nlayout = go.Layout(\n    width=750,\n    height=400,\n    title = \"<b>Proportion of employees in each department<b>\",\n)\nfig=go.Figure([trace],layout=layout)\npy.iplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a4ef4b357d8c72c7d1d8f8018fa93a2351a1ade6"},"cell_type":"markdown","source":"**2.1.3 Trainings**"},{"metadata":{"trusted":true,"_uuid":"06b2a828b664d3cc875f6b170b0dbb052d8c4fe8"},"cell_type":"code","source":"ax=sns.distplot(train[\"no_of_trainings\"],kde=False)\nplt.title(\"Number of Trainings\")\nplt.xlabel(\"Number\")\nplt.ylabel(\"Proportion\")\nfor txt in ax.texts:\n    txt.set_visible(False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a740385bfcc6701dc925b91d53046966cfe4aeb6"},"cell_type":"markdown","source":"**2.2 Comparing variables by groups**"},{"metadata":{"trusted":true,"_uuid":"7f67ba23788c27043004c7ff2f182bbbc0f4634d"},"cell_type":"code","source":"sns.factorplot(y=\"age\",x=\"gender\", hue=\"department\",data=train,kind=\"box\",size=4,aspect=8/5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8949059d11a8d25dc88177cae0a35c389d724fde"},"cell_type":"code","source":"sns.factorplot(y=\"length_of_service\",x=\"gender\", hue=\"department\",data=train,kind=\"box\",size=4,aspect=8/4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"82fc75a038b748eaf83e755ca102afa836aa8b5d"},"cell_type":"code","source":"sns.factorplot(y=\"age\",x=\"gender\", hue=\"education\",data=train,kind=\"box\",size=4,aspect=8/5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c80019d12e2838d398fa350b4c757b4eeecb2688"},"cell_type":"code","source":"sns.factorplot(y=\"age\",x=\"gender\", hue=\"is_promoted\",data=train,kind=\"box\",size=4,aspect=8/5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cb492f4cc793987e9112b4b2e1b24adb81f0268d"},"cell_type":"code","source":"sns.factorplot(y=\"age\",x=\"department\", hue=\"is_promoted\",data=train,kind=\"box\",size=4,aspect=8/2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"49e98237bab177f0dc7dc40ebb820cb660b15c7b"},"cell_type":"code","source":"sns.factorplot(y=\"age\",x=\"education\", hue=\"is_promoted\",data=train,kind=\"box\",size=4,aspect=8/6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e94fa5e32516f16b4f3d65be48a9427ae4559e17"},"cell_type":"code","source":"sns.factorplot(y=\"age\",x=\"is_promoted\", hue=\"education\",data=train,kind=\"box\",size=4,aspect=8/6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d47f196246f1820c5cbd038e8800a67d9712879f"},"cell_type":"code","source":"sns.factorplot(y=\"age\",x=\"is_promoted\", hue=\"department\",data=train,kind=\"box\",size=4,aspect=8/2)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1e42a3c17c3e7f6dac5b84e4e7c936dff35889df"},"cell_type":"markdown","source":"**2.3 Let's check the promotion rate across different variables**"},{"metadata":{"trusted":true,"_uuid":"595918c8516a64dc4d17e5f3d626f20b0f4f8c3a"},"cell_type":"code","source":"def promoted_distribution(variable):\n    num = len(list(train[variable].unique()))\n    data=[]\n    for i in range(num):\n        data.append(go.Bar(\n            x=['Promoted','Not Promoted'],\n            y=[train[train['is_promoted']==1][variable].value_counts()[i],train[train['is_promoted']==0][variable].value_counts()[i]],\n            name=str(train[train['is_promoted']==1][variable].value_counts().index[i])))\n    layout = go.Layout(\n        width=500,\n        height=400,\n        barmode='stack',\n        title = \"Promotion rate among \" + str(variable)\n    )\n\n    fig = go.Figure(data=data, layout=layout)\n    py.iplot(fig, filename='stacked-bar')\n    \n    \ndef promoted_stacked_bar(variable):\n    x1=list(train[variable].unique())\n    trace1 = go.Bar(\n        x=x1,\n        y=[train[train[variable]==x1[i]][\"is_promoted\"].value_counts()[0] for i in range(len(x1))],\n        name='Not Promoted'\n    )\n    trace2 = go.Bar(\n        x=x1,\n        y=[train[train[variable]==x1[i]][\"is_promoted\"].value_counts()[1] for i in range(len(x1))],\n        name='Promoted'\n    )\n    layout = go.Layout(\n        width=500,\n        height=400,\n        barmode='stack',\n        title = \"Promotion rate among \" + str(variable)\n    )\n    data=[trace1,trace2]\n\n    fig = go.Figure(data=data, layout=layout)\n    py.iplot(fig)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d5c7af814cbb88f4f7992f4d7bdb8c4dfd788e11"},"cell_type":"code","source":"ls = ['department', 'region', 'education', 'gender', 'recruitment_channel', 'KPIs_met >80%', 'awards_won?']\nfor i in ls:\n    promoted_stacked_bar(i)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fd75a99be68a01bddaac8612d3c997eef6d09e38"},"cell_type":"markdown","source":"We can see that there isn't any clear obvious bias linking any variables with promotion rate."},{"metadata":{"_uuid":"1ee69671fa2e19dabd47009813085b70647ad728"},"cell_type":"markdown","source":"**Correlation Heatmap**"},{"metadata":{"trusted":true,"_uuid":"09622b495a24d7e0e2ac171ad4709b7a3081b00e"},"cell_type":"code","source":"for cols in [\"department\",\"region\",\"education\",\"recruitment_channel\",\"gender\"]:\n    train[cols] = train[cols].astype('category')\n    train[cols] = train[cols].cat.codes\nf,ax = plt.subplots(figsize=(12,12))\nsns.heatmap(train.corr(), annot=True, linewidths=0.5, fmt= '.2f',ax=ax)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d4b836651ffe303c08192f24a7904bfaa7cc612f"},"cell_type":"markdown","source":"We see again from the correlation heatmap that almost all the variables are not directly correlated with promotion rate. The variables with the highest correlation (0.22 and 0.20 respectively) is whether the KPI is met and whether an award was won. "},{"metadata":{"_uuid":"a7c22596c5e5dd3b871ec0e5fb1d8a891a86c730"},"cell_type":"markdown","source":"# 3.Feature Engineering"},{"metadata":{"_uuid":"d33f4cea74bebca71f54e361ee19e42ed803f4ae"},"cell_type":"markdown","source":"We shall add another variable that states if an employee has won both awards and met KPI."},{"metadata":{"trusted":true,"_uuid":"f1cc024d2a8c1d652d6b9d2dd67c897663921a22"},"cell_type":"code","source":"train[\"KPI&Award\"] = np.where(((train[\"KPIs_met >80%\"]==1) & (train[\"awards_won?\"]==1)),1,0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d9859c800788c0edec96b22fd3f722af471b2294"},"cell_type":"code","source":"promoted_stacked_bar(\"KPI&Award\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"aa6bac34ff854e6b9d3a78b8aafcd19d465f7539"},"cell_type":"code","source":"train_features_eng = train\ntrain_features_eng = train_features_eng.drop(['employee_id','recruitment_channel','no_of_trainings','gender','length_of_service','region'],axis=1) #features that are not needed","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f5b6b71e01ebf9f6e5086bbd78038e70845349ae"},"cell_type":"markdown","source":"**Encode all categorical variables**"},{"metadata":{"trusted":true,"_uuid":"83e9e13060fd1209e22ea5908ee7212232f7ce0a"},"cell_type":"code","source":"train_features_eng=pd.get_dummies(train_features_eng, columns=[\"department\",\"education\"], prefix=[\"Dept\", \"Eduacation\"])\ntrain_features_eng.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"198882fe6931b10c4b112ec5f665ee2a85d4208d"},"cell_type":"markdown","source":"# 4. Classification models\n\nThe models will be optimised using GridSearchCV based on F1 score. F1 score gives a weighted average between precision and accuracy/recall. It tells you how precise your classifier is (how many instances it classifies correctly), as well as how robust it is (it does not miss a significant number of instances).\n\nI have typed in some of the optimised parameters based on the GridSearchCV code output, then commented out the GridSearchCV codes to make the notebook run faster as it won't be re-optimised."},{"metadata":{"trusted":true,"_uuid":"b5bea2098d70806b60ff6a6287d8cb05b93c6aa2"},"cell_type":"code","source":"def train_f1(model):\n    return round(f1_score(y_train,model.predict(x_train),average='macro'),2)\n\ndef test_f1(model):\n    return round(f1_score(y_test,model.predict(x_test),average='macro'),2)\n\ndef confusion_matrix_model(model_used):\n    cm=confusion_matrix(y_test,model_used.predict(x_test))\n    col=[\"Predicted Promoted\",\"Predicted No Promotion\"]\n    cm=pd.DataFrame(cm)\n    cm.columns=[\"Predicted Promoted\",\"Predicted No Promotion\"]\n    cm.index=[\"Actual Promoted\",\"Actual No Promotion\"]\n    return cm.T\n\ndef confusion_matrix_model_train(model_used):\n    cm=confusion_matrix(y_train,model_used.predict(x_train))\n    col=[\"Predicted Promoted\",\"Predicted No Promotion\"]\n    cm=pd.DataFrame(cm)\n    cm.columns=[\"Predicted Promoted\",\"Predicted No Promotion\"]\n    cm.index=[\"Actual Promoted\",\"Actual No Promotion\"]\n    return cm.T\n\ndef importance_of_features(model):\n    features = pd.DataFrame()\n    features['feature'] = x_train.columns\n    features['importance'] = model.feature_importances_\n    features.sort_values(by=['importance'], ascending=True, inplace=True)\n    features.set_index('feature', inplace=True)\n    return features.plot(kind='barh', figsize=(6,6))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cfcc07357dbdfbdfd1068b467643454e57a49292"},"cell_type":"code","source":"x1 = train_features_eng.drop([\"is_promoted\"],axis=1)\ny1 = train_features_eng.loc[:,\"is_promoted\"]\nx_train,x_test,y_train,y_test=train_test_split(x1,y1,test_size=0.2,random_state=0,stratify=y1)\nk_fold = KFold(n_splits=5, shuffle=True, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d98d58e9290173f6bbc6afa32de66b5c54a87147"},"cell_type":"code","source":"x_train.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e30f5ed0a576431594558d71c14e162c07adc9ea"},"cell_type":"markdown","source":"**Logistic Regression (Lasso)**"},{"metadata":{"trusted":true,"_uuid":"0f885acfdc8742f9e3cc840863f88aeecf7a3d27"},"cell_type":"code","source":"#param_grid = dict(C=(0.0001,0.001,0.005,0.01,0.1,0.5))\n#log_reg1 = GridSearchCV(LogisticRegression(penalty=\"l1\"),param_grid=param_grid,scoring=\"f1_macro\")\nlog_reg1=LogisticRegression(penalty=\"l1\",C=0.5)\nlog_reg1.fit(x_train,y_train)\n#print(log_reg1.best_params_)\nprint (\"In-sample F1 Score: \" + str(train_f1(log_reg1)))\nprint (\"Test F1 Score: \" + str(test_f1(log_reg1)))\n#confusion_matrix_model(log_reg1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c321ce5668faf1c3e714bda8c0306c6c482898dd"},"cell_type":"markdown","source":"**Logistic Regression (Ridge)**"},{"metadata":{"trusted":true,"_uuid":"baa0819bd5eef050b65c03ce5b04bb5428a89ffd"},"cell_type":"code","source":"#param_grid = dict(C=(0.0001,0.001,0.005,0.01,0.1,0.5,1))\n#log_reg2 = GridSearchCV(LogisticRegression(penalty=\"l2\"),param_grid=param_grid,scoring=\"f1_macro\")\nlog_reg2=LogisticRegression(penalty=\"l2\",C=1)\nlog_reg2.fit(x_train,y_train)\n#print(log_reg2.best_params_)\nprint (\"In-sample F1 Score: \" + str(train_f1(log_reg2)))\nprint (\"Test F1 Score: \" + str(test_f1(log_reg2)))\n#confusion_matrix_model(log_reg2)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"64c1a78dea5aa40178e71e5c3c4aa69db1c616d2"},"cell_type":"markdown","source":"**SVC (RBF Kernel)**"},{"metadata":{"trusted":true,"_uuid":"bcc29fe9895f5a501a01ec32c96a2b9b117db304"},"cell_type":"code","source":"#param_grid = dict(C=(0.001,0.01,0.1,0.5,1,2),gamma=(0.001,0.01,0.1,0.5,1,2))\n#svc_rbf = GridSearchCV(SVC(kernel=\"rbf\",random_state=0),param_grid=param_grid,scoring=\"f1_macro\")\nsvc_rbf = SVC(kernel='rbf', gamma=0.001, C=0.01,random_state=0)\nsvc_rbf.fit(x_train, y_train)\n#print(svc_rbf.best_params_)\n\nprint (\"In-sample F1 Score: \" + str(train_f1(svc_rbf)))\nprint (\"Test F1 Score: \" + str(test_f1(svc_rbf)))\n#confusion_matrix_model(svc_rbf)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f3a68dc9e81862761ffe9c150d12056d017368ca"},"cell_type":"markdown","source":"**KNN**"},{"metadata":{"trusted":true,"_uuid":"9d22a9c195bdf2e14b3e3bee6e0931ef21800c6e"},"cell_type":"code","source":"#param_grid = dict(n_neighbors=np.arange(10,70),weights=(\"uniform\",\"distance\"),p=(1,2))\n#KNN = GridSearchCV(KNeighborsClassifier(),param_grid=param_grid,scoring=\"f1_macro\")\nKNN=KNeighborsClassifier(n_neighbors=30,p=1,weights='distance')\nKNN.fit(x_train,y_train)\n#print(KNN.best_params_)\nprint (\"In-sample F1 Score: \" + str(train_f1(KNN)))\nprint (\"Test F1 Score: \" + str(test_f1(KNN)))\n#confusion_matrix_model(KNN)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cdb8f36c6f2528b11f1a480426453a6cdb5024be"},"cell_type":"markdown","source":"**Decision Tree**"},{"metadata":{"trusted":true,"_uuid":"181ec922cd193488ec98c27240286f4d6323ba97"},"cell_type":"code","source":"#param_grid = dict(max_depth=np.arange(4,10),min_samples_leaf=np.arange(1,8),min_samples_split=np.arange(2,8),max_leaf_nodes=np.arange(30,100,10))\n#Dec_tree = GridSearchCV(DecisionTreeClassifier(),param_grid=param_grid,scoring=\"f1_macro\")\nDec_tree=DecisionTreeClassifier(max_depth= 9, max_leaf_nodes= 60, min_samples_leaf= 7, min_samples_split= 2)\nDec_tree.fit(x_train,y_train)\n#print(Dec_tree.best_params_)\nprint (\"In-sample F1 Score: \" + str(train_f1(Dec_tree)))\nprint (\"Test F1 Score: \" + str(test_f1(Dec_tree)))\n#confusion_matrix_model(Dec_tree)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dece82de860cf11fe54e4415b3fdcf84c19cba67"},"cell_type":"markdown","source":"**Random Forest**"},{"metadata":{"trusted":true,"_uuid":"11bc6ff473db05826e96882ec31dc843fcebb5d0"},"cell_type":"code","source":"#param_grid = dict(max_depth=np.arange(3,10),min_samples_leaf=np.arange(1,10),min_samples_split=np.arange(2,6),max_leaf_nodes=np.arange(50,120,10))\n#param_grid = dict(n_estimators = np.arange(50,500,50))\n#ranfor = GridSearchCV(RandomForestClassifier(n_estimators=450,max_depth= 9, max_leaf_nodes=110, min_samples_leaf= 1, min_samples_split= 2,random_state=0),param_grid=param_grid,scoring=\"f1_macro\")\n#ranfor = GridSearchCV(RandomForestClassifier(max_depth= 7, max_leaf_nodes=100, min_samples_leaf= 6, min_samples_split= 2,random_state=0),param_grid=param_grid,scoring=\"accuracy\")\nranfor = RandomForestClassifier(n_estimators=450,max_depth= 9, max_leaf_nodes=110, min_samples_leaf= 1, min_samples_split= 2,random_state=0)\nranfor.fit(x_train,y_train)\n#print(ranfor.best_params_)\nprint (\"In-sample F1 Score: \" + str(train_f1(ranfor)))\nprint (\"Test F1 Score: \" + str(test_f1(ranfor)))\n#confusion_matrix_model(ranfor)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f518f989677b754c4b10bcbfb86913f4d034a818"},"cell_type":"markdown","source":"**XGBoost**"},{"metadata":{"trusted":true,"_uuid":"3a187f1e7303c0dcec7b568964ccfeb69e60b9e2"},"cell_type":"code","source":"#param_grid = dict(n_estimators=np.arange(50,500,50),max_depth=np.arange(6,12),learning_rate=(0.0001,0.001,0.01,0.1))\n#xgclass = GridSearchCV(xgb.XGBClassifier(random_state=0),param_grid=param_grid,scoring=\"accuracy\")\nxgclass = xgb.XGBClassifier(max_depth=9, n_estimators=450, learning_rate=0.01)\nxgclass.fit(x_train,y_train)\n#print(xgclass.best_params_)\nprint (\"In-sample F1 Score: \" + str(train_f1(xgclass)))\nprint (\"Test F1 Score: \" + str(test_f1(xgclass)))\nconfusion_matrix_model(xgclass)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"84a4c1b70765b894e8c0d6bbb18573ea37ddcfbd"},"cell_type":"code","source":"importance_of_features(xgclass)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3c19a797e2c56c5d30b2ec074c3be58b97edfe6d"},"cell_type":"code","source":"Classifiers=[\"Logistic Regression (Lasso)\",\"Logistic Regression (Ridge)\",\"SVC (RBF Kernel)\",\"K-Nearest Neighbours\",\"Decision Tree\",\"Random Forest\",\"XGBoost\"]\ntrainf1 = [train_f1(x) for x in [log_reg1,log_reg2,svc_rbf,KNN,Dec_tree,ranfor,xgclass]]\ntestf1 = [test_f1(x) for x in [log_reg1,log_reg2,svc_rbf,KNN,Dec_tree,ranfor,xgclass]]\ncols=[\"Classifier\",\"Training F1 Score\",\"Test F1 Score\"]\npred_results = pd.DataFrame(columns=cols)\npred_results[\"Classifier\"]=Classifiers\npred_results[\"Training F1 Score\"]=trainf1\npred_results[\"Test F1 Score\"]=testf1\npred_results","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a0bff14df2a0c55cf5a713c2ca42c63c06e3a65d"},"cell_type":"markdown","source":"**XGBoost would be the model chosen due to the highest Test-set F1 score**"},{"metadata":{"trusted":true,"_uuid":"aa1b8c440f31929eacaa938d9a5035551caadc26"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4ebeac06468180ed97335392824ab325ee17a7b0"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"}},"nbformat":4,"nbformat_minor":1}