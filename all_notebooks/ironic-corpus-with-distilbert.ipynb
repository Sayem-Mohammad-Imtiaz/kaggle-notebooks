{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Ironic Sentences - with DistilBERT"},{"metadata":{},"cell_type":"markdown","source":"This kernel is taken from [this tutorial kernel](https://www.kaggle.com/maroberti/fastai-with-transformers-bert-roberta) ADD URL, which is a suplement of the Medium article [\"Fastai with ðŸ¤—Transformers (BERT, RoBERTa, XLNet, XLM, DistilBERT)\"](https://medium.com/p/fastai-with-transformers-bert-roberta-xlnet-xlm-distilbert-4f41ee18ecb2?source=email-29c8f5cf1dc4--writer.postDistributed&sk=119c3e5d748b2827af3ea863faae6376). Make sure to upvote the original tutorial, too, if you found this helpful."},{"metadata":{},"cell_type":"markdown","source":"## Transformers: another type of transfer learning"},{"metadata":{},"cell_type":"markdown","source":"This kernel uses pretrained transformers to perform a classification tasks. In another kernel, I use the [ULMFiT tecchnique]().\n\nSince the introduction of ULMFiT, **Transfer Learning** became very popular in NLP and yet Google (BERT, Transformer-XL, XLNet), Facebook (RoBERTa, XLM) or even OpenAI (GPT, GPT-2) begin to pre-train their own model on very large corpora. This time, instead of using the AWD-LSTM neural network, they all used a more powerful architecture based on the Transformer (cf. [Attention is all you need](https://arxiv.org/abs/1706.03762)).\n\nAlthough these models are powerful, ``fastai`` do not integrate all of them. Fortunately, [HuggingFace](https://huggingface.co/) ðŸ¤— created the well know [transformers library](https://github.com/huggingface/transformers). Formerly knew as ``pytorch-transformers`` or ``pytorch-pretrained-bert``, this library brings together over 40 state-of-the-art pre-trained NLP models (BERT, GPT-2, RoBERTa, CTRLâ€¦). The implementation gives interesting additional utilities like tokenizer, optimizer or scheduler.\n\nThe ``transformers`` library can be self-sufficient but incorporating it within the ``fastai`` library provides simpler implementation compatible with powerful fastai tools like  **Discriminate Learning Rate**, **Gradual Unfreezing** or **Slanted Triangular Learning Rates**. The point here is to allow non-NLP-experts to get easily state-of-the-art results and therefore \"make NLP uncool again\".\n\nBefore beginning the implementation, note that integrating ``transformers`` within ``fastai`` can be done in multiple different ways. The tutorial author decided to implement generic and flexible solutions. Specifically, he made the minimum amount of modifications in both libraries while making them compatible with the maximum amount of transformer architectures. Go check out the [original tutorial](https://www.kaggle.com/maroberti/fastai-with-transformers-bert-roberta) to see other transfomer architectures. \n\nThis notebook contains the following sections:\n1. Loading the data\n1. Integrating transformers and fastai for data processing\n    - Custom Tokenizer\n    - Custom Numericalizer\n    - Custom Processor\n1. Preparing cross validations and testing the DataBunch\n1. Creating a custom model\n\n"},{"metadata":{"heading_collapsed":true},"cell_type":"markdown","source":"# Loading the data"},{"metadata":{"hidden":true},"cell_type":"markdown","source":"Before starting the implementation, you will need to install the ``fastai`` and ``transformers`` libraries. To do so, just follow the instructions [here](https://github.com/fastai/fastai/blob/master/README.md#installation) and [here](https://github.com/huggingface/transformers#installation).\n\nIn Kaggle, the ``fastai`` library is already installed. So you just have to instal ``transformers`` with :"},{"metadata":{"hidden":true,"trusted":true},"cell_type":"code","source":"%%bash\npip install transformers","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","hidden":true,"trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom pathlib import Path \nfrom collections import defaultdict\n\nimport os\n\nimport torch\nimport torch.optim as optim\n\nimport random \n\n# fastai\nfrom fastai import *\nfrom fastai.text import *\nfrom fastai.callbacks import *\n\n# transformers\nfrom transformers import PreTrainedModel, PreTrainedTokenizer, PretrainedConfig\nfrom transformers import DistilBertForSequenceClassification, DistilBertTokenizer, DistilBertConfig","execution_count":null,"outputs":[]},{"metadata":{"hidden":true},"cell_type":"markdown","source":"The current versions of the fastai and transformers libraries are respectively 1.0.58 and 2.1.1."},{"metadata":{"hidden":true,"trusted":true},"cell_type":"code","source":"import fastai\nimport transformers\nprint('fastai version :', fastai.__version__)\nprint('transformers version :', transformers.__version__)","execution_count":null,"outputs":[]},{"metadata":{"hidden":true},"cell_type":"markdown","source":"Load and check the Irony Corpus data"},{"metadata":{"trusted":true},"cell_type":"code","source":"for dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","hidden":true,"trusted":true},"cell_type":"code","source":"irony_data = pd.read_csv('/kaggle/input/ironic-corpus/irony-labeled.csv')\nirony_data.head()\nprint(irony_data.shape)\nirony_data.head()","execution_count":null,"outputs":[]},{"metadata":{"hidden":true,"trusted":false},"cell_type":"code","source":"irony_data.label.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"heading_collapsed":true},"cell_type":"markdown","source":"# Integrating transformers and fastai for data processing"},{"metadata":{"hidden":true},"cell_type":"markdown","source":"In ``transformers``, each model architecture is associated with 3 main types of classes:\n* A **model class** to load/store a particular pre-train model.\n* A **tokenizer class** to pre-process the data and make it compatible with a particular model.\n* A **configuration class** to load/store the configuration of a particular model.\n\nFor example, if you want to use the Bert architecture for text classification, you would use [``BertForSequenceClassification``](https://huggingface.co/transformers/model_doc/bert.html#bertforsequenceclassification) for the **model class**, [``BertTokenizer``](https://huggingface.co/transformers/model_doc/bert.html#berttokenizer) for the **tokenizer class** and [``BertConfig``](https://huggingface.co/transformers/model_doc/bert.html#bertconfig) for the **configuration class**.Â \n\nFirst, we set some parameters, and create a utility function for random numbers."},{"metadata":{"hidden":true,"trusted":false},"cell_type":"code","source":"seed = 42\nuse_fp16 = False\nbs = 16\n\nmodel_type = 'distilbert'\npretrained_model_name = 'distilbert-base-uncased'\n\nMODEL_CLASSES = {\n    'distilbert': (DistilBertForSequenceClassification, DistilBertTokenizer, DistilBertConfig)\n}\n\nmodel_class, tokenizer_class, config_class = MODEL_CLASSES[model_type]","execution_count":null,"outputs":[]},{"metadata":{"hidden":true},"cell_type":"markdown","source":"Although I've chosen to use the 'distilbert-base-uncased' model, there are a few distilbert moddels to choose from. "},{"metadata":{"hidden":true,"trusted":false},"cell_type":"code","source":"model_class.pretrained_model_archive_map.keys()","execution_count":null,"outputs":[]},{"metadata":{"hidden":true},"cell_type":"markdown","source":"Function to set the seed for generating random numbers."},{"metadata":{"hidden":true,"trusted":false},"cell_type":"code","source":"def seed_all(seed_value):\n    random.seed(seed_value) # Python\n    np.random.seed(seed_value) # cpu vars\n    torch.manual_seed(seed_value) # cpu  vars\n    \n    if torch.cuda.is_available(): \n        torch.cuda.manual_seed(seed_value)\n        torch.cuda.manual_seed_all(seed_value) # gpu vars\n        torch.backends.cudnn.deterministic = True  #needed\n        torch.backends.cudnn.benchmark = False","execution_count":null,"outputs":[]},{"metadata":{"hidden":true,"trusted":false},"cell_type":"code","source":"seed_all(seed)","execution_count":null,"outputs":[]},{"metadata":{"hidden":true},"cell_type":"markdown","source":"To match pre-training, we have to format the model input sequence in a specific format.\nTo do so, you have to first **tokenize** and then **numericalize** the texts correctly.\nThe difficulty here is that each pre-trained model, that we will fine-tune, requires exactly the same specific pre-processâ€Š-â€Š**tokenization** & **numericalization**â€Š-â€Šthan the pre-process used during the pre-train part.\nFortunately, the **tokenizer class** from ``transformers`` provides the correct pre-process tools that correspond to each pre-trained model.\n\nIn the ``fastai`` library, data pre-processing is done automatically during the creation of the ``DataBunch``. \nAs you will see in the ``DataBunch`` implementation, the **tokenizer** and **numericalizer** are passed in the processor argument under the following format :\n\n``processor = [TokenizeProcessor(tokenizer=tokenizer,...), NumericalizeProcessor(vocab=vocab,...)]``\n\nLet's first analyse how we can integrate the ``transformers`` **tokenizer** within the ``TokenizeProcessor`` function."},{"metadata":{"hidden":true},"cell_type":"markdown","source":"## Custom Tokenizer"},{"metadata":{"hidden":true},"cell_type":"markdown","source":"This part can be a little bit confusing because a lot of classes are wrapped in each other and with similar names.\nTo resume, if we look attentively at the ``fastai`` implementation, we notice thatÂ :\n1. The [``TokenizeProcessor`` object](https://docs.fast.ai/text.data.html#TokenizeProcessor) takes as ``tokenizer`` argument a ``Tokenizer`` object.\n2. The [``Tokenizer`` object](https://docs.fast.ai/text.transform.html#Tokenizer) takes as ``tok_func`` argument a ``BaseTokenizer`` object.\n3. The [``BaseTokenizer`` object](https://docs.fast.ai/text.transform.html#BaseTokenizer) implement the function ``tokenizer(t:str) â†’ List[str]`` that take a text ``t`` and returns the list of its tokens.\n\nTherefore, we can simply create a new class ``TransformersBaseTokenizer`` that inherits from ``BaseTokenizer`` and overwrite a new ``tokenizer`` function."},{"metadata":{"hidden":true,"trusted":false},"cell_type":"code","source":"class TransformersBaseTokenizer(BaseTokenizer):\n    \"\"\"Wrapper around PreTrainedTokenizer to be compatible with fast.ai\"\"\"\n    def __init__(self, pretrained_tokenizer: PreTrainedTokenizer, model_type = 'bert', **kwargs):\n        self._pretrained_tokenizer = pretrained_tokenizer\n        self.max_seq_len = pretrained_tokenizer.max_len\n        self.model_type = model_type\n\n    def __call__(self, *args, **kwargs): \n        return self\n\n    def tokenizer(self, t:str) -> List[str]:\n        \"\"\"Limits the maximum sequence length and add the spesial tokens\"\"\"\n        CLS = self._pretrained_tokenizer.cls_token\n        SEP = self._pretrained_tokenizer.sep_token\n        if self.model_type in ['roberta']:\n            tokens = self._pretrained_tokenizer.tokenize(t, add_prefix_space=True)[:self.max_seq_len - 2]\n        else:\n            tokens = self._pretrained_tokenizer.tokenize(t)[:self.max_seq_len - 2]\n        return [CLS] + tokens + [SEP]","execution_count":null,"outputs":[]},{"metadata":{"hidden":true,"trusted":false},"cell_type":"code","source":"transformer_tokenizer = tokenizer_class.from_pretrained(pretrained_model_name)\ntransformer_base_tokenizer = TransformersBaseTokenizer(pretrained_tokenizer = transformer_tokenizer, model_type = model_type)\nfastai_tokenizer = Tokenizer(tok_func = transformer_base_tokenizer, pre_rules=[], post_rules=[])","execution_count":null,"outputs":[]},{"metadata":{"hidden":true,"trusted":false},"cell_type":"code","source":"tokenizer_class.pretrained_vocab_files_map","execution_count":null,"outputs":[]},{"metadata":{"hidden":true},"cell_type":"markdown","source":"## Custom Numericalizer"},{"metadata":{"hidden":true},"cell_type":"markdown","source":"In ``fastai``, [``NumericalizeProcessor``  object](https://docs.fast.ai/text.data.html#NumericalizeProcessor) takes as ``vocab`` argument a [``Vocab`` object](https://docs.fast.ai/text.transform.html#Vocab). \nHere, we create a new class ``TransformersVocab`` that inherits from ``Vocab`` and uses the functions ``convert_tokens_to_ids`` and ``convert_ids_to_tokens`` respectively to overwrite the ``numericalize`` and ``textify`` functions. "},{"metadata":{"hidden":true,"trusted":false},"cell_type":"code","source":"class TransformersVocab(Vocab):\n    def __init__(self, tokenizer: PreTrainedTokenizer):\n        super(TransformersVocab, self).__init__(itos = [])\n        self.tokenizer = tokenizer\n    \n    def numericalize(self, t:Collection[str]) -> List[int]:\n        \"Convert a list of tokens `t` to their ids.\"\n        return self.tokenizer.convert_tokens_to_ids(t)\n        #return self.tokenizer.encode(t)\n\n    def textify(self, nums:Collection[int], sep=' ') -> List[str]:\n        \"Convert a list of `nums` to their tokens.\"\n        nums = np.array(nums).tolist()\n        return sep.join(self.tokenizer.convert_ids_to_tokens(nums)) if sep is not None else self.tokenizer.convert_ids_to_tokens(nums)","execution_count":null,"outputs":[]},{"metadata":{"hidden":true},"cell_type":"markdown","source":"#### Custom processor\nNow that we have our custom **tokenizer** and **numericalizer**, we can create the custom **processor**. Notice we are passing the ``include_bos = False`` and ``include_eos = False`` options. This is because ``fastai`` adds its own special tokens by default which interferes with the ``[CLS]`` and ``[SEP]`` tokens added by our custom tokenizer."},{"metadata":{"hidden":true,"trusted":false},"cell_type":"code","source":"transformer_vocab =  TransformersVocab(tokenizer = transformer_tokenizer)\nnumericalize_processor = NumericalizeProcessor(vocab=transformer_vocab)\n\ntokenize_processor = TokenizeProcessor(tokenizer=fastai_tokenizer, include_bos=False, include_eos=False)\n\ntransformer_processor = [tokenize_processor, numericalize_processor]","execution_count":null,"outputs":[]},{"metadata":{"hidden":true},"cell_type":"markdown","source":"For the DataBunch creation, you have to pay attention to set the processor argument to our new custom processor ``transformer_processor`` and manage correctly the padding.\n\nAs mentioned in the HuggingFace documentation, DistilBERT models use absolute position embeddings, so it's usually advised to pad the inputs on the right rather than the left. Regarding XLNET, it is a model with relative position embeddings, therefore, you can either pad the inputs on the right or on the left."},{"metadata":{"hidden":true,"trusted":false},"cell_type":"code","source":"pad_first = bool(model_type in ['xlnet'])\npad_idx = transformer_tokenizer.pad_token_id","execution_count":null,"outputs":[]},{"metadata":{"heading_collapsed":true},"cell_type":"markdown","source":"# Preparing cross validations and testing the DataBunch"},{"metadata":{"hidden":true},"cell_type":"markdown","source":"Now that we have integrated the transformers compenents into the fastai architecture, we can begin to create the data structures we will need. Since the original paper used a 5 fold cross validation, we divide the data into 5 folds using `sklearn`."},{"metadata":{"hidden":true,"trusted":false},"cell_type":"code","source":"from sklearn.model_selection import KFold\n\nX = irony_data['comment_text']\ny = irony_data['label']\nkf = KFold(n_splits=5)\nkf.get_n_splits(X) # returns the number of splitting iterations in the cross-validator","execution_count":null,"outputs":[]},{"metadata":{"hidden":true},"cell_type":"markdown","source":"Here, we separate each fold into the 'train' and 'test' (i.e., validation) sets. "},{"metadata":{"hidden":true,"trusted":false},"cell_type":"code","source":"trains = list()\ntests = list()\nfor train_index, test_index in kf.split(X):\n    trains.append(train_index)\n    tests.append(test_index)","execution_count":null,"outputs":[]},{"metadata":{"hidden":true},"cell_type":"markdown","source":"Each fold is then condensed into one dataframe with the validationset labelled in the `is_valid` column."},{"metadata":{"hidden":true,"trusted":false},"cell_type":"code","source":"def create_validation(valnum):\n\n    train = {'comment_text': X[trains[valnum]], 'label': y[trains[valnum]],'is_valid':False}\n    dftrain = pd.DataFrame(data=train)\n    \n    valid = {'comment_text': X[tests[valnum]], 'label': y[tests[valnum]],'is_valid':True}\n    dfvalid = pd.DataFrame(data=valid)\n    \n    alldata = dftrain.append(dfvalid)\n    \n    return alldata","execution_count":null,"outputs":[]},{"metadata":{"hidden":true,"trusted":false},"cell_type":"code","source":"fold1 = create_validation(0)\nfold2 = create_validation(1)\nfold3 = create_validation(2)\nfold4 = create_validation(3)\nfold5 = create_validation(4)","execution_count":null,"outputs":[]},{"metadata":{"hidden":true},"cell_type":"markdown","source":"Fastai uses a structure called a `DataBunch` to hold everything related to the data: the processor, training and testing sets. Let's do a check to make sure it works correctly. "},{"metadata":{"hidden":true,"trusted":false},"cell_type":"code","source":"bs = 10\ndatabunch = (TextList.from_df(fold2, cols='comment_text', processor=transformer_processor)\n             .split_from_df(col='is_valid')\n             .label_from_df(cols= 'label')\n             .databunch(bs=bs, pad_first=pad_first, pad_idx=pad_idx))","execution_count":null,"outputs":[]},{"metadata":{"hidden":true},"cell_type":"markdown","source":"Check batch and tokenizer :"},{"metadata":{"hidden":true,"trusted":false},"cell_type":"code","source":"print('[CLS] token :', transformer_tokenizer.cls_token)\nprint('[SEP] token :', transformer_tokenizer.sep_token)\nprint('[PAD] token :', transformer_tokenizer.pad_token)\ndatabunch.show_batch()","execution_count":null,"outputs":[]},{"metadata":{"hidden":true},"cell_type":"markdown","source":"Check batch and numericalizer :"},{"metadata":{"hidden":true,"trusted":false},"cell_type":"code","source":"print('[CLS] id :', transformer_tokenizer.cls_token_id)\nprint('[SEP] id :', transformer_tokenizer.sep_token_id)\nprint('[PAD] id :', pad_idx)\ntest_one_batch = databunch.one_batch()[0]\nprint('Batch shape : ',test_one_batch.shape)\nprint(test_one_batch)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Creating a custom model"},{"metadata":{},"cell_type":"markdown","source":"As mentioned [here](https://github.com/huggingface/transformers#models-always-output-tuples), every model's forward method always outputs a ``tuple`` with various elements depending on the model and the configuration parameters. In our case, we are interested to access only to the logits.Â [Note here](https://huggingface.co/transformers/_modules/transformers/modeling_distilbert.html#DistilBertForSequenceClassification) in the `outputs` of the DistilBERT classifier, the logits are output in the first position.\n\nHere we access them by creating a custom model."},{"metadata":{"trusted":false},"cell_type":"code","source":"# defining our model architecture \nclass CustomTransformerModel(nn.Module):\n    def __init__(self, transformer_model: PreTrainedModel):\n        super(CustomTransformerModel,self).__init__()\n        self.transformer = transformer_model\n        \n    def forward(self, input_ids, attention_mask=None):\n        \n        #attention_mask = (input_ids!=1).type(input_ids.type()) # Test attention_mask for RoBERTa\n        \n        logits = self.transformer(input_ids,\n                                attention_mask = attention_mask)[0]   \n        return logits","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To make our transformers adapted to multiclass classification, before loading the pre-trained model, we need to precise the number of labels. To do so, you can modify the config instance or either modify like in [Keita Kurita's article](https://mlexplained.com/2019/05/13/a-tutorial-to-fine-tuning-bert-with-fast-ai/) (Section: *Initializing the Learner*) the ``num_labels`` argument.\n\nHere, we have two labels: 'ironic' and 'not ironic'"},{"metadata":{"trusted":false},"cell_type":"code","source":"config = config_class.from_pretrained(pretrained_model_name)\nconfig.num_labels = 2\nconfig.use_bfloat16 = use_fp16\nprint(config)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"transformer_model = model_class.from_pretrained(pretrained_model_name, config = config)\n# transformer_model = model_class.from_pretrained(pretrained_model_name, num_labels = 5)\n\ncustom_transformer_model = CustomTransformerModel(transformer_model = transformer_model)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### LearnerÂ : Custom Optimizer / CustomÂ Metric\nIn ``pytorch-transformers``, HuggingFace had implemented two specific optimizers â€Š-â€Š BertAdam and OpenAIAdam â€Š-â€Š that have been replaced by a single AdamW optimizer.\nThis optimizer matches Pytorch Adam optimizer Api, therefore, it becomes straightforward to integrate it within ``fastai``.\nIt is worth noting that for reproducing BertAdam specific behavior, you have to set ``correct_bias = False``.\n\nHere, because the dataset is imbalanced, we want to use weighted cross entropy. An empty model is created and then saved. Because we are going to use cross validation, we need to be able to return to a clean beginning. If not, we might end up having some items cross over between the training and validation sets, thus inflating our metrics. "},{"metadata":{"trusted":false},"cell_type":"code","source":"weights = [1., 3.]\nclass_weights=torch.FloatTensor(weights).cuda()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from fastai.callbacks import *\nfrom transformers import AdamW\n\nlearner = Learner(databunch, \n                  custom_transformer_model, \n                  loss_func = nn.CrossEntropyLoss(weight=class_weights),\n                  opt_func = lambda input: AdamW(input,correct_bias=False), \n                  metrics=[Precision(),Recall(),FBeta(beta=1)])\n\n# Show graph of learner stats and metrics after each epoch.\nlearner.callbacks.append(ShowGraph(learner))\n\n# Put learn in FP16 precision mode. --> Seems to not working\nif use_fp16: learner = learner.to_fp16()\nlearner.save('untrained')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training the model"},{"metadata":{},"cell_type":"markdown","source":"Although we can use fastai build-in features like **Slanted Triangular Learning Rates**, **Discriminate Learning Rate** and **gradually unfreezing the model**, it is unclear if this will help in training. Here, we unfreeze the whole model for training. Since there is some randomness involved, we will run the model multiple times and average the output."},{"metadata":{"trusted":false},"cell_type":"code","source":"# constants across test\nepochs = 2\nn_reps = 2\n\nfolds = [fold1, fold2, fold3, fold4, fold5]\nlearning_rates = 1e-6\nmoms = (0.825,0.725)\n\nmetrics = np.zeros([len(folds),n_reps,3]) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This loop creates a new databunch and new learner for each fold in the cross validation. The last metrics from each training cycle are retained. "},{"metadata":{"trusted":false},"cell_type":"code","source":"for reps in range(n_reps):\n    \n    for fold in range(0,len(folds)):\n        \n        # create a databunch with the current fold\n        databunch = (TextList.from_df(folds[fold], cols='comment_text', processor=transformer_processor)\n            .split_from_df(col='is_valid')\n            .label_from_df(cols= 'label')\n            .databunch(bs=bs, pad_first=pad_first, pad_idx=pad_idx))\n        learner = Learner(databunch, \n            custom_transformer_model, \n            loss_func = nn.CrossEntropyLoss(weight=class_weights),\n            opt_func = lambda input: AdamW(input,correct_bias=False), \n            metrics=[Precision(),Recall(),FBeta(beta=1)])\n        if use_fp16: learner = learner.to_fp16()\n    \n        # start with empty weights\n        learner.load('untrained')\n        learner.unfreeze()\n        #learner.freeze_to(unfreeze_layers)\n        # train on current parameters\n        learner.fit_one_cycle(epochs, max_lr=learning_rates, moms=moms)\n        metrics[fold,reps:] = learner.recorder.metrics[-1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Interpreting the results"},{"metadata":{},"cell_type":"markdown","source":"Let's see how the results compare to the ones presented in the paper. "},{"metadata":{"trusted":false},"cell_type":"code","source":"metrics.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"avg_per_fold = np.mean(metrics,axis=1);avg_per_fold","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def format_scores(avg_metrics):\n    def print_line(name,arr):\n        print(name,':',format(np.mean(arr), '.3f'), '(range ', np.min(arr), ' - ',np.max(arr))\n    \n    print_line('F1 score',avg_metrics[:,2])\n    print_line('recall',avg_metrics[:,1])\n    print_line('precision',avg_metrics[:,0])\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"format_scores(avg_per_fold)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Scores presented in the paper:\n- average [F1 score](https://en.wikipedia.org/wiki/F1_score): 0.383 (range 0.330 - 0.412)\n- average [recall](https://en.wikipedia.org/wiki/Precision_and_recall): 0.496 (range 0.446 - 0.548)\n- average [precision](https://en.wikipedia.org/wiki/Precision_and_recall): 0.315 (range 0.261 - 0.380)"},{"metadata":{},"cell_type":"markdown","source":"Overall, the tranformers perform comaribly to the SVM. What I notice on the transformers is that occasionally they perform very badly at first. The minimum values can be very low on some folds, thus bringing the average down. "},{"metadata":{},"cell_type":"markdown","source":"## References\n* Hugging Face, Transformers GitHub (Nov 2019), [https://github.com/huggingface/transformers](https://github.com/huggingface/transformers)\n* Fast.ai, Fastai documentation (Nov 2019), [https://docs.fast.ai/text.html](https://docs.fast.ai/text.html)\n* Jeremy Howard & Sebastian Ruder, Universal Language Model Fine-tuning for Text Classification (May 2018), [https://arxiv.org/abs/1801.06146](https://arxiv.org/abs/1801.06146)\n* Keita Kurita's articleÂ : [A Tutorial to Fine-Tuning BERT with Fast AI](https://mlexplained.com/2019/05/13/a-tutorial-to-fine-tuning-bert-with-fast-ai/)Â (May 2019)\n* Dev Sharma's articleÂ : [Using RoBERTa with Fastai for NLP](https://medium.com/analytics-vidhya/using-roberta-with-fastai-for-nlp-7ed3fed21f6c) (Sep 2019)"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.5"},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false}},"nbformat":4,"nbformat_minor":1}