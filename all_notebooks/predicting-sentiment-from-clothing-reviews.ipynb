{"cells":[{"metadata":{"_uuid":"5636cf459775f4c41e79a2117e7049d74f39488d"},"cell_type":"markdown","source":"# <span id=\"2\"></span> Importing Modules and Reading the Dataset\n#### [Return Contents](#0)\n<hr/>","execution_count":null},{"metadata":{"_uuid":"c1436e92c2e44d5142e0947321d7c4b833245ad5"},"cell_type":"markdown","source":"In order to make some analysis, we need to set our environment up. To do this, I firstly imported some modules and read the data. The below output is the head of the data but if you want to see more details, you might try removing ***#*** signs in front of the ***df.describe()*** and ***df.info()***.  \n\nFurther, I decided that I do not need some columns and defined a new dataset which just has the columns I used. ","execution_count":null},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport datetime as dt\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\nimport seaborn as sns\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import confusion_matrix\nimport sklearn.metrics as mt\nfrom plotly import tools\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')\n \ndf1 = pd.read_csv('../input/Womens Clothing E-Commerce Reviews.csv')\ndf = df1[['Review Text','Rating','Class Name','Age']]\n#df1.info()\n#df1.describe()\ndf1.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b3e18f15da8f15842316c82e248a8d8752e5fcba"},"cell_type":"markdown","source":"# <span id=\"3\"></span> Adding the Word Counts to the Dataframe and Finding out How Many Times Some Words Were Used \n#### [Return Contents](#0)\n<hr/>","execution_count":null},{"metadata":{"_uuid":"9f033a6c9940ef4e93c1e621db00e50a70627817"},"cell_type":"markdown","source":"Adding the word counts to a dataframe is a very good practice because we might use these counts to reach some useful information. To do this, I defined the function ***wordcounts***.","execution_count":null},{"metadata":{"trusted":true,"_uuid":"1cda9ceace361f152f30fa199f021528d0f9f2b7","_kg_hide-output":false,"_kg_hide-input":true},"cell_type":"code","source":"# fill NA values by space\ndf['Review Text'] = df['Review Text'].fillna('')\n\n# CountVectorizer() converts a collection \n# of text documents to a matrix of token counts\nvectorizer = CountVectorizer()\n# assign a shorter name for the analyze\n# which tokenizes the string\nanalyzer = vectorizer.build_analyzer()\n\ndef wordcounts(s):\n    c = {}\n    # tokenize the string and continue, if it is not empty\n    if analyzer(s):\n        d = {}\n        # find counts of the vocabularies and transform to array \n        w = vectorizer.fit_transform([s]).toarray()\n        # vocabulary and index (index of w)\n        vc = vectorizer.vocabulary_\n        # items() transforms the dictionary's (word, index) tuple pairs\n        for k,v in vc.items():\n            d[v]=k # d -> index:word \n        for index,i in enumerate(w[0]):\n            c[d[index]] = i # c -> word:count\n    return  c\n\n# add new column to the dataframe\ndf['Word Counts'] = df['Review Text'].apply(wordcounts)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"984a74c85fce1587d0116d6a75610d57a185421c"},"cell_type":"markdown","source":"# <span id=\"4\"></span>  Demonstrating the Densities of Class Names, Some Selected Words and All Words in the Reviews By Using WordCloud \n#### [Return Contents](#0)\n<hr/>","execution_count":null},{"metadata":{"_uuid":"d693c4ce5c6d86a60387de5e034f4991db3d2386"},"cell_type":"markdown","source":"In this section, I demonstrated the word densities which can be very informative. First, I selected some words which show the customer sentiments like love, hate, fantastic or regret. Second, since we do not know the product names, I decided to check the product class names. By doing this, we may at least learn the most prefered classes. Further, I thought that looking at the densities of all words in the reviews might be interesting. Lastly, I used the *WordCloud* module and printed the first five lines of the tables which shows the word counts for the selected words and the class names. \n\nIt can be observed from the below figures and tables that positive words as love, great, super were used more. When we look at the classes, customers mostly prefered dress, knits and blouses. We may also see that dress and love are in the freequently used words within all reviews. ","execution_count":null},{"metadata":{"trusted":true,"_uuid":"ddd45a2082a6a6a70642ec16fca3f4b355da1285","_kg_hide-input":true},"cell_type":"code","source":"# selecting some words to examine detailed \nselectedwords = ['awesome','great','fantastic','extraordinary','amazing','super',\n                 'magnificent','stunning','impressive','wonderful','breathtaking',\n                 'love','content','pleased','happy','glad','satisfied','lucky',\n                 'shocking','cheerful','wow','sad','unhappy','horrible','regret',\n                 'bad','terrible','annoyed','disappointed','upset','awful','hate']\n\ndef selectedcount(dic,word):\n    if word in dic:\n        return dic[word]\n    else:\n        return 0\n    \ndfwc = df.copy()  \nfor word in selectedwords:\n    dfwc[word] = dfwc['Word Counts'].apply(selectedcount,args=(word,))\n    \nword_sum = dfwc[selectedwords].sum()\nprint('Selected Words')\nprint(word_sum.sort_values(ascending=False).iloc[:5])\n\nprint('\\nClass Names')\nprint(df['Class Name'].fillna(\"Empty\").value_counts().iloc[:5])\n\nfig, ax = plt.subplots(1,2,figsize=(20,10))\nwc0 = WordCloud(background_color='white',\n                      width=450,\n                      height=400 ).generate_from_frequencies(word_sum)\n\ncn = df['Class Name'].fillna(\" \").value_counts()\nwc1 = WordCloud(background_color='white',\n                      width=450,\n                      height=400 \n                     ).generate_from_frequencies(cn)\n\nax[0].imshow(wc0)\nax[0].set_title('Selected Words\\n',size=25)\nax[0].axis('off')\n\nax[1].imshow(wc1)\nax[1].set_title('Class Names\\n',size=25)\nax[1].axis('off')\n\nrt = df['Review Text']\nplt.subplots(figsize=(18,6))\nwordcloud = WordCloud(background_color='white',\n                      width=900,\n                      height=300\n                     ).generate(\" \".join(rt))\nplt.imshow(wordcloud)\nplt.title('All Words in the Reviews\\n',size=25)\nplt.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a3f7f0b362d9171cce9eef2f32f1c6ca7d4a74f2"},"cell_type":"markdown","source":" # <span id=\"5\"></span> Viewing the Relation Between Rating, Class Name and Age  \n#### [Return Contents](#0)\n<hr/>","execution_count":null},{"metadata":{"_uuid":"07e834b7754e0b6c78cc557b3bc693bdac5cf775"},"cell_type":"markdown","source":"I thought that examining the relation between the rating, class names and age might be good because some age groups may always give low ratings or make negative reviews or prefer the products in the same class. To examine this relationship, I used the below dynamic charts. ","execution_count":null},{"metadata":{"trusted":true,"_uuid":"6f4a2b818be0f6364e6bec4cebf94ba13a869432","_kg_hide-input":true},"cell_type":"code","source":"df1=df['Rating'].value_counts().to_frame()\navgdf1 = df.groupby('Class Name').agg({'Rating': np.average})\navgdf2 = df.groupby('Class Name').agg({'Age': np.average})\navgdf3 = df.groupby('Rating').agg({'Age': np.average})\n\ntrace1 = go.Bar(\n    x=avgdf1.index,\n    y=round(avgdf1['Rating'],2),\n    marker=dict(\n        color=avgdf1['Rating'],\n        colorscale = 'RdBu')\n)\n\ntrace2 = go.Bar(\n    x=df1.index,\n    y=df1.Rating,\n    marker=dict(\n        color=df1['Rating'],\n        colorscale = 'RdBu')\n)\n\ntrace3 = go.Bar(\n    x=avgdf2.index,\n    y=round(avgdf2['Age'],2),\n    marker=dict(\n        color=avgdf2['Age'],\n        colorscale = 'RdBu')\n)\n\ntrace4 = go.Bar(\n    x=avgdf3.index,\n    y=round(avgdf3['Age'],2),\n    marker=dict(\n        color=avgdf3['Age'],\n        colorscale = 'Reds')\n)\n\nfig = tools.make_subplots(rows=2, cols=2, print_grid=False)\n\nfig.append_trace(trace1, 1, 1)\nfig.append_trace(trace2, 1, 2)\nfig.append_trace(trace3, 2, 1)\nfig.append_trace(trace4, 2, 2)\n\nfig['layout']['xaxis1'].update(title='Class')\nfig['layout']['yaxis1'].update(title='Average Rating')\nfig['layout']['xaxis2'].update(title='Rating')\nfig['layout']['yaxis2'].update(title='Count')\nfig['layout']['xaxis3'].update(title='Class')\nfig['layout']['yaxis3'].update(title='Average Age of the Reviewers')\nfig['layout']['xaxis4'].update(title='Rating')\nfig['layout']['yaxis4'].update(title='Average Age of the Reviewers')\n\nfig['layout'].update(height=800, width=900,showlegend=False)\nfig.update_layout({'plot_bgcolor':'rgba(0,0,0,0)',\n                   'paper_bgcolor':'rgba(0,0,0,0)'})\n#fig['layout'].update(plot_bgcolor='rgba(0,0,0,0)')\n#fig['layout'].update(paper_bgcolor='rgba(0,0,0,0)')\npy.iplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c688babf6822d1b322ae38d34264d2adce7b5e97"},"cell_type":"markdown","source":"It seems that most of the ratings are positive and the average rating between the classes looks close. On the other hand, when we look at ages, average age does not change significantly according to the rating. Also, average age changes slightly between class names except casual bottoms. We can disregard casual bottoms because the below chart shows that there are just two reviews and making an inference will not be right.","execution_count":null},{"metadata":{"trusted":true,"_uuid":"fbf76a6f3e33e712706170704857fd44634571fb","_kg_hide-input":true},"cell_type":"code","source":"cv = df['Class Name'].value_counts()\n\ntrace = go.Scatter3d( x = avgdf1.index,\n                      y = avgdf1['Rating'],\n                      z = cv[avgdf1.index],\n                      mode = 'markers',\n                      marker = dict(size=10,color=avgdf1['Rating']),\n                      hoverinfo =\"text\",\n                      text=\"Class: \"+avgdf1.index+\" \\ Average Rating: \"+avgdf1['Rating'].map(' {:,.2f}'.format).apply(str)+\" \\ Number of Reviewers: \"+cv[avgdf1.index].apply(str)\n                      )\n\ndata = [trace]\nlayout = go.Layout(title=\"Average Rating & Class & Number of Reviewers\",\n                   scene = dict(\n                    xaxis = dict(title='Class'),\n                    yaxis = dict(title='Average Rating'),\n                    zaxis = dict(title='Number of Sales'),),\n                   margin = dict(l=30, r=30, b=30, t=30))\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig)\nplt.savefig('3D_Scatter.png')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"96797ed17ba3cd3c6aa41bdef38cb10deb409341"},"cell_type":"markdown","source":" # <span id=\"6\"></span> Building a Sentiment Classifier\n#### [Return Contents](#0)\n<hr/>","execution_count":null},{"metadata":{"_uuid":"a0ca41ad24d601e6eed1dd42dc592ac927edc7a7"},"cell_type":"markdown","source":"Since we do not have a column which shows the sentiment as positive or negative in the dataset, I defined a new sentiment column. To do this, I assumed the reviews which has **4 or higher ** rating as **positive (True in the new dataframe)** and **2 or lower** rating as **negative (False in the new dataframe)**. Also, I did not include the lines that has **neutral** ratings which are equal to **3**. Following that, I splitted the data as training and test sets.","execution_count":null},{"metadata":{"trusted":true,"_uuid":"ef4e5b9dac84bbbb9bf3a08b5e63e75a70514dfe","_kg_hide-input":true},"cell_type":"code","source":"# Rating of 4 or higher -> positive, while the ones with \n# Rating of 2 or lower -> negative \n# Rating of 3 -> neutral\ndf = df[df['Rating'] != 3]\ndf['Sentiment'] = df['Rating'] >=4\ndf.head()\n\n# split data\ntrain_data,test_data = train_test_split(df,train_size=0.8,random_state=0)\n# select the columns and \n# prepare data for the models \nX_train = vectorizer.fit_transform(train_data['Review Text'])\ny_train = train_data['Sentiment']\nX_test = vectorizer.transform(test_data['Review Text'])\ny_test = test_data['Sentiment']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6123ca5c04391db3023f31c3363474db1c8afee5"},"cell_type":"markdown","source":"Then, I fitted the models one by one. Since, some of them take too much time, running each of them in different cells is a better choice.   ","execution_count":null},{"metadata":{"_uuid":"dd935dbe4f4fdbd0a721695bffb72c2778b16690"},"cell_type":"markdown","source":"## <span id=\"7\"></span> Logistic Regression","execution_count":null},{"metadata":{"trusted":true,"_uuid":"b78e37c89e02fd1dea22b87509dc871f5ad973c6"},"cell_type":"code","source":"start=dt.datetime.now()\nlr = LogisticRegression()\nlr.fit(X_train,y_train)\nprint('Elapsed time: ',str(dt.datetime.now()-start))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start=dt.datetime.now()\ngb = GradientBoostingClassifier()\ngb.fit(X_train,y_train)\nprint('Elapsed time: ',str(dt.datetime.now()-start))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ad4bd4a855ddc670eedf280b3a81efa9a57d470e"},"cell_type":"markdown","source":"## <span id=\"8\"></span> Naive Bayes","execution_count":null},{"metadata":{"trusted":true,"_uuid":"4715c54fec338f8017b2f5597490c9a16ce4f101"},"cell_type":"code","source":"start=dt.datetime.now()\nnb = MultinomialNB()\nnb.fit(X_train,y_train)\nprint('Elapsed time: ',str(dt.datetime.now()-start))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"daaf5d10642b2ba0e89846091f7a6b4a1de4252b"},"cell_type":"markdown","source":"## <span id=\"9\"></span> Support Vector Machine (SVM)","execution_count":null},{"metadata":{"trusted":true,"_uuid":"ab7e1fcbe45995e034124b97ae5d2674554abd66"},"cell_type":"code","source":"start=dt.datetime.now()\nsvm = SVC()\nsvm.fit(X_train,y_train)\nprint('Elapsed time: ',str(dt.datetime.now()-start))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"048d88c13016a4f8a39d1427c3eeaf01d8a4f2b9"},"cell_type":"markdown","source":"## <span id=\"10\"></span> Neural Network","execution_count":null},{"metadata":{"trusted":true,"_uuid":"3dfb24752703bcadb607fe5bd43026941b41a65a"},"cell_type":"code","source":"start=dt.datetime.now()\nnn = MLPClassifier()\nnn.fit(X_train,y_train)\nprint('Elapsed time: ',str(dt.datetime.now()-start))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e37a1b005158073c90e0813313a15428aefb0f02"},"cell_type":"markdown","source":"# <span id=\"11\"></span> Evaluating Models\n#### [Return Contents](#0)\n<hr/>","execution_count":null},{"metadata":{"_uuid":"c5e0df1e9c4fc2e9b6c00c57d5b8be355be89b7e"},"cell_type":"markdown","source":"## <span id=\"12\"></span> Adding Results to the Dataframe","execution_count":null},{"metadata":{"_uuid":"7550e005914243acf6e7928fe76e3e67cae3ae64"},"cell_type":"markdown","source":"At first, I added the prediction results to my training data. However, if you want to observe the prediction probabilies, you might use the commented out code.","execution_count":null},{"metadata":{"trusted":true,"_uuid":"268d92918c01136e0222364060abf37d91422849","_kg_hide-input":true},"cell_type":"code","source":"# define a dataframe for the prediction probablities of the models\n#df1 = train_data.copy()\n#df1['Logistic Regression'] = lr.predict_proba(X_train)[:,1]\n#df1['Naive Bayes'] = nb.predict_proba(X_train)[:,1]\n#df1['SVM'] = svm.decision_function(X_train)\n#df1['Neural Network'] = nn.predict_proba(X_train)[:,1]\n#df1=df1.round(2)\n#df1.head()\n\n# define a dataframe for the predictions\ndf2 = train_data.copy()\ndf2['Logistic Regression'] = lr.predict(X_train)\ndf2['Naive Bayes'] = nb.predict(X_train)\ndf2['SVM'] = svm.predict(X_train)\ndf2['Neural Network'] = nn.predict(X_train)\ndf2['GradientBoostingClassifier']=gb.predict(X_train)\n\ndf2.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0ca48e96a1af34602deeed24ee1df8c09b5f0012"},"cell_type":"markdown","source":"## <span id=\"13\"></span> ROC Curves and AUC","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"I started my evaluation with ROC curve and AUC. As you may observe below, results look pretty good but it does not give much insight. To decide which model is the best we must also examine other evaluation metrics.","execution_count":null},{"metadata":{"trusted":true,"_uuid":"967355c906e983f36af66b7cc3b6fb43f2447e7c","_kg_hide-input":true},"cell_type":"code","source":"pred_lr = lr.predict_proba(X_test)[:,1]\nfpr_lr,tpr_lr,_ = roc_curve(y_test,pred_lr)\nroc_auc_lr = auc(fpr_lr,tpr_lr)\n\npred_nb = nb.predict_proba(X_test)[:,1]\nfpr_nb,tpr_nb,_ = roc_curve(y_test.values,pred_nb)\nroc_auc_nb = auc(fpr_nb,tpr_nb)\n\npred_gb = gb.predict_proba(X_test)[:,1]\nfpr_gb,tpr_gb,_ = roc_curve(y_test.values,pred_gb)\nroc_auc_gb = auc(fpr_gb,tpr_gb)\n\n\n\npred_nn = nn.predict_proba(X_test)[:,1]\nfpr_nn,tpr_nn,_ = roc_curve(y_test.values,pred_nn)\nroc_auc_nn = auc(fpr_nn,tpr_nn)\n\npred_gb = gb.predict_proba(X_test)[:,1]\nfpr_gb,tpr_gb,_ = roc_curve(y_test.values,pred_gb)\nroc_auc_gb = auc(fpr_gb,tpr_gb)\n\n\nf, axes = plt.subplots(2, 2,figsize=(15,10))\naxes[0,0].plot(fpr_lr, tpr_lr, color='darkred', lw=2, label='ROC curve (area = {:0.2f})'.format(roc_auc_lr))\naxes[0,0].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\naxes[0,0].set(xlim=[-0.01, 1.0], ylim=[-0.01, 1.05])\naxes[0,0].set(xlabel ='False Positive Rate', ylabel = 'True Positive Rate', title = 'Logistic Regression')\naxes[0,0].legend(loc='lower right', fontsize=14)\n\naxes[0,1].plot(fpr_nb, tpr_nb, color='darkred', lw=2, label='ROC curve (area = {:0.2f})'.format(roc_auc_nb))\naxes[0,1].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\naxes[0,1].set(xlim=[-0.01, 1.0], ylim=[-0.01, 1.05])\naxes[0,1].set(xlabel ='False Positive Rate', ylabel = 'True Positive Rate', title = 'Naive Bayes')\naxes[0,1].legend(loc='lower right', fontsize=14)\n\naxes[1,0].plot(fpr_svm, tpr_svm, color='darkred', lw=2, label='ROC curve (area = {:0.2f})'.format(roc_auc_svm))\naxes[1,0].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\naxes[1,0].set(xlim=[-0.01, 1.0], ylim=[-0.01, 1.05])\naxes[1,0].set(xlabel ='False Positive Rate', ylabel = 'True Positive Rate', title = 'Support Vector Machine')\naxes[1,0].legend(loc='lower right', fontsize=14)\n\naxes[1,0].plot(fpr_gb, tpr_gb, color='darkred', lw=2, label='ROC curve (area = {:0.2f})'.format(roc_auc_svm))\naxes[1,0].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\naxes[1,0].set(xlim=[-0.01, 1.0], ylim=[-0.01, 1.05])\naxes[1,0].set(xlabel ='False Positive Rate', ylabel = 'True Positive Rate', title = 'GradientBoostingClassifier')\naxes[1,0].legend(loc='lower right', fontsize=14)\n\naxes[1,1].plot(fpr_nn, tpr_nn, color='darkred', lw=2, label='ROC curve (area = {:0.2f})'.format(roc_auc_nn))\naxes[1,1].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\naxes[1,1].set(xlim=[-0.01, 1.0], ylim=[-0.01, 1.05])\naxes[1,1].set(xlabel ='False Positive Rate', ylabel = 'True Positive Rate', title = 'Neural Network')\naxes[1,1].legend(loc='lower right', fontsize=1);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5ea828d90388d25949f285634db0546ef329d869"},"cell_type":"markdown","source":"## <span id=\"14\"></span> Confusion Matrices","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"To reach more information, I also used confusion matrices. It can be seen that SVM does not give healthy results although it has high ROC values.","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"b8eb4577a0c1a15ac00b554eebbcbbbd2dbf3ee8"},"cell_type":"code","source":"# preparation for the confusion matrix\nlr_cm=confusion_matrix(y_test.values, lr.predict(X_test))\nnb_cm=confusion_matrix(y_test.values, nb.predict(X_test))\nsvm_cm=confusion_matrix(y_test.values, svm.predict(X_test))\nnn_cm=confusion_matrix(y_test.values, nn.predict(X_test))\ngb_cm=confusion_matrix(y_test.values, gb.predict(X_test))\n\nplt.figure(figsize=(15,12))\nplt.suptitle(\"Confusion Matrices\",fontsize=24)\n\nplt.subplot(2,2,1)\nplt.title(\"Logistic Regression\")\nsns.heatmap(lr_cm, annot = True, cmap=\"Greens\",cbar=False);\n\nplt.subplot(2,2,2)\nplt.title(\"Naive Bayes\")\nsns.heatmap(nb_cm, annot = True, cmap=\"Greens\",cbar=False);\n\nplt.subplot(2,2,3)\nplt.title(\"Support Vector Machine (SVM)\")\nsns.heatmap(svm_cm, annot = True, cmap=\"Greens\",cbar=False);\n\nplt.subplot(2,2,1)\nplt.title(\"GradientBoostingClassifier\")\nsns.heatmap(gb_cm, annot = True, cmap=\"Greens\",cbar=False);\n\nplt.subplot(2,2,4)\nplt.title(\"Neural Network\")\nsns.heatmap(nn_cm, annot = True, cmap=\"Greens\",cbar=False);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <span id=\"15\"></span> Precision - Recall - F1-Score","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"print(\"Logistic Regression\")\nprint(mt.classification_report(y_test, lr.predict(X_test)))\nprint(\"\\n Naive Bayes\")\nprint(mt.classification_report(y_test, nb.predict(X_test)))\nprint(\"\\n Support Vector Machine (SVM)\")\nprint(mt.classification_report(y_test, svm.predict(X_test)))\nprint(\"\\n Neural Network\")\nprint(mt.classification_report(y_test, nn.predict(X_test)))\nprint(\" GradientBoostingClassifier\")\nprint(mt.classification_report(y_test, gb.predict(X_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ef38d9393b73d3a1bf6f14b4094f54c634aaabb3"},"cell_type":"markdown","source":"# <span id=\"16\"></span> Conclusion\n#### [Return Contents](#0)\n<hr/>","execution_count":null},{"metadata":{"_uuid":"527bf5e24b62b2993574a1ab64e2fa34c08bbd49"},"cell_type":"markdown","source":"When we look at the results of the all evaluation metrics in the *evaluating models section*, **Naive Bayes** and **Logistic Regression** gives the best results for our analysis. Thus, both of them are very effective at predicting sentiment. On the other hand, it seems that **Naive Bayes** takes less time and when we have a bigger dataset, this difference might increase and be an important advantage .\n\n","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}