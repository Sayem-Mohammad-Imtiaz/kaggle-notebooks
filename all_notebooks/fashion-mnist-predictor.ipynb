{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\n\nprint(\"Current version of tensorflow:\", tf.__version__)\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Introduction\n\nIn this notebook I will be using the fashion mnist datasets to predict the classes of the images\n\nThis is a dataset of **60,000** $28 x 28$ grayscale images of 10 fashion categories, along with a test set of 10,000 images. This dataset can be used as a drop-in replacement for MNIST. The class labels are:\n\n|Label|\tDescription|\n|:---:|:---:|\n|0|\tT-shirt/top|\n|1|\tTrouser|\n|2|\tPullover|\n|3|\tDress|\n|4|\tCoat|\n|5|\tSandal|\n|6|Shirt|\n|7|\tSneaker|\n|8|\tBag|\n|9|\tAnkle boot|\n\nIn this I will be making MLP model and CNN Model. Challenge is CNN model will have very few parameters than MLP model and yet it would be more accurate than MLP model"},{"metadata":{},"cell_type":"markdown","source":"### Exploring the Dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"klass = [\"T-shirt/top\", \"Trousers\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle Boot\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = pd.read_csv(\"/kaggle/input/minst-fashion-dataset/fashion-mnist_train.csv\")\ndf_test = pd.read_csv(\"/kaggle/input/minst-fashion-dataset/fashion-mnist_test.csv\")\n\ndf_train.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"images = df_train.sample(25)\n\nidx = 0\n\nplt.figure(figsize=(15,15))\nfor image in images.iterrows():\n    ax = plt.subplot(5, 5, idx + 1)\n    im = image[1].drop([\"label\"]).to_numpy().reshape(28, 28)\n    lbl = image[1][\"label\"]\n    \n    ax.imshow(im, cmap=\"gray\")\n    ax.set_yticks([])\n    ax.set_xticks([])\n    ax.set_title(klass[lbl])\n    idx += 1\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Creating MLP Model\n\nThe following layers would be used\n\n1. Flatten\n2. 2 x Dense\n3. BatchNormalization\n4. 2 x Dense\n5. Dropout\n6. 1 x Dense"},{"metadata":{},"cell_type":"markdown","source":"**Data preprocessing**"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = df_train.drop([\"label\"], axis=1).to_numpy().reshape(60000, 28, 28)\nY_train = df_train[\"label\"].to_numpy()\n\nX_test = df_test.drop([\"label\"], axis=1).to_numpy().reshape(10000, 28, 28)\nY_test = df_test[\"label\"].to_numpy()\n\nprint(X_train.shape)\nprint(Y_train.shape)\nprint(X_test.shape)\nprint(Y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mlp_model = tf.keras.models.Sequential([\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(512, activation=\"relu\"),\n    tf.keras.layers.Dense(1024, activation=\"relu\"),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dense(1024, activation=\"relu\"),\n    tf.keras.layers.Dense(512, activation=\"relu\"),\n    tf.keras.layers.Dropout(rate=0.3),\n    tf.keras.layers.Dense(10, activation=\"softmax\")\n], name=\"fashion_mnist_mlp_model\")\nmlp_model.build((None, 28, 28))\nmlp_model.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Compiling the model**"},{"metadata":{"trusted":true},"cell_type":"code","source":"mlp_model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"acc\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally training the mlp model"},{"metadata":{"trusted":true},"cell_type":"code","source":"mlp_history = mlp_model.fit(X_train, Y_train, epochs=25, batch_size=32, validation_split=0.3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's visualize the training of mlp model"},{"metadata":{"trusted":true},"cell_type":"code","source":"loss = mlp_history.history[\"loss\"]\nacc = mlp_history.history[\"acc\"]\n\nval_loss = mlp_history.history[\"val_loss\"]\nval_acc = mlp_history.history[\"val_acc\"]\n\nfig, axes = plt.subplots(1, 2, figsize=(35, 8))\n\naxes[0].plot(loss, label=\"Loss\")\naxes[0].plot(acc, label=\"Accuracy\")\naxes[0].set_title(\"Training Metrics\")\naxes[0].legend()\naxes[0].set_xlabel(\"Epochs\")\n\naxes[1].plot(val_loss, label=\"Loss\")\naxes[1].plot(acc, label=\"Accuracy\")\naxes[1].set_title(\"Testing Metrics\")\naxes[1].legend()\naxes[1].set_xlabel(\"Epochs\")\naxes[1].set_ylim([0, 1])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's evaluate the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"_, acc = mlp_model.evaluate(X_test, Y_test)\nprint(\"Accuracy of MLP Model: %.2f\" % (acc*100) + \"%\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Creating CNN Model\n\nThe layers for this model will go like this\n\n1. 1 x Conv2D\n2. 1 x MaxPool2D\n3. 1 x Conv2D\n2. 1 x MaxPool2D\n3. 1 x Conv2D\n4. 1 x MaxPool2D\n5. 1 x Flatten\n6. 2 x Dense\n7. 1 x Dropout\n8. 1 x Dense"},{"metadata":{"trusted":true},"cell_type":"code","source":"cnn_model = tf.keras.models.Sequential([\n    tf.keras.layers.Conv2D(16, (3, 3), activation=\"relu\"),\n    tf.keras.layers.MaxPool2D(),\n    tf.keras.layers.Conv2D(32, (3, 3), activation=\"relu\"),\n    tf.keras.layers.MaxPool2D(),\n    tf.keras.layers.Conv2D(64, (3, 3), activation=\"relu\"),\n    \n    tf.keras.layers.Flatten(),\n    \n    tf.keras.layers.Dense(256, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n    tf.keras.layers.Dense(512, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n    tf.keras.layers.Dropout(rate=0.3),\n    tf.keras.layers.Dense(10, activation=\"softmax\")\n], name=\"fashion_mnist_cnn_model\")\ncnn_model.build((None, 28, 28, 1))\ncnn_model.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Huh, what gonna happen with only 100 thousand params. \n\nLet's see"},{"metadata":{},"cell_type":"markdown","source":"**Data Preprocessing**"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = X_train[..., np.newaxis]\nX_test = X_test[..., np.newaxis]\nprint(X_train.shape)\nprint(X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cnn_model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"acc\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cnn_history = cnn_model.fit(X_train, Y_train, epochs=25, batch_size=32, validation_split=0.3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Evaluating the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"_, acc = cnn_model.evaluate(X_test, Y_test)\nprint(\"Accuracy of CNN Model: %.2f\" % (acc*100) + \"%\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Visualizing the training of the model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loss = cnn_history.history[\"loss\"]\nacc = cnn_history.history[\"acc\"]\n\nval_loss = cnn_history.history[\"val_loss\"]\nval_acc = cnn_history.history[\"val_acc\"]\n\nfig, axes = plt.subplots(1, 2, figsize=(35, 8))\n\naxes[0].plot(loss, label=\"Loss\")\naxes[0].plot(acc, label=\"Accuracy\")\naxes[0].set_title(\"Training Metrics\")\naxes[0].legend()\naxes[0].set_xlabel(\"Epochs\")\n\naxes[1].plot(val_loss, label=\"Loss\")\naxes[1].plot(acc, label=\"Accuracy\")\naxes[1].set_title(\"Testing Metrics\")\naxes[1].legend()\naxes[1].set_xlabel(\"Epochs\")\naxes[1].set_ylim([0, 1])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Plotting the accuracy of MLP Model vs CNN Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"cnn_train = cnn_history.history[\"acc\"]\nmlp_train = mlp_history.history[\"acc\"]\n\ncnn_val = cnn_history.history[\"val_acc\"]\nmlp_val = mlp_history.history[\"val_acc\"]\n\nfig, axes = plt.subplots(1, 2, figsize=(35, 10))\n\naxes[0].plot(cnn_train, label=\"CNN Model\")\naxes[0].plot(mlp_train, label=\"MLP Model\")\naxes[0].set_title(\"Training Accuracy\")\naxes[0].legend()\naxes[0].set_xlabel(\"Epochs\")\n\naxes[1].plot(cnn_val, label=\"CNN Model\")\naxes[1].plot(mlp_val, label=\"MLP Model\")\naxes[1].set_title(\"Validation Accuracy\")\naxes[1].legend()\naxes[1].set_xlabel(\"Epochs\")\naxes[1].set_ylim([0, 1])\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}