{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Problem Statement:**\n\n* A stroke occurs when the blood supply to part of your brain is interrupted or reduced, preventing brain tissue from getting oxygen and nutrients. Brain cells begin to die in minutes.\n\n* A stroke is a medical emergency, and prompt treatment is crucial. Early action can reduce brain damage and other complications.\n\n**Goal:**\n\n* To create a prediction system to predict the stroke in its early stages.\n\n**Approach:**\n\n* A mixture of SVM, XGBOOST And MLPCLASSIFIER is used to archieve maximum accuracy.","metadata":{}},{"cell_type":"markdown","source":"**Import Neccessary Files:**","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nfrom sklearn.preprocessing import StandardScaler\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import roc_auc_score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Read Data:**","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv(\"../input/stroke-prediction-dataset/healthcare-dataset-stroke-data.csv\")","metadata":{"id":"4S_IRJvqR5wv","outputId":"e6b67ec5-8f1f-4744-f9eb-bfb4ef5c544d","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Analysing Data Read From CSV File:**","metadata":{}},{"cell_type":"code","source":"data.dtypes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Data PreProcessing:**","metadata":{}},{"cell_type":"markdown","source":"1. As we can see in the above list of datatypes, columns named gender, ever_married, work_type, Residence_type, smoking_status all are of object data type which need to be converted to numerical values before providing them for training the model. So here we mapped them to numerical values such as 1 or 0.  ","metadata":{}},{"cell_type":"code","source":"data[\"Residence_type\"] = data[\"Residence_type\"].apply(lambda x: 1 if x == \"Urban\" else 0)\ndata[\"ever_married\"] = data[\"ever_married\"].apply(lambda x: 1 if x == \"Yes\" else 0)\ndata[\"gender\"] = data[\"gender\"].apply(lambda x: 1 if x == \"Male\" else 0)\ndata = pd.get_dummies(data=data, columns=['smoking_status', 'work_type'])","metadata":{"id":"q4OQHPWWSfTb","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"2. Before providing data for modelling we need to ensure that our dataset does not contain any null values. But as we can see above \"bmi\"column has null values which we have filled using mean of the column. ","metadata":{}},{"cell_type":"code","source":"data['bmi'] = data['bmi'].fillna(data['bmi'].mean())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"3. So the last step of data preprocessing is to make sure that the data does not contain any kind of imbalance. Here, \"Standard Scalar\" is used to remove imbalance in avg_glucose_level, bmi and age columns. ","metadata":{}},{"cell_type":"code","source":"std = StandardScaler()\ncolumns = ['avg_glucose_level', 'bmi', 'age']\ndata[columns] = std.fit_transform(data[['avg_glucose_level', 'bmi', 'age']])","metadata":{"id":"YTJ_wQnOSmPH","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Exploring \"Stroke\" column which shows high level of imbalance which will be dealt with later.**","metadata":{}},{"cell_type":"code","source":"print(\"Data shape : \", data.shape)\nprint(\"stroke Data : \", sum(data.stroke == 1))\nprint(\"stroke Data : \", sum(data.stroke == 0))","metadata":{"id":"L744GDvwSpnf","outputId":"8d726da1-6eee-4eca-b533-65294b35d963","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Here, column \"id\" does not need to be a part of model training so it is removed.**","metadata":{}},{"cell_type":"code","source":"data.drop(columns='id', axis=1, inplace=True)","metadata":{"id":"KaHmHHHOSvXz","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Dataset is divided into Target And Features. Here X contains the features and y contains the target.**","metadata":{}},{"cell_type":"code","source":"X = data[['gender', 'age', 'hypertension', 'heart_disease', 'ever_married',\n          'Residence_type', 'avg_glucose_level', 'bmi',\n          'smoking_status_Unknown', 'smoking_status_formerly smoked',\n          'smoking_status_never smoked', 'smoking_status_smokes',\n          'work_type_Govt_job', 'work_type_Never_worked', 'work_type_Private',\n          'work_type_Self-employed', 'work_type_children']].values\ny = data['stroke'].values","metadata":{"id":"qpQowvWbSx3m","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**As it was explored above that there is a high imbalance so \"smote\" is used which is an oversampling technique where the synthetic samples are generated for the minority class.**","metadata":{}},{"cell_type":"code","source":"smote = SMOTE()\nx_smote, y_smote = smote.fit_resample(X, y)","metadata":{"id":"VjgNGjGyejGh","outputId":"96f82f71-271c-43f2-e838-7bf832259eaa","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Here the datset will be split for training and testing using train_test_split.**","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(x_smote, y_smote, test_size=0.1, stratify = y_smote)","metadata":{"id":"FolBxkqmS05B","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"stroke Data in Train : \", sum(y_train))\nprint(\"stroke Data in Test : \", sum(y_test))","metadata":{"id":"VQzMBtxLS5gr","outputId":"ad7e6bed-726b-4b2e-9ad2-db450e39ca5e","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Now the training and Testing data is provided to SVM and model is trained.**","metadata":{}},{"cell_type":"code","source":"print(\"\\n\\n\")\nprint(\"=\" * 80)\nprint(\"=\" * 15, \"SVM\", \"=\" * 15)\nprint(\"=\" * 80)\nsvc = SVC(random_state=0, kernel='linear', gamma='auto',C=1)\nsvc.fit(X_train, y_train)","metadata":{"id":"End0U6cuS-S9","outputId":"072f910d-18f6-4ce7-8a4d-84abb6808915","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"svc_score = svc.score(X_train, y_train)\nsvc_test = svc.score(X_test, y_test)","metadata":{"id":"YuW1WxmfTNzg","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = svc.predict(X_test)","metadata":{"id":"98iWUPpeTRLp","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('\\nTraining Score', svc_score)\nprint('Testing Score ', svc_test)\nprint(\"confusion_matrix  : \\n\", confusion_matrix(y_test, y_pred))\nprint(\"classification_report : \\n\", classification_report(y_test, y_pred, zero_division=True))","metadata":{"id":"uK1b9neCTVAw","outputId":"8f376793-65d4-41ed-fce1-fc0c79121f3d","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"auc = roc_auc_score(y_test,y_pred)\nauc","metadata":{"id":"CMBiMZeb_s3i","outputId":"35a8bf5a-d222-47c7-d82e-089ba8eaa288","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**So above displayed is the accuracy which we archieved using SVM which is still in minimal terms. In order to enhahnce it more \"GradientBoostingClassifier\" is used.**","metadata":{}},{"cell_type":"code","source":"print(\"\\n\\n\")\nprint(\"=\" * 80)\nprint(\"=\" * 35, \"XGBOOST\", \"=\" * 35)\nprint(\"=\" * 80)\nxgboost = GradientBoostingClassifier(random_state=0)\nxgboost.fit(X_train, y_train)","metadata":{"id":"k59loX7ucAPZ","outputId":"0f8916d2-e16e-4a61-b0f0-77cf77708427","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgboost_score = xgboost.score(X_train, y_train)\nxgboost_test = xgboost.score(X_test, y_test)\n","metadata":{"id":"gx1Lzjt_cWrm","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = xgboost.predict(X_test)","metadata":{"id":"JXG02BcNcX9-","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('\\nTraining Score', xgboost_score)\nprint('Testing Score ', xgboost_test)\nprint(\"confusion_matrix  : \\n\", confusion_matrix(y_test, y_pred))\nprint(\"classification_report : \\n\", classification_report(y_test, y_pred))","metadata":{"id":"tKmt1SlEcaSl","outputId":"d9228ecc-ca86-49a4-9fde-e8f88222c8b2","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"auc = roc_auc_score(y_test,y_pred)\nauc","metadata":{"id":"lQVV2REfcjzz","outputId":"d5b12561-07f9-4de2-e659-cd5ad173c793","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Here the accuracy is 91 which is significantly increasd by the use of GradientBoostingClassifier. But in order to obtain a more enhanced version \"MLPClassifier\" is used.**","metadata":{}},{"cell_type":"code","source":"print(\"\\n\\n\")\nprint(\"=\" * 80)\nprint(\"=\" * 35, \"MLPClassifier\", \"=\" * 35)\nprint(\"=\" * 80)\nmlp = MLPClassifier(hidden_layer_sizes=(300, 300, 300), max_iter=2000, alpha=0.00001,\n                    solver='adam', verbose=1, random_state=21)\nmlp.fit(X_train, y_train)\n\nmlp_score = mlp.score(X_train, y_train)\nmlp_test = mlp.score(X_test, y_test)\n\ny_pred = mlp.predict(X_test)\n\nprint('\\nTraining Score', mlp_score)\nprint('Testing Score ', mlp_test)\nprint(\"confusion_matrix  : \\n\", confusion_matrix(y_test, y_pred))\nprint(\"classification_report : \\n\", classification_report(y_test, y_pred))","metadata":{"id":"6El3RF1Efuds","outputId":"a7d530a9-f678-47cc-aca8-08f1d4921a9b","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"auc = roc_auc_score(y_test,y_pred)\nauc","metadata":{"id":"Kq3Z4xy0gFoy","outputId":"48347324-274a-40c1-934f-909cac82ee6e","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Here as we can see the last recorded accuracy is 96 which we obtained after merging different types of alorithms.**","metadata":{}},{"cell_type":"markdown","source":"Conclusion:\n\nHello Fellow Coders, this is my first submission and beginning to the AI/ML Journey. I recently started learning and got help of friends as well as submissions provided by other coders and just continued trial and errors to complete the task.\n\nSo if you find it helpful give it an Upvote and most importantly any suggestions, description of mistakes in my code and learnings are most welcomed. I would like to gain knowlege and move further in my journey of learning AI/ML. \n\nprint(\"Thank-You\")","metadata":{}}]}