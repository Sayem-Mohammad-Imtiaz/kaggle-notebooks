{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"#Importing libraries\nimport pandas as pd\nimport numpy as np\nimport nltk\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport string\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#importing dataset\nmessages = pd.read_csv(\"../input/spamraw.csv\")\nmessages.head()\nmessages.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3f156112fe313901a757b1c7960e8eaaa52088d0"},"cell_type":"code","source":"#Let's add a column of length to our data\nmessages['length'] = messages['text'].apply(len)\nmessages.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ffe2265c68ae6383264f4bbf17928549d96413bf"},"cell_type":"code","source":"#let's visualize\nmessages['length'].plot.hist(bins=50)\nmessages['length'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e622b199aaeb71f97b9a53e42d23c5268a1b0257"},"cell_type":"code","source":"#Creating histograms for Ham,Spam\nmessages.hist(column = 'length',by = 'type',bins = 60,figsize = (12,4))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ae2b8ce448086a7c22aea3042e0896133e2b1779"},"cell_type":"markdown","source":"**The real game starts from  here**","outputs":[],"execution_count":null},{"metadata":{"trusted":true,"_uuid":"d125231e104d92bcce8743862511229eda8eb5a3"},"cell_type":"code","source":"#Creating Function\nfrom nltk.corpus import stopwords\ndef text_process(text):\n    \"\"\"\n    1.Remove punc\n    2.Remove stop words\n    3.Return list of clean text words\n    \"\"\"\n    nonpunc = [char for char in text if char not in string.punctuation]\n    nonpunc = ''.join(nonpunc)\n    return[word for word in nonpunc.split() if word.lower() not in stopwords.words('english')]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cd7f84f462fa33392e39bc13b82c3890cfba31a7"},"cell_type":"code","source":"messages['text'].head(5).apply(text_process)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"86e94cd5f765b04df8e887056208cee417979773"},"cell_type":"markdown","source":"Upto here the process is called Tokenization, which means converting a normal text string into a list of tokens.\nTokens are the just the word we actually want i.e.,** \"Clean version of the words\"**\n\nNow, converting messages into Vectors step-by-step:\n\n1)Count how many times does a word occurs in each message(known as Term Frequency)\n\n2)Weight the counts, so that frequent tokens get lower weight(Inverse Document Frequency)\n\n3)Normalize the vectors to unit length, to abstract from original text length","outputs":[],"execution_count":null},{"metadata":{"trusted":true,"_uuid":"3e9097de7fb679b92366d9fe882af1874668f772"},"cell_type":"code","source":"#Step 1\nfrom sklearn.feature_extraction.text import  CountVectorizer\nbow_transformer = CountVectorizer(analyzer = text_process).fit(messages['text'])\nprint(len(bow_transformer.vocabulary_)) #This is going to print total no. of vocab words","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"83759e0a72beef82b7f662235d887380e24db6dc"},"cell_type":"code","source":"#lets check transform vocabulary for one message\n#bow is Bag of Words\nmess4 = messages['text'][3]\nprint(mess4)\nbow4 = bow_transformer.transform([mess4])\nprint(bow4)\nprint(bow4.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"158c28d77e0fb86f06ec27605f9d05c8dccf6b50"},"cell_type":"code","source":"messages_bow = bow_transformer.transform(messages['text'])\nprint('shape of sparse matrix :',messages_bow.shape)\n#Check amount of non-zero occurences\nmessages_bow.nnz","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"237f905ebb45d5f01d79a29eadc6f5a00c978613"},"cell_type":"code","source":"#step2 & step3\nfrom sklearn.feature_extraction.text import TfidfTransformer\ntfidf_transformer = TfidfTransformer().fit(messages_bow)\n#lets check \ntfidf4 = tfidf_transformer.transform(bow4)\nprint(tfidf4)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"673638e36e7578fff72ffa397cb41d305ae227f5"},"cell_type":"code","source":"#lets build our model\nfrom sklearn.model_selection import train_test_split\nmsg_train,msg_test,type_train,type_test = train_test_split(messages['text'],\n                                                             messages['type'],\n                                                             test_size = 0.3)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c67fd5ec0d96e88dc37f3cd428dbcba8aa205ada"},"cell_type":"markdown","source":"Whatever we have done so far was just to know the basic idea of Stopwords,punctuations etc.But scikat learn has a in-built function called pipeline to do all such stuff. So, am using that in-built function now.","outputs":[],"execution_count":null},{"metadata":{"trusted":true,"_uuid":"a9e7f4f3d113ac35744791a5f50de1ec29659918"},"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nfrom sklearn.naive_bayes import MultinomialNB\npipeline = Pipeline([\n    ('bow',CountVectorizer(analyzer = text_process)),\n     ('tfidf',TfidfTransformer()),\n      ('classifier',MultinomialNB())])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5511deb48205914e7d48df4848224ec3542ef6df"},"cell_type":"code","source":"#Now we can directly pass our message text and pipeline will do all our pre-processing for us\npipeline.fit(msg_train,type_train)\npredictions = pipeline.predict(msg_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3ea8f7979423bdaf516b9f8ab91addbf4986575c"},"cell_type":"code","source":"from sklearn.metrics import classification_report\nprint(classification_report(type_test,predictions))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a591d10758db7224608afd894ce8cc17cb89be28"},"cell_type":"markdown","source":"You can also use Random Forest by just changing in \"Pipeline\"","outputs":[],"execution_count":null},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"f7f72e0aa4ce0112d3c11693abb73ff7677375d2"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}