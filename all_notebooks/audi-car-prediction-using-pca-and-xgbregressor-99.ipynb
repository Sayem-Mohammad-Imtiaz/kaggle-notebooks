{"cells":[{"metadata":{},"cell_type":"markdown","source":"Hi! This is my first time modelling a dataset with kaggle. I've read several submissions here and they are very great since their model's scores are so high. In my first version, I used two regression analysis, PCR (Principal Component Regression) and PLS (Partial Least Square), then I compared between those two models. Since PCR was the best model, I then realized that what if I combined PCA (Principal Component Analysis) and XGBRegressor since it is the best model this far. And what surprises me is that the score is 0.99! I don't know whether it's true or not (amateur shock). So let's get started and please comment in down below.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Import Packages and Data","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"First, I'm going to import some packages I need and also the data.","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom scipy.signal import savgol_filter\nfrom statsmodels.sandbox.stats.runs import runstest_1samp\nfrom xgboost import XGBRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error, r2_score","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"for dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/used-car-dataset-ford-and-mercedes/audi.csv')\ndata","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Create Dummy Variables","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Since 'model', 'transmission', and 'fuel type' data types are categorical, I will change their values to numeric values. Here, I'll create dummy variables (binary encoding).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data_encode_dummy = pd.get_dummies(data,columns=['model', 'transmission','fuelType'], drop_first = True)\ndata_encode_dummy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"unique_model = data['model'].unique()\ntransmission_unique = data['transmission'].unique()\nfueltype_unique = data['fuelType'].unique()\n\nunique_model.sort()\ntransmission_unique.sort()\nfueltype_unique.sort()\n\nprint(data_encode_dummy.columns)\nprint(unique_model)\nprint(transmission_unique)\nprint(fueltype_unique)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The code above is used to see which is the reference category. We can see that model A1, transmission Automatic, and fuel type Diesel are the reference categories.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Asumptions Check","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Next, we're going to check the asumptions for linear regression. I'll use linear regression because it's the very basic regression.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Linearity","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X = data_encode_dummy.drop(columns = ['price'])\nY = data_encode_dummy['price']\n\ntitle = ['Price vs Year', \n         'Price vs Mileage', \n         'Price vs Tax', \n         'Price vs Mpg', \n         'Price vs Engine Size']\n\nfig,ax = plt.subplots(3,2,figsize=(20,20))\n\ni = 0\nfor rows in range(3):\n    for cols in range(2):\n        if rows == 2 and cols == 1:\n            fig.delaxes(ax[rows,cols])\n            break\n        ax[rows,cols].scatter(x = X[X.columns[i]], y = Y)\n        ax[rows,cols].set_title(title[i])\n        i = i+1\n        \nfig.subplots_adjust(hspace=0.5, wspace=0.5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see from the plots above that not all of the independent variables has a linear relationship with the dependent variable ('price' is the dependent variable and the rest are the independent variables). Therefore, we need to transform the data. Most of them have an exponentially relationship so we will only take the logarithm of the 'price' variable.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Transformation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data_encode_dummy_transform = data_encode_dummy.copy()\ndata_encode_dummy_transform['price'] = np.log(data_encode_dummy_transform['price'])\ndata_encode_dummy_transform","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = data_encode_dummy_transform.drop(columns = ['price'])\nY = data_encode_dummy_transform['price']\n\ntitle = ['Price vs Year', \n         'Price vs Mileage', \n         'Price vs Tax', \n         'Price vs Mpg', \n         'Price vs Engine Size']\n\nfig,ax = plt.subplots(3,2,figsize=(20,20))\n\ni = 0\nfor rows in range(3):\n    for cols in range(2):\n        if rows == 2 and cols == 1:\n            fig.delaxes(ax[rows,cols])\n            break\n        ax[rows,cols].scatter(x = X[X.columns[i]], y = Y)\n        ax[rows,cols].set_title(title[i])\n        i = i+1\n        \nfig.subplots_adjust(hspace=0.5, wspace=0.5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After we transform it, we plot it again to check whether it's already linear or not. We can see that it's already linear so we can continue to the next step.\n\nNote: I'm sorry for the long variables name. I just want to make it clear what those variables are hehe...","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Multicollinearity","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Next, we're going to check whether the independent variables are correlated each other. For an overview, we'll check the correlation matrix.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X = data_encode_dummy_transform[['year','mileage','tax','mpg','engineSize']]\nX.corr()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that the correlation between 'mileage' and 'year' is so high. Not only them, but also 'mpg' and 'tax'. We need to check the VIF to see the overall correlation between the independent variables.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"VIF = pd.Series([variance_inflation_factor(X.values, i) for i in range(X.shape[1])], \n                index=X.columns)\nprint(VIF)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that the VIF for 'year', 'mpg', and 'engineSize' are more than 10. It means that those variables correlate with the other variables (for example, 'year' variable effects the values of all of the other independent variables). So there is a multicollinearity in this data. From the statistics view, we can exclude the variables, but I don't know whether those variables are important to predict the price or not. So, I'll keep those variables.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Split Data","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"As usual, we will split the train and test data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X = data_encode_dummy_transform.drop(columns = ['price'])\nY = data_encode_dummy_transform['price']\n\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)\n\nX_train = X_train.reset_index(drop = True)\nX_test = X_test.reset_index(drop = True)\nY_train = Y_train.reset_index(drop = True)\nY_test = Y_test.reset_index(drop = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Create Model","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"And now for the main event, we will create the model. Because the data has a multicollinearity, we need a regression analysis that can handle multicollinearity. Like I said in the introduction, I've made a model before in my first version using PCR and PLS and then compare these models. Then I realized that what if I combined PCA and XGBRegressor. So first we will use PCA on independent variables matrix, then we will use XGBRegressor to create the model. I will use the code for PCA from [this site](https://nirpyresearch.com/principal-component-regression-python/).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define the PCA object\npca = PCA()\n\n# Preprocessing (1): first derivative\nd1X = savgol_filter(X_train, 25, polyorder = 5, deriv=1)\n\n# Preprocess (2) Standardize features by removing the mean and scaling to unit variance\nXstd = StandardScaler().fit_transform(d1X[:,:])\n\n# Run PCA producing the reduced variable Xreg and select the first pc components\nXreg = pca.fit_transform(Xstd)[:,:]\n\nXGB = XGBRegressor(random_state=0)\n\nXGB.fit(Xreg,Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"XGB.score(Xreg,Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_PCA_XGB = XGB.predict(Xreg)\n\nmean_squared_error(Y_train, Y_PCA_XGB)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you can see, the $R^2$ is $0.989 \\sim 99\\%$ which is high enough and the $MSE$ is $0.0022$ which is small enough.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Diagnostic Checking","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Well I saw that many people skip this step, but I'll keep doing this step.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Normality of Residuals","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"resid = Y_train - Y_PCA_XGB\n\nsns.distplot(resid)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"With the plot looks like that, we can assume that the residuals are normal.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Autocorrelation of Residuals","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"To check the autocorrelation of residuals, we will use run test with the null hypothesis is the residual values are random (there is no autocorrelation in residuals).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"result = runstest_1samp(resid)[1]\nprint('P-value :',result)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"With significance $\\alpha=0.05$, we can see that p-value $> 0.05$. Therefore, we won't reject the null hypothesis so that there is no autocorrelation in residuals.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Homoscedasticity of Residuals","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.xlabel('Fitted Values')\nplt.ylabel('Residuals')\nplt.scatter(Y_PCA_XGB,resid)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that the residuals don't form any shape so that the variance is constant. Therefore, we can say that the homoscedasticity of residuals is achieved.\n\nSince this model has passed all of the diagnostic checkings, we can say that this model is well enough to predict the Audi car.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Test Data","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Now, we will test our model with the test data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Preprocessing (1): first derivative\nd1X = savgol_filter(X_test, 25, polyorder = 5, deriv=1)\n\n# Preprocess (2) Standardize features by removing the mean and scaling to unit variance\nXstd = StandardScaler().fit_transform(d1X[:,:])\n\n# Run PCA producing the reduced variable Xreg and select the first pc components\nXreg = pca.fit_transform(Xstd)[:,:]\n\nprediction = XGB.predict(Xreg)\n\ndata_test = {\n    'Y_test' : Y_test,\n    'Prediction' : prediction\n}\n\npd.DataFrame(data_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_squared_error(Y_test, prediction)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Well the $MSE$ of the data test using this model is higher than using PCR, but it's still low for me haha.... At least, it's great to know how to handle multicollinearity. Please comment in down below so that we can learn each other. :)\n\nNote: if we want to use this model, we must first standardize the independent variables. Then, after we get the predicted dependent variables, we should take the exponent instead to see the exact price.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}