{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Práctica 2: Aprendizaje y selección de modelos de clasificación","metadata":{}},{"cell_type":"markdown","source":"#### Minería de Datos: Curso académico 2020-2021","metadata":{}},{"cell_type":"markdown","source":"#### Profesorado:\n* Juan Carlos Alfaro Jiménez\n* José Antonio Gámez Martín","metadata":{}},{"cell_type":"markdown","source":"#### Integrantes:\n* Gonzalo Pinto Perez\n* Yeremi Martin Huaman Torres","metadata":{}},{"cell_type":"markdown","source":"# 1.-Preliminares","metadata":{}},{"cell_type":"markdown","source":"Indicamos la semilla y las librerias que utilizaremos","metadata":{}},{"cell_type":"code","source":"# Third party\nfrom sklearn.base import clone\nfrom sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import HistGradientBoostingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.preprocessing import Normalizer\nfrom imblearn.pipeline import make_pipeline\nfrom sklearn.impute import *\nfrom sklearn.compose import make_column_selector, make_column_transformer\nfrom sklearn.preprocessing import *\n# Local application\nimport miner_a_de_datos_aprendizaje_modelos_utilidad as utils","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","execution":{"iopub.status.busy":"2021-06-11T17:01:54.470524Z","iopub.execute_input":"2021-06-11T17:01:54.47102Z","iopub.status.idle":"2021-06-11T17:01:56.169927Z","shell.execute_reply.started":"2021-06-11T17:01:54.470969Z","shell.execute_reply":"2021-06-11T17:01:56.168918Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"random_state = 27912","metadata":{"execution":{"iopub.status.busy":"2021-06-11T17:01:56.171706Z","iopub.execute_input":"2021-06-11T17:01:56.172285Z","iopub.status.idle":"2021-06-11T17:01:56.176655Z","shell.execute_reply.started":"2021-06-11T17:01:56.172233Z","shell.execute_reply":"2021-06-11T17:01:56.175391Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Carga de datos","metadata":{}},{"cell_type":"markdown","source":"Se realiza la carga de datos de los conjuntos de datos de Pima Indian Diabetes y Breast Cancer Wisconsin","metadata":{}},{"cell_type":"code","source":"filepathDiabetes = \"../input/pima-indians-diabetes-database/diabetes.csv\"\n\nfilepathWisconsin = \"../input/breast-cancer-wisconsin-data/data.csv\"\n\nindexDiabetes = None\ntargetDiabetes = \"Outcome\"\n\nindexWisconsin = \"id\"\ntargetWisconsin = \"diagnosis\"\n\ntrain_size = 0.7","metadata":{"execution":{"iopub.status.busy":"2021-06-11T17:01:56.178245Z","iopub.execute_input":"2021-06-11T17:01:56.179236Z","iopub.status.idle":"2021-06-11T17:01:56.196973Z","shell.execute_reply.started":"2021-06-11T17:01:56.179171Z","shell.execute_reply":"2021-06-11T17:01:56.195478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.1 Pima Indian Diabetes ","metadata":{}},{"cell_type":"code","source":"dataDiabetes = utils.load_data(filepathDiabetes, indexDiabetes, targetDiabetes)","metadata":{"execution":{"iopub.status.busy":"2021-06-11T17:01:56.198833Z","iopub.execute_input":"2021-06-11T17:01:56.199426Z","iopub.status.idle":"2021-06-11T17:01:56.234871Z","shell.execute_reply.started":"2021-06-11T17:01:56.199386Z","shell.execute_reply":"2021-06-11T17:01:56.233673Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Muestreo y separación de datos de Pima Indian Diabetes","metadata":{}},{"cell_type":"code","source":"dataDiabetes.sample(5, random_state=random_state)","metadata":{"execution":{"iopub.status.busy":"2021-06-11T17:01:56.237825Z","iopub.execute_input":"2021-06-11T17:01:56.238918Z","iopub.status.idle":"2021-06-11T17:01:56.269657Z","shell.execute_reply.started":"2021-06-11T17:01:56.238874Z","shell.execute_reply":"2021-06-11T17:01:56.268755Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"(X_Diabetes, y_Diabetes) = utils.divide_dataset(dataDiabetes, targetDiabetes)","metadata":{"execution":{"iopub.status.busy":"2021-06-11T17:01:56.272161Z","iopub.execute_input":"2021-06-11T17:01:56.272698Z","iopub.status.idle":"2021-06-11T17:01:56.27973Z","shell.execute_reply.started":"2021-06-11T17:01:56.272659Z","shell.execute_reply":"2021-06-11T17:01:56.278497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_Diabetes.sample(5, random_state=random_state)","metadata":{"execution":{"iopub.status.busy":"2021-06-11T17:01:56.281448Z","iopub.execute_input":"2021-06-11T17:01:56.281951Z","iopub.status.idle":"2021-06-11T17:01:56.308041Z","shell.execute_reply.started":"2021-06-11T17:01:56.281897Z","shell.execute_reply":"2021-06-11T17:01:56.306598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_Diabetes.sample(5, random_state=random_state)","metadata":{"execution":{"iopub.status.busy":"2021-06-11T17:01:56.310212Z","iopub.execute_input":"2021-06-11T17:01:56.310716Z","iopub.status.idle":"2021-06-11T17:01:56.324556Z","shell.execute_reply.started":"2021-06-11T17:01:56.310679Z","shell.execute_reply":"2021-06-11T17:01:56.323023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"(X_train_Diabetes, X_test_Diabetes, y_train_Diabetes, y_test_Diabetes) = train_test_split(X_Diabetes, y_Diabetes,\n                                                      stratify=y_Diabetes,\n                                                      train_size=train_size,\n                                                      random_state=random_state)","metadata":{"execution":{"iopub.status.busy":"2021-06-11T17:01:56.326535Z","iopub.execute_input":"2021-06-11T17:01:56.326903Z","iopub.status.idle":"2021-06-11T17:01:56.346874Z","shell.execute_reply.started":"2021-06-11T17:01:56.326867Z","shell.execute_reply":"2021-06-11T17:01:56.345292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_Diabetes.sample(5, random_state=random_state)","metadata":{"execution":{"iopub.status.busy":"2021-06-11T17:01:56.348411Z","iopub.execute_input":"2021-06-11T17:01:56.348726Z","iopub.status.idle":"2021-06-11T17:01:56.368864Z","shell.execute_reply.started":"2021-06-11T17:01:56.348693Z","shell.execute_reply":"2021-06-11T17:01:56.367501Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train_Diabetes.sample(5, random_state=random_state)","metadata":{"execution":{"iopub.status.busy":"2021-06-11T17:01:56.370762Z","iopub.execute_input":"2021-06-11T17:01:56.371208Z","iopub.status.idle":"2021-06-11T17:01:56.386459Z","shell.execute_reply.started":"2021-06-11T17:01:56.371168Z","shell.execute_reply":"2021-06-11T17:01:56.384955Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test_Diabetes.sample(5, random_state=random_state)","metadata":{"execution":{"iopub.status.busy":"2021-06-11T17:01:56.38822Z","iopub.execute_input":"2021-06-11T17:01:56.388719Z","iopub.status.idle":"2021-06-11T17:01:56.414328Z","shell.execute_reply.started":"2021-06-11T17:01:56.388666Z","shell.execute_reply":"2021-06-11T17:01:56.413159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test_Diabetes.sample(5, random_state=random_state)","metadata":{"execution":{"iopub.status.busy":"2021-06-11T17:01:56.415978Z","iopub.execute_input":"2021-06-11T17:01:56.416332Z","iopub.status.idle":"2021-06-11T17:01:56.43597Z","shell.execute_reply.started":"2021-06-11T17:01:56.416296Z","shell.execute_reply":"2021-06-11T17:01:56.434598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.2 Breast Cancer Wisconsin","metadata":{}},{"cell_type":"code","source":"dataWisconsin = utils.load_data(filepathWisconsin,indexWisconsin,targetWisconsin)","metadata":{"execution":{"iopub.status.busy":"2021-06-11T17:01:56.43737Z","iopub.execute_input":"2021-06-11T17:01:56.437704Z","iopub.status.idle":"2021-06-11T17:01:56.470063Z","shell.execute_reply.started":"2021-06-11T17:01:56.437672Z","shell.execute_reply":"2021-06-11T17:01:56.468013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataWisconsin.sample(5, random_state=random_state)","metadata":{"execution":{"iopub.status.busy":"2021-06-11T17:01:56.47142Z","iopub.execute_input":"2021-06-11T17:01:56.472034Z","iopub.status.idle":"2021-06-11T17:01:56.508857Z","shell.execute_reply.started":"2021-06-11T17:01:56.471993Z","shell.execute_reply":"2021-06-11T17:01:56.507497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Como se explicado en la practica 1, se elimina el atributo ruidoso 'Unnamed:32' porque se ha introducido erroneamente en la primera linea del csv una coma razon que da a una nueva variable.","metadata":{}},{"cell_type":"code","source":"dataWisconsin.drop(['Unnamed: 32'], axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-11T17:01:56.510823Z","iopub.execute_input":"2021-06-11T17:01:56.511244Z","iopub.status.idle":"2021-06-11T17:01:56.524856Z","shell.execute_reply.started":"2021-06-11T17:01:56.511208Z","shell.execute_reply":"2021-06-11T17:01:56.523457Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Nuestra variable objetivo necesita ser 0 o 1 para poder utilizar recall por lo tanto realizamos el siguiente cambio:","metadata":{}},{"cell_type":"code","source":"dataWisconsin['diagnosis']=dataWisconsin['diagnosis'].map({'M':1,'B':0})","metadata":{"execution":{"iopub.status.busy":"2021-06-11T17:01:56.526669Z","iopub.execute_input":"2021-06-11T17:01:56.527008Z","iopub.status.idle":"2021-06-11T17:01:56.542779Z","shell.execute_reply.started":"2021-06-11T17:01:56.526976Z","shell.execute_reply":"2021-06-11T17:01:56.541247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataWisconsin.sample(5, random_state=random_state)","metadata":{"execution":{"iopub.status.busy":"2021-06-11T17:01:56.544172Z","iopub.execute_input":"2021-06-11T17:01:56.544671Z","iopub.status.idle":"2021-06-11T17:01:56.582567Z","shell.execute_reply.started":"2021-06-11T17:01:56.544635Z","shell.execute_reply":"2021-06-11T17:01:56.581619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"(X_Wisconsin, y_Wisconsin) = utils.divide_dataset(dataWisconsin, targetWisconsin)","metadata":{"execution":{"iopub.status.busy":"2021-06-11T17:01:56.58397Z","iopub.execute_input":"2021-06-11T17:01:56.58482Z","iopub.status.idle":"2021-06-11T17:01:56.596638Z","shell.execute_reply.started":"2021-06-11T17:01:56.584731Z","shell.execute_reply":"2021-06-11T17:01:56.595497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_Wisconsin.sample(5, random_state=random_state)","metadata":{"execution":{"iopub.status.busy":"2021-06-11T17:01:56.598234Z","iopub.execute_input":"2021-06-11T17:01:56.59866Z","iopub.status.idle":"2021-06-11T17:01:56.635548Z","shell.execute_reply.started":"2021-06-11T17:01:56.59861Z","shell.execute_reply":"2021-06-11T17:01:56.634453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_Wisconsin.sample(5, random_state=random_state)","metadata":{"execution":{"iopub.status.busy":"2021-06-11T17:01:56.637366Z","iopub.execute_input":"2021-06-11T17:01:56.637711Z","iopub.status.idle":"2021-06-11T17:01:56.650838Z","shell.execute_reply.started":"2021-06-11T17:01:56.637678Z","shell.execute_reply":"2021-06-11T17:01:56.649712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"(X_Wisconsin_train, X_Wisconsin_test, y_Wisconsin_train, y_Wisconsin_test) = train_test_split(X_Wisconsin, y_Wisconsin,\n                                                      stratify=y_Wisconsin,\n                                                      random_state=random_state,\n                                                      train_size=train_size)","metadata":{"execution":{"iopub.status.busy":"2021-06-11T17:01:56.652565Z","iopub.execute_input":"2021-06-11T17:01:56.653059Z","iopub.status.idle":"2021-06-11T17:01:56.668723Z","shell.execute_reply.started":"2021-06-11T17:01:56.653008Z","shell.execute_reply":"2021-06-11T17:01:56.667538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_Wisconsin_train.sample(5, random_state=random_state)","metadata":{"execution":{"iopub.status.busy":"2021-06-11T17:01:56.67065Z","iopub.execute_input":"2021-06-11T17:01:56.671129Z","iopub.status.idle":"2021-06-11T17:01:56.709893Z","shell.execute_reply.started":"2021-06-11T17:01:56.671075Z","shell.execute_reply":"2021-06-11T17:01:56.708513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_Wisconsin_test.sample(5, random_state=random_state)","metadata":{"execution":{"iopub.status.busy":"2021-06-11T17:01:56.711418Z","iopub.execute_input":"2021-06-11T17:01:56.71188Z","iopub.status.idle":"2021-06-11T17:01:56.744171Z","shell.execute_reply.started":"2021-06-11T17:01:56.711838Z","shell.execute_reply":"2021-06-11T17:01:56.742734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_Wisconsin_train.sample(5, random_state=random_state)","metadata":{"execution":{"iopub.status.busy":"2021-06-11T17:01:56.745646Z","iopub.execute_input":"2021-06-11T17:01:56.746015Z","iopub.status.idle":"2021-06-11T17:01:56.762226Z","shell.execute_reply.started":"2021-06-11T17:01:56.745978Z","shell.execute_reply":"2021-06-11T17:01:56.760782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_Wisconsin_test.sample(5, random_state=random_state)","metadata":{"execution":{"iopub.status.busy":"2021-06-11T17:01:56.7637Z","iopub.execute_input":"2021-06-11T17:01:56.76405Z","iopub.status.idle":"2021-06-11T17:01:56.779452Z","shell.execute_reply.started":"2021-06-11T17:01:56.764016Z","shell.execute_reply":"2021-06-11T17:01:56.777744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Modelos de clasificación supervisada","metadata":{}},{"cell_type":"markdown","source":"En este apartado generamos los distintos modelos de clasificación supervisada que utilizaremos para esta práctica, siendo estos:\n* el modelo de K números de vecinos \n* el modelo de árbol de decisión\n* el modelo adaboost\n* el modelo baggin\n* el modelo random forest\n* el modelo gradient boosting\n* el modelo de gradient boosting basado en el histograma respectivamente.","metadata":{}},{"cell_type":"code","source":"\n\nk_neighbors_model = KNeighborsClassifier()\n\ndecision_tree_model = DecisionTreeClassifier(random_state=random_state)\n\nadaboost_model = AdaBoostClassifier(random_state=random_state)\n\nbagging_model = BaggingClassifier(random_state=random_state)\n\nrandom_forest_model = RandomForestClassifier(random_state=random_state)\n\ngradient_boosting_model = GradientBoostingClassifier(random_state=random_state)\n\nhist_gradient_boosting_model = HistGradientBoostingClassifier(random_state=random_state)","metadata":{"execution":{"iopub.status.busy":"2021-06-11T17:01:56.781565Z","iopub.execute_input":"2021-06-11T17:01:56.782168Z","iopub.status.idle":"2021-06-11T17:01:56.79102Z","shell.execute_reply.started":"2021-06-11T17:01:56.782126Z","shell.execute_reply":"2021-06-11T17:01:56.789701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Evaluación de modelos (validación cruzada)","metadata":{}},{"cell_type":"markdown","source":"Para evaluar dichos modelos vamos a utilizar validación cruzada para evaluar los modelos de acuerdo con cada una de las configuraciones de hiperparámetros para decidir cuál es la mejor. Para ello utilizaremos GridSearchCV nos mostrara la mejores hiperparametros para cada configuracion de cada algoritmo, ordenados en estos caso por el mejor recall que se puede obtener.\n\n\n","metadata":{}},{"cell_type":"code","source":"n_splits = 10\nn_repeats = 5\n\ncv = RepeatedStratifiedKFold(n_splits=n_splits,\n                             n_repeats=n_repeats,\n                             random_state=random_state)","metadata":{"execution":{"iopub.status.busy":"2021-06-11T17:01:56.792486Z","iopub.execute_input":"2021-06-11T17:01:56.792938Z","iopub.status.idle":"2021-06-11T17:01:56.804842Z","shell.execute_reply.started":"2021-06-11T17:01:56.792888Z","shell.execute_reply":"2021-06-11T17:01:56.803434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#  5. Selección de modelos","metadata":{}},{"cell_type":"markdown","source":"## 5.1 Breast Cancer Wisconsin ","metadata":{}},{"cell_type":"markdown","source":"### Pipeline anterior practica - preprocesamiento\n\nEn la practica 1 hemos definido este preprocesamiento, y nos ha dado buenos resultados por lo tanto utilizaremos este mismo, junto a la aplicacion de los modelos de clasificacion supervisada anteriormente mencionados ","metadata":{}},{"cell_type":"code","source":"preproceserWinsconsin = make_column_transformer(('drop',['radius_mean','radius_se','radius_worst','concave points_mean',\n          'concave points_se','concave points_worst']),\n                                                remainder='passthrough')","metadata":{"execution":{"iopub.status.busy":"2021-06-11T17:01:56.806384Z","iopub.execute_input":"2021-06-11T17:01:56.806911Z","iopub.status.idle":"2021-06-11T17:01:56.818406Z","shell.execute_reply.started":"2021-06-11T17:01:56.806852Z","shell.execute_reply":"2021-06-11T17:01:56.81743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 5.1.1 K Vecinos mas cercanos","metadata":{}},{"cell_type":"markdown","source":"Definimos nuestro estimador","metadata":{}},{"cell_type":"code","source":"estimator = clone(k_neighbors_model)","metadata":{"execution":{"iopub.status.busy":"2021-06-11T17:01:56.819657Z","iopub.execute_input":"2021-06-11T17:01:56.820527Z","iopub.status.idle":"2021-06-11T17:01:56.836166Z","shell.execute_reply.started":"2021-06-11T17:01:56.820471Z","shell.execute_reply":"2021-06-11T17:01:56.834882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Normalizar\nEs necesario normalizar el conjunto de atributos numericos para el algoritmo de KNN para evitar que los atributos con un mayor rango de valores tengan un mayor peso en el calculo de las distancias y, por tanto, en el algoritmo.","metadata":{}},{"cell_type":"code","source":"transformer = Normalizer()","metadata":{"execution":{"iopub.status.busy":"2021-06-11T17:01:56.838431Z","iopub.execute_input":"2021-06-11T17:01:56.839149Z","iopub.status.idle":"2021-06-11T17:01:56.848075Z","shell.execute_reply.started":"2021-06-11T17:01:56.839091Z","shell.execute_reply":"2021-06-11T17:01:56.847206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Discretizar\n\nSe aplicara la discretizacion utilizando las tres estrategias (uniform,quantile,kmeans) y segun el numero de particiones 2,3,4. Se va utilizar discretizacion solo para los algoritmos de KNN y arboles de decision ya que son los algoritmos que mejores resultados se obtiene.","metadata":{}},{"cell_type":"code","source":"discretizer = KBinsDiscretizer()","metadata":{"execution":{"iopub.status.busy":"2021-06-11T17:01:56.85719Z","iopub.execute_input":"2021-06-11T17:01:56.857638Z","iopub.status.idle":"2021-06-11T17:01:56.863096Z","shell.execute_reply.started":"2021-06-11T17:01:56.857601Z","shell.execute_reply":"2021-06-11T17:01:56.861206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Pipeline\n\nEstablecemos nuestro pipeline aplicando nuestro preprocesamiento, el normalizador, el discretizador con quantile ya que nos dio buenos resultados (estos resultados se explicaran despues de la optimizacion de hiperparametros) y finalmente el estimador","metadata":{}},{"cell_type":"code","source":"pipeline_Winsconsin=make_pipeline(preproceserWinsconsin,transformer,discretizer,estimator)","metadata":{"execution":{"iopub.status.busy":"2021-06-11T17:01:56.865488Z","iopub.execute_input":"2021-06-11T17:01:56.865989Z","iopub.status.idle":"2021-06-11T17:01:56.877655Z","shell.execute_reply.started":"2021-06-11T17:01:56.865934Z","shell.execute_reply":"2021-06-11T17:01:56.87635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Los dos hiperparametros mas importantes para KNN son\n- **weights**: es simplemente la funcion de pesado de los vecinos mas cercanos. Por un lado uniform(todos los puntos se ponderan por igual) y por otro distance (los vecinos más cercanos de un punto tendrán una mayor influencia que los vecinos más alejados) \n- **n_neighbors** : para que no halla la posibilidad de empate ya que nuestra BBDD son pares, el numero de vecinos que elegiremos sera impar. Segun investigaciones en el caso de un pequeño número de vecinos, el ruido tendrá una mayor influencia en el resultado, por el contrario un gran número de vecinos lo hace computacionalmente costoso. Tambien una pequeña cantidad de vecinos tienen un ajuste más flexible, lo que tendrá un sesgo bajo pero una varianza alta, y un gran número de vecinos tendrá un límite de decisión más suave, lo que significa una varianza menor pero un sesgo más alto. Esto se traduce si utilizamos vecinos muy pequeños tendriamos más a underfitting y si utilizamos muy grandes podria llegar al overfitting. Por ello utilizaremos valores no muy pequeños por lo menos 2 digitos entre 11 y 29.","metadata":{}},{"cell_type":"code","source":"weights = [\"uniform\", \"distance\"]\nn_neighbors = [11, 13, 15,17, 21,23,25,27,29]\n\nk_neighbors_clf_Wisconsin = utils.optimize_params(pipeline_Winsconsin,\n                                        X_Wisconsin_train, y_Wisconsin_train, cv,\n                                        kneighborsclassifier__weights=weights,\n                                        kneighborsclassifier__n_neighbors=n_neighbors,\n                                        kbinsdiscretizer__n_bins= [2,3,4],\n                                        kbinsdiscretizer__strategy= [\"uniform\", \"quantile\", \"kmeans\"],\n                                        scoring=['recall'],refit='recall')","metadata":{"execution":{"iopub.status.busy":"2021-06-11T17:01:56.879872Z","iopub.execute_input":"2021-06-11T17:01:56.880448Z","iopub.status.idle":"2021-06-11T17:06:33.593694Z","shell.execute_reply.started":"2021-06-11T17:01:56.880405Z","shell.execute_reply":"2021-06-11T17:06:33.592442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Los resultados mas optimos que obtenemos estan alrededor del 11 como el numero de vecinos mas cercanos y como funcion de peso tanto uniform como distance empatando en la mejor configuracion. A lo largo de la lista de resultados que nos devuelve el Grid, podemos observar que a partir de este punto no mejora aumentando el numero de vecinos sino que empeora, por el lado del peso no podemos encontrar una relacion con su orden ya que uniform aparece como la mejor opcion y tambien la peor. \n\nPor otro lado la discretizacion que obtenemos al aplicar a KNN, respecto a n_bins parece dar mejores resultado un numero medio en lugar uno muy alto o baja, por otro lado parece coincidir utilizando quantile como estrategia para discretizar y la peor uniform.","metadata":{}},{"cell_type":"markdown","source":"### 5.1.2 Arboles de decision","metadata":{}},{"cell_type":"code","source":"estimator = clone(decision_tree_model)","metadata":{"execution":{"iopub.status.busy":"2021-06-11T17:06:33.595944Z","iopub.execute_input":"2021-06-11T17:06:33.596352Z","iopub.status.idle":"2021-06-11T17:06:33.605831Z","shell.execute_reply.started":"2021-06-11T17:06:33.596309Z","shell.execute_reply":"2021-06-11T17:06:33.604606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Discretizar\n\nComo hemos dicho tambien utilizaremos para los Arboles decisiones ya que son los mas comunes dando un mejor resultado.","metadata":{}},{"cell_type":"code","source":"discretizer = KBinsDiscretizer()","metadata":{"execution":{"iopub.status.busy":"2021-06-11T17:06:33.607401Z","iopub.execute_input":"2021-06-11T17:06:33.607879Z","iopub.status.idle":"2021-06-11T17:06:33.622286Z","shell.execute_reply.started":"2021-06-11T17:06:33.607829Z","shell.execute_reply":"2021-06-11T17:06:33.62117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Pipeline","metadata":{}},{"cell_type":"code","source":"pipeline_Winsconsin=make_pipeline(preproceserWinsconsin,discretizer,estimator)","metadata":{"execution":{"iopub.status.busy":"2021-06-11T17:06:33.623482Z","iopub.execute_input":"2021-06-11T17:06:33.623828Z","iopub.status.idle":"2021-06-11T17:06:33.639632Z","shell.execute_reply.started":"2021-06-11T17:06:33.623793Z","shell.execute_reply":"2021-06-11T17:06:33.637927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Hiperparametro para los arboles de decision:\n- **criterion**: Criterio utilizado para medir la calidad de una partición, por un lado gini (la impureza de Gini o el índice de Gini) mide las divergencias entre la distribuciones de probabilidades de los valores de la variable clase y divide un nodo de tal forma que de la menor cantidad de impureza  y entropy (la ganancia de información) divide un nodo de modo que proporcione la mayor cantidad de ganancia de informacion. En todo caso muchos investigadores señalan que en la mayoria de los casos la eleccion de los criterios no marca mucho la diferencia.\n- **max_depth** : la profundidad máxima teórica que puede alcanzar un árbol de decisión es uno menos que el número de muestras de entrenamiento, pero ningún algoritmo permitirá llegar a este punto por razones obvias, una de las cuales es el overfitting. Cuanto más profundo permita que nuestro árbol crezca, más complejo se volverá nuestro modelo porque tendrá más divisiones y capturará más información sobre los datos y esta es una de las causas principales del sobreajuste. Por otro lado  tener una profundidad muy baja provocara que nuestro modelo no se ajuste y no podra encontrar el mejor valor. \n- **ccp_alpha**: elegira el subárbol con mayor complejidad de costo que sea más pequeño.","metadata":{}},{"cell_type":"code","source":"criterion = [\"gini\", \"entropy\"]\nmax_depth = [ 2, 4, 6, 8,10,12]\nccp_alpha = [0.0, 0.1, 0.2, 0.3, 0.4]\n\ndecision_tree_clf_Wisconsin = utils.optimize_params(pipeline_Winsconsin,\n                                          X_Wisconsin_train, y_Wisconsin_train, cv,\n                                          decisiontreeclassifier__criterion=criterion,\n                                          decisiontreeclassifier__max_depth=max_depth,\n                                          decisiontreeclassifier__ccp_alpha=ccp_alpha,\n                                                    kbinsdiscretizer__n_bins= [2,3,4],\n                                        kbinsdiscretizer__strategy= [\"uniform\", \"quantile\", \"kmeans\"],\n                                                  scoring=['recall'],refit='recall')","metadata":{"execution":{"iopub.status.busy":"2021-06-11T17:06:33.641537Z","iopub.execute_input":"2021-06-11T17:06:33.641922Z","iopub.status.idle":"2021-06-11T17:17:57.362796Z","shell.execute_reply.started":"2021-06-11T17:06:33.641886Z","shell.execute_reply":"2021-06-11T17:17:57.36166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"En esta caso el criterio mejor valorada es gini, pero podemos observar que los siguientes casos son de entropy y tambien gini esta entre los peores. Por otro lado la profundidad que obtenemos es 2 pero esta empata con un gran numero de otras configuraciones entre un rango par de 2 y 12, por tanto no podemos obtener un conocimiento con esto ya que coinciden todos como la mejor configuracion.\nPor ultimo el ccp_alpha que nos indica la tasa se podara, el rango 0.2, 0.3, 0.4 estan entre los primeros y los siguientes aumentando este rango a 0.4 son peores.\n\nRespecto a la discretizacion parece dar mejores resultado con quantile y 2 en el numero de bins y peores con un mayor numero.","metadata":{}},{"cell_type":"markdown","source":"### 5.1.3 AdaBoost ","metadata":{}},{"cell_type":"code","source":"estimator = clone(adaboost_model)","metadata":{"execution":{"iopub.status.busy":"2021-06-11T17:17:57.364256Z","iopub.execute_input":"2021-06-11T17:17:57.364603Z","iopub.status.idle":"2021-06-11T17:17:57.371013Z","shell.execute_reply.started":"2021-06-11T17:17:57.364568Z","shell.execute_reply":"2021-06-11T17:17:57.369849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Pipeline\n","metadata":{}},{"cell_type":"code","source":"pipeline_Winsconsin=make_pipeline(preproceserWinsconsin,estimator)","metadata":{"execution":{"iopub.status.busy":"2021-06-11T17:17:57.372599Z","iopub.execute_input":"2021-06-11T17:17:57.372943Z","iopub.status.idle":"2021-06-11T17:17:57.386829Z","shell.execute_reply.started":"2021-06-11T17:17:57.372908Z","shell.execute_reply":"2021-06-11T17:17:57.385697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\nHiperparametro para AdaBoost:\n- **base_estimator**: este parametro indica el tipo de estimador que pueden ser arbol de decision, regresor logistico, SVC, etc. No puede ser KNN ya que no se puede asignar el peso a este modelo. En este caso utilizaremos el arbol de decision. \n- **Learning_rate**: parametro que se proporciona para controlar la contribucion de cada clasificador, estos valores comprenderan entre mayor que cero y menor igual a uno. \n- **n_estimatores**: el numero de estimadores que queremos usar en nuestro conjunto de datos, este hiperparametro es el mas importante, la cantidad de arboles agregados al modelo debe ser alta para que el modelo funcione bien.\n\nPara ello estableceremos el valor a partir de 50 hasta 70. En cuanto al resto de metricas utilizaremos las mismo que en el arbol de decision disminuyendo el numero de parametros a probar ya que aumentaria mucho el coste computacional, y lo que buscamos es aprender el uso del grid con adaboost, y la influencia que tiene los hiperparametros para conseguir el mas optimo.","metadata":{}},{"cell_type":"code","source":"base_estimator = DecisionTreeClassifier(random_state=random_state)\n\nbase_estimator = [base_estimator]\nn_estimators= [50,60,70]\nlearning_rate = [0.0001, 0.1,0.3, 1.0]\ncriterion = [\"gini\", \"entropy\"]\nmax_depth = [ 2, 4]\nccp_alpha = [0.0, 0.1, 0.2]\n\nadaboost_clf_Wisconsin = utils.optimize_params(pipeline_Winsconsin,\n                                     X_Wisconsin_train, y_Wisconsin_train, cv,\n                                     adaboostclassifier__base_estimator=base_estimator,\n                                     adaboostclassifier__learning_rate=learning_rate,\n                                     adaboostclassifier__n_estimators=n_estimators,\n                                     adaboostclassifier__base_estimator__criterion=criterion,\n                                     adaboostclassifier__base_estimator__max_depth=max_depth,\n                                     adaboostclassifier__base_estimator__ccp_alpha=ccp_alpha,scoring=['recall'],refit='recall')","metadata":{"execution":{"iopub.status.busy":"2021-06-11T17:17:57.388463Z","iopub.execute_input":"2021-06-11T17:17:57.389056Z","iopub.status.idle":"2021-06-11T17:35:52.140924Z","shell.execute_reply.started":"2021-06-11T17:17:57.389018Z","shell.execute_reply":"2021-06-11T17:35:52.139817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Primero que todo como se comento se utiliza el algoritmo de arbol de decision, empecemos hablando de estos hiperparametros y como afectan, se observa que ccp_alpha obtiene mejores resultados con 0, que con 0.2. En el caso de la profundidad no aporta mucha informacion ya que solo elegimos dos y los resultados no varian eligiendo uno u otro. Por ultimo el criterio parece haber un mayor numero de entropy que de gini.\n\nPor el lado de los hiperparametro parece haber una relacion entre un mayor numero de estimadores con la mejor configuracion, y en el caso de la tasa de aprendizaje parece ser que una mayor aporta mejores resultados que una menor.\n\nEn conclusion los hiperparametros que nos aportan mas informacion a este conjunto de datos parece ser numero de estimadores y la tasa de aprendizaje, y ccp_alpha del arbol de decision.","metadata":{}},{"cell_type":"markdown","source":"### 5.1.4 Bagging (Bootstrap Aggregation)","metadata":{}},{"cell_type":"code","source":"estimator = clone(bagging_model)","metadata":{"execution":{"iopub.status.busy":"2021-06-11T17:35:52.142637Z","iopub.execute_input":"2021-06-11T17:35:52.1434Z","iopub.status.idle":"2021-06-11T17:35:52.149504Z","shell.execute_reply.started":"2021-06-11T17:35:52.14334Z","shell.execute_reply":"2021-06-11T17:35:52.147625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Pipeline","metadata":{}},{"cell_type":"code","source":"pipeline_Winsconsin=make_pipeline(preproceserWinsconsin,estimator)","metadata":{"execution":{"iopub.status.busy":"2021-06-11T17:35:52.151561Z","iopub.execute_input":"2021-06-11T17:35:52.151954Z","iopub.status.idle":"2021-06-11T17:35:52.179582Z","shell.execute_reply.started":"2021-06-11T17:35:52.151917Z","shell.execute_reply":"2021-06-11T17:35:52.177433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Hiperparametro para Bagging:\n- **n_estimators**: segun reportes cientificos normalmente es buen que se aumenten de 10 o 100. El número de árboles aumenta hasta que se estabiliza el rendimiento del modelo. Podría sugerir que más árboles conducirán a un sobreajuste, aunque este no es el caso. \n- **max_samples**: número de muestras utilizadas para adaptarse a cada árbol de decisiónm normalmente para un conjunto de datos pequeños puede aumentar la variacion de los arboles decision resultantes y podria resultar en un mejor rendimiento. \n- **bootstrap** determina que el muestreo de instancias es con reemplazo. \n- **bootstrap_features** determina que el muestreo de características es con reemplazo.\n\nBagging puede recibir diferentes nombres segun algunas modificaciones como: Pasting Ensemble (restrigiendo el numero de muestras), Random Subspaces Ensemble(similar a random forest solo que la muestra de arranque es aleatoria y el subconjunto de caracteristicas se selecciona para todo el arbol de decision en lugar de en cada punto de division del arbol), Random Patches Ensemble (este caso implica un ajuste tanto por caracteristicas como por muestras)\n","metadata":{}},{"cell_type":"code","source":"base_estimator = clone(decision_tree_model)\nbase_estimator = [base_estimator]\nmax_samples= [0.1,1.1,0.1]\nn_estimators=[10,100]\nmax_features=[1,5,10]\nbootstrap= [True,False]\nbootstrap_features=[True,False]\ncriterion = [\"gini\", \"entropy\"]\n\nbagging_clf_Wisconsin = utils.optimize_params(pipeline_Winsconsin,\n                                    X_Wisconsin_train, y_Wisconsin_train, cv,\n                                              baggingclassifier__n_estimators=n_estimators,\n                                              baggingclassifier__max_samples=max_samples,\n                                              baggingclassifier__max_features=max_features,\n                                              baggingclassifier__bootstrap=bootstrap,\n                                              baggingclassifier__bootstrap_features=bootstrap_features,\n                                    baggingclassifier__base_estimator=base_estimator,\n                                    baggingclassifier__base_estimator__criterion=criterion,scoring=['recall'],refit='recall')","metadata":{"execution":{"iopub.status.busy":"2021-06-11T17:35:52.182656Z","iopub.execute_input":"2021-06-11T17:35:52.18309Z","iopub.status.idle":"2021-06-11T17:41:18.480645Z","shell.execute_reply.started":"2021-06-11T17:35:52.183057Z","shell.execute_reply":"2021-06-11T17:41:18.478624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"En este caso los hiperparametros del arbol de decision paracen coincidir que el criterion es entropy para las dos configuraciones primeras.\nPor otros lado el resto de los hiperparametros da mejores resultado cuando no hay reemplazo en el muestreo de instancias pero si en el de caracteristicas.\n\nLos mejores resultado nos lo dan las cuatro mejores configuraciones maximo numero de caracteristicas es de 5 pero en este caso parece que no tiene mucha relacion ya que tambien es una de las peores, con maximo de numero de muestras de 0.1 es decir el menor de todas las opciones y el mayor numero de estimadores.","metadata":{}},{"cell_type":"markdown","source":"### 5.1.5 Random forests","metadata":{}},{"cell_type":"code","source":"estimator = clone(random_forest_model)","metadata":{"execution":{"iopub.status.busy":"2021-06-11T17:41:18.482421Z","iopub.execute_input":"2021-06-11T17:41:18.48282Z","iopub.status.idle":"2021-06-11T17:41:18.488911Z","shell.execute_reply.started":"2021-06-11T17:41:18.482779Z","shell.execute_reply":"2021-06-11T17:41:18.487765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Pipeline","metadata":{}},{"cell_type":"code","source":"pipeline_Winsconsin=make_pipeline(preproceserWinsconsin,estimator)","metadata":{"execution":{"iopub.status.busy":"2021-06-11T17:41:18.490663Z","iopub.execute_input":"2021-06-11T17:41:18.491019Z","iopub.status.idle":"2021-06-11T17:41:18.511575Z","shell.execute_reply.started":"2021-06-11T17:41:18.490984Z","shell.execute_reply":"2021-06-11T17:41:18.5099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Los hiperparametros de Random Forests: \n- **n_estimators** elegiendo una gran cantidad de estimadores en un modelo de bosque aleatorio no es la mejor idea. Aunque no degradará el modelo, puede ahorrarle la complejidad computacional. \n- **max_features** es recomendable establecer la raíz cuadrada del número de características presentes en el conjunto de datos. El número ideal de max_features generalmente tiende a estar cerca de este valor.","metadata":{}},{"cell_type":"code","source":"\nn_estimators=[50,60,70,80,90]\ncriterion = [\"gini\", \"entropy\"]\nmax_features = [\"sqrt\", \"log2\"]\n\nrandom_forest_clf_Wisconsin = utils.optimize_params(pipeline_Winsconsin,\n                                          X_Wisconsin_train, y_Wisconsin_train, cv,\n                                          randomforestclassifier__criterion=criterion,\n                                        randomforestclassifier__n_estimators=n_estimators,\n                                          randomforestclassifier__max_features=max_features,scoring=['recall'],refit='recall')","metadata":{"execution":{"iopub.status.busy":"2021-06-11T17:41:18.51319Z","iopub.execute_input":"2021-06-11T17:41:18.513571Z","iopub.status.idle":"2021-06-11T17:42:45.320815Z","shell.execute_reply.started":"2021-06-11T17:41:18.513535Z","shell.execute_reply":"2021-06-11T17:42:45.319617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"En este caso comprobamos los resultados ya que los mejores resultados dan con los n_estimator menores pero tambien los encontramos entre los peores por lo tanto no podemos establecer una conclusion correcta y tambien utilizando la raiz cuadrada o log2. Por otro lado parace que obtenemos mejores resultados con entropy para los 4 primeras configuraciones.","metadata":{}},{"cell_type":"markdown","source":"### 5.1.6 Gradient boosting","metadata":{}},{"cell_type":"code","source":"estimator = clone(gradient_boosting_model)","metadata":{"execution":{"iopub.status.busy":"2021-06-11T17:42:45.323253Z","iopub.execute_input":"2021-06-11T17:42:45.323774Z","iopub.status.idle":"2021-06-11T17:42:45.330653Z","shell.execute_reply.started":"2021-06-11T17:42:45.323718Z","shell.execute_reply":"2021-06-11T17:42:45.329351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Pipeline","metadata":{}},{"cell_type":"code","source":"pipeline_Winsconsin=make_pipeline(preproceserWinsconsin,estimator)","metadata":{"execution":{"iopub.status.busy":"2021-06-11T17:42:45.332878Z","iopub.execute_input":"2021-06-11T17:42:45.333357Z","iopub.status.idle":"2021-06-11T17:42:45.349131Z","shell.execute_reply.started":"2021-06-11T17:42:45.333304Z","shell.execute_reply":"2021-06-11T17:42:45.347796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Los hiperparametros de Gradient boosting: \n- **n_estimators** es bastante robusto en un mayor número de árboles, aún puede sobreajustarse en un punto. \n- **learning_rate** determina el impacto de cada arbol en el resultado final.Generalmente se prefieren valores más bajos ya que hacen que el modelo sea robusto a las características específicas del árbol y, por lo tanto, permiten que se generalice bien. Los valores más bajos requerirían un mayor número de árboles para modelar todas las relaciones y serían computacionalmente costosos. \n- **max_depth** se usa para controlar el overfitting ya que una mayor profundidad permitirá que el modelo aprenda relaciones muy específicas para una muestra en particular.","metadata":{}},{"cell_type":"code","source":"n_estimators= [50,60,70]\nlearning_rate = [0.01, 0.05, 0.1]\ncriterion = [\"friedman_mse\", \"mse\"]\nmax_depth = [1, 2, 3]\nccp_alpha = [0.0, 0.1]\n\ngradient_boosting_clf_Wisconsin = utils.optimize_params(pipeline_Winsconsin,\n                                              X_Wisconsin_train, y_Wisconsin_train, cv,\n                                              gradientboostingclassifier__learning_rate=learning_rate,\n                                              gradientboostingclassifier__criterion=criterion,\n                                              gradientboostingclassifier__max_depth=max_depth,\n                                              gradientboostingclassifier__ccp_alpha=ccp_alpha,scoring=['recall'],refit='recall')","metadata":{"execution":{"iopub.status.busy":"2021-06-11T17:42:45.350909Z","iopub.execute_input":"2021-06-11T17:42:45.351346Z","iopub.status.idle":"2021-06-11T17:45:25.916753Z","shell.execute_reply.started":"2021-06-11T17:42:45.351266Z","shell.execute_reply":"2021-06-11T17:45:25.915313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Observamos que los dos mejores resultados coinciden cuando la profundidad es la mayor es decir 3, tambien cuando ccp_alpha es nulo. Y la tasa de acierto es la mayor. En el caso del criterio utiliza parace que no podemos llegar a una conclusion ya que la aunque mse se encuentre entre las mejores tambien estan entre las peores","metadata":{}},{"cell_type":"markdown","source":"### 5.1.7 Histogram gradient boosting","metadata":{}},{"cell_type":"code","source":"estimator = clone(hist_gradient_boosting_model)","metadata":{"execution":{"iopub.status.busy":"2021-06-11T17:45:25.918822Z","iopub.execute_input":"2021-06-11T17:45:25.919338Z","iopub.status.idle":"2021-06-11T17:45:25.927906Z","shell.execute_reply.started":"2021-06-11T17:45:25.919286Z","shell.execute_reply":"2021-06-11T17:45:25.925893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Pipeline","metadata":{}},{"cell_type":"code","source":"pipeline_Winsconsin=make_pipeline(preproceserWinsconsin,estimator)","metadata":{"execution":{"iopub.status.busy":"2021-06-11T17:45:25.930559Z","iopub.execute_input":"2021-06-11T17:45:25.931232Z","iopub.status.idle":"2021-06-11T17:45:25.949266Z","shell.execute_reply.started":"2021-06-11T17:45:25.931174Z","shell.execute_reply":"2021-06-11T17:45:25.947806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Este estimador es mucho mas rapido que Gradiante Boosting pero se utiliza normalmente para grandes conjuntos de datos mayores a 10000.\nLos hiperparametros de Gradient boosting: \n- **max_iter** número máximo de iteraciones del algoritmo. \n- **learning_rate** vuelve a ser el mismo que en Gradient Boosting sin optimizar \n- **min_samples_leaf** se recomienda utilizar para conjuntos de datos pequeños.","metadata":{}},{"cell_type":"code","source":"\nmax_iter= [10,50,100]\nlearning_rate = [0.01, 0.05, 0.1]\nmin_samples_leaf=[10,15,20]\n\n\nhist_gradient_boosting_clf_Wisconsin = utils.optimize_params(pipeline_Winsconsin,\n                                                   X_Wisconsin_train, y_Wisconsin_train, cv,\n                                                          histgradientboostingclassifier__max_iter= max_iter,\n                                                   histgradientboostingclassifier__learning_rate=learning_rate,\n                                                   histgradientboostingclassifier__min_samples_leaf=min_samples_leaf,scoring=['recall'],refit='recall')","metadata":{"execution":{"iopub.status.busy":"2021-06-11T17:45:25.951739Z","iopub.execute_input":"2021-06-11T17:45:25.952363Z","iopub.status.idle":"2021-06-11T17:47:55.602881Z","shell.execute_reply.started":"2021-06-11T17:45:25.952307Z","shell.execute_reply":"2021-06-11T17:47:55.601797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Histogram gradient boosting da mejores resultado que el Gradiang Boosting, coinciden en sus resultado con learning_rate una tasa mayor da mejores resultados para este conjunto de datos, una iteracion media de 50 tiene mejores resultado que una alta 100 seguidos de una muy pequeña 10.","metadata":{}},{"cell_type":"markdown","source":"## 5.2 Pima Indian Diabetes","metadata":{}},{"cell_type":"markdown","source":"Ya no comentaremos sobre los hiperparametros ya que en Wisconsin se comento sobre como ajustarlos y estos los utilizaremos en los otras database, asi que nos centraremos en los resultados y que hiperparametros dan el mas optimo.","metadata":{}},{"cell_type":"markdown","source":"### Pipeline anterior practica - preprocesamiento","metadata":{}},{"cell_type":"code","source":"#Variables para sustituir ceros\nfeatures1 = 'Glucose|BloodPressure'\nfeatures2 = 'BMI'\n#Variables que no hay que tocar\nfeatures3 = 'DiabetesPedigreeFunction|Age'\n\n#Estimador para integuers\nreplace0Integer_Estimator = make_pipeline(SimpleImputer(strategy=\"median\",missing_values=0 ))\n#Estimador para reales\nreplace0Float_Estimator = make_pipeline(SimpleImputer(strategy=\"mean\",missing_values=0 ))\n\npreproceserDiabetes = make_column_transformer(\n    (replace0Integer_Estimator, make_column_selector(pattern= features1)),\n    (replace0Float_Estimator, make_column_selector(pattern= features2)),\n     ('passthrough', make_column_selector(pattern= features3)))","metadata":{"execution":{"iopub.status.busy":"2021-06-11T17:47:55.604928Z","iopub.execute_input":"2021-06-11T17:47:55.605675Z","iopub.status.idle":"2021-06-11T17:47:55.616964Z","shell.execute_reply.started":"2021-06-11T17:47:55.605618Z","shell.execute_reply":"2021-06-11T17:47:55.615656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 5.2.1 K Vecinos mas cercanos","metadata":{}},{"cell_type":"code","source":"estimator = clone(k_neighbors_model)","metadata":{"execution":{"iopub.status.busy":"2021-06-11T17:47:55.619189Z","iopub.execute_input":"2021-06-11T17:47:55.619986Z","iopub.status.idle":"2021-06-11T17:47:55.640848Z","shell.execute_reply.started":"2021-06-11T17:47:55.619935Z","shell.execute_reply":"2021-06-11T17:47:55.639165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Normalizar","metadata":{}},{"cell_type":"code","source":"transformer = Normalizer()","metadata":{"execution":{"iopub.status.busy":"2021-06-11T17:47:55.643146Z","iopub.execute_input":"2021-06-11T17:47:55.644135Z","iopub.status.idle":"2021-06-11T17:47:55.657035Z","shell.execute_reply.started":"2021-06-11T17:47:55.644078Z","shell.execute_reply":"2021-06-11T17:47:55.65579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Discretizar\n\nComo se mencionado discretizar parace funcionar mejor con KNN y Arboles de decision.","metadata":{}},{"cell_type":"code","source":"discretizer = KBinsDiscretizer()","metadata":{"execution":{"iopub.status.busy":"2021-06-11T17:47:55.658831Z","iopub.execute_input":"2021-06-11T17:47:55.660132Z","iopub.status.idle":"2021-06-11T17:47:55.672008Z","shell.execute_reply.started":"2021-06-11T17:47:55.660069Z","shell.execute_reply":"2021-06-11T17:47:55.670455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Pipeline","metadata":{}},{"cell_type":"code","source":"pipeline_Diabetes=make_pipeline(preproceserDiabetes,transformer,discretizer,estimator)","metadata":{"execution":{"iopub.status.busy":"2021-06-11T17:47:55.673482Z","iopub.execute_input":"2021-06-11T17:47:55.673884Z","iopub.status.idle":"2021-06-11T17:47:55.686682Z","shell.execute_reply.started":"2021-06-11T17:47:55.673838Z","shell.execute_reply":"2021-06-11T17:47:55.685477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nweights = [\"uniform\", \"distance\"]\nn_neighbors = [9,11, 13, 15,17, 21,23,25,27,29]\n\nk_neighbors_clf_Diabetes = utils.optimize_params(pipeline_Diabetes,\n                                        X_train_Diabetes, y_train_Diabetes, cv,\n                                        kneighborsclassifier__weights=weights,\n                                                 kbinsdiscretizer__n_bins= [2,3,4],\n                                        kbinsdiscretizer__strategy= [\"uniform\", \"quantile\", \"kmeans\"],\n                                        kneighborsclassifier__n_neighbors=n_neighbors,scoring=['recall'],refit='recall')","metadata":{"execution":{"iopub.status.busy":"2021-06-11T17:47:55.688042Z","iopub.execute_input":"2021-06-11T17:47:55.688437Z","iopub.status.idle":"2021-06-11T17:52:29.450172Z","shell.execute_reply.started":"2021-06-11T17:47:55.688395Z","shell.execute_reply":"2021-06-11T17:52:29.4492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Podemos observar que en esta conjunto de datos tiene una mayor relacion weight, los  mejores resultados son uniform, con vecinos de 11 y 15 respectivamente, pero este orden para en los siguientes que son distance como mejor hiperparametro, podemos decir que solo en casos especificos uniform da mejores resultados que distance. En cuanto al numero de vecinos no podemos hallar una relacion ya que aunque 11 es el mejor resultado podemos observar que 9 esta en el ultimo.","metadata":{}},{"cell_type":"markdown","source":"Podemos observa que los mejores resultados son a una mayor cantidad de vecinos, ademas de que el mejor resultado utiliza uniform a diferencia de Wisconsin.","metadata":{}},{"cell_type":"markdown","source":"### 5.2.2 Arboles de decision","metadata":{}},{"cell_type":"code","source":"estimator = clone(decision_tree_model)","metadata":{"execution":{"iopub.status.busy":"2021-06-11T17:52:29.452701Z","iopub.execute_input":"2021-06-11T17:52:29.453087Z","iopub.status.idle":"2021-06-11T17:52:29.459642Z","shell.execute_reply.started":"2021-06-11T17:52:29.453049Z","shell.execute_reply":"2021-06-11T17:52:29.45867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Discretizar\n\nComo se mencionado discretizar parace funcionar mejor con KNN y Arboles de decision.","metadata":{}},{"cell_type":"code","source":"discretizer = KBinsDiscretizer()","metadata":{"execution":{"iopub.status.busy":"2021-06-11T17:52:29.461371Z","iopub.execute_input":"2021-06-11T17:52:29.461693Z","iopub.status.idle":"2021-06-11T17:52:29.478487Z","shell.execute_reply.started":"2021-06-11T17:52:29.46166Z","shell.execute_reply":"2021-06-11T17:52:29.477166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Pipeline","metadata":{}},{"cell_type":"code","source":"pipeline_Diabetes=make_pipeline(preproceserDiabetes,discretizer,estimator)","metadata":{"execution":{"iopub.status.busy":"2021-06-11T17:52:29.480344Z","iopub.execute_input":"2021-06-11T17:52:29.480698Z","iopub.status.idle":"2021-06-11T17:52:29.494242Z","shell.execute_reply.started":"2021-06-11T17:52:29.48066Z","shell.execute_reply":"2021-06-11T17:52:29.49319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"criterion = [\"gini\", \"entropy\"]\nmax_depth = [ 2, 4, 6, 8,10,12]\nccp_alpha = [0.0, 0.1, 0.2, 0.3, 0.4]\n\ndecision_tree_clf_Diabetes = utils.optimize_params(pipeline_Diabetes,\n                                          X_train_Diabetes, y_train_Diabetes, cv,\n                                          decisiontreeclassifier__criterion=criterion,\n                                          decisiontreeclassifier__max_depth=max_depth,\n                                          decisiontreeclassifier__ccp_alpha=ccp_alpha,\n                                                   kbinsdiscretizer__n_bins= [2,3,4],\n                                        kbinsdiscretizer__strategy= [\"uniform\", \"quantile\", \"kmeans\"],\n                                                  scoring=['recall'],refit='recall')","metadata":{"execution":{"iopub.status.busy":"2021-06-11T17:52:29.495563Z","iopub.execute_input":"2021-06-11T17:52:29.495932Z","iopub.status.idle":"2021-06-11T18:01:40.763124Z","shell.execute_reply.started":"2021-06-11T17:52:29.495892Z","shell.execute_reply":"2021-06-11T18:01:40.762123Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"En este caso podemos decir que nuestros hiperparametros de ccp_alpha de 0.1 dan los mejores resultados y utilizando la entropia como criterio. Por el lado de maxima profundidad no podemos realizar una mejor descripcion ya que empatan los 5 profundidades que hemos introducido.\n\nPor otro lado la discretizacion da mejores resultado con kmeans y numero intermedio de bins  3, de las cinco mejores configuraciones que podemos observar.","metadata":{}},{"cell_type":"markdown","source":"### 5.2.3 Adaboost","metadata":{}},{"cell_type":"code","source":"estimator = clone(adaboost_model)","metadata":{"execution":{"iopub.status.busy":"2021-06-11T18:01:40.764687Z","iopub.execute_input":"2021-06-11T18:01:40.76529Z","iopub.status.idle":"2021-06-11T18:01:40.770608Z","shell.execute_reply.started":"2021-06-11T18:01:40.765227Z","shell.execute_reply":"2021-06-11T18:01:40.769629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Pipeline","metadata":{}},{"cell_type":"code","source":"pipeline_Diabetes=make_pipeline(preproceserDiabetes,estimator)","metadata":{"execution":{"iopub.status.busy":"2021-06-11T18:01:40.772008Z","iopub.execute_input":"2021-06-11T18:01:40.772552Z","iopub.status.idle":"2021-06-11T18:01:40.786457Z","shell.execute_reply.started":"2021-06-11T18:01:40.772508Z","shell.execute_reply":"2021-06-11T18:01:40.785101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\nbase_estimator = DecisionTreeClassifier(random_state=random_state)\n\nbase_estimator = [base_estimator]\nn_estimators= [50,60,70]\nlearning_rate = [0.0001, 0.1,0.3, 1.0]\ncriterion = [\"gini\", \"entropy\"]\nmax_depth = [ 2, 4]\nccp_alpha = [0.0, 0.1, 0.2]\n\nadaboost_clf_Diabetes = utils.optimize_params(pipeline_Diabetes,\n                                     X_train_Diabetes, y_train_Diabetes, cv,\n                                     adaboostclassifier__base_estimator=base_estimator,\n                                     adaboostclassifier__learning_rate=learning_rate,\n                                     adaboostclassifier__n_estimators=n_estimators,\n                                     adaboostclassifier__base_estimator__criterion=criterion,\n                                     adaboostclassifier__base_estimator__max_depth=max_depth,\n                                     adaboostclassifier__base_estimator__ccp_alpha=ccp_alpha,scoring=['recall'],refit='recall')","metadata":{"execution":{"iopub.status.busy":"2021-06-11T18:01:40.787903Z","iopub.execute_input":"2021-06-11T18:01:40.788503Z","iopub.status.idle":"2021-06-11T18:13:30.091571Z","shell.execute_reply.started":"2021-06-11T18:01:40.788461Z","shell.execute_reply":"2021-06-11T18:13:30.090365Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Observaremos los datos que nos aporta GridSearchCV de la misma manera que con el otro conjunto de datos, en este caso ccp_alpha a 0 da los mejores resultados en todas las configuraciones, en criterion no podemos establecer una relacion clara pero parece ser el mejor gini en los dos casos de mejor configuracion, pero tambien esta entre los peores, la profundidad parece que coinciden en que la 4 en todas las mejores configuraciones.\n\nPor el lado de los hiperparametros de Adaboost, el numero de estimadores no aporta mucha información pero la tasa de aprendizaje si coincide en la menor numero mejor configuracion.\n\nEn conclusion la mejores configuraciones son cuando el arbol de decision tiene ccp_alpha a 0 , profundidad maxima de 4, y por el lado de adaboost la tasa de aprendizaje a 0.0001\n","metadata":{}},{"cell_type":"markdown","source":"### 5.2.4 Bagging","metadata":{}},{"cell_type":"code","source":"estimator = clone(bagging_model)","metadata":{"execution":{"iopub.status.busy":"2021-06-11T18:13:30.093317Z","iopub.execute_input":"2021-06-11T18:13:30.093887Z","iopub.status.idle":"2021-06-11T18:13:30.099982Z","shell.execute_reply.started":"2021-06-11T18:13:30.09385Z","shell.execute_reply":"2021-06-11T18:13:30.098836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pipeline_Diabetes=make_pipeline(preproceserDiabetes,estimator)","metadata":{"execution":{"iopub.status.busy":"2021-06-11T18:13:30.101899Z","iopub.execute_input":"2021-06-11T18:13:30.102233Z","iopub.status.idle":"2021-06-11T18:13:30.120874Z","shell.execute_reply.started":"2021-06-11T18:13:30.1022Z","shell.execute_reply":"2021-06-11T18:13:30.119104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"base_estimator = clone(decision_tree_model)\nbase_estimator = [base_estimator]\nmax_samples= [0.1,1.1,0.1]\nn_estimators=[10,100]\nmax_features=[1,5,10]\nbootstrap= [True,False]\nbootstrap_features=[True,False]\ncriterion = [\"gini\", \"entropy\"]\n\nbagging_clf_Diabetes = utils.optimize_params(pipeline_Diabetes,\n                                    X_train_Diabetes, y_train_Diabetes, cv,\n                                              baggingclassifier__n_estimators=n_estimators,\n                                              baggingclassifier__max_samples=max_samples,\n                                              baggingclassifier__max_features=max_features,\n                                              baggingclassifier__bootstrap=bootstrap,\n                                              baggingclassifier__bootstrap_features=bootstrap_features,\n                                    baggingclassifier__base_estimator=base_estimator,\n                                    baggingclassifier__base_estimator__criterion=criterion,scoring=['recall'],refit='recall')","metadata":{"execution":{"iopub.status.busy":"2021-06-11T18:13:30.122482Z","iopub.execute_input":"2021-06-11T18:13:30.123014Z","iopub.status.idle":"2021-06-11T18:18:10.117584Z","shell.execute_reply.started":"2021-06-11T18:13:30.122961Z","shell.execute_reply":"2021-06-11T18:18:10.116443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"En este caso los hiperparametros de arbol de decision nos muestras que las configuraciones con mayor recall son criterion igual a entropy aunque tambien vemos casos en que la aparece en el peor configuracion, en cuanto al muestreo parece ser que en ambos casos dan mejores resultados si no lo hay, por ultimo parece que las mejores soluciones son limitando el numero de caracteristicas a 5 el numero de muestras a 0.1 y utilizando el mayor numero de estimadores.","metadata":{}},{"cell_type":"markdown","source":"### 5.2.5 Random Forests","metadata":{}},{"cell_type":"code","source":"estimator = clone(random_forest_model)","metadata":{"execution":{"iopub.status.busy":"2021-06-11T18:18:10.119219Z","iopub.execute_input":"2021-06-11T18:18:10.119779Z","iopub.status.idle":"2021-06-11T18:18:10.126452Z","shell.execute_reply.started":"2021-06-11T18:18:10.119737Z","shell.execute_reply":"2021-06-11T18:18:10.125173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pipeline_Diabetes=make_pipeline(preproceserDiabetes,estimator)","metadata":{"execution":{"iopub.status.busy":"2021-06-11T18:18:10.130003Z","iopub.execute_input":"2021-06-11T18:18:10.130347Z","iopub.status.idle":"2021-06-11T18:18:10.143068Z","shell.execute_reply.started":"2021-06-11T18:18:10.130315Z","shell.execute_reply":"2021-06-11T18:18:10.142055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_estimators=[50,60,70,80,90]\ncriterion = [\"gini\", \"entropy\"]\nmax_features = [\"sqrt\", \"log2\"]\n\nrandom_forest_clf_Diabetes = utils.optimize_params(pipeline_Diabetes,\n                                          X_train_Diabetes, y_train_Diabetes, cv,\n                                          randomforestclassifier__criterion=criterion,\n                                        randomforestclassifier__n_estimators=n_estimators,\n                                          randomforestclassifier__max_features=max_features,scoring=['recall'],refit='recall')","metadata":{"execution":{"iopub.status.busy":"2021-06-11T18:18:10.144521Z","iopub.execute_input":"2021-06-11T18:18:10.145005Z","iopub.status.idle":"2021-06-11T18:19:45.095288Z","shell.execute_reply.started":"2021-06-11T18:18:10.144957Z","shell.execute_reply":"2021-06-11T18:19:45.094334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Coincide con Wisconsin en que la entropy nos da mejores resultados, pero tambien se muestra que los resultados con menor numero de estimadores tienen mas frecuencia en las mejores configuraciones.","metadata":{}},{"cell_type":"markdown","source":"### 5.2.6 Gradient Boosting","metadata":{}},{"cell_type":"code","source":"estimator = clone(gradient_boosting_model)","metadata":{"execution":{"iopub.status.busy":"2021-06-11T18:19:45.097169Z","iopub.execute_input":"2021-06-11T18:19:45.097554Z","iopub.status.idle":"2021-06-11T18:19:45.102177Z","shell.execute_reply.started":"2021-06-11T18:19:45.097516Z","shell.execute_reply":"2021-06-11T18:19:45.101023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pipeline_Diabetes=make_pipeline(preproceserDiabetes,estimator)","metadata":{"execution":{"iopub.status.busy":"2021-06-11T18:19:45.103308Z","iopub.execute_input":"2021-06-11T18:19:45.103886Z","iopub.status.idle":"2021-06-11T18:19:45.118647Z","shell.execute_reply.started":"2021-06-11T18:19:45.103851Z","shell.execute_reply":"2021-06-11T18:19:45.117425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_estimators= [50,60,70]\nlearning_rate = [0.01, 0.05, 0.1]\ncriterion = [\"friedman_mse\", \"mse\"]\nmax_depth = [1, 2, 3]\nccp_alpha = [0.0, 0.1]\n\ngradient_boosting_clf_Diabetes = utils.optimize_params(pipeline_Diabetes,\n                                              X_train_Diabetes, y_train_Diabetes, cv,\n                                              gradientboostingclassifier__learning_rate=learning_rate,\n                                              gradientboostingclassifier__criterion=criterion,\n                                              gradientboostingclassifier__max_depth=max_depth,\n                                              gradientboostingclassifier__ccp_alpha=ccp_alpha,scoring=['recall'],refit='recall')","metadata":{"execution":{"iopub.status.busy":"2021-06-11T18:19:45.120052Z","iopub.execute_input":"2021-06-11T18:19:45.120441Z","iopub.status.idle":"2021-06-11T18:21:18.069762Z","shell.execute_reply.started":"2021-06-11T18:19:45.120405Z","shell.execute_reply":"2021-06-11T18:21:18.06877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Estos casos parecen coincidir con el conjunto de datos de Wisconsin, en lo relativo a la mayor profundidad y una tasa de poda inexistente. En cuanto al criterion parece dar mejores resultados mse que friedman ya que estos son las peores configuracions segun los resultados que vemos, y por ultimo la tasa de aprendizaje parecen coincidir en 0.05 para las mejores configuraciones.","metadata":{}},{"cell_type":"markdown","source":"### 5.2.7 Histogram Gradient Boosting","metadata":{}},{"cell_type":"code","source":"estimator = clone(hist_gradient_boosting_model)","metadata":{"execution":{"iopub.status.busy":"2021-06-11T18:21:18.071033Z","iopub.execute_input":"2021-06-11T18:21:18.071542Z","iopub.status.idle":"2021-06-11T18:21:18.076318Z","shell.execute_reply.started":"2021-06-11T18:21:18.071504Z","shell.execute_reply":"2021-06-11T18:21:18.075477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pipeline_Diabetes=make_pipeline(preproceserDiabetes,estimator)","metadata":{"execution":{"iopub.status.busy":"2021-06-11T18:21:18.077486Z","iopub.execute_input":"2021-06-11T18:21:18.07792Z","iopub.status.idle":"2021-06-11T18:21:18.097276Z","shell.execute_reply.started":"2021-06-11T18:21:18.077887Z","shell.execute_reply":"2021-06-11T18:21:18.096305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_iter= [10,50,100]\nlearning_rate = [0.01, 0.05, 0.1]\nmin_samples_leaf=[10,15,20]\n\n\nhist_gradient_boosting_clf_Diabetes = utils.optimize_params(pipeline_Diabetes,\n                                                   X_train_Diabetes, y_train_Diabetes, cv,\n                                                          histgradientboostingclassifier__max_iter= max_iter,\n                                                   histgradientboostingclassifier__learning_rate=learning_rate,\n                                                   histgradientboostingclassifier__min_samples_leaf=min_samples_leaf,scoring=['recall'],refit='recall')","metadata":{"execution":{"iopub.status.busy":"2021-06-11T18:21:18.098588Z","iopub.execute_input":"2021-06-11T18:21:18.099099Z","iopub.status.idle":"2021-06-11T18:23:19.692101Z","shell.execute_reply.started":"2021-06-11T18:21:18.09906Z","shell.execute_reply":"2021-06-11T18:23:19.691082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"En este caso parece ser que histograma da peores resultados de Gradiant Boosting, respecto a los hiperparametros la tasa de aprendizaje da mejores resultados utilizando la mayor, en cuanto al maximo numero de iteraciones  coincide con Wisconsin que da mejores resultados una mayor que una muy pequeña.","metadata":{}},{"cell_type":"markdown","source":"# 6. Construcción y validación del modelo final","metadata":{}},{"cell_type":"markdown","source":"Una vez obtenido los mejores estimadores para los distintos modelos que se consideran para el problema vamos a evaluar y considerar cual de ellos es el mejor para las bases de estudios que se están analizando utilizando los datos de test ya que hemos utilizado los datos de entrenamiento para generar dichos modelos:","metadata":{}},{"cell_type":"markdown","source":"## 6.1 Pima Indian Diabetes","metadata":{}},{"cell_type":"code","source":"estimators_Diabetes = {\n    \"Nearest neighbors\": k_neighbors_clf_Diabetes,\n    \"Decision tree\": decision_tree_clf_Diabetes,\n    \"AdaBoost\": adaboost_clf_Diabetes,\n    \"Bagging\": bagging_clf_Diabetes,\n    \"Random Forests\": random_forest_clf_Diabetes,\n    \"Gradient Boosting\": gradient_boosting_clf_Diabetes,\n    \"Histogram Gradient Boosting\": hist_gradient_boosting_clf_Diabetes\n}","metadata":{"execution":{"iopub.status.busy":"2021-06-11T18:23:19.697546Z","iopub.execute_input":"2021-06-11T18:23:19.700388Z","iopub.status.idle":"2021-06-11T18:23:19.709172Z","shell.execute_reply.started":"2021-06-11T18:23:19.700311Z","shell.execute_reply":"2021-06-11T18:23:19.708134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_Diabetes = X_test_Diabetes\ny_Diabetes = y_test_Diabetes","metadata":{"execution":{"iopub.status.busy":"2021-06-11T18:23:19.714231Z","iopub.execute_input":"2021-06-11T18:23:19.718119Z","iopub.status.idle":"2021-06-11T18:23:19.726913Z","shell.execute_reply.started":"2021-06-11T18:23:19.718019Z","shell.execute_reply":"2021-06-11T18:23:19.725835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"utils.evaluate_estimators(estimators_Diabetes, X_Diabetes, y_Diabetes)","metadata":{"execution":{"iopub.status.busy":"2021-06-11T18:23:19.731067Z","iopub.execute_input":"2021-06-11T18:23:19.735729Z","iopub.status.idle":"2021-06-11T18:23:19.925905Z","shell.execute_reply.started":"2021-06-11T18:23:19.735645Z","shell.execute_reply":"2021-06-11T18:23:19.924326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Viendo este análisis podemos concluir que segun el recall los modelos que mejor resultado dan los Arboles decision y Adaboost. Siendo estos el mismo resultado. Y el peor resultado que obtenemos es KNN.","metadata":{}},{"cell_type":"markdown","source":"## 6.2 Breast Cancer Wisconsin","metadata":{}},{"cell_type":"code","source":"estimators_Wisconsin = {\n    \"Nearest neighbors\": k_neighbors_clf_Wisconsin,\n    \"Decision tree\": decision_tree_clf_Wisconsin,\n    \"AdaBoost\": adaboost_clf_Wisconsin,\n    \"Bagging\": bagging_clf_Wisconsin,\n    \"Random Forests\": random_forest_clf_Wisconsin,\n    \"Gradient Boosting\": gradient_boosting_clf_Wisconsin,\n    \"Histogram Gradient Boosting\": hist_gradient_boosting_clf_Wisconsin\n}","metadata":{"execution":{"iopub.status.busy":"2021-06-11T18:23:19.927743Z","iopub.execute_input":"2021-06-11T18:23:19.928557Z","iopub.status.idle":"2021-06-11T18:23:19.935452Z","shell.execute_reply.started":"2021-06-11T18:23:19.928507Z","shell.execute_reply":"2021-06-11T18:23:19.934397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_Wisconsin = X_Wisconsin_test\ny_Wisconsin = y_Wisconsin_test","metadata":{"execution":{"iopub.status.busy":"2021-06-11T18:23:19.937308Z","iopub.execute_input":"2021-06-11T18:23:19.938035Z","iopub.status.idle":"2021-06-11T18:23:19.960259Z","shell.execute_reply.started":"2021-06-11T18:23:19.937982Z","shell.execute_reply":"2021-06-11T18:23:19.958487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"utils.evaluate_estimators(estimators_Wisconsin, X_Wisconsin, y_Wisconsin)","metadata":{"execution":{"iopub.status.busy":"2021-06-11T18:23:19.967129Z","iopub.execute_input":"2021-06-11T18:23:19.967759Z","iopub.status.idle":"2021-06-11T18:23:20.133189Z","shell.execute_reply.started":"2021-06-11T18:23:19.967716Z","shell.execute_reply":"2021-06-11T18:23:20.132227Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Viendo este análisis podemos concluir que segun el recall los modelos que mejor resultado dan los arboles decision y el siguiente Histogram Gradient Boosting y Gradient Boosting. Hay que tener en cuenta que en los primeros ejemplos hemos utilizado discretizacion aumentando asi nuestro resultados y tambien confirmamos que Histogram Gradient Boosting dan mejores resultados que Gradient Boosting. Y siendo Bagging el peor resultado obtenido.","metadata":{}}]}