{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Coronary Artery Disease (CAD)\n\nCoronary artery disease is the narrowing or blockage of the coronary arteries. This condition is usually caused by atherosclerosis. Atherosclerosis is the build-up of cholesterol and fatty deposits (called plaques) inside the arteries.\n\nThis notebook analysis the CAD dataset and aims to predict the disease using the 13 parameters given in the dataset.\nDivided into 3 parts:\n1. EDA\n2. Feature Selection\n3. Model building"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom IPython.display import display_html\nimport statsmodels.formula.api as smf\nimport statsmodels.api as sm\nimport matplotlib.gridspec as gridspec\nfrom sklearn import model_selection\nfrom sklearn import metrics\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import classification_report, accuracy_score\n\nimport matplotlib.gridspec as gridspec\n\nblue_red = ['#74a09e','#86c1b2','#98e2c6','#f3c969','#f2a553', '#d96548', '#c14953']\nsns.palplot(sns.color_palette(blue_red))\n\n# Set Style\nsns.set_style(\"whitegrid\")\nsns.despine(left=True, bottom=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/coronary-artery-disease/Coronary_artery.csv')\nprint(print('Features:{}'.format(df.columns.tolist())))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Dataset:\nThe dataset contains 13 independent features and 1 dependent feature (class)"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df.shape)\nprint(df.isnull().values.any())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2 = pd.read_csv('../input/coronary-artery-disease/data.csv')\ndf2.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Treatment of values with a question mark sign"},{"metadata":{"trusted":true},"cell_type":"code","source":"df2 = df2[~df2.isin(['?'])]\ndf2 = df2.dropna(axis=0)\ndf2.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"correlation_mat = df2.corr()\nplt.figure(figsize=(20,20))\nax=sns.heatmap(correlation_mat, annot = True)\nplt.show(ax)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(df2, height = 1.5, palette = 'rocket')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"black_red = [\n    '#1A1A1D', '#4E4E50', '#C5C6C7', '#6F2232', '#950740', '#C3073F'\n]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploratory Data Analysis"},{"metadata":{},"cell_type":"markdown","source":"The graphs below show the exact distribution of disease at various stages. 0,1,2,3 and 4 with respect to categorical features. These count plots show the distribution of stage of disease with respect to categorical features Gender, Chest pain Type, Resting ECG results and Defect Type respectively. We can analyse that the patients with Asymptomatic chest pain have a higher chance of suffering from the disease and least chance with Typical Angimal Pain. Also, the patient with ECG report of Left Ventricular Hypertrophy has a higher chance of disease.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(constrained_layout = True, figsize = (25,12))\n\n#create grid\n\ngrid = gridspec.GridSpec(ncols = 4, nrows = 2, figure = fig)\n\nax1 = fig.add_subplot(grid[0, :2])\nax1.set_title('Gender Distribution')\n\n\nsns.countplot(df['sex'],\n             alpha = 0.9,\n             hue = df['class'],\n             ax = ax1,\n             palette = 'rocket',\n             order=df['sex'].value_counts().index)\n\nax1.legend()\nplt.xticks(fontsize = 14)\n\nax2 = fig.add_subplot(grid[0, 2:])\nax2.set_title('Chest Pain Distribution')\nsns.countplot(df['cp'],\n             alpha = 0.9,\n             hue = df['class'],\n             ax = ax2, \n             palette = 'rocket',\n             order=df['cp'].value_counts().index)\nax2.legend()\nplt.xticks( fontsize = 14)\n\nax3 = fig.add_subplot(grid[1, :2])\nax3.set_title('Resting Electrographic Results Distribution')\nsns.countplot(df['restecg'],\n             alpha = 0.9,\n             hue = df['class'],\n             ax = ax3, \n             palette = 'rocket',\n             order=df['restecg'].value_counts().index)\nax3.legend()\nplt.xticks(fontsize = 14)\n\nax4 = fig.add_subplot(grid[1, 2:])\nax4.set_title('Defect Type Distribution')\nsns.countplot(df['thal'],\n             alpha = 0.9,\n             hue = df['class'],\n             ax = ax4, \n             palette = 'rocket',\n             order=df['thal'].value_counts().index)\nax4.legend()\nplt.xticks(fontsize = 14)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Almost all the persons are in the range of 40 to 70 years of age. Maximum Patients have an age of 55 to 58. All patients have cholesterol level below 300 and maximum have around 220 mm/dL.  A majority is having blood sugar in between 120 and 140 mg/dL. The distribution for Maximum Heart Rate Achieved shows that the majority of patients have a max. Heart rate between 150 and 160.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(constrained_layout = True, figsize = (15,9))\n\n#create grid\n\ngrid = gridspec.GridSpec(ncols = 1, nrows = 4, figure = fig)\nax1 = fig.add_subplot(grid[0, :])\n\nsns.distplot(df.age, ax = ax1, color = blue_red[1])\nax1.set_title('Age Distribution')\n\nax2 = fig.add_subplot(grid[1, :])\nsns.distplot(df.chol, ax = ax2, color = blue_red[2])\nax2.set_title('Cholestrol Distribution')\n\n\nax3 = fig.add_subplot(grid[2, :])\nsns.distplot(df.trestbps, ax = ax3, color = blue_red[3])\nax3.set_title('Resting Blood Sugar Distribution')\n\nax4 = fig.add_subplot(grid[3, :])\nsns.distplot(df.thalach, ax = ax4, color = blue_red[4])\nax4.set_title('Maximum Heart Rate Distribution')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The sunburst chart shows us the distribution of disease stages with respect to gender (male or female) and chest pain type (Typical Angima, Atypical Angima, Nonangimal Pain and Asymptomatic Pain)"},{"metadata":{"trusted":true},"cell_type":"code","source":"import plotly.express as px\nfig = px.sunburst(data_frame = df,\n                 path = [ 'sex','class','cp'],\n                 color = 'class',\n                 maxdepth = -1,\n                 title = 'Sunburst Chart SmokingStatus > Gender > Age')\nfig.update_traces(textinfo = 'label+percent parent')\nfig.update_layout(margin=dict(t=0, l=0, r=0, b=0))\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df2.sex.unique())\nprint(df2.cp.unique())\nprint(df2.fbs.unique())\nprint(df2.restecg.unique())\nprint(df2.exang.unique())\nprint(df2.slope.unique())\nprint(df2.ca.unique())\nprint(df2.thal.unique())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Selection"},{"metadata":{},"cell_type":"markdown","source":"The features were selected based on the p values extracted from the GLM (Generalised Linear Model). All those features with p-value < 0.05 were taken for further analysis and model building. Comparing the p-values < 0.05, 7 features namely ca, cp, restecg, thalach, oldpeak, slope, thal were selected for further processing and building model for prediction of disease.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_new = df2.rename(columns={'class': 'label'})\nformula = 'label ~ age+sex+cp+trestbps+chol+fbs+restecg+thalach+exang+oldpeak+slope+ca+thal'\nresult = smf.glm(formula = formula, data=df_new).fit()\nprint(result.summary())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"These 7 features were then encoded and a binary column for each category of a distinct feature was made by building dummies (One Hot Encoding). This will further help in the successful development of the model where all categorical features will become binary and thus the model will learn more effectively."},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df_new[['cp', 'restecg', 'thalach', 'oldpeak', 'slope', 'ca', 'thal']]\nY = df_new['label']\nX = pd.get_dummies(X, columns=['cp', 'restecg', 'slope', 'ca', 'thal'])\nX.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = model_selection.train_test_split(X, Y, test_size=0.20, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Decision Tree\nDecision Trees are a type of Supervised Machine Learning where the data is continuously split according to a certain parameter."},{"metadata":{"trusted":true},"cell_type":"code","source":"tree_model = DecisionTreeClassifier(max_depth = 25)\ntree_model.fit(X_train, y_train)\ny_pred_tree = tree_model.predict(X_test)\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.metrics import accuracy_score\ny_pred_train_tree = tree_model.predict(X_train)\ntree_confusion = metrics.confusion_matrix(y_train, y_pred_train_tree)\nprint('Confusion Matrix for Train:\\n{}'.format(tree_confusion))\nacc=accuracy_score(y_train, y_pred_train_tree) \nprint('Train case accuracy is :'+ format(acc))\nprint('\\n')\ny_pred_test_tree = tree_model.predict(X_test)\ntree_confusion = metrics.confusion_matrix(y_test, y_pred_test_tree)\nprint('Confusion Matrix for Test:\\n{}'.format(tree_confusion))\nacc= accuracy_score(y_test, y_pred_test_tree)\nprint('Test case accuracy is :'+ format(acc))\nprint('\\n')\nprint(classification_report(y_test, y_pred_test_tree))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_prob_tree = tree_model.predict_proba(X_test)\nimport scikitplot as skplt\nskplt.metrics.plot_roc(y_test, y_pred_prob_tree, figsize = (10, 10))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Random Forest\n\nRandom forest is a flexible, easy to use machine learning algorithm that produces, even without hyper-parameter tuning, a great result most of the time. It is also one of the most used algorithms, because of its simplicity and diversity (it can be used for both classification and regression tasks). \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n# Feature Scaling\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n# Fitting Random Forest Classification to the Training set\nclassifier = RandomForestClassifier(n_estimators = 15, criterion = 'entropy', random_state = 42)\nclassifier.fit(X_train, y_train)\n\ny_pred_train = classifier.predict(X_train)\n\n# Making the Confusion Matrix\nrandom_confusion = metrics.confusion_matrix(y_train, y_pred_train)\nprint('Confusion Matrix for train:\\n{}'.format(random_confusion))\nacc=accuracy_score(y_train, y_pred_train)\nprint('Train case accuracy is :'+ format(acc))\nprint('\\n')\ny_pred = classifier.predict(X_test)\n\n# Making the Confusion Matrix\nrandom_confusion = metrics.confusion_matrix(y_test, y_pred)\nprint('Confusion Matrix for test:\\n{}'.format(random_confusion))\nacc=accuracy_score(y_test, y_pred)\nprint('Test case accuracy is :'+ format(acc))\nprint('\\n')\nprint(classification_report(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_prob_rf = classifier.predict_proba(X_test)\nskplt.metrics.plot_roc(y_test, y_pred_prob_rf, figsize = (10, 10))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# SVM\n\n“Support Vector Machine” (SVM) is a supervised machine learning algorithm which can be used for both classification or regression challenges. However,  it is mostly used in classification problems. In the SVM algorithm, we plot each data item as a point in n-dimensional space (where n is the number of features you have) with the value of each feature being the value of a particular coordinate. Then, we perform classification by finding the hyper-plane that differentiates the two classes very well\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC \nsvm_model_linear = SVC(decision_function_shape='ovo',kernel = 'poly', C = 1, probability = True).fit(X_train, y_train) \n#kernel='poly' gives best result\n\nsvm_predictions_train = svm_model_linear.predict(X_train) \ncm1 = metrics.confusion_matrix(y_train, svm_predictions_train) \nacc1=accuracy_score(y_train, svm_predictions_train)\nprint('Confusion Matrix for train:\\n{}'.format(cm1))\nprint('Train case accuracy is :'+ format(acc1))\nprint('\\n')\nsvm_predictions = svm_model_linear.predict(X_test) \n# creating a confusion matrix  and finding accuracy\ncm = metrics.confusion_matrix(y_test, svm_predictions) \nacc=accuracy_score(y_test, svm_predictions)\nprint('Confusion Matrix for test:\\n{}'.format(cm))\nprint('Test case accuracy is :'+ format(acc))\nprint('\\n')\nprint(classification_report(y_test, svm_predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_prob_svm = svm_model_linear.predict_proba(X_test)\nskplt.metrics.plot_roc(y_test, y_pred_prob_svm, figsize = (10, 10))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# KNN\n\nK-Nearest Neighbour is one of the simplest Machine Learning algorithms based on Supervised Learning technique. K-NN algorithm assumes the similarity between the new case/data and available cases and put the new case into the category that is most similar to the available categories. K-NN algorithm stores all the available data and classifies a new data point based on the similarity. This means when new data appears then it can be easily classified into a good suite category by using K- NN algorithm\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# training a KNN classifier \nfrom sklearn.neighbors import KNeighborsClassifier \nknn = KNeighborsClassifier(n_neighbors = 7).fit(X_train, y_train) \nknn_predictions_train = knn.predict(X_train) \ncm1 = metrics.confusion_matrix(y_train, knn_predictions_train) \nacc1=accuracy_score(y_train, knn_predictions_train)\nprint('Confusion Matrix for train:\\n{}'.format(cm1))\nprint('Train case accuracy is :'+ format(acc1))\nprint('\\n')\nknn_predictions = knn.predict(X_test) \n# creating a confusion matrix  and finding accuracy\ncm = metrics.confusion_matrix(y_test, knn_predictions) \nacc=accuracy_score(y_test, knn_predictions)\nprint('Confusion Matrix for test:\\n{}'.format(cm))\nprint('Test case accuracy is :'+ format(acc))\nprint('\\n')\nprint(classification_report(y_test, knn_predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_prob_knn = knn.predict_proba(X_test)\nskplt.metrics.plot_roc(y_test, y_pred_prob_knn, figsize = (10, 10))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Logistic Regression\n\nLogistic regression measures the relationship between the categorical dependent variable and one or more independent variables by estimating probabilities using a logistic function, which is the cumulative distribution function of the logistic distribution\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\n# instantiate model\nlogreg = LogisticRegression()\n\n# fit model\nlogreg.fit(X_train, y_train)\ny_pred_train = logreg.predict(X_train)\ny_pred_class = logreg.predict(X_test)\nconfusion1 = metrics.confusion_matrix(y_train, y_pred_train)\nacc=accuracy_score(y_train, y_pred_train)\nconfusion = metrics.confusion_matrix(y_test, y_pred_class)\nacc1=accuracy_score(y_test, y_pred_class)\nprint(confusion1)\nprint('Train case accuracy is :'+ format(acc))\nprint('\\n')\nprint(confusion)\nprint('Test case accuracy is :'+ format(acc1))\nprint('\\n')\nprint(classification_report(y_test, y_pred_class))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_prob_log = logreg.predict_proba(X_test)\n#plt.figure(figsize = (10, 10))\nskplt.metrics.plot_roc(y_test, y_pred_prob_log, figsize = (10, 10))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Neural Net"},{"metadata":{},"cell_type":"markdown","source":"Architecturally, an artificial neural network is modelled using layers of artificial neurons, or computational units able to receive input and apply an activation function along with a threshold to determine if messages are passed along.\nIn a simple model, the first layer is the input layer, followed by one hidden layer, and lastly by an output layer. Each layer can contain one or more neurons.\nModels can become increasingly complex, and with increased abstraction and problem-solving capabilities by increasing the number of hidden layers, the number of neurons in any given layer, and/or the number of paths between neurons.\nModel architecture and tuning are therefore major components of ANN techniques, in addition to the actual learning algorithms themselves. All of these characteristics of an ANN can have a significant impact on the performance of the model.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"X= df2[['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach','exang', 'oldpeak', 'slope', 'ca', 'thal']]\ny= df2['class']\nX = np.asarray(X).astype('float32')\ny = np.asarray(y).astype('float32')\n# Split the dataset using a 70:30 split\nX_train1, X_test1, y_train1, y_test1 = model_selection.train_test_split(X, y, test_size=0.20, random_state=0)\n\n#Check the shape of each variable, remember the X variable must be in matrix form and the y varibale a vector\nX_train1.shape, y_train1.shape, X_test1.shape, y_test1.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.utils.np_utils import to_categorical\n\nY_train1 = to_categorical(y_train1, num_classes=None)\nY_test1 = to_categorical(y_test1, num_classes=None)\nprint(Y_train1.shape)\nprint(Y_train1[:10])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom keras.models import Sequential\nfrom keras.layers.core import Flatten, Dense, Dropout\nfrom keras.callbacks import EarlyStopping\nfrom keras.optimizers import Adam\nmodel = Sequential()\nmodel.add(Dense(16, input_dim=13, kernel_initializer='normal', activation='relu'))\nmodel.add(Dense(10, kernel_initializer='normal', activation='relu'))\nmodel.add(Dense(8, kernel_initializer='normal', activation='relu'))\nmodel.add(Dense(5, activation='softmax'))\n#compiling model\nadam = Adam(lr = 0.001)\nmodel.compile(loss=\"categorical_crossentropy\", optimizer=adam, metrics=[\"accuracy\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"estop = EarlyStopping(patience=10, mode='min', min_delta=0.001, monitor='val_loss')\n\nhistory = model.fit(X_train1, Y_train1, validation_data=(X_test1, Y_test1), epochs=100, batch_size=10, verbose = 1, callbacks = [estop])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('MODEL ACCURACY')\nplt.ylabel('Accuracy')\nplt.xlabel('No. of epochs')\nplt.legend(['train', 'val'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"probas = model.predict_proba(X_test1, batch_size=10)\nskplt.metrics.plot_roc(y_test1, probas)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Binary NN\n\nOnly the probability of the presence of the disease is predicted by 0 and 1. This model gives the best accuracy so as to predict the presence of disease."},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_train_binary = y_train1.copy()\nY_test_binary = y_test1.copy()\n\nY_train_binary[Y_train_binary > 0] = 1\nY_test_binary[Y_test_binary > 0] = 1\n\nprint(Y_train_binary[:20])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model2 = Sequential()\nmodel2.add(Dense(8, input_dim=13, kernel_initializer='normal', activation='relu'))\nmodel2.add(Dense(4, kernel_initializer='normal', activation='relu'))\nmodel2.add(Dense(1, activation='sigmoid'))\n#compiling model\nadam = Adam(lr = 0.001)\nmodel2.compile(loss=\"binary_crossentropy\", optimizer=adam, metrics=[\"accuracy\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history2 = model2.fit(X_train1, Y_train_binary, validation_data=(X_test1, Y_test_binary), epochs=100, batch_size=10, verbose = 1, callbacks = [estop])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(history2.history['accuracy'])\nplt.plot(history2.history['val_accuracy'])\nplt.title('MODEL ACCURACY')\nplt.ylabel('Accuracy')\nplt.xlabel('No. of epochs')\nplt.legend(['train', 'val'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\ncategorical_pred = np.argmax(model.predict(X_test1), axis=1)\n\nprint('Results for Categorical Model')\nprint(accuracy_score(y_test1, categorical_pred))\nprint(classification_report(y_test1, categorical_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"binary_pred = np.round(model2.predict(X_test1)).astype(int)\n\nprint('Results for Binary Model')\nprint(accuracy_score(Y_test_binary, binary_pred))\nprint(classification_report(Y_test_binary, binary_pred))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}