{"cells":[{"metadata":{},"cell_type":"markdown","source":"# White Wine Quality: Classification"},{"metadata":{},"cell_type":"markdown","source":"Welcome and thanks for opening this notebook! This notebook is excellent for beginners; I use classification machine learning techniques and build a model to predict the white wine quality. This is a 4-way classification analysis. I have built 4 models to predict the quality of white wine, including the following machine learning techniques: random forest classifier, logistic regression, decision tree, and support vector machine learning. I start with some data exploration. Next, I prepare the data for machine learning. Lastly, I create 4 classification models to predict white wine quality."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#import other packages\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"path = '/kaggle/input/white-wine-quality/winequality-white.csv'\n\ndf = pd.read_csv(path,sep=';')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Count the missing values in the dataset\ndf.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are no missing values in the dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"#unique values for quality\ndf.quality.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It is a pretty large dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Divide the quality of wine into good and bad wine\n#Binary classification\nwinequality_names = ['bad', 'good']\ndf['quality'] = pd.cut(df['quality'], bins = (2, 5.5, 9), labels = winequality_names)\ndf.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.quality.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The wine has been classified into *'good'* wine and *'bad'* wine. A wine is a good wine when its score is higher than 5.5; it is a bad wine when its score is lower than 5.5."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Boxplot\nfig, axes = plt.subplots(4, 3, figsize=(20,20))\n\nfig.suptitle(\"White Wine Quality Distribution\")\nsns.boxplot(ax=axes[0, 0], data=df, x='quality', y='fixed acidity')\nsns.boxplot(ax=axes[0, 1], data=df, x='quality', y='volatile acidity')\nsns.boxplot(ax=axes[0, 2], data=df, x='quality', y='citric acid')\nsns.boxplot(ax=axes[1, 0], data=df, x='quality', y='residual sugar')\nsns.boxplot(ax=axes[1, 1], data=df, x='quality', y='chlorides')\nsns.boxplot(ax=axes[1, 2], data=df, x='quality', y='free sulfur dioxide')\nsns.boxplot(ax=axes[2, 0], data=df, x='quality', y='total sulfur dioxide')\nsns.boxplot(ax=axes[2, 1], data=df, x='quality', y='density')\nsns.boxplot(ax=axes[2, 2], data=df, x='quality', y='pH')\nsns.boxplot(ax=axes[3, 0], data=df, x='quality', y='sulphates')\nsns.boxplot(ax=axes[3, 1], data=df, x='quality', y='alcohol')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Make a correlation diagram\ncorr = df.corr()\n\nax = sns.heatmap(corr,vmin=-1, vmax=1, center=0,cmap=sns.diverging_palette(20, 220, n=200),square=True)\n\nax.set_xticklabels(ax.get_xticklabels(),rotation=45,horizontalalignment='right')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data Preprocessing for Machine Learning"},{"metadata":{},"cell_type":"markdown","source":"*Data Normalization*"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's select the independent variables; these are the variables that contribute to wine quality."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Define the independent variables\nFeatures = df[['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar',\n       'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density',\n       'pH', 'sulphates', 'alcohol']]\nX = Features","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, let's define 'quality' as the dependent variable."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Define the dependent variable\ny = df['quality'].values\ny[0:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X= preprocessing.StandardScaler().fit(X).transform(X)\nX[0:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#split the data in a train and test set\nX_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2)\nprint ('The length of the train set is:', X_train.shape,  y_train.shape)\nprint ('The lenth of the test set equals:', X_test.shape,  y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The training dataset is 80% of the total data; the test set is 20% of the total data."},{"metadata":{},"cell_type":"markdown","source":"The data is ready for a classification machine learning analysis. With classification machine learning techniques, we can predict the quality of a specific white wine. "},{"metadata":{},"cell_type":"markdown","source":"### 1.Random Forest Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"#import sklearn package for the random forest classifier\nfrom sklearn.ensemble import RandomForestClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#import score and metric packages\nfrom sklearn import metrics\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import confusion_matrix, classification_report\nimport itertools","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rfc = RandomForestClassifier(n_estimators=100)\nrfc.fit(X_train,y_train)\ny_pred = rfc.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#What is the accuracy of the above model\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Other performance metrics: how did the model perform?\nprint(classification_report(y_test,y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The random forest model gives an overall accuracy of 85%."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Confusion Matrix\ncm = confusion_matrix(y_test,y_pred)\nprint(cm)\n\nsns.heatmap(cm,cbar=False,annot=True,cmap='Blues',fmt=\"d\")\nplt.xlabel(\"y_pred\")\nplt.ylabel(\"y_test\")\nplt.title(\"Confusion Matrix: Random Forest Classifier\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.Logistic Regression"},{"metadata":{},"cell_type":"markdown","source":"Let's find the optimal logistic regression model first. Which model has the highest accuracy score?"},{"metadata":{"trusted":true},"cell_type":"code","source":"#import the logistic regression packages\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import log_loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#liblinear regression\nLR_a = LogisticRegression(C=0.01, solver='liblinear').fit(X_train,y_train)\nyhat_prob_a = LR_a.predict_proba(X_test)\nlog_loss(y_test, yhat_prob_a)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"LR_b = LogisticRegression(C=0.01, solver='saga').fit(X_train,y_train)\nyhat_prob_b = LR_b.predict_proba(X_test)\nlog_loss(y_test, yhat_prob_b)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"LR_c = LogisticRegression(C=0.01, solver='newton-cg').fit(X_train,y_train)\nyhat_prob_c = LR_c.predict_proba(X_test)\nlog_loss(y_test, yhat_prob_c)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"LR_d = LogisticRegression(C=0.01, solver='lbfgs').fit(X_train,y_train)\nyhat_prob_d = LR_d.predict_proba(X_test)\nlog_loss(y_test, yhat_prob_d)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"LR_e = LogisticRegression(C=0.01, solver='sag').fit(X_train,y_train)\nyhat_prob_e = LR_e.predict_proba(X_test)\nlog_loss(y_test, yhat_prob_e)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The differences in accuracy are rounding differences! So, let's pick the model with liblinear logistic regression."},{"metadata":{"trusted":true},"cell_type":"code","source":"LR = LogisticRegression(C=0.01, solver='liblinear').fit(X_train,y_train)\nLR","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#log loss score\nlr_ypred = LR.predict_proba(X_test)\nlog_loss(y_test,lr_ypred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The logistic regression model has a logg loss score of 52%. The closer the logg loss to zero, the higher the accuracy."},{"metadata":{},"cell_type":"markdown","source":"### 3.Decision Tree"},{"metadata":{"trusted":true},"cell_type":"code","source":"#import the decision tree packages \nfrom sklearn.tree import DecisionTreeClassifier","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Find the Optimal Decision Tree Length**"},{"metadata":{},"cell_type":"markdown","source":"*Approach: Grid Search*"},{"metadata":{"trusted":true},"cell_type":"code","source":"decision_tree = DecisionTreeClassifier(criterion=\"entropy\",random_state=42)\ndecision_tree = decision_tree.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"param_grid = {'max_depth':range(1, decision_tree.tree_.max_depth+1, 2),'max_features': range(1, len(decision_tree.feature_importances_)+1)}\n\nwine_gr = GridSearchCV(DecisionTreeClassifier(criterion=\"entropy\",random_state=42),param_grid=param_grid,scoring='accuracy',n_jobs=-1)\n\nwine_gr = wine_gr.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wine_gr.best_estimator_.tree_.node_count, wine_gr.best_estimator_.tree_.max_depth","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The optimal depth includes 24 leaves. "},{"metadata":{},"cell_type":"markdown","source":"**Best Model: Decision Tree**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create Decision Tree classifer object with the optimal depth\nclf = DecisionTreeClassifier(criterion=\"entropy\", random_state=42, max_depth=24)\n\n# Train Decision Tree Classifer\nDT_Model = clf.fit(X_train,y_train)\n\n#Predict the response for test dataset\nypred_tree = DT_Model.predict(X_test)\n\n# Model Accuracy, how often is the classifier correct?\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, ypred_tree))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Calculate the f1-score\nfscore_tree = f1_score(y_test,ypred_tree, average='weighted') \nprint(\"Accuracy:\",fscore_tree)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Other accuracy measures\naccuracy_dt = classification_report(y_test,ypred_tree)\nprint(accuracy_dt)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The overall accuracy score of the decision tree classifier equals 79%."},{"metadata":{"trusted":true},"cell_type":"code","source":"#decision tree visualization\nimport graphviz\nfrom sklearn.tree import export_graphviz \nfrom IPython.display import Image  \nfrom sklearn import tree\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"featureNames = ['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar',\n       'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density',\n       'pH', 'sulphates', 'alcohol']\nclass_names = df['quality'].unique().tolist()\n\ndot_data = tree.export_graphviz(clf,feature_names = featureNames,class_names=class_names,filled=True, rounded=True)\n\ngraph = graphviz.Source(dot_data)  \ngraph","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.Support Vector Machine Learning"},{"metadata":{},"cell_type":"markdown","source":"Let's select the best model for SVM (support vector machine learning abbreviated). Support Vector Machine Learning is typically used for smaller dataset. This dataset is a quite large, but let's give it a try."},{"metadata":{"trusted":true},"cell_type":"code","source":"#import support vector machine learning packages from sklearn\nfrom sklearn import svm\nfrom sklearn.svm import SVC","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kernel = ['rbf','linear','poly','sigmoid']   \nacc_score_list = []\n\nfor k in kernel:\n    clf = svm.SVC(kernel=k)\n    clf.fit(X_train, y_train)\n    ypred = clf.predict(X_test)\n    acc_score_list.append(f1_score(y_test, ypred, average='weighted')) \n    \nacc_score_list","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The *'rbf'* kernel has the best accuracy for support vector machine learning. The accuracy is 77%."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Best model \nsvmmodel = svm.SVC(kernel='rbf')\nsvmmodel  = svmmodel.fit(X_train, y_train) \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svm_ypred = svmmodel.predict(X_test)\nsvm_score = f1_score(y_test, svm_ypred, average='weighted')\nprint(\"Accuracy using F-score: \",svm_score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Other performance metrics: how well did the support vector model perform?\nprint(classification_report(y_test,svm_ypred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The overall weighted average classification model score equals 78%. The random forest classifier model has a higher accuracy score than the support vector machine learning model."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Confusion Matrix\ncm_svm = confusion_matrix(y_test,svm_ypred)\nprint(cm_svm)\n\nsns.heatmap(cm_svm,cbar=False,annot=True,cmap='Blues',fmt=\"d\")\nplt.xlabel(\"y_pred\")\nplt.ylabel(\"y_test\")\nplt.title(\"Confusion Matrix: Support Vector Machine Learning\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Thanks for going through this notebook! This is the end of this analysis. Hope you enjoyed it! If you find this notebook useful or if you enjoyed it please upvote :)**"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}