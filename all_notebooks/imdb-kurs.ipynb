{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))","execution_count":1,"outputs":[{"output_type":"stream","text":"['imdb_master.csv']\n","name":"stdout"}]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import nltk\nfrom collections import Counter\nimport itertools\nimport torch","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import model_selection\nimdb_df = pd.read_csv('../input/imdb_master.csv', encoding='latin-1')\ndev_df = imdb_df[(imdb_df.type == 'train') & (imdb_df.label != 'unsup')]\ntest_df = imdb_df[(imdb_df.type == 'test')]\ntrain_df, val_df = model_selection.train_test_split(dev_df, test_size=0.05, stratify=dev_df.label)","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class InputFeatures(object):\n    \"\"\"A single set of features of data.\"\"\"\n\n    def __init__(self, input_ids, label_id):\n        self.input_ids = input_ids\n        self.label_id = label_id","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Vocab:\n    def __init__(self, itos, unk_index):\n        self._itos = itos\n        self._stoi = {word:i for i, word in enumerate(itos)}\n        self._unk_index = unk_index\n        \n    def __len__(self):\n        return len(self._itos)\n    \n    def word2id(self, word):\n        idx = self._stoi.get(word)\n        if idx is not None:\n            return idx\n        return self._unk_index\n    \n    def id2word(self, idx):\n        return self._itos[idx]","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm import tqdm_notebook\nclass TextToIdsTransformer:\n    def transform():\n        raise NotImplementedError()\n        \n    def fit_transform():\n        raise NotImplementedError()\n","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class SimpleTextTransformer(TextToIdsTransformer):\n    def __init__(self, max_vocab_size):\n        self.special_words = ['<PAD>', '</UNK>', '<S>', '</S>']\n        self.unk_index = 1\n        self.pad_index = 0\n        self.vocab = None\n        self.max_vocab_size = max_vocab_size\n        \n    def tokenize(self, text):\n        return nltk.tokenize.word_tokenize(text.lower())\n        \n    def build_vocab(self, tokens):\n        itos = []\n        itos.extend(self.special_words)\n        \n        token_counts = Counter(tokens)\n        for word, _ in token_counts.most_common(self.max_vocab_size - len(self.special_words)):\n            itos.append(word)\n            \n        self.vocab = Vocab(itos, self.unk_index)\n    \n    def transform(self, texts):\n        result = []\n        for text in texts:\n            tokens = ['<S>'] + self.tokenize(text) + ['</S>']\n            ids = [self.vocab.word2id(token) for token in tokens]\n            result.append(ids)\n        return result\n    \n    def fit_transform(self, texts):\n        result = []\n        tokenized_texts = [self.tokenize(text) for text in texts]\n        self.build_vocab(itertools.chain(*tokenized_texts))\n        for tokens in tokenized_texts:\n            tokens = ['<S>'] + tokens + ['</S>']\n            ids = [self.vocab.word2id(token) for token in tokens]\n            result.append(ids)\n        return result","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_features(token_ids, label, max_seq_len, pad_index, label_encoding):\n    if len(token_ids) >= max_seq_len:\n        ids = token_ids[:max_seq_len]\n    else:\n        ids = token_ids + [pad_index for _ in range(max_seq_len - len(token_ids))]\n    return InputFeatures(ids, label_encoding[label])","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def features_to_tensor(list_of_features):\n    text_tensor = torch.tensor([example.input_ids for example in list_of_features], dtype=torch.long)\n    labels_tensor = torch.tensor([example.label_id for example in list_of_features], dtype=torch.long)\n    return text_tensor, labels_tensor","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_seq_len=200\nclasses = {'neg': 0, 'pos' : 1}\ntext2id = SimpleTextTransformer(10000)\n\ntrain_ids = text2id.fit_transform(train_df['review'])\nval_ids = text2id.transform(val_df['review'])\ntest_ids = text2id.transform(test_df['review'])","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_df.review.iloc[0][:160])\nprint(train_ids[0][:30])","execution_count":11,"outputs":[{"output_type":"stream","text":"I had no expectations other than to be entertained for 90 minutes, and that is exactly what I was.<br /><br />Of course it is campy, of course some of the dialo\n[2, 18, 82, 73, 1342, 104, 91, 10, 41, 2167, 24, 1160, 250, 5, 7, 20, 11, 607, 63, 18, 5215, 14, 15, 12, 13, 14, 15, 12, 13, 9]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features = [build_features(token_ids, label,max_seq_len, text2id.pad_index, classes) \n                  for token_ids, label in zip(train_ids, train_df['label'])]\n\nval_features = [build_features(token_ids, label,max_seq_len, text2id.pad_index, classes) \n                  for token_ids, label in zip(val_ids, val_df['label'])]\n\ntest_features = [build_features(token_ids, label,max_seq_len, text2id.pad_index, classes) \n                  for token_ids, label in zip(test_ids, test_df['label'])]","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_tensor, train_labels = features_to_tensor(train_features)\nval_tensor, val_labels = features_to_tensor(val_features)\ntest_tensor, test_labels = features_to_tensor(test_features)","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_tensor)","execution_count":14,"outputs":[{"output_type":"stream","text":"tensor([[   2,   18,   82,  ...,    0,    0,    0],\n        [   2,   36,   10,  ...,    0,    0,    0],\n        [   2,   54,   11,  ...,    0,    0,    0],\n        ...,\n        [   2,   19, 3101,  ...,   15,   12,   13],\n        [   2,   18,  136,  ...,  666,   59,    4],\n        [   2,   42,    9,  ...,   23,   67,  707]])\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from torch.utils.data import TensorDataset,DataLoader\nfrom torch.utils import data\n\ntrain_ds = TensorDataset(train_tensor, train_labels)\nval_ds = TensorDataset(val_tensor, val_labels)\ntest_ds = TensorDataset(test_tensor, test_labels)\n\nbatch_size = 32\ntrain_loader = DataLoader(train_ds, batch_size = batch_size)\nval_loader = DataLoader(val_ds, batch_size = batch_size)\ntest_loader = DataLoader(test_ds, batch_size = batch_size)","execution_count":15,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch.nn as nn\nimport torch.nn.functional as F\n\nclass Network(nn.Module):\n    def __init__(self):\n        super(Network, self).__init__()\n        self.embedding = nn.Embedding(10000, 500)\n        self.rec = nn.LSTM(500, 1000, batch_first = True)\n        self.classifier = nn.Sequential(\n            nn.Linear(1000, 1),\n            nn.Sigmoid(),\n        )\n        \n        #self.conv = nn.Sequential(\n        #    nn.Embedding(10000, 100),\n        #    nn.Conv1d(in_channels=1, out_channels=100, kernel_size=4, padding=1), nn.ReLU(), \n        #    nn.Conv1d(in_channels=100, out_channels=100, kernel_size=4, padding=1), nn.ReLU(),nn.MaxPool1d(5),\n        #    nn.Conv1d(in_channels=100, out_channels=120, kernel_size=4, padding=1), nn.MaxPool1d(5))\n        #    \n        #self.classifier1 = nn.Sequential(\n        #    nn.Linear(360,1),nn.Sigmoid())\n        \n    def forward(self, x):       \n        x = self.embedding(x)\n        _, (x, __) = self.rec(x)\n        x = x.view(x.size()[1::])\n        res = self.classifier(x).view(-1)\n        return res\n    \n        #x = torch.transpose(x,-1,-2)\n        #print(\"bef\",x.size())\n        #y = self.conv(x)\n        #print(\"aft\",y.size())\n        #y = y.view(x.size(0), -1)\n        #return self.classifier1(y)\n        ","execution_count":16,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\ndef train_model(epochs, model, optimizer, criterion, train_loader,val_loader, device, n_prints=1):\n    print_every = len(train_loader) // n_prints\n    for epoch in range(epochs):\n        best_acc = 0\n        model.train()\n        model.to(device)\n        running_train_loss = 0.0\n        \n        for iteration, (xx, yy) in enumerate(train_loader):\n            optimizer.zero_grad()\n            xx, yy = xx.to(device), yy.to(device)\n            out = model(xx)\n            loss = criterion(out, yy.type(torch.float32))\n            running_train_loss += loss.item()\n            loss.backward()\n            optimizer.step()\n            \n            if(iteration % print_every == print_every - 1):\n                running_train_loss /= print_every\n                print(f\"Epoch {epoch}, iteration {iteration} training_loss {running_train_loss}\")\n                running_train_loss = 0.0\n            \n        with torch.no_grad():\n            model.eval()\n            running_corrects = 0\n            running_total = 0\n            running_loss = 0.0\n            all_preds = []\n            correct_preds = []\n            \n            for xx, yy in val_loader:\n                batch_size = xx.size(0)\n                xx, yy = xx.to(device), yy.to(device)\n                out = model(xx)\n                \n                loss = criterion(out, yy.type(torch.float32))\n                running_loss += loss.item()\n                for i in out:\n                    if i >= 0.5:\n                        all_preds.append(1)\n                    else:\n                        all_preds.append(0)\n                correct_preds.extend(yy.cpu().tolist())\n                \n               # running_corrects += (predictions == yy).sum().item()\n               # running_total += batch_size\n            cur_acc = metrics.accuracy_score(correct_preds,all_preds)\n            mean_val_loss = running_loss / len(val_loader)\n            \n            if cur_acc > best_acc:\n                best_acc = cur_acc\n                torch.save(model.state_dict(), \"../best_model.pytorch\")\n            \n            print(f\"Epoch {epoch}, val_loss {mean_val_loss}, val_accuracy {cur_acc}\")\n            \n    model.load_state_dict(torch.load(\"../best_model.pytorch\"))\n                ","execution_count":20,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cuda_device = torch.device('cuda')\ndevice = cuda_device\n\nmodel = Network()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.0004)\noptimizer1 = torch.optim.SGD(model.parameters(), lr=0.01,momentum=0.5)\n\ncriterion = nn.BCELoss()\ntrain_model(10, model, optimizer, criterion,train_loader,val_loader, device, n_prints=1)","execution_count":21,"outputs":[{"output_type":"stream","text":"Epoch 0, iteration 742 training_loss 0.6962164158294852\nEpoch 0, val_loss 0.6916838318109513, val_accuracy 0.5248\nEpoch 1, iteration 742 training_loss 0.6890376382689136\nEpoch 1, val_loss 0.6772669807076455, val_accuracy 0.5592\nEpoch 2, iteration 742 training_loss 0.5339960070229154\nEpoch 2, val_loss 0.428869026619941, val_accuracy 0.8136\nEpoch 3, iteration 742 training_loss 0.3344168639058701\nEpoch 3, val_loss 0.37913713650777936, val_accuracy 0.8312\nEpoch 4, iteration 742 training_loss 0.2276059902306962\nEpoch 4, val_loss 0.4053874318487942, val_accuracy 0.8456\nEpoch 5, iteration 742 training_loss 0.13797197050651883\nEpoch 5, val_loss 0.5026156660169363, val_accuracy 0.848\nEpoch 6, iteration 742 training_loss 0.07261371572250011\nEpoch 6, val_loss 0.570650024083443, val_accuracy 0.8448\nEpoch 7, iteration 742 training_loss 0.042511944120692174\nEpoch 7, val_loss 0.6263331020600162, val_accuracy 0.8392\nEpoch 8, iteration 742 training_loss 0.0328064171478596\nEpoch 8, val_loss 0.6139772232767428, val_accuracy 0.836\nEpoch 9, iteration 742 training_loss 0.02458922313393876\nEpoch 9, val_loss 0.7059330556265195, val_accuracy 0.8424\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report\n\nmodel.load_state_dict(torch.load(\"../best_model.pytorch\"))\nall_preds = []\ncorrect_preds = []\nfor xx,yy in test_loader:\n    xx = xx.to(device)\n    y_pred = model.forward(xx)\n    for i in y_pred:\n        if i >= 0.5:\n            all_preds.append(1)\n        else:\n            all_preds.append(0)\n    correct_preds.extend(yy.tolist())\nprint(metrics.accuracy_score(correct_preds,all_preds))\nprint(classification_report(correct_preds,all_preds))","execution_count":22,"outputs":[{"output_type":"stream","text":"0.83948\n              precision    recall  f1-score   support\n\n           0       0.85      0.83      0.84     12500\n           1       0.83      0.85      0.84     12500\n\n   micro avg       0.84      0.84      0.84     25000\n   macro avg       0.84      0.84      0.84     25000\nweighted avg       0.84      0.84      0.84     25000\n\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}