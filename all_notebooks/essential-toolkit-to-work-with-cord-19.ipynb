{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Essential toolkit to work with CORD-19 ðŸ”­\n\n### Collections of plug-and-play python functions to play efficiently with the CORD-19 dataset.\n\n**Motivation**\n - Given the large amount of notebooks (1300+) it's harder to find useful snippet of codes.\n - To avoid reinvent the wheel.\n - To accelerate discovery and development.\n - Universal tools; whatever is your task, you probably needs to go thorugh a quite standard pipeline.\n    \n**Notebook principles**\n - Each function is independent, single-scoped and well-documented.\n - Notebook's output are two dataset: **cord19.csv** (all data in a single dataframe, you can assume they contains the last version) and **cord19_metadata_only.csv** (same as before without the whole research text).\n - By a member of the Kaggle community, to the community. You needs to help me by telling what you needs and I will do my best to develop any function you may need.\n    \n**Updates and requests**\n - In case this notebook will receive any form of attention, it will be updated and enhanced with new features. It's important to me to receive your feedback on what you need or what isn't clear for you. Also, if you find it useful please upvote it. With such a large collection of notebook it's harder to be noticed.\n - If you have any snippet of code you think others may use it as well, you can write it in a comment and I will integrate it into the notebook!\n \n**Disclaimer**\n - Work in progress, thank you for your understandings. \n - Some of the functions makes use of a python package I developed. It's called [Texthero](https://github.com/jbesomi/texthero/) and it's still in early beta version. If you find any bug or have suggestions on new functionalities, just leave a comment here or opean an issue on Github and I will be glad to implement it!"},{"metadata":{},"cell_type":"markdown","source":"#### Install and import packages\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Install and import texthero\n!pip install texthero -q\nimport texthero as hero\n\n# Import the other packages\nimport numpy as np \nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n\nimport seaborn as sns\nsns.set(color_codes=True)\n\nfrom pathlib import Path\nimport glob\nimport json","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1. get_data()\n\n**Description**\n\nReturn the CORD-19 dataset in a Pandas DataFrame.\n\n**Arguments**\n\n- `metadata_only` (`False` by default)\n    If `True`, return only the metadata \n\n\n**Acknolwedgment**\n\n[xhlulu](https://www.kaggle.com/xhlulu) has released a great [notebook](https://www.kaggle.com/xhlulu/cord-19-eda-parse-json-and-generate-clean-csv/output) that parse and clean the JSON data. The ouput of his kernels are four csv-dataset ready to be used for further analysis. The `get_data()` function make use of this module, if you use it, you need to import the notebook. The fastest approach is probably to _fork_ this notebook.\n\n**Code**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Concat\n\ndef get_data(metadata_only=False):\n    \"\"\"\n    Return CORD-19 dataset\n    \n    Parameters\n    ----------\n    \n    metadata_only : bool (False by default )\n        - When True, returns only the metadata Pandas DataFrame\n    \n    \"\"\"\n    if metadata_only:\n        return pd.read_csv(\"/kaggle/input/CORD-19-research-challenge/metadata.csv\")\n    \n    CLEAN_DATA_PATH = Path(\"../input/cord-19-eda-parse-json-and-generate-clean-csv/\")\n\n    biorxiv_df = pd.read_csv(CLEAN_DATA_PATH / \"biorxiv_clean.csv\")\n    biorxiv_df['source'] = 'biorxiv'\n\n    pmc_df = pd.read_csv(CLEAN_DATA_PATH / \"clean_pmc.csv\")\n    pmc_df['source'] = 'pmc'\n\n    comm_use_df = pd.read_csv(CLEAN_DATA_PATH / \"clean_comm_use.csv\")\n    comm_use_df['source'] = 'comm_use'\n\n    noncomm_use_df = pd.read_csv(CLEAN_DATA_PATH / \"clean_noncomm_use.csv\")\n    noncomm_use_df['source'] = 'noncomm_use'\n\n    papers_df = pd.concat(\n        [biorxiv_df,pmc_df, comm_use_df, noncomm_use_df], axis=0\n    ).reset_index(drop=True)\n\n    \n    return papers_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Usage and examples**"},{"metadata":{"trusted":true},"cell_type":"code","source":"papers_df = get_data()\npapers_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Check shape:"},{"metadata":{"trusted":true},"cell_type":"code","source":"papers_df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. tfidf()\n\n**Description**\n\nReturn the TF-IDF representation for the CORD-19 dataset. Vectors are computed using the specified `text` column.\n\n**Argument**\n\n- df: pd.DataFrame with at least one text column\n- columns: list of columns name to compute TF-IDF\n- dim: dimension of the vector space. Default 256.\n\n**Code**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def tfidf(df, columns=['text', 'abstract'], dim=256):\n        \n    if len(columns) == 0:\n        raise ValueError(\"columns argument must be a least and have at least one value.\")\n    \n    # Merge all text columns\n    df['content'] = df[columns[0]]\n    \n    for col in columns[1:]:\n        df['content'] += df[col]\n        \n    # Fill missing NA\n    if df['content'].isna().sum() > 0:\n        print(\"Warning. The dataset contains NA. They will be dropped for TF-IDF computation.\")\n        content = df['content'].dropna()\n    else:\n        content = df['content']\n    \n    # Compute TF-IDF\n    return content.pipe(hero.do_tfidf, max_features=dim)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Usage and examples**"},{"metadata":{},"cell_type":"markdown","source":"Example 1: return a Pandas Series of TF-IDF."},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_df = papers_df.sample(1000)\ntfidf_s = tfidf(sample_df, columns=['abstract'])\ntfidf_s.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Example 2: add the column to the current dataframe"},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_df = papers_df.sample(5000)\nsample_df['tfidf'] = tfidf(sample_df, columns=['abstract'])\nsample_df.head(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since some of the initial values where zero, some of the TF-IDF values haven't been computed."},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_df['tfidf'].isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. pca()\n\n**Description**\n\nReduce the vector space to two dimension to visualize the CORD-19 corpus.\n\n**Argument**\n\ns: pd.Series containing for each element a list of vectors.\n\n**Code**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def pca(s):\n    return hero.do_pca(s)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Usage and examples**"},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_df = sample_df.dropna(how='any')\nsample_df['pca'] = sample_df['tfidf'].pipe(pca)\nsample_df['pca'].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. show_pca()\n\n**Description**\n\nShow the reduced vector space\n\n**Argument**\n\n- df: pd.DataFrame\n- pca_col: the pre-computed pca column\n- color_col: (optional), color each dot according to the label in the color_col  \n- title\n\n**Code**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def show_pca(df, pca_col, color_col=None, title=\"\"):\n    return hero.scatterplot(df, pca_col, color=color_col, title=title)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Usage and examples**"},{"metadata":{"trusted":true},"cell_type":"code","source":"title = \"Vector space representation of CORD-19\"\nshow_pca(sample_df, 'pca', title=title)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5. kmeans()\n\n**Description**\n\nCompute kmeans on the given series and returns the labels\n\n**Argument**\n\n- s: pd.Series\n- n_clusters: int\n\n**Code**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def kmeans(s, n_clusters):\n    return hero.do_kmeans(s, n_clusters=n_clusters)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Usage and examples**"},{"metadata":{"trusted":true},"cell_type":"code","source":"title = \"Vector space representation of CORD-19 with K-means\"\nsample_df['kmeans'] = kmeans(sample_df['tfidf'], 20)\nsample_df['kmeans'] = sample_df['kmeans'].astype(str)  # for a nicer visualization\nshow_pca(sample_df, 'pca', color_col='kmeans', title=title)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 6. topic_modeling()\n\nNext version. Stay tuned."},{"metadata":{},"cell_type":"markdown","source":"**Save output**"},{"metadata":{"trusted":true},"cell_type":"code","source":"papers_df.to_csv(\"cord19.csv\", index=False)\nget_data(metadata_only=True).to_csv(\"cord19_metadata_only.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Temporary conclusion: thank you for having read it all. Hope it will be useful to many of you! "}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}