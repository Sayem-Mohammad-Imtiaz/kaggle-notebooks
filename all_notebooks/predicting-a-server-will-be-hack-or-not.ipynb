{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport missingno as msno\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_data = pd.read_csv(\"/kaggle/input/novartis-data/Train.csv\")\ntest_data = pd.read_csv(\"/kaggle/input/novartis-data/Test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## We can see that we have some null values .So let's remove these null values","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data[\"X_12\"] = train_data[\"X_12\"].ffill()\ntest_data[\"X_12\"] = test_data[\"X_12\"].ffill()\ntrain_data[\"X_12\"] = train_data[\"X_12\"].bfill()\ntest_data[\"X_12\"] = test_data[\"X_12\"].bfill()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Now we can see that, we have no Null Values.So we have successfully remove all the Null Values","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Now let's drop the first two column(INCIDENT_ID & DATE) because we don't need these two column","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.drop([\"INCIDENT_ID\",\"DATE\"],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data.drop([\"INCIDENT_ID\",\"DATE\"],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set()\ntrain_data.hist(figsize=(20,10),bins=15,color=\"purple\")\nplt.title(\"Distribution of Features\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set()\nplt.figure(figsize=(20,10))\nsns.boxplot(data= train_data,palette = \"Set3\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set()\nplt.figure(figsize=(20,10))\nsns.boxplot(data= test_data,palette = \"Set3\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Here we can see that our dataset has a lot of outliers,Let's remove some of the outliers, becouse outliers are gonna make some trouble for us to create a Good Model","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Let's remove these Outliers using Standard Deviation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"lower_limit = train_data[\"X_8\"].mean() - 3*train_data[\"X_8\"].std()\nupper_limit = train_data[\"X_8\"].mean() + 3*train_data[\"X_8\"].std()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train1 = train_data[(train_data[\"X_8\"] > lower_limit) & (train_data[\"X_8\"] < upper_limit)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.shape[0] - df_train1.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lower_limit = test_data[\"X_8\"].mean() - 3*test_data[\"X_8\"].std()\nupper_limit = test_data[\"X_8\"].mean() + 3*test_data[\"X_8\"].std()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test1 = test_data[(test_data[\"X_8\"] > lower_limit) & (test_data[\"X_8\"] < upper_limit)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data.shape[0] - df_test1.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lower_limit = df_train1[\"X_10\"].mean() - 3*df_train1[\"X_10\"].std()\nupper_limit = df_train1[\"X_10\"].mean() + 3*df_train1[\"X_10\"].std()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train2 = df_train1[(df_train1[\"X_10\"] > lower_limit) & (df_train1[\"X_10\"] < upper_limit)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train1.shape[0] - df_train2.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lower_limit = df_test1[\"X_10\"].mean() - 3*df_test1[\"X_10\"].std()\nupper_limit = df_test1[\"X_10\"].mean() + 3*df_test1[\"X_10\"].std()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test2 = df_test1[(df_test1[\"X_8\"] > lower_limit) & (df_test1[\"X_8\"] < upper_limit)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test1.shape[0] - df_test2.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lower_limit = df_train2[\"X_11\"].mean() - 3*df_train2[\"X_11\"].std()\nupper_limit = df_train2[\"X_11\"].mean() + 3*df_train2[\"X_11\"].std()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train3 = df_train2[(df_train2[\"X_11\"] > lower_limit) & (df_train2[\"X_11\"] < upper_limit)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train2.shape[0] - df_train3.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lower_limit = df_test2[\"X_11\"].mean() - 3*df_test2[\"X_11\"].std()\nupper_limit = df_test2[\"X_11\"].mean() + 3*df_test2[\"X_11\"].std()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test3 = df_test2[(df_test2[\"X_11\"] > lower_limit) & (df_test2[\"X_11\"] < upper_limit)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test2.shape[0] - df_test3.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lower_limit = df_train3[\"X_12\"].mean() - 3*df_train3[\"X_12\"].std()\nupper_limit = df_train3[\"X_12\"].mean() + 3*df_train3[\"X_12\"].std()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train4 = df_train3[(df_train3[\"X_12\"] > lower_limit) & (df_train3[\"X_12\"] < upper_limit)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train3.shape[0] - df_train4.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lower_limit = df_test3[\"X_12\"].mean() - 3*df_test3[\"X_12\"].std()\nupper_limit = df_test3[\"X_12\"].mean() + 3*df_test3[\"X_12\"].std()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test4 = df_test3[(df_test3[\"X_12\"] > lower_limit) & (df_test3[\"X_12\"] < upper_limit)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test3.shape[0] - df_test4.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lower_limit = df_train4[\"X_13\"].mean() - 3*df_train4[\"X_13\"].std()\nupper_limit = df_train4[\"X_13\"].mean() + 3*df_train4[\"X_13\"].std()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train5 = df_train4[(df_train4[\"X_13\"] > lower_limit) & (df_train4[\"X_13\"] < upper_limit)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train4.shape[0] - df_train5.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lower_limit = df_test4[\"X_13\"].mean() - 3*df_test4[\"X_13\"].std()\nupper_limit = df_test4[\"X_13\"].mean() + 3*df_test4[\"X_13\"].std()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test5 = df_test4[(df_test4[\"X_13\"] > lower_limit) & (df_test4[\"X_13\"] < upper_limit)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test4.shape[0] - df_test5.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train5.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test5.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train5.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Here we can see that we have totally removed (23856 - 22628) = 1228 outliers from train data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test5.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Here we can see that we have totally removed (15903  - 14981 ) = 922 outliers from test data","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Now Let's see  our Dataset has any null values or not","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"n = msno.bar(df_train5,color=\"purple\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## So we can see that our Dataset has no Null Values","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"x = df_train5.drop(\"MULTIPLE_OFFENSE\",axis=1)\ny = df_train5[\"MULTIPLE_OFFENSE\"]\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## So ,let's now scale our Dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nx = scaler.fit_transform(x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Now the precious time has come to see which parameter is best to create a Good Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_params ={\n    \"svm\": {\n        \"model\" : SVC(gamma=\"auto\"),\n        \"params\": {\n            \"C\" : [1,10,20],\n            \"kernel\": [\"rbf\"],\n            \"random_state\":[0,10,100]\n        }\n    },\n    \n    \"decision_tree\":{\n        \"model\": DecisionTreeClassifier(),\n        \"params\":{\n            \"criterion\": [\"entropy\",\"gini\"],\n            \"max_depth\": [5,8,9],\n            \"random_state\":[0,10,100]\n        }\n    },\n    \"random_forest\":{\n        \"model\": RandomForestClassifier(),\n        \"params\": {\n            \"n_estimators\" : [1,5,10],\n            \"max_depth\" : [5,8,9],\n            \"random_state\":[0,10,100]\n        }\n    },\n    \"naive_bayes\":{\n        \"model\": GaussianNB(),\n        \"params\": {}\n    },\n    \n    \"logistic_regression\":{\n        \"model\" : LogisticRegression(solver='liblinear',multi_class = 'auto'),\n        \"params\":{\n            \"C\" : [1,5,10],\n            \"random_state\":[0,10,100]\n        }\n    },\n    \"knn\" : {\n        \"model\" : KNeighborsClassifier(),\n        \"params\": {\n            \"n_neighbors\" : [5,12,13]\n        }\n    }\n    \n    \n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scores =[]\nfor model_name,mp in model_params.items():\n    clf = GridSearchCV(mp[\"model\"],mp[\"params\"],cv=12,return_train_score=False)\n    clf.fit(x,y)\n    scores.append({\n        \"Model\" : model_name,\n        \"Best_Score\": clf.best_score_,\n        \"Best_Params\": clf.best_params_\n    })","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result_score = pd.DataFrame(scores, columns = [\"Model\",\"Best_Score\",\"Best_Params\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result_score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Wow, we can see that \"Random Forest\" & \"Decision Tree\" gives us almost 99% Accuracy","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import  train_test_split\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.25,random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_dt = DecisionTreeClassifier(criterion= \"gini\",max_depth = 9,random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_dt.fit(x_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = clf_dt.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\naccuracy_score(y_test,y_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Let's see our Actual vs Predicted Values,Which says,if a server will be Hack or Not","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"result = pd.DataFrame({\"Actual_Value\": y_test, \"Predicted_Value\": y_pred})\nresult","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## *** --------If you like it,then please do UpVote-------- ***","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}