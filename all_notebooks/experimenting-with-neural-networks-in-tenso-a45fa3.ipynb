{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"14a3c405-5832-69bd-fd10-b4113ea19727"},"source":"This notebook is a complete end to end introduction on creating a multilayer perceptron in tensorflow.\n\nThe basic outline is as follows:\n\n 1. **Data Ingestion**\n 2. **Data Visualization**\n 3. **Preprocessing Data**\n 4. **Designing the tf graph**\n 5. **Executing the tf graph**\n 6. **Result Visualizations**"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0675f53f-84c8-37be-1286-449ae38fa411"},"outputs":[],"source":"# boilerplate code\nfrom __future__ import print_function\nimport os\nfrom io import BytesIO\nimport numpy as np\nimport pandas as pd\nfrom functools import partial\nimport PIL.Image\nfrom IPython.display import clear_output, Image, display, HTML\n\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\n\n%matplotlib inline"},{"cell_type":"markdown","metadata":{"_cell_guid":"e8e35927-72d2-782b-1001-04ccaa6a2f7d"},"source":"# Data Ingestion\n\nLet us fetch and explore the data before getting into training the neural network"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"59220a6a-cfff-bbc7-34d1-af96f086809d"},"outputs":[],"source":"df = pd.read_csv('../input/voice.csv')\ndf.head()"},{"cell_type":"markdown","metadata":{"_cell_guid":"53fcf2e3-3404-8308-e4db-fa5e301f6583"},"source":"Now let us perform some sanity check on the data to make sure there are no empty fields which we might need to impute further."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"61256254-6f19-7a69-6393-e7d98cfb2564"},"outputs":[],"source":"np.where(pd.isnull(df))"},{"cell_type":"markdown","metadata":{"_cell_guid":"f07876f9-3a64-9d54-139c-c1561e839575"},"source":"Awesome. We don't have any null's in the dataset. One less thing to worry about. Now let us check how the labels are distributed."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7a07cad1-3b2c-bf7d-0ac8-38dcba05c55f"},"outputs":[],"source":"print(\"Number of male: {}\".format(df[df.label == 'male'].shape[0]))\nprint(\"Number of female: {}\".format(df[df.label == 'female'].shape[0]))"},{"cell_type":"markdown","metadata":{"_cell_guid":"93400b7e-d7fb-5c74-4630-1c4ac1c1f5d3"},"source":"# Data Visualization\n\n**Pearson Correlation Heatmap**\n\nLet us perform the pearson correlation heatmap to figure out how features correlate to one another\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f426a0a2-0b72-a089-b502-1bc83b234409"},"outputs":[],"source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\ncolormap = plt.cm.viridis\nplt.figure(figsize=(12,12))\nplt.title('Pearson Correlation of Features', y=1.05, size=15)\nsns.heatmap(df.iloc[:,:-1].astype(float).corr(),linewidths=0.1,vmax=1.0, square=True, cmap=colormap, linecolor='white', annot=True)"},{"cell_type":"markdown","metadata":{"_cell_guid":"922a1da7-4ed1-de8d-16d0-b11b7a21b73f"},"source":"While looking at the plot, we can figure out some interesting correlations. If you look at `meanfreq` vs `centroid` their correlation is maximum possible value of 1. Same is the case with `maxdom` and `dfrange`. So essentially we could filter out these features and still get an equivalent performance as they aren't adding any new information. \n\n*TODO*: refactor the notebook to filter out these columns easily"},{"cell_type":"markdown","metadata":{"_cell_guid":"02395579-1c38-2061-e4b0-f596f1b42b60"},"source":"# Preprocessing Data\n\nThere are multiple steps we will take in this section to transform the original data into format which we can easily plug inside tensorflow's tensors.\n\nThe basic steps are:\n* Creating 1-hot vector from the original labels\n* Normalize each feature\n* Create train/test/validation datasets"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3032e6de-d537-598e-8190-22aaf3e9da26"},"outputs":[],"source":"def convertToOneHot(vector, num_classes=None):\n    \"\"\"\n    Converts an input 1-D vector of integers into an output\n    2-D array of one-hot vectors, where an i'th input value\n    of j will set a '1' in the i'th row, j'th column of the\n    output array.\n\n    Example:\n        v = np.array((1, 0, 4))\n        one_hot_v = convertToOneHot(v)\n        print one_hot_v\n\n        [[0 1 0 0 0]\n         [1 0 0 0 0]\n         [0 0 0 0 1]]\n    \"\"\"\n\n    assert isinstance(vector, np.ndarray)\n    assert len(vector) > 0\n\n    if num_classes is None:\n        num_classes = np.max(vector)+1\n    else:\n        assert num_classes > 0\n        assert num_classes >= np.max(vector)\n\n    result = np.zeros(shape=(len(vector), num_classes))\n    result[np.arange(len(vector)), vector] = 1\n    return result.astype(int)"},{"cell_type":"markdown","metadata":{"_cell_guid":"300b84c8-cc01-2d81-40a1-be06dffa680e"},"source":"Since the labels aren't numbers, let us first convert them into numerical categories and then subsequently convert it to a one-hot vector"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"cc22f5e3-b0f8-66e5-4d03-32f881b4c4e4"},"outputs":[],"source":"from sklearn.preprocessing import LabelEncoder\nlabel=df.iloc[:,-1]\n\n# Encode label category\n# male -> 1\n# female -> 0\n\ngender_encoder = LabelEncoder()\nlabel = gender_encoder.fit_transform(label)\nlabel = convertToOneHot(label, 2)"},{"cell_type":"markdown","metadata":{"_cell_guid":"d8954b09-c12b-16d5-131a-d288b0faf794"},"source":"Let us look at the shape of the labels to confirm it is now a 1-hot vector"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9847b55c-5470-a37f-1c09-f1f11335baea"},"outputs":[],"source":"label.dtype"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"be0e930e-0eee-b70d-c1bf-eb4b07e24306"},"outputs":[],"source":"data = df.iloc[:,:-1]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b82e91c1-4f94-ad0a-4545-3e962622a179"},"outputs":[],"source":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaler.fit(data)\ndata = scaler.transform(data)"},{"cell_type":"markdown","metadata":{"_cell_guid":"3f522ee7-da62-6d16-578f-af0ff388013a"},"source":"Next step is to transform the dataset into train/test/validation datasets. sklearn doesn't directly give 3 way split so we do a 2 way split twice "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6a15e660-1e86-2e9f-9881-6864b520cb3b"},"outputs":[],"source":"from sklearn.cross_validation import train_test_split\nx_train, x_validation_and_test, y_train, y_validation_and_test = train_test_split(data,label,test_size=0.3)\nx_validation, x_test, y_validation, y_test = train_test_split(x_validation_and_test, y_validation_and_test, test_size=0.6)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8e3a101c-0aa3-14dd-1db9-145783f169ed"},"outputs":[],"source":"x_train, x_test, y_train, y_test, x_validation, y_validation = np.array(x_train,dtype='float32'), np.array(x_test,dtype='float32'),np.array(y_train,dtype='float32'),np.array(y_test,dtype='float32'), np.array(x_validation, dtype='float32'), np.array(y_validation, dtype='float32')"},{"cell_type":"markdown","metadata":{"_cell_guid":"baffb3b2-8324-8a27-0179-3d20de5130f9"},"source":"Let us look at the data first and make sure everything looks good"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"dffe5f50-00c5-40a0-f616-955ccafadcdd"},"outputs":[],"source":"def print_shape(data_set, name):\n    print(\"shape of {} is {} and datatype is {}\".format(name, data_set.shape, data_set.dtype))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d81b5b72-d46f-bc0a-7e2d-5d09dd4c7b53"},"outputs":[],"source":"print_shape(x_train, \"x_train\")\nprint_shape(y_train, \"y_train\")\nprint_shape(x_test, \"x_test\")\nprint_shape(y_test, \"y_test\")\nprint_shape(x_validation, \"x_validation\")\nprint_shape(y_validation, \"y_validation\")"},{"cell_type":"markdown","metadata":{"_cell_guid":"ff557cfe-e54d-e1ce-0661-7f93df880c1f"},"source":"# Design the tf graph\n\nNow we are all set to get started with creating the tensorflow graph. It is important to understand that after this step there is no actual computation being done by tensorflow. It just creates a lazy graph according to the nodes we create in the `multilayer_perceptron` method. "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4e10c67d-ede6-b097-9435-cb8c5581d42b"},"outputs":[],"source":"num_features = 20\nnum_hidden_layers = 2\nmlp_layer_sizes = [20, 15, 10, 2]\nweights = {}\nbiases = {}\nnum_features = mlp_layer_sizes[0]\nn_hidden_1 = mlp_layer_sizes[1]  # 1st layer number of features\nn_hidden_2 = mlp_layer_sizes[2]  # 2nd layer number of features\nn_classes = mlp_layer_sizes[-1]\nweights = {\n    'h1': tf.get_variable(\"W1\", shape=[num_features, n_hidden_1],\n                          initializer=tf.contrib.layers.xavier_initializer(uniform=False)),\n    'h2': tf.get_variable(\"W2\", shape=[n_hidden_1, n_hidden_2],\n                          initializer=tf.contrib.layers.xavier_initializer(uniform=False)),\n    'out': tf.get_variable(\"W3\", shape=[n_hidden_2, n_classes],\n                           initializer=tf.contrib.layers.xavier_initializer())\n}\nbiases = {\n    'b1': tf.Variable(tf.random_normal([n_hidden_1]), name=\"B1\"),\n    'b2': tf.Variable(tf.random_normal([n_hidden_2]), name=\"B2\"),\n    'out': tf.Variable(tf.constant(0.1, shape=[n_classes]), name=\"B3\")\n}\n# tf Graph input\nx = tf.placeholder(\"float\", [None, mlp_layer_sizes[0]], name=\"x\")\ny_ = tf.placeholder(\"float\", [None, mlp_layer_sizes[-1]], name=\"y_\")"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b2d9a77e-1adf-5194-6fd0-142d1295b545"},"outputs":[],"source":"def multilayer_perceptron(x, num_hidden_layers, weights,\n                          biases, activation=\"relu\", keep_prob=1.0, multiplication_factor=1.0):\n    out_layer = None\n    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases[\n                     'b1'], name=\"layer_1\")\n    layer_1 = activate(layer_1, activation, tf_name=\"layer_1\")\n    layer_1 = tf.scalar_mul(multiplication_factor, layer_1)\n    layer_1 = tf.nn.dropout(\n        layer_1, tf.constant(keep_prob), name=\"layer_1\")\n    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases[\n                     'b2'], name=\"layer_2\")\n    layer_2 = activate(layer_2, activation, tf_name=\"layer_2\")\n    layer_2 = tf.scalar_mul(multiplication_factor, layer_2)\n    layer_2 = tf.nn.dropout(layer_2, tf.constant(keep_prob), name=\"layer_2\")\n    # Output layer\n    out_layer = tf.add(tf.matmul(layer_2, weights['out']), biases['out'], name=\"out_layer\")\n    return out_layer\n\ndef activate(linear, activation, tf_name):\n    if activation == 'sigmoid':\n        return tf.nn.sigmoid(linear, name=tf_name)\n    elif activation == 'softmax':\n        return tf.nn.softmax(linear, name=tf_name)\n    elif activation == 'linear':\n        return linear\n    elif activation == 'tanh':\n        return tf.nn.tanh(linear, name=tf_name)\n    elif activation == 'relu':\n        return tf.nn.relu(linear, name=tf_name)\n\n\ndef loss_function(predictions, labels, loss, tf_name):\n    if loss == 'cross-entropy':\n        return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n            logits=predictions, labels=labels), name=tf_name)\n    elif loss == 'rmse':\n        return tf.reduce_mean(tf.square(predictions - labels), name=tf_name)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"05c8f6e0-b266-48ed-2b36-cb2f19328ef2"},"outputs":[],"source":"# construct the model using the above defined helper function\ntrain_prediction = multilayer_perceptron(x, num_hidden_layers, weights, biases,\n                                                    activation='relu', keep_prob=0.5)\n\nvalidation_prediction = activate(multilayer_perceptron(x, num_hidden_layers, weights, biases,\n                                                    activation='relu', multiplication_factor=0.5), 'softmax', tf_name=\"validation_prediction\")"},{"cell_type":"markdown","metadata":{"_cell_guid":"18cc8b81-1a96-447d-0c1f-63d6aeb0c3cf"},"source":"In this section we define the optimizer and loss functions. We also define our notion of correct and incorrect prediction and what we mean by accuracy. This is the metric we will track later when we actually execute the graph"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"690b2b68-d7c2-c831-1f79-788ea35df618"},"outputs":[],"source":"# constants\nBATCH_SIZE = 100\nDISPLAY_STEP = 10\n\n# Define loss and optimizer\nglobal_step = tf.Variable(0, name=\"global_step\", trainable=False)\nloss_function = loss_function(\n        train_prediction, y_, 'cross-entropy', tf_name=\"loss_function\")\nlearning_rate = tf.train.exponential_decay(\n        0.0010000000474974513, global_step, 500, 0.98, name=\"learning_rate\")\n\noptimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=0.8999999761581421, beta2=0.9990000128746033,\n                                       name=\"optimizer\").minimize(loss_function, global_step=global_step)\n\ncorrect_prediction = tf.equal(tf.argmax(validation_prediction, 1), tf.argmax(y_, 1), name=\"correct_prediction\")\n\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"), name=\"accuracy\")"},{"cell_type":"markdown","metadata":{"_cell_guid":"ed3386ae-ca2b-660c-976a-b048a676914a"},"source":"# Execute tf graph\n\nNow that the graph is set up, we need to initialize it and run it"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c8231921-899a-101a-5249-8dd136e7e729"},"outputs":[],"source":"sess = tf.Session()\ninit = tf.global_variables_initializer()\nsess.run(init)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b4255969-0c55-0436-45bf-3f73f820ecc2"},"outputs":[],"source":"num_epochs = 10000\nprev_accuracy = 0.0\nunoptimized_count = 0\nfinal_epoch_count = 0\n\ntrain_size = x_train.shape[0]\n\nsess = tf.Session()\ninit = tf.global_variables_initializer()\nsess.run(init)\n\nfor epoch in range(num_epochs):\n    avg_accuracy = 0.0\n    total_batch = int(train_size // BATCH_SIZE)\n    # Loop over all batches\n    for step in range(total_batch):\n        offset = (step * BATCH_SIZE) % train_size\n        batch_data = x_train[offset:(offset + BATCH_SIZE), :]\n        batch_labels = y_train[offset:(offset + BATCH_SIZE)]\n        _, a, current_step = sess.run([optimizer, accuracy, global_step], feed_dict={x: batch_data, y_: batch_labels})\n        # Compute average loss & accuracy\n        avg_accuracy += a / total_batch\n    validation_accuracy = sess.run([accuracy], feed_dict={x: x_validation, y_: y_validation})\n    current_step = tf.train.global_step(sess, global_step)\n    if epoch % DISPLAY_STEP == 0:\n        print(\"Epoch:{} training_accuracy={}\".format(epoch + 1, avg_accuracy))\n        print(\"Epoch:{} validation_accuracy={}\".format(epoch + 1, validation_accuracy))\n    if (avg_accuracy - prev_accuracy) < 0.01:\n        unoptimized_count += 1\n    else:\n        unoptimized_count = 0\n        prev_accuracy = avg_accuracy\n    if unoptimized_count > 50:\n        final_epoch_count = epoch + 1\n        break\nif final_epoch_count == 0:\n    final_epoch_count = range(num_epochs)\nvalidation_accuracy, validation_pred = sess.run([accuracy, validation_prediction],\n                                      feed_dict={x: x_validation, y_: y_validation})  \ntest_accuracy, test_pred = sess.run([accuracy, validation_prediction],\n                                feed_dict={x: x_test, y_: y_test})\nprint(\"Finally, validation_accuracy={}, test_accuracy={}\".format(validation_accuracy, test_accuracy))"},{"cell_type":"markdown","metadata":{"_cell_guid":"5ceb80d9-7475-cf82-b895-c8d821e7d3d2"},"source":"# Result Visualizations\n\nNow that we have our required predictions on test dataset, we can compute the precision, recall and all sorts of other metrics whichever suits our required problem statement."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e2148643-9c46-e38a-400a-cb515f6c9719"},"outputs":[],"source":"from sklearn.metrics import precision_recall_curve, roc_auc_score, roc_curve\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nprecision, recall, thresholds = precision_recall_curve(\n            y_test[:, 1], test_pred[:, 1])\n\nplt.plot(recall, precision, label='Precision-Recall curve')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.ylim([0.0, 1.05])\nplt.xlim([0.0, 1.0])\nplt.title('Precision-Recall')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d62deeb7-9a81-b33a-e9ce-6c659cf95def"},"outputs":[],"source":""}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}