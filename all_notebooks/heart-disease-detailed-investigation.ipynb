{"cells":[{"metadata":{},"cell_type":"markdown","source":"( References : https://www.healthline.com ; https://www.mayoclinic.org)\n# Heart Disease\nHeart disease describes a range of conditions that affect your heart. Diseases under the heart disease umbrella include blood vessel diseases, such as coronary artery disease; heart rhythm problems (arrhythmias); and heart defects you're born with (congenital heart defects), among others.\n\n### Statistics :\n* As of 2016, 28.2 million U.S. adultsTrusted Source were diagnosed with heart disease. In 2015, nearly 634,000 Trusted Source people died of heart disease, making it the leading cause of death.\n\n* According to the American Heart Association, approximately every 40 seconds an American will have a heart attack. The estimated annual incidence of heart attacks in the United States is 720,000 new attacks and 335,000 recurrent attacks.\n\n### Symptoms can include:\n* Chest pain, chest tightness, chest pressure and chest discomfort (angina)\n* Shortness of breath\n* Pain, numbness, weakness or coldness in your legs or arms if the blood vessels in those parts of your body are narrowed\n* Pain in the neck, jaw, throat, upper abdomen or back\n\n### Risk factors :\n* Tobacco use. Chewing tobacco, smoking and long-term exposure to secondhand smoke damage the interior walls of arteries — including arteries to your heart — allowing deposits of cholesterol to collect and block blood flow.\n\n* Diabetes. Diabetes is the inability of your body to produce enough or respond to insulin properly. Insulin, a hormone secreted by your pancreas, allows your body to use glucose, which is a form of sugar from foods. Diabetes increases the risk of coronary artery disease, which leads to angina and heart attacks by speeding up atherosclerosis and increasing your cholesterol levels.\n\n* High blood pressure. Blood pressure is determined by the amount of blood your heart pumps and the amount of resistance to blood flow in your arteries. Over time, high blood pressure damages arteries by accelerating hardening of the arteries.\n\n* High blood cholesterol or triglyceride levels. Cholesterol is a major part of the deposits that can narrow arteries throughout your body, including those that supply your heart. A high level of the wrong kind of cholesterol, known as low-density lipoprotein (LDL) cholesterol (the \"bad\" cholesterol), increases your risk of angina and heart attacks. A high level of triglycerides, a type of blood fat related to your diet, also is undesirable.\n\n* Family history of heart disease. If a family member has coronary artery disease or has had a heart attack, you're at a greater risk of developing angina.\n\n* Older age. Men older than 45 and women older than 55 have a greater risk than do younger adults.\n\n* Lack of exercise. An inactive lifestyle contributes to high cholesterol, high blood pressure, type 2 diabetes and obesity. However, it is important to talk with your doctor before starting an exercise program.\n\n* Obesity. Obesity raises the risk of angina and heart disease because it's associated with high blood cholesterol levels, high blood pressure and diabetes. Also, your heart has to work harder to supply blood to the excess tissue.\n\n* Stress. Stress can increase your risk of angina and heart attacks. Too much stress, as well as anger, also can raise your blood pressure. Surges of hormones produced during stress can narrow your arteries and worsen angina."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport seaborn as sns\nsns.set(style=\"darkgrid\")\n\n#Ignoring the warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.preprocessing import LabelEncoder # Encode Categorical Variable to Numerical Variable\nfrom sklearn.preprocessing import Imputer # Imputer Class to replace missing values\nfrom sklearn.metrics import confusion_matrix # Library for model evaluation\nfrom sklearn.metrics import accuracy_score # Library for model evaluation\nfrom sklearn.model_selection import train_test_split # Library to split datset into test and train\n\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.linear_model  import LogisticRegression # Logistic Regression Classifier\nfrom sklearn.linear_model import SGDClassifier # Stochastic Gradient Descent Classifier\nfrom sklearn.tree import DecisionTreeClassifier # Decision Tree Classifier\nfrom sklearn.ensemble  import RandomForestClassifier # Random Forest Classifier\nfrom sklearn.neighbors import KNeighborsClassifier # K Nearest neighbors Classifier\nfrom sklearn.naive_bayes import GaussianNB #Naive Bayes Classifier\nfrom sklearn.svm import SVC #Support vector Machine Classifier\nfrom sklearn.ensemble import AdaBoostClassifier # Ada Boost Classifier\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix, roc_auc_score\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import average_precision_score\nfrom sklearn.model_selection import cross_val_score, GridSearchCV, KFold, RandomizedSearchCV, train_test_split\nfrom sklearn.metrics import average_precision_score, precision_recall_curve\nfrom sklearn.model_selection import GridSearchCV\n\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom pdpbox import pdp, info_plots\nimport shap\nshap.initjs()\nimport lime\nimport lime.lime_tabular","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%javascript\nIPython.OutputArea.prototype._should_scroll = function(lines) {\n    return false;\n}","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# First Look into the data\nheart_df = pd.read_csv('/kaggle/input/heart-disease-uci/heart.csv')\nprint(f'Rows - {heart_df.shape[0]}, Columns - {heart_df.shape[1]}')\nheart_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the datatype of the columns\nheart_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"heart_df.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All Columns are numeric. Here are the column details\n* age      - Person's Age in years\n\n* sex      - Gender (1 = male; 0 = female)\n\n* cp       - Chest Pain (1: typical angina ; 2: atypical angina ; 3: non-anginal pain ; 4: asymptomatic)\n    * Angina is chest pain or discomfort caused when your heart muscle doesn't get enough oxygen-rich blood. It may feel like pressure or       squeezing in your chest\n    * Atypical angina implies that the complaint is actually angina pectoris, though not conforming in every way to the expected or\n      classic description\n    * Non-anginal pain - A chest pain is very likely nonanginal if its duration is over 30 minutes or less than 5 seconds.\n    * means neither causing nor exhibiting symptoms of disease.\n    \n\n* trestbps - Resting blood pressure (in mm Hg on admission to the hospital) . Optimal blood pressure typically is defined as 120 mm Hg systolic — which is the pressure as your heart beats — over 80 mm Hg diastolic — which is the pressure as your heart relaxes. For your resting heart rate, the target is between 60 and 100 beats per minute (BPM)\n\n* chol     - Serum Cholestoral in mg/dl . Total cholesterol levels less than 200 milligrams per deciliter (mg/dL) are considered desirable for adults. A reading between 200 and 239 mg/dL is considered borderline high and a reading of 240 mg/dL and above is considered high\n\n* fbs      - Fasting blood sugar > 120 (mg/dl) (1 = true; 0 = false) . A fasting blood sugar level less than 100 mg/dL (5.6 mmol/L) is normal. A fasting blood sugar level from 100 to 125 mg/dL (5.6 to 6.9 mmol/L) is considered prediabetes. If it's 126 mg/dL (7 mmol/L) or higher on two separate tests, you have diabetes\n\n* restecg  - Resting electrocardiographic results (0: normal ; 1: having ST-T wave abnormality ; 2: showing probable or definite left  ventricular hypertrophy by Estes' criteria).Resting echocardiography is a non-invasive test that can assess ventricular function, heart valve anatomy and function and regional wall motion abnormalities.\n\n* thalach  - Maximum heart rate achieved\n\n* exang    - Exercise induced angina (1 = yes; 0 = no). When you increase the demand for oxygen, such as when you exercise, this can cause angina\n\n* oldpeak  - ST depression induced by exercise relative to rest . ST depression refers to a finding on an electrocardiogram wherein the trace in the ST segment is abnormally low below the baseline\n\n* slope    - Slope of the peak exercise ST segment (1: upsloping ; 2: flat ; 3: downsloping)\n\n* ca       - Number of major vessels (0-3) colored by flourosopy\n\n* thal     - 3 = normal; 6 = fixed defect; 7 = reversable defect . Thalassemia is a blood disorder passed down through families (inherited) in which the body makes an abnormal form or inadequate amount of hemoglobin.\n\n* target   - Heart Disease (1 - Yes ; 0 - No)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Renaming the columns for better understanding\nheart_df = heart_df.rename(columns={'cp' : 'chest_pain', \n                                    'trestbps' : 'rest_blood_pressure', \n                                    'chol' : 'cholesterol', \n                                    'fbs' : 'fast_blood_sugar', \n                                    'restecg' : 'rest_ecg', \n                                    'thalach' : 'max_heart_rate',\n                                    'exang' : 'exercise_ind_angina', \n                                    'oldpeak' : 'st_dep_ind_exc', \n                                    'slope' : 'slope_peak_exer_st', \n                                    'ca' : 'no_vessels_colored_fl', \n                                    'thal' : 'thalassemia'})\nheart_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"heart_df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Observations :\n\n1. All rows have count=303. So no misssing data.\n2. sex/Gender - Max we have 1 . So Male is majority. We dont know the distribution yet.\n3. blood_pressure in the dataset varies from 94 to 200. Median is 130.\n4. cholestrol varies from 126 to 564 with median as 240 . \n5. Fasting blood sugar > 120 mg/dl is a categorical field having > 120 mg/dl as majority . \n6. rest_ecg is a categorical field with values ranging 0 to 2 with majority as 2.\n7. heart rate varies 71 to 202 with median as 153\n8. exercise_ind_angina is categorical field with 0 & 1 , 1 being the majority\n8. st_dep_ind_exc varies 0 to 6.2 with median depression at 0.8\n9. slope_peak_exer_st is a categorical field with value 0, 1 and 2\n10. no_vessels_colored_fl is a categorical field with value 0 to 4 with 4 being majority\n11. thalassemia is a categorical field with value 0 to 3 with 3 being majority\n12. Target - Has 2 values 0 & 1 with 1 being majority"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Distribution of the target variable\nround(heart_df['target'].value_counts(normalize=True), 2)*100","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Almost balanced dataset with heart disease yes as 54% and  non heart disease as 46%"},{"metadata":{},"cell_type":"markdown","source":"# Univariate Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_cols  = ['target', 'sex', 'chest_pain', 'rest_ecg', 'exercise_ind_angina', 'slope_peak_exer_st', 'no_vessels_colored_fl', 'thalassemia', 'fast_blood_sugar']\ncont_cols = ['age', 'rest_blood_pressure', 'cholesterol', 'max_heart_rate', 'st_dep_ind_exc', 'age']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"heart_df_uv = heart_df.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Printing the unique values of Categorcal columns\nfor cols in cat_cols:\n    print(cols, '-', heart_df_uv[cols].unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create Numeric to Value mapping for better graphical representation\nmap_sex = {0: 'female', 1: 'male'}\nmap_chest_pain = {0:'none', 1:'typ_ang', 2:'atyp_ang', 3:'non_ang'}\nmap_rest_ecg = {0:'normal', 1:'st-t abnorm', 2:'left_vent_hyptrophy'}\nmap_exercise_ind_angina = {0:'no', 1:'yes'}\nmap_slope_peak_exer_st = {0:'unknown', 1:'upsloping', 2:'flat'}\nmap_thal = {0:'unknown0', 1:'unknown1', 2:'unknown2', 3:'normal'}\nmap_fast_blood_sugar = {0:'>120',1:'<=120'}\nmap_target={0:'No', 1:'Yes'}\n\n# Updating the mapping in dataframe\nheart_df_uv[\"sex\"] = heart_df_uv[\"sex\"].map(map_sex)\nheart_df_uv[\"chest_pain\"] = heart_df_uv[\"chest_pain\"].map(map_chest_pain)\nheart_df_uv[\"rest_ecg\"] = heart_df_uv[\"rest_ecg\"].map(map_rest_ecg)\nheart_df_uv[\"exercise_ind_angina\"] = heart_df_uv[\"exercise_ind_angina\"].map(map_exercise_ind_angina)\nheart_df_uv[\"slope_peak_exer_st\"] = heart_df_uv[\"slope_peak_exer_st\"].map(map_slope_peak_exer_st)\nheart_df_uv[\"thalassemia\"] = heart_df_uv[\"thalassemia\"].map(map_thal)\nheart_df_uv[\"fast_blood_sugar\"] = heart_df_uv[\"fast_blood_sugar\"].map(map_fast_blood_sugar)\nheart_df_uv[\"target\"] = heart_df_uv[\"target\"].map(map_target)\nheart_df_uv.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Univariate Analysis of Categorical Columns\nrows = 3 ; columns = 3\ncat_cols_np_arr = np.array(cat_cols).reshape(3, 3)\n\nf, axes = plt.subplots(rows, columns, figsize=(20, 20))\nprint('Univariate Analysis of Categorical Variables')\nfor row in range(rows):\n    for column in range(columns):\n        sns.countplot(heart_df_uv[cat_cols_np_arr[row][column]], \n                      palette = \"Set1\", \n                      ax = axes[row, column])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Fairly balanced datset in terms of target variable.\n* Female sample size is almost half as male.\n* Chest pain none is majority. This filed has data issue and 0 is not defined in mapping. Assuming 0 means no pain\n* thalassemia also has data issues with mapping not provided for 0, 1 and 2 . Majority is 2.\n* Sample size for fasting blood sugar > 120mg/dl is more."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Univariate Analysis of Continuous Columns\nrows = 2 ; columns = 3\ncont_cols_np_arr = np.array(cont_cols).reshape(2,3)\n\nf, axes = plt.subplots(rows, columns, figsize=(20, 10))\nprint('Univariate Analysis of Continuous Variables')\n\nfor row in range(rows):\n    for column in range(columns):\n        sns.distplot(heart_df[cont_cols_np_arr[row][column]], \n                     ax = axes[row, column], \n                     hist=True, \n                     kde=True, \n                     color='b',\n                     hist_kws={\"linewidth\": 3,\n                            \"alpha\": 1, \"color\": \"b\"}\n                     )\nplt.tight_layout()\nf.delaxes(ax= axes[1,2])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* age  field is normal distributed with majority band falling in the category of 50-65 years.\n* resting blood presssure follows normal distribution with majority b/w 120-140, median at 130\n* cholesterol follows normal distribution with 200-300 is majority. Under 200 is ideal.\n* max heart rate , slightly left skewed has heart rate from 50 to 225. Ideal is 60-100 depending on age.\n* ST depression induced by exercise relative to rest is right tailed varying b/w 0 to 6."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Univariate Analysis of Continuous Columns\nrows = 2 ; columns = 3\ncont_cols_np_arr = np.array(cont_cols).reshape(2,3)\n\nf, axes = plt.subplots(rows, columns, figsize=(20, 10))\nprint('Univariate Analysis of Categorical Variables')\nfor row in range(rows):\n    for column in range(columns):\n        sns.boxplot(heart_df[cont_cols_np_arr[row][column]], ax = axes[row, column], orient=\"v\", color=\"skyblue\")\nf.delaxes(ax= axes[1,2])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Resting blood pressure 170 an above stands outlier\n* Cholesterol above 400 is an outlier. Ideal is below 200. Above 400 brings big risk of heart attack.\n* ST depression induced by exercise relative to rest 4-6 seems above the IQR."},{"metadata":{},"cell_type":"markdown","source":"# Bivariate Analysis"},{"metadata":{},"cell_type":"markdown","source":"## Categorical & Target"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Bivariate Analysis of Categorical Columns\ncat_cols  = ['sex', 'chest_pain', 'rest_ecg', 'exercise_ind_angina', 'slope_peak_exer_st', 'no_vessels_colored_fl', 'thalassemia' ,'fast_blood_sugar']\nprint('Bivariate Analysis of Categorical Variables')\nfor row in cat_cols:\n    sns.catplot(x=row, \n                col=\"target\",\n                data=heart_df_uv, \n                kind=\"count\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* More males have heart disease\n* atypcal angina is contributing most to the heart disease. Chest pain 0 (assumed as None definitely means no heart diseases)\n* st-t abnormality is contributing most to the heart disease. if 'normal' , high chances there is no heart disease\n* Majority of the sample having heart disease is not exercise induced.\n* Slop_peak_exer_st 'yes' definitely indicating higher chances of heart disease\n* No of colored vessels 0 indicates heart disease\n* thal, value -2 indicates higher chances\n* high fasting blood sugar also indicates higher chances of heart disease"},{"metadata":{},"cell_type":"markdown","source":"## Continuous & Target"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Bivariate Analysis of Continuous Columns\nprint('Bivariate Analysis of Continuous Variables')\n\ncont_cols = ['age', 'rest_blood_pressure', 'cholesterol', 'max_heart_rate', 'st_dep_ind_exc', 'age']\nrows = 2 ; columns = 3\ncont_cols_np_arr = np.array(cont_cols).reshape(2,3)\n\nf, axes = plt.subplots(rows, columns, figsize=(20, 10))\nfor row in range(rows):\n    for column in range(columns):\n        sns.swarmplot(x='target', \n                    y=cont_cols_np_arr[row][column],\n                    data=heart_df_uv,\n                    ax = axes[row, column],\n                   )\nf.delaxes(ax= axes[1,2])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Impact of age on Heart Disease')\nbins = pd.cut(heart_df_uv['age'], [0, 20, 40, 60, 80, 90, 100])\ndf = heart_df_uv.groupby([bins, 'target'])['target'].agg(['count']).reset_index().pivot(index='age', columns='target', values='count').fillna(0).reset_index()\ndf['Yes(%)'] = round((df['Yes']/(df['Yes'] + df['No'])*100),0)\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Impact of blood pressure on Heart Disease')\nbins = pd.cut(heart_df_uv['rest_blood_pressure'], [0, 100, 120, 140, 160, 180, 200])\ndf = heart_df_uv.groupby([bins, 'target'])['target'].agg(['count']).reset_index().pivot(index='rest_blood_pressure', columns='target', values='count').fillna(0).reset_index()\ndf['Yes(%)'] = round((df['Yes']/(df['Yes'] + df['No'])*100),0)\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Impact of cholesterol on Heart Disease')\nbins = pd.cut(heart_df_uv['rest_blood_pressure'], [0, 100, 120, 140, 160, 180, 200])\ndf = heart_df_uv.groupby([bins, 'target'])['target'].agg(['count']).reset_index().pivot(index='rest_blood_pressure', columns='target', values='count').fillna(0).reset_index()\ndf['Yes(%)'] = round((df['Yes']/(df['Yes'] + df['No'])*100),0)\ndf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Multivariate Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,15)) \nsns.heatmap(heart_df.corr(),annot=True,cmap='cubehelix_r') #draws  heatmap with input as the correlation matrix calculted by(iris.corr())\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Chespain, max_heart_rate and slope_peak shows moderate postive co-relation\n* exercise_ind_angina & st_dep_ind_exc shows moderate negative corelation"},{"metadata":{"trusted":true},"cell_type":"code","source":"# For Single Classifier Creating function for Confusion Matrix , Precision, Recall and F1 Score\ndef plot_confusion_matrix(classifier, y_test, y_pred):\n    cm = confusion_matrix(y_test, y_pred)\n    \n    print(\"\\n\",classifier,\"\\n\")\n    plt.clf()\n    plt.imshow(cm, interpolation='nearest', cmap='RdBu')\n    classNames = ['Heart Disease-No','Heart Disease-Yes']\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    tick_marks = np.arange(len(classNames))\n    plt.xticks(tick_marks, classNames, rotation=45)\n    plt.yticks(tick_marks, classNames)\n    s = [['TN','FP'], ['FN', 'TP']]\n    \n    for i in range(2):\n        for j in range(2):\n            plt.text(j,i, str(s[i][j])+\" = \"+str(cm[i][j]), \n                     horizontalalignment='center', color='White')\n    \n    plt.show()\n        \n    tn, fp, fn, tp = cm.ravel()\n\n    recall = tp / (tp + fn)\n    precision = tp / (tp + fp)\n    F1 = 2*recall*precision/(recall+precision)\n\n    print('Recall={0:0.3f}'.format(recall),'\\nPrecision={0:0.3f}'.format(precision))\n    print('F1={0:0.3f}'.format(F1))\n    return;","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#For Single Classifier Creating function to plot precision_recall_curve\nfrom sklearn.metrics import average_precision_score, precision_recall_curve\ndef plot_prec_rec_curve(classifier, y_test, y_pred):\n    precision, recall, _ = precision_recall_curve(y_test, y_pred)\n    average_precision = average_precision_score(y_test, y_pred)\n\n#     print('Average precision-recall score: {0:0.3f}'.format(\n#           average_precision))\n\n    plt.plot(recall, precision, label='area = %0.3f' % average_precision, color=\"green\")\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('Recall')\n    plt.ylabel('Precision')\n    plt.title('Precision Recall Curve')\n    plt.legend(loc=\"best\")\n    plt.grid(True)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Function for Precsion, Recall and F1 Score\ndef calc_classfier_metric(classifier, y_true, y_pred):\n    '''\n    Function for Precsion, Recall and F1 Score\n    '''\n    accuracy      = accuracy_score(y_test, y_pred)\n    precision     = precision_score(y_test, y_pred)\n    recall        = recall_score(y_test, y_pred)\n    F1_score      = f1_score(y_test, y_pred)\n    roc_auc_scr   = roc_auc_score(y_test, y_pred)\n    conf_mat      = confusion_matrix(y_test, y_pred)\n    return accuracy, precision, recall, F1_score, roc_auc_scr, conf_mat;","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Function for Confusion Matrix\ndef view_confusion_matrix(class_perf_df, columns):\n    '''\n    Function for Confusion Matrix\n    '''\n    rows = int(class_perf_df.shape[0]/ columns)\n    plt.figure(figsize=(15,13))\n\n    for i in range(class_perf_df.shape[0]):\n        plt.subplot(rows,columns,i+1)\n        plt.title(class_perf_df['Classifier'].loc[i])\n        ax=sns.heatmap(class_perf_df['Conf_Mtrx'].loc[i],\n                    annot=True,\n                    cmap=\"coolwarm\",\n                    fmt=\"d\",\n                    cbar=False, \n                    annot_kws={\"size\": 12},\n                    linewidths=1.2,\n                    linecolor='w',\n                   )\n        ax.set_xticklabels(ax.get_xticklabels(), rotation = 0, fontsize = 10)\n        ax.set_yticklabels(ax.get_yticklabels(), rotation = 25, fontsize = 10)\n        ax.set_xlabel('True label') \n        ax.set_ylabel('Predicted label')\n    \n    return;","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Build Classification Model\ndef build_class_model(X_train, X_test, y_train, y_test, classifier_model):\n    '''\n    Functio to Build Classification Model\n    '''\n    classifier_performance = []\n    cnf_lst = []\n\n    for classifier in classifier_model:\n\n        # Fitting the training set into classification model\n        classifier.fit(X_train,y_train)\n\n        # Predicting the output on test datset\n        y_pred = classifier.predict(X_test)    \n\n        # Cross Validation Score on training test\n        scores = cross_val_score(classifier, X_train,y_train, cv=5)\n        cv_score_mean = scores.mean()\n\n        # Classification score\n        accuracy, precision, recall, F1_score, roc_auc_scr, conf_mat = calc_classfier_metric(classifier, y_test, y_pred)\n        classifier_performance.append([classifier.__class__.__name__, cv_score_mean, conf_mat, accuracy, precision, recall, F1_score, roc_auc_scr])\n\n        class_perf_df = pd.DataFrame(classifier_performance, columns=['Classifier', 'Training_CV_Score', 'Conf_Mtrx', 'Accuracy', 'Precision', 'Recall', 'F1_Score', 'ROC_AUC_Scr']).sort_values('F1_Score', ascending = False)\n    \n    return class_perf_df;","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create Test Train Split\nX = heart_df.drop(columns=['target'])\ny = heart_df['target']\n\n# #Dividing data into test & train splitting 70% data for training anf 20% for test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\nprint('There are {} samples in the training set and {} samples in the test set'.format(X_train.shape[0], X_test.shape[0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Machine Learning Model Build\nclassifier_model = [DummyClassifier(strategy= 'most_frequent', random_state=42),\n                    DecisionTreeClassifier(random_state=42),\n                    RandomForestClassifier(random_state=42), \n                    AdaBoostClassifier(random_state=42), \n                    XGBClassifier(objective=\"binary:logistic\", random_state=42),\n                    LGBMClassifier(random_state=42)\n                   ]\n\n# Call Classification module\nclass_perf_df = build_class_model(X_train, X_test, y_train, y_test, classifier_model)\n\n# Show Confusion Matrix\nview_confusion_matrix(class_perf_df, columns=3)\n\n# Show Classification Summary\nclass_perf_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Hyper Paramater Tuning using Grid Search - Random Forest\nfrom sklearn.model_selection import GridSearchCV\nrf_classifier = RandomForestClassifier(random_state = 42)\ngrid_params = {'n_estimators': [1, 2, 5, 7, 10, 15, 20, 50, 100],\n               'criterion':['gini', 'entropy'],\n               'max_depth': [None, 2, 3, 5, 7, 10]\n              }\n\ngrid_search = GridSearchCV(rf_classifier, param_grid = grid_params, scoring='f1', cv=5).fit(X_train, y_train)\nprint('Grid Search best parameters:')\ngrid_search.best_estimator_, grid_search.best_params_, grid_search.best_score_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Random Forest Model with Grid Paramaters\nrf_classifier = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n                        max_depth=None, max_features='auto', max_leaf_nodes=None,\n                        min_impurity_decrease=0.0, min_impurity_split=None,\n                        min_samples_leaf=1, min_samples_split=2,\n                        min_weight_fraction_leaf=0.0, n_estimators=20,\n                        n_jobs=None, oob_score=False, random_state=42, verbose=0,\n                        warm_start=False)\nrf_classifier.fit(X_train,y_train)\n\n# Predicting the output on test datset\ny_pred = rf_classifier.predict(X_test)    \n\n# Classification score\nplot_confusion_matrix(rf_classifier, y_test, y_pred)\nplot_prec_rec_curve(rf_classifier, y_test, y_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* F1 Score improved from 85.4 to 87.1"},{"metadata":{},"cell_type":"markdown","source":"# Model Explanation"},{"metadata":{},"cell_type":"markdown","source":"## 1. Feature Importance"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nfeature_importances = pd.DataFrame(rf_classifier.feature_importances_,\n                                   index = X_train.columns,\n                                   columns=['Feature_Importance']).sort_values('Feature_Importance', ascending=True)\nfeature_importances.plot(kind='barh')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. ELI5"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Permutation Importance\nimport eli5\nfrom eli5.sklearn import PermutationImportance","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Feature Importances\n# Return an explanation of estimator parameters (weights)\n# eli5.explain_weights(rf_classifier, feature_names = X_train.columns.tolist())\neli5.show_weights(rf_classifier, feature_names = X_train.columns.tolist())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Show Permutation Importance\nperm = PermutationImportance(rf_classifier, random_state=42).fit(X_train, y_train)\neli5.show_weights(perm, feature_names = X_train.columns.tolist())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Features\\n\", X_test.iloc[0], \"\\n Decision\", y_test.iloc[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Returns an explanation of estimator prediction\n# Examining individual predictions using Show Prediction for 0 ( No Heart Disease )\neli5.show_prediction(rf_classifier, \n                     doc=X_test.iloc[0], \n                     feature_names = X_test.columns.tolist(),\n                     show_feature_values=True\n                    )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Features\\n\", X_test.iloc[5], \"\\n Decision\", y_test.iloc[5])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Examining individual predictions using Show Prediction for 1 ( Heart Disease )\neli5.show_prediction(rf_classifier, \n                     doc=X_test.iloc[5], \n                     feature_names = X_test.columns.tolist(),\n                     show_feature_values=True\n                    )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. SHAP\n(Reference : https://github.com/slundberg/shap)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# SHAP - Feature Importance\n\n# Create object that can calculate shap values\nexplainer = shap.TreeExplainer(rf_classifier)\n\n# calculate shap values. This is what we will plot.\n# Calculate shap_values for all of val_X rather than a single row, to have more data for plot.\nshap_values = explainer.shap_values(X_test)\n\n# Make plot. Index of [1] is explained in text below.\nshap.summary_plot(shap_values[1], X_test, plot_type=\"bar\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# SHAP - Summary Plot\n\n# Create object that can calculate shap values\nexplainer = shap.TreeExplainer(rf_classifier)\n\n# calculate shap values. This is what we will plot.\n# Calculate shap_values for all of val_X rather than a single row, to have more data for plot.\nshap_values = explainer.shap_values(X_test)\n\n# Make plot. Index of [1] is explained in text below.\nshap.summary_plot(shap_values[1], X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# SHAP -  Force Plot\n\n#shap.DeepExplainer works with Deep Learning models.\n#shap.KernelExplainer works with all models\n\n# Create object that can calculate shap values\nexplainer = shap.TreeExplainer(rf_classifier)\n\n# Calculate Shap values\nshap_values = explainer.shap_values(X_test.iloc[5])\n\n# Force Plot for prediction\nshap.force_plot(explainer.expected_value[1], shap_values[1], X_test.iloc[5])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# use Kernel SHAP to explain test set predictions\nk_explainer = shap.KernelExplainer(rf_classifier.predict_proba, X_train)\nk_shap_values = k_explainer.shap_values(X_test.iloc[0])\nshap.force_plot(k_explainer.expected_value[1], k_shap_values[1], X_test.iloc[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# SHAP - Dependence Plot\n\n# Create object that can calculate shap values\nexplainer = shap.TreeExplainer(rf_classifier)\n\n# calculate shap values. This is what we will plot.\nshap_values = explainer.shap_values(heart_df)\n\n# make plot.\nshap.dependence_plot('chest_pain', shap_values[1], heart_df, interaction_index=\"target\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"In this experimentation, we performed EDA, built a model, improved a model using grid search and explained the predictions.\n\nIf you liked the kernel, please feel free to upvote.\nIf you want to connect over Linkedin - https://www.linkedin.com/in/jagannath-banerjee/\n\nThanks!"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}