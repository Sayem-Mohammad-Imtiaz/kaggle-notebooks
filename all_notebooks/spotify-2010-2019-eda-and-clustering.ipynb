{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-10T11:10:21.9601Z","iopub.execute_input":"2021-06-10T11:10:21.960487Z","iopub.status.idle":"2021-06-10T11:10:21.965584Z","shell.execute_reply.started":"2021-06-10T11:10:21.960456Z","shell.execute_reply":"2021-06-10T11:10:21.964687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly as py\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.offline import plot\nimport numpy as np\nimport seaborn as sb\nfrom matplotlib import rcParams\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import PCA\nfrom scipy.cluster.hierarchy import linkage, dendrogram\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.cluster import AgglomerativeClustering\nimport statsmodels.api as sm\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.cluster import KMeans\nfrom sklearn.cluster import AgglomerativeClustering","metadata":{"execution":{"iopub.status.busy":"2021-06-10T11:09:23.788848Z","iopub.execute_input":"2021-06-10T11:09:23.789261Z","iopub.status.idle":"2021-06-10T11:09:25.079031Z","shell.execute_reply.started":"2021-06-10T11:09:23.789223Z","shell.execute_reply":"2021-06-10T11:09:25.078033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Data input\ndf10 = pd.read_csv('/kaggle/input/top-spotify-songs-from-20102019-by-year/top10s.csv',encoding='ISO-8859-1')","metadata":{"execution":{"iopub.status.busy":"2021-06-10T11:12:03.606955Z","iopub.execute_input":"2021-06-10T11:12:03.607309Z","iopub.status.idle":"2021-06-10T11:12:03.619069Z","shell.execute_reply.started":"2021-06-10T11:12:03.607278Z","shell.execute_reply":"2021-06-10T11:12:03.618024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Drop year, pop columns\nyearless_df = df10.drop(['year', 'pop'], axis=1)\nyearless_df=yearless_df.drop_duplicates()","metadata":{"execution":{"iopub.status.busy":"2021-06-10T11:13:54.151768Z","iopub.execute_input":"2021-06-10T11:13:54.152404Z","iopub.status.idle":"2021-06-10T11:13:54.162526Z","shell.execute_reply.started":"2021-06-10T11:13:54.152365Z","shell.execute_reply":"2021-06-10T11:13:54.161366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#genre check\nyearless_df['top genre'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-06-10T11:14:54.444754Z","iopub.execute_input":"2021-06-10T11:14:54.445128Z","iopub.status.idle":"2021-06-10T11:14:54.460077Z","shell.execute_reply.started":"2021-06-10T11:14:54.445095Z","shell.execute_reply":"2021-06-10T11:14:54.458602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Reorganize genre\nfor i in yearless_df['top genre']:\n    if 'pop' in i :\n        yearless_df['top genre'] = yearless_df['top genre'].replace(i, 'pop')\n    elif 'hip hop' in i :\n        yearless_df['top genre'] = yearless_df['top genre'].replace(i, 'hip hop')\n    elif 'edm' in i:\n        yearless_df['top genre'] = yearless_df['top genre'].replace(i, 'edm')\n    elif 'r&b' in i:\n        yearless_df['top genre'] = yearless_df['top genre'].replace(i, 'pop')\n    elif 'latin' in i:\n        yearless_df['top genre'] = yearless_df['top genre'].replace(i, 'latin')\n    elif 'room' in i:\n        yearless_df['top genre'] = yearless_df['top genre'].replace(i, 'room')\n    elif 'electro' in i:\n        yearless_df['top genre'] = yearless_df['top genre'].replace(i, 'edm')\n    elif 'house' in i:\n        yearless_df['top genre'] = yearless_df['top genre'].replace(i, 'house')\n        \nfor i in df10['top genre']:\n    if 'pop' in i :\n        df10['top genre'] = df10['top genre'].replace(i, 'pop')\n    elif 'hip hop' in i :\n        df10['top genre'] = df10['top genre'].replace(i, 'hip hop')\n    elif 'edm' in i:\n        df10['top genre'] = df10['top genre'].replace(i, 'edm')\n    elif 'r&b' in i:\n        df10['top genre'] = df10['top genre'].replace(i, 'pop')\n    elif 'latin' in i:\n        df10['top genre'] = df10['top genre'].replace(i, 'latin')\n    elif 'room' in i:\n        df10['top genre'] = df10['top genre'].replace(i, 'room')\n    elif 'electro' in i:\n        df10['top genre'] = df10['top genre'].replace(i, 'edm')\n    elif 'house' in i:\n        df10['top genre'] = df10['top genre'].replace(i, 'house')\n    \nyearless_df['top genre'] = yearless_df['top genre'].replace('complextro', 'edm')\nyearless_df['top genre'] = yearless_df['top genre'].replace('chicago rap', 'hip hop')\n\ndf10['top genre'] = df10['top genre'].replace('complextro', 'edm')\ndf10['top genre'] = df10['top genre'].replace('chicago rap', 'hip hop')\n","metadata":{"execution":{"iopub.status.busy":"2021-06-10T11:16:02.443154Z","iopub.execute_input":"2021-06-10T11:16:02.443557Z","iopub.status.idle":"2021-06-10T11:16:02.981722Z","shell.execute_reply.started":"2021-06-10T11:16:02.443524Z","shell.execute_reply":"2021-06-10T11:16:02.980621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Change columns name\ndf = yearless_df\n\ndf.rename(columns={'title': 'song', \n                   'artist':'artist',\n                   'top genre':'genre',\n                   'bpm':'bpm',\n                   'nrgy': 'energy',\n                   'dnce': 'danceability',\n                   'dB':'loudness',\n                   'live': 'liveness',\n                   'val': 'valence',\n                   'dur': 'length',\n                   'acous': 'acoutsticness',\n                   'spch': 'speechiness'}, inplace=True)\n\ndf10.rename(columns={'title': 'song', \n                   'artist':'artist',\n                   'top genre':'genre',\n                   'bpm':'bpm',\n                   'nrgy': 'energy',\n                   'dnce': 'danceability',\n                   'dB':'loudness',\n                   'live': 'liveness',\n                   'val': 'valence',\n                   'dur': 'length',\n                   'acous': 'acoutsticness',\n                   'spch': 'speechiness'}, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T11:16:32.523161Z","iopub.execute_input":"2021-06-10T11:16:32.523551Z","iopub.status.idle":"2021-06-10T11:16:32.532709Z","shell.execute_reply.started":"2021-06-10T11:16:32.523519Z","shell.execute_reply":"2021-06-10T11:16:32.531424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"EDA","metadata":{}},{"cell_type":"code","source":"#Top 10 genre pie chart\ndf1=df['genre'].value_counts().head(10)\ndf1.index\nfig = px.pie(df1, names=df1.index, values='genre', title = 'Popular genre',labels='genre')\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-10T11:18:06.309057Z","iopub.execute_input":"2021-06-10T11:18:06.309626Z","iopub.status.idle":"2021-06-10T11:18:06.442244Z","shell.execute_reply.started":"2021-06-10T11:18:06.309573Z","shell.execute_reply":"2021-06-10T11:18:06.441294Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Popular artist\nfig=px.bar(df, x = df['artist'].value_counts().head(10), y=df['artist'].value_counts().head(10).index, \n           title = 'Popular artist')\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-10T11:19:13.589055Z","iopub.execute_input":"2021-06-10T11:19:13.589444Z","iopub.status.idle":"2021-06-10T11:19:13.679503Z","shell.execute_reply.started":"2021-06-10T11:19:13.58941Z","shell.execute_reply":"2021-06-10T11:19:13.678312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Variable relationships\nfig = px.scatter(df10,x='loudness', y='energy',color='energy',hover_name='song',hover_data=['artist','year'],title='Relationship loudness and energy')\nfig.show()\n\nfig = px.scatter(df10,x='danceability', y='bpm',color='bpm',hover_name='song',hover_data=['artist','year'], title='Relationship Danceability and bpm')\nfig.show()\n\nfig = px.scatter(df10,x='danceability', y='valence',color='valence',hover_name='song',hover_data=['artist','year'],title='Relationship Danceability and valence')\nfig.show()\n","metadata":{"execution":{"iopub.status.busy":"2021-06-10T11:20:58.159271Z","iopub.execute_input":"2021-06-10T11:20:58.15967Z","iopub.status.idle":"2021-06-10T11:20:58.398647Z","shell.execute_reply.started":"2021-06-10T11:20:58.159631Z","shell.execute_reply":"2021-06-10T11:20:58.397523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#most popular artist in 2019\nfig = px.scatter(df10.query('year==2019'), y='pop', x='artist', hover_name='song', color='pop', title = 'Popular artist in 2019' )\nfig.show()\n\n#most popular artist in 2018\nfig = px.scatter(df10.query('year==2018'), y='pop', x='artist', hover_name='song', color='pop', title = 'Popular artist in 2018' )\nfig.show()\n\n#most popular artist in 2017\nfig = px.scatter(df10.query('year==2017'), y='pop', x='artist', hover_name='song', color='pop', title = 'Popular artist in 2017' )\nfig.show()\n\n#most popular artist in 2016\nfig = px.scatter(df10.query('year==2016'), y='pop', x='artist', hover_name='song', color='pop', title = 'Popular artist in 2016' )\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-10T11:22:00.101329Z","iopub.execute_input":"2021-06-10T11:22:00.101683Z","iopub.status.idle":"2021-06-10T11:22:00.372035Z","shell.execute_reply.started":"2021-06-10T11:22:00.101652Z","shell.execute_reply":"2021-06-10T11:22:00.371228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Clustering","metadata":{}},{"cell_type":"code","source":"#Change null values\ndf.bpm = df.bpm.replace(0, df.bpm.mean())\ndf.bpm.unique()\n\ndf.loudness = df.loudness.replace(-60, df.loudness.mean())\ndf.loudness.unique()","metadata":{"execution":{"iopub.status.busy":"2021-06-10T11:24:04.233592Z","iopub.execute_input":"2021-06-10T11:24:04.233974Z","iopub.status.idle":"2021-06-10T11:24:04.246821Z","shell.execute_reply.started":"2021-06-10T11:24:04.233942Z","shell.execute_reply":"2021-06-10T11:24:04.246014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Replace with nominal scale dummy variable\ndf.drop(columns='artist', inplace=True)\nnew_df = df\ntemp_df = pd.get_dummies(new_df[['genre']])\nnew_df = new_df.join(temp_df, how='left')\nnew_df = new_df.drop(columns = ['genre'], axis=1)\nnew_df.set_index('song', inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T11:26:43.956221Z","iopub.execute_input":"2021-06-10T11:26:43.956603Z","iopub.status.idle":"2021-06-10T11:26:43.975913Z","shell.execute_reply.started":"2021-06-10T11:26:43.956572Z","shell.execute_reply":"2021-06-10T11:26:43.975039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#standardscaler\nX_std = StandardScaler().fit_transform(new_df)\n\n#Reduce Dimensions use PCA\npca = PCA(n_components=.95)\nprincipalComponents = pca.fit_transform(X_std) # Plot the explained variances\n","metadata":{"execution":{"iopub.status.busy":"2021-06-10T11:26:45.697156Z","iopub.execute_input":"2021-06-10T11:26:45.697689Z","iopub.status.idle":"2021-06-10T11:26:45.728068Z","shell.execute_reply.started":"2021-06-10T11:26:45.697655Z","shell.execute_reply":"2021-06-10T11:26:45.726769Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Plotting the Cumulative Summation of the Explained Variance\nplt.figure()\nplt.plot(np.cumsum(pca.explained_variance_ratio_), c='r')\nplt.xlabel('Number of Components')\nplt.ylabel('Variance (%)') #for each component\nplt.title('Pulsar Dataset Explained Variance')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-10T11:27:50.989041Z","iopub.execute_input":"2021-06-10T11:27:50.989462Z","iopub.status.idle":"2021-06-10T11:27:51.127709Z","shell.execute_reply.started":"2021-06-10T11:27:50.989428Z","shell.execute_reply":"2021-06-10T11:27:51.126893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pca = PCA(n_components=2)\nprincipalComponents = pca.fit_transform(X_std)\npca_df = pd.DataFrame(principalComponents)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T11:28:15.327843Z","iopub.execute_input":"2021-06-10T11:28:15.328207Z","iopub.status.idle":"2021-06-10T11:28:15.344108Z","shell.execute_reply.started":"2021-06-10T11:28:15.328163Z","shell.execute_reply":"2021-06-10T11:28:15.342608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Set cluster number use SSR elbow","metadata":{}},{"cell_type":"code","source":"# SSR elbow\nsum_of_squared_distances = []\n\nK = range(1,15)\nfor k in K:\n    km = KMeans(n_clusters=k)\n    km = km.fit(pca_df)\n    sum_of_squared_distances.append(km.inertia_)\n    \nax = sns.lineplot(x=K, y = sum_of_squared_distances)\nax.set(xlabel='K', ylabel='sum of squared distances', title='Elbow graph')\n","metadata":{"execution":{"iopub.status.busy":"2021-06-10T11:28:26.31452Z","iopub.execute_input":"2021-06-10T11:28:26.314886Z","iopub.status.idle":"2021-06-10T11:28:27.592187Z","shell.execute_reply.started":"2021-06-10T11:28:26.314853Z","shell.execute_reply":"2021-06-10T11:28:27.591232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#k-means\nk=3\n\nmodel = KMeans(n_clusters = k, algorithm='auto')\nmodel.fit(pca_df)\npredict = pd.DataFrame(model.predict(pca_df))\npredict.columns = ['predict']\nlabel = model.fit(pca_df).labels_\n\nn_clusters_ = len(set(label)) \nprint('Estimated number of clusters: %d' % n_clusters_)\n\n","metadata":{"execution":{"iopub.status.busy":"2021-06-10T11:33:11.8664Z","iopub.execute_input":"2021-06-10T11:33:11.866735Z","iopub.status.idle":"2021-06-10T11:33:11.974349Z","shell.execute_reply.started":"2021-06-10T11:33:11.866706Z","shell.execute_reply":"2021-06-10T11:33:11.973302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#silhouette_score\nlabel = pd.DataFrame(model.fit(pca_df).labels_)\nsilhouette_score(pca_df, model.fit(pca_df).labels_, metric='euclidean')\n","metadata":{"execution":{"iopub.status.busy":"2021-06-10T11:33:08.327408Z","iopub.execute_input":"2021-06-10T11:33:08.328137Z","iopub.status.idle":"2021-06-10T11:33:08.492383Z","shell.execute_reply.started":"2021-06-10T11:33:08.328088Z","shell.execute_reply":"2021-06-10T11:33:08.491242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#cluster result \ndf_scaled = pd.DataFrame(new_df)\ndf_scaled['kmeans'] = model.fit(pca_df).labels_\ndf_mean = (df_scaled.loc[df_scaled.kmeans!=-1, :].groupby('kmeans').mean())\nresults = pd.DataFrame(columns=['Variable', 'Var'])\nfor column in df_mean.columns[1:]:\n    results.loc[len(results), :] = [column, np.var(df_mean[column])]\n    selected_columns = list(results.sort_values('Var', ascending=False,).head(7).Variable.values) + ['kmeans']\n    tidy = df_scaled[selected_columns].melt(id_vars='kmeans')\n\nfor i in range(3):\n    sns.catplot(x='kmeans', y='value', hue='variable', data=tidy[tidy['kmeans']==i], height=5, aspect=.7, kind='bar')    \n","metadata":{"execution":{"iopub.status.busy":"2021-06-10T11:45:55.247697Z","iopub.execute_input":"2021-06-10T11:45:55.24813Z","iopub.status.idle":"2021-06-10T11:45:57.215732Z","shell.execute_reply.started":"2021-06-10T11:45:55.248094Z","shell.execute_reply":"2021-06-10T11:45:57.214307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#means shift clustering\nfrom sklearn.cluster import MeanShift\nms = MeanShift(bandwidth=2)\nms.fit(pca_df)\npredict = pd.DataFrame(ms.predict(pca_df))\n\npredict.columns = ['predict']\nlabel3 = ms.fit(pca_df).labels_\n\nn_clusters_ = len(set(label3)) \nprint('Estimated number of clusters: %d' % n_clusters_)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T11:35:10.97699Z","iopub.execute_input":"2021-06-10T11:35:10.977383Z","iopub.status.idle":"2021-06-10T11:35:14.399601Z","shell.execute_reply.started":"2021-06-10T11:35:10.977349Z","shell.execute_reply":"2021-06-10T11:35:14.398538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#silhouette_score\nlabel3 = pd.DataFrame(ms.fit(pca_df).labels_)\nsilhouette_score(pca_df, ms.fit(pca_df).labels_, metric='euclidean')","metadata":{"execution":{"iopub.status.busy":"2021-06-10T11:35:03.964528Z","iopub.execute_input":"2021-06-10T11:35:03.964933Z","iopub.status.idle":"2021-06-10T11:35:07.341422Z","shell.execute_reply.started":"2021-06-10T11:35:03.964899Z","shell.execute_reply":"2021-06-10T11:35:07.340108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#cluster result \ndf_scaled['ms'] = ms.fit(pca_df).labels_\ndf_mean = (df_scaled.loc[df_scaled.kmeans!=-1, :].groupby('ms').mean())\nresults = pd.DataFrame(columns=['Variable', 'Var'])\nfor column in df_mean.columns[1:]:\n    results.loc[len(results), :] = [column, np.var(df_mean[column])]\n    selected_columns = list(results.sort_values('Var', ascending=False,).head(7).Variable.values) + ['ms']\n    tidy = df_scaled[selected_columns].melt(id_vars='ms')\ndf_mean.columns[1:]\nfor i in range(4):\n    sns.catplot(x='ms', y='value', hue='variable', data=tidy[tidy['ms']==i], height=5, aspect=.7, kind='bar')    ","metadata":{"execution":{"iopub.status.busy":"2021-06-10T11:35:24.953614Z","iopub.execute_input":"2021-06-10T11:35:24.953971Z","iopub.status.idle":"2021-06-10T11:35:29.115745Z","shell.execute_reply.started":"2021-06-10T11:35:24.95394Z","shell.execute_reply":"2021-06-10T11:35:29.114491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# AgglomerativeClustering\nac = AgglomerativeClustering(n_clusters=3, affinity='euclidean', linkage='ward')\nac.fit_predict(pca_df)\nlabel2 = ac.fit(pca_df).labels_\n\nn_clusters_ = len(set(label2)) \nprint('Estimated number of clusters: %d' % n_clusters_)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T11:37:02.547808Z","iopub.execute_input":"2021-06-10T11:37:02.548286Z","iopub.status.idle":"2021-06-10T11:37:02.57947Z","shell.execute_reply.started":"2021-06-10T11:37:02.548243Z","shell.execute_reply":"2021-06-10T11:37:02.578384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#silhouette_score\nlabel2 = pd.DataFrame(ac.fit(pca_df).labels_)\nsilhouette_score(pca_df, ac.fit(pca_df).labels_, metric='euclidean')","metadata":{"execution":{"iopub.status.busy":"2021-06-10T11:37:08.372262Z","iopub.execute_input":"2021-06-10T11:37:08.372842Z","iopub.status.idle":"2021-06-10T11:37:08.424732Z","shell.execute_reply.started":"2021-06-10T11:37:08.37279Z","shell.execute_reply":"2021-06-10T11:37:08.42324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#clustering result\ndf_scaled['ac'] = ac.fit(pca_df).labels_\ndf_mean = (df_scaled.loc[df_scaled.kmeans!=-1, :].groupby('ac').mean())\nresults = pd.DataFrame(columns=['Variable', 'Var'])\nfor column in df_mean.columns[1:]:\n    results.loc[len(results), :] = [column, np.var(df_mean[column])]\n    selected_columns = list(results.sort_values('Var', ascending=False,).head(7).Variable.values) + ['ac']\n    tidy = df_scaled[selected_columns].melt(id_vars='ac')\n\nfor i in range(3):\n    sns.catplot(x='ac', y='value', hue='variable', data=tidy[tidy['ac']==i], height=5, aspect=.7, kind='bar')    \n","metadata":{"execution":{"iopub.status.busy":"2021-06-10T11:37:10.789149Z","iopub.execute_input":"2021-06-10T11:37:10.789551Z","iopub.status.idle":"2021-06-10T11:37:12.606598Z","shell.execute_reply.started":"2021-06-10T11:37:10.789519Z","shell.execute_reply":"2021-06-10T11:37:12.605536Z"},"trusted":true},"execution_count":null,"outputs":[]}]}