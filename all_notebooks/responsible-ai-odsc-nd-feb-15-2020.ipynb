{"cells":[{"metadata":{"_cell_guid":"6876755d-3552-491d-b416-a012fbe221e4","_uuid":"0b46910d15d98513ea81fb24e0f8ab28134c5f5f"},"cell_type":"markdown","source":"##  Hello all. Welcome to XAI day by Open Data science conference, New Delhi.\n\n**Give us an upvote if you find it useful.**\n\nWarm regards,\nTeam ODSC New Delhi"},{"metadata":{},"cell_type":"markdown","source":"#This Notebook contains a collection of methods to weave explainability into AI systems.\nIt can come handy while studying models,debugging ML pipelines and for exploratory purposes. \nA system is only as good as its creator. Today, we acknowledge the limitations of our craft and attempt to understand it better."},{"metadata":{"_cell_guid":"87c7b371-53b1-4d4d-bfbd-373d2b84b33a","_uuid":"da5343fb3f6b3942909c94bf8e2add04fd3ff1a3","trusted":true},"cell_type":"code","source":"#Importing required packages.\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\n%matplotlib inline\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import accuracy_score, classification_report\nimport matplotlib\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Dataset"},{"metadata":{},"cell_type":"markdown","source":"Source: https://www.kaggle.com/uciml/red-wine-quality-cortez-et-al-2009\n\nThis dataset is also available from the UCI machine learning repository, https://archive.ics.uci.edu/ml/datasets/wine+quality , I just shared it to kaggle for convenience. (If I am mistaken and the public license type disallowed me from doing so, I will take this down if requested.)\n\nContent\nFor more information, read [Cortez et al., 2009].\nInput variables (based on physicochemical tests):\n1 - fixed acidity\n2 - volatile acidity\n3 - citric acid\n4 - residual sugar\n5 - chlorides\n6 - free sulfur dioxide\n7 - total sulfur dioxide\n8 - density\n9 - pH\n10 - sulphates\n11 - alcohol\nOutput variable (based on sensory data):\n12 - quality (score between 0 and 10)"},{"metadata":{},"cell_type":"markdown","source":"Columns description\n* fixed acidity:most acids involved with wine or fixed or nonvolatile (do not evaporate readily)\n* \n* volatile acidity:the amount of acetic acid in wine, which at too high of levels can lead to an unpleasant, vinegar taste\n* \n* citric acid:found in small quantities, citric acid can add 'freshness' and flavor to wines\n* \n* residual sugar:the amount of sugar remaining after fermentation stops, it's rare to find wines with less than 1 gram/liter and wines with greater than 45 grams/liter are considered sweet\n* \n* chlorides:the amount of salt in the wine\n* \n* free sulfur dioxide:the free form of SO2 exists in equilibrium between molecular SO2 (as a dissolved gas) and bisulfite ion; it prevents microbial growth and the oxidation of wine\n* \n* total sulfur dioxide:amount of free and bound forms of S02; in low concentrations, SO2 is mostly undetectable in wine, but at free SO2 concentrations over 50 ppm, SO2 becomes evident in the nose and taste of wine\n* \n* density:the density of water is close to that of water depending on the percent alcohol and sugar content\n* \n* pH:describes how acidic or basic a wine is on a scale from 0 (very acidic) to 14 (very basic); most wines are between 3-4 on the pH scale\n* \n* sulphates:a wine additive which can contribute to sulfur dioxide gas (S02) levels, wich acts as an antimicrobial and antioxidant\n* \n* alcohol:the percent alcohol content of the wine\n* \n* quality:output variable (based on sensory data, score between 0 and 10)"},{"metadata":{"_cell_guid":"800f9ff4-79bf-4785-9569-23e1d9b9b03b","_uuid":"07c8409e4eccd80507d9846e8cc70ea42e58cbe6","trusted":true},"cell_type":"code","source":"#Loading dataset\nwine = pd.read_csv('../input/winequality-red.csv')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"019cbcee-6cb2-478c-922b-ccebe4962769","_uuid":"45ff42953e9082cd55612a4774408cc97a05fb11","trusted":true},"cell_type":"code","source":"#Let's check how the data is distributed\nwine.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"704f4830-5804-436d-9b78-6ca00f5ae510","_uuid":"af141503385967f92d409c5e111e2724b4c9636f","trusted":true},"cell_type":"code","source":"#Information about the data columns\nwine.info()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"12b9e3c1-006d-4b1d-b01d-02be5a594bbb","_uuid":"e9ad3ce0e67ea7a5178222164d784a974846bc54"},"cell_type":"markdown","source":"#### **Before moving to sophisticated methods,Let's start with the basics.Graphical representations are very powerful for explaining otherwise complex things.Humans have evolved to process visual stimuli faster than numbers,so lets look at the distribution of some of our variables**"},{"metadata":{},"cell_type":"markdown","source":"# Visual EDA"},{"metadata":{"_cell_guid":"f6a9d2eb-e03c-4c8b-8d7f-c0cb735d9ce9","_uuid":"255e8fd04c90eae23164df043ebf16550ecff8fe","trusted":true},"cell_type":"code","source":"#Here we see that fixed acidity does not give any specification to classify the quality.\nfig = plt.figure(figsize = (10,6))\nsns.barplot(x = 'quality', y = 'fixed acidity', data = wine)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"7d7355dc-9c09-4e5e-8989-87f903197898","_uuid":"158a576b3fb80cc0978c322663bc5060d8977be0","trusted":true},"cell_type":"code","source":"#Here we see that its quite a downing trend in the volatile acidity as we go higher the quality \nfig = plt.figure(figsize = (10,6))\nsns.barplot(x = 'quality', y = 'volatile acidity', data = wine)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"08f073d3-787b-4956-899b-6bd2bdf3cf91","_uuid":"9600cd337636a7860a908fa36ad8084e230829ca","trusted":true},"cell_type":"code","source":"#Composition of citric acid go higher as we go higher in the quality of the wine\nfig = plt.figure(figsize = (10,6))\nsns.barplot(x = 'quality', y = 'citric acid', data = wine)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"7dcb76bc-3436-46bd-9d7f-78ccd436517e","_uuid":"6b017770cef61eefb34b4607218c77f933350389","trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize = (10,6))\nsns.barplot(x = 'quality', y = 'residual sugar', data = wine)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"76463630-67d9-4b07-8076-fba8b49a9921","_uuid":"31fe5d393946e406cbe62d3f4c7951e0b493c454","trusted":true},"cell_type":"code","source":"#Composition of chloride also go down as we go higher in the quality of the wine\nfig = plt.figure(figsize = (10,6))\nsns.barplot(x = 'quality', y = 'chlorides', data = wine)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"6abcf7a2-cc7a-4673-a905-6ee0c2cc9e15","_uuid":"6ba9a0dafe8e1042da4dbcaa93706dd46cf3a85c","trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize = (10,6))\nsns.barplot(x = 'quality', y = 'free sulfur dioxide', data = wine)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"9acea621-2b7b-44b7-a0fa-b984dd8c0e93","_uuid":"d99c6e4033da73cdc9d9977ac045372ff9af1c53","trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize = (10,6))\nsns.barplot(x = 'quality', y = 'total sulfur dioxide', data = wine)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"bf0b1aa4-8a6c-4f95-80c3-7554b38a9c96","_uuid":"ae9a7496f5c238ec9bb95729b17960c6e48efe35","trusted":true},"cell_type":"code","source":"#Sulphates level goes higher with the quality of wine\nfig = plt.figure(figsize = (10,6))\nsns.barplot(x = 'quality', y = 'sulphates', data = wine)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"04f51b1d-6315-44d3-986c-ed82fc996ad3","_uuid":"81efa0b7799cd731aff98cffeaa28c361a7375a5","trusted":true},"cell_type":"code","source":"#Alcohol level also goes higher as te quality of wine increases\nfig = plt.figure(figsize = (10,6))\nsns.barplot(x = 'quality', y = 'alcohol', data = wine)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"9838ca3d-4b89-4503-9d7e-247cc3a9730b","_uuid":"974be3136a2f13bf26a88b26cbddbf73f5cafda9"},"cell_type":"markdown","source":"## Simple Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"wine.quality.describe()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"33e1c5c0-a65a-4918-8d94-db8e4c521d50","_uuid":"3f082340d1b157391052f0fa20a44aa0105ce987","trusted":true},"cell_type":"code","source":"#Making binary classificaion for the response variable.\n#Dividing wine as good and bad by giving the limit for the quality\nbins = (2,6,8)\ngroup_names = ['bad', 'good']\nwine['quality'] = pd.cut(wine['quality'], bins = bins, labels = group_names)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"21a15507-cc97-4c40-835b-51fd79f7cdd9","_uuid":"87127b3c5e9493a9364b3c63401ddb11a51373a1","trusted":true},"cell_type":"code","source":"#Now lets assign a labels to our quality variable\nlabel_quality = LabelEncoder() ","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f4c3a886-4b1f-4533-a660-b7ffb66376c2","_uuid":"a592e2b8ece55e8d5928241eb5b9188226a725bf","trusted":true},"cell_type":"code","source":"#Bad becomes 0 and good becomes 1 \nwine['quality'] = label_quality.fit_transform(wine['quality'])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"faa36f65-7e80-44d7-a4f8-10e482cec684","_uuid":"65a97517331f835cade698b594a7376ea7778eeb","trusted":true},"cell_type":"code","source":"wine['quality'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"2fe7339e-837e-48c0-b988-a36c89632844","_uuid":"d2a4c7e14dc6b8faa31efd1fcbe4f61b80a7d3e1","trusted":true},"cell_type":"code","source":"sns.countplot(wine['quality'])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ba89712a-ad66-4a92-9214-4e35c3802d59","_uuid":"b893deb26bb309c914de12aadf9ad8437deb8358","trusted":true},"cell_type":"code","source":"#Now seperate the dataset as response variable and feature variabes\nX = wine.drop('quality', axis = 1)\ny = wine.quality","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"d7703d9a-3397-4c42-9b38-6f341cac1cde","_uuid":"bdfc0f6ba146f947f265fd493e8327366fdb5c9a","trusted":true},"cell_type":"code","source":"#Train and Test splitting of data \nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"cef31289-9cec-4ab1-afea-2ff34c36fec1","_uuid":"ca68209bcd596b3cfcaf42837e4a6e49b6c12974","trusted":true},"cell_type":"code","source":"#Applying Standard scaling to get optimized result\nsc = StandardScaler()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"705b8e8a-a6d3-4e93-8b53-7c677b5b88c4","_uuid":"7878194b0c4935904c68a88a91e18f9bbbabbda7","trusted":true},"cell_type":"code","source":"# X_train = sc.fit_transform(X_train)\n# X_test = sc.fit_transform(X_test)\n#commented on purpose to demonstrate something","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"type(X_train)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"0a12868b-8780-43b9-ae70-4c849407aac6","_uuid":"093572f70cda59063611c1c888974e6a863ec7ef"},"cell_type":"markdown","source":"    ### Simple Logistic Regression"},{"metadata":{"_cell_guid":"f7d2ca14-40fa-4b7c-b902-d47f09214c75","_uuid":"4c2c2a071bde3b44adf031a6807f51b47f8b5eaa","trusted":true},"cell_type":"code","source":"logreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\npred_logreg = logreg.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"1cc15e56-0589-4fa3-a4f1-6f5e7b5547b2","_uuid":"61e8a79a7db4fef289b8d4e3b3dd8e1c01756697","trusted":true},"cell_type":"code","source":"#Testing time\nprint(classification_report(y_test,pred_logreg))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"6fc282fd-9db3-4c69-bf91-f9daaaec615b","_uuid":"eaa7b2cbb80adbb06133b9cd89a8ebae014dc3e8"},"cell_type":"markdown","source":"A decent accuracy!"},{"metadata":{"_cell_guid":"84334dbd-d36c-4650-87ab-d3e8f73be63c","_uuid":"0fd8c7cd7741be25fd0d5f56d5711e8aaf44ac0f","trusted":true},"cell_type":"code","source":"#Confusion matrix for the random forest classification\nprint(confusion_matrix(y_test, pred_logreg))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Explain to me like I'm 5"},{"metadata":{"_cell_guid":"9880c001-331b-43d2-9a35-7c653909eaf0","_uuid":"6b8d3662f89c436749ee26af98b0f79636c569c2","trusted":true},"cell_type":"code","source":"import eli5\neli5.show_weights(logreg)\n#uses permutation importance to compute feature weights","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"That gives us the weights associated to each feature, that can be seen as the contribution of each feature into predicting that the class will be y=1 (the client will subscribe after the campaign).\n\nThe names for each features aren't really helping though, we can pass a list of column names to eli5 but we'll need to do a little gymnastics first to extract names from our preprocessor in the pipeline (since we've generated new features on the fly with the one hot encoder)"},{"metadata":{"trusted":true},"cell_type":"code","source":"feat_names=wine.columns[:-1].tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feat_names","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"eli5.show_weights(logreg, feature_names=feat_names)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So this is for the whole model.Let's nitpick a particular observation"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\ni = np.random.randint(1,100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"i","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test.iloc[i]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test.iloc[i]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"eli5.show_prediction(logreg, \n                     X_test.iloc[i],\n                     feature_names=feat_names, show_feature_values=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"ELI5 understands text processing utilities from scikit-learn and can highlight text data accordingly. It also allows to debug scikit-learn pipelines which contain HashingVectorizer, by undoing hashing.\n\nXGBoost - show feature importances and explain predictions of XGBClassifier, XGBRegressor and xgboost.Booster.\n\nLightGBM - show feature importances and explain predictions of LGBMClassifier and LGBMRegressor.\n\nCatBoost - show feature importances of CatBoostClassifier and CatBoostRegressor.\n\nlightning - explain weights and predictions of lightning classifiers and regressors.\n\nsklearn-crfsuite. ELI5 allows to check weights of sklearn_crfsuite.CRF models.\n\nKeras - explain predictions of image classifiers via Grad-CAM visualizations.\n\nSource: https://eli5.readthedocs.io/en/latest/overview.html\n    "},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## LIME :Local Interpretable Model-Agnostic Explanations\nLime works on the principle of local fidelity ie that a point behaves in the same manner as that of its immediate neighbors. Fort his purpose,Lime is lighter than ELI5 and often faster."},{"metadata":{"trusted":true},"cell_type":"code","source":"rfc=RandomForestClassifier(n_estimators=200)\nrfm = rfc.fit(X_train,y_train)\npred_rfc= rfc.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Testing time\nprint(classification_report(y_test,pred_rfc))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(confusion_matrix(y_test, pred_rfc))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#looking at eli5 first\neli5.show_weights(rfm, \n                  feature_names=feat_names)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from lime.lime_tabular import LimeTabularExplainer\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The parameters passed to the explainer are:\n\nTraining set sans one hot encoding\nmode: the explainer can be used for classification or regression\nfeature_names: list of labels for our features\ncategorical_features: list of indexes of categorical features\ncategorical_names: dict mapping each index of categorical feature to a list of corresponding labels\ndicretize_continuous: will discretize numerical values into buckets that can be used for explanation. For instance it can tell us that the decision was made because distance is in bucket [5km, 10km] instead of telling us distance is an importante feature."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"explainer = LimeTabularExplainer(X_train.values,\n                                 mode=\"classification\",\n                                 feature_names=X_train.columns.tolist(),\n                                 categorical_names=None,\n                                 categorical_features=None,\n                                 discretize_continuous=True,\n                                 random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prob=lambda x:rfm.predict_proba(X_test[[i]]).astype(float)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The explainer is all set up to explain observations!\nLet's use the old i"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test.iloc[i]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#prediction function\npred_fn = lambda x: rfm.predict_proba(x).astype(float)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Hi I'm LIME,I'm the infamous gossip monger. If I dont know you,I'll take your neighbour's word for it!! \n[Kriti is awesome though! Be like KD] <3"},{"metadata":{"trusted":true},"cell_type":"code","source":"explanation = explainer.explain_instance(X_test.iloc[i], pred_fn)\nexplanation.show_in_notebook(show_table=True, show_all=False,)\nprint(explanation.score)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is simple.sign=direction of relationship and coefficients= weights"},{"metadata":{},"cell_type":"markdown","source":"# Moving on,let's talk about the concept of fairness.How do you define fair?"},{"metadata":{},"cell_type":"markdown","source":"![](http://www.publichealthnotes.com/wp-content/uploads/2017/05/Equality-Vs-Equity..final-edit-1.jpg)"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install https://github.com/adebayoj/fairml/archive/master.zip\n# Installing another package called fairML","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> The basic idea behind FairML is to measure a model’s dependence on its inputs by changing them. If a small change to an input feature dramatically changes the output, the model is sensitive to the feature.\n> Think sensitivty analysis that we studied in economics/Calculus in schools.\nRemember orthogonal vectors?\nSource: https://blog.fastforwardlabs.com/2017/03/09/fairml-auditing-black-box-predictive-models.html"},{"metadata":{"trusted":true},"cell_type":"code","source":"from fairml import audit_model\nimportances, _ = audit_model(rfm.predict, X_test)\nprint(importances)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#inbuilt methods to visualize it\ntotal, _ = audit_model(logreg.predict, X_test)\n# print feature importance\nprint(total)\n\n# generate feature dependence plot\nfrom fairml import plot_dependencies\nfig = plot_dependencies(\n    total.median(),\n    reverse_values=False,\n    title=\"FairML feature dependence\"\n)\nplt.savefig(\"fairml_ldp.eps\", transparent=False, bbox_inches='tight')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"**References:**\n* Bonus resource: a Textbook on Fairness in ML: https://fairmlbook.org/pdf/fairmlbook.pdf\n* Image source: https://www.publichealthnotes.com/equity-vs-equality/\n* https://blog.fastforwardlabs.com/2017/03/09/fairml-auditing-black-box-predictive-models.html\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}