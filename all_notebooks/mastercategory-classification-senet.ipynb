{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Dataset Loading"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import os\nimport torch\nimport torchvision\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nfrom torch import optim\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import DataLoader, Dataset\nfrom sklearn.model_selection import train_test_split\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DATASET_PATH = \"../input/myntradataset/\"\ndf = pd.read_csv(os.path.join(DATASET_PATH, \"styles.csv\"), error_bad_lines=False) # neglect lines with error","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Clean unexisting files\nproblems = []\nfor idx, line in df.iterrows():\n    if not os.path.exists(os.path.join(DATASET_PATH, 'images', str(line.id)+'.jpg')):\n        print(idx)\n        problems.append(idx)\ndf.drop(df.index[problems], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split train and test dataset\ntrain_df, test_df = train_test_split(df, test_size=0.2, random_state=2028)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class FPIDataset(Dataset):\n    \"\"\" Fashion Product Image Dataset\n    \"\"\"\n    cat_list = df['masterCategory'].unique()\n    cat2num = {cat:i for i, cat in enumerate(cat_list)}\n    num2cat = {i:cat for i, cat in enumerate(cat_list)}\n    def __init__(self, root, dataframe, transform=None):\n        super(FPIDataset, self).__init__()\n        self.dataframe = dataframe\n        self.root = root\n        if transform is None:\n            transform = torchvision.transforms.Compose([\n                torchvision.transforms.Resize((224, 224)),\n                torchvision.transforms.ToTensor()\n            ])\n        self.transform = transform\n        \n    def __getitem__(self, idx):\n        line = self.dataframe.iloc[idx]\n        cat = line.masterCategory\n        cat_id = self.cat2num[cat]\n        img_path = os.path.join(self.root, str(line.id)+'.jpg')\n        img = Image.open(img_path).convert('RGB')\n        img_tensor = self.transform(img)\n        return img_tensor, cat_id\n            \n    def __len__(self):\n        return len(self.dataframe)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Construct dataset and dataloader\ntrain_ds = FPIDataset(os.path.join(DATASET_PATH, 'images'), train_df)\ntest_ds = FPIDataset(os.path.join(DATASET_PATH, 'images'), test_df)\ntrain_loader = DataLoader(train_ds, batch_size=16, shuffle=True)\ntest_loader = DataLoader(test_ds, batch_size=16, shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Show cases\nimg_tensor, label = train_ds[0]\nplt.imshow(img_tensor.numpy().transpose(1,2,0))\nplt.title(FPIDataset.num2cat[label])\nplt.axis('off')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model\n\nFor compaison, train a resnet18 firstly."},{"metadata":{"trusted":true},"cell_type":"code","source":"from torchvision.models import ResNet, resnet18\nimport torch.nn as nn","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class FPIModel(nn.Module):\n    \"\"\" Fashion Product Image Model\n    \"\"\"\n    def __init__(self, num_classes):\n        super(FPIModel, self).__init__()\n        backbone = resnet18(pretrained=True)\n        backbone.fc = nn.Linear(backbone.fc.in_features, num_classes)\n        self.backbone = backbone\n    def forward(self, inp):\n        return self.backbone(inp)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = FPIModel(len(df.masterCategory.unique()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train and Valiadation"},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_val_loop(model, save_path):\n    epochs = 1\n    log_step = 20\n    optimizer = optim.RMSprop(model.parameters(), lr=0.01)\n    criterion = nn.CrossEntropyLoss()\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n    model.to(device)\n    loss_hist = []\n    for epoch in range(1, epochs+1):\n        # train phase\n        model.train()\n        for bth_num, batch in enumerate(train_loader, 1):\n            images, labels = batch\n            images, labels = images.to(device), labels.to(device)\n            logits = model(images)\n            loss = criterion(logits, labels)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            if bth_num % log_step == 0:\n                print(\"\\r Epoch: {}, # {} => Loss: {:.4f}\".format(epoch, bth_num, loss.item()), flush=True, end='')\n                loss_hist.append(loss.item())\n        print()\n        # Valid phase\n        model.eval()\n        total_preds = []\n        total_labels = []\n        for bth_num, batch in enumerate(test_loader, 1):\n            images, labels = batch\n            images, labels = images.to(device), labels.to(device)\n            logits = model(images)\n            preds = logits.argmax(dim=-1)\n            total_labels.append(labels.cpu().data.numpy())\n            total_preds.append(preds.cpu().data.numpy())\n        total_preds = np.concatenate(total_preds)\n        total_labels = np.concatenate(total_labels)\n        acc = sum(total_preds == total_labels) / len(total_labels)\n        print(\"Accuracy: {:.2%}\".format(acc))\n    \n    # Save model weights\n    torch.save(model.state_dict(), save_path)\n    \n    return loss_hist","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loss_hist = train_val_loop(model, 'resnet18.pth')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# SENet\n\nTODO: description of SENet\n\nReuse the resnet18 to construct se_resnet18"},{"metadata":{"trusted":true},"cell_type":"code","source":"def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n    \"\"\"3x3 convolution with padding\"\"\"\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=dilation, groups=groups, bias=False, dilation=dilation)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class SELayer(nn.Module):\n    def __init__(self, channel, reduction=16):\n        super(SELayer, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Sequential(\n            nn.Linear(channel, channel // reduction, bias=False),\n            nn.ReLU(inplace=True),\n            nn.Linear(channel // reduction, channel, bias=False),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        b, c, _, _ = x.size()\n        y = self.avg_pool(x).view(b, c)\n        y = self.fc(y).view(b, c, 1, 1)\n        return x * y.expand_as(x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class SEBasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n                 base_width=64, dilation=1, norm_layer=None, reduction=16):\n        super(SEBasicBlock, self).__init__()\n        if norm_layer is None:\n            norm_layer = nn.BatchNorm2d\n        if groups != 1 or base_width != 64:\n            raise ValueError('BasicBlock only supports groups=1 and base_width=64')\n        if dilation > 1:\n            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = norm_layer(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes, 1)\n        self.bn2 = norm_layer(planes)\n        self.se = SELayer(planes, reduction)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.se(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def se_resnet18(num_classes=1_000):\n    \"\"\"Constructs a ResNet-18 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    \"\"\"\n    model = ResNet(SEBasicBlock, [2, 2, 2, 2], num_classes=num_classes)\n    model.avgpool = nn.AdaptiveAvgPool2d(1)\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"se_model = se_resnet18(num_classes=len(df.masterCategory.unique()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"se_loss_hist = train_val_loop(se_model, 'se_resnet18.pth')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot training process\nplt.plot(loss_hist, label='resnet18')\nplt.plot(se_loss_hist, label='se_resnet18')\nplt.title('Train Loss History')\nplt.xlabel('step')\nplt.ylabel('loss')\nplt.legend()\nplt.savefig('loss_hist.png')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# TODO\n\n* Confusion matrix\n* Case presentation"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}