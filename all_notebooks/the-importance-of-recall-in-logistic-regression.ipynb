{"cells":[{"metadata":{"id":"BuIEwMRVAEMZ","colab_type":"text","_uuid":"3ddb54bbb9695a3696bd18967135cca53e1fe5cb"},"cell_type":"markdown","source":"  # **INTRODUCTION - PREDICTING CHURN RATE**\n\n---\n\n\n\n\n\n\nHello and welcome to this online notebook. You may think of it as an interactive article. \n\nWhat we have here is an exercise to try and understand how we can use Machine Learning to predict whether or not a customer is about to leave your company (otherwise known as **Churn**). This tutorial is aimed at non-technical people and is meant to be straightforward to follow. I hope to show that these predictive tools are accessible to everyone and anyone who owns a laptop. No prior knowledge of Machine Learning is required. Therefore, these tools can be just as useful to big corporates as to budding entrepreneurs. Everyone should be using these tools to optimize their business development decisions, simple as.\n\nFor the purpose of this exercise, we are  going to use a dataset from the telecom giant **Telco** to predict if one of their customers is about to cruelly, cold-heartedly leave them for a competitor.The dataset is comprised of more than 7000 different customer profiles. For each customer, we have a set of different type of information ( in this case **21 columns**) and we will try to use these different sets of information as indicators of whether customer left the company or not.\n\nImagine this to be historical data taken from the customer database from last quarter or last year's data as typically corporations would  have a dataset like this comprised of a few million these customer profiles (so today's dataset is tiny in comparison but it will serve nicely as a quick illustration).\n\nOnce the learning model has figured out the key predictors , we can then use this model on our current customers to find out which one of them is likely to leave us soon. Learning from the past to predict the future. That's all we are doing here and that's all we will ever do in Machine Learning.\n\nSo first of all, why would we want to predict customer churn? There are lots of obvious reasons: for example, if you know that a customer is likely to churn but hasn't done so yet then you might want to offer them a promotion or give them a temporary discount to stop them from doing so. Or if they are calling customer service and you see from their data that they are likely to churn, then you might want to give them special customer service attention or offer them some extra benefits.\n\nSo I believe that the use cases here are clearly numerous and I think this is a really good place to start if you were a company and you wanted to play around with some Machine Learning. Predicting the churn rate is quite easy to do ( as I hope to show here) , nonetheless, it will bring instant value to your company.\n\n---\nThese are the three parts of our exercise :\n* **Part 1- Data Cleaning** Formatting the data so that our predictive algorithm can understand it.\n* **Part 2- Data Learning** We give the data to our algorithm for it to learn from it, find the hidden patterns and make accurate predictions.\n* **Part 3- Data Predicting** We use our now trained model to make predictions on our current customers and take the corresponding action.\n\n\n"},{"metadata":{"id":"ueFcRLLl0DLm","colab_type":"text","_uuid":"6cf416ae5ecb32043857240d93054be6e27b0de3"},"cell_type":"markdown","source":"### **Importing Libraries-Step 0**\nFirst of all, as always we need to pre-load the libraries to make our lives a hundred times easier. This is always the first thing for us to do. Consider it as our **Step 0**.\n\nWithout these libraries, we wouldn't be able to use any of the commands from the code. These libraries use opensource code created by other people to make our lives a hundred times easier, allowing  us to execute complex operations with commands that are only a few words long. I always import  all common libraries whether I end up using them or not just so that I don't need to worry about any of this later on. \n\n*To execute the code, you simply need to press the *\"Play\"* button at the top left-hand side of the code.*"},{"metadata":{"colab_type":"code","id":"vjI5pmYg4pDB","colab":{},"trusted":true,"_uuid":"bd11afdbcdde9ca5f30d6732e64e1a51036eb2aa"},"cell_type":"code","source":"import pandas as pd #Pandas is the most popular library for manipulating data. Think of it as an Excel but a million times faster and more practical.\nimport numpy as np # This library allows to easily carry out simple and complex mathematical operations.\nimport matplotlib.pyplot as plt #Allows us to plot data, create graphs and visualize data. Perfect for your Powerpoint slides ;)\nimport sklearn #The one and only. This amazing library holds all the secrets. Containing powerful algorithms packed in a single line of code, this is where the magic will happen.\nimport sklearn.model_selection # more of sklearn. It is a big library, but trust me it is worth it.\nimport sklearn.preprocessing \nfrom sklearn.preprocessing import LabelEncoder,OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score, explained_variance_score,mean_absolute_error,mean_squared_error,precision_score,recall_score, accuracy_score,f1_score\nfrom sklearn.utils import shuffle\n\n\nimport random # Allows us to call random numbers, occasionally very useful.\n#from google.colab import files #Allows to upload and download files directly from the browser.\nimport pprint#Allows us to neatly display text\nfrom collections import OrderedDict\n\n\n\n\n#Classifiers Algorithms\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neural_network import MLPClassifier\nfrom xgboost import XGBClassifier\n\n\n#Regression Algorithms\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.linear_model import MultiTaskElasticNet\nfrom sklearn.linear_model import Lars\nfrom sklearn.linear_model import LassoLars\nfrom sklearn.linear_model import OrthogonalMatchingPursuit\nfrom sklearn.linear_model import orthogonal_mp\nfrom sklearn.linear_model import BayesianRidge \nfrom sklearn.linear_model import ARDRegression\nfrom sklearn.linear_model import RANSACRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.tree import DecisionTreeRegressor\n","execution_count":null,"outputs":[]},{"metadata":{"id":"J4AC8oB-7CRG","colab_type":"text","_uuid":"5d39e17a530d6e75cfd167da16c8c1160c888b42"},"cell_type":"markdown","source":"###Step 1- Upload our data so that we can open it and  manipulate it.\n\nYou can have a look at and even downloadthe spreadsheet we are using at <a href=\"https://docs.google.com/spreadsheets/d/1SMa2YbPsu2ABNeSK1GgapIzOvk21rnqmuL2lBRzydqc/edit?usp=sharing\">the following link</a>.\n \n Meanwhile the next command retrieves that spreadsheet and uploads it to this window.\n"},{"metadata":{"id":"helk8howyb1p","colab_type":"code","colab":{},"trusted":true,"_uuid":"49d0616c8011d3f6943a0aeee21ec44395813a5d"},"cell_type":"code","source":"# We previously uploaded the data to this url and here we simply retrieving it.\nimport os\nprint(os.listdir(\"../input\"))\n","execution_count":null,"outputs":[]},{"metadata":{"id":"_GysIMek9JjL","colab_type":"text","_uuid":"297af92c3dc6e61cf77ce7127bb33b4eecdfc863"},"cell_type":"markdown","source":"####Once our data is uploaded, we can simply open it using the Pandas library to open and view  to see what we are dealing with here:\n\nWhat is Pandas? Think of it as Excel, only 100 times faster and for practical. We use it to manipulate huge  spreadsheets of data in just a few seconds.\n"},{"metadata":{"id":"XJ2revpM9gL6","colab_type":"code","outputId":"0a5da38e-7bd5-408f-9794-3d729f1afbeb","colab":{"base_uri":"https://localhost:8080/","height":119},"trusted":true,"_uuid":"01d86e173524d72fd63bfe1b49a5c2a2abe78574"},"cell_type":"code","source":"# Here we simly use the 'pd.read_excel' command to open the excel file by simply calling the file's url where we uploaded it to.\ndata=pd.read_csv('../input/Telco_Customer_Churn.csv')\n\n# This allows us to look at the name of each column. Notice that now all our data is stored as the word 'data' and we can operate on it by using saying'data.(command)'\ncolumnnames=data.columns \n\nprint(columnnames) #Displays the names of our columns so that can start to get a feel for our datasheet","execution_count":null,"outputs":[]},{"metadata":{"id":"N0GYEsk5-rlz","colab_type":"text","_uuid":"50c7350919befb96782ef66cad974c4790e20880"},"cell_type":"markdown","source":"#### We see that our first column is customer ID, this will do very nicely as an index and will be a better than identifying our customers than using the  default row numbers \"0,1,2...\"italicized text :"},{"metadata":{"id":"yGDaXyuC-mzN","colab_type":"code","colab":{},"trusted":true,"_uuid":"c4595075c64b250b49169fda0f6f235a2c0936e8"},"cell_type":"code","source":"data.set_index('CustomerID', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"id":"dRhJn-6YAh2n","colab_type":"text","_uuid":"044d7a4e1a07d5913db7f4ae1595a1dbf2b92489"},"cell_type":"markdown","source":"####We print out only the first 10 rows to explore our data using the ***''.head'*** command  because we really don't want to print out all 7000 rows do we now:\n\n---\n\n\n"},{"metadata":{"id":"cEHVGcBEAGGg","colab_type":"code","outputId":"eb29b4ee-6bde-4278-b24e-bb5b9ce893a1","colab":{"base_uri":"https://localhost:8080/","height":580},"trusted":true,"_uuid":"09631cf8ea986b68f6bc780aae92b59e83a6c341"},"cell_type":"code","source":"data.head(10)","execution_count":null,"outputs":[]},{"metadata":{"id":"HiyX9Gg5Axoa","colab_type":"text","_uuid":"404477d2924aabca3b521fc53c14aa4d48fdc2c9"},"cell_type":"markdown","source":"# Part I - DATA CLEANING\n\n---\nNo matter how glamorous Data Science and Machine Learning may look in the media, this doesn't change the fact that 80% of the work consists in cleaning up (usually someone else's) mess in our dataset. \n\nThe main thing to understand is that computers only understand numbers. Letters of the Roman Alphabet cannot yet be computed, so every value in every one of our cell needs to be a number, no text allowed. To make things even better, all numbers should be in a common scale in order to make life extra easy for our Machine Learning Algorithm. Hence all our data needs to be represented between a range of **[-1.5 , 1.5]** .  So, let's get mopping."},{"metadata":{"id":"akovQbGPDVp7","colab_type":"text","_uuid":"9abc1140edfb67bdb1b9f5e6e461ef20adf0e4b2"},"cell_type":"markdown","source":"###Step 1- Mean Normalization \n\nThis first step aims to do exactly what we just mentioned. Let's get all our  numbers on the same scale so that they all lie between -1.5 and 1.5. This allows the algorithm to learn faster and better. Think of it as a best practice.\nWe want to avoid  our data being in different scales because right now some of our number columns are between **0-20** and other columns have values ranging from **100-2,000**. Not ideal.\n\nTo do so, let's pick out all the columns that already have numerical data and perform **Mean Normalization** on them.  \n\n**Mean Normalization**  is an extremely simple concept and it allows us to reduce any numerical value to scale between -1.5 and 1.5 (roughly ). Without wanting to go into the details, here how the fromula would look if you wanted to do this in Excel \n\n**=(A1-AVERAGE(A:A)/(STDEV(A:A)))\"**\n\nHowever if you tried to run this in Excel it would take between a few minutes and a few hours depending on the size of your dataset. Here with the **Pandas** library it will take less than a second to execute. If you don't believe me, let me show you :\n\n(In case you want learn more about **Mean Normalization**, you can check out  <a href=\"https://youtu.be/e1nTgoDI_m8?t=340\">this great video</a> by Andrew NG.)"},{"metadata":{"id":"X0-nUNGrF0nc","colab_type":"code","outputId":"c25cca16-4cc1-4552-ef3e-34fbb1005d64","colab":{"base_uri":"https://localhost:8080/","height":580},"trusted":true,"_uuid":"8c5b631dbdc044b44b140cf1ebb7a28f4e0b99a7"},"cell_type":"code","source":"#the column 'Tenure' contains the number of months that the customer has been with the company. As we have said, we need to normalize it using this simple function. We are telling our code that this column ('Tenure') will now be equal this mean normalization operation.\n\ndata['Tenure']=(data['Tenure']-data['Tenure'].mean())/data['Tenure'].std() \n\n# Monthly charge is the amount in $ of how much the customer pays every month. We also need to normalize it.\ndata['MonthlyCharges']=(data['MonthlyCharges']-data['MonthlyCharges'].mean())/data['MonthlyCharges'].std()\n\n# Total charge is the amount in $ that  the customer has paid throughout their life. It is the LTV for each customer. We also need to normalize it.\ndata['TotalCharges']=(data['TotalCharges']-data['TotalCharges'].mean())/data['TotalCharges'].std()\n\n#Let's again print our first ten rows to see how the content of these three columns have changed.\ndata.head(10)","execution_count":null,"outputs":[]},{"metadata":{"id":"rtFHudd6LJoh","colab_type":"text","_uuid":"86c192cdcb2ccd510bc2ed3ebb67aea9558094b2"},"cell_type":"markdown","source":"###Mean Normalization Done! \n\nThose three numerical columns ['Tenure, MonthlyCharges, TotalCharges'] have all been reduced to the same scale. Our algorithm will thank us later."},{"metadata":{"id":"JNqG_QgWLJlC","colab_type":"text","_uuid":"fa86f3e615fbcdf78711f871c987caa3b6fe0435"},"cell_type":"markdown","source":"###Step 2- Label Encoding\n\nRight, now that we have taken care of **Mean Normalization**, we can turn our attention to the real problem here: the letters of the alphabet ! As we discussed earlier, they shouldn't be here. It's time for another key part of the data cleaning process: **Label Encoding**. \n\nThis is a fancy way of saying that we are going to change letters to numbers so that our algorithms can understand them.\n\nIn some of our columns, we have binary outcomes, only two possibilities. A prime example of this is actually our final column, the ***['Churn']*** column, where we have only two possible answers: **Yes** or **No**. The customer either churned or he/she didn't.\n\nWhen the value of a column is either one thing or another, we can simply replace the values by **1s** and **0s**. Here we will set **Yes=1** and **No=0**. Very easy to do and this way our algorithms will be able to understand our data:\n\nIn Excel, the function we used would be :**=IF(A1=\"Yes\",1,0)**. But once again, in Pandas, we can carry this out for all our cells in a few milliseconds with one line of code:"},{"metadata":{"id":"gr3bZ2WYu398","colab_type":"code","outputId":"517f41eb-13ee-40c6-94a2-85cecfe5ed10","colab":{"base_uri":"https://localhost:8080/","height":221},"trusted":true,"_uuid":"0946989acedef838ed3586f2640a59d73a0e53b1"},"cell_type":"code","source":"#We use a conditional command to set all the values as 1s or 0s. You can read this function as \"if the value of the cell is 'Yes' then change the cell value to 1, if not, change it to 0. Repeat for all cells of that column.\"\n\ndata['Churn']=data['Churn'].apply(lambda x:1 if x=='Yes' else 0) \n\n#Let's print out the first 10 rows of that particular column ['Churn'] to see what has happened to it.\n\nprint(data['Churn'].head(10))","execution_count":null,"outputs":[]},{"metadata":{"id":"ccF2L4n9PMbw","colab_type":"text","_uuid":"754d4e55d90e1a708882f40de3b13bc4ea7ec1e5"},"cell_type":"markdown","source":"####Very good. No more letters, just as we wanted.\n\nSo now we should look through our different columns to see which of them can be encoded as 1s or 0s. Basically, this amounts to ascertaining whether the values contained in these columns can be answered by a ***\"Yes or No\" ***  question. \n\nFor example, such a column would be the **['Partner']** column, as in :*\"Is this customer married?\"* There can only be two answers: *\"Yes\"* or *\"No\"*. \n\nLet's go over our columns to see which ones we can transform to 1 or 0:"},{"metadata":{"id":"wqe-dqCHTEIO","colab_type":"code","outputId":"c865cc5c-eae1-4af5-83af-2af97a663030","colab":{"base_uri":"https://localhost:8080/","height":572},"trusted":true,"_uuid":"1affd3aa2cf34c82dd10bd9f621739ba3f451721"},"cell_type":"code","source":"data.head(10)","execution_count":null,"outputs":[]},{"metadata":{"id":"-3gkD64kTHrY","colab_type":"text","_uuid":"3165480c8402fdb3605f0db920d63e2145c95b73"},"cell_type":"markdown","source":"After taking another look at our data, this is the list of columns I found that can be reduced to 1s or 0s:\n\n**['Gender'],['Partner'],['Dependents'],['PhoneService'],[MultipleLines''],['OnlineSecurity '],['OnlineBackup'],['DeviceProtection'], ['TechSupport'], ['StreamingTV'], ['StreamingMovies'], ['PaperlessBilling']**\n\nNote that for some inexplicable reason, the **['Senior']** column has already been encoded beforehand for us so we don't need to touch it. We ain't complaining this time but just be aware that these sort of irregularities in the data are of course all too common.\n\nSo let's apply **Label Encoding** to these columns just as we did with the **['Churn']** one:"},{"metadata":{"id":"st4QGm-cRuM_","colab_type":"code","outputId":"c350d92f-2dd6-43e6-ad4f-f658da198f08","colab":{"base_uri":"https://localhost:8080/","height":580},"trusted":true,"_uuid":"ad301b438548da6ca8939129363ccc1a5e4c9553"},"cell_type":"code","source":"\ndata['Gender']=data['Gender'].apply(lambda x:1 if x=='Female' else 0) # Note here that unlike the other column, the keyword is \"Female\" not \"Yes\", however it is of course still binary class.\ndata['Partner']=data['Partner'].apply(lambda x:1 if x=='Yes' else 0)\ndata['Dependents']=data['Dependents'].apply(lambda x:1 if x=='Yes' else 0)\ndata['PhoneService']=data['PhoneService'].apply(lambda x:1 if x=='Yes' else 0)\ndata['MultipleLines']=data['MultipleLines'].apply(lambda x:1 if x=='Yes' else 0)\ndata['OnlineSecurity']=data['OnlineSecurity'].apply(lambda x:1 if x=='Yes' else 0)\ndata['OnlineBackup']=data['OnlineBackup'].apply(lambda x:1 if x=='Yes' else 0)\ndata['DeviceProtection']=data['DeviceProtection'].apply(lambda x:1 if x=='Yes' else 0)\ndata['TechSupport']=data['TechSupport'].apply(lambda x:1 if x=='Yes' else 0)\ndata['StreamingTV']=data['StreamingTV'].apply(lambda x:1 if x=='Yes' else 0)\ndata['StreamingMovies']=data['StreamingMovies'].apply(lambda x:1 if x=='Yes' else 0)\ndata['PaperlessBilling']=data['PaperlessBilling'].apply(lambda x:1 if x=='Yes' else 0)\n\n# Let's again print out our 10 first rows to see what the data now looks like.\n\ndata.head(10)\n\n\n\n\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"id":"G3CdxLirUNCu","colab_type":"text","_uuid":"b8e45b603fe394e59dcbaed8856ef20d03ad230f"},"cell_type":"markdown","source":"Ok, looking a lot better ! However, there are still three columns that contain text. Let's deal with those now...\n\n---\n\n"},{"metadata":{"id":"Xk5uvkZUUWKN","colab_type":"text","_uuid":"9b816b117cf814b4b66c76330ac06e4cec8497c5"},"cell_type":"markdown","source":"###Step 3 - One Hot Encoding\n\nRight, so now to the next challenge on our hands. Some of our columns, as I'm sure you've noticed, do not have binary outcomes, but rather they contain three or more possible text values. For instance, the column **['InternetService']**, which tells what type of internet service the customer is using, has the following possible outcomes \n\n\n*   **Fiber optic**\n*   **DSL**\n*   **No Internet**\n\n\n\n\n\n"},{"metadata":{"id":"E8GkALLOjDda","colab_type":"code","outputId":"6436597b-de49-4745-f882-6534a257d5e2","colab":{"base_uri":"https://localhost:8080/","height":88},"trusted":true,"_uuid":"e03bac99d10ae7da1c9e16093c5fc7e1fe595cd5"},"cell_type":"code","source":"# This handy command \".value_counts()\" give us all the different values of the ['InternetService'] column and the number of times they appear.\n\ndata['InternetService'].value_counts()\n","execution_count":null,"outputs":[]},{"metadata":{"id":"-rlV9A0hk3iB","colab_type":"text","_uuid":"1232605348dcbd9e58f7811668cca59c3ce14fe6"},"cell_type":"markdown","source":"So, what on earth are we to do here? Should we just encode No=0, DSL= 1 and Fiber Optic =2?  But in that case, if 2<1, then is FiberOptic < DSL? That, unfortunately, doesn't make any sense, we have a ranking problem. In fact, we don't want rank, we only want to differentiate. So how can we represent these values without implying that one is greater than the other?\n\nThe solution here is something called ***One Hot Encoding***. The trick  is actually to **create ** new columns for each possible outcome. So from the column **['InternetService']**, we are going to create three new columns : **['InternetService-FiberOptic'], ['InternetService-DSL'], ['InternetService-NoInternet']**. Each column will either have  a **1** or **0**. So if our customer has DSL Internet, as is the case with the customer ***7590-VHVEG***  in our first row , then our three new columns will look like this in the first row:\n\n*   **['FiberOptic']** =  0\n*   **[DSL']**=  1\n*   **['No']**=  0\n\n\nAnd finally, we can delete our original column  **['InternetService']**. After extrapolating its content we no longer need it.\n\nIf you would like to learn more about **One Hot Encoding**, have a look at  <a href=\"https://youtu.be/0xVqLJe9_CY?t=321\">this fun video</a>  by Siraj Raval, great stuff. \n\nAnyway, let's see what  **One Hot Encoding** looks like in practice. Again, it is possible to do this in Excel however it would take us roughly 5-10 minutes for a small dataset like this, whereas if we use a **Pandas** command, it should only take us a couple of seconds.\n\n\n"},{"metadata":{"id":"ZuGu5KvIp-x4","colab_type":"code","colab":{},"trusted":true,"_uuid":"aa2061a6bb9a9787c0b62f191761dc7f190cbdf0"},"cell_type":"code","source":"# This takes all the different values (here denominated as 'keys') of the column ['InternetService']\n# Here we iteratively create one new column for each value of the ['InternetService'] column. \n#In this particular case, we will be creating three new columns, whose values will either be 1 or 0 based on whether the original column contained that value or not.\n\nfor x in data['InternetService'].value_counts().keys(): \n      data[x]=data['InternetService'].apply(lambda d: 1 if d==x else 0)\n    \n# As discussed previously, we now ditch the original column ['InternetService'], we now longer need it!    \ndata.drop(columns=['InternetService'], inplace=True)\n\n\n# As always let's have a look at what our datasheet now looks like. The new columns are the last three !\ndata.head(10)\n\n","execution_count":null,"outputs":[]},{"metadata":{"id":"4qs4khiaxwIp","colab_type":"text","_uuid":"9ac047b43888c71af089e8ec0d4c6f1c1e79cb93"},"cell_type":"markdown","source":"Excellent, if we now look to the far] right of our sheet, we'll see three new columns: **['FiberOptic'], ['DSL']**, and **['No']**. You will note that our total number of columns as grown from 20 to 22, seeing as we added 3 new columns and dropped 1 ( the **['InternetService']** which is now represented in our three new columns.\n\nLooking through our data, we can see that we need to perform the same procedure on  the **['Contract']** column, which informs us what type of contract our user has.] There are obviously more than two options as we can see below:\n"},{"metadata":{"id":"_zNkxGV0yzLr","colab_type":"code","outputId":"238bcc7f-6ceb-4610-ea44-6d74d8e66c9d","colab":{"base_uri":"https://localhost:8080/","height":85},"trusted":true,"_uuid":"c772c5b2bc74757b3e4f88d73c91dcbd50776c40"},"cell_type":"code","source":"data['Contract'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"id":"BuMrPWNfzBqN","colab_type":"text","_uuid":"68b07d1cf000f7014f2e6d88e334d7b51aa0e366"},"cell_type":"markdown","source":"Again, it looks like we have three possibilities here. So let's carry out **One Hot Encoding** on this column exactly the same way as we did before:"},{"metadata":{"id":"IMjaAPVSzQGm","colab_type":"code","colab":{},"trusted":true,"_uuid":"6a513ae58042277d78b66a1a5d1ae091b0dd9d81"},"cell_type":"code","source":"#Getting the value within the original columns\nfor x in data['Contract'].value_counts().keys():\n    data[x]=data['Contract'].apply(lambda d: 1 if d==x else 0)#Creating the new columns, containing either 1s or 0s\n    \ndata.drop(columns=['Contract'], inplace=True) # delete the original column\n    ","execution_count":null,"outputs":[]},{"metadata":{"id":"izgS1vcy0HHv","colab_type":"text","_uuid":"8ce4e6b91afc794d941146638b289f9e87350b55"},"cell_type":"markdown","source":"And finally, let's do the same for the column ['PaymentMethod'], which indicates what type of Payment the customer uses:"},{"metadata":{"id":"SVx4Us9p0WuK","colab_type":"code","outputId":"4243f5cc-a34d-4402-a414-53dd4274218a","colab":{"base_uri":"https://localhost:8080/","height":102},"trusted":true,"_uuid":"63ae9b8b6fd93548cd6580aed68cb2b759a3d800"},"cell_type":"code","source":"data['PaymentMethod'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"id":"1YnagAjC0iyw","colab_type":"text","_uuid":"1d0b9683ab61f458c202ae2ec959cfabfa8eb3e6"},"cell_type":"markdown","source":"Looks like we have 4 different possibilities here. However, because our code is super smart, we can run it the exact same way as before and it willstill automatically create 4 new columns:"},{"metadata":{"id":"KEAA-jRX0wnJ","colab_type":"code","outputId":"c7640166-9e61-493d-dcb0-ed55dc752d45","colab":{"base_uri":"https://localhost:8080/","height":643},"trusted":true,"_uuid":"113289c139c68687acdc9ca4292069e1a1e32062"},"cell_type":"code","source":"for x in data['PaymentMethod'].value_counts().keys():#Getting the value within the original columns\n    data[x]=data['PaymentMethod'].apply(lambda d: 1 if d==x else 0)#Creating the new columns, containing either 1s or 0s\n    \ndata.drop(columns=['PaymentMethod'], inplace=True) # delete the original column\n\n#Let's now have a look a our data:\ndata.head(10)\n\n    ","execution_count":null,"outputs":[]},{"metadata":{"id":"mdl0qCTc1DRf","colab_type":"text","_uuid":"a259115586a943271d81e36917ac66d8054293ed"},"cell_type":"markdown","source":"####At long last ! \nWe´ve done it, all data in every one of our cells is now a numerical value expressed between [-1.5 and 1.5]. \n\n\nNow, just for safekeeping, let's dowload and save to the hardrive our formatted data so that we don't need to go through all that again."},{"metadata":{"id":"PkWMsDkSKO5E","colab_type":"code","colab":{},"trusted":true,"_uuid":"d27e0411645719ce3e083194007b6db5ddafc025"},"cell_type":"code","source":"#Using the \".to_excel\" command to create a .xlsx file containing the newly formatted\"data\".\ndata.to_excel('Telco_Churn_Formatted_For_ML.xlsx')\n\n\n#The command that allows us to download the data from the browser.\nfiles.download('Telco_Churn_Formatted_For_ML.xlsx')","execution_count":null,"outputs":[]},{"metadata":{"id":"tXPMJr1AKFdx","colab_type":"text","_uuid":"c41c92cb97fd5dcf7c5a81ad04424ed06cedb69b"},"cell_type":"markdown","source":"####Our data is now correctly formatted and we can now feed that data into our learning algorithms.\n\nBut first, there is still one last thing to do, maybe the most important thing of all..."},{"metadata":{"id":"G-GK9Mf93WgW","colab_type":"text","_uuid":"5682153e9e9de3f913c547a32412fd023a3f84ef"},"cell_type":"markdown","source":"###Step 4- Feature Selection\n\nPossibly the most important part of Machine Learning is deciding what information we will actually feed into the algorithm. This is where the humans add their value. You need to think logically but also trust your intuition. Depending on the problem you are trying to solve you can experiment with all types of features- you might uncover a hidden relationship between two features that only you thought to try out. It is a good **Feature Selection** rather than the amount of raw data  that will determine whether you are one step ahead of the competition or not.\n\nIn our case, we are going to explore our data to see which features are insignificant, in order to drop them from our data. Ditching useless columns is an absolute **must**. If we use a column that brings no value to our algorithm, in the best case scenario, our algorithms will take longer than necessary and we'll be wasting memory and computational power ( this can be quite a serious issue if your dataset consists of 7 million users rather than 7 thousand). But in the worst case scenario, our algorithm can actually be confused by this pointless data and make sub-optimal predictions because of them.\n\nThere are plenty of techniques that can be used for finding out which columns are the most important and relevant and drop the ones that  aren´t. Here we are going to use a conjunction of two techniques ( **Correlation** and **Decision Tree Check**). If both techniques tell us that the same column is irrelevant, then we can safely discard it.\n\nFirst, let's seperate our data into the **x** datasheet where are indicators are located and the **y** datasheet that holds the thing we are trying to predict (['Churn']). This will allow us to explore the relationship between the two."},{"metadata":{"id":"4qv7ag-yn1ZN","colab_type":"code","outputId":"84cacb1a-11ae-4631-b3f8-925e6ea4efae","colab":{"base_uri":"https://localhost:8080/","height":221},"trusted":true,"_uuid":"ca6e5f37685393c89b9b9df3f46dbcdc00f7c14f"},"cell_type":"code","source":"#creating our x datasheet, which contains all our data except for the ['Churn'] column.\nx=data.drop(columns='Churn')\n\n\n#The ['Churn'] column extracted.\ny=data['Churn']\n\ny.head(10)","execution_count":null,"outputs":[]},{"metadata":{"id":"h9BRswlson3G","colab_type":"text","_uuid":"404526533dfb281121c2e5366b5259fc39eec538"},"cell_type":"markdown","source":"###Exploring Correlations\n\nNow that we have two datasets, we can explore the correlation between them.\n\nWe can use the \".corr\" command for this, this automatically calculates the correlation coefficient between two columns."},{"metadata":{"id":"S0NjQAh7tg2R","colab_type":"code","outputId":"45666ba9-62f9-4f7a-f465-202011561454","colab":{"base_uri":"https://localhost:8080/","height":479},"trusted":true,"_uuid":"81955acab70fc2ac202169fc112053700f12e37b"},"cell_type":"code","source":"# We create an iterative operation ( also known as a \"For Loop\") to get the correlation between each column and the churn metric (y))\n\nfor columnname in x.columns: \n    print('The Correlation Between',columnname,'and Churn is:', abs(x[columnname].corr(y)))\n    \n#the outcome of the operation is to print out the correlation coefficient along with a text for clarity purpose.\n#we use the \".corr()\" command to calculate the correlation and the \"abs\" to always return a positive number, we don't care whether the correlation is negative or positive, we just want to check that there is one!","execution_count":null,"outputs":[]},{"metadata":{"id":"gQrVfP6-vWBK","colab_type":"text","_uuid":"6c653cb0d9535bd37c7e3fa1a873345218048d1c"},"cell_type":"markdown","source":"![alt text](https://www.researchgate.net/profile/Ying_Wu5/publication/41487763/figure/tbl1/AS:669701021655064@1536680574532/Various-strengths-of-correlation-coefficients-as-a-measure-of-concordance.pngg)\n\nWe can use this table to check which columns have zero correlations. You may use whichever threshold you wish, but in general, if the correlation coefficient is smaller than **0.1**, we will consider it to have no correlation whatsoever with our *['Churn']* outcome. In our case, here are the columns that seem to be pretty useless because they are not correlated to ['Churn']:\n\n\n\n*  **['Gender']** ( hardly surprising, the gender of a customer should have nothing to do with the reason why they churned.)\n*   **['PhoneService ']** \n*  **['MutipleLines ']**  (somewhat surprising, it looks having a phone line with the company has nothing to do with whether the customer is going to churn or not)\n* **['OnlineBackup  ']**\n* **['DeviceProtection']**\n* **['StreamingTV  ']**\n* **['StreamingMovies   ']**\n\n\nSo what we can see is that the services listed above have no almost bearing on whether the customer is going to churn. Could we infer from that customers are generally satisfied with these services? Quite possibly.\n\nBut for now, this is a strong indication that we shouldn't use these columns in our final data. However, we want to make sure that we are not going to drop columns that could have a hidden influence on the Churn.\n\nSo before we go ahead and drop these columns, let us double check with a second test to validate their relevance ( or lack of).\n\n\n\n"},{"metadata":{"id":"vJBsgIVL2Wps","colab_type":"text","_uuid":"f4f33ca554aacbd0f3996ba6e3ff4890d14e60a3"},"cell_type":"markdown","source":"### Using a Decision Tree to tell us which features are useful\n\nThis will be step two of our feature selection process. Here we actually use a learning algorithm to make predictions. In this case, we will use a **Decision Tree**, which is in fact one of the weakest  and simplest algorithms out there. Despite this, the **Decision Tree** algorithm has a very useful property : \nit ranks which features allow it to make good predictions and which features does not help it in the slightest. "},{"metadata":{"id":"iFdjwalNompz","colab_type":"code","outputId":"d93eff53-5f8b-4319-b386-27aa0e4ddc1e","colab":{"base_uri":"https://localhost:8080/","height":459},"trusted":true,"_uuid":"65a2b685b0903ea705d61edc4d712ad6d7f245f3"},"cell_type":"code","source":"#We create the Decision Tree algorithm, simply by loading it from the Sklearn library in a single line.\n\nfeature_importance_indicator=ExtraTreesClassifier(n_estimators = 100)\n\n#Then we ask this algorithm, now dubbed as \"feature_importance_indicator\", to learn from our data x ( the indicators) and y( the churn outcome):\n\nfeature_importance_indicator.fit(x,y)\n\n#We then ask the model politely to create a list of which columns it learnt the most from and which columns didn't help it at all, for this we use the \"feature_importances_\" command:\n\nimportance_dict= dict(zip(x.columns, (feature_importance_indicator.feature_importances_)))\n\n#We sort the list in descending order so that it is quicker to analyse.\n\nimportance_dict_sorted= OrderedDict(sorted(importance_dict.items(), key=lambda x:x[1],reverse=1))\n\n#The \"pprint\" commnad allows us to print things out nicely.\n\npprint.pprint(importance_dict_sorted) \n","execution_count":null,"outputs":[]},{"metadata":{"id":"zL_Yy0U9398e","colab_type":"text","_uuid":"7f85e8b8316ed403489f19d71d0ce7756d4c99ef"},"cell_type":"markdown","source":"Fantastic. So above we have a clear breakdown of how important each column is] to the final prediction. Put very simply, the higher this coefficient is, the more important it is for making these predictions. In case you want to learn more about what these numbers really mean, you can check out [this useful piece of documentation](https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html).\n\nJust like with the correlations, you can use whatever threshold you like, however I'm going to go with the regular **0.05** measure. If the importance of the column is below 0.05, then we can conclude that the **Decision Tree** deems this column completely useless. So we will now check the feature importance of the columns that we had singled out as having no correlation in the previous step. Here they are with their feature importance coefficient:\n\n\n*  **['Gender']** : 0.038\n*   **['PhoneService ']**  : 0.007\n*  **['MutipleLines ']**  : 0.024\n* **['OnlineBackup  ']**: 0.027\n* **['DeviceProtection']** : 0.026\n* **['StreamingTV  ']** : 0.022\n* **['StreamingMovies   ']**: 0.23\n\nYep, that's right, none of our columns have survived the second test. We can now safely eliminate them from our data, thus saving us computer memory and avoiding unnecessary confusion for our dear algorithm.\n\nYou will have of course noted that there are plenty of other columns that fell below the **0.05** mark, however, because they all had some sort of correlation with the ['Churn'] column, they might be useful for other types of algorithms, so we'll be keeping them for now. Remember, we only eliminate our columns that have failed both tests so that we can be 100% confident that they cannot contribute to the accuracy of our prediction.\n\nSo let's get rid of these useless columns.\n"},{"metadata":{"id":"jSEaFLgd-Wam","colab_type":"code","colab":{},"trusted":true,"_uuid":"75b611bd4e7d3159969e05ab5e3b4ecc802fd899"},"cell_type":"code","source":"# with the \".drop\" command, we can delete the columns we have deemed unnecessary.\nx.drop(columns=['Gender','PhoneService','MultipleLines','OnlineBackup','DeviceProtection','StreamingTV','StreamingMovies'],inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"id":"vjJp6MpOAFLS","colab_type":"code","outputId":"07b5a96f-433b-4fb5-f8b3-222dc9d547d8","colab":{"base_uri":"https://localhost:8080/","height":614},"trusted":true,"_uuid":"5be1f5a05082954d2c10253bdd9569e7165d63c4"},"cell_type":"code","source":"#Let's print our final data\nx.head(10)","execution_count":null,"outputs":[]},{"metadata":{"id":"K1W3YsticD93","colab_type":"text","_uuid":"525cc11e76df17674b92e67cb3034160012705e8"},"cell_type":"markdown","source":"Done! As you can see, our dataset is has been reduced to  19 columns. One final thought:\n\n###**Important Features**\n\nThe cool thing about **Feature Selection** is that it also tells us which features seem to be the most important in defining the outcome of what we are seeking to predict. This in itself can be extremely insightful.\n\nFor example, in our case, if we go back and look for the highest coefficients in both tests, these columns clearly stand out:\n\n\n*   **['Tenure']**\n*   **['MonthlyCharge']**\n*   **['TotalCharge']**\n\nAs we see, these columns are related to the cash amount customers have paid. This can be very interesting for two reasons:\n\n\n\n1.   If after training our model, we aren´t happy with its accuracy, then we know that we need to collect more new data related to the amount that the customers pay, as this seems to be highly indicative of whether they churn or not.\n\n2.   At a company level, I think it is important to know that these are the deciding factors in a user to leave the company. We could deduce from this that there is a lot of market competition and/or that the users are not perceiving much differentiation in this company's services. Either way, the user is **very price-elastic**. Fortunately, this also means that price incentives such as discounts should be an effective way of preventing users who are about to churn from actually doing so.\n"},{"metadata":{"id":"PmPJuAXcNQBj","colab_type":"text","_uuid":"90406dcd6447248d6945c3d4452f2df282acf7f0"},"cell_type":"markdown","source":"##Part 2- Data Learning \n\nSo this is actually the surprising easy part. Now that our data has been formatted and we've dropped the irrelevant columns, we can now feed it into our learning algorithm  that will learn from the data to make predictions.\n\n\n"},{"metadata":{"id":"e3Cn7rtJTzVV","colab_type":"text","_uuid":"8a0522df7d9708d88bca51c8f17b9c2cbea0c01e"},"cell_type":"markdown","source":"####Step 1- Splitting the data\n\nFirst things first, we need to split our data into two sections. The first is called **'training data'** which we'll give to the algorithms to learn from. The second set will be called **'testing data'**, we will use this data to evaluate how good the predicitions from our models are. Think of it as an exam for our learning algorithm.\n"},{"metadata":{"id":"xze3AnNjEFcd","colab_type":"text","_uuid":"72fbd3061b100a733e352c889688c5bf326f5e7b"},"cell_type":"markdown","source":""},{"metadata":{"id":"vIhafRjOVT8A","colab_type":"text","_uuid":"121af5dd521eff1ffbf16ec5d5f5e65909d81425"},"cell_type":"markdown","source":"*We* can break down our new data in the following way:\n\n\n\n1.   **x_training** (80% of the data)\n2.   **y_training** (80 % of the answers)\n3.   **x_testing**  (20 % of the data that the model never sees, used for making new predictions to evaluate the model)\n4.   **y_testing**  (20 % of the answers, used to see how they match the predictions made by the model on the 'x_testing' data)\n\n\n\n"},{"metadata":{"id":"8z173VE-OjBH","colab_type":"code","colab":{},"trusted":true,"_uuid":"ae52238fb032f0aa9bd01eee3872cd648a172ba1"},"cell_type":"code","source":"# We use the \"train_test_split\" command to seperate our data into \"training\" and \"testing\". The \"test_size\" variable allows to decide how much of the data we will keep for testing, in this case we do a conventional 80-20 split. \n#Random state shuffles the data, which is allows good practice. The number 18 just allows to shuffle the data the same way every time the code is executed, important for reproducability.\nx_training, x_testing, y_training, y_testing = train_test_split(x, y, test_size=0.2, random_state=18)\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"id":"5WtUwDprWnL8","colab_type":"text","_uuid":"9f28f11cecf6f950ee55d50b5e8c786be7ac2d95"},"cell_type":"markdown","source":"####Step 2 - Choosing The Evaluation Metric\n\nFirst of all, let's define some useful vocabulary. This will help us to understand how well our model is performing. There are four types of predictions that our model could make: **True Positive**, **False Positive, **True Negative**, **False Negative**. Let's define these terms:\n\n* **True Positive**: This is a good prediction, our model predicted that the customer was going to churn (emitted a **1**) and the customer did in reality churn (churn=**1**).\n\n* **False Positive**: This is an incorrect prediction, our model predicted that our customer churned ( emitted a **1**), however it turns out that customer did not in fact churn (churn=**0**)\n\n* **True Negative** : This is again a correct prediction, our model said that the customer **was not** going to churn ( it emitted a **0**) and this turned out to be correct in the real world ( churn was equal to **0**)\n\n* **False Negative**: Again, this is an incorrect prediction, but of a different kind. Here our model predicted that the customer was not going to churn (emitted a **0**), but lo and behold, in reality our customer did in fact leave the company (churn=**1**)\n\nWith this now clearly defined, we need to decide what evaluation metric we will use to evaluate our model.\n\nThis actually is a crucial concept, and we have three evaluation options to choose from:\n\n1.   **Precision** - High precision means that the model does not give many **\"False Positives\"**. \n\n2. **Recall** High Recall means that model has a very low proportion of **\"False Negatives\"**. \n\n3.  **F1 Score** The F1 Score is a balance between Precision and Recall, this a great  measure for overall accuracy. \n\nWhich of these metrics are most important to pay attention to? Well, this wholly depends on your context and what type of error is most costly to your business. Here we need to analyse the consequence of each type of error.\n\nIf, for example a **false positive** is very very costly, such as in the case of Face Recognition or Mortgage Approval, then we will want to maximise the **Precision** metric as much as possible.\n\nHowever, if getting a false negative is even more problematic, as for example in cases such as Fraud Detection or Cancer Diagnosis, then we'll make sure our model has the highest **Recall** measure possible. \n\nIf obtaining a false negative is just as bad as a false positive and there is no difference between them, then we can simply use the **F1 score**. \n\nRemember there is no right or wrong answer here because it is a judgment call based on your own individual context.\n\nSo let's go back to our own specific case and decide which is the most costly. Imagine, we wrongly thought that a customer was about to Churn when in fact he/she had no intention in doing so ( **false positive**), the worst case scenario would be that we gave them an unnecessary discount.\n\nHowever, if we failed to identify that a customer was about to churn ( **false negative**), then the consequences are a lot bleaker: the customer can move to the competition. It is well known that in telecommunications, the LTV ( Life Time Value) is high ( in the case of Telco, our data informs us that it is around **$2,500** per customer ! )  and so losing that customer will be extremely costly and getting  them back will be very lengthy.\n\nThis is indeed a pretty clear case where having a false negative error is a lot more problematic than a false positive, so we'd rather have a model that produces very few **false negatives** (even if that means that it will make more **false positives**). Therefore, we'll use **Recall** as our evaluation metric.\n\n"},{"metadata":{"id":"sn-RpLDUIhJO","colab_type":"text","_uuid":"a678d675d5ea7ba8b0a30f674436510a708a6d7a"},"cell_type":"markdown","source":"####Step 3- Creating, training and testing our algorithms \n\nSo now that we have our data and we know what evaluative metric we'll be using, we can finally use our learning algorithms. We'll be using  with an extremely common and effective algorithm called **Logistic Regression**.\n\n\n\n* **Logistic Regression** Quite possibly the most popular algorithm for binary classification.  We will use this algorithm because it is one of the best for binary classifications and also works well with small datasets ( less than 10,000) like in our case. Most importantly of all, it is very easy to optimize with regards to our evaluation metric.\n\n\n\nHere is a visual example of how **Logistic Regression** is able to discriminate between the data:\n\n\n![alt text](http://api.ning.com/files/BLRhjJ5GSEnu-TjYW2cexTEbLfMnDWRa40PPL0SrRhIgpFmTjY5n9xFH24K1KQqp4U28glRU-UWum3rr50*b8stW2KedAi02/Capture.PNG)\n\n\n\n\nIf you are curious about the inner workings of Logistic Regression, you can learn more [here](https://en.wikipedia.org/wiki/Logistic_regression#Logistic_model) and [here](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html). But as always, there is absolutely no need to understand the algorithm mathematically to use it.\n\nThanks to the **SKLearn Library**, we simply need one line of code to load the algorithm. That's it.\n\n"},{"metadata":{"id":"vqHu8v0dJGwK","colab_type":"code","colab":{},"trusted":true,"_uuid":"b5e03baea1c0644593b0ec63a464118500ab39da"},"cell_type":"code","source":"# We load the Algorithm from the library and give the name of \"logistic_regression\"\nlogistic_regression= LogisticRegression()\n\n","execution_count":null,"outputs":[]},{"metadata":{"id":"mn02H6ZVJs-e","colab_type":"text","_uuid":"73244c8319f4ac831e38f9720d4e97ce558efefe"},"cell_type":"markdown","source":"Next, we only need another line of code to tell our algorithms to start to learn the mapping between our **\"x_training\"** data and the **\"y_training\" ** answers.  For this, we only need to use the ** \".fit\"** command. This is the bulk of the work done automatically for us. Running this might take 5 seconds ( and you can ignore all the gobbledygook).\n\n\n"},{"metadata":{"id":"UlavMTUG_ECL","colab_type":"code","outputId":"28bbdf7d-038a-415d-ff1d-e47cb4f6ddff","colab":{"base_uri":"https://localhost:8080/","height":139},"trusted":true,"_uuid":"8a766a30e908513e3688611d361350b1e07ae695"},"cell_type":"code","source":"logistic_regression.fit(x_training,y_training)","execution_count":null,"outputs":[]},{"metadata":{"id":"UuPNyr4vLDd1","colab_type":"text","_uuid":"4c089c6f007edc4f1a1d97ac46e23f166f3984bb"},"cell_type":"markdown","source":"Our models are now trained! That wasn't so hard now was it?\n\nRight, now that the algorithm has learned how to make churn predictions from this data, we need to evaluate how good those predictions are. \n\nRemember we are using the **recall** metric ( high recall= **low false negatives**). Here we use the \". predict\" command to get our model to make a prediction for the testing examples. We can then compare the model's predictions to the real-world answers of both the training dataset (**y_training**) and the training dataset (**y_testing**).\n\nHere, using a loop, we can easily calculate the number of True Positives (**TP**), False Positives (**FP**), True Negatives (**TN**) and False Negatives(**FN**) that our model produced.\n\nIn simple terms, remember that we are looking for the model's predictions to have as few False Negatives (**FN**) as possible."},{"metadata":{"colab_type":"code","id":"IduOHm8Es3hv","colab":{},"trusted":true,"_uuid":"9686ad878c0c437ffaa15d6f3a6ea34146aee9e7"},"cell_type":"code","source":"#We initialize the following variables that help us count the number of True Positives, False Positives, etc.\nTP=0\nFP=0\nTN=0\nFN=0\n\n\n#We use a loop to count the number of TN, TP, TN, FN. Each time we detech one, we add a '1' to the corresponding variable\nfor i in range(len(x_testing)):\n  \n    if logistic_regression.predict(x_testing.iloc[[i,]])==1 and y_testing.iloc[i]==1:\n                                       TP=TP+1\n    if logistic_regression.predict(x_testing.iloc[[i,]])==1 and y_testing.iloc[i]==0:\n                                       FP=FP+1      \n    if logistic_regression.predict(x_testing.iloc[[i,]])==0 and y_testing.iloc[i]==0:\n                                      TN=TN+1    \n                                      \n    if logistic_regression.predict(x_testing.iloc[[i,]])==0 and y_testing.iloc[i]==1:\n                                       FN=FN+1   \n#Printing our results\nprint (\"Logistic Regression\", \"True Positives:\",TP, \"False Positives:\",FP,\"True Negatives:\",TN , \"False Negatives:\", FN) \n\n\n#Calculating the 'Recall' metric with this simple formula\nlog_recall=TP/(TP+FN)\n\n#Printing our the 'Recall' metric in %\nprint(\"Logistic Regression Recall On Training Data:\", log_recall *100,'%')\n\n","execution_count":null,"outputs":[]},{"metadata":{"id":"BTOfB6sRAJqJ","colab_type":"text","_uuid":"86297ef519790b6186936d7b29a7a267dac89ef3"},"cell_type":"markdown","source":"####Threshold importance\nIt is usually a good idea to set an accuracy threshold to aim for. Once the algorithm has hit that objective on the testing data, then we consider the learning part done and dusted  This threshold is usually set by the higher management. \n\nIn our case, we'll imagine that we've been given a **90%** recall threshold as our target. We want our algorithms to detect if a customer is going to churn 90% of the time. \n\nAfter running our algorithm for the first time, we see we have a recall rate of **51%**, not good enough, our model is only able to recognize the churned customer 55% of the time. \n\n####Model Optimization\n\nWe will now optimize our model. As discussed earlier, we want to be sure to catch all the churned customers. This means we need to ask our model to be more biased ( at the moment it is treating  the churned class (**1**) and the non-churned (**0**) with the same importance (50-50). We can now ask our model to give more priority to churning customers (those with **1** value). Let's first try a 70-30 approach.\n"},{"metadata":{"id":"JmJFjvZNAIVg","colab_type":"code","colab":{},"trusted":true,"_uuid":"a73b6a576006fffbfba3da181fd44719ef49b02a"},"cell_type":"code","source":"#we initialize a class weights for our two labels : 1 (churned) and 0 (churned) at 70-30\n\nclass_weights ={1: 0.70, 0: 0.30}\n\n#we create a new logistic regression model optimized for a better recall and retrain it on our data\nlogistic_regression_opt= LogisticRegression(class_weight=class_weights)\nlogistic_regression_opt.fit(x_training,y_training)","execution_count":null,"outputs":[]},{"metadata":{"id":"bzZEOdzeWYWV","colab_type":"text","_uuid":"9407189f17aa4c2fbaeaf5705099b79afd2b48b3"},"cell_type":"markdown","source":"We then reevaluate the model with the same method as we used earlier."},{"metadata":{"id":"2g32bBxWBork","colab_type":"code","colab":{},"trusted":true,"_uuid":"57a3baec68c72a881b2857f056ea087fb73aa9b7"},"cell_type":"code","source":"TP=0\nTN=0\nFP=0\nFN=0\n\nfor i in range(len(x_testing)):\n    if logistic_regression_opt.predict(x_testing.iloc[[i,]])==1 and y_testing.iloc[i]==1:\n                                       TP=TP+1\n    if logistic_regression_opt.predict(x_testing.iloc[[i,]])==1 and y_testing.iloc[i]==0:\n                                       FP=FP+1      \n    if logistic_regression_opt.predict(x_testing.iloc[[i,]])==0 and y_testing.iloc[i]==0:\n                                       TN=TN+1    \n                                       \n    if logistic_regression_opt.predict(x_testing.iloc[[i,]])==0 and y_testing.iloc[i]==1:\n                                       FN=FN+1   \n\nprint (\"Logistic Regression\", \"True Positives:\",TP, \"False Positives:\",FP,\"True Negatives:\",TN , \"False Negatives:\", FN) \n\nlog_recall=TP/(TP+FN)\n\nprint(\"Logistic Regression Recall On Training Data:\", log_recall *100,'%')\n","execution_count":null,"outputs":[]},{"metadata":{"id":"GEWHhKyNB8Ys","colab_type":"text","_uuid":"937dfe60535f6a427dfa9156430b9faceb7bb179"},"cell_type":"markdown","source":"Great, that seemed to have worked!  Our recall shot  up and is now **75%** ! However we are still short of our 90% target, but we are heading in the right direction. We can see now that our model is starting to get more 'paranoid' about who is going to churn. As a consequence, it identifies more churning customers, but also sometimes over hits the mark and gives us more **False Positives**.\n\nIn order to reach our 90% target, let´s try an even more skewed model, with a 85-15% split prioritization for the churned customers (**1 **  value)."},{"metadata":{"id":"Ry2SEuun7MrL","colab_type":"code","colab":{},"trusted":true,"_uuid":"336b6c96a74adaf6b06c24d6b80770c25cc88e71"},"cell_type":"code","source":"#Update the weights to 85-15%\nclass_weights ={1: 0.85, 0: 0.15}\n\n#Retrain the algorithm\nlogistic_regression_opt= LogisticRegression(class_weight=class_weights)\nlogistic_regression_opt.fit(x_training,y_training)\n","execution_count":null,"outputs":[]},{"metadata":{"id":"IMxUxP1OzXwV","colab_type":"code","colab":{},"trusted":true,"_uuid":"476d333247b51ede570efb653de51a6eda1492eb"},"cell_type":"code","source":"#Re-evaluating the model on the 85-15 split.\n\nTP=0\nTN=0\nFP=0\nFN=0\n\nfor i in range(len(y_testing)):\n  \n    if logistic_regression_opt.predict(x_testing.iloc[[i,]])==1 and y_testing.iloc[i]==1:\n                                       TP=TP+1\n    if logistic_regression_opt.predict(x_testing.iloc[[i,]])==1 and y_testing.iloc[i]==0:\n                                       FP=FP+1      \n    if logistic_regression_opt.predict(x_testing.iloc[[i,]])==0 and y_testing.iloc[i]==0:\n                                       TN=TN+1    \n                                       \n    if logistic_regression_opt.predict(x_testing.iloc[[i,]])==0 and y_testing.iloc[i]==1:\n                                       FN=FN+1   \n\nprint (\"Logistic Regression\", \"True Positives:\",TP, \"False Positives:\",FP,\"True Negatives:\",TN , \"False Negatives:\", FN) \n\nlog_recall=TP/(TP+FN)\n\nprint(\"Logistic Regression Recall On Training Data:\", log_recall *100,'%')\n\n","execution_count":null,"outputs":[]},{"metadata":{"id":"woSJtVS2SG0q","colab_type":"text","_uuid":"c35421e7261f9fb3cc027bddc6779c92f89777b0"},"cell_type":"markdown","source":"Done. Our recall has now reached **91 %**  ! This means that when given the testing data that our model had not previously seen, the model was able to predict a customer churning 91% of the time.  We have reached our target!\n\nHere, you will note that as expected, our number of **False Negatives** fell sharply ( from 175 to 30) while the number of **True Positives** almost doubled as a consequence ( from 186 to 331). However, as a trade-off, the number of **False Positives** rose significantly ( from 99 to 449). We can live with this because as previously discussed, the consequences of a **False Positive** are vastly smaller in comparison to a **False Negative**.\n\n \nOur model learning is now finished, since we now have a model that is able to catch a customer who is  about to churn **91% ** of the time just as our pre-established threshold instructed. This is when you can crack open the champagne . The most critical part is done and dusted. We now need to  put our new super predictive model to good use."},{"metadata":{"id":"GAZL6CgpfrnE","colab_type":"text","_uuid":"5cd6009479144d5afc9cf10f2317a5b16a616d1e"},"cell_type":"markdown","source":"# Part 3- Data Predicting\n\n*Disclaimer: This section is optional and perhaps the least interesting of the three. We will now set up a small program that can alert employees if a customer is about to churn based on new data. This part can also be readily executed with traditional web development tools by a programmer and it is not as relevant for non-technical staff to know how this works. However, if you are curious, please feel free to keep reading.*\n\n#### Step 1- Inputting New Data\n\nIn this first section, we will prompt a user, in this case, let's imagine this to be a customer representative, to input the data concerning a new customer. Later on, we will take this new data and ask our previously trained model to tell us whether this particular customer is likely to churn or not.\n\nWe create the input prompts:\n\n"},{"metadata":{"id":"1AaYYs5MNsOo","colab_type":"code","colab":{},"trusted":true,"_uuid":"a392b7416d08d1ca73b146fda8f1dce20a5efe6d"},"cell_type":"code","source":"\n#These prompts will ask the customer representative to type in the answers so that we can collect the data we need to know so that we can predict their churn\nsen_citizen = input (\"Is this customer Over 65 Years Old? (Yes/No)\")\npartner = input (\"Is this customer Maried? (Yes/No)\")\ndepend=input(\"Does this customer have childreen? (Yes/No)\")\ntenure= int(input(\"How many months has the customer been with the company \"))# We use the 'int' command to turn the input into a whole number (integer)\nonline_security=input(\"Does the customer use the \"\"Online Security\"\" Service? \")\ntech_support=input(\"Does the customer use the \"\"Tech Support \"\" Service? (Yes/No)\")\npplerless_billing=input(\"Does the customer pay their bills online (Yes/No)\" )\nmonth_charges=float(input('How Much Does the Customer Pay Every Month? '))#We use the 'float' command to turn the input into a decimal number.\ntotal_charges= float(input('How much has the customer spent in their entire history with the company?'))\ninternet_type=input(\"What type of Internet Service does the customer have ?(Fiber optic/DSL/ No internet)\")\ncontract_type=input(\"How often does the customer pay?(Month-to-month/ Every year/ Every two years)\")\npayment_type=input('How does the customer pay? (E-check/Mailed check/Bank transfer/Credit card)')\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"id":"qTilDqy1gCpm","colab_type":"text","_uuid":"10356eff74f25f18b6c189b9b93804c8d8c64252"},"cell_type":"markdown","source":"*Feel free to enter above some made up variables just to test out the code*\n\nOnce we've received these inputs, we need to format them into numbers (just like we did in our **Data Cleaning** section) so that our algorithm that read them:"},{"metadata":{"id":"ZU0-Fi62hSVX","colab_type":"code","colab":{},"trusted":true,"_uuid":"9c4dfa9c7fc5b77d9eb6bf106b53dccd12c55290"},"cell_type":"code","source":"#Mean Normalization. First we need to reload our original data to get our original means and standard deviations.\n\noriginal_data=pd.read_excel(data_url)\n\ntenure_formatted= (tenure-original_data['Tenure'].mean())/original_data['Tenure'].std()\nmonth_charges_formatted= (month_charges-original_data['MonthlyCharges'].mean())/original_data['MonthlyCharges'].std()\ntotal_charges_formatted= (total_charges-original_data['TotalCharges'].mean())/original_data['TotalCharges'].std()\n\n\n#Next, following the same process, we carry out Label Encoding ( turning 'Yes' into 1s and 'No' into 0s)\nif sen_citizen=='Yes':\n  sen_citizen_formatted=1\nelse:\n  sen_citizen_formatted=0\n\nif partner=='Yes':\n  partner_formatted=1\nelse:\n  partner_formatted=0\n  \nif depend=='Yes':\n  depend_formatted=1\nelse:\n  depend_formatted=0\n\nif online_security=='Yes':\n  online_security_formatted=1\nelse:\n  online_security_formatted=0\n  \nif tech_support=='Yes':\n  tech_support_formatted=1\nelse:\n  tech_support_formatted=0\n  \nif pplerless_billing=='Yes':\n  pplerless_billing_formatted=1\nelse:\n  pplerless_billing_formatted=0\n\n#finally, we will do one hot encoding for the input with mutliple options. \n\n#create three columns, all at zero to begin with.\nfiber_optic_formatted=0\ndsl_formatted=0\nno_internet_formatted=0\n\n#According to the input received, one of the three variable will have to change to '1' correspondingly, while the other stay at zero.\n\nif internet_type=='Fiber optic':\n  fiber_optic_formatted=1\n  \nif internet_type=='DSL':\n  dsl_formatted=1\n  \nif internet_type=='No internet':\n  no_internet_formatted=1\n\n\n#We repeat with the other two multiple option inputs\n\n\nmonth_to_month_contract=0\none_year_contract=0\ntwo_year_contract=0\n\nif contract_type=='Month-to-month':\n  month_to_month_contract=1\n  \nif contract_type=='Every year':\n  one_year_contract=1\n  \n  \nif contract_type=='Every two years':\n  two_year_contract=1\n  \ne_check_formatted=0\nmail_check_formatted=0\nbank_transfer_formatted=0\ncredit_card_formatted=0\n\nif payment_type=='E-check':\n  e_check_formatted=1\n  \nif payment_type=='Mailed check':\n  mail_check_formatted=1\n\nif payment_type=='Bank transfer':\n  bank_transfer_formatted=1\n  \nif payment_type=='Credit card':\n  credit_card_formatted=1\n\n  \n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"id":"079Nl8dwpRTu","colab_type":"text","_uuid":"b02152d88094dd07e24041a618649e883a2008ac"},"cell_type":"markdown","source":"Now that we have formatted our new inputs into the same format as our training data, we need to list these variables in the same order as in the training data.\n\nEssentially, it is as if we are creating a new row to our dataset and then carrying out a prediction on it.\n\nTo make sure we are respecting the order of the columns of our original data, we can print out the order here below so that we are sure to follow it when creating our new row."},{"metadata":{"id":"M8A3XBaGptLb","colab_type":"code","colab":{},"trusted":true,"_uuid":"7962bf6867abceddd5ae362c4fc5f53d5dc4fd21"},"cell_type":"code","source":"x.columns\n\n","execution_count":null,"outputs":[]},{"metadata":{"id":"gBNsmDw_qKhc","colab_type":"code","colab":{},"trusted":true,"_uuid":"fa2c934fec79844ffa5772c83ef71ad7a47b1422"},"cell_type":"code","source":"#Creating a row containing all the new inputted data that follows the same order as in our training and testing datasets\n\nformatted_inputs_for_prediction= [[sen_citizen_formatted,partner_formatted,depend_formatted,tenure_formatted, online_security_formatted,tech_support_formatted,\n                                  pplerless_billing_formatted, month_charges_formatted, total_charges_formatted, fiber_optic_formatted,dsl_formatted,\n                                  no_internet_formatted,month_to_month_contract,one_year_contract,two_year_contract,e_check_formatted,mail_check_formatted,\n                                  bank_transfer_formatted, credit_card_formatted]]\n\n#Printing these formatted inputs to make sure they look good\nprint(formatted_inputs_for_prediction)","execution_count":null,"outputs":[]},{"metadata":{"id":"64P6gGrxs7yw","colab_type":"text","_uuid":"028f22443e60e275e5f883b2ceafd0823c64a085"},"cell_type":"markdown","source":"And now, with a simple line of code, we can output a prediction to inform our customer rep how to proceed:"},{"metadata":{"id":"S04n15gEtEfP","colab_type":"code","colab":{},"trusted":true,"_uuid":"b2f9761f3669e450c7c8d9beb1c0648300f2f882"},"cell_type":"code","source":"prediction = logistic_regression_opt.predict(formatted_inputs_for_prediction)\n\nprint(prediction)\n\nif prediction == 1:\n  print('This customer is likely to churn! Offer them a %15 discount on their next payment!')\nelse:\n  print('It is unlikely that this customer will churn.')","execution_count":null,"outputs":[]},{"metadata":{"id":"tVVfw7o0fWXR","colab_type":"text","_uuid":"6500c30c3258bc91a9de77e27c04d4cc135eccba"},"cell_type":"markdown","source":"It's important to update our model on a frequent basis, every quarter or even every month, to incorperate  changing consumer trends.\n\n---\n\n\n\n"},{"metadata":{"id":"woCnEz1muG6c","colab_type":"text","_uuid":"29546a85fb5a89670ab33276c9930f81585ce43c"},"cell_type":"markdown","source":"##Conclusion\n\nAnd that's the final part. Of course, once you have the model trained, there are lots of different contexts where you could apply it, the inputs for example could be taken automatically from the customer's profile from an SQL database (rather than entered by an employee)  or when given churn prediction, we could  allocated the customer to a corresponding CRM pipeline.\n\nBut for now, this concludes our tutorial. The aim for me here was to demystify Machine Learning and to demonstrate to non-technical folk how it is possible to obtain very concrete results with a straightforward methodology and an easy to use algorithm so that you can apply it within your company as of today. \n\nI really believe that these sort of statistical predictions can give you a great advantage with almost no effort,  no matter what your industry might be, and most of all it is accessible to everyone who possesses a keyboard and some information on their customers. Feel free to use all the code in this tutorial on your own data and I hope you will find find it easy and convenient to carry out your own \"Churn Predictor\" for your own customer and clients.\n\nIf you have any comments or queries, please get back to me at [conrad.w.s@gmail.com](mailto:conrad.w.s@gmail.com) and I'll try to get back at you as fast as I can.\n\nAnd let´s call it a day !\n "}],"metadata":{"colab":{"name":"Telco Churn Prediction.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":1}