{"cells":[{"metadata":{},"cell_type":"markdown","source":"# BBC News Headlines Classification Model"},{"metadata":{},"cell_type":"markdown","source":"We will use the BBC headline news text, labeled in 5 categories, i.e., 'Tech', 'Sports', 'Business', 'Entertainment', and 'Politics', and train our model with Logistic Regression and Naive Bayes.\n\nFinally we will try using some random out of the dataset headlines to test whether our model correctly classifies them into respective label class."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Importing Libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport sklearn","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Reading and Understanding"},{"metadata":{"trusted":true},"cell_type":"code","source":"bbc_text = pd.read_csv('../input/bbc-fulltext-and-category/bbc-text.csv')\nbbc_text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bbc_text.category.unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are a totla of 5 classes of news that we have here in our dataset. As our model would require it in numeric form, lets map it to numeric form."},{"metadata":{"trusted":true},"cell_type":"code","source":"bbc_text.category = bbc_text.category.map({'tech':0, 'business':1, 'sport':2, 'entertainment':3, 'politics':4})\nbbc_text.category.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bbc_text.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bbc_text.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Train Test Split"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# bbc_news = bbc_text.values\n\nX = bbc_text.text\ny = bbc_text.category\n\n#split\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.7, random_state = 1)\nprint(X_train)\nprint(y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Creating the Bag of Words Representation\n\nWe now have to convert the data into a format which can be used for training the model. We'll use the **bag of words representation** for each sentence (document).\n\nImagine breaking X in individual words and putting them all in a bag. Then we pick all the unique words from the bag one by one and make a dictionary of unique words. \n\nThis is called **vectorization of words**. We have the class ```CountVectorizer()``` in scikit learn to vectorize the words. \n\nWe will also use the `stop_words` in english to clear our data of stop words.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# countVectorizer\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nvec = CountVectorizer(stop_words = 'english')","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# fit the vectorizer on the training data\n\nvec.fit(X_train)\nprint(len(vec.get_feature_names()))\nvec.vocabulary_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# another way of representing the features\nX_transformed = vec.transform(X_train)\nX_transformed","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"print(X_transformed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_transformed.toarray()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# convert X_transformed to sparse matrix, just for readability.\npd.DataFrame(X_transformed.toarray(), columns= [vec.get_feature_names()])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We dont use sparse matrix while model building as it unnecessarily creates a dimensionality expansion, where other than a single position all postion carry zero value"},{"metadata":{"trusted":true},"cell_type":"code","source":"# for test data\nX_test_transformed = vec.transform(X_test)\nX_test_transformed","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"print(X_test_transformed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# convert X_transformed to sparse matrix, just for readability\npd.DataFrame(X_test_transformed.toarray(), columns= [vec.get_feature_names()])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Building the model"},{"metadata":{},"cell_type":"markdown","source":"### Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nlogit = LogisticRegression()\nlogit.fit(X_transformed, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fit\nlogit.fit(X_transformed,y_train)\n\n# predict class\ny_pred_class = logit.predict(X_test_transformed)\n\n# predict probabilities\ny_pred_proba = logit.predict_proba(X_test_transformed)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model Evaluation Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"# printing the overall accuracy\nfrom sklearn import metrics\nmetrics.accuracy_score(y_test, y_pred_class)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"confusion = metrics.confusion_matrix(y_test, y_pred_class)\nprint(confusion)\nTN = confusion[0, 0]\nFP = confusion[0, 1]\nFN = confusion[1, 0]\nTP = confusion[1, 1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sensitivity = TP / float(FN + TP)\nprint(\"sensitivity\",sensitivity)\n\nspecificity = TN / float(TN + FP)\nprint(\"specificity\",specificity)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"PRECISION SCORE :\",metrics.precision_score(y_test, y_pred_class, average = 'micro'))\nprint(\"RECALL SCORE :\", metrics.recall_score(y_test, y_pred_class, average = 'micro'))\nprint(\"F1 SCORE :\",metrics.f1_score(y_test, y_pred_class, average = 'micro'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets now build a Naive Bayes model, and see if we get any better results"},{"metadata":{},"cell_type":"markdown","source":"### Naive Bayes"},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\n\nnb = MultinomialNB()\nnb.fit(X_transformed, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fit\nnb.fit(X_transformed,y_train)\n\n# predict class\ny_pred_class = nb.predict(X_test_transformed)\n\n# predict probabilities\ny_pred_proba = nb.predict_proba(X_test_transformed)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model Evaluation Naive Bayes"},{"metadata":{"trusted":true},"cell_type":"code","source":"# printing the overall accuracy\nfrom sklearn import metrics\nmetrics.accuracy_score(y_test, y_pred_class)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# confusion matrix\nmetrics.confusion_matrix(y_test, y_pred_class)\n# help(metrics.confusion_matrix)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"confusion = metrics.confusion_matrix(y_test, y_pred_class)\nprint(confusion)\nTN = confusion[0, 0]\nFP = confusion[0, 1]\nFN = confusion[1, 0]\nTP = confusion[1, 1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sensitivity = TP / float(FN + TP)\nprint(\"sensitivity\",sensitivity)\n\nspecificity = TN / float(TN + FP)\nprint(\"specificity\",specificity)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"PRECISION SCORE :\",metrics.precision_score(y_test, y_pred_class, average = 'micro'))\nprint(\"RECALL SCORE :\", metrics.recall_score(y_test, y_pred_class, average = 'micro'))\nprint(\"F1 SCORE :\",metrics.f1_score(y_test, y_pred_class, average = 'micro'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Both the Logistic Regression as well as the Naive Bayes model offer similar performance. We will go ahead choosing Naive bayes as our final model"},{"metadata":{},"cell_type":"markdown","source":"------------------------------------------------------------------------------------------------------------"},{"metadata":{},"cell_type":"markdown","source":"## Test from random data outside the dataset"},{"metadata":{},"cell_type":"markdown","source":"   >Lets choose random news headlines from the internet, and see if our model perform well in classifying them"},{"metadata":{"trusted":true},"cell_type":"code","source":"s1 = ['FIR against Delhi Minorities Commission chairman for inflammatory content on social media']\nvec1 = vec.transform(s1).toarray()\nprint('Headline:' ,s1)\nprint(str(list(nb.predict(vec1))[0]).replace('0', 'TECH').replace('1', 'BUSINESS').replace('2', 'SPORTS').replace('3','ENTERTAINMENT').replace('4','POLITICS'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"relabel = {'0': 'tech', '1': 'business', '2': 'sport', '3': 'entertainment', '4': 'politics'}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"s2 = ['Need to restart economy but with caution: Yogi Adityanath at E-Agenda AajTak']\nvec2 = vec.transform(s2).toarray()\nprint('Headline:' ,s2)\nprint(str(list(nb.predict(vec2))[0]).replace('0', 'TECH').replace('1', 'BUSINESS').replace('2', 'SPORTS').replace('3','ENTERTAINMENT').replace('4','POLITICS'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"s3 = ['2 doctors attacked in Andhra Pradesh Vijayawada']\nvec3 = vec.transform(s3).toarray()\nprint('Headline:', s3)\nprint(str(list(nb.predict(vec3))[0]).replace('0', 'TECH').replace('1', 'BUSINESS').replace('2', 'SPORTS').replace('3','ENTERTAINMENT').replace('4','POLITICS'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"s4 = ['If I bat for an hour, you’ll see a big one: How Dravid spelt doom for Pak']\nvec4 = vec.transform(s4).toarray()\nprint('Headline:', s4)\nprint(str(list(nb.predict(vec4))[0]).replace('0', 'TECH').replace('1', 'BUSINESS').replace('2', 'SPORTS').replace('3','ENTERTAINMENT').replace('4','POLITICS'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Our Naive Bayes model is performing pretty well on random News Healines out of the dataset !"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}