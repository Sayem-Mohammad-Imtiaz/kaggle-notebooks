{"cells":[{"metadata":{"id":"026fo1AjnDBA"},"cell_type":"markdown","source":"### Goals of the Notebook are:\n\n1. Prepare EDA, understand main features interactions, prepare new features\n2. Try to estimate pm temperature base on measures of currents, voltages, speed, torque and coolant to understand possibility of operations\n3. Write clear supported Python code using sklearn Pipelines","execution_count":null},{"metadata":{"id":"vLXbrjFtnDBB","trusted":true},"cell_type":"code","source":"from __future__ import annotations\nfrom typing import List, Dict, Tuple, Callable, Iterable\n\nfrom enum import Enum\nfrom itertools import product","execution_count":null,"outputs":[]},{"metadata":{"id":"f7JvuczVHThj","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom pandas.core.groupby import SeriesGroupBy, DataFrameGroupBy\nimport matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"id":"C7jgmQ33nDBE","trusted":true},"cell_type":"code","source":"from sklearn.linear_model import Ridge\nfrom sklearn.metrics import r2_score, make_scorer\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.feature_selection import SelectKBest, f_regression\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.base import TransformerMixin, BaseEstimator","execution_count":null,"outputs":[]},{"metadata":{"id":"ebzg4HrdnDBH","trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/electric-motor-temperature/pmsm_temperature_data.csv')","execution_count":null,"outputs":[]},{"metadata":{"id":"SLnkj_Vj29jf","outputId":"2da86d13-7f98-47d2-ef35-11fe6de90109","trusted":true},"cell_type":"code","source":"df.head(5)","execution_count":null,"outputs":[]},{"metadata":{"id":"FZAltzq-29XK","outputId":"6cdd927f-b2e5-46a0-e8a9-c95959cc27ca","trusted":true},"cell_type":"code","source":"df.tail(5)","execution_count":null,"outputs":[]},{"metadata":{"id":"bz-7neuWI8w4","outputId":"ac7222f9-ff15-4af3-a84b-5536bad2a88b","trusted":true},"cell_type":"code","source":"df[df.isna().any(axis=1)]","execution_count":null,"outputs":[]},{"metadata":{"id":"CurAeAW63G4E","trusted":true},"cell_type":"code","source":"df.dropna(inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"id":"4jxpUgzEnDBK","outputId":"5ec2178b-797e-4416-9cd5-9c018977a71a","trusted":true},"cell_type":"code","source":"df.describe().T","execution_count":null,"outputs":[]},{"metadata":{"id":"B4pOOHGyuSOC"},"cell_type":"markdown","source":"As we see with description, all features are scaled. 'profile_id' columns is just the id of running","execution_count":null},{"metadata":{"id":"SKawJbfNnDBM"},"cell_type":"markdown","source":"### Data exploration\nNote 1: Profile_id is corresponde to single experiment. Rows of dataframe represent samples and are sorted by time. Sample values is measured within 0.5 seconds (2 Hz)\n\nNote 2: Target features are tempretures (pm, stator_yoke, stator_tooth, stator_winding). Here I will try to estimate permanent magnet temperature (pm)\n\nQuestions to answer with DA are:\n\n1. What is duration of profiles?\n2. Plot time series charts and. Does it have anomalies?\n3. Are features have nonlinear time-dependent behavior?","execution_count":null},{"metadata":{"id":"grqD9zGEnDBM","trusted":true},"cell_type":"code","source":"MEASURES_IN_SECOND = 2\nSECONDS_IN_HOUR = 3600\n\nPLOT_STYLE = \"darkgrid\"\nPLOT_PRIMARY_COLOR = \"#333333\"\nPLOT_SECONDARY_COLOR = \"#cc9602\"\n\nTEMPERATURE_FEATURES = ['pm', 'stator_yoke', 'stator_tooth', 'stator_winding']\nTARGET_FEATURES = TEMPERATURE_FEATURES + [\"torque\"]","execution_count":null,"outputs":[]},{"metadata":{"id":"7ZCok9BsnDBO","trusted":true},"cell_type":"code","source":"def plot_profile_durations(df: pd.DataFrame) -> None:\n    '''\n    - get DataFrame of dataset\n    - groupes it by 'profile_id' column\n    - calculate durations for each profile_id \n    - plot barplot of profile durations\n\n    '''\n\n    grouped_df = (df.groupby('profile_id')\n                    .size()\n                    .rename('samples')\n                    .reset_index())\n    \n    with sns.axes_style(PLOT_STYLE):\n        _, ax = plt.subplots(1, 1, figsize=(20,3))\n        sns.barplot(y='samples', x='profile_id', data=grouped_df, color=PLOT_PRIMARY_COLOR, ax=ax)\n        \n        hours_ticks: np.ndarray = np.arange(1, 8)\n        \n        ax.set_yticks(MEASURES_IN_SECOND*SECONDS_IN_HOUR*hours_ticks)\n        ax.set_yticklabels([f'{h}' for h in hours_ticks])\n        ax.set_ylim((0*MEASURES_IN_SECOND*SECONDS_IN_HOUR, 7*MEASURES_IN_SECOND*SECONDS_IN_HOUR))\n        ax.set_ylabel('duration, hours')        ","execution_count":null,"outputs":[]},{"metadata":{"id":"V5157bhdnDBQ","outputId":"b2a7f9b6-77aa-4e25-ead0-62093276e9fa","trusted":true},"cell_type":"code","source":"plot_profile_durations(df)","execution_count":null,"outputs":[]},{"metadata":{"id":"AMT9u1ScnDBT"},"cell_type":"markdown","source":"Here we see that some runnings were very short (about 20 minutes) and potentially they have not any interesting insights. Some of it were long and should have insights in data. Let's look at time series charts of target features","execution_count":null},{"metadata":{"id":"ay5dATKUnDBT","trusted":true},"cell_type":"code","source":"def plot_profile_time_series(df: pd.DataFrame, \n                             profile_id: int, \n                             num_of_points: int = None, \n                             features: List[str]=[]) -> None:\n    '''\n    - get:\n      * DataFrame of dataset, \n      * single profile_id, \n      * number of points to plot from the begining of profile, \n      * list of features to plot \n    - plot timeseries of profile (each feature at single axes)\n\n    '''\n\n    if num_of_points is None:\n        filtered_df=df[features][df.profile_id == profile_id].reset_index()\n    else:\n        filtered_df=df[features][df.profile_id == profile_id].reset_index()[0:num_of_points]\n\n    with sns.axes_style(PLOT_STYLE):\n        _, axes = plt.subplots(len(features), 1, figsize=(15, len(features)*2), sharex=True)\n        plt.xlabel(\"sample\")\n\n        for ax, feature_name in zip(axes, features):\n            ax.plot(filtered_df[feature_name], color=PLOT_PRIMARY_COLOR)\n            ax.set_ylabel(feature_name)","execution_count":null,"outputs":[]},{"metadata":{"id":"7RB2iOIA5GqU","trusted":true},"cell_type":"code","source":"def plot_profile_time_series_single_grid(df: pd.DataFrame, \n                                         profile_id: int, \n                                         num_of_points: int = None, \n                                         features: List[str]=[]) -> None:\n  \n    '''\n    - get:\n      * DataFrame of dataset, \n      * single profile_id, \n      * number of points to plot from the begining of profile, \n      * list of features to plot\n    - plot timeseries of profile (on the single axes)\n\n    '''\n\n    if num_of_points is None:\n        filtered_df=df[features][df.profile_id == profile_id].reset_index()\n    else:\n        filtered_df=df[features][df.profile_id == profile_id].reset_index()[0:num_of_points]\n\n    with sns.axes_style(PLOT_STYLE):\n        _, ax = plt.subplots(1, 1, figsize=(15, 2))\n        for idx, feature in enumerate(features):\n            if idx == 0:\n                ax.plot(filtered_df[feature], color=PLOT_PRIMARY_COLOR, label=feature)\n            elif idx == 1:\n                ax.plot(filtered_df[feature], color=PLOT_SECONDARY_COLOR, label=feature)\n            else:\n                ax.plot(filtered_df[feature], label=feature)\n        ax.set_xlabel('sample')\n        ax.legend()","execution_count":null,"outputs":[]},{"metadata":{"id":"1NhD8EmjnDBV","outputId":"7219ac68-c136-432e-e9fb-689286168c35","trusted":true},"cell_type":"code","source":"# example of short-time motor running\nplot_profile_time_series(df, 46, features = ['pm','coolant','ambient', 'torque', 'i_d'])","execution_count":null,"outputs":[]},{"metadata":{"id":"QHUfuLlJnDBX","outputId":"4c030fb3-9a34-424c-c8f7-c3fb590d9861","trusted":true},"cell_type":"code","source":"# example of long-time motor running\nplot_profile_time_series(df, 20, features=['pm','coolant','ambient', 'torque', 'i_d'])","execution_count":null,"outputs":[]},{"metadata":{"id":"wVxAPE3mV9Wf"},"cell_type":"markdown","source":"So, as we see short runnings of motor do not nave any insights about process.\n\nLong-term runnings have more interesting process changes. Temperature of stator and rotor exponentially grows/declines after step change of the parameters. But we do not saw any strict dependency like step up -> grows or step down -> decline (there are many opposite variants). \n\nLet's add basic stator parameters. We could know about Jouleâ€“Lenz law: Q ~ I**2, where Q is heat emits in the conductor and I is the current magnitude in it)\n\nAlso, ambient temperature values are noizy, though we need to eliminate that noise with smoothing","execution_count":null},{"metadata":{"id":"EEQEDyV5nDBZ","trusted":true},"cell_type":"code","source":"def calculate_magnitude(value_d: pd.Series, value_q: pd.Series) -> pd.Series:\n    '''\n    Returns magnitude of parameter represented by d- and -q axis values.\n    \n    '''\n    return np.sqrt(value_d**2 + value_q**2)\n\n\ndef calculate_apparent_power(current: pd.Series, voltage: pd.Series) -> pd.Series:\n    '''\n    Returns apparent power calculated with current and voltage values.\n    \n    '''\n    return current * voltage\n    \n\ndef calculate_active_power(current_d: pd.Series, \n                           current_q: pd.Series, \n                           voltage_d: pd.Series, \n                           voltage_q: pd.Series) -> pd.Series:\n\n    '''\n    Returns active power calculated with d- and -q axis values.\n    \n    '''\n    return current_d * voltage_d + current_q * voltage_q\n    \n\ndef calculate_reactive_power(current_d: pd.Series, \n                             current_q: pd.Series, \n                             voltage_d: pd.Series, \n                             voltage_q: pd.Series) -> pd.Series:\n    '''\n    Returns reactive power calculated with d- and -q axis values.\n    \n    '''\n    return current_d * voltage_q - current_q * voltage_d","execution_count":null,"outputs":[]},{"metadata":{"id":"kNEQHdm7nK2-","trusted":true},"cell_type":"code","source":"class StatorParametersTransformer(TransformerMixin):\n    '''\n    Custom sklearn feature Transfomer for adding stator parameters values\n    \n    '''\n\n    def __init__(self):\n        pass\n\n    def fit(self, X: pd.DataFrame, y: pd.Series = None) -> StatorParametersTransformer:\n        return self\n\n    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n        '''\n        Get input DataFrame and return new DataFrame with added stator parameters\n        \n        '''\n        return X.assign(**{\n          'current': lambda x: calculate_magnitude(x[\"i_d\"], x[\"i_q\"]),\n          'voltage': lambda x: calculate_magnitude(x[\"u_d\"], x[\"u_q\"]),\n          'apparent_power': lambda x: calculate_apparent_power(x[\"current\"], x[\"voltage\"]),\n          'active_power': lambda x: calculate_active_power(x[\"i_d\"], x[\"i_q\"], x[\"u_d\"], x[\"u_q\"]),\n          'reactive_power': lambda x: calculate_reactive_power(x[\"i_d\"], x[\"i_q\"], x[\"u_d\"], x[\"u_q\"]),\n        })","execution_count":null,"outputs":[]},{"metadata":{"id":"MhcpvkWfFmd7","trusted":true},"cell_type":"code","source":"df_with_stator_parameters = StatorParametersTransformer().fit_transform(df)","execution_count":null,"outputs":[]},{"metadata":{"id":"Y5dxseDcc7Aw","outputId":"a5264c2e-7b1c-4e85-cf63-9e3a917d740c","trusted":true},"cell_type":"code","source":"plot_profile_time_series(df_with_stator_parameters, 46, features=['pm','current'])","execution_count":null,"outputs":[]},{"metadata":{"id":"Mk2X9-4nnDBe","outputId":"d02f4d97-2f93-4964-9800-e82c06d07fbd","trusted":true},"cell_type":"code","source":"plot_profile_time_series(df_with_stator_parameters, 20, features=['pm','current'])","execution_count":null,"outputs":[]},{"metadata":{"id":"Vf9m26Zfij7s"},"cell_type":"markdown","source":"Now it is clear that steps in current magnitude produce exponential changes in temperatures","execution_count":null},{"metadata":{"id":"aI0XhB3hnDBo"},"cell_type":"markdown","source":"### Smoothed features\n\nAs we saw above, we need to know history of other parameters to estimate temperature. So we could generate exponentially smoothed features. More precisely, I will use Exponentially Weighted Moving Average (EWMA)","execution_count":null},{"metadata":{"id":"XPeCPrHpnDB0","trusted":true},"cell_type":"code","source":"def groupby_and_extract(column: str, feature: str) -> Callable:\n    '''\n    Decorator function that get single column and single feature names\n    and returns actual decorator that apply groupby column operation to input DataFrame\n    and select feature column operation\n\n    Usage:\n    @groupby_and_extract(\"profile_id\", feature_name)\n        def calculate_ewma(series: SeriesGroupBy):\n          ...\n    '''\n    def decorator(func: Callable) -> Callable:\n        def wrapper(df: pd.DataFrame) -> DataFrameGroupBy:\n            return func(df.groupby(\"profile_id\")[feature])\n        return wrapper\n    return decorator\n  \n\ndef define_ewma_calculation_func(feature_name: str, span: int) -> Callable:\n    '''\n    Get feature name and span\n    Returns function that will be called to calculate EWMA of feature \n    with span in assing method of DataFrame\n\n    '''\n\n    # TODO: Fine and clear, but group by for each new feature wired and not efficient. \n    # Need refactoring for multicolumn EWMA.\n    @groupby_and_extract(\"profile_id\", feature_name)\n    def calculate_ewma(series: SeriesGroupBy) -> pd.Series:\n        transformed_series: pd.Series = series.transform(lambda x: x.ewm(span, min_periods=1).mean())\n        return transformed_series.reset_index(drop=True)\n      \n    return calculate_ewma\n\n\ndef define_ewma_features(features: List[str], spans: List[int]) -> Dict[str, Callable]:\n    '''\n    Get feature names and spans\n    Returns dictionary with new column names as keys and functions \n    to generate EWMA features as items\n\n    '''\n    result: Dict[str, Callable] = dict()\n\n    for feature, span in product(features, spans):\n        feature_name: str = f\"{feature}_ewma_{span}\"\n        calcualtion_func: Callable = define_ewma_calculation_func(feature, span)\n        result[feature_name] = calcualtion_func\n\n    return result","execution_count":null,"outputs":[]},{"metadata":{"id":"twWmyFJ9GUJv","trusted":true},"cell_type":"code","source":"class EwmaTransformer(BaseEstimator, TransformerMixin):\n    '''\n    Custom sklearn feature Transfomer for adding EWMA features\n    \n    '''\n    def __init__(self, \n                 columns: List[str] = [], \n                 spans: List[int] = [], \n                 drop_transformed: bool = True):\n      \n        self.columns = columns\n        self.spans = spans\n        self.drop_transformed = drop_transformed\n\n    def set_params(self, **params):\n        self.columns = params[\"columns\"]\n        self.spans = params[\"spans\"]\n        self.drop_transformed = params[\"drop_transformed\"]\n\n    def fit(self, X: pd.DataFrame, y: pd.Series = None) -> EwmaTransformer:\n        return self\n\n    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n        '''\n        Get input DataFrame and return new DataFrame with added EWMA features\n        \n        '''\n        ewma_features: Dict[str, Callable] = define_ewma_features(self.columns, self.spans)\n        return X.assign(**ewma_features).drop(self.columns if self.drop_transformed else [], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"id":"B5iOGKcBjEML","trusted":true},"cell_type":"code","source":"class Debug(TransformerMixin):\n    '''\n    Custom sklearn feature Transfomer for debug Pipline steps\n    \n    '''\n    def __init__(self, title: str):\n      self.title = title\n\n    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n        print(self.title)\n        print(f\"Total shape: {X.shape}\")\n        print(f\"NaN shape{X[X.isna().any(axis=1)].shape}\\n\\n\")\n        return X\n\n    def fit(self, X: pd.DataFrame, y: pd.Series = None, **fit_params) -> Debug:\n        return self","execution_count":null,"outputs":[]},{"metadata":{"id":"V7OqHfrGHM8k"},"cell_type":"markdown","source":"Let's combine all our code in pipelines:\n\n- Step 1 : generate new features before grid search, because GridSearchCV make split on data, so ewma will not be correctly calculated\n\n- Step 2: do some feature selection with KBest\n\n- Step 3: evaluate grid search with Ridge regression","execution_count":null},{"metadata":{"id":"7zx9yW2DHK0j","trusted":true},"cell_type":"code","source":"ewma_spans: List[int] = [1300, 3000, 5000]\newma_columns: List[str] = ['ambient', 'coolant', 'u_d', 'u_q', \n                            'motor_speed', 'i_d','i_q', 'current', 'voltage', \n                            'apparent_power', 'active_power', 'reactive_power']\n\nfeature_tarnsformation_pipeline = make_pipeline(\n    StatorParametersTransformer(),\n    EwmaTransformer(ewma_columns, ewma_spans, drop_transformed=False),\n)\n\nnew_features_df = feature_tarnsformation_pipeline.fit_transform(df)","execution_count":null,"outputs":[]},{"metadata":{"id":"IT5IHXAo-DSN","outputId":"6afa9ec3-219c-4c8d-df97-404b748e69d2","trusted":true},"cell_type":"code","source":"plot_profile_time_series_single_grid(new_features_df.reset_index(), 20, features=['current', 'current_ewma_1300'])","execution_count":null,"outputs":[]},{"metadata":{"id":"6FrRiycFLoOZ","outputId":"c17b41cc-bdc4-49da-afe0-07674f7c6a3a","trusted":true},"cell_type":"code","source":"plot_profile_time_series_single_grid(new_features_df.reset_index(), 20, features=['pm', 'current_ewma_1300'])","execution_count":null,"outputs":[]},{"metadata":{"id":"SEPAmNANizQX","outputId":"d17e601e-837d-4870-e244-b92ea8a247db","trusted":true},"cell_type":"code","source":"plot_profile_time_series_single_grid(new_features_df.reset_index(), 20, features=['ambient', 'ambient_ewma_1300'])","execution_count":null,"outputs":[]},{"metadata":{"id":"hkiUsdGGhMxD","trusted":true},"cell_type":"code","source":"# Note that profile_id is needed for tests and evaluation, \n# so we will need to deal with it on the modelling steps\n# maybe I should solved with moving this column to index\n\nX = new_features_df.drop(columns=TARGET_FEATURES + [\"profile_id\"])\ny = new_features_df[\"pm\"]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"id":"GBzVJWBNSoqh","outputId":"b1a29792-7bf1-4957-cbe6-9806336809c3","trusted":true},"cell_type":"code","source":"# Use SelectKBest out of feature_tarnsformation_pipeline because of DataFrame interface\n# We can change k and see how it influence on results\n# K='all' is for all features selected\n\nselector = SelectKBest(f_regression, k='all')\nselector.fit_transform(X_train, y_train)\nmask = selector.get_support()\nselected_features = X_train.columns[mask]\n\nX_train_filtered = X_train[selected_features]\nX_test_filtered = X_test[selected_features]\n\nselected_features","execution_count":null,"outputs":[]},{"metadata":{"id":"P0XkNy80G2Ya","outputId":"222e064f-2980-4ba3-b522-05a332644cd0","trusted":true},"cell_type":"code","source":"model_pipline = Pipeline([\n  (\"regressor\", Ridge())\n])\n\nparams_grid = {\n    \"regressor__alpha\": np.logspace(-8, 3, num=12, base=10),\n    \"regressor__fit_intercept\": [True, False],\n}\n\nmodel = GridSearchCV(model_pipline, \n                     params_grid, \n                     scoring=make_scorer(r2_score), \n                     n_jobs=-1,\n                     cv=10,\n                     verbose=1,\n                     refit=True,\n                     return_train_score=True\n                     )\n\nmodel.fit(X_train_filtered, y_train)\n\ncv_results_df = pd.DataFrame(model.cv_results_)\ncv_results_df[[\"param_regressor__alpha\", \"param_regressor__fit_intercept\", \n               \"mean_train_score\",\"std_train_score\",\n               \"mean_test_score\",\"std_test_score\", \n               \"rank_test_score\"]]","execution_count":null,"outputs":[]},{"metadata":{"id":"xPGPpdem3zET","outputId":"ce0d3d04-8dfd-4701-e447-086f2c4e8f75","trusted":true},"cell_type":"code","source":"model.best_estimator_.named_steps['regressor'].coef_","execution_count":null,"outputs":[]},{"metadata":{"id":"O_3_4C80XTFv"},"cell_type":"markdown","source":"Let's prepare helpers to eveluate our experiments with models","execution_count":null},{"metadata":{"id":"KLGIZCHW3XSc","trusted":true},"cell_type":"code","source":"def evaluate_test(estimator: BaseEstimator, \n                  score: Callable, \n                  X_test: pd.DataFrame, \n                  y_test: pd.Series) -> float:\n    '''\n    Returns specified score for estimator and input data\n\n    '''\n\n    y_pred = estimator.predict(X_test)\n    return score(y_test, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"id":"GG3tL1wHoShx","trusted":true},"cell_type":"code","source":"def score_on_profiles(df: pd.DataFrame, \n                      estimator: BaseEstimator, \n                      score: Callable, \n                      selected_features: Iterable, \n                      profile_ids: List[str]=None) -> List[Tuple]:\n    '''\n    Returns specified scores for estimator and input data\n    Use selected_features for indexing data for concrete estimator\n\n    '''\n    profile_ids_to_score = df.profile_id.unique() if profile_ids is None else profile_ids\n    \n    result = list()\n    for profile_id in profile_ids_to_score:\n        df_profile = df[df.profile_id == profile_id]\n        profile_score = evaluate_test(estimator, score, df_profile[selected_features], df_profile[\"pm\"])\n        result.append((int(profile_id), profile_score))\n\n    return result","execution_count":null,"outputs":[]},{"metadata":{"id":"Tjd7oNQxoS12","trusted":true},"cell_type":"code","source":"# df should have profile_id\ndef plot_fitted_values(df: pd.DataFrame, \n                       estimator: BaseEstimator,\n                       selected_features: Iterable, \n                       profile_ids: List[str]=None) -> None:\n\n    '''\n    Plot true and fitted values for estimator and input data for specified profile_id\n    Use selected_features for indexing data for concrete estimator\n\n    '''\n    profile_ids_to_plot = df.profile_id.unique() if profile_ids is None else profile_ids\n\n    with sns.axes_style(PLOT_STYLE):\n        _, axes = plt.subplots(len(profile_ids_to_plot), \n                              1, \n                              figsize=(15, len(profile_ids_to_plot)*2))\n        plt.xlabel(\"sample\")\n\n        for ax, profile_id in zip(axes, profile_ids_to_plot):\n            df_profile = df[df.profile_id == profile_id]\n            y_pred = estimator.predict(df_profile[selected_features])\n            y_true = df_profile[\"pm\"].values\n\n            ax.plot(y_true, color=PLOT_PRIMARY_COLOR, label=\"true\")\n            ax.plot(y_pred, color=PLOT_SECONDARY_COLOR, label=\"prediction\")\n            ax.legend()\n            ax.set_ylabel(f\"pm: pid {profile_id}\")","execution_count":null,"outputs":[]},{"metadata":{"id":"sqjWRFX0qKld","outputId":"6ef18b26-5f5c-41a0-ab9e-f7219edf6d7d","trusted":true},"cell_type":"code","source":"score = evaluate_test(model.best_estimator_, r2_score, X_test_filtered, y_test)\nf\"Evaluation on test dataset result: {score: 0.3f}\"","execution_count":null,"outputs":[]},{"metadata":{"id":"7z3DNyGos7pT","outputId":"c33d7eea-2401-4ccf-e145-dd1de395e185","trusted":true},"cell_type":"code","source":"scores = score_on_profiles(new_features_df, model.best_estimator_, r2_score, selected_features)\nscores_df = pd.DataFrame(scores, columns=[\"profile_id\", \"score\"]).sort_values(by=['score'], ascending=False).reset_index(drop=True)\nscores_df","execution_count":null,"outputs":[]},{"metadata":{"id":"zW2eBQ_GLFZW","outputId":"ec7df381-61d3-4a95-cecb-edf5dfc62093","trusted":true},"cell_type":"code","source":"# good fittings:\nbest_5_fitted_profiles_id = scores_df.loc[0:5, \"profile_id\"].values\nplot_fitted_values(new_features_df, model.best_estimator_, selected_features, best_5_fitted_profiles_id)","execution_count":null,"outputs":[]},{"metadata":{"id":"3M85WyFCvBJi","outputId":"1c729b01-523b-4aee-fea5-1cc89da78cdb","trusted":true},"cell_type":"code","source":"# worst fittings:\nworst_5_fitted_profiles_id = scores_df.iloc[-5:][\"profile_id\"].values\nplot_fitted_values(new_features_df, model.best_estimator_, selected_features, worst_5_fitted_profiles_id)","execution_count":null,"outputs":[]},{"metadata":{"id":"5Q0AyJt5Rs2n"},"cell_type":"markdown","source":"### Concluton on Ridge\n\n1. PM temperature predicted very well for long-time running.\n2. Due to exponential smoothing of features, prediction became more accurate after some time since start of running\n3. Due to p.2, short runnings prediction is awful\n4. In long runnings we can see some weak predictons\n5. Error distributed not normally (we could see that without any plots)","execution_count":null},{"metadata":{"id":"bhId_d2bXNKr"},"cell_type":"markdown","source":"### Ways to improve\n\n1. Prepare more complex model\n2. Include PCA in pipeline to reduce correlation of features\n3. Change EWMA generation strategy for the beginning of the profile","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}