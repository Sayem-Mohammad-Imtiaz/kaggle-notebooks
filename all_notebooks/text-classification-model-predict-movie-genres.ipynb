{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import *\n\nimport tensorflow as tf\n\nfrom keras.utils import to_categorical\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\nfrom keras.layers import Input, Embedding\nfrom keras.models import Model\n\nnp.random.seed(456789)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Movie Genre Classification\n## Using Deep learning to predict the genre of a movie based on it's plot"},{"metadata":{"trusted":true},"cell_type":"code","source":"movie_data = pd.read_csv(\"../input/wikipedia-movie-plots/wiki_movie_plots_deduped.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"movie_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"movie_data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"count_by_genre = movie_data['Genre'].groupby(movie_data['Genre']) \\\n                             .count() \\\n                             .reset_index(name='count') \\\n                             .sort_values(['count'], ascending=False) \\\n                             .head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"count_by_genre","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"genres = ['drama', 'comedy', 'horror', 'action', 'thriller',\n          'romance', 'western', 'crime', 'adventure', 'musical',\n          'crime drama', 'romantic comedy', 'science fiction', 'mystery', 'animation']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(genres)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"case = movie_data[\"Genre\"].isin(genres)\nmovie_data_selected = movie_data[case]\nmovie_data_selected.reset_index(inplace=True)\nmovie_data_selected.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"movie_data_selected = movie_data_selected.sort_values(\"Genre\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"movie_data_selected.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wordcounter = movie_data_selected['Plot'].apply(lambda x: x.count(' '))\nprint(\"Average number of words per plot: \", int(wordcounter.mean()))\nprint(\"Standard deviation of the words: \", int(wordcounter.std()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(wordcounter, bins='fd')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nltk.download('stopwords')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stopwords = stopwords.words('english')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Text cleaning:\n1. Convert everything to lowercase\n2. Remove (\\\\'s)\n3. Remove (\\r\\n)\n4. Remove the text inside parenthesis ()\n5. Remove punctuations and special characters\n6. Remove stopwords\n7. Remove short words"},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_text(text):\n    '''\n    Clean a string input and prepare it for next steps.\n    '''\n    text = text.lower()\n    # Find and clear all ('s)\n    pattern_s = re.compile(\"\\'s\")\n    text = re.sub(pattern_s, '', text)\n    # Find and clear all (\\r\\n)\n    pattern_rn = re.compile(\"\\\\r\\\\n\")\n    text = re.sub(pattern_rn, '', text)\n    # Find and remove all parentheses and their contents\n    pattern_parentheses = re.compile(\"\\(.*?\\)\")\n    text = re.sub(pattern_parentheses, '', text)\n    # Find and remove punctuation and special characters\n    pattern_punct = re.compile(r\"[^\\w\\s]\")\n    text = re.sub(pattern_punct, '', text)\n    # Broke into tokens and remove stopwords\n    tokens = [w for w in text.split() if not w in stopwords]\n    # Remove short words (under 3 characters) from the tokens\n    long_words = []\n    for token in tokens:\n        if len(token) >= 3:\n            long_words.append(token)\n    # Join the tokens back together\n    cleaned_text = (\" \".join(long_words)).strip()\n    return cleaned_text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Clean the plot text and add it to the dataframe\ncleaned_plot = []\nfor plot in movie_data_selected[\"Plot\"]:\n    cleaned_plot.append(clean_text(plot))","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"movie_data_selected[\"cleaned_plot\"] = cleaned_plot","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stemmer = PorterStemmer()\nmovie_data_selected[\"stemmed_plot\"] = movie_data_selected[\"cleaned_plot\"].str.split().apply(lambda x: ' '.join([stemmer.stem(w) for w in x]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"movie_data_selected.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"movie_data_selected.groupby(movie_data_selected[\"Genre\"]).size()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"80% of the data for training: {int(movie_data_selected.shape[0]*0.8)} samples\")\nprint(f\"10% for training and 10% for validation: {int(movie_data_selected.shape[0]*0.1)} samples each\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grouped_by_genre = movie_data_selected.groupby(movie_data_selected[\"Genre\"], group_keys=False)\n\ntrain_df = pd.DataFrame()\nval_df = pd.DataFrame()\ntest_df = pd.DataFrame()\n# Not exactly what I need, but the general idea is here\nfor g in genres:\n    train_range = int(grouped_by_genre.get_group(g).shape[0]*0.8)\n    val_range = int(grouped_by_genre.get_group(g).shape[0]*0.9)\n    train_df = train_df.append(grouped_by_genre.get_group(g).iloc[0:train_range, :])\n    val_df = val_df.append(grouped_by_genre.get_group(g).iloc[train_range:val_range, :])\n    test_df = test_df.append(grouped_by_genre.get_group(g).iloc[val_range:, :])\n# Combine in one dataframe\ncomb_df = pd.DataFrame()\ncomb_df = comb_df.append(train_df)\ncomb_df = comb_df.append(val_df)\ncomb_df = comb_df.append(test_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_df.shape)\nprint(val_df.shape)\nprint(test_df.shape)\nprint(comb_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Initalise tokenizer with the original data\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(list(comb_df[\"Plot\"]))\nsequences = tokenizer.texts_to_sequences(list(comb_df[\"Plot\"]))\nmax_len = np.max([len(sequence) for sequence in sequences])\nprint(\"Maximum length sequence is\", max_len)\nword_index = tokenizer.word_index\nprint(f\"{len(word_index)} unique tokens have been found.\")\ntoken_data = pad_sequences(sequences, maxlen=max_len, padding='post')\nprint(\"Shape of the token data tensor:\", token_data.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# With the cleaned data\ntokenizer_clean = Tokenizer()\ntokenizer_clean.fit_on_texts(list(comb_df[\"stemmed_plot\"]))\nsequences_clean = tokenizer_clean.texts_to_sequences(list(comb_df[\"stemmed_plot\"]))\nmax_len_clean = np.max([len(sequence) for sequence in sequences_clean])\nprint(\"Maximum length sequence is\", max_len_clean)\nword_index_clean = tokenizer_clean.word_index\nprint(f\"{len(word_index_clean)} unique tokens have been found.\")\ntoken_data_clean = pad_sequences(sequences_clean, maxlen=max_len_clean, padding='post')\nprint(\"Shape of the token data tensor:\", token_data_clean.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"token_data[1952]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It looks like the tokenizer with the cleaned data found a little more unique tokens than the other. Maybe for now we will use the one with the original data."},{"metadata":{"trusted":true},"cell_type":"code","source":"sanity_check_index = {v: k for k, v in tokenizer.word_index.items()}\nprint(sequences[100])\nprint(' '.join([sanity_check_index[word_index] for word_index in sequences[100]]))\nprint(token_data[100][0])\nprint(token_data[100][-1])\nprint(' '.join([sanity_check_index[word_index] for word_index in token_data[100] if word_index!=0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = token_data[0:train_range]\nval_data = token_data[train_range:val_range]\ntest_data = token_data[val_range:]\n\ntrain_labels = train_df[\"Genre\"]\nval_labels = val_df[\"Genre\"]\ntest_labels = test_df[\"Genre\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_labels.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"val_labels.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_labels.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_labels = pd.factorize(train_labels)\nval_labels = pd.factorize(val_labels)\ntest_labels = pd.factorize(test_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_labels = to_categorical(train_labels[0], num_classes=len(genres))\nval_labels = to_categorical(val_labels[0], num_classes=len(genres))\ntest_labels = to_categorical(test_labels[0], num_classes=len(genres))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_labels.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### GloVe"},{"metadata":{"trusted":true},"cell_type":"code","source":"embeddings = {}\nindex = 0\nwith open ('../input/glove-global-vectors-for-word-representation/glove.6B.100d.txt') as file:\n    for embedding_line in file:\n        line_split = embedding_line.split()\n        coefs = np.asarray(line_split[1:], dtype='float32')\n        embeddings[line_split[0]] = coefs\n        index += 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embeddings_matrix = np.zeros((len(word_index)+1, len(embeddings['a'])))\nfor word, i in word_index.items():\n    if word in embeddings:\n        embeddings_matrix[i] = embeddings[word]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Word #125', sanity_check_index[125])\nprint('Index of if', word_index['if'])\nprint('Embedding in embeddings list: ', embeddings['if'][:5])\nprint('Embedding in embeddings matrix: ', embeddings_matrix[125][:5])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_layer = Embedding(len(word_index)+1, \n                            len(embeddings['a']), \n                            weights=[embeddings_matrix], \n                            input_length=max_len, \n                            trainable=False)\nembedding_layer_without_GloVe = Embedding(len(word_index)+1, \n                                          len(embeddings['a']), \n                                          weights=[embeddings_matrix], \n                                          input_length=max_len)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check the layer\nsequence_input = Input(shape=(max_len,), dtype='int32')\nembedded_sequences = embedding_layer(sequence_input)\nembedding_only_Model = Model(sequence_input, embedded_sequences)\n\nprint('Manual Embeddings Result: ', [list(embeddings[sanity_check_index[x]][:3]) if sanity_check_index[x] in embeddings else [0, 0, 0] for x in sequences[500]][-5:])\nprint()\nprint('Model Embeddings Result: ', embedding_only_Model.predict(np.array(token_data[500]).reshape(1, max_len))[0, -5:, :3])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"token_data[500]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"TODO:\n1. Sanity check of the tokenizer - DONE\n2. Train-test-split - DONE\n3. GloVe emeddings\n4. LSTM-CNN model\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}