{"cells":[{"metadata":{},"cell_type":"markdown","source":"# <h1 style='font-family:cursive;color:red'><center>Sentiment Classification on Movie Reviews</center></h1>"},{"metadata":{},"cell_type":"markdown","source":"![](https://static.amazon.jobs/teams/53/images/IMDb_Header_Page.jpg?1501027252)"},{"metadata":{},"cell_type":"markdown","source":"<h3 style='color:green'>What is Sentiment Analysis?</h3>\n<b style='color:blue'>Sentiment analysis is a natural language processing technique used to determine whether data is positive, negative or neutral. Sentiment analysis is often performed on textual data to help businesses monitor brand and product sentiment in customer feedback, and understand customer needs.</b>"},{"metadata":{},"cell_type":"markdown","source":"<h3 style='color:orange'>How we will do sentiment analysis?</h3>\n<b>While tackling with Text Data it is very important that the data is in correct format. For ex. we will remove emojis,contraction, mixed word,wrongly spelled words,punctuation and other unwanted stuff as they can decrease the model accuracy.</b><br>\n<b>We will use a technique called tokenization to transform the data into numerical form so as to use it to train the deep learning model.</b><br>\n<b>We would be using Glove embeddings to make the data in correct format.You can find the dataset for the glove embedding at <a href='https://www.kaggle.com/authman/pickled-glove840b300d-for-10sec-loading'>Glove Embeddings</a>"},{"metadata":{},"cell_type":"markdown","source":"# Importing Libraries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# we will be using various libraries like os for taking the input,etc\n# I have used some libraries like seaborn, wordcloud, matplotlib for data visualization so\n# you can skip them if you don't understand \n\nimport os\nimport pandas as pd\nimport numpy as np\nimport gc\nimport matplotlib.pyplot as plt\nimport operator\nimport seaborn as sns\nfrom wordcloud import WordCloud,STOPWORDS\n\n# re is used for cleaning the dataset \n\nimport re\n\n\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom tensorflow import keras\n\n# callbacks are important here as sometimes you get the best accuracy earlies and then it \n# goes down so as to stop the training there you need to use them\n\n\nfrom keras.callbacks import EarlyStopping,ModelCheckpoint\nfrom keras.utils import to_categorical\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Embedding,Conv1D,LSTM,GRU,BatchNormalization,Flatten,Dense\n\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"df= pd.read_csv('../input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b>Let's find if the data contains any missing value</b>"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b>We will find the count of each type of sentiment in the dataset using seaborn library</b>"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x=df['sentiment'])\nplt.grid()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentences=df['review']\nle=LabelEncoder()\ndf['sentiment']= le.fit_transform(df['sentiment'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b>Data visualization using word cloud for finding the most used words for each type of sentiment</b>"},{"metadata":{"trusted":true},"cell_type":"code","source":"stopwords = set(STOPWORDS) \n\npos=' '.join(map(str,sentences[df['sentiment']==1]))\nneg=' '.join(map(str,sentences[df['sentiment']==0]))\n  \nwordcloud1 = WordCloud(width = 800, height = 800, \n                background_color ='black', \n                stopwords = stopwords, \n                min_font_size = 10).generate(pos) \n\nplt.figure(figsize=(8,8))\nplt.imshow(wordcloud1)\nplt.title('Positive Sentiment')\nplt.axis('off')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,8))\nwordcloud2 = WordCloud(width = 800, height = 800, \n                background_color ='black', \n                stopwords = stopwords, \n                min_font_size = 10).generate(neg) \n\nplt.imshow(wordcloud2)\nplt.title('Negative Sentiment')\nplt.axis('off')\n\nplt.show() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels=to_categorical(df['sentiment'],num_classes=2)\nX_train,X_test,Y_train,Y_test = train_test_split(df['review'],labels,test_size=0.1,random_state=10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Cleaning"},{"metadata":{},"cell_type":"markdown","source":"<h4 style='color:blue'><span style='color:red'>Note: </span>In this model I will be using glove embeddings.It has a large vocabulary and we can find the words from our data which are not present in the glove( these words are contractions, misspelled words, concated words or emojis which can decrease our model's performance. We will then use re library to remove these words from the dataset.</h4>"},{"metadata":{"trusted":true},"cell_type":"code","source":"glove_embeddings= np.load('../input/pickled-glove840b300d-for-10sec-loading/glove.840B.300d.pkl',\n                          allow_pickle=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b>We will build vocabulary and count of each vocabulary using the below function</b>"},{"metadata":{"trusted":true},"cell_type":"code","source":"def vocab_build(review):\n    \n    comments = review.apply(lambda s: s.split()).values\n    vocab={}\n    \n    for comment in comments:\n        for word in comment:\n            try:\n                vocab[word]+=1\n                \n            except KeyError:\n                vocab[word]=1\n    return vocab","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b>Embedding Coverage tells how much percentage of the words in our data are covered by the vocabulary.<br>\n<i>sorted_oov</i> is the list of words which we need to do text cleaning on. </b>"},{"metadata":{"trusted":true},"cell_type":"code","source":"def embedding_coverage(review,embeddings):\n    \n    vocab=vocab_build(review)\n    \n    covered={}\n    word_count={}\n    oov={}\n    covered_num=0\n    oov_num=0\n    \n    for word in vocab:\n        try:\n            covered[word]=embeddings[word]\n            covered_num+=vocab[word]\n            word_count[word]=vocab[word]\n        except:\n            oov[word]=vocab[word]\n            oov_num+=oov[word]\n    \n    vocab_coverage=len(covered)/len(vocab)*100\n    text_coverage = covered_num/(covered_num+oov_num)*100\n    \n    sorted_oov=sorted(oov.items(), key=operator.itemgetter(1))[::-1]\n    sorted_word_count=sorted(word_count.items(), key=operator.itemgetter(1))[::-1]\n    \n    return sorted_word_count,sorted_oov,vocab_coverage,text_coverage\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_covered,train_oov,train_vocab_coverage,train_text_coverage=embedding_coverage(X_train,glove_embeddings)\ntest_covered,test_oov, test_vocab_coverage, test_text_coverage = embedding_coverage(X_test,glove_embeddings)\n\nprint(f\"Glove embeddings cover {round(train_vocab_coverage,2)}% of vocabulary and {round(train_text_coverage,2)}% text in training set\")\nprint(f\"Glove embeddings cover {round(test_vocab_coverage,2)}% of vocabulary and {round(test_text_coverage,2)}% text in testing set\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b>train_oov shows the words which we need to preprocess</b>"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_oov[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_sentences(line):\n    \n    line=re.sub('<.*?>','',line) # removing html tags\n    \n    #removing contractions\n    line=re.sub(\"isn't\",'is not',line)\n    line=re.sub(\"he's\",'he is',line)\n    line=re.sub(\"wasn't\",'was not',line)\n    line=re.sub(\"there's\",'there is',line)\n    line=re.sub(\"couldn't\",'could not',line)\n    line=re.sub(\"won't\",'will not',line)\n    line=re.sub(\"they're\",'they are',line)\n    line=re.sub(\"she's\",'she is',line)\n    line=re.sub(\"There's\",'there is',line)\n    line=re.sub(\"wouldn't\",'would not',line)\n    line=re.sub(\"haven't\",'have not',line)\n    line=re.sub(\"That's\",'That is',line)\n    line=re.sub(\"you've\",'you have',line)\n    line=re.sub(\"He's\",'He is',line)\n    line=re.sub(\"what's\",'what is',line)\n    line=re.sub(\"weren't\",'were not',line)\n    line=re.sub(\"we're\",'we are',line)\n    line=re.sub(\"hasn't\",'has not',line)\n    line=re.sub(\"you'd\",'you would',line)\n    line=re.sub(\"shouldn't\",'should not',line)\n    line=re.sub(\"let's\",'let us',line)\n    line=re.sub(\"they've\",'they have',line)\n    line=re.sub(\"You'll\",'You will',line)\n    line=re.sub(\"i'm\",'i am',line)\n    line=re.sub(\"we've\",'we have',line)\n    line=re.sub(\"it's\",'it is',line)\n    line=re.sub(\"don't\",'do not',line)\n    line=re.sub(\"that´s\",'that is',line)\n    line=re.sub(\"I´m\",'I am',line)\n    line=re.sub(\"it’s\",'it is',line)\n    line=re.sub(\"she´s\",'she is',line)\n    line=re.sub(\"he’s'\",'he is',line)\n    line=re.sub('I’m','I am',line)\n    line=re.sub('I’d','I did',line)\n    line=re.sub(\"he’s'\",'he is',line)\n    line=re.sub('there’s','there is',line)\n    \n    #special characters and emojis\n    line=re.sub('\\x91The','The',line)\n    line=re.sub('\\x97','',line)\n    line=re.sub('\\x84The','The',line)\n    line=re.sub('\\uf0b7','',line)\n    line=re.sub('¡¨','',line)\n    line=re.sub('\\x95','',line)\n    line=re.sub('\\x8ei\\x9eek','',line)\n    line=re.sub('\\xad','',line)\n    line=re.sub('\\x84bubble','bubble',line)\n    \n    # remove concated words\n    line=re.sub('trivialBoring','trivial Boring',line)\n    line=re.sub('Justforkix','Just for kix',line)\n    line=re.sub('Nightbeast','Night beast',line)\n    line=re.sub('DEATHTRAP','Death Trap',line)\n    line=re.sub('CitizenX','Citizen X',line)\n    line=re.sub('10Rated','10 Rated',line)\n    line=re.sub('_The','_ The',line)\n    line=re.sub('1Sound','1 Sound',line)\n    line=re.sub('blahblahblahblahblahblahblahblahblahblahblahblahblahblahblahblahblahblah','blah blah',line)\n    line=re.sub('ResidentHazard','Resident Hazard',line)\n    line=re.sub('iameracing','i am racing',line)\n    line=re.sub('BLACKSNAKE','Black Snake',line)\n    line=re.sub('DEATHSTALKER','Death Stalker',line)\n    line=re.sub('_is_','is',line)\n    line=re.sub('10Fans','10 Fans',line)\n    line=re.sub('Yellowcoat','Yellow coat',line)\n    line=re.sub('Spiderbabe','Spider babe',line)\n    line=re.sub('Frightworld','Fright world',line)\n    \n    #removing punctuations\n    \n    punctuations = '@#!~?+&*[]-%._-:/£();$=><|{}^' + '''\"“´”'`'''\n    for p in punctuations:\n        line = line.replace(p, f' {p} ')\n        \n    line=re.sub(',',' , ',line)\n        \n    # ... and ..\n    line = line.replace('...', ' ... ')\n    \n    if '...' not in line:\n        line = line.replace('..', ' ... ')\n        \n    return line\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<span style='color:green'>After cleaning the dataset we can see that now our vocabulary covers almost 87% on training set and 95.5% on testing set which initially was far less.</span>"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train=X_train.apply(lambda s: clean_sentences(s))\nX_test=X_test.apply(lambda s: clean_sentences(s))\n\ntrain_covered,train_oov,train_vocab_coverage,train_text_coverage=embedding_coverage(X_train,glove_embeddings)\nprint(f\"Glove embeddings cover {round(train_vocab_coverage,2)}% of vocabulary and {round(train_text_coverage,2)}% text in training set\")\n\ntest_covered,test_oov,test_vocab_coverage,test_text_coverage=embedding_coverage(X_test,glove_embeddings)\nprint(f\"Glove embeddings cover {round(test_vocab_coverage,2)}% of vocabulary and {round(test_text_coverage,2)}% text in training set\")\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**using seaborn's barplot let's find out the count of 10 most used words in training and testing set**"},{"metadata":{"trusted":true},"cell_type":"code","source":"punctuations = '@#!~?+&*[]-%._-:/£();$=><|{},^' + '''\"“´”'`'''\ntrain_word=[]\ntrain_count=[]\n\ni=1\nfor word,count in train_covered: \n    if word not in punctuations:\n        train_word.append(word)\n        train_count.append(count)\n        i+=1\n    if(i==15):\n        break","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_word=[]\ntest_count=[]\n\ni=1\nfor word,count in test_covered: \n    if word not in punctuations:\n        test_word.append(word)\n        test_count.append(count)\n        i+=1\n    if(i==15):\n        break","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,8))\nsns.barplot(x=train_count,y=train_word).set_title('Count of 15 most used word in training set')\nplt.grid()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,8))\nsns.barplot(x=test_count,y=test_word).set_title('Count of 15 most used word in testing set')\nplt.grid()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**We will delete the embeddings as it takes too much memory**"},{"metadata":{"trusted":true},"cell_type":"code","source":"del glove_embeddings,train_oov,test_oov\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Building"},{"metadata":{"trusted":true},"cell_type":"code","source":"num_words=80000\nembeddings=256","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer=Tokenizer(num_words=num_words,oov_token='<OOV>')\ntokenizer.fit_on_texts(X_train)\nword_index=tokenizer.word_index\ntotal_vocab=len(word_index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Vocabulary of the dataset is : \",total_vocab)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sequences_train=tokenizer.texts_to_sequences(X_train)\nsequences_test=tokenizer.texts_to_sequences(X_test)\n\nmax_len=max(max([len(x) for x in sequences_train]),max([len(x) for x in sequences_test]))\n\ntrain_padded=pad_sequences(sequences_train,maxlen=max_len)\ntest_padded=pad_sequences(sequences_test,maxlen=max_len)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train,X_val,Y_train,Y_val=train_test_split(train_padded,Y_train,\n                                             test_size=0.05,random_state=10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**We will 2 LSTM layers and Conv1D layer for training the model.<br>\nUsing Dropout reduces the overfitting by decreasing the bias and is a must since there is lot of variance seen.**\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"model= keras.Sequential()\nmodel.add(Embedding(num_words,embeddings,input_length=max_len))\nmodel.add(Conv1D(256,10,activation='relu'))\nmodel.add(keras.layers.Bidirectional(LSTM(128,return_sequences=True)))\nmodel.add(LSTM(64))\nmodel.add(keras.layers.Dropout(0.4))\nmodel.add(Dense(2,activation='softmax'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(loss='binary_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy']\n             )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Callbacks are really helpful as they stop our model when the validation accuracy of our model starts decreasing for consecutive 2 epochs as well save the best possible weights which gives highest validation accuracy**"},{"metadata":{"trusted":true},"cell_type":"code","source":"es= EarlyStopping(monitor='val_accuracy',\n                  patience=2\n                 )\n\ncheckpoints=ModelCheckpoint(filepath='./',\n                            monitor=\"val_accuracy\",\n                            verbose=0,\n                            save_best_only=True\n                           )\n\ncallbacks=[es,checkpoints]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history=model.fit(X_train,Y_train,validation_data=(X_val,Y_val),epochs=5,callbacks=callbacks)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Evaluation"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_graph(history,string):\n    \n    plt.plot(history.history[string],label='training '+string)\n    plt.plot(history.history['val_'+string],label='validation '+string)\n    plt.legend()\n    plt.xlabel('epochs')\n    plt.ylabel(string)\n    plt.title(string+' vs epochs')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_graph(history,'loss')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_graph(history,'accuracy')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save('imdb_model.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Model Performance on test set\")\nresult = model.evaluate(test_padded,Y_test)\nprint(dict(zip(model.metrics_names, result)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3 style='color:red'>I hope you Liked my kernel. An upvote is a gesture of appreciation that will help me to create more kernels and keep me motivated ,be kind to show one ;-)</h3>"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}