{"cells":[{"metadata":{},"cell_type":"markdown","source":"### Supress Warnings and import all the relevant packages and libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Supress Warnings\n\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import the necessary packages and libraries\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\npd.set_option('display.max_columns', 50)\n\nfrom sklearn.model_selection import train_test_split\n\nimport statsmodels.api as sm\n\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nplt.style.use('ggplot')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Reading and preparing the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Read the data\n\ninsurance = pd.read_csv('../input/insurance-premium-prediction/insurance.csv')\ninsurance.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the shape of the dataframe\n\ninsurance.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So there are 1338 records and 7 features."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the info for all the columns\n\ninsurance.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"Comments:-\nIt is observed from the above, that the entire dataset does not contain any missing values."},{"metadata":{},"cell_type":"markdown","source":"### Identifying the categorical and continuous variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"insurance.nunique().sort_values()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"Categorical Variables - sex, smoker, region, children\n\nThe rest of the variables are continuous in nature."},{"metadata":{},"cell_type":"markdown","source":"### Visualising the numeric variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Pairplot of all the numeric variables\n\nsns.pairplot(insurance, vars=['age','bmi','expenses'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"No such clear trend of correlation."},{"metadata":{},"cell_type":"markdown","source":"### Visualising the categorical variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Boxplot for some of the categorical variables with respect to the target varibale 'expenses'\n\nplt.figure(figsize=(20, 12))\nplt.subplot(2,4,1)\nsns.boxplot(x = 'sex', y = 'expenses', data = insurance)\nplt.subplot(2,4,2)\nsns.boxplot(x = 'children', y = 'expenses', data = insurance)\nplt.subplot(2,4,3)\nsns.boxplot(x = 'smoker', y = 'expenses', data = insurance)\nplt.subplot(2,4,4)\nsns.boxplot(x = 'region', y = 'expenses', data = insurance)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"Observations:-\n\n1) Expenses are more for a male individual.\n2) Expenses are more for those having 2 or 3 children.\n3) Expenses are more for the smokers.\n4) Expnenses are more those individuals who belong to the south-east region, and least who belong to the south-west region."},{"metadata":{},"cell_type":"markdown","source":"### Analysis between the target variable - expenses, and the other variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Analysis between sex and expenses\n\nplt.figure(figsize=(10,4))\nsns.barplot('sex','expenses',data=insurance)\nplt.title('Expenses among the genders',fontsize=12)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"Observation:-\nExpenses for the men are comparatively more.   "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Analysis between children and expenses\n\nplt.figure(figsize=(10,4))\nsns.barplot('children','expenses',data=insurance)\nplt.title('Expenses with respect to the number of children',fontsize=12)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"Observation:-\nIndividuals, with 2 or 3 children, are having the most expenses, while those with 5 children, are having the least expenses."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Analysis between smoker and expenses\n\nplt.figure(figsize=(10,4))\nsns.barplot('smoker','expenses',data=insurance)\nplt.title('Expenses with respect to smoker',fontsize=12)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"Observation:-\n\nExpenses for the smokers are much higher than the non-smokers."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Analysis between region and expenses\n\nplt.figure(figsize=(10,4))\nsns.barplot('region','expenses',data=insurance)\nplt.title('Expenses with respect to region',fontsize=12)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Observation:-\nExpenses are highest for those individuals who belong to the south-east region."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Analysis of expenses with age\n\nsns.scatterplot(x='age',y='expenses' ,data=insurance)\nplt.title('Expenses vs Age')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"Observation:-\nNo such clear trend."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Analysis of expenses with bmi\n\nsns.scatterplot(x='bmi',y='expenses' ,data=insurance)\nplt.title('BMI vs Expenses')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"Observation:-\nMuch higher expenses are observed for those, whose bmi is between 30 and 40."},{"metadata":{},"cell_type":"markdown","source":"### Correlation between the variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Heatmap to visualise the correlation between the variables\n\nplt.figure(figsize=(10, 5))\nsns.heatmap(insurance.corr(), cmap=\"YlGnBu\", annot = True)\nplt.title(\"Correlation between the variables\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"Observation:-\n'age' is most correlated with the target variable 'expenses'."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Mapping the variable 'children' for better analysis\n\ninsurance['children'] = insurance.children.map({0:'No Children',1:'One Child',2:'Two Children',3:'Three Children',4:'Four Children',5:'Five Children'})\ninsurance.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Creating Dummy Variables"},{"metadata":{},"cell_type":"markdown","source":"Creating dummy variables for - children, region, sex and smoker"},{"metadata":{"trusted":true},"cell_type":"code","source":"children_dummy = pd.get_dummies(insurance.children,drop_first=True)\nregion_dummy = pd.get_dummies(insurance.region,drop_first=True)\nsex_dummy = pd.get_dummies(insurance.sex,drop_first=True)\nsmoker_dummy = pd.get_dummies(insurance.smoker,prefix='smoker',drop_first=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Adding the dummy variables to the original dataframe\n\ninsurance = pd.concat([insurance,children_dummy,region_dummy,sex_dummy,smoker_dummy],axis=1)\ninsurance.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dropping the original columns - children, region, sex and smoker, since dummy variables have already been created for them\n\ninsurance.drop(['children','region','sex','smoker'], axis = 1, inplace = True)\ninsurance.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Splitting the Data into Training and Testing Sets"},{"metadata":{"trusted":true},"cell_type":"code","source":"np.random.seed(0)\n\ninsurance_train, insurance_test = train_test_split(insurance, train_size = 0.7, random_state = 100)\n\nprint(insurance_train.shape)\nprint(insurance_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Rescaling the Features using MinMax Scaling"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating a list of numeric variables\n\nnum_vars=['age','bmi','expenses']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fit the data\n\ninsurance_train[num_vars] = scaler.fit_transform(insurance_train[num_vars])\ninsurance_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking numeric variables after scaling the features\ninsurance_train.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Dividing into X and y sets for the model building"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = insurance_train.pop('expenses')\nX_train = insurance_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"col = X_train.columns\ncol","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Building our model\n\nWe will be using the statsmodel to build our model, initially with all the features, and keep on removing them manually, based on p-values and VIF."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating the linear model\n\nlm = LinearRegression()\nlm.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model 1"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Adding a constant variable \n  \nX_train_1 = X_train[col]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_1 = sm.add_constant(X_train_1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Running the linear model\n\nlm = sm.OLS(y_train,X_train_1).fit()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Summary of our linear model\nprint(lm.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dropping the const variable\n\nX_train_1_ = X_train_1.drop(['const'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculating the VIFs for the new model\n\nvif = pd.DataFrame()\nX = X_train_1_\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will be following the below rule to drop the variables one by one, as per the priorities mentioned by their sequences:-\n\n* We will first check the summary and VIF\n* If a variable has got high p-value(>0.05) as well as high VIF(>5), we need to drop that first\n* If a variable has got high p-value(>0.05) but low VIF(<5), then we need to drop such\n* Still if we have a variable with low p-value(<0.05) but high VIF(>5), we need to drop such at the very end"},{"metadata":{},"cell_type":"raw","source":"In Model 1, 'male' has got high p-value but low VIF, but 'bmi' has got low p-value but high VIF.\n\nSince, 'male' is insignificant with respect to the other variables, hence this is dropped, and not 'bmi'."},{"metadata":{},"cell_type":"markdown","source":"### Model 2"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Rebuilding the model without 'male'\n\nX_train_2 = X_train_1_.drop(['male'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Adding the contsant variable\n\nX_train_2 = sm.add_constant(X_train_2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Running the linear model\n\nlm = sm.OLS(y_train,X_train_2).fit() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Summary of the new model\nprint(lm.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dropping the const variable\n\nX_train_2_ = X_train_2.drop(['const'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculating the VIFs for the new model\n\nvif = pd.DataFrame()\nX = X_train_2_\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"In Model 2, 'Two Children' is the m ost insignificant variable, and hence this should be dropped now."},{"metadata":{},"cell_type":"markdown","source":"### Model 3"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Rebuilding the model without 'Two Children'\n\nX_train_3 = X_train_2_.drop(['Two Children'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Adding the contsant variable\n\nX_train_3 = sm.add_constant(X_train_3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Running the linear model\n\nlm = sm.OLS(y_train,X_train_3).fit()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Summary of the new model\nprint(lm.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dropping the const variable\n\nX_train_3_ = X_train_3.drop(['const'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculating the VIFs for the new model\n\nvif = pd.DataFrame()\nX = X_train_3_\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"In Model 3, 'Four Children' is insignificant. Hence, this should be dropped now."},{"metadata":{},"cell_type":"markdown","source":"### Model 4"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Rebuilding the model without 'Four Children'\n\nX_train_4 = X_train_3_.drop(['Four Children'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Adding the contsant variable\n\nX_train_4 = sm.add_constant(X_train_4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Running the linear model\n\nlm = sm.OLS(y_train,X_train_4).fit()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Summary of the new model\nprint(lm.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dropping the const variable\n\nX_train_4_ = X_train_4.drop(['const'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculating the VIFs for the new model\n\nvif = pd.DataFrame()\nX = X_train_4_\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"In Model 4, 'Three Children' has got low VIF but high p-value, hence this variable should be dropped now."},{"metadata":{},"cell_type":"markdown","source":"### Model 5"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Rebuilding the model without 'Three Children'\n\nX_train_5 = X_train_4_.drop(['Three Children'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Adding the contsant variable\n\nX_train_5 = sm.add_constant(X_train_5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Running the linear model\n\nlm = sm.OLS(y_train,X_train_5).fit()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Summary of the new model\nprint(lm.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dropping the const variable\n\nX_train_5_ = X_train_5.drop(['const'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculating the VIFs for the new model\n\nvif = pd.DataFrame()\nX = X_train_5_\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"In Model 5, 'northwest' is the least significant, so dropping that."},{"metadata":{},"cell_type":"markdown","source":"### Model 6"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Rebuilding the model without 'northwest'\n\nX_train_6 = X_train_5_.drop(['northwest'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Adding the contsant variable\n\nX_train_6 = sm.add_constant(X_train_6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Running the linear model\n\nlm = sm.OLS(y_train,X_train_6).fit()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Summary of the new model\nprint(lm.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dropping the const variable\n\nX_train_6_ = X_train_6.drop(['const'], axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculating the VIFs for the new model\n\nvif = pd.DataFrame()\nX = X_train_6_\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"In Model 6, 'southeast' is has the highest p-value."},{"metadata":{},"cell_type":"markdown","source":"### Model 7"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Rebuilding the model without 'southeast'\n\nX_train_7 = X_train_6_.drop(['southeast'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Adding the contsant variable\n\nX_train_7 = sm.add_constant(X_train_7)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Running the linear model\n\nlm = sm.OLS(y_train,X_train_7).fit()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Summary of the new model\nprint(lm.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dropping the const variable\n\nX_train_7_ = X_train_7.drop(['const'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculating the VIFs for the new model\n\nvif = pd.DataFrame()\nX = X_train_7_\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"Model 7 seems to be the best model achieved so far, with the p-values of all the 6 dependent variables are less than 0.05, and the VIF values are less than 4.\nHence, this model can be finalised, for making the predictions."},{"metadata":{},"cell_type":"markdown","source":"### Residual Analysis of the train data\n\nSo, now to check if the error terms are also normally distributed (which is infact, one of the major assumptions of linear regression), let us plot the histogram of the error terms and see what it looks like."},{"metadata":{"trusted":true},"cell_type":"code","source":"lm = sm.OLS(y_train,X_train_7).fit()  #As obtained previously\ny_train_count = lm.predict(X_train_7)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot the histogram of the error terms\n\nfig = plt.figure()\nsns.distplot((y_train - y_train_count), bins = 20)\nfig.suptitle('Error Terms', fontsize = 20)                  # Plot heading \nplt.xlabel('Errors', fontsize = 18)                         # X-label\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"It is observed that the error terms are more-or-less normally distributed."},{"metadata":{},"cell_type":"markdown","source":"### Applying the scaling on the test sets"},{"metadata":{"trusted":true},"cell_type":"code","source":"num_vars=['age','bmi','expenses']\n\n# Fit and transform operations are done on the training data but only transform operation will be done on the test data\n\ninsurance_test[num_vars] = scaler.transform(insurance_test[num_vars])\ninsurance_test.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Dividing into X_test and y_test"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test = insurance_test.pop('expenses')\nX_test = insurance_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating X_test_m7 dataframe with the final Model 7 in the training dataset\n\nX_test_m7 = sm.add_constant(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test_m7.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test_m7 = X_test_m7.drop(['Four Children','Three Children','Two Children','northwest','southeast','male'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Making predictions using the seventh model\n\ny_pred_m7 = lm.predict(X_test_m7)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model Evaluation"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting y_test and y_pred to understand the spread\n\nfig = plt.figure()\nplt.scatter(y_test, y_pred_m7)\nfig.suptitle('y_test vs y_pred', fontsize = 20)              # Plot heading \nplt.xlabel('y_test', fontsize = 18)                          # X-label\nplt.ylabel('y_pred', fontsize = 16)      \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Regression plot\n\nsns.regplot(x = y_test, y = y_pred_m7, fit_reg=True,scatter_kws={\"color\": \"blue\"}, line_kws={\"color\": \"red\"})\n\nplt.title('y_test vs y_pred', fontsize=20)              # Plot heading \nplt.xlabel('y_test', fontsize=18)                          # X-label\nplt.ylabel('y_pred', fontsize=16)                          # Y-label\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Calculation of R-square and Adjusted R-square values"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Evaluate R-square for test dataset\n\nfrom sklearn.metrics import r2_score\nr2_score(y_test,y_pred_m7)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Adjusted R^2\n# adj r2 = 1-((1-R2)*(n-1)/(n-p-1))\n\n# n = sample size (in this case the value is 220, as yielded before)\n# p = number of independent variables(in this case the value is 9)\n\nAdj_r2 = 1 - ((1 - 0.7733792659357421) * 401 / (402-6-1))\nprint(Adj_r2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"737For the training dataset, the R^2 value was 0.738 and adjusted R^2 value was 0.737.\n\nFor the testing dataset, the R^2 value obtained is 0.773 and adjusted R^2 value obtained is 0.769.\n\n\n\n\nHence the equation of our best fitted line is:-\n\n$ expenses = 0.1912 \\times age + 0.1644 \\times bmi - 0.0214 \\times No   Children -0.0278 \\times One   Child - 0.0160 \\times southwest + 0.3817 \\times smoker_   yes $\n\n\nOverall we have a decent model, but we also acknowledge that we could do better. "},{"metadata":{},"cell_type":"markdown","source":"### Interpretations\n\n* We have arrived at a very decent model for the the demand for shared bikes with the significant variables.\n\n* We can see that smoking_yes variable is having the highest coefficient of 0.3817, which means if the smoking_yes increases by one unit, the expense increases by 0.3817 units.\n\n* The other significant variables having positive coefficients are age and bmi.\n\n* There are some variables with negative coefficients too, like No Children, One Child and southwest. A negative coefficient suggests that, as the independent variable increases, the dependent variable tends to decrease, and vice-versa."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}