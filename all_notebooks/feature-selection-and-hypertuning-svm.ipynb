{"cells":[{"metadata":{"_uuid":"b5f020b979d9a38fde94631d02a782c2276c4a01"},"cell_type":"markdown","source":"# Libraries Imported"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Required Libraries are imported\nimport warnings  \nimport pandas as pd\nimport numpy as np\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn.metrics import accuracy_score\nfrom sklearn import preprocessing\nfrom sklearn.linear_model import LinearRegression,LogisticRegression\nimport matplotlib.pyplot as plt\nfrom matplotlib.axes import Axes\nimport seaborn as sns\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.feature_selection import RFE\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.decomposition import PCA\nfrom collections import Counter\nfrom sklearn.metrics import confusion_matrix\nimport time\nimport scikitplot as skplt\nimport itertools\nfrom sklearn.model_selection import GridSearchCV\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"550323139df11dd821dd31d528a550af98756e51"},"cell_type":"markdown","source":"# Data Exploration\n### 1.   Read Train and Test Dataset\n### 2. Checked for null values found none "},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"_kg_hide-input":false,"_kg_hide-output":false},"cell_type":"code","source":"# Reading the Dataset\ndf_train=pd.read_csv(\"../input/train.csv\")\ndf_test=pd.read_csv(\"../input/test.csv\")\n\n# Check null values\ndf_train.isnull().values.any()\ndf_test.isnull().values.any()\n# No null values in train and test data\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b7557291b5c0f8329aba465299e950b63823b4f5"},"cell_type":"markdown","source":"## Checking top 5 rows just to get an idea of data we are dealing with"},{"metadata":{"trusted":true,"_uuid":"e6268d4447206d19c3772a9029153bab8c92b0c8"},"cell_type":"code","source":"# Top 5 rows\ndf_train.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"48d60de914243c1812e81ac07ce5b50dad74d8c6"},"cell_type":"markdown","source":"# Subject column removed why?\n###  Based on the Domain knowledge- knowing that subject column does not affect the prediction variable it is removed"},{"metadata":{"trusted":true,"_uuid":"821a609b47914e6410c7d900b3ef61bfe827ab0d"},"cell_type":"code","source":"# Subject col not usefull hence dropped\nif('subject' in df_train.columns):\n    df_train.drop('subject', axis =1, inplace=True)\nif('subject' in df_test.columns):\n    df_test.drop('subject', axis =1, inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8b945bc25af6dd18951699288be2126bdec001cd"},"cell_type":"markdown","source":"# Variable Encoding for classes "},{"metadata":{"trusted":true,"_uuid":"11632803dcf63da8039236fcf9328ff068cacc42"},"cell_type":"code","source":"# Encoding target - converting non-num to num variable\nle = preprocessing.LabelEncoder()\nfor x in [df_train,df_test]:\n    x['Activity'] = le.fit_transform(x.Activity)\n\n# Split into features and class\ndf_traindata, df_trainlabel = df_train.iloc[:, 0:len(df_train.columns) - 1], df_train.iloc[:, -1]\ndf_testdata, df_testlabel = df_test.iloc[:, 0:len(df_test.columns) -1], df_test.iloc[:, -1]\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0292a4ef973776cf63608f5831b283d045f69fff"},"cell_type":"markdown","source":"# Baseline\n\n### Considering all the 563 features, modelling is done with some best classifying algorithms along with comparison of test and train accuracy. "},{"metadata":{"trusted":true,"_uuid":"6163573a8fe0cf380fc2515eece82504b8977525"},"cell_type":"code","source":"warnings.filterwarnings('ignore')\n# Baseline - comparing model accuracy using all features across classifiers \nclassifiers = [\n    DecisionTreeClassifier(),\n    RandomForestClassifier(),\n    KNeighborsClassifier(),\n    SVC(kernel=\"linear\"),\n    GaussianNB(),\n    LogisticRegression()\n    ]\n\n\n# Naive Train Accuracy\nalgo = []\nscores = []\nfor clf in classifiers:\n    algo.append(clf.__class__.__name__)\n    scores.append(cross_val_score(clf,df_traindata,df_trainlabel, cv=5).mean())\nwarnings.filterwarnings('ignore')\nNaivescore_df_Train = pd.DataFrame({'Algorithm': algo, 'Score': scores}).set_index('Algorithm')\n\n\n# Naive Test Accuracy\n\nalgo = []\nscores = []\n\nfor clf in classifiers:\n    clf = clf.fit(df_traindata, df_trainlabel)\n    y_pred = clf.predict(df_testdata)\n    algo.append(clf.__class__.__name__)\n    scores.append(accuracy_score(y_pred, df_testlabel))\nwarnings.filterwarnings('ignore')\nNaivescore_df_Test  = pd.DataFrame({'Algorithm': algo, 'Score': scores}).set_index('Algorithm')\n\n# Bar plot between Train and Test Accuracy\nfig = plt.figure(figsize=(5,5)) # Create matplotlib figure\n\nax = fig.add_subplot(111) # Create matplotlib axes\nax2 = ax.twinx() # Create another axes that shares the same x-axis as a\nwidth = .3\n\nNaivescore_df_Train.Score.plot(kind='bar',color='green',ax=ax,width=width, position=0)\nNaivescore_df_Test.Score.plot(kind='bar',color='red', ax=ax2,width = width,position=1)\n\nax.grid(None, axis=1)\nax2.grid(None)\n\nax.set_ylabel('Train')\nax2.set_ylabel('Test')\n\nax.set_xlim(-1,7)\nplt.show()\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"078b1ee286d879cbe7bc05a4c149cf0ee5e42b83"},"cell_type":"markdown","source":"### Using Naive approch we can clearly see that SVM performs better in train/test across all the other classifiers hence SVM classifier  is selected for further tuning."},{"metadata":{"_uuid":"4150fa7a2151414dba98af907922fdf8028e42d9"},"cell_type":"markdown","source":"# Feature Selection \n## Tree Based Feature Selection\n\n### Tree based estimators are used to compute feature importance which is visualized by their ranking. Further, from their obtained importanceâ€™s, the irrelevant features are discarded using select from model meta transformer. Random forest is chosen as the estimator with its high-performance post evaluation and comparison with extra tree classifier and SVM. Feature is reduced to 83 most important ones for prediction"},{"metadata":{"trusted":true,"_uuid":"848a81c651689a1f9e82a795a280b6fcec3ceeeb"},"cell_type":"code","source":"# Feature selection using Random Forest Classifier\n\n# Bagged decision trees for feature importance- embedded method\nRtree_clf = RandomForestClassifier()\nRtree_clf = Rtree_clf.fit(df_traindata,df_trainlabel)\nmodel = SelectFromModel(Rtree_clf, prefit=True)\nRF_tree_featuresTrain=df_traindata.loc[:, model.get_support()]\nRF_tree_featuresTest = df_testdata.loc[:, model.get_support()]\nwarnings.filterwarnings('ignore')\n\n# Based on Feature Selection only 87 features were selected\n\n# Feature Importance\n\n# Important scores\n# for name, importance in zip(df_traindata, Rtree_clf.feature_importances_):\n#     print(name, \"=\", importance)\n\nimportances = Rtree_clf.feature_importances_\nstd = np.std([tree.feature_importances_ for tree in Rtree_clf.estimators_],axis=0)\nindices = np.argsort(importances)[::-1]\nindices.shape\nindices = indices[:200]\n# Feature Ranking\n#print(\"Feature ranking:\")\n#for f in range(200):\n#    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n\n# Plots feature importances\n\nplt.figure(1, figsize=(14, 13))\nplt.title(\"Feature importances\")\nplt.xlabel(\"# of Features \")\nplt.ylabel(\"Importance Score\")\nplt.bar(range(200), importances[indices],color=\"r\", yerr=std[indices], align=\"center\")\nplt.xlim([0, 200])\nplt.show()\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"44354bf4167738358d56721a6b9a940c2ba112f4"},"cell_type":"markdown","source":"## Why was cross validation done in Training set??\n\n### Compare learning curve with training set and Cross validation training set"},{"metadata":{"trusted":true,"_uuid":"bb985dcf0d3b5b2ddbaf5ae1fb53d78dd07daafb"},"cell_type":"code","source":"\nskplt.estimators.plot_learning_curve(Rtree_clf,RF_tree_featuresTrain,df_trainlabel)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"02a72830ba008efd57fbb775ff2bcbd6aa382559"},"cell_type":"markdown","source":"### We can clearly see that the accuracy remains to 100% which creates a lot of overfitting hence cross validation is done to training to reduce overfitting"},{"metadata":{"_uuid":"5026794cc8cc5e98ef18b5f422e1ba4931d40f70"},"cell_type":"markdown","source":"## Recursive Feature Elmination"},{"metadata":{"_uuid":"6ac90e02a08a01c39df4c1eb0aee63eb7af0fbf5"},"cell_type":"markdown","source":"###  Firstly, feature raking is performed with recursive feature elimination and cross validation for automatic tuning selecting the best number of features. With the optimal number of features from RFECV, RFE is performed. SVM is used as an external estimator for assignment of weights to features. The weights obtained are treated as importance of the features. Further, RFE selects features by consideration of smaller sets of features each time. The least importance features are pruned from the current set of features. This procedure is recursively repeated on the pruned set until the given number of optimal features is reached."},{"metadata":{"trusted":true,"_uuid":"dccbd67910510e0387557b8fa7bff35afcaaea9f"},"cell_type":"code","source":"# Applying RFE Cross validation to find number of features\n# The \"accuracy\" scoring is proportional to the number of correct classifications\n\n# Before we apply RFE we need to know the optimal number of features. Hence RFECV crossvalidation technique is used to find \n# the optimal number of features based on the accuracy score in the training set. \n\n# Applying RFECV with svm classifier\nsvc=SVC(kernel=\"linear\")\nrfecv = RFECV(estimator=svc, step=1, cv=StratifiedKFold(2), # Stratified fold inorder to reduce bias\n              scoring='accuracy')\nrfetrain=rfecv.fit(RF_tree_featuresTrain, df_trainlabel)\nprint('Optimal number of features :', rfecv.n_features_)\n\n\n# Plot showing the Cross Validation score\nplt.figure()\nplt.xlabel(\"Number of features selected\")\nplt.ylabel(\"Cross validation score (nb of correct classifications)\")\nplt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\nplt.show()\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b39b01535970cb57a55f48f87dfd76b3ee19e708"},"cell_type":"markdown","source":"### Applying RFE with optimal number of features found in RFECV"},{"metadata":{"trusted":true,"_uuid":"5e25cdd3ed0241870216d5a5b5884395f9a54102"},"cell_type":"code","source":"\n# Applying RFE with optimal number of features\nrfe = RFE(estimator=svc, n_features_to_select=rfecv.n_features_, step=1)\nrfe = rfe.fit(RF_tree_featuresTrain, df_trainlabel)\n\nrfe_train=RF_tree_featuresTrain.loc[:, rfe.get_support()]\nrfe_test=RF_tree_featuresTest.loc[:, rfe.get_support()]\n\n\n# Checking the Accuracy after rfe\n# Train Accuracy\nprint(\"Train Accuracy:\",cross_val_score(svc,rfe_train,df_trainlabel, cv=5).mean())\n# Test Accuracy\nscv = svc.fit(rfe_train, df_trainlabel)\ny_pred = scv.predict(rfe_test)\nprint(\"Test Accuracy:\",accuracy_score(y_pred, df_testlabel))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"abfb9818697d094bf1ad16ee537b9ce8f9c8426a"},"cell_type":"markdown","source":"##  Applying Variance Threshold method to remove low variance variable\n\n### Finally, a simple approach to feature selection is also performed. Variance threshold method is used through which all features whose variance donâ€™t meet threshold of 0.95 are removed giving rise to 51 features."},{"metadata":{"trusted":true,"_uuid":"bae8f398e78bc0b277416e96984a7176fa3a55a6"},"cell_type":"code","source":"\n# Variance threshold\nselector = VarianceThreshold(0.95*(1-.95))\nvarsel=selector.fit(rfe_train)\nrfe_train.loc[:, varsel.get_support()].shape\n# 55\nvartrain=rfe_train.loc[:, varsel.get_support()]\nvartest=rfe_test.loc[:, varsel.get_support()]\n\n# Checking the Accuracy after Variance threshold\n# Train Accuracy\nprint(\"Train Accuracy:\",cross_val_score(svc,vartrain,df_trainlabel, cv=5).mean())\n\n# Test Accuracy\nscv = svc.fit(vartrain, df_trainlabel)\ny_pred = scv.predict(vartest)\nprint(\"Test Accuracy:\",accuracy_score(y_pred, df_testlabel))\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d401d7c4369806b64041eca54bc50feac426d096"},"cell_type":"markdown","source":"# Feature Extraction using PCA (Principal Component Analysis)"},{"metadata":{"_uuid":"438efb57ed47dc8179b2466495ab88d8127cdb98"},"cell_type":"markdown","source":"## Finding the number of dimensions to transform using PCA\n\n### Running Principal Component analysis, it is found that the model information can be captured with 21 features as they seem to cover the maximum variance. Hence, the 21 new features in an entirely new dimension are chosen as the final set of features removing the other least discriminative ones. The chosen 21 features seem to have an explained variance of 99 % which can be clearly visualized in the graph below."},{"metadata":{"trusted":true,"_uuid":"3fc5628b6de613b141c588607a088cd29e48eaab"},"cell_type":"code","source":"# PCA\npca = PCA(n_components = len(vartrain.columns))\npca_traindata = pca.fit(vartrain)\n\npca_traindata.explained_variance_\npca_traindata.n_components_\npcatrain = pca_traindata.transform(vartrain)\npcatest = pca_traindata.transform(vartest)\ncum_ratio = (np.cumsum(pca_traindata.explained_variance_ratio_))\n\n\n# Visualize PCA result\nplt.plot(np.cumsum(pca_traindata.explained_variance_ratio_))\nplt.xlabel('number of components')\nplt.ylabel('cumulative explained variance')\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f1090b73bc1bdd063cb3667370987d75472a0aab"},"cell_type":"markdown","source":"## Applying PCA with number of components=21 "},{"metadata":{"trusted":true,"_uuid":"351bf3a188dcc8dc409290add0cd8d1e746a2fcc"},"cell_type":"code","source":"# 21 features - constant after that\npca = PCA(n_components = 21)\npca_traindata = pca.fit(vartrain)\n\npca_traindata.explained_variance_\npca_traindata.n_components_\npcatrain = pca_traindata.transform(vartrain)\npcatest = pca_traindata.transform(vartest)\n(np.cumsum(pca_traindata.explained_variance_ratio_))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2dd73caae5a46152bd2577b90bd41f82b0632933"},"cell_type":"markdown","source":"## Visualizing top 2 principal components in scatter plot with data points segregated based on their activities"},{"metadata":{"trusted":true,"_uuid":"e2a3641ac65cb89f722a338787cd75c1666e4989"},"cell_type":"code","source":"# PCA in 2D projection\n \nskplt.decomposition.plot_pca_2d_projection(pca, vartrain, df_trainlabel)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"010e172150db670b6fb5b20f3145a5f318a09bcc"},"cell_type":"markdown","source":"## Accuracy check on Test and Train to see if it has increased"},{"metadata":{"trusted":true,"_uuid":"f663546160743c0eceff6f935aea998e9863e5a9"},"cell_type":"code","source":"# Checking Accuracy after applying PCA\n\n# Train Accuracy\nprint(\"Train Accuracy:\",cross_val_score(svc,pcatrain,df_trainlabel, cv=5).mean())\n\n# Test Accuracy\nscv = svc.fit(pcatrain, df_trainlabel)\ny_pred = scv.predict(pcatest)\nac_score = accuracy_score(y_pred, df_testlabel)\nprint(\"Test Accuracy:\",accuracy_score(y_pred, df_testlabel))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c184eab6261fec1bef679d421aae4f16d58c0f73"},"cell_type":"markdown","source":"# Confusion Matrix after applying PCA\n\n### Confusion matrix above is presented for test data with the highest scoring feature subset and optimal parameters, where the rows correspond to the actual performed activities, while the columns correspond to the predicated activity labels. We can clearly see the number of false positives and false negatives are very low. It is intrepreted that there seem to be very less number of misclasified data."},{"metadata":{"trusted":true,"_uuid":"9922a234bd60bc67279929904bb7ee29271bcf3e"},"cell_type":"code","source":"# Confusion Matrix\n\ncf_mat = confusion_matrix(df_testlabel, y_pred)\nprint(\"Accuracy: %f\" %ac_score) \nactivities = le.classes_\n\n# Plotting Confusion matrix heatmap\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.tight_layout()\n\nplot_confusion_matrix(cf_mat, classes=activities,title=\"Confusion Matrix for Test data\")\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a9cc29999e863b11c291db356bc89c115bacd884"},"cell_type":"markdown","source":"# Hyper Parameter Tuning- finding the best parameters and kernel"},{"metadata":{"trusted":true,"_uuid":"87f810e36364cb4e30ba39fc8ad9f77dbb70046b"},"cell_type":"code","source":"# Parameter Tuning \n\n# Perfromance tuning using GridScore\nparam_grid = [\n  {'C': [1, 10, 100, 1000], 'gamma': [0.001, 0.0001], 'kernel': ['linear']},\n  {'C': [1, 10, 100, 1000], 'gamma': [0.001, 0.0001], 'kernel': ['rbf']},\n ]\nsvr = SVC()\nclf = GridSearchCV(svr, param_grid,cv=5)\nclf.fit(pcatrain,df_trainlabel)\nprint(clf.best_params_)\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"23b471ae52b6826432888c10fab0ea722a5ab1fb"},"cell_type":"markdown","source":"## Train Test Accuracy check with best params and features\n\n### Finally, post tuning of hyper- parameter, the accuracy is checked to see how well the chosen model performed with the selected optimal numer of features"},{"metadata":{"_uuid":"00c37b31549b915a0f22e7cbdf3b2e0340e520d6","trusted":true},"cell_type":"code","source":"\n# Train Accuracy \nsvr = SVC(kernel=\"rbf\",C=1000,gamma=0.001)\nprint(\"Train Accuracy:\",cross_val_score(svr,pcatrain,df_trainlabel, cv=5).mean())\n# Test Accuracy\nscv = svr.fit(pcatrain, df_trainlabel)\ny_pred = scv.predict(pcatest)\nprint(\"Test Accuracy:\",accuracy_score(y_pred, df_testlabel))\n ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"04aeba78391721019807f8769ff358903d8229d5"},"cell_type":"markdown","source":"### With test and train accuracy almost equal to 92%, we are getting a maximum accuracy at this level."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}