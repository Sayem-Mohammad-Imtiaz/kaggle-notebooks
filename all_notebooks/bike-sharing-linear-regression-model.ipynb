{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nimport statsmodels.api as sm\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Loading the Dataset and checking it's contents and checking the data quality","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/boombikes/day.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking it's Shape\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking if instant column has unique entries, if yes, then will convert it to index\ndf['instant'].nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Setting the instant column as index to number of columns\ndf.set_index('instant', inplace=True)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since the Model is to be built for <b>cnt</b> column, <b>casual</b> and <b>registed</b> are redundant here. It should not be used to build the model, thus dropping these two columns before further processing.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop(['casual', 'registered'], inplace=True, axis = 1)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking Columns Data Types\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Converting the dtedat to Date Time\ndf['dteday'] = pd.to_datetime(df['dteday'])\ndf['dteday'].dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Changing the month number to month abbr for better view\nimport calendar\ndf['mnth'] = df['mnth'].apply(lambda x: calendar.month_abbr[x])\ndf['mnth'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Since season, weekday and weathesit are basically categorical values, converting them to string type for future use\ndf[['season','weekday','weathersit']] = df[['season','weekday','weathersit']].astype(str)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the % of null values in each column\nround(df.isnull().sum()/len(df.index)*100,2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are no Null values!","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the range of values, for example temperature, humidity etc\ndf[['temp', 'atemp', 'hum', 'windspeed']].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Converting the weather variable into more understanable text\ndf['weathersit'].replace(['1','2','3','4'],['Good', 'Average', 'Bad', 'Very Bad'], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Converting the seasons into specific season names for better understanding\ndf['season'].replace(['1','2','3','4'],['spring', 'summer', 'fall', 'winter'], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Visualizing the data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking linear relationship between the cnt variable and other numeric variables\nx =sns.pairplot(df, palette='husl', x_vars=['temp', 'atemp', 'hum', 'windspeed'], y_vars=['cnt'] , hue='yr' )\nx._legend.remove()\nplt.legend(labels=['2018', '2019'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that there is some corelation between Feeling Temperature and sales. Also the count in 2019 are much higher than the count in 2018 for all circumstances.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the distribution of rentals across different categorical variables\nplt.figure(figsize=(15,10))\nplt.subplot(2,3,1)\nsns.boxplot(x='season', y='cnt', data=df, palette='husl')\nplt.subplot(2,3,2)\nsns.boxplot(x='yr', y='cnt', data=df, palette='husl')\nplt.subplot(2,3,3)\nsns.boxplot(x='mnth', y='cnt', data=df, palette='husl')\nplt.subplot(2,3,4)\nsns.boxplot(x='holiday', y='cnt', data=df, palette='husl')\nplt.subplot(2,3,5)\nsns.boxplot(x='weekday', y='cnt', data=df, palette='husl')\nplt.subplot(2,3,6)\nsns.boxplot(x='workingday', y='cnt', data=df, palette='husl')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seems that during the summer months, the registration count picks up.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x='weathersit', y='cnt', data=df, palette='husl')\nplt.xlabel('Weather')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Count picks up in Good Weather days","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking business on Holidays\nholiday_df = df.groupby(['holiday'])['cnt'].mean().reset_index()\nsns.barplot(x='holiday', y='cnt', data=holiday_df, palette='husl')\nplt.xticks(np.arange(2),('No','Yes'))\nplt.xlabel('Holiday')\nplt.ylabel('Average Number of Rentals')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Non Holidays have slight higher average rentals","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Total rentals on different days of the week.\nweekday_df = df.groupby(['weekday'])['cnt'].mean().reset_index()\nsns.barplot(x='weekday', y='cnt', data=weekday_df, palette='husl')\nplt.xticks(np.arange(7),('Mon','Tue','Wed','Thu', 'Fri', 'Sat', 'Sun'))\nplt.xlabel('Days of the Week')\nplt.ylabel('Average Number of Rentals')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Rentals are uniform throuout the week but there is a small uptrend as weekend appraches.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking business on Workingdays\nworkingday_df = df.groupby(['workingday'])['cnt'].mean().reset_index()\nsns.barplot(x='workingday', y='cnt', data=workingday_df, palette='husl')\nplt.xticks(np.arange(2),('No','Yes'))\nplt.xlabel('Working Day')\nplt.ylabel('Average Number of Rentals')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Creating Dummy Variables for Categorical Data\n#### We need to create dummy variables for the following columns.\n- season\n- mnth\n- weekday\n- weathersit","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dummy = pd.get_dummies(df[['season','mnth','weekday','weathersit']], drop_first=True)\ndummy.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.concat([df,dummy], axis=1)   #Axis=1 is for horizontal stacking\ndf = df.drop(['season','mnth','weekday','weathersit'], axis=1)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Shape of the new dataframe is:' , df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Since we have the month and the Year in two seperate columns, we do not need the date column anymore, thus dropping it\ndf.drop('dteday', inplace=True, axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Moving the cnt to the end for easier identification\nfirst_col = df.pop('cnt')\ndf['cnt'] = first_col","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Splitting the data into Train and Test Dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train, df_test = train_test_split(df, train_size=0.7, random_state=100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Shape of the Train data is:' , df_train.shape)\nprint('Shape of the Test data is:' , df_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the Train Data\npd.set_option('display.max_columns', None)\ndf_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Other than the numeric fields, all other categorical values have been encoded. Now we can go ahead and scale the data.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Feature Scaling","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# We do a MinMax scaling\nscaler = MinMaxScaler()    #Instantiating the object\ncols = df_train.columns\ndf_train[cols] = scaler.fit_transform(df_train[cols])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the Heatmap\nplt.figure(figsize=(24,15))\nsns.heatmap(df_train.corr(),annot=True, cmap='YlGnBu')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b>cnt</b> has strong colinearity with <b>yr</b>, <b>temp</b>, <b>atemp</b>. <br> But, <b>temp</b> and <b>atemp</b> have almost a perfect colinearity, so both cannot be part of the model. We keep this in mind while building the model in the following cells.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Building the Model\nSince the number of columns is 29, which is manageable, we first build a model with all the columns, and then keep removing the columns based upon Statistical Significance and Co-Linearity.<br>\nWe will stop when we notice that there is no further improvement in the R2 value or all variables are statistically significant with low VIF.\n\n<b>There wil be several iterations before getting the perfect model, so please bear with me!</b>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = df_train.pop('cnt')\nX_train = df_train\nX_train_sm = sm.add_constant(X_train)\nlr = sm.OLS(y_train, X_train_sm)\nlr_model1 = lr.fit()\nlr_model1.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking VIF (Variance Inflation Factor - MultiColinearity)\n# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train.columns\nvif['VIF'] = [variance_inflation_factor(X_train.values, i) for i in range(X_train.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The R-squared is a significant 85%, but there are insignificant variables and variables with strong multicollinearity. We need to get rid of them, in the following cells, we will follow the same process in an itrative manner till we build a robust model. First we will remove all columns with High P Values and then when the P Values are acceptable for all the columns, we will check their VIF and remove them.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Removing 'mnth_Mar' due to high P-Value\nX = X_train.drop('mnth_Mar',axis=1)\nX_train_sm = sm.add_constant(X)\nlr = sm.OLS(y_train, X_train_sm)\nlr_model2 = lr.fit()\nlr_model2.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Removing 'weekday_4' due to high P-Value\nX = X.drop('weekday_4',axis=1)\nX_train_sm = sm.add_constant(X)\nlr = sm.OLS(y_train, X_train_sm)\nlr_model3 = lr.fit()\nlr_model3.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Removing 'mnth_Oct' due to high P-Value\nX = X.drop('mnth_Oct',axis=1)\nX_train_sm = sm.add_constant(X)\nlr = sm.OLS(y_train, X_train_sm)\nlr_model4 = lr.fit()\nlr_model4.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Removing 'mnth_Jun' due to high P-Value\nX = X.drop('mnth_Jun',axis=1)\nX_train_sm = sm.add_constant(X)\nlr = sm.OLS(y_train, X_train_sm)\nlr_model5 = lr.fit()\nlr_model5.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Removing 'weekday_3' due to high P-Value\nX = X.drop('weekday_3',axis=1)\nX_train_sm = sm.add_constant(X)\nlr = sm.OLS(y_train, X_train_sm)\nlr_model6 = lr.fit()\nlr_model6.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Removing 'atemp' due to high P-Value\nX = X.drop('atemp',axis=1)\nX_train_sm = sm.add_constant(X)\nlr = sm.OLS(y_train, X_train_sm)\nlr_model7 = lr.fit()\nlr_model7.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Removing 'weekday_5' due to high P-Value\nX = X.drop('weekday_5',axis=1)\nX_train_sm = sm.add_constant(X)\nlr = sm.OLS(y_train, X_train_sm)\nlr_model8 = lr.fit()\nlr_model8.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Removing 'mnth_Aug' due to high P-Value\nX = X.drop('mnth_Aug',axis=1)\nX_train_sm = sm.add_constant(X)\nlr = sm.OLS(y_train, X_train_sm)\nlr_model9 = lr.fit()\nlr_model9.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Removing 'weekday_2' due to high P-Value\nX = X.drop('weekday_2',axis=1)\nX_train_sm = sm.add_constant(X)\nlr = sm.OLS(y_train, X_train_sm)\nlr_model10 = lr.fit()\nlr_model10.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Removing 'weekday_1' due to high P-Value\nX = X.drop('weekday_1',axis=1)\nX_train_sm = sm.add_constant(X)\nlr = sm.OLS(y_train, X_train_sm)\nlr_model11 = lr.fit()\nlr_model11.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Removing 'mnth_May' due to high P-Value\nX = X.drop('mnth_May',axis=1)\nX_train_sm = sm.add_constant(X)\nlr = sm.OLS(y_train, X_train_sm)\nlr_model12 = lr.fit()\nlr_model12.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Removing 'mnth_Feb' due to high P-Value\nX = X.drop('mnth_Feb',axis=1)\nX_train_sm = sm.add_constant(X)\nlr = sm.OLS(y_train, X_train_sm)\nlr_model13 = lr.fit()\nlr_model13.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We now see that all the variables have a P Value <= 0.05, which signifies that these variables are statistically significant. Let's now check if there is any Multi-Colinearity among these variables.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking VIF (Variance Inflation Factor - MultiColinearity)\n# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Humidity and Temperature have a high VIF, which means they have multicolinearity and one of them must be removed and checked again.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Removing 'hum' due to high VIF\nX = X.drop('hum',axis=1)\nX_train_sm = sm.add_constant(X)\nlr = sm.OLS(y_train, X_train_sm)\nlr_model14 = lr.fit()\nlr_model14.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking the VIF Again\nvif = pd.DataFrame()\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This looks like an acceptable model. We keep the <b>temp</b> variable, because from our EDA, we have seen that Temperature has a direct colinearity with the booking count. On colder days, the bookings are less, whereas on hotter, summer time, the bookings are up significantly. Thus as per business understanding, we finalize this model as the final model.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the co-efficients of the final model lr_model14\nprint(lr_model14.summary())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Validating the assumptions of Linear Regression\n- Linear Relationship\n- Homoscedasticity\n- Absence of Multicollinearity\n- Independence of residuals (absence of auto-correlation)\n- Normality of Errors","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Validating Linear Relationship\nsm.graphics.plot_ccpr(lr_model14, 'temp')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The partial residual plot represents the relationship between the predictor and the dependent variable while taking into account all the other variables. As we can see in the above graph, the linearity is well respected.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Validating Homoscedasticity : The residuals have constant variance with respect to the dependent variable\ny_train_pred = lr_model14.predict(X_train_sm)\nsns.scatterplot(y_train,(y_train - y_train_pred))\nplt.plot(y_train,(y_train - y_train), '-r')\nplt.xlabel('Count')\nplt.ylabel('Residual')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see in the above plot, Homoscedasticity is well respected since the variance of the residuals are almost constant.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Validating Multi Colinearity\nplt.figure(figsize=(15,8))\nsns.heatmap(X.corr(),annot=True, cmap='YlGnBu')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All variables have less than 0.56 correlation with eachother. Checking the VIF now.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(vif)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Taking 10 as the maximum VIF permissible for this model, we decide on keeping these colmns based upon business assumptions.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Independence of residuals (absence of auto-correlation)\n# Autocorrelation refers to the fact that observations’ errors are correlated\n# To verify that the observations are not auto-correlated, we can use the Durbin-Watson test. \n# The test will output values between 0 and 4. The closer it is to 2, the less auto-correlation there is between the various variables\n# (0–2: positive auto-correlation, 2–4: negative auto-correlation)\n\nprint('The Durbin-Watson value for Model No.14 is',round(sm.stats.stattools.durbin_watson((y_train - y_train_pred)),4))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is almost nill auto-correlation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Normality of Errors\ny_train_pred = lr_model14.predict(X_train_sm)\n\n# Ploting the histogram of the error terms\nfig = plt.figure()\nsns.distplot((y_train - y_train_pred))\nfig.suptitle('Error Terms')                  \nplt.xlabel('Errors')     \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sm.qqplot((y_train - y_train_pred), fit=True, line='45')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The error terms are normally distributed","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Making prediction using the final model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Scaling the Test Dataset with the Scaler of the Training Set\ncols = df_test.columns\ndf_test[cols] = scaler.transform(df_test[cols])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dividing into X_test and y_test\ny_test = df_test.pop('cnt')\nX_test = df_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Adding the constant column\nX_test_m14 = sm.add_constant(X_test)\n# Removing all the columns which has been removed from Model 14\nX_test_m14 = X_test_m14.drop(['hum','mnth_Feb','mnth_Mar','mnth_May',\n                              'mnth_Jun','mnth_Aug','mnth_Oct','atemp',\n                              'weekday_1','weekday_2','weekday_3','weekday_4','weekday_5' ], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Making prediction using Model 14\ny_test_pred = lr_model14.predict(X_test_m14)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model Evaluation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print('The R-Squared score of the model for the predicted values is',round(r2_score(y_test, y_test_pred),2))\nprint('The Root Mean Squared Error of the model for the predicted values is',round(np.sqrt(mean_squared_error(y_test, y_test_pred)),4))\nprint('The Mean Absolute Error of the model for the predicted values is',mean_absolute_error(y_test, y_test_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# As asked in problem statement\nfrom sklearn.metrics import r2_score\nr2_score(y_test, y_test_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that the equation of our best fitted line developed by Model 14 is:\n\n$ cnt = 0.1219 + ( 0.2346  \\times  yr - 0.0498  \\times  holiday + 0.0474 \\times workingday + 0.4370 \\times temp - 0.1602 \\times windspeed - 0.0698  \\times season_spring + 0.0356 \\times season_summer + 0.0901 \\times season_winter - 0.0458 \\times December - 0.0517 \\times January - 0.0475 \\times July -0.04078 \\times November + 0.0674 \\times September + 0.0596 \\times weekday_6 - 0.2155 \\times Bad Weather + 0.0821 \\times Good Weather ) $\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Since the bookings increase on good weather days with hotter temperature, the company must increase their bike availibilty and promotions during the summer months to further increase their booking count.\n\nAn R-Squared value of 0.82 on the test data signifies that the model is a very good predictor and 82% of the variance is captured by the model.It can be further improved by using other regression techniques like Random Forest.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}