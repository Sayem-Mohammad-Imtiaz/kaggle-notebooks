{"cells":[{"metadata":{"_uuid":"3a89743a57156cca9091326faae8bfcc619e0068"},"cell_type":"markdown","source":"In this mini tutorial I show the process of picking the best number of variables for predictions using a method called Principal Components Analysis (PCA) ;)"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a75015a59d86d64735fb720dfdfa740ca373e7b4"},"cell_type":"markdown","source":"Importing our dataset:"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"from sklearn import preprocessing\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\n\n#url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv'\ndata = pd.read_csv('../input/winequality-red.csv')\n\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bcdfc9031de5796569efd8486f239ba7c3fee3bc"},"cell_type":"code","source":"# X = data[[data.columns]]\nX = data.drop('quality',axis=1)\ny = data.quality\nX.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ad0121ea3a873430551dbf69cd0c2967683b492c"},"cell_type":"markdown","source":"When standardizing data, the following formula is applied to every data point:\n\nZ = (Sample - Mean)/(Stan.Dev)\n\nIn other words, we are calculating z-scores, centering the samples by the mean and th standard deviation."},{"metadata":{"trusted":true,"_uuid":"ea2576162bb58de7c38f3980f529a917d9013690"},"cell_type":"code","source":"X = preprocessing.StandardScaler().fit(X).transform(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"42e23a8c2eb0d0d1bed24d626978860246daa2d2"},"cell_type":"code","source":"model = PCA()\nresults = model.fit(X)\nplt.plot(results.explained_variance_)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"da470c336fc641ef117836d2968b6da0ff9d2478"},"cell_type":"markdown","source":"As we can see, the more variables we add the more of the information we represent.\n\ni would say that 5 varaibles is a good meausure."},{"metadata":{"trusted":true,"_uuid":"1b3460a4f775a65a4e265ed419bfa60c62c9f6f8"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import confusion_matrix\n\ngnb = GaussianNB()\nfit = gnb.fit(X,y)\npred = fit.predict(X)\nprint (confusion_matrix(pred,y))\nprint(\"accuracy: \")\nprint(confusion_matrix(pred,y).trace()*100/confusion_matrix(pred,y).sum())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6f9082de1dadc38de414fa155275efcc467f9b44"},"cell_type":"markdown","source":"Now,  let's see how much the accuracy get's affected with different number of variables:"},{"metadata":{"trusted":true,"_uuid":"5ede8c8081b96bf841375dec9a333bb6e2addc1d"},"cell_type":"code","source":"predicted_correct = []\nfor i in range(1,10):\n    model = PCA(n_components = i)\n    results = model.fit(X)\n    Z = results.transform(X)\n    fit = gnb.fit(Z,y)\n    pred = fit.predict(Z)\n    predicted_correct.append(confusion_matrix(pred,y).trace())\nplt.plot(predicted_correct)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"da3c1ae0e9b71426f5659fdaa35e815a9d704a19"},"cell_type":"markdown","source":"The plot shows that with only 3  variables. \nalso adding more variables beyond 5 doesn't add much predictive power as the first 5."},{"metadata":{"trusted":true,"_uuid":"f36a12ffb5aa4e6be2459f1b8630575de70c5bf0"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"734d4a028ca55d7ef3922e42f8f7b8699a747cb7"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2b43881debd299b417ae71ee4c3015a92e3c68dd"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}