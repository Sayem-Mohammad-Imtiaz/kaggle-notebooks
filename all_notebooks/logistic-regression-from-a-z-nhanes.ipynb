{"nbformat":4,"nbformat_minor":1,"metadata":{"language_info":{"file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","version":"3.6.3","nbconvert_exporter":"python","mimetype":"text/x-python","pygments_lexer":"ipython3"},"kernelspec":{"language":"python","name":"python3","display_name":"Python 3"}},"cells":[{"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # Library for plotting\nimport seaborn as sns\nfrom pandas.plotting import radviz\nfrom pandas.plotting import parallel_coordinates\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"ba35b45d-5302-4a3f-b7c1-2ed87c256c6f","_uuid":"67a3b2d20d07069dd9ab9ed9bfdcd6f31cef5051"},"cell_type":"code"},{"source":"df = pd.read_csv(\"../input/NHANES_CAT_NEW_130.CSV\")","outputs":[],"execution_count":null,"metadata":{"collapsed":true},"cell_type":"code"},{"source":"# Data Exploration Analysis\nThis Data Exploration Analysis will help to visualize the distribution and behavior of the independent variables with relation to the dependent variable.","metadata":{},"cell_type":"markdown"},{"source":"#We will check the number of rows of the dataset, the type of data of every column and also to check \n#if the columns contain null values\ndf.info()","outputs":[],"execution_count":null,"metadata":{},"cell_type":"code"},{"source":"# We will check if we have a balanced or an unbalanced dataset by checking the number of records for each class.\ndf.HYPCLASS.value_counts()\n# After running this sentence, we can notice that we will be working with an unbalanced dataset.","outputs":[],"execution_count":null,"metadata":{},"cell_type":"code"},{"source":"# Independent Variables definition:\n\n- Gender (GENDER): Categorical variable\n\n - 1: Male\n - 2: Female\n\n- Age Range (AGERANGE): Categorical Variable\n\n - 1: Age between 20 and 30 years old\n - 2: Age between 31 and 40 years old\n - 3: Age between 41 and 50 years old\n - 4: Age between 51 and 60 years old\n - 5: Age between 61 and 70 years old\n - 6: Age between 71 and 80 years old\n\n-. Race (RACE): Categorical variable\n\n - 1: Mexican American\n - 2: Other Hispanic\n - 3: Non-Hispanic White\n - 4: Non-Hispanic Black\n - 5: Other Race - Including Multi-Racial\n\n-. BMI Range (BMIRANGE): Categorical Variable\n\n - 1: Underweight = <18.5\n - 2: Normal weight = 18.5–24.9\n - 3: Overweight = 25–29.9\n - 4: Obesity = BMI of 30 or greater","metadata":{},"cell_type":"markdown"},{"source":"# Number of individuals by Class","metadata":{},"cell_type":"markdown"},{"source":"# Number of patient by Class\nplt.figure(figsize=(10,6))\nlabels = ['0 - No Hypertension', '1 - Yes Hypertension']\n\nax = sns.countplot(x='HYPCLASS',data=df,palette='RdBu_r')\n\nax.set_title('Number of individuals by Class')\nax.set_xticklabels(labels)\nax.set_xlabel('Hypertension Class')\nax.set_ylim(0,18000)\n\n#Bar values\nrects = ax.patches\n#To get the value labels from value_counts()\nv_labels = df['HYPCLASS'].value_counts()\n# Now make some labels with the values \nlabels = [\"%d\" % v_labels[i] for i in range(len(rects))]\n\nfor rect, label in zip(rects, labels):\n       height = rect.get_height()\n       ax.text(rect.get_x() + rect.get_width()/2, height + 5, label, ha='center', va='bottom')","outputs":[],"execution_count":null,"metadata":{},"cell_type":"code"},{"source":"# Hypertensive population by Gender","metadata":{},"cell_type":"markdown"},{"source":"labels = ['0 - No Hypertension', '1 - Yes Hypertension']\nax = sns.countplot(x='HYPCLASS',hue='GENDER',data=df, palette='RdBu_r')\nax.set_xticklabels(labels)\nax.set_title('Hypertensive Population by Gender: 1-Male | 2-Female')","outputs":[],"execution_count":null,"metadata":{},"cell_type":"code"},{"source":"# Hypertensive population by Race","metadata":{},"cell_type":"markdown"},{"source":"labels = ['0 - No Hypertension', '1 - Yes Hypertension']\nax = sns.countplot(x='HYPCLASS',hue='RACE',data=df)\nax.set_xticklabels(labels)\nax.set_title('Hypertensive Population by Race')\nprint(\" 1: Mexican American\\n\",\"2: Other Hispanic\\n\",\"3: Non-Hispanic White\\n\",\"4: Non-Hispanic Black\\n\",\"5: Other Race - Including Multi-Racial\\n\")","outputs":[],"execution_count":null,"metadata":{},"cell_type":"code"},{"source":"# Hypertensive population by Age Range by Gender","metadata":{},"cell_type":"markdown"},{"source":"labels = ['1-Mexican American','2-Other Hispanic','3-Non-Hispanic White','4-Non-Hispanic Black','5-Other Race - Including Multi-Racial']\nax = sns.factorplot(x='RACE',y='AGERANGE',col='GENDER', data = df,hue='HYPCLASS',kind='bar')\nax.set_xticklabels(labels,rotation=90)","outputs":[],"execution_count":null,"metadata":{},"cell_type":"code"},{"source":"# Hypertensive population by BMI Range by Gender","metadata":{},"cell_type":"markdown"},{"source":"plt.figure(figsize=(30,10))\nlabels = ['1-Mexican American','2-Other Hispanic','3-Non-Hispanic White','4-Non-Hispanic Black','5-Other Race - Including Multi-Racial']\nax = sns.factorplot(x='RACE',y='BMIRANGE',col='GENDER',data = df,hue='HYPCLASS',kind='bar')\nax.set_xticklabels(labels,rotation=90)","outputs":[],"execution_count":null,"metadata":{},"cell_type":"code"},{"source":" # Correlation Heatmap for all the variables","metadata":{},"cell_type":"markdown"},{"source":"plt.figure(figsize = (16,5))\nsns.heatmap(df.drop('SEQN',axis=1).corr(),annot=True)","outputs":[],"execution_count":null,"metadata":{},"cell_type":"code"},{"source":"# Some interesting plots for Feature analysis","metadata":{},"cell_type":"markdown"},{"source":"# Visualizing the trend of the Hypclass to every variable in a plane (where the forces acting ). \ndef rad_viz(df,labels):\n    fig = radviz(df, labels, color=sns.color_palette())\n    plt.show()\n\nrad_viz(df.drop('SEQN',axis=1),'HYPCLASS') # Specify which column contains the labels","outputs":[],"execution_count":null,"metadata":{},"cell_type":"code"},{"source":"# Parallel coordinates allows to see clusters in data. Points that tend to cluster will appear closer together  \ndef pcoord_viz(df, labels):\n    fig = parallel_coordinates(df, labels, color=sns.color_palette())\n    plt.xticks(rotation=60)\n    plt.show()\n\npcoord_viz(df.drop('SEQN',axis=1),'HYPCLASS') # Specify which column contains the labels","outputs":[],"execution_count":null,"metadata":{"collapsed":true},"cell_type":"code"},{"source":"# **Feature selection**","metadata":{},"cell_type":"markdown"},{"source":"# Selecting the Features (X) and Class (y) datasets","metadata":{},"cell_type":"markdown"},{"source":"df.drop('SEQN',axis=1,inplace=True)\n\nX = df.drop('HYPCLASS',axis=1)\ny = df['HYPCLASS']","outputs":[],"execution_count":null,"metadata":{"collapsed":true},"cell_type":"code"},{"source":"# Method 1 - Feature ranking with recursive feature elimination and cross-validated selection of the best number of features - RFECV","metadata":{},"cell_type":"markdown"},{"source":"from sklearn.model_selection import StratifiedKFold\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.datasets import make_classification\nfrom sklearn.linear_model import LogisticRegression\n\n\nlogmodel = LogisticRegression()\n\n# The \"accuracy\" scoring is proportional to the number of correct\n# classifications\nrfecv = RFECV(estimator=logmodel, step=1, cv=StratifiedKFold(),\n              scoring='roc_auc')\nrfecv.fit(X, y)\n\nprint(\"Optimal number of features : %d\" % rfecv.n_features_)\n\nprint(\"Optimal number of features : %d\" % rfecv.n_features_)\nprint(\"Selected features mask: \")\nprint(rfecv.support_)\nprint(X.columns)\nprint(\"Total features:\",rfecv.support_.size)\n\n\n# Plot number of features VS. cross-validation scores\nplt.figure()\nplt.xlabel(\"Number of features selected\")\nplt.ylabel(\"Cross validation score (nb of correct classifications)\")\nplt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\nplt.show()","outputs":[],"execution_count":null,"metadata":{},"cell_type":"code"},{"source":"# Method 2- Feature selection - likelihood ratio Chi2 (Categorical Variables)","metadata":{},"cell_type":"markdown"},{"source":"# Compute chi-squared stats between each non-negative feature and class\nfrom sklearn.feature_selection import chi2\n\nscores, pvalues = chi2(X, y)\npvalues=[\"{0:.7f}\".format(x)for x in pvalues]\n\np_values = pd.concat([pd.DataFrame(X.columns,columns=['Feature']),pd.DataFrame(pvalues,columns=['p-value']),pd.DataFrame(scores,columns=['Score'])], axis = 1)\nprint(p_values)","outputs":[],"execution_count":null,"metadata":{},"cell_type":"code"},{"source":"The method 1, feature ranking with recursive feature elimination used the \"roc_auc\" accuracy metrics to select the variables that are significant for the model and need to be included. For the method 1, the least important feature is \"**SMOKE**\". And, for method 2, based on the significance level of p < 0.05, the feature \"**KIDNEY**\" is not significant for the model. However, the clinical importance of these variables will allow us to include them in the prediction model.","metadata":{"collapsed":true},"cell_type":"markdown"},{"source":"# Create dummy Variables\nThis dummy variables or indicator variables will be created to transform the categorical variables to a\nbinary form to have consistent predictors with the outcome.","metadata":{},"cell_type":"markdown"},{"source":"# We will read the dataset again to prevent any undesire change from the preivious DEA and Feature selection.\ndf = pd.read_csv(\"../input/NHANES_CAT_NEW_130.CSV\")","outputs":[],"execution_count":null,"metadata":{"collapsed":true},"cell_type":"code"},{"source":"#We eliminate the sequence number\ndf.drop('SEQN',axis=1,inplace=True)","outputs":[],"execution_count":null,"metadata":{},"cell_type":"code"},{"source":"#Visualization of the columns before the creation of the dummy variables\ndf.describe().transpose()","outputs":[],"execution_count":null,"metadata":{},"cell_type":"code"},{"source":"#Create dummy Variables and eliminate the first column to prevent Multicollinearity. This is simply redundancy in the information contained in predictor variables.\ntrainDfDummies = pd.get_dummies(df, columns=['GENDER', 'AGERANGE', 'RACE', 'BMIRANGE','KIDNEY','SMOKE','DIABETES'], drop_first=True)","outputs":[],"execution_count":null,"metadata":{"collapsed":true},"cell_type":"code"},{"source":"trainDfDummies.describe().transpose()","outputs":[],"execution_count":null,"metadata":{},"cell_type":"code"},{"source":"# Selection of the hyperparameters for the ML model with GridSearch","metadata":{},"cell_type":"markdown"},{"source":"X = trainDfDummies.drop(['HYPCLASS'],axis=1)\ny = trainDfDummies['HYPCLASS']","outputs":[],"execution_count":null,"metadata":{"collapsed":true},"cell_type":"code"},{"source":"# Independent Variables\nX.head()","outputs":[],"execution_count":null,"metadata":{},"cell_type":"code"},{"source":"# Dependent variable\ny.head()","outputs":[],"execution_count":null,"metadata":{},"cell_type":"code"},{"source":"# Train test split","metadata":{},"cell_type":"markdown"},{"source":"# Split our dataset in 70% for training the model and 30% to evaluate the model\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)","outputs":[],"execution_count":null,"metadata":{},"cell_type":"code"},{"source":"# Grid Search Algoritm","metadata":{},"cell_type":"markdown"},{"source":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\n\n#Default Logistic regression Model\nlogmodel = LogisticRegression()\n\n# Param grid will receive all the hyper parameters will be evaluated with the LR model and the scoring metric \n# is \"roc_auc\"\nparam_grid = {'solver' : ['newton-cg', 'lbfgs', 'liblinear', 'sag'], 'C':[0.1,1, 10, 100, 1000],'class_weight':['','balanced'],'max_iter':[100,500,1000,5000,8000,10000]}\ngrid = GridSearchCV(logmodel,param_grid,refit=True,verbose=3,scoring='roc_auc')\ngrid.fit(X_train,y_train)","outputs":[],"execution_count":null,"metadata":{},"cell_type":"code"},{"source":"# get the best parameters\ngrid.best_params_","outputs":[],"execution_count":null,"metadata":{},"cell_type":"code"},{"source":"# Logistic Regression Model","metadata":{},"cell_type":"markdown"},{"source":"# Function to run the Logistic regression model with the best parameters obtained \n#in the previous step with the GridSearch algorithm\ndef modelevaluation(X,y,model):\n    global prob\n    \n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n\n    from collections import Counter\n    print('# of real cases in the Test data (P = 1) and (N = 0)',sorted(Counter(y_test).items()))\n    \n    model.fit(X_train,y_train)\n    predictions = model.predict(X_test)\n    \n    prob = model.predict_proba(X_test)\n    \n    #Uses jaccard_similarity_score\n    # J(y_true,y_predict) = (y_true intersect y_predict) / (y_true Union y_predict)\n    print('accuracy_score: ',accuracy_score(y_test,predictions))\n    print('accuracy_score (number of correctly classified samples): ',accuracy_score(y_test,predictions,normalize=False))\n    \n    print('Zero one loss: ',zero_one_loss(y_test,predictions))\n    print('Zero one loss (number of imperfectly predicted subsets): ',zero_one_loss(y_test,predictions,normalize=False))\n    \n    print(confusion_matrix(y_test,predictions))\n    \n    tn, fp, fn, tp = confusion_matrix(y_test,predictions).ravel()\n    print(\"TN:\",tn)\n    print(\"FN:\",fn)\n    print(\"TP:\",tp)\n    print(\"FP:\",fp)\n    \n    print(classification_report(y_test,predictions))\n   \n    #ROC curve is insensitive to imbalanced classes\n    fpr, tpr, thresholds = roc_curve(y_test,predictions,pos_label=1)\n    #fpr, tpr, thresholds = roc_curve(y,predictions,pos_label=1)\n    print(\"FPR: \",fpr)\n    print(\"TPR: \",tpr)\n    print(\"AUC: \",auc(fpr, tpr))\n    \n    #Compute Area Under the Curve (AUC) from prediction scores\n    #the probability that a classifier will rank a randomly chosen positive instance higher than a randomly chosen negative one\n    print(\"ROC_AUC_SCORE: \",roc_auc_score(y_test,predictions))\n    \n    #Plot of the AUC\n    plt.figure()\n    lw = 2\n    plt.plot(fpr,tpr, color='darkorange', lw=lw, label='ROC curve (area = %0.2f)' % auc(fpr, tpr))\n\n    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic')\n    plt.legend(loc=\"lower right\")\n    plt.show()","outputs":[],"execution_count":null,"metadata":{},"cell_type":"code"},{"source":"from sklearn.metrics import classification_report, confusion_matrix,auc,roc_curve,accuracy_score,roc_auc_score,zero_one_loss\n\nX = trainDfDummies.drop(['HYPCLASS'],axis=1)\ny = trainDfDummies['HYPCLASS']\n\n# Best params: {'C': 0.1, 'class_weight': '', 'max_iter': 100, 'solver': 'sag'}\nlogmodel = LogisticRegression(C=0.1,class_weight='',max_iter=100,solver='sag',warm_start=True)\n\nmodelevaluation(X,y,logmodel)","outputs":[],"execution_count":null,"metadata":{},"cell_type":"code"},{"source":"#Print the intercept and the coefficients\nprint(\"Intercept:\")\nprint(logmodel.intercept_)\n\nprint(\"\\nCoefficients:\")\nprint(logmodel.coef_[0].transpose())\n\n#Calculate the Odd Ratios for clinical interpretation\nprint(\"\\nOdd Ratios:\")\ndf_oodsr = pd.DataFrame({'Features':X.columns,'coefficient':logmodel.coef_[0],'Odds Ratio':np.exp(logmodel.coef_[0])},columns=['Features','coefficient','Odds Ratio'])\n\nprint(df_oodsr)\n\n","outputs":[],"execution_count":null,"metadata":{},"cell_type":"code"},{"source":"# How to make Predictions","metadata":{},"cell_type":"markdown"},{"source":"We assign a value to each Feature or independent variable and get the index of the record in the dataset","metadata":{},"cell_type":"markdown"},{"source":"X[(X.GENDER_2==0) & \n(X.AGERANGE_2==1) &\n(X.AGERANGE_3==0) &\n(X.AGERANGE_4==0) &\n(X.AGERANGE_5==0) &\n(X.AGERANGE_6==0) &\n(X.RACE_2==0) &\n(X.RACE_3==1) &\n(X.RACE_4==0) &\n(X.RACE_5==0) &\n(X.BMIRANGE_2==0) &\n(X.BMIRANGE_3==1) &\n(X.BMIRANGE_4==0) &\n(X.KIDNEY_2==1) &\n(X.SMOKE_2==1) &\n(X.DIABETES_2==0) &\n(X.DIABETES_3==1) ]","outputs":[],"execution_count":null,"metadata":{},"cell_type":"code"},{"source":"X.iloc[6439,:]","outputs":[],"execution_count":null,"metadata":{},"cell_type":"code"},{"source":"# The calculated probability is:\nprint(logmodel.predict_proba(X)[6439][1])","outputs":[],"execution_count":null,"metadata":{},"cell_type":"code"},{"source":"","outputs":[],"execution_count":null,"metadata":{"collapsed":true},"cell_type":"code"}]}