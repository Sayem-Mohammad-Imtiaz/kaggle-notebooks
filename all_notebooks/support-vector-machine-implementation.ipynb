{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Introduction\nThe data set includes observations labeled as malignant (harmful) or benign (not harmful) cancer type. We'll try to classify labels using support vector machine. \n\n> Originally published [here](https://github.com/Bhasfe/ml-algorithms/tree/master/SVC)\n\n### What is SVM ?\nSupport Vector Machines are mainly used for classification problems in Machine Learning. They can also be used for regression problems. A support vector machine tries to find best \"hyperplane\" which separates different classes. Following figure shows that a hyperplane whose margin is maximized.\n\n<img src=\"https://github.com/Bhasfe/ml-algorithms/blob/master/SVC/svm.png?raw=true\" width=\"300px\" height=\"300px\" align=\"left\" />\n<br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br>\nIf the classes are not linearly separable, the kernel trick can be applied to make hyperplane. Kernel trick stands for applying the different kernel functions for multidimensional space to get desired dimensions (for example 2D to 3D ). The most populer kernels are RBF (radial basis function), polynomial and linear. Following figure shows us how kernel trick can be applied for 2D space to 3D space transformation\n\n<img src=\"https://github.com/Bhasfe/ml-algorithms/blob/master/SVC/kernel-trick.png?raw=true\" width=\"500px\" height=\"500px\" align=\"left\" />\n<br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br>\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 1. EDA + FE\n\nFirstly, we need to import necessary libraries/packages","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\n# import Support Vector Classifier\nfrom sklearn.svm import SVC\n\n# for splitting the data into train and test sets\nfrom sklearn.model_selection import train_test_split\n\n# to evaluate the model\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import cross_val_score\n\n# I will keep the resulting plots\n%matplotlib inline\n\n# Enable Jupyter Notebook's intellisense\n%config IPCompleter.greedy=True","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Secondly, load the breast cancer dataset and start to explore it.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load the data set\nbreast_cancer = pd.read_csv(\"/kaggle/input/breast-cancer-wisconsin-data/data.csv\")\n\n# Display first 5 rows of the DataFrame\ndisplay(breast_cancer.head())\n\n# Display the statistics\ndisplay(breast_cancer.describe())\n\n# Print info\nprint(breast_cancer.info())\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Notes:\n   * There are 569 observations and 33 features.\n   * There is no missing values. \n   * **Unnamed: 32** and **id** columns should be dropped","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Let's drop the **Unnamed: 32** and the **id** columns","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop the columns\nbreast_cancer.drop([\"Unnamed: 32\",\"id\"], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Count the labels.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Print the counts\nprint(breast_cancer[\"diagnosis\"].value_counts())\n\n# Visualize the counts\nsns.countplot(breast_cancer[\"diagnosis\"])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualize some 2-D features to see patterns\ndef make_scatterplot(x,y):\n    sns.scatterplot(x,y,data=breast_cancer,hue='diagnosis')\n    plt.title(y + \" vs \" + x)\n    plt.show()\n    \nmake_scatterplot('radius_mean', 'texture_mean')\nmake_scatterplot('perimeter_mean', 'area_mean')\nmake_scatterplot('smoothness_mean', 'smoothness_se')\nmake_scatterplot('concavity_mean', 'compactness_mean')\nmake_scatterplot('fractal_dimension_mean', 'perimeter_se')\nmake_scatterplot('symmetry_worst', 'concave points_worst')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's look at the correlations.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Print the correlation matrix\nprint(breast_cancer.corr())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualize with a heatmap\nfigure, ax = plt.subplots(figsize=(20,20))\nmask = np.triu(np.ones_like(breast_cancer.corr(), dtype=np.bool))\nsns.heatmap(breast_cancer.corr(), mask=mask, annot=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Look at the distributions of some features.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot the histograms\ndef plot_histogram(column):\n    sns.distplot(breast_cancer[column])\n    plt.title(column)\n    plt.show()\n\n\nplot_histogram(\"radius_mean\")\nplot_histogram(\"texture_mean\")\nplot_histogram(\"perimeter_mean\")\nplot_histogram(\"area_mean\")\nplot_histogram(\"smoothness_mean\")\nplot_histogram(\"compactness_mean\")\nplot_histogram(\"concavity_mean\")\nplot_histogram(\"concave points_mean\")\nplot_histogram(\"symmetry_mean\")\nplot_histogram(\"fractal_dimension_mean\")\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's apply dimensionality reduction with t-SNE (t-distributed stochastic neighbor embedding) to see whole picture in 2-D space","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# import TSNE\nfrom sklearn.manifold import TSNE\n\n# fit and transform the TSNE model\ntsne = TSNE(learning_rate =  50)\ntsne_f = tsne.fit_transform(breast_cancer.drop(\"diagnosis\", axis=1))\n\n# Create a new DataFrame to store reduced features\ndf = pd.DataFrame({'x':tsne_f[:,0],'y':tsne_f[:,1]})\n\nprint(\"Before:\",breast_cancer.shape)\nprint(\"After\",df.shape)\n\ndisplay(df.head())\nsns.scatterplot(x='x', y='y', hue=breast_cancer['diagnosis'],data=df)\nplt.title(\"After Dimensionality Reduction\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, it gives us better understanding of the features","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 2. Machine Learning","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Select the features and the target.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get features and the target\nX = breast_cancer.drop(\"diagnosis\", axis=1)\ny = breast_cancer[\"diagnosis\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now it's time to build a machine learning model. First of all, we'll split the data into training and test sets.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split the data as 30% test and 80% training sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=34)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we can build our model and make predictions on test set","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Initialize the Support vector classifier\nsvc = SVC(kernel=\"linear\")\n\n# Fit the SVC with training sets\nsvc.fit(X_train, y_train)\n\nscores = cross_val_score(svc, X_train, y_train, cv=10, scoring='f1_macro')\n\nprint(scores)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's look at the accuracy score and classification report","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make predictions on test set\ny_pred = svc.predict(X_test)\n\nacc = accuracy_score(y_test, y_pred)\nprint(\"Accuracy:\",acc)\n\nprint(\"\\n Classification Report\\n\")\nprint(classification_report(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It looks like our model works pretty well! <br>\nThat's the end of the notebook. I am hoping that it will be helpful to understand basics of SVM.\n","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}