{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"#  Importing Important Packages","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import wordnet\nimport nltk\nfrom nltk.corpus import stopwords \nfrom nltk.tokenize import word_tokenize \nimport re\nimport spacy\nfrom nltk.corpus import sentiwordnet as swn\nfrom IPython.display import clear_output\nimport plotly.express as px\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\nimport plotly\nplotly.offline.init_notebook_mode (connected = True)\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem import LancasterStemmer\nfrom nltk import ngrams\n# The following code creates a word-document matrix.\nfrom sklearn.feature_extraction.text import CountVectorizer\n# Modeling packages\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import classification_report","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Reading Data","metadata":{}},{"cell_type":"code","source":"data=pd.read_csv('../input/imdb-movie-reviews-dataset/movie_data.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Having a look at 1st ten reviews in the data","metadata":{}},{"cell_type":"code","source":"data.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing Function","metadata":{}},{"cell_type":"code","source":"data['reviews_text_new'] = data['review'].str.lower()\n\n# removing special character\ndata['reviews_text_new'] = data['reviews_text_new'].str.replace(r'[^A-Za-z0-9]+', ' ')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# removing stop words\neng_stop_words = stopwords.words('english')\nstop_words = set(eng_stop_words)\ndef stopwords_removal(stop_words, sentence):\n    sent =[word for word in nltk.word_tokenize(sentence) if word not in stop_words]\n    return (' '.join(map(str, sent)))\n\ndata['reviews_text_new'] = data['reviews_text_new'].apply(lambda row: stopwords_removal(stop_words, row))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Lemmatization Function","metadata":{}},{"cell_type":"code","source":"lemmatizer = WordNetLemmatizer()\nwordnet_map = {\"N\":wordnet.NOUN, \"V\":wordnet.VERB, \"J\":wordnet.ADJ, \"R\":wordnet.ADV}\ndef lemmatize_words(text):\n    pos_tagged_text = nltk.pos_tag(text.split())\n    return \" \".join([lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_tagged_text])\n\ndata[\"reviews_text_new\"] = data['reviews_text_new'].apply(lambda text: lemmatize_words(text))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Results of Preprocessing data (Removing stopwords & Lemmatization)","metadata":{}},{"cell_type":"code","source":"data.head(6)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"- Old Review -\")\nprint(data['review'][3])\nprint(\"\\n- Last Edit Review -\")\nprint(data['reviews_text_new'][3])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Replacing Positive -> 1 and Negative -> 0\n\ndata.replace({\"positive\":1,\"negative\":0},inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data[['reviews_text_new','sentiment']].head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Building a machine learning model","metadata":{}},{"cell_type":"markdown","source":"# Bag-of-words and n-grams","metadata":{}},{"cell_type":"markdown","source":"# Divide into training and test sets:","metadata":{}},{"cell_type":"markdown","source":"# Applying logistic regression","metadata":{}},{"cell_type":"code","source":"bow_counts = CountVectorizer(tokenizer= word_tokenize,\n                             lowercase=True,\n                             ngram_range=(1,1))\n\nbow_data = bow_counts.fit_transform(data.reviews_text_new)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_bow, X_test_bow, y_train_bow, y_test_bow = train_test_split(bow_data,\n                                                                    data['sentiment'],\n                                                                    test_size = 0.2,\n                                                                    random_state = 0,\n                                                                    shuffle=False,\n                                                                   stratify=None)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Defining and training the model\nlr_model_all_new = LogisticRegression(max_iter = 200)\nlr_model_all_new.fit(X_train_bow, y_train_bow)\n\n# Predicting the results\ntest_pred_lr_all = lr_model_all_new.predict(X_test_bow)\n\n\n## Calculate key performance metrics\n\n# Print a classification report\nprint(classification_report(y_test_bow,test_pred_lr_all))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_senti, X_test_senti, y_train_senti, y_test_senti = train_test_split(data['reviews_text_new'],\n                                                                            data['sentiment'],\n                                                                            test_size = 0.2,\n                                                                            random_state = 0,\n                                                                           shuffle=False,\n                                                                           stratify=None)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create A New DataFrame For Testing And Analysing \n\ndf_test = pd.DataFrame(columns = ['review_test','actual_score', 'lr_score','swn_score'])\ndf_test['review_test'] = X_test_senti\ndf_test['actual_score'] = y_test_senti\ndf_test['lr_score'] = test_pred_lr_all\ndf_test.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nltk\nimport ssl\nfrom nltk.corpus import wordnet as wn\nfrom nltk.corpus import sentiwordnet as swn\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pos=neg=obj=count=0\n\npostagging = []\n\nfor review in X_test_senti:\n    list = word_tokenize(review)\n    postagging.append(nltk.pos_tag(list))\n\ndf_test['pos_tags'] = postagging\n\ndef penn_to_wn(tag):\n    if tag.startswith('J'):\n        return wn.ADJ\n    elif tag.startswith('N'):\n        return wn.NOUN\n    elif tag.startswith('R'):\n        return wn.ADV\n    elif tag.startswith('V'):\n        return wn.VERB\n    return None\n\n\n# Returns list of pos-neg and objective score. But returns empty list if not present in senti wordnet.\ndef get_sentiment(word,tag):\n    wn_tag = penn_to_wn(tag)\n    \n    if wn_tag not in (wn.NOUN, wn.ADJ, wn.ADV):\n        return []\n\n    #Lemmatization\n    lemma = lemmatizer.lemmatize(word, pos=wn_tag)\n    if not lemma:\n        return []\n\n    #Synset is a special kind of a simple interface that is present in NLTK to look up words in WordNet. \n    #Synset instances are the groupings of synonymous words that express the same concept. \n    #Some of the words have only one Synset and some have several.\n    synsets = wn.synsets(word, pos=wn_tag)\n    if not synsets:\n        return []\n\n    # Take the first sense, the most common\n    synset = synsets[0]\n    swn_synset = swn.senti_synset(synset.name())\n\n    return [synset.name(), swn_synset.pos_score(),swn_synset.neg_score(),swn_synset.obj_score()]\n\n    pos=neg=obj=count=0\n    \n    ###################################################################################\nsenti_score = []\n\nfor pos_val in df_test['pos_tags']:\n    senti_val = [get_sentiment(x,y) for (x,y) in pos_val]\n    for score in senti_val:\n        try:\n            pos = pos + score[1]  #positive score is stored at 2nd position\n            neg = neg + score[2]  #negative score is stored at 3rd position\n        except:\n            continue\n    senti_score.append(pos - neg)\n    pos=neg=0    \n    \ndf_test['senti_score'] = senti_score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(df_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"overall=[]\nfor i in range(40000,50000,1):\n    if df_test['senti_score'][i]>= 0:\n        overall.append(1)\n    elif df_test['senti_score'][i]< 0:\n        overall.append(0)\n    \ndf_test['swn_score']=overall","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"case1=case2=case3=case4=case5=case6=0\nfor i in range(40000,50000,1):\n    if ((df_test['lr_score'][i] == df_test['swn_score'][i]) and (df_test['swn_score'][i] == df_test['actual_score'][i])):\n        case1 = case1+1\n    if ((df_test['lr_score'][i] == df_test['swn_score'][i]) and (df_test['swn_score'][i] != df_test['actual_score'][i])):\n        case2 = case2+1\n    if ((df_test['lr_score'][i] != df_test['swn_score'][i]) and (df_test['lr_score'][i] == df_test['actual_score'][i])):\n        case3 = case3+1\n    if ((df_test['lr_score'][i] != df_test['swn_score'][i]) and (df_test['swn_score'][i] == df_test['actual_score'][i])):\n        case4 = case4+1\n    if ((df_test['lr_score'][i] != df_test['swn_score'][i]) and (df_test['actual_score'][i]== 0)):\n        case5 = case5+1\n    if ((df_test['lr_score'][i] != df_test['swn_score'][i]) and (df_test['actual_score'][i]==1)):\n        case6 = case6+1\n        \nprint(\"case 1\",case1) # 58%\nprint(\"case 2\",case2) # 5%\nprint(\"case 3\",case3) # 30%\nprint(\"case 4\",case4) # 5%\nprint(\"case 5\",case5) # 25%\nprint(\"case 6\",case6) # 10%\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nlr = accuracy_score(y_test_bow, test_pred_lr_all)\nswn = accuracy_score(y_test_bow, overall)\nprint(\"lr_accuracy\",lr)\nprint(\"swn_accuracy\",swn)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_score =[]\nfor i in range(40000,50000,1):\n    if 0.58*(df_test['lr_score'][i] == df_test['swn_score'][i]) or 0.3*(df_test['lr_score'][i] != df_test['swn_score'][i]):\n        final_score.append(df_test['lr_score'][i])\n    elif 0.05*(df_test['lr_score'][i] != df_test['swn_score'][i]):\n        final_score.append(df_test['swn_score'][i])\n    elif 0.25*(df_test['lr_score'][i] != df_test['swn_score'][i]):\n        final_score.append(0)\n    elif 0.1*(df_test['lr_score'][i] != df_test['swn_score'][i]):\n        final_score.append(1)\n        \ndf_test['final_score_upt']=final_score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_upt = accuracy_score(y_test_bow, final_score)\nprint(\"final_hyprid_accuracy\",final_upt)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_score1 =[]\nfor i in range(40000,50000,1):\n    if (df_test['lr_score'][i]==1) and (df_test['swn_score'][i]==1):\n        final_score1.append(1)\n    elif (df_test['lr_score'][i]==0) and (df_test['swn_score'][i]==0):\n        final_score1.append(0)\n    else :\n        final_score1.append(df_test['lr_score'][i])\ndf_test['final_score_upt']=final_score1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_upt1 = accuracy_score(y_test_bow, final_score1)\nprint(\"final_hyprid_accuracy\",final_upt1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = pd.DataFrame(columns = ['lr_score','swn_score'])\ndf_train['swn_score'] = overall\ndf_train['lr_score'] = test_pred_lr_all\ndf_train.head(4)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_trainf, X_testf, y_trainf, y_testf = train_test_split(df_train,\n                                                                    df_test['actual_score'],\n                                                                    test_size = 0.2,\n                                                                    random_state = 0,\n                                                                    shuffle=False,\n                                                                   stratify=None)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hyprid = LogisticRegression(max_iter = 200).fit(X_trainf,y_trainf).predict(X_testf)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"acc = accuracy_score(y_testf, hyprid)\nprint(\"accuracy\",acc)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test['lr_score'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test['swn_score'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"averaged_preds = (df_test['lr_score'] + df_test['swn_score'])//2\nacc = accuracy_score(y_test_bow, averaged_preds)\nprint(acc)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr_model_all_new.feature_names=bow_counts.get_feature_names()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from joblib import dump, load \n\n# save model to file \ndump(lr_model_all_new, filename=\"Sentiment_Analysis_unigram1.joblib\")\ndump(hyprid, filename=\"Stacking_Voting.joblib\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import a saved joblib model \nloaded_model_lr = load(filename=\"Sentiment_Analysis_unigram1.joblib\")\nloaded_model_hyprid = load(filename=\"Stacking_Voting.joblib\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feats = loaded_model_lr.feature_names\nfeats_len = len(feats)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import string\nsent ='the actor was ugly'\nsent =sent.lower()\nsent = sent.translate(str.maketrans('', '', string.punctuation))\nfiltered_sentence = [] \nstop_words = set(stopwords.words('english')) \nword_tokens =word_tokenize(sent)\nfiltered_sentence = [w for w in word_tokens if not w in stop_words ]\nlistToStr = ' '.join(map(str, filtered_sentence))\nlemmatizer = WordNetLemmatizer()\nwordnet_map = {\"N\":wordnet.NOUN, \"V\":wordnet.VERB, \"J\":wordnet.ADJ, \"R\":wordnet.ADV}\ndef lemmatize_words(text):\n    pos_tagged_text = nltk.pos_tag(word_tokenize(text))\n    return ([lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_tagged_text])\nlemmatized_output =[]\nlemmatized_output = lemmatize_words(listToStr)\n    \n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lemmatized_output","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test = pd.DataFrame(columns = ['review_test'])\ndf_test['review_test'] = lemmatized_output\ndf_test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nltk\nimport ssl\nfrom nltk.corpus import wordnet as wn\nfrom nltk.corpus import sentiwordnet as swn\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\n\npos=neg=obj=count=0\n\npostagging = []\n\nfor review in lemmatized_output:\n    list = word_tokenize(review)\n    postagging.append(nltk.pos_tag(list))\n\ndf_test['pos_tags'] = postagging\n\ndef penn_to_wn(tag):\n    if tag.startswith('J'):\n        return wn.ADJ\n    elif tag.startswith('N'):\n        return wn.NOUN\n    elif tag.startswith('R'):\n        return wn.ADV\n    elif tag.startswith('V'):\n        return wn.VERB\n    return None\n\n\n# Returns list of pos-neg and objective score. But returns empty list if not present in senti wordnet.\ndef get_sentiment(word,tag):\n    wn_tag = penn_to_wn(tag)\n    \n    if wn_tag not in (wn.NOUN, wn.ADJ, wn.ADV):\n        return []\n\n    #Lemmatization\n    lemma = lemmatizer.lemmatize(word, pos=wn_tag)\n    if not lemma:\n        return []\n\n    #Synset is a special kind of a simple interface that is present in NLTK to look up words in WordNet. \n    #Synset instances are the groupings of synonymous words that express the same concept. \n    #Some of the words have only one Synset and some have several.\n    synsets = wn.synsets(word, pos=wn_tag)\n    if not synsets:\n        return []\n\n    # Take the first sense, the most common\n    synset = synsets[0]\n    swn_synset = swn.senti_synset(synset.name())\n\n    return [synset.name(), swn_synset.pos_score(),swn_synset.neg_score(),swn_synset.obj_score()]\n\n    pos=neg=obj=count=0\n    \n    ###################################################################################\nsenti_score = []\n\nfor pos_val in df_test['pos_tags']:\n    senti_val = [get_sentiment(x,y) for (x,y) in pos_val]\n    for score in senti_val:\n        try:\n            pos = pos + score[1]  #positive score is stored at 2nd position\n            neg = neg + score[2]  #negative score is stored at 3rd position\n        except:\n            continue\n    senti_score.append(pos - neg)\n    pos=neg=0    \n    \ndf_test['senti_score'] = senti_score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"senti_score_output =senti_score\nsum =0\nfor i in range(0, len(senti_score_output)):\n    sum = sum + senti_score_output[i]  \nif sum >=0:\n    sum =1\nelse:\n    sum=0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Sum of all the elements of an array: \" + str(sum))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#len(feats)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#len(sent_features)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#joblib_y_preds = loaded_joblib_model.predict([sent_features])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#print(joblib_y_preds)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}