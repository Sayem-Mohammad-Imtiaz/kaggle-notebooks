{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Imports:","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport copy\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import make_scorer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.svm import SVC\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras.layers import Dense, Input\nfrom keras.models import Model\nfrom keras.optimizers import Adam","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Project Description","metadata":{}},{"cell_type":"markdown","source":"For this project we will be using the Palmer Archipelago Penguin Dataset to predict the species of penguins.\nMultiple classification methods are used to predict the species of penguin, including:\n* Logistic Regression,\n* K-Nearest Neighbors,\n* Naive Bayes,\n* Linear Discriminant Analysis,\n* Support Vector Machines, and\n* Neural Network\n\nThese were either taught or discussed throughout the course. \nThroughout this notebook, pandas and scikit-learn are used as opposed to the models implemented in class manually. This decision was made because the models from scikit-learn are more likely to be used in future applications/career and are also my personal preference. ","metadata":{}},{"cell_type":"markdown","source":"# Data Exploration\nWe begin by loading the data and looking at the variables available. Pandas provides useful methods for doing this.","metadata":{}},{"cell_type":"code","source":"#load data \ndata_dir = '../input/palmer-archipelago-antarctica-penguin-data/penguins_size.csv'\ndf = pd.read_csv(data_dir)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#view column names, data types, and missing values\ndf.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"numeric_cols = list(df.dtypes[df.dtypes != 'object'].index) #to be used for scaling later\nnumeric_cols","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#lets take a look at the rows with missing values\ndf[df.isna().any(axis=1)]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Above we see that the most common missing value is sex, while other rows are almost completely blank. These rows make up a small portion of the dataset so they can be removed later. \n\nNext, lets take a look at the distribution of the variables. For numerical we can use the describe method and categorical we use frequencies.","metadata":{}},{"cell_type":"code","source":"df.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['species'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['island'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['sex'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[df.sex == '.']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"By looking at the above outputs we see that the dataset is will balances, with the categories being split somewhat evenly.\nIt is strange that one of the rows has sex as '.'. Thus, we will remove this row alongside those having an NA value. ","metadata":{}},{"cell_type":"markdown","source":"## Data Preparation\n\nNow that we have had a chance to look over the dataset, it is necessary to modify it before training models. This includes removing rows with missing values, changing categorical variables to numeric, subsetting into training and validation datasets, and scaling the numerical columns.","metadata":{}},{"cell_type":"markdown","source":"First we remove the rows that have missing values. We want to do this before other operations so it doesn't waste time or interfere with the scaling process. ","metadata":{}},{"cell_type":"code","source":"df.dropna(inplace=True)\ndf.drop(336,axis = 0,inplace=True) #this is the row that has sex == '.'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Earlier we saw that the species, island, and sex columns were objects using the info method. This means they are strings or categorical variables. It is necessary to convert these to numeric so they can be used with all algorithms.\n\nWe use different techniques for each of these columns:\n\n* Because species is our response variable, it is not necessary to scale it in a way that is meaningful for the models. We simply change the 3 species of penguins into numbers 0,1,2. \n\n* The island variable is going to be used as a predictor variable. In its current state, it is a categorical variable with 3 different islands as levels. To convert this to numeric, the one hot encoding method is used, where a column is created for each island and a 0 or 1 is used to indicate boolean membership to the island. Each row only has one of the newly created columns set to 1.\n\n* The sex variable can easily be converted to a binary number, with 0 representing MALE and 1 representing FEMALE. ","metadata":{}},{"cell_type":"code","source":"df['species'].replace({'Adelie': 0, 'Gentoo': 1, 'Chinstrap': 2}, inplace=True)\ndf[['Biscoe','Dream','Torgersen']] = pd.get_dummies(df.island)\ndf['sex'].replace({'MALE': 0, 'FEMALE': 1}, inplace = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.drop(['island'],axis = 1,inplace = True) #original island variable is no longer needed","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A look at the new dataframe:","metadata":{}},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Train-Test split\n\nIt is now necessary to split the dataset. This is vital for evaluating the performance of our model. Note that we stratify on the species variable so that the training and validation sets have similar distributions. We do this before applying the last preprocessing step: Standardization. \n\nWe fit the scaler to the training dataset and then use it on the validation data. It is important not to fit the scaler to the whole dataset.\nAlso note that only the original numeric columns are scaled. If the transformed columns were scaled they would lose the intended meaning. ","metadata":{}},{"cell_type":"code","source":"df = df.sample(frac=1)#shuffle rows\nX = df.drop('species',axis=1) #predictors\nY = df['species'] #labels","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_validation, Y_train, Y_validation = train_test_split(X,Y,stratify=Y)\nX_train.index = range(X_train.shape[0]) #reset index\nX_validation.index = range(X_validation.shape[0]) #reset index","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scaler = StandardScaler()\nX_train_numeric = scaler.fit_transform(X_train[numeric_cols])\nX_validation_numeric = scaler.transform(X_validation[numeric_cols])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = pd.concat([pd.DataFrame(X_train_numeric,columns=numeric_cols), X_train.drop(numeric_cols,axis=1)], axis = 1)\nX_train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_validation = pd.concat([pd.DataFrame(X_validation_numeric,columns=numeric_cols), X_validation.drop(numeric_cols,axis=1)], axis = 1)\nX_validation.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Y_train.value_counts(normalize=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Y_validation.value_counts(normalize=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Definition:\n\nSeveral models will be created and their performance will be shown in terms of accuracy. A confusion matrix will be used so we can see what errors are most common.","metadata":{}},{"cell_type":"markdown","source":"## Logistic Regression","metadata":{}},{"cell_type":"code","source":"logistic_regression = LogisticRegression(C=1e5)\nlogistic_regression.fit(X_train,Y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"logistic_regression.score(X_train,Y_train), logistic_regression.score(X_validation,Y_validation)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"confusion_matrix(Y_validation,logistic_regression.predict(X_validation))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The logistic regression model exhibits very high performance, with 100% accuracy on both the training and validation tests.","metadata":{}},{"cell_type":"markdown","source":"## K-Nearest Neighbors","metadata":{}},{"cell_type":"code","source":"knn = KNeighborsClassifier(n_neighbors=3)\nknn.fit(X_train,Y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"knn.score(X_train, Y_train), knn.score(X_validation, Y_validation)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"confusion_matrix(Y_validation,knn.predict(X_validation))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The K-nearest neighbors algorithm is somewhat simple in comparison to the other models, however it still proves to be effective.\nWith ~99% accuracy on the training and validation sets, this model is certaintly suitable for the task at hand. There is only one of the validation observations misclasified, incorrectly being predicted as Adelie when the true species is Chinstrap.","metadata":{}},{"cell_type":"markdown","source":"## Naive Bayes","metadata":{}},{"cell_type":"code","source":"naive_bayes = GaussianNB()\nnaive_bayes.fit(X_train,Y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"naive_bayes.score(X_train, Y_train), naive_bayes.score(X_validation, Y_validation)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"confusion_matrix(Y_validation,naive_bayes.predict(X_validation))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The naive bayes model is the worse performer so far, with ~70% accuracy on both training and validation sets. This is surprising as the naive bayes model should not have any problems with there being multiple classes as opposed to a simpiler binary classification. Here we see that the Adelie penguins frequently misclassified, with the model only correcly classifying 45% of the Adelie penguins in the validation set.  ","metadata":{}},{"cell_type":"markdown","source":"## Linear Discriminant Analysis","metadata":{}},{"cell_type":"code","source":"lda = LinearDiscriminantAnalysis()\nlda.fit(X_train,Y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lda.score(X_train, Y_train), lda.score(X_validation, Y_validation)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"confusion_matrix(Y_validation,lda.predict(X_validation))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The linear discriminant analysis model exhibits very high performance, with 100% accuracy on the training set and ~99% on the validation sets. We see only one misclassification, identical to the KNN model.","metadata":{}},{"cell_type":"markdown","source":"## Support Vector Machine\n\nIn class we constructed three different SVM classifiers to use for this problem manually.\nScikit learn has implemented the SVM model such that multiple classifiers are created automatically when performing a multiclass task. \nThus, we can only use one model here which uses the one-vs-rest strategy, so 3 classifiers are created in the backend. ","metadata":{}},{"cell_type":"code","source":"svc = SVC(gamma='auto')\nsvc.fit(X_train,Y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"svc.score(X_train, Y_train), svc.score(X_validation, Y_validation)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"confusion_matrix(Y_validation,svc.predict(X_validation))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Yet another model achieves near perfect performance. ","metadata":{}},{"cell_type":"markdown","source":"## Neural Network\n\nNote: this is an extremely simple network as this is not a difficult task. Thus, it is similar to using logistic regression but I wanted to include it as an option.","metadata":{}},{"cell_type":"code","source":"def neural_network():\n    input_layer = Input(shape=(8))\n    x = Dense(64, activation = 'sigmoid')(input_layer)\n    output_layer = Dense(3, activation = 'softmax')(x)\n\n    nn = Model(inputs = [input_layer], outputs = [output_layer])\n    nn.compile(Adam(lr=.001), loss = 'sparse_categorical_crossentropy', metrics =['accuracy'])\n    return nn\nnn = neural_network()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nn.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nn_fit = nn.fit(X_train, Y_train, batch_size=1, epochs = 20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nn.evaluate(X_validation,Y_validation)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"confusion_matrix(Y_validation,np.argmax(nn.predict(X_validation), axis = 1))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Yet again, only one of the validation datapoints is miscalssified.","metadata":{}},{"cell_type":"markdown","source":"# Comparison and Conclusion","metadata":{}},{"cell_type":"markdown","source":"Many of the models shown above have very good accuracy. The only model that did not seem to fit well to this dataset was the Naive Bayes model, achieving an accuracy below 80% while all others reached around 99-100%. \n\nFrom the MATH 3094 course, we learned many of the introductory techniques for modeling. These were demonstrated within this notebook and show to each be effective. \nIt is important to create multiple models on the same dataset to choose the most effective for the task. It was beneficial to learn about all the above models throughout the semester to effectively solve problems and these techniques will be used for future applications. ","metadata":{}}]}