{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn import model_selection\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.svm import SVR\nfrom sklearn.feature_selection import SelectPercentile ,SelectKBest, chi2\nfrom sklearn.feature_selection import f_regression\nfrom sklearn.linear_model import LinearRegression,Ridge,ElasticNet\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn import linear_model\nimport random \n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Read data from CSV file using pandas and convert ['date'] column into datetime64\ndf_house_data=pd.read_csv(\"../input/housesalesprediction/kc_house_data.csv\", parse_dates = ['date'])\n\n#Check for what type of data is present\nprint(df_house_data.info())\n\n#We have all numerical data except the one date","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#3.1.1        Remove Features that do not contribute to the prediction\n#Remove Features : Some of the features may not contribute to the prediction of the final class.  \n  \ndf_house_data = df_house_data.drop(['id','date'], 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_house_data.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#3.1.2 Outlier Detection Seaborn boxplot \nbxplt = sns.boxplot(data=df_house_data.iloc[:,1:])\nplt.setp(bxplt.get_xticklabels(), rotation=90)\nplt.show()\n\n#There are no anomilities or unexpected outliers that could have come from incorrect data. Sq feet (area) can vary quite a bit.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#3.1.3   Dealing with Missing Values\n#Check for missing values\nprint(df_house_data.isnull().sum())\n\n#There are no missing values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#As we do not have Categorical Data, we are not using any kind of pre processing encoding technique to convert Categorical Data to numerical form\n\n\n#As we are building a regression model, we cannot use pre processing for Data imbalance\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#3.1.5     Scaling Data\n#Scaling the data\nscaler = StandardScaler()\nstdData = scaler.fit_transform(df_house_data)\n\n\nX = df_house_data.iloc[:,1:]\ny = df_house_data[\"price\"]\n\n#print(X)\n\n#print(y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#3.1.7    Feature Selection\n\n#Deleting the features which have Percentile score less than 50\nselector = SelectPercentile(f_regression, percentile=25) \nselector.fit(X,y)\nfor n,s in zip(X.columns, selector.scores_): \n    print (\"Score : \", s, \" for feature \", n)\n    if s<50:\n        X = X.drop([n], 1)\n        print(\"deleted:\", n)\n        \nprint(X.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#DecisionTreeRegressor Model\n\ndecreg = DecisionTreeRegressor()\n#Cross_val_score uses the KFold or StratifiedKFold strategies by default.\naccu1 = cross_val_score(decreg,X,y,cv=5)\nprint(\"___DecisionTreeRegressor____\\n\")\nprint(accu1)\nsc1 =  accu1.mean()\nprint(sc1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Linear Regression Model\n\nlinreg=LinearRegression()\n\naccu2 = cross_val_score(linreg,X,y,cv=5)\nprint(\"___Linear Regression____\\n\")\nprint(accu2)\nsc2 =  accu2.mean()\nprint(sc2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#KNeighborsRegressor Model\n\nknr = KNeighborsRegressor()\n\naccu3 = cross_val_score(knr,X,y,cv=5)\nprint(\"___KNeighborsRegressor____\\n\")\nprint(accu3)\nsc3 =  accu3.mean()\nprint(sc3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Lasso Model\n\nlasso = linear_model.Lasso(random_state = 42)\naccu4 = cross_val_score(lasso,X,y,cv=5)\nprint(\"___Lasso____\\n\")\nprint(accu4)\nsc4 =  accu4.mean()\nprint(sc4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Ensemble methods\nfrom sklearn.ensemble import RandomForestRegressor,AdaBoostRegressor,ExtraTreesRegressor,GradientBoostingRegressor,BaggingRegressor\nrf = RandomForestRegressor(random_state = 42)\n\naccu5 = cross_val_score(rf,X,y,cv=5)\nprint(\"___RandomForestRegressor____\\n\")\nprint(accu5)\nsc5 =  accu5.mean()\nprint(sc5)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nbr = BaggingRegressor(random_state = 42)\n\naccu6 = cross_val_score(br,X,y,cv=5)\nprint(\"___BaggingRegressor____\\n\")\nprint(accu6)\nsc6 =  accu6.mean()\nprint(sc6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gb = GradientBoostingRegressor(random_state = 42)\n\naccu7 = cross_val_score(gb,X,y,cv=5)\nprint(\"___GradientBoostingRegressor____\\n\")\nprint(accu7)\nsc7 =  accu7.mean()\nprint(sc7)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"et = ExtraTreesRegressor(random_state = 42)\n\naccu8 = cross_val_score(et,X,y,cv=5)\nprint(\"___ExtraTreesRegressor____\\n\")\nprint(accu8)\nsc8 =  accu8.mean()\nprint(sc8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#4.1 Base Model Results:\n#All the scores:\nprint(\"DecisionTreeRegressor :\",sc1)\nprint(\"LinearRegression :\",sc2)\nprint(\"KNeighborsRegressor :\",sc3)\nprint(\"linear_model->Lasso Regressor:\",sc4)\nprint(\"RandomForestRegressor :\",sc5)\nprint(\"BaggingRegressor :\",sc6)\nprint(\"GradientBoostingRegressor :\",sc7)\nprint(\"ExtraTreesRegressor :\",sc8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Select Top 3 Models\n# To find what all parameters a model has to be set \n#ExtraTreesRegressor.get_params(et)\nGradientBoostingRegressor.get_params(gb)\n#RandomForestRegressor.get_params(rf)\n#aggingRegressor.get_params(br)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# HPO GradientBoostingRegressor\nfrom sklearn.model_selection import GridSearchCV\n\nparam_grid = [ {'n_estimators': [100,150,200] , 'max_depth' : [2,3,4,5] ,'random_state' : list(range(42,43)), 'min_samples_leaf' : [1,2,10],'min_samples_split' : [2,7,15], 'learning_rate' : [0.1,0.2,0.3]  }  ] \nclf = GridSearchCV(GradientBoostingRegressor(), param_grid, cv=3, n_jobs = -1)\n\nclf.fit(X,y)\n\nprint(\"\\n Best parameters set found on development set:\") \nprint(clf.best_params_ , \"with a score of \", clf.best_score_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# HPO BaggingRegressor\n\nparam_grid = [{ 'bootstrap': [True,False], 'bootstrap_features': [True,False],'random_state' : list(range(42,43)), 'max_features': [12,15,16], 'max_samples': [0.8,1.0,2], 'n_estimators': [10,20,30,40,50]}] \nclf = GridSearchCV(BaggingRegressor(), param_grid, cv=3, n_jobs = -1)\n\nclf.fit(X,y)\nprint(\"\\n Best parameters set found on development set:\") \nprint(clf.best_params_ , \"with a score of \", clf.best_score_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# HPO ExtraTreesRegressor\n\nparam_grid=[{'n_estimators': list(range(100,201,25)),'max_features': [15,16],'random_state' : list(range(42,43)),'min_samples_leaf': list(range(1,30,5)),'min_samples_split': list(range(15,36,5))}]\nclf = GridSearchCV(ExtraTreesRegressor(), param_grid, cv=3, n_jobs = -1)\n\nclf.fit(X,y)\nprint(\"\\n Best parameters set found on development set:\") \nprint(clf.best_params_ , \"with a score of \", clf.best_score_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 4.3.1 Tree Based Feature Selection (RandomForestRegressor)\nimport numpy as np\n\n#We again build X to include all features (We had deleted 2 using the SelectPercentile Univariate Feature Selection Tool)\n\nX = df_house_data.iloc[:,1:]\nclf = RandomForestRegressor(random_state=42)\nclf.fit(X, y)\nprint(clf.feature_importances_)\n\n#We use  np.argsort to sort the features according to the ascending order of the indices of the feature scores. Then we  iteratively remove features from the dataset (starting with the weakest features) and calculate cross_val_score every time using Bagging Regressor with the optimal hyper parameters that we found out earlier.\nimp_features_asc = np.argsort(clf.feature_importances_)\nprint(imp_features_asc)\nprint(X.shape[1])\nallAccuracies = []\nnumberOfFeatures = []\nbr = BaggingRegressor(bootstrap= False, bootstrap_features= True, max_samples= 0.8, n_estimators= 50, random_state= 42)\nfor i in range(0,13):\n    numberOfFeatures.append(i)\n    X_new = X.drop(X.columns[[imp_features_asc[:i]]], axis = 1)\n    \n    accur9 = cross_val_score(br,X_new,y,cv=3)\n    allAccuracies.append(accur9.mean())\n\nprint(allAccuracies)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nplt.figure()\nplt.xlabel(\"Number of features removed\")\nplt.ylabel(\"Cross validation score \")\nplt.plot(numberOfFeatures, allAccuracies)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 4.3.2 Feature Selection based on Pearson Coefficient (Correlation)\n\nX = df_house_data.iloc[:,1:]\ncorr = df_house_data.corr()\nplt.figure(figsize=(20,16))\nsns.heatmap(data=corr, square=True , annot=True, cbar=True)\nplt.show()\n\nfrom scipy.stats import pearsonr\n#It helps to measures relationship of every feature with the target regression value\n\n#We use the Pearson Correlation Coefficient (pearsonr) to calculate the correlations between each feature and price i.e. the target regression value. Then we select the top 16 features which have the highest Correlation values and pass them through the ML models to calculate scores.\ncorrelations_all = {}\nfor f in X.columns:\n    temp = df_house_data[[f,'price']]\n    x1 = temp[f].values\n    x2 = temp['price'].values\n    key = f + ' || ' + 'price'\n    correlations_all[key] = pearsonr(x1,x2)[0]\n    \n\ndata_correlations = pd.DataFrame(correlations_all, index=['Value']).T\nprint(data_correlations.loc[data_correlations['Value'].abs().sort_values(ascending=False).index])\n\n\ntop16 = list(data_correlations['Value'].abs().sort_values(ascending=False).index[:16])\n\ntop16 = [x[:-9] for x in top16]\nprint(top16)\nbr = BaggingRegressor(bootstrap= False, bootstrap_features= True, max_samples= 0.8, n_estimators= 50, random_state= 42)\nX_new1 = X[top16]\nprint(\"Mean score:\",cross_val_score(br,X_new1,y,cv=3).mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 4.3.3 Backward Elimination\nimport statsmodels.api as sm\n    \n#Backward Elimination\n#If the pvalue is above 0.05 then we remove the feature, else we keep it.\n#We are using OLS model “Ordinary Least Squares” used for performing linear regression. We will remove the feature which has max pvalue and build the model once again. This is an iterative process and will keep on deleting features and checking pvalues in a loop.\n\ncols = list(X.columns)\npmax = 1\nwhile (len(cols)>0):\n    p= []\n    X_1 = X[cols]\n    X_1 = sm.add_constant(X_1)\n    model = sm.OLS(y,X_1).fit()\n    p = pd.Series(model.pvalues.values[1:],index = cols)\n    print(p.sort_values())\n    pmax = max(p)\n    feature_with_p_max = p.idxmax()\n    if(pmax>0.05):\n        cols.remove(feature_with_p_max)\n        print(feature_with_p_max,\" feature removed\")\n    else:\n        break\nselected_features_BE = cols\nprint(selected_features_BE)\nbr = BaggingRegressor(bootstrap= False, bootstrap_features= True, max_samples= 0.8, n_estimators= 50, random_state= 42)\nprint(\"Mean score:\",cross_val_score(br,X_new1,y,cv=3).mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 4.3.4 RFE (Recursive Feature Elimination) [2]\nfrom sklearn.feature_selection import RFE\n\nX = df_house_data.iloc[:,1:]\ncols = list(X.columns)\nmodel = RandomForestRegressor(random_state=42)\nscores = []\nselected_features = []\n#We increment the features one by one in a loop and calculate the RFE scores for each number of features selected. From the list of all scores, we find the maximum and the features that were used to obtain this maximum score. We then use these features in our optimized Bagging Regressor model and find the accuracy score\nfor i in range(4,15):\n    #Initializing RFE model\n    rfe = RFE(model, i)             \n    #Transform data using RFE\n    X_rfe = rfe.fit_transform(X,y)  \n    #Fitting the data to model\n    model.fit(X_rfe,y)              \n    features_series = pd.Series(rfe.support_,index = cols)\n    selected_features_rfe = features_series[features_series==True].index\n    print(selected_features_rfe)\n    scores.append(model.score(X_rfe,y))\n    selected_features.append(selected_features_rfe)\nprint(scores)\nind = scores.index(max(scores))\nmost_imp_features=selected_features[ind]\nprint(most_imp_features)\nbr = BaggingRegressor(bootstrap= False, bootstrap_features= True, max_samples= 0.8, n_estimators= 50, random_state= 42)\nprint(\"Mean score:\",cross_val_score(br,X[most_imp_features],y,cv=3).mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}