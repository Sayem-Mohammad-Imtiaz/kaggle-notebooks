{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer \nimport string\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom skmultilearn.problem_transform import ClassifierChain\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.svm import LinearSVC\nfrom sklearn.linear_model import SGDClassifier,LogisticRegression\nfrom sklearn.neural_network import MLPClassifier\nfrom imblearn.over_sampling import SMOTE\n%matplotlib inline\nfrom sklearn import tree\nimport matplotlib\nfrom collections import Counter\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import svm\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score\n\nfrom keras.models import Sequential\nfrom keras import layers\nnltk.download('stopwords')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfref =  pd.read_csv(r'../input/new-data/FR-Dataset.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndfref.dropna(subset = [\"Text\"], inplace=True)\ndfref = dfref.drop_duplicates(subset=['Text'])\ndfref.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def createArrayOfLabels(df):\n    mlb = MultiLabelBinarizer()\n    tempdf = pd.DataFrame(columns=['labels'])\n    for i in df:\n        temp = []\n        try:\n            i = i.replace(' ','')\n            for j in i.split(','):\n                if j != '':\n                    temp.append(j.strip())\n        except:\n            pass\n        tempdf = tempdf.append(pd.DataFrame({'labels': [temp]}))\n        \n    tempdf.apply(lambda x: tuple(x.values))\n    mlb.fit(tempdf['labels'])\n    tempdf = mlb.transform(tempdf['labels'])\n    tempdf = pd.DataFrame(tempdf,columns =list(mlb.classes_) )\n    return tempdf\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess(x):\n    try:\n        stop_words = stopwords.words('english')\n        lemmatizer = WordNetLemmatizer() \n        x = x.lower()\n        x = x.translate(str.maketrans('', '', string.punctuation))\n        x = x.split()\n        x = [word for word in x if word not in stop_words]\n        x = [lemmatizer.lemmatize(word) for word in x]\n        x = str(x).replace(',',' ').replace(\"'\",\"\")[1:-1]\n        return x\n    except:\n        print(f'There is an error in {x}')\n        return 'empty'\n    \n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Class = createArrayOfLabels(dfref['Class'])\ndfref = dfref.reset_index(drop=True)\ntocsv = pd.concat([dfref, Class], axis = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ntocsv.dropna(subset = [\"Text\"], inplace=True)\ntocsv = tocsv.drop(columns=['ID', 'Class'])\ntocsv['Text'] = tocsv['Text'].apply(preprocess)\ntocsv.to_csv(r'C:Fr2.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"v = TfidfVectorizer(max_features=2500)\nx = v.fit_transform(tocsv['Text'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tocsv = pd.concat([pd.DataFrame(x.toarray()), tocsv], axis = 1)\nprint(tocsv.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"columns = list(tocsv.columns)\nX_train,X_test, y_train , y_test  = train_test_split(tocsv[columns[:-4]],tocsv[columns[-4:]], test_size=0.25,random_state=42)\n\n\nX_train = X_train.drop(columns=['Text'])\nX_test = X_test.drop(columns=['Text'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"classifier = ClassifierChain(RandomForestClassifier(class_weight='balanced'))\nclassifier.fit(X_train, y_train)\nprint(X_test.shape)\npredictions = classifier.predict(X_test)\nprint(\"Accuracy = \",accuracy_score(y_test,predictions))\nprint(classification_report(y_test,predictions))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"classifier = ClassifierChain(MultinomialNB())\nclassifier.fit(X_train, y_train)\ntarget_names = ['Extract', 'Move', 'Inline' ,'Rename']\npredictions = classifier.predict(X_test)\nprint(classification_report(y_test,predictions,target_names=target_names))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"classifier = RandomForestClassifier(class_weight='balanced')\nclassifier.fit(X_train, y_train)\nprint(X_test.shape)\npredictions = classifier.predict(X_test)\nprint(\"Accuracy = \",accuracy_score(y_test,predictions))\nprint(classification_report(y_test,predictions))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport random\nfrom sklearn.datasets import make_classification\nfrom sklearn.neighbors import NearestNeighbors\n\n\n\ndef get_tail_label(df):\n    \"\"\"\n    Give tail label colums of the given target dataframe\n\n    args\n    df: pandas.DataFrame, target label df whose tail label has to identified\n\n    return\n    tail_label: list, a list containing column name of all the tail label\n    \"\"\"\n    columns = df.columns\n    n = len(columns)\n    irpl = np.zeros(n)\n    for column in range(n):\n        irpl[column] = df[columns[column]].value_counts()[1]\n    irpl = max(irpl)/irpl\n    mir = np.average(irpl)\n    tail_label = []\n    for i in range(n):\n        if irpl[i] > mir:\n            tail_label.append(columns[i])\n    return tail_label\n\ndef get_index(df):\n    \"\"\"\n    give the index of all tail_label rows\n    args\n    df: pandas.DataFrame, target label df from which index for tail label has to identified\n\n    return\n    index: list, a list containing index number of all the tail label\n    \"\"\"\n    tail_labels = get_tail_label(df)\n    index = set()\n    for tail_label in tail_labels:\n        sub_index = set(df[df[tail_label]==1].index)\n        index = index.union(sub_index)\n    return list(index)\n\ndef get_minority_instace(X, y):\n    \"\"\"\n    Give minority dataframe containing all the tail labels\n\n    args\n    X: pandas.DataFrame, the feature vector dataframe\n    y: pandas.DataFrame, the target vector dataframe\n\n    return\n    X_sub: pandas.DataFrame, the feature vector minority dataframe\n    y_sub: pandas.DataFrame, the target vector minority dataframe\n    \"\"\"\n    index = get_index(y)\n    X_sub = X[X.index.isin(index)].reset_index(drop = True)\n    y_sub = y[y.index.isin(index)].reset_index(drop = True)\n    return X_sub, y_sub\n\ndef nearest_neighbour(X):\n    \"\"\"\n    Give index of 5 nearest neighbor of all the instance\n\n    args\n    X: np.array, array whose nearest neighbor has to find\n\n    return\n    indices: list of list, index of 5 NN of each element in X\n    \"\"\"\n    nbs=NearestNeighbors(n_neighbors=5,metric='euclidean',algorithm='kd_tree').fit(X)\n    euclidean,indices= nbs.kneighbors(X)\n    return indices\n\ndef MLSMOTE(X,y, n_sample):\n    \"\"\"\n    Give the augmented data using MLSMOTE algorithm\n\n    args\n    X: pandas.DataFrame, input vector DataFrame\n    y: pandas.DataFrame, feature vector dataframe\n    n_sample: int, number of newly generated sample\n\n    return\n    new_X: pandas.DataFrame, augmented feature vector data\n    target: pandas.DataFrame, augmented target vector data\n    \"\"\"\n    indices2 = nearest_neighbour(X)\n    n = len(indices2)\n    new_X = np.zeros((n_sample, X.shape[1]))\n    target = np.zeros((n_sample, y.shape[1]))\n    for i in range(n_sample):\n        reference = random.randint(0,n-1)\n        neighbour = random.choice(indices2[reference,1:])\n        all_point = indices2[reference]\n        nn_df = y[y.index.isin(all_point)]\n        ser = nn_df.sum(axis = 0, skipna = True)\n        target[i] = np.array([1 if val>2 else 0 for val in ser])\n        ratio = random.random()\n        gap = X.loc[reference,:] - X.loc[neighbour,:]\n        new_X[i] = np.array(X.loc[reference,:] + ratio * gap)\n    new_X = pd.DataFrame(new_X, columns=X.columns)\n    target = pd.DataFrame(target, columns=y.columns)\n    new_X = pd.concat([X, new_X], axis=0)\n    target = pd.concat([y, target], axis=0)\n    return new_X, target\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"    X_sub, y_sub = get_minority_instace(X_train, y_train)   #Getting minority instance of that datframe\n    X_res,y_res =MLSMOTE(X_sub, y_sub, 1000)     #Applying MLSMOTE to augment the dataframe\n    print(y_res.sum(),y_train.sum())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"classifier = ClassifierChain(RandomForestClassifier(class_weight='balanced'))\nclassifier.fit(X_res, y_res)\n\npredictions = classifier.predict(X_test)\nprint(classification_report(y_test,predictions))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}