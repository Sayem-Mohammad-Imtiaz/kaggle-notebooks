{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1. Load the dataset.\nThe original data was saved with an index, so specify this when loading."},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/foreign-exchange-rates-per-dollar-20002019/Foreign_Exchange_Rates.csv', index_col=[0])\ndata.tail()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2. Rename the columns.\nAn Exchange Rate is just a price - the price (in local currency) for 1 unit of foreign currency.  For example, on 2000-01-03 the price of USD1 was AUD1.5172, ie. it cost an Australian AUD1.5172 to buy one unit of US currency.  All of the prices in the dataset have been expressed this way, ie. the cost (in local) to buy 1 US dollar.  Consequently, we can simplify the column headings to be the 3-character ISO curreny code."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data.rename({'Time Serie':'COBDate',\n           'AUSTRALIA - AUSTRALIAN DOLLAR/US$':'AUD',\n           'EURO AREA - EURO/US$':'EUR',\n           'NEW ZEALAND - NEW ZELAND DOLLAR/US$':'NZD',\n           'UNITED KINGDOM - UNITED KINGDOM POUND/US$':'GBP',\n           'BRAZIL - REAL/US$':'BRL',\n           'CANADA - CANADIAN DOLLAR/US$':'CAD',\n           'CHINA - YUAN/US$':'CNY',\n           'HONG KONG - HONG KONG DOLLAR/US$':'HKD',\n           'INDIA - INDIAN RUPEE/US$':'INR',\n           'KOREA - WON/US$':'KRW',\n           'MEXICO - MEXICAN PESO/US$':'MXN',\n           'SOUTH AFRICA - RAND/US$':'ZAR',\n           'SINGAPORE - SINGAPORE DOLLAR/US$':'SGD',\n           'DENMARK - DANISH KRONE/US$':'DKK',\n           'JAPAN - YEN/US$':'JPY',\n           'MALAYSIA - RINGGIT/US$': 'MYR',\n           'NORWAY - NORWEGIAN KRONE/US$':'NOK',\n           'SWEDEN - KRONA/US$':'SEK',\n           'SRI LANKA - SRI LANKAN RUPEE/US$':'LKR',\n           'SWITZERLAND - FRANC/US$':'CHF',\n           'TAIWAN - NEW TAIWAN DOLLAR/US$':'TWD',\n           'THAILAND - BAHT/US$':'THB',\n          }, axis='columns', inplace=True)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3. Remove rows with missing data and adjust datatypes.\nThis dataset contains an 'ND' string to indicate missing data, so ensure those rows are removed as well.  Given 'ND' was present, all numeric values will have been cast as strings ('object' in pandas) so this needs to be corrected."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(data.shape)\n\ndata.dropna(inplace=True)\ndata = data[~data.eq('ND').any(1)]\n\nprint(data.shape)\n\nprint(data.dtypes)\n\nfor column in data.columns:\n    if column == 'COBDate':\n        data[column] = data[column].astype('datetime64')\n    else:\n        data[column] = data[column].astype('float64')\n        \nprint(data.dtypes)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4. Reduce the dataset to the areas of interest.\nFor the task at hand I'm onky going to look at the currencies of the USA's top 4 trading partners, as revealed by the US Census Bureau Statistics (https://www.census.gov/foreign-trade/statistics/highlights/top/index.html)' ie. CAD, MXN, CNY and JPY.  I will be looking to see if three of these exchange rates (MXN' CNY and JPY) can predicted the exchange rate of its major trading partner, CAD.\n\nAdditionally, I will restrict the dataset to the most recent calendar year (2019) so that the results cover the most recent time period."},{"metadata":{"trusted":true},"cell_type":"code","source":"predictors = ['MXN','CNY','JPY']\npredicted  = 'CAD'\n\n## Uncomment the below statement if analysing all (non-predicted) currencies.\n#predictors = [ 'AUD', 'EUR', 'NZD', 'GBP', 'BRL', 'CNY', 'HKD',\n#       'INR', 'KRW', 'MXN', 'ZAR', 'SGD', 'DKK', 'JPY', 'MYR', 'NOK', 'SEK',\n#       'LKR', 'CHF', 'TWD', 'THB']\n\ndata = data[data.COBDate >= '2019-01-01'].reset_index(drop=True)\nprint(data.shape)\ndata = pd.concat([data[predictors], data[predicted]], axis='columns').reset_index(drop=True)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5. Plot the data.\n\nWe can visualize the data by plotting individual predictors against the predicted currency and see if there are any discernible patterns."},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndef chart_it(x,y):\n    plt.scatter(x, y)\n    plt.xlabel(x.name)\n    plt.ylabel(y.name)\n    plt.title('Relationship between ' + y.name + ' and ' + x.name)\n    plt.show()\n\n[chart_it(data[column], data[predicted]) for column in predictors];","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 6. Perform Principal Component Analysis (PCA) to investigate the possibility of reducing the number of predictor dimensions.\nWe can use a Scree Plot and observe the 'elbow' to see where we may cut-off our dimensions. "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\nfrom sklearn.preprocessing import MinMaxScaler\n\nscaled = MinMaxScaler().fit_transform(data[predictors])\nn = scaled.shape[1]\npca = PCA(n_components = n)\npca.fit(scaled)\n\nx = list(range(1,n+1))\ny = list(pca.explained_variance_ratio_)\nplt.plot(x, y, 'o-', linewidth=1)\nplt.xticks(x, x)\nplt.xlabel('Principal Component')\nplt.yticks(np.arange(0,1.1,step=0.1), np.arange(0,110,step=10))\nplt.ylabel('Percentage of Variance Explained')\nplt.title('Scree Plot')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 7. Examine Projected Predictors' relationships with predicted variable.\nSciKit-Learn's X_new = pca.fit_transform(X) method projects (the higher dimension data) X onto the lower-dimensional space (X_new).  By plotting each projection in X_new against the predicted variable, we can examine the suitability of each projection (a.k.a. Principal Component) against the regressor variable."},{"metadata":{"trusted":true},"cell_type":"code","source":"def chart_it(x,y, chart_type='scatter'):\n    plt.scatter(x, y)\n    plt.xlabel(x.name)\n    plt.ylabel(y.name)\n    plt.title('Relationship between ' + y.name + ' and ' + x.name)\n    plt.xticks(np.arange(-1, 1.2, step=0.2))\n    plt.show()\n\nprojections = pca.fit_transform(scaled)\nfor i in range(projections.shape[1]):\n    data['PC' + str(i+1)] = projections[:,i]\n    chart_it(data['PC' + str(i+1)], data[predicted])","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}