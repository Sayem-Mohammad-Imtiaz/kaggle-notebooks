{"cells":[{"metadata":{},"cell_type":"markdown","source":"# In this article, we will look at some common techniques to structure your CNN model"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import tensorflow as tf\nimport keras\nfrom keras.callbacks import *\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom keras.models import Sequential\nfrom keras.layers import *","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"traindata = pd.read_csv('../input/sign-language-mnist/sign_mnist_train/sign_mnist_train.csv')\ntrainlabel=traindata['label'].values\ntraindata.drop('label',inplace=True,axis=1)\ntrainimages = traindata.values\ntrainimages=trainimages.reshape(-1,28,28,1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"testdata = pd.read_csv('../input/sign-language-mnist/sign_mnist_test/sign_mnist_test.csv')\ntestlabel=testdata['label'].values\ntestdata.drop('label',inplace=True,axis=1)\ntestimages = testdata.values\ntestimages=testimages.reshape(-1,28,28,1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.image import ImageDataGenerator","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"traingen=ImageDataGenerator(rescale=1/255.0,validation_split=0.2)\ntraindata_generator = traingen.flow(trainimages,trainlabel,batch_size=64, subset='training')\nvalidationdata_generator = traingen.flow(trainimages,trainlabel,batch_size=64,subset='validation')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"testgen=ImageDataGenerator(rescale=1/255.0)\ntestdata_generator = testgen.flow(testimages,testlabel)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model=Sequential([])\n\nmodel.add(Conv2D(64,(3,3),activation=\"relu\",input_shape=(28,28,1)))\nmodel.add(MaxPooling2D(2,2))\n\nmodel.add(Conv2D(128,(3,3),activation=\"relu\"))\nmodel.add(MaxPooling2D(2,2))\n\nmodel.add(Flatten())\nmodel.add(Dense(256,activation=\"relu\"))\nmodel.add(Dense(25,activation=\"softmax\"))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(loss=\"sparse_categorical_crossentropy\",optimizer='adam',metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history0=model.fit(traindata_generator,epochs=15,validation_data=validationdata_generator)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"acc = history0.history['accuracy']\nval_acc = history0.history['val_accuracy']\nloss = history0.history['loss']\nval_loss = history0.history['val_loss']\n\nepochs = range(len(acc))\n\nplt.plot(epochs, acc, 'r', label='Training accuracy')\nplt.plot(epochs, val_acc, 'b', label='Validation accuracy')\nplt.title('Training and validation accuracy')\nplt.legend()\nplt.figure()\n\nplt.plot(epochs, loss, 'r', label='Training Loss')\nplt.plot(epochs, val_loss, 'b', label='Validation Loss')\nplt.title('Training and validation loss')\nplt.legend()\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"test accuracy: \"+ str(model.evaluate_generator(testdata_generator)[1]*100))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Training and Validation Accuracy: 100\n#### Test Accuracy: 93.2\n#### Problem: Overfitting Solution: Data Augmentation"},{"metadata":{},"cell_type":"markdown","source":"# Model Using Data Augmentation"},{"metadata":{"trusted":true},"cell_type":"code","source":"traingen=ImageDataGenerator(rotation_range=20,zoom_range=0.1,width_shift_range=0.1,height_shift_range=0.1,\n                  shear_range=0.1,horizontal_flip=True,rescale=1/255.0,validation_split=0.2)\ntraindata_generator = traingen.flow(trainimages,trainlabel,batch_size=128,subset='training')\nvalidationdata_generator = traingen.flow(trainimages,trainlabel,batch_size=128,subset='validation')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"testgen=ImageDataGenerator(rescale=1/255.0)\ntestdata_generator = testgen.flow(testimages,testlabel)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model=Sequential([])\n\nmodel.add(Conv2D(64,(3,3),activation=\"relu\",input_shape=(28,28,1)))\nmodel.add(MaxPooling2D(2,2))\n\nmodel.add(Conv2D(128,(3,3),activation=\"relu\"))\nmodel.add(MaxPooling2D(2,2))\n\nmodel.add(Flatten())\nmodel.add(Dense(256,activation=\"relu\"))\nmodel.add(Dense(25,activation=\"softmax\"))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(loss=\"sparse_categorical_crossentropy\",optimizer='adam',metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history1=model.fit(traindata_generator,epochs=40,validation_data=validationdata_generator)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"acc = history1.history['accuracy']\nval_acc = history1.history['val_accuracy']\nloss = history1.history['loss']\nval_loss = history1.history['val_loss']\n\nepochs = range(len(acc))\n\nplt.plot(epochs, acc, 'r', label='Training accuracy')\nplt.plot(epochs, val_acc, 'b', label='Validation accuracy')\nplt.title('Training and validation accuracy')\nplt.legend()\nplt.figure()\n\nplt.plot(epochs, loss, 'r', label='Training Loss')\nplt.plot(epochs, val_loss, 'b', label='Validation Loss')\nplt.title('Training and validation loss')\nplt.legend()\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"test accuracy: \"+ str(model.evaluate_generator(testdata_generator)[1]*100))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Train Accuracy: 98.2 Test Accuracy: 98.5\n### If we train for more epochs, the accuracy will increase again and loss will converge. But is there any Solution, to achieve same accuracy in less epoch.\n### Yes, Batch Normalisation allows to train the same model in less epoch\n"},{"metadata":{},"cell_type":"markdown","source":"# Batch Normalisation"},{"metadata":{"trusted":true},"cell_type":"code","source":"model=Sequential([])\n\nmodel.add(Conv2D(64,(3,3),activation=\"relu\",input_shape=(28,28,1)))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(2,2))\n\nmodel.add(Conv2D(128,(3,3),activation=\"relu\"))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(2,2))\n\n\nmodel.add(Flatten())\nmodel.add(BatchNormalization())\nmodel.add(Dense(256,activation=\"relu\"))\nmodel.add(BatchNormalization())\nmodel.add(Dense(25,activation=\"softmax\"))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(loss=\"sparse_categorical_crossentropy\",optimizer='adam',metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history2=model.fit(traindata_generator,epochs=40,validation_data=validationdata_generator)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"acc = history2.history['accuracy']\nval_acc = history2.history['val_accuracy']\nloss = history2.history['loss']\nval_loss = history2.history['val_loss']\n\nepochs = range(len(acc))\n\nplt.plot(epochs, acc, 'r', label='Training accuracy')\nplt.plot(epochs, val_acc, 'b', label='Validation accuracy')\nplt.title('Training and validation accuracy')\nplt.legend()\nplt.figure()\n\nplt.plot(epochs, loss, 'r', label='Training Loss')\nplt.plot(epochs, val_loss, 'b', label='Validation Loss')\nplt.title('Training and validation loss')\nplt.legend()\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"test accuracy: \"+ str(model.evaluate_generator(testdata_generator)[1]*100))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Training Accuracy:99.5 Test Accuracy: 98.3\n### But the validation loss graph has not converged as compared Training loss, this means there is uncertainity in the model.\n### Lets run it again to understand what exactly it is"},{"metadata":{"trusted":true},"cell_type":"code","source":"model=Sequential([])\n\nmodel.add(Conv2D(64,(3,3),activation=\"relu\",input_shape=(28,28,1)))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(2,2))\n\nmodel.add(Conv2D(128,(3,3),activation=\"relu\"))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(2,2))\n\n\nmodel.add(Flatten())\nmodel.add(BatchNormalization())\nmodel.add(Dense(256,activation=\"relu\"))\nmodel.add(BatchNormalization())\nmodel.add(Dense(25,activation=\"softmax\"))\n\nmodel.compile(loss=\"sparse_categorical_crossentropy\",optimizer='adam',metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history3=model.fit(traindata_generator,epochs=40,validation_data=validationdata_generator)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"acc = history3.history['accuracy']\nval_acc = history3.history['val_accuracy']\nloss = history3.history['loss']\nval_loss = history3.history['val_loss']\n\nepochs = range(len(acc))\n\nplt.plot(epochs, acc, 'r', label='Training accuracy')\nplt.plot(epochs, val_acc, 'b', label='Validation accuracy')\nplt.title('Training and validation accuracy')\nplt.legend()\nplt.figure()\n\nplt.plot(epochs, loss, 'r', label='Training Loss')\nplt.plot(epochs, val_loss, 'b', label='Validation Loss')\nplt.title('Training and validation loss')\nplt.legend()\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"test accuracy: \"+ str(model.evaluate_generator(testdata_generator)[1]*100))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### We can see there is high variance in validation and test accuracy, based on where we end up in last epoch, the accuracy for the model may be great or worse. \n\n### This is happening because of large learning rate which is causing to overshoot the optima during last phases of training. We can use Decaying Learning Rate, which reduces learning rate after each epoch, hence allowing the graph to converge more smoothly."},{"metadata":{},"cell_type":"markdown","source":"# Decaying Learning Rate"},{"metadata":{"trusted":true},"cell_type":"code","source":"model=Sequential([])\n\nmodel.add(Conv2D(64,(3,3),activation=\"relu\",input_shape=(28,28,1)))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(2,2))\n\nmodel.add(Conv2D(128,(3,3),activation=\"relu\"))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(2,2))\n\n\nmodel.add(Flatten())\nmodel.add(BatchNormalization())\nmodel.add(Dense(256,activation=\"relu\"))\nmodel.add(BatchNormalization())\nmodel.add(Dense(25,activation=\"softmax\"))\n\nmodel.compile(loss=\"sparse_categorical_crossentropy\",optimizer='adam',metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"decaylr = LearningRateScheduler(lambda x: 1e-3 * 0.9 ** x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history4=model.fit(traindata_generator,epochs=40,validation_data=validationdata_generator,callbacks=[decaylr])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"acc = history4.history['accuracy']\nval_acc = history4.history['val_accuracy']\nloss = history4.history['loss']\nval_loss = history4.history['val_loss']\n\nepochs = range(len(acc))\n\nplt.plot(epochs, acc, 'r', label='Training accuracy')\nplt.plot(epochs, val_acc, 'b', label='Validation accuracy')\nplt.title('Training and validation accuracy')\nplt.legend()\nplt.figure()\n\nplt.plot(epochs, loss, 'r', label='Training Loss')\nplt.plot(epochs, val_loss, 'b', label='Validation Loss')\nplt.title('Training and validation loss')\nplt.legend()\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### All the curves have converged smoothly by end of 15 epochs and hence there is high confidence in the model."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"test accuracy: \"+ str(model.evaluate_generator(testdata_generator)[1]*100))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Test Accuracy is 100%,(though it  might not be the case in every run but it will be almost close to 100% in every run."},{"metadata":{},"cell_type":"markdown","source":"### P.S: There's no need to train the model for 40 epochs, we can use callback to stop training once we reached desired accuracy"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}