{"cells":[{"metadata":{"trusted":true,"_uuid":"bcf244fb2623377e6c472520181f55b570f22225"},"cell_type":"code","source":"import keras\nimport matplotlib\nimport nltk\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"questions = pd.read_csv(\"../input/tweets-with-sarcasm-and-irony/train.csv\")\nquestions.columns = [\"tweets\", \"label\"]\nquestions.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def standardize_text(df, text_field):\n    df[text_field] = df[text_field].str.replace(r\"http\\S+\", \"\")\n    df[text_field] = df[text_field].str.replace(r\"http\", \"\")\n    df[text_field] = df[text_field].str.replace(r\"@\\S+\", \"\")\n    df[text_field] = df[text_field].str.replace(r\"[^A-Za-z0-9(),!?@\\'\\`\\\"\\_\\n]\", \" \")\n    df[text_field] = df[text_field].str.replace(r\"@\", \"at\")\n    df[text_field] = df[text_field].str.lower()\n    return df\n\nquestions = standardize_text(questions, \"tweets\")\n\nquestions.to_csv(\"clean_data.csv\")\nquestions.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clean_questions = pd.read_csv(\"clean_data.csv\")\n\nclean_questions.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clean_questions.groupby(\"label\").count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.tokenize import RegexpTokenizer\n\ntokenizer = RegexpTokenizer(r'\\w+')\n\nclean_questions[\"tokens\"] = clean_questions[\"tweets\"].apply(str).apply(tokenizer.tokenize)\nclean_questions.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils import to_categorical\nall_words = [word for tokens in clean_questions[\"tokens\"] for word in tokens]\nsentence_lengths = [len(tokens) for tokens in clean_questions[\"tokens\"]]\nVOCAB = sorted(list(set(all_words)))\nprint(\"%s words total, with a vocabulary size of %s\" % (len(all_words), len(VOCAB)))\nprint(\"Max sentence length is %s\" % max(sentence_lengths))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n\ndef cv(data):\n    count_vectorizer = CountVectorizer()\n    emb = count_vectorizer.fit_transform(data)\n    return emb, count_vectorizer\n\nlist_corpus = clean_questions[\"tweets\"].astype('U').tolist()\nlist_labels = clean_questions[\"label\"].astype('U').tolist()\n\nX_train, X_test, y_train, y_test = train_test_split(list_corpus, list_labels, test_size = 0.2, random_state=40)\n\nX_train_counts, count_vectorizer = cv(X_train)\nX_test_counts = count_vectorizer.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nclf = LogisticRegression(C=30.0, class_weight='balanced', solver='newton-cg',\nmulti_class='multinomial', n_jobs=-1, random_state=40)\nclf.fit(X_train_counts, y_train)\ny_predicted_counts = clf.predict(X_test_counts)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report\n\ndef get_metrics(y_test, y_predicted):\n    # true positives / (true positives+false positives)\n    precision = precision_score(y_test, y_predicted, pos_label=None,\n    average='weighted')\n    # true positives / (true positives + false negatives)\n    recall = recall_score(y_test, y_predicted, pos_label=None,\n    average='weighted')\n    # harmonic mean of precision and recall\n    f1 = f1_score(y_test, y_predicted, pos_label=None, average='weighted')\n    # true positives + true negatives/ total\n    accuracy = accuracy_score(y_test, y_predicted)\n    return accuracy, precision, recall, f1\n\naccuracy, precision, recall, f1 = get_metrics(y_test, y_predicted_counts)\nprint(\"accuracy = %.3f, precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracy, precision, recall, f1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_most_important_features(vectorizer, model, n=5):\n    index_to_word = {v:k for k,v in vectorizer.vocabulary_.items()}\n    \n    classes ={}\n    for class_index in range(model.coef_.shape[0]):\n        word_importances = [(el, index_to_word[i]) for i,el in enumerate(model.coef_[class_index])]\n        sorted_coeff = sorted(word_importances, key = lambda x : x[0], reverse=True)\n        tops = sorted(sorted_coeff[:n], key = lambda x : x[0])\n        bottom = sorted_coeff[-n:]\n        classes[class_index] = {\n        'tops':tops,\n        'bottom':bottom\n            }\n        return classes\n\nimportance = get_most_important_features(count_vectorizer, clf, 10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"importance = get_most_important_features(count_vectorizer, clf, 10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"importance","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def tfidf(data):\n    tfidf_vectorizer = TfidfVectorizer()\n    train = tfidf_vectorizer.fit_transform(data)\n    return train, tfidf_vectorizer\n\nX_train_tfidf, tfidf_vectorizer = tfidf(X_train)\nX_test_tfidf = tfidf_vectorizer.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_tfidf = LogisticRegression(C=30.0, class_weight='balanced', solver='newton-cg', multi_class='multinomial', n_jobs=-1, random_state=40)\nclf_tfidf.fit(X_train_tfidf, y_train)\ny_predicted_tfidf = clf_tfidf.predict(X_test_tfidf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy_tfidf, precision_tfidf, recall_tfidf, f1_tfidf = get_metrics(y_test, y_predicted_tfidf)\nprint(\"accuracy = %.3f, precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracy_tfidf, precision_tfidf, recall_tfidf, f1_tfidf))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"importance_tfidf = get_most_important_features(tfidf_vectorizer, clf_tfidf, 10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"importance_tfidf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import gensim\n\nword2vec_path = \"../input/google-pretrain-model/GoogleNews-vectors-negative300.bin\"\nword2vec = gensim.models.KeyedVectors.load_word2vec_format(word2vec_path, binary=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_average_word2vec(tokens_list, vector, generate_missing=False, k=300):\n    if len(tokens_list)<1:\n        return np.zeros(k)\n    if generate_missing:\n        vectorized = [vector[word] if word in vector else np.random.rand(k) for word in tokens_list]\n    else:\n        vectorized = [vector[word] if word in vector else np.zeros(k) for word in tokens_list]\n    length = len(vectorized)\n    summed = np.sum(vectorized, axis=0)\n    averaged = np.divide(summed, length)\n    return averaged\n\ndef get_word2vec_embeddings(vectors, clean_questions, generate_missing=False):\n    embeddings = clean_questions['tokens'].apply(lambda x: get_average_word2vec(x, vectors,\n    generate_missing=generate_missing))\n    return list(embeddings)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embeddings = get_word2vec_embeddings(word2vec, clean_questions)\nX_train_word2vec, X_test_word2vec, y_train_word2vec, y_test_word2vec = train_test_split(embeddings, list_labels, test_size=0.2,random_state=40)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_w2v = LogisticRegression(C=30.0, class_weight='balanced', solver='newton-cg',multi_class='multinomial', random_state=40)\nclf_w2v.fit(X_train_word2vec, y_train_word2vec)\n\ny_predicted_word2vec = clf_w2v.predict(X_test_word2vec)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy_word2vec, precision_word2vec, recall_word2vec, f1_word2vec = get_metrics(y_test_word2vec, y_predicted_word2vec)\nprint(\"accuracy = %.3f, precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracy_word2vec, precision_word2vec, recall_word2vec, f1_word2vec))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\n\ncm = confusion_matrix(y_test, y_predicted_counts)\ncm2 = confusion_matrix(y_test, y_predicted_tfidf)\ncm_w2v = confusion_matrix(y_test_word2vec, y_predicted_word2vec)\n\nprint(\"Word2Vec confusion matrix\")\nprint(cm_w2v)\nprint(\"TFIDF confusion matrix\")\nprint(cm2)\nprint(\"BoW confusion matrix\")\nprint(cm)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}