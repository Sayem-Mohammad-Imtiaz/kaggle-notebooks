{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Importing The Packages"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom nltk.stem import WordNetLemmatizer\nimport nltk\nfrom nltk.corpus import stopwords \nfrom nltk.tokenize import word_tokenize \nimport re\nimport spacy\nfrom nltk.corpus import sentiwordnet as swn\nfrom IPython.display import clear_output\nimport plotly.express as px\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\nimport plotly\nplotly.offline.init_notebook_mode (connected = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Importing The Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"data=pd.read_csv('../input/trip-advisor-hotel-reviews/tripadvisor_hotel_reviews.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Having a look at the data"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"data=data[['Review']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preprocessing The Data"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Function to preprocess the hotel data\ndef preprocess_hotel_data(data,name):\n    # Proprocessing the data\n    data[name]=data[name].str.lower()\n    # Code to remove the Hashtags from the text\n    data[name]=data[name].apply(lambda x:re.sub(r'\\B#\\S+','',x))\n    # Code to remove the links from the text\n    data[name]=data[name].apply(lambda x:re.sub(r\"http\\S+\", \"\", x))\n    # Code to remove the Special characters from the text \n    data[name]=data[name].apply(lambda x:' '.join(re.findall(r'\\w+', x)))\n    # Code to substitute the multiple spaces with single spaces\n    data[name]=data[name].apply(lambda x:re.sub(r'\\s+', ' ', x, flags=re.I))\n    # Code to remove all the single characters in the text\n    data[name]=data[name].apply(lambda x:re.sub(r'\\s+[a-zA-Z]\\s+', '', x))\n    # Remove the hotel handlers\n    data[name]=data[name].apply(lambda x:re.sub('@[^\\s]+','',x))\n\n# Function to tokenize and remove the stopwords    \ndef rem_stopwords_tokenize(data,name):\n      \n    def getting(sen):\n        example_sent = sen\n\n        stop_words = set(stopwords.words('english')) \n\n        word_tokens = word_tokenize(example_sent) \n\n        filtered_sentence = [w for w in word_tokens if not w in stop_words] \n\n        filtered_sentence = [] \n\n        for w in word_tokens: \n            if w not in stop_words: \n                filtered_sentence.append(w) \n        return filtered_sentence\n    x=[]\n    for i in data[name].values:\n        x.append(getting(i))\n    data[name]=x\n# Making a function to lemmatize all the words\nlemmatizer = WordNetLemmatizer() \ndef lemmatize_all(data,name):\n    arr=data[name]\n    a=[]\n    for i in arr:\n        b=[]\n        for j in i:\n            x=lemmatizer.lemmatize(j,pos='a')\n            x=lemmatizer.lemmatize(x)\n            b.append(x)\n        a.append(b)\n    data[name]=a\n# Function to make it back into a sentence \ndef make_sentences(data,name):\n    data[name]=data[name].apply(lambda x:' '.join([i+' ' for i in x]))\n    # Removing double spaces if created\n    data[name]=data[name].apply(lambda x:re.sub(r'\\s+', ' ', x, flags=re.I))\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Using the preprocessing function to preprocess the hotel data\npreprocess_hotel_data(data,'Review')\n# Using tokenizer and removing the stopwords\nrem_stopwords_tokenize(data,'Review')\n# Converting all the texts back to sentences\nmake_sentences(data,'Review')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Using spacy to Position of a words and working it with sentiwordnet"},{"metadata":{},"cell_type":"markdown","source":"So when we are working with sentiwordnet we need to know the characterstic of the word for which we want to know the sentiment . So for finding that position of the word here we are gonna use Spacy.pos_ which tells us about the position of the word which then is used to get the sentiment using the sentiwordnet . We then average out the score for both the positive and the negative score from the whole sentence .\nThe positions compatible with the sentiwordnet are:\n* n - NOUN\n* v - VERB\n* a - ADJECTIVE\n* s - ADJECTIVE SATELLITE\n* r - ADVERB"},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\nimport ssl\nfrom nltk.corpus import wordnet as wn\nfrom nltk.corpus import sentiwordnet as swn\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\npos=neg=obj=count=0\n\npostagging = []\n\nfor review in data['Review']:\n    list = word_tokenize(review)\n    postagging.append(nltk.pos_tag(list))\n\ndata['pos_tags'] = postagging\n\ndef penn_to_wn(tag):\n    if tag.startswith('J'):\n        return wn.ADJ\n    elif tag.startswith('N'):\n        return wn.NOUN\n    elif tag.startswith('R'):\n        return wn.ADV\n    elif tag.startswith('V'):\n        return wn.VERB\n    return None\n\n\n# Returns list of pos-neg and objective score. But returns empty list if not present in senti wordnet.\ndef get_sentiment(word,tag):\n    wn_tag = penn_to_wn(tag)\n    \n    if wn_tag not in (wn.NOUN, wn.ADJ, wn.ADV):\n        return []\n\n    #Lemmatization\n    lemma = lemmatizer.lemmatize(word, pos=wn_tag)\n    if not lemma:\n        return []\n\n    #Synset is a special kind of a simple interface that is present in NLTK to look up words in WordNet. \n    #Synset instances are the groupings of synonymous words that express the same concept. \n    #Some of the words have only one Synset and some have several.\n    synsets = wn.synsets(word, pos=wn_tag)\n    if not synsets:\n        return []\n\n    # Take the first sense, the most common\n    synset = synsets[0]\n    swn_synset = swn.senti_synset(synset.name())\n\n    return [synset.name(), swn_synset.pos_score(),swn_synset.neg_score(),swn_synset.obj_score()]\n\n    pos=neg=obj=count=0\nsenti_score = []\n\nfor pos_val in data['pos_tags']:\n    senti_val = [get_sentiment(x,y) for (x,y) in pos_val]\n    for score in senti_val:\n        try:\n            pos = pos + score[1]  #positive score is stored at 2nd position\n            neg = neg + score[2]  #negative score is stored at 3rd position\n        except:\n            continue\n    senti_score.append(pos - neg)\n    pos=neg=0    \n    \ndata['senti_score'] = senti_score\nprint(data['senti_score'])\n\nprint(data.head)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"overall=[]\nfor i in range(len(data)):\n    if data['senti_score'][i]>= 0.05:\n        overall.append('Positive')\n    elif data['senti_score'][i]<= -0.05:\n        overall.append('Negative')\n    else:\n        overall.append('Neutral')\ndata['Overall Sentiment']=overall","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nsns.countplot(data['Overall Sentiment'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The following code creates a word-document matrix.\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nvec = CountVectorizer()\nX = vec.fit_transform(data['Review'])\ndf = pd.DataFrame(X.toarray(), columns = vec.get_feature_names())\ndf.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Creating a python object of the class CountVectorizer\n\nbow_counts = CountVectorizer(tokenizer= word_tokenize, # type of tokenization\n                             ngram_range=(1,1)) # number of n-grams\n\nbow_data = bow_counts.fit_transform(data['Review'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train_bow, X_test_bow, y_train_bow, y_test_bow = train_test_split(bow_data, # Features\n                                                                    data['Overall Sentiment'], # Target variable\n                                                                    test_size = 0.2, # 20% test size\n                                                                    random_state = 0) # random","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n### Training the model \nlr_model_all = LogisticRegression() # Logistic regression\nlr_model_all.fit(X_train_bow, y_train_bow) # Fitting a logistic regression model\n\n## Predicting the output\ntest_pred_lr_all = lr_model_all.predict(X_test_bow) # Class prediction\n\n\n## Calculate key performance metrics\n\nfrom sklearn.metrics import classification_report\n# Print a classification report\nprint(classification_report(y_test_bow,test_pred_lr_all))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}