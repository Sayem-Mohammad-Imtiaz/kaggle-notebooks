{"cells":[{"metadata":{"_uuid":"678f6ee9c0cfa8df59697f7ec3801a3e5ca47ce8"},"cell_type":"markdown","source":"## Ever read a bunch of sentences and wished to reduce it all as themes or categories so as to interpret it and document them easily? Yes, those wishes do have real-life answers to them and that is exactly the topic of this notebook"},{"metadata":{"_uuid":"422da27345dd32740cd7d662f8d36fa7d1a304e9"},"cell_type":"markdown","source":"![](https://github.com/rakash/Scripts/blob/master/bokeh_plot.png?raw=true)"},{"metadata":{"_uuid":"d120f72b29fcfac372334f067b01f2e2689e3b72"},"cell_type":"markdown","source":"## WHAT IS A TOPIC MODEL?"},{"metadata":{"_uuid":"cd8f2531fd7ce9e30cdea1885b65c8e97682b756"},"cell_type":"markdown","source":"*****\nA type of statistical model for discovering the abstract \"topics\" that occur in a collection of documents. A document typically concerns multiple topics in different proportions; thus, in a document that is 10% about cats and 90% about dogs, there would probably be about 9 times more dog words than cat words. The \"topics\" produced by topic modeling techniques are clusters of similar words. A topic model captures this intuition in a mathematical framework, which allows examining a set of documents and discovering, based on the statistics of the words in each, what the topics might be and what each document's balance of topics is.\n*****"},{"metadata":{"_uuid":"b35d86d94b1e1cfb6f1c5e0075704495f8fa575c"},"cell_type":"markdown","source":"![](https://theintelligenceofinformation.files.wordpress.com/2016/12/topic-modeling-for-learning-analytics-researchers-lak15-tutorial-15-638.jpg)"},{"metadata":{"_uuid":"99187c83c79f93582e1e42995725dffa8172a266"},"cell_type":"markdown","source":"### Popular topic modeling algorithms include Latent Semantic Analysis (LSA) a.k.a Latent Semantic Indexing , Hierarchical Dirichlet Process (HDP), Latent Dirichlet Allocation (LDA) and Non-negative Matrix factorization among which LDA has shown great results in practice and therefore widely adopted. We'll look at them all one by one. Lets get to it. "},{"metadata":{"_uuid":"781cb72d2cef9b18b661cd36a0dfa70d844bc514"},"cell_type":"markdown","source":"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   "},{"metadata":{"_uuid":"898ebf97b3efd5ce95a81bdd337d88c21cd08210"},"cell_type":"markdown","source":"1. [Latent Semantic Analysis](#lsa)\n2. [Latent Dirichlet Allocation](#lda)\n3. [Hierarchical Dirichlet Process](#hdp)\n4. [Non-negative Matrix factorization](#nmf)"},{"metadata":{"_uuid":"035cfc246297591467f539d16378e9807251e335"},"cell_type":"markdown","source":"### Importing the important libraries and setting up the important libraries and cleaning datasets before we get into the methods"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nprint(os.listdir(\"../input\"))\n\nimport re\nimport numpy as np\nimport pandas as pd\nfrom pprint import pprint\n\n# Gensim\nimport gensim\nimport gensim.corpora as corpora\nfrom gensim.utils import simple_preprocess\nfrom gensim.models import CoherenceModel, LdaModel, LsiModel, HdpModel\n\n# spacy for lemmatization\nimport spacy\n\n# Plotting tools\nimport pyLDAvis\nimport pyLDAvis.gensim \nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Enable logging for gensim - optional\nimport logging\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n\nimport warnings\nwarnings.filterwarnings(\"ignore\",category=DeprecationWarning)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8d41d3411ea54d7061ae5883a99c7eeb8a395930"},"cell_type":"markdown","source":"### Stopwords are words that are commonly used. Using the popular NLTK package in python, lets import the stopwords in the english language\n### and save it. It'll be used later for modeling purposes"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# NLTK Stop words\n\nfrom nltk.corpus import stopwords\nstop_words = stopwords.words('english')\nstop_words.extend(['from', 'subject', 're', 'edu', 'use','a','about', 'above', 'across'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b05e1c949cf52d5ea94e413374efbc652eda045d"},"cell_type":"markdown","source":"### The extended stopwords list from the scikit learn library"},{"metadata":{"trusted":true,"_uuid":"cde7ba30189b92d775e07a847a1425dc47093c56"},"cell_type":"code","source":"st1= ['after', 'afterwards','again','against', 'all', 'almost','alone','along',\n           'already',\n           'also',\n           'although',\n           'always',\n           'am',\n           'among',\n           'amongst',\n           'amoungst',\n           'amount',\n           'an',\n           'and',\n           'another',\n           'any',\n           'anyhow',\n           'anyone',\n           'anything',\n           'anyway',\n           'anywhere',\n           'are',\n           'around',\n           'as',\n           'at',\n           'back',\n           'be',\n           'became',\n           'because',\n           'become',\n           'becomes',\n           'becoming',\n           'been',\n           'before',\n           'beforehand',\n           'behind',\n           'being',\n           'below',\n           'beside',\n           'besides',\n           'between',\n           'beyond',\n           'bill',\n           'both',\n           'bottom',\n           'but',\n           'by',\n           'call',\n           'can',\n           'cannot',\n           'cant',\n           'co',\n           'con',\n           'could',\n           'couldnt',\n           'cry',\n           'de',\n           'describe',\n           'detail',\n           'do',\n           'done',\n           'down',\n           'due',\n           'during',\n           'each',\n           'eg',\n           'eight',\n           'either',\n           'eleven',\n           'else',\n           'elsewhere',\n           'empty',\n           'enough',\n           'etc',\n           'even',\n           'ever',\n           'every',\n           'everyone',\n           'everything',\n           'everywhere',\n           'except',\n           'few',\n           'fifteen',\n           'fifty',\n           'fill',\n           'find',\n           'fire',\n           'first',\n           'five',\n           'for',\n           'former',\n           'formerly',\n           'forty',\n           'found',\n           'four',\n           'from',\n           'front',\n           'full',\n           'further',\n           'get',\n           'give',\n           'go',\n           'had',\n           'has',\n           'hasnt',\n           'have',\n           'he',\n           'hence',\n           'her',\n           'here',\n           'hereafter',\n           'hereby',\n           'herein',\n           'hereupon',\n           'hers',\n           'herself',\n           'him',\n           'himself',\n           'his',\n           'how',\n           'however',\n           'hundred',\n           'i',\n           'ie',\n           'if',\n           'in',\n           'inc',\n           'indeed',\n           'interest',\n           'into',\n           'is',\n           'it',\n           'its',\n           'itself',\n           'keep',\n           'last',\n           'latter',\n           'latterly',\n           'least',\n           'less',\n           'ltd',\n           'made',\n           'many',\n           'may',\n           'me',\n           'meanwhile',\n           'might',\n           'mill',\n           'mine',\n           'more',\n           'moreover',\n           'most',\n           'mostly',\n           'move',\n           'much',\n           'must',\n           'my',\n           'myself',\n           'name',\n           'namely',\n           'neither',\n           'never',\n           'nevertheless',\n           'next',\n           'nine',\n           'no',\n           'nobody',\n           'none',\n           'noone',\n           'nor',\n           'not',\n           'nothing',\n           'now',\n           'nowhere',\n           'of',\n           'off',\n           'often',\n           'on',\n           'once',\n           'one',\n           'only',\n           'onto',\n           'or',\n           'other',\n           'others',\n           'otherwise',\n           'our',\n           'ours',\n           'ourselves',\n           'out',\n           'over',\n           'own',\n           'part',\n           'per',\n           'perhaps',\n           'please',\n           'put',\n           'rather',\n           're',\n           'same',\n           'see',\n           'seem',\n           'seemed',\n           'seeming',\n           'seems',\n           'serious',\n           'several',\n           'she',\n           'should',\n           'show',\n           'side',\n           'since',\n           'sincere',\n           'six',\n           'sixty',\n           'so',\n           'some',\n           'somehow',\n           'someone',\n           'something',\n           'sometime',\n           'sometimes',\n           'somewhere',\n           'still',\n           'such',\n           'system',\n           'take',\n           'ten',\n           'than',\n           'that',\n           'the',\n           'their',\n           'them',\n           'themselves',\n           'then',\n           'thence',\n           'there',\n           'thereafter',\n           'thereby',\n           'therefore',\n           'therein',\n           'thereupon',\n           'these',\n           'they',\n           'thick',\n           'thin',\n           'third',\n           'this',\n           'those',\n           'though',\n           'three',\n           'through',\n           'throughout',\n           'thru',\n           'thus',\n           'to',\n           'together',\n           'too',\n           'top',\n           'toward',\n           'towards',\n           'twelve',\n           'twenty',\n           'two',\n           'un',\n           'under',\n           'until',\n           'up',\n           'upon',\n           'us',\n           'very',\n           'via',\n           'was',\n           'we',\n           'well',\n           'were',\n           'what',\n           'whatever',\n           'when',\n           'whence',\n           'whenever',\n           'where',\n           'whereafter',\n           'whereas',\n           'whereby',\n           'wherein',\n           'whereupon',\n           'wherever',\n           'whether',\n           'which',\n           'while',\n           'whither',\n           'who',\n           'whoever',\n           'whole',\n           'whom',\n           'whose',\n           'why',\n           'will',\n           'with',\n           'within',\n           'without',\n           'would',\n           'yet',\n           'you',\n           'your',\n           'yours',\n           'yourself',\n           'yourselves']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"91be8de47f1c1a0c77933d88eeb0027797dd82b3"},"cell_type":"code","source":"stop_words.extend(st1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"36887b8961bd42c8867c2b3e939f3d5bd25884b1"},"cell_type":"code","source":"data = pd.read_csv(\"../input/uci-news-aggregator.csv\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b28ca0b9076329e7a3c8a3594cd7cd86bdc01cf8"},"cell_type":"markdown","source":"### Selecting sample rows from all categories(in this case, topics) so computations are quicker. "},{"metadata":{"trusted":true,"_uuid":"f6a59d7682cef374a895fa71b1d022de7fbd7388"},"cell_type":"code","source":"bg = data[data.CATEGORY == 'b']\ntg = data[data.CATEGORY == 't']\neg = data[data.CATEGORY == 'e']\nmg = data[data.CATEGORY == 'm']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"eec3cd78c856201bacc51c9ea6ea3f0b205e17a9"},"cell_type":"code","source":"bg_rows = np.random.choice(bg.index.values, 750)\nbg_data = bg.ix[bg_rows]\n\ntg_rows = np.random.choice(tg.index.values, 750)\ntg_data = tg.ix[tg_rows]\n\neg_rows = np.random.choice(eg.index.values, 750)\neg_data = eg.ix[eg_rows]\n\nmg_rows = np.random.choice(mg.index.values, 750)\nmg_data = mg.ix[mg_rows]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c7c088ddd75f069b497c56ef25a7bbdddf592792"},"cell_type":"code","source":"data = bg_data.append([tg_data, eg_data, mg_data])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"145cf4c39456fb69b226c04f7e843bf31419f03d"},"cell_type":"code","source":"title = data['TITLE']\ncategory = data['CATEGORY']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8489dad614210dfe86f9ef55879ecbd7e1dc02c3"},"cell_type":"markdown","source":"### Now that we have cleaned and set the variables that will be needed for the algorithms, lets dive into the 1st type, "},{"metadata":{"_uuid":"7fc23415b99a94b1a83b75909add8178048a0bf5"},"cell_type":"markdown","source":"<a id='lsa'></a>\n\n# Latent Semantic Analysis"},{"metadata":{"_uuid":"943067c0bd4a883b91a50d2f8523e4f1ae915233"},"cell_type":"markdown","source":"![](https://nlp.stanford.edu/IR-book/html/htmledition/img1822.png)"},{"metadata":{"_uuid":"887ae92f25651473031e28ab28f98c7707bf974d"},"cell_type":"markdown","source":"**Latent Semantic Analysis (LSA) is a mathematical method that tries to bring out latent relationships within a collection of documents on to a lower dimensional space. LSA assumes that words that are close in meaning will occur in similar pieces of text (the distributional hypothesis). A matrix containing word counts per paragraph (rows represent unique words and columns represent each paragraph) is constructed from a large piece of text and a mathematical technique called singular value decomposition (SVD) is used to reduce the number of rows while preserving the similarity structure among columns.  Rather than looking at each document isolated from the others it looks at all the documents as a whole and the terms within them to identify relationships.**"},{"metadata":{"_uuid":"61eef27222e56c4b505b5e0e6edc6d7d43b5df86"},"cell_type":"markdown","source":"Singular value decomposition can be used to solve the low-rank matrix approximation problem. We then derive from it an application to approximating term-document matrices. We invoke the following three-step procedure to this end:\n***\n * Given $C$, construct its SVD in the form $C=U\\Sigma V^T$.\n * Derive from $\\Sigma$ the matrix $\\Sigma_k$ formed by replacing by zeros the $r-k$ smallest singular values on the diagonal of $\\Sigma$.\n * Compute and output $C_k$= $U\\Sigma_k V^T$ as the rank-$k$ approximation to $C$.;\n\nWhere C is the term-document matrix and $U$, $\\Sigma$ and $V^T$ are SVD computed matrices.\n***"},{"metadata":{"_uuid":"c1007878014bf052f27f31a79bf66731d7e24b26"},"cell_type":"markdown","source":"**Done with theory, on to Python now!  For LSI,  i'll be using scikit learn module**\n\n**Importing modules from scikit learn and countvector-izing them first. ** \n\n**In Scikit learn, Dimensionality reduction for LSA is done using truncated SVD (aka LSA).**"},{"metadata":{"trusted":true,"_uuid":"a587ff06982e5f228c1f21d02b5e1f20f6f784d9"},"cell_type":"code","source":"from sklearn.decomposition import NMF, LatentDirichletAllocation, TruncatedSVD\nfrom sklearn.feature_extraction.text import CountVectorizer\n \nNUM_TOPICS = 4\n\n# Converting the document to a matrix of token counts\n\nvectorizer = CountVectorizer(min_df=5, max_df=0.9, \n                             stop_words='english', lowercase=True, \n                             token_pattern='[a-zA-Z\\-][a-zA-Z\\-]{2,}')\ndata_vectorized = vectorizer.fit_transform(title)\n \n# Build a Latent Semantic Indexing Model using SVD\n\nlsi_model = TruncatedSVD(n_components=NUM_TOPICS)\nlsi_Z = lsi_model.fit_transform(data_vectorized)\nprint(lsi_Z.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"691c18ad6f8bdab70efb1c6870fa9519f083aa5b"},"cell_type":"code","source":"def print_topics(model, vectorizer, top_n=10):\n    for idx, topic in enumerate(model.components_):\n        print(\"Topic %d:\" % (idx))\n        print([(vectorizer.get_feature_names()[i], topic[i])\n                        for i in topic.argsort()[:-top_n - 1:-1]])\n \nprint(\"LSI Model:\")\nprint_topics(lsi_model, vectorizer)\nprint(\"=\" * 20)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1b2d26ad142fd8025bd31fa70976f23957ad8b98"},"cell_type":"markdown","source":"### above are the topics by the LSI model. "},{"metadata":{"trusted":true,"_uuid":"e05e420fca4e7602e2118be03f2412a8ea0a897d"},"cell_type":"code","source":"from sklearn.manifold import TSNE\n# NLTK\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk.stem.snowball import SnowballStemmer\nfrom nltk.corpus import stopwords\nimport re\n\n# Visualization\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nimport matplotlib\n%matplotlib inline\nimport seaborn as sns\n\n# Bokeh\nfrom bokeh.io import output_notebook\nfrom bokeh.plotting import figure, show\nfrom bokeh.models import HoverTool, CustomJS, ColumnDataSource, Slider\nfrom bokeh.layouts import column\nfrom bokeh.palettes import all_palettes\noutput_notebook()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"aff159623d6df59ee2363bc4b6f548b40fcf7782"},"cell_type":"markdown","source":"### Lets now visualise the LSI model and see the words and documents that are close to each other according to the model"},{"metadata":{"_uuid":"31923dd5ef9b6f73ca74e3e455fe570e7b0ff696"},"cell_type":"markdown","source":"### Documents"},{"metadata":{"trusted":true,"_uuid":"03d04a8a51728b3dcda1d8bfb862f985fd9fc7d4"},"cell_type":"code","source":"import pandas as pd\nfrom bokeh.io import push_notebook, show, output_notebook\nfrom bokeh.plotting import figure\nfrom bokeh.models import ColumnDataSource, LabelSet\noutput_notebook()\n\nsvd = TruncatedSVD(n_components=100)\ndocuments_2d = svd.fit_transform(data_vectorized)\n \ndf = pd.DataFrame(columns=['x', 'y', 'document'])\ndf['x'], df['y'], df['document'] = documents_2d[:,0], documents_2d[:,1], range(len(data))\n \nsource = ColumnDataSource(ColumnDataSource.from_df(df))\nlabels = LabelSet(x=\"x\", y=\"y\", text=\"document\", y_offset=8,\n                  text_font_size=\"8pt\", text_color=\"#555555\",\n                  source=source, text_align='center')\n \nplot = figure(plot_width=600, plot_height=600)\nplot.circle(\"x\", \"y\", size=12, source=source, line_color=\"black\", fill_alpha=0.8)\nplot.add_layout(labels)\nshow(plot, notebook_handle=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9b26b839dd55adbbb9b100830db24c89bf352e00"},"cell_type":"markdown","source":"### The Inference from the plot above is that similar documents are plotted close by in a low dimensional space. Same with words(although not the objective here) shown below, "},{"metadata":{"_uuid":"73260fcc92e3c8d0e7d989258d70e2eb15a3c8d1"},"cell_type":"markdown","source":"### Words"},{"metadata":{"trusted":true,"_uuid":"cab030f9316b8ea0e4e5504407051ff9899f91e0"},"cell_type":"code","source":"svd = TruncatedSVD(n_components=100)\nwords_2d = svd.fit_transform(data_vectorized.T)\n \ndf = pd.DataFrame(columns=['x', 'y', 'word'])\ndf['x'], df['y'], df['word'] = words_2d[:,0], words_2d[:,1], vectorizer.get_feature_names()\n \nsource = ColumnDataSource(ColumnDataSource.from_df(df))\nlabels = LabelSet(x=\"x\", y=\"y\", text=\"word\", y_offset=8,\n                  text_font_size=\"8pt\", text_color=\"#555555\",\n                  source=source, text_align='center')\n \nplot = figure(plot_width=600, plot_height=600)\nplot.circle(\"x\", \"y\", size=12, source=source, line_color=\"black\", fill_alpha=0.8)\nplot.add_layout(labels)\nshow(plot, notebook_handle=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d3dc92fa60bb47220ce8e79e713afe0b4c76f37d"},"cell_type":"markdown","source":"### Zoom in a bit to find some meaningful representations.\n\n### However, LSI has one major weakness – ambiguity.  For example, how could a system determine if you are talking about Microsoft office, or the office in which you work. This is where LDA comes in, lets look at that."},{"metadata":{"_uuid":"1077e7e862895c6d75b074155a504d223dd52ce2"},"cell_type":"markdown","source":"<a id='lda'></a>\n\n# Latent Dirichlet Allocation\n\n### Corpus - Document - Word : Topic Generation"},{"metadata":{"_uuid":"4de99e1d7f91e734c3ace714964ca939e15c508f"},"cell_type":"markdown","source":"***\n**Is a generative statistical model that allows sets of observations to be explained by unobserved groups that explain why some parts of the data are similar. For example, if observations are words collected into documents, it posits that each document is a mixture of a small number of topics and that each word's presence is attributable to one of the document's topics.**\n***"},{"metadata":{"_uuid":"348bfebc823e5c18408921376ac342a43b8b70b9"},"cell_type":"markdown","source":" <hr>\n           \n           LSI examines the words used in a document and looks for their relationships with other words. LSI allows a system to determine the kind of words that a document might be relevant for, even if they are not actually used on the document itself.  But having content that is full of words that have relationships with each other, you are strengthening the document for all of those words.\n        \n           LDA is a significant extension of LSI.  Words are grouped into topics.  They can exist in more than one topic, in fact most do.  LDA tackles ambiguity by comparing a document to two topics and determining which topic is closer to the document, across all combinations of topics which seem broadly relevant. In doing so, LDA helps to determine which documents are most relevant to which topics.\n<hr/>"},{"metadata":{"_uuid":"251a7adda61b7a4d3cfb72d5010103728f626710"},"cell_type":"markdown","source":"![](https://upload.wikimedia.org/wikipedia/commons/thumb/d/d3/Latent_Dirichlet_allocation.svg/593px-Latent_Dirichlet_allocation.svg.png)"},{"metadata":{"_uuid":"c29cb29bcaf97f963f84dd28c52258a875692fe9"},"cell_type":"markdown","source":"***\nα is the parameter of the Dirichlet prior on the per-document topic distributions,\nβ is the parameter of the Dirichlet prior on the per-topic word distribution,\n$ \\theta _{m}$ is the topic distribution for document m,\n$\\varphi _{k}$ is the word distribution for topic k,\n$z_{mn}$ is the topic for the n-th word in document m, and\n$w_{mn}$ is the specific word.\n\n***\nThe fact that W is grayed out means that words $\\displaystyle w_{ij}$ are the only observable variables, and the other variables are latent variables. A sparse Dirichlet prior can be used to model the topic-word distribution, following the intuition that the probability distribution over words in a topic is skewed, so that only a small set of words have high probability. The resulting model is the most widely applied variant of LDA today. The plate notation for this model is shown below, where $\\displaystyle K$  denotes the number of topics and $\\displaystyle \\varphi _{1},\\dots ,\\varphi _{K}$ are $\\displaystyle V$-dimensional vectors storing the parameters of the Dirichlet-distributed topic-word distributions ( $\\displaystyle V$ is the number of words in the vocabulary).\n\nIt is helpful to think of the entities represented by $\\displaystyle \\theta$ and $\\displaystyle \\varphi$ as matrices created by decomposing the original document-word matrix that represents the corpus of documents being modeled. In this view, $\\displaystyle \\theta$  consists of rows defined by documents and columns defined by topics, while $\\displaystyle \\varphi$ consists of rows defined by topics and columns defined by words. Thus, $\\displaystyle \\varphi _{1},\\dots ,\\varphi _{K}$ refers to a set of rows, or vectors, each of which is a distribution over words, and $\\displaystyle \\theta _{1},\\dots ,\\theta _{M}$ refers to a set of rows, each of which is a distribution over topics.\n\n***"},{"metadata":{"_uuid":"eee32a423ca6e2962944acdfd404c55663d6bf65"},"cell_type":"markdown","source":"![](https://upload.wikimedia.org/wikipedia/commons/4/4d/Smoothed_LDA.png)"},{"metadata":{"_uuid":"5cf17f9148ed172cfa16cfe01f4b73aa3ac3bf1a"},"cell_type":"markdown","source":"**LDA assumes that each document in a corpus contains a mix of topics that are found throughout the entire corpus. The topic structure is hidden - we can only observe the documents and words, not the topics themselves. Because the structure is hidden (also known as latent), this method seeks to infer the topic structure given the known words and documents.**"},{"metadata":{"_uuid":"37078761e76e0ba964087f7929905fc8e9dc9957"},"cell_type":"markdown","source":"### LDA Document Structure\n\n### LDA represents documents as mixtures of topics that spit out words with certain probabilities. It assumes that documents are produced in the following fashion: when writing each document, you\n\n***\n* Decide on the number of words N the document will have.\n* Choose a topic mixture for the document (according to a Dirichlet probability distribution over a fixed set of K topics). \n* Generate each word in the document by:\n    *  First picking a topic.\n    * Then using the topic to generate the word itself (according to the topic’s multinomial distribution).\n    * Assuming this generative model for a collection of documents, LDA then tries to backtrack from the documents to find a set of topics that are likely to have generated the collection.\n***"},{"metadata":{"_uuid":"5ef2d7abe6ce93cc1016e32585fea07369192c6c"},"cell_type":"markdown","source":"### Now that we saw the theory aspects and the maths behind, lets see how to do it in python,"},{"metadata":{"_uuid":"33720c8e402101402a6a99c3ced8c5b548325e84"},"cell_type":"markdown","source":"### Convert to list so we can do some pre-processing"},{"metadata":{"trusted":true,"_uuid":"2d1f52ae350d7a12c5fe3d7d77a6c95cab7cd2d2"},"cell_type":"code","source":"# Convert to list\ndf = data.TITLE.values.tolist()\n\ndf = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in df]\n\n# Remove new line characters\ndf = [re.sub('\\s+', ' ', sent) for sent in df]\n\n# Remove distracting single quotes\ndf = [re.sub(\"\\'\", \"\", sent) for sent in df]\n\npprint(df[:1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b921d32aee7a155c3d548d3861c5ca1f328b9f59"},"cell_type":"code","source":"df = [re.sub(\"-\", \" \", sent) for sent in df]\ndf = [re.sub(\":\", \"\", sent) for sent in df]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"067945326877500288c9c1150fc0da4aa95ca034"},"cell_type":"markdown","source":"### All punctuations need to be removed, so lets write a function for that and we'll use the gensim package for that. deacc=TRUE does that. "},{"metadata":{"trusted":true,"_uuid":"cbc69f7c596cfeab659f3493f17978422cbab426"},"cell_type":"code","source":"def sent_to_words(sentences):\n    for sentence in sentences:\n        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n\ndf_words = list(sent_to_words(df))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d1a30c55d32e8c94ba52daa1dd3714cc068289bb"},"cell_type":"markdown","source":"### N-gram models"},{"metadata":{"_uuid":"1e06ed63fe561fcd490ecc181fc151ed822c3faa"},"cell_type":"markdown","source":"### Creating Bigram and Trigram Models\n\n### What are they? \n\n### Bigrams are two words frequently occurring together in the document. Trigrams are 3 words frequently occurring. Gensim's Phrases method helps us with that. "},{"metadata":{"trusted":true,"_uuid":"ad8cfd4a81efb9904bc2d574afcaaef66a1d00de"},"cell_type":"code","source":"# Build the bigram and trigram models\n\nbigram = gensim.models.Phrases(df_words, min_count=5, threshold=100) # higher threshold fewer phrases.\ntrigram = gensim.models.Phrases(bigram[df_words], threshold=100)  \n\n# Faster way to get a sentence clubbed as a trigram/bigram\n\nbigram_mod = gensim.models.phrases.Phraser(bigram)\ntrigram_mod = gensim.models.phrases.Phraser(trigram)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5406ca53fc9a4ccc734b09e51b5761a42108863c"},"cell_type":"markdown","source":"### We need to apply the bigrams, trigrams and lemmatize the set of documents we are working with.\n\n###  mm lemmatize ? ok  It is the process of grouping together the inflected forms of a word so they can be analysed as a single item, identified by the word's lemma, or dictionary form.\n\n### To do that, lets create few functions. "},{"metadata":{"trusted":true,"_uuid":"a648a054921a9749e3a713b81608722f2be13e16"},"cell_type":"code","source":"def remove_stopwords(texts):\n    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n\ndef make_bigrams(texts):\n    return [bigram_mod[doc] for doc in texts]\n\ndef make_trigrams(texts):\n    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n\ndef lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n    texts_out = []\n    for sent in texts:\n        doc = nlp(\" \".join(sent)) \n        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n    return texts_out","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bab0edd83cbbf920de2784171cff4b5094a85cc4"},"cell_type":"markdown","source":"### Applying the custom functions to our dataset"},{"metadata":{"trusted":true,"_uuid":"9b66d09cff951253420585fc381e1dd844122d67"},"cell_type":"code","source":"# Remove Stop Words\n\ndata_words_nostops = remove_stopwords(df_words)\n\n# Form Bigrams\n\ndata_words_bigrams = make_bigrams(data_words_nostops)\n\nnlp = spacy.load('en', disable=['parser', 'ner'])\n\ndata_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"67619094922c6a4f515490b19880fbaf3539853a"},"cell_type":"markdown","source":"### As we saw in the definition above, converting our document set into a document-term matrix."},{"metadata":{"trusted":true,"_uuid":"2761780c1fa12b72bdebe51190bd12d487da91b6"},"cell_type":"code","source":"# Create Dictionary\nid2word = corpora.Dictionary(data_lemmatized)\n\n# Create Corpus\ntexts = data_lemmatized\n\n# Term Document Frequency\ncorpus = [id2word.doc2bow(text) for text in texts]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c02f2bb2e04220e63a92cbc4021b10d7cf8d410f"},"cell_type":"markdown","source":"### Readable format of corpus (term-frequency)"},{"metadata":{"trusted":true,"_uuid":"a51bb44d6d9b90cfd199dbb578370bd0f72c9d39"},"cell_type":"code","source":"[[(id2word[id], freq) for id, freq in cp] for cp in corpus[11:12]]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ba07f02ffec1e820daee172a46c1e53eababf578"},"cell_type":"markdown","source":"### Building the LDA model"},{"metadata":{"trusted":true,"_uuid":"6a98ab98bd1225b67662fadd10257b831eafee31"},"cell_type":"code","source":"lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n                                           id2word=id2word,\n                                           num_topics=5, \n                                           random_state=100,\n                                           update_every=1,\n                                           chunksize=100,\n                                           passes=10,\n                                           alpha='auto',\n                                           per_word_topics=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"07c2733fa6465e756353241846fc7c003099e405"},"cell_type":"markdown","source":"### Now that the model is built, lets print the Keyword in the 10 topics"},{"metadata":{"trusted":true,"_uuid":"7631adbfcd004151dd5dbd74137d171377c788fa"},"cell_type":"code","source":"pprint(lda_model.print_topics())\ndoc_lda = lda_model[corpus]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5e1186f31269e61d0ae82f67d557481bb231fefe"},"cell_type":"markdown","source":"### Right, now that the LDA model is built, there has to be a metric to evaluate how good the model is. Lets compute that to check how good it is. \n\n\n### There are 2 such metrics. \n\n### 1)   Perplexity -  Is a statistical measure of how well a probability model predicts a sample. As applied to LDA, for a given value of k, you estimate the LDA model. Then given the theoretical word distributions represented by the topics, compare that to the actual topic mixtures, or distribution of words in your documents. - Lower the better.\n### 2)  Coherence Score - Is defined as the average / median of the pairwise word-similarity scores of the words in the topic - Higher the better.\n                          \n                         "},{"metadata":{"trusted":true,"_uuid":"12d16b5ba97abc479506201d8576e746c6bf89ac"},"cell_type":"code","source":"# Compute Perplexity\n\nprint('\\nPerplexity: ', lda_model.log_perplexity(corpus)) \n\n# Compute Coherence Score\n\ncoherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\ncoherence_lda = coherence_model_lda.get_coherence()\nprint('\\nCoherence Score: ', coherence_lda)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b474e2624e85ced497861f161270929639ad2d41"},"cell_type":"markdown","source":"> ** From the model we created, lets look at the most salient terms under each topic that contributes to it the most** "},{"metadata":{"trusted":true,"_uuid":"d87c9b1c0a47abdc403f99279e740c13a5a77f9e"},"cell_type":"code","source":"# Visualize the topics\n\n#pyLDAvis.enable_notebook()\n#vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n#vis","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4356268c2d17a14f9266cf0c11bfa4c74def6a0a"},"cell_type":"markdown","source":"### Note: The graph below is not interactive as the interactive version is causing display issues after the notebook is committed. To play around, fork the notebook and check."},{"metadata":{"_uuid":"41f8ba735f0f8bff7efb748eae9f7c4d08459ad3"},"cell_type":"markdown","source":"![](https://github.com/rakash/Scripts/blob/master/pyldavis.png?raw=true)"},{"metadata":{"_uuid":"a5b8a699015a47e21b7c38a708e00e31aece5ba7"},"cell_type":"markdown","source":"**Let’s interpret the topic visualization. Notice how topics are shown on the left while words are on the right. Here are the main things you should consider:**\n\n**Larger topics are more frequent in the corpus**\n**Topics closer together are more similar, topics further apart are less similar.**\n**When you select a topic, you can see the most representative words for the selected topic. This measure can be a combination of how frequent or how discriminant the word is.**\n**You can adjust the weight of each property using the slider.**\n**Hovering over a word will adjust the topic sizes according to how representative the word is for the topic.**"},{"metadata":{"trusted":true,"_uuid":"0d3b795737e5476bcf9491872bd0c62375d8a374"},"cell_type":"markdown","source":"### Now that we saw the coherence value for 5 topics, lets create a function to get the coherence values for the count of topics < 5"},{"metadata":{"trusted":true,"_uuid":"dbcef8c63ebb5e6811687e72942fece03d8bf473"},"cell_type":"code","source":"def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n    coherence_values = []\n    model_list = []\n    for num_topics in range(start, limit, step):\n        model = LdaModel(corpus=corpus, num_topics=num_topics, id2word=id2word)\n        model_list.append(model)\n        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n        coherence_values.append(coherencemodel.get_coherence())\n\n    return model_list, coherence_values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a055ac0a399e09cbaa3a96e87970e615fc3c4f0b"},"cell_type":"code","source":"model_list, coherence_values = compute_coherence_values(dictionary=id2word, corpus=corpus, texts=data_lemmatized, start=1, limit=6, step=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d1a9f229ba496db1866ac06f7f0e030aa3c104e4"},"cell_type":"code","source":"limit=6; start=1; step=1;\nx = range(start, limit, step)\nplt.figure(figsize=(12,12))\nplt.plot(x, coherence_values)\nplt.xlabel(\"Num Topics\")\nplt.ylabel(\"Coherence score\")\nplt.legend((\"coherence_values\"), loc='best')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"384e9f3fa99a24263f76f5851bab58199ca414c5"},"cell_type":"code","source":"for m, cv in zip(x, coherence_values):\n    print(\"Num Topics =\", m, \" has Coherence Value of\", round(cv, 4))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fc5e9cd5dd15232a6087eaf025a52b139191fb13"},"cell_type":"code","source":"# Select the model and print the topics\n\noptimal_model = model_list[3]\nmodel_topics = optimal_model.show_topics(formatted=False)\npprint(optimal_model.print_topics(num_words=10))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1d767072d2936cf0cf3329f35bdd606d578f97be"},"cell_type":"markdown","source":"### Now which one do we go for? For this example, I am gonna go with 4 topics because the target lables in this dataset was classified into 4. Otherwise, its better to go with the one having the highest coherence score.\n\n### The purpose of LDA is also to compute how much of the document was generated by which topic. Lets look at that."},{"metadata":{"trusted":true,"_uuid":"08eb946ab54ed81db6c1605d3900a68661338ad4","scrolled":true},"cell_type":"code","source":"def format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=data):\n   \n    sent_topics_df = pd.DataFrame()\n\n   \n    for i, row in enumerate(ldamodel[corpus]):\n        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n        # Get the Dominant topic, Perc Contribution and Keywords for each document\n        for j, (topic_num, prop_topic) in enumerate(row):\n            if j == 0:  # -- dominant topic\n                wp = ldamodel.show_topic(topic_num)\n                topic_keywords = \", \".join([word for word, prop in wp])\n                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n            else:\n                break\n    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n\n    # Add original text to the end of the output\n    \n    contents = pd.Series(texts)\n    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n    return(sent_topics_df)\n    \n\ndf_topic_sents_keywords = format_topics_sentences(ldamodel=optimal_model, corpus=corpus, texts=df)\n\n\ndf_dominant_topic = df_topic_sents_keywords.reset_index()\ndf_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n\n\ndf_dominant_topic.head(5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"af7173a295ff9b8ef9718dfec42d79b8b0284173"},"cell_type":"markdown","source":"### We see above, the dominant topic and percentage contribution for each document. "},{"metadata":{"_uuid":"061946140df615f02c78c11e4852ddee64c2c811"},"cell_type":"markdown","source":"<a id='hdp'></a>\n\n# Hierarchical Dirichlet Process"},{"metadata":{"_uuid":"0740315fa83fe0b3d7973c75ab659ad91b4e2c25"},"cell_type":"markdown","source":"***\nis a nonparametric Bayesian approach to clustering grouped data. It uses a Dirichlet process for each group of data, with the Dirichlet processes for all groups sharing a base distribution which is itself drawn from a Dirichlet process\n***"},{"metadata":{"_uuid":"a140a5f00cf42057b07489fdf54bc2b412423d0f"},"cell_type":"markdown","source":"###  This technique is not really suitable to a dataset or a problem where the number of topics is predetermined & thats where the difference lies between HDP and LDA"},{"metadata":{"_uuid":"eb5d3da0cabba8f485386740ab53c0979b63012e"},"cell_type":"markdown","source":"HDP is an extension of LDA, designed to address the case where the number of mixture components (the number of \"topics\" in document-modeling terms) is not known apriori. \n\nUsing LDA for document modeling, one treats each \"topic\" as a distribution of words in some known vocabulary. For each document a mixture of topics is drawn from a Dirichlet distribution, and then each word in the document is an independent draw from that mixture (that is, selecting a topic and then using it to generate a word).\n\nFor HDP (applied to document modeling), one also uses a Dirichlet process to capture the uncertainty in the number of topics. So a common base distribution is selected which represents the countably-infinite set of possible topics for the corpus, and then the finite distribution of topics for each document is sampled from this base distribution."},{"metadata":{"_uuid":"3fe9c5523ee279026bf29ea0c594293a1d9f4552"},"cell_type":"markdown","source":"### We will be using the hdpmodel module from gensim"},{"metadata":{"trusted":true,"_uuid":"5224bedc3f9fce022f48ea939ac592b59ff603da"},"cell_type":"code","source":"from gensim.models import CoherenceModel, HdpModel","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6b756cfe90f5e3d77346855cd486a84bc41929bb"},"cell_type":"code","source":"hdpmodel = HdpModel(corpus=corpus, id2word=id2word)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fbe0b7cea6f0cb850502541a9ef794e24d3269c3"},"cell_type":"code","source":"hdptopics = hdpmodel.show_topics(formatted=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"172e284890de13920d8dbb653025e4d3aff7218c"},"cell_type":"code","source":"hdptopics[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e083c09d03511ed798afeea6ceb3b11b6bef3ed1"},"cell_type":"code","source":"len(hdptopics)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"406d4894763e0f33fb44e19d48bd342ea2ee9c29"},"cell_type":"markdown","source":"### The topics generated by HDP are as above. Lets look at the terms of the 1st topic.  We also see that the model has generated 20 topics. We'll be comparing the coherence scores of the other models later, But before that, lets look at the next technique under topic modeling. "},{"metadata":{"_uuid":"4b21e7b0f896e5ec788b2a570d30ba3a4e75920d"},"cell_type":"markdown","source":"<a id='nmf'></a>\n\n# Non-Negative Matrix Factorization"},{"metadata":{"_uuid":"b08c5131871154617d61a2dceb2554f212cf2b1f"},"cell_type":"markdown","source":"![](https://upload.wikimedia.org/wikipedia/commons/f/f9/NMF.png)"},{"metadata":{"_uuid":"e0983415da3ba29dc02083a671230fb542990c7b"},"cell_type":"markdown","source":"Is a group of algorithms where a matrix V is factorized into (usually) two matrices W and H, with the property that all three matrices have no negative elements. This non-negativity makes the resulting matrices easier to inspect."},{"metadata":{"_uuid":"2fdb8d9d99af55706f891f54d9048dfffad3f5db"},"cell_type":"markdown","source":"$\\mathbf {V}$= $\\mathbf {W} \\mathbf {H}$\nMatrix multiplication can be implemented as computing the column vectors of V as linear combinations of the column vectors in W using coefficients supplied by columns of H. \n\nThat is, each column of V can be computed as follows:\n\n$\\mathbf {v} _{i}$= $\\mathbf {W} \\mathbf {h} _{i}$\nwhere $\\mathbf {v} _{i}$ is the i-th column vector of the product matrix $\\mathbf {V}$ and $\\mathbf {h} _{i}$ is the i-th column vector of the matrix $\\mathbf {H}$."},{"metadata":{"_uuid":"f0ce2f1fe9ce40b288176329144c7f3ffbb919fb"},"cell_type":"markdown","source":"Again, difference between LDA and NMF? \n\nLatent Dirichlet Allocation (where documents are represented by latent topics, and topics are represented by a distribution over words) \nNon-negative Matrix Factorization (where a document-term matrix is approximately factorized into term-feature and feature-document matrices)."},{"metadata":{"_uuid":"bda5c6b792a32ceb0cd595d8f7377b0ee6220ace"},"cell_type":"markdown","source":"### NMF Structure"},{"metadata":{"_uuid":"7b0fd3edfd58fa97e3af9dfd3d7797868c920655"},"cell_type":"markdown","source":"***\nThus, given a set of multivariate $\\displaystyle n$ -dimensional data vectors, they are put into an $\\displaystyle n*m$ matrix $\\displaystyle V$ as its columns, where $\\displaystyle m$ is the number of examples in the data set. This matrix $\\displaystyle V$ is approximately factorized into an $\\displaystyle n*t$ matrix $\\displaystyle W$ and an $\\displaystyle t*m$ matrix $\\displaystyle H$, where $\\displaystyle t$ is generally less than $\\displaystyle n$ or $\\displaystyle m$. Hence, this results in a compression of the original data matrix.\n\nIn terms of topic modeling, the input document-term matrix $\\displaystyle V$ is factorized into a $\\displaystyle n*t$ document-topic matrix and a $\\displaystyle t*m$  topic-term matrix, where $\\displaystyle t$ is the number of topics produced.\n***"},{"metadata":{"_uuid":"2f35456a4be8a482ba96dd6f5280fc2b7acebe4c"},"cell_type":"markdown","source":"### On to python now,"},{"metadata":{"_uuid":"56145f67177ab6e2cf30bbf5d3ce42e85e907d76"},"cell_type":"markdown","source":"### Forming the matrix, getting the features and normalizing them"},{"metadata":{"trusted":true,"_uuid":"42ae3505d7c9ac9578f22a3486b2c48ba0ab6a5b"},"cell_type":"code","source":"from nltk.corpus import stopwords;\nimport nltk;\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer;\nfrom sklearn.decomposition import NMF;\nfrom sklearn.preprocessing import normalize;\n\nvectorizer = CountVectorizer(analyzer='word', max_features=5000, stop_words='english', lowercase=True, \n                             token_pattern='[a-zA-Z\\-][a-zA-Z\\-]{2,}');\nx_counts = vectorizer.fit_transform(title);\nprint( \"Created %d X %d document-term matrix\" % (x_counts.shape[0], x_counts.shape[1]) )\ntransformer = TfidfTransformer(smooth_idf=False);\nx_tfidf = transformer.fit_transform(x_counts);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8a11364f944fa61bffec0b506a66ae04ded19c00"},"cell_type":"code","source":"terms = vectorizer.get_feature_names()\nprint(\"Vocabulary has %d distinct terms\" % len(terms))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"246bcb122c8a9e240eddb9a8eebdeaf7e9d895ce"},"cell_type":"code","source":"xtfidf_norm = normalize(x_tfidf, norm='l1', axis=1)\nmodel = NMF(n_components=5, init='nndsvd');\nmodel.fit(xtfidf_norm)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"99657550b002e2945f47191fc1f239a47f02d511"},"cell_type":"markdown","source":"### In order to get the topics, we'll again be creating a function. The NMF doesn't have the show topics method and hence we'll be taking the custom approach."},{"metadata":{"trusted":true,"_uuid":"50cedc356c2755ad244ba9915b1368e498ea7746"},"cell_type":"code","source":"def get_nmf_topics(model, n_top_words):\n    \n    feat_names = vectorizer.get_feature_names()\n    \n    word_dict = {};\n    for i in range(num_topics):\n        \n        words_ids = model.components_[i].argsort()[:-20 - 1:-1]\n        words = [feat_names[key] for key in words_ids]\n        word_dict['Topic # ' + '{:02d}'.format(i+1)] = words;\n    \n    return pd.DataFrame(word_dict);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7f6fbffeb6ce5de214983fee962a97e5e96bc86a"},"cell_type":"code","source":"num_topics = 5\nnmf_df = get_nmf_topics(model, 5)\nnmf_df","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7a25589cda401db9257c242a025560e322303454"},"cell_type":"markdown","source":"### The table above shows the terms under each topic, as per the NMF model"},{"metadata":{"trusted":true,"_uuid":"971bd10f78270a1b37ce8ed5a73b32fa9071ef34"},"cell_type":"code","source":"raw_documents = title.str.strip()\nraw_documents= raw_documents.str.lower()\nraw_documents = raw_documents.tolist()\nraw_doc1 = [i.split() for i in raw_documents]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b3d68623e61c1a91d7ce814f3b6bf6459fe46ff6"},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\n# use a custom stopwords list, set the minimum term-document frequency to 20\nvectorizer = CountVectorizer(stop_words = stop_words, min_df = 20) #custom_stop_words\nA = vectorizer.fit_transform(raw_documents)\nprint( \"Created %d X %d document-term matrix\" % (A.shape[0], A.shape[1]) )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"53b6216058bb62d0c24ff9c332629e5dca6b9012"},"cell_type":"code","source":"terms = vectorizer.get_feature_names()\nprint(\"Vocabulary has %d distinct terms\" % len(terms))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0fb4717ec1fa8a6115537460ae2719186020d9b7"},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n# we can pass in the same preprocessing parameters\nvectorizer = TfidfVectorizer(stop_words= stop_words, min_df = 20) #custom_stop_words\nA = vectorizer.fit_transform(raw_documents)\nprint( \"Created %d X %d TF-IDF-normalized document-term matrix\" % (A.shape[0], A.shape[1]) )","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ca3fcf3ec4a2f5f06a58711a4631ed44d26284a9"},"cell_type":"markdown","source":"### Until now, we created the term matrix. lets rank the terms and build the model."},{"metadata":{"trusted":true,"_uuid":"dcafeeaf0e525165120c52fd903c795619e8459b"},"cell_type":"code","source":"import operator\ndef rank_terms( A, terms ):\n    # get the sums over each column\n    sums = A.sum(axis=0)\n    # map weights to the terms\n    weights = {}\n    for col, term in enumerate(terms):\n        weights[term] = sums[0,col]\n    # rank the terms by their weight over all documents\n    return sorted(weights.items(), key=operator.itemgetter(1), reverse=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"77e01a363f215fd559b4f6060c61c0257be6bd92"},"cell_type":"code","source":"ranking = rank_terms( A, terms )\nfor i, pair in enumerate( ranking[0:20] ):\n    print( \"%02d. %s (%.2f)\" % ( i+1, pair[0], pair[1] ) )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"11fd687f57d8fca774e48a194d82a0ca30a97b12"},"cell_type":"code","source":"k = 10\n# create the model\nfrom sklearn import decomposition\nmodel = decomposition.NMF( init=\"nndsvd\", n_components=k ) \n# apply the model and extract the two factor matrices\nW = model.fit_transform( A )\nH = model.components_","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6a008200005a1d5f506667ac341dc0243c97c834"},"cell_type":"markdown","source":"### checking the index for a term"},{"metadata":{"trusted":true,"_uuid":"1874daef5585719e877faa79c839e380ff6fe63b"},"cell_type":"code","source":"term_index = terms.index('samsung')\n# round to 2 decimal places for display purposes\nH[:,term_index].round(2)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e8bcb37850ffc750d4cd7c1b8ae6e8696fc9c3de"},"cell_type":"markdown","source":"### Sorting the indexes we got using the model to plot and see the top terms."},{"metadata":{"trusted":true,"_uuid":"db82312365e6b35a16107119e8c0a6bbe7d6cc91"},"cell_type":"code","source":"import numpy as np\ndef get_descriptor( terms, H, topic_index, top ):\n    # reverse sort the values to sort the indices\n    top_indices = np.argsort( H[topic_index,:] )[::-1]\n    # now get the terms corresponding to the top-ranked indices\n    top_terms = []\n    for term_index in top_indices[0:top]:\n        top_terms.append( terms[term_index] )\n    return top_terms","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5d33c95607a86c45407165ce03da4fc6bb09bf99"},"cell_type":"code","source":"descriptors = []\nfor topic_index in range(k):\n    descriptors.append( get_descriptor( terms, H, topic_index, 10 ) )\n    str_descriptor = \", \".join( descriptors[topic_index] )\n    print(\"Topic %02d: %s\" % ( topic_index+1, str_descriptor ) )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4a173e52d7e45c8e3ee3936dec53f8e8cb14cbbc"},"cell_type":"code","source":"%matplotlib inline\nimport numpy as np\nimport matplotlib\nimport matplotlib.pyplot as plt\nplt.style.use(\"ggplot\")\nmatplotlib.rcParams.update({\"font.size\": 14})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"234acc26e2092dfd344daa25a8089603caab5268"},"cell_type":"code","source":"def plot_top_term_weights( terms, H, topic_index, top ):\n    # get the top terms and their weights\n    top_indices = np.argsort( H[topic_index,:] )[::-1]\n    top_terms = []\n    top_weights = []\n    for term_index in top_indices[0:top]:\n        top_terms.append( terms[term_index] )\n        top_weights.append( H[topic_index,term_index] )\n    # note we reverse the ordering for the plot\n    top_terms.reverse()\n    top_weights.reverse()\n    # create the plot\n    fig = plt.figure(figsize=(13,8))\n    # add the horizontal bar chart\n    ypos = np.arange(top)\n    ax = plt.barh(ypos, top_weights, align=\"center\", color=\"green\",tick_label=top_terms)\n    plt.xlabel(\"Term Weight\",fontsize=14)\n    plt.tight_layout()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bfbf7523ab04734989017fb95afb22ccc3726b2d"},"cell_type":"markdown","source":"### The top 15 terms in the 2nd topic"},{"metadata":{"trusted":true,"_uuid":"a6627d611b6e9a380faa4bb5fb5a4b537d359c42"},"cell_type":"code","source":"plot_top_term_weights( terms, H, 1, 15 )","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7a499cd0d9a6171aa9669eeb04a094a372b5b58c"},"cell_type":"markdown","source":"### Create the Topic Models\n### A common approach for parameter selection is to Measure and compare the topic coherence of models generated for different values of k.\n### We need to start by pre-specifying an initial range of \"sensible\" values"},{"metadata":{"trusted":true,"_uuid":"84287360584ee073cd260a393dc807877c5de09f"},"cell_type":"code","source":"def get_top_snippets( all_snippets, W, topic_index, top ):\n    # reverse sort the values to sort the indices\n    top_indices = np.argsort( W[:,topic_index] )[::-1]\n    # now get the snippets corresponding to the top-ranked indices\n    top_snippets = []\n    for doc_index in top_indices[0:top]:\n        top_snippets.append( all_snippets[doc_index] )\n    return top_snippets","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"896737d613b9aa96f91141ffe4bfaa90af0e4bac"},"cell_type":"code","source":"kmin, kmax = 2, 8","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b837b6dfb7b0c9f43cbe84e5d479f82f8e62e6f5"},"cell_type":"markdown","source":"### Apply NMF for each of these values:"},{"metadata":{"trusted":true,"_uuid":"81d759ee7381f87dc2c069d717e79bae56b7be44"},"cell_type":"code","source":"from sklearn import decomposition\ntopic_models = []\n# try each value of k\nfor k in range(kmin,kmax+1):\n    print(\"Applying NMF for k=%d ...\" % k )\n    # run NMF\n    model = decomposition.NMF( init=\"nndsvd\", n_components=k ) \n    W = model.fit_transform( A )\n    H = model.components_    \n    # store for later\n    topic_models.append( (k,W,H) )","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6bc35009371e96edd5c911c91a4b22552dd08b2e"},"cell_type":"markdown","source":"### we need to define a class that will generate documents in a form that can be consumed by Gensim's Word2Vec implementation"},{"metadata":{"trusted":true,"_uuid":"bfcd678adf90df31915140441c9cd61d41a0fe66"},"cell_type":"code","source":"import re\nclass TokenGenerator:\n    def __init__( self, documents, stopwords ):\n        self.documents = documents\n        self.stopwords = stopwords\n        self.tokenizer = re.compile( r\"(?u)\\b\\w\\w+\\b\" )\n\n    def __iter__( self ):\n        print(\"Building Word2Vec model ...\")\n        for doc in self.documents:\n            tokens = []\n            for tok in self.tokenizer.findall( doc ):\n                if tok in self.stopwords:\n                    tokens.append( \"<stopword>\" )\n                elif len(tok) >= 2:\n                    tokens.append( tok )\n            yield tokens","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"47431c4228b2d5c3cf23950db655de904fdfaa7f"},"cell_type":"markdown","source":"### Now build a Skipgram Word2Vec model from all documents in the input file using Gensim"},{"metadata":{"trusted":true,"_uuid":"44d31abf4d24938f940fbe4cf76051983c43b7ae"},"cell_type":"code","source":"import gensim\ndocgen = TokenGenerator(raw_documents, stop_words )\nw2v_model = gensim.models.Word2Vec(docgen, size=500, min_count=20, sg=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c2db1778c597725eee4e0906b6071e35a04f11af"},"cell_type":"markdown","source":"### Selecting the Number of Topics\n\n### Once we have our Word2vec model, we can use it as part of our topic coherence approach to evaluate the different NMF topic models that we created previously. To do this, we will implement a simple version of the TC-W2V coherence measure.\n\n### We use the Word2vec model to calculate coherence scores for each of these models. We will define this coherence score as follows, "},{"metadata":{"trusted":true,"_uuid":"18a268e8c38b564b8954f4f6383db46939358151"},"cell_type":"code","source":"def calculate_coherence( w2v_model, term_rankings ):\n    overall_coherence = 0.0\n    for topic_index in range(len(term_rankings)):\n        # check each pair of terms\n        pair_scores = []\n        for pair in combinations( term_rankings[topic_index], 2 ):\n            pair_scores.append( w2v_model.similarity(pair[0], pair[1]))\n        # get the mean for all pairs in this topic\n        topic_score = sum(pair_scores) / len(pair_scores)\n        overall_coherence += topic_score\n    # get the mean score across all topics\n    return overall_coherence / len(term_rankings)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"41fc0aefc69ddccf53df930c8d81d8612f5507e6"},"cell_type":"markdown","source":"### We also define a function to get the topic descriptor (i.e. list of top terms) for each topic"},{"metadata":{"trusted":true,"_uuid":"be51953c92c316a41329af785eebc23608975279"},"cell_type":"code","source":"import numpy as np\ndef get_descriptor( all_terms, H, topic_index, top ):\n    # reverse sort the values to sort the indices\n    top_indices = np.argsort( H[topic_index,:] )[::-1]\n    # now get the terms corresponding to the top-ranked indices\n    top_terms = []\n    for term_index in top_indices[0:top]:\n        top_terms.append( all_terms[term_index] )\n    return top_terms","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"630841c0a0cbb18fe8437c971143c8453952a912"},"cell_type":"markdown","source":"### process each of the models for different values of k"},{"metadata":{"trusted":true,"_uuid":"628f201d3e616be5fd226c43e2b9a4be1010f60c"},"cell_type":"code","source":"from itertools import combinations\nk_values = []\ncoherences = []\nfor (k,W,H) in topic_models:\n    # Get all of the topic descriptors - the term_rankings, based on top 10 terms\n    term_rankings = []\n    for topic_index in range(k):\n        term_rankings.append( get_descriptor( terms, H, topic_index, 10 ) )\n    # Now calculate the coherence based on our Word2vec model\n    k_values.append( k )\n    coherences.append( calculate_coherence( w2v_model, term_rankings ) )\n    print(\"K=%02d: Coherence=%.4f\" % ( k, coherences[-1] ) )","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3bea7422701397f13baa9c3c7814faa01f957fe7"},"cell_type":"markdown","source":"### Surprisingly, for this text document, the coherence is all the same for k = 2 to k = 8. Lets plot that & see and note this value and we'll compare it with other techniques."},{"metadata":{"trusted":true,"_uuid":"49f43d87c1dbe26920b242f9e27555247b9f37ac"},"cell_type":"code","source":"%matplotlib inline\nimport matplotlib\nimport matplotlib.pyplot as plt\nplt.style.use(\"ggplot\")\nmatplotlib.rcParams.update({\"font.size\": 14})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"06d84c489ca7caa10f6620ece8327649e438053b"},"cell_type":"code","source":"fig = plt.figure(figsize=(13,7))\n# create the line plot\nax = plt.plot( k_values, coherences )\nplt.xticks(k_values)\nplt.xlabel(\"Number of Topics\")\nplt.ylabel(\"Mean Coherence\")\n# add the points\nplt.scatter( k_values, coherences, s=120)\n# find and annotate the maximum point on the plot\nymax = max(coherences)\nxpos = coherences.index(ymax)\nbest_k = k_values[xpos]\nplt.annotate( \"k=%d\" % best_k, xy=(best_k, ymax), xytext=(best_k, ymax), textcoords=\"offset points\", fontsize=16)\n# show the plot\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"95d8f353c6aa526a9c933b4764c5022a291d99e9"},"cell_type":"code","source":"k = best_k\n# get the model that we generated earlier.\nW = topic_models[k-kmin][1]\nH = topic_models[k-kmin][2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e03c693f47eb32518a53af7af39256dc8018549f"},"cell_type":"code","source":"for topic_index in range(k):\n    descriptor = get_descriptor( terms, H, topic_index, 10 )\n    str_descriptor = \", \".join( descriptor )\n    print(\"Topic %02d: %s\" % ( topic_index+1, str_descriptor ) )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"01f0baad35aa3fbf502cf50248678b13d45a3aa7"},"cell_type":"markdown","source":"### The words under each topic can be seen above and as per the best K value chosen by NMF. We also saw the coherence value of the NMF model. We will compare those results with LDA, LSI and HDP."},{"metadata":{"_uuid":"3d53f7db69e1f89717dfafb634588e7fcc8eda7d"},"cell_type":"markdown","source":"# Comparing the techniques"},{"metadata":{"trusted":true,"_uuid":"84d19f4b8cf144395527fbf74a711c7809f0e475"},"cell_type":"code","source":"lsimodel = LsiModel(corpus=corpus, num_topics=5, id2word=id2word)\nlsitopics = lsimodel.show_topics(formatted=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6732df3b6ffb1f980761d2fbb00a4dc108f76812"},"cell_type":"code","source":"ldatopics = lda_model.show_topics(formatted=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7ae8fdadfd68d558faee41d8046fe364ba7b76c2"},"cell_type":"code","source":"lsitopics = [[word for word, prob in topic] for topicid, topic in lsitopics]\n\nhdptopics = [[word for word, prob in topic] for topicid, topic in hdptopics]\n\nldatopics = [[word for word, prob in topic] for topicid, topic in ldatopics]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"96e919b7d0d511583a8302640712bc6f6903166b"},"cell_type":"code","source":"lsi_coherence = CoherenceModel(topics=lsitopics[:10], texts=data_lemmatized, dictionary=id2word, window_size=10).get_coherence()\n\nhdp_coherence = CoherenceModel(topics=hdptopics[:10], texts=data_lemmatized, dictionary=id2word, window_size=10).get_coherence()\n\nlda_coherence = CoherenceModel(topics=ldatopics, texts=data_lemmatized, dictionary=id2word, window_size=10).get_coherence()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7eb1caaecfdca890e3ff0d4ea14e062679aeaaf8"},"cell_type":"code","source":"def evaluate_bar_graph(coherences, indices):\n    assert len(coherences) == len(indices)\n    n = len(coherences)\n    x = np.arange(n)\n    plt.figure(figsize=(12,12))\n    plt.bar(x, coherences, width=0.2, tick_label=indices, align='center')\n    plt.xlabel('Models')\n    plt.ylabel('Coherence Value')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dd2d1f9330edb5e121b5cfc9a0a853fde3e412b9"},"cell_type":"code","source":"evaluate_bar_graph([lsi_coherence, hdp_coherence, lda_coherence],\n                   ['LSI', 'HDP', 'LDA'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dbfbfb815ac6774f2ded8c2d5d226c071a5abd88"},"cell_type":"markdown","source":"### The NMF model has the highest coherence score(not in the plot above -- NMF = .9)."},{"metadata":{"_uuid":"0953d6a4f97f63401c144a741783bec1eecaf538"},"cell_type":"markdown","source":"## Conclusion\n\n### The idea of this notebook was to implement the widely used topic modeling techniques and compare. Although NMF gave the highest coherence score, LDA is the most used technique and considered to be consistent as it is likely to provide more \"coherent\" topics. NMF performs better where the topic probabilities should remain fixed per document. HDP on the other hand is less preferred since the number of topics is not determined in prior and hence used rarely. \n\n### Of course, it depends on the dataset but to check for your own set of documents, fork the notebook and play around. Share your thoughts & upvote if you found the kernel useful."},{"metadata":{"_uuid":"6eaefe3bfa0c29cbc04f416a972026ac590b85dc"},"cell_type":"markdown","source":""},{"metadata":{"_uuid":"80e6d2bc09f9db5a0a39a795e37970147ebe10e9"},"cell_type":"markdown","source":""}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}