{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nfrom time import time\nimport re\nimport string\nimport os\n#import emoji\nfrom pprint import pprint\nimport collections\nfrom keras import Sequential\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style=\"darkgrid\")\nsns.set(font_scale=1.3)\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.metrics import classification_report\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.externals import joblib\n#import gensim\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.tokenize import word_tokenize\nimport warnings\nwarnings.filterwarnings('ignore')\nnp.random.seed(37)\nimport json\nimport string\nfrom pandas.io.json import json_normalize\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ncolor = sns.color_palette()\n\n\nfrom sklearn import model_selection, preprocessing, metrics, ensemble, naive_bayes, linear_model\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nimport lightgbm as lgb\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, CuDNNGRU,CuDNNLSTM, Conv1D\nfrom keras.layers import Bidirectional, GlobalMaxPool1D\nfrom keras.models import Model\nfrom keras import initializers, regularizers, constraints, optimizers, layers\nfrom keras.utils.np_utils import to_categorical\nfrom sklearn.preprocessing import LabelEncoder\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9a737173f4322ae49e7b0d6da4ba7a926b5bddb0"},"cell_type":"code","source":"os.listdir(\"../input/\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4b979ec226289061bb0d7ea0b90f61bfcd6bb575"},"cell_type":"code","source":"df = pd.read_csv('../input/twitter-airline-sentiment/Tweets.csv')\ndf = df.reindex(np.random.permutation(df.index))\ndf.shape\ndataset = df[['text', 'airline_sentiment']]\ndataset=dataset.rename(index=str, columns={\"airline_sentiment\": \"target\"})\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b3448decb43b79edc868fef4d1cd3b23bce88a09"},"cell_type":"code","source":"train_df, full_df = train_test_split(dataset, test_size=0.30, random_state=2018)\ntest_df, val_df = train_test_split(full_df, test_size=0.50, random_state=2018)\n\nembed_size = 300 # how big is each word vector\nmax_features = 10000 # how many unique words to use (i.e num rows in embedding vector)\nmaxlen = 100 # max number of words in a question to use\n\n## fill up the missing values\ntrain_X = train_df[\"text\"].fillna(\"_na_\").values\nval_X = val_df[\"text\"].fillna(\"_na_\").values\ntest_X = test_df[\"text\"].fillna(\"_na_\").values\n\n## Tokenize the sentences\ntokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(train_X))\ntrain_X = tokenizer.texts_to_sequences(train_X)\nval_X = tokenizer.texts_to_sequences(val_X)\ntest_X = tokenizer.texts_to_sequences(test_X)\n\n## Pad the sentences \ntrain_X = pad_sequences(train_X, maxlen=maxlen)\nval_X = pad_sequences(val_X, maxlen=maxlen)\ntest_X = pad_sequences(test_X, maxlen=maxlen)\n\n## Get the target values\ntrain_y = train_df['target'].values\nval_y = val_df['target'].values\ntest_y=test_df['target'].values\n\nle = LabelEncoder()\ny_train_le = le.fit_transform(train_y)\ny_val_le = le.transform(val_y)\ny_test_le = le.transform(test_y)\ny_train_oh = to_categorical(y_train_le)\ny_val_oh = to_categorical(y_val_le)\ny_test_oh = to_categorical(y_test_le)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0fce0c58caa7a8c05a9168440c0ccf3d7eb6e687"},"cell_type":"code","source":"EMBEDDING_FILE = '../input/quora-insincere-questions-classification/embeddings/glove.840B.300d/glove.840B.300d.txt'\ndef get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\nembeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE, encoding='utf8'))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d8146bc99343571fb29b66ae19dabe7871cf8119"},"cell_type":"code","source":"nb_words","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e5a86de0f7cadc2aca9e5f3f50a5ca8ee716f237"},"cell_type":"code","source":"all_embs = np.stack(embeddings_index.values())\nemb_mean,emb_std = all_embs.mean(), all_embs.std()\nembed_size = all_embs.shape[1]\n\nword_index = tokenizer.word_index\nnb_words = min(max_features, len(word_index))\nembedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\nfor word, i in word_index.items():\n    if i >= max_features: continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n        \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"276af67efd6fba4da41e676588b5e327f2ab38f6"},"cell_type":"code","source":"def build_vocab(texts):\n    sentences = texts.apply(lambda x: x.split()).values\n    vocab = {}\n    for sentence in sentences:\n        for word in sentence:\n            try:\n                vocab[word] += 1\n            except KeyError:\n                vocab[word] = 1\n    return vocab","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"42c7e50c2655a8ae5fa4aaac3cfdb633a56280e6"},"cell_type":"code","source":"def check_coverage(vocab, embeddings_index):\n    known_words = {}\n    unknown_words = {}\n    nb_known_words = 0\n    nb_unknown_words = 0\n    for word in vocab.keys():\n        try:\n            known_words[word] = embeddings_index[word]\n            nb_known_words += vocab[word]\n        except:\n            unknown_words[word] = vocab[word]\n            nb_unknown_words += vocab[word]\n            pass\n\n    print('Found embeddings for {:.2%} of vocab'.format(len(known_words) / len(vocab)))\n    print('Found embeddings for  {:.2%} of all text'.format(nb_known_words / (nb_known_words + nb_unknown_words)))\n    unknown_words = sorted(unknown_words.items(), key=operator.itemgetter(1))[::-1]\n\n    return unknown_words","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9ff11201fdda08a335a00ed0ee0278dfa21104af"},"cell_type":"code","source":"import operator\nembed_glove = embeddings_index\nvocab = build_vocab(df['text'])\nprint(\"Glove : \")\noov_glove = check_coverage(vocab, embed_glove)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"02f153cda74d40290d96ba0b62430a7f90a045bc"},"cell_type":"code","source":"df['lowered_question'] = df['text'].apply(lambda x: x.lower())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4e7b62a402030f4dac772641ec9a90ba79196543"},"cell_type":"code","source":"def add_lower(embedding, vocab):\n    count = 0\n    for word in vocab:\n        if word in embedding and word.lower() not in embedding:  \n            embedding[word.lower()] = embedding[word]\n            count += 1\n    print(f\"Added {count} words to embedding\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b42238c42f7df5bd4a3a567665f6eac9f27c296b"},"cell_type":"code","source":"print(\"Glove : \")\noov_glove = check_coverage(vocab, embed_glove)\nadd_lower(embed_glove, vocab)\noov_glove = check_coverage(vocab, embed_glove)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6e472b07ad32e2693ec4595519188909361db464"},"cell_type":"code","source":"contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\" }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"47bf8917954e6b53971ad2d0947b7e6c68737411"},"cell_type":"code","source":"def known_contractions(embed):\n    known = []\n    for contract in contraction_mapping:\n        if contract in embed:\n            known.append(contract)\n    return known","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a468d186edac25a11434567587a7a3651cc519d3"},"cell_type":"code","source":"print(\"- Known Contractions -\")\nprint(\"   Glove :\")\nprint(known_contractions(embed_glove))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"965452ab54d70535e19b716df8cb89a47de44526"},"cell_type":"code","source":"def clean_contractions(text, mapping):\n    specials = [\"’\", \"‘\", \"´\", \"`\"]\n    for s in specials:\n        text = text.replace(s, \"'\")\n    text = ' '.join([mapping[t] if t in mapping else t for t in text.split(\" \")])\n    return text\ndf['treated_question'] = df['lowered_question'].apply(lambda x: clean_contractions(x, contraction_mapping))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"029c4bbc7f2593ea6c68c9f23ac8511a363f5185"},"cell_type":"code","source":"vocab = build_vocab(df['treated_question'])\nprint(\"Glove : \")\noov_glove = check_coverage(vocab, embed_glove)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"293976f02b0a582f232027bdbd5f42ae2efe49ae"},"cell_type":"code","source":"punct = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~\" + '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&'\ndef unknown_punct(embed, punct):\n    unknown = ''\n    for p in punct:\n        if p not in embed:\n            unknown += p\n            unknown += ' '\n    return unknown\nprint(\"Glove :\")\nprint(unknown_punct(embed_glove, punct))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7e547f4193863699b2e14936a6b1a45a344255df"},"cell_type":"code","source":"punct_mapping = {\"‘\": \"'\", \"₹\": \"e\", \"´\": \"'\", \"°\": \"\", \"€\": \"e\", \"™\": \"tm\", \"√\": \" sqrt \", \"×\": \"x\", \"²\": \"2\", \"—\": \"-\", \"–\": \"-\", \"’\": \"'\", \"_\": \"-\", \"`\": \"'\", '“': '\"', '”': '\"', '“': '\"', \"£\": \"e\", '∞': 'infinity', 'θ': 'theta', '÷': '/', 'α': 'alpha', '•': '.', 'à': 'a', '−': '-', 'β': 'beta', '∅': '', '³': '3', 'π': 'pi', }\ndef clean_special_chars(text, punct, mapping):\n    for p in mapping:\n        text = text.replace(p, mapping[p])\n    \n    for p in punct:\n        text = text.replace(p, f' {p} ')\n    \n    specials = {'\\u200b': ' ', '…': ' ... ', '\\ufeff': '', 'करना': '', 'है': ''}  # Other special characters that I have to deal with in last\n    for s in specials:\n        text = text.replace(s, specials[s])\n    \n    return text\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9fb8549c624a63c71205b72d392f09eeee4902c6"},"cell_type":"code","source":"df['treated_question'] = df['treated_question'].apply(lambda x: clean_special_chars(x, punct, punct_mapping))\nvocab = build_vocab(df['treated_question'])\nprint(\"Glove : \")\noov_glove = check_coverage(vocab, embed_glove)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c9406830e7e55dcff39f8529184539db72a4d379"},"cell_type":"code","source":"mispell_dict = {'colour': 'color', 'centre': 'center', 'favourite': 'favorite', 'travelling': 'traveling', 'counselling': 'counseling', 'theatre': 'theater', 'cancelled': 'canceled', 'labour': 'labor', 'organisation': 'organization', 'wwii': 'world war 2', 'citicise': 'criticize', 'youtu ': 'youtube ', 'Qoura': 'Quora', 'sallary': 'salary', 'Whta': 'What', 'narcisist': 'narcissist', 'howdo': 'how do', 'whatare': 'what are', 'howcan': 'how can', 'howmuch': 'how much', 'howmany': 'how many', 'whydo': 'why do', 'doI': 'do I', 'theBest': 'the best', 'howdoes': 'how does', 'mastrubation': 'masturbation', 'mastrubate': 'masturbate', \"mastrubating\": 'masturbating', 'pennis': 'penis', 'Etherium': 'Ethereum', 'narcissit': 'narcissist', 'bigdata': 'big data', '2k17': '2017', '2k18': '2018', 'qouta': 'quota', 'exboyfriend': 'ex boyfriend', 'airhostess': 'air hostess', \"whst\": 'what', 'watsapp': 'whatsapp', 'demonitisation': 'demonetization', 'demonitization': 'demonetization', 'demonetisation': 'demonetization', 'pokémon': 'pokemon'}\ndef correct_spelling(x, dic):\n    for word in dic.keys():\n        x = x.replace(word, dic[word])\n    return x\ndf['treated_question'] = df['treated_question'].apply(lambda x: correct_spelling(x, mispell_dict))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ba7eeaa52845696af1e9e35cee8bfc171ce925d6"},"cell_type":"code","source":"vocab = build_vocab(df['treated_question'])\nprint(\"Glove : \")\noov_glove = check_coverage(vocab, embed_glove)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9869a3599ee1c224f2b89af8a0620b745947b542"},"cell_type":"code","source":"datasetclean = df[['treated_question', 'airline_sentiment']]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fbf92cf3df3b194e385f40a9ff8ddd4394870c87"},"cell_type":"code","source":"def remove_mentions(input_text):\n        return re.sub(r'@\\w+', '', input_text)\n       \ndatasetclean.treated_question = df.treated_question.apply(remove_mentions)\ndatasetclean.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1028aa2db7eb6953e52880e7b3bf626b251cd8df"},"cell_type":"code","source":"train_df, full_df = train_test_split(datasetclean, test_size=0.30, random_state=2018)\ntest_df, val_df = train_test_split(full_df, test_size=0.50, random_state=2018)\n\nembed_size = 300 # how big is each word vector\nmax_features = 10000 # how many unique words to use (i.e num rows in embedding vector)\nmaxlen = 100 # max number of words in a question to use\n\n## fill up the missing values\ntrain_X = train_df[\"treated_question\"].fillna(\"_na_\").values\nval_X = val_df[\"treated_question\"].fillna(\"_na_\").values\ntest_X = test_df[\"treated_question\"].fillna(\"_na_\").values\n\n## Tokenize the sentences\ntokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(train_X))\ntrain_X = tokenizer.texts_to_sequences(train_X)\nval_X = tokenizer.texts_to_sequences(val_X)\ntest_X = tokenizer.texts_to_sequences(test_X)\n\n## Pad the sentences \ntrain_X = pad_sequences(train_X, maxlen=maxlen)\nval_X = pad_sequences(val_X, maxlen=maxlen)\ntest_X = pad_sequences(test_X, maxlen=maxlen)\n\n## Get the target values\ntrain_y = train_df['airline_sentiment'].values\nval_y = val_df['airline_sentiment'].values\ntest_y=test_df['airline_sentiment'].values\n\nle = LabelEncoder()\ny_train_le = le.fit_transform(train_y)\ny_val_le = le.transform(val_y)\ny_test_le = le.transform(test_y)\ny_train_oh = to_categorical(y_train_le)\ny_val_oh = to_categorical(y_val_le)\ny_test_oh = to_categorical(y_test_le)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0626119d8b15543901eb977a73ad4a1f2920dd6e"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e1fa56aa483137d3784b8372eb0545ad15ee1c05"},"cell_type":"code","source":"inp = Input(shape=(maxlen,))\nx = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\nx = Bidirectional(CuDNNGRU(64, return_sequences=True))(x)\nx = GlobalMaxPool1D()(x)\nx = Dense(16, activation=\"relu\")(x)\nx = Dropout(0.1)(x)\nx = Dense(3, activation=\"softmax\")(x)\nmodel = Model(inputs=inp, outputs=x)\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nprint(model.summary())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ffc73e7d449ba8e5a2f0213d62c17acac32e0bb8"},"cell_type":"code","source":"x=model.fit(train_X, y_train_oh, batch_size=512, epochs=20, validation_data=(val_X, y_val_oh))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"17325517f3634300b38b96cd00d568106fd8d4d4"},"cell_type":"code","source":"Epoch 1/2\n10248/10248 [==============================] - 4s 430us/step - loss: 0.8597 - acc: 0.6394 - val_loss: 0.7429 - val_acc: 0.6935\nEpoch 2/2\n10248/10248 [==============================] - 2s 183us/step - loss: 0.6976 - acc: 0.7160 - val_loss: 0.6284 - val_acc: 0.7468","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"59317a205878ba0bd57584667e62b437c9080d73"},"cell_type":"code","source":"\n\nmodel = Sequential()\n\nmodel.add(CuDNNLSTM(10, dropout=0.5, return_sequences=True))\nmodel.add(CuDNNLSTM(10, dropout=0.5, return_sequences=True))\nmodel.add(CuDNNLSTM(10, dropout=0.5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3da6fd54801454946347509af26ee4fa925866c6"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"31e7c5f10f710cc7a6e00d00d646d90027c0f34b"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4af69e7e1e0315a1f7ed1d4bf324a60c3542bacf"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b9eef05e67d98c573d29c8a53276f15c1b3a03ca"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"98913d19b255224b9caf036679a5a0ff6580b7f3"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7589993073d7b6129dfa88a011d5b6a12fca9b32"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d0e6f10417d6bcf995a820bf0ce50212d1683da9"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"86aefb6532808868d22aae145393109c381e2bca"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"287c7e20e38f1b6d0b8cdf39610f09e0b1cd6767"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b8dcbf2c6ea9434e657eb67ed5df95b0f0f96780"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"93277e19d41881fbc8c24f62349927f6639bb525"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"344e7a4049b0aee77ced6d39a30083c41b21c65e"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5c9139305f612c5a2889be2cf362c909a77ac9e2"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"17faa763a026f02a53502bcedf61bc14eb7c2b22"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0b23d027cb507d0ccc94f959969301655c35b943"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"28eafb21d4821ac2ffffc5ea8702faed8d746990"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5ea2f549530d054ee0b9944f2e56e2c52bd39283"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8d295ee831ff481e07906ffdf680e5bb2d233ec7"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"03bfb9ac64bb7929ea873e8e1bc99fd49ff42588"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e1cf9841045072cb53dbd6a6e8fc4292399d0bc0"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7e60cbfbd1f06abe7e20e59ac99b607eeb1224a6"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"172aab112d0e71152cabb682f486c2fa464fd2f4"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}