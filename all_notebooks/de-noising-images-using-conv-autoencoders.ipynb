{"cells":[{"metadata":{},"cell_type":"markdown","source":"In this notebook, I have trained an autoencoder to mitigate noisy images form the MNIST dataset. The Autoencoder consists of two components :-\n\n(1) Enoder - which ecnodes data onto some latent space, that usually has significantly less dimensionality than the original data.\n\n(2) Decoder - which tries to reconstruct the original data back from its encoded latent space representation.\n\nSo, essentially the latent representation serves as a bottleneck, that can be utilized for a wide range of tasks. The common ones include data compression, dimensionality reduction, new data generation etc.\n\nTo prevent the network from simply learning some form of identity mapping, adding certain amount of noise to the input data became popular practice. After a while, it was realized that autoencoders could in fact be used for de-noising data applications alone.\n\nI imported the the MNIST dataset, added some noise to it, and then trained an autoencoder on it which uses Convolution layers for encoding and decoding part. The images are of thge shape (28, 28), and the latent space is a vector of length (16). The code is pretty straighforward, and easy to follow. At least, I hope so. This is a simple example, but it can be used as a template for de-noising more sophisticated data.","execution_count":null},{"metadata":{"id":"q5YMzYk5CYzz","trusted":true},"cell_type":"code","source":"from tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.layers import Conv2D\nfrom tensorflow.keras.layers import Conv2DTranspose \nfrom tensorflow.keras.layers import LeakyReLU \nfrom tensorflow.keras.layers import Activation \nfrom tensorflow.keras.layers import Flatten \nfrom tensorflow.keras.layers import Dense \nfrom tensorflow.keras.layers import Reshape \nfrom tensorflow.keras.layers import Input\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.datasets import mnist\nfrom tensorflow.keras.utils import plot_model\nimport matplotlib.pyplot as plt\nimport numpy as np","execution_count":null,"outputs":[]},{"metadata":{"id":"2pJys5SYAyr6","trusted":true},"cell_type":"code","source":"def ConvAutoEncoder(width, height, depth, filters=(32, 64), latentDim=16):\n\n  input_shape = (width, height, depth)\n  chanDim = -1\n\n  encInp = Input(shape=input_shape)\n  x = encInp\n\n  for f in filters:\n    x = Conv2D(f, (3, 3), strides=2, padding='same')(x)\n    x = LeakyReLU(alpha=0.2)(x)\n    x = BatchNormalization(axis=chanDim)(x)\n\n  volumeSize = K.int_shape(x)\n  x = Flatten()(x)\n  latent = Dense(latentDim)(x)\n\n  encoder = Model(encInp, latent, name='encoder')\n\n  decInp = Input(shape=(latentDim, ))\n  x = Dense(np.prod(volumeSize[1:]))(decInp)\n  x = Reshape((volumeSize[1], volumeSize[2], volumeSize[3]))(x)\n\n  for f in filters[::-1]:\n    x = Conv2DTranspose(f, (3, 3), strides=2, padding='same')(x)\n    x = LeakyReLU(alpha=0.2)(x)\n    x = BatchNormalization(axis=chanDim)(x)\n\n  x = Conv2DTranspose(depth, (3, 3), padding='same')(x)\n  output = Activation('sigmoid')(x)\n\n  decoder = Model(decInp, output, name='decoder')\n\n  autoEncoder = Model(encInp, decoder(encoder(encInp)), name='autoencoder')\n\n  return (encoder, decoder, autoEncoder)","execution_count":null,"outputs":[]},{"metadata":{"id":"Emx4gHStCa91","trusted":true},"cell_type":"code","source":"data = mnist.load_data()\n(trainX, _), (testX, _) = mnist.load_data()\n\ntrainX = np.expand_dims(trainX, axis=-1)\ntestX = np.expand_dims(testX, axis=-1)\ntrainX = trainX.astype(\"float32\") / 255.0\ntestX = testX.astype(\"float32\") / 255.0","execution_count":null,"outputs":[]},{"metadata":{"id":"tAn_c4RQGOj1","trusted":true},"cell_type":"code","source":"trainNoise = np.random.uniform(low=-800, high=800, size=trainX.shape)/1000.0\ntestNoise = np.random.uniform(low=-800, high=800, size=testX.shape)/1000.0\ntrainXNoisy = np.clip(trainX + trainNoise, 0, 1)\ntestXNoisy = np.clip(testX + testNoise, 0, 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_set_len = len(testXNoisy)\nsplit_at = int(0.7*test_set_len)\n\nvalidXNoisy = testXNoisy[:split_at]\ntestXNoisy = testXNoisy[split_at:]\n\nvalidX = testX[:split_at]\ntestX = testX[split_at:]","execution_count":null,"outputs":[]},{"metadata":{"id":"6rwksC4CGreU","trusted":true},"cell_type":"code","source":"(encoder, decoder, autoencoder) = ConvAutoEncoder(28, 28, 1)","execution_count":null,"outputs":[]},{"metadata":{"id":"cW4SsaaRHWQ1","outputId":"8c0a3cda-7238-48b5-e150-48c6295575c7","trusted":true},"cell_type":"code","source":"plot_model(autoencoder)","execution_count":null,"outputs":[]},{"metadata":{"id":"T1OGeyYTHJnJ","outputId":"8421f167-88be-4199-9a87-bc89cf597561","trusted":true},"cell_type":"code","source":"encoder.summary()","execution_count":null,"outputs":[]},{"metadata":{"id":"FN_HLmXwHgSM","outputId":"cebe5a56-cd89-4a6c-d4f3-4342f0ad6c36","trusted":true},"cell_type":"code","source":"decoder.summary()","execution_count":null,"outputs":[]},{"metadata":{"id":"AnTzc1nnHpXU","trusted":true},"cell_type":"code","source":"opt = Adam(lr=1e-3)\nautoencoder.compile(loss=\"mse\", optimizer=opt)","execution_count":null,"outputs":[]},{"metadata":{"id":"Ap9DtdyMHwiD","outputId":"ea702d63-45eb-4e2f-d410-04e4b4586181","trusted":true},"cell_type":"code","source":"EPOCHS = 25\nBS = 32\n\nH = autoencoder.fit(\n    trainXNoisy, trainX,\n    validation_data=(validXNoisy, validX),\n    epochs=EPOCHS,\n    batch_size=BS)","execution_count":null,"outputs":[]},{"metadata":{"id":"CvQGO3d6H1VU","outputId":"cd52b265-a9e9-4f6d-d64d-2dc55e7661aa","trusted":true},"cell_type":"code","source":"fontdict = {'weight':'bold', 'fontsize':14}\nplt.figure(figsize=(8, 5))\nplt.plot(H.history['loss'], label='trainLoss', linewidth=2)\nplt.plot(H.history['val_loss'], label='valLoss', linewidth=2)\nplt.xlabel('Epoch #', fontdict=fontdict )\nplt.ylabel('Loss #', fontdict=fontdict )\nplt.xticks(fontsize=14)\nplt.yticks(fontsize=14)\nplt.legend()\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"1wnKPKB2H9ru","trusted":true},"cell_type":"code","source":"def plot_samples(n_samples, noisy_data):\n\n  total_samples = len(noisy_data)\n  sample_inds = np.random.choice([i for i in range(total_samples)], n_samples)\n\n  y_hat = autoencoder.predict(noisy_data[sample_inds]).reshape(-1, 28, 28)\n  y = noisy_data[sample_inds].reshape(-1, 28, 28)\n\n  fig, ax = plt.subplots(n_samples, 2, figsize=(1.95,n_samples), gridspec_kw = {'wspace':0, 'hspace':0})\n  for i in range(n_samples):\n    ax[i, 0].imshow(y[i], 'gray')\n    ax[i, 1].imshow(y_hat[i], 'gray')\n    ax[i, 0].axis('off')\n    ax[i, 1].axis('off')","execution_count":null,"outputs":[]},{"metadata":{"id":"qFkaGjP0IBKQ","outputId":"680d8c9e-c9f2-4b9e-bd7a-7e4d97674cb3","trusted":true},"cell_type":"code","source":"plot_samples(8, testXNoisy)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}