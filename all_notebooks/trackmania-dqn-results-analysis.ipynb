{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# DQN on Trackmania\n<center><img src=\"https://cdn1.epicgames.com/b04882669b2e495e9f747c8560488c93/offer/TM_StarterEdition_Store_Landscape_2560x1440-2560x1440-e748deac61ee274e1ef8faa5f40b03cd.jpg?h=270&resize=1&w=480\"/></center><br/>\n\nTrackmania is avery interesting game for AI research. It's a complex environment with a quite sparse reward function. Furthermore, the game has been farly optimised by humans players, as Trackmania is a very competitive game.\nUsing the [TMForge tool](https://github.com/TheoBoyer/TMForge) I implemented the DQN algorithm on Trackmania, and this notebook's purpose is to analyse the results.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport importlib\nimport json","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-05-29T00:09:04.049294Z","iopub.execute_input":"2021-05-29T00:09:04.049958Z","iopub.status.idle":"2021-05-29T00:09:04.059292Z","shell.execute_reply.started":"2021-05-29T00:09:04.049859Z","shell.execute_reply":"2021-05-29T00:09:04.058245Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# The benchmark\nThe map I ran the algorithm on is a custom one that I created especillay for Reinforcement learning algorithm. It contains a lot of checkpoints, reducing the sparsity of the reward function. Also, the multiple half turn parts help the agent because oonce it learned how to passe them, it can generalize it to all half-turns of the map. The ultimate goal is to finish the map to sort of pretrain the agent and then test it on other ones.\n\n<img style=\"margin: auto\" src=\"https://i.imgur.com/QofS9Mz.png\">\n<p style=\"text-align: center; padding: 15px\">\n    <span><i>The reference map for benchmark on TMForge. Available in the TMForge club</i></span>\n</p>\n\n","metadata":{}},{"cell_type":"markdown","source":"# The data\nThe data available is the result folder of the experiment. It contains code of the agent, used hyperparameters, the TMForge config file, the model's weights, the replay buffer, and a csv file containing metrics.\nThe later we be the one we will analyze in the Notebook","metadata":{}},{"cell_type":"code","source":"def load_config(experiment_folder):\n    \"\"\"\n        Check that a certain function exist in a given script. Raise an error if it's not the case\n        Return the actual function if it was found\n    \"\"\"\n    experiment_folder = os.path.join(experiment_folder, \"config.py\")\n    spec = importlib.util.spec_from_file_location(\"config\", experiment_folder)\n    module = importlib.util.module_from_spec(spec)\n    spec.loader.exec_module(module)\n    return module\n\n\n# Feel free to upload your own experiments, fork the notebook and  change this line so you can see the performances of your agent\nexperiment_folder = '../input/tmforge-experiments/frames_buffer'\n\n# Config\nconfig = load_config(experiment_folder)\nwith open(os.path.join(experiment_folder, \"hyperparameters.json\")) as f:\n    hyperparameters = json.load(f)\nmetrics = pd.read_csv(os.path.join(experiment_folder, 'metrics.csv'))\nprint(\"Hyperparameters:\")\nprint(hyperparameters)\nprint(\"Metrics:\")\nmetrics","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-05-29T00:09:04.061005Z","iopub.execute_input":"2021-05-29T00:09:04.06147Z","iopub.status.idle":"2021-05-29T00:09:04.46645Z","shell.execute_reply.started":"2021-05-29T00:09:04.061428Z","shell.execute_reply":"2021-05-29T00:09:04.465444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"frames = np.load(os.path.join(experiment_folder, \"frames_buffer.npy\"))\nactions = np.load(os.path.join(experiment_folder, \"actions_buffer.npy\"))\nrewards = np.load(os.path.join(experiment_folder, \"rewards_buffer.npy\"))\ndones = np.load(os.path.join(experiment_folder, \"dones_buffer.npy\"))","metadata":{"execution":{"iopub.status.busy":"2021-05-29T00:09:04.467843Z","iopub.execute_input":"2021-05-29T00:09:04.468132Z","iopub.status.idle":"2021-05-29T00:09:24.568867Z","shell.execute_reply.started":"2021-05-29T00:09:04.468088Z","shell.execute_reply":"2021-05-29T00:09:24.567736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"idx = -227\nsample = frames[idx]\nprint(actions[idx])\nprint(rewards[idx])\nprint(dones[idx])\nplt.imshow(sample[-1], cmap='gray')\nplt.show()\nplt.imshow(sample[-2], cmap='gray')\nplt.show()\nplt.imshow(np.abs(sample[-1] - sample[-2]), cmap='gray')\nplt.show()\nprint(sample[-3][0, 0])","metadata":{"execution":{"iopub.status.busy":"2021-05-29T00:10:44.285632Z","iopub.execute_input":"2021-05-29T00:10:44.285982Z","iopub.status.idle":"2021-05-29T00:10:44.721582Z","shell.execute_reply.started":"2021-05-29T00:10:44.285949Z","shell.execute_reply":"2021-05-29T00:10:44.720586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Performances of the agent\nFirst let's see how the agent performed ","metadata":{}},{"cell_type":"code","source":"metrics['cp_crossed'] = metrics[\"Reward\"] == 1.0\ncp_reached = metrics[['Episode', 'cp_crossed']].groupby(['Episode']).sum()\nplt.bar(cp_reached.index.values, cp_reached[\"cp_crossed\"].values)\nplt.title(\"Cp reached by the agent during training\")\nplt.xlabel(\"Episode\")\nplt.ylabel(\"Max cp reached\")\nprint(\"The best run reached CP\", np.max(cp_reached[\"cp_crossed\"].values))\nplt.show()\n\nepisode_reward = metrics[\"Episode Reward\"].values\nepisode_reward = episode_reward[episode_reward == episode_reward]\nplt.plot(episode_reward, label='Cumulative rewards')\nplt.title(\"Cumulative rewards obtained during training\")\nplt.xlabel(\"Episode\")\nplt.ylabel(\"Cumulative reward\")\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-05-29T00:09:25.114567Z","iopub.execute_input":"2021-05-29T00:09:25.114899Z","iopub.status.idle":"2021-05-29T00:09:26.143727Z","shell.execute_reply.started":"2021-05-29T00:09:25.114864Z","shell.execute_reply":"2021-05-29T00:09:26.142436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cum_dis_r = 0\ncurr_ep_n = None\nq_values = []\nfor r, n_ep in zip(reversed(metrics[\"Reward\"].values), reversed(metrics[\"Episode\"].values)):\n    if curr_ep_n != n_ep:\n        cum_dis_r = 0\n        curr_ep_n = n_ep\n    cum_dis_r = r + hyperparameters[\"reward_discount_factor\"] * cum_dis_r\n    q_values.append(cum_dis_r)\nq_values = list(reversed(q_values))\nmetrics[\"True Q-value\"] = q_values\n\napprox_error = np.abs(metrics[\"Q-value\"].values - metrics[\"True Q-value\"].values)\n\nplt.plot(metrics[\"Q-value\"].values, label=\"Q-value estimation\")\nplt.plot(metrics[\"True Q-value\"].values, label=\"Q-value truth\")\nplt.title(\"Computed Q-values\")\nplt.xlabel(\"Game steps\")\nplt.ylabel(\"Q-value\")\nplt.legend()\nplt.show()\n\nplt.plot(approx_error)\nplt.title(\"True Q_value approximation error\")\nplt.xlabel(\"Game steps\")\nplt.ylabel(\"Absolute error\")\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-05-29T00:09:26.145175Z","iopub.execute_input":"2021-05-29T00:09:26.145549Z","iopub.status.idle":"2021-05-29T00:09:27.042655Z","shell.execute_reply.started":"2021-05-29T00:09:26.145514Z","shell.execute_reply":"2021-05-29T00:09:27.040996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The Q-value estimation is too high. I used the [double-DQN algorithm](https://arxiv.org/pdf/1509.06461.pdf) which is supposed to overcome this common issue in vanilla DQN. It's possible that my implementation is wrong","metadata":{}},{"cell_type":"markdown","source":"## Technical performances\nAs we do not have a perfect emulator for Trackmania like on the atari games, the deterministic assumption is wrong. \n<center><img src=\"https://i.imgur.com/V51ucym.png\"/></center><br/>\n\nIn the classical framework, theduration of the decision process does not matter because the emulator is paused during it. However it's not the case for Trackmania because the so called emulator is in fact an interface with the game. The later never stops even during the decision process. This can have really bad consequences on the training process because it adds a lot of noise. The implementation of the DQN tries everything to keep the time happening between observing the state and performing the action as cojnstant as possible. Let's study the metric","metadata":{}},{"cell_type":"code","source":"DISPLAY_UPDATE_FRQUENCY = 60\naction_latency = metrics[\"Action Latency\"].values\npercentiles = np.percentile(action_latency, np.arange(100))\nplt.title(\"Distribution of the latency\")\nn_removed = len(action_latency) - np.sum(action_latency < percentiles[-1])\nprint(\"Removed {} samples ({:.3f} % of the data) for clarty\".format(n_removed,100 * n_removed / len(action_latency)))\nplt.xlabel(\"latency\")\nv = plt.hist(action_latency[action_latency < percentiles[-1]], bins=100, density=True)\nlatency_std = action_latency.std()\nlatency_99_std = action_latency[action_latency < percentiles[-1]].std()\nframes_dt = 1/DISPLAY_UPDATE_FRQUENCY\nprint(\"Standard deviation of the latency:\", latency_std)\nprint(\"Standard deviation of the latency of 99% of the frames:\", latency_99_std)\n\nprint(\"Time between two frames:\", frames_dt)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-05-29T00:09:27.044607Z","iopub.execute_input":"2021-05-29T00:09:27.045041Z","iopub.status.idle":"2021-05-29T00:09:27.357825Z","shell.execute_reply.started":"2021-05-29T00:09:27.045006Z","shell.execute_reply":"2021-05-29T00:09:27.35661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see, despite some extreme cases happening rarely (probably when a backup is done, it involves writing muliple Gos files), 99% of the time we have ~8ms latency which more than enough to have no big consequences on a 60hz display.","metadata":{}}]}