{"cells":[{"metadata":{},"cell_type":"markdown","source":"<a id=\"toc_section\"></a>\n## Contents of this notebook\n\n[**Raghav Rastogi**](https://www.kaggle.com/raghavrastogi75) \n\n\n* [Data import](#1)\n* [Checking NULL values](#2)\n* [Exploratory Data Analysis](#3)\n* [Excluding the test set before further analysis](#4)\n* [Continuing the EDA](#5)\n    - [Visualising the Geographical data](#6)   \n    - [Correlation of the data](#7)\n    - [Scatter plot for highly correlated attributes](#8)\n* [Data Preparation](#9)\n* [Machine Learning model selection and training](#10)\n* [Evaluation and RMSE](#11)\n* [Conclusion](#12)\n   ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"I have referred the book 'Hands-on MAchine Learning with scikit-Learn, Keras, and Tensorflow' by AurÃ©lien GÃ©ron and applied on this dataset. I highly recommend it if you are a beginner. If you have a question or feedback, do not hesitate to write and if you find this kernal helpful, please <b><font color=\"orange\">do not forget to </font><font color=\"green\">UPVOTE </font></b> ðŸ™‚","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* # <span id=\"1\"></span> Data import\nLet's import the data and have a look at some rows and data types","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"housing = pd.read_csv('/kaggle/input/housesalesprediction/kc_house_data.csv')\nprint(housing.info())\n#housing.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <span id=\"2\"></span> Checking Null Values\nLet's check if there are any blank vaules in bulk","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nhousing.isna().count()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are no 'NULL' values in the data set, however, there are a lot of '0s' in the data set which is fine. Had there been null values, there are 3 ways to approach it:\n\n1. Remove the column itself if it is not important\n2. Remove the missing rows if the NULLs are very less\n3. Replace the NULL values with the median or mean or '0' in case of numerical columns.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"The columns 'yr_renovated', 'sqft_basement', 'view' and 'waterfront' has huge number of '0' values filled. We should check how important they are using correlation.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#removing id and date as they are not important for prediction\nhousing = housing.drop(columns = ['id','date'])\n#print(housing.shape)\n                        ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <span id=\"3\"></span> Exploratry Data analysis\nLet's a look at the data to know more about it and gain some insights before trying to predict it.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#looking at the data types\nhousing.dtypes\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All of the datatypes are either integers or floats","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"housing.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's plot a histogram to get a better feel of the data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline\nimport matplotlib.pyplot as plt\nhousing.hist(bins=50, figsize=(20,15))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We first observe that some are categorical and some are continuos. For example 'view' and 'condition' are categorical. Also many histograms are tail heavy: they extend much farther to the right of the median than to the left. This might make the ML Algorithms a little harder to detect patterns. We will have to use Standardisation to make them symmetrical suc that ML algos are able to perform better on the dataset.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# <span id=\"4\"></span> Excluding the test set before further analysis\n\nI am doing this to ensure that I have no insight whatsoever about the test set and this will help in getting completely unbiased results in the end.\n\nWe use stratified shuffle split to evenly separate the training set and test set from the total data. For example we a male/female values on the data as 60%/30%, we would have the same distribution of the male/female ratio in our data set. Using this strategy makes our validation set much more reliable.\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Let's choose 'year built' as the criteria of stratified shuffle shift as it seems like a good distributed factor of the data. Plotting it to get a view of the distribution.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"housing['yr_built'].hist()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looking at the numerical values.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"housing[\"year_cat\"] = pd.cut(housing[\"yr_built\"],bins=[0, 1920, 1940, 1960, 1980,2000, np.inf], labels=[1, 2, 3, 4, 5,6])\nhousing[\"year_cat\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see we have a good enough values for each 20 year bucket. This makes sure that we are not using an uneven distribution as out test test.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Separating the test set from the data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import StratifiedShuffleSplit\n\nsplit = StratifiedShuffleSplit(n_splits = 1, test_size = 0.2, random_state = 42)\nfor train_index,test_index in split.split(housing, housing[\"year_cat\"]):\n    strat_train_set = housing.loc[train_index]\n    strat_test_set = housing.loc[test_index]\n# looking at the percentage wise distribution of bucket of years\nstrat_test_set[\"year_cat\"].value_counts() / len(strat_test_set)\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <span id=\"5\"></span> Continuing the EDA\nNow that we are done with the separation of train and test set. Lets explore the train set more.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"housing = strat_train_set.copy()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <span id=\"4\"></span>Visualising the geographical data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"housing.plot(kind = 'scatter', x = 'long', y = 'lat', alpha = 2, figsize = (15,15),c = 'price',colorbar = True,cmap=plt.get_cmap(\"cool\"))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We are kind of able to see the boundaries of the actual location and where the densities are higher. We are also able to see the darker areas with a higher price.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## <span id=\"7\"></span> Correlation of the data\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_matrix = housing.corr()\nplt.figure(figsize = (10,10))\ns = corr_matrix['price'].sort_values(ascending = False)\nprint(s)\ns.plot.bar()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We observe that sqft_living has the highest correlation with the price of the house which seems natural. Followed by 'grade' and 'Sqft_above'","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## <span id=\"8\"></span> Scatter plot for highly correlated attributes\nLet's now have a look at the scatter plot with the price and other highly correlated attributes","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nattributes = ['price','sqft_living','grade','sqft_above','sqft_living15']\nhousing_at = housing.loc[:,attributes]\n#print(housing_at)\nsns.pairplot(housing_at)\nplt.show()\n\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can now clearly observe the linear correlation of the attributes specially with the price.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Looking at the most correlated attribute even closer\nLet's have a look at the most correlated attribute more closely.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (15,15))\nsns.scatterplot(x = housing['price'], y = housing['sqft_living'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We do have some outliers but they are along the same trend. So we are good to keep these. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# <span id=\"9\"></span> Data Preparation\nLet's now prepare the data to perform Machine Learning.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"housing = strat_train_set.drop(\"price\", axis=1)\nhousing_labels = strat_train_set[\"price\"].copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's now scale the data using StandardScaler which will subtract the mean value so that mean is 0 and then divide by standard deviation so that it has unit variance. Using pipeline for it's implementation.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nnum_pipeline = Pipeline([('std_scaler', StandardScaler())])\n\nhousing_std_train = num_pipeline.fit_transform(housing)\nhousing_prepared = pd.DataFrame(housing_std_train, columns=housing.columns, index=housing.index)\nhousing_prepared\nhousing_labels = np.log1p(housing_labels)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <span id=\"10\"></span> Machine learning model selection and training\nNow it's time to finally select and train a model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestRegressor\nparam_grid = [\n{'n_estimators': [25,50], 'max_features': [8 ,10, 15]},\n{'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},\n]\nforest_reg = RandomForestRegressor()\ngrid_search = GridSearchCV(forest_reg, param_grid, cv=5,scoring='neg_mean_squared_error',return_train_score=True)\ngrid_search.fit(housing_prepared, housing_labels)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looking at the best parameters","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_search.best_params_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can keep changing the max features and n_estimators to get the best value","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_importances = grid_search.best_estimator_.feature_importances_\nfeature_importances\n#print(strat_train_set.columns.tolist())\nsorted(zip(feature_importances,housing.columns.tolist()),reverse = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As expected we get 'sqft_living' as the best attribute to predict the price of the house.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# <span id=\"11\"></span> Evaluation and RMSE\nEvaluating the model on test set","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"final_model = grid_search.best_estimator_\nX_test = strat_test_set.drop(\"price\", axis=1)\ny_test = strat_test_set[\"price\"].copy()\nX_test_prepared = num_pipeline.transform(X_test)\ny_test_prep = np.log1p(y_test)\nfinal_predictions = final_model.predict(X_test_prepared)\nfinal_rmse = mean_squared_error(y_test_prep, final_predictions,squared = False)\nprint(final_rmse)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We get a decent enough RMSE score. We can try different models to reduce this number further down.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 95% confidence range\nFinally let's get a 95% confidence range ont the predicted values","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy import stats\nconfidence = 0.95\nsquared_errors = (np.expm1(final_predictions) - np.expm1(y_test_prep)) ** 2\nnp.sqrt(stats.t.interval(confidence, len(squared_errors) - 1,loc=squared_errors.mean(),scale=stats.sem(squared_errors)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <span id=\"12\"></span> Conclusion\n\nThis is my first attempt at end to end EDA and Machine Learning prediction. Please let me know how I can improve this code even more.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}