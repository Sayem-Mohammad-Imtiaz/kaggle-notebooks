{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Start here"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(os.listdir(\"../input/hpcc20steps/\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install tensorflow==1.14.0","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"variables_name = pd.read_csv(\"../input/hpcc20steps/variables_name.csv\", header=None)\nfeatures = variables_name.values[:,1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"variables_name","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import json\nwith open(\"../input/hpcc20steps/X_train_HPCC_1_20.json\") as of:\n    X_train = np.array(json.load(of))\nwith open(\"../input/hpcc20steps/y_train_HPCC_1_20.json\") as of:\n    y_train = np.array(json.load(of))\nwith open(\"../input/hpcc20steps/X_test_HPCC_1_20.json\") as of:\n    X_test = np.array(json.load(of))\nwith open(\"../input/hpcc20steps/y_test_HPCC_1_20.json\") as of:\n    y_test = np.array(json.load(of))    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape, y_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test.shape, y_test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras import regularizers\nfrom keras.models import Sequential\nfrom keras.layers import LSTM\nfrom keras.layers import Dense\nfrom keras.layers import Activation\nfrom keras.layers import Dropout\nfrom keras.layers import Flatten\nfrom keras.optimizers import Adam\n\n\ndef createModel(l1Nodes, l2Nodes, d1Nodes, d2Nodes, inputShape):\n    # input layer\n    lstm1 = LSTM(l1Nodes, input_shape=inputShape, return_sequences=True)\n    lstm2 = LSTM(l2Nodes, return_sequences=True)\n    flatten = Flatten()\n    dense1 = Dense(d1Nodes)\n    dense2 = Dense(d2Nodes)\n\n    # output layer\n#     outL = Dense(1, activation='relu')\n    outL = Dense(1)\n    # combine the layers\n    layers = [lstm1, lstm2, flatten,  dense1, dense2, outL]\n    # create the model\n    model = Sequential(layers)\n    opt = Adam(learning_rate=0.005)\n    model.compile(optimizer=opt, loss='mse')\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create model\nmodel = createModel(8, 8, 8, 4, (X_train.shape[1], X_train.shape[2]))\nmodel.fit(X_train, y_train, batch_size=8, epochs=30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error as mse","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_train = model.predict(X_train)\nmse(y_train, y_pred_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = model.predict(X_test)\nmse(y_test, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Save the entire model to a HDF5 file.\n# The '.h5' extension indicates that the model shuold be saved to HDF5.\nmodel.save('HPCCv1_model.h5') ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# SHAP"},{"metadata":{"trusted":true},"cell_type":"code","source":"import shap","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\ntf.__version__","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## DeepSHAP"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use the training data for deep explainer => can use fewer instances\nexplainer = shap.DeepExplainer(model, X_train)\n# explain the the testing instances (can use fewer instanaces)\n# explaining each prediction requires 2 * background dataset size runs\nshap_values = explainer.shap_values(X_test)\n# init the JS visualization code\nshap.initjs()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"explainer.expected_value","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(shap_values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shap_values[0].shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shap_values[0][0].shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# shap.force_plot(explainer.expected_value[0], shap_values[0][0][0,:], features)\nprint(features)\nprint(len(features))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"i=0\nj=0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shap_values[0][i][j]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test[i][j].shape\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# shap.force_plot(explainer.expected_value[0], shap_values[0][0], features)\ni = 0\nj = 0\nx_test_df = pd.DataFrame(data=X_test[i][j].reshape(1,10), columns = features)\nshap.force_plot(explainer.expected_value[0], shap_values[0][i][j], x_test_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Check sum of shap values vs prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"shap.__version__","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shap_values[0][i].shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shap_values[0][0].shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"i = 11\npred_i = model.predict(X_test[i:i+1])\nsum_shap_i = shap_values[0][i].sum() + explainer.expected_value[0]\n\npred_i, sum_shap_i","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"They are the same. It looks ok"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot SHAP for ONLY one observation i\ni = 0\nshap.initjs()\n\nx_test_df = pd.DataFrame(data=X_test[i], columns = features)\nshap.force_plot(explainer.expected_value[0], shap_values[0][i], x_test_df)\n## Problem:  Can not take into account many observations at the same time.\n### The pic below explain for only 1 observation of 20 time steps, each time step has 10 features.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### AVERAGE shap for ALL Obs"},{"metadata":{"trusted":true},"cell_type":"code","source":"################# Plot AVERAGE shap values for ALL observations  #####################\n## Consider ABSOLUTE of SHAP values ##\nshap_average_value = np.abs(shap_values[0]).mean(axis=0)\n\nx_average_value = pd.DataFrame(data=X_test.mean(axis=0), columns = features)\nshap.force_plot(0, shap_average_value, x_average_value)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"################# Plot AVERAGE shap values for ALL observations  #####################\n## Consider average (+ is different from -)\nshap_average_value = shap_values[0].mean(axis=0)\n\nx_average_value = pd.DataFrame(data=X_test.mean(axis=0), columns = features)\nshap.force_plot(explainer.expected_value[0], shap_average_value, x_average_value)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shap_values_2D = shap_values[0].reshape(-1,10)\nX_test_2D = X_test.reshape(-1,10)\n\n\nshap_values_2D.shape, X_test_2D.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_test_2d = pd.DataFrame(data=X_test_2D, columns = features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_test_2d.corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shap.summary_plot(shap_values_2D, x_test_2d)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shap.summary_plot(shap_values_2D, x_test_2d, plot_type=\"bar\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len_test_set = X_test_2D.shape[0]\nlen_test_set","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## SHAP for each time step\nNUM_STEPS = 20\nNUM_FEATURES = 10\n\n\n# step = 0\nfor step in range(NUM_STEPS):\n    indice = [i for i in list(range(len_test_set)) if i%NUM_STEPS == step]\n    shap_values_2D_step = shap_values_2D[indice]\n    x_test_2d_step = x_test_2d.iloc[indice]\n    print(\"_______ time step {} ___________\".format(step))\n    shap.summary_plot(shap_values_2D_step, x_test_2d_step, plot_type=\"bar\")\n    shap.summary_plot(shap_values_2D_step, x_test_2d_step)\n    print(\"\\n\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## Outliers vs Non-Outliers"},{"metadata":{"trusted":true},"cell_type":"code","source":"# X_train_outlier\nwith open(\"../input/hpcc20steps/X_train_outlier.json\") as of:\n    X_train_outlier = np.array(json.load(of))\nwith open(\"../input/hpcc20steps/y_train_outlier.json\") as of:\n    y_train_outlier = np.array(json.load(of))\n\n    # X_train_normal\nwith open(\"../input/hpcc20steps/X_train_not_outlier.json\") as of:\n    X_train_not_outlier = np.array(json.load(of))\nwith open(\"../input/hpcc20steps/y_train_not_outlier.json\") as of:\n    y_train_not_outlier = np.array(json.load(of))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## OUTLIERS\nshap_values = explainer.shap_values(X_train_outlier)\ni = 0\nx_test_df = pd.DataFrame(data=X_train_outlier[i], columns = features)\nshap.force_plot(explainer.expected_value[0], shap_values[0][i], x_test_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## NON-OUTLIERS\nshap_values = explainer.shap_values(X_train_not_outlier)\ni = 0\nx_test_df = pd.DataFrame(data=X_train_not_outlier[i], columns = features)\nshap.force_plot(explainer.expected_value[0], shap_values[0][i], x_test_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_not_outlier[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_outlier[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## GradientExplainer"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use the training data for deep explainer => can use fewer instances\nexplainer_2 = shap.GradientExplainer(model, X_train)\n# explain the the testing instances (can use fewer instanaces)\n# explaining each prediction requires 2 * background dataset size runs\nshap_values_2 = explainer_2.shap_values(X_test)\n# init the JS visualization code\nshap.initjs()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"################# Plot AVERAGE shap values for ALL observations  #####################\n## Consider ABSOLUTE of SHAP values ##\nshap_average_abs_value_2 = np.abs(shap_values_2[0]).mean(axis=0)\n\nx_average_value = pd.DataFrame(data=X_test.mean(axis=0), columns = features)\nshap.force_plot(0, shap_average_abs_value_2, x_average_value)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Importance for each training instance with SHAP GradientExplainer "},{"metadata":{"trusted":true},"cell_type":"code","source":"################# Plot AVERAGE shap values for ALL observations  #####################\n## Consider ABSOLUTE of SHAP values ##\nshap.initjs()\nshap_values_train = explainer.shap_values(X_train)\n\nshap_average_abs_value_train = np.abs(shap_values_train[0]).mean(axis=0)\n\nx_average_value_train = pd.DataFrame(data=X_train.mean(axis=0), columns = features)\nshap.force_plot(0, shap_average_abs_value_train, x_average_value_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shap_values_train_2D = shap_values_train[0].reshape(-1,10)\nX_train_2D = X_train.reshape(-1,10)\n\n\nshap.summary_plot(shap_values_train_2D, X_train_2D, features)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Some Comments**:\n- *CPU1 Temp* is the most important feature\n- *Fan3 Speed* and *Fan2 Speed* seem to have positive corr with the output. Conversely, *Fan4* and *Fan1* have negative relationships with the output.\n- *CPU Load* doesn't have a clear linear relationship with the output (red dots at the both sides)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# COLOR: https://seaborn.pydata.org/tutorial/color_palettes.html\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfor i, feature in enumerate(features):\n    print(feature)\n\n    plt.figure(figsize = (8,6)) \n    tmp = shap_values_train[0][:,:,i].reshape((-1,20))\n    print(tmp.shape)\n    plot_shap = sns.heatmap(tmp, cmap=\"coolwarm\")\n    plt.show(plot_shap)\n    print(\"-----------\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"** Some Comments **\n\n- *CPU1 Temp*: Light color at early time steps. It starts bolder from 10th to 18th steps => These steps play an important roles in prediction.\n(recall that output is the sum of 20*10 importance scores)\n- *CPU2 Temp*: time step 9th, 10th have light color; some in the end have darker color.\n- *Inlet Temp* & *Power consumption*: Early time steps have the most impact on the prediction.\n- *CPU Load*: Almost in blue => Has negative impact on the prediction\n- *Memory Usage*: As opposed to *CPU Load*\n- *Fan1* & *Fan 3*: almost blue; while *Fan2 & 4* are in orange"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}