{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import sklearn\nimport numpy as np\nimport re\nimport string\nimport pandas as pd\nimport nltk\nimport os\nimport sys\nfrom sklearn import feature_extraction\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.cluster import KMeans\nfrom datetime import datetime\nfrom nltk.stem.snowball import SnowballStemmer\nfrom sklearn.datasets import fetch_20newsgroups\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics.pairwise import euclidean_distances\n\nstemmer = SnowballStemmer(\"english\")\nnltk.download('stopwords')\nnltk.download('punkt')\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef tokenize_and_stem(text):\n    #Clean Headers\n    text = re.sub(r'(From:\\s+[^\\n]+\\n)', '', text)\n    text = re.sub(r'(Subject:[^\\n]+\\n)', '', text)\n    text = re.sub(r'(([\\sA-Za-z0-9\\-]+)?[A|a]rchive-name:[^\\n]+\\n)', '', text)\n    text = re.sub(r'(Last-modified:[^\\n]+\\n)', '', text)\n    text = re.sub(r'(Version:[^\\n]+\\n)', '', text)\n    #Clean More Text\n    text = text.lower()\n    text = text.strip()\n    re_url = re.compile(r'(?:http|ftp|https)://(?:[\\w_-]+(?:(?:\\.[\\w_-]+)+))(?:[\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?')\n    re_email = re.compile('(?:[a-z0-9!#$%&\\'*+/=?^_`{|}~-]+(?:\\.[a-z0-9!#$%&\\'*+/=?^_`{|}~-]+)*|\"(?:[\\x01-\\x08\\x0b\\x0c\\x0e-\\x1f\\x21\\x23-\\x5b\\x5d-\\x7f]|\\\\[\\x01-\\x09\\x0b\\x0c\\x0e-\\x7f])*\")@(?:(?:[a-z0-9](?:[a-z0-9-]*[a-z0-9])?\\.)+[a-z0-9](?:[a-z0-9-]*[a-z0-9])?|\\[(?:(?:(2(5[0-5]|[0-4][0-9])|1[0-9][0-9]|[1-9]?[0-9]))\\.){3}(?:(2(5[0-5]|[0-4][0-9])|1[0-9][0-9]|[1-9]?[0-9])|[a-z0-9-]*[a-z0-9]:(?:[\\x01-\\x08\\x0b\\x0c\\x0e-\\x1f\\x21-\\x5a\\x53-\\x7f]|\\\\[\\x01-\\x09\\x0b\\x0c\\x0e-\\x7f])+)\\])')\n    text = re.sub(re_url, '', text)\n    text = re.sub(re_email, '', text)\n    text = re.sub(f'[{re.escape(string.punctuation)}]', '', text)\n    text = re.sub(r'(\\d+)', ' ', text)\n    text = re.sub(r'(\\s+)', ' ', text)\n    # Remove repeating chars\n    text = re . sub (r\"!+\", \"! \", text )\n    text = re . sub (r\" \\.+ \", \". \", text )\n    text = re . sub (r\" \\?+ \", \"? \", text )\n    text = re . sub (r\" \\*+ \", \"* \", text )\n    text = re . sub (r\"\\ >+\", \"> \", text )\n    text = re . sub (r\"\\ <+\", \"< \", text )\n    # Clean shorthands\n    text = re . sub (\"\\’s\",\" \", text )\n    text = re . sub (\"\\’ve\",\" have \", text )\n    text = re . sub (\"\\’re\", \" are \", text )\n    text = re . sub (\"\\’ll\", \" will \", text )\n    text = re . sub (\"I’m\", \"I am\", text )\n    text = re . sub (\"\\’d\", \" would \", text )\n    text = re . sub (\"n’t\", \" not \", text )\n    text = re . sub (\" can ’t\", \" can not \", text , flags = re . IGNORECASE )\n    text = re . sub (\"i\\.e\\.\", \"id est \", text , flags = re . IGNORECASE )\n    text = re . sub (\"e\\.g\\.\", \" for example \", text , flags = re . IGNORECASE )\n    text = re . sub (\"e- mail \", \" email \", text , flags = re . IGNORECASE )\n    # Special characters\n    text = re . sub (\"\\$\",\" dollar \", text )\n    text = re . sub (\"\\&\", \" and \", text )\n    text = re . sub (\"\\%\", \" percent \", text )\n    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n    filtered_tokens = []\n    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n    for token in tokens:\n        if re.search('[a-zA-Z]', token):\n            filtered_tokens.append(token)\n    stems = [stemmer.stem(t) for t in filtered_tokens]\n    return stems","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def splite_data(data,target,size):\n    X_train, X_test, y_train, y_test = train_test_split(data,target,random_state=42, test_size=size)\n    print(\"Data Train\",(1-size)*100,'%#################',\"Data Test\",size*100,'%')\n    print ('Train Size:', len(X_train),'Test Size:', len(X_test))\n    print ('#####################################')\n    return X_train, X_test, y_train, y_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset=fetch_20newsgroups(subset='all',shuffle=True,random_state=42)\n\nX_train_80, X_test_20, y_train_80, y_test_20=splite_data(dataset.data,dataset.target,0.20)\nX_train_70, X_test_30, y_train_70, y_test_30=splite_data(dataset.data,dataset.target,0.30)\nX_train_60, X_test_40, y_train_60, y_test_40=splite_data(dataset.data,dataset.target,0.40)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def clusters_visualization(PCA,clusters,group,dataset,test_type):\n    \n    print(\"Cluster Type\",test_type)\n    #create data frame that has the result of the MDS plus the cluster numbers and titles\n    \n    df = pd.DataFrame(dict(x=PCA[:,0], y=PCA[:, 1], z=PCA[:, 2], cluster=clusters,news_group=group, title=[dataset.target_names[i] for i in group])) \n    #group by cluster\n    groups = df.groupby('cluster')\n    # For Lable Checking For Give Cluster\n    df22 = df.groupby(['cluster', 'news_group'])['news_group'].size()\n    df2 = df22.reset_index(level='cluster').groupby('cluster')['news_group'].idxmax().reset_index(name='news_group')\n    news_groups=['alt.atheism', 'comp.graphics','comp.os.ms-windows.misc','comp.sys.ibm.pc.hardware','comp.sys.mac.hardware', 'comp.windows.x',   'misc.forsale',   'rec.autos',   'rec.motorcycles', 'rec.sport.baseball',  'rec.sport.hockey',  'sci.crypt',  'sci.electronics',  'sci.med',   'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n    \n    print (\"########External Evaluation IntraCluster (Purity)#########\")\n    \n    df3 = df22.reset_index(level='cluster').groupby('cluster')['news_group'].max().reset_index(name='news_group')\n    df4 = df22.reset_index(level='cluster').groupby('cluster')['news_group'].sum().reset_index(name='news_group')\n    print(pd.to_numeric(df3['news_group'])/pd.to_numeric(df4['news_group']))\n    \n    print (\"###################################################\")\n    \n    \n    \n    # set up plot\n    fig = plt.figure(figsize=(15, 9))\n    ax = fig.add_subplot(111, projection='3d')\n    ax.margins(0.07) # Optional, just adds 7% padding to the autoscaling\n\n    for name, group in groups:\n        ax.plot(group.x, group.y, group.z, marker='o', linestyle='', ms=10, label=news_groups[pd.to_numeric(df2.loc[df2['cluster'] == name, 'news_group'].iloc[0])], mec='none')\n        ax.set_aspect('auto')\n        ax.tick_params(\\\n            axis= 'x',          # changes apply to the x-axis\n            which='both',      # both major and minor ticks are affected\n            bottom='off',      # ticks along the bottom edge are off\n            top='off',         # ticks along the top edge are off\n            labelbottom='off')\n        ax.tick_params(\\\n            axis= 'y',         # changes apply to the y-axis\n            which='both',      # both major and minor ticks are affected\n            left='off',      # ticks along the bottom edge are off\n            top='off',         # ticks along the top edge are off\n            labelleft='off')\n\n    ax.legend(numpoints=1)  #show legend with only 1 point   \n  \n\n    plt.show() #show the plot\n    plt.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def compute_KMEAN_cluster(X_train, X_test, y_train, y_test,num_clusters,dataset):\n    \n    #Train\n    tfidf_vectorizer = TfidfVectorizer(max_df=0.55, max_features=20000,\n                                 min_df=1, stop_words='english',\n                                 use_idf=True, tokenizer=tokenize_and_stem)\n    \n    tfidf = tfidf_vectorizer.fit_transform(X_train)\n\n    #dist = 1 - cosine_similarity(tfidf)\n    \n    kmeans = KMeans(n_clusters=num_clusters ,init='k-means++',max_iter=100, n_init=1 ).fit(tfidf)\n    clusters = kmeans.labels_.tolist()\n    \n    pca = PCA(n_components = 3).fit(tfidf.toarray())\n    X_pca = pca.transform(tfidf.toarray())\n    \n    clusters_visualization(X_pca,clusters,y_train,dataset,'Train'+' & clusters ('+str(num_clusters)+')' )\n    \n    #Test\n    tfidf2 = tfidf_vectorizer.transform(X_test)\n\n    #dist2 = 1 - cosine_similarity(tfidf2)\n   \n    \n    Y=kmeans.predict(tfidf2)\n    \n    pca = PCA(n_components = 3).fit(tfidf2.toarray())\n    X_pca = pca.transform(tfidf2.toarray())\n    \n    clusters_visualization(X_pca,Y,y_test,dataset,'Test'+' & clusters ('+str(num_clusters)+')' )\n    \n    #InterCluster Distance  ('Distance Between Clusters')\n    distances = euclidean_distances(kmeans.cluster_centers_)\n    tri_dists = distances[np.triu_indices(num_clusters, 1)]\n    print(\"InterCluster Distance  ('Distance Between Clusters')\")\n    print( \"Complete Linkage Distance\",tri_dists.max())\n    print( \"Average Linkage Distance\", tri_dists.mean())\n    print( \"Single Linkage Distance\", tri_dists.min())\n    print(\"InterCluster Distance  ('Distance Between Clusters') END\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for cluster in [3,6,12,20]:\n    compute_KMEAN_cluster(X_train_80, X_test_20, y_train_80, y_test_20,cluster,dataset)\n    compute_KMEAN_cluster(X_train_70, X_test_30, y_train_70, y_test_30,cluster,dataset)\n    compute_KMEAN_cluster(X_train_60, X_test_40, y_train_60, y_test_40,cluster,dataset)  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}