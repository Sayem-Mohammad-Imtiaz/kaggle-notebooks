{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-09T03:50:23.007379Z","iopub.execute_input":"2021-06-09T03:50:23.007778Z","iopub.status.idle":"2021-06-09T03:50:25.936908Z","shell.execute_reply.started":"2021-06-09T03:50:23.007695Z","shell.execute_reply":"2021-06-09T03:50:25.936066Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#!pip install torchvision\n\n#!pip install cv2\n#!pip install albumentations\n#!pip3 install torch==1.8.1+cu111 torchvision==0.9.1+cu111 torchaudio===0.8.1 -f https://download.pytorch.org/whl/torch_stable.html\n#!pip install seaborn\n#!conda install pytorch torchvision torchaudio cudatoolkit=10.2 -c pytorch\n!pip install -q -U segmentation-models-pytorch albumentations > /dev/null","metadata":{"execution":{"iopub.status.busy":"2021-06-09T03:50:25.939857Z","iopub.execute_input":"2021-06-09T03:50:25.940139Z","iopub.status.idle":"2021-06-09T03:50:37.457818Z","shell.execute_reply.started":"2021-06-09T03:50:25.940113Z","shell.execute_reply":"2021-06-09T03:50:37.456806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\ntorch.cuda.is_available()","metadata":{"execution":{"iopub.status.busy":"2021-06-09T03:50:37.459824Z","iopub.execute_input":"2021-06-09T03:50:37.460094Z","iopub.status.idle":"2021-06-09T03:50:38.661596Z","shell.execute_reply.started":"2021-06-09T03:50:37.460065Z","shell.execute_reply":"2021-06-09T03:50:38.660771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os, cv2\nos.environ['CUDA_VISIBLE_DEVICES']='2, 3'\nimport numpy as np\nimport pandas as pd\nimport random, tqdm\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport glob\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nimport albumentations as album\nimport segmentation_models_pytorch as smp","metadata":{"execution":{"iopub.status.busy":"2021-06-09T03:50:38.663421Z","iopub.execute_input":"2021-06-09T03:50:38.66379Z","iopub.status.idle":"2021-06-09T03:50:41.730822Z","shell.execute_reply.started":"2021-06-09T03:50:38.663753Z","shell.execute_reply":"2021-06-09T03:50:41.730001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#conda install pytorch torchvision torchaudio cudatoolkit=11.1 -c pytorch -c conda-forge","metadata":{"execution":{"iopub.status.busy":"2021-06-09T03:50:41.732192Z","iopub.execute_input":"2021-06-09T03:50:41.732523Z","iopub.status.idle":"2021-06-09T03:50:41.738402Z","shell.execute_reply.started":"2021-06-09T03:50:41.732488Z","shell.execute_reply":"2021-06-09T03:50:41.736897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DATA_DIR = '../input/deepglobe-land-cover-classification-dataset'\n\nmetadata_df = pd.read_csv(os.path.join(DATA_DIR, 'metadata.csv'))\nmetadata_df = metadata_df[metadata_df['split']=='train']\nmetadata_df = metadata_df[['image_id', 'sat_image_path', 'mask_path']]\nmetadata_df['sat_image_path'] = metadata_df['sat_image_path'].apply(lambda img_pth: os.path.join(DATA_DIR, img_pth))\nmetadata_df['mask_path'] = metadata_df['mask_path'].apply(lambda img_pth: os.path.join(DATA_DIR, img_pth))\n# Shuffle DataFrame\nmetadata_df = metadata_df.sample(frac=1).reset_index(drop=True)\n\n# Perform 90/10 split for train / val\nvalid_df = metadata_df.sample(frac=0.1, random_state=42)\ntrain_df = metadata_df.drop(valid_df.index)\nlen(train_df), len(valid_df)","metadata":{"execution":{"iopub.status.busy":"2021-06-09T03:50:41.739657Z","iopub.execute_input":"2021-06-09T03:50:41.740054Z","iopub.status.idle":"2021-06-09T03:50:41.783127Z","shell.execute_reply.started":"2021-06-09T03:50:41.740019Z","shell.execute_reply":"2021-06-09T03:50:41.782335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class_dict = pd.read_csv(os.path.join(DATA_DIR, 'class_dict.csv'))\n# Get class names\nclass_names = class_dict['name'].tolist()\n# Get class RGB values\nclass_rgb_values = class_dict[['r','g','b']].values.tolist()\n\nprint('All dataset classes and their corresponding RGB values in labels:')\nprint('Class Names: ', class_names)\nprint('Class RGB values: ', class_rgb_values)","metadata":{"execution":{"iopub.status.busy":"2021-06-09T03:50:41.784417Z","iopub.execute_input":"2021-06-09T03:50:41.784736Z","iopub.status.idle":"2021-06-09T03:50:41.801977Z","shell.execute_reply.started":"2021-06-09T03:50:41.784702Z","shell.execute_reply":"2021-06-09T03:50:41.801164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Useful to shortlist specific classes in datasets with large number of classes\nselect_classes = ['urban_land', 'agriculture_land', 'rangeland', 'forest_land', 'water', 'barren_land', 'unknown']\n\n# Get RGB values of required classes\nselect_class_indices = [class_names.index(cls.lower()) for cls in select_classes]\nselect_class_rgb_values =  np.array(class_rgb_values)[select_class_indices]\n\nprint('Selected classes and their corresponding RGB values in labels:')\nprint('Class Names: ', class_names)\nprint('Class RGB values: ', class_rgb_values)","metadata":{"execution":{"iopub.status.busy":"2021-06-09T03:50:41.805377Z","iopub.execute_input":"2021-06-09T03:50:41.805625Z","iopub.status.idle":"2021-06-09T03:50:41.815282Z","shell.execute_reply.started":"2021-06-09T03:50:41.8056Z","shell.execute_reply":"2021-06-09T03:50:41.814289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# helper function for data visualization\ndef visualize(**images):\n    \"\"\"\n    Plot images in one row\n    \"\"\"\n    n_images = len(images)\n    plt.figure(figsize=(20,8))\n    for idx, (name, image) in enumerate(images.items()):\n        plt.subplot(1, n_images, idx + 1)\n        plt.xticks([]); \n        plt.yticks([])\n        # get title from the parameter names\n        plt.title(name.replace('_',' ').title(), fontsize=20)\n        plt.imshow(image)\n    plt.show()\n\n# Perform one hot encoding on label\ndef one_hot_encode(label, label_values):\n    \"\"\"\n    Convert a segmentation image label array to one-hot format\n    by replacing each pixel value with a vector of length num_classes\n    # Arguments\n        label: The 2D array segmentation image label\n        label_values\n        \n    # Returns\n        A 2D array with the same width and hieght as the input, but\n        with a depth size of num_classes\n    \"\"\"\n    semantic_map = []\n    for colour in label_values:\n        equality = np.equal(label, colour)\n        class_map = np.all(equality, axis = -1)\n        semantic_map.append(class_map)\n    semantic_map = np.stack(semantic_map, axis=-1)\n\n    return semantic_map\n    \n# Perform reverse one-hot-encoding on labels / preds\ndef reverse_one_hot(image):\n    \"\"\"\n    Transform a 2D array in one-hot format (depth is num_classes),\n    to a 2D array with only 1 channel, where each pixel value is\n    the classified class key.\n    # Arguments\n        image: The one-hot format image \n        \n    # Returns\n        A 2D array with the same width and hieght as the input, but\n        with a depth size of 1, where each pixel value is the classified \n        class key.\n    \"\"\"\n    x = np.argmax(image, axis = -1)\n    return x\n\n# Perform colour coding on the reverse-one-hot outputs\ndef colour_code_segmentation(image, label_values):\n    \"\"\"\n    Given a 1-channel array of class keys, colour code the segmentation results.\n    # Arguments\n        image: single channel array where each value represents the class key.\n        label_values\n\n    # Returns\n        Colour coded image for segmentation visualization\n    \"\"\"\n    colour_codes = np.array(label_values)\n    x = colour_codes[image.astype(int)]\n\n    return x","metadata":{"execution":{"iopub.status.busy":"2021-06-09T03:50:41.820452Z","iopub.execute_input":"2021-06-09T03:50:41.820704Z","iopub.status.idle":"2021-06-09T03:50:41.832236Z","shell.execute_reply.started":"2021-06-09T03:50:41.820679Z","shell.execute_reply":"2021-06-09T03:50:41.831419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class LandCoverDataset(torch.utils.data.Dataset):\n\n    \"\"\"DeepGlobe Land Cover Classification Challenge Dataset. Read images, apply augmentation and preprocessing transformations.\n    \n    Args:\n        df (str): DataFrame containing images / labels paths\n        class_rgb_values (list): RGB values of select classes to extract from segmentation mask\n        augmentation (albumentations.Compose): data transfromation pipeline \n            (e.g. flip, scale, etc.)\n        preprocessing (albumentations.Compose): data preprocessing \n            (e.g. noralization, shape manipulation, etc.)\n    \n    \"\"\"\n    def __init__(\n            self, \n            df,\n            class_rgb_values=None, \n            augmentation=None, \n            preprocessing=None,\n    ):\n        self.image_paths = df['sat_image_path'].tolist()\n        self.mask_paths = df['mask_path'].tolist()\n        \n        self.class_rgb_values = class_rgb_values\n        self.augmentation = augmentation\n        self.preprocessing = preprocessing\n    \n    def __getitem__(self, i):\n        \n        # read images and masks\n        image = cv2.cvtColor(cv2.imread(self.image_paths[i]), cv2.COLOR_BGR2RGB)\n        mask = cv2.cvtColor(cv2.imread(self.mask_paths[i]), cv2.COLOR_BGR2RGB)\n        \n        # one-hot-encode the mask\n        mask = one_hot_encode(mask, self.class_rgb_values).astype('float')\n        \n        # apply augmentations\n        if self.augmentation:\n            sample = self.augmentation(image=image, mask=mask)\n            image, mask = sample['image'], sample['mask']\n        \n        # apply preprocessing\n        if self.preprocessing:\n            sample = self.preprocessing(image=image, mask=mask)\n            image, mask = sample['image'], sample['mask']\n            \n        return image, mask\n        \n    def __len__(self):\n        # return length of \n        return len(self.image_paths)","metadata":{"execution":{"iopub.status.busy":"2021-06-09T03:50:41.834519Z","iopub.execute_input":"2021-06-09T03:50:41.834851Z","iopub.status.idle":"2021-06-09T03:50:41.84713Z","shell.execute_reply.started":"2021-06-09T03:50:41.834816Z","shell.execute_reply":"2021-06-09T03:50:41.846273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = LandCoverDataset(train_df, class_rgb_values=select_class_rgb_values)\nrandom_idx = random.randint(0, len(dataset)-1)\nimage, mask = dataset[2]\n\nvisualize(\n    original_image = image,\n    ground_truth_mask = colour_code_segmentation(reverse_one_hot(mask), select_class_rgb_values),\n    one_hot_encoded_mask = reverse_one_hot(mask)\n)","metadata":{"execution":{"iopub.status.busy":"2021-06-09T03:50:41.848447Z","iopub.execute_input":"2021-06-09T03:50:41.848752Z","iopub.status.idle":"2021-06-09T03:50:45.3647Z","shell.execute_reply.started":"2021-06-09T03:50:41.848727Z","shell.execute_reply":"2021-06-09T03:50:45.363844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_training_augmentation():\n    train_transform = [\n        album.RandomCrop(height=1024, width=1024, always_apply=True),\n        album.HorizontalFlip(p=0.5),\n        album.VerticalFlip(p=0.5),\n    ]\n    return album.Compose(train_transform)\n\n\ndef get_validation_augmentation():\n    train_transform = [\n        album.CenterCrop(height=1024, width=1024, always_apply=True),\n    ]\n    return album.Compose(train_transform)\n\n\ndef to_tensor(x, **kwargs):\n    return x.transpose(2, 0, 1).astype('float32')\n\n\ndef get_preprocessing(preprocessing_fn=None):\n    \"\"\"Construct preprocessing transform    \n    Args:\n        preprocessing_fn (callable): data normalization function \n            (can be specific for each pretrained neural network)\n    Return:\n        transform: albumentations.Compose\n    \"\"\"\n    _transform = []\n    if preprocessing_fn:\n        _transform.append(album.Lambda(image=preprocessing_fn))\n    _transform.append(album.Lambda(image=to_tensor, mask=to_tensor))\n        \n    return album.Compose(_transform)","metadata":{"execution":{"iopub.status.busy":"2021-06-09T03:50:45.366103Z","iopub.execute_input":"2021-06-09T03:50:45.366621Z","iopub.status.idle":"2021-06-09T03:50:45.37627Z","shell.execute_reply.started":"2021-06-09T03:50:45.366587Z","shell.execute_reply":"2021-06-09T03:50:45.375518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"augmented_dataset = LandCoverDataset(\n    train_df, \n    augmentation=get_training_augmentation(),\n    class_rgb_values=select_class_rgb_values,\n)\n\nrandom_idx = random.randint(0, len(augmented_dataset)-1)\n\n# Different augmentations on image/mask pairs\nfor idx in range(3):\n    image, mask = augmented_dataset[idx]\n    visualize(\n        original_image = image,\n        ground_truth_mask = colour_code_segmentation(reverse_one_hot(mask), select_class_rgb_values),\n        one_hot_encoded_mask = reverse_one_hot(mask)\n    )","metadata":{"execution":{"iopub.status.busy":"2021-06-09T03:50:45.377784Z","iopub.execute_input":"2021-06-09T03:50:45.378425Z","iopub.status.idle":"2021-06-09T03:50:51.379574Z","shell.execute_reply.started":"2021-06-09T03:50:45.378389Z","shell.execute_reply":"2021-06-09T03:50:51.378353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ENCODER = 'resnet50'\nENCODER_WEIGHTS = 'imagenet'\nCLASSES = select_classes\nACTIVATION = 'sigmoid' # could be None for logits or 'softmax2d' for multiclass segmentation\n\n# create segmentation model with pretrained encoder\nmodel = smp.DeepLabV3Plus(\n    encoder_name=ENCODER, \n    encoder_weights=ENCODER_WEIGHTS, \n    classes=len(CLASSES), \n    activation=ACTIVATION,\n)\n\npreprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)","metadata":{"execution":{"iopub.status.busy":"2021-06-09T03:50:51.380938Z","iopub.execute_input":"2021-06-09T03:50:51.381327Z","iopub.status.idle":"2021-06-09T03:50:58.0143Z","shell.execute_reply.started":"2021-06-09T03:50:51.381291Z","shell.execute_reply":"2021-06-09T03:50:58.01345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = LandCoverDataset(\n    train_df, \n    augmentation=get_training_augmentation(),\n    preprocessing=get_preprocessing(preprocessing_fn),\n    class_rgb_values=select_class_rgb_values,\n)\n\nvalid_dataset = LandCoverDataset(\n    valid_df, \n    augmentation=get_validation_augmentation(),\n    preprocessing=get_preprocessing(preprocessing_fn),\n    class_rgb_values=select_class_rgb_values,\n)\n\n# Get train and val data loaders\ntrain_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=2)\nvalid_loader = DataLoader(valid_dataset, batch_size=1, shuffle=False, num_workers=4)","metadata":{"execution":{"iopub.status.busy":"2021-06-09T03:50:58.015573Z","iopub.execute_input":"2021-06-09T03:50:58.016083Z","iopub.status.idle":"2021-06-09T03:50:58.022501Z","shell.execute_reply.started":"2021-06-09T03:50:58.016044Z","shell.execute_reply":"2021-06-09T03:50:58.021684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#!pip install torch ","metadata":{"execution":{"iopub.status.busy":"2021-06-09T03:50:58.023741Z","iopub.execute_input":"2021-06-09T03:50:58.024243Z","iopub.status.idle":"2021-06-09T03:50:58.032445Z","shell.execute_reply.started":"2021-06-09T03:50:58.024206Z","shell.execute_reply":"2021-06-09T03:50:58.031621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#print(torch.cuda.device_count())\ntorch.cuda.is_available()","metadata":{"execution":{"iopub.status.busy":"2021-06-09T03:50:58.033658Z","iopub.execute_input":"2021-06-09T03:50:58.034248Z","iopub.status.idle":"2021-06-09T03:50:58.043374Z","shell.execute_reply.started":"2021-06-09T03:50:58.03421Z","shell.execute_reply":"2021-06-09T03:50:58.042525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TRAINING = True\n\n# Set num of epochs\nEPOCHS = 5\n\n# Set device: `cuda` or `cpu`\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# define loss function\nloss = smp.utils.losses.DiceLoss()\n\n# define metrics\nmetrics = [\n    smp.utils.metrics.IoU(threshold=0.5),\n]\n\n# define optimizer\noptimizer = torch.optim.Adam([ \n    dict(params=model.parameters(), lr=0.00008),\n])\n\n# define learning rate scheduler (not used in this NB)\nlr_scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n    optimizer, T_0=1, T_mult=2, eta_min=5e-5,\n)\n\n# load best saved model checkpoint from previous commit (if present)\nif os.path.exists('../input/deepglobe-land-cover-classification-deeplabv3/best_model.pth'):\n    model = torch.load('../input/deepglobe-land-cover-classification-deeplabv3/best_model.pth', map_location=DEVICE)\n    print('Loaded pre-trained DeepLabV3+ model!')","metadata":{"execution":{"iopub.status.busy":"2021-06-09T03:50:58.046723Z","iopub.execute_input":"2021-06-09T03:50:58.047091Z","iopub.status.idle":"2021-06-09T03:50:58.059272Z","shell.execute_reply.started":"2021-06-09T03:50:58.047031Z","shell.execute_reply":"2021-06-09T03:50:58.058263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_epoch = smp.utils.train.TrainEpoch(\n    model, \n    loss=loss, \n    metrics=metrics, \n    optimizer=optimizer,\n    device=DEVICE,\n    verbose=True,\n)\n\nvalid_epoch = smp.utils.train.ValidEpoch(\n    model, \n    loss=loss, \n    metrics=metrics, \n    device=DEVICE,\n    verbose=True,\n)","metadata":{"execution":{"iopub.status.busy":"2021-06-09T03:50:58.060735Z","iopub.execute_input":"2021-06-09T03:50:58.061161Z","iopub.status.idle":"2021-06-09T03:51:02.316247Z","shell.execute_reply.started":"2021-06-09T03:50:58.061121Z","shell.execute_reply":"2021-06-09T03:51:02.315332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\nif TRAINING:\n\n    best_iou_score = 0.0\n    train_logs_list, valid_logs_list = [], []\n\n    for i in range(0, EPOCHS):\n\n        # Perform training & validation\n        print('\\nEpoch: {}'.format(i))\n        train_logs = train_epoch.run(train_loader)\n        valid_logs = valid_epoch.run(valid_loader)\n        train_logs_list.append(train_logs)\n        valid_logs_list.append(valid_logs)\n\n        # Save model if a better val IoU score is obtained\n        if best_iou_score < valid_logs['iou_score']:\n            best_iou_score = valid_logs['iou_score']\n            torch.save(model, './best_model.pth')\n            print('Model saved!')","metadata":{"execution":{"iopub.status.busy":"2021-06-09T03:51:02.317569Z","iopub.execute_input":"2021-06-09T03:51:02.317905Z","iopub.status.idle":"2021-06-09T05:13:33.868471Z","shell.execute_reply.started":"2021-06-09T03:51:02.317871Z","shell.execute_reply":"2021-06-09T05:13:33.867512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if os.path.exists('./best_model.pth'):\n    best_model = torch.load('./best_model.pth', map_location=DEVICE)\n    print('Loaded DeepLabV3+ model from this run.')\n\n# load best saved model checkpoint from previous commit (if present)\nelif os.path.exists('../input/deepglobe-land-cover-classification-deeplabv3/best_model.pth'):\n    best_model = torch.load('../input/deepglobe-land-cover-classification-deeplabv3/best_model.pth', map_location=DEVICE)\n    print('Loaded DeepLabV3+ model from a previous commit.')","metadata":{"execution":{"iopub.status.busy":"2021-06-09T05:13:33.87165Z","iopub.execute_input":"2021-06-09T05:13:33.871905Z","iopub.status.idle":"2021-06-09T05:13:33.98694Z","shell.execute_reply.started":"2021-06-09T05:13:33.871877Z","shell.execute_reply":"2021-06-09T05:13:33.986042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset = LandCoverDataset(\n    valid_df, \n    augmentation=get_validation_augmentation(),\n    preprocessing=get_preprocessing(preprocessing_fn),\n    class_rgb_values=select_class_rgb_values,\n)\n\ntest_dataloader = DataLoader(test_dataset)\n\n# test dataset for visualization (without preprocessing augmentations & transformations)\ntest_dataset_vis = LandCoverDataset(\n    valid_df,\n    augmentation=get_validation_augmentation(),\n    class_rgb_values=select_class_rgb_values,\n)\n\n# get a random test image/mask index\nrandom_idx = random.randint(0, len(test_dataset_vis)-1)\nimage, mask = test_dataset_vis[random_idx]\n\nvisualize(\n    original_image = image,\n    ground_truth_mask = colour_code_segmentation(reverse_one_hot(mask), select_class_rgb_values),\n    one_hot_encoded_mask = reverse_one_hot(mask)\n)","metadata":{"execution":{"iopub.status.busy":"2021-06-09T05:13:33.989933Z","iopub.execute_input":"2021-06-09T05:13:33.990191Z","iopub.status.idle":"2021-06-09T05:13:35.875798Z","shell.execute_reply.started":"2021-06-09T05:13:33.990165Z","shell.execute_reply":"2021-06-09T05:13:35.874868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_preds_folder = 'sample_predictions/'\nif not os.path.exists(sample_preds_folder):\n    os.makedirs(sample_preds_folder)","metadata":{"execution":{"iopub.status.busy":"2021-06-09T05:13:35.877295Z","iopub.execute_input":"2021-06-09T05:13:35.877643Z","iopub.status.idle":"2021-06-09T05:13:35.882625Z","shell.execute_reply.started":"2021-06-09T05:13:35.877606Z","shell.execute_reply":"2021-06-09T05:13:35.881641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_model = model","metadata":{"execution":{"iopub.status.busy":"2021-06-09T05:13:35.884135Z","iopub.execute_input":"2021-06-09T05:13:35.884483Z","iopub.status.idle":"2021-06-09T05:13:35.894961Z","shell.execute_reply.started":"2021-06-09T05:13:35.884448Z","shell.execute_reply":"2021-06-09T05:13:35.893932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for idx in range(len(test_dataset)):\n\n    image, gt_mask = test_dataset[idx]\n    image_vis = test_dataset_vis[idx][0].astype('uint8')\n    x_tensor = torch.from_numpy(image).to(DEVICE).unsqueeze(0)\n    # Predict test image\n    pred_mask = best_model(x_tensor)\n    pred_mask = pred_mask.detach().squeeze().cpu().numpy()\n    # Convert pred_mask from `CHW` format to `HWC` format\n    pred_mask = np.transpose(pred_mask,(1,2,0))\n    # Get prediction channel corresponding to foreground\n    pred_forest_land_heatmap = pred_mask[:,:,select_classes.index('forest_land')]\n    pred_mask = colour_code_segmentation(reverse_one_hot(pred_mask), select_class_rgb_values)\n    # Convert gt_mask from `CHW` format to `HWC` format\n    gt_mask = np.transpose(gt_mask,(1,2,0))\n    gt_mask = colour_code_segmentation(reverse_one_hot(gt_mask), select_class_rgb_values)\n    cv2.imwrite(os.path.join(sample_preds_folder, f\"sample_pred_{idx}.png\"), np.hstack([image_vis, gt_mask, pred_mask])[:,:,::-1])\n    \n    visualize(\n        original_image = image_vis,\n        ground_truth_mask = gt_mask,\n        predicted_mask = pred_mask,\n        pred_forest_land_heatmap = pred_forest_land_heatmap\n    )","metadata":{"execution":{"iopub.status.busy":"2021-06-09T05:13:35.896343Z","iopub.execute_input":"2021-06-09T05:13:35.896729Z","iopub.status.idle":"2021-06-09T05:18:11.324356Z","shell.execute_reply.started":"2021-06-09T05:13:35.896695Z","shell.execute_reply":"2021-06-09T05:18:11.323495Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_epoch = smp.utils.train.ValidEpoch(\n    model,\n    loss=loss, \n    metrics=metrics, \n    device=DEVICE,\n    verbose=True,\n)                                                                                    \n\nvalid_logs = test_epoch.run(test_dataloader)\nprint(\"Evaluation on Test Data: \")\nprint(f\"Mean IoU Score: {valid_logs['iou_score']:.4f}\")\nprint(f\"Mean Dice Loss: {valid_logs['dice_loss']:.4f}\")","metadata":{"execution":{"iopub.status.busy":"2021-06-09T05:18:11.325646Z","iopub.execute_input":"2021-06-09T05:18:11.325999Z","iopub.status.idle":"2021-06-09T05:20:04.544337Z","shell.execute_reply.started":"2021-06-09T05:18:11.325963Z","shell.execute_reply":"2021-06-09T05:20:04.543366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_logs_df = pd.DataFrame(train_logs_list)\nvalid_logs_df = pd.DataFrame(valid_logs_list)\ntrain_logs_df.T","metadata":{"execution":{"iopub.status.busy":"2021-06-09T05:20:04.548282Z","iopub.execute_input":"2021-06-09T05:20:04.548532Z","iopub.status.idle":"2021-06-09T05:20:04.563392Z","shell.execute_reply.started":"2021-06-09T05:20:04.548507Z","shell.execute_reply":"2021-06-09T05:20:04.562417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(20,8))\nplt.plot(train_logs_df.index.tolist(), train_logs_df.iou_score.tolist(), lw=3, label = 'Train')\nplt.plot(valid_logs_df.index.tolist(), valid_logs_df.iou_score.tolist(), lw=3, label = 'Valid')\nplt.xlabel('Epochs', fontsize=20)\nplt.ylabel('IoU Score', fontsize=20)\nplt.title('IoU Score Plot', fontsize=20)\nplt.legend(loc='best', fontsize=16)\nplt.grid()\nplt.savefig('iou_score_plot.png')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-09T05:20:04.565368Z","iopub.execute_input":"2021-06-09T05:20:04.565964Z","iopub.status.idle":"2021-06-09T05:20:04.867308Z","shell.execute_reply.started":"2021-06-09T05:20:04.565911Z","shell.execute_reply":"2021-06-09T05:20:04.866396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(20,8))\nplt.plot(train_logs_df.index.tolist(), train_logs_df.dice_loss.tolist(), lw=3, label = 'Train')\nplt.plot(valid_logs_df.index.tolist(), valid_logs_df.dice_loss.tolist(), lw=3, label = 'Valid')\nplt.xlabel('Epochs', fontsize=20)\nplt.ylabel('Dice Loss', fontsize=20)\nplt.title('Dice Loss Plot', fontsize=20)\nplt.legend(loc='best', fontsize=16)\nplt.grid()\nplt.savefig('dice_loss_plot.png')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2021-06-09T05:20:04.868519Z","iopub.execute_input":"2021-06-09T05:20:04.868848Z","iopub.status.idle":"2021-06-09T05:20:05.140015Z","shell.execute_reply.started":"2021-06-09T05:20:04.868811Z","shell.execute_reply":"2021-06-09T05:20:05.139108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#input satellite images\nfrom PIL import Image\nSATELLITE_IMG_PATH = os.path.join('/', 'kaggle', 'input', 'satelliteimages06062021')\nOUTPUT_PATH = os.path.join('/', 'kaggle', 'working')\nFIXED_SIZE = (1024, 1024)\n\nsatellite_folder = 'satellite_predictions/'\nif not os.path.exists(OUTPUT_PATH + '/' + satellite_folder):\n    os.makedirs(OUTPUT_PATH + '/' + satellite_folder)\n    print(f\"{OUTPUT_PATH + '/' + satellite_folder} created!\")\n    \nSATELLITE_PRED_PATH = os.path.join(OUTPUT_PATH, satellite_folder)\n\nif os.path.isdir(SATELLITE_PRED_PATH) != True:\n    for filename in os.listdir(SATELLITE_IMG_PATH):\n        if filename.endswith(\".png\"):\n            image = Image.open(SATELLITE_IMG_PATH + '/' + filename)\n            if image.size != FIXED_SIZE:\n                image.resize(FIXED_SIZE)\n                image.save(SATELLITE_PRED_PATH + '/' + filename)\n                print(f\"{filename} resized to {FIXED_SIZE}\\nSaved to {SATELLITE_PRED_PATH}!\")\nprint('Process complete!')","metadata":{"execution":{"iopub.status.busy":"2021-06-09T05:20:05.14125Z","iopub.execute_input":"2021-06-09T05:20:05.141585Z","iopub.status.idle":"2021-06-09T05:20:05.151746Z","shell.execute_reply.started":"2021-06-09T05:20:05.141548Z","shell.execute_reply":"2021-06-09T05:20:05.150892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"satellite_images =[cv2.imread(file) for file in glob.glob(\"/kaggle/working/satellite_predictions/*.png\")]  ","metadata":{"execution":{"iopub.status.busy":"2021-06-09T05:20:28.108476Z","iopub.execute_input":"2021-06-09T05:20:28.108785Z","iopub.status.idle":"2021-06-09T05:20:28.112943Z","shell.execute_reply.started":"2021-06-09T05:20:28.108755Z","shell.execute_reply":"2021-06-09T05:20:28.111908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Run it","metadata":{"execution":{"iopub.status.busy":"2021-06-09T05:20:05.301165Z","iopub.status.idle":"2021-06-09T05:20:05.301886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for idx in range(len(satellite_images)):\n\n    image, gt_mask = test_dataset[idx]\n    image_vis = test_dataset_vis[idx][0].astype('uint8')\n    x_tensor = torch.from_numpy(image).to(DEVICE).unsqueeze(0)\n    # Predict test image\n    pred_mask = best_model(x_tensor)\n    pred_mask = pred_mask.detach().squeeze().cpu().numpy()\n    # Convert pred_mask from `CHW` format to `HWC` format\n    pred_mask = np.transpose(pred_mask,(1,2,0))\n    # Get prediction channel corresponding to foreground\n    pred_forest_land_heatmap = pred_mask[:,:,select_classes.index('forest_land')]\n    pred_mask = colour_code_segmentation(reverse_one_hot(pred_mask), select_class_rgb_values)\n    # Convert gt_mask from `CHW` format to `HWC` format\n    gt_mask = np.transpose(gt_mask,(1,2,0))\n    gt_mask = colour_code_segmentation(reverse_one_hot(gt_mask), select_class_rgb_values)\n    cv2.imwrite(os.path.join(satellite_folder, f\"sample_pred_{idx}.png\"), np.hstack([image_vis, gt_mask, pred_mask])[:,:,::-1])\n    \n    visualize(\n        original_image = image_vis,\n        ground_truth_mask = gt_mask,\n        predicted_mask = pred_mask,\n        pred_forest_land_heatmap = pred_forest_land_heatmap\n    )","metadata":{"execution":{"iopub.status.busy":"2021-06-09T05:20:45.198249Z","iopub.execute_input":"2021-06-09T05:20:45.198577Z","iopub.status.idle":"2021-06-09T05:20:45.206101Z","shell.execute_reply.started":"2021-06-09T05:20:45.198547Z","shell.execute_reply":"2021-06-09T05:20:45.204981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%cd /kaggle/working","metadata":{"execution":{"iopub.status.busy":"2021-06-09T05:20:05.304862Z","iopub.status.idle":"2021-06-09T05:20:05.305578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n%cd working\n%","metadata":{"execution":{"iopub.status.busy":"2021-06-09T05:20:05.306725Z","iopub.status.idle":"2021-06-09T05:20:05.307285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nos.chdir(r'/kaggle/working')","metadata":{"execution":{"iopub.status.busy":"2021-06-09T05:20:05.308437Z","iopub.status.idle":"2021-06-09T05:20:05.309067Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import FileLink \nFileLink(r'best_model.pth')","metadata":{"execution":{"iopub.status.busy":"2021-06-09T05:20:05.310227Z","iopub.status.idle":"2021-06-09T05:20:05.310842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%","metadata":{"execution":{"iopub.status.busy":"2021-06-09T05:20:05.312007Z","iopub.status.idle":"2021-06-09T05:20:05.312618Z"},"trusted":true},"execution_count":null,"outputs":[]}]}