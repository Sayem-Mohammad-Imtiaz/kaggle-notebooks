{"cells":[{"metadata":{},"cell_type":"markdown","source":"Hello Fellow Kagglers!\n\nThis is **Abel Ofinni** from **Nigeria**. A **Data Science Nigeria AI+ Community Member** This Kernel is provided to teach XGBoost in its simplistic style to Beginners. So, I will try to make it Beginner-friendly as much as I can. It's a excerpt from recent training which i also intend to save here for future reference. **Kindly upvote this Kernel if found helpful.** Thanks."},{"metadata":{},"cell_type":"markdown","source":"**Prologue**\n\nXGBoost is an implementation of gradient boosting that is being used to win Machine Learning competitions. It is powerful but it can be hard to get started. In this guide you will discover a 7-Part crash course on **XGBoost with Python**. This mini course is designed for Python machine learning Beginners that are already comfortable with scikit-learn and the SciPy ecosystem. \n\nNow, let’s get started.\n"},{"metadata":{},"cell_type":"markdown","source":"**Course Overview** (what to expect)\n\nThis course stems from a 7-day crash course on XGBoost from one of my Mentors. I would mention him once he approves his mention. The course is divided into 7 parts. Each topic was designed to take the average developer about 30 minutes. You might ﬁnish some much sooner and others you may choose to go deeper and spend more time for more research into them. You can complete each part as quickly or as slowly as you like. A comfortable schedule may be to complete one lesson per day over a one week period. Highly recommended.  \n\nThe 7 Topics you will cover are as follows:\n\n **Introduction to Gradient Boosting.**\n\n **Introduction to XGBoost.**\n\n **Develop Your First XGBoost Model.**\n\n **Monitor Performance and Early Stopping.**\n\n **Feature Importance with XGBoost.**\n\n **How to Conﬁgure Gradient Boosting.**\n\n **XGBoost Hyperparameter Tuning.**\n\nWe will be using the UCI Machine Learning Pima-Indians-Diabetes Dataset in this short tutorial.\n\nGrab your **Coffee** and let's explore Extreme Gradient Boosting (XGBoost) together.\n"},{"metadata":{},"cell_type":"markdown","source":"Topic 01: **INTRODUCTION TO GRADIENT BOOSTING**\n\nGradient boosting is one of the most powerful techniques for building predictive models. The idea of boosting came out of the idea of whether a weak learner can be modiﬁed to become better. The ﬁrst realization of boosting that saw great success in application was **Adaptive Boosting** or **AdaBoost** for short. The weak learners in **AdaBoost** are **Decision Trees** with a single split, called **Decision Stumps** for their shortness. AdaBoost and related algorithms were recast in a statistical framework and became known as **Gradient Boosting Machines** (GBM). The statistical framework cast boosting as a *Numerical Optimization* problem where the objective is to minimize the loss of the model by adding *Weak Learners* using a gradient descent-like procedure, hence the name. The Gradient Boosting *algorithm* involves three elements:\n\n1. A loss function to be optimized, such as cross entropy for classiﬁcation or mean squared error for regression problems.\n\n2. A weak learner to make predictions, such as a greedily constructed decision tree.\n\n3. An additive model, used to add weak learners to minimize the loss function.\n\nNew weak learners are added to the model in an eﬀort to correct the residual errors of all previous trees. The result is a powerful predictive modeling algorithm, perhaps more powerful than random forest. \n\nHang on there! Now, let's take a look at the XGBoost implementation of gradient boosting.\n"},{"metadata":{},"cell_type":"markdown","source":"(http://)Topic 02: **INTRODUCTION TO XGBOOST**\n\nXGBoost is an implementation of gradient boosted decision trees designed for speed and performance. XGBoost stands for **eXtreme Gradient Boosting**. It was developed by ***Tianqi Chen*** and is laser-focused on *computational speed* and *model performance*, as such there are few frills. In addition to supporting all key variations of the technique, the real interest is the speed provided by the careful engineering of the implementation, including:\n\n* Parallelization of tree construction using all of your CPU cores during training.\n\n* Distributed Computing for training very large models using a cluster of machines.\n\n* Out-of-Core Computing for very large datasets that don’t ﬁt into memory.\n\n* Cache Optimization of data structures and algorithms to make best use of hardware.\n\nTraditionally, gradient boosting implementations are slow because of the sequential nature in which each tree must be constructed and added to the model. The on performance in the development of XGBoost has resulted in one of the best predictive modeling algorithms that can now harness the full capability of your hardware platform, or very large computers you might rent in the cloud. As such, XGBoost has been a cornerstone in competitive machine learning, being the technique used to win and recommended by winners.\n1 http://goo.gl/AHkmWx 2 http://goo.gl/sGyGtu\n"},{"metadata":{},"cell_type":"markdown","source":"**We shall be developing our first XGB model right away**\n\nYou will need XGBoost installed. Visit the XGBoost Documentation for installation guide here https://xgboost.readthedocs.io/en/latest/ . \n\nIf you already have it installed, let's move on."},{"metadata":{},"cell_type":"markdown","source":"Topic 03: **DEVELOP YOUR FIRST XGBOOST MODEL**"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\nfrom numpy import loadtxt\n\n#Import XGBoost Model\nfrom xgboost import XGBClassifier\nfrom xgboost import plot_importance\nfrom matplotlib import pyplot\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n#Success\nprint ('Run Successful')\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"#Import the Pina Indians Diabetes Dataset\ndataset = loadtxt(\"../input/Diabetes.csv\" , delimiter = \",\")\nprint (\"Run Successfully\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We should consider an overview of our dataset. We will need the profile of each feature here to understand our Dataset"},{"metadata":{},"cell_type":"markdown","source":"You might have quite a few things to say using the above overview of our dataset. But to keep things short. We have 0 missing data. So, Let's proceed."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Split the Dataset into X and Y\nX = dataset[:, 0:8]\nY = dataset [:,8]\nprint ('Ran Successfully')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Split the Dataset into into Train and Test \nseed = 7\ntest_size = 0.33\nX_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size=test_size, random_state = seed)\nprint ('Ran Successfully')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**WE will fit XGBoost default model on our Train dataset**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's fit our model on the training data\nxgb = XGBClassifier()\nxgb.fit(X_train, Y_train)\nprint('Ran Successfully')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have trained our default XGB Model. Let's try and fit it on our test dataset to see how well the trained default XGB Model performed"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Predict usng our model now\npredictions1 = xgb.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Evaluate Predictions\naccuracy = accuracy_score(Y_test, predictions1)\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"With an **Accuracy Score of 77.95%**. We need to improve our Model to perform better. We can move conveniently to our next topic now.\n\nTake a sip, your coffee is now cold or still hot. </Winks/>\n\nLet's move to the next topic"},{"metadata":{},"cell_type":"markdown","source":"TOPIC 04: **MONITOR PERFORMANCE AND EARLY STOPPING**\n\nThe XGBoost model can evaluate and report on the performance on a test set for the model during training. It supports this capability by specifying both a test dataset and an evaluation metric on the call to model.fit() when training the model and specifying verbose output (verbose=True). For example, we can report on the binary classiﬁcation error rate (error) on a standalone test set (eval set) while training an XGBoost model.\n\nWe can use this evaluation to stop training once no further improvements have been made to the model. We can do this by setting the early stopping rounds parameter when calling *model.fit()* to the number of iterations that no improvement is seen on the validation dataset before training is stopped. The full example using the Pima Indians Onset of Diabetes dataset is provided below.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# split data into train and test sets \nseed = 7\ntest_size = 0.33 \nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=test_size, random_state=seed)\nmodel = XGBClassifier()\neval_set = [(X_test, y_test)]\n\n#Set eval_metrics as logloss, early_stopping_round as 5 \nmodel.fit(X_train, y_train, early_stopping_rounds=5, eval_metric=\"logloss\", eval_set=eval_set, verbose=True) \n# make predictions for test data \ny_predictions = model.predict(X_test)  \n# evaluate predictions\naccuracy = accuracy_score(y_test, y_predictions)\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fit model on training data \nmodel = XGBClassifier() \neval_set = [(X_test, y_test)] \n#Set eval_metrics as logloss, early_stopping_round as 5 \nmodel.fit(X_train, y_train, early_stopping_rounds=5, eval_metric=\"error\", eval_set=eval_set, verbose=True) \n# make predictions for test data\ny_predictions = model.predict(X_test) \n# evaluate predictions \naccuracy = accuracy_score(y_test, y_predictions) \nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A subtle introduction of my Mentor.\n**Enjoy reading this** https://machinelearningmastery.com/avoid-overfitting-by-early-stopping-with-xgboost-in-python/\n\nI have some articles you can read up to better undersatnd XGBoost Model. I will update the links in the next version of this Kernel. I'm still learning too. Do not hesistate to share any Article that could help beginners in comment section. I'll appreciate it. Thanks in advance.\n\nLet's move on to the next topic."},{"metadata":{},"cell_type":"markdown","source":"TOPIC 05: **FEATURE IMPORTANCE WITH XGBOOST**"},{"metadata":{},"cell_type":"markdown","source":"A beneﬁt of using ensembles of decision tree methods like gradient boosting is that they can automatically provide estimates of feature importance from a trained predictive model. A trained XGBoost model automatically calculates feature importance on your predictive modeling problem. These importance scores are available in the feature importances member variable of the trained model."},{"metadata":{},"cell_type":"markdown","source":"The XGBoost library provides a built-in function to plot features ordered by their importance. The function is called plot importance() and can be used. The importance scores can help us decide what input variables to keep or discard. They can also be used as the basis for automatic feature selection techniques. We will now plot the feature importance scores using the Pima Indians Onset of Diabetes dataset.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot feature importance using built-in function \n# fit model on training data\nmodel = XGBClassifier() \nmodel.fit(X_train, y_train) \n# plot feature importance \nplot_importance(model) \npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our model feature importance is now plotted. It's pretty simple. just know the simple syntax."},{"metadata":{},"cell_type":"markdown","source":"TOPIC 06: **HOW TO CONFIGURE GRADIENT BOOSTING**\n\n**Gradient boosting** is one of the most powerful techniques for applied machine learning and as such is quickly becoming one of the most popular. **But how do you conﬁgure gradient boosting on your problem?**\n\nA number of conﬁguration heuristics were published in the original gradient boosting papers. They can be summarized as:\n\n Learning rate or shrinkage (learning rate in XGBoost) should be set to 0.1 or lower, and smaller values will require the addition of more trees.\n\n The depth of trees (tree depth in XGBoost) should be conﬁgured in the range of 2-to-8, where not much beneﬁt is seen with deeper trees.\n\n Row sampling (subsample in XGBoost) should be conﬁgured in the range of 30% to 80% of the training dataset, and compared to a value of 100% for no sampling.\n\nThese are a good starting points when conﬁguring your model. A good general conﬁguration strategy is as follows:\n\n1. Run the default conﬁguration and review plots of the learning curves on the training and validation datasets.\n\n2. If the system is overlearning, decrease the learning rate and/or increase the number of trees.\n\n3. If the system is underlearning, speed the learning up to be more aggressive by increasing the learning rate and/or decreasing the number of trees.\n\nOwen Zhang, the former #1 ranked Competitor on Kaggle and now CTO at Data Robot proposes an interesting strategy to conﬁgure XGBoost5. He suggests to set the number of trees to a target value such as 100 or 1000, then tune the learning rate to ﬁnd the best model. This is an eﬃcient strategy for quickly ﬁnding a good model. In the next and ﬁnal lesson, we will look at an example of tuning the XGBoost hyperparameters.\n\nCheck Owen proposition here - http://www.slideshare.net/odsc/owen-zhangopen-sourcetoolsanddscompetitions1\n"},{"metadata":{},"cell_type":"markdown","source":"TOPIC 07: XGBOOST HYPERPARAMETER TUNING"},{"metadata":{},"cell_type":"markdown","source":"The scikit-learn framework provides the capability to search combinations of parameters. This capability is provided in the GridSearchCV class and can be used to discover the best way to conﬁgure the model for top performance on your problem. For example, we can deﬁne a grid of the number of trees (n estimators) and tree sizes (max depth) to evaluate by deﬁning a grid. And then evaluate each combination of parameters using 10-fold cross-validation.\n\nWe can then review the results to determine the best combination and the general trends in varying the combinations of parameters. This is the best practice when applying XGBoost to your own problems. \n\nThe parameters to consider tuning are:\n\n* The number and size of trees (n estimators and max depth).\n\n* The learning rate and number of trees (learning rate and n estimators).\n\n* The row and column subsampling rates (subsample, colsample bytree and colsample bylevel).\n"},{"metadata":{},"cell_type":"markdown","source":"Now we should tune our model's learning rate"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Tune learning_rate \nfrom sklearn.model_selection import GridSearchCV \nfrom sklearn.model_selection import KFold, StratifiedKFold","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Grid Search for the best model"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Split Dataset\nX = dataset[:,0:8] \nY = dataset[:,8] \n# grid search \nmodel = XGBClassifier() \nlearning_rate = [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3] \nparam_grid = dict(learning_rate=learning_rate) \nkfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=7) \ngrid_search = GridSearchCV(model, param_grid, scoring=\"neg_log_loss\", n_jobs=-1, cv=kfold) \ngrid_result = grid_search.fit(X, Y) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's provide a summary report of our result here now. as we conclude this tutorial."},{"metadata":{"trusted":true},"cell_type":"code","source":"# summarize results \nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_)) \nmeans = grid_result.cv_results_['mean_test_score'] \nstds = grid_result.cv_results_['std_test_score'] \nparams = grid_result.cv_results_['params'] \nfor mean, stdev, param in zip(means, stds, params): \n    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Just before You Go...**\n\nYou made it. Well done! Take a moment and look back at how far you have come:\n\n You learned about the gradient boosting algorithm and the XGBoost library.\n\n You developed your ﬁrst XGBoost model.\n\n You learned how to use advanced features like early stopping and feature importance.\n\n You learned how to conﬁgure gradient boosted models and how to design controlled experiments to tune XGBoost hyperparameters.\n\nDon’t make light of this, you have come a long way in a short amount of time. This is just the beginning of your journey with XGBoost in Python. Keep practicing and developing your skills. I have not stopped learning and sharing what I've learnt. I'm continually inspired by [https://www.kaggle.com/bayoadekanmbi] and www.datasciencenigeria.org. Thanks to https://www.kaggle.com/afolaborn for his immense contribution too. The learning continues as i proceed in the Data Science journey.\n"},{"metadata":{},"cell_type":"markdown","source":"**If You enjoy this Kernel, kindly upvote**it. It's my first kernel before. I look forward to your awesome comments and upvotes. I love you all."},{"metadata":{"trusted":true},"cell_type":"code","source":"print ('Thank you all for stopping by to learn and as you comment')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}