{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"# Cleaning and EDA  \n  \n**Summary**  \n  \nWe aim to get more familiar with the datasets through creating wordclouds using the same underlying ideas as tf-idf vectorization.  \nGenerally, top terms were as expected, except for the `biorxiv` dataset, where we've identified a very frequent, but useless, sentence in this process.  \n  \nThe sentence is something like:  \n*`\"The copyright holder for this preprint, which was not peer-reviewed, is the author or funder.\"`*  \n  \n**Note**  \nData used in this notebook has been parsed [here](https://www.kaggle.com/imbano/cord-19-parse-data-to-flat-format/output). The structure is as follows:  \n\n| column | type | description |\n|---------------|------|---------------------------------------------------|\n| paper_id | str |  |\n| supsec_order | int | supersection order (as it appears in the paper) |\n| supersection | str | values = {\"title\", \"abstract\", \"body_text\", \"back_matter\"} |\n| section_order | int | section order (as it appears in the paper) |\n| section | str | based on provided \"section\" value |\n| text | str | parsed text of the section |  \n  \n  \n**DISCLAIMER**  \nI am very much still on the process of learning. Any corrections are greatly appreciated.  "},{"metadata":{},"cell_type":"markdown","source":"Import required packages..."},{"metadata":{"trusted":true},"cell_type":"code","source":"#optionally install ujson\n!pip install ujson","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import functools\nimport itertools as it\nimport math\nimport multiprocessing as mp\nimport re\nimport types\nfrom typing import List, Dict, Tuple, Pattern\n\nimport numpy as np\nimport pandas as pd\nimport scipy.sparse as sp\n\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction import stop_words\nimport joblib\n\nfrom IPython.display import display_html\nimport ipywidgets as widgets\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nfrom tqdm.notebook import tqdm\nfrom wordcloud import WordCloud\n\ntry:\n    import ujson as json \nexcept ImportError:\n    import json \n\n\nwnl = WordNetLemmatizer()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Load Files"},{"metadata":{"trusted":true},"cell_type":"code","source":"FILE_PATTERN = \"/kaggle/input/cord-19-parse-data-to-flat-format/{}-parsed.csv.gz\"\n\ndatasets = [\n    \"custom_license\",\n    \"noncomm_use\",\n    \"biorxiv\",\n    \"comm_use\"\n]\n\nall_dfs = {\n    ds: pd.read_csv(\n        FILE_PATTERN.format(ds),\n        usecols=[\"paper_id\", \"supersection\",\"text\", \"source\"],\n        compression=\"gzip\",\n    )\n    for ds in tqdm(datasets, total=len(datasets), desc=\"Reading Data\")\n}\n\nfor df in all_dfs.values():\n    df[\"text\"] = df[\"text\"].fillna(\"\")\n\nall_dfs.keys()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tab = widgets.Tab(\n    children = [widgets.Output() for ds in datasets],\n)\n\nfor idx,(ds, out) in enumerate(zip(datasets, tab.children)):\n    tab.set_title(idx, ds)\n    with out:\n        display_html(all_dfs[ds].head())\n        display_html(all_dfs[ds].info())\n        \ndisplay(tab)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n## Clean Text\n\nThe rationale for cleaning is to standardize some texts based on what they are instead of their actual value.  \n  \nA `clean_text` function is implemented below. The regex patterns used for cleaning need to specified. For our purposes, the replacement value will be in UPPERCASE, to help us differentiate raw values from cleaned values."},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_text(text: str, clean_patterns: Tuple[str or Pattern, str]) -> str:\n    \"\"\"Clean text based on provided patterns\n    \n    Parameters\n    ----------\n    text: str\n    clean_patterns: Tuple[str or Pattern, str]\n        tuple of `(regex_pattern, replacement_value)`\n        **Order is important**; (e.g. floats parsed first, to prevent INT.INT)\n    \n    Returns\n    -------\n    str\n    \"\"\"\n    clean_text = text\n    for pattern, new_val in clean_patterns:\n        if isinstance(pattern, Pattern):\n            clean_text = pattern.sub(new_val, clean_text)\n        else:\n            clean_text = re.sub(pattern, new_val, clean_text)\n\n    return clean_text\n\n\n# list of (regex, new value) tuples\n# order is important\n# text converted to lowercase before cleaning \nCLEAN_PATTERNS = (\n    (r\"(?<=[A-Za-z])'(?=[A-Za-z])\", \"\"),  # remove contractions\n    (r\"\\b[atcgu]{3,}\\b\", \" DNASEQ \"),  # not sure what the min char threshold should be...\n    (r\"http\\S+\", \" URL \"),\n    (\"(?<!\\d)2019(?!\\d)\", \" COVIDYEAR \"),  # might be important?\n    (r\"(?<!\\d)\\d+\\.\\d+(?!\\d)\", \" FLOAT \"),\n    (r\"(?<!\\d)\\d+(?!\\d)\", \" INT \"),\n    (r\"[\\s_]+\", \" \"),\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# compile regex patterns\ncomp_cleanpat = tuple(\n    (re.compile(pat, flags=re.I), new_val) for pat, new_val in CLEAN_PATTERNS\n)\n\n# note on RAM usage: +~3 GB to peak memory usage, adds +2 GB once done with all datasets\nfor ds in tqdm(datasets, desc=\"Cleaning texts...\"):\n    df = all_dfs[ds]\n    with mp.Pool() as pool:\n        df[\"text_clean\"] = list(\n            tqdm(\n                pool.imap(\n                    functools.partial(clean_text, clean_patterns=comp_cleanpat),\n                    df[\"text\"].str.lower(),\n                ),\n                total=len(df),\n                desc=ds,\n            )\n        )\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## EDA\n\nOur EDA process will be guided by the ideas of [tf-idf](https://en.m.wikipedia.org/wiki/Tfâ€“idf), which are basically:\n* A \"term\" is important if it appears in a \"document\" frequently (term frequency)\n* A \"term\" would be less important if it appears in a lot of other \"documents\" (inverse document frequency)"},{"metadata":{},"cell_type":"markdown","source":"Since our data structure is parsed by section, we merge texts into one document first per `paper_id`"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Texts by paper_id\ntexts_by_paperid = {\n    ds: all_dfs[ds].groupby([\"paper_id\"])[\"text_clean\"].apply(lambda x: \" \".join(x)).values\n    for ds in tqdm(datasets, desc=\"Combining by `paper_id`\")\n}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Vectorizer\n\nTrain `CountVectorizer` on full dataset. *This may take some time...* "},{"metadata":{"trusted":true},"cell_type":"code","source":"all_texts = np.concatenate([texts for texts in texts_by_paperid.values()])\n\nvectorizer = CountVectorizer(stop_words=\"english\", analyzer=\"word\", lowercase= False)\n_ = vectorizer.fit(all_texts)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Add parallelized implementation of `CountVectorizer.transform`, and export."},{"metadata":{"trusted":true},"cell_type":"code","source":"@functools.wraps(CountVectorizer.transform)\ndef transform_parallel(self, raw_documents):\n    with mp.Pool() as pool:\n        docs_split = np.array_split(raw_documents, mp.cpu_count())\n        X = sp.vstack(pool.map(self.transform, docs_split), format='csr')\n\n    return X\n\nvectorizer.transform_parallel = types.MethodType(transform_parallel, vectorizer)\n\njoblib.dump(vectorizer, \"CORD19_countvec.joblib\")  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Lemmatizer\n\nIn an ideal world, we would identify a token's part of speech based on how it is used in the sentence, then lemmatize. However, the processing required makes it prohibitive.<br><br>\nThe workaround is to lemmatize a token as a noun, verb, and adjective, then take the shortest form..."},{"metadata":{"trusted":true},"cell_type":"code","source":"def lemmatize(word: str, pos_tags: list= None):\n    \"\"\"`nltk` WordNet-based Lemmatization, regardless of part-of-speech\n    \n    Parameters\n    ----------\n    word: str\n    pos_values: list, optional\n        Part of Speech tags to attempt\n        defaults to `[\"n\", \"v\", \"a\"]`\n    \n    Returns\n    -------\n    str\n        lemmatized word\n    \"\"\"\n    pos_tags = pos_tags or [\"n\", \"v\", \"a\"]    \n    lemms = [wnl.lemmatize(word, pos) for pos in pos_tags]\n    lemms = [lemm for lemm in lemms if lemm != word]\n    if lemms:\n        lemm_lens = [len(lemm) for lemm in lemms]\n        return lemms[lemm_lens.index(min(lemm_lens))]\n    else:\n        return word\n\n\ndef construct_lemm_matrix(words: List[str], as_df=False):\n    \"\"\"Construct a Lemmatization Matrix\n    \n    Parameters\n    ----------\n    words: list[str]\n    as_df: boolean, optional\n        return output as `pandas.DataFrame`, defaults to `False`\n\n    Returns\n    -------\n    dict or pandas.DataFrame\n        if `as_df` returns a pandas.DataFrame\n        else {\"matrix\": sp.csr_matrix, \"columns\": list[str], \"vocabulary\": dict[str, int]}\n    \"\"\"\n    col_idx, columns = [], []\n    lemm_vocab = {}\n    for word in words:\n        lemm = lemmatize(word)\n        if lemm not in lemm_vocab:\n            lemm_vocab[lemm] = len(lemm_vocab)\n            columns.append(lemm)\n\n        col_idx.append(lemm_vocab[lemm])\n\n    no_of_words = len(words)\n    res = {\n        \"matrix\": sp.csr_matrix((np.ones(no_of_words), (np.arange(no_of_words), col_idx))),\n        \"columns\": columns,\n        \"vocabulary\": lemm_vocab,\n    }\n    \n    return pd.DataFrame(res[\"matrix\"].toarray(), columns=res[\"columns\"], index=words) if as_df else res\n\n\nL = construct_lemm_matrix(vectorizer.get_feature_names())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Generate word count data..."},{"metadata":{"trusted":true},"cell_type":"code","source":"def gen_wordcount(X, words):\n    # sparse matrices are awesome...\n    wordcount = pd.DataFrame(\n        np.concatenate([X.sum(axis=0), (X > 0).sum(axis=0)], axis=0),\n        columns=pd.Index(words, name=\"word\"),\n        index=[\"count\", \"count_of_doc\"]\n    ).T\n\n    wordcount[\"percent_of_doc\"] = wordcount[\"count_of_doc\"] / X.shape[0] \n    return wordcount\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_dt = {ds: vectorizer.transform_parallel(texts) for ds, texts in tqdm(texts_by_paperid.items(), desc=\"Transforming\")}\nlemm_dt = {ds: X @ L[\"matrix\"] for ds, X in raw_dt.items()}\n\n# change this line to use raw or lemmatized document term matrix\nwordcounts = {ds: gen_wordcount(X, L[\"columns\"]) for ds, X in lemm_dt.items()}\n\ntab = widgets.Tab(\n    children = [widgets.Output() for _ in texts_by_paperid],\n)\nfor idx,(out, (ds, wordcount)) in enumerate(zip(tab.children, wordcounts.items())):\n    tab.set_title(idx, ds)\n    with out:\n        display_html(wordcount.describe())\n    \ndisplay(tab)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Word Cloud for All Terms  \n\n*Note: Terms in UPPERCASE are cleaned values.*"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_wordcounts(wordcounts: Dict[str, pd.DataFrame], filter_func: callable=None, wc_args: dict = None):    \n    wc_args = {**{\"width\":800, \"height\": 500}, **(wc_args or {})}\n    tab = widgets.Tab(\n        children = [widgets.Output() for _ in wordcounts],\n    )\n    for idx, (out, (ds,wordcount)) in enumerate(zip(tab.children, wordcounts.items())):\n        if filter_func:\n            wc_df = filter_func(wordcount)\n        else:\n            wc_df = wordcount\n\n        tab.set_title(idx, ds)\n        with out:\n            display(\n                WordCloud(**wc_args).generate_from_frequencies(\n                    wc_df[\"count\"].to_dict()\n                ).to_image()\n            )\n    display(tab)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_wordcounts(wordcounts)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We look at the concentration of words by document occurence"},{"metadata":{"trusted":true},"cell_type":"code","source":"COLUMNS = 3\nxaxis, yaxis = \"% of docs\", \"% of words (cumulative)\"\n\nplot_titles = list(wordcounts.keys())\n\nfig = make_subplots(\n    rows=math.ceil(len(plot_titles)/ COLUMNS), cols=COLUMNS,\n    subplot_titles=plot_titles,\n    horizontal_spacing=0.05, vertical_spacing=0.1\n)\n\nfor idx, ds in enumerate(plot_titles):\n    wordcount = wordcounts[ds]\n    row_i, col_i = int(idx / COLUMNS) + 1, idx % COLUMNS + 1\n    hist_data = pd.DataFrame(np.histogram(wordcount[\"percent_of_doc\"], bins=50), index=[\"count\", \"% of docs\"]).T\n    hist_data[\"% of words (cumulative)\"] = hist_data[\"count\"].cumsum() / hist_data[\"count\"].sum()\n    fig.add_trace(\n        go.Scatter(x=hist_data[xaxis], y=hist_data[yaxis], mode=\"lines+markers\"),\n        row=row_i, col=col_i\n    )\n\nfig.update_layout(\n    height=400 * row_i, width=1200, title_text=\"Word Concentration by Document Occurence\", showlegend=False\n)\nfig.update_xaxes(title_text=xaxis, row=2, col=1)\nfig.update_yaxes(title_text=yaxis, row=2, col=1)\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Stop Words\n\nThese terms are supposedly less important."},{"metadata":{"trusted":true},"cell_type":"code","source":"docpct_maxthresh = 0.7","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_wordcounts(\n    wordcounts, filter_func=lambda x: x[x[\"percent_of_doc\"] >= docpct_maxthresh]\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Some terms like \"copyright\", \"preprint\", \"author\", \"funder\" in the biorxiv dataset are suspect."},{"metadata":{"trusted":true},"cell_type":"code","source":"wc_df = wordcounts[\"biorxiv\"]\nwc_df[\n    (wc_df[\"percent_of_doc\"] >= 0.9) & (wc_df.index.str.upper() != wc_df.index)\n].sort_values(\"count_of_doc\", ascending=False)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Upon closer investigation, it seems biorvix documents have copyright notices (*i.e. `\"International license is made available...\"`)*. These are probably artifacts from data encoding..."},{"metadata":{"trusted":true},"cell_type":"code","source":"biorxiv_df = all_dfs[\"biorxiv\"]\nwith pd.option_context('display.max_colwidth', -1):\n    display(biorxiv_df.loc[biorxiv_df[\"text_clean\"].str.contains(\"preprint\", flags=re.I),[\"text\"]].head())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There was an attempt to parse out these sentences using the `clean_text` function above, but I found the actual values to have variations that are hard to capture in a regex pattern. In addition, a copyright notice sentence might be spread across different sections.  \n  \nA workaround would be to add the following stop words when using a bag-of-words approach: `\"author\"`,`\"biorxiv\"`,`\"copyright\"`, `\"doi\"`, `\"funder\"`, `\"holder\"`, `\"medrxiv\"`, `\"peer\"`,`\"preprint\"`, `\"reviewed\"`.  \n  \nI am not sure how word embedding approaches would be affected by these copyright notices."},{"metadata":{"trusted":true},"cell_type":"code","source":"addl_stop_words = {\n    \"biorxiv\": {\n        \"stop_sentence\": \"The copyright holder for this preprint, which was not peer-reviewed, is the author or funder.\",\n        \"stop_words\": [\n            \"license\",\n            \"copyright\",\n            \"holder\",\n            \"preprint\",\n            \"peer\",\n            \"reviewed\",\n            \"author\",\n            \"funder\",\n            \"doi\",\n            \"biorxiv\",\n            \"medrxiv\",\n            \"preprint\",\n        ],\n    }\n}\n\njson.dump(addl_stop_words, open(\"addl_stop_words.json\", \"w\"))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}