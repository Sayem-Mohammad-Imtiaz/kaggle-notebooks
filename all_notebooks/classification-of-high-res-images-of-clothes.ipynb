{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport pandas as pd \nimport matplotlib.pyplot as plt \nimport torch\nimport torch.nn.functional as F\nimport torchvision\nimport torchvision.transforms as transforms\nimport numpy as np\n\n\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split\n","metadata":{"execution":{"iopub.status.busy":"2021-06-16T13:05:52.233681Z","iopub.execute_input":"2021-06-16T13:05:52.233979Z","iopub.status.idle":"2021-06-16T13:05:52.241944Z","shell.execute_reply.started":"2021-06-16T13:05:52.233951Z","shell.execute_reply":"2021-06-16T13:05:52.241045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls","metadata":{"execution":{"iopub.status.busy":"2021-06-13T14:26:19.435429Z","iopub.execute_input":"2021-06-13T14:26:19.435797Z","iopub.status.idle":"2021-06-13T14:26:20.165216Z","shell.execute_reply.started":"2021-06-13T14:26:19.43577Z","shell.execute_reply":"2021-06-13T14:26:20.16405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os, os.path\nprint(len(list(os.listdir('../input/clothing-dataset-full/images_original'))))","metadata":{"execution":{"iopub.status.busy":"2021-06-15T14:46:54.126453Z","iopub.execute_input":"2021-06-15T14:46:54.126858Z","iopub.status.idle":"2021-06-15T14:46:54.430967Z","shell.execute_reply.started":"2021-06-15T14:46:54.126821Z","shell.execute_reply":"2021-06-15T14:46:54.429815Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nlabels = pd.read_csv('../input/clothing-dataset-full/images.csv')\nlabels.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-16T13:05:52.539055Z","iopub.execute_input":"2021-06-16T13:05:52.53943Z","iopub.status.idle":"2021-06-16T13:05:52.613693Z","shell.execute_reply.started":"2021-06-16T13:05:52.539399Z","shell.execute_reply":"2021-06-16T13:05:52.612829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels['label'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-06-13T14:26:20.974279Z","iopub.execute_input":"2021-06-13T14:26:20.974546Z","iopub.status.idle":"2021-06-13T14:26:20.990333Z","shell.execute_reply.started":"2021-06-13T14:26:20.97452Z","shell.execute_reply":"2021-06-13T14:26:20.989379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels['image'] = labels['image'] + '.jpg'\nlabel_df = labels[['image', 'label']]\nclasses=list(label_df['label'].unique())\nrepl={}\nfor i in range(len(classes)):\n    repl[classes[i]]=i\nfor i in range(len(label_df)):\n    label_df['label'][i]=repl[label_df['label'][i]]\n\n#deli=[]\n#for i in range(len(label_df)):\n  #if(not(os.path.isfile('input/clothing-dataset-full/images_original/'+label_df['image'][i]))):\n    \n    #deli.append(i)\n#label_df = label_df.drop(label_df.index[deli])\n    \n#label_df.to_csv ('./train_csv.csv', index = False, header=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-16T13:06:00.696772Z","iopub.execute_input":"2021-06-16T13:06:00.697268Z","iopub.status.idle":"2021-06-16T13:06:01.2046Z","shell.execute_reply.started":"2021-06-16T13:06:00.697239Z","shell.execute_reply":"2021-06-16T13:06:01.2038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ntrain_label_df, test_label_df = train_test_split(label_df, test_size=0.10)\ntrain_label_df.to_csv ('./train_csv.csv', index = False, header=True)\ntest_label_df.to_csv ('./test_csv.csv', index = False, header=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-16T13:06:05.129918Z","iopub.execute_input":"2021-06-16T13:06:05.130436Z","iopub.status.idle":"2021-06-16T13:06:05.161986Z","shell.execute_reply.started":"2021-06-16T13:06:05.130391Z","shell.execute_reply":"2021-06-16T13:06:05.161097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"label_df","metadata":{"execution":{"iopub.status.busy":"2021-06-13T14:26:21.551652Z","iopub.execute_input":"2021-06-13T14:26:21.55194Z","iopub.status.idle":"2021-06-13T14:26:21.563374Z","shell.execute_reply.started":"2021-06-13T14:26:21.551911Z","shell.execute_reply":"2021-06-13T14:26:21.56254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import Dataset\nimport pandas as pd\nimport os\nfrom PIL import Image\nimport torch\n\nclass DressDataset(Dataset):\n    def __init__(self, root_dir, annotation_file, transform=None):\n        self.root_dir = root_dir\n        self.annotations = pd.read_csv(annotation_file)\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.annotations)\n\n    def __getitem__(self, index):\n        img_id = self.annotations.iloc[index, 0]\n        img = Image.open(self.root_dir+ img_id).convert(\"RGB\")\n        y_label = torch.tensor(float(self.annotations.iloc[index, 1]))\n\n        if self.transform is not None:\n            img = self.transform(img)\n\n        return (img, y_label)","metadata":{"execution":{"iopub.status.busy":"2021-06-16T13:06:12.955181Z","iopub.execute_input":"2021-06-16T13:06:12.955735Z","iopub.status.idle":"2021-06-16T13:06:12.963399Z","shell.execute_reply.started":"2021-06-16T13:06:12.955694Z","shell.execute_reply":"2021-06-16T13:06:12.96264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transform = transforms.Compose(\n    [transforms.Resize((356, 356)),transforms.ToTensor(),\n     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n\nbatch_size = 4\n\ntrainset = DressDataset(root_dir='../input/clothing-dataset-full/images_original/', annotation_file='./train_csv.csv', transform=transform)\n\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n                                          shuffle=True, num_workers=2)\n\ntestset = DressDataset(root_dir='../input/clothing-dataset-full/images_original/', annotation_file='./test_csv.csv', transform=transform)\ntestloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n                                         shuffle=False, num_workers=2)\n","metadata":{"execution":{"iopub.status.busy":"2021-06-16T13:06:26.904136Z","iopub.execute_input":"2021-06-16T13:06:26.904768Z","iopub.status.idle":"2021-06-16T13:06:26.924301Z","shell.execute_reply.started":"2021-06-16T13:06:26.904731Z","shell.execute_reply":"2021-06-16T13:06:26.92354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass CustomConv(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(3, 6, 5)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        self.fc1 = nn.Linear(118336, 20)\n        \n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = torch.flatten(x, 1) # flatten all dimensions except batch\n        x = self.fc1(x)\n        return x\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch.nn as nn\nimport torch.nn.functional as F\n\nclass Myconv2D(torch.autograd.Function):\n\n    # Note that both forward and backward are @staticmethods\n    @staticmethod\n    # bias is an optional argument\n    def forward(ctx, input, weight, bias=None):\n        ctx.save_for_backward(input, weight, bias)\n       \n        # input dim [batch_size, channels, height, width]\n\n        batch_size = len(input)\n        channels = input[0][0]\n        h, w = input.size(2), input.size(3)\n        #image =  torch.tensor(batch_size, channels, h, w) # input image\n        #print (image)\n        #print (image.size())\n        #print (image[0])\n\n        kh, kw = 3, 3# kernel size\n        dh, dw = 2, 2 # stride\n\n        #filt = nn.parameter(channels, kh, kw) # filter (create this as nn.Parameter if you want to train it)\n        #print (filt)\n        patches = input.unfold(2, kh, dh).unfold(3, kw, dw)\n        #print(patches.shape)\n        patches = patches.contiguous().view(batch_size, channels, -1, kh, kw)\n        #print(patches.shape)\n        patches = patches.permute(0, 2, 1, 3, 4)\n        patches = patches.view(-1, channels, kh, kw)\n        #print (patches.shape)\n        #print (\"Filter shape\",filt.shape)\n        #print(patches)\n        #print (patches,\"\\n\",\"\\n\",\"\\n\",filt)\n        #print (\"Builtin multiplication\")\n        \n        dummy = patches\n\n        patches = patches * weight # same is done below with 4 nested loops with custom operation\n        \n\n        #print(patches)\n        patches = patches.sum(1) # previous and this patch are same what is it doing\n        #print (patches)\n        output = patches # is it right ?\n        if bias is not None:\n            output += bias.unsqueeze(0).expand_as(output)\n        return output\n\n    # This function has only a single output, so it gets only one gradient\n    @staticmethod\n    def backward(ctx, grad_output):\n        # This is a pattern that is very convenient - at the top of backward\n        # unpack saved_tensors and initialize all gradients w.r.t. inputs to\n        # None. Thanks to the fact that additional trailing Nones are\n        # ignored, the return statement is simple even when the function has\n        # optional inputs.\n        input, weight, bias = ctx.saved_tensors\n        grad_input = grad_weight = grad_bias = None\n\n        # These needs_input_grad checks are optional and there only to\n        # improve efficiency. If you want to make your code simpler, you can\n        # skip them. Returning gradients for inputs that don't require it is\n        # not an error.\n        if ctx.needs_input_grad[0]:\n            grad_input = grad_output.mm(weight)\n        if ctx.needs_input_grad[1]:\n            grad_weight = grad_output.t().mm(input)\n        if bias is not None and ctx.needs_input_grad[2]:\n            grad_bias = grad_output.sum(0).squeeze(0)\n\n        return grad_input, grad_weight, grad_bias\n\n\nclass Myconv2D(nn.Module):\n    def __init__(self):\n        super(Myconv2D, self).__init__()\n        self.fn = Myconv2D.apply\n        # weight tensor = out_channels× in_channels/groups ×kH×kW\n        self.weight = nn.Parameter(torch.randn(3, 1, 2, 2)) # when groups=1\n\n    def forward(self, x):\n        x = self.fn(x, self.weight)\n        return x\n","metadata":{"execution":{"iopub.status.busy":"2021-06-16T13:07:23.436042Z","iopub.execute_input":"2021-06-16T13:07:23.436377Z","iopub.status.idle":"2021-06-16T13:07:23.451258Z","shell.execute_reply.started":"2021-06-16T13:07:23.436348Z","shell.execute_reply":"2021-06-16T13:07:23.450515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 2\nchannels = 5\nh, w = 12, 12\nimage = torch.randn(batch_size, channels, h, w) # input image\n\nkh, kw = 3, 3 # kernel size\ndh, dw = 3, 3 # stride\n\n# Create conv\nconv = nn.Conv2d(5, 7, (kh, kw), stride=(dh, dw), bias=False)\nfilt = conv.weight\n\n# Manual approach\npatches = image.unfold(2, kh, dh).unfold(3, kw, dw)\nprint(patches.shape) # batch_size, channels, h_windows, w_windows, kh, kw\n\npatches = patches.contiguous().view(batch_size, channels, -1, kh, kw)\nprint(patches.shape) # batch_size, channels, windows, kh, kw\n\nnb_windows = patches.size(2)\n\n# Now we have to shift the windows into the batch dimension.\n# Maybe there is another way without .permute, but this should work\npatches = patches.permute(0, 2, 1, 3, 4)\nprint(patches.shape) # batch_size, nb_windows, channels, kh, kw\n\n# Calculate the conv operation manually\nres = (patches.unsqueeze(2) * filt.unsqueeze(0).unsqueeze(1)).sum([3, 4, 5])\nprint(res.shape) # batch_size, output_pixels, out_channels\nres = res.permute(0, 2, 1) # batch_size, out_channels, output_pixels\n# assuming h = w\nh = w = int(res.size(2)**0.5)\nres = res.view(batch_size, -1, h, w)\n\n# Module approach\nout = conv(image)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Polynomial3(torch.nn.Module):\n    def __init__(self):\n        \"\"\"\n        In the constructor we instantiate four parameters and assign them as\n        member parameters.\n        \"\"\"\n        super().__init__()\n        self.conv = nn.Conv2d(5, 7, (3, 3), stride=(3, 3), bias=False)\n        self.weight = self.conv.weight\n\n    def forward(self, x):\n        \"\"\"\n        In the forward function we accept a Tensor of input data and we must return\n        a Tensor of output data. We can use Modules defined in the constructor as\n        well as arbitrary operators on Tensors.\n        \"\"\"\n        batch_size = 2\n        channels = 5\n        h, w = 12, 12\n        image = x # input image\n\n        kh, kw = 3, 3 # kernel size\n        dh, dw = 3, 3 # stride\n\n        # Create conv\n        \n        filt = self.conv.weight\n\n        # Manual approach\n        patches = image.unfold(2, kh, dh).unfold(3, kw, dw)\n        print(patches.shape) # batch_size, channels, h_windows, w_windows, kh, kw\n\n        patches = patches.contiguous().view(batch_size, channels, -1, kh, kw)\n        print(patches.shape) # batch_size, channels, windows, kh, kw\n\n        nb_windows = patches.size(2)\n\n        # Now we have to shift the windows into the batch dimension.\n        # Maybe there is another way without .permute, but this should work\n        patches = patches.permute(0, 2, 1, 3, 4)\n        print(patches.shape) # batch_size, nb_windows, channels, kh, kw\n\n        # Calculate the conv operation manually\n        res = (patches.unsqueeze(2) * filt.unsqueeze(0).unsqueeze(1)).sum([3, 4, 5])\n        print(res.shape) # batch_size, output_pixels, out_channels\n        res = res.permute(0, 2, 1) # batch_size, out_channels, output_pixels\n        # assuming h = w\n        h = w = int(res.size(2)**0.5)\n        res = res.view(batch_size, -1, h, w)\n\n        # Module approach\n        out = self.conv(image)\n        return out","metadata":{"execution":{"iopub.status.busy":"2021-06-16T13:53:44.615579Z","iopub.execute_input":"2021-06-16T13:53:44.616377Z","iopub.status.idle":"2021-06-16T13:53:44.627546Z","shell.execute_reply.started":"2021-06-16T13:53:44.616325Z","shell.execute_reply":"2021-06-16T13:53:44.626745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Net(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 =Polynomial3()\n        self.pool = nn.MaxPool2d(2, 2)\n        self.fc1 = nn.Linear(118336, 20)\n        \n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = torch.flatten(x, 1) # flatten all dimensions except batch\n        x = self.fc1(x)\n        return x\n\n\nnet = Net()","metadata":{"execution":{"iopub.status.busy":"2021-06-16T13:53:50.346036Z","iopub.execute_input":"2021-06-16T13:53:50.346384Z","iopub.status.idle":"2021-06-16T13:53:50.377262Z","shell.execute_reply.started":"2021-06-16T13:53:50.346353Z","shell.execute_reply":"2021-06-16T13:53:50.37619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch.optim as optim\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)","metadata":{"execution":{"iopub.status.busy":"2021-06-16T13:13:41.597472Z","iopub.execute_input":"2021-06-16T13:13:41.597846Z","iopub.status.idle":"2021-06-16T13:13:41.603237Z","shell.execute_reply.started":"2021-06-16T13:13:41.597815Z","shell.execute_reply":"2021-06-16T13:13:41.602505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#outputs = 0\n#labels = 0\nlosses=[]\nfor epoch in range(20):  \n\n    running_loss = 0.0\n    i=0\n    for inputs, labels in trainloader:\n        \n        optimizer.zero_grad()\n        outputs = net(inputs.float())\n        #break\n        loss = criterion(outputs,labels.long())\n        loss.backward()\n        optimizer.step()        \n        running_loss += loss.item()\n        i+=1\n        if i % 200 == 0:    \n            print('[%d, %5d] loss: %.3f' %\n                  (epoch + 1, i + 1, running_loss / 200))\n            running_loss = 0.0\n    with torch.no_grad():\n        val_loss=0\n        for val_inputs,val_labels in testloader:\n            val_outputs = net(val_inputs.float())\n            #break\n            loss = criterion(val_outputs,val_labels.long())     \n            val_loss += loss.item()\n        losses.append(val_loss)\n        print(\"Validation loss:\",val_loss)\n        l=len(losses)\n        if(l>1):\n            if(losses[l-1]>losses[l-2]):\n                break\n\nprint('Finished Training')","metadata":{"execution":{"iopub.status.busy":"2021-06-16T13:53:55.655233Z","iopub.execute_input":"2021-06-16T13:53:55.655613Z","iopub.status.idle":"2021-06-16T13:53:58.747457Z","shell.execute_reply.started":"2021-06-16T13:53:55.655557Z","shell.execute_reply":"2021-06-16T13:53:58.745555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(losses)","metadata":{"execution":{"iopub.status.busy":"2021-06-13T15:02:45.685511Z","iopub.execute_input":"2021-06-13T15:02:45.685918Z","iopub.status.idle":"2021-06-13T15:02:45.907113Z","shell.execute_reply.started":"2021-06-13T15:02:45.685879Z","shell.execute_reply":"2021-06-13T15:02:45.905909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"losses","metadata":{"execution":{"iopub.status.busy":"2021-06-13T15:02:45.90946Z","iopub.execute_input":"2021-06-13T15:02:45.909959Z","iopub.status.idle":"2021-06-13T15:02:45.917029Z","shell.execute_reply.started":"2021-06-13T15:02:45.909905Z","shell.execute_reply":"2021-06-13T15:02:45.915865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(net, \"./demo1.pth\")","metadata":{"execution":{"iopub.status.busy":"2021-06-13T15:02:45.919088Z","iopub.execute_input":"2021-06-13T15:02:45.919592Z","iopub.status.idle":"2021-06-13T15:02:45.953806Z","shell.execute_reply.started":"2021-06-13T15:02:45.919536Z","shell.execute_reply":"2021-06-13T15:02:45.952497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inputs=0\nfor inputs, labels in trainloader:\n    break","metadata":{"execution":{"iopub.status.busy":"2021-06-16T14:25:37.323842Z","iopub.execute_input":"2021-06-16T14:25:37.324313Z","iopub.status.idle":"2021-06-16T14:25:39.731655Z","shell.execute_reply.started":"2021-06-16T14:25:37.324278Z","shell.execute_reply":"2021-06-16T14:25:39.730562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inputs.shape","metadata":{"execution":{"iopub.status.busy":"2021-06-16T14:25:52.56949Z","iopub.execute_input":"2021-06-16T14:25:52.569906Z","iopub.status.idle":"2021-06-16T14:25:52.577938Z","shell.execute_reply.started":"2021-06-16T14:25:52.569869Z","shell.execute_reply":"2021-06-16T14:25:52.576785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"patches = inputs.unfold(2, 4, 4).unfold(3, 4, 4)\nprint(patches.shape)","metadata":{"execution":{"iopub.status.busy":"2021-06-16T14:30:21.614695Z","iopub.execute_input":"2021-06-16T14:30:21.615099Z","iopub.status.idle":"2021-06-16T14:30:21.620709Z","shell.execute_reply.started":"2021-06-16T14:30:21.615064Z","shell.execute_reply":"2021-06-16T14:30:21.619628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"patches = patches.contiguous().view(4, 3, -1, 4, 4)\nprint(patches.shape)","metadata":{"execution":{"iopub.status.busy":"2021-06-16T14:30:54.66627Z","iopub.execute_input":"2021-06-16T14:30:54.666963Z","iopub.status.idle":"2021-06-16T14:30:54.680923Z","shell.execute_reply.started":"2021-06-16T14:30:54.666909Z","shell.execute_reply":"2021-06-16T14:30:54.679686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nb_windows = patches.size(2)\n\n# Now we have to shift the windows into the batch dimension.\n# Maybe there is another way without .permute, but this should work\npatches = patches.permute(0, 2, 1, 3, 4)\nprint(patches.shape)","metadata":{"execution":{"iopub.status.busy":"2021-06-16T14:31:45.922061Z","iopub.execute_input":"2021-06-16T14:31:45.92244Z","iopub.status.idle":"2021-06-16T14:31:45.928152Z","shell.execute_reply.started":"2021-06-16T14:31:45.922408Z","shell.execute_reply":"2021-06-16T14:31:45.927334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"res = (patches.unsqueeze(2) * filt.unsqueeze(0).unsqueeze(1)).sum([3, 4, 5])\nprint(res.shape)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"conv = nn.Conv2d(4, 7, (3, 3), stride=(3, 3), bias=False)\nfilt=conv.weight\nprint(filt.shape)","metadata":{"execution":{"iopub.status.busy":"2021-06-16T15:19:37.904006Z","iopub.execute_input":"2021-06-16T15:19:37.904379Z","iopub.status.idle":"2021-06-16T15:19:37.911213Z","shell.execute_reply.started":"2021-06-16T15:19:37.904348Z","shell.execute_reply":"2021-06-16T15:19:37.910457Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(filt.unsqueeze(0).shape)","metadata":{"execution":{"iopub.status.busy":"2021-06-16T15:16:28.265308Z","iopub.execute_input":"2021-06-16T15:16:28.265823Z","iopub.status.idle":"2021-06-16T15:16:28.271486Z","shell.execute_reply.started":"2021-06-16T15:16:28.265757Z","shell.execute_reply":"2021-06-16T15:16:28.270511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(filt.unsqueeze(0).unsqueeze(1).shape)","metadata":{"execution":{"iopub.status.busy":"2021-06-16T15:16:34.030361Z","iopub.execute_input":"2021-06-16T15:16:34.030759Z","iopub.status.idle":"2021-06-16T15:16:34.036037Z","shell.execute_reply.started":"2021-06-16T15:16:34.030727Z","shell.execute_reply":"2021-06-16T15:16:34.035028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#res = (patches.unsqueeze(2) * filt.unsqueeze(0).unsqueeze(1)).sum([3, 4, 5])\nprint(patches.unsqueeze(2).shape)\nprint(filt.unsqueeze(0).unsqueeze(1).shape)","metadata":{"execution":{"iopub.status.busy":"2021-06-16T15:22:44.08642Z","iopub.execute_input":"2021-06-16T15:22:44.086853Z","iopub.status.idle":"2021-06-16T15:22:44.094454Z","shell.execute_reply.started":"2021-06-16T15:22:44.086818Z","shell.execute_reply":"2021-06-16T15:22:44.093287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"patches.unsqueeze(2) * filt.unsqueeze(0).unsqueeze(1)","metadata":{"execution":{"iopub.status.busy":"2021-06-16T15:19:45.538374Z","iopub.execute_input":"2021-06-16T15:19:45.538915Z","iopub.status.idle":"2021-06-16T15:19:45.565312Z","shell.execute_reply.started":"2021-06-16T15:19:45.53888Z","shell.execute_reply":"2021-06-16T15:19:45.563941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"patches = image.unfold(2, kh, dh).unfold(3, kw, dw)\n        print(patches.shape) # batch_size, channels, h_windows, w_windows, kh, kw\n\n        patches = patches.contiguous().view(batch_size, channels, -1, kh, kw)\n        print(patches.shape) # batch_size, channels, windows, kh, kw\n\n        nb_windows = patches.size(2)\n\n        # Now we have to shift the windows into the batch dimension.\n        # Maybe there is another way without .permute, but this should work\n        patches = patches.permute(0, 2, 1, 3, 4)\n        print(patches.shape) # batch_size, nb_windows, channels, kh, kw\n\n        # Calculate the conv operation manually\n        res = (patches.unsqueeze(2) * filt.unsqueeze(0).unsqueeze(1)).sum([3, 4, 5])\n        print(res.shape) # batch_size, output_pixels, out_channels\n        res = res.permute(0, 2, 1) # batch_size, out_channels, output_pixels\n        # assuming h = w\n        h = w = int(res.size(2)**0.5)\n        res = res.view(batch_size, -1, h, w)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 1\nchannels = 5\nh, w = 12, 12\nimage = torch.randn(batch_size, channels, h, w) # input image\n\nkh, kw = 3, 3 # kernel size\ndh, dw = 3, 3 # stride\nprint(\"image\",image.shape)\nfilt = torch.randn(channels, kh, kw) # filter (create this as nn.Parameter if you want to train it)\nprint(\"filt = torch.randn(channels, kh, kw)\",filt.shape) \n\npatches = image.unfold(2, kh, dh).unfold(3, kw, dw)\nprint(\"image.unfold(2, kh, dh)\",image.unfold(2, kh, dh).shape)\nprint(\"image.unfold(2, kh, dh).unfold(3, kw, dw)\",image.unfold(2, kh, dh).unfold(3, kw, dw).shape)\n#> torch.Size([1, 5, 4, 4, 3, 3]) # batch_size, channels, h_windows, w_windows, kh, kw\n\npatches = patches.contiguous().view(batch_size, channels, -1, kh, kw)\nprint(\"patches.contiguous().view(batch_size, channels, -1, kh, kw)\",patches.shape) \n#> torch.Size([1, 5, 16, 3, 3]) # batch_size, channels, windows, kh, kw\n\n# Now we have to shift the windows into the batch dimension.\n# Maybe there is another way without .permute, but this should work\npatches = patches.permute(0, 2, 1, 3, 4)\nprint(\"patches = patches.permute(0, 2, 1, 3, 4)\",patches.shape) \npatches = patches.view(-1, channels, kh, kw)\nprint(\"patches = patches.view(-1, channels, kh, kw)\",patches.shape)\n#> torch.Size([16, 5, 3, 3]) # windows * batch_size, channels, kh, kw\n\n# Now we can use our filter and sum over the channels\npatches = patches * filt\nprint(\"patches = patches * filt\",patches.shape,\"*\",filt.shape)\n\npatches = patches.sum(1)\nprint(patches.shape)\n#> torch.Size([16, 3, 3])","metadata":{"execution":{"iopub.status.busy":"2021-06-16T16:17:43.061992Z","iopub.execute_input":"2021-06-16T16:17:43.062396Z","iopub.status.idle":"2021-06-16T16:17:43.079828Z","shell.execute_reply.started":"2021-06-16T16:17:43.062363Z","shell.execute_reply":"2021-06-16T16:17:43.078694Z"},"trusted":true},"execution_count":null,"outputs":[]}]}