{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1. Information and Importing Data: "},{"metadata":{"trusted":true},"cell_type":"code","source":"df=pd.read_csv(\"../input/lending-club-loans/lending_club_loan_two.csv\")\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"----\n-----\nHere is the information on this particular data set:\n\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>LoanStatNew</th>\n      <th>Description</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>loan_amnt</td>\n      <td>The listed amount of the loan applied for by the borrower. If at some point in time, the credit department reduces the loan amount, then it will be reflected in this value.</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>term</td>\n      <td>The number of payments on the loan. Values are in months and can be either 36 or 60.</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>int_rate</td>\n      <td>Interest Rate on the loan</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>installment</td>\n      <td>The monthly payment owed by the borrower if the loan originates.</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>grade</td>\n      <td>LC assigned loan grade</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>sub_grade</td>\n      <td>LC assigned loan subgrade</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>emp_title</td>\n      <td>The job title supplied by the Borrower when applying for the loan.*</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>emp_length</td>\n      <td>Employment length in years. Possible values are between 0 and 10 where 0 means less than one year and 10 means ten or more years.</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>home_ownership</td>\n      <td>The home ownership status provided by the borrower during registration or obtained from the credit report. Our values are: RENT, OWN, MORTGAGE, OTHER</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>annual_inc</td>\n      <td>The self-reported annual income provided by the borrower during registration.</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>verification_status</td>\n      <td>Indicates if income was verified by LC, not verified, or if the income source was verified</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>issue_d</td>\n      <td>The month which the loan was funded</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>loan_status</td>\n      <td>Current status of the loan</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>purpose</td>\n      <td>A category provided by the borrower for the loan request.</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>title</td>\n      <td>The loan title provided by the borrower</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>zip_code</td>\n      <td>The first 3 numbers of the zip code provided by the borrower in the loan application.</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>addr_state</td>\n      <td>The state provided by the borrower in the loan application</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>dti</td>\n      <td>A ratio calculated using the borrower’s total monthly debt payments on the total debt obligations, excluding mortgage and the requested LC loan, divided by the borrower’s self-reported monthly income.</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>earliest_cr_line</td>\n      <td>The month the borrower's earliest reported credit line was opened</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>open_acc</td>\n      <td>The number of open credit lines in the borrower's credit file.</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>pub_rec</td>\n      <td>Number of derogatory public records</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>revol_bal</td>\n      <td>Total credit revolving balance</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>revol_util</td>\n      <td>Revolving line utilization rate, or the amount of credit the borrower is using relative to all available revolving credit.</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>total_acc</td>\n      <td>The total number of credit lines currently in the borrower's credit file</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>initial_list_status</td>\n      <td>The initial listing status of the loan. Possible values are – W, F</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>application_type</td>\n      <td>Indicates whether the loan is an individual application or a joint application with two co-borrowers</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>mort_acc</td>\n      <td>Number of mortgage accounts.</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>pub_rec_bankruptcies</td>\n      <td>Number of public record bankruptcies</td>\n    </tr>\n  </tbody>\n</table>\n\n---\n----"},{"metadata":{},"cell_type":"markdown","source":"# 2. Explarotory Data Analysis:"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set_style(\"whitegrid\")\nplt.figure(figsize=(15,10))\nsns.countplot(x=\"loan_status\",data=df,palette=\"viridis\")\n#This is the target column our ML algorithm will base on its predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,10))\nplt.hist(df[\"loan_amnt\"],bins=50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.corr()\n# here we explore correlation between the continuous feature variables","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,10))\nsns.heatmap(df.corr(),cmap=\"viridis\",annot=True,linewidths=0.1)\n#Here we visualize these correlations","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#We notice the perfect correlation between the \"installment\" and loan amount features\nplt.figure(figsize=(15,10))\nsns.scatterplot(x=\"installment\",y=\"loan_amnt\", data=df, color=\"red\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,10))\nsns.boxplot(x=\"loan_status\", y=\"loan_amnt\",data=df)\n# Here we create a boxplot showing the relationship between the loan status and the Loan Amount","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Hello**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# from the box plot above we can see any difference between fully paind and charged off loans with respect to loan amoun\n#here we want to get more details \ndf.groupby(\"loan_status\")[\"loan_amnt\"].describe()\n#The loan amount mean of fully paid loans are clearly fewer than charged off loans","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"grade\"].unique()\n#There are 7 different unique grades","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"sub_grade\"].unique()\n#There are 35 different sub grades\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,10))\nsns.countplot(x=\"grade\", data=df, hue=\"loan_status\",palette=\"magma\")\n# Here we creats a countplot per grade with the hue to the loan_status label","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,10))\nsns.countplot(x=\"sub_grade\",data=df,palette=\"coolwarm\",order=sorted(df[\"sub_grade\"].unique()))\n#Here we explore both all loans made per subgrade as well being separated based on the loan_status\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,10))\nsns.countplot(x=\"sub_grade\",data=df,palette=\"coolwarm\",hue=\"loan_status\",order=sorted(df[\"sub_grade\"].unique()))\n#We can clearly notice that higher grades tend to pay back more than lower grades","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,10))\nsns.countplot(x=\"sub_grade\",data=df[(df['grade']=='G') | (df['grade']=='F')],palette=\"coolwarm\",hue=\"loan_status\",order=sorted(df[(df['grade']=='G') | (df['grade']=='F')][\"sub_grade\"].unique()))\n# Here we see only the grades F and G","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"loan_repaid\"]=df[\"loan_status\"].map({\"Fully Paid\":1, \"Charged Off\":0})\ndf[[\"loan_repaid\",\"loan_status\"]]\n# Here create a new column called 'loan_repaid' which will contain a 1 if the loan status was \"Fully Paid\" and a 0 if it was \"Charged Off\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,10))\ndf.corr()[\"loan_repaid\"][:-1].sort_values().plot(kind=\"bar\",color=\"red\")\n#Here we create a bar plot showing the correlation of the numeric features to the new loan_repaid column","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Data Preprocesing:"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()\n#here we want to find out the columns with missing values\n#There 6 columns with missing values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"100*df.isnull().sum()/len(df)\n#Here we see the percentages of missing values per column","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The data with highest misisng rate pertages are emp_title(represents job title of the borrower), emp_length(employment length in years), and mort_acc(Number of mortgage accounts)."},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"emp_title\"].nunique()\n#we see that there are 173105 employment titles","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"emp_title\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop(\"emp_title\",axis=1, inplace=True)\ndf.columns\n# We will drop this column because there are too many unique job titles to try to convert this to a dummy variable feature","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"emp_length\"].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"emp_length\"].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,10))\nsns.set_style(\"whitegrid\")\nsns.countplot(x=\"emp_length\",data=df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,10))\nsns.countplot(x=\"emp_length\", data=df, hue=\"loan_status\",palette=\"magma\")\n#From the plot we can not a distinctive difference between employment length and loan status","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[df[\"loan_status\"]==\"Fully Paid\"].groupby(\"emp_length\").count()[\"loan_status\"]\n#Here we get number of person who has the loan status fully paid according to employment length","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.groupby(\"emp_length\").count()[\"loan_status\"]\n#Here we get number of person  according to employment length","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[df[\"loan_status\"]==\"Charged Off\"].groupby(\"emp_length\").count()[\"loan_status\"]\n#Here we get number of person who has the loan status charged of according to employment length","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We need percentage in order to evaluate imporance of this column"},{"metadata":{"trusted":true},"cell_type":"code","source":"(df[df[\"loan_status\"]==\"Charged Off\"].groupby(\"emp_length\").count()[\"loan_status\"])/(df.groupby(\"emp_length\").count()[\"loan_status\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,10))\n((df[df[\"loan_status\"]==\"Charged Off\"].groupby(\"emp_length\").count()[\"loan_status\"])/(df.groupby(\"emp_length\").count()[\"loan_status\"])).plot(kind=\"bar\",color=\"green\")\n# here we understand that the length of employment foes not make any difference, we can just drop this column","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop(\"emp_length\",axis=1,inplace=True)\ndf.isnull().sum()\n#Now we have just 4 columns with missing values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"title\"].nunique()\n#There are 48817 separate titles","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"title\"].head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"purpose\"].nunique()\n#There are 14 different unique data in purpose column","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"purpose\"].unique()\n# Here we see that the title column is simply a string subcategory/description of the purpose column","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop(\"title\",axis=1,inplace=True)\n#Therefore we will just drop this column because another column includes already the same features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.corr()[\"mort_acc\"].sort_values()\n# we see thta the total_account feature correlates with the mort_acc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"total_acc_average=df.groupby(\"total_acc\").mean()[\"mort_acc\"]\ntotal_acc_average\n#Here wegroup the dataframe by the total_acc and calculate the mean value for the mort_acc per total_acc entry","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#here we create a function in order to fill mort_account column with the mean of the corressponding total account column\ndef fill_mort_acc(total_acc,mort_acc):\n    if np.isnan(mort_acc):\n        return total_acc_average[total_acc]\n    else:\n        return mort_acc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"mort_acc\"]=df.apply(lambda x: fill_mort_acc(x[\"total_acc\"],x[\"mort_acc\"]),axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()\n#No we have revol_util and the pub_rec_bankruptcies have missing data points, with 0.5% of the total data.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.dropna(inplace=True)\n#Therefore we drop all the rows with missing values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()\n# Now we have not missing values in any column","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4.Dealing with Categorical Variables and Transforming them for the Algorithm:"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.select_dtypes(\"object\").columns\n#These are the columns with non numerical data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"term\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"term\"]=df[\"term\"].apply(lambda term:int(term[:3]))\n#Here we get the numerical data from the string form of the column term","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"term\"].value_counts()\n#Now this column is also  numerical","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop(\"grade\",axis=1, inplace=True)\n#This column is already represented in sub_grade columns, so we can just drop it","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dummies=pd.get_dummies(df[\"sub_grade\"], drop_first=True)\ndummies","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df=pd.concat([df.drop(\"sub_grade\", axis=1), dummies],axis=1)\ndf.head()\n#here we concetenate the dummies with the original data withou sub_grade column","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.select_dtypes(\"object\").columns\n#There are 8 categorical columns left","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[['verification_status', 'application_type','initial_list_status','purpose']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dummies=pd.get_dummies(df[['verification_status', 'application_type','initial_list_status','purpose']],drop_first=True)\ndf=pd.concat([df.drop(['verification_status', 'application_type','initial_list_status','purpose'],axis=1), dummies],axis=1)\ndf.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.select_dtypes(\"object\").columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"home_ownership\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"home_ownership\"]=df[\"home_ownership\"].replace([\"NONE\",\"ANY\"],\"OTHER\")\ndf[\"home_ownership\"].value_counts()\n#we replace NONE and ANY with OTHER before we get dummies because there are very few people in these two status","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dummies=pd.get_dummies(df[\"home_ownership\"],drop_first=True)\ndf=pd.concat([df.drop(\"home_ownership\",axis=1),dummies],axis=1)\ndf.select_dtypes(\"object\")\n#here we convert this column into dummy values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"address\"] \n#we will get just numerical zip codes by creating a new column adn dropping address column","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"zip_code\"]=df[\"address\"].apply(lambda address: address[-5:])\ndf[\"zip_code\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"loan_repaid\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dummies=pd.get_dummies(df[\"zip_code\"],drop_first=True)\ndf=pd.concat([df.drop(\"zip_code\",axis=1), dummies],axis=1)\ndf.select_dtypes(\"object\").columns\n# we also transform this colum into dummy values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop(\"address\",axis=1, inplace=True)\ndf.select_dtypes(\"object\").columns\n#we drop the address column ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"earliest_cr_line\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"earliest_cr_year\"]=df[\"earliest_cr_line\"].apply(lambda date: int(date[-4:]))\n#here we get the year part of it\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop(\"earliest_cr_line\",axis=1,inplace=True)\ndf.select_dtypes(\"object\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#We can not know beforehand whether or not a loan will be issued beforehand,so this data is useless for the algorithm\ndf.drop([\"issue_d\",\"loan_status\"],axis=1, inplace=True)\n# Loan status is also numerically represented in another column, so we will just drop this column\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.select_dtypes(\"object\").columns\n#There is not any categorical column and the data is ready for machine learning algorithm","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5. Training the Deep Learning Algorithm "},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"loan_repaid\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X=df.drop(\"loan_repaid\", axis=1).values\nX","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.shape # Features has 78 columns and 395219 rows and we transformed the into a numpy array for deep learning algorithm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y=df[\"loan_repaid\"].values\ny","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test=train_test_split(X,y, test_size=0.3)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Now we need to standartize the X features\nfrom sklearn.preprocessing import MinMaxScaler","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler=MinMaxScaler()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train=scaler.fit_transform(X_train) # here the scaler fit and transform the X training set\nX_test=scaler.transform(X_test) #there is no need to fit again, so we just transform the X test set","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**The next step is to create the deep learning algorithm**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense,Dropout","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model=Sequential()\nmodel.add(Dense(units=78, activation=\"relu\"))\nmodel.add(Dense(units=35, activation=\"relu\"))\nmodel.add(Dense(units=17,activation=\"relu\"))\nmodel.add(Dense(units=1,activation=\"sigmoid\"))\n#In the last layer we will use sigmoid activation function because we will predict a binary class,not continuous\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(loss=\"binary_crossentropy\",optimizer=\"adam\")\n#This configures model for training, we select binary cross entropy as the loss function and adam as the optimizer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(x=X_train,y=y_train,epochs=25,validation_data=(X_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 6. Evaluation of the Model's Performance:"},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(model.history.history) #here we create a data fram that shows the values in our model and correspoding real values\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(model.history.history).plot(figsize=(15,10))\n#There is huge distance between our model and validation data, so it means there is overfitting problem","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions=model.predict_classes(X_test)\npredictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test,predictions))\nprint(\"**************************\")\nprint(confusion_matrix(y_test,predictions))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The performance of the model is not good, so we will retrain our data and make new predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.callbacks import EarlyStopping\n#we will add early stopping which will work when the divergence between the training data and validation data happen","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"early_stop=EarlyStopping(monitor=\"val_loss\",patience=10,verbose=1,mode=\"min\")\n#here we create our early stopping which will stop model after 10 number patience ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model2=Sequential()\nmodel2.add(Dense(units=78, activation=\"relu\"))\nmodel2.add(Dropout(0.2)) # we can choose the precentage between 0 and 100, 0.5 represents %50 of the layer\nmodel2.add(Dense(units=39, activation=\"relu\"))\nmodel2.add(Dropout(0.2))\nmodel2.add(Dense(units=19,activation=\"relu\"))\nmodel2.add(Dropout(0.2))\nmodel2.add(Dense(units=1,activation=\"sigmoid\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model2.compile(loss=\"binary_crossentropy\",optimizer=\"adam\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model2.fit(x=X_train,y=y_train,epochs=25, validation_data=(X_test, y_test), callbacks=[early_stop])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(model2.history.history).plot(figsize=(15,10))\n#This plot is far better than previous one","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions2=model2.predict_classes(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test,predictions2))\nprint(\"**************************\")\nprint(confusion_matrix(y_test,predictions2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"TP:10269, FP:13235, TN=94928, FN:134. This model is very good to predict the 0 class, but not as goog as in prediction of 1 class.Overall it is better than the previous one"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}