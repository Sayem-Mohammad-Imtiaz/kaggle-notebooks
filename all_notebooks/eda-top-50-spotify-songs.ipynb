{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Introduction\n![](http://www.hd-tecnologia.com/imagenes/articulos/2018/07/Spotify-lanza-su-versi%C3%B3n-lite.jpg)\n<B/>\nWelcome to Top 50 Spotify Songs In this data set is suitable for practice EDA and learning. Before doing machine learning, you have to understand and confident your data ready to be used with machine learning algorithms right out of the box.\n<B/>\nIn this kernel, I will visualize the data with Matplotlib and Plotly and then demonstrate some essential methods of EDA and concept during EDA\n\n<span style=\"color:red\"> *Please upvote this kernel if you like it. It motivates me to produce more quality content :)*</span>"},{"metadata":{},"cell_type":"markdown","source":"<B/> What is EDA: EDA is an approach to analyze data. The first and foremost task that the data analysts do is to view the data and tries to make some sense out of it. Later we figure out what questions we want to ask and how to use the available data to get the answers we need.\n<B/>\nEDA helps 1) Delve into the data set 2)Examine the relationships among the variables 3)Identify any interesting observation 4) Develop an initial idea of possible associations among the predictors and the target variable.\n<B/>\n\nMETHODS OF EDA\n- Univariate Visualization: Summary statistics\n- Bivariate Visualization: Find relation between variable\n- Multivariate Visualization: Understand interactions between different fields in the data set\n- Dimensionality reduction help understand variance "},{"metadata":{},"cell_type":"markdown","source":"# Key Concepts in Notebook "},{"metadata":{},"cell_type":"markdown","source":"## <B/> 2 types of Data Analysis \n- Confirmatory Data Analysis\n- Exploratory Data Analysis\n\n## <B/> 4 Objectives of EDA\n- Discover Patterns\n- Spot Anomalies\n- Frame Hypothesis\n- Check Assumptions\n\n## <B/> 2 Objectives of EDA\n- Univariate Analysis\n- Bivariate Analysis\n\n\n"},{"metadata":{},"cell_type":"markdown","source":"# Stuff done during EDA \n- Trends\n- Distribution \n- Mean\n- Median\n- Outlier\n- Spread measurement(SD)\n- Correlations\n- Visual Exploration"},{"metadata":{},"cell_type":"markdown","source":"# Import Libraries"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport scipy.stats as st\nfrom sklearn import ensemble, tree, linear_model\nimport missingno as msno","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<pass>To start exploring your data, you’ll need to start by actually loading your data.You’ll probably know this already, but thanks to the Pandas library, this becomes an easy task: you import the package as pd, You use the read_csv() function to which you pass the URL in which the data can be found and a header argument. \n<pass>\nAlternatively, there are also other arguments that you can specify to ensure that your data is read incorrectly: you can specify the delimiter to use with the sep or delimiter arguments, the column names to use with names or the column to use as the row labels for the resulting DataFrame with index_col."},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/top50spotify2019/top50.csv' , encoding=\"ISO-8859-1\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"One of the most elementary steps to do this is by getting a basic description of your data. A basic description of your data is indeed a very broad term: you can interpret it as a quick and dirty way to get some information on your data, as a way of getting some simple, easy-to-understand information on your data, to get a basic feel for your data. We can use the describe() function to get various summary statistics that exclude NaN values."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now that you have got a general idea about your data set, it’s also a good idea to take a closer look at the data itself. Pandas library head() and tail() functions can check out the first and last line of your DataFrame, respectively."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.tail()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Renaming the columns\ndf.rename(columns={'Track.Name':'track_name','Artist.Name':'artist_name','Beats.Per.Minute':'beats_per_minute','Loudness..dB..':'Loudness(dB)','Valence.':'Valence','Length.':'Length', 'Acousticness..':'Acousticness','Speechiness.':'Speechiness'},inplace=True)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Examine numerical features"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"numeric_feature = df.select_dtypes(include=[np.number])\nnumeric_feature.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let check data type in data set"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### MISSING VALUE"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# The Challenges of Your Data\nNow that we have gathered some necessary information on your data, it’s a good idea to just go a little bit deeper into the challenges that the data might pose. In this chapter, check the shape of data, which is outliers."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"df.skew()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = df['Energy']\nplt.figure(1); plt.title('Johnson')\nsns.distplot(y , kde=False , fit=st.johnsonsu)\nplt.figure(2); plt.title('Normal')\nsns.distplot(y , kde=False , fit=st.norm)\nplt.figure(3); plt.title('Log Normal')\nsns.distplot(y , kde=False , fit=st.lognorm)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It show Energy doesn't follow normal distribution, so before performing regression it has to be transformed.Best fit is unbounded Johnson distribution."},{"metadata":{},"cell_type":"markdown","source":"Finding Correlation coefficients between numeric features and SalePrice"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"correlation = numeric_feature.corr()\nprint(correlation['Energy'].sort_values(ascending =False))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<B/> We will start with the following visualization methods to analyze the data better:\n- Correlation Heat Map\n- Zoomed Heat Map\n- Pair Plot\n- Scatter Plot\n"},{"metadata":{},"cell_type":"markdown","source":"## Correlation Heat Map"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"f , ax = plt.subplots(figsize = (20,10))\nsns.heatmap(correlation , square = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<pass>\nThe heatmap is the best way to get a quick overview of correlated feature.\n<pass>\n\nI observed energy correlation. As it is observed  loudness  and valence most correlate with energy. \nHeatmaps are great to detect multicollinearity situations and in problems related to feature selection. It comes as an excellent exploratory tool."},{"metadata":{},"cell_type":"markdown","source":"## Zoomed HeatMap"},{"metadata":{},"cell_type":"markdown","source":"### Energy Correlation matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"k  = 5\ncols = correlation.nlargest(k,'Energy')['Energy'].index\nprint(cols)\ncm = np.corrcoef(df[cols].values.T)\nf ,ax = plt.subplots(figsize = (14,12))\nsns.heatmap(cm , linewidths=0.01,square=True,annot=True,cmap='viridis',\n            linecolor=\"white\",xticklabels = cols.values ,annot_kws = {'size':12},yticklabels = cols.values)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From above zoomed heatmap, it is observed that Energy & Loudness are correlated."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set()\ncolumns = ['Energy','Loudness(dB)','Valence','Length','Liveness']\nsns.pairplot(df[columns], size = 2 , kind = 'scatter' ,diag_kind ='kde')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A pairplot plot a pairwise relationships in a dataset. The pairplot function creates a grid of Axes such that each variable in data will by shared in the y-axis across a single row and in the x-axis across a single column.\n</pre>\n\nAlthough we already know some of the main figures, this pair plot gives us a reasonable overview insight into the correlated features. Here are some of my analysis.\n\n- Observation between 'Energy' and 'Loudness' you will see loudness and energy have a tendency correlated. It fits in the area not spread.\n\n- 'Energy' and 'Liveness' are not fit in the model. We respect it not correlated. "},{"metadata":{},"cell_type":"markdown","source":"## Scatter Plot"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(nrows = 2 , ncols = 2 ,figsize = (15,10))\nEnergy_scatter_plot = pd.concat([df['Energy'],df['Loudness(dB)']],axis = 1)\nsns.regplot(x='Loudness(dB)',y = 'Energy',data = Energy_scatter_plot,scatter= True, fit_reg=True, ax=ax1)\n\nValence_scatter_plot = pd.concat([df['Energy'],df['Valence']],axis = 1)\nsns.regplot(x='Valence',y = 'Energy',data = Valence_scatter_plot,scatter= True, fit_reg=True, ax=ax2)\n\nLength_scatter_plot = pd.concat([df['Energy'],df['Length']],axis = 1)\nsns.regplot(x='Length',y = 'Energy',data = Length_scatter_plot,scatter= True, fit_reg=True, ax=ax3)\n\nLength_scatter_plot = pd.concat([df['Energy'],df['Liveness']],axis = 1)\nsns.regplot(x='Liveness',y = 'Energy',data = Length_scatter_plot,scatter= True, fit_reg=True, ax=ax4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"Artistname = df.pivot_table(index = 'Genre' , values = 'Energy' , aggfunc = np.median).sort_values('Energy' , ascending = False)\nArtistname.plot(kind = 'bar' , color = 'blue')\nplt.xlabel('Genre')\nplt.ylabel('Energy')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Box plot  - 0verall loudness"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"var = 'Loudness(dB)'\ndata = pd.concat([df['Energy'] , df[var]], axis = 1)\nf , ax = plt.subplots(figsize = (12,8))\nfig = sns.boxplot(x = var ,y = 'Energy', data =data)\nfig.axis (ymin = 0 , ymax = 100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## If you like this kernel greatly appreciate an upvote."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}