{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"!ls ../input/forest-forest-dataset","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Importing Librabries**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/forest-forest-dataset/forestfires.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# checking columns of dataset \ndf.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# checking null values \ndf.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,10))\ncorr= df.corr()\nsns.heatmap(corr,annot=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"here we can see that RH and temp are less corelated. \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.dtypes\n#month and days are object type hence applying labelencoder and onehotencoder  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# convering categorical data \ndf = pd.get_dummies(df, prefix=['month','day'],drop_first=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = df.iloc[:,[27]].values\nx = df.iloc[:,:-1].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test= train_test_split(x,y ,test_size = 0.25,random_state= 42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fitting Random Forest Regression to the dataset\nfrom sklearn.ensemble import RandomForestRegressor\nregressor = RandomForestRegressor(n_estimators = 10, random_state = 0)\nregressor.fit(x, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(y_test.shape, y_train.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = regressor.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\nprint('mean_absolute_error : {} '.format(metrics.mean_absolute_error(y_test, y_pred)))\nprint('mean_squared_error : {} '.format(metrics.mean_squared_error(y_test, y_pred)))\nprint('mean_squared_error : {} '.format(np.sqrt(metrics.mean_squared_error(y_test, y_pred))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# checking current parameter used in random forest regressor \nregressor.get_params","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Hyper parameter tuning using RandomizedSearchCV**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(random_grid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use the random grid to search for best hyperparameters\n# First create the base model to tune\nrf = RandomForestRegressor()\n# Random search of parameters, using 3 fold cross validation, \n# search across 100 different combinations, and use all available cores\nrf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"rf_random.fit(x,y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_random.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hyper_regressor = RandomForestRegressor(n_estimators=800,min_samples_split=2,min_samples_leaf=2,max_features='sqrt',max_depth=20,bootstrap=False)\nhyper_regressor.fit(x, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = hyper_regressor.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"after hyper parameter tuning mse,mae has decreesd"},{"metadata":{"trusted":true},"cell_type":"code","source":"# checking mean absolute error , mean square error , RMSE\nfrom sklearn import metrics\nprint('mean_absolute_error : {} '.format(metrics.mean_absolute_error(y_test, y_pred)))\nprint('mean_squared_error : {} '.format(metrics.mean_squared_error(y_test, y_pred)))\nprint('root_mean_squared_error : {} '.format(np.sqrt(metrics.mean_squared_error(y_test, y_pred))))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Prediction using ANN**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# importing required librabries \nimport keras \nfrom keras.models import Sequential\nfrom keras.layers import Dense ,Flatten\nfrom keras.layers import Dropout\nfrom keras.layers import LeakyReLU,PReLU,ELU","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#intilaizing the ANN\nann_regressor = Sequential()\n# adding input layer or first hiden layer to regressor \nann_regressor.add(Dense(output_dim=50,init = 'he_uniform',activation='relu',input_dim =27))\n# Adding the second hidden layer\nann_regressor.add(Dense(output_dim = 25, init = 'he_uniform',activation='relu'))\n# Adding the third hidden layer\nann_regressor.add(Dense(output_dim = 50, init = 'he_uniform',activation='relu'))\n# adding output layer \n# The Output Layer :\nann_regressor.add(Dense(1, init= 'he_uniform',activation='linear'))\n# Compile the network :\nann_regressor.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\nann_regressor.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"model_hist= ann_regressor.fit(x_train,y_train,validation_split=0.20, batch_size = 10, nb_epoch = 50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ann_pred = ann_regressor.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('mean_absolute_error : {} '.format(metrics.mean_absolute_error(y_test, ann_pred)))\nprint('mean_squared_error : {} '.format(metrics.mean_squared_error(y_test, ann_pred)))\nprint('mean_squared_error : {} '.format(np.sqrt(metrics.mean_squared_error(y_test, ann_pred))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}