{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd \nimport numpy as np \n\n\n#loading the dataset\ndf = pd.read_excel(\"../input/ensemblemodels/Coca_Rating_Ensemble.xlsx\") \n\n\n#2.\tWork on each feature of the dataset to create a data dictionary as displayed in the below image\n#######feature of the dataset to create a data dictionary\n\n#######feature of the dataset to create a data dictionary\n\n\nd_types =[\"nominal\",\"nominal\",\"Count\",\"Ratio\",\"Ratio\",\"Nominal\",\"Interval\",\"nominal\",\"Nominal\"]\ndata_details =pd.DataFrame({\"column name\":df.columns,\n                            \"data types \":d_types,\n                            \"data types-p\":df.dtypes})\n\n\n            #3.\tData Pre-processing  \n          #3.1 Data Cleaning, Feature Engineering, etc\n          \n          \n#details of df \ndf.info()\ndf.describe()          \n\n\n#data types        \ndf.dtypes\n\n\n#checking for na value\ndf.isna().sum()\ndf.isnull().sum()\ndf.dropna(inplace=True)\n\n#checking unique value for each columns\ndf.nunique()\n\n#variance of df\ndf.var()\n\n\n\n\"\"\"4.\tExploratory Data Analysis (EDA):\n4.1.\tSummary\n4.2.\tUnivariate analysis\n4.3.\tBivariate analysis\n\t \"\"\"\n    \n\n\nEDA ={\"column \": df.columns,\n      \"mean\": df.mean(),\n      \"median\":df.median(),\n      \"mode\":df.mode(),\n      \"standard deviation\": df.std(),\n      \"variance\":df.var(),\n      \"skewness\":df.skew(),\n      \"kurtosis\":df.kurt()}\n\nEDA\n\n# covariance for data set \ncovariance  =  df.cov()\ncovariance\n\n# Remove two columns name is 'C' and 'D'\ndf.drop(['Name', 'Bean_Type'], axis = 1 , inplace = True )\n\n####### graphical repersentation \n\n##historgam and scatter plot\nimport seaborn as sns\nsns.pairplot(df.iloc[:, :])\n\n\n\n# Normalization function using z std. all are continuous data.\ndef norm_func(x):\n    y=(x-x.mean())/(x.std())\n    return (y) \n\n# Normalized data frame (considering the numerical part of data)\ndf_norm = norm_func(df.iloc[:,[1,2,3,5]])\ndf_norm.describe()\n\nfrom sklearn.preprocessing import LabelEncoder\n# creating instance of labelencoder\nlabelencoder = LabelEncoder()\n\ndf[\"Company\"] = labelencoder.fit_transform(df[\"Company\"])\ndf[\"Company_Location\"] = labelencoder.fit_transform(df[\"Company_Location\"])\ndf[\"Origin\"] = labelencoder.fit_transform(df[\"Origin\"])\n\n\ndf_dummy = df.iloc[:,[4,6]]\n\npredictors_df = pd.concat([df_norm,df_dummy],axis=1)\ntarget_df = df.iloc[:,0]\n\n\n\"\"\"\n5.\tModel Building\n5.1\tBuild the model on the scaled data (try multiple options)\n5.2\tPerform Bagging, Boosting, Voting, Stacking on given datasets\n5.3\tTrain and Test the data, use grid search cross validation, compare accuracies using confusion matrix\n5.4\tBriefly explain the model output in the documentation\n \"\"\"\n\n                                   #building bagging model \n\n# Splitting data into training and testing data set\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(predictors_df,target_df, test_size = 0.2,random_state=7)\n                                         #for bagging we use label encoded data      \n\n\nfrom sklearn import tree \nclftree = tree.DecisionTreeClassifier()\nfrom sklearn.ensemble import BaggingClassifier\n\n\nbag_clf = BaggingClassifier(base_estimator = clftree, n_estimators =500,\n                            bootstrap = True, n_jobs = 1, random_state = 77)\n\nbag_clf.fit(x_train, y_train)\n\nfrom sklearn.metrics import accuracy_score, confusion_matrix\n\n# Evaluation on Testing Data\nconfusion_matrix(y_test, bag_clf.predict(x_test))\naccuracy_score(y_test, bag_clf.predict(x_test))\n\n# Evaluation on Training Data\nconfusion_matrix(y_train, bag_clf.predict(x_train))\naccuracy_score(y_train, bag_clf.predict(x_train))\n\n\n \n\n\n                                     #building  boosting model \n\n\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\n\n# Splitting data into training and testing data set\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(predictors_df,target_df, test_size = 0.2,random_state=7)\n                                            \n\n#decision tree\ndt = DecisionTreeClassifier() #storing the classifer in dt\n\ndt.fit(x_train, y_train) #fitting te model \n\ndt.score(x_test, y_test) #checking the score like accuracy\n\ndt.score(x_train, y_train)\n#so our model is overfitting \n\n                                      # Ada boosting \nada = AdaBoostClassifier(DecisionTreeClassifier(), n_estimators=10, learning_rate=7)\nada.fit(x_train,y_train)\n\nada.score(x_test,y_test)\n\nada.score(x_train,y_train)\n       \n\n                                   #building  voting model \n                                   \n# Splitting data into training and testing data set\n\nx_train, x_test, y_train, y_test = train_test_split(predictors_df,target_df, test_size = 0.2,random_state=7)\n                                          \nfrom sklearn.ensemble import VotingClassifier\n# Voting Classifier \nfrom sklearn.linear_model import LogisticRegression # importing logistc regression\nfrom sklearn.svm import SVC # importing Svm \n\nlr = LogisticRegression() \ndt = DecisionTreeClassifier()\nsvm = SVC(kernel= 'poly', degree=2)\n\nevc = VotingClassifier(estimators=[('lr', lr),('dt', dt),('svm', svm)], voting='hard')\n\nevc.fit(x_train, y_train)\n\nevc.score(x_test, y_test)\n\nevc.score(x_train, y_train)\n\n                                  #building stacking model  \n #Libraries and data loading\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import KFold\nfrom sklearn import metrics\n\n\n                                   \n# Splitting data into training and testing data set\n\npredictors_df_array= predictors_df.to_numpy() \nfrom sklearn.preprocessing import LabelEncoder\n# creating instance of labelencoder\nlabelencoder = LabelEncoder()\ntarget_df_array = labelencoder.fit_transform(df[\"Company\"])\n\n\n\n\ntrain_x, test_x, train_y, test_y = train_test_split(predictors_df_array,target_df_array, test_size = 0.2,random_state=7)\n                                        \n\n# Create the ensemble's base learners and meta learner\n# Append base learners to a list\nbase_learners = []\n\nknn = KNeighborsClassifier(n_neighbors=2)\nbase_learners.append(knn)\n\ndtr = DecisionTreeClassifier(max_depth=4, random_state=123456)\nbase_learners.append(dtr)\n\nmlpc = MLPClassifier(hidden_layer_sizes =(100, ), solver='lbfgs', random_state=123456)\nbase_learners.append(mlpc)\n\n\nmeta_learner = LogisticRegression(solver='lbfgs')\n\n\n# Create the training meta data\n\n# Create variables to store meta data and the targets\nmeta_data = np.zeros((len(base_learners), len(train_x)))\nmeta_targets = np.zeros(len(train_x))\n\n# Create the cross-validation folds\nKF = KFold(n_splits = 5)\nmeta_index = 0  \nfor train_indices, test_indices in KF.split(train_x):\n    # Train each learner on the K-1 folds and create meta data for the Kth fold\n    for i in range(len(base_learners)):\n        learner = base_learners[i]\n\n        learner.fit(train_x[train_indices], train_y[train_indices])\n        predictions = learner.predict_proba(train_x[test_indices])[:,0]\n\n        meta_data[i][meta_index:meta_index+len(test_indices)] = predictions\n\n    meta_targets[meta_index:meta_index+len(test_indices)] = train_y[test_indices]\n    meta_index += len(test_indices)\n\n# Transpose the meta data to be fed into the meta learner\nmeta_data = meta_data.transpose()\n\n# Create the meta data for the test set and evaluate the base learners\ntest_meta_data = np.zeros((len(base_learners), len(test_x)))\nbase_acc = []\n\nfor i in range(len(base_learners)):\n    learner = base_learners[i]\n    learner.fit(train_x, train_y)\n    predictions = learner.predict_proba(test_x)[:,0]\n    test_meta_data[i] = predictions\n\n    acc = metrics.accuracy_score(test_y, learner.predict(test_x))\n    base_acc.append(acc)\ntest_meta_data = test_meta_data.transpose()\n\n# Fit the meta learner on the train set and evaluate it on the test set\nmeta_learner.fit(meta_data, meta_targets)\nensemble_predictions = meta_learner.predict(test_meta_data)\n\nacc = metrics.accuracy_score(test_y, ensemble_predictions)\n\n# Print the results\nfor i in range(len(base_learners)):\n    learner = base_learners[i]\n\n    print(f'{base_acc[i]:.2f} {learner.__class__.__name__}')\n    \nprint(f'{acc:.2f} Ensemble')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}