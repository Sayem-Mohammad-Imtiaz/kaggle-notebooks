{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#daily cases covid tested positive...\n#daily death rate\ncovid=pd.read_csv('/kaggle/input/us-counties-covid19-weather-sociohealth-data/US_counties_COVID19_health_weather_data.csv')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"covid","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"covidgb=covid.groupby(['state','date']).mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"covidgb2=covid.groupby(['state','date']).sum()\ncovidgb2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lijst=['cases','deaths','area_sqmi','num_deaths']\nfor li in lijst:\n    covidgb[li]=covidgb2[li]/covidgb2['total_population']\ncovidgb['total_population']=covidgb2['total_population']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"col=covidgb.describe().T.index\ncol","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def clustertechniquestf(dtrain,termtf,label,indexv):\n    #print(dtrain)    \n    ydata=dtrain.iloc[:,7:]\n    !pip install dabl\n    import dabl\n    dtrain = dabl.clean(dtrain, verbose=1)\n    dabl.plot(dtrain, label)\n    if False:\n        from sklearn.decomposition import TruncatedSVD\n        tsvd=TruncatedSVD(n_components=50, n_iter=7, random_state=42)\n        from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n\n        print('#vectorizing text')\n        vect=TfidfVectorizer()\n        dfidf=vect.fit_transform(dtrain[termtf]+' '+dtrain['Genres']+ dtrain['Category']) \n        dfidf=tsvd.fit_transform(dfidf)\n        for xi in range(50):\n            dtrain['svd'+str(xi)]=dfidf[:,xi]\n        dtrain=dtrain.drop(termtf,axis=1)\n    if False:\n        kolom=dtrain.describe().T\n        from sklearn.preprocessing import OneHotEncoder\n        toencode=[ci for ci in dtrain.columns if ci not in list(kolom.index)+[indexv,label]]\n        print('to encode',toencode)\n        ohe=OneHotEncoder()\n        data2=ohe.fit_transform(dtrain[toencode].fillna('')).toarray()\n        dtrain=dtrain.drop(toencode,axis=1)\n        #from sklearn.preprocessing import OneHotEncoder\n        #data2=OneHotEncoder().fit_transform(data[['Gender']]).toarray()\n        tel=0\n        for ci in ohe.get_feature_names():\n            dtrain[ci]=data2[:,tel]\n            tel+=1\n        dtrain[label]=ydata[label]\n    #add random columns\n    if False:\n        for xi in range(10):\n            labnm='rand'+str(xi)\n            dtrain[labnm]=np.random.randint(0,10,size=(len(dtrain), 1))\n    \n    print('#encodings',dtrain.shape)\n    cols=[ci for ci in dtrain.columns if ci not in [indexv,'index',label]]\n    dtest=dtrain[dtrain[label].isnull()==True][[indexv,label]]\n    print(dtest)\n\n    print('encodings  after shape',dtrain.shape)\n    from sklearn.metrics.pairwise import cosine_similarity,laplacian_kernel,manhattan_distances\n    \n   ########### cosine similarity check\n    if False:\n        ydata=dtrain[[indexv,label]]        \n        dtrain['grb']=np.round(dtrain[label]/dtrain[label].max())\n        print(dtrain)\n        datagb=dtrain.groupby(['grb']).median()\n        print(datagb)\n        dtrain=datagb.dropna().append(dtrain)\n        ydata=dtrain[[indexv,label]]\n        print(ydata)\n        data2=pd.DataFrame(cosine_similarity(dtrain.drop(['grb',label],axis=1),datagb.drop(label,axis=1) ))\n        tel=0\n        for ci in ohe.get_feature_names():\n            dtrain[ci]=data2[:,tel]\n            tel+=1\n        \n        dtrain[label]=ydata[label]\n        dtrain[indexv]=ydata[indexv]\n        ydata=dtrain[[indexv,label]]    \n    \n    #split data or use splitted data\n    Xtrain=dtrain[dtrain[label].isnull()==False].drop([indexv,label],axis=1).fillna(0)\n    Y_train=dtrain[dtrain[label].isnull()==False][label]\n    Xtest=dtrain[dtrain[label].isnull()==True].drop([indexv,label],axis=1).fillna(0)\n    Y_test=dtrain[dtrain[label].isnull()==True][label].fillna(0)\n    print(Y_test)\n    for xi in range(len(Y_test)):\n        Y_test.iloc[xi]=np.random.random((1,1))[0]\n    print(Y_test)\n    if len(Xtest)==0:\n        from sklearn.model_selection import train_test_split\n        Xtrain,Xtest,Y_train,Y_test = train_test_split(dtrain.drop(label,axis=1).fillna(0),dtrain[label],test_size=0.25,random_state=0)\n    lenxtr=len(Xtrain)\n\n    print('splitting data train test X-y',Xtrain.shape,Y_train.shape,Xtest.shape,Y_test.shape)\n   \n    n_folds = 5\n    def rmsle_cv(model):\n        kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(X_total_clu[:lenxtr])\n        rmse= np.sqrt(-cross_val_score(model, X_total_clu[:lenxtr], Y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n        return(rmse)\n\n    import matplotlib.pyplot as plt \n    from sklearn.preprocessing import QuantileTransformer,RobustScaler,Normalizer,MaxAbsScaler,MinMaxScaler\n    \n    from sklearn.decomposition import PCA,TruncatedSVD,NMF,FastICA\n    from umap import UMAP  # knn lookalike of tSNE but faster, so scales up\n    from sklearn.manifold import TSNE #limit number of records to 100000\n    import xgboost as xgb\n    scalers =[Dummy(1),\n              #MinMaxScaler(),\n             #RobustScaler(),\n             #MaxAbsScaler(),\n             #Normalizer(),\n             #QuantileTransformer(output_distribution='uniform')\n    ]\n    clusters = [Dummy(1),\n               PCA(n_components=20,random_state=0,whiten=True),\n               #FastICA(n_components=15,random_state=0),\n               #TruncatedSVD(n_components=15, n_iter=7, random_state=42),\n                #NMF(n_components=10,random_state=0),            \n                #UMAP(n_neighbors=5,n_components=3, min_dist=0.3,metric='minkowski'),\n                #TSNE(n_components=2,random_state=0)\n                ] \n    clunaam=['raw','PCA','ica','tSVD','UMAP','tSNE']#,'ICA','tSVD','nmf','UMAP','tSNE']\n    \n    from lightgbm import LGBMClassifier,LGBMRegressor\n    \n    from sklearn.neighbors import KNeighborsClassifier,KNeighborsRegressor\n    from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\n    from sklearn.model_selection import KFold, cross_val_score, train_test_split\n\n    from sklearn.svm import SVC, LinearSVC,NuSVC\n    from sklearn.multiclass import OneVsRestClassifier\n    from sklearn.ensemble import RandomForestRegressor,ExtraTreesRegressor, RandomForestClassifier,ExtraTreesClassifier, AdaBoostClassifier, GradientBoostingClassifier\n    from sklearn.neural_network import MLPClassifier,MLPRegressor\n    from sklearn.linear_model import PassiveAggressiveClassifier,Perceptron,SGDClassifier,LogisticRegression,Lasso\n    import xgboost as xgb\n    from sklearn.tree import DecisionTreeClassifier\n    from sklearn.naive_bayes import GaussianNB\n    from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n    from sklearn.linear_model import ElasticNetCV,ridge_regression,HuberRegressor,LinearRegression,BayesianRidge,RANSACRegressor\n    from sklearn.ensemble import RandomForestRegressor,GradientBoostingRegressor\n    from sklearn.metrics import mean_squared_error,r2_score,mean_absolute_error,mean_squared_log_error\n    from sklearn.ensemble import BaggingRegressor,VotingRegressor\n\n    classifiers = [LGBMRegressor(),\n                   xgb.XGBRegressor(n_estimators=100, reg_lambda=1,gamma=0,  max_depth=3),\n                   #GradientBoostingRegressor(),\n                   ExtraTreesRegressor(n_jobs=4),\n                   RandomForestRegressor(random_state=1, n_estimators=10),      \n                   ElasticNet(),\n                   #VotingRegressor([('lrg', LGBMRegressor()), ('xg', xgb.XGBRegressor()),('xt',ExtraTreesRegressor(n_jobs=4)),('kn',RandomForestRegressor(random_state=1, n_estimators=10))],n_jobs=4),\n                   #BaggingRegressor(base_estimator=Lasso(alpha =0.0005, random_state=1)),\n                   BayesianRidge(),\n                   # Lasso(alpha =0.0005, random_state=1),\n                   #RANSACRegressor(),\n                   KNeighborsRegressor(),\n                   #ElasticNetCV(cv=5, random_state=0),\n                   #HuberRegressor(),\n                   #LinearRegression(),\n                  ]\n    clanaam= ['lgbm','xgb','Gboost','Xtree','rFor','vote','bagg','lasso','BaysR','Ransac','KNNr','elast','huber','linear',]\n    from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n    \n    results=[]\n\n    for sca in scalers:\n        X_train = sca.fit_transform(Xtrain)\n        X_test = sca.transform(Xtest)\n            \n        #cluster data\n        for clu in clusters:\n            clunm=clunaam[clusters.index(clu)] #find naam\n            clunm=str(clu)[:10]\n            X_total_clu = clu.fit_transform(np.concatenate( (X_train,X_test),axis=0))\n            if False:\n                X_total_clu=np.concatenate((X_total_clu,np.concatenate( (X_train,X_test),axis=0)),axis=1)\n            print(X_total_clu.shape)\n            plt.scatter(X_total_clu[:lenxtr,0],X_total_clu[:lenxtr,1],c=Y_train.values,cmap='prism')\n            plt.title(clu)\n            plt.show()\n\n            #classifiy \n            for cla in classifiers:\n                import datetime\n                start = datetime.datetime.now()\n                #clanm=clanaam[classifiers.index(cla)] #find naam\n                clanm=str(cla)[:10]\n                print('    ',cla)\n                print(rmsle_cv(cla).mean())\n                #cla.fit(X_total_clu,np.concatenate( (Y_train,Y_test)) )\n                cla.fit(X_total_clu[:lenxtr],Y_train )\n\n                #predict\n                trainpredi=cla.predict(X_total_clu[:lenxtr])\n\n                #print(classification_report(trainpredi,Y_train))            \n                testpredi=cla.predict(X_total_clu[lenxtr:])  \n                try:\n                    trainprediprob=cla.predict(X_total_clu[:lenxtr])\n                    testprediprob=cla.predict(X_total_clu[lenxtr:]) \n                    plt.scatter(x=trainprediprob,y=Y_train,marker=\"x\",alpha=0.53)\n                    plt.show()\n                    plt.scatter(x=testprediprob, y=testpredi, marker='.', alpha=0.53)\n                    plt.show()\n                except:\n                    print()\n                #testpredi=converging(pd.DataFrame(X_train),pd.DataFrame(X_test),Y_train,pd.DataFrame(testpredi),Y_test,clu,cla) #PCA(n_components=10,random_state=0,whiten=True),MLPClassifier(alpha=0.510,activation='logistic'))\n                try:\n                    feat=pd.DataFrame(cla.feature_importances_)\n                    feat['fields']=Xtrain.columns\n                    print(feat.sort_values(0)[-10:])\n                except:\n                    print()\n                if len(dtest)==0:\n                    test_score=cla.score(X_total_clu[lenxtr:],Y_test)\n                    mse = mean_squared_error(testpredi,Y_test)\n                    train_score=cla.score(X_total_clu[:lenxtr],Y_train)\n\n                    \n                    \n                    r2s=r2_score(testpredi,Y_test)  \n                    mse=mean_squared_error(testpredi,Y_test)  \n                    mae=mean_absolute_error(testpredi,Y_test)\n                    li = [clunm,clanm,train_score,test_score,mse,mae,r2s,np.sqrt(mse)]\n                    results.append(li)\n                    print('Test r2score',r2s,'mse',mse,'rmse',np.sqrt(mse),'mae',mae,pd.DataFrame(results) )\n\n                    plt.title(clanm+'test corr & mse:'+np.str(test_score)+' '+np.str(mse)+' and test confusionmatrix')\n                    plt.scatter(x=Y_test, y=testpredi, marker='.', alpha=1)\n                    plt.scatter(x=[np.mean(Y_test)], y=[np.mean(testpredi)], marker='o', color='red')\n                    plt.xlabel('Real test'); plt.ylabel('Pred. test')\n                    plt.show()\n\n\n                else:\n    #                testpredlabel=le.inverse_transform(testpredi)  #use if you labellezid the classes \n\n\n\n                    testpredlabel=testpredi\n                    try:\n                        rmsle=np.sqrt(mean_squared_log_error( trainpredi, Y_train ))\n                    except:\n                        rmsle='xxx'\n                    print('train correl',r2_score(trainpredi,Y_train),'mse ',mean_squared_error(trainpredi,Y_train),'rmse',np.sqrt(mean_squared_error(trainpredi,Y_train)),'mae',mean_absolute_error(trainpredi,Y_train),'rmsle',rmsle)\n                    results.append([str(sca)[:10],clunm,clanm,r2_score(trainpredi,Y_train),mean_squared_error(trainpredi,Y_train),np.sqrt(mean_squared_error(trainpredi,Y_train)),mean_absolute_error(trainpredi,Y_train),rmsle])\n                    print(pd.DataFrame(results).sort_values(6))\n                    submit = pd.DataFrame({'Id': dtest[indexv].astype('int'),'PredictedValue': testpredlabel})\n                    #submit['id']=submit['id'].astype('int')\n\n                    filenaam='subm_'+clunm+'_'+clanm+'_'+str(sca)[:10]+'.csv'\n                    submit.to_csv(path_or_buf =filenaam, index=False)\n                    print(submit.head())\n\n                print(clanm,clunm,sca,'0 classifier time',datetime.datetime.now()-start)\n            \n    if len(dtest)==0:       \n        print(pd.DataFrame(results).sort_values(3))\n        submit=[]\n    return submit\n\n#Custom Transformer that extracts columns passed as argument to its constructor \nclass Dummy( ):\n    #Class Constructor \n    def __init__( self, feature_names ):\n        self._feature_names = feature_names \n    \n    #Return self nothing else to do here    \n    def fit( self, X, y = None ):\n        return self \n    \n    #Method that describes what we need this transformer to do\n    def fit_transform(self, X, y = None ):\n        return X\n    \n    def transform(self, X, y = None ):\n        return X\n\n#total.columns=[0,1,2,3,4,5,6,7,'strength','index']\n\nclustertechniquestf(covidgb2.drop(['cases'],axis=1),\"\",\"deaths\",'area_sqmi') \n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}