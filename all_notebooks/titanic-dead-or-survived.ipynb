{"cells":[{"metadata":{"_cell_guid":"a0d4e45b-7b63-4860-86a3-8a459df47a1f","_uuid":"3639e10ed4ac8e93c5d2c3f4b2120154fa793173"},"cell_type":"markdown","source":"<ol> <b>Summary</b>\n    <li>Logistics Regression is used for predicting the Survival status of a traveller</li>\n    <li>Data engineering for dropping columns which contain redundant or obvious imformation.</li>\n    <li>Cross validation is used to select best regularization parameter for achieving better accuracy on training dataset.</li>\n    <li>Test dataset is used to compute the accuracy of our model</li>\n</ol>"},{"outputs":[],"metadata":{"_cell_guid":"1d5a5db6-233c-470b-80a9-4066d9b230ea","_uuid":"e2849e97b862bfbfb0748972cd05b21373cd52e0"},"execution_count":null,"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.linear_model import LogisticRegression # Logistics Regression Model.\nimport matplotlib.pyplot as matty #For plotting.\nimport seaborn as sns\n%matplotlib inline\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output."},{"outputs":[],"metadata":{"_kg_hide-input":false,"_cell_guid":"a5ad8db6-8a47-469a-b960-a47cbe39a005","_kg_hide-output":false,"_uuid":"066ccb7dbc23dcc47f087acc25445bd22686dc31"},"execution_count":null,"cell_type":"code","source":"def readTrainingDataset():\n    \n    # Reading training dataset into the software using pandas.\n    data = pd.read_csv('../input/train.csv');\n    \n    print(data.shape);\n    column_list = data.columns;\n    \n    # Check mean, max, min, count in the given dataset.\n    print(data.describe());\n    \n    # find features which have a missing value. \n    MISSING_FEATURE_LIST = list(data.columns[data.isnull().any()]);\n    \n    # showing missing feature list.     \n    print(\"Is their any data missing from any rows ?\");\n    print(\"Yes\" if len(MISSING_FEATURE_LIST) else \"No\");\n    print(MISSING_FEATURE_LIST);\n    \n    # Total rows in the dataset.    \n    TOTAL_VALUES = data.shape[0];\n    print(TOTAL_VALUES);\n    \n    # displaying % of values unique in each column with respect to total values.   \n    print(\"Column Name  Total Unique Values %percentage\");\n    for column in column_list:\n        total_unique_values = len(data[column].value_counts());\n        print(column, total_unique_values, total_unique_values/float(TOTAL_VALUES));\n        \n    return data;\n\ndata = readTrainingDataset();"},{"metadata":{"_cell_guid":"ff4299f0-e59a-454b-a563-588cec852237","_uuid":"344de774cc7774b41583eca7793256efc33068a9"},"cell_type":"markdown","source":"<ol>Exploratory Data Analysis\n<li>Titanic dataset have 891 travellers.</li>\n<li>Multiple Features (Total 11) storing important information like traveller's name, boarding location, sex, age etc. is provided with this dataset.</li>\n<li>Fare, Cabin, Ticket, Name and Age contain most number of unique values.</li>\n<li>Embarked feature is irrelevant in predicting the survival status of each traveller.</li>\n<li>We have 3 ordinal features in our dataset i.e. PClass (1-3), SibSp(0-6), Parch(0-6).</li>\n<li>Feature Age, Cabin and Embarked contain some missing values.</li>\n<li>Age is a real valued feature.</li>\n<li>Class Variable Survived is a categorical feature containing only 2 values i.e. 0 for Dead and 1 for Survived.</li>\n<li>61.62% of travellers in our training dataset didn't survived  the catastrophic accident whereas rest (38.38%) survived it.</li> \n</ol>"},{"outputs":[],"metadata":{"_kg_hide-input":false,"_cell_guid":"80d83b48-3ab9-49c6-babc-9ef6c22541cd","_kg_hide-output":false,"_uuid":"25a7efdaa28d967aa52f36dc7d5b9fb685154a73"},"execution_count":null,"cell_type":"code","source":"# Plot Age Feature for Further Division\ndef plotAge(data):\n    data['Age'].hist(cumulative=True, normed=1);\n    matty.title(\"Age Histogram\");\n    matty.xlabel(\"Age\");\n    matty.ylabel(\"Frequencies\");\n    matty.show();\n\nplotAge(data);"},{"metadata":{"_cell_guid":"049081e8-bb67-4065-939c-980b5c7214bb","_uuid":"e55fcec64fe3461c55384cf003c6f82d7f73db08"},"cell_type":"markdown","source":"<ul><b>Analysis:</b>\n<li>As Age is a real valued feature, we can use binning for dividing age into multiple categories.</li>\n<li>Divide Age into four main categories(bins) i.e. <b>Child (0-12)</b>, <b>Teenage (13-19)</b>, <b>Adult(20-59)</b>, <b>Old(60-)</b>.</li>\n<li>As Feature Age contain some missing value, we are replacing them with <b>mean of age</b> for further analysis </li>\n</ul>\n"},{"outputs":[],"metadata":{"_cell_guid":"a4f8d3ec-f1e7-442a-a8eb-3d2679c33e0f","_uuid":"3cc9805a5ebe2766477bca3777fb8eb0a3ac95bf","collapsed":true},"execution_count":null,"cell_type":"code","source":"# Replace missing value in Feature Age with mean of Age.\ndef replaceNanWithAgeMean(data):\n    AGE_MEAN = 29.699118;\n    data.loc[data['Age'].isnull(),'Age'] = AGE_MEAN;"},{"outputs":[],"metadata":{"_cell_guid":"dc7c5118-1f4b-410c-8607-fb489a6f5d6a","_uuid":"e6ec6689696ec4c2efcb05a61760195d545d1942","collapsed":true},"execution_count":null,"cell_type":"code","source":"replaceNanWithAgeMean(data);"},{"outputs":[],"metadata":{"_cell_guid":"c1e6a1f4-cfaf-431b-8cc1-eb52cf5de8bf","_uuid":"1870ebc4db20d420e55912b8005fbc2f011190cb","collapsed":true},"execution_count":null,"cell_type":"code","source":"def createAgeCategoryFeature(data):\n    # bins in which we have to divide out dataset.\n    bins = [0,13,19,50,100];\n    # 0 represents Child.\n    # 1 represents Teenage.\n    # 2 represents Adult.\n    # 3 represents Old.    \n    category_name = [0, 1, 2, 3];\n    data['AgeCategory'] = pd.cut(data['Age'], bins, labels=category_name);"},{"outputs":[],"metadata":{"_cell_guid":"3a8a2357-75d2-4e2e-a950-e8713abcc838","_uuid":"b7bbaf37868614b17964895d90bb4c07e072c1b0","collapsed":true},"execution_count":null,"cell_type":"code","source":"createAgeCategoryFeature(data);"},{"outputs":[],"metadata":{"_cell_guid":"816f21d0-28ef-4524-acbc-8ea2c738aaa3","_uuid":"66c6aee1cc8bf586fb129cb6f1afee708920dc1a","collapsed":true},"execution_count":null,"cell_type":"code","source":"def dropFeature(data, DROP_COLUMN_LIST):\n    for column in DROP_COLUMN_LIST:\n        data.drop([column], axis = 1, inplace = True);"},{"metadata":{"_cell_guid":"1367515b-72b8-41a5-ad84-7a4d0de1f2df","_uuid":"1d9ffb25b607406c49e41cbb65f2be43d46f9d44"},"cell_type":"markdown","source":"<ol>\n<li>Untill now, we are cleared that out of 12 features (including AgeCategory) we need only 5 features i.e. PClass, Sex, SibSp, Parch, AgeCategory for further analysis</li>\n<li>Dropping extra features from our dataset for efficient and accurate prediction. (Data Cleaning)</li>\n</ol>"},{"outputs":[],"metadata":{"_cell_guid":"b66a0b3e-c2fd-418d-a01b-0e22ebc7a9d4","_uuid":"c08e4fe263177ae45909e9dcfb9bad235f07930b","collapsed":true},"execution_count":null,"cell_type":"code","source":"EXTRA_FEATURES = ['Fare', 'PassengerId', 'Name', 'Ticket', 'Cabin', 'Age', 'Embarked'];\ndropFeature(data, EXTRA_FEATURES);"},{"outputs":[],"metadata":{"_cell_guid":"1d35a280-45d2-41ec-a5fb-a915e9dbca78","_uuid":"90f8c36bacb920746eb429cf31f75195670a30ec"},"execution_count":null,"cell_type":"code","source":"print(data.head());"},{"metadata":{"_cell_guid":"f318e8db-0a9a-40cf-9922-ac8f0d30194c","_uuid":"f42fb960f83ebd3ae543c05a0d96cc231df49396"},"cell_type":"markdown","source":"<p>Looking Nice :-)</p>"},{"outputs":[],"metadata":{"_cell_guid":"768c30df-1f8f-4530-8321-39be7667e5c9","_uuid":"db9070e428f1d2d6810f739c08a31b4fbbd8a5ac","collapsed":true},"execution_count":null,"cell_type":"code","source":"# Use binary values to represent gender in Feature sex.\n# Use 1 to represent Male and 0 to represent Female.\ndata['Sex'] = data['Sex'].apply(lambda value: int(value == 'male'));"},{"outputs":[],"metadata":{"_cell_guid":"331a3d0a-cd1b-4e76-b16f-a8260325df85","_uuid":"571b855d8d43355905218f59e9337d3f1f64d3c4"},"execution_count":null,"cell_type":"code","source":"print(data.head());"},{"outputs":[],"metadata":{"_cell_guid":"76533e09-5e68-4ae4-8090-44a48727a837","_uuid":"b73f7c406687d936d1d440c9f12f7386042d6aca","collapsed":true},"execution_count":null,"cell_type":"code","source":"# Generating training and cross_validation dataset using pandas dataframe.\ndef getTrainAndCvDataset(data):\n    # Shuffling dataset for randomnly selecting training and cv dataset.     \n    data.sample(frac=1).reset_index(drop=True)\n    TRAINING_DATA_SIZE = int(0.70*len(data));\n    train_data = data[:TRAINING_DATA_SIZE];\n    cv_data = data[TRAINING_DATA_SIZE:];\n    return train_data, cv_data;"},{"outputs":[],"metadata":{"_cell_guid":"5cc649fc-590e-445b-9b6e-9d86c4b5de31","_uuid":"c69bae97aeee4357e4aa0fde68b9ce96a39f5a06","collapsed":true},"execution_count":null,"cell_type":"code","source":"training_data, cross_validation_data = getTrainAndCvDataset(data);"},{"outputs":[],"metadata":{"_cell_guid":"f617365f-3dce-4eba-9587-c73e21dd99f2","_uuid":"4c39d18179b71f7629c7a3004ba250abb15095ec"},"execution_count":null,"cell_type":"code","source":"print(len(data));\nprint(len(training_data));\nprint(len(cross_validation_data));"},{"outputs":[],"metadata":{"_cell_guid":"f865e2a9-af43-4d2d-a0ba-da4b0431fdd9","_uuid":"8ea1d09782e57eb9224a68b61890d4f1efc16c47","collapsed":true},"execution_count":null,"cell_type":"code","source":"# pandas dataframe to numpy matrix for further computation.\ntrain_data = training_data.as_matrix();\ncv_data = cross_validation_data.as_matrix();"},{"outputs":[],"metadata":{"_cell_guid":"c8d3129c-4140-4d0f-b6ef-f653988775fd","_uuid":"addaf98bb8220504186d4a34b101b6b3831bb2ea","collapsed":true},"execution_count":null,"cell_type":"code","source":"# return model trained over given dataset.\n# dataset is a n*m matrix in which columns 1 contains class variable and \n# remaining columns contains features variable.\ndef getModelLogisticsRegression(dataset, regularization_parameter = 1):\n    inverse = 1/float(regularization_parameter);\n    model = LogisticRegression(C=inverse);\n    model.fit(dataset[:,1:], list(dataset[:,0]));\n    return model;"},{"outputs":[],"metadata":{"_cell_guid":"e44d2540-3217-40b5-8fbd-2d4621d42c83","_uuid":"c42b919cfaa394734a18f54cd5498931ad08d484","collapsed":true},"execution_count":null,"cell_type":"code","source":"# train_data and cv_data are n*m matrix.\ndef trainModelLogisticsRegression(train_data, cv_data, regularization_parameter = 1):\n    model = getModelLogisticsRegression(train_data, regularization_parameter);\n    prediction = list(model.predict(cv_data[:,1:]));\n    actual = list(cv_data[:,0]);\n    return prediction, actual;"},{"outputs":[],"metadata":{"_cell_guid":"949296f2-caf4-4c9c-aff3-f96898a47205","_uuid":"775be36af8fa8bf6a45d8cd7593aa435d839e6d1","collapsed":true},"execution_count":null,"cell_type":"code","source":"# prediction contain list of values generated from our trained model.\n# actual is a list of actual_values which are helpful in finding the strength of our model.\ndef computeStrengthOfModel(prediction, actual):\n    confusion_matrix = np.array(0).repeat(4).reshape(2,2);\n    for index, predicted_value in enumerate(prediction):\n        confusion_matrix[int(actual[index])][int(predicted_value)] = confusion_matrix[int(actual[index])][int(predicted_value)] + 1;\n    \n    total_travellers = len(prediction);\n    recall = 0.0;\n    precision = 0.0;\n    accuracy = 0.0;\n    \n    for index, row in enumerate(confusion_matrix):\n        accuracy = accuracy + confusion_matrix[index][index];\n    accuracy = accuracy/float(total_travellers);\n    \n    precision = confusion_matrix[1][1]/float(sum(confusion_matrix[:,1]));\n    recall = confusion_matrix[1][1]/float(sum(confusion_matrix[1]));\n    \n    return [precision, recall, accuracy];"},{"outputs":[],"metadata":{"_cell_guid":"cebaed1f-fb57-4ba3-9b11-ab9b065b999b","_uuid":"0558ec9836998297e646b080ecf26e9e517d22d8","collapsed":true},"execution_count":null,"cell_type":"code","source":"# Used for plotting line plot.\ndef plotData(x_axis_dataset, y_axis_dataset, x_axis_label, y_axis_label, plot_title, plot_style=\"darkgrid\"):\n    sns.set_style(plot_style);\n    matty.plot(x_axis_dataset, y_axis_dataset);\n    matty.xlabel(x_axis_label);\n    matty.ylabel(y_axis_label);\n    matty.title(plot_title);\n    matty.show();"},{"outputs":[],"metadata":{"_cell_guid":"896c1d3f-b3a6-4c37-a420-5348a1384201","_uuid":"de27dbfe8f9ee335e7fd65ee4998d6333b8a41f1","collapsed":true},"execution_count":null,"cell_type":"code","source":"# Use to generate Regularization v/s Accuracy plot.\n# This is useful for determining best alpha value for a particular model. \ndef accuracyVsRegularizationPlot(train_data, cv_data, UPPER_LIMIT = 50):\n    accuracy_list = [];\n    regularization_values = range(1,UPPER_LIMIT);\n    for regularization in regularization_values:\n        prediction, actual = trainModelLogisticsRegression(train_data, cv_data, regularization);\n        accuracy_list.append(computeStrengthOfModel(prediction, actual)[2]);\n    plotData(regularization_values, accuracy_list, \"Regularization\", \"Accuracy\", \"Regularization v/s Accuracy\");"},{"outputs":[],"metadata":{"_cell_guid":"d40ef372-1520-4cf4-8251-9d21eb4c5da3","_uuid":"6dbe816341ca14c84bad672f830365b3cc94fd46"},"execution_count":null,"cell_type":"code","source":"accuracyVsRegularizationPlot(train_data, cv_data);"},{"metadata":{"_cell_guid":"d04c281f-66fb-4846-b879-62bec00c9396","_uuid":"53174176d80e14137c21fe3f4f0cd440aac8aa30"},"cell_type":"markdown","source":"<ol>\n<li>From preceding figure, we can see that accuracy is dropping marginally (only 2%) for higher alpha value.</li>\n<li>Around alpha = 10, accuracy is increasing marginally by 0.4%.</li>\n</li>"},{"outputs":[],"metadata":{"_cell_guid":"7995de13-87fe-4f16-9dce-7972b84a7afe","_uuid":"69753f3fe89ec8fbc23337a9ccbd644e61215152"},"execution_count":null,"cell_type":"code","source":"accuracyVsRegularizationPlot(train_data, cv_data, UPPER_LIMIT=15);"},{"metadata":{"_cell_guid":"317d428c-8089-4d2a-83c8-584ab3857e05","_uuid":"9f3fe8bac95d17fcfea652a9b51a539eb96bf369"},"cell_type":"markdown","source":"<p>We will take alpha = 10 for further computations</p>"},{"outputs":[],"metadata":{"_cell_guid":"9dfd581d-ba38-4ad0-8260-241cd6c464a5","_uuid":"0ca8a9e6e0887a3e1990f6fa1856b724ecde675b","collapsed":true},"execution_count":null,"cell_type":"code","source":"# Use to generate preprocessed test dataset.\ndef getPreprocessedTestDataset():\n    data = pd.read_csv(\"../input/test.csv\");\n    replaceNanWithAgeMean(data);\n    createAgeCategoryFeature(data);\n    EXTRA_FEATURES = ['Fare', 'PassengerId', 'Name', 'Ticket', 'Cabin', 'Age', 'Embarked'];\n    dropFeature(data, EXTRA_FEATURES);\n    data['Sex'] = data['Sex'].apply(lambda value: int(value == 'male'));\n    return data;\n\ntest_data = getPreprocessedTestDataset();"},{"outputs":[],"metadata":{"_cell_guid":"d9a8c57c-077d-4383-9cf1-0dd541c55f14","_uuid":"257206c6f374addd39f6d7793deface0093b6b24","collapsed":true},"execution_count":null,"cell_type":"code","source":"model = getModelLogisticsRegression(train_data, regularization_parameter = 10);\npredicted_test_values = list(model.predict(test_data));"},{"outputs":[],"metadata":{"_cell_guid":"80756161-57c0-420e-a47c-49f9dc0a2b0e","_uuid":"95d30a335e20cfbc3d29904fbaf4e502bf183418","collapsed":true},"execution_count":null,"cell_type":"code","source":"survival_test_data = pd.read_csv('../input/genderclassmodel.csv');\nactual_test_values = list(survival_test_data['Survived']);"},{"outputs":[],"metadata":{"_cell_guid":"040cc46b-812d-4336-96f3-c912c047ec3f","_uuid":"75763896b131cec8c24a396f5a69076337cc75de"},"execution_count":null,"cell_type":"code","source":"precision, recall, accuracy = computeStrengthOfModel(predicted_test_values, actual_test_values);\nprint(\"Precision :-\",precision);\nprint(\"Recall :-\",recall)\nprint(\"Accuracy :-\",accuracy)"},{"metadata":{"_cell_guid":"db00374a-c951-4a8a-bae2-5b699627c7d2","_uuid":"a1192b427b3b889693d791ae695c62968ed85bbb"},"cell_type":"markdown","source":"<ol> <b>Analysis</b>\n<li>Recall shows us that we are able to identify all the passengers which survived the catastrophic accident.</li>\n<li>Precision shows us that which have wrongly identified some of the passengers as survived whereas in actual they died in that catastrophic accident</li>\n<li>After applying simple data engineering, we were able to get an accuracy of 97.8% which in itself is impressive</li>\n</ol>"},{"metadata":{"_cell_guid":"60781ffd-d8d2-4c5b-b290-1d3a9a75a732","_uuid":"7a354f3c0542d0cbd2614e57f9b5ba517a510118"},"cell_type":"markdown","source":"<ol> <b>Further Analysis</b>\n<li>Check wrongly identifying rows for creating new feature</li>\n<li>Some of the features are ordinal (PClass, AgeCategory), we can apply one hot encoding on these features for better classification</li>\n<li>Chance of survival will be same for a person travelling with a single or multiple SibSp. We can create a binary feature using ordinal feature which reflects the presence or absence of a sibling/spouse.</li>\n<li>We can apply same method for Parch</li>\n</ol>"}],"nbformat":4,"metadata":{"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","name":"python","codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","version":"3.6.1"},"kernelspec":{"language":"python","name":"python3","display_name":"Python 3"}},"nbformat_minor":1}