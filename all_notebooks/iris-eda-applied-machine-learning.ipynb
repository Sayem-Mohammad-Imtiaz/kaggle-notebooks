{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Feature Selection\n#Univariate Selection\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\n\n#Feature Importance\nfrom sklearn.ensemble import ExtraTreesClassifier\n\n#Importing alll the necessary packages to use the various classification algorithms\nfrom sklearn.linear_model import LogisticRegression  # for Logistic Regression algorithm\nfrom sklearn.model_selection import train_test_split #to split the dataset for training and testing\nfrom sklearn.neighbors import KNeighborsClassifier  # for K nearest neighbours\nfrom sklearn import svm  #for Support Vector Machine (SVM) Algorithm\nfrom sklearn import metrics #for checking the model accuracy\nfrom sklearn.tree import DecisionTreeClassifier #for using Decision Tree Algoithm\nfrom sklearn.metrics import confusion_matrix #Summarises Count values of Predictions\nfrom sklearn.metrics import accuracy_score \nfrom sklearn.metrics import classification_report ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's import the required Iris.csv dataset\ndf=pd.read_csv(\"../input/iris/Iris.csv\")\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's check for Missing Values\n#Let's see how many categorical and numerical variables we have in our Dataset.\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#let's see the stats and understand the average of all the features ad the distribution of data in percentiles.\n#If its a big data set you can use BokPlots to see the density of data located in percentile and also check for Outliers.\n#Since Iris is a clean and a normalized Dataset there is very little that we can do with Exploratory Data Analysis.\ndf.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Since the 'Id' column is irrelevant to our Analysis we drop the column.\ndf=df.drop('Id',axis=1) \ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's try and see how each feature is correlated with one another.\ncorrelation=df.corr()\nprint(correlation)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Heat maps are great for making trends in this kind of data more readily apparent. \n# Particularly when the data is ordered and there is clustering.\nplt.figure(figsize=(5,5))\nsns.heatmap(correlation, annot=True,cmap=\"YlGnBu\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#A pairplot plot a pairwise relationships in a dataset.\nsns.pairplot(df, size=2.5, hue=\"Species\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Selection","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Univariate Selection\n\nStatistical tests can be used to select those features that have the strongest relationship with the output variable.\nThe scikit-learn library provides the SelectKBest class that can be used with a suite of different statistical tests to select a specific number of features.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.iloc[:,0:4]  #independent columns\nY = df.iloc[:,-1]    #target column i.e Species\nprint(\"Feature Variable X:\",\"\\n\",X,\"\\n\"*2,\"Target Variable Y:\",\"\\n\",Y, )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bestfeatures = SelectKBest(score_func=chi2, k=3)\nfit = bestfeatures.fit(X,Y)\ndfscores = pd.DataFrame(fit.scores_)\ndfcolumns = pd.DataFrame(X.columns)\nfeatureScores = pd.concat([dfcolumns,dfscores],axis=1)\nfeatureScores.columns = ['Specs','Score']\nprint(featureScores)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"featureScores.plot(kind='barh')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Feature Importance\n\nYou can get the feature importance of each feature of your dataset by using the feature importance property of the model.\nFeature importance gives you a score for each feature of your data, the higher the score more important or relevant is the feature towards your output variable.\nFeature importance is an inbuilt class that comes with Tree Based Classifiers.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model = ExtraTreesClassifier()\nmodel.fit(X,Y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(model.feature_importances_)\nfeat_importances = pd.Series(model.feature_importances_, index=X.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feat_importances.nlargest(5).plot(kind='barh')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Classification","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Classification can be performed on structured or unstructured data. Classification is a technique where we categorize data into a given number of classes. The main goal of a classification problem is to identify the category/class to which a new data will fall under. In this case, wea re classifying the Data under the Classes i.e., Species.\n\n\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"a confusion matrix will summarize the results of testing the algorithm for further inspection. It Summarises Count values of Predictions in each model.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#Split the Data Train and test","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train, test = train_test_split(df, test_size = 0.3)\nprint(train.shape)\nprint(test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_X=train[['SepalLengthCm','SepalWidthCm','PetalLengthCm','PetalWidthCm']]\ntrain_Y=train['Species']\ntest_X= test[['SepalLengthCm','SepalWidthCm','PetalLengthCm','PetalWidthCm']]\ntest_Y =test['Species']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**#Support Vector Machine (SVM)**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Support vector machine is a representation of the training data as points in space separated into categories by a clear gap that is as wide as possible. New examples are then mapped into that same space and predicted to belong to a category based on which side of the gap they fall.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"SVM = svm.SVC()\nSVM.fit(train_X,train_Y)\nprediction=SVM.predict(test_X)\nprint('The accuracy of the SVM is:',metrics.accuracy_score(prediction,test_Y))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**#Confusion matrix**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_pred=SVM.predict(test_X)\nY_true=test_Y\ncm=confusion_matrix(Y_true,Y_pred)\nf, ax=plt.subplots(figsize=(5,5))\nsns.heatmap(cm,annot=True,linewidths=0.5,linecolor=\"red\",fmt=\".0f\",ax=ax)\nplt.xlabel(\"Y_pred\")\nplt.ylabel(\"Y_true\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**#Logistic Regression**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"In the Logistic Regression algorithm, the probabilities describing the possible outcomes of a single trial are modelled using a logistic function.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"LR = LogisticRegression()\nLR.fit(train_X,train_Y)\nprediction=LR.predict(test_X)\nprint('The accuracy of the Logistic Regression is',metrics.accuracy_score(prediction,test_Y))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**#Confusion matrix**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_pred=LR.predict(test_X)\nY_true=test_Y\ncm=confusion_matrix(Y_true,Y_pred)\nf, ax=plt.subplots(figsize=(5,5))\nsns.heatmap(cm,annot=True,linewidths=0.5,linecolor=\"red\",fmt=\".0f\",ax=ax)\nplt.xlabel(\"Y_pred\")\nplt.ylabel(\"Y_true\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**#Decision Tree Classifier**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Given a data of attributes together with its classes, a decision tree produces a sequence of rules that can be used to classify the data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"DTC=DecisionTreeClassifier()\nDTC.fit(train_X,train_Y)\nprediction=DTC.predict(test_X)\nprint('The accuracy of the Decision Tree is',metrics.accuracy_score(prediction,test_Y))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#Confusion matrix","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_pred=DTC.predict(test_X)\nY_true=test_Y\ncm=confusion_matrix(Y_true,Y_pred)\nf, ax=plt.subplots(figsize=(5,5))\nsns.heatmap(cm,annot=True,linewidths=0.5,linecolor=\"red\",fmt=\".0f\",ax=ax)\nplt.xlabel(\"Y_pred\")\nplt.ylabel(\"Y_true\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**K-Nearest Neighbours**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Neighbours based classification is a type of lazy learning as it does not attempt to construct a general internal model, but simply stores instances of the training data. Classification is computed from a simple majority vote of the k nearest neighbours of each point.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"KNN=KNeighborsClassifier(n_neighbors=3) #this examines 3 neighbours for putting the new data into a class\nKNN.fit(train_X,train_Y)\nprediction=KNN.predict(test_X)\nprint('The accuracy of the KNN is',metrics.accuracy_score(prediction,test_Y))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**#Confusion matrix**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_pred=KNN.predict(test_X)\nY_true=test_Y\ncm=confusion_matrix(Y_true,Y_pred)\nf, ax=plt.subplots(figsize=(5,5))\nsns.heatmap(cm,annot=True,linewidths=0.5,linecolor=\"red\",fmt=\".0f\",ax=ax)\nplt.xlabel(\"Y_pred\")\nplt.ylabel(\"Y_true\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Wrapping Up!","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"For a starter in Data Science, Iris Dataset is a good place to understand Classsification problems. \n\nThe Feature Selection methods that you see i.e, Univariate Selection and Feature Importance are to give you an Idea about selecing best features that fit your model. \n\nI hope the Simple data visualization techinques and Machine Learning Algorithims used above helped you understand the lifecycle of Data Science, you can create with pandas, seaborn, and matplotlib in Python and Sklearn!\n\nI encourage you to run through these examples yourself.\n\nif you liked it pleaase upvote. I will post more kernels for Data Science Beginners.\n","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}