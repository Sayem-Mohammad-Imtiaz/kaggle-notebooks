{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Introduction and Imports\n\n![Credit: Pexels](https://images.pexels.com/photos/3862130/pexels-photo-3862130.jpeg?auto=compress&cs=tinysrgb&dpr=2&h=750&w=1260)\n\nIn this notebook, I will be using only **Machine Learning** methods to get decent prediction scores. There are much better and sophisticated ways (like RNN, GRU, Fine-tuning BERT, etc) but you have seen them on a lot of notebook already.\n\nThe main aim of this notebook is to just show how quickly and easily you can do Text Classification using Basic Machine Learning Methods, rather than spend waiting 1 hour for a model to train!","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<p style=\"color:red\">If you like this notebook, please make sure to give an upvote, it helps a lot and motivates me to make much more good-quality content</p>\n<p style=\"color:blue\">If you don't like my work, please leave a comment on what can I do to make it better!</p>\n<hr>\n<h3 style=\"color:aqua\">Edits:</h3>\n<ul>\n<li style=\"color:green\">All Classifiers now classify for all 3 categories and not just 2. Good Validation Accuracy is maintained.</li>\n</ul>","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport re\nimport random\nimport warnings\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import roc_auc_score, confusion_matrix, plot_confusion_matrix, plot_precision_recall_curve\n\nwarnings.simplefilter(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def plot_metric(clf, testX, testY, name):\n    \"\"\"\n    Small function to plot ROC-AUC values and confusion matrix\n    \"\"\"\n    styles = ['bmh', 'classic', 'fivethirtyeight', 'ggplot']\n\n    plt.style.use(random.choice(styles))\n    plot_confusion_matrix(clf, testX, testY)\n    plt.title(f\"Confusion Matrix [{name}]\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Preprocessing and Some EDA","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Read the data and don't use the low quality edit data","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"data = pd.read_csv(\"../input/60k-stack-overflow-questions-with-quality-rate/data.csv\")\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All the open questions are grouped under a single class (1), while the closed one is grouped under (0)","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"data = data.drop(['Id', 'Tags', 'CreationDate'], axis=1)\ndata['Y'] = data['Y'].map({'LQ_CLOSE':0, 'LQ_EDIT': 1, 'HQ':2})\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"labels = ['Open Questions', 'Low Quality Question - Close', 'Low Quality Question - Edit']\nvalues = [len(data[data['Y'] == 2]), len(data[data['Y'] == 0]), len(data[data['Y'] == 1])]\nplt.style.use('classic')\nplt.figure(figsize=(16, 9))\nplt.pie(x=values, labels=labels, autopct=\"%1.1f%%\")\nplt.title(\"Target Value Distribution\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's join the title and the body of the text data so that we can use both of them in our classification","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"data['text'] = data['Title'] + ' ' + data['Body']\ndata = data.drop(['Title', 'Body'], axis=1)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Clean the data\ndef clean_text(text):\n    text = text.lower()\n    text = re.sub(r'[^(a-zA-Z)\\s]','', text)\n    return text\ndata['text'] = data['text'].apply(clean_text)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Splitting the Data\nLet's now split the dataset into training and validation sets","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"# Define how much percent data you wanna split\nsplit_pcent = 0.20\nsplit = int(split_pcent * len(data))\n\n# Shuffles dataframe\ndata = data.sample(frac=1).reset_index(drop=True)\n\n# Training Sets\ntrain = data[split:]\ntrainX = train['text']\ntrainY = train['Y'].values\n\n# Validation Sets\nvalid = data[:split]\nvalidX = valid['text']\nvalidY = valid['Y'].values\n\nassert trainX.shape == trainY.shape\nassert validX.shape == validY.shape\n\nprint(f\"Training Data Shape: {validX.shape}\\nValidation Data Shape: {validX.shape}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Vectorizing the Data\nLet's vectorize the data so it's in the numerical format","execution_count":null},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"# Load the vectorizer, fit on training set, transform on validation set\nvectorizer = TfidfVectorizer()\ntrainX = vectorizer.fit_transform(trainX)\nvalidX = vectorizer.transform(validX)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modelling\nLet's start with different non-deep learning approaches for this task.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 1. Logistic Regression\nLet's first start with our good old, Logistic Regression!","execution_count":null},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"# Define and fit the classifier on the data\nlr_classifier = LogisticRegression(C=1.)\nlr_classifier.fit(trainX, trainY)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"# Print the accuracy score of the classifier\nprint(f\"Validation Accuracy of Logsitic Regression Classifier is: {(lr_classifier.score(validX, validY))*100:.2f}%\")","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"# Also plot the metric\nplot_metric(lr_classifier, validX, validY, \"Logistic Regression\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Multinomial Naive Bayes\nLet's now switch to the naive the bayes, the NAIVE BAYES!","execution_count":null},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"# Define and fit the classifier on the data\nnb_classifier = MultinomialNB()\nnb_classifier.fit(trainX, trainY)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"# Print the accuracy score of the classifier\nprint(f\"Validation Accuracy of Naive Bayes Classifier is: {(nb_classifier.score(validX, validY))*100:.2f}%\")","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"# Also plot the metric\nplot_metric(nb_classifier, validX, validY, \"Naive Bayes\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. Random Forest Classifier\nLet's now enter the forest with the Random Forest Classifier and see where it takes us!","execution_count":null},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"# Define and fit the classifier on the data\nrf_classifier = RandomForestClassifier()\nrf_classifier.fit(trainX, trainY)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"# Print the accuracy score of the classifier\nprint(f\"Validation Accuracy of Random Forest Classifier is: {(rf_classifier.score(validX, validY))*100:.2f}%\")","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"# Also plot the metric\nplot_metric(nb_classifier, validX, validY, \"Random Forest\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4. Decision Tree Classifier\nLet's now take some decisions using the Decision Tree Classifer","execution_count":null},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"# Define and fit the classifier on the data\ndt_classifier = DecisionTreeClassifier()\ndt_classifier.fit(trainX, trainY)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"# Print the accuracy score of the classifier\nprint(f\"Validation Accuracy of Decision Tree Clf. is: {(dt_classifier.score(validX, validY))*100:.2f}%\")","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"# Also plot the metric\nplot_metric(dt_classifier, validX, validY, \"Decision Tree Classifier\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5. KNN Classifier\nWe now are going to use KNN Classifier for this task.","execution_count":null},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"# Define and fit the classifier on the data\nkn_classifier = KNeighborsClassifier()\nkn_classifier.fit(trainX, trainY)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"# Print the accuracy score of the classifier\nprint(f\"Validation Accuracy of KNN Clf. is: {(kn_classifier.score(validX, validY))*100:.2f}%\")","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"# Also plot the metric\nplot_metric(dt_classifier, validX, validY, \"Decision Tree Classifier\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 6. XGBoost\nFinally, let's use the XGBoost Classifier and then we'll compare all the different classifiers so far","execution_count":null},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"# Define and fit the classifier on the data\nxg_classifier = XGBClassifier()\nxg_classifier.fit(trainX, trainY)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"# Print the accuracy score of the classifier\nprint(f\"Validation Accuracy of XGBoost Clf. is: {(xg_classifier.score(validX, validY))*100:.2f}%\")","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"# Also plot the metric\nplot_metric(xg_classifier, validX, validY, \"XGBoost Classifier\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}