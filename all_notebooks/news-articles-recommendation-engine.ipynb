{"cells":[{"metadata":{},"cell_type":"markdown","source":"# In this notebook i'll try to create a recommendation engine using Kmeans with TF-IDF and networkx (For graphs)"},{"metadata":{},"cell_type":"markdown","source":"**This notebook is inspired from the work of Mr.Yann Claudel from his notebook in Netflix movie , great notebook i recommend highly :**\n[https://www.kaggle.com/yclaudel/recommendation-engine-with-networkx](http://)"},{"metadata":{},"cell_type":"markdown","source":"# **Importation**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport networkx as nx\nimport matplotlib.pyplot as plt\nimport math as math\nimport time ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"../input/internet-articles-data-with-users-engagement/articles_data.csv\")\ndf.drop('Unnamed: 0',axis=1,inplace=True)\nprint(df.shape)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df['source_id'].unique())\nprint(df['source_name'].unique())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see the two columns Source_id and source_name are almost the same , we'll be using only source name and there is a value in source name equals to 460 which is weird let's check the rows containing that value"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df[df['source_name']==\"460.0\"])\n#Since its all full of NAN value we'll drop this useless row\ndf = df[df['source_name']!=\"460.0\"]\nprint(\"Row Dropped\")\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## *In this work we're creating a Recommendation engine , we won't be interested in all columns , like the last ones about facebook sharing and stuff we dont need that , the first thing to do is creating a tf-idf clustering by description*"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have 24 rows without description we'll need to clean that :"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df[df['description'].isna()].isna().sum())\ndf = df[~df['title'].isna()]\ndf_2 = df.copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you can see 18 of them are without contents , these 18 we'll be droping them cause we cant do something to fix them if we don't even know the content of this article"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df_2.copy()\nempty_desc = df[df['description'].isna()]\ndf = df[~df['description'].isna()]\nempty_desc = empty_desc[~empty_desc['content'].isna()]\ndf = pd.concat([df,empty_desc],axis=0)\ndf.isna().sum()\n# print(indexes)\n# print(df.iloc[indexes[-1], : ])\n# df.drop(df.index[[indexes]])\n# print(df[df['description'].isna()].isna().sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## For the last 6 empty descriptions we're gonna use summarize from gensim library to summarize the content and save it as a description"},{"metadata":{"trusted":true},"cell_type":"code","source":"from gensim.summarization.summarizer import summarize\nfrom gensim.summarization import keywords","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"empty_desc = df[df['description'].isna()]\ndf = df[~df['description'].isna()]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"empty_desc['description'] = empty_desc.apply(lambda x:summarize(x['content'],ratio=0.5),axis=1)\n#Grabing back our new descriptions\ndf = pd.concat([df,empty_desc],axis=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Now we have our description values well clained it's time to start our Kmeans using TF IDF"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer #The Vector creator\nfrom sklearn.metrics.pairwise import linear_kernel #Cosine similarity\nfrom sklearn.cluster import MiniBatchKMeans #Kmeans Clustering Batch","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cluster_content = df['description']\nvector = TfidfVectorizer(max_df=0.5,min_df=1,stop_words=\"english\",lowercase=True,use_idf=True,norm=u'l2',smooth_idf=True)\ntfidf = vector.fit_transform(cluster_content)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"k = 200\nkmeans = MiniBatchKMeans(n_clusters = k)\nkmeans.fit(tfidf)\ncenters = kmeans.cluster_centers_.argsort()[:,::-1]\nterms = vector.get_feature_names()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Adding our Data (Descritpions) and predict their classes :"},{"metadata":{"trusted":true},"cell_type":"code","source":"request_transform = vector.transform(df['description'])\ndf['cluster'] = kmeans.predict(request_transform)\ndf['cluster'].value_counts().head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I think i won't be using this clustering column because it's so unbalanced"},{"metadata":{},"cell_type":"markdown","source":"## Now we're Going to use Cosine Similarity to compute the similarity between docs"},{"metadata":{"trusted":true},"cell_type":"code","source":"def find_similar(matrix,index,top_n=5):\n    cosine_similarities = linear_kernel(matrix[index:index+1],matrix).flatten()\n    related_docs_indices = [i for i in cosine_similarities.argsort()[::-1] if i != index]\n    return [index for index in related_docs_indices][0:top_n]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Now let's Create our graph**"},{"metadata":{},"cell_type":"markdown","source":"#### Nodes Are : \n\n* Title \n* Person (Author)\n* Press (Source_name)\n* Cluster ( Description ) \n* Sim\n\n#### Edges are :\n\n* Wrote : relation between title and person\n* CAT : Relation between title and Press\n* Description : Relation between cluster and a movie\n* Similarity in sense of description"},{"metadata":{"trusted":true},"cell_type":"code","source":"G = nx.Graph(label=\"Article\")\nstart_time = time.time()\nfor i,rowi in df.iterrows() :\n    if (i > 3000) :\n        continue\n    if (i%1000 == 0) : \n        print(\"Iter  {} --- {} secondes --\".format(i,time.time()-start_time))\n    G.add_node(rowi['title'],key=i,label=\"Article\")\n    G.add_node(rowi['author'],label=\"Person\")\n    G.add_edge(rowi['title'],rowi['author'],label=\"Wrote\")\n    G.add_node(rowi['source_name'],label=\"Press\")\n    G.add_edge(rowi['title'],rowi['source_name'],label=\"CAT\")\n    #Similarity Node :\n    indices = find_similar(tfidf, i, top_n = 5)\n    snode=\"Sim(\"+rowi['title'][:15].strip()+\")\"        \n    G.add_node(snode,label=\"SIMILAR\")\n    G.add_edge(rowi['title'], snode, label=\"SIMILARITY\")\n    for element in indices:\n        G.add_edge(snode, df['title'].iloc[element], label=\"SIMILARITY\")\nprint(\" finish -- {} seconds --\".format(time.time() - start_time))   ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Function to draw Our graph , no need to understand the details just a general idea , u can copie it and use it on other projects but change the nodes labels"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_all_adj_nodes(list_in):\n    sub_graph=set()\n    for m in list_in:\n        sub_graph.add(m)\n        for e in G.neighbors(m):        \n                sub_graph.add(e)\n    return list(sub_graph)\n\ndef draw_sub_graph(sub_graph):\n    subgraph = G.subgraph(sub_graph)\n    colors=[]\n    for e in subgraph.nodes():\n        if G.nodes[e]['label']==\"Article\":\n            colors.append('blue')\n        elif G.nodes[e]['label']==\"Person\":\n            colors.append('red')\n        elif G.nodes[e]['label']==\"Press\":\n            colors.append('green')\n        elif G.nodes[e]['label']==\"SIMILAR\":\n            colors.append('yellow')\n\n\n    nx.draw(subgraph, with_labels=True, font_weight='bold',node_color=colors)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Checking our graph with two exemples ( Here only two so we can visualize it )"},{"metadata":{"trusted":true},"cell_type":"code","source":"list_in=[df['title'].loc[1],df['title'].loc[2]]\nsub_graph = get_all_adj_nodes(list_in)\ndraw_sub_graph(sub_graph)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## The next function is going to get the neighbors nodes in our graph and compute the weight (like degree of similarity according to the  graph ) then we're going to sort the neighbors by this weight value "},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_recommendation(root):\n    commons_dict = {}\n    for e in G.neighbors(root):\n        for e2 in G.neighbors(e):\n            if e2==root:\n                continue\n            try :\n                if G.nodes[e2]['label']==\"Article\":\n                    commons = commons_dict.get(e2)\n                    if commons==None:\n                        commons_dict.update({e2 : [e]})\n                    else:\n                        commons.append(e)\n                        commons_dict.update({e2 : commons})\n            except :\n                pass\n    articles=[]\n    weight=[]\n    for key, values in commons_dict.items():\n        w=0.0\n        for e in values:\n            w=w+1/math.log(G.degree(e))\n        articles.append(key) \n        weight.append(w)\n    \n    result = pd.Series(data=np.array(weight),index=articles)\n    result.sort_values(inplace=True,ascending=False)        \n    return result;","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result = get_recommendation(df['title'].loc[40])\nprint(\"*\"*40+\"\\n Recommendation for :\"+str(df['title'].loc[40])+\"\\n\"+\"*\"*40)\nprint(result.head())","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}