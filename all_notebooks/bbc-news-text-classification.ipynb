{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Text Classification - BBC News Data\n\n## Overview\n\nThe following notebook is created out of inspiration from the source [here](https://colab.research.google.com/github/srushtidhope/bbc-text-classification/blob/master/bbc_text_classification.ipynb#scrollTo=22L7TrqYtFiz) \n","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n# NLTK modules\nimport nltk\nnltk.download('punkt')\nnltk.download('stopwords')\nnltk.download('wordnet')\n\nimport re\n\nfrom gensim.models import Word2Vec # Word2Vec module\n\nfrom sklearn.model_selection import GridSearchCV, train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.naive_bayes import GaussianNB, MultinomialNB\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Loading Dataset","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0"}},{"cell_type":"code","source":"news_data = pd.read_csv('/kaggle/input/bbc-fulltext-and-category/bbc-text.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"news_data.tail(6)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Shape : {news_data.shape}, \\n\\nColumns: {news_data.columns}, \\n\\nCategories: {news_data.category.unique()}\")\n\n# print sample data\nnews_data.head().append(news_data.tail())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot category data\nplt.figure(figsize=(10,6))\nsns.countplot(news_data.category)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Preparation","metadata":{}},{"cell_type":"code","source":"class DataPreparation:\n    def __init__(self, data, column='text'):\n        self.df = data\n        self.column = column\n    \n    def preprocess(self):\n        self.tokenize()\n        self.remove_stopwords()\n        self.remove_non_words()\n        self.lemmatize_words()\n        \n        return self.df\n    \n    def tokenize(self):\n        self.df['clean_text'] = self.df[self.column].apply(nltk.word_tokenize)\n        print(\"Tokenization is done.\")\n    \n    def remove_stopwords(self):\n        stopword_set = set(nltk.corpus.stopwords.words('english'))\n        \n        rem_stopword = lambda words: [item for item in words if item not in stopword_set]\n        \n        self.df['clean_text'] = self.df['clean_text'].apply(rem_stopword)\n        print(\"Remove stopwords done.\")\n    \n    def remove_non_words(self):\n        \"\"\"\n            Remove all non alpha characters from the text data\n            :numbers: 0-9\n            :punctuation: All english punctuations\n            :special characters: All english special characters\n        \"\"\"\n        regpatrn = '[a-z]+'\n        rem_special_chars = lambda x: [item for item in x if re.match(regpatrn, item)]\n        self.df['clean_text'] = self.df['clean_text'].apply(rem_special_chars)\n        print(\"Removed non english characters is done.\")\n        \n    def lemmatize_words(self):\n        lemma = nltk.stem.wordnet.WordNetLemmatizer()\n        \n        on_word_lemma = lambda x: [lemma.lemmatize(w, pos='v') for w in x]\n        \n        self.df['clean_text'] = self.df['clean_text'].apply(on_word_lemma)\n        print(\"Lemmatization on the words.\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_prep = DataPreparation(news_data)\n\ncleanse_df = data_prep.preprocess()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cleanse_df['clean_text']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature Engineering","metadata":{"trusted":true}},{"cell_type":"code","source":"vec_model = Word2Vec(cleanse_df['clean_text'])\n\nw2v = dict(zip(vec_model.wv.index2word, vec_model.wv.syn0))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Classification with Custom Vectorizer","metadata":{}},{"cell_type":"code","source":"class Vectorizer(object):\n    def __init__(self, vec):\n        self.vec = vec\n        self.dim = len(vec.values())\n        \n    def fit(self, X, y):\n        return self\n    \n    def transform(self, X):\n        return np.array([np.mean([self.vec[w] for w in words if w in self.vec] or [np.zeros(self.dim)], axis=0) for words in X])\n    \n\n    \n\n# Classifier class\nclass Classifier(object):\n    def __init__(self, model, param):\n        self.model = model\n        self.param = param\n        self.gsearch = GridSearchCV(self.model, self.param, cv=5, error_score=0, refit=True)\n        \n    def fit(self, X, y):\n        return self.gsearch.fit(X, y)\n    \n    def predict(self, X):\n        return self.gsearch.predict(X)\n\nclf_models = {\n    'Naive Bayes': GaussianNB(),\n    'SVC': SVC(),\n    'Decision Tree': DecisionTreeClassifier(),\n    'SGD Classifier': SGDClassifier(),\n    'Perceptron': MLPClassifier()\n}\n\nclf_params = {\n    'Naive Bayes': {},\n    'SVC' : {'kernel': ['linear', 'rbf']},\n    'Decision Tree': {'min_samples_split': [2, 5]},\n    'SGD Classifier': { 'penalty': ['l2', 'l1', 'elasticnet'] },\n    'Perceptron': {'activation': ['tanh', 'relu']}\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Split the dataset","metadata":{}},{"cell_type":"code","source":"X_train, X_valid, y_train, y_valid = train_test_split(cleanse_df['clean_text'], cleanse_df['category'], test_size=0.2, shuffle=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Iterate through the model names\nfor key in clf_models.keys():\n    \n    clf = Pipeline([('Word2Vec', Vectorizer(w2v)), ('Classifier', Classifier(clf_models[key], clf_params[key]))])\n    \n    # Fitting the data\n    clf.fit(X_train, y_train)\n    \n    y_preds = clf.predict(X_valid)\n    \n    \n    print(key, \":\")\n    print(\"Accuracy: %1.3f \\tPrecision: %1.3f \\tRecall: %1.3f \\tF1-Score: %1.3f\\n\" % (accuracy_score(y_valid, y_preds),\n                                                                                     precision_score(y_valid, y_preds, average='macro'),\n                                                                                     recall_score(y_valid, y_preds, average='macro'),\n                                                                                     f1_score(y_valid, y_preds, average='macro')))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Vectorization using TFIDF","metadata":{"trusted":true}},{"cell_type":"code","source":"def vectorize(vector, X_train, X_test):\n    vector_fit = vector.fit(X_train)\n    \n    X_train_vec = vector_fit.transform(X_train)\n    X_test_vec = vector_fit.transform(X_test)\n    \n    print(\"Vectorization is completed.\")\n    return X_train_vec, X_test_vec,vector_fit\n\ndef label_encoding(y_train):\n    \"\"\"\n        Encode the given list of class labels\n        :y_train_enc: returns list of encoded classes\n        :labels: actual class labels\n    \"\"\"\n    lbl_enc = LabelEncoder()\n    \n    y_train_enc = lbl_enc.fit_transform(y_train)\n    labels = lbl_enc.classes_\n    \n    return y_train_enc, labels,lbl_enc\n\ndef algorithm_stack(models, params, X_train, X_test, y_train, y_test):\n    \n    if not set(models.keys()).issubset(set(params.keys())):\n        raise ValueError('Keys do not match')\n        \n    for key in models.keys():\n        model = models[key]\n        param = params[key]\n        \n        gs = GridSearchCV(model, param, cv=5, error_score=0, refit=True)\n        gs.fit(X_train, y_train)\n        \n        y_pred = gs.best_estimator_.predict(X_test)\n        \n        print(key, \":\")\n        print(\"Accuracy: %1.3f \\tPrecision: %1.3f \\tRecall: %1.3f \\tF1-Score: %1.3f\\n\" % (accuracy_score(y_test, y_pred),\n                                                                                     precision_score(y_test, y_pred, average='macro'),\n                                                                                     recall_score(y_test, y_pred, average='macro'),\n                                                                                     f1_score(y_test, y_pred, average='macro')))\n    return gs.best_estimator_\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Encode the class labels\ny_enc_train, labels,encoder = label_encoding(news_data['category'])\n\n# Split from the loaded dataset\nX_train, X_valid, y_train, y_test = train_test_split(news_data['text'], y_enc_train, test_size=0.2, shuffle=True)\n\n# TFIDFVectorizer \nX_train_vec, X_valid_vec,vectorizer = vectorize(TfidfVectorizer(), X_train, X_valid)\n\nprint(X_train_vec.shape, X_valid_vec.shape)\n\nclf_models = {\n    'SGD Classifier': SGDClassifier()\n}\n\n\n# Modified parameters\nclf_params = {\n    'SGD Classifier': { 'penalty': ['l2', 'l1', 'elasticnet'] }\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# y_train","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model =algorithm_stack(clf_models, clf_params, X_train_vec, X_valid_vec, y_train, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = model.predict(X_valid_vec)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encoder.inverse_transform(y_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pickle\n\npickle_out = open(\"vectorizer.pickle\",\"wb\")\npickle.dump(vectorizer, pickle_out)\npickle_out = open(\"encoder.pickle\",\"wb\")\npickle.dump(encoder, pickle_out)\npickle_out.close()\nclf_out = open(\"model.pickle\",\"wb\")\npickle.dump(model, clf_out)\npickle_out.close()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# For deployment","metadata":{}},{"cell_type":"code","source":"import pickle\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport warnings\nwarnings.filterwarnings('ignore')\n# NLTK modules\nimport nltk\nnltk.download('punkt')\nnltk.download('stopwords')\nnltk.download('wordnet')\nimport re\nfrom gensim.models import Word2Vec # Word2Vec module\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.linear_model import SGDClassifier\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DataPreparation:\n    def __init__(self, data, column='text'):\n        self.df = data\n        self.column = column\n    \n    def preprocess(self):\n        self.tokenize()\n        self.remove_stopwords()\n        self.remove_non_words()\n        self.lemmatize_words()\n        \n        return self.df\n    \n    def tokenize(self):\n        self.df['clean_text'] = self.df[self.column].apply(nltk.word_tokenize)\n        print(\"Tokenization is done.\")\n    \n    def remove_stopwords(self):\n        stopword_set = set(nltk.corpus.stopwords.words('english'))\n        \n        rem_stopword = lambda words: [item for item in words if item not in stopword_set]\n        \n        self.df['clean_text'] = self.df['clean_text'].apply(rem_stopword)\n        print(\"Remove stopwords done.\")\n    \n    def remove_non_words(self):\n        \"\"\"\n            Remove all non alpha characters from the text data\n            :numbers: 0-9\n            :punctuation: All english punctuations\n            :special characters: All english special characters\n        \"\"\"\n        regpatrn = '[a-z]+'\n        rem_special_chars = lambda x: [item for item in x if re.match(regpatrn, item)]\n        self.df['clean_text'] = self.df['clean_text'].apply(rem_special_chars)\n        print(\"Removed non english characters is done.\")\n        \n    def lemmatize_words(self):\n        lemma = nltk.stem.wordnet.WordNetLemmatizer()\n        \n        on_word_lemma = lambda x: [lemma.lemmatize(w, pos='v') for w in x]\n        \n        self.df['clean_text'] = self.df['clean_text'].apply(on_word_lemma)\n        print(\"Lemmatization on the words.\")\n        \nnews_data= pd.DataFrame([\"kilroy unveils immigration policy ex-chatshow\"],columns=[\"text\"])\ndata_prep = DataPreparation(news_data) #news_data should have the pandas df input text\ncleanse_df = data_prep.preprocess() #cleanse_df will have the cleaned data\n#load vectorizer\nfile = open(\"vectorizer.pickle\",'rb')\nvectorizer = pickle.load(file)\nfile.close()\n#load label encoder\nfile = open(\"encoder.pickle\",'rb')\nencoder = pickle.load(file)\nfile.close()\n#load model\nfile = open(\"model.pickle\",'rb')\nmodel = pickle.load(file)\nfile.close()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ny = model.predict(vectorizer.transform(cleanse_df['text']))\ny = encoder.inverse_transform(y)\ny","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}