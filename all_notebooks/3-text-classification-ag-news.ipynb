{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nimport matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<1> Load train, test dataset","metadata":{}},{"cell_type":"code","source":"test_text = pd.read_csv(\"../input/ag-news-classification-dataset/test.csv\")\ntrain_text = pd.read_csv(\"../input/ag-news-classification-dataset/train.csv\")\n\nprint(f\"shape of train dataset >> {train_text.shape}\")\nprint(f\"shape of test dataset >> {test_text.shape}\")\n\ntest_text.sample(5)\ntrain_text.sample(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<2> split contents and label","metadata":{}},{"cell_type":"code","source":"index_to_label ={\n    1:'World',\n    2:'Sports',\n    3:'Business',\n    4:'Sci/Tech'\n}\n\ndef return_dataset(dataset):\n    label = dataset[\"Class Index\"]\n    label = pd.get_dummies(label)\n    data = dataset[\"Title\"] + \" \" + dataset[\"Description\"]\n    print(f\"shape of data >> {data.shape}\")\n    print(f\"shape of label >> {label.shape}\\n\")\n    return data,label\n\ntrain_data,train_label = return_dataset(train_text)\ntest_data,test_label = return_dataset(test_text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.set_style(\"whitegrid\")\n\nfig = plt.figure(figsize=(8,8))\naxe1 = fig.add_subplot(1,2,1)\nsns.countplot(data=train_label)\naxe1.set_title(\"train dataset\")\naxe1.set_xlabel([index_to_label[i] for i in range(1,5)])\n\naxe2 = fig.add_subplot(1,2,2)\nsns.countplot(data=test_label)\naxe2.set_title(\"test dataset\")\naxe2.set_xlabel([index_to_label[i] for i in range(1,5)])\n\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<3> Preprocess text data (tokenize then apply padding) ","metadata":{}},{"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\n\ntok = Tokenizer()\ntok.fit_on_texts(train_data)\nprint(f\"numbers of words used >> {len(tok.word_index)}\")\n\nword_size = 999\nvocab_size = word_size+1 #1000\n\ntok = Tokenizer(num_words=word_size)\ntok.fit_on_texts(train_data)\n\nword_index = tok.word_index\nindex_word = tok.index_word\n\ntrain_data = tok.texts_to_sequences(train_data)\ntest_data = tok.texts_to_sequences(test_data)\n\nprint(\"First two samples\")\nprint(train_data[0])\nprint(train_data[1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.preprocessing.sequence import pad_sequences\n\nprint(f\"maximum >> {np.max([len(s)for s in train_data])}\")\nprint(f\"minimum >> {np.min([len(s)for s in train_data])}\")\nprint(f\"average >> {np.mean([len(s)for s in train_data])}\")\nprint(f\"median >> {np.median([len(s)for s in train_data])}\\n\")\n\nlens = [len(s) for s in train_data]\nplt.hist(lens,bins=50)\nplt.show()\n\nsequence_len = 50\n\ntrain_data = pad_sequences(train_data,maxlen=sequence_len,padding='post',truncating='post')\ntest_data = pad_sequences(test_data,maxlen=sequence_len,padding='post',truncating='post')\n\nprint(\"print first two samples\")\nprint(train_data[0])\nprint(train_data[1])\n\nprint(\"\\ntrain data shape >>\",train_data.shape)\nprint(\"test data shape >>\",test_data.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<4> Make models and train and test","metadata":{}},{"cell_type":"markdown","source":"<4-1> Model without RNNs","metadata":{}},{"cell_type":"code","source":"from keras.layers import Input,Embedding,GlobalAveragePooling1D,Dense,LSTM,Bidirectional,TimeDistributed\n\ndef create_simple_model(word_vec_size=64):\n    X = Input(shape=[sequence_len])\n    H = Embedding(vocab_size,word_vec_size,input_length=sequence_len)(X)\n    H = GlobalAveragePooling1D()(H)\n    Y = Dense(4,activation='softmax')(H)\n    \n    model = keras.models.Model(X,Y)\n    model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n    \n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.utils import plot_model\nfrom keras.callbacks import ReduceLROnPlateau\n\nreduceLR = ReduceLROnPlateau(monitor='val_loss',patience=3,factor=0.5,min_lr=0.0001,verbose=1)\n\ndef fit_test(model,n):\n    hist = model.fit(train_data,train_label,batch_size=64,validation_split=0.2,epochs=n,verbose=0,callbacks=[reduceLR])\n    result = model.evaluate(test_data,test_label)\n    \n    return hist,result","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"simple1 = create_simple_model(64)\nfit_test(simple1,7)\nplot_model(simple1)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"simple2 = create_simple_model(128)\nfit_test(simple2,7)\nplot_model(simple2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<4-2> Model with LSTM(RNN) : bidirectional,many-to-one,stacked","metadata":{}},{"cell_type":"code","source":"from keras.layers import Dropout\n\ndef create_LSTM(word_vec_size=64,hidden_size=64):\n    X = Input(shape=[sequence_len])\n    H = Embedding(vocab_size,word_vec_size,input_length=sequence_len)(X)\n    H = Bidirectional(LSTM(hidden_size,return_sequences=True))(H)\n    H = Bidirectional(LSTM(int(hidden_size/2),return_sequences=True))(H)\n    H = GlobalAveragePooling1D()(H)\n    \n    H = Dropout(0.2)(H)\n    H = Dense(1024)(H)\n    H = Dropout(0.2)(H)\n    H = Dense(256)(H)\n    H = Dropout(0.2)(H)\n    H = Dense(32)(H)\n    H = Dropout(0.1)(H)\n    \n    Y = Dense(4,activation='softmax')(H)\n    # 선택지 2개 : (1)GlobalAveragePooling으로 timestep slice들 하나로 모아주거나\n    #             (2)이전 LSTM에서 return_sequences=False 하거나\n    \n    model = keras.models.Model(X,Y)\n    model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n    \n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lstm1 = create_LSTM(256,256)\nhist = lstm1.fit(train_data,train_label,batch_size=256,validation_split=0.2,epochs=10,verbose=1,callbacks=[reduceLR])\nev = lstm1.evaluate(test_data,test_label)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<4-3> Naive Bayes Classifiers","metadata":{}},{"cell_type":"code","source":"# from sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer\n\n# X_train,y_train = return_dataset(train_text)\n# X_test,y_test = return_dataset(test_text)\n\n# print(f\"Train dataset shape before preprocessing >> {train_data.shape}\")\n# print(X_train[22])\n\n# print(\"\\n\\nPreprocessing!!\\n\")\n# vectorizer = CountVectorizer()\n# transformer = TfidfTransformer()\n\n# train_data_dtm = vectorizer.fit_transform(X_train)\n# train_data_tfidf = transformer.fit_transform(train_data_dtm)\n# train_data_tfidf = train_data_tfidf.toarray()\n\n# def preprocess(data):\n#     result = vectorizer.transform(data)\n#     result = transformer.transform(result)\n#     result = result.toarray()\n#     return result\n# print(\"Preprocessing done!\\n\\n\")\n\n# test_data_tfidf = preprocess(X_test)\n\n# print(f\"Train dataset shape after preprocessing >> {train_data_tfidf.shape}\")\n# print(train_data_tfidf[22])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# y_train = train_text['Class Index']\n# y_test = test_text['Class Index']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # memory 용량 때문에 실행 안됨\n# from sklearn.model_selection import train_test_split\n\n# train_data_tfidf,_,y_train,_ = train_test_split(train_data_tfidf,y_train,test_size=0.8,stratify=y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from sklearn.naive_bayes import GaussianNB,BernoulliNB,MultinomialNB\n# from sklearn.metrics import accuracy_score\n\n# gaussian = GaussianNB()\n# bernoulli = BernoulliNB()\n# multinomial = MultinomialNB()\n\n# def test(model):\n#     print(model)\n#     model.fit(train_data_tfidf,y_train)\n    \n#     train_pred = model.predict(train_data_tfidf)\n#     train_acc = accuracy_score(y_train,train_pred)\n#     print(\"train accuracy >>\", train_acc)\n    \n#     test_pred = model.predict(test_data_tfidf)\n#     test_acc = accuracy_score(y_test,test_pred)\n#     print(\"test accuracy >>\", test_acc)\n#     print(\"\\n\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test(gaussian)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"(4-4) LSTM (last hidden cell +(concatenate) average of all hidden cells)\n<4-2 acc 87% --> let's compare which is better>","metadata":{}},{"cell_type":"code","source":"from keras.layers import Concatenate,Dropout\n\ndef create_new_lstm(word_vec_size=64,hidden_size=64):\n    X = Input(shape=[sequence_len])\n    H = Embedding(vocab_size,word_vec_size,input_length=sequence_len,mask_zero=True)(X)\n    \n    H = Bidirectional(LSTM(hidden_size,return_sequences=True))(H)\n    all_hidden = Bidirectional(LSTM(int(hidden_size/2),return_sequences=True))(H)\n    average = GlobalAveragePooling1D()(all_hidden)\n    last_hidden = all_hidden[:,-1,:]\n    \n    H = Concatenate()([average,last_hidden])\n    \n    H = Dropout(0.2)(H)\n    H = Dense(1024)(H)\n    H = Dropout(0.2)(H)\n    H = Dense(256)(H)\n    H = Dropout(0.2)(H)\n    H = Dense(32)(H)\n    H = Dropout(0.1)(H)\n    \n    Y = Dense(4,activation='softmax')(H)\n    # 선택지 2개 : (1)GlobalAveragePooling으로 timestep slice들 하나로 모아주거나\n    #             (2)이전 LSTM에서 return_sequences=False 하거나\n    \n    model = keras.models.Model(X,Y)\n    model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n    \n    return model\n\n\nlstm2 = create_new_lstm(256,256)\nhist = lstm2.fit(train_data,train_label,batch_size=256,validation_split=0.2,epochs=10,verbose=1,callbacks=[reduceLR])\nev = lstm2.evaluate(test_data,test_label)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"(4-5) LSTM (last hidden cell +(concatenate) maximum of all hidden cells)\n<4-2 acc 87% --> let's compare which is better>","metadata":{}},{"cell_type":"code","source":"from keras.layers import GlobalMaxPooling1D\n\ndef create_new_lstm(word_vec_size=64,hidden_size=64):\n    X = Input(shape=[sequence_len])\n    H = Embedding(vocab_size,word_vec_size,input_length=sequence_len,mask_zero=True)(X)\n    \n    H = Bidirectional(LSTM(hidden_size,return_sequences=True))(H)\n    all_hidden = Bidirectional(LSTM(int(hidden_size/2),return_sequences=True))(H)\n    average = GlobalMaxPooling1D()(all_hidden)\n    last_hidden = all_hidden[:,-1,:]\n    \n    H = Concatenate()([average,last_hidden])\n    \n    H = Dropout(0.2)(H)\n    H = Dense(1024)(H)\n    H = Dropout(0.2)(H)\n    H = Dense(256)(H)\n    H = Dropout(0.2)(H)\n    H = Dense(32)(H)\n    H = Dropout(0.1)(H)\n    \n    Y = Dense(4,activation='softmax')(H)\n    # 선택지 2개 : (1)GlobalAveragePooling으로 timestep slice들 하나로 모아주거나\n    #             (2)이전 LSTM에서 return_sequences=False 하거나\n    \n    model = keras.models.Model(X,Y)\n    model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n    \n    return model\n\n\nlstm3 = create_new_lstm(256,256)\nhist = lstm3.fit(train_data,train_label,batch_size=256,validation_split=0.2,epochs=10,verbose=1,callbacks=[reduceLR])\nev = lstm3.evaluate(test_data,test_label)","metadata":{},"execution_count":null,"outputs":[]}]}