{"cells":[{"metadata":{},"cell_type":"markdown","source":"Hello there. Welcome to my first notebook. Today we will look at The Estonia Disaster Passenger List dataset.So first thing first we will import some of the libraries which we will need. And lets have a look at our dataset."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"dataset = pd.read_csv('../input/passenger-list-for-the-estonia-ferry-disaster/estonia-passenger-list.csv')\nprint(len(dataset.index))\ndataset.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see we have 8 columns and 989 rows of data in this dataset. My next step will be checking the empty values in dataset. "},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.isna().values.any()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see from code we have no NaN values so therefore we do not need to handle them. Lets see some graphics representations of the Country, Sex, Age, Category, Survived columns. "},{"metadata":{},"cell_type":"markdown","source":"Lets start with the Country column."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"print(dataset['Country'].value_counts())\nplt.bar(dataset['Country'].value_counts().index,dataset['Country'].value_counts().values)\nplt.xticks(rotation=60)\nplt.xlabel('Country')\nplt.ylabel('Number of passengers')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Data from Country column show us that most people on the ship was from Estonia and Sweden what is expected as we know that the ship went from Estonia to Stockholm. Lets continue with Sex column."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"print(\"The number of people on the boat divided by sex:\" )\nprint(dataset['Sex'].value_counts())\n\nfig1, ax1 = plt.subplots()\nax1.pie(dataset['Sex'].value_counts(), explode=(0.1, 0), labels=['Men', 'Woman'], autopct='%1.1f%%',\n        shadow=True, startangle=90)\nax1.axis('equal')\nplt.title('Sex')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So the total count is 503 men and 486 women on the ship. Now lets see about the age range on the boat. "},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"print(\"Age range:\" )\nprint(dataset['Age'].value_counts())\nk=dataset['Age'].value_counts()\nplt.bar(k.index,k.values)\nplt.xlabel('Age')\nplt.ylabel('Number of occurences')\nplt.title('Age range on the boat')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we see from the graph the age range is pretty much evenly distributed."},{"metadata":{"trusted":true,"_kg_hide-output":false,"_kg_hide-input":true},"cell_type":"code","source":"print(\"The number of people on the boat divided by the category atribute:\" )\nprint(dataset['Category'].value_counts())\n\nfig1, ax1 = plt.subplots()\nax1.pie(dataset['Category'].value_counts(), explode=(0.1, 0), labels=['Passenger', 'Crew'], autopct='%1.1f%%',\n        shadow=True, startangle=90,colors=['grey','Yellow'])\nax1.axis('equal')\nplt.title('Category')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"Here we can see that one fifth of the all people on the boat was crew.Now lets have a look on our attribute which we will want to predict- Survived."},{"metadata":{"trusted":true,"_kg_hide-output":false,"_kg_hide-input":true},"cell_type":"code","source":"print(dataset['Survived'].value_counts())\nfig1, ax1 = plt.subplots()\nax1.pie(dataset['Survived'].value_counts(), explode=(0.1, 0), labels=['Dead', 'Alive'], autopct='%1.1f%%',\n        shadow=True, startangle=90,colors=['orange','Grey'])\n\nax1.axis('equal')\nplt.title('Survived')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So we have basic info about the dataset. Now lets prepare this for some multilinear regression. From the dataset we already know that name of the passenger would not play any significat role for prediction because it is unique for every member as well as Passenger Id so we can get rid of those 3 columns."},{"metadata":{"trusted":true,"_kg_hide-output":false,"_kg_hide-input":true},"cell_type":"code","source":"dataset=dataset.drop(['Firstname','Lastname','PassengerId'],axis=1)\ndataset.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next thing we need to do to put our dataset into multilinear regression model is to change the categorical data to the numerical representation. So bassicaly we will create the new columns which will represent the categorical values with either 0/1. But we have over 15 countries in the dataset which bassicly means that we will have to create another 15+ new columns even if there was only 1 person from other countries. We decided to separate the country column into only 3 columns with either person being from Sweden or Estonia or from some other country. "},{"metadata":{"trusted":true,"_kg_hide-output":false,"_kg_hide-input":true},"cell_type":"code","source":"dataset[\"Country_Sweden\"] = np.where(dataset[\"Country\"]==\"Sweden\",1, 0)\ndataset[\"Country_Estonia\"] = np.where(dataset[\"Country\"]==\"Estonia\", 1 ,0)\ndataset[\"Other_Country\"] = np.where((dataset[\"Country\"]!=\"Sweden\") & (dataset[\"Country\"]!=\"Estonia\")  , 1, 0)\ndataset=dataset.drop(\"Country\",axis=1)\n\ndataset.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see our Country column get replaced by 3 other columns with every country in dataset. Lets do the same for the Sex and Category columns as well. After we will create the 2 other columns for Sex and Category column we will drop first of them because we already know that they have correlation between them and therefore if in one of them is 0 we for sure know that the other will be 1 and vice versa. "},{"metadata":{"trusted":true,"_kg_hide-output":false,"_kg_hide-input":true},"cell_type":"code","source":"dataset=pd.get_dummies(dataset, columns=[\"Category\"],drop_first=True)\ndataset=pd.get_dummies(dataset, columns=[\"Sex\"],drop_first=True)\n\ndataset.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So right now we have 7 columns in total. What we are going to do is to split Age into 2categories defined by the mean value."},{"metadata":{"trusted":true,"_kg_hide-output":false,"_kg_hide-input":true},"cell_type":"code","source":"print(dataset['Age'].describe()[['mean']])\n\ndataset[\"Age_under44\"] = np.where(dataset[\"Age\"]<45, 1, 0)\ndataset[\"Age_over44\"] = np.where(dataset[\"Age\"]>=45, 1, 0)\ndataset=dataset.drop(\"Age\",axis=1)\ndataset.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"OK se we have our data sorted and now we can split the into Train and Test datasets. We choose to have the 1:4 ratio of Test to Train data. "},{"metadata":{"trusted":true,"_kg_hide-output":false,"_kg_hide-input":true},"cell_type":"code","source":"y = dataset.iloc[:, 0].values\nX = dataset.iloc[:, 1:].values\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, stratify=y,random_state=6)\n\n\nfrom sklearn.linear_model import LinearRegression\nregressor = LinearRegression()\nregressor.fit(X_train, y_train)\n\n# Predicting the Test set results\ny_pred = regressor.predict(X_test)\nscore = regressor.score(X_test,y_test)\nprint(\"Multilinear regression score is: %.2f%% \" % (score* 100.00))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see on the code above we are getting only 0.14 score(only with random state other outcomes was around 0.10) on this dataset learned by multilinear regression which is pretty bad. Next we will try the Decision tree classifier. "},{"metadata":{},"cell_type":"markdown","source":"So lets have a look at Decision tree classifier. We will be using the Scikit library again. This library is using the \"CART\" trees. So lets have a look at what kind of score we can get with this. (Unfortunatelly at version 0.23.2 scikit does not support the categorical attributes so we will stick with numercial)."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from sklearn import tree\nclf = tree.DecisionTreeClassifier()\nfig, ax = plt.subplots(figsize=(24, 12))\ntree.plot_tree(clf.fit(X_train, y_train), max_depth=4, fontsize=10)\nplt.show()\n\nprint(\"Mean average accuracy is: %.2f%% \"% (clf.score(X_test,y_test)*100.0))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So we got our score to be 0.85.Score is returning the mean accuracy on the given test data and labels. Here we can see the much better score when we compare it with the multilinear regression."},{"metadata":{},"cell_type":"markdown","source":"Now lets compare this Decision trees from scikit with the XgBoost trees. "},{"metadata":{"_kg_hide-output":true,"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import xgboost as xgb\n\ndtrain = xgb.DMatrix(X_train,y_train)\ndtest = xgb.DMatrix(X_test,y_test)\nparam = {'max_depth': 6, 'eta': 0.3, 'objective': 'binary:logistic'}\nparam['eval_metric'] = ['auc', 'rmse','map']\n\nevallist = [(dtest, 'eval'), (dtrain, 'train')]\nnum_round = 100\nevals_result ={}\nbst = xgb.train(param, dtrain, num_round, evallist, evals_result=evals_result)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\ny_pred = bst.predict(dtest)\npredictions = [round(value) for value in y_pred]\naccuracy = accuracy_score(y_test, predictions)\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\nprint(\"Area under curve score is:\"+\"{}\".format(evals_result['eval']['auc'][-1]*100.0)+\"%\")\nprint(\"Mean average precision is:\"+\"{}\".format(evals_result['eval']['map'][-1]*100.0)+\"%\")\nypred = bst.predict(dtest, ntree_limit=bst.best_ntree_limit)\nxgb.plot_importance(bst)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see we are getting 0.807 AUC metric and 0.33 mean average precisionbut same as in the decission trees we are getting 85.35% accuracy. We can see that this tree had picked feature number four as the most important for the model. "},{"metadata":{},"cell_type":"markdown","source":"Lets now use some simple neural network with few layers and compare the accuracy. We will use 5 layers with \"rectified linear unit\" activation function and last one with \"Sigmoid\" activation function. We will train it on 50 epochs with batch_size = 5 just to demonstrate the outcome."},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"from tensorflow import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nimport tensorflow as tf\n\nmodel = Sequential()\nmodel.add(Dense(24, input_dim=7, activation='relu'))\nmodel.add(Dense(18, activation='relu'))\nmodel.add(Dense(12, activation='relu'))\nmodel.add(Dense(6, activation='relu'))\nmodel.add(Dense(4, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.fit(X_train, y_train, epochs=50, batch_size=3)\n\n","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"score = model.evaluate(X_test, y_test,verbose=0)\n\nprint('Test loss: %.2f%%'% (score[0]*100)) \nprint('Test accuracy: %.2f%%'% (score[1]*100))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So as we can see we are getting 86.36% accuracy which is only slightly better than from the Xgboost method we used. "},{"metadata":{},"cell_type":"markdown","source":"It all seems nice and getting 86% accuracy is really good, BUT on how much is this model really good at predicting? Lets have a look at the confusion matrix with test dataset."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"clas = ['0','1']\nimport sklearn\nfrom sklearn.metrics import classification_report, confusion_matrix\npredictions = model.predict(X_test)\nrounded_predictions = np.argmax(predictions, axis=-1)\ncm = confusion_matrix(y_test, rounded_predictions)\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import ConfusionMatrixDisplay\n\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm,\n                              display_labels=clas)\ndisp = disp.plot(cmap='Greens')\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So here we can see that all the lines of our test dataset was classified as 0. It has high accuracy because of the inconsistent dataset and therefore we have too many '0' and just a few '1'. We might try oversampling to break down the difference from this dataset but we just wanted to show what we can get from this dataset with usage of simple machine learning methods and neural network. Thanks for reading, all comments are welcomed."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}