{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Power Quality Classification using Muti Layer Perceptron (Dataset 2)","metadata":{}},{"cell_type":"markdown","source":"This notebook focusses on developing a Multi Layer perceptron which classifies a particular power signal into its respective power quality condition. The dataset used here contains signals which belong to one of the 6 classes(power quality condition). Each signal is characterized by 256 data points. Here the signals provided are in time domain.","metadata":{}},{"cell_type":"code","source":"#importing the required libraries\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\nimport datetime\nfrom scipy.fft import fft,fftfreq\nfrom sklearn.preprocessing import StandardScaler\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Activation\nfrom tensorflow.keras.optimizers import Adam","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#loading the dataset using pandas\nx_train = pd.read_csv(\"../input/power-quality-distribution-dataset-2/VoltageL1Train.csv\")\ny_train = pd.read_csv(\"../input/power-quality-distribution-dataset-2/outputTrain.csv\")\nx_test = pd.read_csv(\"../input/power-quality-distribution-dataset-2/VoltageL1Test.csv\")\ny_test = pd.read_csv(\"../input/power-quality-distribution-dataset-2/outputTest.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"x_train\",x_train.shape)\nprint(\"y_train\",y_train.shape)\nprint(\"x_test\",x_test.shape)\nprint(\"y_test\",y_test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Preprocessing","metadata":{}},{"cell_type":"markdown","source":"This segment of notebook contains all the preprocessing steps which are performed on the data.","metadata":{}},{"cell_type":"markdown","source":"### Data cleaning","metadata":{}},{"cell_type":"code","source":"#dropna() function is used to remove all those rows which contains NA values\nx_train.dropna(axis=0,inplace=True)\ny_train.dropna(axis=0,inplace=True)\nx_test.dropna(axis=0,inplace=True)\ny_test.dropna(axis=0,inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#shape of the data frame after dropping the rows containing NA values\nprint(\"x_train\",x_train.shape)\nprint(\"y_train\",y_train.shape)\nprint(\"x_test\",x_test.shape)\nprint(\"y_test\",y_test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#here we are constructing the array which will finally contain the column names\nheader =[]\nfor i in range(1,x_train.shape[1]+1):\n    header.append(\"Col\"+str(i))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#assigning the column name array to the respectinve dataframes\nx_train.columns = header\nx_test.columns = header","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#assigning the column name array to the respectinve dataframes\nheader = [\"output\"]\ny_train.columns = header\ny_test.columns = header","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#here we are splitting the training set in the ratio of 70%,30% (training set,validation set)\nfrom sklearn.model_selection import train_test_split\nx_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.30, random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get_dummies function is used here to perform one hot encoding of the y_* numpy arrays\ny_train_hot = pd.get_dummies(y_train['output'])\ny_test_hot = pd.get_dummies(y_test['output'])\ny_val_hot = pd.get_dummies(y_val['output'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train_hot.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data transformation","metadata":{}},{"cell_type":"markdown","source":"The data transformation steps employed here are as follows:<br>\n\n1) Fourier Transform<br>\n2) Normalization","metadata":{}},{"cell_type":"code","source":"x_train = x_train.to_numpy()\nx_test = x_test.to_numpy()\nx_val = x_val.to_numpy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#here we are overwritting the dataframe with the respective waves which we obtained after doing fourier \n#transformation\nfor i in range(0,x_train.shape[0]):\n    x_train[i][:] = np.abs(fft(x_train[i][:]))\n    \nfor i in range(0,x_test.shape[0]):\n    x_test[i][:] = np.abs(fft(x_test[i][:]))\n\nfor i in range(0,x_val.shape[0]):\n    x_val[i][:] = np.abs(fft(x_val[i][:]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#here we are performing normalization\ntransform = StandardScaler()\nx_train_tr = transform.fit_transform(x_train)\nx_test_tr = transform.fit_transform(x_test)\nx_val_tr = transform.fit_transform(x_val)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train_tr = np.log(x_train)\nx_test_tr = np.log(x_test)\nx_val_tr = np.log(x_val)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#final dimensions of the data\nprint(\"Training\",x_train_tr.shape)\nprint(y_train_hot.shape)\nprint(\"Validation\",x_val_tr.shape)\nprint(y_val_hot.shape)\nprint(\"Test\",x_test_tr.shape)\nprint(y_test_hot.shape)\nsampling_rate = x_train_tr.shape[1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model creation and training","metadata":{}},{"cell_type":"code","source":"def model_training(no_of_classes,sampling_rate):\n    model = Sequential()\n\n    model.add(Dense(64, input_shape=(sampling_rate,), activation = 'relu'))\n    model.add(Dense(32, activation = 'relu'))\n    #model.add(Dropout(0.6))\n    model.add(Dense(16, activation = 'relu'))\n    #model.add(Dropout(0.6))\n    model.add(Dense(no_of_classes, activation = 'softmax'))\n\n    model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n    return(model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nlog_dir = \"logs2/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\ntensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n\nmodel = model_training(6,sampling_rate)\nhistory = model.fit(x_train_tr, y_train_hot, batch_size=64, epochs=30, validation_data=(x_val_tr, y_val_hot), callbacks=[tensorboard_callback])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%load_ext tensorboard\n%tensorboard --logdir logs2/fit","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model evaluation","metadata":{}},{"cell_type":"code","source":"print(\"min val:\",min(history.history['val_accuracy']))\nprint(\"avg val\",np.mean(history.history['val_accuracy']) )\nprint(\"max val:\",max(history.history['val_accuracy']))\nprint()\nprint(\"min train:\",min(history.history['accuracy']))\nprint(\"avg train\",np.mean(history.history['accuracy']) )\nprint(\"max train:\",max(history.history['accuracy']))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_acc = model.evaluate(x_test_tr,y_test_hot)\nprint(\"Test accuracy is {}\".format(pred_acc))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nimport seaborn as sn","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"array = confusion_matrix(y_test_hot.to_numpy().argmax(axis=1), model.predict(x_test_tr).argmax(axis=1))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"array","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"to_cm = pd.DataFrame(array, index = [i for i in [\"Type-1\",\"Type-2\",\"Type-3\",\"Type-4\",\"Type-5\",\"Type-6\"]],\n                  columns = [i for i in [\"Type-1\",\"Type-2\",\"Type-3\",\"Type-4\",\"Type-5\",\"Type-6\"]])\nplt.figure(figsize = (13,9))\nsn.heatmap(to_cm, annot=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}