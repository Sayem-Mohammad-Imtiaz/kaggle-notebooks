{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Importing the libraries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\nfrom sklearn.pipeline import Pipeline\n\n#to data preprocessing\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\n\n#NLP tools\nimport re\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n#train split and fit models\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom nltk.tokenize import TweetTokenizer\n\n#model selection\nfrom sklearn.metrics import confusion_matrix, accuracy_score, plot_confusion_matrix, classification_report\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Importing the dataset"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"dataset = pd.read_csv('../input/hate-speech-and-offensive-language-dataset/labeled_data.csv')\ndataset.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.describe().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dt_transformed = dataset[['class', 'tweet']]\ny = (dt_transformed.iloc[:, :-1].values).ravel()\ndt_transformed","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dividindo o df em treino e teste\ndf_train, df_test = train_test_split(dt_transformed, test_size = 0.10, random_state = 42, stratify=dt_transformed['class'])\ndf_train.shape, df_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['class'].value_counts().plot(kind='bar')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"0 - Discurso de ódio\n\n1 - Linguagem ofensiva\n\n2 - nenhum dos dois"},{"metadata":{},"cell_type":"markdown","source":"## Cleaning the texts"},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocessing(data):\n    stemmer = nltk.stem.RSLPStemmer()\n    all_stopwords = stopwords.words('english')\n    all_stopwords.remove('not')\n    corpus = []\n    for tweet in data:\n      review = re.sub(r\"@[A-Za-z0-9_]+\", \" \", tweet)\n      review = re.sub('RT', ' ', review)\n      review = re.sub(r\"https?://[A-Za-z0-9./]+\", \" \", review)\n      review = re.sub(r\"https?\", \" \", review)\n      review = re.sub('[^a-zA-Z]', ' ', review)\n      review = review.lower()\n      review = review.split()\n      ps = PorterStemmer()\n      review = [ps.stem(word) for word in review if not word in set(all_stopwords) if len(word) > 2]\n      review = ' '.join(review)\n      corpus.append(review)\n\n    return np.array(corpus)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus = preprocessing(df_train['tweet'].values)\ncorpus.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# treino e validação do corpus\nc_train, c_vad, y_train, y_vad = train_test_split(corpus, df_train['class'], test_size = 0.10, random_state = 42, stratify=df_train['class'])\nc_train.shape, c_vad.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Extraindo as features utilizando tokenização"},{"metadata":{"trusted":true},"cell_type":"code","source":"def tokenize(c_train, c_vad):\n    tweet_tokenizer = TweetTokenizer() \n    vectorizer = CountVectorizer(analyzer=\"word\", tokenizer=tweet_tokenizer.tokenize, max_features = 1010)\n    X_train = vectorizer.fit_transform(c_train).toarray()\n    X_vad = vectorizer.transform(c_vad).toarray()\n    return X_train, X_vad","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_vad = tokenize(c_train, c_vad)\nX_train.shape, X_vad.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Treinando Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Logistic Regression\nmodel = LogisticRegression(multi_class='ovr', solver='liblinear', random_state = 0)\nmodel.fit(X_train, y_train.ravel())\ny_pred = model.predict(X_vad)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Making the Confusion Matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"def set_confusion_matrix(clf, X, y, title):\n    plot_confusion_matrix(clf, X, y)\n    plt.title(title)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"set_confusion_matrix(model, X_vad, y_vad, type(model).__name__)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target_names = ['class 0', 'class 1', 'class 2']\nprint(classification_report(y_vad, y_pred, target_names=target_names))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Analisando melhor as palavras de cada classe"},{"metadata":{"trusted":true},"cell_type":"code","source":"conjunto = c_train\nhate_tweets = [sentence for sentence, label in zip(conjunto, y) if label == 0]\noff_tweets = [sentence for sentence, label in zip(conjunto, y) if label == 1]\nnone_tweets = [sentence for sentence, label in zip(conjunto, y) if label == 2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hate_words = ' '.join(hate_tweets)\noff_words = ' '.join(off_tweets)\nnone_words = ' '.join(none_tweets)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_wordcloud(text):\n    # Create and generate a word cloud image:\n    wordcloud = WordCloud().generate(text)\n\n    # Display the generated image:\n    plt.imshow(wordcloud, interpolation='bilinear')\n    plt.axis(\"off\")\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_wordcloud(hate_words)\nget_wordcloud(off_words)\nget_wordcloud(none_words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def wordListToFreqDict(wordlist):\n    wordfreq = [(wordlist.count(p))/len(wordlist) for p in wordlist]\n    return dict(list(zip(wordlist,wordfreq)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def sortFreqDict(freqdict):\n    aux = [(freqdict[key], key) for key in freqdict]\n    aux.sort()\n    aux.reverse()\n    return aux","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hate_dict = sortFreqDict(wordListToFreqDict(hate_words.split()))\noff_dict = sortFreqDict(wordListToFreqDict(off_words.split()))\nnone_dict = sortFreqDict(wordListToFreqDict(none_words.split()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(hate_dict), len(off_dict), len(none_dict)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Pegando as palavras que mais aparecem em cada classe"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_common(wordlist, n):\n    return ([w[1] for w in wordlist])[:n]\n\ncommon_words = list()\ncommon_words.append(get_common(hate_dict, 2000))\ncommon_words.append(get_common(off_dict, 1000))\ncommon_words.append(get_common(none_dict, 1000))\ncommon_words = np.unique(np.hstack(common_words))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"common_words_dict = ({i:j for i, j in zip(common_words, range(len(common_words)))})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vectorizer = CountVectorizer(analyzer=\"word\", tokenizer=tweet_tokenizer.tokenize, vocabulary=common_words_dict)\nX_train = vectorizer.fit_transform(c_train).toarray()\nX_vad = vectorizer.transform(c_vad).toarray()\nX_train.shape, X_vad.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Logistic Regression\nmodel = LogisticRegression(multi_class='ovr', solver='liblinear', random_state = 0)\nmodel.fit(X_train, y_train.ravel())\ny_pred = model.predict(X_vad)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"set_confusion_matrix(model, X_vad, y_vad, type(model).__name__)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target_names = ['class 0', 'class 1', 'class 2']\nprint(classification_report(y_vad, y_pred, target_names=target_names))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ### UNDERSAMPLING\n\nVemos que os algoritmos ainda continuam confundindo bastante hate speech (0) com offensive language(1). Vamos tentar melhorar o problema de balanceamento desses dados."},{"metadata":{"trusted":true},"cell_type":"code","source":"n_off, n_none, n_hate = df_train['class'].value_counts()\nn_hate, n_off, n_none ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_hate = df_train[df_train['class'] == 0]\ndf_off = df_train[df_train['class'] == 1]\ndf_none = df_train[df_train['class'] == 2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_off_under = df_off.sample(n_hate, random_state=0)\ndf_none_under = df_none.sample(n_hate, random_state=0)\n\ndf_under = pd.concat([df_hate, df_off_under, df_none_under], axis=0)\nprint(df_under['class'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Treinando agora os modelos com estes dados:"},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus_under = preprocessing(df_under['tweet'].values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# treino e validação do corpus\nc_train, c_vad, y_train, y_vad = train_test_split(corpus_under, df_under['class'], test_size = 0.10, random_state = 42, stratify=df_under['class'])\nc_train.shape, c_vad.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_vad = tokenize(c_train, c_vad)\nX_train.shape, X_vad.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Logistic Regression\nmodel = LogisticRegression(multi_class='ovr', solver='liblinear', random_state = 0)\nmodel.fit(X_train, y_train.ravel())\ny_pred = model.predict(X_vad)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"set_confusion_matrix(model, X_vad, y_vad, type(model).__name__)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target_names = ['class 0', 'class 1', 'class 2']\nprint(classification_report(y_vad, y_pred, target_names=target_names))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### OVERSAMPLING"},{"metadata":{"trusted":true},"cell_type":"code","source":"n_off, n_none, n_hate = df_train['class'].value_counts()\nn_hate, n_off, n_none ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_hate_over = df_hate.sample(n_off, replace=True, random_state=0)\ndf_none_over = df_none.sample(n_off, replace=True, random_state=0)\ndf_over = pd.concat([df_off, df_hate_over, df_none_over], axis=0)\n\nprint('Random over-sampling:')\nprint(df_over['class'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus_over = preprocessing(df_over['tweet'].values)\n# treino e validação do corpus\nc_train, c_vad, y_train, y_vad = train_test_split(corpus_over, df_over['class'], test_size = 0.10, random_state = 42, stratify=df_over['class'])\nc_train.shape, c_vad.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_vad = tokenize(c_train, c_vad)\nX_train.shape, X_vad.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Logistic Regression\nmodel = LogisticRegression(multi_class='ovr', solver='liblinear', random_state = 0)\nmodel.fit(X_train, y_train.ravel())\ny_pred = model.predict(X_vad)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"set_confusion_matrix(model, X_vad, y_vad, type(model).__name__)\ntarget_names = ['class 0', 'class 1', 'class 2']\nprint(classification_report(y_vad, y_pred, target_names=target_names))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Aumentar as amostras para que ficassem balanceadas resultou em resultados melhores, mas como os dados estão repetidos, não sei se o modelo generaliza bem o problema. O ideal seria aumentar as amostras com certa variância entre elas."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}