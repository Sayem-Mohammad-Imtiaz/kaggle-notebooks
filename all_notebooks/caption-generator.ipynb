{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Loading the Libraries","metadata":{}},{"cell_type":"code","source":"# import tensorflow as tf\n# import keras\n# print(tf.__version__)\n# print(keras.__version__)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import string\nimport numpy as np\nimport PIL.Image\n\n\nfrom os import listdir\nfrom pickle import dump, load\n\nfrom numpy import array\nfrom numpy import argmax\n\n# from keras.applications.vgg19 import VGG19, preprocess_input\n\n# from keras import applications\n# from keras.applications.inception_v3  import inception_v3 , preprocess_input\n\nfrom keras.applications.vgg16 import VGG16, preprocess_input\n\nfrom keras.preprocessing.image import load_img, img_to_array\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils import to_categorical\nfrom keras.layers.merge import add\nfrom keras.models import Model, load_model\n# from keras.layers import Input, Dense, LSTM, Embedding, Dropout\nfrom keras.layers import Input, Dense, GRU, Embedding, Dropout\nfrom keras.callbacks import ModelCheckpoint\n\nfrom nltk.translate.bleu_score import corpus_bleu\nfrom tqdm import tqdm_notebook as tqdm","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Extract features from each photo in the directory\ndef extract_features(directory):\n    \n    # Loading the model\n#     model = VGG19()\n    #model = applications.InceptionV3 ()\n    model = VGG16()\n\n    # Removing the last layer from the loaded model as we require only the features not the classification \n    model.layers.pop()\n    model = Model(inputs=model.inputs, outputs=model.layers[-1].output)\n    \n    # Summarizing the model \n    print(model.summary())\n\n    # Extracting features from each photo and storing it in a dictionary \n    features = dict()\n\n    for name in tqdm(listdir(directory)):\n\n        # Defining the path of the image \n        filename = directory + '/' + name\n        \n        # Loading an image and converting it into size 224 * 224\n        image = load_img(filename, target_size=(224, 224))\n        #image = load_img(filename, target_size=(299, 299)) #for inception\n        \n        # Converting the image pixels into a numpy array\n        image = img_to_array(image)\n        \n        # Reshaping data for the model\n        image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n\n        \n        # Preprocessing the images for the VGG model\n        # The preprocess_input function is meant to adequate your image to the format the model requires.\n        #Some models use images with values ranging from 0 to 1. Others from -1 to +1 in preprocess_input\n        image = preprocess_input(image)\n\n        # Getting features of an image\n        feature = model.predict(image, verbose=0)\n        \n        # Getting the image name\n        image_id = name.split('.')[0]\n\n        # Storing the feature corresponding to the image in the dictionary\n        features[image_id] = feature\n        \n        # print('>%s' % name)\n        \n    return features","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Defining the directory we are using\ndirectory = '../input/flicker8k-dataset/Flickr8k_Dataset/Flicker8k_Dataset'\n#directory=\"../input/flickr30k/Images/flickr30k_images\"\n\n# Extracting features from all the images\nfeatures = extract_features(directory)\n\nprint('Extracted Features: ', len(features))\n\n# Dumping the features in a pickle file for further use\ndump(features, open('features.pkl', 'wb'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# listdir('../input/flicker8k-dataset/Flickr8k_Dataset/Flicker8k_Dataset')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Preparing Text Data","metadata":{}},{"cell_type":"code","source":"# Loading the file containg all the descriptions into memory\n\ndef load_doc(filename):\n    # Opening the file as read only\n    file = open(filename, 'r')\n\n    # Reading all text and storing it.\n    text = file.read()\n\n    # Closing the file\n    file.close()\n    \n    return text\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def photo_to_description_mapping(descriptions):\n    \n    # Dictionary to store the mapping of photo identifiers to descriptions\n    description_mapping = dict() \n    \n    # Iterating through each line of the descriptions\n    for line in descriptions.split('\\n'):\n        \n        # Splitting the lines by white space\n        words = line.split()\n        \n        # Skipping the lines with length less than 2\n        if len(line)<2:\n            continue\n            \n        # The first word is the image_id and the rest are the part of the description of that image\n        image_id, image_description = words[0], words[1:]\n        \n        # Retaining only the name of the image and removing the extension from it\n        image_id = image_id.split('.')[0]\n        \n        # Image_descriptions contains comma separated words of the description, hence, converting it back to string\n        image_description = ' '.join(image_description)\n        \n        # There are multiple descriptions per image, \n        # hence, corresponding to every image identifier in the dictionary, there is a list of description\n        # if the list does not exist then we need to create it\n        \n        if image_id not in description_mapping:\n            description_mapping[image_id] = list()\n            \n        # Now storing the descriptions in the mapping\n        description_mapping[image_id].append(image_description)\n    \n    return description_mapping","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def clean_descriptions(description_mapping):\n    \n    # Preapring a translation table for removing all the punctuation\n    table = str.maketrans('','', string.punctuation)\n    \n    # Traversing through the mapping we created\n    for key, descriptions in description_mapping.items():\n        for i in range(len(descriptions)):\n            description = descriptions[i]\n            description = description.split()\n            \n            # Converting all the words to lower case\n            description = [word.lower() for word in description]\n            \n            # Removing the punctuation using the translation table we made\n            description = [word.translate(table) for word in description]\n            \n            # Removing the words with length =1\n            description = [word for word in description if len(word)>1]\n            \n            # Removing all words with number in them\n            description = [word for word in description if word.isalpha()]\n            \n            # Converting the description back to string and overwriting in the descriptions list\n            descriptions[i] = ' '.join(description)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Converting the loaded descriptions into a vocabulary of words\n\ndef to_vocabulary(descriptions):\n    \n    # Build a list of all description strings\n    all_desc = set()\n    \n    for key in descriptions.keys():\n        [all_desc.update(d.split()) for d in descriptions[key]]\n    \n    return all_desc","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# save descriptions to file, one per line\ndef save_descriptions(descriptions, filename):\n    lines = list()\n    for key, desc_list in descriptions.items():\n        for desc in desc_list:\n            lines.append(key + ' ' + desc)\n    data = '\\n'.join(lines)\n    file = open(filename, 'w')\n    file.write(data)\n    file.close()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# listdir('../input/flicker8k-dataset/flickr8k_text')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"filename = '../input/flicker8k-dataset/flickr8k_text/Flickr8k.token.txt'\n#filename=\"../input/flickr30k/captions.txt\"\n\n# Loading descriptions\ndoc = load_doc(filename)\n\n# Parsing descriptions\ndescriptions = photo_to_description_mapping(doc)\nprint('Loaded: %d ' % len(descriptions))\n\n# Cleaning the descriptions\nclean_descriptions(descriptions)\n\n# Summarizing the vocabulary\nvocabulary = to_vocabulary(descriptions)\nprint('Vocabulary Size: %d' % len(vocabulary))\n\n# Saving to the file\nsave_descriptions(descriptions, 'descriptions.txt')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# descriptions","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#  Exploratory Analysis","metadata":{}},{"cell_type":"code","source":"# import pandas as pd\n# import numpy as np\n# from collections import Counter \n\n# dir_Flickr_text = '../input/flicker8k-dataset/flickr8k_text/Flickr8k.token.txt'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# #Finding the captions for each image.\n# file = open(dir_Flickr_text,'r', encoding='utf8')\n# text = file.read()\n# file.close()\n\n\n# datatxt = []\n# for line in text.split('\\n'):\n#     col = line.split('\\t')\n#     if len(col) == 1:\n#         continue\n#     w = col[0].split(\"#\") # Splitting the caption dataset at the required position\n#     datatxt.append(w + [col[1].lower()])\n\n# df_txt = pd.DataFrame(datatxt,columns=[\"filename\",\"index\",\"caption\"])\n\n\n# uni_filenames = np.unique(df_txt.filename.values)\n# print(\"The number of unique file names : {}\".format(len(uni_filenames)))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Cleaning captions for further analysis**","metadata":{}},{"cell_type":"code","source":"# # Defining a function to calculate the top 3 words in all the captions available for the images\n# def df_word(df_txt):\n#     vocabulary = []\n#     for txt in df_txt.caption.values:\n#         vocabulary.extend(txt.split())\n#     print('Vocabulary Size: %d' % len(set(vocabulary)))\n#     ct = Counter(vocabulary)\n#     dfword = pd.DataFrame({\"word\":list(ct.keys()),\"count\":list(ct.values())})\n#     dfword = dfword.sort_values(\"count\",ascending=False)\n#     dfword = dfword.reset_index()[[\"word\",\"count\"]]\n#     return(dfword)\n# dfword = df_word(df_txt)\n# dfword.head(20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#  df_txt[:5]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import matplotlib.pyplot as plt\n# import seaborn as sns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sns.set(style=\"whitegrid\")\n# plt.figure(figsize=(20,8))\n# sns.barplot(x=\"word\",y=\"count\",data=dfword.iloc[:50,:])\n# plt.xticks(rotation=\"vertical\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sns.set(style=\"whitegrid\")\n# plt.figure(figsize=(20,8))\n# sns.barplot(x=\"word\",y=\"count\",data=dfword.iloc[-50:,:])\n# plt.xticks(rotation=\"vertical\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# topn = 50\n\n# def plthist(dfsub, title=\"The top 50 most frequently appearing words\"):\n#     plt.figure(figsize=(20,3))\n#     plt.bar(dfsub.index,dfsub[\"count\"])\n#     plt.yticks(fontsize=20)\n#     plt.xticks(dfsub.index,dfsub[\"word\"],rotation=90,fontsize=20)\n#     plt.title(title,fontsize=20)\n#     plt.show()\n# dfword = df_word(df_txt)\n# plthist(dfword.iloc[:topn,:],\n#         title=\"The top 50 most frequently appearing words\")\n# plthist(dfword.iloc[-topn:,:],\n#         title=\"The least 50 most frequently appearing words\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import string\n\n# def remove_punctuation(text_original):\n#     text_no_punctuation = text_original.translate(str.maketrans('','',string.punctuation))\n#     return(text_no_punctuation)\n\n\n# def remove_single_character(text):\n#     text_len_more_than1 = \"\"\n#     for word in text.split():\n#         if len(word) > 1:\n#             text_len_more_than1 += \" \" + word\n#     return(text_len_more_than1)\n\n\n# def remove_numeric(text,printTF=False):\n#     text_no_numeric = \"\"\n#     for word in text.split():\n#         isalpha = word.isalpha()\n#         if printTF:\n#             print(\"    {:10} : {:}\".format(word,isalpha))\n#         if isalpha:\n#             text_no_numeric += \" \" + word\n#     return(text_no_numeric)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def text_clean(text_original):\n#     text = remove_punctuation(text_original)\n#     text = remove_single_character(text)\n#     text = remove_numeric(text)\n#     return(text)\n\n\n# for i, caption in enumerate(df_txt.caption.values):\n#     newcaption = text_clean(caption)\n#     df_txt[\"caption\"].iloc[i] = newcaption","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Plotting the top 50 words that appear in the cleaned dataset**","metadata":{}},{"cell_type":"code","source":"# topn = 50\n\n# def plthist(dfsub, title=\"The top 50 most frequently appearing words\"):\n#     plt.figure(figsize=(20,3))\n#     plt.bar(dfsub.index,dfsub[\"count\"])\n#     plt.yticks(fontsize=20)\n#     plt.xticks(dfsub.index,dfsub[\"word\"],rotation=90,fontsize=20)\n#     plt.title(title,fontsize=20)\n#     plt.show()\n# dfword = df_word(df_txt)\n# plthist(dfword.iloc[:topn,:],\n#         title=\"The top 50 most frequently appearing words\")\n# plthist(dfword.iloc[-topn:,:],\n#         title=\"The least 50 most frequently appearing words\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sns.set(style=\"whitegrid\")\n# plt.figure(figsize=(20,8))\n# sns.barplot(x=\"word\",y=\"count\",data=dfword.iloc[:50,:])\n# plt.xticks(rotation=\"vertical\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sns.set(style=\"whitegrid\")\n# plt.figure(figsize=(20,8))\n# sns.barplot(x=\"word\",y=\"count\",data=dfword.iloc[-50:,:])\n# plt.xticks(rotation=\"vertical\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Developing Deep Learning Model","metadata":{}},{"cell_type":"markdown","source":"## A.) Loading the data","metadata":{}},{"cell_type":"code","source":"# Function for loading a file into memory and returning text from it\ndef load_file(filename):\n    file = open(filename, 'r')\n    text = file.read()\n    file.close()\n    return text\n\n# Function for loading a pre-defined list of photo identifiers\ndef load_photo_identifiers(filename):\n    \n    # Loading the file containing the list of photo identifier\n    file = load_file(filename)\n    \n    # Creating a list for storing the identifiers\n    photos = list()\n    \n    # Traversing the file one line at a time\n    for line in file.split('\\n'):\n        if len(line) < 1:\n            continue\n        \n        # Image name contains the extension as well but we need just the name\n        identifier = line.split('.')[0]\n        \n        # Adding it to the list of photos\n        photos.append(identifier)\n        \n    # Returning the set of photos created\n    return set(photos)\n\n\n# loading the cleaned descriptions that we created earlier\n# we will only be loading the descriptions of the images that we will use for training\n# hence we need to pass the set of train photos that the above function will be returning\n\ndef load_clean_descriptions(filename, photos):\n    \n    #loading the cleaned description file\n    file = load_file(filename)\n    \n    #creating a dictionary of descripitions for storing the photo to description mapping of train images\n    descriptions = dict()\n    \n    #traversing the file line by line\n    for line in file.split('\\n'):\n        # splitting the line at white spaces\n        words = line.split()\n        \n        # the first word will be the image name and the rest will be the description of that particular image\n        image_id, image_description = words[0], words[1:]\n        \n        # we want to load only those description which corresponds to the set of photos we provided as argument\n        if image_id in photos:\n            #creating list of description if needed\n            if image_id not in descriptions:\n                descriptions[image_id] = list()\n            \n            #the model we will develop will generate a caption given a photo, \n            #and the caption will be generated one word at a time. \n            #The sequence of previously generated words will be provided as input. \n            #Therefore, we will need a ‘first word’ to kick-off the generation process \n            #and a ‘last word‘ to signal the end of the caption.\n            #we will use 'startseq' and 'endseq' for this purpose\n            #also we have to convert image description back to string\n            \n            desc = 'startseq ' + ' '.join(image_description) + ' endseq'\n            descriptions[image_id].append(desc)\n            \n    return descriptions\n\n# function to load the photo features created using the VGG16 model\ndef load_photo_features(filename, photos):\n    \n    #this will load the entire features\n    all_features = load(open(filename, 'rb'))\n    \n    #we are interested in loading the features of the required photos only\n    features = {k: all_features[k] for k in photos}\n    \n    return features","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Splitting data","metadata":{}},{"cell_type":"code","source":"# token=[]\n# for i in uni_filenames:\n#     i=i.split(\".jpg\")\n#     token.append(i)\n\n# print(len(token))\n# token_1=[]\n# for k in range(len(token)):\n#     token_1.append(token[k][0])\n    \n# print(len(token_1))    \n# train_ratio = 0.75\n# validation_ratio = 0.125\n# test_ratio = 0.125\n\n# x_train, x_test = train_test_split(token_1,test_size=1 - train_ratio)\n\n# # test is now 10% of the initial data set\n# # validation is now 15% of the initial data set\n# x_val, x_test = train_test_split(x_test, test_size=test_ratio/(test_ratio + validation_ratio)) \n\n# print(\"Training data length:\",len(x_train))\n# print(\"validation data length:\",len( x_val))\n# print(\"Testing data length:\",len(x_test)) \n\n# train = x_train\n# print('Dataset: ',len(train))\n\n# train_descriptions = load_clean_descriptions('descriptions.txt', train)\n# print('Descriptions: train=', len(train_descriptions))\n\n# train_features = load_photo_features('features.pkl', train)\n# print('Photos: train=', len(train_features))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"filename = '../input/flicker8k-dataset/flickr8k_text/Flickr_8k.trainImages.txt'\n#filename=\"../input/flickr-30k-text-data/train.txt\"\n#filename=\"../input/flickr-30k-70/train_70.txt\"\n\ntrain = load_photo_identifiers(filename)\nprint('Dataset: ',len(train))\n\ntrain_descriptions = load_clean_descriptions('descriptions.txt', train)\nprint('Descriptions: train=', len(train_descriptions))\n\ntrain_features = load_photo_features('features.pkl', train)\nprint('Photos: train=', len(train_features))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# convert a dictionary of clean descriptions to a list of descriptions\ndef to_lines(descriptions):\n    all_desc = list()\n    for key in descriptions.keys():\n        [all_desc.append(d) for d in descriptions[key]]\n    return all_desc\n\n# Given the descriptions, fit a tokenizer\n\n# TOKENIZER CLASS:\n# This class allows to vectorize a text corpus, \n# by turning each text into either a sequence of integers \n# (each integer being the index of a token in a dictionary) \n# or, into a vector where the coefficient for each token could be binary, based on word count, based on tf-idf...\n\ndef create_tokenizer(descriptions):\n    lines = to_lines(descriptions)\n    tokenizer = Tokenizer()\n    tokenizer.fit_on_texts(lines)\n    return tokenizer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = create_tokenizer(train_descriptions)\n\nvocab_size = len(tokenizer.word_index) + 1\nprint('Vocabulary Size: ', vocab_size)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#calculated the length of description with most words\ndef max_lengthTEMP(descriptions):\n    lines = to_lines(descriptions)\n    return max(len(d.split()) for d in lines)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## B.) Defining the Model","metadata":{}},{"cell_type":"code","source":"#the below function loop forever with a while loop and within this, \n#loop over each image in the image directory. \n#For each image filename, we can load the image and \n#create all of the input-output sequence pairs from the image’s description.\n\n#data generator, intended to be used in a call to model.fit_generator()\ndef data_generator(descriptions, photos, tokenizer, max_length):\n    while 1:\n        for key, description_list in descriptions.items():\n            #retrieve photo features\n            photo = photos[key][0]\n            input_image, input_sequence, output_word = create_sequences(tokenizer, max_length, description_list, photo)\n            yield [[input_image, input_sequence], output_word]\n\n            \n#we are calling the create_sequence() function to create \n#a batch worth of data for a single photo rather than an entire dataset. \n#This means that we must update the create_sequences() function \n#to delete the “iterate over all descriptions” for-loop.            \n#Updated create sequence function for data_generator\ndef create_sequences(tokenizer, max_length, desc_list, photo):\n    X1, X2, y = list(), list(), list()\n    # walk through each description for the image\n    for desc in desc_list:\n        # encode the sequence\n        seq = tokenizer.texts_to_sequences([desc])[0]\n        # split one sequence into multiple X,y pairs\n        for i in range(1, len(seq)):\n            # split into input and output pair\n            in_seq, out_seq = seq[:i], seq[i]\n            # pad input sequence\n            in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n            # encode output sequence\n            out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n            # store\n            X1.append(photo)\n            X2.append(in_seq)\n            y.append(out_seq) #The model will output a prediction, which will be a probability distribution over all words in the vocabulary.\n                               #The output data will therefore be a one-hot encoded version of each word, representing an idealized probability distribution with 0 values at all word positions except the actual word position, which has a value of 1.\n\n#     file = open(\"X1.txt\", \"w+\")\n#     file.write(str(array(X1)))\n#     file.close()\n#     file = open(\"X2.txt\", \"w+\")\n#     file.write(str(array(X2)))\n#     file.close()\n#     file = open(\"y.txt\", \"w+\")\n#     file.write(str(array(y)))\n#     file.close()\n    return array(X1), array(X2), array(y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.utils import plot_model\n# define the captioning model\n\ndef define_model(vocab_size, max_length):\n    \n    # feature extractor model\n    inputs1 = Input(shape=(4096,)) #for vgg-19/16\n    #inputs1 = Input(shape=(2048,)) #for inception\n    fe1 = Dropout(0.4)(inputs1)\n    fe2 = Dense(512, activation='relu')(fe1)\n\n    # sequence model\n    inputs2 = Input(shape=(max_length,))\n    se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2) #Masking and padding in Keras reshape the variable length input sequence to sequence of the same length.\n    se2 = Dropout(0.5)(se1)\n#     se3 = LSTM(512)(se2)\n    se3 = GRU(512)(se2)\n\n    # decoder model\n    decoder1 = add([fe2, se3])\n    decoder2 = Dense(512, activation='relu')(decoder1)\n    outputs = Dense(vocab_size, activation='softmax')(decoder2)\n    \n    # tie it together [image, seq] [word]\n    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n    model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])\n    \n    # summarize model\n    print(model.summary())\n    plot_model(model, to_file='model.png', show_shapes=True,dpi=1000)\n    \n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nfilename = '../input/flicker8k-dataset/flickr8k_text/Flickr_8k.trainImages.txt'\n#filename=\"../input/flickr-30k-text-data/train.txt\"\n#filename=\"../input/flickr-30k-70/train_70.txt\"\ntrain = load_photo_identifiers(filename)\nprint('Dataset: ', len(train))\ntrain_descriptions = load_clean_descriptions('descriptions.txt', train)\nprint('Descriptions: train=', len(train_descriptions))\ntrain_features = load_photo_features('features.pkl', train)\nprint('Photos: train=', len(train_features))\ntokenizer = create_tokenizer(train_descriptions)\nvocab_size = len(tokenizer.word_index) + 1\nprint('Vocabulary Size:', vocab_size)\nmax_length = max_lengthTEMP(train_descriptions)\nprint('Description Length: ', max_length)\n\nmodel = define_model(vocab_size, max_length)\n#model.optmizer.lr = 0.0001\nepochs = 20\nsteps = len(train_descriptions)\n\nfor i in tqdm(range(epochs)):\n    generator = data_generator(train_descriptions, train_features, tokenizer, max_length)\n    model.fit_generator(generator, epochs=1, steps_per_epoch=steps, verbose=1)\n    model.save('model_' + str(i) + '.h5')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# listdir()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Flickr_text_dir=\"./X2.txt\"\n# file = open(Flickr_text_dir,'r') # Opening File\n# text = file.read() # Reading File (text file contains:1000268201_693b08cb0e.jpg#0\\tA child in a pink dress is climbing up a set of stairs in an entry way .\\n1000268201_693b08cb0e.jpg#1\\tA girl going into a wooden building .\\n1000268201_693b08cb0e.jpg)\n# file.close()\n# print(text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Evaluate the model","metadata":{}},{"cell_type":"code","source":"#this function maps an integer to a word\ndef word_for_id(integer, tokenizer):\n    for word, index in tokenizer.word_index.items():\n        if index == integer:\n            return word\n    return None\n\n#The function below generates a textual description given a trained model, \n#and a given prepared photo as input. It calls the function word_for_id() \n#in order to map an integer prediction back to a word.\ndef generate_desc(model, tokenizer, photo, max_length):\n    #start tge generation process\n    in_text = 'startseq'\n    #iterating over the max_length since the maximum length of the description can be that only\n    for i in range(max_length):\n        #integer ncoding input sequence\n        sequence = tokenizer.texts_to_sequences([in_text])[0]\n        #padding the input\n        sequence = pad_sequences([sequence], maxlen=max_length)\n        #predicting next word\n        #the predict function will return probability\n        prob = model.predict([photo,sequence], verbose=0)\n        #converting the probability to integer\n        prob = argmax(prob)\n        #calling the word_for_id function in order to map integer to word\n        word = word_for_id(prob, tokenizer)\n        #breaking if word cannot be mapped\n        if word is None:\n            break\n        #appending as input\n        in_text += ' ' + word\n        #break if end is predicted\n        if word == 'endseq':\n            break\n    return in_text\n\n#the below function evaluates the skill of the model\ndef evaluate_model(model, descriptions, photos, tokenizer, max_length):\n    actual, predicted = list(), list()\n    for key, desc_list in tqdm (descriptions.items()):\n        prediction = generate_desc(model, tokenizer, photos[key], max_length)\n        actual_desc = [d.split() for d in desc_list]\n        actual.append(actual_desc)\n        predicted.append(prediction.split())\n\n    print('BLEU-1: ', corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n    print('BLEU-2: ', corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n    print('BLEU-3: ', corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n    print('BLEU-4: ', corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\n    \ndef max_length(descriptions):\n    lines = to_lines(descriptions)\n    return max(len(d.split()) for d in lines)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nfilename = '../input/flicker8k-dataset/Flickr8k_text/Flickr_8k.trainImages.txt'\n#filename=\"../input/flickr-30k-text-data/train.txt\"\n#filename=\"../input/flickr-30k-70/train_70.txt\"\n\ntrain = load_photo_identifiers(filename)\nprint('Dataset: ', len(train))\ntrain_descriptions = load_clean_descriptions('descriptions.txt', train)\nprint('Descriptions: train=', len(train_descriptions))\ntokenizer = create_tokenizer(train_descriptions)\nvocab_size = len(tokenizer.word_index) + 1\nprint('Vocabulary Size: ', vocab_size)\nmax_length = max_lengthTEMP(train_descriptions)\nprint('Description Length: ,', max_length)\n\nfilename = '../input/flicker8k-dataset/Flickr8k_text/Flickr_8k.testImages.txt'\n#filename =\"../input/flickr-30k-text-data/test.txt\"\n#filename=\"../input/flickr-30k-70/test_70.txt\"\ntest = load_photo_identifiers(filename)\nprint('Dataset: ', len(test))\ntest_descriptions = load_clean_descriptions('descriptions.txt', test)\nprint('Descriptions: test=', len(test_descriptions))\ntest_features = load_photo_features('features.pkl', test)\nprint('Photos: test=', len(test_features))\n\nfilename = 'model_0.h5'\nmodel = load_model(filename)\nevaluate_model(model, test_descriptions, test_features, tokenizer, max_length)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"filename = 'model_1.h5'\nmodel = load_model(filename)\nevaluate_model(model, test_descriptions, test_features, tokenizer, max_length)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"filename = 'model_2.h5'\nmodel = load_model(filename)\nevaluate_model(model, test_descriptions, test_features, tokenizer, max_length)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"filename = 'model_3.h5'\nmodel = load_model(filename)\nevaluate_model(model, test_descriptions, test_features, tokenizer, max_length)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"filename = 'model_4.h5'\nmodel = load_model(filename)\nevaluate_model(model, test_descriptions, test_features, tokenizer, max_length)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"filename = 'model_5.h5'\nmodel = load_model(filename)\nevaluate_model(model, test_descriptions, test_features, tokenizer, max_length)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"filename = 'model_6.h5'\nmodel = load_model(filename)\nevaluate_model(model, test_descriptions, test_features, tokenizer, max_length)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"filename = 'model_7.h5'\nmodel = load_model(filename)\nevaluate_model(model, test_descriptions, test_features, tokenizer, max_length)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"filename = 'model_8.h5'\nmodel = load_model(filename)\nevaluate_model(model, test_descriptions, test_features, tokenizer, max_length)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"filename = 'model_9.h5'\nmodel = load_model(filename)\nevaluate_model(model, test_descriptions, test_features, tokenizer, max_length)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"filename = 'model_10.h5'\nmodel = load_model(filename)\nevaluate_model(model, test_descriptions, test_features, tokenizer, max_length)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"filename = 'model_11.h5'\nmodel = load_model(filename)\nevaluate_model(model, test_descriptions, test_features, tokenizer, max_length)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"filename = 'model_12.h5'\nmodel = load_model(filename)\nevaluate_model(model, test_descriptions, test_features, tokenizer, max_length)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"filename = 'model_13.h5'\nmodel = load_model(filename)\nevaluate_model(model, test_descriptions, test_features, tokenizer, max_length)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"filename = 'model_14.h5'\nmodel = load_model(filename)\nevaluate_model(model, test_descriptions, test_features, tokenizer, max_length)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"filename = 'model_15.h5'\nmodel = load_model(filename)\nevaluate_model(model, test_descriptions, test_features, tokenizer, max_length)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"filename = 'model_16.h5'\nmodel = load_model(filename)\nevaluate_model(model, test_descriptions, test_features, tokenizer, max_length)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"filename = 'model_17.h5'\nmodel = load_model(filename)\nevaluate_model(model, test_descriptions, test_features, tokenizer, max_length)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"filename = 'model_18.h5'\nmodel = load_model(filename)\nevaluate_model(model, test_descriptions, test_features, tokenizer, max_length)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"filename = 'model_19.h5'\nmodel = load_model(filename)\nevaluate_model(model, test_descriptions, test_features, tokenizer, max_length)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. Generate new descriptions","metadata":{}},{"cell_type":"code","source":"filename = '../input/flicker8k-dataset/Flickr8k_text/Flickr_8k.trainImages.txt'\ntrain = load_photo_identifiers(filename)\nprint('Dataset: ', len(train))\ntrain_descriptions = load_clean_descriptions('descriptions.txt', train)\nprint('Descriptions: train=', len(train_descriptions))\ntokenizer = create_tokenizer(train_descriptions)\ndump(tokenizer, open('tokenizer.pkl', 'wb'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def extract_features(filename):\n    model = VGG19()\n    model.layers.pop()\n    model = Model(inputs=model.inputs, outputs=model.layers[-1].output)\n    image = load_img(filename, target_size=(224, 224))\n    image = img_to_array(image)\n    image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n    image = preprocess_input(image)\n    feature = model.predict(image, verbose=0)\n    return feature\n\ndef word_for_id(integer, tokenizer):\n    for word, index in tokenizer.word_index.items():\n        if index == integer:\n            return word\n    return None\n\ndef generate_desc(model, tokenizer, photo, max_length):\n    in_text = 'startseq'\n    for i in range(max_length):\n        sequence = tokenizer.texts_to_sequences([in_text])[0]\n        sequence = pad_sequences([sequence], maxlen=max_length)\n        yhat = model.predict([photo,sequence], verbose=0)\n        yhat = argmax(yhat)\n        word = word_for_id(yhat, tokenizer)\n        if word is None:\n            break\n        in_text += ' ' + word\n        if word == 'endseq':\n            break\n    return in_text\n\ntokenizer = load(open('tokenizer.pkl', 'rb'))\nmax_length = 34","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.preprocessing import image\nimport matplotlib.pyplot as plt\nfrom PIL import Image","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = load_model('model_0.h5')\npath = '../input/flicker8k-dataset/Flickr8k_Dataset/Flicker8k_Dataset/2885891981_6b02620ae9.jpg'\nphoto = extract_features(path)\n\ndescription = generate_desc(model, tokenizer, photo, max_length)\nprint(description[9:len(description)-6])\nimg=image.load_img('../input/flicker8k-dataset/Flickr8k_Dataset/Flicker8k_Dataset/2885891981_6b02620ae9.jpg')\nimg=image.img_to_array(img)\nplt.imshow((img/255)*1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path = \"../input/flicker8k-dataset/Flickr8k_Dataset/Flicker8k_Dataset/2995935078_beedfe463a.jpg\"\nphoto = extract_features(path)\n\ndescription = generate_desc(model, tokenizer, photo, max_length)\n\nprint(description[9:len(description)-6])\nimg=Image.open(\"../input/flicker8k-dataset/Flickr8k_Dataset/Flicker8k_Dataset/2995935078_beedfe463a.jpg\")\nfig, ax = plt.subplots()\nax.imshow(img)\nax.spines['top'].set_visible(False)\nax.spines['left'].set_visible(False)\nax.spines['bottom'].set_visible(False)\nax.spines['right'].set_visible(False)\nax.set_xticks([])\nax.set_yticks([])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path = '../input/flicker8k-dataset/Flickr8k_Dataset/Flicker8k_Dataset/174466741_329a52b2fe.jpg'\nphoto = extract_features(path)\n\ndescription = generate_desc(model, tokenizer, photo, max_length)\n\nprint(description[9:len(description)-6])\nimg=Image.open('../input/flicker8k-dataset/Flickr8k_Dataset/Flicker8k_Dataset/174466741_329a52b2fe.jpg')\nfig, ax = plt.subplots()\nax.imshow(img)\nax.spines['top'].set_visible(False)\nax.spines['left'].set_visible(False)\nax.spines['bottom'].set_visible(False)\nax.spines['right'].set_visible(False)\nax.set_xticks([])\nax.set_yticks([])\nplt.show()\nplt.savefig('Result_1.png',dpi=1000)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path = '../input/flicker8k-dataset/Flickr8k_Dataset/Flicker8k_Dataset/49553964_cee950f3ba.jpg'\nphoto = extract_features(path)\n\ndescription = generate_desc(model, tokenizer, photo, max_length)\n\nprint(description[9:len(description)-6])\nimg=image.load_img('../input/flicker8k-dataset/Flickr8k_Dataset/Flicker8k_Dataset/49553964_cee950f3ba.jpg')\nimg=image.img_to_array(img)\n\nplt.imshow((img/255)*1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\npath = '../input/flicker8k-dataset/Flickr8k_Dataset/Flicker8k_Dataset/49553964_cee950f3ba.jpg'\nphoto = extract_features(path)\n\ndescription = generate_desc(model, tokenizer, photo, max_length)\n\nprint(description[9:len(description)-6])\nimg=Image.open('../input/flicker8k-dataset/Flickr8k_Dataset/Flicker8k_Dataset/49553964_cee950f3ba.jpg')\nfig, ax = plt.subplots()\nax.imshow(img)\nax.spines['top'].set_visible(False)\nax.spines['left'].set_visible(False)\nax.spines['bottom'].set_visible(False)\nax.spines['right'].set_visible(False)\nax.set_xticks([])\nax.set_yticks([])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"248174959_2522871152\npath = '../input/flicker8k-dataset/Flickr8k_Dataset/Flicker8k_Dataset/248174959_2522871152.jpg'\nphoto = extract_features(path)\n\ndescription = generate_desc(model, tokenizer, photo, max_length)\n\nprint(description[9:len(description)-6])\nimg=Image.open('../input/flicker8k-dataset/Flickr8k_Dataset/Flicker8k_Dataset/248174959_2522871152.jpg')\nfig, ax = plt.subplots()\nax.imshow(img)\nax.spines['top'].set_visible(False)\nax.spines['left'].set_visible(False)\nax.spines['bottom'].set_visible(False)\nax.spines['right'].set_visible(False)\nax.set_xticks([])\nax.set_yticks([])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}