{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Introduction\n\nHello people, welcome to my kernel! In this kernel I am going to compare different classification algorithms with **Biomechanical Features of Orthopedic Patients** dataset.\n\nIn this kernel I am going to use 6 different algorithm:\n\n* K-Nearest Neighbour\n* Logistic Regression\n* Support Vector Machine (SVC)\n* Naive Bayes Classification\n* Decision Tree Classification\n* Random Forest Classification\n\nLet's take a look at our schedule\n\n# Schedule\n1. Importing Libraries and Data\n2. Having Idea About Data\n1. Data Preprocessing for Machine Learning\n1. Confusion Matrix Function\n1. Machine Learning Algorithms\n    * KNN\n    * Logistic Regression\n    * SVM Classification\n    * Naive Bayes Classification\n    * Decision Tree Classification\n    * Random Forest Classification\n1. Result\n1. Conclusion\n"},{"metadata":{},"cell_type":"markdown","source":"# Importing Libraries and Data\n\nIn this section I am going to only import the libraries that about the visualization and data processing. I am going to add machine learning libraries when I need them."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\nimport warnings as wrn\nwrn.filterwarnings('ignore')\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Importing Data Using Pandas\ndata = pd.read_csv(\"/kaggle/input/biomechanical-features-of-orthopedic-patients/column_2C_weka.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I've imported two labeled data because I want to use Logistic Regression"},{"metadata":{},"cell_type":"markdown","source":"# Having Idea About Data\n\nIn this section I am going to examine dataset because before the preprocessing I have to have an idea about the data. In order to do this I am going to use head(),tail() and info() methods"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Our data does not have any NaN values so we will not fill NaN values\n* Our label is object, we have to convert it to int64\n* There are only 6 features in our dataset."},{"metadata":{},"cell_type":"markdown","source":"# Data Preprocessing for Machine Learning\n\nIn this section I am going to prepare data for machine learning. In order to do this I am going to follow these steps.\n\n* Converting Label to Int64\n* Normalizing Data\n* Splitting Data \n\nLet's start.\n"},{"metadata":{},"cell_type":"markdown","source":"## Converting Label to Int64\n\nMaybe, you remember. There are two labels in our dataset. They are:\n* Normal\n* Abnormal\n\nI am going to convert *Normal* to 1 and *Abnormal* to 0. In order to do this I am going to use list comprehension\n\n(Python is really usefull language :D )"},{"metadata":{"trusted":true},"cell_type":"code","source":"data[\"class\"] = [1 if each == \"Normal\" else 0 for each in data[\"class\"]] ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.tail()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Normalizing Data\n\nIn this section I am going to normalize data. In order to do this I am going to use this formula.\n\n#### normalized = (value - min value of the feature) / (max value of the feature - min value of the feature)\n\nLet's implement this in python!"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = (data-np.min(data)) /(np.max(data)-np.min(data))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Splitting Data\n\nIn this section I am going to split data into two pieces. Train and test. In order to do this I am going to use SKLearn library's Train Test Split Function. Let's do this!"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx = data.drop(\"class\",axis=1)\ny = data[\"class\"]\n\nx_train,x_test,y_train,y_test = train_test_split(x,y,random_state=1,test_size=0.3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Our train and test splits are ready, we are ready to train some machine learning features!"},{"metadata":{},"cell_type":"markdown","source":"# Confusion Matrix Function\n\nIn this section I am going to define a function that creates a seaborn heatmap from a confusion matrix."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\ndef plot_confusionMatrix(y_true,y_pred):\n    cn = confusion_matrix(y_true=y_true,y_pred=y_pred)\n    \n    fig,ax = plt.subplots(figsize=(5,5))\n    sns.heatmap(cn,annot=True,linewidths=1.5)\n    plt.show()\n    return cn","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our function is ready"},{"metadata":{},"cell_type":"markdown","source":"# Machine Learning Algorithms\n\nFinally we've came our main section. In this section I am going to train some different machine learning algorithms and at the final of this section I am going to compare accuracy scores. Let's start with KNN"},{"metadata":{},"cell_type":"markdown","source":"## KNN Classification\n\nIn this section I am going to train a KNN model using sklearn library. Then I am going to save the score of this model in a variable. (I am going to save it because I want to compare the scores at the final)"},{"metadata":{"trusted":true},"cell_type":"code","source":"score_list = {} # I've created this dict for saving score variables into it ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier \nKNN = KNeighborsClassifier(n_neighbors=22) #I've tried more than 50 values. 22 is the best value\n\nKNN.fit(x_train,y_train)\nknn_score = KNN.score(x_test,y_test)\nscore_list[\"KNN Classifier\"] = knn_score\nprint(f\"Score is {knn_score}\")\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our first algorithm's score is %81. I think it is a bit low, but not bad."},{"metadata":{"trusted":true},"cell_type":"code","source":"y_true = y_test\ny_pred = KNN.predict(x_test)\nplot_confusionMatrix(y_true,y_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Logistic Regression \n\nIn this section I am going to train a logistic regression model. I hope it will be better than KNN. "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nLR = LogisticRegression()\nLR.fit(x_train,y_train)\n\nlr_score = LR.score(x_test,y_test)\nscore_list[\"Logistic Regression\"] = lr_score\n\nprint(f\"Score is {lr_score}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Our logistic regression score is %74"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = LR.predict(x_test)\nplot_confusionMatrix(y_true,y_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Support Vector Machine Classification\nIn this section I am going to use SVM classification."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC \n\nsvc = SVC()\nsvc.fit(x_train,y_train)\nsvc_score = svc.score(x_test,y_test)\nscore_list[\"SVC\"] = svc_score\nprint(f\"Score is {svc_score}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Our SVC score is %80. This is better than Logistic Regression Score "},{"metadata":{"trusted":true},"cell_type":"code","source":"y_true = y_test\ny_pred = svc.predict(x_test)\nplot_confusionMatrix(y_true,y_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Naive Bayes Classification\n\nIn this section I am going to train a NBC model."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\n\nnbc = GaussianNB()\nnbc.fit(x_train,y_train)\nnbc_score = nbc.score(x_test,y_test)\nscore_list[\"GaussianNBC\"] = nbc_score\n\nprint(f\"Score is {nbc_score}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Our score is %81. It is very similar to our KNN score"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_true = y_test\ny_pred = nbc.predict(x_test)\nplot_confusionMatrix(y_true,y_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Decision Tree Classification\n\nIn this section I am going to train a decision tree classification model."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\ndtc = DecisionTreeClassifier(random_state=1)\ndtc.fit(x_train,y_train)\n\ndtc_score = dtc.score(x_test,y_test)\nscore_list[\"DTC\"] = dtc_score\nprint(f\"Score is {dtc_score}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our score is %78. Although our score is low, it is still better than Logistic Regression score"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_true = y_test\ny_pred = dtc.predict(x_test)\nplot_confusionMatrix(y_true,y_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Random Forest Classification\n\nIn this section I am going to train our last model, Random Forest Classification."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nrfc = RandomForestClassifier(n_estimators=50,random_state=1)\nrfc.fit(x_train,y_train)\nrfc_score = rfc.score(x_test,y_test)\nscore_list[\"RFC\"]=rfc_score\n\nprint(f\"Score is {rfc_score}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Finally! Our score is %87. It is the best score of this kernel.\n* Our algorithms finished. We are ready to compare the scores."},{"metadata":{"trusted":true},"cell_type":"code","source":"y_true = y_test\ny_pred = rfc.predict(x_test)\nplot_confusionMatrix(y_true,y_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Result\n\nIn this section I am going to compare the scores of all the algorithms. At the first of Machine Learning Section. I've defined a dictionary that contains the scores of algorithms. In this section I am going to use that."},{"metadata":{"trusted":true},"cell_type":"code","source":"score_list = list(score_list.items())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for alg,score in score_list:\n    print(f\"{alg} Score is {str(score)[:4]} \")\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We've saw our scores. Let's sort them.\n\n1. Random Forest Classification %87 Accuracy\n2. Naive Bayes and KNN Classification %81 Accuracy\n3. Support Vector Machine Classification %80 Accuracy\n4. Decision Tree Classification %78 Accuracy\n5. Logistic Regression %74 Accuracy\n\nAs we can see, for classification, the best algorithm is Random Forest Classification.\n\n## But do not forget that, in different datasets, it will be a different algorithm \n\n## So it does not show that, Random Forest Classification is not the best algorithm for classification - however for this dataset it is :) -"},{"metadata":{},"cell_type":"markdown","source":"# Conclusion\n\nI am a beginner in Machine Learning so I might made some mistakes. If there is a problem please contact me.\n\nHowever if there is not any problems. If you upvote this kernel, I would be glad."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}