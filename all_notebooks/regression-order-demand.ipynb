{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Introduction #\nRestaurants are constantly looking for ways to cut costs while continue to serve food with the same quality. You can do this by finding efficiencies in labour, production or raw materials. In this project, we will look at a dataset to determine whether we can run a model to more accurately predict when people will order a certain item. If it is completed effectively, this will allow the restaurant to do more accurate purchases. This help save costs by reducing waste and finding sales when making required purchases to meet the demand."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from matplotlib import pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ful = pd.read_csv('/kaggle/input/food-demand/foodDemand_train/fulfilment_center_info.csv')\nmeal = pd.read_csv('/kaggle/input/food-demand/foodDemand_train/meal_info.csv')\ndf = pd.read_csv('/kaggle/input/food-demand/foodDemand_train/train.csv')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ful.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"meal.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ful.region_code.nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ful.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I merged the columns from all of the datasets to see if there is additional information that can help the model more accurately "},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.merge(meal, on='meal_id')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.merge(ful, on='center_id')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## EDA ##\nFirst off I will look at a breakdown of the number unique id's (This can have multiple order per id) broken down by the cuisine. We can see that Beverages are clearly sold the most. In general, beverages would be seen as cheaper than full meals so we would expect to see higher sales of this. The rest of the meals are relatively evenly spread."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(15,15))\n\ng = sns.displot(data=df, x='category', hue = 'cuisine',height = 6, aspect = 2, multiple = 'stack')\ng.set_xticklabels(rotation=30)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next, I just wanted to show the average base price of each of the meals. Seafood, Fish and Pizza have the highest price among the meals/categories."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.groupby(['category'])['base_price'].mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the below graph, we're looking at the total price received by week and cuisine. It's interesting to see that there is such a large discrepancy between the Continental cuisine and the rest of the cuisine's, when it come to the total base_price sold. It would seems the higher priced meals (pizza and seafood) fall under the Continental cuisine. In addition, we can see that total base price drops around the same time among all of the cuisine's (The percentage drops are different among all of the cuisine's)."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,8))\nfor typ in list(df['cuisine'].unique()):\n    weekwise = df[df['cuisine'] == typ].groupby('week').base_price.sum()\n    weekwise.plot()\nplt.ylabel('Total Sales Per Cuisine')\nplt.legend(list(df['cuisine'].unique()))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"top_cont = df[df['cuisine'] == 'Continental'].groupby('category').num_orders.sum()\ntop_cont","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next, I wanted to see what the price difference is betweent he base and checkout price. This could show whether the total prices are mainly driven by the discount. Thai food seemed to have the most consistent price between the base and checkout price, while the Continental food had the largest difference between the base and checkout price. "},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, (ax1, ax2, ax3, ax4) = plt.subplots(4, figsize = (15,15))\nsubs = [ax1, ax2, ax3, ax4]\ncolor = ['green', 'red', 'blue', 'orange']\nfor typ, sub, col in zip(list(df['cuisine'].unique()), subs, color):\n    weekwise = df[df['cuisine'] == typ].groupby('week').base_price.sum() - df[df['cuisine'] == typ].groupby('week').checkout_price.sum()\n    sub.plot(weekwise, color = col)\n    sub.set_title(typ)\n    sub.set_xlabel('Week')\n    sub.set_ylabel('Difference in Base and Checkout Price')\n   \nfig.suptitle('Price Difference Per Week', fontsize = 20)\nfig.subplots_adjust(top=0.90)\nplt.xlabel('Week')\nfig.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After looking at the price, I wanted to take a look at which categories were the leading the overall number of order. For the graph, I thought it would be reasonable to look at the weekly sales of the top 6 number of orders. The lower cost items were the leaders for the number of orders, with Pizza being the lone higher priced item that made it into the top 6. "},{"metadata":{"trusted":true},"cell_type":"code","source":"top6 = df.groupby('category').num_orders.sum()\ntop6","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, (ax1, ax2, ax3,ax4, ax5,ax6) = plt.subplots(6, figsize = (15,15))\ntop6 = ['Beverages', 'Rice Bowl', 'Sandwich', 'Salad', 'Pizza', 'Other Snacks']\ncolor = ['purple', 'red', 'blue', 'orange','purple', 'purple']\nsubs = [ax1,ax2,ax3,ax4,ax5,ax6]\nfor typ, sub, col in zip(top6, subs, color):\n    weekwise = df[df['category'] == typ].groupby('week').num_orders.sum() \n    sub.plot(weekwise, color = col)\n    sub.set_title(typ)\n    sub.set_xlabel('Week')\n    sub.set_ylabel('Number of Orders by Category')\n   \nfig.suptitle('Orders Per Week: Top 6', fontsize = 20)\nfig.subplots_adjust(top=0.90)\nplt.xlabel('Week')\nfig.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The next graphs that are presented a violin plot showing the density in the Checkout Price by a Promotion on a certain item and whether it has been Featured on the HomePage. In both cases when there was a promotion or featured on the HomePage (1), are mostly congregated around 300 and 500.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.clf()\nsns.violinplot(data=df, x='emailer_for_promotion', y= 'checkout_price')\nplt.title('Promotion Compared to Checkout Price')\n\nplt.show()\nsns.violinplot(data=df, x='homepage_featured', y= 'checkout_price')\nplt.title('Featured on The Homepage to Checkout Price')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"norm_df = df\nnorm_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In order to present a more normally distributed base and checkout price, I took the log of each of the columns."},{"metadata":{"trusted":true},"cell_type":"code","source":"norm_df['log_check'] = np.log(df['checkout_price'])\nnorm_df['log_base'] = np.log(df['base_price'])\nnorm_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.displot(data=norm_df, x='log_check', kind='kde')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Preparation ##\nIn this section I will be dropping some columns that will no longer be required and normalized."},{"metadata":{"trusted":true},"cell_type":"code","source":"norm_df = norm_df.drop(['checkout_price', 'base_price', 'center_id', 'meal_id', 'id'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dtyp = []\nfor col in norm_df.columns[0:]:\n    if col == 'num_orders':\n        next\n    else:\n        dtyp.append(col)\ndtyp.append('num_orders')\ndtyp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"norm_df = norm_df[dtyp]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import preprocessing\n\nfor col in ['op_area', 'log_check', 'log_base']:\n    x = np.array(norm_df[col]) #returns a numpy array\n    x = np.reshape(x,(-1,1))\n    min_max_scaler = preprocessing.MinMaxScaler()\n    x_scaled = min_max_scaler.fit_transform(x)\n    norm_df[col] = x_scaled","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in norm_df.columns[:8]:\n    norm_df = pd.get_dummies(norm_df, columns=[col], prefix = [col])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target = norm_df.iloc[:,3]\n\nxinfo = norm_df.drop(['num_orders'],axis=1)\nxinfo","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Use the train_test_split to split the data into a training and testing dataset.\nfrom sklearn.model_selection import train_test_split\n\n\n\nx_train, x_test, y_train, y_test = train_test_split(xinfo, target, test_size=.2, random_state=10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Models ##\nI chose to run a couple different linear models, a decision tree and the support vector regressor. After running the linear regression, lasso, SGD Regression and support vector regression model, they resulted in an R sqaured value lower than 50%. The decision tree provides an R squared value of over 70%. "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Lasso\nfrom sklearn import svm\nfrom sklearn.linear_model import SGDRegressor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"linreg = LinearRegression()\nlinreg.fit(x_train, y_train)\npre_linear = linreg.predict(x_test)\nr_sq = linreg.score(x_test, y_test)\nprint('Coefficient of determination:', r_sq)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Ran a for loop to determine the best tradeoff between number of branches and accuracy of the results\nscore_list = []\n\nfor i in range(2,20):\n    decreg = DecisionTreeRegressor(max_depth = i)\n    decreg.fit(x_train, y_train)\n    pre_tree = decreg.predict(x_test)\n    r_sq = decreg.score(x_test, y_test)\n    score_list.append(r_sq)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure()\nplt.plot(list(range(2,20)), score_list)\nplt.title(\"Best Depth For The Tree\")\nplt.xticks(list(range(2,20)))\nplt.ylabel(\"R-Squared Score\")\nplt.xlabel(\"Depth of Tree\")\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"decreg = DecisionTreeRegressor(max_depth = 12)\ndecreg.fit(x_train, y_train)\nr_sq = decreg.score(x_test, y_test)\nprint('Coefficient of determination:', r_sq)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_alp = []\n\nfor i in np.arange(0.5,5,.5):\n    lasreg = Lasso(alpha=i)\n    lasreg.fit(x_train, y_train)\n    pre_linear = lasreg.predict(x_test)\n    r_sq = lasreg.score(x_test, y_test)\n    new_alp.append(r_sq)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure()\nplt.plot(np.arange(0.5,5,.5), new_alp)\nplt.title(\"Best Depth For The Tree\")\nplt.xticks(np.arange(0.5,5,.5))\nplt.ylabel(\"R-Squared Score\")\nplt.xlabel(\"Alpha Application\")\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loss_func = []\n\nfor loss in ['squared_loss', 'huber', 'epsilon_insensitive', 'squared_epsilon_insensitive']:\n    SGDreg = SGDRegressor(loss= loss)\n    SGDreg.fit(x_train, y_train)\n    pre_SGD = linreg.predict(x_test)\n    loss_func.append(pre_SGD)\n    r_sq = SGDreg.score(x_test, y_test)\n    print('Coefficient of determination:', r_sq)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(0,5):\n    svreg = svm.LinearSVR(epsilon=i)\n    svreg.fit(x_train, y_train)\n    pre_svr = svreg.predict(x_test)\n    r_sq = svreg.score(x_test, y_test)\n    print('Coefficient of determination:', r_sq)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Conclusion ##\nAlthough the decision tree regressor provides a much higher R squared value (accuracy) than the other models, but this would probably be unacceptable for any restaurant to use for determining the number of orders. With an accuracy slightly above 70%, it may be result in some more accurate predictions for future orders, but it may give the user too much confidence in the model. This could result in under investment in the food required, which may not be enough for the demand. When customers start losing trust in a restaurant, it can result in a lot of lost sales in the future.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}