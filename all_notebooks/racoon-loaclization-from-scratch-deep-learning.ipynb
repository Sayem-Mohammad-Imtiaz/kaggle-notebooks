{"cells":[{"metadata":{},"cell_type":"markdown","source":"This Notebook contains very basic implementation to search and put a object in a bounding box (localization) with deep learning using pytorch."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Import modules "},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as ttf\nimport PIL.Image as imgs\nfrom PIL import ImageDraw","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class RacoonsDataSet(Dataset):\n    def __init__(self,p):\n#         super(self).__init__()\n        self.df=pd.read_csv(p)\n        self.transform=ttf.Compose([ttf.Resize([64,64]),ttf.ToTensor(),ttf.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])])\n        \n    def __len__(self):\n        return len(self.df)\n    def __getitem__(self,i):\n        image=imgs.open('../input/racoon-detection/Racoon Images/images/{}'.format(self.df['filename'][i])).convert('RGB')\n        X=image.size[0]\n        Y=image.size[1]\n        try:\n            image=self.transform(image)\n        except Exception as e:\n            print(X,\"   \",Y)\n            print(e)\n            return\n        box=torch.from_numpy(np.array([[(64/X)*self.df[x][i],(64/Y)*self.df[y][i]] for [x,y] in zip(['xmin','xmax'],['ymin','ymax'])]).ravel()).float()\n        return [image,box]\n    def drawBox(self,img,box):\n        draw=ImageDraw.Draw(img)\n        draw.rectangle([int(x) for x in box], outline=(255, 0, 0),width=1)\n        return img\n    def getImage(self,i):\n        image=imgs.open('../input/racoon-detection/Racoon Images/images/{}'.format(self.df['filename'][i])).convert('RGB')\n        \n        X=image.size[0]\n        Y=image.size[1]\n        image=image.resize((64,64))\n        return image,np.array([[(64/X)*self.df[x][i],(64/Y)*self.df[y][i]] for [x,y] in zip(['xmin','xmax'],['ymin','ymax'])])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Create the Custom Dataset\n\nDataset returns the racoon image with bounding box with proper scalling\nOur class also have a function to directly get pil images implemented seprately\n\nLets Test the outputs"},{"metadata":{"trusted":true},"cell_type":"code","source":"p=RacoonsDataSet('../input/racoon-detection/train_labels_.csv')\np[3]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"k=p.getImage(23)\nk[1].ravel()\np.drawBox(k[0],k[1].ravel())\n    \n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Bounding Box Scalling seems fine"},{"metadata":{"trusted":true},"cell_type":"code","source":"class Network(nn.Module):\n    def __init__(self):\n        super(Network,self).__init__()\n        model = torch.hub.load('pytorch/vision:v0.6.0', 'resnet18', pretrained=True)        \n        self.fc1=nn.Sequential(*self.get_req_features(model))\n        self.fc_classifier=nn.Sequential(nn.Linear(64*16*16,2),nn.ReLU())\n        self.boundingBox=nn.Sequential(nn.Linear(64*16*16,4),nn.ReLU())\n    def forward(self,X):\n        X=self.fc1(X)\n        X=X.reshape(-1,64*16*16)\n        class_preds=self.fc_classifier(X)\n        bound_box=self.boundingBox(X)\n        \n        return class_preds,bound_box\n    def get_req_features(self,model):\n        fc=list(model.children())\n        req_features=[]\n        k=torch.zeros([1,3,64,64]).float()\n        for i in fc:\n            k=i(k)\n            if k.size()[2] <800//80:\n                break\n            req_features.append(i)\n        print(\"++++++++++++++++++++Processing To Extract Features++++++++++++++++++++++++\")\n        print(len(req_features))\n        print(k.size())\n        return req_features","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets create network. We are using a pretrained network (resnet) to get basic features of image"},{"metadata":{"trusted":true},"cell_type":"code","source":"model2=Network()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"p[2][0].shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Loss For bounding box is MSE loss as it is a regression problem"},{"metadata":{"trusted":true},"cell_type":"code","source":"loss_boundingBox=nn.MSELoss()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"optimizer = torch.optim.Adam(model2.parameters(), lr=1e-4, weight_decay=1e-5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"optimizer.zero_grad()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# c,bb=model2(p[2][0].unsqueeze(0))\n# l2=loss_boundingBox(bb,p[2][1])\n# print(l2)\n# l2.backward()\n# optimizer.step()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataloader=DataLoader(p,batch_size=64,shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel2.to(device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install tensorboard\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from torch.utils.tensorboard import SummaryWriter\nwriter = SummaryWriter()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from matplotlib.pyplot import imshow,show","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So we will train in batch of 64\nAlso We use a custom loss function where we also try to minimize size of bounding box. it is controlled by tolerance factor\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size=64\ntolerance=0.01","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"get sample can be used to see the results"},{"metadata":{"trusted":true},"cell_type":"code","source":"def getSample(i):    \n    img,_=p.getImage(i)\n    k=model2.forward(p[i][0].unsqueeze(0).to(device))\n    k=k[1].to('cpu').detach().numpy()\n#     k=k*128/64\n    p.drawBox(img,k.ravel())\n    show(img)\n    return img","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let the training began"},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm import tqdm\nfor j in tqdm(range(1000)):\n    for D,B in dataloader:\n        optimizer.zero_grad()\n        D,B=D.to(device),B.to(device)\n        \n        c,bb=model2(D)\n        l2=loss_boundingBox(bb,B)+tolerance*torch.sum((bb[2]-bb[0])**2+(bb[3]-bb[1]))/(2*batch_size)\n        l2.backward()\n        optimizer.step()\n        writer.add_scalar('Loss/train',l2.item())\n        \n    if j%100==0:\n        print(l2.item())\n        with torch.no_grad():\n            k=model2.forward(p[1][0].unsqueeze(0).to(device))        \n            k=k[1].cpu().detach().numpy()\n            img=p.drawBox(p.getImage(1)[0],k.ravel())\n            imshow(img)\n            show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"p[0][1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%load_ext tensorboard","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%tensorboard --logdir=runs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a=ttf.ToPILImage()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets Test with first 10 images\n"},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(10):\n\n    imshow(getSample(i+3))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Seems fine, Next target is to make a racoon detection and localization project"},{"metadata":{"trusted":true},"cell_type":"code","source":"torch.save(model2,'Model.pt')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}