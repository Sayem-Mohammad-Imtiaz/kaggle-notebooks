{"cells":[{"metadata":{},"cell_type":"markdown","source":"### Raw Operational Data from Enterprise Application\nA dataset for anomaly detection in operations of distributed software systems\n\n### Software Operational Data, Processed and Labeled\nA dataset for anomaly detection in operations of distributed software systems"},{"metadata":{},"cell_type":"markdown","source":"# Introduction\nAnomaly detection has applications in many fields, such as system health monitoring, fraud detection, and intrusion detection.\n\n![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F3595464%2F4088133a20318f4e47e1e2d738509d12%2F__results___5_0.png?generation=1590869249365044&alt=media)\n"},{"metadata":{},"cell_type":"markdown","source":"# Linkedin luminol\nThere is a great deal of literature on anomaly detection out in the world — from open-source packages like Twitter’s [AnomalyDetection](https://github.com/twitter/AnomalyDetection?source=post_page---------------------------) or Linkedin’s [Luminol](https://github.com/linkedin/luminol/?source=post_page---------------------------), to academic works like Rob Hyndman’s papers on [feature-based anomaly detection](https://robjhyndman.com/publications/oddstream/?source=post_page---------------------------). \n\nLuminol is a lightweight Python library for time series data analysis. The two major functionalities it supports are anomaly detection and correlation. It can be used to investigate possible causes of anomaly.\nGithub link: https://github.com/linkedin/luminol\nLicense: Apache License 2.0\n\n"},{"metadata":{},"cell_type":"markdown","source":"### Import Libs"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport gc\nimport warnings\nwarnings.filterwarnings('ignore')\npd.set_option('max_columns', 150)\npd.set_option('max_rows', 150)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom matplotlib import rcParams\nimport seaborn as sns\nfrom scipy import stats\n#To plot figs on jupyter\n%matplotlib inline\n# figure size in inches\nrcParams['figure.figsize'] = 14,6\n\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nimport warnings\nwarnings.filterwarnings('ignore')\npd.set_option('max_columns', 200)\npd.set_option('max_rows', 200)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Test in the dataset -  Read in data."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"PATH =   r\"/kaggle/input/features/FEATURES-2014-2015/\"\ndf = pd.read_csv(PATH+\"part-067.csv\")\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Glipse Data"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Anomaly value counts"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"df.isAnomaly.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"df[df['isAnomaly']==True].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Visualizations"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Plots the disribution of a variable colored by value of the target\ndef kde_target(var_name, df):\n    \n    # Calculate the correlation coefficient between the new variable and the target\n    corr = df['isAnomaly'].corr(df[var_name])\n    \n    # Calculate medians for repaid vs not repaid\n    avg_highr = df.loc[df['isAnomaly'] == 0, var_name].median()\n    avg_lowr = df.loc[df['isAnomaly'] == 1, var_name].median()\n    \n    plt.figure(figsize = (12, 6))\n    \n    # Plot the distribution for target == 0 and target == 1\n    sns.kdeplot(df.loc[df['isAnomaly'] == 0, var_name], label = 'isAnomaly == 0')\n    sns.kdeplot(df.loc[df['isAnomaly'] == 1, var_name], label = 'isAnomaly == 1')\n    \n    # label the plot\n    plt.xlabel(var_name); plt.ylabel('Density'); plt.title('%s Distribution' % var_name)\n    plt.legend();\n    \n    # print out the correlation\n    print('The correlation between %s and the TARGET is %0.4f' % (var_name, corr))\n    # Print out average values\n    print('Median value for request with high runtime value = %0.4f' % avg_highr)\n    print('Median value for request with low runtime value =     %0.4f' % avg_lowr)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Thread CPU time : (MXBean(java.lang:type=Threading).CurrentThreadCpuTime)"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"    \nkde_target(r'Thread CPU time : (MXBean(java.lang:type=Threading).CurrentThreadCpuTime)', df[[r'Thread CPU time : (MXBean(java.lang:type=Threading).CurrentThreadCpuTime)','isAnomaly']].dropna(),)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Memory space usage : ((MXBean(java.lang:name=Code Cache,type=MemoryPool).Usage.committed / MXBean(java.lang:name=Code Cache,type=MemoryPool).Usage.max))"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"kde_target(r'Memory space usage : ((MXBean(java.lang:name=Code Cache,type=MemoryPool).Usage.committed / MXBean(java.lang:name=Code Cache,type=MemoryPool).Usage.max))', df[[r'Memory space usage : ((MXBean(java.lang:name=Code Cache,type=MemoryPool).Usage.committed / MXBean(java.lang:name=Code Cache,type=MemoryPool).Usage.max))','isAnomaly']].dropna(),)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Heap usage activity : (d/dx (MXBean(java.lang:type=Memory).HeapMemoryUsage.used))"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"kde_target('Heap usage activity : (d/dx (MXBean(java.lang:type=Memory).HeapMemoryUsage.used))', df[['Heap usage activity : (d/dx (MXBean(java.lang:type=Memory).HeapMemoryUsage.used))','isAnomaly']].dropna(),)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Generic Data \n> We prepared three kinds of data sets: 1) jumping mean with constant variance, 2) jumping mean with varying variance,\nand 3) jumping variance with constant mean."},{"metadata":{},"cell_type":"markdown","source":"- dataset 1, the Gaussian random variable have mean 0 and variance = 1 and we let a1 =0,6 and a2 = 0,5. This data\nset consists of 10,000 records. Change points occur at time * 1000 (x=1,2,3...,9).\n\n- The second data set is a data sequence such that each data between change points was drawn according to the AR model ((6), a1 = 0,6 and a2 = 0,5) in which variance\nof the noise term changes gradually over time.\n\n- The third data set is a data sequence such that at each change point, variance suddenly changes, and each data in\na range between the change points is i.i.d. (independently identically distributed).\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"## DATASET 1\nmu, sigma = 0.0, 1.0\nent1 = np.zeros((10000))\nfor i in range(10):\n#     print(mu)\n    for j in range(1000):\n        ent1[1000*i+j] = np.random.normal(mu, sigma)\n    mu = mu + 9 - i\n\na1 = 0.6\na2 = -0.5\nds1 = np.zeros((10000))\nds1[0] = ent1[0]\nds1[1] = ent1[1]\nfor i in range(2,10000):\n    ds1[i] = a1*ds1[i-1] + a2*ds1[i-2] + ent1[i]\n## DATASET 2\nmu = 0.0\nent2 = np.zeros((10000))\nfor i in range(10):\n#     print(mu)\n    for j in range(1000):\n        sigma = 0.1/(0.01 + (10000 - (i*1000 + j))/10000)\n        ent2[1000*i+j] = np.random.normal(mu, sigma)\n    mu = mu + 1\n\na1 = 0.6\na2 = -0.5\nds2 = np.zeros((10000))\nds2[0] = ent1[0]\nds2[1] = ent1[1]\nfor i in range(2,10000):\n    ds2[i] = a1*ds2[i-1] + a2*ds2[i-2] + ent2[i]\n\n## DATASET 3\nmu, sigma1, sigma3 = 0.0, 1.0, 3.0\nds3 = np.zeros((10000))\nfor i in range(10):\n    if i in {0,2,4,6,8}:\n        for j in range(1000):\n            ds3[1000*i+j] = np.random.normal(mu, sigma1)\n    else:\n        for j in range(1000):\n            ds3[1000*i+j] = np.random.normal(mu, sigma3) ","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,4))\nplt.plot(ent1)\nplt.title('Dataset 1')\nplt.ylabel('Values')\nplt.xlabel('Count')\nplt.legend()\n\nplt.figure(figsize=(16,4))\nplt.plot(ent2)\nplt.title('Dataset 2')\nplt.ylabel('Values')\nplt.xlabel('Count')\nplt.legend()\n\nplt.figure(figsize=(16,4))\nplt.plot(ds1)\nplt.title('Dataset 3')\nplt.ylabel('Values')\nplt.xlabel('Count')\nplt.legend()\n\nplt.figure(figsize=(16,4))\nplt.plot(ds2)\nplt.title('Dataset 4')\nplt.ylabel('Values')\nplt.xlabel('Count')\nplt.legend()\n\nplt.figure(figsize=(16,4))\nplt.plot(ds3)\nplt.title('Dataset 5')\nplt.ylabel('Values')\nplt.xlabel('Count')\nplt.legend()\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Luminol \n> Detecting Outliers and Change Points from Time Series\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install luminol\nimport luminol","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from luminol import anomaly_detector,correlator\n\nfrom luminol.anomaly_detector import AnomalyDetector\nfrom luminol.correlator import Correlator\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Luminol - only if module 'luminol was installed'\n#data preprocessing for the framework\n\ndata = np.array(ent1)\nts_s = pd.Series(data)\nts_dict = ts_s.to_dict()\n\ndata2 = np.array(ent2)\nts_s2 = pd.Series(data)\nts_dict2 = ts_s.to_dict()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### conduct anomaly detection on a single time series ts."},{"metadata":{"trusted":true},"cell_type":"code","source":"detector = anomaly_detector.AnomalyDetector(ts_dict)\nanomalies = detector.get_anomalies()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### if there is anomaly, correlate the first anomaly period with a secondary time series ts2."},{"metadata":{"trusted":true},"cell_type":"code","source":"if anomalies:\n    time_period = anomalies[0].get_time_window()\n    correlator = correlator.Correlator(ts_dict, ts_dict2, time_period)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### print the correlation coefficient"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(correlator.get_correlation_result().coefficient)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### score example for the ts1"},{"metadata":{"trusted":true},"cell_type":"code","source":"# score = detector.get_all_scores()\nfor timestamp, value in score.iteritems():\n    print(timestamp, value)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def scoreLuminolALLData(ts_dict):    \n    data = np.array(ts_dict)\n    ts_s = pd.Series(data)\n    ts_dict = ts_s.to_dict()\n\n\n    detector = anomaly_detector.AnomalyDetector(ts_dict)\n    score = detector.get_all_scores()\n    score_v = []\n    for timestamp, value in score.iteritems():\n        score_v.append(value)\n#         print(timestamp, value)\n    return score_v","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataplot1 = scoreLuminolALLData(ent1)    \ndataplot2 = scoreLuminolALLData(ent2) \ndataplot3 = scoreLuminolALLData(ds1)    \ndataplot4 = scoreLuminolALLData(ds2)        \ndataplot5 = scoreLuminolALLData(ds3) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataLUMINOL_dataset1 = np.array(dataplot1)\nfrom scipy import stats\ndataLUMINOL_dataset1 = stats.describe(dataplot1)\ndataLUMINOL_dataset1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"qt25_ds1 = np.percentile(dataplot1, 25)  # Q1\nqt50_ds1 = np.percentile(dataplot1, 50)  # Q2\nqt75_ds1 = np.percentile(dataplot1, 75)  # Q3\nqt25_ds1,qt50_ds1, qt75_ds1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dfLUMINOL_dataset1 = pd.DataFrame(dataplot1, columns=['Score'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plot / Threashold in 75% Quartil"},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"def plot_anomaly_score_low_higt(datascore, data):\n    datascore_ = np.array(datascore)\n    from scipy import stats\n    datascore_ = stats.describe(datascore)\n    \n    datascore_ = pd.DataFrame(datascore, columns=['Score'])\n\n    delta = np.percentile(datascore, 75)\n    print('Threashold ',delta)\n\n    plt.figure(figsize=(16,6))\n    plt.plot(data)\n    plt.title(\"data count\")        \n\n    plt.figure(figsize=(16,6))\n    plt.plot(datascore)\n    plt.title(\"data count\")        \n\n    \n    plt.figure(figsize=(16,6))\n    df_high_data_ = datascore_[datascore_ <= delta]\n    df_high_score_ = datascore_[datascore_ > delta]\n    \n    plt.plot(datascore_.index, datascore_.Score.fillna(1), c='gray', alpha=0.4)\n    plt.scatter(df_high_data_.index, df_high_data_.values, label='Inline', s=10)\n    plt.scatter(df_high_score_.index, df_high_score_.values, label='Outlier', c='red', s=10)\n    plt.margins(x=0.01,y=0.2)\n    plt.title('Anomaly Score ')\n    plt.ylabel('Score')\n    plt.xlabel('Data Count')\n    plt.legend()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### df LUMINOL TESTING dataset1"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_anomaly_score_low_higt(dataplot1, ent1)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plot_anomaly_score_low_higt(dfLUMINOL_dataset1, ent1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### df LUMINOL dataset2"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataLUMINOL_dataset2 = np.array(dataplot2)\nfrom scipy import stats\ndataLUMINOL_dataset2 = stats.describe(dataplot2)\ndataLUMINOL_dataset2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"qt25_ds2 = np.percentile(dataplot2, 25)  # Q1\nqt50_ds2 = np.percentile(dataplot2, 50)  # Q2\nqt75_ds2 = np.percentile(dataplot2, 75)  # Q3\nqt25_ds2,qt50_ds2, qt75_ds2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dfLUMINOL_dataset2 = pd.DataFrame(dataplot2, columns=['Score'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plots/ Threashold in 75%  Quartil"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plot_anomaly_score_low_higt(dfLUMINOL_dataset2, ent2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### df LUMINOL dataset 3"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataLUMINOL_dataset3 = np.array(dataplot3)\nfrom scipy import stats\ndataLUMINOL_dataset3 = stats.describe(dataplot3)\ndataLUMINOL_dataset3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"qt25_ds3 = np.percentile(dataplot3, 25)  # Q1\nqt50_ds3 = np.percentile(dataplot3, 50)  # Q2\nqt75_ds3 = np.percentile(dataplot3, 75)  # Q3\nqt25_ds3,qt50_ds3, qt75_ds3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dfLUMINOL_dataset3 = pd.DataFrame(dataplot3, columns=['Score'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plots/ Threashold in 75%  Quartil"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plot_anomaly_score_low_higt(dfLUMINOL_dataset3, ds1)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### df LUMINOL dataset4"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataLUMINOL_dataset4 = np.array(dataplot4)\nfrom scipy import stats\ndataLUMINOL_dataset4 = stats.describe(dataplot4)\ndataLUMINOL_dataset4","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"qt25_ds4 = np.percentile(dataplot4, 25)  # Q1\nqt50_ds4 = np.percentile(dataplot4, 50)  # Q2\nqt75_ds4 = np.percentile(dataplot4, 75)  # Q3\nqt25_ds4, qt50_ds4, qt75_ds4","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dfLUMINOL_dataset4 = pd.DataFrame(dataplot4, columns=['Score'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plots/ Threashold in 75%  Quartil"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plot_anomaly_score_low_higt(dfLUMINOL_dataset4, ds2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### df LUMINOL dataset5"},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndataLUMINOL_dataset5 = np.array(dataplot5)\nfrom scipy import stats\ndataLUMINOL_dataset5 = stats.describe(dataplot5)\ndataLUMINOL_dataset5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"qt25_ds5 = np.percentile(dataplot5, 25)  # Q1\nqt50_ds5 = np.percentile(dataplot5, 50)  # Q2\nqt75_ds5 = np.percentile(dataplot5, 75)  # Q3\nqt25_ds5, qt50_ds5, qt75_ds5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dfLUMINOL_dataset5 = pd.DataFrame(dataplot5, columns=['Score'])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plots/ Threashold in 75%  Quartil"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plot_anomaly_score_low_higt(dfLUMINOL_dataset5, ds3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Anomaly Detection - Thread CPU time, Memory space usage."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from scipy.stats import norm, skew","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(mu, sigma) = norm.fit(df.loc[df['isAnomaly'] == 1, r'Memory space usage : ((MXBean(java.lang:name=Code Cache,type=MemoryPool).Usage.committed / MXBean(java.lang:name=Code Cache,type=MemoryPool).Usage.max))'])\nprint(\n    'Memory space usage : anomaly {:.1f} and standard deviation = {:.1f}'.format(mu, sigma))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(mu, sigma) = norm.fit(df.loc[df['isAnomaly'] == 1, r'Thread CPU time : (MXBean(java.lang:type=Threading).CurrentThreadCpuTime)'])\nprint(\n    'Thread CPU time : anomaly {:.1f} and standard deviation = {:.1f}'.format(mu, sigma))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# KDE - Thread CPU time"},{"metadata":{},"cell_type":"markdown","source":"Let's create a kernel density estimation (KDE) plot colored by the **Thread CPU time** of the **Anomaly**. A kernel density estimation (KDE) is a non-parametric way to estimate the probability density function of a random variable. It will allow us to identify if there is a correlation between the **Thread CPU time** and **Anomaly**."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,6))\nplt.style.use('seaborn-colorblind')\nplt.grid(True, alpha=0.5)\nsns.kdeplot(df.loc[df['isAnomaly'] == 0, r'Thread CPU time : (MXBean(java.lang:type=Threading).CurrentThreadCpuTime)'], label = 'Inline')\nsns.kdeplot(df.loc[df['isAnomaly'] == 1, r'Thread CPU time : (MXBean(java.lang:type=Threading).CurrentThreadCpuTime)'], label = 'Outlier')\nplt.xlabel('Thread CPU time anomaly analysis')\nplt.xlim(left=0)\nplt.ylabel('Density')\nplt.title('Thread CPU time Distribution in Percent by Anomaly Event');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# KDE - Memory space usage"},{"metadata":{},"cell_type":"markdown","source":"Let's create a kernel density estimation (KDE) plot colored by the **Memory space usage** of the **Anomaly**. A kernel density estimation (KDE) is a non-parametric way to estimate the probability density function of a random variable. It will allow us to identify if there is a correlation between the **Memory space usage** and **Anomaly**."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,6))\nplt.style.use('seaborn-colorblind')\nplt.grid(True, alpha=0.5)\nsns.kdeplot(df.loc[df['isAnomaly'] == 0, r'Memory space usage : ((MXBean(java.lang:name=Code Cache,type=MemoryPool).Usage.committed / MXBean(java.lang:name=Code Cache,type=MemoryPool).Usage.max))'], label = 'Inline')\nsns.kdeplot(df.loc[df['isAnomaly'] == 1, r'Memory space usage : ((MXBean(java.lang:name=Code Cache,type=MemoryPool).Usage.committed / MXBean(java.lang:name=Code Cache,type=MemoryPool).Usage.max))'], label = 'Outlier')\nplt.xlabel('Memory space usage anomaly analysis')\nplt.xlim(left=0)\nplt.ylabel('Density')\nplt.title('Memory space usage Distribution in Percent by Anomaly Event');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"\n\ndf[[r'timestamp','isAnomaly', r'Thread CPU time : (MXBean(java.lang:type=Threading).CurrentThreadCpuTime)', r'Memory space usage : ((MXBean(java.lang:name=Code Cache,type=MemoryPool).Usage.committed / MXBean(java.lang:name=Code Cache,type=MemoryPool).Usage.max))']][df['isAnomaly']==1].head()  \n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Thread CPU time"},{"metadata":{"trusted":true},"cell_type":"code","source":"df[r'Thread CPU time : (MXBean(java.lang:type=Threading).CurrentThreadCpuTime)'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Memory space usage"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Memory space usage : ((MXBean(java.lang:name=Code Cache,type=MemoryPool).Usage.committed / MXBean(java.lang:name=Code Cache,type=MemoryPool).Usage.max))'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cpu_values = df[r'Thread CPU time : (MXBean(java.lang:type=Threading).CurrentThreadCpuTime)'].values\ndataplot_ThreadCPUtime = scoreLuminolALLData(cpu_values)    \n# # dataplot_ThreadCPUtime","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"memory_values = df['Memory space usage : ((MXBean(java.lang:name=Code Cache,type=MemoryPool).Usage.committed / MXBean(java.lang:name=Code Cache,type=MemoryPool).Usage.max))'].values\ndataplot_memoryspaceusage = scoreLuminolALLData(memory_values)    \n# dataplot_memoryspaceusage\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# CPU and Memory use correlation"},{"metadata":{},"cell_type":"markdown","source":"### if there is anomaly, correlate the first anomaly period with a secondary time series ts2.\n"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def correlation_anomaly(ts_dict,ts_dict2,scoreThreshold,period):\n    data = np.array(ts_dict)\n    ts_s = pd.Series(data)\n    ts1 = ts_s.to_dict()\n\n    data2 = np.array(ts_dict2)\n    ts_s2 = pd.Series(data2)\n    ts2 = ts_s2.to_dict()\n#     print('shape ts1, ts2 ',len(ts1), len(ts2))\n    \n    \n    my_detector = AnomalyDetector(ts1)\n    score = my_detector.get_all_scores()\n    anomalies = my_detector.get_anomalies()\n    for a in anomalies[:period]:\n        time_period = a.get_time_window()\n        my_correlator = Correlator(ts1, ts2, time_period)\n        if my_correlator.is_correlated(threshold=scoreThreshold):\n            print(\"ts2 correlate with ts1 at time period (%d, %d)\" % time_period)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def correlation_anomaly(ts_dict,ts_dict2,scoreThreshold, size):\n    from luminol.correlator import Correlator\n    data = np.array(cpu_values)\n    ts_s = pd.Series(data)\n    ts1 = ts_s.to_dict()\n\n\n    data2 = np.array(memory_values)\n    ts_s2 = pd.Series(data2)\n    ts2 = ts_s2.to_dict()\n\n    detector = anomaly_detector.AnomalyDetector(ts1)\n    anomalies = detector.get_anomalies()\n\n    if anomalies:  \n        for a in range(size):\n            time_period = anomalies[a].get_time_window()\n            my_correlator = Correlator(ts1, ts2, time_period)\n#             print(correlator.get_correlation_result().coefficient)\n#             if my_correlator.is_correlated(threshold=scoreThreshold):\n            print(\"CPU  correlate with  MEMORY at time period (%d, %d)\" % time_period)\n            print('CPU x MEMORY correlation ',correlator.get_correlation_result().coefficient, '\\n')\n                                \n\ncorrelation_anomaly(cpu_values, memory_values,0.10, 11)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### df LUMINOL Memory Space Usage \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndataLUMINOL_memoryspaceusage = np.array(dataplot_memoryspaceusage)\nfrom scipy import stats\ndataLUMINOL_memoryspaceusage = stats.describe(dataplot_memoryspaceusage)\ndataLUMINOL_memoryspaceusage\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"qt25_memoryusage = np.percentile(dataplot_memoryspaceusage, 25)  # Q1\nqt50_memoryusage = np.percentile(dataplot_memoryspaceusage, 50)  # Q2\nqt75_memoryusage = np.percentile(dataplot_memoryspaceusage, 75)  # Q3\nqt25_memoryusage, qt50_memoryusage, qt75_memoryusage\ndataLUMINOL_memoryspaceusage = pd.DataFrame(dataplot_memoryspaceusage, columns=['Score'])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Plots/ Threashold in 75%  Quartil"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plot_anomaly_score_low_higt(dataplot_memoryspaceusage, df[['Memory space usage : ((MXBean(java.lang:name=Code Cache,type=MemoryPool).Usage.committed / MXBean(java.lang:name=Code Cache,type=MemoryPool).Usage.max))']].values)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Memory space usage"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,6))\ndf_high_data_ = df[[r'timestamp','isAnomaly', r'Thread CPU time : (MXBean(java.lang:type=Threading).CurrentThreadCpuTime)', r'Memory space usage : ((MXBean(java.lang:name=Code Cache,type=MemoryPool).Usage.committed / MXBean(java.lang:name=Code Cache,type=MemoryPool).Usage.max))']][df['isAnomaly']==1][ r'Memory space usage : ((MXBean(java.lang:name=Code Cache,type=MemoryPool).Usage.committed / MXBean(java.lang:name=Code Cache,type=MemoryPool).Usage.max))']\ndf_high_score_ = df[[r'timestamp','isAnomaly', r'Thread CPU time : (MXBean(java.lang:type=Threading).CurrentThreadCpuTime)', r'Memory space usage : ((MXBean(java.lang:name=Code Cache,type=MemoryPool).Usage.committed / MXBean(java.lang:name=Code Cache,type=MemoryPool).Usage.max))']][df['isAnomaly']==0][ r'Memory space usage : ((MXBean(java.lang:name=Code Cache,type=MemoryPool).Usage.committed / MXBean(java.lang:name=Code Cache,type=MemoryPool).Usage.max))']\n\n    \nplt.plot(df.index, df.isAnomaly.fillna(1), c='gray', alpha=0.4)\nplt.scatter(df_high_data_.index, df_high_data_.values, label='Inline', s=30)\nplt.scatter(df_high_score_.index, df_high_score_.values, label='Outlier', c='red', s=10)\nplt.margins(x=0.01,y=0.2)\nplt.title('Values Grand truth ')\nplt.ylabel('Values')\nplt.xlabel('Data Count')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n### df LUMINOL Memory Space Usage \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataLUMINOL_ThreadCPUtime = np.array(dataplot_ThreadCPUtime)\nfrom scipy import stats\ndataLUMINOL_ThreadCPUtime = stats.describe(dataplot_ThreadCPUtime)\ndataLUMINOL_ThreadCPUtime\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"qt25_ThreadCPUtime = np.percentile(dataplot_ThreadCPUtime, 25)  # Q1\nqt50_ThreadCPUtime = np.percentile(dataplot_ThreadCPUtime, 50)  # Q2\nqt75_ThreadCPUtime = np.percentile(dataplot_ThreadCPUtime, 75)  # Q3\nqt25_ThreadCPUtime, qt50_ThreadCPUtime, qt75_ThreadCPUtime\ndataLUMINOL_ThreadCPUtime = pd.DataFrame(dataplot_ThreadCPUtime, columns=['Score'])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Plots/ Threashold in 75%  Quartil"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plot_anomaly_score_low_higt(dataplot_ThreadCPUtime, df[['Thread CPU time : (MXBean(java.lang:type=Threading).CurrentThreadCpuTime)']].values)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Thread CPU time"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,6))\ndf_high_data_ = df[[r'timestamp','isAnomaly', r'Thread CPU time : (MXBean(java.lang:type=Threading).CurrentThreadCpuTime)', r'Memory space usage : ((MXBean(java.lang:name=Code Cache,type=MemoryPool).Usage.committed / MXBean(java.lang:name=Code Cache,type=MemoryPool).Usage.max))']][df['isAnomaly']==1][ r'Thread CPU time : (MXBean(java.lang:type=Threading).CurrentThreadCpuTime)']\ndf_high_score_ = df[[r'timestamp','isAnomaly', r'Thread CPU time : (MXBean(java.lang:type=Threading).CurrentThreadCpuTime)', r'Memory space usage : ((MXBean(java.lang:name=Code Cache,type=MemoryPool).Usage.committed / MXBean(java.lang:name=Code Cache,type=MemoryPool).Usage.max))']][df['isAnomaly']==0][ r'Thread CPU time : (MXBean(java.lang:type=Threading).CurrentThreadCpuTime)']\n\n    \nplt.plot(df.index, df.isAnomaly.fillna(1), c='gray', alpha=0.4)\nplt.scatter(df_high_data_.index, df_high_data_.values, label='Inline', s=40)\nplt.scatter(df_high_score_.index, df_high_score_.values, label='Outlier', c='red', s=10)\nplt.margins(x=0.01,y=0.2)\nplt.title('Values Grand truth ')\nplt.ylabel('Values')\nplt.xlabel('Data Count')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Propeth Anomaly Detection\n\n> see this: (Anomaly detection in time series with Prophet library)[https://towardsdatascience.com/anomaly-detection-time-series-4c661f6f165f]"},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"!pip install fbprophet\nfrom fbprophet import Prophet\nimport os","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# View the data as a table\ndf_ = pd.DataFrame(df, columns=['timestamp', r'Thread CPU time : (MXBean(java.lang:type=Threading).CurrentThreadCpuTime)'])\ndf_['ds']=df_['timestamp']\ndf_['y']=df_[r'Thread CPU time : (MXBean(java.lang:type=Threading).CurrentThreadCpuTime)'].astype(float)\ndf_=df_.drop(['timestamp',r'Thread CPU time : (MXBean(java.lang:type=Threading).CurrentThreadCpuTime)'],axis=1)\ndf_.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Propeth Prediction"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def fit_predict_model(dataframe, interval_width = 0.99, changepoint_range = 0.8):\n    m = Prophet(daily_seasonality = False, yearly_seasonality = False, weekly_seasonality = False,\n#                 seasonality_mode = 'multiplicative', \n                interval_width = interval_width,\n                changepoint_range = changepoint_range)\n    m = m.fit(dataframe)\n    \n    forecast = m.predict(dataframe)\n    forecast['fact'] = dataframe['y'].reset_index(drop = True)\n    print('Displaying Prophet plot')\n    fig1 = m.plot(forecast)\n    return forecast\n    \npred = fit_predict_model(df_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred.tail()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Detecting Anomalies:\n- The light blue boundaries in the above graph are yhat_upper and yhat_lower.\n- If y value is greater than yhat_upper and less than yhat lower then it is an anomaly.\n- Also getting the importance of that anomaly based on its distance from yhat_upper and yhat_lower."},{"metadata":{},"cell_type":"markdown","source":"Detect Anomalies"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def detect_anomalies(forecast):\n    forecasted = forecast[['ds','trend', 'yhat', 'yhat_lower', 'yhat_upper', 'fact']].copy()\n    #forecast['fact'] = df['y']\n\n    forecasted['anomaly'] = 0\n    forecasted.loc[forecasted['fact'] > forecasted['yhat_upper'], 'anomaly'] = 1\n    forecasted.loc[forecasted['fact'] < forecasted['yhat_lower'], 'anomaly'] = 1 #-1\n\n    #anomaly importances\n    forecasted['importance'] = 0\n    forecasted.loc[forecasted['anomaly'] ==1, 'importance'] = \\\n        (forecasted['fact'] - forecasted['yhat_upper'])/forecast['fact']\n    forecasted.loc[forecasted['anomaly'] ==-1, 'importance'] = \\\n        (forecasted['yhat_lower'] - forecasted['fact'])/forecast['fact']\n    \n    return forecasted\n\npred = detect_anomalies(pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred[ r'anomaly'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":false,"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,6))\ndf_outlier_ = pred[[r'ds','anomaly', r'yhat']][pred['anomaly']==1][ r'yhat']\n# df_outlier = pred[[r'ds','anomaly', r'yhat']][pred['anomaly']==-1][ r'yhat']\n\ndf_inline_ = pred[[r'ds','anomaly', r'yhat']][pred['anomaly']==0][ r'yhat']\n\nplt.plot(pred.index, pred.anomaly.fillna(1), c='gray', alpha=0.4)\nplt.scatter(df_inline_.index, df_inline_.values, label='Inline', s=40)\nplt.scatter(df_outlier_.index, df_outlier_.values, label='Outlier', c='red', s=10)\n# plt.scatter(df_outlier.index, df_outlier.values, label='Outlier', c='black', s=10)\nplt.margins(x=0.01,y=0.2)\nplt.title('Values Grand truth ')\nplt.ylabel('Values')\nplt.xlabel('Data Count')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plt.figure(figsize=(16,6))\ndf_high_data_ = df[[r'timestamp','isAnomaly', r'Thread CPU time : (MXBean(java.lang:type=Threading).CurrentThreadCpuTime)', r'Memory space usage : ((MXBean(java.lang:name=Code Cache,type=MemoryPool).Usage.committed / MXBean(java.lang:name=Code Cache,type=MemoryPool).Usage.max))']][df['isAnomaly']==1][ r'Memory space usage : ((MXBean(java.lang:name=Code Cache,type=MemoryPool).Usage.committed / MXBean(java.lang:name=Code Cache,type=MemoryPool).Usage.max))']\ndf_high_score_ = df[[r'timestamp','isAnomaly', r'Thread CPU time : (MXBean(java.lang:type=Threading).CurrentThreadCpuTime)', r'Memory space usage : ((MXBean(java.lang:name=Code Cache,type=MemoryPool).Usage.committed / MXBean(java.lang:name=Code Cache,type=MemoryPool).Usage.max))']][df['isAnomaly']==0][ r'Memory space usage : ((MXBean(java.lang:name=Code Cache,type=MemoryPool).Usage.committed / MXBean(java.lang:name=Code Cache,type=MemoryPool).Usage.max))']\n\n    \nplt.plot(df.index, df.isAnomaly.fillna(1), c='gray', alpha=0.4)\nplt.scatter(df_high_data_.index, df_high_data_.values, label='Inline', s=30)\nplt.scatter(df_high_score_.index, df_high_score_.values, label='Outlier', c='red', s=10)\nplt.margins(x=0.01,y=0.2)\nplt.title('Values Grand truth ')\nplt.ylabel('Values')\nplt.xlabel('Data Count')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import plotly.graph_objs as go\nimport plotly as py\nfrom plotly.offline import init_notebook_mode\ninit_notebook_mode(connected=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Thread CPU time results"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"trace = go.Scatter(\n        name = 'Thread CPU time',\n       mode = 'markers',\n       x = list(df['timestamp']),\n       y = list(df['Thread CPU time : (MXBean(java.lang:type=Threading).CurrentThreadCpuTime)']),\n       marker=dict(\n              color='#FFBAD2',\n              line=dict(width=1)\n       )\n)\ntrace1 = go.Scatter(\n    name = 'trend',\n       mode = 'lines',\n       x = list(pred['ds']),\n       y = list(pred['yhat']),\n       marker=dict(\n              color='red',\n              line=dict(width=1)\n       )\n)\nupper_band = go.Scatter(\n    name = 'upper band',\n        mode = 'lines',\n        x = list(pred['ds']),\n        y = list(pred['yhat_upper']),\n        line= dict(color='#57b88f'),\n        fill = 'tonexty'\n)\nlower_band = go.Scatter(\n    name= 'lower band',\n        mode = 'lines',\n        x = list(pred['ds']),\n        y = list(pred['yhat_lower']),\n        line= dict(color='#1705ff')\n)\ndata = [trace, trace1, lower_band, upper_band]\nlayout = dict(title='Thread CPU time Estimation',\n             xaxis=dict(title = 'Dates', ticklen=2, zeroline=False))\nfigure=dict(data=data,layout=layout)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"py.offline.iplot(figure)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Remember the upvote button is next to the fork button, and it's free too! ;)\n\n### Don't hesitate to give your suggestions in the comment section"},{"metadata":{},"cell_type":"markdown","source":"## End Notebook"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}