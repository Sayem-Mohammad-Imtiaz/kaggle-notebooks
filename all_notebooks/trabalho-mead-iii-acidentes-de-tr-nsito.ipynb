{"cells":[{"metadata":{},"cell_type":"markdown","source":"----\n# ANÁLISE ESTATÍSTICA DOS ACIDENTES NAS RODOVIAS FEDERAIS BRASILEIRAS \n\n* Leandro Alencar – 1931133007\n* Maycon Alves – 1931133015\n* Nilson Michiles - 1931133032\n\n## Table of contents\n* [Préprocessamento de Dados](#1)\n* [Análises Descritivas e Estatísticas](#2)\n* [Análises de Componentes Principais](#3)\n* [K-Means (K-médias)](#4)\n* [Série Temporal](#5)\n----"},{"metadata":{},"cell_type":"markdown","source":"# Préprocessamento de Dados <a name=\"1\"></a>\nComo etapa inicial, será necessário realizar o préprocessamento dos dados a fim de remover eventuais registros duplicados, remover ou substituir registros nulos e aplicar a correta formatação dos dados (data, númerico, categórico, etc).\n\n> Foram encontrados registros nulos com o campo escrito \"(null)\", foi necessário desenvolver função customizada para, também, atender a essas peculiaridades.\n\nA função em seguida tem por objetivo:\n- remover registros duplicados;\n- substituir valores nulos numéricos por -1 (e também valores de string como \"(null)\" em colunas numéricas que foram encontrados);\n- valores nulos qualitativos pelo registro mais frequente, dada que a quantidade encontrada foi mínima (máximo de 30 registros \"(null)\" por coluna)\n- manter e ordenar as colunas que aparecem em todos os datasets - os anos de 2017 a 2019 possuem as colunas \"latitude\", \"longitude\", \"regional\", \"delegacia\" e \"uop\", que não constam nos datasets anteriores."},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"# Bibliotecas\nimport os\nimport pandas as pd\nimport re\nimport unidecode\nimport numpy as np\nimport warnings\nimport seaborn as sns\nsns.set()\nsns.set_context(\"paper\")\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def cleanDF(df) -> pd.DataFrame:\n    # Remover Duplicados\n    df = df.drop_duplicates(keep='first')\n    \n    # Transformar coluna KM e BR, em que há strings, em float e substituir \"(null)\" por -1\n    if df['km'].dtype == 'object':\n        df['km'] = df['km'].str.replace(r'\\(null\\)', '-1.0')\n        df['km'] = df['km'].str.replace(',', '.').apply(float)\n        \n    if df['br'].dtype == 'object':\n        df['br'] = df['br'].str.replace(r'\\(null\\)', '-1.0')\n        df['br'] = df['br'].str.replace(',', '.').apply(float)\n        \n    else:\n        pass\n\n    # Formatar para minusculo, remover acentos e espacos\n    for col in df.select_dtypes(include='object').columns:\n        if df[col].isna().sum() > 0:\n            df[col] = df[col].replace(np.nan, df[col].value_counts().idxmax())\n        else:\n            pass\n        try:\n            df[col] = df[col].apply(lambda x: unidecode.unidecode(x).lower().strip())\n        except:\n            print('Error in col: ', col)\n    \n    # Formatar Data YYYY-MM-DD\n    df['data_inversa'] = pd.to_datetime(df['data_inversa'], dayfirst=True)\n    df['ano'] = df['data_inversa'].dt.year\n    df['mes'] =  df['data_inversa'].dt.month\n    \n    \n    # Ordenação das colunas\n    cols = ['id', 'data_inversa', 'dia_semana', 'horario', 'uf', 'br', 'km',\n       'municipio', 'causa_acidente', 'tipo_acidente',\n       'classificacao_acidente', 'fase_dia', 'sentido_via',\n       'condicao_metereologica', 'tipo_pista', 'tracado_via', 'uso_solo',\n       'pessoas', 'mortos', 'feridos_leves', 'feridos_graves', 'ilesos',\n       'ignorados', 'feridos', 'veiculos','ano','mes']\n    \n    df = df[cols]\n    return df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Importação, tratamento dos dados e concatenação"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"%%time\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    dfs = list()\n    for filename in filenames:\n        try:\n            df = pd.read_csv(os.path.join(dirname, filename), sep=';', encoding='latin1', low_memory=False)\n            dfs.append(cleanDF(df))\n        except:\n            print(\"Error in file: \", filename)\n      \nacidentes_df = pd.concat(dfs, ignore_index=True )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Exploração quanto a ocorrências de \"Missings\""},{"metadata":{"trusted":true},"cell_type":"code","source":"acidentes_df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Tratamento dos missings"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nacidentes_df = acidentes_df.replace(np.nan, -1)\n\nobj_nulls = list()\nfor col in acidentes_df.select_dtypes(include='object').columns:\n    if acidentes_df[col].str.contains('null').sum() > 0:\n        obj_nulls.append(col)\n        print(col)\n    else:\n        pass","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Onde a UF era missing, foi utilizado o valor em que o municipio era o mesmo e a UF não era nulo para a correção:"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfor mun in acidentes_df[acidentes_df[obj_nulls[0]].str.contains('null')]['municipio']:\n    acidentes_df[obj_nulls[0]][acidentes_df['municipio'] == mun] = acidentes_df[acidentes_df['municipio'] == mun][obj_nulls[0]].mode()[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"acidentes_df[acidentes_df['causa_acidente'] == '(null)']['causa_acidente'] = 'outras'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Nos demais, foram substituidos pelo valor mais frequente."},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfor col in obj_nulls[1:]:\n    acidentes_df[col][acidentes_df[col].str.contains('null')] = acidentes_df[col].mode()[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Dataframe sem missings"},{"metadata":{"trusted":true},"cell_type":"code","source":"acidentes_df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"acidentes_df = acidentes_df.sort_values(by='data_inversa')\nacidentes_df.head(1).T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline\nacidentes_df.hist(figsize=(20,10))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature Engineering\nCriação de uma coluna chamada \"regiao\" para possibilitar análises de dados agrupados."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"def regiao(x):\n    \n    if x in ['al' , 'ba', 'ce', 'ma', 'pb', 'pe', 'pi', 'rn','se']:\n        return 'nordeste'\n    if x in ['ac' , 'ap', 'am', 'pa', 'ro', 'rr', 'to']:\n        return 'norte'\n    if x in ['df' , 'go', 'ms', 'mt']:\n        return 'centro oeste'\n    if x in ['es' , 'mg', 'sp', 'rj']:\n        return 'sudeste'\n    if x in ['pr' , 'sc', 'rs']:\n        return 'sul'\n    \n\n\nacidentes_df['regiao'] = acidentes_df['uf'].transform(regiao)\n\n# Removendo a causa de acidente \"outras\", pois não informa muita coisa.\nacidentes_df = acidentes_df[acidentes_df['causa_acidente'] != 'outras']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#gerando output do DF para gerar a MCA - Analise de correspondencia multipla no R\nacidentes_df.to_csv('acidentes_df.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Análise Descritiva Exploratória <a name=\"2\"></a>"},{"metadata":{"trusted":false},"cell_type":"code","source":"%%javascript\nIPython.OutputArea.prototype._should_scroll = function(lines) {\n    return false;\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Caso nao queira rodar toda a etapa de tratamento dos dados\n#acidentes_df = pd.read_csv('./kaggle/acidentes_df_csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#import pandas_profiling as pp\n#pp.ProfileReport(acidentes_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Análise Estatística"},{"metadata":{"trusted":false},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(20, 10))\n\nax = sns.countplot(x=\"regiao\", hue=\"causa_acidente\", data=acidentes_df[acidentes_df['regiao'] == 'norte'], palette=\"Set3\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(20, 10))\nax = sns.countplot(x=\"regiao\", hue=\"causa_acidente\", data=acidentes_df[acidentes_df['regiao'] == 'nordeste'], palette=\"Set3\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(20, 10))\nax = sns.countplot(x=\"regiao\", hue=\"causa_acidente\", data=acidentes_df[acidentes_df['regiao'] == 'centro oeste'], palette=\"Set3\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(20, 10))\nax = sns.countplot(x=\"regiao\", hue=\"causa_acidente\", data=acidentes_df[acidentes_df['regiao'] == 'sul'], palette=\"Set3\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(20, 10))\nax = sns.countplot(x=\"regiao\", hue=\"causa_acidente\", data=acidentes_df[acidentes_df['regiao'] == 'sudeste'], palette=\"Set3\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Análise de Componentes Principais <a name=\"3\"></a>"},{"metadata":{},"cell_type":"markdown","source":"A Análise de Componentes Principais (em inglês PCA) é o nome comum dado à técnica que usa princípios de álgebra linear para transformar variáveis, possivelmente correlacionadas, em um número menor de variáveis chamadas de Componentes Principais.\n\nIremos analisar as causas mais comuns de acidente por região no Brasil."},{"metadata":{},"cell_type":"markdown","source":"## Causa de acidentes x Região"},{"metadata":{},"cell_type":"markdown","source":"### Criando tabela cruzada entre região e causas de acidentes."},{"metadata":{"trusted":false},"cell_type":"code","source":"centro_oeste = acidentes_df[acidentes_df['regiao'] == 'centro oeste']","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"centro_oeste = centro_oeste[['causa_acidente', 'municipio']]\ncausa_acidentes_x_municipio = pd.crosstab(centro_oeste['causa_acidente'], centro_oeste['municipio'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(causa_acidentes_x_municipio.shape)\ncausa_acidentes_x_municipio","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Sem realizar ajustes lineares para cada par de dados fica quase impossível visualizar algum padrão ou tendência nos dados acima. Esse é um caso onda a PCA pode ajudar. \n\nDefinindo uma função para normalizar os dados."},{"metadata":{"trusted":false},"cell_type":"code","source":"def z_score(x):\n    \"\"\"Remove a média e normaliza os pelo desvio padrão\"\"\"\n    return (x - x.mean()) / x.std()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.decomposition import PCA\n\n\npca = PCA(n_components=None)\nscore = pca.fit_transform(causa_acidentes_x_municipio.apply(z_score))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"loadings = pd.DataFrame(pca.components_)\nloadings.index =   ['PC %s' % pc for pc in loadings.index + 1]\nloadings.columns = causa_acidentes_x_municipio.columns\nloadings","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"PCs = np.dot(loadings.values.T, causa_acidentes_x_municipio)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"font = {'family' : 'monospace',\n        'weight' : 'normal',\n        'size'   : 14}\n\nplt.rc('font', **font)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" ### Plotando a primeira dimensão(PC1)"},{"metadata":{"trusted":false},"cell_type":"code","source":"marker = dict(linestyle='none', marker='o', markersize=7, color='blue', alpha=0.5)\n\nfig, ax = plt.subplots(figsize=(16, 8))\n\nax.plot(PCs[0], np.zeros_like(PCs[0]), label=\"Scores\", **marker)\n\n[ax.text(x, y, t) for x, y, t in zip(PCs[0], loadings.values[0, :], loadings.columns)]\n\nax.set_xlabel(\"PC1\")\n\n_ = ax.set_ylim(-1, 1)\nmarker = dict(linestyle='none', marker='o', markersize=7, color='blue', alpha=0.5)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Facilmente vemos que a região sul e sudeste têm uma causa de acidente diferente do norte, centro oeste e nordeste, esses três últimos parecem se agrupar em um grupo de causa de acidente similar."},{"metadata":{},"cell_type":"markdown","source":" ### Plotando a primeira e segunda dimensão(PC1 e PC2)"},{"metadata":{},"cell_type":"markdown","source":"Agora vamos plotar a primeira e segunda PCs juntas. Esse tipo de gráfico é chamado de Score plot."},{"metadata":{"trusted":false},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(16, 8))\n\nax.plot(PCs[0], PCs[1], label=\"Scores\", **marker)\n\nax.set_xlabel(\"PC1\")\nax.set_ylabel(\"PC2\")\n\ntext = [ax.text(x, y, t) for x, y, t in zip(PCs[0], PCs[1], loadings.columns)]\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Note que na \"segunda\" dimensão estão as diferenças entre as três regiões que agrupamos no gráfico anterior."},{"metadata":{"trusted":false},"cell_type":"code","source":"perc = pca.explained_variance_ratio_ * 100\n\nperc = pd.DataFrame(perc, columns=['Percentual de razão explicada'], index=['PC %s' % pc for pc in np.arange(len(perc)) + 1])\nax = perc.plot(kind='bar')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Em geral se busca componentes o suficiente para explicar entre 70-80% dos dados. nota-se que utlizamos mais que 70% da variância dos dados. Saímos de uma dimensão de (28 x 228) para uma dimensão de (2x5)."},{"metadata":{"trusted":false},"cell_type":"code","source":"marker = dict(linestyle='none', marker='o', markersize=7, color='blue', alpha=0.5)\n\nfig, ax = plt.subplots(figsize=(10, 5))\nax.plot(loadings.iloc[:, 0], loadings.iloc[:, 1], label=\"Loadings\", **marker)\nax.set_xlabel(\"non-projected PC1\")\nax.set_ylabel(\"non-projected PC2\")\nax.axis([-1, 1, -1, 1])\ntext = [ax.text(x, y, t) for x, y, t in zip(loadings.iloc[:, 0], loadings.iloc[:, 1], causa_acidentes_x_municipio.index)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Outro gráfico comum para explorar os resultados é o Loadings plot ou seja, a influência de cada variável original nas componentes principais. Note que velocidade incompatível, defeito mecânico em veículo, desobediência de sinalização, não guardar distância de segurança e falta de atenção a condução se destacam da aglomeração central."},{"metadata":{},"cell_type":"markdown","source":"# K-Means <a name=\"4\"></a>"},{"metadata":{"trusted":false},"cell_type":"code","source":"def describe_cluster(variavel, cluster_id):\n    \n    for x in range(0, cluster_id):\n        \n        \n        font = {'family' : 'monospace',\n        'weight' : 'normal',\n        'size'   : 22}\n\n        plt.rc('font', **font)\n    \n        fig, ax = plt.subplots(figsize=(26, 10))\n\n        plt.subplot(1, 2, 1)\n\n        summary = pd.DataFrame(variavel[variavel['cluster_id'] == x].describe()).T\n        summary.columns = ['Quantidade', 'Média', 'Desvio Padrão', 'Minímo','25%','50%','75%', 'Máximo']\n        summary = summary.T\n\n        table = plt.table(cellText=summary.values,\n                  rowLabels=summary.index,\n                  colLabels=summary.columns,\n                              cellLoc = 'right', rowLoc = 'center',\n          loc='right', bbox=[.1,.05,1.3,.95])\n\n        plt.axis('off')\n\n        plt.title(\"Descrição do Cluster \" + str(x) + \" - Quantidade de Acidentes \")\n        table.set_fontsize(22)\n        table.scale(3, 3)  \n        \n        plt.subplot(1, 2, 2)\n        plt.title(\"Amostra dos Municípios do Cluster \" + str(x))\n        quantidade = variavel[variavel['cluster_id'] == x]['cluster_id'].count()\n        if  quantidade < 10:\n            sample = variavel[variavel['cluster_id'] == x].sample(quantidade).index\n        else: \n            sample = variavel[variavel['cluster_id'] == x].sample(10).index\n            \n        table = plt.table(cellText=pd.DataFrame(sample).values,\n                          cellLoc = 'right', rowLoc = 'center',\n          loc='top',\n                          bbox=[.25,.55,.45,.45])\n\n        plt.axis('off')\n        # may help","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Importar o algoritimo/modelo\nfrom sklearn.cluster import KMeans","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"causa_acidentes_x_regiao_t = causa_acidentes_x_municipio.T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### K-Means - Falta de Atenção x Ingestão de Alcool"},{"metadata":{"trusted":false},"cell_type":"code","source":"W = causa_acidentes_x_regiao_t[['falta de atencao', 'ingestao de alcool']]\n\nfig, ax = plt.subplots(figsize=(16, 5))\n\n\nsummary = pd.DataFrame(W.describe()).T\nsummary.columns = ['Quantidade', 'Média', 'Desvio Padrão', 'Minímo','25%','50%','75%', 'Máximo']\nsummary = summary.T\n\ntable = plt.table(cellText=summary.values,\n          rowLabels=summary.index,\n          colLabels=summary.columns,\n          cellLoc = 'right', rowLoc = 'center',\n          loc='right', bbox=[.1,.05,.75,.75])\n\nplt.axis('off')\n\ntable.set_fontsize(22)\ntable.scale(3, 3)  # may help","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Método Elbow\n# Cálculo do SSE - Sum of Squared Erros\nsse = []\n\nfor k in range(1, 15):\n    kmeans = KMeans(n_clusters=k, random_state=42).fit(W)\n    sse.append(kmeans.inertia_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Plotando o gráfico\nimport matplotlib.pyplot as plt\n\nplt.plot(range(1, 15), sse, 'bx-')\nplt.title('Método Elbow')\nplt.xlabel('Número de clusters')\nplt.ylabel('SSE')\nplt.xticks(range(1, 15))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Vamos usar 3 clusters\nkmeans = KMeans(n_clusters=4, random_state=42)\ncluster_id = kmeans.fit_predict(W)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Agora vamos guardar os resultados no dataframe\nW['cluster_id'] = cluster_id","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"describe_cluster(W, 4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Plotando os agrupamentos e os centroídes\nfig, ax = plt.subplots(figsize=(10, 5))\n\n\n\nsns.scatterplot(x=\"falta de atencao\", y=\"ingestao de alcool\", hue=\"cluster_id\", data=W, s=200, palette=\"viridis\")\nplt.setp(ax.get_legend().get_texts(), fontsize='15') # for legend text\n\n\n\nax.scatter(kmeans.cluster_centers_[:,0] ,\n           kmeans.cluster_centers_[:,1], \n           color='red', \n           marker=\"x\", s=100)\n\n\n#text = [ax.text(x, y, t) for x, y, t in zip(W.values[:,0], \n#                                            W.values[:,1], \n#                                            W.values[:,2])]\n\n\n\nplt.xlabel('Falta de Atenção')\nplt.ylabel('Ingestão de Alcool')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### K-Means - Falta de Atenção x Velocidade Incompatível"},{"metadata":{"trusted":false},"cell_type":"code","source":"X = causa_acidentes_x_regiao_t[['falta de atencao', 'velocidade incompativel']]\nfig, ax = plt.subplots(figsize=(16, 5))\n\n\nsummary = pd.DataFrame(X.describe()).T\nsummary.columns = ['Quantidade', 'Média', 'Desvio Padrão', 'Minímo','25%','50%','75%', 'Máximo']\nsummary = summary.T\n\ntable = plt.table(cellText=summary.values,\n          rowLabels=summary.index,\n          colLabels=summary.columns,\n          cellLoc = 'right', rowLoc = 'center',\n          loc='right', bbox=[.1,.05,.75,.75])\n\nplt.axis('off')\n\ntable.set_fontsize(22)\ntable.scale(3, 3)  # may help","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Método Elbow\n# Cálculo do SSE - Sum of Squared Erros\nsse = []\n\nfor k in range(1, 15):\n    kmeans = KMeans(n_clusters=k, random_state=42).fit(X)\n    sse.append(kmeans.inertia_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Plotando o gráfico\nimport matplotlib.pyplot as plt\n\nplt.plot(range(1, 15), sse, 'bx-')\nplt.title('Método Elbow')\nplt.xlabel('Número de clusters')\nplt.ylabel('SSE')\nplt.xticks(range(1, 15))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Vamos usar 3 clusters\nkmeans = KMeans(n_clusters=4, random_state=42)\ncluster_id = kmeans.fit_predict(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Agora vamos guardar os resultados no dataframe\nX['cluster_id'] = cluster_id\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"describe_cluster(X, 4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Plotando os agrupamentos e os centroídes\nfig, ax = plt.subplots(figsize=(10, 5))\n\n\n\nsns.scatterplot(x=\"falta de atencao\", y=\"velocidade incompativel\", hue=\"cluster_id\", data=X, s=300, palette=\"viridis\")\nplt.setp(ax.get_legend().get_texts(), fontsize='15') # for legend text\n\n\nax.scatter(kmeans.cluster_centers_[:,0] ,\n           kmeans.cluster_centers_[:,1], \n           color='red', \n           marker=\"x\", s=100)\n\n\n\nplt.xlabel('Falta de Atenção')\nplt.ylabel('Velocidade Incompatível')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### K-Means - Não guardar distância de segurança x Velocidade incompatível"},{"metadata":{"trusted":false},"cell_type":"code","source":"Y = causa_acidentes_x_regiao_t[['nao guardar distancia de seguranca', 'velocidade incompativel']]\n\nfig, ax = plt.subplots(figsize=(16, 5))\n\n\nsummary = pd.DataFrame(Y.describe()).T\nsummary.columns = ['Quantidade', 'Média', 'Desvio Padrão', 'Minímo','25%','50%','75%', 'Máximo']\nsummary = summary.T\n\ntable = plt.table(cellText=summary.values,\n          rowLabels=summary.index,\n          colLabels=summary.columns,\n          cellLoc = 'right', rowLoc = 'center',\n          loc='right', bbox=[.1,.05,.75,.75])\n\nplt.axis('off')\n\ntable.set_fontsize(22)\ntable.scale(3, 3)  # may help","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Método Elbow\n# Cálculo do SSE - Sum of Squared Erros\nsse = []\n\nfor k in range(1, 15):\n    kmeans = KMeans(n_clusters=k, random_state=42).fit(Y)\n    sse.append(kmeans.inertia_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Plotando o gráfico\nimport matplotlib.pyplot as plt\n\nplt.plot(range(1, 15), sse, 'bx-')\nplt.title('Método Elbow')\nplt.xlabel('Número de clusters')\nplt.ylabel('SSE')\nplt.xticks(range(1, 15))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Vamos usar 3 clusters\nkmeans = KMeans(n_clusters=4, random_state=42)\ncluster_id = kmeans.fit_predict(Y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Agora vamos guardar os resultados no dataframe\nY['cluster_id'] = cluster_id\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"describe_cluster(Y, 4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Plotando os agrupamentos e os centroídes\nfig, ax = plt.subplots(figsize=(10, 5))\n\n\n\nsns.scatterplot(x=\"nao guardar distancia de seguranca\", y=\"velocidade incompativel\", hue=\"cluster_id\", data=Y, s=300, palette=\"viridis\")\nplt.setp(ax.get_legend().get_texts(), fontsize='15') # for legend text\n\n\nax.scatter(kmeans.cluster_centers_[:,0] ,\n           kmeans.cluster_centers_[:,1], \n           color='red', \n           marker=\"x\", s=100)\n\n\n\nplt.xlabel('Não guardar distância de segurança')\nplt.ylabel('Velocidade incompatível')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Levando em considereção as causas de acidentes: Não guardar distância de segurança. Velocidade Incompatível, Falta de Atenção e Ingestão de Alcool.\n\nFacilmente vemos que Norte e Centro-Oeste têm uma quantidade de acidentes bem menor que Nordeste, Sudeste e Sul, dessa forma se agrupam em um cluster que tem como caracteristica o menor numero de acidentes por essas causas.\n\nSul e Sudeste se agrupam pois tem valores altos de acidentes por essas causas, sendo que nessa região é onde se possuem mais rodovias pavimentadas.\n\nNordeste fica em um grupo sozinho. tem quantidade de acidentes por essas causas abaixo de sul e sudeste e acima de norte e centro-oeste."},{"metadata":{},"cell_type":"markdown","source":"#### K-Means - PC 1 x PC 2"},{"metadata":{"trusted":false},"cell_type":"code","source":"pcs = pd.DataFrame(PCs)\n\npcs.columns = loadings.columns\n\nZ = pcs.T[[0, 1]]\n\nfig, ax = plt.subplots(figsize=(16, 5))\n\n\nsummary = pd.DataFrame(Z.describe()).T\nsummary.columns = ['Quantidade', 'Média', 'Desvio Padrão', 'Minímo','25%','50%','75%', 'Máximo']\nsummary = summary.T\n\ntable = plt.table(cellText=summary.values,\n          rowLabels=summary.index,\n          colLabels=summary.columns,\n          cellLoc = 'right', rowLoc = 'center',\n          loc='right', bbox=[.1,.05,.75,.75])\n\nplt.axis('off')\n\ntable.set_fontsize(22)\ntable.scale(3, 3)  # may help","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Método Elbow\n# Cálculo do SSE - Sum of Squared Erros\nsse = []\n\nfor k in range(1, 15):\n    kmeans = KMeans(n_clusters=k, random_state=42).fit(Z)\n    sse.append(kmeans.inertia_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Plotando o gráfico\nimport matplotlib.pyplot as plt\n\nplt.plot(range(1, 15), sse, 'bx-')\nplt.title('Método Elbow')\nplt.xlabel('Número de clusters')\nplt.ylabel('SSE')\nplt.xticks(range(1, 15))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Vamos usar 3 clusters\nkmeans = KMeans(n_clusters=4, random_state=42)\ncluster_id = kmeans.fit_predict(Z)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Agora vamos guardar os resultados no dataframe\nZ['cluster_id'] = cluster_id","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"describe_cluster(Z, 4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Plotando os agrupamentos e os centroídes\nfig, ax = plt.subplots(figsize=(10, 5))\n\n\n\nsns.scatterplot(x=0, y=1, hue=\"cluster_id\", data=Z, s=300, palette=\"viridis\")\nplt.setp(ax.get_legend().get_texts(), fontsize='15') # for legend text\n\n\nax.scatter(kmeans.cluster_centers_[:,0] ,\n           kmeans.cluster_centers_[:,1], \n           color='red', \n           marker=\"x\", s=100)\n\n\n\n\nplt.xlabel('PC 1')\nplt.ylabel('PC 2')\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Realizando um agrupamento por componentes principais, vemos que Nordeste, Norte e Centro-Oeste já ficam no mesmo grupo, Sul e Sudeste ficam agora em grupos diferentes. como os componentes principais leva em consideração todas as causas de acidentes, trás uma maior compreensão da similaridade de acidentes entre as regiões."},{"metadata":{},"cell_type":"markdown","source":"# Série Temporal <a name=\"5\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Bilbiotecas\n!pip install pmdarima\nimport statsmodels.api as sm\nfrom pmdarima.arima import auto_arima\nimport datetime as dt","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Iremos realizar a previsão da quantidade de acidentes com vítimas para o ano de 2020 por meio de um modelo auto-regressivo integrado de médias móveis sazonal, em inglês chamado de SARIMA (Seasonal Autoregressive Integrated Moving Average). Séries Temporais, em geral, podem ser classificadas como aditivas ou multiplicativas mas ambas possuem os mesmos 3 componentes:\n\nTendência, que representa a direção geral como os dados se desenvolvem ao longo do tempo;\nSazonalidade, padrões de como os dados mudam em relação a período determinado;\nErro, são variações irregulares não explicadas pela tendência ou sazonalidade.\nNo caso de uma Séria Temporal Aditiva, um modelo pode ser explicado pela fórmula:\n\nY[t] = T[t] + S[t] + e[t]\n\n> Onde:\n- Y[t] : Saída do Modelo\n- T[t] : Componente de Tendência do modelo\n- S[t] : Componente de Sazonalidade do modelo\n- e[t] : erro ou resíduo\n\nAntes, façamos uma breve análise exploratória dos dados de acidentes totais e acidentes com vítimas."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Acidentes Time Series Centro Oeste\nacidentes_ts = acidentes_df[(acidentes_df['regiao'] == 'centro oeste')]\nacidentes_ts['com_vitimas'] = np.where((acidentes_ts['mortos'] > 0) |\n                                       (acidentes_ts['feridos'] > 0) |\n                                       (acidentes_ts['feridos_leves'] > 0) |\n                                       (acidentes_ts['feridos_graves'] > 0),\n                                       1,0)\n\nacidentes_ts['mes'] = acidentes_ts['data_inversa'].dt.month.apply(lambda x: \"{:02d}\".format(x))\nacidentes_ts['ano'] = acidentes_ts['data_inversa'].dt.year.astype(str)\n\n# Acidentes com Vitimas\nacidentes_cvit = acidentes_ts.groupby(['mes','ano'])['com_vitimas'].sum().reset_index()\nacidentes_cvit['dia'] = '01'\nacidentes_cvit['data'] = pd.to_datetime(acidentes_cvit['ano']+acidentes_cvit['mes']+acidentes_cvit['dia'])\nacidentes_cvit = acidentes_cvit[['data','com_vitimas']].set_index('data').sort_values('data')\n\n# Acidentes Totais\nacidentes_tot = acidentes_ts.groupby(['mes','ano'])['com_vitimas'].count().reset_index()\nacidentes_tot['dia'] = '01'\nacidentes_tot['data'] = pd.to_datetime(acidentes_tot['ano']+acidentes_tot['mes']+acidentes_tot['dia'])\nacidentes_tot = acidentes_tot[['data','com_vitimas']].set_index('data').rename(columns={'com_vitimas':'total'}).sort_values('data')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = acidentes_tot.plot(title='Série Histórica', figsize=(20,10))\nacidentes_cvit.plot(ax=ax)\n\nplt.title('Acidentes nas rodovias do Centro-Oeste | 2009-2019', fontsize=15, rotation=0)\nplt.xticks(fontsize=15, rotation=0)\nplt.yticks(fontsize=15)\nplt.xlabel('')\n\nax.legend(['Total de Acidentes', 'Acidentes com Vítimas'], fontsize=14)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Pode-se perceber que houve redução no número total de acidentes com a evolução do tempo, porém o número de acidentes com vítimas não acompanhou essa redução, aumentando do índice de acidentes com vítimas. \n\nNo gráfico abaixo, fica mais evidente o aumento progressivo no número total de acidentes com vítimas. Houve redução no período de 2013 a 2016, porém um grande aumento em 2017 e tendência de crescimento  futuro, após sua leve queda em 2018."},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"contagem = acidentes_ts.groupby('ano')['com_vitimas'].sum().reset_index().set_index('ano')\n\nfig, ax = plt.subplots(1,1, figsize=(20,10))\n\nplt.scatter(x=contagem.index, y=contagem['com_vitimas'], color='purple', linewidths=5, zorder=2)\nplt.plot(contagem['com_vitimas'], linewidth=3, zorder=1)\nplt.xticks(fontsize=18)\nplt.tick_params(axis='y', which='both', left=False, labelleft=False)\nplt.title('Total de Acidentes com Vítimas por Ano', size=20)\n\nfor data, count in zip(contagem.index, contagem.values): \n    ax.annotate(count[0]\n               ,xytext=(data, count+50)\n               ,fontsize=15 \n               ,xy=(data, count)\n               )\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Mortalidade nos Acidentes\nA mortalidade dos acidentes nas rodovias do Centro-Oeste mostrou-se baixa em comparação com o total de acidentes com vítimas, e demonstra um comportamento, aparentemente, estável."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Mortos\nacidentes_mort = acidentes_ts.groupby(['mes','ano'])['mortos'].sum().reset_index()\nacidentes_mort['dia'] = '01'\nacidentes_mort['data'] = pd.to_datetime(acidentes_mort['ano']+acidentes_mort['mes']+acidentes_mort['dia'])\nacidentes_mort = acidentes_mort[['data','mortos']].set_index('data').sort_values('data')\n\nax = acidentes_cvit.plot(title='Série Histórica', figsize=(20,10))\nacidentes_mort.plot(ax=ax)\n\nplt.title('Mortalidade nos Acidentes em rodovias do Centro-Oeste | 2009-2019', fontsize=15, rotation=0)\nplt.xticks(fontsize=15, rotation=0)\nplt.yticks(fontsize=15)\nplt.xlabel('')\n\nax.legend(['Acidentes com Vítimas', 'Acidentes com Óbitos'], fontsize=14)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Decomposição da Série Temporal\n\nÉ possível, e muito importante, analisarmos esses três componentes separadamente de modo a avaliar melhor o comportamento individual de cada item, para isso iremos realizar sua decomposição. Como nossos dados tratam-se de observações mensais, a frequência passada como parâmetro será 12, o que significa que para cada ponto iremos analisar a média dos 6 meses anteriores e 6 posteriores. Por meio da decomposição, é possível identificar que nossos dados possuem uma clara tendência de crescimento, e quanto à sazonalidade, o gráfico demonstra que existe efeito sazonal com picos máximos e mínimos nos meses de Dezembro e Fevereiro, respectivamente."},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"res = sm.tsa.seasonal_decompose(acidentes_cvit,freq=12)\ndef plot_decompose(res):\n    fig, axes = plt.subplots(ncols=1, nrows=4, sharex=True, figsize=(16,12))\n    \n    res.observed.plot(ax=axes[0], legend=False)\n    axes[0].set_ylabel('Observado')\n    \n    res.trend.plot(ax=axes[1], legend=False)\n    axes[1].set_ylabel('Tendência')\n    \n    res.seasonal.plot(ax=axes[2], legend=False)\n    axes[2].set_ylabel('Sazonalidade')\n    \n    axes[2].annotate('Fevereiro',fontsize=10 \n            ,xytext=(dt.datetime(2010,5,1), -33)\n            ,xy=(dt.datetime(2010,2,1), -31) \n            ,arrowprops=dict(arrowstyle=\"->\", connectionstyle=\"arc3\", color='black'))\n\n    axes[2].annotate('Dezembro', fontsize=10\n            ,xytext=(dt.datetime(2011,3,1), 40)\n            ,xy=(dt.datetime(2010,12,1), 41)\n           ,arrowprops=dict(arrowstyle=\"->\", connectionstyle=\"arc3\", color='black'))\n    \n    res.resid.plot(ax=axes[3], legend=False)\n    axes[3].set_ylabel('Resíduo')\n    \n    plt.xlabel('')\n    \nplot_decompose(res)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Seleção dos melhores parâmetros com o \"auto_arima\" <a name=\"4\"></a>\nDando continuidade às previsões, iremos selecionar o modelo automaticamente através da função \"auto_arima\", que irá nos entregar os melhores componentes para o modelo ARIMA e suas estatísticas.\n\nEm geral, as duas estatísticas levadas em consideração para a seleção do modelo, de tal forma que quanto menor o valor, melhor, são:\n\n- AIC: Akaike Information Criteria\n- BIC: Bayesian Information Criteria"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"%%time\nstep_wise=auto_arima(acidentes_cvit,\n                     start_p=0, start_q=0, \n                     max_p=3, max_q=3,\n                     start_P=0, start_Q=0, \n                     max_P=3, max_Q=3,\n                     d=1, max_d=1,\n                     D=1, max_D=1,\n                     m=12,\n                     trace=True, \n                     error_action='ignore', \n                     suppress_warnings=True, \n                     stepwise=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"step_wise.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Ajustando o modelo SARIMA <a name=\"5\"></a>\nApós as diversas iterações que duraram aproximadamente 6 minutos, o modelo indicado foi o \"SARIMAX(0, 1, 1)x(0, 1, 1, 12)\". Observe que o \"X\" em SARIMA\"X\" exite pois é possível utilizar uma séria \"exógena\", ou externa, mas não será o nosso caso. Iremos aplicar o modelo SARIMA (Seasonal Autoregressive Integrated Moving Average)."},{"metadata":{"trusted":false},"cell_type":"code","source":"model = sm.tsa.statespace.SARIMAX(acidentes_cvit\n                                 ,order=(0, 1, 1)\n                                 ,seasonal_order=(0, 1, 1, 12)\n                                 ,enforce_stationarity=False\n                                 ,enforce_invertibility=False\n                                 )\nresults = model.fit()\nprint(results.summary()) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Aqui nossa principal preocupação deve ser de garantir que os resíduos do modelo não são correlacionados e igualmente distribuídos. Se isto não for alcançado, é indicativo de que os parâmetros podem ser melhorados.\n\nNo gráfico do canto superior direito, quando a linha vermelha do KDE (Kernel Density Estimation) está próxima da linha verde N(0,1), que significa \"Distribuição Normal com média 0 e desvio padrão 1\", é um bom indicativo de que os resíduos são igualmente distribuídos.\n\nNo canto inferior esquerdo, o gráfico mostra que a distribuição ordenada dos resíduos seguem a linha de tendência das amostras tiradas de uma distribuição normal com N(0, 1), com alguns desvios.\n\nNo gráfico de resíduos ao longo do tempo, no canto superior esquerdo, não há nenhuma sazonalidade aparente e parece se tratar de \"ruído branco\". O conceito de ruído branco significa que os dados são aleatórios e não podem ser preditos, pois não seguem um padrão. Isto é confirmado pelo \"correlograma\", no canto inferior direito, que mostra que os resíduos possuem baixa correrelação com os próprios lags."},{"metadata":{"trusted":false},"cell_type":"code","source":"results.plot_diagnostics(figsize=(16, 8))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Validando as previsões <a name=\"6\"></a>\nJá ajustamos o modelo e agora podemos utilizá-lo para realizar previsões. Iremos começar comparando os valores preditos com os valores reais para nos auxiliar a entender sua precisão.\n\nIremos utilizar **\"get_predition\"** a partir de uma data **\"X\"** e com um intervalo de confiança **\"conf_int\"**."},{"metadata":{"trusted":false},"cell_type":"code","source":"pred = results.get_prediction(start=pd.to_datetime('2015-01-01'), dynamic=False)\npred_ci = pred.conf_int()\n\nfig, ax = plt.subplots(figsize=(20, 8))\n\nacidentes_cvit.plot(ax=ax)\npred.predicted_mean.plot(ax=ax, alpha=0.8, color='r')\n\nax.legend(['Histórico', 'Previsão'], fontsize=15)\n\nax.fill_between(pred_ci.index,\n                pred_ci.iloc[:, 0],\n                pred_ci.iloc[:, 1], color='k', alpha=0.07)\nax.set_xlabel('')\nax.set_ylabel('Preço')\nplt.xticks(fontsize=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"y_forecasted = pred.predicted_mean\ny_truth = acidentes_cvit.loc['2015-01-01':, 'com_vitimas']\n\n# Compute the mean square error\nmse = ((y_forecasted - y_truth) ** 2).mean()\nprint('O Erro Quadrático Médio é {}'.format(round(mse, 2)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Realizando e Visualizando Previsões <a name=\"7\"></a>\nChegamos ao ponto principal da nossa análise, realizar as previões. Para isso iremos utilizar o atributo **\"get_forecast\"** que consegue computar os valores previstos com **N** passos a frente."},{"metadata":{"scrolled":false,"trusted":false},"cell_type":"code","source":"pred_uc = results.get_forecast(steps=12)\npred_ci = pred_uc.conf_int()\n\nfig, ax = plt.subplots(figsize=(20, 8))\n\nax = acidentes_cvit.plot(ax=ax)\npred_uc.predicted_mean.plot(ax=ax, color='g')\n\nax.legend(['Histórico', 'Previsão'], fontsize=15)\n\nax.fill_between(pred_ci.index,\n                pred_ci.iloc[:, 0],\n                pred_ci.iloc[:, 1], color='k', alpha=.06)\nax.set_xlabel('')\nax.set_ylabel('Preço')\nplt.xticks(fontsize=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":false},"cell_type":"code","source":"# Intervalo de Confiança e valor médio\npred_ci['Previsão Média'] = (pred_ci.iloc[:, 0] +  pred_ci.iloc[:, 1]) / 2\npred_ci","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Previsão 2020\nApós realizados todos os passos e encontrados os valores inferiores e superiores do intervalo de confiança, podemos dizer que previsão do número total de acidentes com vítimas para o ano de 2020 será de, aproximadamente:\n\n>**6.940** acidentes com vítimas.\n\nSeguindo a tendência de crescimento que fora identificada nos gráficos da análise exploratória."},{"metadata":{"trusted":false},"cell_type":"code","source":"round(pred_ci['Previsão Média'].sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Visualização Completa\nAbaixo iremos visualizar a ilustração gráfica de todo histórico conhecido mais a previzão realizada pelo modelo SARIMAX, em verde."},{"metadata":{"scrolled":false,"trusted":false},"cell_type":"code","source":"fig, ax = plt.subplots(1,1, figsize=(20,10))\n\npreds = pd.DataFrame(pd.concat([acidentes_cvit, pred_ci['Previsão Média']]).sum(axis=1)).rename(columns={0:'com_vitimas'})\nplt.plot(preds, linewidth=2, zorder=1)\nplt.plot(preds.iloc[-12:,0], linewidth=3, color='y')\nax.legend(['Histórico', 'Previsão'], fontsize=15)\nplt.yticks(fontsize=15)\nplt.xticks(fontsize=15)\n\nplt.axvline(x=\"2020-01-01\", ymin=0, ymax=700, ls='--')\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"nbformat":4,"nbformat_minor":4}