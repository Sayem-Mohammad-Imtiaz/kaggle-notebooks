{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/input/carla-driver-behaviour-dataset/full_data_carla.csv\",index_col=0)\ndata.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**In this section we filter data into label batches. We made it to use it on Conv1d layers. The input of Conv Layers are (20,6). 20 is timesteps, 6 is features. So, the dataset have to divide 20 and remaining must be 0.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data_1 = data[data['class']=='apo']\ndata_2 = data[data['class']=='gonca']\ndata_3 = data[data['class']=='onder']\ndata_4 = data[data['class']=='berk']\ndata_5 = data[data['class']=='selin']\ndata_6 = data[data['class']=='hurcan']\ndata_7 = data[data['class']=='mehdi']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**A funct to drop residual rows into mini dataframes.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data_7.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def dataTuner(data):\n    residual = data.shape[0]%20\n    data = data.drop(data.index[-residual:])\n    return data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Lets concentate the mini datas into one dataframe**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data_full = pd.DataFrame()\nfor i in [data_1,data_2,data_3,data_4,data_5,data_6]:\n    i = dataTuner(i)\n    print(i.shape)\n    data_full = pd.concat([data_full,i],ignore_index=True)\ndata_full = pd.concat([data_full,data_7],ignore_index=True)\ndata_full.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Splitting data into Features and Labels**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"x = data_full.drop([\"class\"],axis=1)\ny = data_full[\"class\"].values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Data Normalization speeds up training time. So, I normalized the data.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nx = StandardScaler().fit_transform(x)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**The labels in the format of string. I convert it from string to integer.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\ny = LabelEncoder().fit_transform(y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**The input of neural network is (20,6). And, we will use conv layers. So, we have to convert our data from 2D to 3D.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"x = pd.DataFrame(x)\nx = np.asarray(x).reshape(-1,20,6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = np.array(y).reshape(-1,20)\ny = pd.DataFrame(y).iloc[:,0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**I will use categorical crossentropy as loss function. So, I convert labels to right format**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.python.keras.utils.np_utils import to_categorical\ny = to_categorical(y, num_classes=7)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**The model I builded contain Conv, LSTM, Dense layers.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.python.keras.models import Sequential\nfrom tensorflow.python.keras.layers import Dense,LSTM,Conv1D,BatchNormalization,Activation\nfrom tensorflow.python.keras.layers import Dropout\nwith tf.device(\"/GPU:0\"):\n    print(\"gpu is ok\")\n    model = Sequential()\n    \n    model.add(Conv1D(filters=64, kernel_size=4, input_shape=(20,6),padding='same'))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    \n    model.add(Conv1D(filters=64, kernel_size=4,padding='same'))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    \n    model.add(Conv1D(filters=64, kernel_size=4,padding='same'))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    \n    model.add(Conv1D(filters=64, kernel_size=4,padding='same'))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    \n    model.add(LSTM(128, return_sequences=True))\n    model.add(BatchNormalization())\n    model.add(Activation('tanh'))\n    \n    model.add(LSTM(128))\n    model.add(BatchNormalization())\n    model.add(Activation('tanh'))\n    \n    model.add(Dense(128, kernel_initializer='random_uniform'))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))    \n\n    model.add(Dense(128, kernel_initializer='random_uniform'))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))    \n    \n    model.add(Dense(7, kernel_initializer='random_uniform',activation='softmax'))\n    \n    \n    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n    model.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**BEST PLACE IS FITTING PLACE :))**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"hist = model.fit(x , y , epochs=480    , validation_data=(x,y) )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**COOL. FINALLY I ACHIEVED GOOD RESULTS....**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nplt.plot(hist.history['accuracy'])\nplt.plot(hist.history['val_accuracy'])\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}