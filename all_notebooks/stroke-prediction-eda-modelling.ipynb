{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# loading dataframe\ndf = pd.read_csv(\"../input/stroke-prediction-dataset/healthcare-dataset-stroke-data.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"****EDA****","metadata":{}},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## cleaning missing values\ndf.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[df.duplicated()] ## no duplicates","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#df['ever_married'] = df['ever_married'].map({'Yes': 1, 'No': 0})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['gender'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#df['gender'] = df['gender'].replace({'Female': 1, 'Male': 0, 'Other': 2})\n# 2 - Other\n# 1 - Female\n# 0 - Male","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.work_type.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.Residence_type.value_counts()\n# almost equal number of people from rural and urban areas in the ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['avg_glucose_level'].describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['bmi'].describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['smoking_status'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['stroke'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## handling missing data - BMI - by Prediction through decision tree - variables used: age and gneder\n\n\nfrom sklearn.tree import DecisionTreeRegressor,DecisionTreeClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler,LabelEncoder\n\n\n\nDT_bmi_pipeline = Pipeline(steps = [\n    ('scale', StandardScaler()),\n    ('lr', DecisionTreeRegressor(random_state = 42))\n])\n\n\nX = df[['age', 'gender', 'bmi']].copy()\nX.gender = X.gender.replace({'Male': 0, 'Female': 1, 'Other': -1})\n\nX.gender = X.gender.astype(np.uint8)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Separting those values that have Missing BMI\n\n# test set would contain all bmi missing 201 values\nMissing = X[X.bmi.isna()]\n\n# train set would contain all non-missing bmi values\nX_train = X[~X.bmi.isna()]\nY_train = X_train.pop('bmi')\n\n# Fit regressor\nDT_bmi_pipeline.fit(X_train, Y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Predicting missing BMI and substituting in the original dataset\npredicted_bmi = pd.Series(DT_bmi_pipeline.predict(Missing[['age', 'gender']]), index = Missing.index)\npredicted_bmi","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# substituting in original dataset\ndf.loc[Missing.index, 'bmi'] = predicted_bmi","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## PLOTS TO UNDERSTAND CORRELATIONS","metadata":{}},{"cell_type":"code","source":"df.corr(method = 'pearson')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"variables = [variable for variable in df.columns]\n\nconts = ['age','avg_glucose_level','bmi']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"variables","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nfig = plt.figure(figsize=(12, 12), facecolor='#f6f6f6')\ngs = fig.add_gridspec(4, 3)\ngs.update(wspace=0.1, hspace=0.4)\ngs","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr = df.corr()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mask = np.triu(np.ones_like(corr, dtype=bool))\nmask","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(11, 9))\n\nimport seaborn as sns\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0, square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Age, hypertension, heartdisease, ever_married, avg_glucose_level are the most correlated","metadata":{}},{"cell_type":"code","source":"# singular plots\n\n\n# hypertension\n# heart_disease\n# ever_married\n# gender\n# stroke\n# smoking\n\n\nsns.set_theme(style = 'darkgrid')\n#plot_hypertension = sns.countplot(data = df, x = 'hypertension')\n#plot_heart_disease = sns.countplot(data = df, x = 'heart_disease')\nplot = sns.countplot(data = df, x = 'work_type')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# For Avergae Glucose level\nfig = plt.figure(figsize=(7,7))\nsns.displot(df.avg_glucose_level, color=\"green\", label=\"avg_glucose_level\", kde= True)\nplt.legend()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# For BMI distribution\nfig = plt.figure(figsize=(10,10))\nsns.displot(df.bmi, color = 'blue', label = 'BMI', kde = True)\nplt.legend()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Stroke vs No Stroke Avergae Glucose Level\nplt.figure(figsize=(12,10))\n\nsns.distplot(df[df['stroke'] == 0][\"bmi\"], color='green') # No Stroke - green\nsns.distplot(df[df['stroke'] == 1][\"bmi\"], color='red') # Stroke - Red\n\nplt.title('Stroke vs No Stroke BMI', fontsize = 15)\nplt.xlim([10,100])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12,10))\nsns.distplot(df[df['stroke']==0]['avg_glucose_level'], color = 'green')\nsns.distplot(df[df['stroke'] == 1]['avg_glucose_level'], color = 'red')\nplt.title('Stroke vs No stroke Average Glucose Level', fontsize = 15)\nplt.xlim([30,300]) ## limit of x-axis\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Scatter Plot to see 3 variable variation","metadata":{}},{"cell_type":"code","source":"fig = plt.figure(figsize=(7,7))\ngraph = sns.scatterplot(data = df, x = \"age\", y = \"bmi\", hue = \"gender\")\n## to draw a line within the graph\ngraph.axhline(y = 25, linewidth = 4, color = \"r\", linestyle = '--')\ngraph.axhline()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize=(7,7))\ngraph = sns.scatterplot(data= df, x = \"age\", y = \"avg_glucose_level\", hue=\"gender\")\ngraph.axhline(y = 150, linewidth = 4, color = \"r\", linestyle = \"--\")\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"According to the graph, all people above glucose level 150 and above BMI 25 are considered as having more risk to Stroke","metadata":{}},{"cell_type":"code","source":"# With respect to smokers and non-smokers\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize=(7,7))\ngraph = sns.scatterplot(data = df, x = \"smoking_status\", y = \"avg_glucose_level\")\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pair plot - all possible scatter plots\nfig = plt.figure(figsize=(10,10))\nsns.pairplot(df)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Preprocessing","metadata":{}},{"cell_type":"code","source":"# Getting predictor and the target variables separate\nx = df.iloc[:, 1:-1].values\nx","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = df.iloc[:, -1].values\ny","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## One hot encoding for categorical variables: gender, work_type, smoking_status\n## Label encoding of binary variables: ever_married, residence_type","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.rename(columns= {'Residence_type': 'residence_type'}, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['residence_type'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# one hot encoding\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\n\nohe = OneHotEncoder()\n\nct = ColumnTransformer(transformers= [('encoder', ohe, [0])], remainder = 'passthrough')\nx = np.array(ct.fit_transform(x))\nx","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = np.delete(x, 2, 1) # deleting the 3rd column so as to delete one gender one hot encoded variable to prevent multicollinearity","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x[0][6]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# one hot encoding of work_type\nct = ColumnTransformer(transformers = [('encoder', ohe, [6])], remainder = 'passthrough')\nx = np.array(ct.fit_transform(x))\nx","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"This should be private: \", x[0][2])\nprint(\"This should also be private: \", x[2][2])\nprint(\"This should also be private: \", x[3][2])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Where does the work_type end\n# 0,1: gender\n# 2,3,4,5,6: work_type\nx[0][7] # age","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# drop number 6 of work_type\nx = np.delete(x, 6, 1)\nx[0][6]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# what is the current index of smoking status\nx[0][13]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# One hot encoding of smoking status\nct = ColumnTransformer(transformers = [('encoder', ohe, [13])], remainder = 'passthrough')\nx = np.array(ct.fit_transform(x))\nx","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 0,1 : gender\n# 2,3,4,5: work_type\n# 6,7,8,9: smoking status\nx = np.delete(x, 9, 1)\nx.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Final Tally:\n\n### 0,1 index are gender\n### 2,3,4,5 are work_type\n### 6,7,8: smoking status\n\n### Ellimination of one hot encoded variable per predictor is done to avoid Multicollineartity","metadata":{}},{"cell_type":"code","source":"# Label Encoding\ni,j = np.where(x == \"Urban\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"i # row number","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"j # Column number","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Index 15 is ever_married and index 16 is residence_type\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nx[:, 12] = le.fit_transform(x[:, 12])\nx[:, 13] = le.fit_transform(x[:, 13])\nx","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Shape x: \", x.shape) \nprint(\"Shape y: \", y.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Number of records in training set: \", x_train.shape)\nprint(\"Shape of training set's labels: \", y_train.shape)\nprint(\"Number of records in test set: \", x_test.shape)\nprint(\"Shape of test set's labels: \", y_test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature Scaling\n\n##### StandardScaler does normalization (x-x_bar)/std_dev --> converts variables into normal dist. with mean 0 and standard dev = 1","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nx_train = sc.fit_transform(x_train)\nx_test = sc.transform(x_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Handling Imbalanced Data using SMOTE","metadata":{}},{"cell_type":"code","source":"print(\"Shape of training data: {} and {}\".format(x_train.shape, y_train.shape))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### SMOTE is Synthetic Minority Oversampling Technique - overcoming the overfitting problem and class imbalance problem by generating synthetic minority records by random over-sampling","metadata":{}},{"cell_type":"code","source":"from imblearn.over_sampling import SMOTE\nsm = SMOTE(random_state = 0)\nx_train_res, y_train_res = sm.fit_resample(x_train, y_train.ravel())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train_res.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Shape of training data after synthetic oversampling: {} and {} \".format(x_train_res.shape, y_train_res.shape))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Number of label 1 records before over_sampling: \", sum(y_train == 1))\nprint(\"Number of label 0 records before over_sampling: \", sum(y_train == 0))\nprint(\"\\n\")\nprint(\"Number of label 1 records before SMOTE: \", sum(y_train_res == 1))\nprint(\"Number of label 0 records after SMOTE: \", sum(y_train_res == 0))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train_res","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Testing out Baseline Classification Models before Tuning","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# importing all metrics\nfrom sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay,precision_score, recall_score, f1_score, classification_report, roc_auc_score, roc_curve, plot_roc_curve, precision_recall_curve, plot_precision_recall_curve, auc, average_precision_score\n\nfrom sklearn.model_selection import cross_val_score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"models_array = []\n\n## Adding it in an array so that could be made into a dataframe later\nmodels_array.append(['Logistic Regression', LogisticRegression(random_state = 0)])\nmodels_array.append(['K Nearest Neighbours', KNeighborsClassifier()])\nmodels_array.append(['SVM Classifier', SVC(random_state=0)])\nmodels_array.append(['Gaussian Naive Bayes', GaussianNB()])\nmodels_array.append(['Bernoulli Naive Bayes', BernoulliNB()])\nmodels_array.append(['Decision Tree Classifier', DecisionTreeClassifier(random_state = 0)])\nmodels_array.append(['Random Forest', RandomForestClassifier(random_state = 0)])\nmodels_array.append(['XGBoost Classifier', XGBClassifier(eval_metric = 'error',use_label_encoder=False)])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"list1= []\n\nfor m in range(len(models_array)):\n    list2 = []\n    model = models_array[m][1]\n    model.fit(x_train_res, y_train_res)\n    y_pred = model.predict(x_test)\n    \n    # confusion matrix\n    cm = confusion_matrix(y_test, y_pred)\n    \n    # training accuracies - K-fold cross validation\n    accuracies = cross_val_score(estimator = model, X = x_train_res, y = y_train_res, cv = 10)\n    \n    # Accuracy score of test set\n    accuracy = accuracy_score(y_test, y_pred)\n    \n    # Area Score under Reciever Operating Characteristic Curve\n    roc = roc_auc_score(y_test, y_pred) \n    \n    # Precision\n    precision = precision_score(y_test, y_pred)\n    \n    # Recall\n    recall = recall_score(y_test, y_pred)\n    \n    # F1-score\n    f1 = f1_score(y_test, y_pred)\n    \n    # Printing the results for every model\n    print(models_array[m][0], \":\")\n    print(\"Training set mean accuracy: {:.2f} %\".format(accuracies.mean()*100))\n    print(\"Training set standard deviation: {:.2f} %\".format(accuracies.std()*100))\n    print(\"Test set accuracy: {:.2f} %\".format(accuracy*100))\n    print(\"Confusion Matrix: \", cm)\n    print(\"Precision: {:.2f} %\".format(precision*100))\n    print(\"Recall: {:.2f} %\".format(recall*100))\n    print(\"F1 score: {:.2f}\".format(f1))\n    print(\"AUROC score: {:.2f}\".format(roc))\n    print(\"-----------------------------------------\")\n    print(\"\\n\")\n    \n    \n    ## Adding all scores to a list so that we can form a dataframe later\n    list2.append(models_array[m][0])\n    list2.append(accuracies.mean()*100)\n    list2.append(accuracies.std()*100)\n    list2.append(accuracy)\n    list2.append(precision)\n    list2.append(recall)\n    list2.append(f1)\n    list2.append(roc)\n    \n    list1.append(list2) ## now list1 contains all the information of the results of the models - this can be put in a data frame\n    \n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From above results we can see that on training and test set the following models performed well:\n1. KNN\n2. Decision Tree\n3. Random Forest\n4. XGBoost\n\n\nThe following performed poorly:\n1. SVM\n2. Logistics Regression\n3. Naive Bayes (both Gausian and Bernoulli)\n\nHowever, before eliminating any models, we should try to select the best hyperparameter and see the performance of each","metadata":{}},{"cell_type":"code","source":"## Adding the results in a dataframe\ndf_results = pd.DataFrame(list1, columns = ['Model','Training Mean Accuracy', 'Training Standard Deviation', 'Test Accuracy', 'Precision', 'Recall', 'F1 Score', 'AUC Value'])\ndf_results.sort_values(by = ['Test Accuracy', 'Training Mean Accuracy', 'AUC Value'], inplace=True, ascending = False)\ndf_results","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Tuning Models","metadata":{}},{"cell_type":"code","source":"# Grid search to select hyper-parameters \nfrom sklearn.model_selection import GridSearchCV","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Hyperparameter C is used to prevent overfitting and provide regularization strength:\n#### For more depth, I refered to this stackoverflow link: https://stackoverflow.com/questions/22851316/what-is-the-inverse-of-regularization-strength-in-logistic-regression-how-shoul\n\n\n#### var_smoothing in Gaussian Naive Bayes artificially adds a user-defined standard deviation on how many samples to be included near or away from the mean in a gaussian distribution. Link: https://stackoverflow.com/questions/58046129/can-someone-give-a-good-math-stats-explanation-as-to-what-the-parameter-var-smoo\n\n\n#### cv in grid search means the number of cross validation we have for each set of hyperparameters\n","metadata":{}},{"cell_type":"code","source":"grid_models = [\n    (LogisticRegression(), [{'C':[0.25, 0.5, 0.75, 1], 'random_state':[0]}]),\n    #(KNeighborsClassifier(), [{'n_neighbors': [5,7,8,10], 'metric': ['euclidian', 'manhattan', 'chebyshev', 'minkowski']}]),\n    (SVC(), [{'C': [0.25, 0.5, 0.75, 1], 'kernel': ['linear', 'rbf'], 'random_state':[0]}]),\n    (GaussianNB(), [{'var_smoothing': [1e-09]}]),\n    (BernoulliNB(), [{'alpha': [0.25, 0.5, 1]}]),\n    (DecisionTreeClassifier(), [{'criterion': ['gini', 'entropy'], 'random_state': [0]}]),\n    (RandomForestClassifier(), [{'n_estimators': [100, 150, 200], 'criterion': ['gini', 'entropy'], 'random_state': [0]}]),\n    (XGBClassifier(), [{'learning_rate': [0.01, 0.05, 0.1], 'eval_metric': ['error'], 'use_label_encoder': [False]}])\n]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i,j in grid_models:\n    grid = GridSearchCV(estimator = i, param_grid = j, scoring = 'accuracy', cv = 10)\n    grid.fit(x_train_res, y_train_res)\n    best_accuracy = grid.best_score_\n    best_param = grid.best_params_\n    \n    print(\"{}: \", i)\n    print(\"Best Accuracy:  {:.2f} %\".format(best_accuracy*100))\n    print(\"Best parameters selected: \", best_param)\n    print(\"------------------------------\")\n    print(\"\\n\")\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Random Forest and XGBoost after hyperparmarater tuning","metadata":{}},{"cell_type":"markdown","source":"## Random Forest","metadata":{}},{"cell_type":"code","source":"# Fitting the random forest model with the above hyper parameters obtained by grid search\n\nclassifier = RandomForestClassifier(criterion = 'entropy', n_estimators = 150, random_state = 0)\nclassifier.fit(x_train_res, y_train_res)\ny_pred = classifier.predict(x_test)\ny_prob = classifier.predict_proba(x_test)[:,1] #probability with which the prediction is made\ncm = confusion_matrix(y_test, y_pred)\n\nprint(\"predicted y: \", y_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cm","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rf_results_df = pd.DataFrame({'Predicted Stroke': y_pred, \n                             'Predicted Stroke Probability': y_prob,\n                             'Actual Stroke': y_test})\nrf_results_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(y_test, y_pred))\nprint(\"ROC AUC Score: \", roc_auc_score(y_test, y_prob))\nprint(\"Test Accuracy: \", accuracy_score(y_test, y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualizing Confusion Matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.figure(figsize = (10,10))\nsns.heatmap(cm, cmap = 'Blues', annot = True, fmt='d', linewidths = 5, cbar = False,\n           annot_kws = {'fontsize': 15}, yticklabels= ['No Stroke', 'Stroke'], \n           xticklabels= ['Predicted No Stroke', 'Predicted Stroke'])\nplt.yticks(rotation = 0)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualizing the ROC-AUC curve\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_prob)\nroc_auc_score = auc(false_positive_rate, true_positive_rate)\nroc1 = roc_auc_score\nprint(\"AUC of tuned RF is: \", roc_auc_score)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualizing the ROC curve\n\nsns.set_theme(style = 'white')\nplt.figure(figsize = (10,10))\nplt.plot(false_positive_rate, true_positive_rate, color='#b01717', label = 'AUC = %0.4f' %roc_auc_score)\nplt.legend(loc = 'lower right')\nplt.plot([0,1], [0,1], linestyle='--', color = '#174ab0')\nplt.axis('tight') # to remove additional white space in the plot\nplt.ylabel('True Positive Rate (Sensitivity)')\nplt.xlabel('False Positive rate (1-Specificity)')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## XG Boost with tuning","metadata":{}},{"cell_type":"code","source":"classifier = XGBClassifier(eval_metric = 'error', use_label_encoder = False, learning_rate = 0.1)\nclassifier.fit(x_train_res, y_train_res)\ny_pred = classifier.predict(x_test)\ny_prob = classifier.predict_proba(x_test)[:,1]\ncm = confusion_matrix(y_test, y_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Putting the results in a dataframe\ndf_xgboost_results = pd.DataFrame({'Predicted Stroke Variable': y_pred, \n                                  'Predicted Stroke Probability': y_prob,\n                                  'Actual Stroke Variable': y_test})\ndf_xgboost_results.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cm","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracy_score(y_test, y_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ROC AUC curve\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_prob)\nroc_auc_score = auc(false_positive_rate, true_positive_rate)\nroc_auc_score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Visualization - Confusion Matrix\nplt.figure(figsize = (10,10))\nsns.heatmap(cm, cmap = \"Blues\", annot = True, fmt = 'd', linewidth = 5, cbar = False, annot_kws = {'fontsize': 15},\n           xticklabels = ['Predicted No Stroke', 'Predicted Stroke'], yticklabels = ['No Stroke', 'Stroke'])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Visualizaton - ROC AUC Curve\n\nsns.set_theme(style = 'white')\nplt.figure(figsize = (10,10))\nplt.plot(false_positive_rate, true_positive_rate, color='#b01717', label = 'AUC = %0.4f' %roc_auc_score)\nplt.legend(loc = 'lower right')\nplt.plot([0,1], [0,1], linestyle='--', color = '#174ab0')\nplt.axis('tight') # to remove additional white space in the plot\nplt.ylabel('True Positive Rate (Sensitivity)')\nplt.xlabel('False Positive rate (1-Specificity)')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Keras ANN","metadata":{}},{"cell_type":"code","source":"from keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom keras.regularizers import l2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Building the ANN with 3 dense layers\nimport tensorflow as tf\n\ndef ann_classifier():\n    ann = tf.keras.models.Sequential() ## Sequential layer has one input tensor and one output tensor\n    \n    # Dense layer 1\n    ann.add(tf.keras.layers.Dense(units=8, kernel_regularizer = l2(0.01), bias_regularizer = l2(0.01), activation = 'relu'))\n    \n    #Dense layer 2\n    ann.add(tf.keras.layers.Dense(units=8, kernel_regularizer = l2(0.01), bias_regularizer = l2(0.01), activation = 'relu'))\n    \n    # Dense layer 3\n    ann.add(tf.keras.layers.Dense(units=1, activation = 'sigmoid'))\n    \n    \n    ann.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n    \n    return ann","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = KerasClassifier(build_fn = ann_classifier, batch_size = 32, epochs = 50)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Evaluating the basic untuned ANN\naccuracies = cross_val_score(estimator = model, X=x_train_res, y=y_train_res, cv = 5)","metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Checking the meand and std dev of the accuracies obtained\nmean = accuracies.mean()\nmean","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"std_dev = accuracies.std()\nstd_dev","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Tuning ANN using GridSearch","metadata":{}},{"cell_type":"code","source":"def ann_classifier(optimizer = 'adam'):\n    ann = tf.keras.models.Sequential() ## Sequential layer has one input tensor and one output tensor\n    \n    # Dense layer 1\n    ann.add(tf.keras.layers.Dense(units=8, kernel_regularizer = l2(0.01), bias_regularizer = l2(0.01), activation = 'relu'))\n    \n    #Dense layer 2\n    ann.add(tf.keras.layers.Dense(units=8, kernel_regularizer = l2(0.01), bias_regularizer = l2(0.01), activation = 'relu'))\n    \n    # Adding dropout which is a technique to randomly ignore neurons while training to reduce overfitting\n    tf.keras.layers.Dropout(0.6)\n     \n    # Dense layer 3\n    ann.add(tf.keras.layers.Dense(units=1, activation = 'sigmoid'))\n    \n    \n    ann.compile(optimizer = optimizer, loss = 'binary_crossentropy', metrics = ['accuracy'])\n    \n    return ann","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = KerasClassifier(build_fn = ann_classifier, batch_size = 32, epochs = 50)\n\nparameters = {\n    'batch_size': [25, 32],\n    'epochs': [50,100,150],\n    'optimizer': ['adam', 'rmsprop']\n}\n\n\ngrid_search = GridSearchCV(estimator = model, param_grid = parameters, scoring = 'accuracy', cv = 5, n_jobs = -1)\ngrid_search.fit(x_train_res, y_train_res)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_accuracy = grid_search.best_score_\nbest_params = grid_search.best_params_\n\nprint(\"Best Accuracy: {:.2f} %\".format(best_accuracy*100))\nprint(\"Best Parameters: \", best_params)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n# Final Tuned ANN Model","metadata":{}},{"cell_type":"code","source":"ann = tf.keras.models.Sequential()\nann.add(tf.keras.layers.Dense(units = 32, kernel_regularizer = l2(0.01), bias_regularizer = l2(0.01), activation = 'relu'))\nann.add(tf.keras.layers.Dense(units = 32, kernel_regularizer = l2(0.01), bias_regularizer = l2(0.01), activation = 'relu'))\ntf.keras.layers.Dropout(0.6)\nann.add(tf.keras.layers.Dense(units = 1, activation = 'sigmoid'))\n\nann.compile(optimizer = 'rmsprop', loss = 'binary_crossentropy', metrics = ['accuracy'])\n\nmodel = ann.fit(x_train_res, y_train_res, batch_size = 25, epochs = 150, validation_split = 0.2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Loss Graph\n# Here model is actually just the ann history\n\n\nloss_training = model.history['loss']\nloss_validation = model.history['val_loss']\nepochs = range(1,151)\n\nplt.plot(epochs, loss_training, 'g', label = 'Training Loss')\nplt.plot(epochs, loss_validation, 'b', label = 'Validation Loss')\n\nplt.title('Training and Validation Loss of Tuned ANN')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\n\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Training and Validation accuracy plot\ntraining_accuracy = model.history['accuracy']\nvalidation_accuracy = model.history['val_accuracy']\nepochs = range(1,151)\n\nplt.plot(epochs, training_accuracy, 'g', label= 'Training Accuracy')\nplt.plot(epochs, validation_accuracy, 'b', label = 'Validation Accuracy')\nplt.title('Training and Validation Accuracy for Tuned ANN')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = ann.predict(x_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def threshold_prediction(y_pred):\n    y_pred_result = []\n    for i in y_pred:\n        if i > 0.5:\n            y_pred_result.append(1)\n        else:\n            y_pred_result.append(0)\n    return y_pred_result\n            \ny_pred_result = threshold_prediction(y_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cm = confusion_matrix(y_test, y_pred_result)\ncm","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracy_test = accuracy_score(y_test, y_pred_result)\naccuracy_test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred)\nroc_auc_score = auc(false_positive_rate, true_positive_rate)\nroc1 = roc_auc_score\nprint(\"AUC of tuned ANN is: \", roc_auc_score)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(8,5))\nsns.heatmap(cm, cmap = \"Blues\", annot = True, fmt = 'd', linewidths = 5, cbar = False, annot_kws ={'fontsize': 15}, yticklabels = ['No Stroke', 'Stroke'], xticklabels = ['Predicted No Stroke', 'Predicted Stroke'])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Summary of Results","metadata":{}},{"cell_type":"markdown","source":"Clearly the 3 good performing models for Stroke Prediction are as follows:\n1. Random Forest: accuracy: 0.89 , auc: 0.75\n2. XG Boost: accuracy: 0.87 , auc: 0.74\n3. ANN: accuracy: 0.85 , auc: 0.73\n\nSo a tuned Random Forest performed best for this data","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}