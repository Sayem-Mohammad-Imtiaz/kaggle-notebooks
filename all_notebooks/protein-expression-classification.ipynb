{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Mice Protein Expression Classification"},{"metadata":{},"cell_type":"markdown","source":"The dataset contains expression levels of 77 proteins measured in the cerebral cortex of 8 classes of control and Down syndrome mice exposed to context fear conditioning, a task used to assess associative learning.\n\nWe will explore the data and train models for classification."},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\n\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/mice-protein-expression/Data_Cortex_Nuclear.csv')\ndf.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#count and visualize missing values\nprint('total number of missing values:',df.isnull().sum().sum())\nsns.heatmap(df.isnull());","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above heatmap we see there are too many missing values for us to simply delete the rows that contain them. We do not want to remove the columns that contain numerous missing values, because there are five such columns, and they may contain important information.\n\nThere are two options. One is to perform correlation analysis and see if the columns with many (>100) missing values correlate highly with columns that are complete, in which case we can delete them. Second, we can impute the missing values, which is what we will do here, but before we do that let's do some exploratory analysis."},{"metadata":{},"cell_type":"markdown","source":"## Exploratory Data Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#correlation heatmap\nplt.figure(figsize=(10,6));\nsns.heatmap(df.corr());","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#clustermap\nsns.clustermap(df.corr(),cmap='vlag');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Categorical Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"# function to separate categorical from numeric features\ndef feature_types(df):\n    categ = []\n    numer = []\n    for c in df.columns:\n        if df[c].dtype == 'object': categ.append(c)\n        else: numer.append(c)\n    return categ, numer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#display information on categorical features\ncateg, numer = feature_types(df)\nprint('categorical features:',categ)\nprint('')\n\nfor f in categ[1:-1]:\n    print('value counts of feature: {}'.format(f))\n    #print('')\n    print(df[f].value_counts())\n    print('        ---------------')\n    print('')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#information on classes\nprint('number of classes: ',df['class'].nunique())\nprint('class names: ', df['class'].unique())\n\nplt.figure(figsize=(12,4));\nplt.subplot(1,2,1);\nplt.title('class_count');\nsns.countplot(x='class', data=df, color='Grey');\nplt.subplot(1,2,2);\nplt.title('class_proportions');\ndf['class'].value_counts().plot(kind='pie',autopct='%1.1f%%');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the countplots below, we see that the classes are derived from the values of the categorical features. For example, class 'c-CS-m' means that the Genotype feature has value 'control', the Behavior feature has value 'C/S', and the Treatment feature has value 'memantine'. Therefore these features are encoded into the classes and we will not use them for model training.\n\nHere are the countplots"},{"metadata":{"trusted":true},"cell_type":"code","source":"#visualize classes for categorical features\nplt.figure(figsize=(7,3))\nsns.countplot(x='class', data=df, hue='Genotype',palette=['firebrick','tomato']);\nplt.legend(loc=(1.01,0.8));\nplt.title('Classes for feature: Genotype');\nplt.tight_layout(True)\n\nplt.figure(figsize=(7,3))\nsns.countplot(x='class', data=df, hue='Treatment',palette=['firebrick','tomato']);\nplt.legend(loc=(1.01,0.8));\nplt.title('Classes for feature: Treatment');\nplt.tight_layout(True)\n\nplt.figure(figsize=(7,3))\nsns.countplot(x='class', data=df, hue='Behavior',palette=['firebrick','tomato']);\nplt.legend(loc=(1.01,0.8));\nplt.title('Classes for feature: Behavior');\nplt.tight_layout(True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Before we impute the data and train models for classification, let's visualize the non-null entries to see what they look like.\n\nFirst, we will min-max scale them so they assume values for 0 to 1, then display the dataset as an image. Then, we will plot a PCA scatter. Finally, we will select the most important features and visualize their distributions."},{"metadata":{"trusted":true},"cell_type":"code","source":"# temporarily remove null-values and min-max scale data\nfrom sklearn.preprocessing import MinMaxScaler\n\nnonna=df.dropna(axis=1,thresh=901)\nnonna=nonna.dropna(axis=0,how='any')\ncateg,numeri=feature_types(nonna)\nscaler = MinMaxScaler()\nscaled = scaler.fit_transform(nonna[numeri].values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#display dataset as image\nplt.figure(figsize=(20,10));\nsns.heatmap(nonna[numeri].values);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### PCA visualization"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\n\npca = PCA()\ndec = pca.fit_transform(scaled)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#2d scatterplot\nsns.scatterplot(x=dec[:,0],y=dec[:,1],hue=nonna['class']);\nplt.title('PCA__ 2 components')\nplt.legend(loc=(1.01,0.35));\n\n#3d scatterplot\nfrom mpl_toolkits import mplot3d\nfrom sklearn.preprocessing import OrdinalEncoder\nordinal = OrdinalEncoder()\nlabels = ordinal.fit_transform(nonna['class'].values[:,np.newaxis]).astype('int').squeeze()\nplt.figure(figsize=(10,6));\nax = plt.axes(projection='3d');\nax.scatter(dec[:,0], dec[:,1], dec[:,2], c=labels, cmap='winter');\nplt.title('PCA__ 3 components');\n\n#explained variance\nplt.figure(figsize=(15,3));\nsns.barplot(x=np.arange(pca.explained_variance_ratio_.shape[0])+1, \n            y=pca.explained_variance_ratio_,color='Grey');\nplt.title('Explained Variance');\n\n#cumulative explained variance\nplt.figure(figsize=(5,3));\nsns.lineplot(x=np.arange(pca.explained_variance_ratio_.shape[0])+1, \n             y=pca.explained_variance_ratio_.cumsum());\nplt.title('Cumulative Explained Variance');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"TSNE"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.manifold import TSNE\n\ntsne = TSNE(n_components=3, random_state=33)\nts = tsne.fit_transform(scaled)\n\nsns.scatterplot(ts[:,0], ts[:,1],hue=labels);\nplt.figure();\nax = plt.axes(projection='3d');\nax.scatter(ts[:,0], ts[:,1], ts[:,2], c=labels, cmap='winter');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Other Dimensionality Reduction Methods"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.manifold import Isomap,LocallyLinearEmbedding,MDS,SpectralEmbedding\n\niso = Isomap().fit_transform(scaled)\nlle = LocallyLinearEmbedding().fit_transform(scaled)\nmds = MDS().fit_transform(scaled)\nse = SpectralEmbedding().fit_transform(scaled)\n\nplt.figure(figsize=(14,10))\nplt.subplot(2,2,1)\nsns.scatterplot(x=iso[:,0],y=iso[:,1],hue=nonna['class']);\nplt.legend(loc='best');\nplt.title('Isomap');\n\nplt.subplot(2,2,2)\nsns.scatterplot(x=lle[:,0],y=lle[:,1],hue=nonna['class']);\nplt.legend(loc='best');\nplt.title('Locally Linear Embedding');\n\nplt.subplot(2,2,3)\nsns.scatterplot(x=mds[:,0],y=mds[:,1],hue=nonna['class']);\nplt.legend(loc='best');\nplt.title('MDS');\n\nplt.subplot(2,2,4)\nsns.scatterplot(x=se[:,0],y=se[:,1],hue=nonna['class']);\nplt.legend(loc='best');\nplt.title('Spectral Embedding');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Feature Selection and Exploration"},{"metadata":{"trusted":true},"cell_type":"code","source":"#identify most important features via Random Forest\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import OrdinalEncoder\n\nordinal = OrdinalEncoder()\nlabels = ordinal.fit_transform(nonna['class'].values[:,np.newaxis]).astype('int').squeeze()\n\nforest = RandomForestClassifier(n_estimators=1000, max_depth=8, random_state=33)\nforest.fit(scaled, labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feats = np.argsort(forest.feature_importances_)[-10:]\nplt.figure(figsize=(6,4));\nsns.barplot(y=nonna[numeri].columns[feats][::-1], \n            x=forest.feature_importances_[feats][::-1], \n            color='Grey');\nplt.title('Significance of Most Important Features');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The features are too many to plot all of them but, for demonstration purposes, we will pick the five most important features and visualize their distributions through pairgrids, violin plots, kde plots, and histograms."},{"metadata":{"trusted":true},"cell_type":"code","source":"feats = ['SOD1_N', 'pPKCG_N', 'pERK_N', 'APP_N', 'CaNA_N']\n\n#pairplot\nsns.pairplot(nonna, vars=feats, hue='class');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#boxenplots\nfor f in feats[::-1]:\n    plt.figure(figsize=(6,4))\n    sns.boxenplot(y=f, x='class', data=nonna);\n    plt.title('{}'.format(f));","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#kde plots of most important features\ncolors = ['b', 'y', 'g', 'r', 'm', 'darkgoldenrod', 'tab:pink', 'grey']\nclasses = nonna['class'].unique()\n\nfor f in feats:\n    plt.figure(figsize=(12,4))\n    for color, clas in zip(colors,classes):\n        sns.distplot(nonna[nonna['class']==clas][f], hist=False, color=color, label=clas);\n    plt.legend(loc='best');\n    plt.title(f);\n    plt.tight_layout();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#distribution histograms of most important features\ncolors = ['b', 'y', 'g', 'r', 'm', 'darkgoldenrod', 'tab:pink', 'grey']\nclasses = nonna['class'].unique()\n\nfor f in feats:\n    plt.figure(figsize=(12,4))\n    for color, clas in zip(colors,classes):\n        sns.distplot(nonna[nonna['class']==clas][f], kde=False, color=color, label=clas);\n    plt.legend(loc='best');\n    plt.title(f);\n    plt.tight_layout();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will try one more Feature Selection algorithm, namely chi-square test, and compare the results with Random Forrest"},{"metadata":{"trusted":true},"cell_type":"code","source":"#compute chi2-selected features\nfrom sklearn.feature_selection import SelectKBest, chi2\nfrom sklearn.preprocessing import OrdinalEncoder\n\nOE = OrdinalEncoder()\nlabs = OE.fit_transform(nonna['class'].values[:,np.newaxis]).squeeze().astype('int')\ns_feats = SelectKBest(chi2, k=10).fit(scaled,labs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#create comparative dataframe\nFS = pd.DataFrame([])\nFS['Random Forest'] = pd.Series(df[numer].columns[np.argsort(forest.feature_importances_)[-10:]][::-1])\nFS['chi2'] = pd.Series(df[numer].columns[np.argsort(s_feats.scores_)[-10:]][::-1])\nFS","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"These are the features both algorithms have in common in their selected ten most important features (they are seven):"},{"metadata":{"trusted":true},"cell_type":"code","source":"FS[FS['Random Forest'].isin(FS['chi2'])]['Random Forest']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Classification"},{"metadata":{},"cell_type":"markdown","source":"#### Preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OrdinalEncoder,MinMaxScaler\nfrom sklearn.model_selection import train_test_split\n\nx = df.loc[:,numer].values\ny = df['class']\n\nimp = SimpleImputer(missing_values=np.nan, strategy='mean')\nimp.fit(df.loc[:,numer].dropna(how='any',inplace=False).values)\nx = imp.transform(x)\n\nminmax = MinMaxScaler()\nx = minmax.fit_transform(x)\n\nordi = OrdinalEncoder()\ny = ordi.fit_transform(y[:,np.newaxis]).squeeze().astype('int')\ncat = ordi.categories_[0]\n\nx_tr, x_ts, y_tr, y_ts = train_test_split(x,y,test_size=0.1, random_state=33)\n\nx_tr.shape,y_tr.shape,x_ts.shape,y_ts.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Machine Learning"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n\nresults = {} #dictionary to store accuracy and f1-score of models","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"knn = KNeighborsClassifier()\nknn.fit(x_tr,y_tr)\npred_knn = knn.predict(x_ts)\n\nacc_knn = accuracy_score(y_ts, pred_knn)\nf1_knn = f1_score(y_ts, pred_knn, average='macro')\nresults['K-Nearest Neighbors'] = [acc_knn, f1_knn]\n\nprint('accuracy:  ', acc_knn)\nprint('f1_score:  ', f1_knn)\nprint('')\n\nsns.heatmap(confusion_matrix(y_ts,pred_knn),\n            xticklabels=cat, yticklabels=cat,\n            annot=True, fmt='1d', cbar=False);\nplt.title('Confusion Matrix');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = LogisticRegression()\nlr.fit(x_tr,y_tr)\npred_lr = lr.predict(x_ts)\n\nacc_lr = accuracy_score(y_ts, pred_lr)\nf1_lr = f1_score(y_ts, pred_lr, average='macro')\nresults['Logistic Regression'] = [acc_lr, f1_lr]\n\nprint('accuracy:  ', acc_lr)\nprint('f1_score:  ', f1_lr)\nprint('')\n\nsns.heatmap(confusion_matrix(y_ts,pred_lr),\n            xticklabels=cat, yticklabels=cat,\n            annot=True, fmt='1d', cbar=False);\nplt.title('Confusion Matrix');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf = RandomForestClassifier(n_estimators=1000,max_depth=8,random_state=33)\nrf.fit(x_tr,y_tr)\npred_rf = rf.predict(x_ts)\n\nacc_rf = accuracy_score(y_ts, pred_rf)\nf1_rf = f1_score(y_ts, pred_rf, average='macro')\nresults['Random Forest'] = [acc_rf, f1_rf]\n\nprint('accuracy:  ', acc_rf)\nprint('f1_score:  ', f1_rf)\nprint('')\n\nsns.heatmap(confusion_matrix(y_ts,pred_rf),\n            xticklabels=cat, yticklabels=cat,\n            annot=True, fmt='1d', cbar=False);\nplt.title('Confusion Matrix');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gb = GradientBoostingClassifier(n_estimators=1000,max_depth=5,random_state=33)\ngb.fit(x_tr,y_tr)\npred_gb = gb.predict(x_ts)\n\nacc_gb = accuracy_score(y_ts, pred_gb)\nf1_gb = f1_score(y_ts, pred_gb, average='macro')\nresults['Gradient Boosting Tree'] = [acc_gb, f1_gb]\n\nprint('accuracy:  ', acc_gb)\nprint('f1_score:  ', f1_gb)\nprint('')\n\nsns.heatmap(confusion_matrix(y_ts,pred_gb),\n            xticklabels=cat, yticklabels=cat,\n            annot=True, fmt='1d', cbar=False);\nplt.title('Confusion Matrix');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tree = DecisionTreeClassifier()\nada = AdaBoostClassifier(base_estimator=tree,n_estimators=1000,random_state=33)\nada.fit(x_tr,y_tr)\npred_ada = ada.predict(x_ts)\n\nacc_ada = accuracy_score(y_ts, pred_ada)\nf1_ada = f1_score(y_ts, pred_ada, average='macro')\nresults['AdaBoost Tree'] = [acc_ada, f1_ada]\n\nprint('accuracy:  ', acc_ada)\nprint('f1_score:  ', f1_ada)\nprint('')\n\nsns.heatmap(confusion_matrix(y_ts,pred_ada),\n            xticklabels=cat, yticklabels=cat,\n            annot=True, fmt='1d', cbar=False);\nplt.title('Confusion Matrix');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Deep Learning"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense,Conv2D,MaxPooling2D,Flatten\nfrom keras.callbacks import EarlyStopping\nfrom keras.optimizers import Adam, RMSprop\nfrom keras.losses import mean_squared_error as mse","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"DNN"},{"metadata":{"jupyter":{"outputs_hidden":true},"trusted":true},"cell_type":"code","source":"dnn = Sequential()\ndnn.add(Dense(110, input_shape=[x_tr.shape[1]], activation='relu'))\ndnn.add(Dense(220,activation='relu'))\ndnn.add(Dense(8,activation='sigmoid'))\ndnn.compile(optimizer='adam',loss='sparse_categorical_crossentropy',\n            metrics=['acc'])\ndnn.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stop = EarlyStopping(patience=4,verbose=1)\ndnn.fit(x_tr,y_tr,validation_split=0.1,callbacks=[stop],verbose=0,epochs=30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_dnn = np.argmax(dnn.predict(x_ts),axis=1)\n\nacc_dnn = accuracy_score(y_ts, pred_dnn)\nf1_dnn = f1_score(y_ts, pred_dnn, average='macro')\nresults['DNN'] = [acc_dnn, f1_dnn]\n\nprint('accuracy:  ', acc_dnn)\nprint('f1_score:  ', f1_dnn)\nprint('')\n\nsns.heatmap(confusion_matrix(y_ts,pred_dnn),\n            xticklabels=cat, yticklabels=cat,\n            annot=True, fmt='1d', cbar=False);\nplt.title('Confusion Matrix');","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"markdown","source":"CNN2d"},{"metadata":{"trusted":true},"cell_type":"code","source":"#convert training data to images for CNN training\nims_tr = np.zeros((x_tr.shape[0],81))\nims_ts = np.zeros((x_ts.shape[0],81))\n\nfor i, x in enumerate(x_tr): ims_tr[i,:77] = x\nfor i, x in enumerate(x_ts): ims_ts[i,:77] = x\n\nims_tr = np.reshape(ims_tr,(x_tr.shape[0],9,9))\nims_ts = np.reshape(ims_ts,(x_ts.shape[0],9,9))\n\nplt.matshow(ims_tr[1]);\nplt.tight_layout(True);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cnn = Sequential()\n\ncnn.add(Conv2D(64,(3,3),input_shape=[9,9,1],activation='relu',padding='same'))\ncnn.add(MaxPooling2D((2,2),padding='same'))\ncnn.add(Conv2D(128,(3,3),activation='relu',padding='same'))\ncnn.add(MaxPooling2D((2,2),padding='same'))\ncnn.add(Flatten())\ncnn.add(Dense(100,activation='relu'))\ncnn.add(Dense(8,activation='softmax'))\n\ncnn.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc'])\ncnn.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stop = EarlyStopping(patience=4,verbose=1)\ncnn.fit(ims_tr[:,:,:,np.newaxis],y_tr,validation_split=0.1,\n        epochs=20,verbose=1,callbacks=[stop])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_cnn = np.argmax(cnn.predict(ims_ts[:,:,:,np.newaxis]),axis=1)\n\nacc_cnn = accuracy_score(y_ts, pred_cnn)\nf1_cnn = f1_score(y_ts, pred_cnn, average='macro')\nresults['CNN'] = [acc_cnn, f1_cnn]\n\nprint('accuracy:  ', acc_cnn)\nprint('f1_score:  ', f1_cnn)\nprint('')\n\nsns.heatmap(confusion_matrix(y_ts,pred_cnn),\n            xticklabels=cat, yticklabels=cat,\n            annot=True, fmt='1d', cbar=False);\nplt.title('Confusion Matrix');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Classification on Extracted Features"},{"metadata":{},"cell_type":"markdown","source":"#### PCA-to-DNN"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\n\npca = PCA()\ndec_tr = pca.fit_transform(x_tr)\ndec_ts = pca.transform(x_ts)\n\n\ndnn = Sequential()\ndnn.add(Dense(110, input_shape=[x_tr.shape[1]], activation='relu'))\ndnn.add(Dense(220,activation='relu'))\ndnn.add(Dense(8,activation='sigmoid'))\ndnn.compile(optimizer='adam',loss='sparse_categorical_crossentropy',\n            metrics=['acc'])\n\nstop = EarlyStopping(patience=4,verbose=1)\n\ndnn.fit(dec_tr,y_tr,validation_split=0.1,callbacks=[stop],verbose=0,epochs=30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_dnn = np.argmax(dnn.predict(dec_ts),axis=1)\n\nacc_dnn = accuracy_score(y_ts, pred_dnn)\nf1_dnn = f1_score(y_ts, pred_dnn, average='macro')\nresults['PCA->DNN'] = [acc_dnn, f1_dnn]\n\nprint('accuracy:  ', acc_dnn)\nprint('f1_score:  ', f1_dnn)\nprint('')\n\nsns.heatmap(confusion_matrix(y_ts,pred_dnn),\n            xticklabels=cat, yticklabels=cat,\n            annot=True, fmt='1d', cbar=False);\nplt.title('Confusion Matrix');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### AutoEncoder-to-DNN"},{"metadata":{"trusted":true},"cell_type":"code","source":"dim = x_tr.shape[1]\n\nae = Sequential()\nae.add(Dense(dim,activation='elu',kernel_initializer='he_uniform',input_shape=[dim]))\nae.add(Dense(int(dim//1.5),activation='elu',kernel_initializer='he_uniform'))\nae.add(Dense(dim//2,activation='elu',kernel_initializer='he_uniform'))\nae.add(Dense(dim//3,activation='elu',kernel_initializer='he_uniform'))\nae.add(Dense(dim//4,activation='elu',kernel_initializer='he_uniform'))\nae.add(Dense(dim//5,activation='elu',kernel_initializer='he_uniform'))\nae.add(Dense(dim//6,activation='elu',kernel_initializer='he_uniform'))\nae.add(Dense(dim//7,activation='elu',kernel_initializer='he_uniform'))\nae.add(Dense(dim//8,activation='linear',kernel_initializer='he_uniform', name='encoder'))\nae.add(Dense(dim//7,activation='elu',kernel_initializer='he_uniform'))\nae.add(Dense(dim//6,activation='elu',kernel_initializer='he_uniform'))\nae.add(Dense(dim//5,activation='elu',kernel_initializer='he_uniform'))\nae.add(Dense(dim//4,activation='elu',kernel_initializer='he_uniform'))\nae.add(Dense(dim//3,activation='elu',kernel_initializer='he_uniform'))\nae.add(Dense(dim//2,activation='elu',kernel_initializer='he_uniform'))\nae.add(Dense(int(dim//1.5),activation='elu',kernel_initializer='he_uniform'))\nae.add(Dense(dim,activation='sigmoid'))\nae.compile(optimizer=RMSprop(learning_rate=0.001),loss='mse')\n\nae.fit(x_tr,x_tr,epochs=30,verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#extract features\nfrom keras import Model\nencoder = Model(ae.input, ae.get_layer('encoder').output)\n\nx_ae_tr = encoder.predict(x_tr)\nx_ae_ts = encoder.predict(x_ts)","execution_count":null,"outputs":[]},{"metadata":{"jupyter":{"outputs_hidden":true},"trusted":true},"cell_type":"code","source":"#DNN\n#train model with AE-extracted features\ndnn = Sequential()\ndnn.add(Dense(15, input_shape=[x_ae_tr.shape[1]], activation='relu'))\ndnn.add(Dense(30,activation='relu'))\ndnn.add(Dense(8,activation='sigmoid'))\ndnn.compile(optimizer='adam',loss='sparse_categorical_crossentropy',\n            metrics=['acc'])\n\nstop = EarlyStopping(patience=4,verbose=1)\n\ndnn.fit(x_ae_tr,y_tr,validation_split=0.1,callbacks=[stop],verbose=0,epochs=30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_dnn = np.argmax(dnn.predict(x_ae_ts),axis=1)\n\nacc_dnn = accuracy_score(y_ts, pred_dnn)\nf1_dnn = f1_score(y_ts, pred_dnn, average='macro')\nresults['AE->DNN'] = [acc_dnn, f1_dnn]\n\nprint('accuracy:  ', acc_dnn)\nprint('f1_score:  ', f1_dnn)\nprint('')\n\nsns.heatmap(confusion_matrix(y_ts,pred_dnn),\n            xticklabels=cat, yticklabels=cat,\n            annot=True, fmt='1d', cbar=False);\nplt.title('Confusion Matrix');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Pre-trained CNN"},{"metadata":{"trusted":true},"cell_type":"code","source":"#convert training data to images for CNN training\nx_train = np.zeros((x_tr.shape[0],81))\nx_test = np.zeros((x_ts.shape[0],81))\n\nfor i, x in enumerate(x_tr): x_train[i,:77] = x\nfor i, x in enumerate(x_ts): x_test[i,:77] = x\n\nx_tr = np.reshape(x_train,(x_train.shape[0],9,9))\nx_ts = np.reshape(x_test,(x_test.shape[0],9,9))\n\nplt.matshow(x_tr[1]);\nplt.tight_layout(True);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#magnify images\nx_tr = np.repeat(x_tr,10,axis=1)\nx_tr = np.repeat(x_tr,10,axis=2)\n\nx_ts = np.repeat(x_ts,10,axis=1)\nx_ts = np.repeat(x_ts,10,axis=2)\n\nprint(x_tr.shape, x_ts.shape)\nplt.matshow(x_tr[1]);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#create RGB dimension, three channels\nx_tr = np.stack([x_tr,x_tr,x_tr],axis=-1)\nx_ts = np.stack([x_ts,x_ts,x_ts],axis=-1)\nx_tr.shape, x_ts.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"import keras\nfrom keras import backend as K\nfrom keras import models\nfrom keras.models import Model, load_model\nfrom keras import layers\nfrom keras.layers import Dense,Conv2D,MaxPooling2D, Flatten, Input\nfrom keras.layers.core import Dropout\nfrom keras.optimizers import Adam\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\nfrom tensorflow.keras.applications.xception import Xception\n\n\n#weight and notops from pretrained models\n!mkdir ~/.keras\n!mkdir ~/.keras/models\n!cp ../input/keras-pretrained-models/*notop* ~/.keras/models/\n!cp ../input/keras-pretrained-models/imagenet_class_index.json ~/.keras/models/\n\n\nfrom tensorflow.keras.applications.xception import Xception\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n\n# create the base pre-trained model\nbase_model = Xception(weights='imagenet', include_top=False, input_shape=(90,90,3))\n\n# first: train only the top layers (which were randomly initialized)\n# i.e. freeze all convolutional InceptionV3 layers\nfor layer in base_model.layers:\n    layer.trainable = False\nbase_model_output = base_model.output\nnew_concatenated_model = Flatten()(base_model_output)\nnew_concatenated_model = (Dense(8, activation='softmax'))(new_concatenated_model)\n\n# this is the model we will train\nmodel = Model(inputs=base_model.input, outputs=new_concatenated_model)\n\n\n# compile the model (should be done *after* setting layers to non-trainable)\nstop= EarlyStopping(patience=4, verbose=1)\n\nmodel.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc'])\nmodel.fit(x_tr,y_tr,epochs=10,batch_size=20,callbacks=[stop],validation_split=0.1)\n\npred = model.predict(x_ts)\nprint('-----')\npred = np.argmax(pred, axis=1)\nprint('accuracy:',accuracy_score(y_ts,pred))\nprint('f1-score:',f1_score(y_ts,pred,average='macro'))\nconf=confusion_matrix(y_ts,pred)\nsns.heatmap(conf, annot=True, fmt='1d');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_keras = np.argmax(model.predict(x_ts),axis=1)\n\nacc_keras = accuracy_score(y_ts, pred_keras)\nf1_keras = f1_score(y_ts, pred_keras, average='macro')\nresults['Pretrained Xception'] = [acc_keras, f1_keras]\n\nprint('accuracy:  ', acc_keras)\nprint('f1_score:  ', f1_keras)\nprint('')\n\nsns.heatmap(confusion_matrix(y_ts,pred_keras),\n            xticklabels=cat, yticklabels=cat,\n            annot=True, fmt='1d', cbar=False);\nplt.title('Confusion Matrix');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Overall Results"},{"metadata":{"trusted":true},"cell_type":"code","source":"#display performance of models in a dataframe \npd.DataFrame(data=results.values(),index=results.keys(),columns=['accuracy','f1-score'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#prepare the results-data for barplot\nmodels = list(results.keys())\naccuracies = [results[m][0] for m in models]\nf1_scores = [results[m][1] for m in models]\nml_models = pd.Series(np.hstack((models,models)))\nvalue = pd.Series(np.hstack((accuracies,f1_scores)))\nmetric = pd.Series(np.repeat(['accuracy','f1_score'],len(models)))\nresults_df = pd.DataFrame([])\nresults_df['model'] = ml_models\nresults_df['value'] = value\nresults_df['metric'] = metric\n\n#generate barplot\nplt.figure(figsize=(10,5));\nsns.barplot(y='model', x='value', data=results_df, \n            hue='metric',palette=['firebrick','lightcoral']);\nplt.legend(loc=(1.01,0.8));\nplt.tight_layout(True);\nplt.title('Model Performance');","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}