{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"10f9d1ab66ff0ff3e19eed18930b8c2ce0c17661"},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport tensorflow as tf\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom sklearn.utils import class_weight\nfrom sklearn.model_selection import StratifiedKFold, KFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score\nfrom keras.layers import Input, Dense, Flatten, Average\nfrom keras.layers.core import Dropout\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.models import Model, load_model, Sequential\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom keras import backend as K\nfrom keras.applications.vgg16 import VGG16\nfrom keras.applications.vgg16 import preprocess_input as prep_inputVgg16\nfrom keras.applications.resnet50 import ResNet50\nfrom keras.applications.resnet50 import preprocess_input as prep_inputResNet50\nfrom keras.optimizers import Adam, RMSprop\n\nsns.set()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ed33ff64ea73aa9fb910eb36e9bf7e94fe14a1cb"},"cell_type":"code","source":"np.set_printoptions(suppress=True)\nnp.set_printoptions(precision=2)\nnp.set_printoptions(edgeitems=10)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"TRAIN_PATH = '../input/volcanoesvenus/volcanoes_train/'\nTEST_PATH = '../input/volcanoesvenus/volcanoes_test/'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3bff43411181d3bd79d90ccea78423f7254a244d"},"cell_type":"code","source":"IMAGE_HEIGHT_TARGET = 110\nIMAGE_WIDTH_TARGET = 110","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"67a523b0842d3bf404afa34c71a39cb809d896fd"},"cell_type":"code","source":"# Load train data\ntrain_images = pd.read_csv(TRAIN_PATH + 'train_images.csv', header=None)\ntrain_labels = pd.read_csv(TRAIN_PATH + 'train_labels.csv', header=None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c03bdb7dacc7ac6012ac8925e269df82f673d4e9"},"cell_type":"code","source":"# Load test data\ntest_images = pd.read_csv(TEST_PATH + 'test_images.csv', header=None)\ntest_labels = pd.read_csv(TEST_PATH + 'test_labels.csv', header=None)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1d3197ecf4db55c3fcde5f4456f3f1e5228a37d3"},"cell_type":"markdown","source":"#### Check data"},{"metadata":{"trusted":true,"_uuid":"965d6fad16e6162d395809ed1bab1abac3b34984"},"cell_type":"code","source":"train_images.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9c65691cff2411b36e424e838700c740da54d621"},"cell_type":"code","source":"train_labels.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b7733a8726ca7c9812e3e6f08a68852eae30124f"},"cell_type":"markdown","source":" We can see that the first line is the header!\n We must remove it"},{"metadata":{"trusted":true,"_uuid":"280c79e6d133a5b29f40abda51c7c58cc9a68a2a"},"cell_type":"code","source":"train_labels = train_labels.drop([0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0aad65337257b8a19650a9d6f094b249b53d2c81"},"cell_type":"markdown","source":"We can see that when we have Volcanoe value 0 , we have all the rest columns as NaN (because there is no volcanoe)\nWe want to check if we have any nan values when there is a volcanoe (so index is 1)"},{"metadata":{"trusted":true,"_uuid":"ba5be17fe8b20085452f97e37b0401547fd3105b"},"cell_type":"code","source":"nulls = []\nfor i in range(len(train_labels.index)):\n    if (train_labels.iloc[i][0] == 1):\n        nulls.append(train_labels.isnull().iloc[i][0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6c7d277091a322bf9f46977eafa789f208d35888"},"cell_type":"code","source":"# count nan values (true) in list\ncount_nans = sum(nulls)\ncount_nans","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8212ab1872e5174e50e5b828bde33d7a6a0157be"},"cell_type":"markdown","source":"So, we don't have any nan values when we have a volcanoe"},{"metadata":{"trusted":true,"_uuid":"4585839cb9a9884c2742e931f2c42e43e6d1cfed"},"cell_type":"code","source":"# Do the same for test labels\nnulls = []\nfor i in range(len(test_labels.index)):\n    if (test_labels.iloc[i][0] == 1):\n        nulls.append(test_labels.isnull().iloc[i][0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"31a3a8c3fc18bae3a378b7764bf41aa80a0627cb"},"cell_type":"code","source":"count_nans = sum(nulls)\ncount_nans","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cfb175f4ca82d5e3b3cbea0fc438c651fafbf464"},"cell_type":"markdown","source":"We don't have an nan values in test set either."},{"metadata":{"trusted":true,"_uuid":"fafe280c31a1f21ac419417e0e44893d22941de0"},"cell_type":"code","source":"# Do the same for train and test images.\ntrain_images.isnull().values.any()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"015cfc3b48568886f842ded36ae2b22256379b4e"},"cell_type":"code","source":"test_images.isnull().values.any()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c66eeaea153fec2ccbd7605451a99ee6551e5112"},"cell_type":"markdown","source":"We are ok with our dataset , so we can continue our analysis"},{"metadata":{"_uuid":"3b3bb7fecff3bbd40e30246001170bba91a86fa0"},"cell_type":"markdown","source":"How many Volcanoes"},{"metadata":{"trusted":true,"_uuid":"ba2267139ae8543441e6a6eb56e37fae58fd9f2d"},"cell_type":"code","source":"ax = sns.countplot(data = train_labels,x=train_labels[0][1:])\nax.set(xlabel='Volcanoes', ylabel='Count')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9cedb2a3f18b08521a1bf556d54c5fbf7d83b3f2"},"cell_type":"markdown","source":"We can see that we have an unbalanced set of data.Non volcanoes are 6000 and volcanoes are 1000.We must take that into acocunt when we are going to design our model."},{"metadata":{"_uuid":"d79ffeea7b164a4907ee79e5d318c853dffdb31f"},"cell_type":"markdown","source":"What kind of type"},{"metadata":{"trusted":true,"_uuid":"95e9f6e96ba02424fdde34181667441462bbcb6b"},"cell_type":"code","source":"ax = sns.countplot(data = train_labels,x=train_labels[1][1:])\nax.set(xlabel='Type', ylabel='Count')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9f6363d0201b78300edca0b5dc6c83dd20562233"},"cell_type":"markdown","source":"Number of volcanoes "},{"metadata":{"trusted":true,"_uuid":"243d6e6c902f5b27fc2311d6c4b7226ebfc527f5"},"cell_type":"code","source":"ax = sns.countplot(data = train_labels,x=train_labels[3][1:])\nax.set(xlabel='Number of volcanoes', ylabel='Count')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2040f4babdac76a3cd610f7c51c3dbfd86c4c926"},"cell_type":"markdown","source":"## Multi label classification"},{"metadata":{"trusted":true,"_uuid":"b2c23d8fbf2138ed000fd98224d7a71c667952b8"},"cell_type":"markdown","source":"**We are going to perfrom multilabel categorical classification (each sample can have several classes).**\n\n**In order to do so , we are going to take into account only the categories where a volcanoe exists because in that case we have\ntype, radius and number of volcanoes.**\n\n**In the other cases all these values are nan.**"},{"metadata":{"trusted":true,"_uuid":"ec99787075720fdaeba5884e7b09c06c3198e753"},"cell_type":"markdown","source":"**Extract only the cases where volcanoe exists**"},{"metadata":{"trusted":true,"_uuid":"6b83342bb1eccee3decbb83e79d4fd8562720ff5"},"cell_type":"code","source":"indices_train = np.where(train_labels.iloc[:, 0].astype(np.float) == 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"331beeba5d897b0883d6db1b0a22d40a8290434b"},"cell_type":"code","source":"train_labels.iloc[indices_train].shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9ed72134938c56bdd17453bbd1cd8cef7f9dd4a0"},"cell_type":"code","source":"train_images.iloc[indices_train].shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7e23cf761691397721c8c6247b18c798fcf18a23"},"cell_type":"markdown","source":"So, we have 1000 volcanoes and the rest 6000 are no volcanoes"},{"metadata":{"trusted":true,"_uuid":"e154a71a5fb7c386fa229c7e389033eb098c75ff"},"cell_type":"code","source":"train_labels.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"356e31fdb0963c611dc11cfc258f4c77992c870f"},"cell_type":"markdown","source":"We are going to replace the strings columns data with more convenient"},{"metadata":{"_uuid":"b68d8dc2fa052fc79b1010319db93f0f3d7b7b5a"},"cell_type":"markdown","source":"**Replace all float nans with string nans in order for not to have any problems with values nan in the classifier.**"},{"metadata":{"trusted":true,"_uuid":"9a4845f8fac08e4b4ff35b874aa782ecc7ee1310"},"cell_type":"code","source":"train_labels = train_labels.fillna('nan')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fb2dc362ca73ef506820735836d6c54603999171"},"cell_type":"markdown","source":"**Replace 0 or 1 with No or Yes**"},{"metadata":{"trusted":true,"_uuid":"fec9080a066b7774aa08aee9efe27856c21a51d6"},"cell_type":"code","source":"train_labels.iloc[:, 0] = (train_labels.iloc[:, 0]).str.replace('0', 'No')\ntrain_labels.iloc[:, 0] = (train_labels.iloc[:, 0]).str.replace('1', 'Yes')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a6bc04667f4712f38578836efb1fede934dcd14f"},"cell_type":"markdown","source":"** Replace 1, 2, 3, 4 with Type1, 2, 3, 4**"},{"metadata":{"trusted":true,"_uuid":"d72b2ece3b9c91f89b1556f58fa4d81d14a54ee9"},"cell_type":"code","source":"train_labels.iloc[:, 1] = (train_labels.iloc[:, 1]).str.replace('1', 'Type 1')\ntrain_labels.iloc[:, 1] = (train_labels.iloc[:, 1]).str.replace('2', 'Type 2')\ntrain_labels.iloc[:, 1] = (train_labels.iloc[:, 1]).str.replace('3', 'Type 3')\ntrain_labels.iloc[:, 1] = (train_labels.iloc[:, 1]).str.replace('4', 'Type 4')\ntrain_labels.iloc[:, 1] = (train_labels.iloc[:, 1]).str.replace('nan', 'Type nan')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8510bfefbd1ff6927180e1cbde717f32dca5024c"},"cell_type":"markdown","source":"**Replace nan value with Nb volcanoes nan**"},{"metadata":{"trusted":true,"_uuid":"95f591e290b1c083f69da0d101c088b8f44e6c6c"},"cell_type":"code","source":"train_labels.iloc[:, 3] = (train_labels.iloc[:, 3]).str.replace('nan', 'Nb volcanoes nan')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d31d3e3170b1ed8e7c33d9b8825f287646b7d371"},"cell_type":"code","source":"train_labels[:10]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7340f0c51a498845628192f1cae6c4172a3e65bc"},"cell_type":"markdown","source":"**Create a labels list which will hold all the available labels**"},{"metadata":{"trusted":true,"_uuid":"7d614fc913cbd8478c154e9c3c41670bf1964b9a"},"cell_type":"code","source":"labels = []\nfor idx in range(len(train_labels)):\n    # index 0: Volcanoe or not\n    # index 1: Type\n    # index 3: Nb of volcanoes\n    labels.append([train_labels.iloc[:, 0].values.item(idx), train_labels.iloc[:, 1].values.item(idx), train_labels.iloc[:, 3].values.item(idx)]) \n    \nlabels = np.array(labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"11ddd0b7be5bd1037ca1eea31842946a1b95c1bd"},"cell_type":"code","source":"#Show a few labels\nlabels[:4]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7eabf594bee85789f79c0788f9efcf44a10a9785"},"cell_type":"code","source":"# Binarize the labels\nmlb = MultiLabelBinarizer()\nlabels = mlb.fit_transform(labels)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6f1e873d285a3091ec1fd0889c1ffdf2e4d10140"},"cell_type":"markdown","source":"**We can see all the classes**\n\n**We see the order by which the labels are given.Note,that in order to find out if we have a volcanoe we must check the last index**"},{"metadata":{"trusted":true,"_uuid":"2a0cd2bb90a6a3638403f026dee5bee4487eb8e0"},"cell_type":"code","source":"mlb.classes_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4feae320f134300bd092f92f9bcb37c28f3a6a81"},"cell_type":"code","source":"# Check the binarized labels\nlabels[:4]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1fc024ab8249c4bdb9d6caf087f8d13515e6b78e"},"cell_type":"markdown","source":"**We can see for example the first line.\nThe last index show us that there is a volcanoe , the 4th index from the end, that it is of type 3 and the first index , shows that we have only 1 volcanoe.**"},{"metadata":{"_uuid":"7034f5982398f7bdadc67d268fe7eb448acd4e0a"},"cell_type":"markdown","source":"**Split data into train and validation sets**"},{"metadata":{"trusted":true,"_uuid":"5b19ecebb3068dcb5e24f5cdc0cc81dd5f346664"},"cell_type":"code","source":"def data():\n    X_train, X_val, y_train, y_val  = train_test_split(train_images.values,\n                                                       labels,\n                                                       test_size=0.2,\n                                                       stratify=labels,\n                                                       random_state=1340)\n        \n    return X_train, X_val, y_train, y_val","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dc4d11af0527e17ef9c3b30ecd22e1b152d79b61"},"cell_type":"code","source":"X_train, X_val, y_train, y_val = data()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d5f6b3d61f8b43796dd29ba076c94833cebfe7ec"},"cell_type":"code","source":"X_train_res = X_train.reshape((-1, IMAGE_HEIGHT_TARGET, IMAGE_WIDTH_TARGET, 1))\nX_val_res = X_val.reshape((-1, IMAGE_HEIGHT_TARGET, IMAGE_WIDTH_TARGET, 1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7469ddb1c5e3d035a7e39cecbf896e8154930da6"},"cell_type":"code","source":"# Stack, in order to have 3 channels\nX_train_vggnet = np.stack((np.squeeze(X_train_res),) * 3, -1)\nX_val_vggnet = np.stack((np.squeeze(X_val_res),) * 3, -1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"512808d12ba3422a95d2e19ae005d0da77144b4d"},"cell_type":"code","source":"# Preprocess input\nX_train_vggnet = prep_inputVgg16(X_train_vggnet)\nX_val_vggnet = prep_inputVgg16(X_val_vggnet)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"243fe16ed32599d2e5f057b496d933fcbdacade8"},"cell_type":"markdown","source":"**Use data augmentation**"},{"metadata":{"trusted":true,"_uuid":"654501b2c3debd704e67e1cdf722aff6350ac5a1"},"cell_type":"code","source":"train_data_gen = ImageDataGenerator(horizontal_flip=True,\n                                    rotation_range=40,\n                                    width_shift_range=0.1,\n                                    height_shift_range=0.1,\n                                    zoom_range=0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"43926a37a108b166d546a2dd3930f8cde3d4163e"},"cell_type":"markdown","source":"**We are going to use the vggnet16**\n\n**Lets's create a class**"},{"metadata":{"trusted":true,"_uuid":"26695b7068e7c9880f9cff7a8e3b6e5b3012d862"},"cell_type":"code","source":"class VGGNet:\n    @staticmethod\n    def build(width, height, depth, classes, final_activ):\n        # Initialize the model to use channels last\n        input_shape = (height, width, depth)\n        \n        # In case where channels first is used\n        if K.image_data_format() == \"channels_first\":\n            input_shape = (depth, height, width)\n            \n        # Load pretrained weights\n        imagenet_weights = '../input/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5'\n        base_vgg16 = VGG16(include_top=False, weights=imagenet_weights, input_shape=input_shape)\n        last_layer = base_vgg16.output\n        \n        x = Flatten()(last_layer)\n        x = Dense(512, activation='relu')(x)\n        x = Dropout(0.4)(x)\n        preds_base_vgg16 = Dense(classes, activation=final_activ)(x)\n        \n        # Before compiling and train the model it is very important to freeze the convolutional base (resnet base).That means, preventing the weights from being updated during training.\n        # If you omit this step, then the representations that were learned previously by the convolutional base will be modified during training.\n        base_vgg16.trainable = False\n        \n        model_vgg16 = Model(base_vgg16.input, preds_base_vgg16)\n               \n        return model_vgg16\n    \n    def train(model, X, y, batch_size, epochs, class_weights, k_fold, loss, optimizer, metrics, model_checkpoint, early_stopping):\n            \n        # use k-fold cross validation test\n        histories = []\n        nb_validation_samples = len(X) // k_fold\n        for fold in range(k_fold):\n            x_training_data = np.concatenate([X[:nb_validation_samples * fold], X[nb_validation_samples * (fold + 1):]])\n            y_training_data = np.concatenate([y[:nb_validation_samples * fold], y[nb_validation_samples * (fold + 1):]])\n\n            x_validation_data = X[nb_validation_samples * fold:nb_validation_samples * (fold + 1)]\n            y_validation_data = y[nb_validation_samples * fold:nb_validation_samples * (fold + 1)]\n\n            model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n\n            history = model.fit_generator(train_data_gen.flow(x_training_data, y_training_data, batch_size=batch_size),\n                                                              validation_data=[x_validation_data, y_validation_data],\n                                                              epochs = epochs,\n                                                              shuffle=True,\n                                                              verbose=2,\n                                                              class_weight=class_weights,\n                                                              steps_per_epoch = int(len(X_train) / batch_size),\n                                                              validation_steps =int(len(X_val) / batch_size),\n                                                              callbacks=[model_checkpoint, early_stopping])\n            histories.append(history)\n        \n        return histories, model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"949dcf89e7c8375f3e741a6e991b7aa3f0c275b8"},"cell_type":"code","source":"model_vgg16 = VGGNet.build(IMAGE_HEIGHT_TARGET, IMAGE_WIDTH_TARGET, 3, len(mlb.classes_), 'sigmoid')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e15f2696c8924d44b65422a955a678e3800c7eb7"},"cell_type":"code","source":"final_activation = 'sigmoid'\nbatch_size = 32\nepochs = 100\nk_fold = 3\nloss = 'binary_crossentropy'\nadam = Adam(lr=0.0001)\noptimizer = adam\nmetrics = ['accuracy']\nearly_stopping = EarlyStopping(patience=10, verbose=1)\nmodel_vgg16_checkpoint = ModelCheckpoint('./b_32_relu_optim_adam.hdf5', verbose=1, save_best_only=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4e165af0c758a0cf3a8c14ed9f8ae2920a243262"},"cell_type":"markdown","source":"**Define the class weights that we are going to use with our model because it has imbalanced set of data**"},{"metadata":{"trusted":true,"_uuid":"c6c3464f9425cac80f2c4f4afb8babc6fe97db04"},"cell_type":"code","source":"class_weights = class_weight.compute_class_weight('balanced', np.unique(y_train[:, -1]), y_train[:, -1]) #-1 is the lats index (volcanoe or not)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c7bb8a49b877c93e6537112fc390f2a8370d70e9"},"cell_type":"code","source":"# Concatenate train and test data\nX_data = np.concatenate((X_train_vggnet, X_val_vggnet))\ny_data = np.concatenate((y_train, y_val))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"68b982ab7b7f5ffe5ff45a82bdeefdf36aa07cfe"},"cell_type":"markdown","source":"**Take only the last index (volcanoe or not)**"},{"metadata":{"trusted":true,"_uuid":"34b766fcc75223d423c7c1595d2697056ee8ecea"},"cell_type":"code","source":"history_vgg16, model_vgg16 = VGGNet.train(model_vgg16,\n                                          X_data,\n                                          y_data,\n                                          batch_size,\n                                          epochs,\n                                          class_weights,\n                                          k_fold,\n                                          loss,\n                                          optimizer,\n                                          metrics,\n                                          model_vgg16_checkpoint,\n                                          early_stopping)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8ecab3dd3dedb5b958f58ba76f50a7f34785a81a","scrolled":false},"cell_type":"code","source":"fig, axes = plt.subplots(k_fold, 2, figsize=(20, 12))\n\nfor i in range(k_fold):\n    \n    axes[i, 0].plot(history_vgg16[i].epoch, history_vgg16[i].history['loss'], label='Train loss')\n    axes[i, 0].plot(history_vgg16[i].epoch, history_vgg16[i].history['val_loss'], label='Val loss')\n    axes[i, 0].legend()\n\n    axes[i, 1].plot(history_vgg16[i].epoch, history_vgg16[i].history['acc'], label = 'Train acc')\n    axes[i, 1].plot(history_vgg16[i].epoch, history_vgg16[i].history['val_acc'], label = 'Val acc')\n    axes[i, 1].legend()\n\n \nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"94b52a48b9ef53b4171c1d5884e486fdbec87fc3"},"cell_type":"code","source":"class ResNet:\n    @staticmethod\n    def build(width, height, depth, classes, final_activ):\n        # Initialize the model to use channels last\n        input_shape = (height, width, depth)\n        \n        # In case where channels first is used\n        if K.image_data_format() == \"channels_first\":\n            input_shape = (depth, height, width)\n            \n        # Load pretrained weights\n        imagenet_weights = '../input/keras-pretrained-models/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5'\n        base_resnet = ResNet50(include_top=False, weights=imagenet_weights, input_shape=input_shape)\n        last_layer = base_resnet.output\n        \n        x = Flatten()(last_layer)\n        x = Dense(512, activation='relu')(x)\n        x = Dropout(0.4)(x)\n        preds_base_resnet = Dense(classes, activation=final_activ)(x)\n        \n        # Before compiling and train the model it is very important to freeze the convolutional base (resnet base).That means, preventing the weights from being updated during training.\n        # If you omit this step, then the representations that were learned previously by the convolutional base will be modified during training.\n        base_resnet.trainable = False\n        \n        model_resnet = Model(base_resnet.input, preds_base_resnet)\n               \n        return model_resnet\n    \n    def train(model, X, y, batch_size, epochs, class_weights, k_fold, loss, optimizer, metrics, model_checkpoint, early_stopping):\n        \n        # use k-fold cross validation test\n        histories = []\n        nb_validation_samples = len(X) // k_fold\n        for fold in range(k_fold):\n            x_training_data = np.concatenate([X[:nb_validation_samples * fold], X[nb_validation_samples * (fold + 1):]])\n            y_training_data = np.concatenate([y[:nb_validation_samples * fold], y[nb_validation_samples * (fold + 1):]])\n            \n            x_validation_data = X[nb_validation_samples * fold:nb_validation_samples * (fold + 1)]\n            y_validation_data = y[nb_validation_samples * fold:nb_validation_samples * (fold + 1)]\n            \n            model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n            \n            history = model.fit_generator(train_data_gen.flow(x_training_data, y_training_data, batch_size=batch_size),\n                                                              validation_data=[x_validation_data, y_validation_data],\n                                                              epochs = epochs,\n                                                              shuffle=True,\n                                                              verbose=2,\n                                                              class_weight=class_weights,\n                                                              steps_per_epoch = int(len(X_train) / batch_size),\n                                                              validation_steps =int(len(X_val) / batch_size),\n                                                              callbacks=[model_checkpoint, early_stopping])\n            histories.append(history)\n            \n        return histories, model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8653769b1b99bc636131ae7ac43f426bd66c1ac1"},"cell_type":"code","source":"model_resnet = ResNet.build(IMAGE_HEIGHT_TARGET, IMAGE_WIDTH_TARGET, 3, len(mlb.classes_), 'sigmoid')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fd48f11411dfe24ebb51dabab49bd669357985eb"},"cell_type":"code","source":"X_train_resnet = np.stack((np.squeeze(X_train_res),) * 3, -1)\nX_val_resnet = np.stack((np.squeeze(X_val_res),) * 3, -1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0c3f11718ba3abdf6f5923061462fb3175f4059f"},"cell_type":"code","source":"X_train_resnet = prep_inputResNet50(X_train_resnet)\nX_val_resnet = prep_inputResNet50(X_val_resnet)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dce400a2fb7f2cebc1c45c23bba96f61a407c93f"},"cell_type":"code","source":"# Concatenate train and test data\nX_data_resnet = np.concatenate((X_train_resnet, X_val_resnet))\ny_data_resnet = np.concatenate((y_train, y_val))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"15eb582854779f0083222a9d9e722e64270fdbae"},"cell_type":"code","source":"model_checkpoint_resnet = ModelCheckpoint('./b_32_relu_optim_adam_resnet.hdf5', verbose=1, save_best_only=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c9a1fd4f32eab9312f02b22f21047aadb3b8c3c6"},"cell_type":"code","source":"history_resnet, model_resnet = ResNet.train(model_resnet,\n                                            X_data_resnet,\n                                            y_data_resnet,\n                                            batch_size,\n                                            epochs,\n                                            class_weights,\n                                            k_fold,\n                                            loss,\n                                            optimizer,\n                                            metrics,\n                                            model_checkpoint_resnet,\n                                            early_stopping)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9cdee9330f7e477014c900ff60f31dccae9065a1"},"cell_type":"code","source":"fig, axes = plt.subplots(k_fold, 2, figsize=(20, 12))\n\nfor i in range(k_fold):\n    \n    axes[i, 0].plot(history_resnet[i].epoch, history_resnet[i].history['loss'], label='Train loss')\n    axes[i, 0].plot(history_resnet[i].epoch, history_resnet[i].history['val_loss'], label='Val loss')\n    axes[i, 0].legend()\n\n    axes[i, 1].plot(history_resnet[i].epoch, history_resnet[i].history['acc'], label = 'Train acc')\n    axes[i, 1].plot(history_resnet[i].epoch, history_resnet[i].history['val_acc'], label = 'Val acc')\n    axes[i, 1].legend()\n\n \nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1e31b48b1003c420597aff1c16025db0108e564c"},"cell_type":"markdown","source":"**Uncomment the `load_weights` lines in order to load the best saved weights. (for some reason kernel couldn't load them even though I have them in my output)**"},{"metadata":{"trusted":true,"_uuid":"1a49f86a5a73bbb1a5e96b0cf5869dcd633414a1"},"cell_type":"code","source":"# Load the best saved model\n#model_vgg16.load_weights(filepath='../input/volcanoes-on-venus-ensemble-imbalanced/b_32_relu_optim_adam.hdf5')\n#model_resnet.load_weights(filepath='../input/volcanoes-on-venus-ensemble-imbalanced/b_32_relu_optim_adam_resnet.hdf5')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"68f27ec7f15fe36e722a83284c3d1ba14638f735"},"cell_type":"markdown","source":"### Create an esemble model.\n\n#### Define a function where we take the average of our two best models."},{"metadata":{"trusted":true,"_uuid":"79202ed04c27f99552f0946453b32fa78972beb3"},"cell_type":"code","source":"def ensemble(models):\n    input_image = Input(shape=(IMAGE_HEIGHT_TARGET, IMAGE_WIDTH_TARGET, 3))\n    \n    vgg16_out = models[0](input_image)\n    resnet_out = models[1](input_image)\n\n    output = Average()([vgg16_out, resnet_out])\n    model = Model(input_image, output)\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2b5afe7bd80f2066ef9403e262b4127cce937ce1"},"cell_type":"code","source":"# Combine all models\nmodels = [model_vgg16, model_resnet]\nensemble_model = ensemble(models)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"75801037aa708368c00a6d5ef5e994baf3b307ef"},"cell_type":"code","source":"test_images.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6f30ba21c1b8ab369582ea746ee6499d0ae369ef"},"cell_type":"code","source":"test_labels.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"60300e13423daf5226e45c285166905aace37dc0"},"cell_type":"markdown","source":"We can see that the first line is the header!\nWe must remove it"},{"metadata":{"trusted":true,"_uuid":"00860a5917d3631d7f80e6b28e420b041e869290"},"cell_type":"code","source":"test_labels = test_labels.drop([0])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0ceaf1e1d725ae36883cf98f3cae145aaaa7b58a"},"cell_type":"markdown","source":"Apply multilabelbinarizer in test data as we did in train data"},{"metadata":{"trusted":true,"_uuid":"6a35a610168a3852c24856def90c70f72c7d1861"},"cell_type":"code","source":"# Replace all float nans with string nans.\ntest_labels = test_labels.fillna('nan')\n# Replace 0 or 1 with No or Yes\ntest_labels.iloc[:, 0] = (test_labels.iloc[:, 0]).str.replace('0', 'No')\ntest_labels.iloc[:, 0] = (test_labels.iloc[:, 0]).str.replace('1', 'Yes')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3cba0b78b7768c17ccd3404447fac8f9efeac717"},"cell_type":"code","source":"# Replace 1,2,3,4 with Type1,2,3,4\ntest_labels.iloc[:, 1] = (test_labels.iloc[:, 1]).str.replace('1', 'Type 1')\ntest_labels.iloc[:, 1] = (test_labels.iloc[:, 1]).str.replace('2', 'Type 2')\ntest_labels.iloc[:, 1] = (test_labels.iloc[:, 1]).str.replace('3', 'Type 3')\ntest_labels.iloc[:, 1] = (test_labels.iloc[:, 1]).str.replace('4', 'Type 4')\ntest_labels.iloc[:, 1] = (test_labels.iloc[:, 1]).str.replace('nan', 'Type nan')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7dad6f5abf4e84771ea7ae39c66dd256c47773fd"},"cell_type":"code","source":"test_labels.iloc[:, 3] = (test_labels.iloc[:, 3]).str.replace('nan', 'Nb volcanoes nan')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9d5235d5d549b1e9840a01e9da3bde6c1866e842"},"cell_type":"markdown","source":"**Before proceeding with mlb I noticed that in the X_test data the number of volcanoes go up to 3 , not 5 as \nin the training data.This results in giving 11 classes instead of 13 as in train mlb.\nSo, we are going to copy 2 rows which contain 4 and 5 nb of volcanoes from train data to test data.\nThe 3rd column which contains the number of volcanoes, it is of type string and contains \neither 'Nb volcanoes nan' either the nb of volcanoes in string format.\n So we must take into consideration only the nb of volcanoes**"},{"metadata":{"trusted":true,"_uuid":"d88edb3d37cf158c9fb47b2fcb9b1428b928bffb"},"cell_type":"code","source":"tmp = train_labels.iloc[:, 3].values\nidx, = np.where(tmp != 'Nb volcanoes nan')\nidx_greater = idx[tmp[idx].astype(int) > 3]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"de97c6a30b9d5cd30c580070ef99abb50f6bf900"},"cell_type":"code","source":"idx_greater","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"abb4e084a67965e577ea363dbbe8954192bac19d"},"cell_type":"markdown","source":"Update the test images and labels with 2 new rows which contain 4 and 5 nb of volcanoes"},{"metadata":{"trusted":true,"_uuid":"6fdf7a4a2be4c12b10f925c5c2a6a2cd3999a7ec"},"cell_type":"code","source":"series_list_images = [pd.Series(train_images.iloc[425, :], index=test_images.columns ) ,\n                      pd.Series(train_images.iloc[1513, :], index=test_images.columns )]\n\nseries_list_labels = [pd.Series(train_labels.iloc[425, :], index=test_labels.columns ) ,\n                      pd.Series(train_labels.iloc[1513, :], index=test_labels.columns )]\n\ntest_images_full = test_images.append(series_list_images , ignore_index=True)\ntest_labels_full = test_labels.append(series_list_labels , ignore_index=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a7dbf37ecbca5e5ffb2b8b33f9d6898ae58290c1"},"cell_type":"markdown","source":"Create a labels list which will hold all the available labels"},{"metadata":{"trusted":true,"_uuid":"2ab0f8ff89fe7553c11604c46b6f0c9f60732531"},"cell_type":"code","source":"labels_test = []\nfor idx in range(len(test_labels_full)):\n    # index 0: Volcanoe or not\n    # index 1: Type\n    # index 3: Nb of volcanoes\n    labels_test.append([test_labels_full.iloc[:, 0].values.item(idx), test_labels_full.iloc[:, 1].values.item(idx), test_labels_full.iloc[:, 3].values.item(idx)]) \n    \nlabels_test = np.array(labels_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cc4a8536ee4e69a760d3276c2329e737f8984ec0"},"cell_type":"code","source":"# Binarize the labels\nlabels_test = mlb.fit_transform(labels_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"74af449d53837730d18479b8dfa5480703741b81"},"cell_type":"code","source":"# Check classes\nmlb.classes_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9952ec4c98cebecab807990161002dc5b3a7ad86"},"cell_type":"code","source":"X_test = test_images_full\ny_test = labels_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"30417faf81c498b5720dec0e2acd7b0573e00b50"},"cell_type":"code","source":"# Reshape test data and create 3 channels\nX_test = X_test.values.reshape((-1, IMAGE_HEIGHT_TARGET, IMAGE_WIDTH_TARGET, 1))\nX_test = np.stack((np.squeeze(X_test),) * 3, -1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"049fa777fd28c057dfdd6a7a7f33a08f013d4903"},"cell_type":"code","source":"# Preprocess data\nX_test = prep_inputVgg16(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b0f20ce65df4a93e5ad61197ba9178f0cbb54941"},"cell_type":"code","source":"# predict on validation and test data\npred_val = ensemble_model.predict(X_val_vggnet) \npred_test = ensemble_model.predict(X_test, batch_size=batch_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bfdfca032bcea86f3600e87c3ce262d9b0f760cb"},"cell_type":"code","source":"# Squeeze one dimension to be able to plot\nX_train_squeeze = X_train_vggnet.squeeze()\ny_train_squeeze = y_train.squeeze()\npred_val_squeeze = pred_val.squeeze()\nX_val_squeeze = X_val_vggnet.squeeze()\ny_val_squeeze = y_val.squeeze()\nX_test_squeeze = X_test.squeeze()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"547c452912c10401963550f7405f32f9d29492d2"},"cell_type":"markdown","source":"#### Create a function in order to be able to scale images from original to target normalization."},{"metadata":{"trusted":true,"_uuid":"58cebb5403ca881747d8d26ff5991317ddd029ac"},"cell_type":"code","source":"def scale_image(input_data, min_orig, max_orig, min_target, max_target):\n    orig_range = max_orig - min_orig\n    target_range = max_target - min_target\n    scaled_data = np.array((input_data - min_orig) / float(orig_range))\n    return min_target + (scaled_data * target_range)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fceb5d81e869cae96bf4139e9b7a09633d0b255d"},"cell_type":"markdown","source":"Denormalize our image in order to properly show it in plot"},{"metadata":{"trusted":true,"_uuid":"b90a15def5e23a238df7f84f47873d3a53689c95"},"cell_type":"code","source":"X_test_denorm = scale_image(X_test_squeeze, X_test_squeeze.min(), X_test_squeeze.max(), 0, 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5a9100e2925e723f92135e424a4b46ffbc87d668"},"cell_type":"code","source":"plt.rc('text', usetex=False)\nmax_images = 6\n\nfig, axes = plt.subplots(max_images//2, 2, figsize=(22, 18))\naxes = axes.ravel()\n\nidxlist = [0, 1, 2, 3, 4, 5]\nfor i in  range(max_images):   \n\n    #idx = np.random.randint(0, len(X_test)-1)\n    idx = idxlist[i]\n    \n    axes[i].grid(False)\n    axes[i].imshow(X_test_denorm[idx], cmap='Greens')\n    axes[i].set_title(mlb.inverse_transform(y_test[idx:idx+1, :]))\n    \n    label_img = []\n    for (label, p) in zip(mlb.classes_, pred_test[idx]):\n        #label_img.append(\"{0}: {1}%\".format(label.astype(np.str), (p * 100).astype(np.float32) ))\n        label_img.append((p * 100).astype(np.float32))\n        \n    text = 'Volcanoe\\nYes: {0:.2f}% No: {1:.2f}%\\nType\\n1: {2:.2f}% 2: {3:.2f}% 3: {4:.2f}% 4: {5:.2f}% nan: {6:.2f}%\\n\\\n            Nb.Volcanoes\\n1: {7:.2f}% 2: {8:.2f}% 3: {9:.2f}% 4: {10:.2f}% 5: {11:.2f}% nan: {12:.2f}%'\\\n            .format(label_img[12], label_img[6], label_img[7], label_img[8], label_img[9], label_img[10], label_img[11], label_img[0], label_img[1],\\\n                   label_img[2], label_img[3], label_img[4], label_img[5])\n    \n    axes[i].text(55, 95, text , size=12, ha=\"center\", va=\"center\",\n            bbox=dict( fc=(1., 1., 0.8), alpha=0.7))\n    \n    plt.subplots_adjust(hspace=0.5, wspace=0.5)\n    plt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8f39ed2ce87c131733aea61f3f6957f358411d78"},"cell_type":"markdown","source":"If you want to play with the plot you must change the `axes[i].text(55, 95,..`  the positions 55, 95 in order for the results to show up. Also, uncomment the idx and use `idx = np.random.randint(0, len(X_test)-1)` in order to get random samples."},{"metadata":{"trusted":true,"_uuid":"89a7a9bbcccf02ebaadb9c25217d07a9942e14af"},"cell_type":"markdown","source":"### Now let's deal with regression in order to find out the radius of the volcanoe."},{"metadata":{"trusted":true,"_uuid":"59ed59c46aff2b78a40a747997240a82de031411"},"cell_type":"code","source":"class Regression:\n    @staticmethod\n    def build(X):\n        model = Sequential()\n        model.add(Dense(1024, activation='relu', input_shape=(X.shape[1],)))\n        model.add(Dropout(0.3))\n        model.add(Dense(256, activation='relu'))\n        model.add(Dropout(0.3))\n        model.add(Dense(128, activation='relu'))\n        model.add(Dense(1))\n       \n        return model\n    \n    def train(X, y, k, optimizer, batch_size, epochs):\n                    \n        # use k-fold cross validation test\n        kfold = KFold(n_splits=k, shuffle=True, random_state=1340)\n        histories = []\n        for train, test in kfold.split(X):\n            model = Regression.build(X)\n            model.compile(loss='mse', optimizer=optimizer, metrics=['mae'])\n            \n            history = model.fit(X[train],\n                                y[train],\n                                validation_data=[X[test], y[test]],\n                                batch_size=batch_size,\n                                epochs = epochs,\n                                verbose=0)\n            \n            # evaluate the model\n            #val_mse, val_mae = model.evaluate(X[test], y[test], verbose=0)\n            mae_history = history.history['val_mean_absolute_error']\n            histories.append(mae_history)\n                    \n        return histories, model","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d2c3cb706c2cc8b111ffb052604027752c183944"},"cell_type":"markdown","source":"**Volcanoes images\nFind the indices where we have a volcanoe,since only volcanoes have radius.**"},{"metadata":{"trusted":true,"_uuid":"e7a57c0f9abbba48ddc5b2bfb9f3ca2d0899a8c8"},"cell_type":"code","source":"indices_radius, = np.where(train_labels.iloc[:, 0] == 'Yes')\nX_volcanoes_radius = train_images.iloc[indices_radius]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"353268507a80cecf4125a740aa54fe16a07b102a"},"cell_type":"code","source":"# Volcanoes labels\ny_volcanoes_radius = train_labels.iloc[indices_radius]\n# take only the radius\ny_volcanoes_radius = y_volcanoes_radius.iloc[:, 2]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0547c14f0f9a343bdae60dd8843a912bbe84cf47"},"cell_type":"markdown","source":"**Split data into train and validation sets**"},{"metadata":{"trusted":true,"_uuid":"f62fa1f231cfb4e7083a30042fbb64eb48dfdeb4"},"cell_type":"code","source":"def data_radius():\n    X_train, X_val, y_train, y_val  = train_test_split(X_volcanoes_radius,\n                                                       y_volcanoes_radius.values.astype(np.float32).reshape(-1, 1),\n                                                       test_size=0.2,\n                                                       random_state=1340)\n        \n    return X_train, X_val, y_train, y_val","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"60b5d3afed82560fc1c4287d79e6ba3efbf24582"},"cell_type":"code","source":"X_train_radius, X_val_radius, y_train_radius, y_val_radius = data_radius()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"87b228c93b8fbad990e4970901551f3e72317430"},"cell_type":"code","source":"# Concatenate train and test data\nX_data_radius = np.concatenate((X_train_radius, X_val_radius))\ny_data_radius = np.concatenate((y_train_radius, y_val_radius))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1b23cf4210771836c067659b229b9435479b20c2","scrolled":true},"cell_type":"code","source":"# initialize scaler\nscaler = StandardScaler()\n# scale data\nscaler.fit(X_data_radius)\nX_std_radius = scaler.transform(X_data_radius)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e8bb3da0c69478479afe5b465ffdf715321bec65"},"cell_type":"code","source":"batch_size_reg = 16\nepochs = 200\nk = 3\nrmsprop = RMSprop(lr=0.001)\n\nhistory_mae, model_reg = Regression.train(X_std_radius, y_data_radius, k, rmsprop, batch_size_reg, epochs)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"27c78d5137fcff1b6879be7dfaf55a87e40b4343"},"cell_type":"markdown","source":"Lets's check the radius values range in order to have a grasp for comparing to mean absolute error"},{"metadata":{"trusted":true,"_uuid":"99def33543b46ef3f26a0a4f83ec0b42dcd14d0c"},"cell_type":"code","source":"y_data_check = y_data_radius.astype(np.float32)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"09f5b3f0495e4d756b8ddd9f3c80ef084ed683dc"},"cell_type":"code","source":"# Find out max radius value\nnp.amax(y_data_check)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d320344cfbc387ab1cda20d3d0576e0dea22518c"},"cell_type":"code","source":"# Find out min radius value\nnp.amin(y_data_check)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"00c50bd3d1c556cb946206460ba556884972d8c4"},"cell_type":"code","source":"# Let's plot the mae\navg_mae = [np.mean([x[i] for x in history_mae]) for i in range(epochs)]\n\nplt.plot(range(1, len(avg_mae) + 1), avg_mae)\nplt.xlabel('Epochs')\nplt.ylabel('Val mae')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"38038604b1c1e062eb729b405c86f98bdbde9534"},"cell_type":"markdown","source":"### Process test data"},{"metadata":{"_uuid":"789f9ee0d0d617bd5ac4487a491753619edbceb0"},"cell_type":"markdown","source":"**Let's work with the indices where we predicted that we have a volcanoe with possibilty >= 50%**"},{"metadata":{"trusted":true,"_uuid":"4de0e110588223272b1b4b37318f31520e7195db"},"cell_type":"code","source":"indices_test,  = np.where(pred_test[:, 12] >= 0.5)\n# Volcanoes test images\nX_volcanoes_test = test_images_full.iloc[indices_test]\n# scale test data\nX_volcanoes_test_std = scaler.transform(X_volcanoes_test.values.astype(np.float32))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"616650b3cb060b248056013fe1fa3704c7c9458e"},"cell_type":"code","source":"# Volcanoes test labels\ny_volcanoes_test = test_labels_full.iloc[indices_test]\n# take only the radius\ny_volcanoes_test = y_volcanoes_test.iloc[:, 2]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0128b656b4062f1905c84ddb9d86ebb68b29cf40"},"cell_type":"markdown","source":"**We can see that in test data we have nan values in cases where we don't have a volcanoe.**\n**We can replace nan values with zero value for radius when we don't have a volcanoe**"},{"metadata":{"trusted":true,"_uuid":"2c7fcddc191f27a6d2722dfa292a6d42b58058c5"},"cell_type":"code","source":"y_volcanoes_test = pd.to_numeric(y_volcanoes_test, errors='coerce')\ny_volcanoes_test = y_volcanoes_test.fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"08986de3b59ea33be4fed46bddcec4326eea8a55"},"cell_type":"markdown","source":"### Evaluate regression model on test data"},{"metadata":{"trusted":true,"_uuid":"c7c2b33830fa845557eea054404c4afe74449a1f"},"cell_type":"code","source":"test_mse, test_mae = model_reg.evaluate(X_volcanoes_test_std, y_volcanoes_test.values.astype(np.float32))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4ff85ba18780b2e423d532013d857d8dc5bdbfb4"},"cell_type":"code","source":"test_mse","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"16f572742b7a78025b66a6ee6c8ce48cbcf9f727"},"cell_type":"code","source":"test_mae","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f58669936844e606738a4b16603af79d9adba581"},"cell_type":"markdown","source":"Mean absolute error returns a value between 8.5-9 which isn't so good. I would expect a value of 1-2 to be good."},{"metadata":{"trusted":true,"_uuid":"50d67bc491845e2354495ee3f25a0587134c5e16"},"cell_type":"code","source":"pred_regression = model_reg.predict(X_volcanoes_test_std)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3b6dea1d7c1dbe0f4dcb2a5f48216908ffd5028e"},"cell_type":"code","source":"# Check a few values\nfor i in range(100):\n    idx = np.random.randint(0, len(y_volcanoes_test.values)-1)\n    print('Real: {0}\\t Pred: {1:.2f}'.format(y_volcanoes_test.values[idx], pred_regression.squeeze()[idx]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c01f12d98e52c14c38eb61fd9845aa35c29028b9"},"cell_type":"markdown","source":"### Conclusion"},{"metadata":{"_uuid":"475a6d0703a805f565b3fbbc43068f23c872921e"},"cell_type":"markdown","source":"We saw that the vggnet and resnet did a pretty good job to classify if an image is volcanoe or not. We had an accuracy of around ! Predicting the type of volcanoe was not that accurate but in general I think it was good. The regression model didn't succeed to predict accurate the radius of the volcanoe. I have tried various combinations and various models but I couldn't make a good model for that. Maybe the information included in the images is not enough to predict the radius?Or maybe due to my inexperience I can't find a good solution."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}