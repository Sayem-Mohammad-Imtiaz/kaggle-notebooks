{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd\n\ndf = pd.read_csv('../input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['review'].duplicated().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop_duplicates(subset = 'review' , keep = False , inplace = True)\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['review'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\ndef clean_html(raw_html):\n  cleanr = re.compile('<.*?>')\n  cleantext = re.sub(cleanr, '', raw_html)\n  return cleantext\n\nimport string\ndef remove_punctuation(text):\n    s = string.punctuation  # Sample string\n    rem_punc = text.translate(str.maketrans('', '', s))\n    return rem_punc\n\ndef remove_url(text):\n    rem_url = re.sub(r'https?://\\S+', '', text)\n    return rem_url\n    \ndef denoise_text(text):\n    text = clean_html(text)\n    text = remove_punctuation(text)\n    text = remove_url(text)\n    return text\n\ndf['review'] = df['review'].apply(denoise_text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df['review'].head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def parse_sentence(sentence):\n    sentence = sentence.lower()\n    return sentence\n\ndf['review'] = df[\"review\"].map(parse_sentence)\nprint(df['review'].head())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\nfrom nltk.corpus import stopwords\n\nstopwords = set(stopwords.words('english'))\n\ndef remove_stopwords(text):   \n    stopwords_removed = \" \".join([word for word in text.split() if word not in stopwords])\n    return stopwords_removed\n\ndf['review'] = df['review'].apply(remove_stopwords)\ndf['review'].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.stem import PorterStemmer \n\ndef simple_stemmer(text): \n    ps = nltk.porter.PorterStemmer()\n    text= ' '.join([ps.stem(word) for word in text.split()])\n    return text \n\ndf['review'] = df['review'].apply(simple_stemmer)\ndf['review'].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.stem import WordNetLemmatizer\n\ndef lemmatize(text):\n    lemmatizer = WordNetLemmatizer()\n    lemmatized_text = ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n    return lemmatized_text\n\ndf['review'] = df['review'].apply(lemmatize)\n\n#Example\ntext = WordNetLemmatizer().lemmatize('eating', 'v')\nprint(text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import spacy\nnlp = spacy.load('en_core_web_sm')\ntext = 'Tesla, Inc. is an American electric vehicle and clean energy company based in  California. The company specializes in electric vehicle manufacturing.Elon Musk is the current CEO. The company values today at $300 billion'\ndoc = nlp(text)\nfor ent in doc.ents:\n      print(ent.text, ent.label_)\n\nfrom spacy import displacy \ndisplacy.render(doc, style='ent')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.tag import pos_tag\nfrom nltk.chunk import ne_chunk","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nTEXT =  'Tesla, Inc. is an American electric vehicle and clean energy company based in  California. The company specializes in electric vehicle manufacturing.Elon Musk is the current CEO. The company values today at $300 billion'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Apply word tokenization and part-of-speech tagging to the sentence.\ndef preprocess(sent):\n    tokenized = nltk.word_tokenize(sent)\n    tagged = nltk.pos_tag(tokenized)\n    return tagged","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Sentence filtered with Word Tokenization\n\nresult = preprocess(TEXT)\nprint(\"POS_Tags for Sentence\")\nresult","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Chunking Pattern \npattern = 'NP: {<DT>?<JJ>*<NN>}'\n#create a chunk parser and test it on our sentence.\ncp = nltk.RegexpParser(pattern)\ncs = cp.parse(result)\nprint(cs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#split the dataset \n\nfrom sklearn.model_selection import train_test_split\ntrain, test = train_test_split(df, test_size = 0.2, random_state=42, shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train.shape, test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer  \n\ncount_vect = CountVectorizer(max_features=5000)\nbow_data = count_vect.fit_transform(train['review'])\nprint(bow_data[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"count_vect = CountVectorizer(ngram_range=(1,2))\nBigram_data = count_vect.fit_transform(train['review'])\nprint(Bigram_data[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer \ntf_idf = TfidfVectorizer(max_features=5000)\ntf_data = tf_idf.fit_transform(train['review'])\nprint(tf_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from gensim.models import Word2Vec\n\nsplitted = []\nw2v_data =  train['review']\n\nfor row in w2v_data: \n    splitted.append([word for word in row.split()])  #splitting words\n    \ntrain_w2v = Word2Vec(splitted,min_count=5,size=50, workers=4)\navg_data = []\n\nfor row in splitted:\n    vec = np.zeros(50)\n    count = 0\n    for word in row:\n        try:\n            vec += train_w2v[word]\n            count += 1\n        except:\n            pass\n    avg_data.append(vec/count)\n    \nprint(avg_data[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['review']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = train['review']\ntf_idf = TfidfVectorizer(max_features=5000)\ntf_idf_data = tf_idf.fit_transform(data)\ntf_w_data = []\ntf_idf_data = tf_idf_data.toarray() # converting to non-sparse array\n\ni = 0\nfor row in splitted:\n    vec = [0 for i in range(50)]\n    \n    temp_tfidf = []\n    for val in tf_idf_data[i]:\n        if val != 0:         \n            temp_tfidf.append(val)\n    \n    count = 0\n    tf_idf_sum = 0\n    for word in row:\n        try:\n            count += 1\n            tf_idf_sum = tf_idf_sum + temp_tfidf[count-1]\n            vec += (temp_tfidf[count-1] * train_w2v[word])\n        except:\n            pass\n    vec = (float)(1/tf_idf_sum) * vec\n    tf_w_data.append(vec)\n    i = i + 1\n\nprint(tf_w_data[1])\n\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"i = 0\nfor row in splitted:\n    vec = [0 for i in range(50)]\n    \n    temp_tfidf = []\n    for val in tf_idf_data[i]:\n        if val != 0:\n            temp_tfidf.append(val)\n    \n    count = 0\n    tf_idf_sum = 0\n    for word in row:\n        try:\n            count += 1\n            tf_idf_sum = tf_idf_sum + temp_tfidf[count-1]\n            vec += (temp_tfidf[count-1] * train_w2v[word])\n        except:\n            pass\n    vec = (float)(1/tf_idf_sum) * vec\n    tf_w_data.append(vec)\n    i = i + 1\n\nprint(tf_w_data[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}