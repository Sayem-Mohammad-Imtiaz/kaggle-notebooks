{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\n#from google.colab.patches import cv2_imshow\nimport matplotlib.pyplot as plt\nimport tensorflow.keras.backend as K\nimport tensorflow as tf\nimport numpy as np\nimport cv2\n\nfrom tensorflow.keras.applications import EfficientNetB0\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Dense,Input,Lambda\n\nimport tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow_datasets as tfds\nfrom functools import partial\nfrom albumentations import (\n    Compose, RandomBrightness, JpegCompression, HueSaturationValue, RandomContrast, HorizontalFlip,Normalize,RandomBrightnessContrast,\n    VerticalFlip,\n    Rotate\n)\nimport pandas as pd\n#from google.colab.patches import cv2_imshow\nimport matplotlib.pyplot as plt\nimport tensorflow.keras.backend as K\nimport tensorflow as tf\nimport numpy as np\nimport cv2\n\nfrom tensorflow.keras.applications import EfficientNetB0\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Dense,Input,Lambda\n\n#from albumentations import *\nAUTOTUNE = tf.data.experimental.AUTOTUNE","metadata":{"id":"99dLJGwSVUkE","execution":{"iopub.status.busy":"2021-06-08T04:07:54.570044Z","iopub.execute_input":"2021-06-08T04:07:54.570286Z","iopub.status.idle":"2021-06-08T04:08:01.26346Z","shell.execute_reply.started":"2021-06-08T04:07:54.570263Z","shell.execute_reply":"2021-06-08T04:08:01.262654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pictureCsv = pd.read_csv(\"../input/mnist-zeroshot-learning/trainPictureArray.csv\")\nsigmoidTrain = pd.read_csv(\"../input/mnist-zeroshot-learning/pictureIndexData.csv\")","metadata":{"id":"y5hEGgMBvdiP","execution":{"iopub.status.busy":"2021-06-08T04:08:01.264798Z","iopub.execute_input":"2021-06-08T04:08:01.265115Z","iopub.status.idle":"2021-06-08T04:08:05.931261Z","shell.execute_reply.started":"2021-06-08T04:08:01.265081Z","shell.execute_reply":"2021-06-08T04:08:05.930384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(\"../input/trainvalsplit/trainSplit.csv\")\nvalidation = pd.read_csv(\"../input/trainvalsplit/validationSplit.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-06-08T04:08:05.934184Z","iopub.execute_input":"2021-06-08T04:08:05.934798Z","iopub.status.idle":"2021-06-08T04:08:05.972744Z","shell.execute_reply.started":"2021-06-08T04:08:05.934756Z","shell.execute_reply":"2021-06-08T04:08:05.971958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pictureCsvNoLabel = pictureCsv.iloc[:,:-1]\npictureCsvNoLabel = tf.convert_to_tensor(pictureCsvNoLabel.to_numpy())","metadata":{"id":"Q2To4a4K-gc9","execution":{"iopub.status.busy":"2021-06-08T04:08:05.975535Z","iopub.execute_input":"2021-06-08T04:08:05.975828Z","iopub.status.idle":"2021-06-08T04:08:08.010928Z","shell.execute_reply.started":"2021-06-08T04:08:05.975801Z","shell.execute_reply":"2021-06-08T04:08:08.010034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"firstPictureIndex = train[\"firstPictureIndex\"].values\nsecondPictureIndex = train[\"secondPictureIndex\"].values\n\nfirstPictureIndexValidation = validation[\"firstPictureIndex\"].values\nsecondPictureIndexValidation = validation[\"secondPictureIndex\"].values","metadata":{"id":"rP4lFqKtv3fC","execution":{"iopub.status.busy":"2021-06-08T04:08:08.015003Z","iopub.execute_input":"2021-06-08T04:08:08.016919Z","iopub.status.idle":"2021-06-08T04:08:08.026761Z","shell.execute_reply.started":"2021-06-08T04:08:08.016877Z","shell.execute_reply":"2021-06-08T04:08:08.025773Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image = pictureCsv.iloc[0][:-1].to_numpy()\nimageV2 = np.stack([image,image,image])","metadata":{"id":"XNb3rqhc69yA","execution":{"iopub.status.busy":"2021-06-08T04:08:08.03045Z","iopub.execute_input":"2021-06-08T04:08:08.032605Z","iopub.status.idle":"2021-06-08T04:08:08.040596Z","shell.execute_reply.started":"2021-06-08T04:08:08.032554Z","shell.execute_reply":"2021-06-08T04:08:08.039838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pictureCsv","metadata":{"id":"dbm4rOZb9DFW","outputId":"fee6b069-fc87-4436-d66a-0875437fc7b2","execution":{"iopub.status.busy":"2021-06-08T04:08:08.044423Z","iopub.execute_input":"2021-06-08T04:08:08.046711Z","iopub.status.idle":"2021-06-08T04:08:08.108774Z","shell.execute_reply.started":"2021-06-08T04:08:08.046673Z","shell.execute_reply":"2021-06-08T04:08:08.108033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pixels = image.reshape((28, 28))\n\n        # Plot\nplt.title('Label is {label}'.format(label=2))\nplt.imshow(pixels, cmap='gray')\nplt.show()","metadata":{"id":"AQggZCB49GOe","outputId":"83211b00-f034-436b-c7bb-bee6d16e15f5","execution":{"iopub.status.busy":"2021-06-08T04:08:08.113481Z","iopub.execute_input":"2021-06-08T04:08:08.115381Z","iopub.status.idle":"2021-06-08T04:08:08.285298Z","shell.execute_reply.started":"2021-06-08T04:08:08.115343Z","shell.execute_reply":"2021-06-08T04:08:08.284511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transforms = Compose([\n            HorizontalFlip(),\n            VerticalFlip(),\n            #Normalize(),\n            #RandomBrightnessContrast(),\n           Normalize()\n        ])","metadata":{"id":"GRmt2gDjWArE","execution":{"iopub.status.busy":"2021-06-08T04:08:08.28702Z","iopub.execute_input":"2021-06-08T04:08:08.28728Z","iopub.status.idle":"2021-06-08T04:08:08.293879Z","shell.execute_reply.started":"2021-06-08T04:08:08.287238Z","shell.execute_reply":"2021-06-08T04:08:08.290461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess_image(index,process_type = \"train\",target_shape = (224,224)):\n    \"\"\"\n    Load the specified file as a JPEG image, preprocess it and\n    resize it to the target shape.\n    \"\"\"\n    image = tf.reshape(pictureCsvNoLabel[index],(28,28))\n    image = tf.stack([image, image, image],axis=2)\n    #image = tf.image.decode_jpeg(image_string, channels=3)\n    image = tf.reshape(image,(28,28,3))*255\n\n    image = tf.image.convert_image_dtype(image, tf.float32)\n    image = tf.image.resize(image, target_shape)\n    \n    if process_type == \"train\":\n        image = process_Augment_data(image,img_size = target_shape[0])\n        \n    return image\n\ndef aug_fn(image, img_size):\n    data = {\"image\":image}\n    aug_data = transforms(image=image)\n    aug_img = aug_data[\"image\"]\n    aug_img = tf.image.resize(aug_img, size=[img_size, img_size])\n    return aug_img\n\ndef process_Augment_data(image, img_size):\n    aug_img = tf.numpy_function(func=aug_fn, inp=[image, img_size], Tout=tf.float64)\n    return aug_img\n\ndef preprocess_siamese(positive, negative):\n    \"\"\"\n    Given the filenames corresponding to the three images, load and\n    preprocess them.\n    \"\"\"\n\n    return (\n        preprocess_image(positive),\n        preprocess_image(negative),\n    )","metadata":{"id":"tQ9wA4GqIIpH","execution":{"iopub.status.busy":"2021-06-08T04:08:08.295308Z","iopub.execute_input":"2021-06-08T04:08:08.295689Z","iopub.status.idle":"2021-06-08T04:08:08.306454Z","shell.execute_reply.started":"2021-06-08T04:08:08.295633Z","shell.execute_reply":"2021-06-08T04:08:08.305475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"imageFinalize = preprocess_image(0)","metadata":{"id":"H1Kk__cv-l1L","outputId":"91250bd6-6556-41a2-9a1f-4c48d0e3721c","execution":{"iopub.status.busy":"2021-06-08T04:08:08.307842Z","iopub.execute_input":"2021-06-08T04:08:08.308199Z","iopub.status.idle":"2021-06-08T04:08:08.67177Z","shell.execute_reply.started":"2021-06-08T04:08:08.308164Z","shell.execute_reply":"2021-06-08T04:08:08.670887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image = pictureCsv.iloc[0][:-1].to_numpy().reshape((28,28))\nimageV2 = np.stack([image,image,image],axis=2)\n#imageV2 = imageV2.reshape((28,28,3))\n#imageV2 = np.swapaxes(imageV2,0,2)\nresized = cv2.resize(imageV2, (224,224), interpolation = cv2.INTER_AREA)\nresized = resized*255","metadata":{"id":"XSXt-gfY7x3C","execution":{"iopub.status.busy":"2021-06-08T04:08:08.673115Z","iopub.execute_input":"2021-06-08T04:08:08.673482Z","iopub.status.idle":"2021-06-08T04:08:08.689733Z","shell.execute_reply.started":"2021-06-08T04:08:08.673445Z","shell.execute_reply":"2021-06-08T04:08:08.688933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import cv2","metadata":{"execution":{"iopub.status.busy":"2021-06-08T04:08:08.690905Z","iopub.execute_input":"2021-06-08T04:08:08.691235Z","iopub.status.idle":"2021-06-08T04:08:08.695072Z","shell.execute_reply.started":"2021-06-08T04:08:08.691201Z","shell.execute_reply":"2021-06-08T04:08:08.694156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(1):  \n  plt.subplot(330 + 1 + i)\n  plt.imshow(imageV2 , cmap=plt.get_cmap('gray'))\n  plt.show()","metadata":{"id":"tkkpypbs7Xcg","outputId":"b2dcbe97-602d-4033-f90a-b3adbb4bb89c","execution":{"iopub.status.busy":"2021-06-08T04:08:08.69662Z","iopub.execute_input":"2021-06-08T04:08:08.697009Z","iopub.status.idle":"2021-06-08T04:08:08.787725Z","shell.execute_reply.started":"2021-06-08T04:08:08.696974Z","shell.execute_reply":"2021-06-08T04:08:08.786825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Add Data Augmentation","metadata":{}},{"cell_type":"code","source":"transforms = Compose([\n            HorizontalFlip(),\n            VerticalFlip(),\n           Normalize()\n        ])\n\nval_transforms = Compose([\n           Normalize()\n        ])","metadata":{"execution":{"iopub.status.busy":"2021-06-08T04:08:08.78902Z","iopub.execute_input":"2021-06-08T04:08:08.789344Z","iopub.status.idle":"2021-06-08T04:08:08.79397Z","shell.execute_reply.started":"2021-06-08T04:08:08.78931Z","shell.execute_reply":"2021-06-08T04:08:08.793077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess_image(index,process_type = \"train\",target_shape = (224,224)):\n    \"\"\"\n    Load the specified file as a JPEG image, preprocess it and\n    resize it to the target shape.\n    \"\"\"\n    image = tf.reshape(pictureCsvNoLabel[index],(28,28))\n    image = tf.stack([image, image, image],axis=2)\n    #image = tf.image.decode_jpeg(image_string, channels=3)\n    image = tf.reshape(image,(28,28,3))*255\n\n    image = tf.image.convert_image_dtype(image, tf.float32)\n    image = tf.image.resize(image, target_shape)\n    \n    if process_type == \"train\":\n        image = process_Augment_data(image,img_size = target_shape[0])\n    elif process_type == \"validation\":\n        image = process_Augment_data_validation(image,img_size = target_shape[0])\n    return image\n\ndef aug_fn(image, img_size):\n    data = {\"image\":image}\n    aug_data = transforms(image=image)\n    aug_img = aug_data[\"image\"]\n    aug_img = tf.image.resize(aug_img, size=[img_size, img_size])\n    return aug_img\n\ndef aug_fn_test(image, img_size):\n    data = {\"image\":image}\n    aug_data = val_transforms(image=image)\n    aug_img = aug_data[\"image\"]\n    aug_img = tf.image.resize(aug_img, size=[img_size, img_size])\n    return aug_img\n\ndef process_Augment_data(image, img_size):\n    aug_img = tf.numpy_function(func=aug_fn, inp=[image, img_size], Tout=tf.float32)\n    return aug_img\n\ndef process_Augment_data_validation(image, img_size):\n    aug_img = tf.numpy_function(func=aug_fn_test, inp=[image, img_size], Tout=tf.float32)\n    return aug_img\n\ndef preprocess_siamese(positive, negative,mode=\"train\"):\n    \"\"\"\n    Given the filenames corresponding to the three images, load and\n    preprocess them.\n    \"\"\"\n\n    return (\n        preprocess_image(positive,mode),\n        preprocess_image(negative,mode),\n    )","metadata":{"id":"AULtmTCMwHn1","execution":{"iopub.status.busy":"2021-06-08T04:08:08.795467Z","iopub.execute_input":"2021-06-08T04:08:08.795935Z","iopub.status.idle":"2021-06-08T04:08:08.809035Z","shell.execute_reply.started":"2021-06-08T04:08:08.795902Z","shell.execute_reply":"2021-06-08T04:08:08.808127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def contrastive_loss(y, preds, margin=1):\n\t# explicitly cast the true class label data type to the predicted\n\t# class label data type (otherwise we run the risk of having two\n\t# separate data types, causing TensorFlow to error out)\n\ty = tf.cast(y, preds.dtype)\n\t# calculate the contrastive loss between the true labels and\n\t# the predicted labels\n\tsquaredPreds = K.square(preds)\n\tsquaredMargin = K.square(K.maximum(margin - preds, 0))\n\tloss = K.mean(y * squaredPreds + (1 - y) * squaredMargin)\n\t# return the computed contrastive loss to the calling function\n\treturn loss","metadata":{"id":"E3AV_jyuxNSn","execution":{"iopub.status.busy":"2021-06-08T04:08:08.810193Z","iopub.execute_input":"2021-06-08T04:08:08.810542Z","iopub.status.idle":"2021-06-08T04:08:08.818182Z","shell.execute_reply.started":"2021-06-08T04:08:08.810509Z","shell.execute_reply":"2021-06-08T04:08:08.817353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sigmoidTrain[\"symmetrylabel\"].values","metadata":{"id":"5ffu7mVJGnq1","outputId":"1de54536-cabe-4534-b557-826a37c51dac","execution":{"iopub.status.busy":"2021-06-08T04:08:08.819451Z","iopub.execute_input":"2021-06-08T04:08:08.81981Z","iopub.status.idle":"2021-06-08T04:08:08.832385Z","shell.execute_reply.started":"2021-06-08T04:08:08.819768Z","shell.execute_reply":"2021-06-08T04:08:08.831549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_dataset(listFirstPictureIndex,listSecondPictureIndex,batchsize,labels,useType=\"train\"):\n    FirstIndex = listFirstPictureIndex\n    SecondIndex = listSecondPictureIndex\n    dataset1 = tf.data.Dataset.from_tensor_slices(FirstIndex)\n    dataset2 = tf.data.Dataset.from_tensor_slices(SecondIndex)\n    labelDatasets = tf.data.Dataset.from_tensor_slices(labels)\n    \n    dataset = tf.data.Dataset.zip((dataset1, dataset2))\n    dataset = dataset.map(lambda x,y:preprocess_siamese(x,y,useType))\n    \n    dataset = tf.data.Dataset.zip((dataset,labelDatasets))\n    dataset = dataset.batch(batchsize, drop_remainder=False).prefetch(8)\n    \n    return dataset","metadata":{"execution":{"iopub.status.busy":"2021-06-08T04:08:08.835446Z","iopub.execute_input":"2021-06-08T04:08:08.835759Z","iopub.status.idle":"2021-06-08T04:08:08.841991Z","shell.execute_reply.started":"2021-06-08T04:08:08.835724Z","shell.execute_reply":"2021-06-08T04:08:08.840874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train[\"symmetrylabel\"].values","metadata":{"execution":{"iopub.status.busy":"2021-06-08T04:08:08.843355Z","iopub.execute_input":"2021-06-08T04:08:08.843704Z","iopub.status.idle":"2021-06-08T04:08:08.852386Z","shell.execute_reply.started":"2021-06-08T04:08:08.843671Z","shell.execute_reply":"2021-06-08T04:08:08.851443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_ds = create_dataset(firstPictureIndex,secondPictureIndex,64,train[\"symmetrylabel\"].values,\"train\")\nval_ds = create_dataset(firstPictureIndexValidation,secondPictureIndexValidation,64,validation[\"symmetrylabel\"].values,\"validation\")","metadata":{"execution":{"iopub.status.busy":"2021-06-08T04:08:08.853553Z","iopub.execute_input":"2021-06-08T04:08:08.854019Z","iopub.status.idle":"2021-06-08T04:08:09.168832Z","shell.execute_reply.started":"2021-06-08T04:08:08.853983Z","shell.execute_reply":"2021-06-08T04:08:09.168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"positive_images = firstPictureIndex \nnegative_images = secondPictureIndex\n\nimage_count = len(positive_images )\n\npositive_dataset = tf.data.Dataset.from_tensor_slices(positive_images)\nnegative_dataset = tf.data.Dataset.from_tensor_slices(negative_images)\nlabel_dataset = tf.data.Dataset.from_tensor_slices(sigmoidTrain[\"symmetrylabel\"].values)\n# To generate the list of negative images, let's randomize the list of\n# available images and concatenate them together.\n\n#negative_images = triplettrain[\"Negative\"].values\n#np.random.RandomState(seed=32).shuffle(negative_images)\n\n#negative_dataset = tf.data.Dataset.from_tensor_slices(negative_images)\n#negative_dataset = negative_dataset.shuffle(buffer_size=4096)\n\ndataset = tf.data.Dataset.zip((positive_dataset, negative_dataset))\n#dataset = dataset.shuffle(buffer_size=1024)\ndataset = dataset.map(preprocess_siamese)\n\ndataset = tf.data.Dataset.zip((dataset,label_dataset))\ndataset = dataset.shuffle(buffer_size=1024)\n\n# Let's now split our dataset in train and validation.\ntrain_dataset = dataset.take(round(image_count * 0.8))\nval_dataset = dataset.skip(round(image_count * 0.8))\n\ntrain_dataset = train_dataset.batch(32, drop_remainder=False)\ntrain_dataset = train_dataset.prefetch(8)\n\nval_dataset = val_dataset.batch(32, drop_remainder=False)\nval_dataset = val_dataset.prefetch(8)\"\"\"","metadata":{"id":"yBU3kmd7-Bce","execution":{"iopub.status.busy":"2021-06-08T04:08:09.170057Z","iopub.execute_input":"2021-06-08T04:08:09.170372Z","iopub.status.idle":"2021-06-08T04:08:09.180039Z","shell.execute_reply.started":"2021-06-08T04:08:09.170339Z","shell.execute_reply":"2021-06-08T04:08:09.178951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for data in train_ds.take(3):\n  break","metadata":{"id":"M3AE4VB4a46U","outputId":"ee84da72-ea79-4f87-f1cc-5d813aa620d7","execution":{"iopub.status.busy":"2021-06-08T04:08:09.184167Z","iopub.execute_input":"2021-06-08T04:08:09.184432Z","iopub.status.idle":"2021-06-08T04:08:10.481002Z","shell.execute_reply.started":"2021-06-08T04:08:09.184388Z","shell.execute_reply":"2021-06-08T04:08:10.480167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for data in val_ds.take(3):\n  break","metadata":{"execution":{"iopub.status.busy":"2021-06-08T04:08:10.482602Z","iopub.execute_input":"2021-06-08T04:08:10.482914Z","iopub.status.idle":"2021-06-08T04:08:11.734735Z","shell.execute_reply.started":"2021-06-08T04:08:10.482885Z","shell.execute_reply":"2021-06-08T04:08:11.732914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def euclidean_distance(vectors):\n\t# unpack the vectors into separate lists\n\t(featsA, featsB) = vectors\n\t# compute the sum of squared distances between the vectors\n\tsumSquared = K.sum(K.square(featsA - featsB), axis=1,keepdims=True)\n\t# return the euclidean distance between the vectors\n\treturn K.sqrt(K.maximum(sumSquared, K.epsilon()))","metadata":{"id":"9qPnG9oC25s-","execution":{"iopub.status.busy":"2021-06-08T04:08:11.736256Z","iopub.execute_input":"2021-06-08T04:08:11.736643Z","iopub.status.idle":"2021-06-08T04:08:11.743467Z","shell.execute_reply.started":"2021-06-08T04:08:11.736597Z","shell.execute_reply":"2021-06-08T04:08:11.742014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input1 = tf.keras.Input(shape=(224,224,3))\ninput2 = tf.keras.Input(shape=(224,224,3))\neffinetB0 = EfficientNetB0(include_top=False,weights=\"imagenet\")(input1)\neffinetB0_siamese = EfficientNetB0(include_top=False,weights=\"imagenet\")(input2)\npooling1= tf.keras.layers.GlobalAveragePooling2D()(effinetB0)\npooling2 = tf.keras.layers.GlobalAveragePooling2D()(effinetB0_siamese)\ndistance = Lambda(euclidean_distance)([pooling1, pooling2])\n\nfirstEffinet = tf.keras.Model(\n    inputs=input1, outputs=pooling1,name = \"effinetB0\"\n)\ntwinEffinet = tf.keras.Model(\n    inputs = input2,outputs= pooling2,name=\"effinetB0_siamese\"\n)\n\ninputA = tf.keras.Input(shape=(224,224,3))\ninputB = tf.keras.Input(shape=(224,224,3))\nfeatureFirst = firstEffinet(inputA)\nfeatureSecond = twinEffinet(inputB)\n\ndistance = Lambda(euclidean_distance)([featureFirst , featureSecond ])\nsiamese_network = Model(inputs=[inputA,inputB], outputs=distance)","metadata":{"id":"piKvWUJ8wehM","outputId":"b4684f85-c209-40a5-8f8a-1778c01d77dc","execution":{"iopub.status.busy":"2021-06-08T04:08:11.745723Z","iopub.execute_input":"2021-06-08T04:08:11.746467Z","iopub.status.idle":"2021-06-08T04:08:17.485862Z","shell.execute_reply.started":"2021-06-08T04:08:11.746427Z","shell.execute_reply":"2021-06-08T04:08:17.485024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def contrastive_loss(y, preds, margin=1):\n\t# explicitly cast the true class label data type to the predicted\n\t# class label data type (otherwise we run the risk of having two\n\t# separate data types, causing TensorFlow to error out)\n\ty = tf.cast(y, preds.dtype)\n\t# calculate the contrastive loss between the true labels and\n\t# the predicted labels\n\tsquaredPreds = K.square(preds)\n\tsquaredMargin = K.square(K.maximum(margin - preds, 0))\n\tloss = K.mean(y * squaredPreds + (1 - y) * squaredMargin)\n\t# return the computed contrastive loss to the calling function\n\treturn loss","metadata":{"id":"Dc8lMAwj6gW4","execution":{"iopub.status.busy":"2021-06-08T04:08:17.487267Z","iopub.execute_input":"2021-06-08T04:08:17.487602Z","iopub.status.idle":"2021-06-08T04:08:17.492451Z","shell.execute_reply.started":"2021-06-08T04:08:17.487557Z","shell.execute_reply":"2021-06-08T04:08:17.491693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"siamese_network.summary()","metadata":{"id":"E1BE4ISHx7JX","outputId":"e17adf87-2f47-440e-ecc3-892c45228fd6","execution":{"iopub.status.busy":"2021-06-08T04:08:17.493672Z","iopub.execute_input":"2021-06-08T04:08:17.494199Z","iopub.status.idle":"2021-06-08T04:08:17.534017Z","shell.execute_reply.started":"2021-06-08T04:08:17.494163Z","shell.execute_reply":"2021-06-08T04:08:17.533281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"siamese_network.compile(loss=contrastive_loss, optimizer=\"adam\")","metadata":{"id":"zUPZU-gq6y_3","execution":{"iopub.status.busy":"2021-06-08T04:08:17.535159Z","iopub.execute_input":"2021-06-08T04:08:17.535469Z","iopub.status.idle":"2021-06-08T04:08:17.557153Z","shell.execute_reply.started":"2021-06-08T04:08:17.535437Z","shell.execute_reply":"2021-06-08T04:08:17.556352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"checkpoint_filepath = \"./siamesemodel{epoch:02d}-validationloss{val_loss:.2f}.h5\"\nmodel_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n    filepath=checkpoint_filepath,\n    save_freq = \"epoch\",\n    save_weights_only=True,\n    monitor='val_loss',\n    save_best_only=False,\n    verbose = 1\n    )","metadata":{"id":"7ffDAJBbGKDG","execution":{"iopub.status.busy":"2021-06-08T04:08:17.558321Z","iopub.execute_input":"2021-06-08T04:08:17.558798Z","iopub.status.idle":"2021-06-08T04:08:17.563559Z","shell.execute_reply.started":"2021-06-08T04:08:17.558755Z","shell.execute_reply":"2021-06-08T04:08:17.562567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"siamese_network.fit(train_ds, epochs=10, validation_data=val_ds,callbacks=[model_checkpoint_callback])","metadata":{"id":"ASSMShXPyKbk","outputId":"3ecc66fd-aea9-4d5d-f3c1-7238248d4a58","execution":{"iopub.status.busy":"2021-06-08T04:08:17.564749Z","iopub.execute_input":"2021-06-08T04:08:17.565103Z","iopub.status.idle":"2021-06-08T05:19:21.444377Z","shell.execute_reply.started":"2021-06-08T04:08:17.565069Z","shell.execute_reply":"2021-06-08T05:19:21.442501Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"ZnIOPkEjGNy_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}