{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Importing required libraries","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nfrom mpl_toolkits.mplot3d import Axes3D","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import plotly.offline as pyoff\nimport plotly.graph_objs as go","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import plotly.graph_objs as go","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Reading Files","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fake = pd.read_csv('/kaggle/input/fake-and-real-news-dataset/Fake.csv')\nreal = pd.read_csv('/kaggle/input/fake-and-real-news-dataset/True.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Data Cleaning","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fake['Target']=1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"real['Target']=0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"frames = [fake, real]\n\ndf = pd.concat(frames)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"removing urls","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"patternDel = \"http\"\nfilter1 = df['date'].str.contains(patternDel)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df[~filter1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pattern = \"Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec\"\nfilter2 = df['date'].str.contains(pattern)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df=df[filter2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['date'] = pd.to_datetime(df['date'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_sub=df.groupby(['subject', 'Target'])['text'].count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_sub","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_sub = df_sub.unstack().fillna(0)\ndf_sub","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualize this data in bar plot\nax = (df_sub).plot(\nkind='bar',\nfigsize=(10, 7),\ngrid=True\n)\nax.set_ylabel('Count')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now it can be said that true news is from subjects like politicsNews and worldnews.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_sub['Count']=df_sub[0]+df_sub[1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Pie chart for Subjects","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import plotly.graph_objects as go\n\nlabels = df_sub.index\nvalues = df_sub['Count']\n\nfig = go.Figure(data=[go.Pie(labels=labels, values=values)])\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_=df.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_=df_.sort_values(by=['date'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_=df_.reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_1=df_[df_['Target']==1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_1=df_1.groupby(['date'])['Target'].count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_1=pd.DataFrame(df_1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_1['Target']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_0=df_[df_['Target']==0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_0=df_0.groupby(['date'])['Target'].count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_0=pd.DataFrame(df_0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Day-wise count of Fake and Real News","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_data = [\n    go.Scatter(\n        x=df_0.index,\n        y=df_0['Target'],\n        name='True',\n        #x_axis=\"OTI\",\n        #y_axis=\"time\",\n    ),\n    go.Scatter(\n        x=df_1.index,\n        y=df_1['Target'],\n        name='Fake'\n    )\n    \n]\nplot_layout = go.Layout(\n        title='Day-wise',\n        yaxis_title='Count',\n        xaxis_title='Time',\n        plot_bgcolor='rgba(0,0,0,0)'\n    )\nfig = go.Figure(data=plot_data, layout=plot_layout)\npyoff.iplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud, STOPWORDS ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" tag cloud (word cloud or wordle or weighted list in visual design) is a novelty visual representation of text data, typically used to depict keyword metadata (tags) on websites, or to visualize free form text. Tags are usually single words, and the importance of each tag is shown with font size or color.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Word Cloud for Fake news","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"comment_words = '' \nstopwords = set(STOPWORDS) \n  \n# iterate through the csv file \nfor val in df[df['Target']==1]['text']: \n      \n    # typecaste each val to string \n    val = str(val) \n  \n\n    tokens = val.split() \n      \n    # Converts each token into lowercase \n    for i in range(len(tokens)): \n        tokens[i] = tokens[i].lower() \n      \n    comment_words += \" \".join(tokens)+\" \"\n  \nwordcloud = WordCloud(width = 800, height = 800, \n                background_color ='black', \n                stopwords = stopwords, \n                min_font_size = 10).generate(comment_words) \n  \n# plot the WordCloud image                        \nplt.figure(figsize = (8, 8), facecolor = None) \nplt.imshow(wordcloud) \nplt.axis(\"off\") \nplt.tight_layout(pad = 0) \n  \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Word Cloud for Real News","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"comment_words = '' \nstopwords = set(STOPWORDS) \n  \n# iterate through the csv file \nfor val in df[df['Target']==0]['text']: \n      \n    # typecaste each val to string \n    val = str(val) \n  \n\n    tokens = val.split() \n      \n    # Converts each token into lowercase \n    for i in range(len(tokens)): \n        tokens[i] = tokens[i].lower() \n      \n    comment_words += \" \".join(tokens)+\" \"\n  \nwordcloud = WordCloud(width = 800, height = 800, \n                background_color ='black', \n                stopwords = stopwords, \n                min_font_size = 10).generate(comment_words) \n  \n# plot the WordCloud image                        \nplt.figure(figsize = (8, 8), facecolor = None) \nplt.imshow(wordcloud) \nplt.axis(\"off\") \nplt.tight_layout(pad = 0) \n  \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Combining 'Subject', 'title' and 'text' into one column.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_['news']=df_['subject']+' '+df_['title']+' '+df_['text']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_['news'] = df_.apply(lambda x: x['news'].lower(),axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_[\"news\"] = df_['news'].str.replace('[^\\w\\s]','')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_news=pd.DataFrame(pd.Series(' '.join(df_['news']).split()).value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"allnews1=all_news.head(30)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Top 30 most frequently occuring words","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_data = [\n    go.Bar(\n        x=allnews1.index,\n        y=allnews1[0],\n        name='True',\n        #x_axis=\"OTI\",\n        #y_axis=\"time\",\n        marker = dict(\n            color = allnews1[0]\n        )\n    )\n]\nplot_layout = go.Layout(\n        title='Top 30 words',\n        yaxis_title='Count',\n        xaxis_title='Word',\n        plot_bgcolor='rgba(0,0,0,0)'\n    )\nfig = go.Figure(data=plot_data, layout=plot_layout)\npyoff.iplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fake_news=pd.DataFrame(pd.Series(' '.join(df_[df_['Target']==1]['news']).split()).value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fake_news30=fake_news.head(30)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Top 30 most Frequently occuring words from Fake news","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_data = [\n    go.Bar(\n        x=fake_news30.index,\n        y=fake_news30[0],\n        #name='True',\n        #x_axis=\"OTI\",\n        #y_axis=\"time\",\n        marker = dict(\n            color = fake_news30[0]\n        )\n    )\n]\nplot_layout = go.Layout(\n        title='Top 30 words from Fake news',\n        yaxis_title='Count',\n        xaxis_title='Word',\n        plot_bgcolor='rgba(0,0,0,0)'\n    )\nfig = go.Figure(data=plot_data, layout=plot_layout)\npyoff.iplot(fig)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Top 30 most Frequently occuring words from True news","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"real_news=pd.DataFrame(pd.Series(' '.join(df_[df_['Target']==0]['news']).split()).value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"real_news30=real_news.head(30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_data = [\n    go.Bar(\n        x=real_news30.index,\n        y=real_news30[0],\n        #name='True',\n        #x_axis=\"OTI\",\n        #y_axis=\"time\",\n        marker = dict(\n            color = real_news30[0]\n        )\n    )\n]\nplot_layout = go.Layout(\n        title='Top 30 words from Real news',\n        yaxis_title='Count',\n        xaxis_title='Word',\n        plot_bgcolor='rgba(0,0,0,0)'\n    )\nfig = go.Figure(data=plot_data, layout=plot_layout)\npyoff.iplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk, re, string, collections\nfrom nltk.util import ngrams","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\nimport unicodedata\nimport nltk\nfrom nltk.corpus import stopwords","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finding N-grams:","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"In the fields of computational linguistics and probability, an n-gram is a contiguous sequence of n items from a given sample of text or speech. The items can be phonemes, syllables, letters, words or base pairs according to the application. The n-grams typically are collected from a text or speech corpus.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def basic_clean(text):\n  \"\"\"\n  A simple function to clean up the data. All the words that\n  are not designated as a stop word is then lemmatized after\n  encoding and basic regex parsing are performed.\n  \"\"\"\n  wnl = nltk.stem.WordNetLemmatizer()\n  stopwords = nltk.corpus.stopwords.words('english')\n  text = (unicodedata.normalize('NFKD', text)\n    .encode('ascii', 'ignore')\n    .decode('utf-8', 'ignore')\n    .lower())\n  words = re.sub(r'[^\\w\\s]', '', text).split()\n  return [wnl.lemmatize(word) for word in words if word not in stopwords]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"words = basic_clean(''.join(str(df_['news'].tolist())))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Bi-grams for news","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Finding top 30 most frequent Bi-grams","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"bigram_all=(pd.Series(nltk.ngrams(words, 2)).value_counts())[:30]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bigram_all=pd.DataFrame(bigram_all)\nbigram_all","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bg_a=bigram_all.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bg_a['in']=bg_a.index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bg_a['in'] = bg_a.apply(lambda x: '('+x['in'][0]+', '+x['in'][1]+')',axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_data = [\n    go.Bar(\n        x=bg_a['in'],\n        y=bg_a[0],\n        #name='True',\n        #x_axis=\"OTI\",\n        #y_axis=\"time\",\n        marker = dict(\n            color = 'blue'\n        )\n    )\n]\nplot_layout = go.Layout(\n        title='Top 30 bigrams from News',\n        yaxis_title='Count',\n        xaxis_title='Word',\n        plot_bgcolor='rgba(0,0,0,0)'\n    )\nfig = go.Figure(data=plot_data, layout=plot_layout)\npyoff.iplot(fig)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finding top 30 most frequent Tri-grams","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"trigram_all=(pd.Series(nltk.ngrams(words, 3)).value_counts())[:30]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trigram_all=pd.DataFrame(trigram_all)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trigram_all","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trigram_all['in']=trigram_all.index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trigram_all['in'] = trigram_all.apply(lambda x: '('+x['in'][0]+', '+x['in'][1]+', '+x['in'][2]+')',axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trigram_all","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_data = [\n    go.Bar(\n        x=trigram_all['in'],\n        y=trigram_all[0],\n        #name='True',\n        #x_axis=\"OTI\",\n        #y_axis=\"time\",\n        marker = dict(\n            color = 'blue'\n        )\n    )\n]\nplot_layout = go.Layout(\n        title='Top 30 trigrams from News',\n        yaxis_title='Count',\n        xaxis_title='tri-gram',\n        plot_bgcolor='rgba(0,0,0,0)'\n    )\nfig = go.Figure(data=plot_data, layout=plot_layout)\npyoff.iplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fake_words = basic_clean(''.join(str(df_[df_['Target']==1]['news'].tolist())))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finding top 30 most frequent Bi-grams from Fake News","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"bigram_fake=(pd.Series(nltk.ngrams(fake_words, 2)).value_counts())[:30]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bigram_fake=pd.DataFrame(bigram_fake)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bigram_fake","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bigram_fake['in']=bigram_fake.index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bigram_fake['in'] = bigram_fake.apply(lambda x: '('+x['in'][0]+', '+x['in'][1]+')',axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_data = [\n    go.Bar(\n        x=bigram_fake['in'],\n        y=bigram_fake[0],\n        #name='True',\n        #x_axis=\"OTI\",\n        #y_axis=\"time\",\n        marker = dict(\n            color = 'Red'\n        )\n    )\n]\nplot_layout = go.Layout(\n        title='Top 30 bi-grams from Fake News',\n        yaxis_title='Count',\n        xaxis_title='bi-gram',\n        plot_bgcolor='rgba(0,0,0,0)'\n    )\nfig = go.Figure(data=plot_data, layout=plot_layout)\npyoff.iplot(fig)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finding top 30 most frequent Tri-grams from Fake News","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"trigram_fake=(pd.Series(nltk.ngrams(fake_words, 3)).value_counts())[:30]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trigram_fake=pd.DataFrame(trigram_fake)\ntrigram_fake","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trigram_fake['in']=trigram_fake.index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trigram_fake['in'] = trigram_fake.apply(lambda x: '('+x['in'][0]+', '+x['in'][1]+', '+x['in'][2]+')',axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_data = [\n    go.Bar(\n        x=trigram_fake['in'],\n        y=trigram_fake[0],\n        #name='True',\n        #x_axis=\"OTI\",\n        #y_axis=\"time\",\n        marker = dict(\n            color = 'Red'\n        )\n    )\n]\nplot_layout = go.Layout(\n        title='Top 30 tri-grams from Fake News',\n        yaxis_title='Count',\n        xaxis_title='tri-gram',\n        plot_bgcolor='rgba(0,0,0,0)'\n    )\nfig = go.Figure(data=plot_data, layout=plot_layout)\npyoff.iplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"true_words = basic_clean(''.join(str(df_[df_['Target']==0]['news'].tolist())))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finding top 30 most frequent Bi-grams from True News","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"bigram_true=(pd.Series(nltk.ngrams(true_words, 2)).value_counts())[:30]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bigram_true=pd.DataFrame(bigram_true)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bigram_true","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bigram_true['in']=bigram_true.index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bigram_true['in'] = bigram_true.apply(lambda x: '('+x['in'][0]+', '+x['in'][1]+')',axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_data = [\n    go.Bar(\n        x=bigram_true['in'],\n        y=bigram_true[0],\n        #name='True',\n        #x_axis=\"OTI\",\n        #y_axis=\"time\",\n        marker = dict(\n            color = 'Green'\n        )\n    )\n]\nplot_layout = go.Layout(\n        title='Top 30 bi-grams from True News',\n        yaxis_title='Count',\n        xaxis_title='bi-gram',\n        plot_bgcolor='rgba(0,0,0,0)'\n    )\nfig = go.Figure(data=plot_data, layout=plot_layout)\npyoff.iplot(fig)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finding top 30 most frequent Tri-grams from Fake News","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"trigram_true=(pd.Series(nltk.ngrams(true_words, 3)).value_counts())[:30]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trigram_true=pd.DataFrame(trigram_true)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trigram_true","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trigram_true['in']=trigram_true.index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trigram_true['in'] = trigram_true.apply(lambda x: '('+x['in'][0]+', '+x['in'][1]+', '+x['in'][2]+')',axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_data = [\n    go.Bar(\n        x=trigram_true['in'],\n        y=trigram_true[0],\n        #name='True',\n        #x_axis=\"OTI\",\n        #y_axis=\"time\",\n        marker = dict(\n            color = 'Green'\n        )\n    )\n]\nplot_layout = go.Layout(\n        title='Top 30 tri-grams from True News',\n        yaxis_title='Count',\n        xaxis_title='tri-gram',\n        plot_bgcolor='rgba(0,0,0,0)'\n    )\nfig = go.Figure(data=plot_data, layout=plot_layout)\npyoff.iplot(fig)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"word2vec:\n1. Word2vec is a two-layer neural net that processes text by “vectorizing” words. Its input is a text corpus and its output is a set of vectors: feature vectors that represent words in that corpus. While Word2vec is not a deep neural network, it turns text into a numerical form that deep neural networks can understand.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y = df_['Target'].values\n#Converting X to format acceptable by gensim, removing annd punctuation stopwords in the process\nX = []\nstop_words = set(nltk.corpus.stopwords.words(\"english\"))\ntokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\nfor par in df_[\"news\"].values:\n    tmp = []\n    sentences = nltk.sent_tokenize(par)\n    for sent in sentences:\n        sent = sent.lower()\n        tokens = tokenizer.tokenize(sent)\n        filtered_words = [w.strip() for w in tokens if w not in stop_words and len(w) > 1]\n        tmp.extend(filtered_words)\n    X.append(tmp)\n\n#del data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import gensim","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Dimension of vectors we are generating\nEMBEDDING_DIM = 100\n\n#Creating Word Vectors by Word2Vec Method (takes time...)\nw2v_model = gensim.models.Word2Vec(sentences=X, size=EMBEDDING_DIM, window=5, min_count=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(w2v_model.wv.vocab)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"w2v_model[\"trump\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finding most similar words using word2vec","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"w2v_model.wv.most_similar(\"trump\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"w2v_model.wv.most_similar(\"obama\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"w2v_model.wv.most_similar(\"news\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Modeling (To Detect Fake news)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n\nX_train, X_test, y_train, y_test = train_test_split(df_['news'], \n                                                    df_['Target'], \n                                                    random_state=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using CountVectorizer","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\n\n# Fit the CountVectorizer to the training data\nvect = CountVectorizer().fit(X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_vectorized = vect.transform(X_train)\n\nX_train_vectorized","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Logistic Regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\n# Train the model\nmodel = LogisticRegression()\nmodel.fit(X_train_vectorized, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\n\n# Predict the transformed test documents\npredictions = model.predict(vect.transform(X_test))\n\nprint('AUC: ', roc_auc_score(y_test, predictions))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"AUC is 0.9984 which means this model is a good fit.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\nprint(metrics.confusion_matrix(y_test, predictions, labels=[0, 1]))\n# Printing the precision and recall, among other metrics\nprint(metrics.classification_report(y_test, predictions, labels=[0, 1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, predictions , labels=[0, 1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Confusion Matrix","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ax= plt.subplot()\nsns.heatmap(cm, annot=True, ax= ax)\nax.set_ylabel('y_test')\nax.set_xlabel('predictions')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get the feature names as numpy array\nfeature_names = np.array(vect.get_feature_names())\n\n# Sort the coefficients from the model\nsorted_coef_index = model.coef_[0].argsort()\n\n# Find the 10 smallest and 10 largest coefficients\n# The 10 largest coefficients are being indexed using [:-11:-1] \n# so the list returned is in order of largest to smallest\nprint('Smallest Coefs:\\n{}\\n'.format(feature_names[sorted_coef_index[:10]]))\nprint('Largest Coefs: \\n{}'.format(feature_names[sorted_coef_index[:-11:-1]]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Multinomial Naive Bayes","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB \nvectorizer = CountVectorizer() \nX_train_transformed = vectorizer.fit_transform(X_train) \nX_test_transformed = vectorizer.transform(X_test)\n\nclf = MultinomialNB(alpha=0.1) \nclf.fit(X_train_transformed, y_train)\n\ny_predicted = clf.predict(X_test_transformed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('AUC: ', roc_auc_score(y_test, y_predicted))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Confusion Matrix","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\nprint(metrics.confusion_matrix(y_test, y_predicted, labels=[0, 1]))\n# Printing the precision and recall, among other metrics\nprint(metrics.classification_report(y_test, y_predicted, labels=[0, 1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax= plt.subplot()\nsns.heatmap(cm, annot=True, ax= ax)\nax.set_ylabel('y_test')\nax.set_xlabel('predictions')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using TfidfVectorizer","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\n# Fit the TfidfVectorizer to the training data specifiying a minimum document frequency of 5\nvect = TfidfVectorizer(min_df=5).fit(X_train)\nlen(vect.get_feature_names())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Logistic Regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_vectorized = vect.transform(X_train)\n\nmodel = LogisticRegression()\nmodel.fit(X_train_vectorized, y_train)\n\npredictions = model.predict(vect.transform(X_test))\n\nprint('AUC: ', roc_auc_score(y_test, predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\nprint(metrics.confusion_matrix(y_test, predictions, labels=[0, 1]))\n# Printing the precision and recall, among other metrics\nprint(metrics.classification_report(y_test, predictions, labels=[0, 1]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Confsion Matrix","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cm = confusion_matrix(y_test, predictions , labels=[0, 1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax= plt.subplot()\nsns.heatmap(cm, annot=True, ax= ax)\nax.set_ylabel('y_test')\nax.set_xlabel('predictions')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_names = np.array(vect.get_feature_names())\n\nsorted_tfidf_index = X_train_vectorized.max(0).toarray()[0].argsort()\n\nprint('Smallest tfidf:\\n{}\\n'.format(feature_names[sorted_tfidf_index[:10]]))\nprint('Largest tfidf: \\n{}'.format(feature_names[sorted_tfidf_index[:-11:-1]]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nsorted_coef_index = model.coef_[0].argsort()\n\nprint('Smallest Coefs:\\n{}\\n'.format(feature_names[sorted_coef_index[:10]]))\nprint('Largest Coefs: \\n{}'.format(feature_names[sorted_coef_index[:-11:-1]]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Multinomial Naive Bayes","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"vectorizer = TfidfVectorizer(min_df=3)\nX_train_transformed = vectorizer.fit_transform(X_train)\nX_test_transformed = vectorizer.transform(X_test)\n\nclf = MultinomialNB(alpha=0.1)\nclf.fit(X_train_transformed, y_train)\n\n# y_predicted_prob = clf.predict_proba(X_test_transformed)[:, 1]\ny_predicted = clf.predict(X_test_transformed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('AUC: ', roc_auc_score(y_test, y_predicted))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(metrics.confusion_matrix(y_test, y_predicted, labels=[0, 1]))\n# Printing the precision and recall, among other metrics\nprint(metrics.classification_report(y_test, y_predicted, labels=[0, 1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cm = confusion_matrix(y_test, y_predicted , labels=[0, 1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax= plt.subplot()\nsns.heatmap(cm, annot=True, ax= ax)\nax.set_ylabel('y_test')\nax.set_xlabel('predictions')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"N-gram Analysis","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#ngrams\n# Fit the CountVectorizer to the training data specifiying a minimum \n# document frequency of 5 and extracting 1-grams and 2-grams\nvect = CountVectorizer(min_df=5, ngram_range=(1,2)).fit(X_train)\n\nX_train_vectorized = vect.transform(X_train)\n\nlen(vect.get_feature_names())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Logistic Regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\nmodel = LogisticRegression()\nmodel.fit(X_train_vectorized, y_train)\n\npredictions = model.predict(vect.transform(X_test))\n\nprint('AUC: ', roc_auc_score(y_test, predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(metrics.confusion_matrix(y_test, predictions, labels=[0, 1]))\n# Printing the precision and recall, among other metrics\nprint(metrics.classification_report(y_test, predictions, labels=[0, 1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cm = confusion_matrix(y_test, predictions , labels=[0, 1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax= plt.subplot()\nsns.heatmap(cm, annot=True, ax= ax)\nax.set_ylabel('y_test')\nax.set_xlabel('predictions')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_names = np.array(vect.get_feature_names())\n\nsorted_coef_index = model.coef_[0].argsort()\n\nprint('Smallest Coefs:\\n{}\\n'.format(feature_names[sorted_coef_index[:10]]))\nprint('Largest Coefs: \\n{}'.format(feature_names[sorted_coef_index[:-11:-1]]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now the winner is Logistic Regression with n-gram range 1, 2 for the detection of fake news.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}