{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport cv2\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\nimport torchvision\nfrom torchvision import transforms\nfrom PIL import Image\nimport pytorch_lightning as pl\nimport torchvision.transforms as T\nimport torch.nn as nn\nfrom torch import optim\nimport torch.nn.functional as F\nfrom pytorch_lightning.core.lightning import LightningModule\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/pokemon-images-and-types/pokemon.csv')\ndf2 = df.sort_values(by=['Name'], ascending=True).reset_index(drop=True)\nimage_names = os.listdir('../input/pokemon-images-and-types/images/images/')\ndf2['Image'] = image_names\ndf2['Valid'] = df2['Image'].apply(lambda x: 0 if x[-2]=='p' else 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df3 = df2[df2['Valid']==1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_size = len(df2)\nprint(data_size)\ndf_train = df3.iloc[:621,:].reset_index(drop=True)\ndf_val = df3.iloc[621:721,:] .reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Data Loader"},{"metadata":{"trusted":true},"cell_type":"code","source":"class DatasetPokemon(Dataset):\n    \n    def __init__(self, df,image_dir,transform=None):\n        #self.data = pd.read_csv(file_path)\n        self.transform = transform\n        self.df = df\n        self.image_dir = image_dir\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, index):\n        # load image as ndarray type (Height * Width * Channels)\n        # be carefull for converting dtype to np.uint8 [Unsigned integer (0 to 255)]\n        # in this example, i don't use ToTensor() method of torchvision.transforms\n        # so you can convert numpy ndarray shape to tensor in PyTorch (H, W, C) --> (C, H, W)\n        img_path = self.image_dir + self.df['Image'][index]\n        image = Image.open(img_path).convert('RGB')\n        label = Image.open(img_path).convert('RGB')\n        image = np.asarray(image, dtype=np.float32)/255\n        label = np.asarray(label, dtype=np.float32)/255\n        image = torch.from_numpy(image)\n        label = torch.from_numpy(label)\n#         print(image.size())\n        image = image.permute(2,0,1)\n        label = label.permute(2,0,1)\n#         if self.transform is not None:\n#             image = self.transform(image)\n#             label = self.transform(label)\n\n        return image, label","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img = Image.open('../input/pokemon-images-and-types/images/images/brionne.jpg').convert('RGB')\n# img.crop(l,t,r,b)\nimg.size\nimg","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"im_transform = transform(img)\nim_transform[im_transform<1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"transform = T.Compose([\n        T.ToTensor()\n#     ,transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"file_path = '../input/pokemon-images-and-types/pokemon.csv'\nimage_dir = '../input/pokemon-images-and-types/images/images/'\nbatch_size = 8\ntrain_dataset = DatasetPokemon(df_train,image_dir,transform=transform)\ntrain_loader = DataLoader(train_dataset, batch_size, shuffle=True, num_workers=3, pin_memory=True)\n\nval_dataset = DatasetPokemon(df_val,image_dir,transform=transform)\nval_loader = DataLoader(val_dataset, batch_size, shuffle=True, num_workers=3, pin_memory=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for x,y in train_loader:\n    print(x.size())\n    break","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_params ={'train_loss':[],'val_loss':[]}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class VAE(LightningModule):\n    def __init__(self,in_channels,latent_dim):\n        super(VAE, self).__init__()\n        hidden_dims = [32, 64, 128]\n        self.latent_dim = latent_dim\n        modules=[]\n        for h_dim in hidden_dims:\n            modules.append(\n                nn.Sequential(\n                    nn.Conv2d(in_channels, out_channels=h_dim,\n                              kernel_size= 3, stride= 2, padding  = 1),\n                    nn.BatchNorm2d(h_dim),\n                    nn.LeakyReLU())\n            )\n            in_channels = h_dim\n        \n        self.encoder = nn.Sequential(*modules)\n        self.fc_mu = nn.Linear(hidden_dims[-1]*15*15, latent_dim)\n        self.fc_var = nn.Linear(hidden_dims[-1]*15*15, latent_dim)\n        self.avg_train_loss = {'loss': 0, 'Reconstruction_Loss':0, 'KLD':0}\n        self.avg_val_loss = {'loss': 0, 'Reconstruction_Loss':0, 'KLD':0}\n        self.temp_train = {}\n        self.temp_val = {}\n        hidden_dims.reverse()\n        modules = []\n        for i in range(len(hidden_dims) - 1):\n            modules.append(\n                nn.Sequential(\n                    nn.ConvTranspose2d(hidden_dims[i],\n                                       hidden_dims[i + 1],\n                                       kernel_size=3,\n                                       stride = 2,\n                                       padding=1,\n                                       output_padding=1),\n                    nn.BatchNorm2d(hidden_dims[i + 1]),\n                    nn.LeakyReLU())\n            )\n\n        self.decoder_input = nn.Linear(latent_dim, hidden_dims[0]*15*15)\n\n        self.decoder = nn.Sequential(*modules)\n\n        self.final_layer = nn.Sequential(\n                            nn.ConvTranspose2d(hidden_dims[-1],\n                                               hidden_dims[-1],\n                                               kernel_size=3,\n                                               stride=2,\n                                               padding=1,\n                                               output_padding=1),\n                            nn.BatchNorm2d(hidden_dims[-1]),\n                            nn.LeakyReLU(),\n                            nn.Conv2d(hidden_dims[-1], out_channels= 3,\n                                      kernel_size= 3, padding= 1),\n                            nn.Sigmoid())\n    \n    def train_dataloader(self):\n        return train_loader\n    \n    def val_dataloader(self):\n        return val_loader\n        \n    def encode(self,input):\n\n        result = self.encoder(input)\n\n        result = torch.flatten(result, start_dim=1)\n\n        # Split the result into mu and var components\n        # of the latent Gaussian distribution\n        mu = self.fc_mu(result)\n        log_var = self.fc_var(result)\n        return mu, log_var\n    \n    def decode(self,z):\n        result = self.decoder_input(z)\n\n        result = result.view(-1, 128, 15, 15)\n        result = self.decoder(result)\n        result = self.final_layer(result)\n        return result\n    \n    def reparameterize(self, mu, logvar):\n#         print(\"reparam\")\n        std = torch.exp(0.5 * logvar)\n        eps = torch.randn_like(std)\n        return eps * std + mu\n    \n    def forward(self, input):\n        mu,log_var = self.encode(input)\n        z = self.reparameterize(mu, log_var)\n        return  self.decode(z), mu, log_var\n    \n    def loss_function(self,recons,label,mu,log_var):\n        \n        recons_loss =F.mse_loss(recons, label,reduction='mean')\n#         print(\"prediction:\",recons[recons>0],\"output:\",label[label>0])\n        kld_loss = torch.mean(-0.5 * torch.sum(1 + log_var - mu ** 2 - log_var.exp(), dim = 1), dim = 0)\n#         kld_loss = -0.5 * torch.sum(1 + log_var - mu ** 2 - log_var.exp())\n        \n#         recons_loss = F.binary_cross_entropy_with_logits(recons, label, size_average=False)\n#         kld_loss = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp()).mean()\n        \n        loss = recons_loss + kld_loss\n        loss_dict = {'loss': loss.item(), 'Reconstruction_Loss':recons_loss.item(), 'KLD':-kld_loss.item()}\n        self.temp_train = loss_dict\n        self.log('train_loss', loss_dict, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n        return loss\n    \n    def sample(self,\n               num_samples,\n               current_device):\n        z = torch.randn(num_samples,\n                        self.latent_dim)\n\n\n        samples = self.decode(z)\n        return samples\n                       \n    def generate(self, x):\n        return self.forward(x)[0]\n                       \n    def training_step(self, batch, batch_idx):\n        \n        real_img, label = batch\n#         print(real_img.size(),label.size())\n        self.curr_device = real_img.device\n\n        recons,mu,log_var = self.forward(real_img)\n        train_loss = self.loss_function(recons,label,mu,log_var)\n\n        self.avg_train_loss['loss'] =self.avg_train_loss['loss'] + self.temp_train['loss']\n        self.avg_train_loss['Reconstruction_Loss'] =self.avg_train_loss['Reconstruction_Loss'] + self.temp_train['Reconstruction_Loss']\n        self.avg_train_loss['KLD'] =self.avg_train_loss['KLD'] + self.temp_train['KLD']\n        return train_loss\n    \n    def training_epoch_end(self,outputs):\n        size = 89\n        print('Train:','Total Loss: ',self.avg_train_loss['loss']/size,' Recon Loss: ',self.avg_train_loss['Reconstruction_Loss']/size,\n             'KLD: ',self.avg_train_loss['KLD']/size)\n        model_params['train_loss'].append(self.avg_train_loss)\n        self.avg_train_loss = {'loss': 0, 'Reconstruction_Loss':0, 'KLD':0}\n        \n    \n    def validation_step(self,batch,batch_idx):\n        real_img, label = batch\n        self.curr_device = real_img.device\n#         print(real_img.size(),label.size())\n        recons,mu,log_var = self.forward(real_img)\n#         print(recons.size(),label.size(),input.size())\n        val_loss = self.loss_function(recons,label,mu,log_var)\n        \n        self.avg_val_loss['loss'] =self.avg_val_loss['loss'] + self.temp_train['loss']\n        self.avg_val_loss['Reconstruction_Loss'] =self.avg_train_loss['Reconstruction_Loss'] + self.temp_train['Reconstruction_Loss']\n        self.avg_val_loss['KLD'] =self.avg_val_loss['KLD'] + self.temp_train['KLD']\n\n        self.log('val_loss', val_loss.item(), on_step=True, on_epoch=True, prog_bar=True, logger=True)\n        return val_loss\n    \n    def validation_epoch_end(self,outputs):\n        size = 13\n        print('Val:','Total Loss: ',self.avg_val_loss['loss']/size,' Recon Loss: ',self.avg_val_loss['Reconstruction_Loss']/size,\n             'KLD: ',self.avg_val_loss['KLD']/size)\n    \n        model_params['val_loss'].append(self.avg_val_loss)\n        self.avg_val_loss = {'loss': 0, 'Reconstruction_Loss':0, 'KLD':0}\n    \n    def configure_optimizers(self):\n        return optim.Adam(self.parameters())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# %debug\nmodel = VAE(3,512)\ntrainer = pl.Trainer(max_epochs=5)\ntrainer.fit(model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.display import Image","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(model_params['train_loss'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## VAE"},{"metadata":{"trusted":true},"cell_type":"code","source":"def show_image(x):\n    img = x[0]\n    img = img.detach().numpy()\n    img = img.transpose((1, 2, 0))\n    im = Image.fromarray((img*255).astype(np.uint8)).convert('RGB')\n#     print((img*255).astype(np.uint8))\n#     mean = np.array([0.485, 0.456, 0.406])\n#     std = np.array([0.229, 0.224, 0.225])\n#     img_norm = std * img + mean\n#     img_norm = np.clip(img_norm, 0, 1)\n#     plt.imshow(im)\n#     print(img)\n    return im","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = model.sample(8,0)\nprint(x)\nim = show_image(x)\nim","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img = Image.open('../input/pokemon-images-and-types/images/images/abomasnow.png').convert('RGB')\n# img.crop(l,t,r,b)\nimg.size\nim_transform = transform(img).unsqueeze(0)\n\nx = model.generate(im_transform)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x[x>.9]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"im_transform[im_transform>.9]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"im = show_image(x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = model.sample(1,0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x*255","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nimg.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean = np.array([0.485, 0.456, 0.406])\nstd = np.array([0.229, 0.224, 0.225])\nimg_norm = std * img + mean\nimg_norm = np.clip(img_norm, 0, 1)\nplt.imshow(img_norm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.imshow(img)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Hierarchical VAE"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}