{"cells":[{"metadata":{"trusted":true,"_uuid":"3a4e8f90b576638b135a42ddf315a6081d8b260d"},"cell_type":"code","source":"\n#Team Members\n#Vinay Kumar S R - 01FB16ECS446\n#Vivek R - 91FB16ECS455\n\nimport numpy as np \n\nimport pandas as pd\n\nFile=pd.read_csv(\"../input/Absenteeism_at_work.csv\")\nFile.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"02ebe9f83ce82fee2cafe63e91a4038f4740ccfa"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nFile.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c53abd1b5532bc67b4ed1ec6d320ac4075e27aed"},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cfdab7412d7268a7af0c9cc0c66904f8954ef396"},"cell_type":"code","source":"#Preprocessing data - stage 1 (Removing outliers in label)\nsns.boxplot(File['Absenteeism time in hours'])\n\nmedian = np.median(File['Absenteeism time in hours'])\nq75, q25 = np.percentile(File['Absenteeism time in hours'], [75 ,25])\niqr = q75 - q25\nprint(\"Lower outlier bound:\",q25 - (1.5*iqr))\nprint(\"Upper outlier bound:\",q75 + (1.5*iqr))\n\nFile= File[File['Absenteeism time in hours']<=17]\nFile= File[File['Absenteeism time in hours']>=-7]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"54fb5b8968e2c4798ffee65a84f168bf85762c2c"},"cell_type":"code","source":"print(\"count for Output class:\")\nFile['Absenteeism time in hours'].value_counts(sort = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c5b3dfb3910406c135a997b1c9699269f56d8fa9"},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(20, 20)) \nsns.heatmap(File.corr(), annot = True, ax = ax)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"29882dacc6b0db5aafd6f0f663ed3c603cfd5937"},"cell_type":"code","source":"#Splitting data into training and testing\nfrom sklearn.model_selection import train_test_split\ny=File['Absenteeism time in hours']\nX=File.drop('Absenteeism time in hours',axis=1)#Extracting only the features\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,random_state=42)\nprint(File.shape)\nprint(X_train.shape, y_train.shape)\nprint(X_test.shape, y_test.shape)\nprint(\"Number of unique ouput classes after preprocessing:\",((np.unique(y_train))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7ba5cee879d836aa5d71d68498bbc18b34ee3db3"},"cell_type":"code","source":"\n#Dropping the following attributes due to multi-collinearity\nX_train=X_train.drop('Service time',axis=1)\nX_test=X_test.drop('Service time',axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d3d6e88f7266860bd9d944836e379b9f033ef94e"},"cell_type":"code","source":"#Normalizing features\nfrom sklearn import preprocessing\nX_scaled_train = preprocessing.scale(X_train)\nX_scaled_test = preprocessing.scale(X_test)\nX_scaled_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"33be0f407b57cab9334ed72c7625c3b01c26572d"},"cell_type":"code","source":"# 1 ->Classification technique using SVM (SV classifier)\nfrom sklearn import metrics, svm\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import classification_report\n\n\nsvm=svm.SVC()\nsvm.fit(X_scaled_train, y_train)\ny_pred = svm.predict(X_scaled_test)\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\nprint(\"------------------------\\n\")\nprint(classification_report(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"38b3d252e9cf0a7f35a9884fa8f87875202dde8e"},"cell_type":"code","source":"#  2 -> Classification technique using KNN \n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import metrics","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a15f0681e39e9a9b362df797fc79321d30264308"},"cell_type":"code","source":"error=[]\naccuracy=[]\nfor i in range(1, 40):    \n    knn = KNeighborsClassifier(n_neighbors=i)\n    knn.fit(X_train, y_train)\n    y_pred = knn.predict(X_test)\n    error.append(np.mean(y_pred != y_test))\n    accuracy.append(metrics.accuracy_score(y_test,y_pred))\n\nprint(\"Error Rate:\\n\",error)\nprint(\"Accuracy Score:\\n\",accuracy)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7dbcb2b2c2ea3e4007298fdeca7be423824e498b"},"cell_type":"code","source":"#For choosing the K in the KNN, an iterative approach was practised.\n#The K value and the model with the lowest error was choosen from 1 - 40 to determine \n#the required output\nplt.figure(figsize=(12, 6))  \nplt.plot(range(1, 40), error, color='red', linestyle='dashed', marker='o',  \n         markerfacecolor='blue', markersize=10)\nplt.title('Error Rate K Value')  \nplt.xlabel('K Value')  \nplt.ylabel('Mean Error') ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b091df3e3d3dbfd6aa0937340762feeec73ccf84"},"cell_type":"code","source":"k = np.argmin(error) + 1\nknn = KNeighborsClassifier(n_neighbors = k)\nknn.fit(X_train, y_train)\ny_pred = knn.predict(X_test)\n\nfrom sklearn.metrics import classification_report, confusion_matrix  \nprint(\"Accuracy when k={}: \".format(k),metrics.accuracy_score(y_test, y_pred)) \nprint(\"Confusion matrix: \\n\",confusion_matrix(y_test, y_pred))  \nprint(classification_report(y_test, y_pred)) ","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"3c747011f17492c19dfc503fde0288bfc58e046e"},"cell_type":"code","source":"#3 ->Classification technique using Decision Tree \nfrom sklearn.tree import DecisionTreeClassifier \n\ndtree_model = DecisionTreeClassifier(max_depth = 2).fit(X_scaled_train, y_train) \ny_pred = dtree_model.predict(X_scaled_test) \nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\nprint(\"------------------------\\n\")\nprint(classification_report(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"07c1b777665ac4b28255831a6662069e1f82cb47"},"cell_type":"code","source":"#Conclusion\n\n# 1 - SVM (SVC)\n#The accuracy for SV classifier is the highest amongst all other models.\n# Justification - The kernel used for SVC is Radial Basis Function(RBF).\n#One property of the RBF kernel is that it is infinitely smooth\n#They are relatively easy to calibrate, as opposed to other kernels.\n#It has localized and finite response along the entire x-axis.\n\n# Higher Overall Average Precison over majority of output classes\n# Higher Overall F1-Score over majority of Output classes.\n#Also,\n#Takes lesser computation power(time).\n#Simpler and better for a multi-class classification problem\n\n#Hence the Best model for this dataset would be the SV clasiffier\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"12374b61e60f1b76246569a639aeff0d670f0f1a"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"}},"nbformat":4,"nbformat_minor":1}