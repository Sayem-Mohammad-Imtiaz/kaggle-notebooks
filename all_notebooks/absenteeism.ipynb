{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Importing Necessary Libraries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot \ninit_notebook_mode(connected=True)  \nimport cufflinks as cf  \ncf.go_offline() \ndf = pd.read_csv('../input/Absenteeism_at_work.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we need to check is the dataset's attributes/columns have any missing values which could be replaced by a potential statistical figure such as mean/median etc"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"df.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So there are no missing values"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EDA"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.iplot(kind='box')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = df.columns.tolist()\ncols.pop(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in cols:\n    print(i)\n    df[i].iplot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"AvgR = df[cols[1:]].mean()\nAvgR = AvgR.sort_values()\nplt.figure(figsize=(10,7))\nplt.barh(np.arange(len(cols[1:])), AvgR.values, align='center')\nplt.yticks(np.arange(len(cols[1:])), AvgR.index)\nplt.ylabel('Categories')\nplt.xlabel('Average')\nplt.title('Average')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(20, 20)) \nsns.heatmap(df.corr(), annot = True, ax = ax)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we're saving a copy of the dataset for the purpose of hierarhical clustering"},{"metadata":{"trusted":true},"cell_type":"code","source":"DF = df.copy()\nDF.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Clustering\n# 1.1 KMean Clustering\nFor KMean clustering we used Elbow Method with WCSS (within-cluster sum of squares) "},{"metadata":{"trusted":true},"cell_type":"code","source":"vals = DF.iloc[ :, 1:].values\n\nfrom sklearn.cluster import KMeans\nwcss = []\nfor ii in range( 1, 30 ):\n    kmeans = KMeans(n_clusters=ii, init=\"k-means++\", n_init=10, max_iter=300) \n    kmeans.fit_predict( vals )\n    wcss.append( kmeans.inertia_ )\n    \nplt.plot( wcss, 'ro-', label=\"WCSS\")\nplt.title(\"Computing WCSS for KMeans++\")\nplt.xlabel(\"Number of clusters\")\nplt.ylabel(\"WCSS\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Graph Description:** Observing the above graph we decided the value of k as 5"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.drop(['ID'],axis=1).values\nY = df['ID'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"km = KMeans(n_clusters=5, init=\"k-means++\", n_init=10, max_iter=500) \ny_pred = kmeans.fit_predict(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DF[\"Cluster\"] = y_pred\ncols = list(DF.columns)\ncols.remove(\"ID\")\n\nsns.pairplot( DF[cols], hue=\"Cluster\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1.2 Hierachical Clustering"},{"metadata":{"trusted":true},"cell_type":"code","source":"import scipy.cluster.hierarchy as sch\nfrom sklearn.preprocessing import scale as s\nfrom scipy.cluster.hierarchy import dendrogram, linkage","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Below is special function designed specifically for drawing a line on the generated dendrogram, in order to bring out number of clusters"},{"metadata":{"trusted":true},"cell_type":"code","source":"def fd(*args, **kwargs):\n    max_d = kwargs.pop('max_d', None)\n    if max_d and 'color_threshold' not in kwargs:\n        kwargs['color_threshold'] = max_d\n    annotate_above = kwargs.pop('annotate_above', 0)\n\n    ddata = dendrogram(*args, **kwargs)\n\n    if not kwargs.get('no_plot', False):\n        plt.title('Hierarchical Clustering Dendrogram (truncated)')\n        plt.xlabel('sample index or (cluster size)')\n        plt.ylabel('distance')\n        for i, d, c in zip(ddata['icoord'], ddata['dcoord'], ddata['color_list']):\n            x = 0.5 * sum(i[1:3])\n            y = d[1]\n            if y > annotate_above:\n                plt.plot(x, y, 'o', c=c)\n                plt.annotate(\"%.3g\" % y, (x, y), xytext=(0, -5),\n                             textcoords='offset points',\n                             va='top', ha='center')\n        if max_d:\n            plt.axhline(y=max_d, c='k')\n    return ddata","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We picked Ward Linkage for the Dendrogram as minimum variance criterion minimizes the total within-cluster variance"},{"metadata":{"trusted":true},"cell_type":"code","source":"Z = sch.linkage(df,method='ward')  \nden = sch.dendrogram(Z)\nplt.tick_params(\n    axis='x',          \n    which='both',      \n    bottom=False,     \n    top=False,         \n    labelbottom=False) \nplt.title('Hierarchical Clustering')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Z = linkage(df,method='ward')\nfd(Z,leaf_rotation=90.,show_contracted=True,annotate_above=750,max_d=1250)\nplt.tick_params(\n    axis='x',          \n    which='both',      \n    bottom=False,     \n    top=False,         \n    labelbottom=False) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Graph Description:** Now if we draw a horizontal line from 1250, we come across the 2 clusters generated by Ward hierarhical clustering "},{"metadata":{},"cell_type":"markdown","source":"In a very similar manner we also did hierarhical clustering with Complete Linkage as well. In Complete Linkage it's the similarity of two clusters is the similarity of their most dissimilar members."},{"metadata":{"trusted":true},"cell_type":"code","source":"Z = sch.linkage(df,method='complete')  \nden = sch.dendrogram(Z)\nplt.tick_params(\n    axis='x',          \n    which='both',      \n    bottom=False,     \n    top=False,         \n    labelbottom=False) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Z = linkage(df,method='complete')\nfd(Z,leaf_rotation=90.,show_contracted=True,annotate_above=160,max_d=280)\nplt.tick_params(\n    axis='x',          \n    which='both',      \n    bottom=False,     \n    top=False,         \n    labelbottom=False) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We come across 2 clusters again with Complete Linkage as well."},{"metadata":{},"cell_type":"markdown","source":"# Predicting Absenteeism time in hours\nNow since our problem deals with Absenteeism time in hours, which is regression in nature, we will be engaging algorithms and neural networks. The data would be scaled through Standard Scaler for data normalization. Model performance and score will be judged through R2-Score and Mean Squared Error. Implementation of each algorithm and neural networks has been done below:-"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom xgboost import XGBRegressor\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.preprocessing import scale as s\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.model_selection import train_test_split as t\nimport sklearn.metrics as mt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = df.drop(['ID','Absenteeism time in hours'],axis=1).values\ny = df['Absenteeism time in hours'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = s(x)\ny = s(y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Train Test splitting is mandatory to find model's performance on testing dataset. Data is split in 80:20 ratio"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_x,test_x,train_y,test_y = t(x,y,test_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2.1 Random Forest \nWe choose 100 number of Trees/Estimators for our Random Forest Regressor with maximum depth of 4"},{"metadata":{"trusted":true},"cell_type":"code","source":"rfr = RandomForestRegressor(n_estimators=100,max_depth=4)\nrfr.fit(train_x,train_y)\nprint(f'Score = {rfr.score(test_x,test_y)}')\nprint(f'MSE = {mt.mean_squared_error(test_y,rfr.predict(test_x))}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2.2 Linear Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = LinearRegression()\nlr.fit(train_x,train_y)\nprint(f'Score = {mt.r2_score(test_y,lr.predict(test_x))}')\nprint(f'MSE = {mt.mean_squared_error(test_y,lr.predict(test_x))}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2.3 K-Nearest Neighbors\nHere we took 10 number of neighbors"},{"metadata":{"trusted":true},"cell_type":"code","source":"knr = KNeighborsRegressor(n_neighbors=10)\nknr.fit(train_x,train_y)\nprint(f'Score = {mt.r2_score(test_y,knr.predict(test_x))}')\nprint(f'MSE = {mt.mean_squared_error(test_y,knr.predict(test_x))}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2.4 Extreme Gradient Boosting"},{"metadata":{"trusted":true},"cell_type":"code","source":"xgbr = XGBRegressor()\nxgbr.fit(train_x,train_y)\nprint(f'Score = {xgbr.score(test_x,test_y)}')\nprint(f'MSE = {mt.mean_squared_error(test_y,xgbr.predict(test_x))}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2.5 Multi-Layer Perceptron\nFor our Multi-Layer Perceptron Regressor we made 3 layers, with 100 nodes, followed by 1 hidden layers with 50, ending with 1 node since it's a regression problem. Maximum number of iterations is 500 at a time."},{"metadata":{"trusted":true},"cell_type":"code","source":"mlpr = MLPRegressor(hidden_layer_sizes=(100,50,1), max_iter=500)\nmlpr.fit(train_x,train_y)\nprint(f'Score = {mlpr.score(test_x,test_y)}')\nprint(f'MSE = {mt.mean_squared_error(test_y,mlpr.predict(test_x))}')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}