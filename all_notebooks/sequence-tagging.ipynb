{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%load_ext autoreload\n%autoreload 2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%pip install 'tensorflow==1.13.1' 'keras==2.2.4' git+https://www.github.com/keras-team/keras-contrib.git","metadata":{"_kg_hide-output":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from random import randint\nfrom future.utils import iteritems\n\nimport numpy as np\nimport pandas as pd","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Turkish\n!curl -o /kaggle/working/cc.tr.300.bin.gz https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.tr.300.bin.gz\n!gunzip /kaggle/working/cc.tr.300.bin.gz","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Italian\n!curl -o /kaggle/working/cc.it.300.bin.gz https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.it.300.bin.gz\n!gunzip /kaggle/working/cc.it.300.bin.gz","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import fasttext\n#fasttext.util.download_model('tr', if_exists='ignore')\nft = fasttext.load_model('/kaggle/working/cc.tr.300.bin')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv(\"../input/dodiom-dataset/it_corpus_second_run.csv\", converters={\n    'idiom_indices': eval,\n    'idiom_words': eval,\n    'lemmas': eval,\n    'words': eval\n})\ndata = data[data.words.map(len) <= 16]\ndata = data[data.likes + data.dislikes + data.reports >= 0]\ndata = data[data.rating >= 0.0]\ndata = data.reset_index()\nlen(data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sentences = []\nwords = set()\n\nfor index, row in data.iterrows():\n    sentence = []\n    for iw, word in enumerate(row[\"lemmas\"]):\n        tag = 'O'\n        if iw == row[\"idiom_indices\"][0]:\n            if row.category == \"idiom\":\n                tag = 'B-idiom'\n            else:\n                tag = \"B-nonidiom\"\n        elif iw in row[\"idiom_indices\"][1:]:\n            if row.category == \"idiom\":\n                tag = 'I-idiom'\n            else:\n                tag = \"I-nonidiom\"\n        sentence.append((word, tag))\n        words.add(word)\n    sentences.append(sentence)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sentences[randint(0, len(sentences))]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"words = list(words)\nwords.append(\"</s>\")\nn_words = len(words)\n\ntags = ['B-idiom', 'B-nonidiom', 'I-idiom', 'I-nonidiom', 'O']\nn_tags = len(tags)\n\nword2idx = {w: i for i, w in enumerate(words)}\nidx2word = {v: k for k, v in iteritems(word2idx)}\ntag2idx = {t: i for i, t in enumerate(tags)}\nidx2tag = {v: k for k, v in iteritems(tag2idx)}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.preprocessing.sequence import pad_sequences\nfrom keras.utils import to_categorical\nfrom sklearn.model_selection import train_test_split\n\nmaxlen = max([len(s) for s in sentences])\n\n#X = [[ft.get_word_vector(w[0]) for w in s] for s in sentences]\n#X = pad_sequences(maxlen=maxlen, sequences=X, padding=\"post\",value=ft.get_word_vector(\"</s>\"))\n\nX = [[word2idx[w[0]] for w in s] for s in sentences]\nX = pad_sequences(maxlen=maxlen, sequences=X, padding=\"post\",value=n_words - 1)\n\ny = [[tag2idx[w[1]] for w in s] for s in sentences]\ny = pad_sequences(maxlen=maxlen, sequences=y, padding=\"post\", value=tag2idx[\"O\"])\ny = [to_categorical(i, num_classes=n_tags) for i in y]\n\n# Split train and test data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.models import Model, Input\nfrom keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\nimport keras as k\nfrom keras_contrib.layers import CRF\n\ninput = Input(shape=X_train[0].shape)\nword_embedding_size = 150\n\n# Embedding Layer\nmodel = Embedding(input_dim=n_words, output_dim=word_embedding_size, input_length=maxlen)(input)\n\n# BI-LSTM Layer\nmodel = Bidirectional(LSTM(units=word_embedding_size, \n                           return_sequences=True, \n                           dropout=0.5, \n                           recurrent_dropout=0.5, \n                           kernel_initializer=k.initializers.he_normal()))(model)\nmodel = LSTM(units=word_embedding_size * 2, \n             return_sequences=True, \n             dropout=0.5, \n             recurrent_dropout=0.5, \n             kernel_initializer=k.initializers.he_normal())(model)\n\n# TimeDistributed Layer\nmodel = TimeDistributed(Dense(n_tags, activation=\"relu\"))(model)  \n\n# CRF Layer\ncrf = CRF(n_tags)\n\nout = crf(model)  # output\nmodel = Model(input, out)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.callbacks import ModelCheckpoint\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\n\n#Optimiser \nadam = k.optimizers.Adam(lr=0.0005, beta_1=0.9, beta_2=0.999)\n\n# Compile model\nmodel.compile(optimizer=adam, loss=crf.loss_function, metrics=[crf.accuracy])\n\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\n# Saving the best model only\nfilepath=\"ner-bi-lstm-td-model-{val_crf_viterbi_accuracy:.2f}.hdf5\"\ncheckpoint = ModelCheckpoint(filepath, monitor='val_crf_viterbi_accuracy', verbose=0, save_best_only=True, mode='max')\nearlystopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=15)\ncallbacks_list = [checkpoint, earlystopping]\n\n# Fit the best model\nhistory = model.fit(X_train, np.array(y_train), \n                    validation_data=(X_test, np.array(y_test)),\n                    batch_size=256,\n                    epochs=100,\n                    verbose=1,\n                    callbacks=callbacks_list)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot the graph \nplt.style.use('ggplot')\n\ndef plot_history(history):\n    accuracy = history.history['crf_viterbi_accuracy']\n    val_accuracy = history.history['val_crf_viterbi_accuracy']\n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n    x = range(1, len(accuracy) + 1)\n\n    plt.figure(figsize=(12, 5))\n    plt.subplot(1, 2, 1)\n    plt.plot(x, accuracy, 'b', label='Training acc')\n    plt.plot(x, val_accuracy, 'r', label='Validation acc')\n    plt.title('Training and validation accuracy')\n    plt.legend()\n    plt.subplot(1, 2, 2)\n    plt.plot(x, loss, 'b', label='Training loss')\n    plt.plot(x, val_loss, 'r', label='Validation loss')\n    plt.title('Training and validation loss')\n    plt.legend()\n\nplot_history(history)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred = model.predict(X_test)\nprint(len(X_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def onehot_to_IOB(arr):\n    if np.array_equal(arr, [1, 0, 0, 0, 0]):\n        return \"B-idiom\"\n    elif np.array_equal(arr, [0, 1, 0, 0, 0]):\n        return \"B-nonidiom\"\n    elif np.array_equal(arr, [0, 0, 1, 0, 0]):\n        return \"I-idiom\"\n    elif np.array_equal(arr, [0, 0, 0, 1, 0]):\n        return \"I-nonidiom\"\n    elif np.array_equal(arr, [0, 0, 0, 0, 1]):\n        return \"O\"\n    raise Exception(\"Invalid one-hot\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tabulate import tabulate\nfrom random import randint\n\nrand_item = randint(0, len(X_test))\nprint(f\"Item: {rand_item}\")\nresult = [[idx2word[x], onehot_to_IOB(y), onehot_to_IOB(y_pred)] for x, y, y_pred in zip(X_test[rand_item], y_test[rand_item], pred[rand_item])]\nprint(tabulate(result, headers=[\"word\", \"target\", \"prediction\"]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}