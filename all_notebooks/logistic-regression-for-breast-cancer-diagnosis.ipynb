{"nbformat":4,"nbformat_minor":0,"cells":[{"outputs":[],"metadata":{"trusted":false,"_cell_guid":"51611eee-a08c-46ca-b933-3e25dac25dbb","_uuid":"67756806a28fcdce9bc380bc36c5c490555d9348"},"execution_count":null,"source":"Logistic Regression for breast cancer diagnosis \n\n1-This is a notebook for for breast cancer diagnostic using UCI dataset which contains30 different columns and we have to predict the Stage of Breast Cancer M (Malignant) and B (Bengin)\n2-This analysis has been done using Basic Machine Learning Algorithm with detailed explanation\n3-Attribute information-\n\n -  ID number\n - Diagnosis (M = malignant, B = benign)\n 3-32 Ten real-valued features are computed for each cell nucleus:\n \n - radius (mean of distances from center to points on the perimeter)\n - texture (standard deviation of gray-scale values)\n - perimeter\n - area\n - smoothness (local variation in radius lengths)\n - compactness (perimeter^2 / area - 1.0)\n - concavity (severity of concave portions of the contour)\n - concave points (number of concave portions of the contour)\n - symmetry\n - fractal dimension (\"coastline approximation\" - 1)\n5 here 3- 32 are divided into three parts first is Mean (3-13), Stranded Error(13-23) and Worst(23-32) and each contain 10 parameter (radius, texture,area, perimeter, smoothness,compactness,concavity,concave points,symmetry and fractal dimension)\n\n  ","cell_type":"markdown"},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{"_execution_state":"idle","_uuid":"5d8897347ae05120cb025ab2a7cde5752c8432d7","collapsed":false},"source":"#Adding all the libraries\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv), data manipulation as in SQL\nimport matplotlib.pyplot as plt # this is used for the plot the graph \nimport seaborn as sns # used for plot interactive graph. I like it most for plot\n%matplotlib inline\nfrom sklearn.linear_model import LogisticRegression # to apply the Logistic regression\nfrom sklearn.model_selection import train_test_split # to split the data into two parts\nfrom sklearn.cross_validation import KFold # use for cross validation\nfrom sklearn.model_selection import GridSearchCV# for tuning parameter\nfrom sklearn.ensemble import RandomForestClassifier # for random forest classifier\nfrom sklearn import metrics #"},{"outputs":[],"cell_type":"markdown","metadata":{"_execution_state":"idle","_uuid":"aecd6b494a28115364704be06a5488965c927e62","collapsed":false},"source":"**IMPORT DATA**","execution_count":null},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{"_execution_state":"idle","_uuid":"4a555f6ccb7c9de47696819421eb437a32f7d602","collapsed":false},"source":"data = pd.read_csv(\"../input/data.csv\",header=0)# here header 0 means the 0 th row is our coloumn \n                                                # header in data"},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{"_execution_state":"idle","_uuid":"61de4332cf1a481a38c93417d2198bf00357ebfc","collapsed":false},"source":"#printing top two rows of data\nprint(data.head(2))"},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{"_execution_state":"busy","_uuid":"a65bb4bc0baa94ca6fc3dce380b447d32aa527f5","collapsed":false},"source":"#look what kind of data we have\ndata.info()"},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{"_execution_state":"idle","_uuid":"854ec62b8f97771446618ee7689862a36e964fbb","collapsed":false},"source":"# we will drop the unnamed column\ndata.drop(\"Unnamed: 32\",axis=1,inplace=True)\n#axis=1 means it will drop column"},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{"_execution_state":"idle","_uuid":"b6f0a0a5011c284fe3b194361caa264cf5e7eab1","collapsed":false},"source":"#check the remaining column\ndata.columns"},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{"_execution_state":"idle","_uuid":"19cbe1da4acc0a9c21b02c239a6c1444cc6667bf","collapsed":false},"source":"# we also don't need ID column for aur analysis so we will also drop it\ndata.drop(\"id\",axis=1,inplace=True)"},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{"_execution_state":"idle","_uuid":"8e8d528f129947001e9fe2e1f716414e3afcc8f5","collapsed":false},"source":"# now we will divide data into three parts\nfeatures_mean= list(data.columns[1:11])\nfeatures_se= list(data.columns[11:20])\nfeatures_worst=list(data.columns[21:31])\nprint(features_mean)\nprint(\"-----------------------------------\")\nprint(features_se)\nprint(\"------------------------------------\")\nprint(features_worst)"},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{"_execution_state":"idle","_uuid":"5d462b74fb78786348090ca24ad5103348c70019","collapsed":false},"source":"# now we will map diagnosis column into integer values M=1 and B=0\ndata['diagnosis']=data['diagnosis'].map({'M':1,'B':0})"},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{"_execution_state":"idle","_uuid":"a0e93bc45056346dc97af62ec0256edc2d535811","collapsed":false},"source":"# this will describe the all statistical function of our data\ndata.describe()"},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{"_execution_state":"idle","_uuid":"74b8072493f2f7ee87e705df02a0716ea1e3a757","collapsed":false},"source":"#get the frequency of cancer stages\nsns.countplot(data['diagnosis'],label='count')"},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{"_execution_state":"idle","_uuid":"83143c13162ecdfb1197b3132af8d89b2930dd13","collapsed":false},"source":"# now lets draw a correlation graph so that we can remove multi colinearity it means the columns are\n# dependenig on each other so we should avoid it because what is the use of using same column twice\n# lets check the correlation between features\n# now we will do this analysis only for features_mean then we will do for others and will see who is\n#doing best\ncorr = data[features_mean].corr() # .corr is used for find corelation\nplt.figure(figsize=(14,14))\nsns.heatmap(corr, cbar = True,  square = True, annot=True, fmt= '.2f',annot_kws={'size': 15},\n           xticklabels= features_mean, yticklabels= features_mean,\n           cmap= 'coolwarm') \n# for more on heatmap you can visit Link(http://seaborn.pydata.org/generated/seaborn.heatmap.html)"},{"outputs":[],"cell_type":"markdown","execution_count":null,"metadata":{"_execution_state":"idle","_uuid":"adb616aabbfafdf6e9337fdba51a8c6310ab4947","collapsed":false},"source":"observation\n\n 1. The radius, parameter and area are highly correlated as expected from their relation so from these we will use anyone of them\n 2. Compactness_mean, concavity_mean and concave point_mean are highly correlated so we will use \n      compactness_mean from here\n3. So selected Parameter for use is perimeter_mean, texture_mean, compactness_mean, symmetry_mean*"},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{"_execution_state":"idle","_uuid":"4b8023ed897705604bd0f92fe745026a6a2b471f","collapsed":false},"source":"# now these are the variables which will use for prediction\nprediction_var = ['texture_mean','perimeter_mean','smoothness_mean','compactness_mean','symmetry_mean']"},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{"_execution_state":"idle","_uuid":"1a80bcbaaecd8628d646e1a68337a048ef7976e9","collapsed":false},"source":"#now split our data into train and test\ntrain, test = train_test_split(data, test_size = 0.3)# in this our main data is splitted into train and test\n# we can check their dimension\nprint(train.shape)\nprint(test.shape)"},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{"_execution_state":"idle","_uuid":"b56230f2136678eac5f4bd0fa9de7c449954c8b9","collapsed":false},"source":"# taking training and testing data\ntrain_X = train[prediction_var]# taking the training data input \ntrain_y=train.diagnosis# This is output of our training data\n# same we have to do for test\ntest_X= test[prediction_var] # taking test data inputs\ntest_y =test.diagnosis   #output value of test dat\n"},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{"_execution_state":"idle","_uuid":"57d383c4e85772f17d1680b3b677b801d3da3b9f","collapsed":false},"source":"# Now explore a little bit more\n# now from features_mean i will try to find the variable which can be use for classify\n# so lets plot a scatter plot for identify those variable who have a separable boundary between two class\n#of cancer\n# Lets start with the data analysis for features_mean\n# Just try to understand which features can be used for prediction\n# I will plot scatter plot for the all features_mean for both of diagnosis Category\n# and from it we will find which are easily can used for differenciate between two category\n\ncolor_function = {0: \"blue\", 1: \"red\"}\n# Here Red color will be 1 which means M and blue foo 0 means B\ncolors = data[\"diagnosis\"].map(lambda x: color_function.get(x))\n# mapping the color fuction with diagnosis column\npd.plotting.scatter_matrix(data[features_mean], c=colors, alpha = 0.5, figsize = (15, 15)); \n# plotting scatter plot matrix"},{"outputs":[],"cell_type":"markdown","execution_count":null,"metadata":{"_execution_state":"idle","_uuid":"25df43d4740756cc348bf063d5f691a4a0561de1","collapsed":false},"source":"We can observe that-\n1. Radius, area and perimeter have a strong linear relationship as expected 2 As graph shows the features like as texture_mean, smoothness_mean, symmetry_mean and fractal_dimension_mean can t be used for classify two category because both category are mixed there is no separable plane\nSo we can remove them from our prediction_var"},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{"_execution_state":"idle","_uuid":"0577041374052a30a5e95215b8dd8352700240cd","collapsed":false},"source":"# So predicton features will be \npredictor_var = ['radius_mean','perimeter_mean','area_mean','compactness_mean','concave points_mean']"},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{"_execution_state":"idle","_uuid":"1b04c2b936c7a1a1fd51216ede142da00ae45308","collapsed":false},"source":"def model(model,data,prediction,outcome):\n    # This function will be used for to check accuracy of different model\n    # model is the m\n    kf = KFold(data.shape[0], n_folds=10) # if you have refer the link then you must understand what is n_folds\n    "},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{"_execution_state":"idle","_uuid":"b44eb05dbe51f975199d72c0c51134963817d01d","collapsed":false},"source":"# As we are going to use many models lets make a function\n# Which we can use with different models\ndef classification_model(model,data,prediction_input,output):\n    # here the model means the model \n    # data is used for the data \n    #prediction_input means the inputs used for prediction\n    # output mean the value which are to be predicted\n    # here we will try to find out the Accuarcy of model by using same data for fiiting and \n    #comparison for same data\n    #Fit the model:\n    model.fit(data[prediction_input],data[output]) #Here we fit the model using training set\n  \n    #Make predictions on training set:\n    predictions = model.predict(data[prediction_input])\n  \n    #Print accuracy\n    # now checkin accuracy for same data\n    accuracy = metrics.accuracy_score(predictions,data[output])\n    print(\"Accuracy : %s\" % \"{0:.3%}\".format(accuracy))\n \n    \n    kf = KFold(data.shape[0], n_folds=5)\n    # About cross validitaion please follow this link\n    #https://www.analyticsvidhya.com/blog/2015/11/improve-model-performance-cross-validation-in-\n    #python-r/\n    #let me explain a little bit data.shape[0] means number of rows in data\n    #n_folds is for number of folds\n    error = []\n    for train, test in kf:\n        # as the data is divided into train and test using KFold\n        # now as explained above we have fit many models \n        # so here also we are going to fit model\n        #in the cross validation the data in train and test will change for evry iteration\n        train_X = (data[prediction_input].iloc[train,:])# in this iloc is used for index of trainig data\n        # here iloc[train,:] means all row in train in kf amd the all columns\n        train_y = data[output].iloc[train]# here is only column so it repersenting only row in train\n        # Training the algorithm using the predictors and target.\n        model.fit(train_X, train_y)\n    \n        # now do this for test data also\n        test_X=data[prediction_input].iloc[test,:]\n        test_y=data[output].iloc[test]\n        error.append(model.score(test_X,test_y))\n        # printing the score \n        print(\"Cross-Validation Score : %s\" % \"{0:.3%}\".format(np.mean(error)))"},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{"_execution_state":"idle","_uuid":"dd5c455421dcd728c6740a15e38ba28ab9b8e43b","collapsed":false},"source":"outcome_var= \"diagnosis\"\n# lets try with logistic regression\nmodel=LogisticRegression()\nclassification_model(model,data,prediction_var,outcome_var)"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"version":3,"name":"ipython"},"pygments_lexer":"ipython3","name":"python","nbconvert_exporter":"python","version":"3.6.1","file_extension":".py","mimetype":"text/x-python"}}}