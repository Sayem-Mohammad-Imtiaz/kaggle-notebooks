{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"# Loading Dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Reading the Bar-S datafile\nBar = pd.read_csv('../input/stock-market-small-wide-dataset/bar-S.csv')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Analysing the columns of Bar File\nBar.head()\nBar = Bar.drop(['epoch_time_at_the_beginning','epoch_time_at_the_ending'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Open** is the price of the stock at the beginning of the trading day (it need not be the closing price of the previous trading day), \n**high** is the highest price of the stock on that trading day, \n**low** the lowest price of the stock on that trading day, and \n**close** the price of the stock at closing time. \n**Volume** indicates how many stocks were traded. \n**Adjusted prices (such as the adjusted close)** is the price of the stock that adjusts the price for corporate actions. While stock prices are considered to be set mostly by traders"},{"metadata":{"trusted":true},"cell_type":"code","source":"Bar.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Bar.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Bar.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Bar.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Considering only the first 1000 rows of the data**"},{"metadata":{"trusted":true},"cell_type":"code","source":"Bar = Bar.head(1000)\nBar['changeduringday'] = ((Bar['high_price'] - Bar['low_price'] )/ Bar['low_price'])*100\n\nBar['changefrompreviousday'] = (abs(Bar['close_price'].shift() - Bar['close_price'] )/ Bar['close_price'])*100\n\nBar.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt  #IMporting Data Visualiztion Library\n#This line is necessary for the plot to appear in notebook\n%matplotlib inline\n#Controlling Default size of figures in the notebook\n%pylab inline\npylab.rcParams['figure.figsize'] = (10,6)\nBar[['time','average_price']].plot(grid=True)\nplt.title(\"Average Price VS Time\")\nplt.xlabel('time')\nplt.ylabel('average price')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Bar.hist(bins=50,figsize=(20,15))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Bar[['time','close_price']].plot(grid=True)\nplt.xlabel('time')\nplt.ylabel('price')\nplt.legend('close_price')\nplt.title('Close Price Vs Time')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Building the correlation matirix to know the correlation bewtween close price(target) and the other features"},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_matrix = Bar.corr()\ncorr_matrix['close_price'].sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pandas.plotting import scatter_matrix\n\nattributes = [\"high_price\", \"low_price\", \"open_price\", \"changefrompreviousday\", \"changeduringday\", \"volume\"]\n\nscatter_matrix(Bar[attributes], figsize=(20, 15))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\ncorr = Bar[[\"high_price\", \"low_price\", \"open_price\", \"changefrompreviousday\", \"changeduringday\", \"volume\"]].corr()\n\n# generate a mask for the lower triangle\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n# set up the matplotlib figure\nf, ax = plt.subplots(figsize=(18, 12))\n\n# generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n# draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3,\n            square=True, \n            linewidths=.5, cbar_kws={\"shrink\": .5}, ax=ax);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Stock Clustering using K-Means"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import KMeans   #importing the Kmeans module\n\n#Taking only price columns\nBar_modified = Bar[['symbol','open_price','close_price','average_price']]\n\n#Dropping duplicates symbol from Bar\nBar_modified.drop_duplicates(subset={'symbol'},keep='first')\n\n#Stting Index of dataframe to symbol\nBar_modified = Bar_modified.set_index('symbol')\nprint(Bar_modified.head())\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In order to determine the optimal number of clusters k for the Bar dataset, we will fit different models of the K-means algorithm while varying the k parameter in the range 2 to 10. For each model we calculate the Sum Squared Error (SSE) by using the inertia_ method of the model fitted. In each iteration we append the inertia to the sse list. Then we take the model with the less value of SSE. (Inertia tells how far away the points within a cluster are. The small the inertia value is better.)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Storing values of price in different columns\nX = Bar_modified.values\nsse = []\nfor k in range(2,10):\n    kmeans = KMeans(n_clusters=k)\n    kmeans.fit(X)\n    \n    #SSE for each cluster\n    sse.append(kmeans.inertia_)\n    \nplt.plot(range(2,10),sse,'bx-')\nplt.title('Elbow Curve')\nplt.xlabel(\"Values of k\")\nplt.ylabel('Distortion')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**According to the Elbow Curve, we choose k = 4**"},{"metadata":{"trusted":true},"cell_type":"code","source":"kmeans = KMeans(n_clusters = 4).fit(X)\ncentroids = kmeans.cluster_centers_\nplt.scatter(X[:,0],X[:,1],X[:,2],c = kmeans.labels_,cmap='rainbow')\nplt.title(\"Cluster of Dataset with n=4\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"avgorder = Bar_modified.sort_values('close_price',ascending=False)\nfirst_symbol = avgorder.index[0]\nBar_modified.drop(first_symbol,inplace=True)\nX= Bar_modified.values\nkmeans = KMeans(n_clusters=4).fit(X)\ncentroids = kmeans.cluster_centers_\nplt.scatter(X[:,0],X[:,1],c = kmeans.labels_,cmap='rainbow')\nplt.title('Clusters of Dataset without outliers k =4')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally we will assign to each stock it correspondent number of cluster(1,2,3,and 4) and make a dataframe with this information. Having the information of cluster number for each stock, we can create a diversified portfolio in the long term, between stocks from different clusters."},{"metadata":{"trusted":true},"cell_type":"code","source":"stocks = pd.DataFrame(Bar_modified.index)\ncluster_labels = pd.DataFrame(kmeans.labels_)\nstockClusters = pd.concat([stocks,cluster_labels],axis=1)\nstockClusters.columns = ['symbol','Cluster']\nprint(stockClusters)\nprint(stockClusters.Cluster.unique()) #All the four clusters in the array\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Loading Quote-S Dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"Quote = pd.read_csv('../input/stock-market-small-wide-dataset/quote-S.csv')\nQuote.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Quote.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Quote.dtypes\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Considering only first 1000 rows\nQuote = Quote.head(1000)\nQuote.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt  #IMporting Data Visualiztion Library\n#This line is necessary for the plot to appear in notebook\n%matplotlib inline\n#Controlling Default size of figures in the notebook\n%pylab inline\npylab.rcParams['figure.figsize'] = (10,6)\nQuote[['time','bid_price']].plot(grid=True)\nplt.title(\"Bid Price VS Time\")\nplt.xlabel('time')\nplt.ylabel('Bid price')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt  #IMporting Data Visualiztion Library\n#This line is necessary for the plot to appear in notebook\n%matplotlib inline\n#Controlling Default size of figures in the notebook\n%pylab inline\npylab.rcParams['figure.figsize'] = (10,6)\nQuote[['time','ask_price']].plot(grid=True)\nplt.title(\"Ask Price VS Time\")\nplt.xlabel('time')\nplt.ylabel('Ask price')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt  #IMporting Data Visualiztion Library\n#This line is necessary for the plot to appear in notebook\n%matplotlib inline\n#Controlling Default size of figures in the notebook\n%pylab inline\npylab.rcParams['figure.figsize'] = (10,6)\nQuote[['bid_size','bid_price']].plot(grid=True)\nplt.title(\"Bid Price VS bid size\")\nplt.xlabel('bid size')\nplt.ylabel('Bid price')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt  #IMporting Data Visualiztion Library\n#This line is necessary for the plot to appear in notebook\n%matplotlib inline\n#Controlling Default size of figures in the notebook\n%pylab inline\npylab.rcParams['figure.figsize'] = (10,6)\nQuote[['ask_size','ask_price']].plot(grid=True)\nplt.title(\"Ask Price VS ask size\")\nplt.xlabel('ask size')\nplt.ylabel('Ask price')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Stock Clustering using K-Means"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import KMeans   #importing the Kmeans module\n\n#Taking only price columns\nQuote_modified = Quote[['ticker','bid_price','ask_price']]\nprint(Quote_modified.head())\n#Dropping duplicates symbol from Bar\nQuote_modified.drop_duplicates(subset=['ticker'],inplace =True)\n\n#Stting Index of dataframe to symbol\nQuote_modified = Quote_modified.set_index('ticker')\nprint(Quote_modified.head())\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Storing values of price in different columns\nX = Quote_modified.values\nsse = []\nfor k in range(2,10):\n    kmeans = KMeans(n_clusters=k)\n    kmeans.fit(X)\n    \n    #SSE for each cluster\n    sse.append(kmeans.inertia_)\n    \nplt.plot(range(2,10),sse,'bx-')\nplt.title('Elbow Curve')\nplt.xlabel(\"Values of k\")\nplt.ylabel('Distortion')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**According to the Elbow Curve, we choose k = 4**"},{"metadata":{"trusted":true},"cell_type":"code","source":"kmeans = KMeans(n_clusters = 4).fit(X)\ncentroids = kmeans.cluster_centers_\nplt.scatter(X[:,0],X[:,1],c = kmeans.labels_,cmap='rainbow')\nplt.title(\"Cluster of Dataset with n=4\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stocks = pd.DataFrame(Quote_modified.index)\ncluster_labels = pd.DataFrame(kmeans.labels_)\nstockClusters = pd.concat([stocks,cluster_labels],axis=1)\nstockClusters.columns = ['ticker','Cluster']\nprint(stockClusters)\nprint(stockClusters.Cluster.unique()) #All the four clusters in the array\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Returns"},{"metadata":{},"cell_type":"markdown","source":"Again importing the Bar-S file and changing index to time column "},{"metadata":{"trusted":true},"cell_type":"code","source":"#Reading the Bar-S datafile\nBar = pd.read_csv('../input/stock-market-small-wide-dataset/bar-S.csv')\nBar['time'] = Bar.time.apply(lambda x: x[:10])\n#Bar=Bar.drop_duplicates(subset = ['time'],keep = 'first')\n\nBar_AAPL = Bar[Bar.symbol=='AAPL']\nBar_AAPL = Bar_AAPL.sort_values(by=['time'])\nBar_AAPL = Bar_AAPL.set_index(['time'])\nBar_AAPL.shape\nBar_AAPL","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Simple plot**"},{"metadata":{"trusted":true},"cell_type":"code","source":"Bar_AAPL['average_price'].plot()\nplt.xlabel(\"time\")\nplt.ylabel(\"Adjusted\")\nplt.title(\"Apple Price data\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Calculating the daily returns for individual stock"},{"metadata":{"trusted":true},"cell_type":"code","source":"AAPL_daily_returns = Bar_AAPL['average_price'].pct_change()\n#AAPL_monthly_returns = Bar_AAPL['average_price'].resample('M').ffill().pct_change()\nAAPL_daily_returns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure()\nax1 = fig.add_axes([0.1,0.1,0.8,0.8])\nax1.plot(AAPL_daily_returns)\nax1.set_xlabel(\"Time\")\nax1.set_ylabel(\"percent\")\nax1.set_title(\"Apple Daily Returns\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Calculating the cumulative returns for the Apple stock"},{"metadata":{"trusted":true},"cell_type":"code","source":"AAPL_cum_returns = (AAPL_daily_returns+1).cumprod()\nfig = plt.figure()\nax1 = fig.add_axes([0.1,0.1,0.8,0.8])\nAAPL_cum_returns.plot()\nax1.set_xlabel(\"Date\")\nax1.set_ylabel(\"Growth of $1 investment\")\nax1.set_title(\"Apple daily cumulative returns data\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load the required modules and packages\nimport numpy as np\nimport pandas as pd\n\n\n# Pull NIFTY data from Yahoo finance \n\n# Compute the logarithmic returns using the Closing price \nBar_AAPL['Log_Ret'] = np.log(Bar_AAPL['close_price'] / Bar_AAPL['close_price'].shift(1))\n\n# Compute Volatility using the pandas rolling standard deviation function\nBar_AAPL['Volatility'] = Bar_AAPL['Log_Ret'].rolling(window=252).std() * np.sqrt(252)\n\n\n# Plot the NIFTY Price series and the Volatility\nBar_AAPL[['close_price', 'Volatility','Log_Ret']].plot(subplots=True, color='blue',figsize=(8, 6))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Applying ARIMA model on the Dataset"},{"metadata":{},"cell_type":"markdown","source":"**Importing the Bar Dataset again**"},{"metadata":{"trusted":true},"cell_type":"code","source":"Bar = pd.read_csv('../input/stock-market-small-wide-dataset/bar-S.csv')\nBar.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Dropping unnecessary column\nBar = Bar.drop(['epoch_time_at_the_beginning','epoch_time_at_the_ending'],axis=1)\nBar.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Bar_AAPL = Bar[Bar.symbol=='AAPL']\nBar_AAPL.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pandas.plotting import lag_plot\nfrom pandas import datetime\nfrom statsmodels.tsa.arima_model import ARIMA\nfrom sklearn.metrics import mean_squared_error\nplt.figure(figsize=(10,10))\nlag_plot(Bar_AAPL['open_price'],lag=5)\nplt.title(\"Apple Autocorrelation plot\")\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Successfully i divided data into a training and test data.Once done i plot both on the same figure to get a feeling of how does our TIme Series looks like. "},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data,test_data = Bar_AAPL[0:int(len(Bar_AAPL)*0.8)],Bar_AAPL[int(len(Bar_AAPL)*0.8):]\nplt.figure(figsize=(12,7))\nplt.title('Apple Prices')\nplt.xlabel('Dates')\nplt.ylabel('Prices')\nplt.plot(Bar_AAPL['open_price'],'blue',label='Training Data')\nplt.plot(test_data['open_price'],'green',label='Testing Data')\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**In order to evaluate the ARIMA model, I decided to use two different error functions: Mean Squared Error (MSE) and Symmetric Mean Absolute Percentage Error (SMAPE). SMAPE is commonly used as an accuracy measure based on relative errors**"},{"metadata":{},"cell_type":"markdown","source":"**SMAPE is not currently supported in Scikit-learn as a loss function I, therefore, had first to create this function on my own**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def smape_kun(y_true, y_pred):\n    return np.mean((np.abs(y_pred - y_true) * 200/ (np.abs(y_pred) +       np.abs(y_true))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_ar = train_data['open_price'].values\ntest_ar = test_data['open_price'].values\n\nhistory = [x for x in train_ar]\nprint(type(history))\npredictions = list()\nfor t in range(len(test_ar)):\n    model  = ARIMA(history,order=(5,1,0))\n    model_fit = model.fit(disp=0)\n    output = model_fit.forecast()\n    yhat = output[0]\n    predictions.append(yhat)\n    obs = test_ar[t]\n    history.append(obs)\nerror = mean_squared_error(test_ar,predictions)\nprint(\"Testing Mean Squared Error: %.3f\"% error)\nerror2 = smape_kun(test_ar,predictions)\nprint(\"Symmetric mean absolute perecentage error: %.3f\"%error2)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,7))\nplt.plot(Bar_AAPL['open_price'], 'green', color='blue', label='Training Data')\nplt.plot(test_data.index, predictions, color='green', marker='o', linestyle='dashed', \n         label='Predicted Price')\nplt.plot(test_data.index, test_data['open_price'], color='red', label='Actual Price')\nplt.title('Apple Prices Prediction')\nplt.xlabel('Dates')\nplt.ylabel('Prices')\n#plt.xticks(np.arange(0,7982, 1300), df['Date'][0:7982:1300])\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,7))\nplt.plot(test_data.index, predictions, color='green', marker='o', linestyle='dashed',label='Predicted Price')\nplt.plot(test_data.index, test_data['open_price'], color='red', label='Actual Price')\nplt.legend()\nplt.title('Apple Prices Prediction')\nplt.xlabel('Dates')\nplt.ylabel('Prices')\n#plt.xticks(np.arange(6386,7982, 300), df['Date'][6386:7982:300])\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**This analysis using ARIMA lead overall to appreciable results. This model demonstrated in fact to offer good prediction accuracy and to be relatively fast compared to other alternatives such as RRNs (Recurrent Neural Networks).**"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}