{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport seaborn as sns\nimport pandas_profiling\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Preparation","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"First we merge the dataset and add a column of the car manufacturer, the dataset has different versions and mantains old data, so we need to focus only on the most recent version.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data_folder = '/kaggle/input/used-car-dataset-ford-and-mercedes/'\ndataset_names = ['bmw', 'merc', 'hyundi', 'ford', 'vauxhall', 'vw', 'audi','skoda', 'toyota']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.DataFrame()\nfor dataset_name in dataset_names:\n    dataset = pd.read_csv(data_folder+dataset_name + '.csv')\n    if(dataset_name == 'hyundi'):\n        dataset.rename(columns={\"tax(£)\": \"tax\"}, inplace=True)\n    dataset['manufacturer'] = dataset_name\n    df = pd.concat([df, dataset], ignore_index=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Exploration","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The general distribution of the prices is the following:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,7))\n\nplt.title('Price distribution')\n\nsns.distplot(df['price'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see there a are plenty of outliers and one thing we can explore is if models increase in accuracy if we remove those, (or if we perform some data augmentation).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(14,11))\nsns.boxplot(x=\"manufacturer\", y=\"price\", data=df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that Mercedes, Audi  and Bmw have outliers with high prices in respect to the brand mean, this is probably due to them manufacturing faster car models.\nWe can take a look at cars that cost more than 100k £.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"costly = df[df.price > 100000]\ncostly.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list(costly['manufacturer'].unique())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that a lot of them are recent models, and that engineSize is also pretty big on average.\nAs commons sense tells, **we can expect year and engineSize to be good predictors of price**.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cheap = df[df.price < 1000]\ncheap.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list(cheap['manufacturer'].unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cheap[cheap['manufacturer'] == 'merc']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"The only mercedes model is one from 2003 with very high mileage and also small engine Size.\n**Mileage also can be a good predictor for pricing** (nothing too surprising here).","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Another interesting thing to look is the comparison of how much data for each manufacturer we have.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(7,6))\nsns.countplot(x=\"manufacturer\", data=df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can clearly see we have less data for hyudai skoda and toyota but nonetheless it is still succifient to create predictive models in machine learning.\nOne possibility to explore is that to use the same number of entries for each of the manufacturers","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"As a last analysis step i'm going to generate a report from pandas profiling package and take a look at the things we missed:","execution_count":null},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"profile = df.profile_report(title=\"Pandas Profiling Report\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The widget version is easier to access for future reference inside notebooks","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"profile.to_widgets()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Correlation matrix plot confirms what we where saying about the data: year and engineSize can be good predictors for price.\nAlso there is as expected a negative correlation between mileage and price.\n\nThe positive correlation between tax and price needs to be further explored, being not very high it makes us a little confident about it being not something that is calculated on the price (so a variable that can't be used to predict price), but also the fact that it is not higly correlated to things like engineSize makes me wonder what it is.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Looking more at the data from the kaggle dataset page i can see that it is \"road tax\". It can be good to increase topic knowledge before even thinking about using it as a predictor.\n\nResearching on the internet i found this article that explains well the topic of road tax in UK:\nhttps://www.autoexpress.co.uk/car-news/consumer-news/88361/tax-disc-changes-everything-you-need-to-know-about-uk-road-tax\n\n\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"The annual standard rate is £145 and there is the following statement\n\"**Cars above £40,000 pay £325 annual supplement for five years from the second year of registration.**\"\n\nRate is calculated on the first registration of the car so the listing price doesn't  affect this last part (and should be totally uncorrelated with it)\n\nCars **older than 40 years are tax-exempt**.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Also is good to take a look at the warnings, as we find out that there are 1475 duplicates (worth exploring to check for scraping errors).\n\nWorth exploring is if cars with 0 tax are effectively 40+ years old, otherwise it could be another scraping error. This **can also be useful to generate some features if the car is near the 40 years date (could increase in price as after that you are tax-exempt)**\n\nThe high-cardinality of model is a not a big warning for us, rather than that we shouldn't use one-hot encoding for it in our predictive models","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Lastly we saw that there are duplicates in the dataset, a good idea is to take a look at them and then to drop","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"duplicates = df[df.duplicated(keep=False)] # Just for visualization\nduplicates.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop_duplicates(ignore_index=True, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that dropping duplicates didn't change much, so our analysis is still valid (but in case we want to increase model accuracy even more in the future we can always go back and explore more)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Feature selection","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_cols = list(df.select_dtypes('object').columns)\ncategorical_cols","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The intuitive idea is to encode using one-hot enconding both transmission and fuelType (and maybe even manufacturer if we decide to create a more specific model given the absence of other manufacturer data).\nBut let's first check their cardinality","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"threshold = 10 # If less than 10 unique values we suppose it is low cardinality\n\nlow_cardinality_cols = [col for col in categorical_cols if df[col].nunique() < threshold]\n\nhigh_cardinality_cols = list(set(categorical_cols)-set(low_cardinality_cols))\n\nprint(low_cardinality_cols)\nprint(high_cardinality_cols)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We could also check how the model performs without this column(and model) entirely (so that we can derive a really general model).\nActually this is probably the best way to go since we want to derive the most general model possible and it should be extensible to other car brands and models.\nThis also makes it really easy to adapt input without destroying our pipelines in case there are new values.\n\nLater will be discussed the utilization of more models based on input data (and how it could be hidden to the final user too).\n\nN.B. We should always have in mind that the **model performance will degrade overtime** as car values decrease more more, some strategy should be used to account this (the simplest of them is to scrape new data periodically and to retrain a new model, some advanced strategy that let us retain old data is to create a deflation model or to add a column of the period the data was scraped on and let machine learning understand the pattern).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"general_df = df.drop(['model', 'manufacturer'],axis=1)\ngeneral_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The only thing that remains to do is to one-hot encode transmission and fueltype columns\n\nBe careful of the 'Dummy variable trap' when one-hot encoding in regressions models.\nCheck out https://www.algosome.com/articles/dummy-variable-trap-regression.html for more informations about this problem.\nWith pandas get_dummies we can easily fix it by setting to true the option drop_first.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"general_df_encoded = pd.get_dummies(general_df, drop_first=True)\ngeneral_df_encoded","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = general_df_encoded['price']\nX = general_df_encoded.drop('price', axis=1)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=4242)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Baseline model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"linreg = LinearRegression().fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = linreg.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"baseline_rmse = mean_squared_error(y_test, preds, squared=False) # RMSE\nbaseline_rmse","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"baseline_mae = mean_absolute_error(y_test, preds)\nbaseline_mae","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The big difference between them is that the model tends to get wrong cars with big prices as they are usually outliers. This is something to take into consideration, but for the moment we can skip on that.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Regression model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def score_result(y_test, preds):\n    print(\"----------------\")\n    rmse = mean_squared_error(y_test, preds, squared=False)\n    mae = mean_absolute_error(y_test, preds)\n    print(\"RMSE: \", rmse)\n    print(\"MAE: \", mae)\n    print(\"\\nImprovement from baseline:\")\n    print(\"RMSE Improvement:\",  baseline_rmse - rmse)\n    print(\"MAE Improvement:\", baseline_mae - mae)\n    print(\"----------------\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The algorithm we will use is Random Forest as it usually performs very well for regression problems.\nDepending on the task sometimes we don't need a model beast, something that works and has good accuracy is just enough to test an idea, you can think of more complex architecture later (for example using multiple algorithms of different types and using some kind of weighted average to get an optimal price estimation)\n\nAn alternative could also be that to throw the data to an Auto-ML library or cloud service and use that model and then later thin of complex feature engineering and improvements.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_basic = RandomForestRegressor(random_state=4242)\nrf_basic.fit(X_train, y_train)\n\nrf_basic_preds = rf_basic.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score_result(rf_basic_preds, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Final considerations","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"This is just a simple notebook that aims to to basic analysis and model creation for the dataset at hand. In future releases i plan to analyze further the data and create a production architecture that could increase dramatically performance by choosing the right model based on data input.\n\nThe model as is can already provide some kind of range of the price with some accuracy (for example we can use prediction-rmse and prediction+rmse as the range)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}