{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"6595097f-cfad-4c07-f112-78851b1bc424"},"source":"##Neural net\nImplemented in Keras, with a lot of help from scikit-learn.\nWe first train a medium-sized net and see that it instantly overfits. We then get similar and more stable results with a tiny net."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"46df6b7e-ae9a-2e33-bd16-2d69d5890846"},"outputs":[],"source":"import numpy as np # linear algebra\nnp.set_printoptions(precision=0, suppress=True)\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\nfrom sklearn.preprocessing import Imputer, StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.manifold import TSNE\nfrom sklearn.metrics import confusion_matrix, roc_curve, roc_auc_score\n\nimport matplotlib.pyplot as plt\n%matplotlib inline"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"047fcdbc-643c-c527-7bf1-6d293b418617"},"outputs":[],"source":"from keras.models import Sequential, Model\nfrom keras.optimizers import Adam\nfrom keras.regularizers import l2\nfrom keras.layers import Dense, BatchNormalization, Dropout\nfrom keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\nimport keras.backend as K"},{"cell_type":"markdown","metadata":{"_cell_guid":"1b620312-90e3-eaea-7fe1-b32c1239f698"},"source":"## Load the data"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c95abfcd-ca73-96f8-e0d9-8700b319880e"},"outputs":[],"source":"with open('../input/diabetes.csv') as f:\n    print('\\n'.join(f.readline().split(',')[:-1]))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c3015526-ca4a-66d2-9347-85d808b9af57"},"outputs":[],"source":"raw_data = np.loadtxt('../input/diabetes.csv', skiprows=1, delimiter=',')\nraw_feat = raw_data[:,:-1]\nraw_labels = raw_data[:,-1] "},{"cell_type":"markdown","metadata":{"_cell_guid":"450f65ea-b69e-7f43-eddc-fc15a27b4a37"},"source":"## Preprocessing"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"07ba6821-5062-e138-6fa3-03fb0ef96bb9"},"outputs":[],"source":"x_train, x_test, y_train, y_test = train_test_split(raw_feat, raw_labels, \n                                                    test_size=0.3, random_state=700)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a84bc1c7-d732-5e8d-b833-e2d738653cc1"},"outputs":[],"source":"imp = Imputer(missing_values = 0) #replace zero values by mean\nclean_feat = raw_feat.copy()\nclean_feat[:,1:] = imp.fit_transform(raw_feat[:,1:]) #We don't want to do this for pregnancies\nprint(raw_feat[:8,4], '\\n', clean_feat[:8,4])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"fc30f0c5-8ecf-e662-1a1a-160d07542401"},"outputs":[],"source":"#We must do this again for the train test, or we would get data leakage from the test set\n#in taking the mean.\nx_train = x_train.copy()\nx_train[:,1:] = imp.fit_transform(x_train[:,1:])\nx_test[:,1:] = imp.transform(x_test[:,1:])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7589204f-5380-806c-1041-7e2b914daa04"},"outputs":[],"source":"scaler = StandardScaler() #scale to zero mean and unit variance\nclean_feat = scaler.fit_transform(clean_feat)\nx_train = scaler.fit_transform(x_train)\nx_test = scaler.transform(x_test)"},{"cell_type":"markdown","metadata":{"_cell_guid":"c0c19ad1-c14b-2099-006b-c6f61cb02aa8"},"source":"This time we just do PCA and t-SNE for some visualization. We could also use the PCA data for training, but with the neural nets here, it doesn't really affect performance."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"aab2805a-b3e1-e48b-d839-0465586ad238"},"outputs":[],"source":"pca = PCA(n_components=7)\nclean_pca = pca.fit_transform(clean_feat)\nplt.scatter(clean_pca[:,0], clean_pca[:,1], color=np.where(raw_labels>0.5,'r','g'))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5c30abf7-0cd6-c6f6-5978-c57875537991"},"outputs":[],"source":"tsne = TSNE(n_iter=3000)\ntsne_data = tsne.fit_transform(clean_pca)\nplt.scatter(tsne_data[:,0], tsne_data[:,1], color=np.where(raw_labels>0.5,'r','g'))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d253f002-3c2e-a124-1327-1bb089b645ac"},"outputs":[],"source":"pca_train = pca.fit_transform(x_train)\npca_test = pca.transform(x_test)"},{"cell_type":"markdown","metadata":{"_cell_guid":"0b130461-18d0-8a5a-c05a-021a092b5e1b"},"source":"We can now choose whether to train our nets on original or PCA data. "},{"cell_type":"markdown","metadata":{"_cell_guid":"1cfe227f-8695-438b-fd9f-515bcece83d8"},"source":"##Train the model\nFirst we choose a medium-sized neural net. We have trained this on original data, as PCA does not make a huge difference. Instead, the problem is that it soon starts to overfit."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c4c5d7f2-0b6c-cb1d-4b53-2463775a8b5d"},"outputs":[],"source":"model = Sequential()\nmodel.add(Dense(32, activation='relu', input_dim = x_train.shape[1]))\nmodel.add(Dense(64, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.5))\nmodel.add(Dense(256, activation='relu'))\nmodel.add(Dropout(0.75))\nmodel.add(Dense(1, activation='sigmoid'))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2b3ea11b-cccd-eb37-7e85-289a439d3ebd"},"outputs":[],"source":"model.compile(optimizer=Adam(1e-4), loss='binary_crossentropy', metrics=['accuracy'])\nhist = model.fit(x_train, y_train, validation_data=(x_test, y_test), \n          epochs=50, verbose=0, batch_size=8)\nprint(model.evaluate(x_train, y_train, verbose=0))\nprint(model.evaluate(x_test, y_test, verbose=0))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5c4149e1-9214-7d1b-f5e1-7db182ab4199"},"outputs":[],"source":"plt.plot(hist.history['loss'], color='b')\nplt.plot(hist.history['val_loss'], color='r')\nplt.show()\nplt.plot(hist.history['acc'], color='b')\nplt.plot(hist.history['val_acc'], color='r')\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"93c6a881-3891-c225-cfc9-767a65b818d3"},"source":"We have achieved 77% test accuracy on average, although this will vary depending on the random initializations of the test set, and the net weights. Training longer will not help, as the net is already starting to overfit. Overfitting is expected, as the net has almost 20.000 parameters and we train on less than 5000 data values. Getting more data would be very helpful.\n\nWe can't expect to achieve much better on this dataset - the original article reports 76% accuracy."},{"cell_type":"markdown","metadata":{"_cell_guid":"66e750b6-b6f3-4343-cda9-69b0a7f45e35"},"source":"## Visualization"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c3dfcb4d-b327-4d0f-f766-2b4d8b8bae2b"},"outputs":[],"source":"y_true = (raw_labels + 0.5).astype(\"int\")\ny_pred = (model.predict(clean_feat) + 0.5).astype(\"int\").reshape(-1,)\ncolor = np.where(y_true * y_pred == 1, 'g', 'r')\ncolor[np.where(y_true * (1-y_pred) == 1)[0]] = 'b'\ncolor[np.where(y_pred * (1-y_true) == 1)[0]] = 'y'"},{"cell_type":"markdown","metadata":{"_cell_guid":"1cf61058-a56b-4e61-0dd4-2ee9477a308e"},"source":"In the plots below, blue is false-negatives, yellow false-positives, green is where the net correctly predicts positive, and red where the net correctly predicts negative. With a perfect net, it would be all green and red."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"daf6368c-3006-3cdb-6bd0-c9f38cf67c44"},"outputs":[],"source":"for i in range(4):\n    for j in range(i+1,4):\n        plt.scatter(clean_pca[:,i], clean_pca[:,j], color=color)\n        plt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"b968ad92-e8b9-a39d-eefc-0372ee4c685b"},"source":"Overall, we see that the net has been rather conservative, with classification close to linear. "},{"cell_type":"markdown","metadata":{"_cell_guid":"e2bb4a80-6ccb-6ef1-5294-b3ae931dd5e9"},"source":"##Overfitting\nIf we continue to train, we can get over 95% training set accuracy, but poor generalization.  In fact, let's forget about the test set and train on our full data."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6b35ac73-1a69-15a3-e6ff-39f2f4af6a46"},"outputs":[],"source":"model.compile(optimizer=Adam(3e-4), loss='binary_crossentropy', metrics=['accuracy'])\nhist = model.fit(clean_feat, raw_labels, \n          epochs=500, verbose=0, batch_size=8)\nprint(model.evaluate(x_train, y_train, verbose=0))"},{"cell_type":"markdown","metadata":{"_cell_guid":"e6359646-989b-cace-1701-3422d5bdc29d"},"source":"We now plot the results of the overfitted net."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2d72bde2-b7c5-d0bb-437f-818ea6a0ed8c"},"outputs":[],"source":"y_true = (raw_labels + 0.5).astype(\"int\")\ny_pred = (model.predict(clean_feat) + 0.5).astype(\"int\").reshape(-1,)\ncolor = np.where(y_true * y_pred == 1, 'g', 'r')\ncolor[np.where(y_true * (1-y_pred) == 1)[0]] = 'b'\ncolor[np.where(y_pred * (1-y_true) == 1)[0]] = 'y'"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8aced91d-40b9-44b0-c36b-834e47aaff3c"},"outputs":[],"source":"for i in range(4):\n    for j in range(i+1,4):\n        plt.scatter(clean_pca[:,i], clean_pca[:,j], color=color)\n        plt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"79353377-3d25-f383-940d-53c2f5f7a6a5"},"source":"As expected, it is an almost perfect (green) fit, with only a few blue and yellow dots."},{"cell_type":"markdown","metadata":{"_cell_guid":"51082a1c-3602-d1b9-2df5-d974cbffdb5d"},"source":"##Small is beautiful\nLet's finish this by training a tiny net, to see if we can avoid overfitting. In addition to reducing the size, I have also added both L2 and Dropout regularization"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b20bce8c-8b63-3d1c-1b75-d20f08d5f08d"},"outputs":[],"source":"model = Sequential()\nmodel.add(Dense(8, activation='relu', kernel_regularizer=l2(.1), input_dim = x_train.shape[1]))\nmodel.add(Dense(8, activation='relu', kernel_regularizer=l2(.05)))\nmodel.add(BatchNormalization())\nmodel.add(Dense(16, activation='relu'))\nmodel.add(Dropout(0.75))\nmodel.add(Dense(1, activation='sigmoid'))"},{"cell_type":"markdown","metadata":{"_cell_guid":"ceb39497-2317-a16b-4ee7-765b60eb06c1"},"source":"These three additions are not strictly necessary, but could improve performance slightly"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1ab8aaaf-3d52-5da5-baf1-46360b8435c6"},"outputs":[],"source":"#To keep the best weights, in case of overfitting\ncallback1 = ModelCheckpoint('tiny.h5', monitor='val_loss', \n                           save_best_only=True, save_weights_only=True)\n\n#To reduce learning rate over time\ncallback2 = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=100, \n                              mode='min', epsilon=0.05, min_lr=1e-8)\n\n#To adjust for the fact y_train has twice as many 0's as 1's\n#Does not seem to improve results in this case\n#sample_weight = (1 + y_train) "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f076fde0-d6f4-2dd0-e844-4fe11b7c01f8"},"outputs":[],"source":"# Label smoothing, sometimes improves net training by avoiding \n# areas where the activation function has flat gradient.\ny_train = 0.9 * y_train + 0.05  \ny_test = 0.9 * y_test + 0.05"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5b7b53aa-ba0e-bbfd-b402-27522a6fd522"},"outputs":[],"source":"def float_accuracy(y_true, y_pred):\n    \"\"\"\n    Equivalent to Keras' built-in binary_accuracy, but can be used with label smoothing.\n    \"\"\"\n    return K.mean(K.equal(K.round(y_true), K.round(y_pred)), axis=-1)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4540980a-a01b-1d26-d503-2d430cab602f"},"outputs":[],"source":"model.compile(optimizer=Adam(1e-4), loss='binary_crossentropy', metrics=[float_accuracy])\n\nhist = model.fit(x_train, y_train, validation_data=(x_test, y_test),\n                 epochs=1000, verbose=0, batch_size=8, \n                 callbacks = [callback1, callback2])\nprint(model.evaluate(x_train, y_train, verbose=0))\nprint(model.evaluate(x_test, y_test, verbose=0))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b5a50de3-c206-3fee-c268-d185ced8cff3"},"outputs":[],"source":"plt.plot(hist.history['loss'], color='b')\nplt.plot(hist.history['val_loss'], color='r')\nplt.show()\nplt.plot(hist.history['float_accuracy'], color='b')\nplt.plot(hist.history['val_float_accuracy'], color='r')\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"f6a429b9-314c-2d18-adee-08de7cbc49eb"},"source":"Even with the regularizations, the net will overfit if trained long enough. Fortunately we saved the best weights with 'ModelCheckpoint':"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ea5d6a8f-4c21-ed0c-0435-ba52d18257c1"},"outputs":[],"source":"model.load_weights('tiny.h5')\nprint(model.evaluate(x_train, y_train, verbose=0))\nprint(model.evaluate(x_test, y_test, verbose=0))"},{"cell_type":"markdown","metadata":{"_cell_guid":"08a931f7-c734-7bf2-57b0-b1542edc5b2c"},"source":"##Sensitivity / specificity trade-off\nWe first create a Confusion Matrix of our result"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0804c6f8-d149-0c76-1743-cc5afa13891b"},"outputs":[],"source":"threshold = 0.5\ny_true = np.where(y_test > 0.5, 1, 0).astype(\"int\")\ny_pred = np.where(model.predict(x_test) > threshold, 1, 0).astype(\"int\").reshape(-1,)\ncm = confusion_matrix(y_true, y_pred)\npos = np.sum(cm[0])\nprint(cm)"},{"cell_type":"markdown","metadata":{"_cell_guid":"f963a68c-7e04-fdcc-3cda-1876136afb03"},"source":"Remember that a confusion matrix is defined as:\n\n[true negatives, false positives]\n\n[false negatives, true positives]\n\nWhat if we wanted to avoid false positives, even if that led to a higher proportion of false-negatives? The easy way is just to change the cutoff:"},{"cell_type":"markdown","metadata":{"_cell_guid":"0cd6456a-192e-8661-a6d0-a6efb9311268"},"source":"Scikit-learn has methods for doing this automatically, returning all thresholds and corresponding parameters. We here draw two graphs that can be directly compared to the end page of the original article from which this dataset is taken: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2245318/pdf/procascamc00018-0276.pdf"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"bad50508-217c-be3d-44db-8f14f20e3ad0"},"outputs":[],"source":"threshold = 0.6\ny_true = np.where(y_test > 0.5, 1, 0).astype(\"int\")\ny_pred = np.where(model.predict(x_test) > threshold, 1, 0).astype(\"int\").reshape(-1,)\ncm = confusion_matrix(y_true, y_pred)\nprint(cm)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"101e4c29-08e8-fb28-e450-7b692d2d3e94"},"outputs":[],"source":"y_score = model.predict(x_test)\nfpr, tpr, thresholds = roc_curve(y_true, y_score)\nplt.plot(thresholds, 1.-fpr)\nplt.plot(thresholds, tpr)\nplt.show()\ncrossover_index = np.min(np.where(1.-fpr <= tpr))\ncrossover_cutoff = thresholds[crossover_index]\ncrossover_specificity = 1.-fpr[crossover_index]\nprint(\"Crossover at {0:.2f} with specificity {1:.2f}\".format(crossover_cutoff, crossover_specificity))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d255dd5d-8529-10f2-81e0-4c715da3e55d"},"outputs":[],"source":"plt.plot(fpr, tpr)\nplt.show()\nprint(\"ROC area under curve is {0:.2f}\".format(roc_auc_score(y_true, y_score)))"},{"cell_type":"markdown","metadata":{"_cell_guid":"51f2d066-56bb-d7ef-6fc6-fb736aec8585"},"source":"We have achieved results no worse than the 0.76 of the original article. The exact result varies quite a bit between different runs, but is usually between 0.70 and 0.80."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d9e7796d-22de-891a-5bbe-dd63be571b59"},"outputs":[],"source":""}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}