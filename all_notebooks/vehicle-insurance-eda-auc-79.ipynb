{"cells":[{"metadata":{},"cell_type":"markdown","source":"Kindly upvote if you like this notebook.<br>\nAny issues or mistake kindly let me know in comments, happy to correct."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler \nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix,classification_report\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.metrics import roc_auc_score,accuracy_score\n\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"insurance_df = pd.read_csv('/kaggle/input/health-insurance-cross-sell-prediction/train.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"insurance_df.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Data Analysis**"},{"metadata":{"trusted":true},"cell_type":"code","source":"insurance_df.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"insurance_df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"insurance_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_columns=[]\ncontinuous_columns=[]\nfor col in insurance_df.columns:\n    if insurance_df[col].dtype!='object':\n        continuous_columns.append(col)\n    else:\n        categorical_columns.append(col)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"continuous_columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,16))\nfor i, col in enumerate(['id','Age','Region_Code','Annual_Premium','Policy_Sales_Channel','Vintage']):\n    plt.subplot(4,4,i+1)\n    sns.boxplot(insurance_df[col])\n    plt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"insurance_df.loc[insurance_df.Annual_Premium> 400000,'Annual_Premium']=400000","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"    I don't see much outliers except in Annual_Premium, We will replace premium values greater than 400000 with 400000"},{"metadata":{"trusted":true},"cell_type":"code","source":"insurance_df['Gender'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax =plt.subplots(1,2,figsize=(15,5))\nsns.countplot(data=insurance_df,x='Gender',hue='Vehicle_Damage',ax=ax[0])\nsns.countplot(data=insurance_df,x='Gender',hue='Previously_Insured',ax=ax[1])\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We clearly see that male have more vehicle damage than female, even then male don't have insurance."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax =plt.subplots(1,2,figsize=(15,5))\nsns.countplot(data=insurance_df,x='Gender',hue='Vehicle_Age',ax=ax[0])\nsns.countplot(data=insurance_df,x='Previously_Insured',hue='Vehicle_Damage',ax=ax[1])\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I see most of the vehicles are new(less than two years). People have't got insuranced for new vehicles.<BR>\nIt is surprsing that many vehicles within 2 years have got so much damage."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax =plt.subplots(1,2,figsize=(15,5))\n# fig, ax = plt.subplots() \nsns.countplot(data=insurance_df,x='Gender',hue='Previously_Insured',ax=ax[0])\nsns.countplot(data=insurance_df,x='Gender',hue='Vehicle_Damage',ax=ax[1])\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,9))\nsns.FacetGrid(insurance_df, hue = 'Response',\n             height = 6,xlim = (0,150)).map(sns.kdeplot, 'Age', shade = True,bw=2).add_legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Age is almost normally distributed for people who are interested in buying insurance. People with age nearly 30 are more interested in buying insurance.<br>\nI think young people doesn't like to get insurance."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,9))\nsns.FacetGrid(insurance_df, hue = 'Gender',\n             height = 6,xlim = (0,150)).map(sns.kdeplot, 'Age', shade = True,bw=2).add_legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I see no much signifiant difference in age vs gender"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,5))\nsns.boxplot(y='Age', x ='Gender', hue=\"Previously_Insured\", data=insurance_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Females have got insurance at young age."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,5))\nsns.violinplot(y='Age', x ='Gender', hue=\"Response\", data=insurance_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As I said previously young doesn't like to get insurance, when we drill down further we see that\n* People who like to get insurence their age is normally distributed. The mean age of both are nearly 45 years.\n* Both young male and female doesn't like to buy insurance, distributed is right skewed.\n* But the mean age of male and female, who is not interested to buy insurance, has huge difference."},{"metadata":{},"cell_type":"markdown","source":"**With this we move to modelling**"},{"metadata":{"trusted":true},"cell_type":"code","source":"le = LabelEncoder()\ninsurance_df['Gender'] = le.fit_transform(insurance_df['Gender'])\ninsurance_df['Driving_License'] = le.fit_transform(insurance_df['Driving_License'])\ninsurance_df['Previously_Insured'] = le.fit_transform(insurance_df['Previously_Insured'])\ninsurance_df['Vehicle_Damage'] = le.fit_transform(insurance_df['Vehicle_Damage'])\ninsurance_df['Driving_License'] = le.fit_transform(insurance_df['Driving_License'])\ninsurance_df['Vehicle_Age'] = le.fit_transform(insurance_df['Vehicle_Age'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"insurance_df=insurance_df[['Gender', 'Age', 'Driving_License', 'Region_Code',\n       'Previously_Insured', 'Vehicle_Age', 'Vehicle_Damage', 'Annual_Premium',\n       'Policy_Sales_Channel', 'Vintage', 'Response']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,12))\nsns.heatmap(insurance_df.corr())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I see"},{"metadata":{"trusted":true},"cell_type":"code","source":"def evaluation_stats(model,X_train, X_test, y_train, y_test,algo,is_feature=False):\n    print('Train Accuracy')\n    y_pred_train = model.predict(X_train)                           \n    print(accuracy_score(y_train, y_pred_train))\n    print('Validation Accuracy')\n    y_pred_test = model.predict(X_test)                           \n    print(accuracy_score(y_test, y_pred_test))\n    print(\"\\n\")\n    print(\"Train AUC Score\")\n    print(roc_auc_score(y_train, y_pred_train))\n    print(\"Test AUC Score\")\n    print(roc_auc_score(y_test, y_pred_test))\n    \n    if is_feature:\n        plot_feature_importance(rf_model.feature_importances_,X.columns,algo)\n\ndef training(model,X_train, y_train):\n    return model.fit(X_train, y_train)\n\ndef plot_feature_importance(importance,names,model_type):\n    feature_importance = np.array(importance)\n    feature_names = np.array(names)\n\n    #Create a DataFrame using a Dictionary\n    data={'feature_names':feature_names,'feature_importance':feature_importance}\n    fi_df = pd.DataFrame(data)\n\n    #Sort the DataFrame in order decreasing feature importance\n    fi_df.sort_values(by=['feature_importance'], ascending=False,inplace=True)\n    #Define size of bar plot\n    plt.figure(figsize=(10,8))\n    #Plot Searborn bar chart\n    sns.barplot(x=fi_df['feature_importance'], y=fi_df['feature_names'])\n    #Add chart labels\n    plt.title(model_type + ' FEATURE IMPORTANCE')\n    plt.xlabel('FEATURE IMPORTANCE')\n    plt.ylabel('FEATURE NAMES')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"insurance_df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"insurance_df['Response'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Data is highly imbalanced, but still we will try to train few models without over sampling"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = insurance_df.drop([\"Response\"], axis=1)\ny = insurance_df[\"Response\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state = 101)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_model = training(RandomForestClassifier(),X_train,y_train)\nevaluation_stats(rf_model,X_train, X_test, y_train, y_test,'RANDOM FOREST')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This RF with out oversampling clearly over fits, Train accuracy and AUC is very high. Model is not able to generalization."},{"metadata":{"trusted":true},"cell_type":"code","source":"xbg_model = training(XGBClassifier(),X_train,y_train)\nevaluation_stats(xbg_model,X_train, X_test, y_train, y_test,'XGB')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"XBG is not completely overfitting, but AUC is low"},{"metadata":{},"cell_type":"markdown","source":"**Now we will try with over sampling**"},{"metadata":{"trusted":true},"cell_type":"code","source":"sm = SMOTE(random_state=101)\nX_res, y_res = sm.fit_resample(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_model = training(RandomForestClassifier(),X_res, y_res)\nevaluation_stats(rf_model,X_res, X_test, y_res, y_test,'RANDOM FOREST')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Model is overfitting to train dataset, but it is performing good on validation dataset. <br> This is little tricky"},{"metadata":{"trusted":true},"cell_type":"code","source":"xbg_model = training(XGBClassifier(),X_train,y_train)\nevaluation_stats(xbg_model,X_res, X_test, y_res, y_test,'XGB')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets check with adding parameters to the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_model = training(RandomForestClassifier(criterion='entropy',n_estimators=200,max_depth=3),X_res, y_res)\nevaluation_stats(rf_model,X_res, X_test, y_res, y_test,'RANDOM FOREST')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This model is not over fitting on the train dataset AUC is quiet decent enough "},{"metadata":{"trusted":true},"cell_type":"code","source":"xbg_model = training(XGBClassifier(n_estimators=1000,max_depth=10),X_res, y_res)\nevaluation_stats(xbg_model,X_res, X_test, y_res, y_test,'XGB',is_feature=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Over fitting"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nclf = make_pipeline(StandardScaler(), LogisticRegression())\nclf.fit(X_res, y_res)\nevaluation_stats(clf,X_train, X_test, y_train, y_test,'LR',is_feature=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I think this is also good model, it is not completely overfitting, accuracy is .68 but AUC is .74"},{"metadata":{},"cell_type":"markdown","source":"**RF with parameters criterion = entropy ,n_estimators = 200 and max_depth = 3 were giving best results, that is AUC of 79%**"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}