{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Importing useful libraries\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_selection import RFE\nimport statsmodels.api as sm\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import precision_score, recall_score\nfrom sklearn.metrics import precision_recall_curve\n\npd.set_option(\"display.max_columns\",150)\npd.set_option(\"display.max_rows\",150)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Importing the data\ndf = pd.read_csv(\"../input/lead-scoring-x-online-education/Leads X Education.csv\")\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking the shape of data\ndf.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Checking info of variables in dateframe like data type and null count\ndf.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's check statistical details of numerical variables\ndf.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## EDA","metadata":{}},{"cell_type":"markdown","source":"### Data Cleaning and preparation\n### Univariate analysis","metadata":{}},{"cell_type":"code","source":"# checking null values percentage in descending order\nnull_values = round(df.isnull().sum().sort_values(ascending = False)/len(df)*100,2)\nnull_values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- We can drop Asymmetrique Profile Index & Asymmetrique Profile Score, since Asymmetrique Activity Index & Asymmetrique Activity Score will resembles same","metadata":{}},{"cell_type":"code","source":"# Dropping 'Asymmetrique Profile Index' &'Asymmetrique Activity Index'\ndf = df.drop(['Asymmetrique Profile Index','Asymmetrique Activity Index'],axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Asymmetrique Profile Score","metadata":{}},{"cell_type":"code","source":"df['Asymmetrique Profile Score'].describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Data visualization\nsns.distplot(df['Asymmetrique Profile Score'])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Multimodal distrubution is observed.\n- Asymmetrique Profile Score of 16 has highest count.","metadata":{}},{"cell_type":"code","source":"# Treating the missing values with median value as per observation from above stats\ndf['Asymmetrique Profile Score']= df['Asymmetrique Profile Score'].fillna(df['Asymmetrique Profile Score'].median())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Asymmetrique Activity Score","metadata":{}},{"cell_type":"code","source":"df['Asymmetrique Activity Score'].describe()","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Data visualization\nsns.distplot(df['Asymmetrique Activity Score'])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Treating the missing values with median value as per observation from above stats\ndf['Asymmetrique Activity Score'] = df['Asymmetrique Activity Score'].fillna(df['Asymmetrique Activity Score'].median())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Multimodal distrubution is observed.\n- Asymmetrique Profile Score of 14 has highest count.","metadata":{}},{"cell_type":"markdown","source":"### Let's check Other columns having missing values before dropping them\n### Tags","metadata":{}},{"cell_type":"code","source":"df['Tags'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['Tags'].value_counts().index","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Lets fill the values less than 1% and null values with others\n- This may be an good interpreter for modeling since it is a feedback by customer","metadata":{}},{"cell_type":"code","source":"Others = ['invalid number',\n       'Diploma holder (Not Eligible)', 'wrong number given', 'opp hangup',\n       'number not provided', 'in touch with EINS', 'Lost to Others',\n       'Want to take admission but has financial problems', 'Still Thinking',\n       'Interested in Next batch', 'In confusion whether part time or DLP',\n       'Lateral student', 'University not recognized',\n       'Shall take in the next coming month',\n       'Recognition issue (DEC approval)']\ndf['Tags'] = df['Tags'].replace(Others,'Others')\ndf['Tags'].fillna('Others',inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['Tags'].value_counts(normalize=True)*100","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(14,6))\nsns.countplot(df['Tags'],hue=df['Converted'])\nplt.xticks(rotation=90)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### What matters most to you in choosing a course","metadata":{}},{"cell_type":"code","source":"df['What matters most to you in choosing a course'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- As a general perspective for upskilling is looking for better career prospects so this column can be dropped","metadata":{}},{"cell_type":"code","source":"df = df.drop('What matters most to you in choosing a course',axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Lead Profile","metadata":{}},{"cell_type":"code","source":"df['Lead Profile'].value_counts()/len(df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- From above data we can observe that 29% of data is missing and 45% of the data is not defined. In total unavailable data for this column will be ~74%. So, we can drop this variable","metadata":{}},{"cell_type":"code","source":"df = df.drop('Lead Profile',axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### What is your current occupation","metadata":{}},{"cell_type":"code","source":"df['What is your current occupation'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['What is your current occupation'].fillna('Unemployed',inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Missing values can be imputed with Unemployed, since it most occuring value and a reasonable value","metadata":{}},{"cell_type":"code","source":"# Data visualization\nplt.figure(figsize=(12,5))\nsns.countplot(df['What is your current occupation'],hue=df['Converted'])\nplt.xticks(rotation=45)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- From the above plot we can observe and in terms of business aspect below observations are made\n    - Unemployed are the one who are willing to upskill for getting better jobs\n    - Working professional who are looking for professional growth\n    - So, Working professionals and unemployed are good to target","metadata":{}},{"cell_type":"markdown","source":"### Country","metadata":{}},{"cell_type":"code","source":"# Checking for Percentage of missing values\ndf['Country'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"other_countires = df['Country'].value_counts().index[1:]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['Country'] = df['Country'].replace(other_countires,'Other_Country')\ndf['Country'].fillna(df['Country'].mode(),inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['Country'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- About 27% of data is missing in the variable country. And most of the leads are from India only and others countires all together contributing around 2 %. So can create a new value as other country to proceed further for modelling.\n- For missing value treatment we can use mode to impute the missing values.\n- This results in skewed data to India, So it is better to drop this column.","metadata":{}},{"cell_type":"code","source":"df = df.drop('Country',axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### City","metadata":{}},{"cell_type":"code","source":"df['City'].value_counts(normalize=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['City'] = df['City'].replace('Select',)\ndf['City'].fillna(df['City'].mode(),inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['City'].value_counts(normalize=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df.drop('City', axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- About 15 % of values are missing and 28% are undefined. In total City has about 43% of missing values. If we treat by imputing most occuring value. \n- Thane and Ouskirts also belongs to Mumbai.\n- City is biased to Mumbai i.e., about 71%, So we should drop this column","metadata":{}},{"cell_type":"markdown","source":"### How did you hear about X Education","metadata":{}},{"cell_type":"code","source":"# Checking for various responses given by leads\ndf['How did you hear about X Education'].value_counts()/len(df)*100","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df.drop('How did you hear about X Education',axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- From above data we can observe that 24% of data is missing and 54% of the data is not defined. In total unavailable data for this column will be ~78%. So, we can drop this variable\n- Most of the things now a days we are hearing through internet i.e., online search.\n- If we impute the undefined value Select with Online search data will skew towrds Online search, So we can drop this column.","metadata":{}},{"cell_type":"markdown","source":"### Lead Source","metadata":{}},{"cell_type":"code","source":"df['Lead Source'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Based on above observations correcting the values of Lead Source\ndf['Lead Source'] = df['Lead Source'].replace({'google':'Google','welearnblog_Home':'WeLearn'})\ndf['Lead Source'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Dropping the records which are having very less number of count from above observation say < 20\ndf['Lead Source'] = df['Lead Source'].replace(['bing','Click2call','Press_Release','Social Media','WeLearn',\n                                'Live Chat','Pay per Click Ads','NC_EDM','testone','youtubechannel',\n                                'blog'],)\ndf['Lead Source'].fillna(df['Lead Source'].mode(),inplace=True)\n\ndf['Lead Source'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Data visualization\nplt.figure(figsize=(12,5))\nsns.countplot(df['Lead Source'],hue=df['Converted'])\nplt.xticks(rotation=90)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Leads generated from google, direct landed on webpage, Quark chat, Reference and Organic search can be a good targetting audience.\n- People from Reference and Wellingak website are more likely to convert","metadata":{}},{"cell_type":"markdown","source":"### Lead Quality","metadata":{}},{"cell_type":"code","source":"# Lead quality has highest missing values of 52%.\n# Since lead Quality can be a good intepreting variable let's see how it is distrubuted\ndf['Lead Quality'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Missing values are about 50% and if we replace the null values by 'Not Sure' in the perspective of business \n#this could be an important variable\ndf['Lead Quality'] = df['Lead Quality'].fillna('Not Sure')\ndf['Lead Quality'].value_counts(normalize=True)*100","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Data visualization\nplt.figure(figsize=(12,5))\nsns.countplot(df['Lead Quality'],hue=df['Converted'])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Lead quality is an unpredictable variable, because not sure leads are more and modelrately likely to convert\n- Might be, high in relevance, low in relevance are most likely to converted and can be targeted.","metadata":{}},{"cell_type":"markdown","source":"### TotalVisits","metadata":{}},{"cell_type":"code","source":"df['TotalVisits'].describe()","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Data visualization\nplt.figure(figsize=(12,5))\nplt.subplot(1,2,1)\nsns.distplot(df['TotalVisits'], color='Green')\nplt.subplot(1,2,2)\nsns.boxplot(df['TotalVisits'])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Since outliers are present in the above variable we shall trat them by soft capping for further analysis\npercentiles = df['TotalVisits'].quantile([0.01,0.99]).values\ndf['TotalVisits'][df['TotalVisits'] <= percentiles[0]] = percentiles[0]\ndf['TotalVisits'][df['TotalVisits'] >= percentiles[1]] = percentiles[1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"percentiles","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['TotalVisits'].fillna(df['TotalVisits'].mean(),inplace=True) # A few missing values are treated by mean value","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Data is showing Bi-modal distrubution. Clearly indicates leads are from different background i.e., by occupation, Education level, gender etc.\n- Since outliers are present, in order to reduce the computation and model get unaffected, we have treated by soft capping\n- Missing values are abou 1.5% and since it is a contineous variable we have imputed with mean.","metadata":{}},{"cell_type":"markdown","source":"#### Total Time Spent on Website","metadata":{}},{"cell_type":"code","source":"df['Total Time Spent on Website'].describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Univariate Analysis\n# Data visualization\nplt.figure(figsize=(12,5))\nplt.subplot(1,2,1)\nsns.distplot(df['Total Time Spent on Website'], color='Green')\nplt.subplot(1,2,2)\nsns.boxplot(df['Total Time Spent on Website'])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Data is showing Bi-modal distrubution. Clearly indicates leads are from different background i.e., by occupation, Education level, gender etc are spending different time on website.\n- No outliers are observed","metadata":{}},{"cell_type":"markdown","source":"#### Page Views Per Visit","metadata":{}},{"cell_type":"code","source":"df['Total Time Spent on Website'].describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Data visualization\nplt.figure(figsize=(12,5))\nplt.subplot(1,2,1)\nsns.distplot(df['Page Views Per Visit'], color='Green')\nplt.subplot(1,2,2)\nsns.boxplot(df['Page Views Per Visit'])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Treating the missing values\ndf['Page Views Per Visit'].fillna(df['Page Views Per Visit'].mean(),inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Since outliers are present in the above variable we shall trat them by soft capping for further analysis\npercentiles = df['Page Views Per Visit'].quantile([0.01,0.99]).values\ndf['Page Views Per Visit'][df['Page Views Per Visit'] <= percentiles[0]] = percentiles[0]\ndf['Page Views Per Visit'][df['Page Views Per Visit'] >= percentiles[1]] = percentiles[1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Data is showing Bi-modal distrubution. Clearly indicates leads are from different background i.e., by occupation, Education level, gender etc.\n- Missing values percentage is 1.5% and are imputed with mean\n- Since outliers are present, in order to reduce the computation and model get unaffected, we have treated by soft capping","metadata":{}},{"cell_type":"markdown","source":"### Last Activity","metadata":{}},{"cell_type":"code","source":"df['Last Activity'].value_counts() # Check for values in the data for cleansing","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['Last Activity'].value_counts().index","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Replacing the very low values by most occuring value to make model simpler\nOthers = ['Email Bounced',\n       'Email Link Clicked', 'Form Submitted on Website', 'Unreachable',\n       'Unsubscribed', 'Had a Phone Conversation', 'Approached upfront',\n       'View in browser link Clicked', 'Email Received', 'Email Marked Spam',\n       'Visited Booth in Tradeshow', 'Resubscribed to emails']\ndf['Last Activity']=df['Last Activity'].replace(Others,'Others')\ndf['Last Activity'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Data visualization\nplt.figure(figsize=(12,5))\nsns.countplot(df['Last Activity'],hue=df['Converted'])\nplt.xticks(rotation=90)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Leads with status of last activity as SMS sent can be most targettable\n- Leads with Email opened are moderatly likely to target for conversion.","metadata":{}},{"cell_type":"markdown","source":"### Specialization","metadata":{}},{"cell_type":"code","source":"df['Specialization'].value_counts(normalize=True)/len(df)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['Specialization'].value_counts().index","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Customers less than 4 % can be considered as other category, which make ease for further model analysis\nothers = ['Travel and Tourism', 'Media and Advertising', 'International Business',\n       'Healthcare Management', 'Hospitality Management', 'E-COMMERCE',\n       'Retail Management', 'Rural and Agribusiness', 'E-Business',\n       'Services Excellence','Select']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['Specialization'] = df['Specialization'].replace(others,'Others')\ndf['Specialization'].fillna('Others',inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['Specialization'].value_counts(normalize=True)*100","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Data visualization\nplt.figure(figsize=(12,5))\nsns.countplot(df['Specialization'], hue=df['Converted'])\nplt.xticks(rotation=90)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- From above data we can observe that 16% of data is missing and 24% of the data is not defined. In total unavailable data for this column will be ~40%. \n- So we can consider this could be an important variable, we shall treat them by assigning as Others","metadata":{}},{"cell_type":"markdown","source":"### Let's check for unique values in the variables","metadata":{}},{"cell_type":"code","source":"df.nunique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Dropping the variable which are unique from above data\ndf = df.drop(['Lead Number','Magazine','Receive More Updates About Our Courses','Update me on Supply Chain Content',\n        'Get updates on DM Content','I agree to pay the amount through cheque'],axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Lets check the value counts of Yes/No variable from above data to decide the vaiable for modeling","metadata":{}},{"cell_type":"markdown","source":"### Do Not Call","metadata":{}},{"cell_type":"code","source":"df['Do Not Call'].value_counts(normalize=True)*100","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.drop('Do Not Call',axis=1,inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- All the leads are likely to receive calls about the course, very less number who ignore.\n- And we can drop the column","metadata":{}},{"cell_type":"markdown","source":"### Do Not Email","metadata":{}},{"cell_type":"code","source":"df['Do Not Email'].value_counts(normalize=True)*100","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- All the leads are likely to receive mails about the course, there are about 8% people who ignore.","metadata":{}},{"cell_type":"markdown","source":"### Converted","metadata":{}},{"cell_type":"code","source":"df['Converted'].value_counts(normalize=True)*100","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- About 39% of the leads are converted into business","metadata":{}},{"cell_type":"markdown","source":"### Search","metadata":{}},{"cell_type":"code","source":"df['Search'].value_counts(normalize=True)*100","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# dropping the Search variable, since it is skewed\ndf = df.drop('Search',axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Newspaper Article","metadata":{}},{"cell_type":"code","source":"df['Newspaper Article'].value_counts(normalize=True) # Check for values in the data for cleansing","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# dropping the newspaper article variable, since it is skewed\ndf = df.drop('Newspaper Article',axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Newspaper","metadata":{}},{"cell_type":"code","source":"df['Newspaper'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# dropping the newspaper variable, since it is skewed\ndf = df.drop('Newspaper',axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### X Education Forums","metadata":{}},{"cell_type":"code","source":"df['X Education Forums'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# dropping the X Education Forums variable, since it is skewed\ndf = df.drop('X Education Forums',axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Digital Advertisement","metadata":{}},{"cell_type":"code","source":"df['Digital Advertisement'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# dropping the Digital Advertisement variable, since it is skewed\ndf = df.drop('Digital Advertisement',axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Through Recommendations","metadata":{}},{"cell_type":"code","source":"df['Through Recommendations'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# dropping the Through Recommendations variable, since it is skewed\ndf = df.drop('Through Recommendations',axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### A free copy of Mastering The Interview","metadata":{}},{"cell_type":"code","source":"df['A free copy of Mastering The Interview'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Data visualization\nsns.countplot(df['A free copy of Mastering The Interview'],hue=df['Converted'])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- As per business aspect, most of the people who are looking for upskilling will be either internal promotion or upgrading for current trends.\n- A few category of people will be looking for mastering the interview. \n- That is well understandable from above plot","metadata":{}},{"cell_type":"markdown","source":"### Checking Other variables","metadata":{}},{"cell_type":"code","source":"df['Last Notable Activity'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's categorize the low value counts less than 1% i.e, 90 to others to make modeling easier\ndf['Last Notable Activity'].value_counts().index","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Others = ['Email Bounced',\n       'Unsubscribed', 'Unreachable', 'Had a Phone Conversation',\n       'Email Marked Spam', 'Form Submitted on Website', 'Email Received',\n       'Resubscribed to emails', 'Approached upfront',\n       'View in browser link Clicked']\ndf['Last Notable Activity'] = df['Last Notable Activity'].replace(Others,'Others')\ndf['Last Notable Activity'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Data Visualization\nplt.figure(figsize=(12,5))\nsns.countplot(df['Last Notable Activity'],hue=df['Converted'])\nplt.xticks(rotation=90)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Leads with status of last activity as SMS sent can be most targettable\n- Leads with Email opened are moderatly likely to target for conversion.","metadata":{}},{"cell_type":"markdown","source":"## Bivariate Analysis","metadata":{}},{"cell_type":"markdown","source":"### Total Visits V/s Time spent on website","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(8,6))\nsns.scatterplot(df['TotalVisits'],df['Total Time Spent on Website'],hue=df['Converted'])\nplt.legend(loc='best')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Leads with spending more time on website with 3 or more visits are more likely to get converted","metadata":{}},{"cell_type":"markdown","source":"### Page Views Per Visit v/s Total Time Spent on Website","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(8,6))\nsns.scatterplot(df['Page Views Per Visit'],df['Total Time Spent on Website'],hue=df['Converted'])\nplt.legend(loc='best')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Leads with spending more time are more likely to get converted","metadata":{}},{"cell_type":"markdown","source":"### Page Views Per Visit v/s TotalVisits","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(8,6))\nsns.scatterplot(df['Page Views Per Visit'],df['TotalVisits'],hue=df['Converted'])\nplt.legend(loc='best')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Total visits about 3 are more likely to convert","metadata":{}},{"cell_type":"markdown","source":"### Creating dummy variables for the remaining categorical variables and dropping originals","metadata":{}},{"cell_type":"code","source":"# Creating dummy variables for the variable 'Lead Origin'\nLead_origin = pd.get_dummies(df['Lead Origin'], prefix='LO',drop_first=True)\n#Adding the results to the master dataframe\ndf1 = pd.concat([df,Lead_origin],axis=1)\n# Dropping Lead_origin\ndf1.drop('Lead Origin', axis=1,inplace=True)\n\n# Creating dummy variables for the variable 'Lead Source'\nLead_source = pd.get_dummies(df1['Lead Source'], prefix='LS',drop_first=True)\n#Adding the results to the master dataframe\ndf1 = pd.concat([df1,Lead_source],axis=1)\n# Dropping Lead_origin\ndf1.drop('Lead Source', axis=1,inplace=True)\n\n# Creating dummy variables for the variable 'Lead Source'\nLast_activity = pd.get_dummies(df1['Last Activity'], prefix='LA',drop_first=True)\n#Adding the results to the master dataframe\ndf1 = pd.concat([df1,Last_activity],axis=1)\n# Dropping Last_activity\ndf1.drop('Last Activity', axis=1,inplace=True)\n\n# Creating dummy variables for the variable 'Specialization'\ntags = pd.get_dummies(df1['Tags'], prefix='T',drop_first=True)\n#Adding the results to the master dataframe\ndf1 = pd.concat([df1,tags],axis=1)\n# Dropping Lead_origin\ndf1.drop('Tags', axis=1,inplace=True)\n\n# Creating dummy variables for the variable 'Specialization'\nLead_Quality = pd.get_dummies(df1['Lead Quality'], prefix='LQ',drop_first=True)\n#Adding the results to the master dataframe\ndf1 = pd.concat([df1,Lead_Quality],axis=1)\n# Dropping Lead_origin\ndf1.drop('Lead Quality', axis=1,inplace=True)\n\n# Creating dummy variables for the variable 'Specialization'\nSpecialization = pd.get_dummies(df1['Specialization'], prefix='Sp',drop_first=True)\n#Adding the results to the master dataframe\ndf1 = pd.concat([df1,Specialization],axis=1)\n# Dropping Lead_origin\ndf1.drop('Specialization', axis=1,inplace=True)\n\n# Creating dummy variables for the variable 'What is your current occupation'\nOccupation = pd.get_dummies(df1['What is your current occupation'], prefix='Occupation',drop_first=True)\n#Adding the results to the master dataframe\ndf1 = pd.concat([df1,Occupation],axis=1)\n# Dropping Lead_origin\ndf1.drop('What is your current occupation', axis=1,inplace=True)\n\n# Creating dummy variables for the variable 'Last Notable Activity'\nLast_notable_activity = pd.get_dummies(df1['Last Notable Activity'], prefix='LNA',drop_first=True)\n#Adding the results to the master dataframe\ndf1 = pd.concat([df1,Last_notable_activity],axis=1)\n# Dropping Lead_origin\ndf1.drop('Last Notable Activity', axis=1,inplace=True)\n\n\n\ndf1.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# One hot encoding for 'Do Not Email' & 'A free copy of Mastering The Interview'\ndummy = pd.get_dummies(df1[['Do Not Email', 'A free copy of Mastering The Interview']], drop_first=True)\ndf1 = pd.concat([df1,dummy], axis=1)\ndf1.drop(['Do Not Email', 'A free copy of Mastering The Interview'], axis=1, inplace=True)\ndf1.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1_corr = df1.corr()\ncorr_final = df1_corr.where(np.triu(np.ones(df1_corr.shape),k=1).astype(np.bool))\ncorr_final","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr_final = corr_final.unstack()\ncorr_final = corr_final.sort_values(ascending=False).drop_duplicates()\ncorr_final.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Defining X & y\nX = df1.drop(['Prospect ID','Converted'],axis=1)\ny = df1['Converted']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Splitting of train-test data","metadata":{}},{"cell_type":"code","source":"X_train, X_test,y_train , y_test = train_test_split(X, y, train_size=0.7, test_size = 0.3, random_state = 0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature Scaling","metadata":{}},{"cell_type":"code","source":"scaler = StandardScaler()\n\nX_train[['TotalVisits','Total Time Spent on Website','Page Views Per Visit',\n         'Asymmetrique Activity Score','Asymmetrique Profile Score']] = scaler.fit_transform(\n    X_train[['TotalVisits','Total Time Spent on Website','Page Views Per Visit',\n         'Asymmetrique Activity Score','Asymmetrique Profile Score']])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Logistic regression\n\nlr = LogisticRegression()\nrfe = RFE(lr,15) # Logistic regression with 20 variable\nrfe = rfe.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rfe.support_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"list(zip(X_train.columns,rfe.support_,rfe.ranking_))","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"col = X_train.columns[rfe.support_]\nX_train.columns[~rfe.support_]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1_corr = df1[['Converted','TotalVisits', 'Page Views Per Visit', 'Asymmetrique Activity Score',\n       'Asymmetrique Profile Score', 'LO_Landing Page Submission',\n       'LO_Lead Import', 'LO_Quick Add Form', 'LS_Google', 'LS_Organic Search',\n       'LS_Reference', 'LS_Referral Sites', 'LA_Email Opened', 'LA_Others',\n       'LA_Page Visited on Website', 'T_Graduation in progress',\n       'T_Interested  in full time MBA', 'T_Others', 'LQ_Low in Relevance',\n       'LQ_Might be', 'LQ_Not Sure', 'Sp_Business Administration',\n       'Sp_Finance Management', 'Sp_Human Resource Management',\n       'Sp_IT Projects Management', 'Sp_Marketing Management',\n       'Sp_Operations Management', 'Sp_Others', 'Sp_Supply Chain Management',\n       'Occupation_Housewife', 'Occupation_Other', 'Occupation_Student',\n       'Occupation_Unemployed', 'Occupation_Working Professional',\n       'LNA_Modified', 'LNA_Olark Chat Conversation',\n       'LNA_Page Visited on Website',\n       'A free copy of Mastering The Interview_Yes']].corr()\ndf1_corr","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Model building\nX_train_sm = sm.add_constant(X_train[col])\nlog1 = sm.GLM(y_train,X_train_sm, sm.families.Binomial())\nres = log1.fit()\nres.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Model accuracy is good but there are some features that are having p-value > 0.05. So, proceeding with VIF score and feature elimination.","metadata":{}},{"cell_type":"markdown","source":"### VIF","metadata":{}},{"cell_type":"code","source":"# Check for the VIF values of the feature variables. \nfrom statsmodels.stats.outliers_influence import variance_inflation_factor","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train[col].columns\nvif['VIF'] = [variance_inflation_factor(X_train[col].values, i) for i in range(X_train[col].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Dropping high p-value variable LS_Olark Chat\ncol = col.drop('LS_Olark Chat',1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_sm = sm.add_constant(X_train[col])\nlog1 = sm.GLM(y_train,X_train_sm, sm.families.Binomial())\nres = log1.fit()\nres.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train[col].columns\nvif['VIF'] = [variance_inflation_factor(X_train[col].values, i) for i in range(X_train[col].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Dropping high p-value variable LO_Lead Add Form\ncol = col.drop('LO_Lead Add Form',1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_sm = sm.add_constant(X_train[col])\nlog1 = sm.GLM(y_train,X_train_sm, sm.families.Binomial())\nres = log1.fit()\nres.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train[col].columns\nvif['VIF'] = [variance_inflation_factor(X_train[col].values, i) for i in range(X_train[col].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- All p-values equal to or less than 0.05, so we can final the model and find the probablity","metadata":{}},{"cell_type":"code","source":"# Prediction of y value\ny_train_pred = res.predict(X_train_sm)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train_pred = y_train_pred.values.reshape(-1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train_pred_final = pd.DataFrame({'Converted':y_train.values, 'Conv_Prob':y_train_pred})\ny_train_pred_final.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Plotting the ROC Curve","metadata":{}},{"cell_type":"code","source":"def draw_roc( actual, probs ):\n    fpr, tpr, thresholds = metrics.roc_curve( actual, probs,\n                                              drop_intermediate = False )\n    auc_score = metrics.roc_auc_score( actual, probs )\n    plt.figure(figsize=(5, 5))\n    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic example')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n\n    return None","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"An ROC curve demonstrates several things:\n\n- It shows the tradeoff between sensitivity and specificity (any increase in sensitivity will be accompanied by a decrease in specificity).\n- The closer the curve follows the left-hand border and then the top border of the ROC space, the more accurate the test.\n- The closer the curve comes to the 45-degree diagonal of the ROC space, the less accurate the test.","metadata":{}},{"cell_type":"code","source":"fpr, tpr, thresholds = metrics.roc_curve( y_train_pred_final.Converted, y_train_pred_final.Conv_Prob, \n                                             drop_intermediate = False )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"draw_roc(y_train_pred_final.Converted, y_train_pred_final.Conv_Prob)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Finding Optimal Cutoff Point","metadata":{}},{"cell_type":"markdown","source":"Optimal cutoff probability is that prob where we get balanced sensitivity and specificity","metadata":{}},{"cell_type":"code","source":"# Let's create columns with different probability cutoffs \nnumbers = [float(x)/10 for x in range(10)]\nfor i in numbers:\n    y_train_pred_final[i]= y_train_pred_final.Conv_Prob.map(lambda x: 1 if x > i else 0)\ny_train_pred_final.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now let's calculate accuracy sensitivity and specificity for various probability cutoffs.\ncutoff_df = pd.DataFrame( columns = ['prob','accuracy','sensi','speci'])\nfrom sklearn.metrics import confusion_matrix\n\n# TP = confusion[1,1] # true positive \n# TN = confusion[0,0] # true negatives\n# FP = confusion[0,1] # false positives\n# FN = confusion[1,0] # false negatives\n\nnum = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\nfor i in num:\n    cm1 = metrics.confusion_matrix(y_train_pred_final.Converted, y_train_pred_final[i] )\n    total1=sum(sum(cm1))\n    accuracy = (cm1[0,0]+cm1[1,1])/total1\n    \n    speci = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n    sensi = cm1[1,1]/(cm1[1,0]+cm1[1,1])\n    cutoff_df.loc[i] =[ i ,accuracy,sensi,speci]\nprint(cutoff_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's plot accuracy sensitivity and specificity for various probabilities.\ncutoff_df.plot.line(x='prob', y=['accuracy','sensi','speci'])\nplt.grid()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- From the curve above, 0.20 is the optimum point to take it as a cutoff probability.","metadata":{}},{"cell_type":"code","source":"y_train_pred_final['final_predicted'] = y_train_pred_final.Conv_Prob.map( lambda x: 1 if x > 0.20 else 0)\n\ny_train_pred_final.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's check the overall accuracy.\nprint(metrics.accuracy_score(y_train_pred_final.Converted, y_train_pred_final.final_predicted))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's take a look at the confusion matrix again \nconfusion = metrics.confusion_matrix(y_train_pred_final.Converted, y_train_pred_final.final_predicted )\nconfusion","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TP = confusion[1,1] # true positive \nTN = confusion[0,0] # true negatives\nFP = confusion[0,1] # false positives\nFN = confusion[1,0] # false negatives","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's see the sensitivity of our logistic regression model\nTP / float(TP+FN)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let us calculate specificity\nTN / float(TN+FP)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate false postive rate - predicting churn when customer does not have churned\nFP/ float(TN+FP)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# positive predictive value \nTP / float(TP+FP)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Negative predictive value\nTN / float(TN+ FN)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- We have acheived the very good scores of Specificity(0.91), Sensitivity(0.92), false positive(0.09), true positive(0.87) and true negative(0.94). Indicates model is well fit.\n- Let's check the same on test data","metadata":{}},{"cell_type":"markdown","source":"### Precision and Recall & F1 score","metadata":{}},{"cell_type":"markdown","source":"#### Precision","metadata":{}},{"cell_type":"code","source":"precision_score(y_train_pred_final.Converted, y_train_pred_final.final_predicted)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Recall","metadata":{}},{"cell_type":"code","source":"recall_score(y_train_pred_final.Converted, y_train_pred_final.final_predicted)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f1_score(y_train_pred_final.Converted, y_train_pred_final.final_predicted)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Precision is 0.92, recall is 0.87 and F1 Score is 0.89 which are good parameters of the model.\n- Where F1 score is a relative average weight of precision and recall.","metadata":{}},{"cell_type":"markdown","source":"### Precision and recall tradeoff","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import precision_recall_curve\np, r, thresholds = precision_recall_curve(y_train_pred_final.Converted, y_train_pred_final.Conv_Prob)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(thresholds, p[:-1], \"g-\")\nplt.plot(thresholds, r[:-1], \"r-\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Making predictions on the test set","metadata":{}},{"cell_type":"code","source":"X_test[['TotalVisits','Total Time Spent on Website','Page Views Per Visit',\n         'Asymmetrique Activity Score','Asymmetrique Profile Score']] = scaler.transform(\n    X_test[['TotalVisits','Total Time Spent on Website','Page Views Per Visit',\n         'Asymmetrique Activity Score','Asymmetrique Profile Score']])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test_sm = sm.add_constant(X_test[col])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test_pred = res.predict(X_test_sm) # Predicting the y","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Converting y_pred to a dataframe which is an array\ny_pred_1 = pd.DataFrame(y_test_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Converting y_test to dataframe\ny_test_df = pd.DataFrame(y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Removing index for both dataframes to append them side by side \ny_pred_1.reset_index(drop=True, inplace=True)\ny_test_df.reset_index(drop=True, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Appending y_test_df and y_pred_1\ny_pred_final = pd.concat([y_test_df, y_pred_1],axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_final.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Renaming the column \ny_pred_final.columns = ['Converted', 'Conv_prob']\ny_pred_final.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_final['final_predicted'] = y_pred_final.Conv_prob.map(lambda x: 1 if x > 0.2 else 0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_final['Lead_score'] = y_pred_final.Conv_prob*100","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_final.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's check the overall accuracy.\nmetrics.accuracy_score(y_pred_final.Converted, y_pred_final.final_predicted)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"confusion2 = metrics.confusion_matrix(y_pred_final.Converted, y_pred_final.final_predicted )\nconfusion2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TP = confusion2[1,1] # true positive \nTN = confusion2[0,0] # true negatives\nFP = confusion2[0,1] # false positives\nFN = confusion2[1,0] # false negatives","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's see the sensitivity of our logistic regression model\nTP / float(TP+FN)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let us calculate specificity\nTN / float(TN+FP)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f1_score(y_pred_final.Converted, y_pred_final.final_predicted)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Values of Sensitivity, specificity and F1 score are nearest to the train data set and we can say model is good.","metadata":{}},{"cell_type":"code","source":"df1.head()","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_sm = sm.add_constant(df1[col])\ndf1['Lead Score'] = round(pd.DataFrame(res.predict(df_sm))*100)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1['Lead Score'].describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(df1['Lead Score'][df1['Lead Score']>20])/len(df1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['Converted'].value_counts(normalize=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1['final_predicted'] = df1['Lead Score'].map(lambda x: 1 if x > 20 else 0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1[['Prospect ID','Lead Score','Converted','final_predicted']].head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}