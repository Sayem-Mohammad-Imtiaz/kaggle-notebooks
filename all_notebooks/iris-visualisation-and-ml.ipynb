{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Iris Flower Classification using Machine Learning\n<hr>\n\n![Iris Flowers on unsplash](https://raw.githubusercontent.com/Dutta-SD/Images_Unsplash/master/Kaggle/olga-mandel-gK6f8bKKic0-unsplash.jpg)\n\nIris Flower Dataset is a very useful datset. It is usually the first dataset that one usually comes across when starting on their ML journey.\n\nIt consists of dimensions of various flowers and we are required to predict what is the species of the flower.\n\nLet's explore this dataset and apply ML on it.\n\nSpecifically what we will be using : \n* **_Repeated Stratified K Fold Cross Validation_**\n* **Removing Feature interdependence using _PCA_**\n* **_Naive Bayes_ Model**\n* **Random Forest Model**"},{"metadata":{},"cell_type":"markdown","source":"## _Initial Kaggle Environment Setup_\n\nThis is the initial cell for most Kaggle Notebooks. The output of the below cell will give you the path for the dataset"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* As we see, the dataset is a `.csv` file named *_IRIS.csv_*. \nWe import the dataset using <b><u>pandas(pd)</u></b> and store it in a DataFrame object."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Importing the data\niris_data = pd.read_csv(\"/kaggle/input/iris-flower-dataset/IRIS.csv\")\n# See the top columns of the datset\niris_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we see the dataset consists of 5 columns.\n\nLet us use `DataFrame.info()` to get insights about various datatypes of the data."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Information about DataType\niris_data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Some insights which we derive from the info:\n* There are 5 columns, 4 have datatype `float64` and 1 has datatype `object`(Can consider it to be string)\n* There are 150 entries in each column. \n* As we can see, all entries are non-null. This means that every cell of DataFrame has some value. Usually in real world data, we have `NaN` values(which serves as placeholder for missing data)\n* `sepal_length`, `sepal_width`, `petal_length`, `petal_width` are the features we have. We have to use it to predict `species`."},{"metadata":{},"cell_type":"markdown","source":"### Converting species column to numeric form\n\nWe will convert the species to Label Encoded form using `DataFrame.map()`"},{"metadata":{"trusted":true},"cell_type":"code","source":"# as species is string, this will create problem for the model. convert to integer data\niris_data['species'] = iris_data['species'].map({  \n                                                'Iris-setosa' : 0, \n                                                'Iris-versicolor' : 1,\n                                                'Iris-virginica' : 2\n                                                  })\niris_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get dependent and independent features\nX, y = iris_data.iloc[:, :-1], iris_data.iloc[:, -1]   ### The last feature is 'species'. This is dependent feature\nX.shape, y.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Visualisation\n\nData visualisation is an important step in Data Science. We can use it for:\n* **Outlier Detection** - See if any datapoint is very different from the others. If this happens, we need to remove it as otherwise it can skew our observations heavily. Visualising can help in this case\n\n\n* **Class Imbalance Detection** - Often in real world data, you can have observations from one class in a large number and other class in small numbers. This is problematic as if we have a dumb model that predicts the majority class all the time, it is going to get a good accuracy.<br><br>**If we have 99 'YES' and 1 'NO' in the dataset then the model predicting 'YES' all the time will get a 99% Accuracy. Visualisation helps us in removing this.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualisation Libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1. PairPlot of all data Points\nPairplot generates plots between all pairs of data. This is helpful for visualising the whole data in one go."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"### hue = species colours plot as per species\n### It will give 3 colours in the plot\n\nsns.set_style('whitegrid')   ### Sets grid style\nsns.pairplot(data = iris_data,\n             hue = 'species',\n             palette = sns.color_palette(\"husl\"),\n             markers=[\"o\", \"s\", \"D\"],\n             corner = True,\n             diag_kind = 'kde'             \n             ); ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### PairPlot insights into the data\n1. `petal-length` and `petal-width` seem to be positively correlated(seem to be having a linear relationship). \n2. Iris-Setosa seems to have smaller petal length and petal width as compared to others. \n3. Looking at the overall scenario, it seems to  be the case that Iris-Setosa has smaller dimensions than other flowers."},{"metadata":{},"cell_type":"markdown","source":"### 2. BarPlot for Class Distribution\nWe will generate a bar plot to see of there is some data imbalance or not (is some flower species more frequent than the other)"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Bar plot\nsns.set_style('darkgrid')\nsns.barplot(x = iris_data['species'].unique(),\n            y = iris_data['species'].value_counts(),\n            palette=sns.color_palette([\"#e74c3c\", \"#34495e\", \"#2ecc71\"]));","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Bar Plot Inference\n1. There are exactly 50 observations in each class. So the model has no class imbalance. (Lucky for us! :P)"},{"metadata":{},"cell_type":"markdown","source":"### 3. HeatMap\n\nHeatMap of the _correlation matrix_ will give the amount of correlation between two variables. **Correlation** measures the amount by which two variables vary together. So variables which vary with the dependent variable will be a better predictor of the dependent variables."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# Correlation matrix will be generated by DataFrame.corr() function\nplt.figure(figsize = (8, 8))\nsns.heatmap(iris_data.corr(), \n            annot=True,\n            linewidths=3,\n           cmap=\"terrain\",\n           square = True);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Heatmap Inference\n\n* Petal Length, Petal width, and Sepal length have the most stong positive correlation to species.\nSepal width has small negative correlation to species\nThis indicates that petal length, petal width, sepal width are enough predictors. \n\n* However, we see that the features are intercorrelated to each other. So we have multicollinearity, that is multiple features giving the same information. So our model is being fed the same data. \n\nThis means we need to remove it by **PCA**"},{"metadata":{},"cell_type":"markdown","source":"# Plotly Plots"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"import plotly.express as px\nfig = px.sunburst(iris_data,\n                  path = ['species', 'petal_length'],\n                  color ='petal_width',\n                 title = 'Petal Dimension Distribution')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Removing Interdependency by PCA\n\nPCA reduces dimensions as well as is used to reduce interdependency of features\n\n* **NOTE** : PCA assumes data is centered. That is mean of data points is 0. We need to ensure this."},{"metadata":{"trusted":true},"cell_type":"code","source":"## PCA\nfrom sklearn.decomposition import PCA\n\npca_cleaner = PCA(4, whiten=True) ## First one is number of components\n\n## Remove the mean of the data, to center it.\nX -= X.mean()\n\niris_data_X = pca_cleaner.fit_transform(X)\niris_data_X.shape    ### This becomes a numpy array","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Splitting(Repeated Stratified K-Fold)\n\nNow we split the data into training and validation set. This is a small dataset, so let us use $Repeated\\ Stratified\\ K\\ Fold\\ Cross\\ Validation$ for this.\n\nLet us understand what is Repeated Stratified K Fold Cross Validation:\n* **K Fold Cross Validation** - We take $k$ splits of the data, that is split data into $k$ equal parts. Take one part as validation set and other as training data. Then train model further by taking any other of the $k$ splits and the remaining part as training data.\n\n* **Stratified** - We will split the k folds in such a way that each part will have roughly equal number of classes.$$[Here,\\ each\\ split\\ must\\ have\\ roughly\\ equal\\ number\\ of\\ flowers\\ of\\ same\\ species]$$\n\n* **Repeated** - We will repeat this k fold splitting process a number of times.\n\nThis technique is to ensure we get maximum amount of training data for our model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import RepeatedStratifiedKFold\n\nrskfSplitter = RepeatedStratifiedKFold(random_state = 40)    ### RSKF object declaration. Use it when training model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model Training\n\nWe will select multiple models to check accuracy and see what gives us the best results.\n\n\n<hr>\n\n## Evaluation metric\nThe evaluation metric will be cross validation score\n"},{"metadata":{},"cell_type":"markdown","source":"### 1. Naive Bayes Model\nNow we will make a `Naive Bayes` Model for predicting the flower type.\n\nNaive Bayes outputs probabilites for the data point to belong to each class. The class with the highest probability is taken to be the class of the flower.\n\nIt uses Bayes Theorem to calculate the probabilities for a flower having particular characteristics to belong to a class. When it sees new data, it uses its previous calculations to predict the class of new data.\n\nIt assumes that the features of the data are independent from one another."},{"metadata":{"trusted":true},"cell_type":"code","source":"# import Naive Bayes Model\nfrom sklearn.naive_bayes import GaussianNB\n\n# import cross validation score\nfrom sklearn.model_selection import cross_val_score\n\nnaiveBayesModel = GaussianNB()    ### Predicting Model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2. Get cross Validation Scores"},{"metadata":{"trusted":true},"cell_type":"code","source":"# get Cross Validation Scores\nscores = cross_val_score(naiveBayesModel, iris_data_X, y, cv = rskfSplitter, n_jobs = 4) #### n_jobs - for parallelization","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scores","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Get final Score\n\nWe also plot the cross validation scores using pyplot of matplotlib."},{"metadata":{"trusted":true},"cell_type":"code","source":"### Plotting using Matplotlib\n### We customise each element like this\nplt.figure(figsize = (20, 2))\nplt.title(\"Cross Validation Scores in Naive Bayes\")\nplt.xlabel(\"Number of Tests\")\nplt.ylabel(\"Model Accuracy\")\nplt.plot(scores);\nplt.show();\n\n# Final Score\nprint(f\"Final Accuracy of Gaussian Naive Bayes Model is:  {scores.mean()}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Random Forest Model\n\nWe are going to use `Random Forest Model` for prediction next. \n\nIdea of Random Forest Model :\n* Makes many small decision trees which do not classify all that well on their own\n* Takes a vote of the predictions of all the decision trees to classify.\n\nThis is a very high level description of Random Forest."},{"metadata":{"trusted":true},"cell_type":"code","source":"## This code will be very similar to the Naive Bayes model code.\n\n#---------------------------------------------------------------\n\n# import Random Forest Model\nfrom sklearn.ensemble import RandomForestClassifier\n\nrForestModel = RandomForestClassifier()    ### Predicting Model\n\n# get Cross Validation Scores\nscoresRF = cross_val_score(rForestModel, iris_data_X, y, cv = rskfSplitter, n_jobs = 4) #### n_jobs - for parallelization\n\n### Plotting using Matplotlib\nplt.figure(figsize = (20, 2))\nplt.title(\"Cross Validation Scores in Random Forest\")\nplt.xlabel(\"Number of Tests\")\nplt.ylabel(\"Model Accuracy\")\nplt.plot(scoresRF);\nplt.show();\n\n# Final Score\nprint(f\"Final Accuracy of Random Forest Model is:  {scoresRF.mean()}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Final Conclusion\n\nWe achieved a cv-score of **0.952** with the Random Forest Model. Tuning Hyperparameters can improve the score maybe."},{"metadata":{},"cell_type":"markdown","source":"This is one analysis of the iris datset. It can be imporved in many ways.\n<hr>\n<center>!!Any suggestions to improve is sincerely welcome!!</center>"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}