{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install albumentations > /dev/null\n!git clone https://github.com/qubvel/efficientnet.git","execution_count":1,"outputs":[{"output_type":"stream","text":"\u001b[33mYou are using pip version 19.0.3, however version 19.1.1 is available.\nYou should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\nCloning into 'efficientnet'...\nremote: Enumerating objects: 116, done.\u001b[K\nremote: Counting objects: 100% (116/116), done.\u001b[K\nremote: Compressing objects: 100% (77/77), done.\u001b[K\nremote: Total 116 (delta 62), reused 75 (delta 35), pack-reused 0\u001b[K\nReceiving objects: 100% (116/116), 764.50 KiB | 0 bytes/s, done.\nResolving deltas: 100% (62/62), done.\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport sys\nimport numpy as np\nfrom skimage.io import imread\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport keras\nimport pandas as pd\nfrom PIL import Image\nimport cv2\n\nfrom keras.applications.resnet50 import preprocess_input\n\nfrom keras.applications.imagenet_utils import decode_predictions\n\nfrom efficientnet import EfficientNetB0,EfficientNetB3\nfrom efficientnet import center_crop_and_resize, preprocess_input\nfrom keras.optimizers import SGD, Adam","execution_count":2,"outputs":[{"output_type":"stream","text":"Using TensorFlow backend.\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"class DataGenerator(keras.utils.Sequence):\n    'Generates data for Keras'\n    def __init__(self, list_IDs, labels,augmentations, batch_size=32, dim=(32,32,32), n_channels=3,\n                 n_classes=5, shuffle=True):\n        'Initialization'\n        self.dim = dim\n        self.batch_size = batch_size\n        self.labels = labels\n        self.list_IDs = list_IDs\n        self.n_channels = n_channels\n        self.n_classes = n_classes\n        self.shuffle = shuffle\n        self.path = '../input/game-of-deep-learning-ship-datasets/train/images/'\n        self.augment = augmentations\n        self.on_epoch_end()\n\n    def __len__(self):\n        'Denotes the number of batches per epoch'\n        return int(np.floor(len(self.list_IDs) / self.batch_size))\n\n    def __getitem__(self, index):\n        'Generate one batch of data'\n        # Generate indexes of the batch\n        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n\n        # Find list of IDs\n        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n\n        # Generate data\n        X, y = self.data_generation(list_IDs_temp)\n\n        return np.stack([\n            self.augment(image=x)[\"image\"] for x in X\n        ], axis=0), np.array(y)\n\n    def on_epoch_end(self):\n        'Updates indexes after each epoch'\n        self.indexes = np.arange(len(self.list_IDs))\n        if self.shuffle == True:\n            np.random.shuffle(self.indexes)\n\n    def data_generation(self, list_IDs_temp):\n        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n        # Initialization\n        X = np.empty((self.batch_size, *self.dim, self.n_channels))\n#         print(X.shape,self.dim)\n        y = np.empty((self.batch_size), dtype=int)\n\n        # Generate data\n        for i, ID in enumerate(list_IDs_temp):\n            \n            im = np.array(Image.open(self.path+ID))\n            if len(im.shape)==2:\n                im = cv2.cvtColor(im,cv2.COLOR_GRAY2RGB)\n\n#             # Resize sample\n            X[i,] = cv2.resize(im,(self.dim[0],self.dim[1]))\n\n            # Store class\n            y[i] = self.labels.loc[ID].category\n\n#         print(X.shape)\n        return np.uint8(X), keras.utils.to_categorical(y, num_classes=self.n_classes)","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import cv2\nfrom albumentations import (\n    Compose, HorizontalFlip, CLAHE, HueSaturationValue,\n    RandomBrightness, RandomContrast, RandomGamma,OneOf,\n    ToFloat, ShiftScaleRotate,GridDistortion, ElasticTransform, JpegCompression, HueSaturationValue,\n    RGBShift, RandomBrightness, RandomContrast, Blur, MotionBlur, MedianBlur, GaussNoise,CenterCrop,\n    IAAAdditiveGaussianNoise,GaussNoise,Cutout\n)\n\nAUGMENTATIONS_TRAIN = Compose([\n    HorizontalFlip(p=0.5),\n    RandomContrast(limit=0.2, p=0.5),\n    RandomGamma(gamma_limit=(80, 120), p=0.5),\n    RandomBrightness(limit=1.2, p=0.5),\n    HueSaturationValue(hue_shift_limit=5, sat_shift_limit=20,\n                       val_shift_limit=10, p=.5),\n#     CenterCrop(height=128, width=128, p=0.5),\n    Cutout(p=0.5),\n    OneOf([\n            MotionBlur(p=0.2),\n            MedianBlur(blur_limit=3, p=0.1),\n            Blur(blur_limit=3, p=0.1),\n        ], p=0.3),\n    OneOf([\n            IAAAdditiveGaussianNoise(),\n            GaussNoise(),\n        ], p=0.2),\n    # CLAHE(p=1.0, clip_limit=2.0),\n    ShiftScaleRotate(\n        shift_limit=0.0625, scale_limit=0.1, \n        rotate_limit=15, border_mode=cv2.BORDER_REFLECT_101, p=0.5), \n    ToFloat(max_value=255)\n],p=1)\n\n\nAUGMENTATIONS_TEST = Compose([\n    # CLAHE(p=1.0, clip_limit=2.0),\n    ToFloat(max_value=255)\n],p=1)","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class TestDataGenerator(keras.utils.Sequence):\n    'Generates data for Keras'\n    def __init__(self, list_IDs,augmentations, batch_size=32, dim=(32,32,32), n_channels=3,\n                 n_classes=5, shuffle=False,flip=False,path=None):\n        'Initialization'\n        self.dim = dim\n        self.batch_size = batch_size\n        self.list_IDs = list_IDs\n        self.n_channels = n_channels\n        self.n_classes = n_classes\n        self.shuffle = shuffle\n        if path is not None:\n            self.path = path\n        else:\n            self.path = '../input/avgameofdltestjpg/test-jpg/test-jpg/'\n        self.on_epoch_end()\n        self.augment = augmentations\n        self.flip = flip\n#         print(len(self.list_IDs))\n\n    def __len__(self):\n        'Denotes the number of batches per epoch'\n        return int(np.ceil(len(self.list_IDs) / self.batch_size))\n\n    def __getitem__(self, index):\n        'Generate one batch of data'\n        # Generate indexes of the batch\n#         if  im_left < self.batch_size:\n#             print('*','index',index,len(self.list_IDs) - (index)*self.batch_size )\n#             indexes = self.indexes[(index)*self.batch_size:]            \n#         elif im_left<0: return \n#         else:\n#             indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n\n        indexes = self.indexes[index*self.batch_size:min((index+1)*self.batch_size,len(self.list_IDs))]\n\n        # Find list of IDs\n        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n\n        # Generate data\n        X = self.data_generation(list_IDs_temp)\n        \n        return np.stack([\n                    self.augment(image=x)[\"image\"] for x in X\n                ], axis=0)\n#         return X\n\n    def on_epoch_end(self):\n        'Updates indexes after each epoch'\n        self.indexes = np.arange(len(self.list_IDs))\n\n    def data_generation(self, list_IDs_temp):\n        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n        # Initialization\n        X = np.empty((len(list_IDs_temp), *self.dim, self.n_channels))\n\n        # Generate data\n        for i, ID in enumerate(list_IDs_temp):\n            \n            im = np.array(Image.open(self.path+ID))\n            if len(im.shape)==2:\n                im = cv2.cvtColor(im,cv2.COLOR_GRAY2RGB)\n\n            # Resize sample\n            if self.flip:\n                X[i,] = np.fliplr(cv2.resize(im,(self.dim[0],self.dim[1])))\n            else:\n                X[i,] = cv2.resize(im,(self.dim[0],self.dim[1]))\n\n\n        return np.uint8(X)","execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/game-of-deep-learning-ship-datasets/train/train.csv')\ntrain['category'] = train['category'] - 1\n\n# Parameters\nparams = {'dim': (128,128),\n          'batch_size': 32,\n          'n_classes': 5,\n          'n_channels': 3,\n          'shuffle': True}","execution_count":15,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.utils import plot_model\nfrom keras.models import Model\nfrom keras.layers import Input\nfrom keras.layers import Dense\nfrom keras.layers import Flatten\nfrom keras.layers import Activation\nfrom keras.layers import Dropout\nfrom keras.layers import Maximum\nfrom keras.layers import ZeroPadding2D\nfrom keras.layers.convolutional import Conv2D\nfrom keras.layers.pooling import MaxPooling2D\nfrom keras.layers.merge import concatenate\nfrom keras import regularizers\nfrom keras.layers import BatchNormalization\nfrom keras.optimizers import Adam, SGD\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\nfrom keras.layers.advanced_activations import LeakyReLU\nfrom keras.utils import to_categorical\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom skimage.transform import resize as imresize\nfrom tqdm import tqdm\n","execution_count":16,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.layers import Dense, Activation, Flatten, Dropout\nfrom keras.models import Sequential, Model\n\ndef build_finetune_model(base_model, dropout, fc_layers, num_classes):\n#     for layer in base_model.layers:\n#         layer.trainable = False\n\n    x = base_model.output\n    x = Flatten()(x)\n    for fc in fc_layers:\n        # New FC layer, random init\n        x = Dense(fc, activation='relu')(x) \n        x = Dropout(dropout)(x)\n\n    # New softmax layer\n    predictions = Dense(num_classes, activation='softmax')(x) \n    \n    finetune_model = Model(inputs=base_model.input, outputs=predictions)\n\n    return finetune_model","execution_count":17,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_loss_acc(history):\n    plt.figure(figsize=(20,7))\n    plt.subplot(1,2,1)\n    plt.plot(history.history['loss'][1:])    \n    plt.plot(history.history['val_loss'][1:])    \n    plt.title('model loss')    \n    plt.ylabel('val_loss')    \n    plt.xlabel('epoch')    \n    plt.legend(['Train','Validation'], loc='upper left')\n    \n    plt.subplot(1,2,2)\n    plt.plot(history.history['acc'][1:])\n    plt.plot(history.history['val_acc'][1:])\n    plt.title('Model Accuracy')\n    plt.ylabel('val_acc')\n    plt.xlabel('epoch')\n    plt.legend(['Train','Validation'], loc='upper left')\n    plt.show()","execution_count":18,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv('../input/game-of-deep-learning-ship-datasets/test_ApKoW4T.csv')","execution_count":19,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras import backend as K\nimport tensorflow as tf\n'''\nCompatible with tensorflow backend\n'''\ndef focal_loss(gamma=2., alpha=.25):\n    def focal_loss_fixed(y_true, y_pred):\n        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n        return -K.sum(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1))-K.sum((1-alpha) * K.pow( pt_0, gamma) * K.log(1. - pt_0))\n    return focal_loss_fixed","execution_count":20,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class SnapshotCallbackBuilder:\n    def __init__(self, nb_epochs, nb_snapshots, init_lr=0.1):\n        self.T = nb_epochs\n        self.M = nb_snapshots\n        self.alpha_zero = init_lr\n\n    def get_callbacks(self, model_prefix='Model'):\n\n        callback_list = [\n#             callbacks.ModelCheckpoint(\"./keras.model\",monitor='val_loss', \n#                                    mode = 'min', save_best_only=True, verbose=1),\n            swa,\n            callbacks.LearningRateScheduler(schedule=self._cosine_anneal_schedule)\n        ]\n\n        return callback_list\n\n    def _cosine_anneal_schedule(self, t):\n        cos_inner = np.pi * (t % (self.T // self.M))  # t - 1 is used when t has 1-based indexing.\n        cos_inner /= self.T // self.M\n        cos_out = np.cos(cos_inner) + 1\n        return float(self.alpha_zero / 2 * cos_out)\n\nimport keras.callbacks as callbacks\n\nclass SWA(keras.callbacks.Callback):\n    \n    def __init__(self, filepath, swa_epoch):\n        super(SWA, self).__init__()\n        self.filepath = filepath\n        self.swa_epoch = swa_epoch \n    \n    def on_train_begin(self, logs=None):\n        self.nb_epoch = self.params['epochs']\n        print('Stochastic weight averaging selected for last {} epochs.'\n              .format(self.nb_epoch - self.swa_epoch))\n        \n    def on_epoch_end(self, epoch, logs=None):\n        \n        if epoch == self.swa_epoch:\n            self.swa_weights = self.model.get_weights()\n            \n        elif epoch > self.swa_epoch:    \n            for i in range(len(self.swa_weights)):\n                self.swa_weights[i] = (self.swa_weights[i] * \n                    (epoch - self.swa_epoch) + self.model.get_weights()[i])/((epoch - self.swa_epoch)  + 1)  \n\n        else:\n            pass\n        \n    def on_train_end(self, logs=None):\n        self.model.set_weights(self.swa_weights)\n        print('Final model parameters set to stochastic weight average.')\n        self.model.save_weights(self.filepath)\n        print('Final stochastic averaged weights saved to file.')","execution_count":21,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def focal_loss(gamma=2., alpha=.25):\n    def focal_loss_fixed(y_true, y_pred):\n        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n\n        pt_1 = K.clip(pt_1, 1e-3, .999)\n        pt_0 = K.clip(pt_0, 1e-3, .999)\n\n        return -K.sum(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1))-K.sum((1-alpha) * K.pow( pt_0, gamma) * K.log(1. - pt_0))\n    return focal_loss_fixed","execution_count":22,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"skf = StratifiedKFold(5)\nall_preds,fold = [],1\n\nHEIGHT = 128\nWIDTH = 128\n\ninput_shape=(HEIGHT, WIDTH, 3)\n\nFC_LAYERS = [1024]\ndropout = 0.25\nepochs = 80\nswa = SWA('./keras_swa.model',epochs-3)\n\noof_preds = np.zeros((len(train), 6))\n\nfor train_indices,val_indices in skf.split(train.image.values,train.category.values):\n    \n    print('*'*50)\n    print('Fold',fold)\n    fold += 1\n    \n    # Datasets\n    partition_train = train.loc[train_indices].image.values\n    labels_train = train.loc[train_indices].set_index('image')\n    \n    partition_valid = train.loc[val_indices].image.values\n    labels_valid = train.loc[val_indices].set_index('image')\n\n    # Generators\n    training_generator = DataGenerator(partition_train, labels_train,augmentations=AUGMENTATIONS_TRAIN, **params)\n    validation_generator = DataGenerator(partition_valid,labels_valid,augmentations=AUGMENTATIONS_TEST, **params)\n    \n    base_model = EfficientNetB0(weights='imagenet',\n                                include_top=False,\n                                input_shape=(HEIGHT, WIDTH, 3))\n\n    finetune_model = build_finetune_model(base_model, \n                                          dropout=dropout, \n                                          fc_layers=FC_LAYERS, \n                                          num_classes=5)\n    \n    finetune_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n    \n    snapshot = SnapshotCallbackBuilder(nb_epochs=epochs,nb_snapshots=1,init_lr=1e-3)\n\n\n    history = finetune_model.fit_generator(generator=training_generator,\n                                            validation_data=validation_generator,\n                                            use_multiprocessing=True,\n                                            workers=8,epochs=epochs,verbose=0,callbacks=snapshot.get_callbacks())\n    \n    finetune_model.load_weights('./keras_swa.model')\n    \n    try:\n        plot_loss_acc(history)\n    except:\n        print('no plot')\n    \n    test_generator = TestDataGenerator(test.image.values,augmentations=AUGMENTATIONS_TEST, **params)\n    test_generator_flipped = TestDataGenerator(test.image.values,augmentations=AUGMENTATIONS_TEST, **params,\n                                               flip=True)\n    \n    validation_generator = TestDataGenerator(partition_valid,augmentations=AUGMENTATIONS_TEST, **params,\n                                            path='../input/game-of-deep-learning-ship-datasets/train/images/')\n    validation_generator_flipped = TestDataGenerator(partition_valid,augmentations=AUGMENTATIONS_TEST,\n                                             **params,flip=True,\n                                                path='../input/game-of-deep-learning-ship-datasets/train/images/')\n    preds1 = []\n    for im in validation_generator:    \n        preds1.extend(finetune_model.predict(im))\n        \n    preds2 = []\n    for im in validation_generator_flipped:    \n        preds2.extend(finetune_model.predict(im))\n        \n    preds1 = np.array(preds1)\n    preds2 = np.array(preds2)\n    \n    preds = (preds1 + preds2)/2\n    \n    oof_preds[val_indices, :5] = preds\n    oof_preds[val_indices, 5] = train.loc[val_indices,'image'].map(lambda x: x[:-4])\n    \n    preds1 = []\n    for im in test_generator:    \n        preds1.extend(finetune_model.predict(im))\n        \n    preds2 = []\n    for im in test_generator_flipped:    \n        preds2.extend(finetune_model.predict(im))\n        \n    preds1 = np.array(preds1)\n    preds2 = np.array(preds2)\n    \n    preds = (preds1 + preds2)/2\n        \n    all_preds.append(preds)","execution_count":23,"outputs":[{"output_type":"stream","text":"**************************************************\nFold 1\nWARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nColocations handled automatically by placer.\nWARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/function.py:1007: calling Graph.create_op (from tensorflow.python.framework.ops) with compute_shapes is deprecated and will be removed in a future version.\nInstructions for updating:\nShapes are always computed; don't use the compute_shapes as it has no effect.\nWARNING:tensorflow:From /kaggle/working/efficientnet/efficientnet/layers.py:29: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nDeprecated in favor of operator or tf.math.divide.\nDownloading data from https://github.com/qubvel/efficientnet/releases/download/v0.0.1/efficientnet-b0_imagenet_1000_notop.h5\n16719872/16717576 [==============================] - 0s 0us/step\nWARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n**************************************************\nFold 2\n**************************************************\nFold 3\n**************************************************\nFold 4\n**************************************************\nFold 5\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(oof_preds).to_csv('oof_preds.csv',index=False)","execution_count":24,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(oof_preds).head()","execution_count":25,"outputs":[{"output_type":"execute_result","execution_count":25,"data":{"text/plain":"          0         1         2         3         4          5\n0  0.167743  0.194756  0.158509  0.347778  0.131214  2823080.0\n1  0.183753  0.119034  0.147734  0.399794  0.149684  2870024.0\n2  0.106863  0.194998  0.172080  0.301459  0.224600  2662125.0\n3  0.263663  0.052659  0.067346  0.455981  0.160351  2900420.0\n4  0.190428  0.097942  0.131463  0.363798  0.216370  2804883.0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.167743</td>\n      <td>0.194756</td>\n      <td>0.158509</td>\n      <td>0.347778</td>\n      <td>0.131214</td>\n      <td>2823080.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.183753</td>\n      <td>0.119034</td>\n      <td>0.147734</td>\n      <td>0.399794</td>\n      <td>0.149684</td>\n      <td>2870024.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.106863</td>\n      <td>0.194998</td>\n      <td>0.172080</td>\n      <td>0.301459</td>\n      <td>0.224600</td>\n      <td>2662125.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.263663</td>\n      <td>0.052659</td>\n      <td>0.067346</td>\n      <td>0.455981</td>\n      <td>0.160351</td>\n      <td>2900420.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.190428</td>\n      <td>0.097942</td>\n      <td>0.131463</td>\n      <td>0.363798</td>\n      <td>0.216370</td>\n      <td>2804883.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = np.ones((2680,5))\nfor preds_fold in all_preds:\n    preds = preds*preds_fold\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = preds**(1/skf.n_splits)\ntest = pd.read_csv('../input/game-of-deep-learning-ship-datasets/test_ApKoW4T.csv')\nlabelled_preds = np.argmax(preds,1)+1\n# fnames = [f.name for f in learn.data.test_ds.items]\nfnames = test.image.values\ndf = pd.DataFrame({'image':fnames, 'category':labelled_preds}, columns=['image', 'category'])\ndf.to_csv('submission_gm.csv', index=False)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['class1_prob'] = preds[:,0]\ndf['class2_prob'] = preds[:,1]\ndf['class3_prob'] = preds[:,2]\ndf['class4_prob'] = preds[:,3]\ndf['class5_prob'] = preds[:,4]\ndf.to_csv('raw_prob.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!rm -rf efficientnet/","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}