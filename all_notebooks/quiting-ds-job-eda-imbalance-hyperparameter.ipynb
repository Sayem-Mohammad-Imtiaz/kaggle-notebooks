{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Introduction","metadata":{}},{"cell_type":"markdown","source":"Problem Statement:\n    \nHere our objective is to build a model which will predict wheather a person is trying to change his/her job or not? ","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport missingno as msno\n%matplotlib inline\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve, classification_report, precision_recall_curve\nfrom sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV, RandomizedSearchCV\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('../input/hr-analytics-job-change-of-data-scientists/aug_train.csv')\ntest = pd.read_csv('../input/hr-analytics-job-change-of-data-scientists/aug_test.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Before Dig into the main analysing process process Let's get familliar with the datasets.\n","metadata":{}},{"cell_type":"markdown","source":"#Meaning of each individual features.\nenrollee_id : Unique ID for candidate\n\ncity: City code\n\ncity_ development _index : Developement index of the city (scaled)\n\ngender: Gender of candidate\n\nrelevent_experience: Relevant experience of candidate\n\nenrolled_university: Type of University course enrolled if any\n\neducation_level: Education level of candidate\n\nmajor_discipline :Education major discipline of candidate\n\nexperience: Candidate total experience in years\n\ncompany_size: No of employees in current employer's company\n\ncompany_type : Type of current employer\n\nlastnewjob: Difference in years between previous job and current job\n\ntraining_hours: training hours completed\n\ntarget: 0 – Not looking for job change, 1 – Looking for a job change\n","metadata":{}},{"cell_type":"code","source":"train.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here one interesting things to notice is all the Numerical features are not contains Null values.","metadata":{}},{"cell_type":"code","source":"#Let's have a look of missing value\npercent_missing = train.isnull().sum() * 100 / len(train)\nmissing_value_df = pd.DataFrame({#'column_name': train.columns,\n                                 'percent_missing': percent_missing})\npercent_missing","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"msno.heatmap(train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we are seeing that most of the features that contains missing values are categorical \nand we can fill those value by mode , but this could leads to imbalanced data and make our ml model baised\n so it's better to delete those.","metadata":{}},{"cell_type":"code","source":"train.dropna(inplace=True)\ntest.dropna(inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.isna().mean()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I don't think enrolle_id, city, company_size will be handy to take in our process.So drop this three columns. ","metadata":{}},{"cell_type":"code","source":"train.drop(['enrollee_id','city','company_size'], axis = 1, inplace = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['experience'].unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we are seeing that there are > and < sign with 20 and 1. So before Modeling let's solve this by adding and Substracting 1  with 20 and 1 ","metadata":{}},{"cell_type":"code","source":"def replace(experience):\n    if experience == '>20':\n        return 21\n    elif experience == '<1':\n        return 0\n\n    else:\n        return experience","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.experience = train.experience.map(replace)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['experience'].unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def replace(last_new_job):\n    if last_new_job == '>4':\n        return 5\n    elif last_new_job == 'never':\n        return 0\n\n    else:\n        return last_new_job\n\ntrain.last_new_job = train.last_new_job.map(replace)\ntrain['last_new_job'].unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Now let's have look at my data and hope it's clean now\ntrain.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#First Check weather our dataset is balanced or not?\nvalues = train['target'].value_counts().values.tolist()\nlabels = train['target'].value_counts().index\nplt.figure(figsize= (10,10))\nplt.title('Comparing labels of target feature')\nplt.pie(x = values, labels = labels, autopct='%1.1f%%', pctdistance= .5)\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we are seeing that our dataset is imbalanced we have to fix this before modeling.  ","metadata":{}},{"cell_type":"code","source":"\nfig, ax = plt.subplots(3,2, figsize = (12,12))\n((ax1, ax2), (ax3, ax4), (ax5, ax6)) = ax\n\nlabels = train['gender'].value_counts().index\nvalues = train['gender'].value_counts().tolist()\nax1.pie(x=values, labels=labels, autopct=\"%1.2f%%\", shadow=True)\nax1.set_title(\"Gender Distribution Pie Chart\", fontdict={'fontsize': 14})\n\nlabels = train['relevent_experience'].value_counts().index\nvalues = train['relevent_experience'].value_counts().tolist()\nax2.pie(x=values, labels=labels, autopct=\"%1.2f%%\", shadow=True, explode=[0, 0.2])\nax2.set_title(\"Experience Distribution Pie Chart\", fontdict={'fontsize': 14})\n\nlabels = train['enrolled_university'].value_counts().index\nvalues = train['enrolled_university'].value_counts().tolist()\nax3.pie(x=values, labels=labels, autopct=\"%1.2f%%\", shadow=True, explode=[0, 0.2,.3])\nax3.set_title(\"Enrooled University Distribution Pie Chart\", fontdict={'fontsize': 14})\n\nlabels = train['education_level'].value_counts().index\nvalues = train['education_level'].value_counts().tolist()\nax4.pie(x=values, labels=labels, autopct=\"%1.2f%%\", shadow=True, explode=[0, 0.05,.1])\nax4.set_title(\"Education label Distribution Pie Chart\", fontdict={'fontsize': 14})\n\nlabels = train['major_discipline'].value_counts().index\nvalues = train['major_discipline'].value_counts().tolist()\nax5.pie(x=values, labels=labels, autopct=\"%1.2f%%\", shadow=True, explode=[0.1, 0.1, 0.1, 0.1, 0.2, 0.1])\nax5.set_title(\"Major_discipline Distribution Pie Chart\", fontdict={'fontsize': 14})\n\n\n\nlabels = train['company_type'].value_counts().index\nvalues = train['company_type'].value_counts().tolist()\nax6.pie(x=values, labels=labels, autopct=\"%1.2f%%\", shadow=True, explode=[0, 0.1,.1,.1, .15,.1])\nax6.set_title(\"Company Type Pie Chart\", fontdict={'fontsize': 14})\n\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's Make Some obsevations from the visualizations.\n\n1. Almost 90% people are male who were takong that course.\n\n2. Almost 87% candidate has realvent experience in Data Science and rest of them don't have any\nexperience but interasted in this field.\n\n3. Almost 85% candidate were enrolled in University.\n\n4. ALmost 70% guy were graduate and interesting is around 3% phd guy also there.\n\n5. It's natural that most of the candidate will be form STEM background. But From rest of the groups\ncandidates from humanities are interested more.\n\n6. And most of the guys are from private sector.","metadata":{}},{"cell_type":"markdown","source":"Let's see how all the categorical features effecting in target variable.","metadata":{}},{"cell_type":"code","source":"fig_dims = (20, 14)\nfig, ax =plt.subplots(3,2,figsize = fig_dims)\nsns.countplot(x = train['gender'],hue = train['target'], ax=ax[0,0], edgecolor=sns.color_palette(\"dark\", 60))\nsns.countplot(train['education_level'],hue = train['target'], ax=ax[0,1])\nsns.countplot(x = train['relevent_experience'],hue = train['target'], ax=ax[1,0])\nsns.countplot(train['enrolled_university'],hue = train['target'], ax=ax[1,1])\nsns.countplot(x = train['major_discipline'],hue = train['target'], ax=ax[2,0])\nsns.countplot(x = train['company_type'],hue = train['target'], ax=ax[2,1])\n\n\nfig.suptitle('Features distribution based on target ',fontsize=40)\nfig.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's make some observations.\n\nHere we are seeing that in each Chart any one class having majority of the data points.That's why we cann't\ncompare confidently. So here ratio could be a good factor to compare.\n\n1. Though something to see is even people from public sector are also getting interest in Data Science.\n\n2. Peopler are from Arts background are completly not interested in switching job.\n","metadata":{}},{"cell_type":"markdown","source":"Let's try to explore Numerical coulumns.And have their distribution with respect to target columns.\n","metadata":{}},{"cell_type":"code","source":"g = sns.kdeplot(train['city_development_index'][(train[\"target\"] == 0) & (train['city_development_index'].notnull())], color=\"Red\", shade = True)\ng = sns.kdeplot(train['city_development_index'][(train[\"target\"] == 1) & (train['city_development_index'].notnull())], ax =g, color=\"Blue\", shade= True)\ng.set_xlabel('city_development_index')\ng.set_ylabel(\"Frequency\")\ng = g.legend([\"Not looking for job change,\",\"looking for job change,\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Make Obsebations.\n\n1. Peoples city_development_index ranging in around .666 and and around .9 are interested in changing.\n\n2. Peoples are toatally not interested in job change whose city_development_index are nearly .9","metadata":{}},{"cell_type":"code","source":"## new City_devlopment_cat feature based on this obserbations.\nbins = [0,.45,.67,.84,1]\nlabels=[0,1,2,3]\ntrain['City_devlopment_cat'] = pd.cut(train['city_development_index'], bins=bins, labels=labels)\ntrain[['City_devlopment_cat', 'target']].groupby(['City_devlopment_cat'], as_index=False).mean().sort_values(by='target', ascending=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['experience'] = train['experience'].astype(int)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"g = sns.kdeplot(train['experience'][(train[\"target\"] == 0) & (train['experience'].notnull())], color=\"Red\", shade = True)\ng = sns.kdeplot(train['experience'][(train[\"target\"] == 1) & (train['experience'].notnull())], ax =g, color=\"Blue\", shade= True)\ng.set_xlabel('experience')\ng.set_ylabel(\"Frequency\")\ng = g.legend([\"Not looking for job change,\",\"looking for job change,\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Make Obserbations.\n\n1. People ranging  experience from 1 to 10 years are most likely to change.\n2. People  having experience of around 20 years are not looking to change the job.","metadata":{}},{"cell_type":"code","source":"train['last_new_job'] = train['last_new_job'].astype(int)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"g = sns.kdeplot(train['last_new_job'][(train[\"target\"] == 0) & (train['last_new_job'].notnull())], color=\"Red\", shade = True)\ng = sns.kdeplot(train['last_new_job'][(train[\"target\"] == 1) & (train['last_new_job'].notnull())], ax =g, color=\"Blue\", shade= True)\ng.set_xlabel('last_new_job')\ng.set_ylabel(\"Frequency\")\ng = g.legend([\"Not looking for job change,\",\"looking for job change,\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"make obserbations\n\n1.People left their last job from 1 to 2 years ago are most likely to change job\n and also same for who are not wanted to change job.","metadata":{}},{"cell_type":"code","source":"g = sns.kdeplot(train['training_hours'][(train[\"target\"] == 0) & (train['training_hours'].notnull())], color=\"Red\", shade = True)\ng = sns.kdeplot(train['training_hours'][(train[\"target\"] == 1) & (train['training_hours'].notnull())], ax =g, color=\"Green\", shade= True)\ng.set_xlabel('training_hours')\ng.set_ylabel(\"Frequency\")\ng = g.legend([\"Not looking for job change,\",\"looking for job change,\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Nothing to say about this graph.","metadata":{}},{"cell_type":"code","source":"sns.heatmap(train.corr(), annot = True, vmin=-1, vmax=1, center= 0,\n            cmap= 'Blues_r', linewidths=3, linecolor='black')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['target'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Let's use pivot table to analyze it.\ntable = pd.pivot_table(train,index=['gender'])\ntable\ntable.plot(kind='bar')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here one interesting point to notice is Female have a higher training hours then others.","metadata":{}},{"cell_type":"code","source":"table = pd.pivot_table(train,index=['gender','target'])\ntable\ntable.plot(kind='line')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Obervations:\n    \n1. Who are wanted to change the job their training hour is less then \nwho don't want in all three catrgory.\n\n2. Who are wanted to change the job have less experience then is  then \nwho don't want in all three catrgory.\n ","metadata":{}},{"cell_type":"code","source":"table = pd.pivot_table(train,index=['gender','target','education_level'])\nprint(table)\ntable.plot(kind='bar',\n           figsize = (15,8),\n           colormap = 'RdGy')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Obsevations\n\n1. Female who want to change job have a much more higher training\nhours then others and Education_level is PHD and much more experience also.\n\n2. And in Others ccategory only graduate are wanted to change job.","metadata":{}},{"cell_type":"code","source":"table = pd.pivot_table(train,\n                       index=['gender','education_level'],\n                       aggfunc={'target':np.sum}\n                      )\nprint(table)\ntable.plot(kind='barh',\n           figsize = (15,8),\n           colormap = 'tab10_r',\n           title = 'Gender and Educatiuon lebel Relationship')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"observations:\n\n1. Female graduate candidates are more in numbers in changing job.\n\n2. Same case for male also as graduate completed students have a intent for a job.","metadata":{}},{"cell_type":"code","source":"table = pd.pivot_table(train,\n                       index=['gender','enrolled_university','education_level'],\n                       aggfunc={'target':np.sum}\n                      )\nprint(table)\ntable.plot(kind='bar',\n           figsize = (15,8),\n           colormap = 'tab10_r',\n           title = 'Gender and Educatiuon lebel and Enrolled University Relationship')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Observations:\n    \n1. male Graduate candidate having no_enrollment are much more\ninterested in job then part time and Full time. Same cases for Female also.\n\n2. Tree structure based algorithm could be handy for this particuler case.","metadata":{}},{"cell_type":"code","source":"table = pd.pivot_table(train,\n                       index=['company_type','last_new_job'],\n                       aggfunc={'target':np.sum}\n                      )\nprint(table)\ntable.plot(kind='bar',\n           figsize = (15,8),\n           colormap = 'tab10_r',\n           title = 'Company_type and last_new_job Relationship')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Observations\n\n1. Working in various types of company but having 1 years experienced candudate are mostly interested \nin changing job.\n\n2. And Candidate Working in NGO and Public Sector having experience of 5 years also partly interested in changing job.","metadata":{}},{"cell_type":"code","source":"table = pd.pivot_table(train,\n                       index=['last_new_job'],\n                       columns = ['enrolled_university'],\n                       aggfunc={'target':np.sum}\n                      )\nprint(table)\ntable.plot(kind='bar',\n           figsize = (15,8),\n           colormap = 'Set1_r',\n           title = 'Enrolled and last_new_job Relationship')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Observations\n1. Candidate who are enrolled in a part time courese are less interested in changing job.\n\n2. Candidate emrolled in a university and having 1 years experience are mostly interesrted in changing job.","metadata":{}},{"cell_type":"code","source":"table = pd.pivot_table(train,\n                       index=['major_discipline','gender'],\n                       columns = ['enrolled_university'],\n                       values = ['target'],\n                       aggfunc=np.sum\n                       \n                      )\nprint(table)\ntable.plot(kind='bar',\n           figsize = (15,8),\n           colormap = 'Set2_r',\n           title = 'Enrolled and major discipline Relationship')          ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Encoding The columns","metadata":{"trusted":true}},{"cell_type":"code","source":"df = train.copy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.get_dummies(df,\n                    columns=['gender', 'relevent_experience', 'enrolled_university', 'major_discipline', 'company_type'],\n                    drop_first=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As education_level is an ordinal categorical features that's why we have to map the each lavel with keeping order in mind.\n","metadata":{}},{"cell_type":"code","source":"df['education_level'] = df['education_level'].map( {'Graduate': 0, 'Masters': 1,'Phd': 2} ).astype(int)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = df.drop(['target'], axis=1)\ny = df['target']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.20, random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nScalerX = StandardScaler()\nX_train = ScalerX.fit_transform(X_train)\nX_test = ScalerX.transform(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Handling imbalance data using SMOTE based techniques","metadata":{}},{"cell_type":"markdown","source":"### A) SMOTE Technique¶","metadata":{}},{"cell_type":"code","source":"from imblearn.over_sampling import SMOTE\nfrom collections import Counter\n\ncounter = Counter(y_train)\nprint('Before',counter)\n# oversampling the train dataset using SMOTE\nsmt = SMOTE()\n#X_train, y_train = smt.fit_resample(X_train, y_train)\nX_train_sm, y_train_sm = smt.fit_resample(X_train, y_train)\n\ncounter = Counter(y_train_sm)\nprint('After',counter)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### B) ADASYN Technique","metadata":{}},{"cell_type":"code","source":"from imblearn.over_sampling import ADASYN\n\ncounter = Counter(y_train)\nprint('Before',counter)\n# oversampling the train dataset using ADASYN\nada = ADASYN(random_state=130)\nX_train_ada, y_train_ada = ada.fit_resample(X_train, y_train)\n\ncounter = Counter(y_train_ada)\nprint('After',counter)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### C) Hybrid Techniques\n","metadata":{}},{"cell_type":"markdown","source":"#### C.1) SMOTE + Tomek Links","metadata":{}},{"cell_type":"code","source":"from imblearn.combine import SMOTETomek\n\ncounter = Counter(y_train)\nprint('Before',counter)\n# oversampling the train dataset using SMOTE + Tomek\nsmtom = SMOTETomek(random_state=139)\nX_train_smtom, y_train_smtom = smtom.fit_resample(X_train, y_train)\n\ncounter = Counter(y_train_smtom)\nprint('After',counter)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### C.2) SMOTE + ENN\n","metadata":{}},{"cell_type":"code","source":"from imblearn.combine import SMOTEENN\n\ncounter = Counter(y_train)\nprint('Before',counter)\n#oversampling the train dataset using SMOTE + ENN\nsmenn = SMOTEENN()\nX_train_smenn, y_train_smenn = smenn.fit_resample(X_train, y_train)\n\ncounter = Counter(y_train_smenn)\nprint('After',counter)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### C.3) SMOTE + Under Sampling ","metadata":{}},{"cell_type":"code","source":"from imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.pipeline import Pipeline\nsm = SMOTE(sampling_strategy = .3)\nrus =  RandomUnderSampler(sampling_strategy=.4)\n\npipeline = Pipeline(steps = [('smote', sm),('under',rus)])\n\ncounter = Counter(y_train)\nprint('Before',counter)\n#over and undersampling the train dataset using SMOTE + RandomUnderSampler\nX_train_smrus, y_train_smrus = pipeline.fit_resample(X_train, y_train)\n\ncounter = Counter(y_train_smrus)\nprint('After',counter)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Building","metadata":{}},{"cell_type":"code","source":"model = list()\nresample = list()\nprecision = list()\nrecall = list()\nF1score = list()\nAUCROC = list()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def test_eval(clf_model, X_test, y_test, algo=None, sampling=None):\n    # Test set prediction\n    y_prob=clf_model.predict_proba(X_test)\n    y_pred=clf_model.predict(X_test)\n\n    print('Confusion Matrix')\n    print('='*60)\n    plot_confusion_matrix(clf_model, X_test, y_test)  \n    plt.show() \n    #print(confusion_matrix(y_test,y_pred),\"\\n\")\n    print('Classification Report')\n    print('='*60)\n    print(classification_report(y_test,y_pred),\"\\n\")\n    print('AUC-ROC')\n    print('='*60)\n    print(roc_auc_score(y_test, y_prob[:,1]))\n          \n    model.append(algo)\n    precision.append(precision_score(y_test,y_pred))\n    recall.append(recall_score(y_test,y_pred))\n    F1score.append(f1_score(y_test,y_pred))\n    AUCROC.append(roc_auc_score(y_test, y_prob[:,1]))\n    resample.append(sampling)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model-1: Logistic Regression","metadata":{}},{"cell_type":"markdown","source":"### 1. Original Unsampled Data","metadata":{}},{"cell_type":"code","source":"log_model=LogisticRegression()\n\nparams={'C':np.logspace( -10, 1, 15),'class_weight':[None,'balanced'],'penalty':['l1','l2']}\n\ncv = StratifiedKFold(n_splits=5, random_state=100, shuffle=True)\n\n# Create grid search using 5-fold cross validation\nclf_LR = GridSearchCV(log_model, params, cv=cv, scoring='roc_auc', n_jobs=-1)\nclf_LR.fit(X_train, y_train)\nclf_LR.best_estimator_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_eval(clf_LR, X_test, y_test, 'Logistic Regression', 'actual')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.SMOTE Resampling","metadata":{}},{"cell_type":"code","source":"clf_LR.fit(X_train_sm, y_train_sm)\nclf_LR.best_estimator_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_eval(clf_LR, X_test, y_test, 'Logistic Regression', 'smote')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.ADASYN Resampling","metadata":{}},{"cell_type":"code","source":"clf_LR.fit(X_train_ada, y_train_ada)\nclf_LR.best_estimator_\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_eval(clf_LR, X_test, y_test, 'Logistic Regression', 'adasyn')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.SMOTE + Tomek Resampling","metadata":{}},{"cell_type":"code","source":"clf_LR.fit(X_train_smtom, y_train_smtom)\nclf_LR.best_estimator_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_eval(clf_LR, X_test, y_test, 'Logistic Regression', 'smote+tomek')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 5.SMOTE + ENN Resampling","metadata":{}},{"cell_type":"code","source":"clf_LR.fit(X_train_smenn, y_train_smenn)\nclf_LR.best_estimator_\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ntest_eval(clf_LR, X_test, y_test, 'Logistic Regression', 'smote+enn')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 6. SMOTE + Under Sampling","metadata":{}},{"cell_type":"code","source":"clf_LR.fit(X_train_smrus, y_train_smrus)\nclf_LR.best_estimator_\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_eval(clf_LR, X_test, y_test, 'Logistic Regression', 'smote+rus')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model-2: Decision Tree","metadata":{}},{"cell_type":"code","source":"estimators = [2,10,30,50,100]\n# Maximum number of depth in each tree:\nmax_depth = [i for i in range(5,16,2)]\n# Minimum number of samples to consider to split a node:\nmin_samples_split = [2, 5, 10, 15, 20, 50, 100]\n# Minimum number of samples to consider at each leaf node:\nmin_samples_leaf = [1, 2, 5]\n#Impurity\ncriterion = ['gini', 'entropy']\n#The number of features to consider when looking for the best split\nmax_features = ['log2', 'sqrt', 'auto']\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1. Original Unsampled Data****","metadata":{}},{"cell_type":"code","source":"tree_model = DecisionTreeClassifier()\ntree_param_grid = { \n    'max_features':max_features,\n    'criterion':criterion,\n    'max_depth': max_depth,\n    'min_samples_split': min_samples_split,\n    'min_samples_leaf': min_samples_leaf\n}\n\nclf_DT = RandomizedSearchCV(tree_model, tree_param_grid, cv=cv, scoring='roc_auc', n_jobs=-1, verbose=2)\nclf_DT.fit(X_train, y_train)\nclf_DT.best_estimator_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_eval(clf_DT, X_test, y_test, 'Decision Tree', 'actual')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.SMOTE Resampling","metadata":{}},{"cell_type":"code","source":"clf_DT.fit(X_train_sm, y_train_sm)\nclf_DT.best_estimator_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_eval(clf_DT, X_test, y_test, 'Decision Tree', 'smote')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.ADASYN Resampling","metadata":{}},{"cell_type":"code","source":"clf_DT.fit(X_train_ada, y_train_ada)\nclf_DT.best_estimator_\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_eval(clf_DT, X_test, y_test, 'Decision Tree', 'adasyn')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4. SMOTE + Tomek Resampling","metadata":{}},{"cell_type":"code","source":"clf_DT.fit(X_train_smtom, y_train_smtom)\nclf_DT.best_estimator_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_eval(clf_DT, X_test, y_test, 'Decision Tree', 'smote+tomek')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 5.SMOTE + ENN Resampling","metadata":{}},{"cell_type":"code","source":"clf_DT.fit(X_train_smenn, y_train_smenn)\nclf_DT.best_estimator_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_eval(clf_DT, X_test, y_test, 'Decision Tree', 'smote+enn')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 6. SMOTE + Under Sampling","metadata":{}},{"cell_type":"code","source":"clf_DT.fit(X_train_smrus, y_train_smrus)\nclf_DT.best_estimator_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_eval(clf_LR, X_test, y_test, 'Decision Tree', 'smote+rus')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model-3: Random Forest","metadata":{}},{"cell_type":"markdown","source":"### 1. Original Unsampled Data","metadata":{}},{"cell_type":"code","source":"rf_model = RandomForestClassifier()\n\nrf_params={'n_estimators':estimators,\n           'max_features':max_features,\n           'criterion':criterion,\n           'max_depth': max_depth,\n            'min_samples_split': min_samples_split,\n            'min_samples_leaf': min_samples_leaf}\n\nclf_RF = RandomizedSearchCV(rf_model, rf_params, cv=cv, scoring='roc_auc', n_jobs=-1, n_iter=20, verbose=2)\nclf_RF.fit(X_train, y_train)\nclf_RF.best_estimator_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_eval(clf_RF, X_test, y_test, 'Random Forest', 'actual')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.SMOTE Resampling","metadata":{}},{"cell_type":"code","source":"clf_RF.fit(X_train_sm, y_train_sm)\nclf_RF.best_estimator_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_eval(clf_RF, X_test, y_test, 'Random Forest', 'smote')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.ADASYN Resampling","metadata":{}},{"cell_type":"code","source":"clf_RF.fit(X_train_ada, y_train_ada)\nclf_RF.best_estimator_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_eval(clf_RF, X_test, y_test, 'Random Forest', 'adasyn')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4. SMOTE + Tomek Resampling","metadata":{}},{"cell_type":"code","source":"clf_RF.fit(X_train_smtom, y_train_smtom)\nclf_RF.best_estimator_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_eval(clf_RF, X_test, y_test, 'Random Forest', 'smote+tomek')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 5. SMOTE + ENN Resampling","metadata":{}},{"cell_type":"code","source":"clf_RF.fit(X_train_smenn, y_train_smenn)\nclf_RF.best_estimator_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_eval(clf_RF, X_test, y_test, 'Random Forest', 'smote+enn')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 6. SMOTE + Under Sampling","metadata":{}},{"cell_type":"code","source":"clf_RF.fit(X_train_smrus, y_train_smrus)\nclf_RF.best_estimator_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_eval(clf_LR, X_test, y_test, 'Random Forest', 'smote+rus')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model-4: AdaBoast","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import AdaBoostClassifier\nclf_ada=AdaBoostClassifier()\nclf_ada.fit(X_train, y_train)\n#Actual data\ntest_eval(clf_ada, X_test, y_test, 'AdaBoast', 'actual')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#smote\nclf_ada.fit(X_train_sm, y_train_sm)\ntest_eval(clf_ada, X_test, y_test, 'AdaBoast', 'Smote')\n\n#Adasyn\nclf_ada.fit(X_train_ada, y_train_ada)\ntest_eval(clf_ada, X_test, y_test, 'AdaBoast', 'adasyn')\n\n#smote + tomek\nclf_ada.fit(X_train_smtom, y_train_smtom)\ntest_eval(clf_ada, X_test, y_test, 'AdaBoast', 'smote+tomek')\n\n#smote + enn\nclf_ada.fit(X_train_smenn, y_train_smenn)\ntest_eval(clf_ada, X_test, y_test, 'AdaBoast', 'smote+enn')\n\n#smote + \nclf_ada.fit(X_train_smrus, y_train_smrus)\ntest_eval(clf_ada, X_test, y_test, 'AdaBoast', 'smote+rus')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n# Model-5: GradientBoast","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier\n\ngb_model = GradientBoostingClassifier()\n\ngb_params = { \n    \"n_estimators\":[1,3,5,10,15,20,30,40,50,],\n    'learning_rate': [0.1, 0.05, 0.02, 0.01],\n    'max_depth': max_depth,\n    'min_samples_split': min_samples_split,\n    'min_samples_leaf': min_samples_leaf\n}\n\nclf_gb=RandomizedSearchCV(gb_model,gb_params,cv=cv, scoring='roc_auc',n_jobs=1)\n\nclf_gb.fit(X_train, y_train)\nclf_gb.best_estimator_\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Actual data\n#clf_gb.fit(X_train, y_train)\n#clf_gb.best_estimator_\ntest_eval(clf_gb, X_test, y_test, 'GradientBoast', 'actual')\n#smote\nclf_gb.fit(X_train_sm, y_train_sm)\nclf_gb.best_estimator_\ntest_eval(clf_gb, X_test, y_test, 'GradientBoast', 'Smote')\n\n#Adasyn\nclf_gb.fit(X_train_ada, y_train_ada)\nclf_gb.best_estimator_\ntest_eval(clf_gb, X_test, y_test, 'GradientBoast', 'adasyn')\n\n#smote + tomek\nclf_gb.fit(X_train_smtom, y_train_smtom)\nclf_gb.best_estimator_\ntest_eval(clf_gb, X_test, y_test, 'GradientBoast', 'smote+tomek')\n\n#smote + enn\nclf_gb.fit(X_train_smenn, y_train_smenn)\nclf_gb.best_estimator_\ntest_eval(clf_gb, X_test, y_test, 'GradientBoast', 'smote+enn')\n\n#smote + rus\nclf_gb.fit(X_train_smrus, y_train_smrus)\nclf_gb.best_estimator_\ntest_eval(clf_gb, X_test, y_test, 'GradientBoast', 'smote+rus')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model-6: SGDClassifier","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import SGDClassifier\nsgd_params = {\n    \"loss\" : [\"hinge\", \"log\", \"squared_hinge\", \"modified_huber\"],\n    \"alpha\" : [0.0001, 0.001, 0.01, 0.1],\n    \"penalty\" : [\"l2\", \"l1\", \"none\"],\n}\n\nsgd_model = SGDClassifier()\nclf_sgd=RandomizedSearchCV(sgd_model,sgd_params,cv=cv, scoring='roc_auc',n_jobs=1)\n\nclf_sgd.fit(X_train, y_train)\nclf_sgd.best_estimator_\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Actual data\n#clf_gb.fit(X_train, y_train)\n#clf_gb.best_estimator_\ntest_eval(clf_sgd, X_test, y_test, 'SGDClassifier', 'actual')\n#smote\nclf_sgd.fit(X_train_sm, y_train_sm)\nclf_sgd.best_estimator_\ntest_eval(clf_sgd, X_test, y_test, 'SGDClassifier', 'Smote')\n\n#Adasyn\nclf_sgd.fit(X_train_ada, y_train_ada)\nclf_sgd.best_estimator_\ntest_eval(clf_sgd, X_test, y_test, 'SGDClassifier', 'adasyn')\n\n#smote + tomek\nclf_sgd.fit(X_train_smtom, y_train_smtom)\nclf_sgd.best_estimator_\ntest_eval(clf_sgd, X_test, y_test, 'SGDClassifier', 'smote+tomek')\n\n#smote + enn\nclf_sgd.fit(X_train_smenn, y_train_smenn)\nclf_sgd.best_estimator_\ntest_eval(clf_sgd, X_test, y_test, 'SGDClassifier', 'smote+enn')\n\n#smote + rus\nclf_sgd.fit(X_train_smrus, y_train_smrus)\nclf_sgd.best_estimator_\ntest_eval(clf_sgd, X_test, y_test, 'SGDClassifier', 'smote+rus')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model-7: LGBMClassifier","metadata":{}},{"cell_type":"code","source":"import lightgbm as lgbm","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nfrom scipy.stats import randint as sp_randint\nfrom scipy.stats import uniform as sp_uniform\n\nlgb_model = lgbm.LGBMClassifier()\nlgb_params ={'num_leaves': sp_randint(6, 50), \n             'min_child_samples': sp_randint(100, 200), \n             'min_child_weight': [1e-5, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3, 1e4],\n             'subsample': sp_uniform(loc=0.2, scale=0.8), \n             'colsample_bytree': sp_uniform(loc=0.4, scale=0.6),\n             'reg_alpha': [0, 1e-1, 1, 2, 5, 7, 10, 50, 100],\n             'reg_lambda': [0, 1e-1, 1, 5, 10, 20, 50, 100],\n             'bagging_fraction': sp_uniform(0.5, 0.8),\n             #'bagging_frequency': sp_randint(5, 8),\n             'feature_fraction': sp_uniform(0.5, 0.8),\n             'max_depth': sp_randint(10, 13),\n             'min_data_in_leaf': sp_randint(50, 80),}\n#clf = lgb.LGBMClassifier(max_depth=-1, random_state=314, silent=True, metric='None', n_jobs=4, n_estimators=5000)\nclf_lgb=RandomizedSearchCV(lgb_model,lgb_params,cv=cv, scoring='roc_auc',n_jobs=1)\n\nclf_lgb.fit(X_train, y_train)\nclf_lgb.best_estimator_\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Actual data\n#clf_gb.fit(X_train, y_train)\n#clf_gb.best_estimator_\ntest_eval(clf_lgb, X_test, y_test, 'LGBMClassifier', 'actual')\n#smote\nclf_lgb.fit(X_train_sm, y_train_sm)\nclf_lgb.best_estimator_\ntest_eval(clf_lgb, X_test, y_test, 'LGBMClassifier', 'Smote')\n\n#Adasyn\nclf_lgb.fit(X_train_ada, y_train_ada)\nclf_lgb.best_estimator_\ntest_eval(clf_lgb, X_test, y_test, 'LGBMClassifier', 'adasyn')\n\n#smote + tomek\nclf_lgb.fit(X_train_smtom, y_train_smtom)\nclf_lgb.best_estimator_\ntest_eval(clf_lgb, X_test, y_test, 'LGBMClassifier', 'smote+tomek')\n\n#smote + enn\nclf_lgb.fit(X_train_smenn, y_train_smenn)\nclf_lgb.best_estimator_\ntest_eval(clf_lgb, X_test, y_test, 'LGBMClassifier', 'smote+enn')\n\n#smote + rus\nclf_lgb.fit(X_train_smrus, y_train_smrus)\nclf_lgb.best_estimator_\ntest_eval(clf_lgb, X_test, y_test, 'LGBMClassifier', 'smote+rus')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model-8: MLPClassifier","metadata":{}},{"cell_type":"code","source":"from sklearn.neural_network import MLPClassifier\nclf_mlp  = MLPClassifier()\nclf_mlp.fit(X_train, y_train)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Actual data\n#clf_gb.fit(X_train, y_train)\n#clf_gb.best_estimator_\ntest_eval(clf_mlp, X_test, y_test, 'MLPClassifier', 'actual')\n#smote\nclf_mlp.fit(X_train_sm, y_train_sm)\n#clf_mlp.best_estimator_\ntest_eval(clf_mlp, X_test, y_test, 'MLPClassifier', 'Smote')\n\n#Adasyn\nclf_mlp.fit(X_train_ada, y_train_ada)\n#clf_mlp.best_estimator_\ntest_eval(clf_mlp, X_test, y_test, 'MLPClassifier', 'adasyn')\n\n#smote + tomek\nclf_mlp.fit(X_train_smtom, y_train_smtom)\n#clf_mlp.best_estimator_\ntest_eval(clf_mlp, X_test, y_test, 'MLPClassifier', 'smote+tomek')\n\n#smote + enn\nclf_mlp.fit(X_train_smenn, y_train_smenn)\n#clf_mlp.best_estimator_\ntest_eval(clf_mlp, X_test, y_test, 'MLPClassifier', 'smote+enn')\n\n#smote + rus\nclf_mlp.fit(X_train_smrus, y_train_smrus)\n#clf_mlp.best_estimator_\ntest_eval(clf_mlp, X_test, y_test, 'MLPClassifier', 'smote+rus')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Comparision","metadata":{}},{"cell_type":"code","source":"clf_eval_df = pd.DataFrame({'model':model,\n                            'resample':resample,\n                            'precision':precision,\n                            'recall':recall,\n                            'f1-score':F1score,\n                            'AUC-ROC':AUCROC})\nclf_eval_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.set(font_scale=1.2)\n#sns.palplot(sns.color_palette())\ng = sns.FacetGrid(clf_eval_df, col=\"model\", height=5)\ng.map(sns.barplot, \"resample\", \"precision\", palette='twilight', order=[\"actual\", \"smote\", \"adasyn\", \"smote+tomek\", \"smote+enn\",\"smote+rus\"])\ng.set_xticklabels(rotation=30)\ng.set_xlabels(' ', fontsize=14)\ng.set_ylabels('Precision', fontsize=14)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}