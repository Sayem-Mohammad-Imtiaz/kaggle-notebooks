{"cells":[{"metadata":{},"cell_type":"markdown","source":"**Overview**\n\nFor my mini project, I dove into Twitter data through the Twitter API. Reminder that I mixed up the MP1 and A5 assignments, so this Mini Project is using the same data as A5. I hope that's ok! I haven't seen any feedback on A5 or MP1, so I just went with it.\n\nHere are the topics I examined:\n\n- How do trending Twitter topics in Seattle compare from one week to the next?\n- How much overlap is there between trending Twitter topics in Seattle and the topics on my personal timeline?\n\nThese topics are interesting to me because I follow Twitter for a lot of local news and perspectives. So it will be interesting to see how closely my personal feed follows local trends."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n\n        \nimport os\nfor dirname, _, filenames in os.walk('/kaggle/working'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Data Profile**\n\nI looked at this page for reference on how to access the Twitter API: \nhttp://socialmedia-class.org/twittertutorial.html\n\nTo use the Twitter API, you need to import the tweepy console (pip install tweepy). You also need 4 different keys/tokens to access the Twitter API. \n\nBelow I'm setting up these two items. I also imported tweepy into the console below. \n"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"\nimport json\nimport tweepy\n\n\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nCONSUMER_KEY = user_secrets.get_secret(\"TwitterKey\")\nCONSUMER_SECRET = user_secrets.get_secret(\"TwitterSecretKey\")\nACCESS_SECRET  = user_secrets.get_secret(\"TwitterSecretToken\")\nACCESS_TOKEN = user_secrets.get_secret(\"TwitterToken\")\n\n\nauth = tweepy.OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET)\nauth.set_access_token(ACCESS_TOKEN, ACCESS_SECRET)\n\napi = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True, compression=True)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Just to test if it worked, this should print the tweets on my home page and save them to a json:"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfor status in tweepy.Cursor(api.home_timeline).items(500):\n    print(json.dumps(status._json, indent=4))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"    \nwith open('homepage0225.txt', 'w') as outfile:\n    json.dump(status._json, outfile, indent=4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The below code prints a json of the trending topics for a location based on the WOEID. I found the WOEID for Seattle through some Google searching."},{"metadata":{"trusted":true},"cell_type":"code","source":"sea_trends = api.trends_place(id = 2490383)\nprint(json.dumps(sea_trends, indent=4))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"However, the above code doesn't drill down deep enough into my data. It basically created one big dictionary nested under \"trends.\" If I were to plot this, it wouldn't be very interesting data. What I want is to parse out the values one level down - under \"name,\" and \"tweet volume.\" So I need to tell the json dump to pull from the 0th item under \"trends.\""},{"metadata":{"trusted":true},"cell_type":"code","source":"#Now I'm just calling up each tweet trend individually, rather than having them all nested under the \"trends\" header.\n\nsea_trends = api.trends_place(id = 2490383)\nprint(json.dumps(sea_trends[0][\"trends\"], indent = 4))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next, I want to create a json file with the above data."},{"metadata":{"trusted":true},"cell_type":"code","source":"with open('sea_trends0225.txt', 'w') as outfile:\n    json.dump(sea_trends[0][\"trends\"], outfile, indent=4)\n    \n#The below code prints the file name for my new file, as well as a couple files I created earlier this week with trends\n#from 2/17 and 2/23. These dates are a bit arbitrary, but it's what I've got!\n    \nfor dirname, _, filenames in os.walk('/kaggle/working'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Analysis**\n\nTo do my analysis, I'm importing matplotlib and pandas and creating dataframes out of my json data. I made one dataframe for each date that I have data for."},{"metadata":{"trusted":true},"cell_type":"code","source":"\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n\ndf17 = pd.read_json(\"/kaggle/input/sea_trends0217.txt\")\ndf23 = pd.read_json(\"/kaggle/input/sea_trends0223.txt\")\ndf25 = pd.read_json(\"/kaggle/working/sea_trends0225.txt\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The below code takes out the NaN data from the sea_trends json files and plots them as bar charts. I only want to look at Twitter trends with actual values in the Tweet Volume field. However, this shows the data on three separate charts. I want to overlay them on top of each other to see which trending topics extended from last week into this week."},{"metadata":{"trusted":true},"cell_type":"code","source":"df17 = df17[df17['tweet_volume'].notna()]\ndf23 = df23[df23['tweet_volume'].notna()]\ndf17.plot(kind=\"bar\", x = \"name\", y = \"tweet_volume\", use_index=\"false\", color = \"red\",figsize=(20,10))\ndf23.plot(kind=\"bar\", x = \"name\", y = \"tweet_volume\", use_index=\"false\", color = \"blue\",figsize=(20,10))\ndf25.plot(kind=\"bar\", x = \"name\", y = \"tweet_volume\", use_index=\"false\", color = \"green\", figsize=(20,10))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I found some instruction in the Pandas documenation about how to combine different charts into one. So I did that below and added some pretty colors! I also updated the label names to be more descriptive and increased the font of the trending names."},{"metadata":{"trusted":true},"cell_type":"code","source":"df17 = df17[df17['tweet_volume'].notna()]\ndf23 = df23[df23['tweet_volume'].notna()]\n\nax = df25.plot(kind=\"bar\", x = \"name\", y = \"tweet_volume\", use_index=\"false\", color = \"mediumspringgreen\",\n               label = \"Number of Tweets 2/25\", fontsize= 22, figsize=(20,15))\n\nax2 = df23.plot(kind=\"bar\", x = \"name\", y = \"tweet_volume\", use_index=\"false\", color = \"seagreen\", figsize=(20,15), \n          label = \"Number of Tweets 2/23\", fontsize = 22, ax=ax)\n\ndf17.plot(kind=\"bar\", x = \"name\", y = \"tweet_volume\", use_index=\"false\", \n          label = \"Number of Tweets 2/17\", color = \"darkgreen\",fontsize = 22, figsize=(20,15), ax=ax2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Conclusion / Future Work**\n\nUnfortunately I wasn't able to figure out how to parse through the data from my Twitter home page. There's just so much NaN data, I can't really figure out what's happening there, or how to find the keywords that would correspond to my \"trending topics\" files...\n\nMy future work would definitely involve parsing the home page data and figuring out how to visualize it in the same way I did for the trending topics data. Unfortunately I'm out of time for this assignment! "},{"metadata":{"trusted":true},"cell_type":"code","source":"homepage = pd.read_json(\"/kaggle/working/homepage0225.txt\")\nhomepage","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}