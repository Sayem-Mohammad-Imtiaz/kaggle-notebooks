{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scipy as sp\nfrom scipy import stats\nimport warnings\nwarnings.filterwarnings('ignore')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.set_option('display.max_columns',14)\ncolumn_name=['CRIM','ZN','INDUS','CHAS','NOX','RM','AGE','DIS','RAD','TAX','PTRATIO','B','LSTAT','MEDV']\ndata=pd.read_csv('../input/housing.csv',delim_whitespace=True, names=column_name)\n#Preview data\nprint(data.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(data.info()) \n#View the data set, the type of dataset features, and missing conditions.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The initial state of the feature of the data set is continuous data, and there are no missing values."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(data.describe())\n#Let us look at the overall description of the data set distribution.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"CHAS is a discrete attribute.We need to look further at the distribution of the data.\nThe next step is to explore the data quality of the data set."},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\n\nfig, axs = plt.subplots(ncols=7, nrows=2, figsize=(12, 8))\ni = 0\naxs = axs.flatten()\nfor key,value in data.items():   \n    sns.boxplot(y=key, data=data, ax=axs[i],sym='*')\n    i += 1\nplt.tight_layout(pad=0.4, w_pad=0.5, h_pad=5.0)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Features 'CRIM', 'ZN', 'CHAS', 'RM', 'DIS', 'PTRATIO', 'B', 'LSTAT', 'MEDV' all have outliers, but I have not found a solution to deal with missing values Method.\nSo I analyze the statistics of the data."},{"metadata":{"trusted":true},"cell_type":"code","source":"i=0\nfig, axs = plt.subplots(ncols=7, nrows=2, figsize=(12, 8))\naxs = axs.flatten()\nfor k,v in data.items():\n    sns.distplot(v, bins=10,ax=axs[i])\n    i += 1\nplt.tight_layout(pad=0.4, w_pad=0.5, h_pad=5.0)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The left partial distribution is: AGE, B\nThe right deviation distribution is: CRIM, ZN\nBimodal distribution: INDUS, RAD, TAX\nDiscrete distribution: CHAS\nQuasi-normal distribution: NOX, RM, DIS, PTRATIO, LSTAT, MEDV"},{"metadata":{"trusted":true},"cell_type":"code","source":"#let's plot the pairwise correlation on data now.\nprint(data.corr(method='pearson'))\nplt.figure(figsize=(12,8))\nsns.heatmap(data.corr(),annot=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"According to the correlation coefficient, it can be seen that the correlation coefficient between the variable MEDV and each variable is higher than LSTAT (-0.74), PTRATIO (-0.51), RM (0.7), INDUS (-0.48), and TAX (-0.47). I initially consider these five variables as independent variables."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Taking arguments\nx=data[['INDUS','RM','TAX','PTRATIO','LSTAT']]\n#Dependent variable\ny=data['MEDV']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Variance expansion factor to judge multicollinearity\n# def vif(df,col_i):\n#     from statsmodels.formula.api import ols\n\n#     cols=list(df.columns)\n#     cols.remove(col_i)\n#     cols_noti=cols\n#     formula=col_i+'~'+'+'.join(cols_noti)\n#     r2=ols(formula,df).fit().rsquared\n#     return 1./(1.-r2)\n\n# for i in x.columns:\n#     print(i,'\\t',vif(df=x,col_i=i))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we intend to use the variance expansion factor to judge multicollinearity, but there is an error condition, and the VIF is smaller than the set threshold of 10, so there is no multiple strong collinearity between the independent variables."},{"metadata":{"trusted":true},"cell_type":"code","source":"#\nplt.figure(figsize=(12,5))\nfor i,col in enumerate(x.columns):\n        plt.subplot(1,5,i+1)\n        plt.plot(x[col],y,'o')\n        plt.xlabel(col)\n        plt.ylabel('MEDV')\n        plt.plot(np.unique(x[col]), np.poly1d(np.polyfit(x[col], y, 1))(np.unique(x[col])))\n\nplt.show()\nplt.savefig('regression_plot.png')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Fitting a preliminary regression exploration of the respective variables and dependent variables.\nNow let's compare the effects of each regression model."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge,RidgeCV\nfrom sklearn.linear_model import Lasso,LassoCV\nfrom sklearn.svm import SVR\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.ensemble import BaggingRegressor\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error,r2_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Standardized data\nscaler=StandardScaler()\nX=scaler.fit_transform(x)\n#Divide the data set into a training set and a test set.\nX_train,X_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=123)\nkf=KFold(n_splits=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#LinearRegression\nlr_model=LinearRegression()\n\n# Ridge\nRidge_model=Ridge()\nalpha=np.logspace(-2,3,100,base=10)\nrcv=RidgeCV(alphas=alpha,store_cv_values=True)\nrcv.fit(X_train,y_train)\nprint('the best alpha of Ridge is {}'.format(rcv.alpha_)) #6.7341506577508214\nRidge_model.set_params(alpha=6.7)\n\n#Lasso\nLasso_model=Lasso()\nalpha_L=np.logspace(-2,3,100,base=10)\nLcv=LassoCV(alphas=alpha_L,cv=10)\nLcv.fit(X_train,y_train)\nprint('the best alpha of Lasso is {}'.format(rcv.alpha_)) #6.7341506577508214\nLasso_model.set_params(alpha=6.7)\n\n#SVR\nkernel=('linear','rbf')\ngamma=np.arange(0.001,1.0,0.1)\nC=np.arange(0.001,1.0,1)\ngrid={'kernel':kernel,'gamma':gamma,'C':C}\nsvr_search=GridSearchCV(estimator=SVR(),param_grid=grid,cv=10)\nsvr_search.fit(X_train,y_train)\nprint(svr_search.best_params_)  #{'C': 0.001, 'gamma': 0.001, 'kernel': 'linear'}\nSVR_model=SVR(kernel='linear',gamma=0.001,C=0.001)\n\n#DecisionTreeRegressor\nTree_model=DecisionTreeRegressor(max_depth=5)\n\n#KNeighborsRegressor\nKNN_model=KNeighborsRegressor(n_neighbors=5)#KNN_model:-24.950518 8.678390\n\n# ensemble\nBR_model=BaggingRegressor()\nABR_model=AdaBoostRegressor()\nRFR_model=RandomForestRegressor()\nGBR_model=GradientBoostingRegressor()\n\n# neural_network\nMLPR_model=MLPRegressor(hidden_layer_sizes=(100,),activation='logistic',alpha=0.001,max_iter=1000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr_score=cross_val_score(lr_model,X_train,y_train,cv=kf,scoring='neg_mean_squared_error')\nRidge_score=cross_val_score(Ridge_model,X_train,y_train,cv=kf,scoring='neg_mean_squared_error')\nLasso_score=cross_val_score(Lasso_model,X_train,y_train,cv=kf,scoring='neg_mean_squared_error')\nSVR_score=cross_val_score(SVR_model,X_train,y_train,cv=kf,scoring='neg_mean_squared_error')\nDTR_score=cross_val_score(Tree_model,X_train,y_train,cv=kf,scoring='neg_mean_squared_error')\nKNN_score=cross_val_score(KNN_model,X_train,y_train,cv=kf,scoring='neg_mean_squared_error')\nBR_score=cross_val_score(BR_model,X_train,y_train,cv=kf,scoring='neg_mean_squared_error')\nABR_score=cross_val_score(ABR_model,X_train,y_train,cv=kf,scoring='neg_mean_squared_error')\nRFR_score=cross_val_score(RFR_model,X_train,y_train,cv=kf,scoring='neg_mean_squared_error')\nGBR_score=cross_val_score(GBR_model,X_train,y_train,cv=kf,scoring='neg_mean_squared_error')\nMLPR_score=cross_val_score(MLPR_model,X_train,y_train,cv=kf,scoring='neg_mean_squared_error')\n\nprint('lr_model:%f %f'% (lr_score.mean(), lr_score.std()))          \nprint('Ridge_model:%f %f'% (Ridge_score.mean(), Ridge_score.std())) \nprint('Lasso_model:%f %f'%(Lasso_score.mean(),Lasso_score.std()))   \nprint('SVR_model:%f %f'%(SVR_score.mean(),SVR_score.std()))         \nprint('DTR_model:%f %f'%(DTR_score.mean(),DTR_score.std()))         \nprint('KNN_model:%f %f'%(KNN_score.mean(),KNN_score.std()))         \nprint('BR_model:%f %f'%(BR_score.mean(),BR_score.std()))            \nprint('ABR_model:%f %f'%(ABR_score.mean(),ABR_score.std()))         \nprint('RFR_model:%f %f'%(RFR_score.mean(),RFR_score.std()))         \nprint('GBR_model:%f %f'%(GBR_score.mean(),GBR_score.std()))         \nprint('MLPR_model:%f %f'%(MLPR_score.mean(),MLPR_score.std()))      ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The effect of GradientBoostingRegressor is best in these models, so choose it as our predictive model."},{"metadata":{"trusted":true},"cell_type":"code","source":"model=GradientBoostingRegressor()\nkfold=KFold(n_splits=10)\nparam_Grid=dict(n_estimators=np.array([50,100,150,200,250,300,350,400,450,500]))\nGrid=GridSearchCV(estimator=model,param_grid=param_Grid,scoring='neg_mean_squared_error',cv=kfold)\nresult=Grid.fit(X_train,y_train)\nprint(result.best_score_,result.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model=GradientBoostingRegressor(n_estimators=150,random_state=123).fit(X_train,y_train)\nprediction=model.predict(X_test)\nprint('mean_squared_error:',mean_squared_error(y_test, prediction))\nprint('r2_score:',r2_score(y_test, prediction))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next I may re-select the model and the independent variables for analysis. I hope to get better results."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}