{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas_profiling\nimport nltk\nimport numpy as np\nimport pandas as pd\nimport re\nimport matplotlib.pyplot as plt \nimport seaborn as sb\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport unidecode\nfrom wordcloud import WordCloud\nfrom nltk.stem import WordNetLemmatizer \nnltk.download('wordnet')\nfrom nltk.stem import PorterStemmer\nnltk.download('punkt')\nfrom nltk.tokenize import word_tokenize \nimport matplotlib.animation as animation\nimport operator\nimport plotly.express as px\nfrom collections import Counter\n%matplotlib inline","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nprint(os.listdir(\"../input/twitter-hate-speech\"))\ndf = pd.read_csv(\"../input/twitter-hate-speech/train_E6oV3lV.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pandas_profiling.ProfileReport(df)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.drop_duplicates(inplace = True)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['tweet'].isna().sum()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Code to remove @\ndf['clean_tweet'] = df['tweet'].apply(lambda x : ' '.join([tweet for tweet in x.split()if not tweet.startswith(\"@\")]))\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Removing numbers\ndf['clean_tweet'] = df['clean_tweet'].apply(lambda x : ' '.join([tweet for tweet in x.split() if not tweet == '\\d*']))\ndf.head()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Removing all the greek characters using unidecode library\ndf['clean_tweet'] = df['clean_tweet'].apply(lambda x : ' '.join([unidecode.unidecode(word) for word in x.split()])) \ndf.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Removing the word 'hmm' and it's variants\ndf['clean_tweet'] = df['clean_tweet'].apply(lambda x : ' '.join([word for word in x.split() if not word == 'h(m)+' ]))\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Code for removing slang words\nd = {'luv':'love','wud':'would','lyk':'like','wateva':'whatever','ttyl':'talk to you later',\n               'kul':'cool','fyn':'fine','omg':'oh my god!','fam':'family','bruh':'brother',\n               'cud':'could','fud':'food'} ## Need a huge dictionary\nwords = \"I luv myself\"\nwords = words.split()\nreformed = [d[word] if word in d else word for word in words]\nreformed = \" \".join(reformed)\nreformed","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['clean_tweet'] = df['clean_tweet'].apply(lambda x : ' '.join(d[word] if word in d else word for word in x.split()))\ndf.head(20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Finding words with # attached to it\ndf['#'] = df['clean_tweet'].apply(lambda x : ' '.join([word for word in x.split() if word.startswith('#')]))\ndf.head()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"frame = df['#']\nframe.head()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"frame = pd.DataFrame(frame)\nframe = frame.rename({'#':'Count(#)'},axis = 'columns')\nframe.head()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"frame[frame['Count(#)'] == ''] = 'No hashtags'\nframe.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_frame = pd.concat([df,frame],axis = 1)\ndata_frame.head(10)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_frame.drop('#',axis = 1,inplace = True)\ndata_frame.head(10)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Column showing whether the corresponding tweet has a hash tagged word or not\ndata_frame = data_frame.rename({'Count(#)':'Hash words'},axis = 'columns')\ndata_frame.head(10)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Removing stopwords\ndata_frame['clean_tweet'] = data_frame['clean_tweet'].apply(lambda x : ' '.join([word for word in x.split() if not word in set(stopwords.words('english'))]))\ndata_frame.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Lemmatization\nlemmatizer = WordNetLemmatizer()\ndata_frame['clean_tweet'] = data_frame['clean_tweet'].apply(lambda x : ' '.join([lemmatizer.lemmatize(word) for word in x.split()]))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Stemming\nps = PorterStemmer()\nadwait = data_frame\n#adwait.head()\ndata_frame['clean_tweet'] = data_frame['clean_tweet'].apply(lambda x : ' '.join([ps.stem(word) for word in x.split()]))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_frame.head()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_frame_clean = data_frame[['label', 'clean_tweet']]\ndata_frame_clean","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_frame_clean.to_csv('data_frame_clean.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Tokenization\ncorpus = []\nfor i in range(0,31962):\n    tweet = data_frame['clean_tweet'][i]\n    tweet = tweet.lower()\n    tweet = tweet.split()\n    tweet = [ps.stem(word) for word in tweet if not word in set(stopwords.words('english'))]\n    tweet = ' '.join(tweet)\n    corpus.append(tweet)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Ensuring all the tweets are tokenized into individual words\nlen(corpus)\ncorpus","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"normal_words = ' '.join([word for word in data_frame['clean_tweet'][data_frame['label'] == 0]])\nwordcloud = WordCloud(width = 800, height = 500, max_font_size = 110,max_words = 100).generate(normal_words)\nprint('Normal words')\nplt.figure(figsize= (12,8))\nplt.imshow(wordcloud, interpolation = 'bilinear',cmap='viridis')\nplt.axis('off')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"normal_words = ' '.join([word for word in data_frame['clean_tweet'][data_frame['label'] == 1]])\nwordcloud = WordCloud(width = 800, height = 500, max_font_size = 110,max_words = 100).generate(normal_words)\nprint('Normal words')\nplt.figure(figsize= (12,8))\nplt.imshow(wordcloud, interpolation = 'bilinear')\nplt.axis('off')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Collecting positive hashtags\nhash_positive = []\nhash_negative = []\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def hashtag_extract(x):\n    hashtags = []\n    # Loop over the words in the tweet\n    for i in x:\n        ht = re.findall(r\"#(\\w+)\", i)\n        hashtags.append(ht)\n\n    return hashtags","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hash_positive = hashtag_extract(data_frame['clean_tweet'][data_frame['label'] == 0])\n\n# extracting hashtags from racist/sexist tweets\nhash_negative = hashtag_extract(data_frame['clean_tweet'][data_frame['label'] == 1])\n\n# Converting a multidimensional list to a 1-D list\nhash_positive = sum(hash_positive,[])\nhash_negative = sum(hash_negative,[])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"q = Counter(hash_positive)\nq = dict(q.most_common())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"l_positive_count = list(q.values())\nl_positive_count[0:4]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"r = Counter(hash_negative)\nr = dict(r.most_common())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"l_negative_count = list(r.values())\nl_negative_count[0:4]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"l_positive_values = list(q.keys())\nl_positive_values[0:4]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"l_negative_values = list(r.keys())\nl_negative_values[0:4]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Creating a dataframe to represent top 20 positive and negative hash words\nl1 = pd.DataFrame(l_positive_values[0:20],columns = ['Positive_Words'])\nl2 = pd.DataFrame(l_positive_count[0:20],columns = ['Positive_Count'])\nl3 = pd.DataFrame(l_negative_values[0:20],columns = ['Negative_Words'])\nl4 = pd.DataFrame(l_negative_count[0:20],columns = ['Negative_Count'])\nz = pd.concat([l1,l2,l3,l4],axis = 1)\nz","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Animated plot for positive words with their frequency\nfig = px.bar(z, x=\"Positive_Words\", y=\"Positive_Count\",animation_frame=\"Positive_Count\",\n            hover_name=\"Positive_Words\")\nfig.layout.updatemenus[0].buttons[0].args[1][\"frame\"][\"duration\"] = 1200\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Animated plot for negative words with their frequency\nfig = px.bar(z, x=\"Negative_Words\", y=\"Negative_Count\",animation_frame=\"Negative_Count\",\n            hover_name=\"Negative_Words\")\nfig.layout.updatemenus[0].buttons[0].args[1][\"frame\"][\"duration\"] = 1200\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Normal histogram of positive words\nfig = px.bar(z, x=\"Positive_Words\", y=\"Positive_Count\",\n            hover_name=\"Positive_Words\",color = 'Positive_Count')\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Normal histogram of negative words\nfig = px.bar(z, x=\"Negative_Words\", y=\"Negative_Count\",\n            hover_name=\"Negative_Words\",color= 'Negative_Count')\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Techniques to convert the tweets into Bag-of-Words, TF-IDF, and Word Embeddings\n#Building various classifiers: -\n#TF-IDF approach\nfrom sklearn.feature_extraction.text import TfidfVectorizer\ntfidf_vectorizer = TfidfVectorizer(max_df=0.90, min_df=2,stop_words='english')\n# TF-IDF feature matrix\nX1 = tfidf_vectorizer.fit_transform(corpus).toarray()\nY1 = df.loc[:,'label'].values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX1_train, X1_test, Y1_train, Y1_test = train_test_split(data_frame['clean_tweet'], data_frame['label'], test_size = 0.3, random_state=0, shuffle = True, stratify=data_frame['label'])\nvectorizer = TfidfVectorizer()\nX1_train_vect = vectorizer.fit_transform(X1_train)\nY1 = df.loc[:,'label'].values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Random Forest using pipelines\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.ensemble import RandomForestClassifier\nrf = Pipeline([('tfidf', TfidfVectorizer()), ('rf', RandomForestClassifier())])\nrf.fit(X1_train, Y1_train)\ny_pred = rf.predict(X1_test)\nprint(pd.crosstab(Y1_test,y_pred,rownames=['Actual'],colnames=['Predicted']))\nprint(classification_report(Y1_test, y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(rf.predict([\"hate allahsoil worshiping muslims obama\"]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}