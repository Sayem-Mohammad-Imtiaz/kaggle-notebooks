{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Volume Forecasting : SKU future volume analysis and prediction"},{"metadata":{},"cell_type":"markdown","source":"1. [**Introduction**](#1.-Introduction)<br>\n2. [**Collection of Data**](#2.-Collection-of-Data)<br>\n    [2.1 Load Data](#2.1-Load-Data)<br>\n3. [**SKU Recommendation**](#3.-SKU-Recommendation)<br>\n    [3.1 by weather data](#3.1-by-weather-data)<br>\n    [3.2 by demographis](#3.2-by-demographis)<br>\n    [3.3 combination of weather and demographis](#3.3-combination-of-weather-and-demographis)<br>\n4. [**Data preparation and Data Distribution**](#4.-Data-Preparation-and-Data-Distribution)<br>\n    [4.1 Check Null and Missing Values](#4.1-Check-Null-and-Missing-Values)<br>\n5. [**Train Algorithm on Train Data**](#5.-Train-Algorithm-on-Train-Data)<br>\n    [5.1 Validation Data](#5.1-Validation-Data)<br>\n    [5.2 Modelling](#5.2-Modelling)<br>\n    [5.2.1 GradientBoostingRegressor](#5.2.1-GradientBoostingRegressor)<br>\n    [5.2.2 RandomForestRegressor](#5.2.3-RandomForestRegressor)<br>\n    [5.2.3 Support Vector Regression](#5.2.3-Support_Vector_Regression)<br>\n    [5.2.4 KNeighborsRegressor](#5.2.4-KNeighborsRegressor)<br>\n    [5.2.5 XGBRegressor](#5.2.5-XGBRegressor)<br>\n    [5.2.6 Linear Regression](#5.2.6-Linear-Regression)<br>\n    [5.2.7 Result Modeling](#5.2.7-Result-Modeling)<br>\n6. [**Test Algorithm on Test Data**](#6.-Test-Algorithm-on-Test-Data)<br> \n7. [**Forecasting Multivariate**](#7.-Forecasting-Multivariate)<br> \n    [7.1 Numerical Data Correlation](#7.1-Numerical-Data-Correlation)<br>\n    [7.2 Feature Engineering](#7.2-Feature-Engineering)<br>\n    [7.3 Modelling](#7.3-Modelling)<br>\n8. [**Forecasting Unvariate**](#8.-Forecasting-Unvariate)<br>\n        "},{"metadata":{},"cell_type":"markdown","source":"## 1. Introduction"},{"metadata":{},"cell_type":"markdown","source":"Country Beeristan, a high potential market, accounts for nearly 10% of Stallion & Co.’s global beer sales. Stallion & Co. has a large portfolio of products distributed to retailers through wholesalers (agencies). There are thousands of unique wholesaler-SKU/products combinations. In order to plan its production and distribution as well as help wholesalers with their planning, it is important for Stallion & Co. to have an accurate estimate of demand at SKU level for each wholesaler.<br>\n\nCurrently demand is estimated by sales executives, who generally have a “feel” for the market and predict the net effect of forces of supply, demand and other external factors based on past experience. The more experienced a sales exec is in a particular market, the better a job he does at estimating. Joshua, the new Head of S&OP for Stallion & Co. just took an analytics course and realized he can do the forecasts in a much more effective way. He approaches you, the best data scientist at Stallion, to transform the exercise of demand forecasting.<br>\n\n$\\textbf{Arif Romadhan}$ <br>\nemail : arifromadhan19@gmail.com<br><br>\n[Link my kaggle](https://www.kaggle.com/utathya/future-volume-prediction)<br>\n[Link github](https://github.com/arifromadhan19/kaggle/blob/master/Volume%20Forecasting/Volume%20Forecasting.ipynb)<br>"},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport joblib\nimport xgboost\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n\nfrom math import sqrt\nfrom numpy import concatenate\n\nfrom sklearn.svm import SVR\nfrom sklearn.metrics import r2_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.metrics.pairwise import euclidean_distances\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n\nfrom keras.layers import LSTM\nfrom keras.layers import Dense\nfrom keras.models import Sequential\nfrom keras.callbacks import EarlyStopping","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Collection of Data"},{"metadata":{},"cell_type":"markdown","source":"###  2.1 Load Data"},{"metadata":{},"cell_type":"markdown","source":"### Data Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nprint()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1 = pd.read_csv('../input/demographics.csv')\ndf1['Year'] = '2017'\nprint(len(df1))\ndf1.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df2 = pd.read_csv('../input/event_calendar.csv')\nprint(len(df2))\ndf2['YearMonth'] = df2['YearMonth'] .astype(str)\ndf2['YearMonth'] = df2['YearMonth'].apply(lambda x: x[0:4]+'-'+x[4:6])\ndf2['YearMonth']  = pd.to_datetime(df2['YearMonth'])\ndf2.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df3 = pd.read_csv('../input/historical_volume.csv')\nprint(len(df3))\ndf3['YearMonth'] = df3['YearMonth'] .astype(str)\ndf3['YearMonth'] = df3['YearMonth'].apply(lambda x: x[0:4]+'-'+x[4:6])\ndf3['YearMonth']  = pd.to_datetime(df3['YearMonth'])\ndf3 = df3.sort_values('YearMonth', ascending=True).reset_index(drop=True)\ndf3.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df4 = pd.read_csv('../input/industry_soda_sales.csv')\nprint(len(df4))\ndf4['YearMonth'] = df4['YearMonth'] .astype(str)\ndf4['YearMonth'] = df4['YearMonth'].apply(lambda x: x[0:4]+'-'+x[4:6])\ndf4['YearMonth']  = pd.to_datetime(df4['YearMonth'])\ndf4.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df5 = pd.read_csv('../input/industry_volume.csv')\nprint(len(df5))\ndf5['YearMonth'] = df5['YearMonth'] .astype(str)\ndf5['YearMonth'] = df5['YearMonth'].apply(lambda x: x[0:4]+'-'+x[4:6])\ndf5['YearMonth']  = pd.to_datetime(df5['YearMonth'])\ndf5.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df6 = pd.read_csv('../input/price_sales_promotion.csv')\nprint(len(df6))\ndf6['YearMonth'] = df6['YearMonth'] .astype(str)\ndf6['YearMonth'] = df6['YearMonth'].apply(lambda x: x[0:4]+'-'+x[4:6])\ndf6['YearMonth']  = pd.to_datetime(df6['YearMonth'])\ndf6 = df6.sort_values('YearMonth', ascending=True).reset_index(drop=True)\ndf6.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df7 = pd.read_csv('../input/weather.csv')\nprint(len(df7))\ndf7['YearMonth'] = df7['YearMonth'] .astype(str)\ndf7['YearMonth'] = df7['YearMonth'].apply(lambda x: x[0:4]+'-'+x[4:6])\ndf7['YearMonth']  = pd.to_datetime(df7['YearMonth'])\ndf7.head(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Merge industry_soda_sales, industry_volume"},{"metadata":{"trusted":false},"cell_type":"code","source":"dfa = df4.merge(df5, on='YearMonth', how='inner')\ndfa = dfa.merge(df2, on='YearMonth', how='inner')\nprint(len(dfa))\ndfa.head(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Merge historical_volume, price_sales_promotion, weather, industry_soda_sales, industry_volume"},{"metadata":{"trusted":false},"cell_type":"code","source":"df = df3.merge(df6, on=['Agency','SKU','YearMonth'], how='inner')\ndf = df.merge(df7, on=['Agency','YearMonth'], how='inner')\n\ndf['Year'] = df['YearMonth'].dt.year\ndf['Year'] = df['Year'].astype(str)\n\nprint(len(df))\ndf = df.merge(df1, on=['Agency','Year'], how ='left')\n\nprint(len(df))\ndf['SKU'] = df['SKU'].apply(lambda x: x[4:])\ndf['SKU'] = df['SKU'].astype(int)\n\ndf['Agency'] = df['Agency'].apply(lambda x: x[7:])\ndf['Agency'] = df['Agency'].astype(int)\n\ndf = df.merge(dfa, on='YearMonth', how='left')\ndf = df.drop('Year', axis=1)\nprint(len(df))\n\ndf.to_csv('../input/train.csv',index=False)\ndf.head(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. SKU Recommendation"},{"metadata":{},"cell_type":"markdown","source":"### 3.1 by weather data"},{"metadata":{"trusted":false},"cell_type":"code","source":"# df7['Agency'] = df7['Agency'].apply(lambda x: x[7:])\n# df7['Agency'] = df7['Agency'].astype(int)\nprint(len(df7))\ndf7.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df7 = df7.groupby('Agency')['Avg_Max_Temp'].agg(['mean','median']).reset_index()\nprint(len(df7))\ndf7.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"agen6 = df7[(df7['mean']>=29.007939)&(df7['mean'] <29.007940)].reset_index(drop=True)\nagen6","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"agen14 = df7[(df7['mean']>= 25.085280)&(df7['mean'] <25.085282)].reset_index(drop=True)\nagen14","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**jika kita analisa by mean dan median<br>\nAgency 6 memiliki kesamaan dengan agency 5, 40, 8, dan 50<br>\nAgency 14 memiliki kesamaan dengan agency 12, 13, 15, 16. 17, 20, 38, 39, 57, 58, 59, 60**"},{"metadata":{},"cell_type":"markdown","source":"### 3.2 by demographis"},{"metadata":{"trusted":false},"cell_type":"code","source":"df1 = df1.drop('Year',axis=1)\ndf1 = df1.sort_values('Agency',ascending=True).reset_index(drop=True)\ndf1.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"agen_614 = df1[(df1['Agency']=='Agency_06')|(df1['Agency']=='Agency_14')]\nagen_614","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df1_temp = df1[(df1['Avg_Population_2017'] >= 1800000)]\ndf1_temp = df1_temp[(df1_temp['Avg_Population_2017'] < 2500000)]\ndf1_temp.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df1_temp = df1_temp[df1_temp['Avg_Yearly_Household_Income_2017']> 185000]\ndf1_temp = df1_temp[df1_temp['Agency'] != 'Agency_06']\ndf1_temp = df1_temp[df1_temp['Agency'] != 'Agency_14'].reset_index(drop=True)\ndf1_temp.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(15,10), sharex=True, sharey=True)\ng = sns.scatterplot(x='Avg_Population_2017',y='Avg_Yearly_Household_Income_2017', hue='Agency', data=df1, ax=ax)\ng = sns.scatterplot(x='Avg_Population_2017',y='Avg_Yearly_Household_Income_2017', marker='X', s=400 , data=agen_614, ax=ax)\ng = sns.scatterplot(x='Avg_Population_2017',y='Avg_Yearly_Household_Income_2017', marker='X', s=150 , data=df1_temp, ax=ax)\n\n\nax.legend( df1['Agency'].values, loc='upper lef', ncol=2, borderaxespad=0,frameon=False, bbox_to_anchor= (1.01, 1.0))\nplt.savefig('../fig/sku reco analysis 1.png',bbox_inches='tight')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**jika kita analisa dari graph tersebut maka,<br>\nAgency_06 dekat dengan agency 55 dan 60<br>\nAgency_14 dekat dengan 57 dan 56**"},{"metadata":{},"cell_type":"markdown","source":"### 3.3 combination of weather and demographis"},{"metadata":{},"cell_type":"markdown","source":"**dari kedua hasil analisa tersebut maka<br>\nAgency_06 : 55, 60, 5, 40, 8, dan 50<br>\nAgency_14 : 57, 56, 12, 13, 15, 16. 17, 20, 38, 39, 57, 58, 59, 60<br><br>\nMencari SKU dengan volume terbaik dari agency diatas di dalam data historical_volume**"},{"metadata":{},"cell_type":"markdown","source":"### Agency_06"},{"metadata":{"trusted":false},"cell_type":"code","source":"agen6 = df3[df3['Agency'].isin(['Agency_55','Agency_60','Agency_5','Agency_40','Agency_8','Agency_50'])]\nagen6 = agen6.sort_values('Agency', ascending=True).reset_index(drop=True)\nagen6 = agen6.drop('YearMonth',axis=1)\nprint(len(agen6))\nagen6.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"agen6 = agen6.groupby(['Agency','SKU'])['Volume'].agg(['mean','median']).reset_index()\nagen6 = agen6.drop('Agency',axis=1)\nagen6.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# agen6['SKU'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"agen6 = agen6.groupby('SKU')['mean','median'].mean().reset_index()\nagen6 = agen6.sort_values('mean', ascending=False).reset_index(drop=True)\nagen6.head(6)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Agency_14"},{"metadata":{"trusted":false},"cell_type":"code","source":"dagen14 = df3[df3['Agency'].isin(['Agency_57','Agency_56','Agency_12','Agency_13','Agency_15',\n                                    'Agency_16','Agency_17','Agency_20','Agency_38','Agency_39',\n                                    'Agency_58','Agency_59','Agency_60'])]\nprint(len(dagen14))\ndagen14.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"dagen14 = dagen14.sort_values('Agency', ascending=True).reset_index(drop=True)\ndagen14 = dagen14.drop('YearMonth',axis=1)\ndagen14.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"dagen14 = dagen14.groupby(['Agency','SKU'])['Volume'].agg(['mean','median']).reset_index()\ndagen14.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"dagen14 = dagen14.drop('Agency',axis=1)\ndagen14.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# dagen14['SKU'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"dagen14 = dagen14.groupby('SKU')['mean','median'].mean().reset_index()\ndagen14 = dagen14.sort_values('mean', ascending=False).reset_index(drop=True)\ndagen14.head(6)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4. Data Preparation and Data Distribution"},{"metadata":{"trusted":false},"cell_type":"code","source":"df = pd.read_csv('../input/train.csv')\ndf['Volume'] = df['Volume'].round(2)\ndf['Price'] = df['Price'].round(2)\ndf['Sales'] = df['Sales'].round(2)\ndf['Promotions'] = df['Promotions'].round(2)\ndf['Avg_Max_Temp'] = df['Avg_Max_Temp'].round(2)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df = df[['Agency','SKU','Volume']].copy()\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.1 Check Null and Missing Values"},{"metadata":{"trusted":false},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5. Train Algorithm on Train Data"},{"metadata":{"trusted":false},"cell_type":"code","source":"X = df.iloc[:,0:2]\nX.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"y = df.iloc[:,2:3]\ny.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sc_x = StandardScaler()\n\nX = sc_x.fit_transform(X.astype(float))\ny = y.values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5.1 Validation Data"},{"metadata":{"trusted":false},"cell_type":"code","source":"X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size = 0.33, random_state = 42)\nprint(X_train.shape)\nprint(Y_train.shape)\nprint(X_test.shape)\nprint(Y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"kfold = KFold(n_splits=5, shuffle=True, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5.2 Modelling"},{"metadata":{},"cell_type":"markdown","source":"### 5.2.1 GradientBoostingRegressor"},{"metadata":{"trusted":false},"cell_type":"code","source":"arr_gb_val_r2 = []\narr_gb_val_mse = []\n\narr_gb_test_r2 = []\narr_gb_test_mse = []\n\nfor train, test in kfold.split(X_train):\n    clf_gb = GradientBoostingRegressor()\n    clf_gb.fit(X[train],y[train])\n\n    Y_pred_val = clf_gb.predict(X[test])\n    val_mse_gb = mean_squared_error(y[test],Y_pred_val)\n    val_r2_gb = r2_score(y[test], Y_pred_val) \n\n    print('r2 Val : ',val_r2_gb)\n    print('mse Val : ',val_mse_gb)\n    arr_gb_val_r2.append(val_r2_gb)\n    arr_gb_val_mse.append(val_mse_gb)\n    \n    Y_pred_test = clf_gb.predict(X_test)\n    test_mse_gb = mean_squared_error(Y_test,Y_pred_test)\n    test_r2_gb = r2_score(Y_test, Y_pred_test) \n\n    print('r2 Test : ',test_r2_gb)\n    print('mse Val : ',test_mse_gb)\n    arr_gb_test_r2.append(test_r2_gb)\n    arr_gb_test_mse.append(test_mse_gb)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(np.mean(arr_gb_val_r2))\nprint(np.mean(arr_gb_val_mse))\nprint(np.mean(arr_gb_test_r2))\nprint(np.mean(arr_gb_test_mse))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5.2.2 RandomForestRegressor"},{"metadata":{"trusted":false},"cell_type":"code","source":"arr_rf_val_r2 = []\narr_rf_val_mse = []\n\narr_rf_test_r2 = []\narr_rf_test_mse = []\n\nfor train, test in kfold.split(X_train):\n    clf_rf = RandomForestRegressor(n_estimators=500, random_state=0,max_depth=2)\n    clf_rf.fit(X[train],y[train])\n\n    Y_pred_val = clf_rf.predict(X[test])\n    val_mse_rf = mean_squared_error(y[test],Y_pred_val)\n    val_r2_rf = r2_score(y[test], Y_pred_val) \n\n    print('r2 Val : ',val_r2_rf)\n    print('mse Val : ',val_mse_rf)\n    arr_rf_val_r2.append(val_r2_rf)\n    arr_rf_val_mse.append(val_mse_rf)\n    \n    Y_pred_test = clf_rf.predict(X_test)\n    test_mse_rf = mean_squared_error(Y_test,Y_pred_test)\n    test_r2_rf = r2_score(Y_test, Y_pred_test) \n\n    print('r2 Test : ',test_r2_rf)\n    print('mse Val : ',test_mse_rf)\n    arr_rf_test_r2.append(test_r2_rf)\n    arr_rf_test_mse.append(test_mse_rf)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(np.mean(arr_rf_val_r2))\nprint(np.mean(arr_rf_val_mse))\nprint(np.mean(arr_rf_test_r2))\nprint(np.mean(arr_rf_test_mse))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5.2.3 Support Vector Regression"},{"metadata":{"trusted":false},"cell_type":"code","source":"arr_svm_val_r2 = []\narr_svm_val_mse = []\n\narr_svm_test_r2 = []\narr_svm_test_mse = []\n\nfor train, test in kfold.split(X_train):\n    clf_svm = SVR(kernel='rbf', C=1e3, gamma=0.1)\n    clf_svm.fit(X[train],y[train])\n\n    Y_pred_val = clf_svm.predict(X[test])\n    val_mse_svm = mean_squared_error(y[test],Y_pred_val)\n    val_r2_svm = r2_score(y[test], Y_pred_val) \n\n    print('r2 Val : ',val_r2_svm)\n    print('mse Val : ',val_mse_svm)\n    arr_svm_val_r2.append(val_r2_svm)\n    arr_svm_val_mse.append(val_mse_svm)\n    \n    Y_pred_test = clf_svm.predict(X_test)\n    test_mse_svm = mean_squared_error(Y_test,Y_pred_test)\n    test_r2_svm = r2_score(Y_test, Y_pred_test) \n\n    print('r2 Test : ',test_r2_svm)\n    print('mse Val : ',test_mse_svm)\n    arr_svm_test_r2.append(test_r2_svm)\n    arr_svm_test_mse.append(test_mse_svm)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(np.mean(arr_svm_val_r2))\nprint(np.mean(arr_svm_val_mse))\nprint(np.mean(arr_svm_test_r2))\nprint(np.mean(arr_svm_test_mse))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5.2.4 KNeighborsRegressor"},{"metadata":{"trusted":false},"cell_type":"code","source":"arr_knn_val_r2 = []\narr_knn_val_mse = []\n\narr_knn_test_r2 = []\narr_knn_test_mse = []\n\nfor train, test in kfold.split(X_train):\n    clf_knn = KNeighborsRegressor(n_neighbors=2)\n    clf_knn.fit(X[train],y[train])\n\n    Y_pred_val = clf_knn.predict(X[test])\n    val_mse_knn = mean_squared_error(y[test],Y_pred_val)\n    val_r2_knn = r2_score(y[test], Y_pred_val) \n\n    print('r2 Val : ',val_r2_knn)\n    print('mse Val : ',val_mse_knn)\n    arr_knn_val_r2.append(val_r2_knn)\n    arr_knn_val_mse.append(val_mse_knn)\n    \n    Y_pred_test = clf_knn.predict(X_test)\n    test_mse_knn = mean_squared_error(Y_test,Y_pred_test)\n    test_r2_knn = r2_score(Y_test, Y_pred_test) \n\n    print('r2 Test : ',test_r2_knn)\n    print('mse Val : ',test_mse_knn)\n    arr_knn_test_r2.append(test_r2_knn)\n    arr_knn_test_mse.append(test_mse_knn)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(np.mean(arr_knn_val_r2))\nprint(np.mean(arr_knn_val_mse))\nprint(np.mean(arr_knn_test_r2))\nprint(np.mean(arr_knn_test_mse))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5.2.5 XGBRegressor"},{"metadata":{"trusted":false},"cell_type":"code","source":"arr_xgb_val_r2 = []\narr_xgb_val_mse = []\n\narr_xgb_test_r2 = []\narr_xgb_test_mse = []\n\ni = 1\nfor train, test in kfold.split(X_train):\n    clf_xgb = xgboost.XGBRegressor(n_estimators=100, learning_rate=0.08, gamma=0, subsample=0.75,colsample_bytree=1, max_depth=7)\n    clf_xgb.fit(X[train],y[train])\n    #save model\n    joblib.dump(clf_xgb, '../model/xgb_'+str(i)+'.dat') \n    i = i+1\n    \n    Y_pred_val = clf_xgb.predict(X[test])\n    val_mse_xgb = mean_squared_error(y[test],Y_pred_val)\n    val_r2_xgb = r2_score(y[test], Y_pred_val) \n\n    print('r2 Val : ',val_r2_xgb)\n    print('mse Val : ',val_mse_xgb)\n    arr_xgb_val_r2.append(val_r2_xgb)\n    arr_xgb_val_mse.append(val_mse_xgb)\n    \n    Y_pred_test = clf_xgb.predict(X_test)\n    test_mse_xgb = mean_squared_error(Y_test,Y_pred_test)\n    test_r2_xgb = r2_score(Y_test, Y_pred_test) \n\n    print('r2 Test : ',test_r2_xgb)\n    print('mse Val : ',test_mse_xgb)\n    arr_xgb_test_r2.append(test_r2_xgb)\n    arr_xgb_test_mse.append(test_mse_xgb)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(np.mean(arr_xgb_val_r2))\nprint(np.mean(arr_xgb_val_mse))\nprint(np.mean(arr_xgb_test_r2))\nprint(np.mean(arr_xgb_test_mse))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5.2.6 Linear Regression"},{"metadata":{"trusted":false},"cell_type":"code","source":"arr_lr_val_r2 = []\narr_lr_val_mse = []\n\narr_lr_test_r2 = []\narr_lr_test_mse = []\n\nfor train, test in kfold.split(X_train):\n    clf_lr = LinearRegression()\n    clf_lr.fit(X[train],y[train])\n\n    Y_pred_val = clf_lr.predict(X[test])\n    val_mse_lr = mean_squared_error(y[test],Y_pred_val)\n    val_r2_lr = r2_score(y[test], Y_pred_val) \n\n    print('r2 Val : ',val_r2_lr)\n    print('mse Val : ',val_mse_lr)\n    arr_lr_val_r2.append(val_r2_lr)\n    arr_lr_val_mse.append(val_mse_lr)\n    \n    Y_pred_test = clf_lr.predict(X_test)\n    test_mse_lr = mean_squared_error(Y_test,Y_pred_test)\n    test_r2_lr = r2_score(Y_test, Y_pred_test) \n\n    print('r2 Test : ',test_r2_lr)\n    print('mse Val : ',test_mse_lr)\n    arr_lr_test_r2.append(test_r2_lr)\n    arr_lr_test_mse.append(test_mse_lr)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(np.mean(arr_lr_val_r2))\nprint(np.mean(arr_lr_val_mse))\nprint(np.mean(arr_lr_test_r2))\nprint(np.mean(arr_lr_test_mse))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5.2.7 Result Modeling"},{"metadata":{"trusted":false},"cell_type":"code","source":"result_modelling = pd.DataFrame({\n    'model': ['GradientBoostingRegressor', 'RandomForestRegressor', 'Support Vector Regression', \n            'KNeighborsRegressor', 'XGBRegressor', 'Linear Regression'\n             ],\n    'val mse': [np.mean(arr_gb_val_mse), np.mean(arr_rf_val_mse), np.mean(arr_svm_val_mse),\n                np.mean(arr_knn_val_mse), np.mean(arr_xgb_val_mse), np.mean(arr_lr_val_mse)\n        \n    ],\n    \n    'val r2': [np.mean(arr_gb_val_r2), np.mean(arr_rf_val_r2), np.mean(arr_svm_val_r2),\n               np.mean(arr_knn_val_r2), np.mean(arr_xgb_val_r2), np.mean(arr_lr_val_r2)\n        \n    ],\n    'test mse': [np.mean(arr_gb_test_mse), np.mean(arr_rf_test_mse), np.mean(arr_svm_test_mse),\n                 np.mean(arr_knn_test_mse), np.mean(arr_xgb_test_mse), np.mean(arr_lr_test_mse)\n        \n    ],\n    \n    'test r2': [np.mean(arr_gb_test_r2), np.mean(arr_rf_test_r2), np.mean(arr_svm_test_r2),\n                np.mean(arr_knn_test_r2), np.mean(arr_xgb_test_r2), np.mean(arr_lr_test_r2)\n        \n    ],\n})\nresult_modelling['val mse'] = np.round(result_modelling['val mse'], decimals = 3)\nresult_modelling['val r2'] = np.round(result_modelling['val r2'], decimals = 3)\nresult_modelling['test mse'] = np.round(result_modelling['test mse'], decimals = 3)\nresult_modelling['test r2'] = np.round(result_modelling['test r2'], decimals = 3)\nresult_modelling = result_modelling.sort_values(by='val r2', ascending=False).reset_index(drop=True)\nresult_modelling","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"result_modelling.to_csv('../result/result_modelling.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"val = result_modelling['val r2']\ntest = result_modelling['test r2']\n\nbars = result_modelling['model']\nbarwidth = 0.3\n\ntotal_pos = np.arange(len(bars))\nval_pos = [x + barwidth for x in total_pos]\ntest_pos = [x + 2*barwidth for x in total_pos]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"fig,ax =plt.subplots(figsize=(15,10))\nplt.bar(val_pos,val,width=barwidth, color = '#800000', alpha=0.9)\nplt.bar(test_pos,test,width=barwidth, color = '#008000', alpha=0.9)\n\nplt.bar(0.3,0,width=barwidth, color = '#800000',label='Val ')\nplt.bar(0.6,0,width=barwidth, color = '#008000', label='Test ')\n# plt.bar(0.3,0.9450,width=barwidth, color = '#800000')\n# plt.bar(0.6,0.946977,width=barwidth, color = '#008000')\n\ntitle = 'Model Analysis (Cross Validation and Test)'\ntxpos = 4 #title x coordinate\ntypos = 1.1 #title y coordinate\nax.text(txpos,typos,title,horizontalalignment='center',color='#800000',fontsize=20,fontweight='bold')\n\n\n# insight = '''\n\n# '''\n# ixpos = 0.1 #insight x coordinate\n# iypos = 1.05 #insight y coordinate\n# ax.text(ixpos,iypos,insight,horizontalalignment='left',color='grey',fontsize=16,fontweight='normal')\n\nplt.xticks(val_pos, bars,rotation=45)\nax.legend(loc='upper left', bbox_to_anchor= (1.01, 1.0), ncol=1, borderaxespad=0,frameon=False)\nax.set_ylim(0,1.2)\n\n\n\n\nplt.savefig('../fig/result_modelling.png',bbox_inches='tight')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 6. Test Algorithm on Test Data"},{"metadata":{"trusted":false},"cell_type":"code","source":"df_test = pd.read_csv('../input/volume_forecast.csv')\ndf_test.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_test.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_test['SKU'] = df_test['SKU'].apply(lambda x: x[4:])\ndf_test['SKU'] = df_test['SKU'].astype(int)\n\ndf_test['Agency'] = df_test['Agency'].apply(lambda x: x[7:])\ndf_test['Agency'] = df_test['Agency'].astype(int)\ndf_test.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_test = df_test.drop('Volume', axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"X_test = sc_x.fit_transform(df_test.astype(float))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"model_xgb = joblib.load('../model/xgb_2.dat')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"y_pred = model_xgb.predict(X_test)\ny_pd = pd.DataFrame({'Volume':y_pred})\nprint('finish predict')\ny_pd.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Concat"},{"metadata":{"trusted":false},"cell_type":"code","source":"df_test = pd.read_csv('../input/volume_forecast.csv') \ndf_test = df_test.drop('Volume', axis=1)\ndf_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"frames = [df_test, y_pd]\nresult = pd.concat(frames, axis=1)\nresult.to_csv('../result/volume_forecast.csv',index=False)\nresult.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 7. Forecasting Multivariate"},{"metadata":{},"cell_type":"markdown","source":"## 7.1 Numerical Data Correlation¶"},{"metadata":{"trusted":false},"cell_type":"code","source":"df = pd.read_csv('../input/train.csv')\ndf['Volume'] = df['Volume'].round(2)\ndf['Price'] = df['Price'].round(2)\ndf['Sales'] = df['Sales'].round(2)\ndf['Promotions'] = df['Promotions'].round(2)\ndf['Avg_Max_Temp'] = df['Avg_Max_Temp'].round(2)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# df = df.drop('YearMonth', axis=1)\n# df_temp = df\ncols = list(df)\n\ncorr_ =df[cols].corr()\nplt.figure(figsize=(16,10))\nsns.heatmap(corr_, annot=True, fmt = \".2f\", cmap = \"BuPu\")\nplt.savefig('../fig/Data Numeric Corr.png',bbox_inches='tight')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 7.2 Feature Engineering"},{"metadata":{"trusted":false},"cell_type":"code","source":"df = df.drop('Soda_Volume',axis=1)\ndf = df.drop('Good Friday',axis=1)\ndf = df.drop('Sales',axis=1)\ndf = df.drop('Revolution Day Memorial',axis=1)\ndf = df.drop('Independence Day',axis=1)\ndf = df.drop('Beer Capital',axis=1)\ndf = df.drop('New Year',axis=1)\ndf = df.drop('Avg_Max_Temp',axis=1)\ndf = df.drop('FIFA U-17 World Cup',axis=1)\ndf = df.drop('Football Gold Cup',axis=1)\ndf = df.drop('Avg_Population_2017',axis=1)\ndf = df.drop('Avg_Yearly_Household_Income_2017',axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"corr_ =df[list(df)].corr()\nplt.figure(figsize=(16,10))\nsns.heatmap(corr_, annot=True, fmt = \".2f\", cmap = \"BuPu\")\nplt.savefig('../fig/Data Numeric Corr 2.png',bbox_inches='tight')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df = df[['YearMonth','Volume','Price','Promotions']].copy()\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df = df.groupby('YearMonth')['Volume','Price','Promotions'].agg(['sum','mean','std']).reset_index()\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df.columns = ['YearMonth','v_sum','v_mean','v_std','p_sum','p_mean','p_std','pr_sum','pr_mean','pr_std']","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"len(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df = df.sort_values('YearMonth', ascending=True).reset_index(drop=True)\ndf['YearMonth'] = pd.to_datetime(df['YearMonth'])\ndf = df.merge(df2, on='YearMonth', how='inner')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df = df.drop('Good Friday',axis=1)\ndf = df.drop('Revolution Day Memorial',axis=1)\ndf = df.drop('Independence Day',axis=1)\ndf = df.drop('Beer Capital',axis=1)\ndf = df.drop('New Year',axis=1)\ndf = df.drop('FIFA U-17 World Cup',axis=1)\ndf = df.drop('Football Gold Cup',axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"data = df\ndata= data.drop('YearMonth', axis=1)\ndata.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"len(list(data))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"fig,ax = plt.subplots(14,1,figsize=(20,15))\nfor i,column in enumerate([col for col in data.columns if col != 'wnd_dir']):\n    data[column].plot(ax=ax[i])\n    ax[i].set_title(column)\nplt.savefig('../fig/Dataset1.png',bbox_inches='tight')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 7.3 Modelling "},{"metadata":{"trusted":false},"cell_type":"code","source":"def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n    n_vars = 1 if type(data) is list else data.shape[1]\n    df = pd.DataFrame(data)\n    cols, names = list(), list()\n    # input sequence (t-n, ... t-1)\n    for i in range(n_in, 0, -1):\n        cols.append(df.shift(i))\n        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n    # forecast sequence (t, t+1, ... t+n)\n    for i in range(0, n_out):\n        cols.append(df.shift(-i))\n        if i == 0:\n            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n        else:\n            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n    # put it all together\n    agg = pd.concat(cols, axis=1)\n    agg.columns = names\n    # drop rows with NaN values\n    if dropnan:\n        agg.dropna(inplace=True)\n    return agg","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"values = data.values\nprint(values.shape)\nvalues = values.astype('float32')\nseries_to_supervised(values,1,1).head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"reframed = series_to_supervised(values,1,1)\n# drop columns we don't want to predict\nreframed.drop(reframed.columns[[15,16,17,18,19,20,21,22,23,24,25,26,27]], axis=1, inplace=True)\nprint(len(list(reframed)))\nreframed.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"scaler = MinMaxScaler(feature_range=(0,1))\nscaled = scaler.fit_transform(values)\nreframed = series_to_supervised(scaled,1,1)\nreframed.drop(reframed.columns[[15,16,17,18,19,20,21,22,23,24,25,26,27]], axis=1, inplace=True)\nreframed.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"values = reframed.values\nn_train = 49\ntrain = values[:n_train]\ntest = values[n_train:]\ntrainX,trainY = train[:,:-1],train[:,-1]\ntestX,testY = test[:,:-1],test[:,-1]\n\nprint(trainX.shape,trainY.shape,testX.shape,testY.shape)\n\ntrainX = trainX.reshape(trainX.shape[0],1,trainX.shape[1])\ntestX = testX.reshape(testX.shape[0],1,testX.shape[1])\n\n\nprint(trainX.shape)\nprint(testX.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"len(testX)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"stop_noimprovement = EarlyStopping(patience=10)\nmodel = Sequential()\nmodel.add(LSTM(50,input_shape=(trainX.shape[1],trainX.shape[2]),dropout=0.2))\nmodel.add(Dense(1))\nmodel.compile(loss=\"mae\",optimizer=\"adam\")\n\nhistory= model.fit(trainX,trainY,validation_data=(testX,testY),epochs=100,verbose=2,callbacks=[stop_noimprovement],shuffle=False)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"fig,ax =plt.subplots(figsize=(15,10))\n\nplt.plot(history.history['loss'],label='train loss')\nplt.plot(history.history['val_loss'],label='val loss')\nplt.legend()\n# plt.savefig('../fig/trai val forecasting.png',bbox_inches='tight')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" ### Predictions"},{"metadata":{"trusted":false},"cell_type":"code","source":"len(testX)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"predicted = model.predict(testX)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"testXRe = testX.reshape(testX.shape[0],testX.shape[2])\npredicted = np.concatenate((predicted,testXRe[:,1:]),axis=1)\nprint('predicted.shape : ',predicted.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"predicted = model.predict(testX)\n\ntestXRe = testX.reshape(testX.shape[0],testX.shape[2])\npredicted = np.concatenate((predicted,testXRe[:,1:]),axis=1)\nprint('predicted.shape : ',predicted.shape)\n\npredicted = scaler.inverse_transform(predicted)\ntestY = testY.reshape(len(testY),1)\nprint('testY.shape : ',testY.shape)\n\ntestY = np.concatenate((testY,testXRe[:,1:]),axis=1)\ntestY = scaler.inverse_transform(testY)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"pd.DataFrame(testY)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"np.sqrt(mean_squared_error(testY[:,0],predicted[:,0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"result = pd.concat([pd.Series(predicted[:,0]),pd.Series(testY[:,0])],axis=1)\nresult.columns = ['thetahat','theta']\nresult['diff'] = result['thetahat'] - result['theta']","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"result = pd.concat([pd.Series(predicted[:,0]),pd.Series(testY[:,0])],axis=1)\nresult.columns = ['thetahat','theta']\nresult['diff'] = result['thetahat'] - result['theta']","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"result.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 8. Forecasting Unvariate"},{"metadata":{"trusted":false},"cell_type":"code","source":"df_new = df[['YearMonth','Volume','Price','Promotions']].copy()\ndf_new.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_new = df_new.groupby('YearMonth')['Volume'].agg(['sum']).reset_index()\ndf_new.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_new['YearMonth'] = pd.to_datetime(df_new['YearMonth'])\ndf_new = df_new.sort_values(by='YearMonth').reset_index(drop=True)\ndf_new = df_new.drop('YearMonth', axis=1)\ndf_new.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"fig,ax =plt.subplots(figsize=(15,10))\nplt.plot(df_new)\nplt.xlabel('month')\nplt.ylabel('sum of volume')\nplt.savefig('../fig/forecasting unvariate 1.png',bbox_inches='tight')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train = df_new[0:50]\ntest = df_new[50:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# load the trainset\ntrain = train.values\ntrain = train.astype('float32')\nscaler = MinMaxScaler(feature_range=(0, 1))\ntrain = scaler.fit_transform(train)\n\n# load the testset\ntest = test.values\ntest = test.astype('float32')\nscaler = MinMaxScaler(feature_range=(0, 1))\ntest = scaler.fit_transform(test)\nprint(test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# convert an array of values into a dataset matrix\ndef create_dataset(dataset, look_back=1):\n    dataX, dataY = [], []\n    for i in range(len(dataset)-look_back-1):\n        a = dataset[i:(i+look_back), 0]\n        dataX.append(a)\n        dataY.append(dataset[i + look_back, 0])\n    return np.array(dataX), np.array(dataY)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"look_back = 1\ntestX, testY = create_dataset(test, look_back)\n\n\ntrainX, trainY = create_dataset(train, look_back)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"trainX.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"trainY.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"testX.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# reshape input to be [samples, time steps, features]\ntrainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\ntestX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"model = Sequential()\nmodel.add(LSTM(4, input_shape=(1, look_back)))\nmodel.add(Dense(1))\nmodel.compile(loss='mean_squared_error', optimizer='adam')\n# model.fit(trainX, trainY, epochs=100, batch_size=1, verbose=2)\n\nhistory= model.fit(trainX,trainY,validation_data=(testX,testY),epochs=100,verbose=2,callbacks=[stop_noimprovement],shuffle=False)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"fig,ax =plt.subplots(figsize=(15,10))\n\nplt.plot(history.history['loss'],label='train loss')\nplt.plot(history.history['val_loss'],label='val loss')\nplt.legend()\nplt.savefig('../fig/unvariate 1.png',bbox_inches='tight')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"testPredict = model.predict(testX)\nprint(testPredict.shape)\ntestPredict = scaler.inverse_transform(testPredict)\ntestPredict","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"len(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"test = df_new[55:]\ntest","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"test.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# load the testset\ntest = test.values\ntest = test.astype('float32')\nscaler = MinMaxScaler(feature_range=(0, 1))\ntest = scaler.fit_transform(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"test = np.expand_dims(test, axis=1)\ntest.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"data= test\n# data = np.expand_dims(data, axis=1)\n\nprint('data n',data)\nprint('\\n')\n\n\npredict = model.predict(data)\nprint('predict data n ',predict)\nprint('\\n')\n\n\nfuture = predict\nfuture = np.expand_dims(future, axis=1)\n\nfuture_predict = model.predict(future)\nprint('predict data n + 1 ',future_predict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"future_predict = scaler.inverse_transform(future_predict)\nfuture_predict","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"env_kaggle","language":"python","name":"env_kaggle"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"nbformat":4,"nbformat_minor":1}