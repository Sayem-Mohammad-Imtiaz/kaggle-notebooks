{"cells":[{"metadata":{"_cell_guid":"803a7e0b-74e8-428b-8ee3-e88007302d45","_uuid":"3141d0403b9c4b57e27032556bb34af12e1ba908"},"cell_type":"markdown","source":"# Foreword\n\nThis notebook is forked from the \"Phrases That Characterize Each Neighborhood\" notebook by Brittany ( https://www.kaggle.com/bnsmith3/phrases-that-charactertize-each-neighborhood). The original notebook uses TFIDF to extract top-10 words that describe each neighborhood. This notebook takes the same pre-processing step as the original notebook, but took a different direction in analysis. Instead of describing neighrbohoods, this notebook attempts a two-step processes: (1) Create clusters of rooms based on how the owners describe the rooms, (2) Take top 20 words that best describe each cluster. We will take a look at clustering based on 6-clusters and 12-clusters\n"},{"metadata":{"_cell_guid":"3225c704-3784-41ae-8c4e-3491b6313bb3","_uuid":"b5d47e3aea26612ff171a3da0f3b7f80871994fb"},"cell_type":"markdown","source":"# **Pre-processing**"},{"metadata":{"_kg_hide-input":false,"_kg_hide-output":true,"_cell_guid":"ec58b503-7d47-4382-56f5-3d7bf396662f","collapsed":true,"_uuid":"1686615fef24936213c9da87ea5aaab6a0c63322"},"cell_type":"code","outputs":[],"source":"# Load required libraries\nimport numpy as np\nimport pandas as pd\nimport sklearn\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.cluster import KMeans, MiniBatchKMeans  # MiniBatchKMeans really helps to fasten processing time\nfrom nltk import wordpunct_tokenize\nfrom nltk.stem import WordNetLemmatizer\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud, STOPWORDS\nimport math as math","execution_count":null},{"metadata":{"_cell_guid":"c4f2e60b-c15a-e909-b8d4-40b863cd05b0","_uuid":"2d8922e3aba0cc601f0f214a904f4176dd1bf5e5"},"cell_type":"markdown","source":"### Function and class definitions"},{"metadata":{"_cell_guid":"b780b956-b8be-b307-53dc-e2a1ae890967","collapsed":true,"_uuid":"8ac76c77ee5043bcbd02ee5b2f5ba4b397c0b968"},"cell_type":"code","outputs":[],"source":"class LemmaTokenizer(object):\n    \"\"\"Custom tokenizer class that stems tokens\"\"\"\n    def __init__(self):\n        self.wnl = WordNetLemmatizer()\n    def __call__(self,doc):\n        return [self.wnl.lemmatize(t) for t in wordpunct_tokenize(doc) if len(t.strip()) > 1]\n    \ndef show_topn(classifier,vectorizer,categories,n):\n    \"\"\"Returns the top n features that characterize eachc category\"\"\"\n    feature_names = np.asarray(vectorizer.get_feature_names())\n    for i, category in enumerate(categories):\n        topn = np.argsort(classifier.coef_[i])[-n:]\n        print('{}: {}'.format(category,\", \".join(feature_names[topn])))\n        \ndef save_topn(classifier,vectorizer,categories,n,outdict):\n    \"\"\"Returns the top n features that characterize eachc category, and save result in outdict\"\"\"\n    feature_names = np.asarray(vectorizer.get_feature_names())\n    for i, category in enumerate(categories):\n        topn = np.argsort(classifier.coef_[i])[-n:]\n        outdict[i] = feature_names[topn]\n","execution_count":null},{"metadata":{"_cell_guid":"1fe0fb70-7495-8274-5193-39f2fa938ec8","_uuid":"692408cfa5dd8168e8fb6f0fad2fd9a9bad27a11"},"cell_type":"markdown","source":"### Let's get on with exploring"},{"metadata":{"_cell_guid":"e4381162-6f24-7167-5603-27222a8d4218","collapsed":true,"_uuid":"9d4de758abfbd6ebe34e697cb8e8163aaa19f62e"},"cell_type":"code","outputs":[],"source":"# read in a few columns from the data and show the top of the resulting dataframe\ndf = pd.read_csv('../input/listings.csv', usecols = ['id', 'name', 'space', 'description', 'neighborhood_overview', 'neighbourhood_cleansed'])\n\ndf.head()","execution_count":null},{"metadata":{"_kg_hide-input":true,"_cell_guid":"ec103d9a-8cb8-4bbf-8451-bdcc3592ab7a","collapsed":true,"_uuid":"fab5a73adbb16299595ea79d1bdcafa69a195d45"},"cell_type":"code","outputs":[],"source":"# Check the full text in each of the column\nfor i in range(len(df.columns)):\n    print(df.columns[i],\": \")\n    print(df.iloc[0,i])\n    print('=======================')","execution_count":null},{"metadata":{"_cell_guid":"7135bdb1-8795-e589-615e-0f9fcc8d2443","collapsed":true,"_uuid":"e2f51b1b838259a1540cb6131c75e9c09d4b028d"},"cell_type":"code","outputs":[],"source":"# let's combine the name, space, description, and neighborhood_overview into a new column\ndf['combined_description'] = df.apply(lambda x: '{} {} {} {}'.format(x['name'], x['space'], x['description'], x['neighborhood_overview']), axis=1)\nprint(df.loc[0,'combined_description'])","execution_count":null},{"metadata":{"_cell_guid":"6302b320-5c93-41cb-a016-3fdb9cdfd991","collapsed":true,"_uuid":"6b69d0dbe7de0b2c658e54d8dcabcbce845d7f87"},"cell_type":"code","outputs":[],"source":"# Transform combined_description into tfidf format\ntfidf = TfidfVectorizer(ngram_range=(1,2),stop_words='english',tokenizer=LemmaTokenizer())\ntfidf.fit(df['combined_description'])\nDescTfidf = tfidf.transform(df['combined_description'])","execution_count":null},{"metadata":{"_cell_guid":"8c6fbf85-8685-93cb-06f2-0c8b4203d409","_uuid":"135dc3864452fb557c02a84ff85507843d238340"},"cell_type":"markdown","source":"**How many listings are there for each neighborhood?**"},{"metadata":{"_kg_hide-output":false,"_cell_guid":"a738d6c3-cb40-0d80-951f-75ce0baee4e6","collapsed":true,"_uuid":"c3d2eaf9f3d3768daa8481cba1186a3a4f34ab8d"},"cell_type":"code","outputs":[],"source":"# I added a chart to replace tabulation in the original notebook\n\nneighborRank = df.groupby(by='neighbourhood_cleansed').count()[['id']].sort_values(by='id', ascending=False)\n# print(neighborRank)\nplt.figure(figsize=(10,10))\ng = sns.barplot(y=neighborRank.index,x=neighborRank[\"id\"])\n# The line below adds the value label in each bar\n[g.text(p[1]+1,p[0],p[1], color='black') for p in zip(g.get_yticks(), neighborRank[\"id\"])]\nplt.title('Number of Listings in Each Neighbourhood')","execution_count":null},{"metadata":{"_cell_guid":"a4cfb59c-edd2-46f5-8d25-856c8d148e14","_uuid":"0bc58de87d1a97abf0d79e7268da9840d21c7203"},"cell_type":"markdown","source":"# **K-Means Clustering with 6 segments**"},{"metadata":{"_cell_guid":"0641baf3-432b-4ab5-a20b-a723e98f0ea9","collapsed":true,"_uuid":"6b10f3fdad56cc4d036988ec8e3c358ddced28fd"},"cell_type":"code","outputs":[],"source":"# Create K-Means using MiniBatchKMeans. The MiniBatch version works much faster than regular KMeans\nkmeans6 = MiniBatchKMeans(n_clusters=6)\nDescKmeans6 = kmeans6.fit_predict(DescTfidf.todense())","execution_count":null},{"metadata":{"_cell_guid":"7fb6add2-9c88-407c-b955-7b18c4924ace","collapsed":true,"_uuid":"3c60bb343ed8e14b21ebdc042b6befda017ad246"},"cell_type":"code","outputs":[],"source":"# Combine description, cluster, and neighborhood into one dataframe. \nFullDescKmeans6 = pd.concat([pd.DataFrame(DescKmeans6),df[['combined_description','neighbourhood_cleansed']]],axis=1)\nFullDescKmeans6.columns = ['Cluster','Description','Neighbourhood']  \nprint(FullDescKmeans6.head())","execution_count":null},{"metadata":{"_cell_guid":"89ade4ce-ffd9-40a6-aad4-54cd7f84944b","_uuid":"0880e74d9346f133709a05ff6fde95fa0d95011b"},"cell_type":"markdown","source":"### How many listings in each cluster?"},{"metadata":{"_cell_guid":"164ae32c-d79d-45f0-90f1-218ad15f3a44","collapsed":true,"_uuid":"2bb6e726c207b96e06f005aa204711c17b2869f3"},"cell_type":"code","outputs":[],"source":"# Show and plot the number of listings in each cluster\nClusterCount = FullDescKmeans6['Cluster'].value_counts().sort_index()\nClusterCount = pd.DataFrame(ClusterCount)\nClusterCount.columns=['NumListings']\ng = sns.barplot(x=FullDescKmeans6['Cluster'].value_counts().index,y=FullDescKmeans6['Cluster'].value_counts())\n[g.text(p[0]-0.15,p[1]+5,p[1], color='black') for p in zip(g.get_xticks(), ClusterCount[\"NumListings\"])]\nplt.title('Number of Listings in Each Description-based Clusters')","execution_count":null},{"metadata":{"_cell_guid":"e599bf56-9f80-4d77-a3b3-4e4876f99fd4","_uuid":"eb88f2888585d7224724940ea4658ae90013a044"},"cell_type":"markdown","source":"Looks like the description clustering is imbalanced, with majority of descriptions fall into cluster 1 and 3"},{"metadata":{"_cell_guid":"fb67e2ad-9b47-4472-8680-62a9491e1738","_uuid":"d6713116fc5249a5a5918ac1789e501d88783c5d"},"cell_type":"markdown","source":"### Any close linkage between cluster and neighborhood?"},{"metadata":{"_cell_guid":"f6666380-9bba-4a36-be68-18d242ffae9d","collapsed":true,"_uuid":"944043ee486504e5989bbd9dc8157cbe90688b29"},"cell_type":"code","outputs":[],"source":"# Create crosstab between Cluster and Neighbourhood \nctab = pd.crosstab(index=FullDescKmeans6['Neighbourhood'],columns=FullDescKmeans6['Cluster'])\nplt.figure(figsize=(10,10))\nsns.heatmap(ctab,annot=True,cmap='Blues', fmt='g')\nplt.title(\"Crosstab of Cluster and Neighbourhood\")","execution_count":null},{"metadata":{"_cell_guid":"15613bb2-4682-4b41-b632-bd9173a37328","_uuid":"d1df1873ddcdb40bb60623e378623a92e4f3cdfd"},"cell_type":"markdown","source":"### Examine the full description of a couple of samples in each cluster"},{"metadata":{"_kg_hide-input":false,"_kg_hide-output":false,"_cell_guid":"2123bd27-0a01-45c4-a275-d432a643bc04","collapsed":true,"_uuid":"574473fbf348a1336627f5a15db413bc36c3fa87"},"cell_type":"code","outputs":[],"source":"# Let's take a look at the full description from a couple of listings\n#for i in range(6):\n#    subset = FullDescKmeans6[FullDescKmeans6['Cluster']==i]\n#    print('We are at cluster..')\n#    print(i)\n#    for j in range(1):\n#        print(subset.iloc[j,1])\n#        print('--------------------------------')","execution_count":null},{"metadata":{"_cell_guid":"9a67fa70-f02c-0122-06cf-df653fd2ad42","_uuid":"6d9a350398dfe541e0ee8750b3f150cbf30e4746"},"cell_type":"markdown","source":"### Top 30 words that describe each cluster"},{"metadata":{"_cell_guid":"5e327baf-9cca-4af6-93a3-ccbeb4427c87","collapsed":true,"_uuid":"5f832af5fe55f8c1fd8cc5c51a2887224425f17b"},"cell_type":"code","outputs":[],"source":"# Pipeline to identify top 30 words that are \"best predictor\" of a cluster\npipeline = Pipeline([('tfidf', TfidfVectorizer(ngram_range=(1,2), stop_words='english', tokenizer=LemmaTokenizer())),\n                     ('clf', SGDClassifier(loss='hinge', penalty='l2',\n                                           alpha=1e-3, n_iter=5, random_state=42)),\n])\n","execution_count":null},{"metadata":{"_cell_guid":"24e3f51f-a2c8-12a7-3435-8cfb2d08d185","collapsed":true,"_uuid":"94bbad090cf8ed418adca97055a2f590983004be"},"cell_type":"code","outputs":[],"source":"modelSegment = pipeline.fit(df['combined_description'],FullDescKmeans6['Cluster'])\n","execution_count":null},{"metadata":{"_cell_guid":"e7ff7ea0-f43e-48e4-8b25-215713cbf096","collapsed":true,"_uuid":"77e400a3059aa0c9e13856ea9427717751658f1a"},"cell_type":"code","outputs":[],"source":"# Create wordcloud based on top-30 words\nKeywords6 = {}\nsave_topn(modelSegment.named_steps['clf'], modelSegment.named_steps['tfidf'], [str(i) for i in range(6)], 30,outdict=Keywords6)\nfig,axes=plt.subplots(2,3,figsize=(30,12))\nfor i in range(6):\n    wordlist = list(Keywords6[i])\n    wc = WordCloud(background_color='white',max_words=30,relative_scaling=0.2).generate(\" \".join(wordlist))\n    print(wc)\n    axes[math.floor(i/3),i%3].imshow(wc)","execution_count":null},{"metadata":{"_cell_guid":"a502d8f6-3b0b-4114-abd0-95fe732a1ddf","_uuid":"e84137efd4760273cf4a315b229f666e7c0b5b6d"},"cell_type":"markdown","source":"# **K-Means Clustering with 12 segments**"},{"metadata":{"_cell_guid":"024b0832-7863-4100-972d-79181de35142","collapsed":true,"_uuid":"9ea45ca9dc3c476588d01845056bf8de39e2f2da"},"cell_type":"code","outputs":[],"source":"# Now we'll try to create a cluster of 12. Hopefully we could get a well spread clustering\nkmeans12 = MiniBatchKMeans(n_clusters=12,batch_size=128)\nDescKmeans12 = kmeans12.fit_predict(DescTfidf.todense())","execution_count":null},{"metadata":{"_cell_guid":"e1817688-42b4-45cc-9591-11566f683cd2","_uuid":"905512ce5dc1761518c9f740c43506b9d27b0f52"},"cell_type":"markdown","source":"### Check number of listings in each cluster"},{"metadata":{"_cell_guid":"436c2f73-af58-43f0-ab0b-fce5ad2df34c","collapsed":true,"_uuid":"3401daba3fbc0350cf40642766a2dda07bd74432"},"cell_type":"code","outputs":[],"source":"FullDescKmeans12 = pd.concat([pd.DataFrame(DescKmeans12),df[['combined_description','neighbourhood_cleansed']]],axis=1)\nFullDescKmeans12.columns = ['Cluster','Description','Neighbourhood']\ng = sns.barplot(x=FullDescKmeans12['Cluster'].value_counts().index,y=FullDescKmeans12['Cluster'].value_counts()) \nplt.title(\"Number of listings in each cluster\")","execution_count":null},{"metadata":{"_cell_guid":"86547d78-7f43-41af-b578-4a6969bb29e5","_uuid":"c1c964418cc84b24242304fb9d1f89b1301d6a80"},"cell_type":"markdown","source":"### Check cross-tab between cluster and neighbourhood"},{"metadata":{"_cell_guid":"fba1cf70-1eae-46c1-ab78-c535355433d3","collapsed":true,"_uuid":"e3b87ef0dc3a22de7f2c2a08ff41ca2153dcf80c"},"cell_type":"code","outputs":[],"source":"# Create crosstab between Cluster and Neighbourhood \nctab = pd.crosstab(index=FullDescKmeans12['Neighbourhood'],columns=FullDescKmeans12['Cluster'])\nplt.figure(figsize=(10,10))\nsns.heatmap(ctab,annot=True,cmap='Blues', fmt='g')\nplt.title(\"Crosstab of Cluster and Neighbourhood\")","execution_count":null},{"metadata":{"_cell_guid":"10aaa9ef-f8ad-4a1a-a0df-83f99e572ce3","_uuid":"66def7a61e760b1e0b95a32377437cd774619920"},"cell_type":"markdown","source":"### Show top-30 words that best describe each listing cluster"},{"metadata":{"_cell_guid":"291db5e0-02da-44ed-8e02-ce6f7f209d02","collapsed":true,"_uuid":"8d32e42e63ed8bdd03aa2d5e1c1274e141f8a7c7"},"cell_type":"code","outputs":[],"source":"# I previously use a regular print-out of the words, but now I am using a wordcloud instead\nmodelSegment = pipeline.fit(df['combined_description'],FullDescKmeans12['Cluster'])\n# show_topn(modelSegment.named_steps['clf'], modelSegment.named_steps['tfidf'], [str(i) for i in range(12)], 20)","execution_count":null},{"metadata":{"_cell_guid":"0490cdeb-9f5e-46d5-ad9b-e6e8fc231448","collapsed":true,"_uuid":"cb2d6a1d184d1674bc393293b038a2d9340dc25c"},"cell_type":"code","outputs":[],"source":"# Create wordcloud based on top-30 words\nKeywords12 = {}\nsave_topn(modelSegment.named_steps['clf'], modelSegment.named_steps['tfidf'], [str(i) for i in range(12)], 30,outdict=Keywords12)\nfig,axes=plt.subplots(4,3,figsize=(20,20))\nfor i in range(12):\n    wordlist = list(Keywords12[i])\n    wc = WordCloud(background_color='white',max_words=30,relative_scaling=0.2).generate(\" \".join(wordlist))\n    print(wc)\n    axes[math.floor(i/3),i%3].imshow(wc)","execution_count":null},{"metadata":{"_cell_guid":"30d2515d-8771-49d0-8fc2-8d8360184ece","_uuid":"cea8485b03c4fe1360cdb81b93820f9a2cecadc2"},"cell_type":"markdown","source":"## Next Steps\n\nA couple of interesting next steps to be pursued further:\n\n1. **Meaningless Words**.  In the first clustering, there is a cluster that contains many of the following meaningless word: ll, nan,  --. It would be useful to revew the stop words / text pre-processing to clear this issue out\n2. **Most Representative Samples**. When printing out some sample listing, instead of taking first few observations, I would be interested to take top-n nearest listing to each of the cluster centers\n3. **Word Clouds**. Word clouds would be helpful to visualize either the top-20 characterizing word, or the sample top-n most representative observations **Done!**\n"}],"metadata":{"_is_fork":false,"_change_revision":0,"kernelspec":{"display_name":"Python 3","name":"python3","language":"python"},"language_info":{"name":"python","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.3","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}