{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nos.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\";\n \nos.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\";  \nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom sklearn.metrics import classification_report, confusion_matrix,accuracy_score\n\nfrom keras.preprocessing import sequence\nfrom keras.preprocessing.text import Tokenizer\nfrom sklearn import preprocessing\nfrom keras.preprocessing import sequence\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation, Embedding, SpatialDropout1D, MaxPooling1D, Embedding, Conv1D, Flatten, Dropout\nfrom keras.layers import Bidirectional, GlobalMaxPool1D, LSTM\nfrom keras.optimizers import Adam\nfrom sklearn.utils import shuffle\nfrom sklearn.metrics import f1_score, confusion_matrix\nfrom sklearn.naive_bayes import MultinomialNB\nfrom nltk.corpus import stopwords\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem.snowball import SnowballStemmer\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport re\n\n# load data\ninput_file = \"../input/imdb-review-dataset/imdb_master.csv\"\n\n# comma delimited is the default\ndata = pd.read_csv(input_file, header = 0, encoding='ISO-8859-1', engine='python')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.python.client import device_lib\nprint(device_lib.list_local_devices())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"------------------------------------------------------------------------------------------------------------------------------------------------------\n------------------------------------------------------------------------------------------------------------------------------------------------------\n*Bayes*"},{"metadata":{"trusted":true},"cell_type":"code","source":"Xtrain = data['review'][indexes_train[0]].values\nYtrain = data['label'][indexes_train[0]].values\nXtest = data['review'][indexes_test[0]].values\nYtest = data['label'][indexes_test[0]].values\n\nindex_unsup = np.where(Y_train == 'unsup')\nYtrain = np.delete(Y_train, index_unsup)\nXtrain = np.delete(X_train, index_unsup)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_classif\nfrom sklearn.feature_selection import chi2\nfrom nltk.corpus import stopwords\n\nstopwords = set(stopwords.words(\"english\")) \ndef ngram_vectorize(train_texts, train_labels, val_texts):\n    kwargs = {\n        'ngram_range' : (1, 2),\n        'strip_accents' : 'unicode',\n        'dtype' : 'int32',\n        'decode_error' : 'replace',\n        'analyzer' : 'word',\n        'min_df' : 1,\n    }\n    \n    tfidf_vectorizer = TfidfVectorizer(**kwargs, stop_words = stopwords, sublinear_tf=True)\n    x_train = tfidf_vectorizer.fit_transform(train_texts)\n    x_val = tfidf_vectorizer.transform(val_texts)\n    \n    selector = SelectKBest(f_classif, k=min(6000, x_train.shape[1]))\n    selector.fit(x_train, train_labels)\n    x_train = selector.transform(x_train).astype('float32')\n    x_val = selector.transform(x_val).astype('float32')\n    return x_train, x_val","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_bag_train, df_bag_test = ngram_vectorize(Xtrain, Ytrain, Xtest)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df_bag_train[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nb = MultinomialNB()\nnb.fit(df_bag_train, Ytrain)\nnb_pred = nb.predict(df_bag_test)\nprint('Accuracy ',accuracy_score(Ytest, nb_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"------------------------------------------------------------------------------------------------------------------------------------------------------\n------------------------------------------------------------------------------------------------------------------------------------------------------\n*Word embeddings*"},{"metadata":{"trusted":true},"cell_type":"code","source":"indexes_train = np.where(data['type'] == 'train')\nindexes_test = np.where(data['type'] == 'test')\n\nX_train = data['review'][indexes_train[0]].values\nY_train = data['label'][indexes_train[0]].values\n\nX_test = data['review'][indexes_test[0]].values\nY_test = data['label'][indexes_test[0]].values\n\nindex_unsup = np.where(Y_train == 'unsup')\nY_train = np.delete(Y_train, index_unsup)\nX_train = np.delete(X_train, index_unsup)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"le = preprocessing.LabelEncoder()\nle.fit(Y_train)\n\nY_train_encod = le.transform(Y_train) \nY_test_encod = le.transform(Y_test) \n\nX_train, Y_train_encod = shuffle(X_train, Y_train_encod)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_features = 10000\nmaxlen = 100\nembedding_dimenssion = 100\n\nVALIDATION_SPLIT = 0.1\nCLASSES = 1\nNB_EPOCH = 20\nBATCH_SIZE = 64\nOPTIMIZER = Adam(lr=0.001)\n\n# Tokenization and encoding text corpus\ntk = Tokenizer(num_words=max_features)\ntk.fit_on_texts(X_train)\nX_train_en = tk.texts_to_sequences(X_train)\nX_test_en = tk.texts_to_sequences(X_test)\n\nword2index = tk.word_index\nindex2word = tk.index_word","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_new = sequence.pad_sequences(X_train_en, maxlen=maxlen)\nX_test_new = sequence.pad_sequences(X_test_en, maxlen=maxlen)\n\nglove_dir = ''.join(['../input/glove6b/glove.6B.', str(embedding_dimenssion),'d.txt'])\n\nembeddings_index = {}\n\nwith open(glove_dir, encoding='utf-8') as f:\n    for line in f:\n        values = line.split()\n        word = values[0]\n        embedding = np.asarray(values[1:], dtype='float32')\n        embeddings_index[word] = embedding \n        \nprint('Found {:,} word vectors in GloVe.'.format(len(embeddings_index)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_matrix = np.zeros((max_features, embedding_dimenssion))\n\nfor word, i in word2index.items():\n    if i >= max_features:\n        break\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\n\nmodel.add(Embedding(max_features, embedding_dimenssion, input_length=maxlen,\n                    weights=[embedding_matrix], trainable=False))\nmodel.add(LSTM(125, dropout=0.2, recurrent_dropout=0.2))\nmodel.add(Dense(1, activation=\"sigmoid\"))\nmodel.summary()\n\n\nmodel.compile(loss='binary_crossentropy', optimizer=OPTIMIZER, metrics=['accuracy'])\n\nmodel.fit(X_train_new, Y_train_encod, batch_size=BATCH_SIZE, epochs=10, validation_split=VALIDATION_SPLIT, verbose=1)\n\nscores = model.evaluate(X_test_new, Y_test_encod)\nprint('losses: {}'.format(scores[0]))\nprint('TEST accuracy: {}'.format(scores[1]))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}