{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # l|inear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tqdm\nfrom tqdm import tqdm\nfrom keras_tqdm import TQDMNotebookCallback\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport pandas as pd\nimport keras\nimport tensorflow as tf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Model,Sequential\nfrom keras.layers import GRU, Input, Dense, TimeDistributed, Activation, RepeatVector, Bidirectional, Dropout, LSTM \nfrom keras.layers.embeddings import Embedding\nfrom keras.optimizers import Adam ,Adagrad ,RMSprop\nfrom keras.losses import sparse_categorical_crossentropy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_d=pd.read_csv('../input/english-to-french/small_vocab_en.csv',sep=\"\\t\",header=None,names=['Text'])\ntrain_d=train_d.rename(columns={'0':'En_text'})\ntest_d=pd.read_csv('../input/english-to-french/small_vocab_fr.csv',sep=\"\\t\",header=None,names=['Text'])\ntest_d=test_d.rename(columns={'0':'Fr_text'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_d.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Add multiprocessing/threading to make this nigga faster\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_stopwords(sentence,language='english'):\n    \"\"\"\n    Parameters\n    ----------\n    sentence : string\n        String containing text\n    language : str, optional\n        The language code to be used for filtering stopwords. The default is 'english'.\n\n    Raises\n    ------\n    TypeError\n        When wrong input to the fucntion.\n\n    Returns\n    -------\n    filtered_sentence : str\n        The filtered sentence\n\n    \"\"\"\n\n    if sentence is type(str):\n            import nltk\n            nltk.download('stopwords')\n            from nltk.corpus import stopwords\n            from nltk import word_tokenize\n            stop_words = stopwords.words(language)\n            word_tokens = word_tokenize(sentence) \n            filtered_sentence =' '.join(map(str,[w for w in word_tokens if not w in stop_words]))\n            return filtered_sentence\n    else:\n        raise TypeError(\"Expected type str\")\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def words_distribution(dataframe,topx=10,stopwords=False,stopwordslist=None):\n    \"\"\"\n    \n\n    Parameters\n    ----------\n    dataframe : pd.Dataframe\n        The dataframe containing text.\n    topx : int, optional\n        Display the top n words in the given distribution. The default is 10.\n    stopwords : bool, optional\n        Stopwords in various lagnuages. The default is False.\n    stopwordslist : TYPE, optional\n        DESCRIPTION. The default is None.\n\n    Raises\n    ------\n    TypeError\n        When wrong type of iterable is entered in the given fucntion\n\n    Returns\n    -------\n    None.\n\n    \"\"\"\n    \n    if  isinstance(dataframe,pd.DataFrame):\n        if topx is not type(int):\n            from nltk import FreqDist\n            from nltk import word_tokenize\n            sens=[sen[0] for sen in dataframe.values if sen!=']' or sen!='[']\n            sens=[''.join(sen[0].lower()) for sen in dataframe.values if sen!=']' or sen!='[']\n            words=word_tokenize(str([sens[i] for i in range(len(sens))]))\n            freq=FreqDist(words)\n            freq.plot(topx)\n        else:\n                 raise TypeError(\"Expected type int got type {0}\".format(type(topx)))\n                \n    else:\n        raise TypeError(\"Expected type pd.DataFrame got type {0}\".format(type(dataframe)))          ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Usage defined below"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#words_distribution(dataframe=test_d)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Steps to be performed :\n**Tokenizing**\n**Paddding to prevent length issues.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def tokenize(dataframe,char_level=False):\n    \"\"\"\n    Parameters\n    ----------\n    dataframe : pd.Dataframe\n        Dataframe to generate tokens\n    char_level : bool\n        Create character level or word level tokens. The default is False.\n\n    Raises\n    ------\n    TypeError\n        When character level is not type bool\n\n    Returns\n    -------\n    text_sequences : list\n        A list containing the tokenized vocabulary\n    tk : keras_preprocessing.text.Tokenizer\n       Keras tokenizer\n\n    \"\"\"\n    if  isinstance(dataframe,pd.DataFrame):\n        data=np.array(dataframe.values).ravel()\n        if char_level is not type(bool):\n                from keras.preprocessing.text import Tokenizer\n                if char_level==False:\n                    tk=Tokenizer(lower=True ,char_level=False)\n                    tk.fit_on_texts(data)\n                    text_sequences=tk.texts_to_sequences(data)\n                else:\n                    tk=Tokenizer(lower=True ,char_level=True)\n                    tk.fit_on_texts(data)\n                    text_sequences=tk.texts_to_sequences(data)  \n                return text_sequences,tk\n        else:\n            TypeError(\"Expected type bool got type {0}\".format(type(topx)))\n    else:\n        raise TypeError(\"Expected type pd.DataFrame got type {0}\".format(type(dataframe))) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def pad(x, length=None):\n    \"\"\"\n    Pad x\n    :param x: List of sequences.\n    :param length: Length to pad the sequence to.  If None, use length of longest sequence in x.\n    :return: Padded numpy array of sequences\n    \"\"\"\n    from keras.preprocessing.sequence import pad_sequences\n    return pad_sequences(x, maxlen=length, padding='post',value=0.0) #Pad at the end","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sequences,tokenizer=tokenize(test_d,char_level=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(tokenizer.word_index)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Verifying padding"},{"metadata":{"trusted":true},"cell_type":"code","source":"padseq=pad(sequences,10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Created a list of sequences with number\n\nCreates dictionary of** * Word *:*Frequency***\n\n**Note:0 is reserved for Padding**\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess(x, y):\n    \"\"\"\n    Preprocess x and y\n    :param x: Feature List of sentences\n    :param y: Label List of sentences\n    :return: Tuple of (Preprocessed x, Preprocessed y, x tokenizer, y tokenizer)\n    \"\"\"\n    preprocess_x, x_tk = tokenize(x)\n    preprocess_y, y_tk = tokenize(y)\n    \n    preprocess_x = pad(preprocess_x)\n    preprocess_y = pad(preprocess_y)\n\n    # Keras's sparse_categorical_crossentropy function requires the labels to be in 3 dimensions\n    preprocess_y = preprocess_y.reshape(*preprocess_y.shape, 1)\n\n    return preprocess_x, preprocess_y, x_tk, y_tk","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preproc_english_sentences, preproc_french_sentences, english_tokenizer, french_tokenizer =\\\n    preprocess(train_d,test_d)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preproc_french_sentences.shape[::]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Visualize the English vocabulary"},{"metadata":{"trusted":true},"cell_type":"code","source":"english_tokenizer.word_index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def logits_to_text(logits, tokenizer):\n    \"\"\"\n    Turn logits from a neural network into text using the tokenizer\n    :param logits: Logits from a neural network\n    :param tokenizer: Keras Tokenizer fit on the labels\n    :return: String that represents the text of the logits\n    \"\"\"\n    index_to_words = {id: word for word, id in tokenizer.word_index.items()}\n    index_to_words[0] = '<PAD>'\n\n    return ' '.join([index_to_words[prediction] for prediction in np.argmax(logits, 1)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"padding_english_sent = pad(preproc_english_sentences,preproc_french_sentences.shape[1])\npadding_english_sent= padding_english_sent.reshape((-1, preproc_french_sentences.shape[-2], 1)) #Both should be of same dimensions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"padding_english_sent.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"english_vocab_size = len(english_tokenizer.word_index)\nfrench_vocab_size = len(french_tokenizer.word_index)\n#print(\"Prediction:\")\n#print(logits_to_text(model.predict(np.ndarray('How are you doing'), french_tokenizer)))\n#padding_english_sent.shape\npreproc_french_sentences.shape\n#padding_english_sent[:1][0]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def gru_rnn_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):   \n        model = Sequential()\n        model.add(GRU(256, input_shape=input_shape[1:], return_sequences=True))\n        model.add(TimeDistributed(Dense(1024, activation='tanh')))\n        model.add(Dropout(0.5))\n        model.add(TimeDistributed(Dense(french_vocab_size, activation='softmax'))) \n\n        # Compile model\n        model.compile(loss=sparse_categorical_crossentropy,\n                      optimizer=Adam(0.0054),\n                      metrics=['accuracy','sparse_categorical_crossentropy'])\n        return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nmodel = gru_rnn_model(padding_english_sent.shape,preproc_french_sentences.shape[1],len(english_tokenizer.word_index),len(french_tokenizer.word_index))\nprint(model.summary())\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Trying Early Stopping**\n\nHowever,has no effect since loss is high and also not overfitting as much. An overall bad model tbh."},{"metadata":{"trusted":true},"cell_type":"code","source":"padding_english_sent.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.callbacks.callbacks import EarlyStopping\nes=EarlyStopping(monitor='val_accuracy',mode='max',patience=5) #Stop when model loss cannot reach a greater minmum value","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history=model.fit(padding_english_sent, preproc_french_sentences, batch_size=1024, epochs=30,validation_split=0.2,callbacks=None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_model_performance(*criteria):\n    for c in criteria:\n            fig, axes= plt.subplots()\n            axes.plot(history.history[''+c])\n            axes.plot(history.history['val_'+c])\n            axes.set_title('Model '+c)\n            axes.set_ylabel(''+c)\n            axes.set_xlabel('Epoch')\n            axes.legend(['Train', 'Test'], loc='upper right')\n    return axes\n          \n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**GRU PLOT**\n![](http://wikimedia.org/api/rest_v1/media/math/render/svg/12dd26f9c68a2dc8aab60bb9627d9440d4e6952b)\n**Original Paper**\n                    [https://arxiv.org/pdf/1406.1078.pdf](http://)"},{"metadata":{"trusted":true},"cell_type":"code","source":"gru=plot_model_performance('accuracy','loss')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save('my_model.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def lstm_rnn_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):   \n    model = Sequential()\n    model.add(LSTM(256, input_shape=input_shape[1:], return_sequences=True))\n    model.add(TimeDistributed(Dense(1024, activation='relu')))\n    #model.add(Dropout(0.5))\n    model.add(TimeDistributed(Dense(french_vocab_size, activation='softmax'))) \n\n    # Compile model\n    model.compile(loss=sparse_categorical_crossentropy,\n                  optimizer=Adam(0.0054),\n                  metrics=['accuracy','sparse_categorical_crossentropy'])\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = lstm_rnn_model(padding_english_sent.shape,preproc_french_sentences.shape[1],len(english_tokenizer.word_index),len(french_tokenizer.word_index))\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history=model.fit(padding_english_sent, preproc_french_sentences, batch_size=1024, epochs=30,validation_split=0.2,callbacks=[TQDMNotebookCallback()])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**LSTM PLOT**\n![](https://wikimedia.org/api/rest_v1/media/math/render/svg/2db2cba6a0d878e13932fa27ce6f3fb71ad99cf1)\n\nAlso note GRU is faster to train than LSTM due to reduced gates and also less number of Parameters(813K vs 879K)"},{"metadata":{"trusted":true},"cell_type":"code","source":"axes=plot_model_performance('accuracy','loss')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"** Model with Embedding**\n"},{"metadata":{},"cell_type":"markdown","source":"THIS PADDING STEP IS A BIT DIFFERENT FROM  OTHER MODELS"},{"metadata":{"trusted":true},"cell_type":"code","source":"padding_english_sent= pad(preproc_english_sentences, preproc_french_sentences.shape[1])\npadding_english_sent.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Still use tanh "},{"metadata":{"trusted":true},"cell_type":"code","source":"def gru_embedding_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):   \n    model = Sequential()\n    model.add(Embedding(input_dim=english_vocab_size+1,output_dim=128,input_length=input_shape[1:][0]))\n    model.add(GRU(256, input_shape=input_shape[1:], return_sequences=True,recurrent_dropout=True))\n    model.add(TimeDistributed(Dense(1024, activation='tanh')))\n    model.add(Dropout(0.6))\n    model.add(TimeDistributed(Dense(french_vocab_size, activation='softmax'))) \n\n    # Compile model\n    model.compile(loss=sparse_categorical_crossentropy,\n                  optimizer=Adam(0.001),\n                  metrics=['accuracy','sparse_categorical_crossentropy'])\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"modele = gru_embedding_model(padding_english_sent.shape,preproc_french_sentences.shape[1],len(english_tokenizer.word_index),len(french_tokenizer.word_index))\nmodele.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history=modele.fit(padding_english_sent,preproc_french_sentences, batch_size=1024, epochs=35,validation_split=0.2,callbacks=[TQDMNotebookCallback()])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"axes=plot_model_performance('accuracy','loss')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Word Embeddings Visualized"},{"metadata":{"trusted":true},"cell_type":"code","source":"weights=np.asarray(modele.layers[0].get_weights())\nW=weights.reshape(200,128)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nsc=StandardScaler()\nWnorm=sc.fit_transform(W)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"l=list(english_tokenizer.word_index.keys())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Embedding Layer Visualized"},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfig, ax = plt.subplots(figsize=(40,30))  \nax.set_yticklabels(english_tokenizer.word_index.keys()) \nax.set_xticklabels(np.arange(0,128))\nsns.heatmap(Wnorm,cmap=\"Accent_r\",linewidths=1,square=True,ax=ax)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nsns.clustermap(W,cmap='Accent',metric=\"cosine\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from mpl_toolkits.mplot3d import axes3d\nfrom sklearn.manifold import TSNE\nimport seaborn as sns\ntsne=TSNE(n_components=2, perplexity=5,  learning_rate=200.0, n_iter=10000, metric='cosine', verbose=1,random_state=24)\nX=tsne.fit_transform(W)\nwords = list(english_tokenizer.word_index.keys())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib widget","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.manifold import TSNE\nimport seaborn as sns\ntsne=TSNE(n_components=3, perplexity=5,  learning_rate=200.0, n_iter=10000, metric='cosine', verbose=1,random_state=24)\nX=tsne.fit_transform(W)\nwords = list(english_tokenizer.word_index.keys())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from mpl_toolkits import mplot3d\nax = plt.axes(projection='3d')\nx=X[:,0]\ny=X[:,1]\nz=X[:,2]\n\nax.scatter(x,y,z,c=z,cmap='hsv')\nfor x,y,z,i in zip(x,y,z,range(len(words))):\n    ax.text(x,y,z,words[i])\n#for i, word in enumerate(words):\n#    plt.annotate(word, xy=(X[i, 0], X[i, 1]))\n#plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Now final layer weights"},{"metadata":{"trusted":true},"cell_type":"code","source":"FrW=modele.layers[4].weights[0]\nW=FrW.numpy()\nWfr=W.reshape(344,1024)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nfig, axes = plt.subplots(figsize=(80,70))  \nsns.heatmap(Wfr,linewidths=1,square=True,ax=axes)\naxes.set_yticklabels(list(french_tokenizer.word_index.keys())) \n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nsns.clustermap(Wfr,cmap='Accent',metric=\"cosine\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This honestly made no sense.\n- So let us try and visualize them in 3D"},{"metadata":{"trusted":true},"cell_type":"code","source":"tsne=TSNE(n_components=3, perplexity=5,  learning_rate=200.0, n_iter=10000, metric='cosine', verbose=1,random_state=24)\nXfr=tsne.fit_transform(Wfr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib widget","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Basically trying to interpret the model by visualizing the last layer"},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = plt.axes(projection='3d')\nx=Xfr[:,0]\ny=Xfr[:,1]\nz=Xfr[:,2]\nwordsfr=list(french_tokenizer.word_index.keys())\nax.scatter(x,y,z,c=z,cmap='hsv')\nfor x,y,z,i in zip(x,y,z,range(len(wordsfr))):\n    ax.text(x,y,z,wordsfr[i])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# PreTrained Word Embeddings"},{"metadata":{},"cell_type":"markdown","source":"Description of Pre-Trained Embeddings\n> The pre-trained Google word2vec model was trained on Google news data (about 100 billion words); it contains 3 million words and phrases and was fit using 300-dimensional word vectors.\n\nI try to use Word2Vec pretrained embeddings from google in custom GRU model. Then compare these Word2Vec embeddings to the embeddings created by older GRU model"},{"metadata":{},"cell_type":"markdown","source":"Thx to KerasDocs for explaining how tio load them .\nWas particulary keen on Gensim having used them b4"},{"metadata":{"trusted":true},"cell_type":"code","source":"from gensim.models.keyedvectors import KeyedVectors\nfilename = '/kaggle/input/googlevec/GoogleNews-vectors-negative300.bin'\nmodel = KeyedVectors.load_word2vec_format(filename, binary=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_Vector(str):\n    if str in model:\n             return model[str][:256]\n    else:\n        return None","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"s=len(english_tokenizer.word_index)+1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_matrix = np.zeros((s,256))\nfor word, i in english_tokenizer.word_index.items():\n         embedding_vector = get_Vector(word)\n         if embedding_vector is not None:\n                 embedding_matrix[i] = embedding_vector\n        \n            ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.initializers import Constant","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def gru_pretrained_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n    model = Sequential()\n    model.add(Embedding(input_dim=english_vocab_size+1,output_dim=256,input_length=input_shape[1:][0],embeddings_initializer=Constant(embedding_matrix),trainable=False))\n    model.add(GRU(256, input_shape=input_shape[1:], return_sequences=True))\n    model.add(TimeDistributed(Dense(1024, activation='tanh')))\n    model.add(Dropout(0.6))\n    model.add(TimeDistributed(Dense(french_vocab_size, activation='softmax'))) \n\n    # Compile model\n    model.compile(loss=sparse_categorical_crossentropy,\n                  optimizer=Adam(0.001),\n                  metrics=['accuracy','sparse_categorical_crossentropy'])\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"modele = gru_pretrained_model(padding_english_sent.shape,preproc_french_sentences.shape[1],len(english_tokenizer.word_index),len(french_tokenizer.word_index))\nmodele.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history=modele.fit(padding_english_sent,preproc_french_sentences, batch_size=1024, epochs=35,validation_split=0.2,callbacks=[TQDMNotebookCallback()])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"res=logits_to_text(modele.predict(padding_english_sent[:1])[0], french_tokenizer).replace('<PAD>','').strip()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"res","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(padding_english_sent[:1][0],end=\"\\n\\n\")\nprint(preproc_french_sentences[:1][0].reshape(1,21))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logits_to_text(preproc_french_sentences[:1][0].reshape(1,21),french_tokenizer)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"axes=plot_model_performance('accuracy','loss')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#weights=np.asarray(modele.layers[0].get_weights())\n#W2Vec=weights.reshape(200,128)\n#from sklearn.preprocessing import StandardScaler\n#sc=StandardScaler()\n#W2Vecnorm=sc.fit_transform(W2Vec)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#import matplotlib.pyplot as plt\n#import seaborn as sns\n#fig, ax = plt.subplots(figsize=(40,30))  \n#sns.heatmap(W2Vecnorm,cmap=\"Accent_r\",linewidths=1,square=True,ax=ax)\n#ax.set_yticklabels(list(english_tokenizer.word_index.keys())) \n#ax.set_xticklabels(np.arange(0,128))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#from sklearn.metrics.pairwise import cosine_similarity as cosine\n#list=[]\n#for i in tqdm(range(0,200)):\n#    list.append(cosine(Wnorm[i].reshape(1,128),W2Vecnorm[i].reshape(1,128)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# PART II\n\nAs we can clearly see that the loss metrics for both the GRU and LSTM RNN's are really high despite an approx accuracy around 82.xx%\nWith this aim I try and explore other models which can further reduce the SPRSCATCRENTRPY Loss\n\n## BiGRU"},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"def bidirectional_gru_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):   \n    model = Sequential()\n    model.add(Embedding(input_dim=english_vocab_size+1,output_dim=128,input_length=input_shape[1:][0]))\n    model.add(Bidirectional(LSTM(256, input_shape=input_shape[1:], return_sequences=True,recurrent_dropout=0.2)))\n    model.add(TimeDistributed(Dense(1024, activation='tanh')))\n    model.add(Dropout(0.6))\n    model.add(TimeDistributed(Dense(french_vocab_size, activation='softmax'))) \n\n    # Compile model\n    model.compile(loss=sparse_categorical_crossentropy,\n                  optimizer=Adam(0.001),\n                  metrics=['accuracy','sparse_categorical_crossentropy'])\n    return model\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#modelBi=bidirectional_gru_model(padding_english_sent.shape,preproc_french_sentences.shape[1],len(english_tokenizer.word_index),len(french_tokenizer.word_index))\n#modelBi.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#history=modelBi.fit(padding_english_sent,preproc_french_sentences, batch_size=1024, epochs=35,validation_split=0.2,callbacks=[TQDMNotebookCallback()])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#axes=plot_model_performance('accuracy','loss')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#modelBi.save('BILSTMRNN.h5')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Part III \nEncoder-Decoder models .\nConsisting of a rnn acting as encoder and another acting as decoder"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nimport tensorflow\ntensorflow.random.set_seed(1)\n\nfrom numpy.random import seed\nseed(1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef encdec_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n    embedding_size = 128\n    rnn_cells = 200\n    dropout = 0.0\n    learning_rate = 1e-3\n    from keras.layers import LSTM\n    encoder_input_seq = Input(shape=input_shape[1:], name=\"enc_input\")\n \n    # Encoder (Return the internal states of the RNN -> 1 hidden state for GRU cells, 2 hidden states for LSTM cells))\n    encoder_output, state_t = GRU(units=rnn_cells, \n                                  dropout=dropout,\n                                  return_sequences=False,\n                                  return_state=True,\n                                  name=\"enc_rnn\")(encoder_input_seq)\n          #or for LSTM cells: encoder_output, state_h, state_c = LSTM(...)\n        \n    # Decoder Input   \n    decoder_input_seq = RepeatVector(output_sequence_length)(encoder_output)\n\n    # Decoder RNN (Take the encoder returned states as initial states)\n    decoder_out = GRU(units=rnn_cells,\n                      dropout=dropout,\n                      return_sequences=True,\n                      return_state=False)(decoder_input_seq, initial_state=state_t)\n                                         #or for LSTM cells: (decoder_input_seq, initial_state=[state_h, state_c])\n    \n    # Decoder output \n    logits = TimeDistributed(Dense(units=french_vocab_size))(decoder_out) \n    \n    # Model\n    model = Model(encoder_input_seq, Activation('softmax')(logits))\n    model.compile(loss=sparse_categorical_crossentropy,\n                  optimizer=Adam(lr=learning_rate),\n                  metrics=['accuracy'])\n     \n    return model    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tmp_x = pad(preproc_english_sentences,preproc_french_sentences.shape[1])\ntmp_x = tmp_x.reshape((-1, preproc_french_sentences.shape[-2], 1))\nencdec_rnn_model = encdec_model(input_shape = tmp_x.shape,\n                                output_sequence_length =preproc_french_sentences.shape[1],\n                                english_vocab_size = english_vocab_size+1,\n                                french_vocab_size = french_vocab_size+1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encdec_rnn_model.fit(tmp_x, preproc_french_sentences, batch_size=1024, epochs=10, validation_split=0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}