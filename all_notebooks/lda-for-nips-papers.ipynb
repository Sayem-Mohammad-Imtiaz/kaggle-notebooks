{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\n\n# Bokeh\nfrom bokeh.io import output_notebook\nfrom bokeh.plotting import figure, show\nfrom bokeh.models import HoverTool, CustomJS, ColumnDataSource, Slider, Range1d\nfrom bokeh.layouts import column\nfrom bokeh.palettes import all_palettes\noutput_notebook()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1. Loading data\nWe load docs from [NIPS Papers](https://www.kaggle.com/benhamner/nips-papers) dataset."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"np.random.seed(42)\ndf = pd.read_csv(\"../input/papers.csv\")\nprint(df.paper_text[0][:500] + ' ...')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Lemmatization\n\nApply lemmatization `spaCy` [framework](https://spacy.io/). **Lemmatization** is the redusing a word to its \"dictionary form\" (word's *lemma*). "},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nimport spacy\n\nnlp = spacy.load('en', disable=['parser', 'ner'])\ndf['paper_text_lemma'] = df.paper_text.map(lambda x: [token.lemma_ for token in nlp(x) if token.lemma_ != '-PRON-' and token.pos_ in {'NOUN', 'VERB', 'ADJ', 'ADV'}])\n\n# Final cleaning\ndf['paper_text_lemma'] = df.paper_text_lemma.map(lambda x: [t for t in x if len(t) > 1])\n\n# Example\nprint(df['paper_text_lemma'].iloc[0][:25], end='\\n\\n')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. TFIDF and UMAP\n\nConstructing [TFIDF-matrix](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)."},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nn_features=2000\ntfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, max_features=n_features, ngram_range=(1,2), stop_words='english')\ntfidf = tfidf_vectorizer.fit_transform(df.paper_text_lemma.map(lambda x: ' '.join(x)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nimport umap\n\numap_embr = umap.UMAP(n_neighbors=10, metric='cosine', min_dist=0.1, random_state=42)\nembedding = umap_embr.fit_transform(tfidf.todense())\nembedding = pd.DataFrame(embedding, columns=['x','y'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4. Gensim LDA model and Coherence\n\nLet's organize the text into a datastructure sutable for `gensim` [LDA](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation) model."},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfrom nltk.corpus import stopwords\n\nstop_en = stopwords.words('english')\ndf['paper_text_lemma'] = df.paper_text_lemma.map(lambda x: [t for t in x if t not in stop_en]) \nprint(df['paper_text_lemma'].iloc[0][:25])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfrom gensim import corpora, models\nnp.random.seed(42)\n\n# Create a corpus from a list of texts\ntexts = df.sample(n=1500, random_state=43)['paper_text_lemma'].values\ndictionary = corpora.Dictionary(texts)\ncorpus = [dictionary.doc2bow(text) for text in texts]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfrom gensim.models.ldamodel import LdaModel\nfrom gensim.models.coherencemodel import CoherenceModel\n\nmax_topics = 30\ncoh_list = []\nfor n_topics in range(3,max_topics+1):\n    # Train the model on the corpus\n    my_lda = LdaModel(corpus, num_topics=n_topics, id2word=dictionary, random_state=42, alpha=0.1)\n    # Estimate coherence\n    cm = CoherenceModel(model=my_lda, texts=texts, dictionary=dictionary, coherence='c_v', topn=20)\n    coherence = cm.get_coherence_per_topic() # get coherence value\n    coh_list.append(coherence)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let plot the coherence scores and guess the number of topics. First, we calculate mean score and the standard deviation for each model. The blue line shows the means and the green region represents the standard deviations."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Coherence scores:\ncoh_means = np.array([np.mean(l) for l in coh_list])\ncoh_stds = np.array([np.std(l) for l in coh_list])\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.xticks(np.arange(3, max_topics+1, 3.0));\nplt.plot(range(3,max_topics+1), coh_means);\nplt.fill_between(range(3,max_topics+1), coh_means-coh_stds, coh_means+coh_stds, color='g', alpha=0.05);\nplt.vlines([8, 9], 0.24, 0.26, color='red', linestyles='dashed',  linewidth=1);\nplt.hlines([0.253], 3, max_topics, color='black', linestyles='dotted',  linewidth=0.5);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5. LDA model in details"},{"metadata":{},"cell_type":"markdown","source":"### 5.1 LDA-6"},{"metadata":{},"cell_type":"markdown","source":"#### 5.1.1. Topics"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfrom gensim import corpora, models\nnp.random.seed(42)\n\n# Create a corpus from a list of texts\ntexts = df['paper_text_lemma'].values\ndictionary = corpora.Dictionary(texts)\ncorpus = [dictionary.doc2bow(text) for text in texts]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nn_topics=9\nn_top_words = 25\nmy_lda = LdaModel(corpus, num_topics=n_topics, id2word=dictionary, random_state=42, minimum_probability=0)\nfor index, topic in my_lda.show_topics(formatted=False, num_words= n_top_words):\n        print('Topic: {} \\nWords: {}'.format(index, [w[0] for w in topic]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"topics = ['T{}'.format(i) for i in range(n_topics)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hm = np.array([[y for (x,y) in my_lda[corpus[i]]] for i in range(len(corpus))])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding['hue'] = hm.argmax(axis=1)\nmy_colors = [(all_palettes['Category20'][20] + all_palettes['Category20'][20])[i] for i in embedding.hue]\nsource = ColumnDataSource(\n        data=dict(\n            x = embedding.x,\n            y = embedding.y,\n            colors = my_colors,\n            topic = [topics[i] for i in embedding.hue],\n            title = df.title,\n            year = df.year,\n            alpha = [0.7] * embedding.shape[0],\n            size = [7] * embedding.shape[0]\n        )\n    )\nhover_emb = HoverTool(names=[\"df\"], tooltips=\"\"\"\n    <div style=\"margin: 10\">\n        <div style=\"margin: 0 auto; width:300px;\">\n            <span style=\"font-size: 12px; font-weight: bold;\">Topic:</span>\n            <span style=\"font-size: 12px\">@topic</span>\n            <span style=\"font-size: 12px; font-weight: bold;\">Title:</span>\n            <span style=\"font-size: 12px\">@title</span>\n            <span style=\"font-size: 12px; font-weight: bold;\">Year:</span>\n            <span style=\"font-size: 12px\">@year</span>\n        </div>\n    </div>\n    \"\"\")\ntools_emb = [hover_emb, 'pan', 'wheel_zoom', 'reset']\nplot_emb = figure(plot_width=700, plot_height=700, tools=tools_emb, title='Papers')\nplot_emb.circle('x', 'y', size='size', fill_color='colors', \n                 alpha='alpha', line_alpha=0, line_width=0.01, source=source, name=\"df\", legend='topic')\n\nplot_emb.legend.location = \"bottom_left\"\nplot_emb.legend.label_text_font_size= \"8pt\"\nplot_emb.legend.spacing = -5\nplot_emb.x_range = Range1d(-9, 7)\nplot_emb.y_range = Range1d(-9, 7)\n\ncallback = CustomJS(args=dict(source=source), code=\n    \"\"\"\n    var data = source.data;\n    var f = cb_obj.value\n    x = data['x']\n    y = data['y']\n    colors = data['colors']\n    alpha = data['alpha']\n    title = data['title']\n    year = data['year']\n    size = data['size']\n    for (i = 0; i < x.length; i++) {\n        if (year[i] <= f) {\n            alpha[i] = 0.9\n            size[i] = 7\n        } else {\n            alpha[i] = 0.05\n            size[i] = 4\n        }\n    }\n    source.change.emit();\n    \"\"\")\n\nslider = Slider(start=df.year.min()-1, end=df.year.max(), value=2016, step=1, title=\"Before year\")\nslider.js_on_change('value', callback)\n\nlayout = column(slider, plot_emb)\nshow(layout)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\n%matplotlib inline\n\nlegend_list = []\nfor color in all_palettes['Category20'][20][:n_topics]:   \n    legend_list.append(mpatches.Ellipse((0, 0), 1, 1, fc=color))\n    \nfig,ax = plt.subplots(figsize=(12,13))\nax.scatter(embedding.x, embedding.y, c=my_colors, alpha=0.7)\nax.set_title('6 topics found via NMF');\nfig.legend(legend_list, topics, loc=(0.18,0.87), ncol=3)\nplt.subplots_adjust(top=0.82)\nplt.suptitle(\"NIPS clustered by topic\", **{'fontsize':'14','weight':'bold'});\nplt.figtext(.51,0.95, 'topic modeling with NMF + 2D-embedding with UMAP', \n            **{'fontsize':'12','weight':'light'}, ha='center');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pyLDAvis.gensim\npyLDAvis.enable_notebook()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pyLDAvis.gensim.prepare(my_lda, corpus, dictionary)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}