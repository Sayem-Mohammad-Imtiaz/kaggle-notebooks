{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Решение\nСунгатуллин Руслан tg: @sunruslan"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport matplotlib.pyplot as plt\nfrom IPython.display import clear_output\nimport spacy\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\nfrom collections import Counter\nimport spacy\nfrom tqdm import tqdm, tqdm_notebook, tnrange\nimport seaborn as sns\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\nfrom torch.autograd import Variable\n\nSEED = 43\nnp.random.seed(SEED)\ntqdm.pandas(desc='Progress')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"columns = [\"text\", \"parent_text\", \"score\"]\ndf = pd.concat([\n    pd.read_csv(\"../input/reddit-comment-score-prediction/comments_positive.csv\", usecols=columns, na_filter=False),\n    pd.read_csv(\"../input/reddit-comment-score-prediction/comments_negative.csv\", usecols=columns, na_filter=False)\n], ignore_index=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = df['score']\ndf.drop(columns='score', inplace=True)\nX = df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05, random_state=SEED)\n\n# To be sure we don't use indices to predict something\nX_train.reset_index(drop=True, inplace=True)\nX_test.reset_index(drop=True, inplace=True)\ny_train.reset_index(drop=True, inplace=True)\ny_test.reset_index(drop=True, inplace=True)\n\nprint(\"Train shape: {}\".format(X_train.shape))\nprint(\"Test shape: {}\".format(X_test.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nlp = spacy.load('en',disable=['parser', 'tagger', 'ner'])\nX_train['text'] = X_train.text.progress_apply(lambda x: x.strip())\nX_train['parent_text'] = X_train.parent_text.progress_apply(lambda x: x.strip())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"words = Counter()\nfor sent in tqdm(X_train.values):\n    words.update(w.text.lower() for w in nlp(sent[0]))\n    words.update(w.text.lower() for w in nlp(sent[1]))\n   \nwords = sorted(words, key=words.get, reverse=True)\nwords = ['_PAD','_UNK'] + words\n\nword2idx = {o:i for i,o in enumerate(words)}\nidx2word = {i:o for i,o in enumerate(words)}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def indexer(s): \n    return [word2idx[w.text.lower()] for w in nlp(s)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train, x_val, y_train, y_val = train_test_split(X_train, y_train, shuffle=False, test_size=0.1, random_state=SEED)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class DataframeDataset(Dataset):\n    \n    def __init__(self, X, y, maxlen=10):\n        self.maxlen = maxlen\n        self.X = X\n        self.y = y\n        self.X['text_idx'] = self.X.text.progress_apply(indexer)\n        self.X['text_lengths'] = self.X.text_idx.progress_apply(lambda x: self.maxlen if len(x) > self.maxlen else len(x))\n        self.X['text_padded'] = self.X.text_idx.progress_apply(self.pad_data)\n        self.X['parent_text_idx'] = self.X.parent_text.progress_apply(indexer)\n        self.X['parent_text_lengths'] = self.X.parent_text_idx.progress_apply(lambda x: self.maxlen if len(x) > self.maxlen else len(x))\n        self.X['parent_text_padded'] = self.X.parent_text_idx.progress_apply(self.pad_data)\n        \n    def __len__(self):\n        return len(self.X)\n    \n    def __getitem__(self, index):\n        row = self.X.iloc[index, :]\n        x1 = row.text_padded\n        l1 = row.text_lengths\n        x2 = row.parent_text_padded\n        l2 = row.parent_text_lengths\n        y = self.y[index]\n        return x1, l1, x2, l2, y\n    \n    def pad_data(self, s):\n        padded = np.zeros((self.maxlen,), dtype=np.int64)\n        if len(s) > self.maxlen: \n            padded[:] = s[:self.maxlen]\n        else: \n            padded[:len(s)] = s\n        return padded","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Net(nn.Module):\n    def __init__(self, vocab_size):\n        super(Net, self).__init__()\n        self.criterion = nn.MSELoss()\n        self.emb_dim = 128\n        self.hidden_size = 64\n        self.num_layers = 2\n        self.emb = nn.Embedding(vocab_size, self.emb_dim)\n        self.rnn_comment = nn.GRU(input_size=self.emb_dim, hidden_size=self.hidden_size, num_layers=self.num_layers, \n                          dropout=0.5,batch_first=True)\n        \n        self.rnn_parent = nn.GRU(input_size=self.emb_dim, hidden_size=self.hidden_size, num_layers=self.num_layers, \n                          dropout=0.5,batch_first=True)\n        self.model = nn.Sequential(\n            nn.Linear(2*self.hidden_size, 128), \n            nn.LeakyReLU(0.01), \n            nn.Dropout(0.1),\n            nn.Linear(128, 1)\n        )\n        \n        \n    def forward(x1, x1_lengths, x2, x2_lengths):\n        bs = x1.size(1)\n        embs = self.emb(seq)\n        embs = pack_padded_sequence(embs, x1_lengths)\n        self.h = self.init_hidden(bs) \n        _, h1 = self.rnn_comment(embs, self.h)\n        \n        bs = x2.size(1)\n        embs = self.emb(seq)\n        embs = pack_padded_sequence(embs, x2_lengths)\n        self.h = self.init_hidden(bs) \n        _, h2 = self.rnn_parent(embs, self.h)\n        \n        x = torch.cat((x1, x2), dim=1)\n        x = self.model(x)\n        return x\n    \n    def init_hidden(self, batch_size):\n        return Variable(torch.zeros((1,batch_size,self.hidden_size)))\n        \n    def loss(x1, x1_lengths, x2, x2_lengths, y):\n        prediction = self.forward(x1, x1_lengths, x2, x2_lengths)\n        return self.criterion(prediction, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BATCH_SIZE = 32\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntrain_dataset = DataframeDataset(x_train.head(5), y_train[:5])\ntrain_loader = DataLoader(train_dataset, shuffle=True, batch_size=BATCH_SIZE)\nval_dataset = DataframeDataset(x_val.head(5), y_val[:5])\nval_loader = DataLoader(val_dataset, shuffle=True, batch_size=BATCH_SIZE)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train(model, loader, optimizer, device):\n    model.train()\n    losses = []\n    for x1, x1_lengths, x2, x2_lengths, y in loader:\n        optimizer.zero_grad()\n        loss = model.loss(x1, x1_lengths, x2, x2_lengths, y)\n        loss.backward()\n        opimizer.step()\n        losses.append(loss.item())\n    return np.mean(losses)\n    \ndef test(model, loader, device):\n    model.eval()\n    losses = []\n    with torch.no_grad():\n        for x1, x1_lengths, x2, x2_lengths, y in loader:\n            prediction = model.forward(x1, x1_lengths, x2, x2_lengths)\n            losses.append(mean_squared_error(prediction.cpu().data.numpy(), y))\n    return np.mean(losses)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"epochs = 1000\nvocab_size = len(words)\nmodel = Net(vocab_size)\noptimizer = torch.optim.Adam(model.parameters())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_loss = 99999.0\nendure = 0\ntrain_history = []\nval_history = []\n\nfor e in range(epochs):\n    train_loss = train(model, train_loader, optimizer, device)\n    val_loss = test(model, val_loader, device)\n    \n    train_history.append(train_loss)\n    val_loss.append(val_loss)\n    \n    clear_output(True)\n    plt.plot(train_history, label=\"train\")\n    plt.plot(val_history, label=\"val\")\n    plt.legend()\n    plt.show()\n    \n    if best_loss > val_loss:\n        endure = 0\n        best_loss = val_loss\n    else:\n        endure += 1\n    if endure == 5:\n        break\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.eval()\ny_pred = []\ntest_dataset = DatframeDataset(X_test, y_test)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\nfor x1, x2, _ in test_loader:\n    x1, x2 = x1.to(device), x2.to(device)\n    prediction = model.forward(x1, x2)\n    y_pred.extend(list(prediction.cpu().data))\nmean_squared_error(y_test, y_pred)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}