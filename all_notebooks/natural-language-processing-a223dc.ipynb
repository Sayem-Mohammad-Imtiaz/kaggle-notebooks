{"cells":[{"metadata":{"_cell_guid":"2d518942-aa71-5320-baea-0e46170db6d0","_uuid":"d81fef313bca175dc3ebf88a5e52562725ff1758"},"cell_type":"markdown","source":"**This notebook gives basic NLP works like clustering data and data visualization.**"},{"metadata":{"_cell_guid":"aedc4651-27c4-c5f0-96dc-10f6e1e2f7f3","_uuid":"75bf4606ccfbf47d972a653f65916c8a2621ce4a","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd \nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom collections import Counter\nfrom subprocess import check_output\nfrom wordcloud import WordCloud, STOPWORDS\nimport re\nimport sys\nimport nltk\nfrom nltk.corpus import stopwords\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport nltk\nfrom nltk.stem.porter import *\nimport numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.cluster import KMeans\nfrom gensim.models import word2vec\n\nfrom sklearn.manifold import TSNE\nfrom sklearn import metrics\nimport pandas as pd \nimport sklearn\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics import jaccard_similarity_score\ncv = CountVectorizer()\nfrom nltk.corpus import stopwords\nfrom sklearn.metrics.pairwise import cosine_similarity\nstop = set(stopwords.words(\"english\"))\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom datetime import datetime\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input/obama-white-house.csv\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":65,"outputs":[]},{"metadata":{"_cell_guid":"92791eee-9270-6977-18b4-32e91a56c9cd","_uuid":"25fd0fa600c118db50fbdf9b5cdcb1bccafade23","trusted":true,"scrolled":true},"cell_type":"code","source":"data = pd.read_csv(\"../input/obama-white-house.csv\",nrows=1000)\n\ndata.head(100)","execution_count":19,"outputs":[]},{"metadata":{"_cell_guid":"032844f3-bc2d-d9e3-784b-c3bed3584137","_uuid":"7bfceb59cf7283a30b863dfc81261aa226d1cd91","trusted":true},"cell_type":"code","source":"stopwords = set(STOPWORDS)\nwordcloud_title = WordCloud(\n                          background_color='white',\n                          stopwords=stopwords,\n                          max_words=200,\n                          max_font_size=40, \n                          random_state=42\n                         ).generate(str(data['title']))\nplt.imshow(wordcloud_title)\nplt.axis('off')\nplt.title(\"Title\")","execution_count":16,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"203f1da812a95c62c4f4681127d39726ac856159"},"cell_type":"code","source":"print(wordcloud_title.words_)","execution_count":10,"outputs":[]},{"metadata":{"_cell_guid":"6e4f56d3-5571-ccbd-408f-4abbba1080d6","_uuid":"647ffb344183aced802b7e0d6cc8429bc2f53dda","trusted":true},"cell_type":"code","source":"stopwords = set(STOPWORDS)\nwordcloud_content = WordCloud(\n                          background_color='white',\n                          stopwords=stopwords,\n                          max_words=200,\n                          max_font_size=40, \n                          random_state=42\n                         ).generate(str(data['content']))\nplt.imshow(wordcloud_content)\nplt.axis('off')\nplt.title('Content')","execution_count":17,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6af3b3493ddb127a1bec11475f5c7a60226d95f1"},"cell_type":"code","source":"words = wordcloud_content.words_\nwords_top10 = list(words.keys())[1:11]\n\nword_dates = []\ncontent = data['content']\nfor word in words_top10:\n    dates = []\n    \n    for i in range(content.size):\n        string = content[i]\n        if string.find(word) != -1:\n            dates.append(data['document_date'][i])\n    \n    ##[datetime.date(x, '%Y-%m-%d') for x in dates] - ось тут виникають проблеми, дати не конвертуються\n    \n    word_dates.append(dates)","execution_count":76,"outputs":[]},{"metadata":{"_cell_guid":"9169b7d1-cb69-81a7-9f17-22009290149e","_uuid":"379318d624c9bddd538a045659e548838359c4d8"},"cell_type":"markdown","source":"**Data Cleaning**"},{"metadata":{"_cell_guid":"5febecf4-e199-0bfd-7874-5e371374a589","_uuid":"8688e56d34a3caab8cb37a7033328cf1d226ffe1","collapsed":true,"trusted":true},"cell_type":"code","source":"%%timeit\ndef cleaning(s):\n    s = str(s)\n    s = s.lower()\n    s = re.sub('\\s\\W',' ',s)\n    s = re.sub('\\W,\\s',' ',s)\n    s = re.sub(r'[^\\w]', ' ', s)\n    s = re.sub(\"\\d+\", \"\", s)\n    s = re.sub('\\s+',' ',s)\n    s = re.sub('[!@#$_]', '', s)\n    s = s.replace(\"co\",\"\")\n    s = s.replace(\"https\",\"\")\n    s = s.replace(\",\",\"\")\n    s = s.replace(\"[\\w*\",\" \")\n    return s\ndata['content'] = [cleaning(s) for s in data['content']]\ndata['title'] = [cleaning(s) for s in data['title']]\n\n\n#StopWordsRemove\n\n#data['content'] = data.apply(lambda row: nltk.word_tokenize(row['content']),axis=1)\n#data['title'] = data.apply(lambda row: nltk.word_tokenize(row['title']),axis=1)\n\n#data['content'] = data['content'].apply(lambda x : [item for item in x if item not in stop])\n#data['title'] = data['title'].apply(lambda x : [item for item in x if item not in stop])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"878dbd24-23aa-d5ad-72f6-51fab65fb086","_uuid":"aacad1bba3141bbe932c3dbd545af8adf23b273e"},"cell_type":"markdown","source":"**Tf-idf and Kmeans**"},{"metadata":{"_cell_guid":"2c67a0a7-2a1e-1e27-31f5-2f91cb96772b","_uuid":"1767f82c9bff177093e275aa1f0b08a550c94298","collapsed":true,"trusted":true},"cell_type":"code","source":"vectorizer = TfidfVectorizer(stop_words='english',use_idf=True)\nmodel = vectorizer.fit_transform(data['content'].str.upper())\nkm = KMeans(n_clusters=5,init='k-means++',max_iter=200,n_init=1)\n\nk=km.fit(model)\nterms = vectorizer.get_feature_names()\norder_centroids = km.cluster_centers_.argsort()[:,::-1]\nfor i in range(5):\n    print(\"cluster of words %d:\" %i)\n    for ind in order_centroids[i,:10]:\n        print(' %s' % terms[ind])\n    print() \n    ","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"8c7336c0-b6f8-0ba7-1b09-4981ebe8e2bf","_uuid":"d2d454e07dfbe1da1228b5350d24bf8c0ac96ee9"},"cell_type":"markdown","source":"**Building corpus from Title and Contents**"},{"metadata":{"_cell_guid":"29ce8bfe-9513-fe47-3112-6f5d753fd5a6","_uuid":"451de84bdf98806e59aefce0d33d99d0968cc7e6","collapsed":true,"trusted":true},"cell_type":"code","source":"def build_corpus(data):\n    \"Creates a list of lists containing words from each sentence\"\n    corpus = []\n    for col in ['title', 'content']:\n        for sentence in data[col].iteritems():\n            word_list = sentence[1].split(\" \")\n            corpus.append(word_list)\n            \n    return corpus\n\ncorpus = build_corpus(data)        \ncorpus[0:2]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"cf7dda9a-01e2-92ed-fc45-3ce305cba21d","_uuid":"62c95b0d27b2f7d85ec15f5273b0e114cb746f34"},"cell_type":"markdown","source":"**Words to Vector**"},{"metadata":{"_cell_guid":"c695e7f9-9cf7-1314-44ab-3f9c28653449","_uuid":"327d2e87542c387ff8e0e2b16d476fecb4a91a5f","collapsed":true,"trusted":true},"cell_type":"code","source":"\nmodel = word2vec.Word2Vec(corpus, size=100, window=20, min_count=400, workers=4)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"d4d08a4f-eca7-069c-695c-d7a0c03068aa","_uuid":"c4034f984732ef3ff0f67ea5e19101163a8d21d2","collapsed":true,"trusted":true},"cell_type":"code","source":"model.wv['states']","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"6f23a336-9a21-788e-3441-a0226dc99c02","_uuid":"fa5137d2068157c4ed19b5ee30faed67c665b12c"},"cell_type":"markdown","source":"**Data Visualization**"},{"metadata":{"_cell_guid":"77e5647c-5549-edcd-62c5-6f36e7845977","_uuid":"06db74161047407676dc66f5d5c0aa2ccd7d28fe","collapsed":true,"trusted":true},"cell_type":"code","source":"def tsne_plot(model):\n    \"Creates and TSNE model and plots it\"\n    labels = []\n    tokens = []\n\n    for word in model.wv.vocab:\n        tokens.append(model[word])\n        labels.append(word)\n    \n    tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2500, random_state=23)\n    new_values = tsne_model.fit_transform(tokens)\n\n    x = []\n    y = []\n    for value in new_values:\n        x.append(value[0])\n        y.append(value[1])\n        \n    plt.figure(figsize=(16, 16)) \n    for i in range(len(x)):\n        plt.scatter(x[i],y[i])\n        plt.annotate(labels[i],\n                     xy=(x[i], y[i]),\n                     xytext=(5, 2),\n                     textcoords='offset points',\n                     ha='right',\n                     va='bottom')\n    plt.show()\n\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f07672f6-e75e-0d9c-5322-4dbb00712e3f","_uuid":"5e34dc72b9a3ea47e9a76bfc68c059eb0e9a70c5","collapsed":true,"trusted":true},"cell_type":"code","source":"tsne_plot(model)","execution_count":null,"outputs":[]}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}