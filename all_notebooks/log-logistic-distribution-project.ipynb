{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# The Log-Logistic Distribution\n\nWelcome to our Kaggle project! Today, we will attempt to further our understanding of the log-logistic distribution by fiddling around with various sliders and graphs! You might need to click the large black button in the upper right-hand corner that says Save and Edit in order to get all the functionality out of this document. You'll need to run each cell in order. To run a cell, select it and press Shift+Enter simultaneously.\n\nBelow is just a lot of code, don't worry!\n\n","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # plotting library\nimport scipy\nimport random\n\nfrom scipy.stats import beta as betafunction # Just want to try something...\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"id":"cJDO986-I2RG","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"By the way, these are markdown cells – they're basically just presentable text. They will provide instructions to the readers (that's you) throughout the lesson. First, we need to run a bunch of cells to initialize our plotting.","metadata":{"id":"TCQJ1V0-I64D"}},{"cell_type":"code","source":"#Takes in an array x, and alpha, beta parameter values, and makes sure they are valid\ndef log_logistic_test_PDF(x, alpha, beta):\n    #The idiot loop\n    if beta <=0:\n        print(\"Beta is either 0, or less than 0 and that's not allowed. Stop it.\")\n        return False\n    if alpha <=0:\n        print(\"The funciton is undefined for values of alpha that are less than 0, or equal to 0. I'm going to have to ask you to stop.\")\n        return False\n    \n    pdf = []\n    for i in x:\n        if i<0:\n            print(\"Warning: cannot compute a negative probability\")\n        pdf.append(log_logistic_evalPDF(i, alpha, beta))\n    return pdf","metadata":{"id":"_RY6W81wI7z6","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Takes in an array x, and alpha, beta parameter values, and makes sure they are valid\ndef log_logistic_test_CDF(x, alpha, beta):\n    #The idiot loop\n    if beta <=0:\n        print(\"Beta is either 0, or less than 0 and that's not allowed. Stop it.\")\n        return False\n    if alpha <=0:\n        print(\"The funciton is undefined for values of alpha that are less than 0, or equal to 0. I'm going to have to ask you to stop.\")\n        return False\n    \n    cdf = []\n    for i in x:\n        if i<0:\n            print(\"Warning: cannot compute a negative probability\")\n        cdf.append(log_logistic_evalCDF(i, alpha, beta))\n    return cdf","metadata":{"id":"alcpFz3XI8_A","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Evaluates the PDF at a point, x\ndef log_logistic_evalPDF(x, alpha, beta):\n    if beta >=1:\n        val = ((beta/alpha)*(x/alpha)**(beta-1))/((1+(x/alpha)**beta)**2)\n    elif (beta > 0 and beta < 1):\n        # Treat discontinuities at 0\n        if(x==0):\n            return np.inf\n        else:\n            try:\n                power = 1-beta\n                val = ((beta/alpha))/((x/alpha)**(power)*(1+(x/alpha)**beta)**2)\n            except Error as err:\n                print(\"Caught an error: {0}. Returning infinity.\".format(err))\n                return np.inf\n    return val","metadata":{"id":"mx76H7IlI-MV","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import all the interaction widgets\nfrom __future__ import print_function\nfrom ipywidgets import interact, interactive, fixed, interact_manual\nimport ipywidgets as widgets","metadata":{"id":"HwfNmR8nI_Of","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Evaluates the CDF at a point, x\ndef log_logistic_evalCDF(x, alpha, beta): \n    if beta==0:\n        print(\"Error, beta cannot be equal to 0.\")\n        return\n    elif beta >= 1: \n        val=(x**beta)/((alpha**beta)+(x**beta))\n    else: # For beta between 0 and 1\n        if (x==0): # Account for numpy's weird behaviour at 0\n            return 0\n        else:\n            val=(x**beta)/((alpha**beta)+(x**beta))\n    return val","metadata":{"id":"sg3guR7jJAe3","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def moment(alpha, beta, n):\n    try:\n        if(beta > n):\n            return alpha**n*(n*np.pi/beta)/(np.sin(n*np.pi/beta))\n        else:\n            return np.inf\n    except Exception as err:\n        return np.inf\n    \n# Not sure yet if I can generate arbitrary moments, or just one at a time\ndef moment_generator(alpha, beta):\n    # From our four moments about the origin...\n    mu1=moment(alpha, beta, 1)\n    mu2=moment(alpha, beta, 2)\n    mu3=moment(alpha, beta, 3)\n    mu4=moment(alpha, beta, 4)\n    \n    #... we can compute the moments about the mean\n    if(np.isfinite(mu1)):\n      mean = mu1\n      mean = np.around(mean, 3)\n    else:\n      mean = \"Undefined\"\n    if(np.isfinite(mu2)):\n      stdev = np.sqrt((mu2 - mu1**2))\n      stdev = np.around(stdev, 3)\n    else:\n      stdev = \"Undefined\"\n    if(np.isfinite(mu3)):\n      skewness = (mu3 - 3*mu1*mu2 + 2*(mu1**2))/(stdev**3)\n      skewness = np.around(skewness, 3)\n    else:\n      skewness = \"Undefined\"\n    if(np.isfinite(mu4)):\n      kurtosis = ((mu4 - 4*mu1*mu3 + 6*(mu1**2)*mu2 - 3*(mu1**4))/(stdev**4))-3\n      kurtosis = np.around(kurtosis, 3)\n    else:\n      kurtosis = \"Undefined\"\n\n    return mean, stdev, skewness, kurtosis","metadata":{"id":"fBUC5-j6JB0D","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"First, we will use sliders to visualize how different values of our scale parameter alpha ($\\alpha$) and our shape parameter beta ($\\beta$) affect the pdf and the cdf of the log-logistic distribution. Throughout these exercises, try to evaluate what  alpha and beta do. What happens when either is large? Small? How do each have an effect on kurtosis and skewness? Does one parameter seem to dominate over the other?\n\nFor reference, the pdf of the log-logistic distribution is given by the following function:\n$$ f(x;\\alpha,\\beta)=\\frac{(\\beta/\\alpha)(x/\\alpha)^{\\beta-1}}{(1+(x/\\alpha)^\\beta)^2} $$\n\nConveniently, our scale parameter $\\alpha$ gives the median of this distribution.\n\n","metadata":{"id":"ub-DVI5XJE38"}},{"cell_type":"code","source":"# Creates a PDF we can interact with!\ndef log_logistic_interactionPDF(alpha, beta):\n    fig, ax = plt.subplots(figsize=(12, 6))\n    \n    #Concatenate two arrays to improve resolution near 0\n    x = np.concatenate((np.linspace(0,0.1,10), np.linspace(0.1,10,100)), axis=None)\n    pdf = log_logistic_test_PDF(x, alpha, beta)\n    \n    ax.plot(x, pdf)\n    \n    plt.xlabel(\"x\")\n    plt.ylabel(\"f(x)\")\n    plt.xlim([0, 10])\n    plt.ylim([0, 2])\n    plt.title(\"Probability Distribution\")\n    \n    moments = moment_generator(alpha, beta)\n    plt.text(6, 1.8, \"Mean = {0}\".format(moments[0]))\n    plt.text(6, 1.6, \"Standard Deviation = {0}\".format(moments[1]))\n    plt.text(6, 1.4, \"Skewness = {0}\".format(moments[2]))\n    plt.text(6, 1.2, \"Kurtosis = {0}\".format(moments[3]))\n    fig.patch.set_facecolor('white') # we want to see the axis values in dark mode\n    #plt.plot(x, pdf)\n    #plt.yscale(\"log\") # The log scale makes it difficult to interpret these graphs...","metadata":{"id":"WD6ypZ4DJFGg","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As a first exercise, let's play around with some sliders, and see how they affect the log-logistic PDF. As you move the sliders, try making observations about how the graph changes – how do alpha and beta affect skewness and kurtosis? What happens when alpha and beta are big or small?\n\nYou can change the values with the sliders, or just input the values you desire in the textbox beside the sliders. Explore what happens to the distribution as each parameter becomes large or small.\n\nTake note of any observations you are able to make about the behaviour of log-logistic PDF's.\n\n","metadata":{"id":"dQ7Snw1ZJKJl"}},{"cell_type":"code","source":"interact(log_logistic_interactionPDF, alpha=widgets.FloatSlider(min=0.1, max=10, step=0.1, value=1), beta=widgets.FloatSlider(min=0.1, max=10, step=0.1, value=1))","metadata":{"id":"3QX_NuNyJKeq","outputId":"1c7a4e05-e338-44fc-f1d0-185d7f653e6e","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Notice that the mean is undefined for beta less than 1 – why do you think that is?  More subtly, the standard deviation is undefined for beta less than 2. As a hint, the  $r^\\text{th}$ moment of the log-logistic distribution (about the origin) is given by $$\\mu_r=\\alpha^r\\frac{r\\pi/\\beta}{\\sin(\\pi/\\beta)}$$\nThe four moments we are primarily interested in are the Mean, Standard Deviation, Skewness, and Kurtosis. As in Section 3.6 of the textbook, the second, third, and fourth moment *about the mean* are given by:\n$$\\mu_2'=\\mu_2-{\\mu_1}^2$$\n$$\\mu_3'=\\mu_3-3\\mu_1\\mu_2+2{\\mu_1}^3 $$\n$$ \\mu_4' = \\mu_4-4\\mu_1\\mu_3+6{\\mu_1}^2\\mu_2-3{\\mu_1}^4$$\n","metadata":{"id":"cyZrCozYXHg7"}},{"cell_type":"markdown","source":"Now that you've had a chance to investigate the parameters for yourself, let's discuss what they represent, for good measure! Beta dominates the effects of skewness. Smaller values of $\\beta$ yield a distribution which is more skewed to the left. As $\\beta$ increases, the distribution becomes more skewed towards the right.\n\nMeanwhile, alpha dominates the kurtosis of this distribution. As $\\alpha$ grows larger, the distribution becomes less peaked and more tailed.\n\nNotice that as $\\beta$ gets large, the mean of the distribution tends to $\\alpha$.\n","metadata":{"id":"pDs7ysAhJMWO"}},{"cell_type":"markdown","source":"We can also look at the cumulative distribution. Once again, run the cells below and explore the limiting cases of each parameter. The cdf is given by the following formula: $$ F(x;\\alpha,\\beta)=\\frac{x^\\beta}{\\alpha^\\beta+x^\\beta} $$\n\nwhere $\\alpha$ and $\\beta$ are the same scale and shape parameters as before. Fiddle around, and try to take note of any other observations.","metadata":{"id":"edIg1uCOdyOR"}},{"cell_type":"code","source":"# Makes an interactive CDF\ndef log_logistic_interactionCDF(alpha, beta):\n    fig, ax = plt.subplots(figsize=(12, 6))\n    \n    #Concatenate two arrays to improve resolution near 0\n    x = np.concatenate((np.linspace(0,0.1,100), np.linspace(0.1,10,100)), axis=None)\n    pdf = log_logistic_test_CDF(x, alpha, beta)\n    \n    ax.plot(x, pdf)\n    \n    plt.xlabel(\"x\")\n    plt.ylabel(\"F(x)\")\n    plt.xlim([0, 10])\n    plt.ylim([0, 2])\n    plt.title(\"Cumulative Distribution\")\n    fig.patch.set_facecolor('white') # we want to see the axis values in dark mode\n    #plt.plot(x, pdf)\n    #plt.yscale(\"log\")","metadata":{"id":"0aFUhPxfJPJv","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"interact(log_logistic_interactionCDF, alpha=widgets.FloatSlider(min=0.1, max=10, step=0.1, value=1), beta=widgets.FloatSlider(min=0.1, max=10, step=0.1, value=1))","metadata":{"id":"hoLJ7X1pJRNW","outputId":"e375753a-45be-4077-e4b6-e0ae1a643a21","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Why not look at both side by side? In this next exercise, try to see how changes affect both the PDF and CDF, and how they relate to each other. Perhaps you will be able to make further observations about their behaviour.","metadata":{"id":"JGhK1Zi9JS3g"}},{"cell_type":"code","source":"# Displays both at the sane time\ndef log_logistic_double_display(alpha, beta):\n    fig = plt.figure(figsize=(12, 6))\n    \n    #Concatenate two arrays to improve resolution near 0\n    x = np.concatenate((np.linspace(0,0.1,10), np.linspace(0.1,5,50)), axis=None)\n    pdf = log_logistic_test_PDF(x, alpha, beta)\n    cdf = log_logistic_test_CDF(x, alpha, beta)\n    \n    ax = fig.add_subplot(121)\n    ax2 = fig.add_subplot(122)\n    \n    ax.plot(x, pdf)\n    ax2.plot(x, cdf)\n    \n    ax.set_xlabel(\"x\")\n    ax.set_ylabel(\"f(x)\")\n    ax2.set_xlabel(\"x\")\n    ax2.set_ylabel(\"F(x)\")\n    \n    ax.set_xlim([0, 5])\n    ax.set_ylim([0, 2])\n    ax.set_title(\"Probability Distribution\")\n    ax2.set_xlim([0, 5])\n    ax2.set_ylim([0, 2])\n    ax2.set_title(\"Cumulative Distribution\")\n    \n    fig.patch.set_facecolor('white') # we want to see the axis values in dark mode","metadata":{"id":"RGDPN_RSJUKp","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"interact(log_logistic_double_display, alpha=widgets.FloatSlider(min=0.1, max=10, step=0.1, value=1), beta=widgets.FloatSlider(min=0.1, max=10, step=0.1, value=1))","metadata":{"id":"pKWe6hXVJV5s","outputId":"49fe97b2-fdc6-43d4-8836-2d3d885ec28f","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We hope this demonstration has shed some light on the behaviour of the log-logistic distribution!","metadata":{"id":"JYpaluToJYXY"}},{"cell_type":"markdown","source":"## The log-logistic distribution in Economics: The Fisk Distribution\n\nIn Economics, the log-logistic distribution takes on new meaning as the Fisk Distribution, where it is used as a simple model of wealth distribution. Our shape parameter ($\\beta$) takes on a new meaning too: The Gini coefficient is defined as $G=\\frac{1}{\\beta}$, and is a measure of wealth dispersion. Essentially, $G$ assigns a value to the magnitude of income inequality in a country. A Gini coefficient of 0 denotes perfect equality, while a coefficient of 1 indicates complete inequality. Since $G=\\frac{1}{\\beta}$, we can only have perfect equality in the limit as Beta tends to infinity, wheras complete inequality occurs when Beta equals 1.\n\nSources:\n\nhttps://corporatefinanceinstitute.com/resources/knowledge/economics/gini-coefficient/\n\nhttps://uwaterloo.ca/canadian-index-wellbeing/what-we-do/domains-and-indicators/gini-coefficient-income-gap\n\nBelow is a data set that we will try and model with the Fisk distribution; we will use a $\\chi^2$ method to evaluate our fit. If we are trying to fit $n$ data points, then we sum the squares of the residuals between our model and the data at each point $i\\in\\{1,2,...,n\\}$ to obtain our $\\chi^2$ value: $$ \\chi^2 = \\sum_i^n(x_{\\text{model, }i} -  x_{\\text{observed, }i})^2$$ The $\\chi^2$ regression is a method of evaluating the fit of an arbitrary curve to data points. A high $\\chi^2$ value means that there is significant difference between your model and the observed data, whereas a small one means your model describes the data relatively well. So, as a statistician, our goal is to choose parameters for our log-logsitic distribution that minimize $\\chi^2$.\n\n","metadata":{"id":"3IpEW1PoJej8"}},{"cell_type":"code","source":"pandaData = pd.read_csv(\"../input/various-wealth-distributions/dfa-income-levels.csv\")\n\npandaData","metadata":{"id":"LfjgiUmvJe3_","outputId":"f2b210fb-68dc-4533-a178-f3d4b06c5895","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Note that the data set describes wealth. Along the $x$-axis, we have the income bracket, which is basically lists the population by percentage in increasing order of wealth. The below code is able to give us the total wealth percentage of each income bracket, so by adding them up, they give us the points of the cumulative distribution (CDF) relatively naturally. From here, your goal is to now choose parameters for a log-logsitic CDF using the sliders below that will minimize $\\chi^2$.\n","metadata":{"id":"o0HqGNmjTZBf"}},{"cell_type":"code","source":"#incbrak = [0.10, 0.30, 0.50, 0.70, 0.895, 0.995] # Need to interpret our income brackets as x-values...\nincbrak = [0.2, 0.4, 0.6, 0.8, 1] # Use this one. We combine the two lowest data bins\nyrqrtr = 110 # we pick the quarter we want (pick between 0 and 124)\n\nprint(\"Quarter picked: \" + pandaData['Date'][yrqrtr*6]) # prints the quarter we picked\nnworth = []\nfor i in range(0, 6):\n    nworth.append(pandaData['Net worth'][i+yrqrtr*6])\n    \nprint(\"(Low to high) Net worth by income: \" + str(nworth)) # prints net worth of various income brackets\n\ntotalnetworth=0\n\nfor i in range(0, len(nworth)):\n    totalnetworth+=nworth[i]\nprint(\"Total net worth: \" + str(totalnetworth)) # prints total wealth\n\nnworthnormal=nworth/totalnetworth\n\nprint(\"(Low to high) Net worth by income by %: \" + str(nworthnormal)) # prints net worth of various income brackets as a % of total net worth\n\ncumnetworth=[]\nsumnetworth=nworthnormal[0]\nfor i in range (1, len(nworth)):\n    sumnetworth+=nworthnormal[i]\n    cumnetworth.append(round(sumnetworth,8))\n    \nprint(\"(High to low) Cumulative net worth: \" + str(cumnetworth))","metadata":{"id":"yaGlljPfTgK9","outputId":"c0a8e803-fe11-4924-e203-966983f0a1ca","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"If you are having difficulty loading the above data, then uncomment code below and run the cell. We have hardcoded the values for the wealth distribution of the first quarter of 2017 (2017:Q1) here for your convenience. To uncomment, delete the \\# character at the start of each line.","metadata":{}},{"cell_type":"code","source":"# If you can't load the data, run this cell!\n#incbrak = [0.2, 0.4, 0.6, 0.8, 1]\n#cumnetworth = [0.71378521, 0.85827444, 0.9341023,  0.97637864, 1.]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def chi2(expected, measured):\n    chi2 = 0\n    for i in range(0, len(expected)):\n        chi2 += (expected[i] - measured[i])**2\n    return chi2","metadata":{"id":"6-D8xLErJoxx","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This is the fisk cumulative distribtion function\ndef fisk_CDF(alpha, beta):\n    fig, ax = plt.subplots(figsize=(12, 6))\n    \n    #Concatenate two arrays to improve resolution near 0\n    x = np.concatenate((np.linspace(0,0.1,50), np.linspace(0.1,1,50)), axis=None)\n    cdf = log_logistic_test_CDF(x, alpha, beta)\n    # Need to evaluate the CDF at each point in the income bracket for our tests\n    compare_points = log_logistic_test_CDF(incbrak, alpha, beta)\n    \n    ax.plot(x, cdf)\n    ax.plot(incbrak, cumnetworth, 'r.')\n    \n    plt.xlabel(\"Fraction of population\")\n    plt.ylabel(\"Fraction of Wealth\")\n    plt.xlim([0, 1])\n    plt.ylim([0, 1])\n    plt.title(\"Cumulative Net Worth\")\n    plt.title(\"Cumulative Distribution. Current fit: Chi^2 = {0}\".format(np.round(chi2(cumnetworth, compare_points), 4)))\n    fig.patch.set_facecolor('white') # we want to see the axis values in dark mode","metadata":{"id":"mCZjldwFJqJV","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Below, you will try to minimize $\\chi^2$ using the sliders for $\\alpha$ and $\\beta$. The red data points represent the (normalized) cumulative wealth of the population at that percentage of the total population. What is the lowest value of $\\chi^2$ you can get? Would you say that this approximation describes the data well? What can you say about this graph's skewness and kurtosis. Also, try plotting your values of alpha and beta into the pdf, and try to see if it makes sense with our initial problem. Given this, what could you say about wealth equality or inequality in the United States? Recall that the Gini coefficient is $G=\\frac{1}{\\beta}$.\n\nTo get you started, recall that $\\alpha$ gives the median of our distribution. Since we plot the income brackets as a fraction between 0 and 1, this probably means that $\\alpha$ should be less than 1.\n","metadata":{"id":"Qy3jLFngJsqU"}},{"cell_type":"code","source":"interact(fisk_CDF, alpha=widgets.FloatSlider(min=0.01, max=10, step=0.01, value=1), beta=widgets.FloatSlider(min=0.01, max=10, step=0.01, value=1))","metadata":{"id":"pr-q9ldNJrLU","outputId":"791c9550-249e-4d98-e565-14975821ebee","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This graph suggests things about the wealth distribution: $x$ percent of the population holds $F(x)$ percent of the wealth. For example, extrapolating based on the 2017:Q1 data, it appears that a mere 20\\% of the population of the United States holds just over 50\\% of the country's wealth.\n\nNow that you have estimated alpha and beta, let's see how your answer compares to our algorithm, which computes $\\chi^2$ for every combination of alpha and beta very quickly. Were you close? Far? If you were further off than what you would have thought, why? Below is our code to construct our best fit alpha and beta!","metadata":{"id":"Vo2hO6_2JwgS"}},{"cell_type":"code","source":"# to find our minimal chi^2\ndef minimizechi2(expected):\n    alpha=0.01\n    beta=0.01\n    \n    minalpha=alpha\n    minbeta=beta\n    \n    compare_points=log_logistic_test_CDF(incbrak, alpha, beta)\n    minchi2value=chi2(cumnetworth, compare_points)\n    \n    for i in range(499):\n        \n        for j in range(499):\n            \n            compare_points=log_logistic_test_CDF(incbrak, alpha, beta)\n            chi2value=chi2(cumnetworth, compare_points)\n            \n            if(chi2value <= minchi2value):\n                minchi2value=chi2value\n                minalpha=alpha\n                minbeta=beta\n            beta+=0.01\n            \n        beta=0.01            \n        compare_points=log_logistic_test_CDF(incbrak, alpha, beta)\n        chi2value=chi2(cumnetworth, compare_points)\n        \n        if(chi2value <= minchi2value):\n            minchi2value=chi2value\n            minalpha=alpha\n            minbeta=beta\n        alpha+=0.01\n        \n    return minchi2value, minalpha, minbeta","metadata":{"id":"Z-g5Q4L4Jt7L","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"minchi2, minAlpha, minBeta = minimizechi2(cumnetworth) #Just for this example specifically\nminchi2 = np.round(minchi2, 5)\nminAlpha = np.round(minAlpha, 2)\nminBeta = np.round(minBeta, 2)\nprint(\"Minimum Chi-Squared Value: {0}\".format(minchi2))\nprint(\"Minimum alpha: {0}\".format(minAlpha))\nprint(\"Minimum beta: {0}\".format(minBeta))","metadata":{"id":"aFZ_bzI0Jx2x","outputId":"c35014c9-8a15-4088-c051-3ee351e5180a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Above, we list the best-fit values for $\\alpha$ and $\\beta$ to match the quarter you chose. Were you able to find something similar playing around with the sliders? If not, what did you get? Try plotting these values in the sliders below to see what the pdfs and cdfs look like! Do they look similar to the ones you plotted before knowing the exact answer? If it differs, what would you say about G here? How do you think the United States compares to other countries?\n\nFeel free to also try out different quarters, and see what the Gini Coefficient ends up being at different times. Is there a big change, or does it stay relatively the same? Has income inequality gotten worse or better over the years? If you have the opportunity (and if our data loads correctly), try computing the Gini coefficient in 2008 – there was a pretty big housing crisis back then that made everyone upset. Did it have any impact on the wealth disparity in the United States?\n","metadata":{"id":"dmXX5QGEJ4ts"}},{"cell_type":"markdown","source":"# Examining the PDF from our wealth distribution fit\n\nBelow we have included another graph which compares the PDF to the CDF, this time at the best-fit values found by our little method. What does the kurtiosis and skewness tell us about wealth distribution? Do these answers align with qualitative observations made from the graph?\n\nUsing what you know about the Gini coefficient, try to plot distributions for a country which has high income equality. Compare that to a model of a country with high ineqaulity.","metadata":{"id":"R5fNWYDvZKXR"}},{"cell_type":"code","source":"def log_logistic_double_display_wealth(alpha, beta):\n    fig = plt.figure(figsize=(12, 6))\n    \n    #Concatenate two arrays to improve resolution near 0\n    x = np.concatenate((np.linspace(0,0.2,50), np.linspace(0.2,1,50)), axis=None)\n    pdf = log_logistic_test_PDF(x, alpha, beta)\n    cdf = log_logistic_test_CDF(x, alpha, beta)\n\n    ax = fig.add_subplot(121)\n    ax2 = fig.add_subplot(122)\n    \n    ax.plot(x,pdf)\n    ax2.plot(x, cdf)\n    \n    ax.set_xlabel(\"Income Bracket (%)\")\n    ax.set_ylabel(\"Change in Total Wealth\")\n    ax2.set_xlabel(\"Income Bracket (%)\")\n    ax2.set_ylabel(\"Total Wealth\")\n    \n    ax.set_xlim([0, 1])\n    ax.set_ylim([0, 6])\n    ax.set_title(\"Probability Distribution of Wealth by Income Bracket\")\n    ax2.set_xlim([0, 1])\n    ax2.set_ylim([0, 1])\n    ax2.set_title(\"Cumulative Distribution of Wealth by Income Bracket\")\n    \n    fig.patch.set_facecolor('white') # we want to see the axis values in dark mode","metadata":{"id":"rhppK-TuJ48p","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"interact(log_logistic_double_display_wealth, alpha=widgets.FloatSlider(min=0.01, max=10, step=0.01, value=minAlpha), beta=widgets.FloatSlider(min=0.01, max=10, step=0.01, value=minBeta))\n","metadata":{"id":"D6LbvvZjT3Ly","outputId":"851fbfa0-f95a-4dc6-8b71-b1ec5cbab24b","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let us take note that the log-logistic distribution is used to model skewed data: phenomena which initially rapidly increase, then slowly decrease. The PDF for this wealth distribution tells us something about the marginal wealth of the United States, or how much we expect wealth to change given a small change in income bracket. The easier one to visualize wealth equality with is the CDF. Notice that the top 50% income bracket has around 83% of the wealth. And how the top 20% has 55% of the wealth. What about how the bottom 20% has around 5%? \n\nLooking at the PDF, we have the idea of marginal wealth; so this measures how wealth changes if we change the income bracket % slightly... this abstract idea tells us what happens to total wealth of an income bracket relative to the one before it, in a way. Our result actually makes sense as well, since even though the 1% has the most wealth individually, there are far more people in the 99-80% bracket, so they have more total wealth. This idea is hard to wrap our heads around since the amount of people is discrete, but we have a continuous function, but we get that \"everyone\" in at around the top 15% level of income have the most wealth.\n\nThe CDF is a lot more intuitive in this case, and it also the one that it more widely used, since it shows how wealth is distributed vs how it changes with the PDF.\n\nAnother thing we want to note is that given our total data, it is really hard to give a good value for G. Recall that $G=\\frac{1}{\\beta}$, so it depends entirely on $\\beta$. However, the range of beta values that make for a reasonable model is very wide. For example, any value between $\\beta$=1.7 and $\\beta$=2.3 result in the chi^2 term to be less 0.01. However, this makes $G$ range between 0.43 and 0.58 - a pretty wide interval. For reference, the Gini Coefficient of the United States is sitting at around 0.45.","metadata":{"id":"MwAkkGJvAA1S"}},{"cell_type":"code","source":"","metadata":{"id":"SH5b6Lz7A9O7"},"execution_count":null,"outputs":[]}]}