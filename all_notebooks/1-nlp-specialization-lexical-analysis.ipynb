{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom tqdm import tqdm\n\npd.options.display.max_colwidth = 200\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames[:5]:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"execution":{"iopub.status.busy":"2021-06-26T14:08:10.650867Z","iopub.execute_input":"2021-06-26T14:08:10.651484Z","iopub.status.idle":"2021-06-26T14:08:11.055755Z","shell.execute_reply.started":"2021-06-26T14:08:10.651381Z","shell.execute_reply":"2021-06-26T14:08:11.054752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install https://med7.s3.eu-west-2.amazonaws.com/en_core_med7_lg.tar.gz","metadata":{"execution":{"iopub.status.busy":"2021-06-26T14:08:11.058876Z","iopub.execute_input":"2021-06-26T14:08:11.059194Z","iopub.status.idle":"2021-06-26T14:10:26.923997Z","shell.execute_reply.started":"2021-06-26T14:08:11.059166Z","shell.execute_reply":"2021-06-26T14:10:26.922976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Contents\n\n* [Read the dataset](#datasetreading)\n* [Data Cleaning](#regex)\n* [Stemming & Lemmatization](#stemming)\n* [Tokenization](#tokenization)\n* [Stop word removal](#stopword)","metadata":{}},{"cell_type":"markdown","source":"<a id='datasetreading'></a>\n\n### Read the dataset\n\nThe dataset is collected from https://www.kaggle.com/c/medical-notes/data. It contains 800 anonymised transcribed medical reports with the disease category (specialty). For more information browse original source - https://www.mtsamples.com/","metadata":{}},{"cell_type":"code","source":"# We read all the medical notes from the directory\ndir = '/kaggle/input/nlp-specialization-data/Medical_Notes/Medical_Notes'\nprint (\"Total {} files in directory\".format(len(os.listdir(dir))))","metadata":{"execution":{"iopub.status.busy":"2021-06-26T14:15:03.545246Z","iopub.execute_input":"2021-06-26T14:15:03.545618Z","iopub.status.idle":"2021-06-26T14:15:03.552234Z","shell.execute_reply.started":"2021-06-26T14:15:03.545586Z","shell.execute_reply":"2021-06-26T14:15:03.551238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels = pd.read_csv(\"/kaggle/input/nlp-specialization-data/Labels_Medical_Notes.csv\",header=None)\nlabels.columns = ['file','label']","metadata":{"execution":{"iopub.status.busy":"2021-06-26T14:15:05.696568Z","iopub.execute_input":"2021-06-26T14:15:05.696904Z","iopub.status.idle":"2021-06-26T14:15:05.728147Z","shell.execute_reply.started":"2021-06-26T14:15:05.69687Z","shell.execute_reply":"2021-06-26T14:15:05.727342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels.head(5)","metadata":{"execution":{"iopub.status.busy":"2021-06-26T14:15:07.516082Z","iopub.execute_input":"2021-06-26T14:15:07.516466Z","iopub.status.idle":"2021-06-26T14:15:07.547828Z","shell.execute_reply.started":"2021-06-26T14:15:07.51643Z","shell.execute_reply":"2021-06-26T14:15:07.546525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# read each medical notes and the corresponding label (disease category)\ntexts = []\nclasses = []\n\nfor i in tqdm(range(labels.shape[0])):\n    filename = os.path.join(dir,labels.iloc[i]['file'])\n    text = \" \".join(open(filename,'r',errors='ignore').readlines())\n    texts.append(text)\n    classes.append(labels.iloc[i]['label'])\n    \ndata = pd.DataFrame()\ndata['text'] = texts\ndata['label'] = classes","metadata":{"execution":{"iopub.status.busy":"2021-06-26T14:15:20.513319Z","iopub.execute_input":"2021-06-26T14:15:20.513819Z","iopub.status.idle":"2021-06-26T14:15:23.743169Z","shell.execute_reply.started":"2021-06-26T14:15:20.513787Z","shell.execute_reply":"2021-06-26T14:15:23.742529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print (data.shape)","metadata":{"execution":{"iopub.status.busy":"2021-06-26T14:15:26.652584Z","iopub.execute_input":"2021-06-26T14:15:26.653556Z","iopub.status.idle":"2021-06-26T14:15:26.659517Z","shell.execute_reply.started":"2021-06-26T14:15:26.65351Z","shell.execute_reply":"2021-06-26T14:15:26.658433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head(5)","metadata":{"execution":{"iopub.status.busy":"2021-06-26T14:15:51.16148Z","iopub.execute_input":"2021-06-26T14:15:51.161824Z","iopub.status.idle":"2021-06-26T14:15:51.172316Z","shell.execute_reply.started":"2021-06-26T14:15:51.161798Z","shell.execute_reply":"2021-06-26T14:15:51.171315Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='regex'></a>\n\n### Basic data cleaning\n\nNatural language in its pure form can bring lot of noise. We need to clean the data in order to use any statistical/machine learning model. Below are the few techniques for cleaning the text data.\n\n* Using RegEx (regular expressions) to identify the irrelevant text sections for removal\n* Standardizing/normalizing texts like - abbreviations, spelling mistakes\n* For social media data - remove smileys, email ids if these information are not relevant for downstream analysis\n","metadata":{}},{"cell_type":"code","source":"sample_text = data.text.iloc[3]\nprint (sample_text)","metadata":{"execution":{"iopub.status.busy":"2021-06-26T14:18:08.162473Z","iopub.execute_input":"2021-06-26T14:18:08.162803Z","iopub.status.idle":"2021-06-26T14:18:08.168364Z","shell.execute_reply.started":"2021-06-26T14:18:08.162769Z","shell.execute_reply":"2021-06-26T14:18:08.167224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re","metadata":{"execution":{"iopub.status.busy":"2021-06-26T14:16:39.076139Z","iopub.execute_input":"2021-06-26T14:16:39.076475Z","iopub.status.idle":"2021-06-26T14:16:39.080208Z","shell.execute_reply.started":"2021-06-26T14:16:39.076447Z","shell.execute_reply":"2021-06-26T14:16:39.07904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We remove all the special characters like - \"\\n\", HTML tags from the texts","metadata":{}},{"cell_type":"code","source":"def remove_html(text):\n    text = text.replace(\"\\n\",\" \")\n    pattern = re.compile('<.*?>') #all the HTML tags\n    return pattern.sub(r'', text)","metadata":{"execution":{"iopub.status.busy":"2021-06-26T14:16:48.47546Z","iopub.execute_input":"2021-06-26T14:16:48.475789Z","iopub.status.idle":"2021-06-26T14:16:48.480587Z","shell.execute_reply.started":"2021-06-26T14:16:48.475763Z","shell.execute_reply":"2021-06-26T14:16:48.479562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_text_processed = remove_html(sample_text)\nprint (sample_text_processed)","metadata":{"execution":{"iopub.status.busy":"2021-06-26T14:16:50.738779Z","iopub.execute_input":"2021-06-26T14:16:50.739135Z","iopub.status.idle":"2021-06-26T14:16:50.744818Z","shell.execute_reply.started":"2021-06-26T14:16:50.739105Z","shell.execute_reply":"2021-06-26T14:16:50.743506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Remove all the headings from text","metadata":{}},{"cell_type":"code","source":"def remove_headings(text):\n    pattern = re.compile('\\w+:')\n    return pattern.sub(r'', text)","metadata":{"execution":{"iopub.status.busy":"2021-06-26T14:17:00.735711Z","iopub.execute_input":"2021-06-26T14:17:00.736054Z","iopub.status.idle":"2021-06-26T14:17:00.740681Z","shell.execute_reply.started":"2021-06-26T14:17:00.736026Z","shell.execute_reply":"2021-06-26T14:17:00.739529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_text_processed = remove_headings(sample_text_processed)\nprint (sample_text_processed)","metadata":{"execution":{"iopub.status.busy":"2021-06-26T14:17:09.011596Z","iopub.execute_input":"2021-06-26T14:17:09.011925Z","iopub.status.idle":"2021-06-26T14:17:09.017441Z","shell.execute_reply.started":"2021-06-26T14:17:09.011897Z","shell.execute_reply":"2021-06-26T14:17:09.016241Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Remove &quot marks and other characters. Replace multiple spaces with single space","metadata":{}},{"cell_type":"code","source":"def replace_mult_spaces(text):\n    text = text.replace(\"&quot\",\"\")\n    pattern = re.compile(' +')\n    text = pattern.sub(r' ', text)\n    text = text.strip()\n    return text","metadata":{"execution":{"iopub.status.busy":"2021-06-26T14:17:30.919569Z","iopub.execute_input":"2021-06-26T14:17:30.9199Z","iopub.status.idle":"2021-06-26T14:17:30.925004Z","shell.execute_reply.started":"2021-06-26T14:17:30.919872Z","shell.execute_reply":"2021-06-26T14:17:30.923547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_text_processed = replace_mult_spaces(sample_text_processed)\nprint (sample_text_processed)","metadata":{"execution":{"iopub.status.busy":"2021-06-26T12:21:03.063524Z","iopub.execute_input":"2021-06-26T12:21:03.0639Z","iopub.status.idle":"2021-06-26T12:21:03.07467Z","shell.execute_reply.started":"2021-06-26T12:21:03.063866Z","shell.execute_reply":"2021-06-26T12:21:03.073576Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"remove multiple consecutive spaces and replace with single space","metadata":{}},{"cell_type":"code","source":"def replace_other_chars(text):\n    pattern = re.compile(r'[()!@&;]')\n    text = pattern.sub(r'', text)\n    return text","metadata":{"execution":{"iopub.status.busy":"2021-06-26T12:21:03.076322Z","iopub.execute_input":"2021-06-26T12:21:03.077011Z","iopub.status.idle":"2021-06-26T12:21:03.086127Z","shell.execute_reply.started":"2021-06-26T12:21:03.076965Z","shell.execute_reply":"2021-06-26T12:21:03.08532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_text_processed = replace_other_chars(sample_text_processed)\nprint (sample_text_processed)","metadata":{"execution":{"iopub.status.busy":"2021-06-26T12:21:03.087852Z","iopub.execute_input":"2021-06-26T12:21:03.088815Z","iopub.status.idle":"2021-06-26T12:21:03.100491Z","shell.execute_reply.started":"2021-06-26T12:21:03.088766Z","shell.execute_reply":"2021-06-26T12:21:03.099128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Putting everything together in a function and apply the cleaning on all the texts. Further, convert everything into lower case.","metadata":{}},{"cell_type":"code","source":"def clean_text(text):\n    text = remove_html(text)\n    text = remove_headings(text)\n    text = replace_mult_spaces(text)\n    text = replace_other_chars(text)\n    text = text.lower()\n    return text","metadata":{"execution":{"iopub.status.busy":"2021-06-26T12:21:03.101894Z","iopub.execute_input":"2021-06-26T12:21:03.102249Z","iopub.status.idle":"2021-06-26T12:21:03.108214Z","shell.execute_reply.started":"2021-06-26T12:21:03.102201Z","shell.execute_reply":"2021-06-26T12:21:03.107253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['clean_text'] = data.text.apply(clean_text)","metadata":{"execution":{"iopub.status.busy":"2021-06-26T12:21:03.109618Z","iopub.execute_input":"2021-06-26T12:21:03.109949Z","iopub.status.idle":"2021-06-26T12:21:03.494495Z","shell.execute_reply.started":"2021-06-26T12:21:03.109918Z","shell.execute_reply":"2021-06-26T12:21:03.493481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='eda'></a>\n\n### Basic descriptive analysis on the texts","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndata.clean_text.apply(len).plot.hist()\ndata.text.apply(len).plot.hist()\nplt.title(\"Distribution of total number of characters in the clinical notes\")\nplt.legend([\"before cleaning\",\"after cleaning\"])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-26T12:21:03.495843Z","iopub.execute_input":"2021-06-26T12:21:03.496142Z","iopub.status.idle":"2021-06-26T12:21:03.740981Z","shell.execute_reply.started":"2021-06-26T12:21:03.496111Z","shell.execute_reply":"2021-06-26T12:21:03.739938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.clean_text.apply(lambda x: len(x.split())).plot.hist()\ndata.text.apply(lambda x: len(x.split())).plot.hist()\nplt.title(\"Distribution of total number of words in the clinical notes\")\nplt.legend([\"before cleaning\",\"after cleaning\"])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-26T12:21:03.742407Z","iopub.execute_input":"2021-06-26T12:21:03.742736Z","iopub.status.idle":"2021-06-26T12:21:04.014936Z","shell.execute_reply.started":"2021-06-26T12:21:03.742704Z","shell.execute_reply":"2021-06-26T12:21:04.014062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='stemming'></a>\n\n### Stemming and Lemmatization\n\nStemming changes word into its root stem. \n\n<img src = https://miro.medium.com/max/359/1*l65c30sY9fQsWPKIckqmCQ.png>\n\nHowever, the root stem may not be lexicographically a correct word. Lemmatization on the other hand standardizes a word into its root word. Lemmatization deals with higher level of abstraction.\n\n<img src = https://devopedia.org/images/article/227/6785.1570815200.png>\n","metadata":{}},{"cell_type":"code","source":"sample_text = data.clean_text.iloc[1]\nprint (sample_text)","metadata":{"execution":{"iopub.status.busy":"2021-06-26T12:21:04.016137Z","iopub.execute_input":"2021-06-26T12:21:04.016566Z","iopub.status.idle":"2021-06-26T12:21:04.021704Z","shell.execute_reply.started":"2021-06-26T12:21:04.016534Z","shell.execute_reply":"2021-06-26T12:21:04.020674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nltk\n\ndef simple_stemmer(text):\n    ps = nltk.stem.SnowballStemmer('english')\n    text = ' '.join([ps.stem(word) for word in text.split()])\n    return text","metadata":{"execution":{"iopub.status.busy":"2021-06-26T12:21:04.022872Z","iopub.execute_input":"2021-06-26T12:21:04.02327Z","iopub.status.idle":"2021-06-26T12:21:04.035979Z","shell.execute_reply.started":"2021-06-26T12:21:04.02324Z","shell.execute_reply":"2021-06-26T12:21:04.034553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stemmed_text = simple_stemmer(sample_text)\nprint (stemmed_text)","metadata":{"execution":{"iopub.status.busy":"2021-06-26T12:21:04.037368Z","iopub.execute_input":"2021-06-26T12:21:04.03797Z","iopub.status.idle":"2021-06-26T12:21:04.053921Z","shell.execute_reply.started":"2021-06-26T12:21:04.037931Z","shell.execute_reply":"2021-06-26T12:21:04.052056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import spacy\nimport en_core_med7_lg #en_core_web_sm\n\nnlp = en_core_med7_lg.load()\n\ndef simple_lemmatizer(text):\n    text = nlp(text)\n    text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n    return text","metadata":{"execution":{"iopub.status.busy":"2021-06-26T12:23:42.698926Z","iopub.execute_input":"2021-06-26T12:23:42.699392Z","iopub.status.idle":"2021-06-26T12:23:51.466367Z","shell.execute_reply.started":"2021-06-26T12:23:42.699338Z","shell.execute_reply":"2021-06-26T12:23:51.465459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lemmatized_text = simple_lemmatizer(sample_text)\nprint (lemmatized_text)","metadata":{"execution":{"iopub.status.busy":"2021-06-26T12:23:51.467773Z","iopub.execute_input":"2021-06-26T12:23:51.468047Z","iopub.status.idle":"2021-06-26T12:23:51.646094Z","shell.execute_reply.started":"2021-06-26T12:23:51.468021Z","shell.execute_reply":"2021-06-26T12:23:51.645167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='tokenization'></a>\n\n### Tokenization\n\nTokenization splits a text into tokens or, words. Typically, words are splitted based on blank spaces. But tokenizations can also split words joined by other characters.","metadata":{}},{"cell_type":"code","source":"sample_text = data.clean_text.iloc[1]\ndoc = nlp(sample_text)\nfor token in doc:\n    print(token.text, token.pos_)","metadata":{"execution":{"iopub.status.busy":"2021-06-26T12:23:51.647347Z","iopub.execute_input":"2021-06-26T12:23:51.647612Z","iopub.status.idle":"2021-06-26T12:23:51.836485Z","shell.execute_reply.started":"2021-06-26T12:23:51.647585Z","shell.execute_reply":"2021-06-26T12:23:51.835404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='stopword'></a>\n\n### Stop word removal\n\nLet us first see the most frequent words in the dataset","metadata":{}},{"cell_type":"code","source":"pd.Series(\" \".join(data.clean_text.values).split()).value_counts().head(20)","metadata":{"execution":{"iopub.status.busy":"2021-06-26T12:23:51.837998Z","iopub.execute_input":"2021-06-26T12:23:51.838416Z","iopub.status.idle":"2021-06-26T12:23:52.04648Z","shell.execute_reply.started":"2021-06-26T12:23:51.838372Z","shell.execute_reply":"2021-06-26T12:23:52.045371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Top 10 words based on frequency are english words like - articles, conjuctions, prepositions etc. These words often do not play in significant roles in the downstream applications. We need to remove these words to reduce the model complexity.","metadata":{}},{"cell_type":"code","source":"stopword_list = nltk.corpus.stopwords.words('english')\n\nprint (stopword_list[:10])","metadata":{"execution":{"iopub.status.busy":"2021-06-26T12:23:52.04806Z","iopub.execute_input":"2021-06-26T12:23:52.048457Z","iopub.status.idle":"2021-06-26T12:23:52.089281Z","shell.execute_reply.started":"2021-06-26T12:23:52.048413Z","shell.execute_reply":"2021-06-26T12:23:52.088583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def lemmatize_and_remove_stopwords(text):\n    doc = nlp(text)\n    tokens = [word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in doc]\n    filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n    filtered_text = ' '.join(filtered_tokens)    \n    return filtered_text","metadata":{"execution":{"iopub.status.busy":"2021-06-26T12:23:52.09048Z","iopub.execute_input":"2021-06-26T12:23:52.090769Z","iopub.status.idle":"2021-06-26T12:23:52.096754Z","shell.execute_reply.started":"2021-06-26T12:23:52.090742Z","shell.execute_reply":"2021-06-26T12:23:52.095395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_text","metadata":{"execution":{"iopub.status.busy":"2021-06-26T12:23:52.098258Z","iopub.execute_input":"2021-06-26T12:23:52.098539Z","iopub.status.idle":"2021-06-26T12:23:52.11085Z","shell.execute_reply.started":"2021-06-26T12:23:52.098512Z","shell.execute_reply":"2021-06-26T12:23:52.109745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_text_processed = lemmatize_and_remove_stopwords(sample_text)\nprint (sample_text_processed)","metadata":{"execution":{"iopub.status.busy":"2021-06-26T12:23:52.112474Z","iopub.execute_input":"2021-06-26T12:23:52.112956Z","iopub.status.idle":"2021-06-26T12:23:52.244583Z","shell.execute_reply.started":"2021-06-26T12:23:52.112913Z","shell.execute_reply":"2021-06-26T12:23:52.243614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.clean_text = data.clean_text.apply(lemmatize_and_remove_stopwords)","metadata":{"execution":{"iopub.status.busy":"2021-06-26T12:23:52.246102Z","iopub.execute_input":"2021-06-26T12:23:52.246501Z","iopub.status.idle":"2021-06-26T12:27:00.251445Z","shell.execute_reply.started":"2021-06-26T12:23:52.246457Z","shell.execute_reply":"2021-06-26T12:27:00.250193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.clean_text.apply(lambda x: len(x.split())).plot.hist()\nplt.title(\"Distribution of total number of words in the texts\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-26T12:27:00.253606Z","iopub.execute_input":"2021-06-26T12:27:00.254236Z","iopub.status.idle":"2021-06-26T12:27:00.465443Z","shell.execute_reply.started":"2021-06-26T12:27:00.254195Z","shell.execute_reply":"2021-06-26T12:27:00.46358Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#data.to_csv(\"clinical_notes_cleaned.csv\",index=False)","metadata":{"execution":{"iopub.status.busy":"2021-06-26T12:27:00.467551Z","iopub.execute_input":"2021-06-26T12:27:00.468067Z","iopub.status.idle":"2021-06-26T12:27:00.474364Z","shell.execute_reply.started":"2021-06-26T12:27:00.468014Z","shell.execute_reply":"2021-06-26T12:27:00.472676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### References for further reading\n\n<strong> NLP overview - </strong> https://towardsdatascience.com/a-practitioners-guide-to-natural-language-processing-part-i-processing-understanding-text-9f4abfd13e72\n\n<strong> Regular Expressions - </strong> https://regex101.com/ \n\n<strong> Spacy - </strong> https://spacy.io/usage/spacy-101\n\n<strong> NLTK - </strong> https://www.nltk.org/book/\n\n","metadata":{}}]}