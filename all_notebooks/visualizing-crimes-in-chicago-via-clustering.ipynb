{"cells":[{"metadata":{"_uuid":"e7e0bc81c3215463b666e29c81517f9a1e19011b"},"cell_type":"markdown","source":"![](http://freechicagowalkingtours.com/wp-content/uploads/2016/09/Crime-Scene-Banner-Size-1024x397.jpg)"},{"metadata":{"_uuid":"a941c35794b4b7b0b1631a1753ffacab8a7954ea"},"cell_type":"markdown","source":"**In this kernel we will try and exploit the information made available to kagglers regarding crimes in Chicago. The dataset contains records from 2001 to present day, however it only has 65-66k records compared to the original dataset which has around 6.6M records. If anyone is interested in analyzing the original dataset, it can be found [here](https://data.cityofchicago.org/Public-Safety/Crimes-2001-to-present/ijzp-q8t2). Nonetheless, 65k instances should be a fair amount of information to give us some good insights into the crime scene in Chicago, so let's get started by importing the necessary libraries first.**"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport folium\nimport folium.plugins\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff\nfrom plotly import tools\nimport plotly.plotly as py\nfrom sklearn.cluster import KMeans\nimport pylab as pl\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom bubbly.bubbly import bubbleplot \nfrom plotly.graph_objs import Scatter, Figure, Layout\nfrom __future__ import division\nimport datetime","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7eaa1023a4ce5e92a285de2598beea1b20ef73e7"},"cell_type":"markdown","source":"**Loading the data now! Plus a quick peek into the first few instances to see what it looks like**"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"data = pd.read_csv(\"../input/Crimes_2001_to_present_sample.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ad77a26adf7b1e9d9758695ab85ba6cfd854786a","_kg_hide-input":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b5f3299d95fc27a1a10f0499985a64a559f14685"},"cell_type":"markdown","source":"**Dropping some unnecessary or redundant columns**"},{"metadata":{"trusted":true,"_uuid":"d69ede347b7698a3c56c3a03158a4b7f92e6cbea","collapsed":true,"_kg_hide-input":true},"cell_type":"code","source":"data.drop(['X Coordinate', 'Y Coordinate', 'Updated On', 'Location', 'Beat'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"84532e7dd9fff0c5b76247a637fe5ddd3fbf8e1b"},"cell_type":"markdown","source":"**Doing some date time preprocessing**"},{"metadata":{"trusted":true,"_uuid":"28a0e8135038e81bb617595314a8efc9bfca6ee4","collapsed":true,"_kg_hide-input":true},"cell_type":"code","source":"data['Date'] = pd.to_datetime(data.Date) \ndata['date'] = [d.date() for d in data['Date']]\ndata['time'] = [d.time() for d in data['Date']]\n\ndata['time'] = data['time'].astype(str)\nempty_list = []\nfor timestr in data['time'].tolist():\n    ftr = [3600,60,1]\n    var = sum([a*b for a,b in zip(ftr, map(int,timestr.split(':')))])\n    empty_list.append(var)\n    \ndata['seconds'] = empty_list","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"67654144a8d7f2d0ddceac5a919111350e0b42c3"},"cell_type":"markdown","source":"![](https://img.ifun01.com/images/2016/12/31/18/2414_afxSAU_zvZ7fg.jpg!r800x0.jpg)"},{"metadata":{"_uuid":"6234528cbe1e40b5416dfb7dcb01003ae20b23af"},"cell_type":"markdown","source":"**For the purpose of clustering, we will be doing a kmeans clustering on:**\n1. **First, we will cluster the data according to the District, Ward and Primary Type(as per IUCR code), i feel it will help us identify which portions of the city experience criminal attacks of which type.**\n2. **Then we shall cluster it on the basis of Time, District and Primary Type(as per IUCR code), it should help us classify which districts are more prone to what sort of attacks at what time etc.**                                                                                                                                                                                                                                                                                      \n3.**At last we will cluster according to the Time, Date and Primary Type(as per IUCR code) which should help us answer questions like which sort of attacks the city are most prone to, say on New Year's Eve and at what time.**\n**The KMeans problem is solved using Lloyd's algorithm. In practice, the k-means algorithm is very fast (one of the fastest clustering algorithms available), but it falls in local minima. Thatâ€™s why it can be useful to restart it several times.**\n\n**If you want to know more about KMeans and Lloyd's algorithms, go here [KMeans](https://en.wikipedia.org/wiki/K-means_clustering) and here [Lloyd](https://en.wikipedia.org/wiki/Lloyd%27s_algorithm)**"},{"metadata":{"_uuid":"bcb22941141658ae753a0773883bec7de3fe83fa"},"cell_type":"markdown","source":"**Creating a subset of our dataset with IUCR codes , District codes and Ward codes**\n**IUCR stands for Illinois Uniform Crime Reporting, it encodes different nature of crime using a specific code table; the list of IUCR codes for different crimes can be found [here](https://data.cityofchicago.org/Public-Safety/Chicago-Police-Department-Illinois-Uniform-Crime-R/c7ck-438e/data). The District codes and Ward codes can be found [here](https://www.cityofchicago.org/city/en/about/wards.html) and [here](https://home.chicagopolice.org/community/districts/)**"},{"metadata":{"trusted":true,"_uuid":"ace4654fe84f701af62a521a3532bd9b484a0bd9","_kg_hide-input":true,"collapsed":true},"cell_type":"code","source":"sub_data = data[['Ward', 'IUCR', 'District']]\nsub_data = sub_data.apply(lambda x:x.fillna(x.value_counts().index[0]))\nsub_data['IUCR'] = sub_data.IUCR.str.extract('(\\d+)', expand=True).astype(int)\nsub_data.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bd1af6080b588bc8cdddd1f3245f2af374b227f3"},"cell_type":"markdown","source":"![](https://www.robotlab.com/hs-fs/hub/314265/file-2565558338-jpg/images/Blog/MathJoke.jpg?t=1532994469695&width=650&name=MathJoke.jpg)"},{"metadata":{"_uuid":"8ecc0a72ad6811c119932debc0160ffb0d6befc0"},"cell_type":"markdown","source":"**Before getting into clustering, some things to note: For finding the optimal number of clusters, I will go with the elbow rule, which basically states that on the curve of score vs number of clusters, the optimal point is that where the first bend(or elbow) occurs primarily because after that the the score eventually decreases to zero implying each point starts behaving as its own cluster. However for the purpose of KMeans we will have to normalize the data first as without it, KMeans will simply cluster the data based on the euclidean distances of IUCR code as it has a larger range of values than District or Ward codes. I will show both the clusters with and without normalization, so that you can see the results for yourself**"},{"metadata":{"trusted":true,"_uuid":"37acd295afed5450eed9e778798fc33511dd015e","_kg_hide-input":true,"collapsed":true},"cell_type":"code","source":"N = range(1, 20)\nkmeans = [KMeans(n_clusters=i) for i in N]\nkmeans\nscore = [kmeans[i].fit(sub_data).score(sub_data) for i in range(len(kmeans))]\nscore\npl.plot(N,score)\npl.xlabel('Number of Clusters')\npl.ylabel('Score')\npl.title('Elbow Curve')\npl.show() ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"729782909a04d399a45d611372191c6ebc8034ac"},"cell_type":"markdown","source":"**Ok, so without normalizing the data, the best number of cluster is around 4, so let's try that out**"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"c4c7e6f4f69d7acbf3c7fdc3a06a60101e380482","_kg_hide-input":true},"cell_type":"code","source":"km = KMeans(n_clusters=4)\nkm.fit(sub_data)\ny = km.predict(sub_data)\nlabels = km.labels_\nsub_data['Cluster'] = y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2a723cc52afae3245baa08fe70f4778f51b09089","_kg_hide-input":true,"collapsed":true},"cell_type":"code","source":"fig = plt.figure(figsize=(12,10))\nax = fig.add_subplot(111, projection='3d')\nx = np.array(sub_data['Ward'])\ny = np.array(sub_data['IUCR'])\nz = np.array(sub_data['District'])\n\nax.set_xlabel('Ward')\nax.set_ylabel('IUCR')\nax.set_zlabel('District')\n\nax.scatter(x,y,z, marker=\"o\", c = sub_data[\"Cluster\"], s=60, cmap=\"jet\")\nax.view_init(azim=0)\n#print(ax.azim)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fea6ee69aab5fa68d1f44b719494695253aab4aa"},"cell_type":"markdown","source":"**As expected, KMeans simply clusters the data based on the euclidean distances of the IUCR codes. So let's fix that by normalizing the data**"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"2630f6b0fccf1f013c80a37bae40e89a33669a20","_kg_hide-input":true},"cell_type":"code","source":"sub_data['IUCR'] = (sub_data['IUCR'] - sub_data['IUCR'].min())/(sub_data['IUCR'].max()-sub_data['IUCR'].min())\nsub_data['Ward'] = (sub_data['Ward'] - sub_data['Ward'].min())/(sub_data['Ward'].max()-sub_data['Ward'].min())\nsub_data['District'] = (sub_data['District'] - sub_data['District'].min())/(sub_data['District'].max()-sub_data['District'].min())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"88c95a7f5843d322558c902d567946c46e5bb62a"},"cell_type":"markdown","source":"**Let's find the optimum clusters again**"},{"metadata":{"trusted":true,"_uuid":"a43e72b8eb32a18c0867e1819c266c3f1ed7bee8","_kg_hide-input":true,"collapsed":true},"cell_type":"code","source":"N = range(1, 20)\nkmeans = [KMeans(n_clusters=i) for i in N]\nkmeans\nscore = [kmeans[i].fit(sub_data).score(sub_data) for i in range(len(kmeans))]\nscore\npl.plot(N,score)\npl.xlabel('Number of Clusters')\npl.ylabel('Score')\npl.title('Elbow Curve')\npl.show()\n\ndel sub_data['Cluster']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"817b8a041bcb50eb38b8526b2e41a21ca0401843"},"cell_type":"markdown","source":"**The elbow seems more close to being 3 now! Let's run KMeans again on the normalized data.**"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"05c4d3def851a3cf0a157367940b998be51c3d8a","_kg_hide-input":true},"cell_type":"code","source":"km = KMeans(n_clusters=3)\nkm.fit(sub_data)\ny = km.predict(sub_data)\nlabels = km.labels_\nsub_data['Clusters'] = y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5b8c30d74f75b5decb9cd7449feb5b24b0332488","_kg_hide-input":true,"collapsed":true},"cell_type":"code","source":"fig = plt.figure(figsize=(12,10))\nax = fig.add_subplot(111, projection='3d')\nx = np.array(sub_data['Ward'])\ny = np.array(sub_data['IUCR'])\nz = np.array(sub_data['District'])\n\nax.set_xlabel('Ward')\nax.set_ylabel('IUCR')\nax.set_zlabel('District')\n\nax.scatter(x,y,z, marker=\"o\", c = sub_data[\"Clusters\"], s=60, cmap=\"winter\")\nax.view_init(azim=0)\n#print(ax.azim)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d87484a621f740ff35a125fcdf412a1ce2eb94e3"},"cell_type":"markdown","source":"**The clustering does'nt seems to be based solely on the euclidean distances of IUCR codes now, which is good! Next up, lets check out the distribution of crime type district wise using an animated bubble chart which can be easily plotted using the package bubbly. More of bubbly's use [in this kernel](https://www.kaggle.com/aashita/guide-to-animated-bubble-charts-using-plotly).Click on autoscale in case the data is not properly distributed**"},{"metadata":{"trusted":true,"_uuid":"9a0804c0cb8657dacdc6bf917eadabb3a2e059e8","_kg_hide-input":true,"collapsed":true},"cell_type":"code","source":"data['IUCR'] = data.IUCR.str.extract('(\\d+)', expand=True).astype(int)\nfigure = bubbleplot(dataset=data, x_column='Latitude', y_column='Longitude', \n    bubble_column='Primary Type', time_column='Year',size_column='IUCR',color_column='District', \n    x_title=\"Latitude\", y_title=\"Longitude\", title='District wise distribution of Crime types in Chicago',\n    x_logscale=False)\n\niplot(figure, config={'scrollzoom': True})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cb7af483afaec719fdf393a0ba0ac2b3dade4b77","collapsed":true},"cell_type":"markdown","source":"**Let's do the clustering now based on Time, District and Primary Type(as per IUCR codes). Here also we will scale the time in seconds to be between 1 and 0, with 0.5 representing the time 12:00 noon(else clusters will only be based on time segments), that way the clusters will be divided into sections of morning, afternoon and night.**"},{"metadata":{"trusted":true,"_uuid":"3261a13bfe2433b47ca7f94df7c7988bed6498ef","collapsed":true,"_kg_hide-input":true},"cell_type":"code","source":"#Normalizing the time to be between 0 and 1, this way lower values would indicate midnight to early morning\n#medium values would indicate the afternoon sessions, and high values would indicate evening and night time\n#also kmeans then won't cluster just based on the time as the range of euclidean distances in time column will be very high without scaling\ndata['Normalized_time'] = (data['seconds'] - data['seconds'].min())/(data['seconds'].max()-data['seconds'].min())","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"4d5722f5fc4a29955d9de893fa89916c8a602393","collapsed":true},"cell_type":"code","source":"sub_data1 = data[['IUCR', 'Normalized_time', 'District']]\n#sub_data1['IUCR'] = sub_data1.IUCR.str.extract('(\\d+)', expand=True).astype(int)\nsub_data1['IUCR'] = (sub_data1['IUCR'] - sub_data1['IUCR'].min())/(sub_data1['IUCR'].max()-sub_data1['IUCR'].min())\nsub_data1['District'] = (sub_data1['District'] - sub_data1['District'].min())/(sub_data1['District'].max()-sub_data1['District'].min())\nsub_data1.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6b6a3783d2059e9d3fcd8fd6f86fe49dd9d41aa1"},"cell_type":"markdown","source":"**Let's run  KMeans on the above data now! Like before we shall start off by plotting the elbow curve to find the optimal number of clusters**"},{"metadata":{"trusted":true,"_uuid":"5b5d9fd3a2de7c3db2011c1a5b880b23e4671a26","_kg_hide-input":true,"collapsed":true},"cell_type":"code","source":"N = range(1, 20)\nkmeans = [KMeans(n_clusters=i) for i in N]\nkmeans\nscore = [kmeans[i].fit(sub_data1).score(sub_data1) for i in range(len(kmeans))]\nscore\npl.plot(N,score)\npl.xlabel('Number of Clusters')\npl.ylabel('Score')\npl.title('Elbow Curve')\npl.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e11012fa391739f3bff28bdf5e54921942e3fe6d"},"cell_type":"markdown","source":"**The optimal number of clusters seem to be around 4-5, let's try it out with 4 first, and then we will plot the clusters on a 3d plot for 5 clusters as well and see how it turns out.**"},{"metadata":{"trusted":true,"_uuid":"db11ee5ff858faf7edafcd3b00726b6422b731ac","_kg_hide-input":true,"collapsed":true},"cell_type":"code","source":"km = KMeans(n_clusters=4)\nkm.fit(sub_data1)\ny = km.predict(sub_data1)\nlabels = km.labels_\nsub_data1['Clusters'] = y\nsub_data1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ffa0717a142350106f42d69396877a0e2d634af5","_kg_hide-input":true,"collapsed":true},"cell_type":"code","source":"fig = plt.figure(figsize=(12,10))\nax = fig.add_subplot(111, projection='3d')\nx = np.array(sub_data1['Normalized_time'])\ny = np.array(sub_data1['IUCR'])\nz = np.array(sub_data1['District'])\n\nax.set_xlabel('Time')\nax.set_ylabel('IUCR')\nax.set_zlabel('District')\n\nax.scatter(x,y,z, marker=\"o\", c = sub_data1[\"Clusters\"], s=60, cmap=\"jet\")\nax.view_init(azim=-20)\n#print(ax.azim)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"2e1361fb987223416086462bc527dd371cf25764"},"cell_type":"markdown","source":"**Let's check out the clustering by setting n_clusters=5**"},{"metadata":{"trusted":true,"_uuid":"9e917a9be0c82dba036d860bdb28c138cd819998","_kg_hide-input":true,"collapsed":true},"cell_type":"code","source":"km = KMeans(n_clusters=5)\nkm.fit(sub_data1)\ny = km.predict(sub_data1)\nlabels = km.labels_\nsub_data1['Clusters'] = y\nsub_data1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"44bc157e17d51c30d5e2652f35ce7e768c53f0b3","_kg_hide-input":true,"collapsed":true},"cell_type":"code","source":"#Plotting the results of 5 clusters\nfig = plt.figure(figsize=(12,10))\nax = fig.add_subplot(111, projection='3d')\nx = np.array(sub_data1['Normalized_time'])\ny = np.array(sub_data1['IUCR'])\nz = np.array(sub_data1['District'])\n\nax.set_xlabel('Time')\nax.set_ylabel('IUCR')\nax.set_zlabel('District')\n\nax.scatter(x,y,z, marker=\"o\", c = sub_data1[\"Clusters\"], s=60, cmap=\"jet\")\nax.view_init(azim=-20)\n#print(ax.azim)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"830eca36c6f0ada79557e2a887e715ddf6d414b8"},"cell_type":"markdown","source":"**So much of clustering going on, what do these clusters even indicate??? Hopefully we will get to know that in the next section!**\n![](https://thumb1.shutterstock.com/display_pic_with_logo/173932116/739897180/stock-vector-confused-kid-face-expression-cute-cartoon-girl-illustration-739897180.jpg)"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"3951f74a10c4bc0e8b5d8f97a2b1fbe1b9172347"},"cell_type":"markdown","source":"**Observations:**\n1. **To be added soon, have to make sure first whether the clusters are ok or flawed; any inputs from your side would surely be appreciated.**"},{"metadata":{"_uuid":"2f614df897e4fec93afba754045ebde344c2ab76"},"cell_type":"markdown","source":"**Now let's plot some more interesting graphs to get  further insights into the data we have in hand. After that we will move on to the date time serializing process and clustering them on that basis. Hopefully after that we will be able to predict crimes by either of the two approaches:**\n1. **Minimizing the distance between a crime occurrence and the centroid of a cluster**\n2. **Performing regression analysis on the identified clusters and fitting crimes to the best fit line**\n"},{"metadata":{"_kg_hide-input":true,"trusted":true,"collapsed":true,"_uuid":"b74b9675279b455f96864cca96887a74c5790080"},"cell_type":"code","source":"# convert dates to pandas datetime format\ndata.Date = pd.to_datetime(data.Date, format='%m/%d/%Y %I:%M:%S %p')\n# setting the index to be the date will help us a lot later on\ndata.index = pd.DatetimeIndex(data.Date)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"da288e956359c227680c18184f31d7bae8ee2b60"},"cell_type":"code","source":"plt.figure(figsize=(11,6))\ndata.resample('BM').size().plot(legend=False)\nplt.title('Number of crimes per month (2001 - 2016)')\nplt.xlabel('Months')\nplt.ylabel('Number of crimes')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4265ab10d315bedb7fa8f7d4a0ab08e9fe44b1ec"},"cell_type":"markdown","source":"**From the looks of it, the overall crime scene seems to be decreasing from 2001 onwards, as the graph consistently follows a periodic and decreasing pattern. However, this does'nt provide the entire picture and to get a general idea its always better to plot individual crime patterns and then investigate as to what has decreased or increased over the years.**"},{"metadata":{"trusted":true,"_uuid":"8068b21fb4e077656e3b1dbfd7c331d4d43aa868","_kg_hide-input":true},"cell_type":"code","source":"crimes_count_date = data.pivot_table('ID', aggfunc=np.size, columns='Primary Type', index=data.index.date, fill_value=0)\ncrimes_count_date.index = pd.DatetimeIndex(crimes_count_date.index)\nplo = crimes_count_date.rolling(365).sum().plot(figsize=(30, 30), subplots=True, layout=(-1, 3), sharex=False, sharey=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"e58760a473b27ed5aa2708625353b7265e36f3e9"},"cell_type":"markdown","source":"**From the plots above, Battery charges were at it's least in 2016, Weapons Violation also decreased in 2016, although it did started rising after that, Theft was at it's lowest too in 2016, Stalking charges were very high though, Public Peace Violation was decreasing etc. Overall, positive takeaway is that most of the criminal charges were decreasing as 2016 approached compared to the previous years, although a few did rise significantly, like Stalking, Sex offences, Offences involving children, Interference with Public Officers etc. Seems like as the years advanced, victims were mostly from the feminine side or children side, maybe because as law enforcements cracked down severely on HVT's, these people became soft/easy targets for the offenders.**"},{"metadata":{"trusted":true,"_uuid":"640439000ac3d86ecda40b2577f7210691b7752d","_kg_hide-input":true},"cell_type":"code","source":"days = ['Monday','Tuesday','Wednesday',  'Thursday', 'Friday', 'Saturday', 'Sunday']\ndata.groupby([data.index.dayofweek]).size().plot(kind='barh', figsize=(5, 6))\nplt.ylabel('Days of the week')\nplt.yticks(np.arange(7), days)\nplt.xlabel('Number of crimes')\nplt.title('Number of crimes by day of the week')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"d124dbf666f3e317a7d2e755db78e51d7712a5fa"},"cell_type":"markdown","source":"**Well I guess nothing special out here, almost all the days experience the same number of crimes, the Friday count being slightly higher than the others, but still nothing significant to come to any conclusion.**"},{"metadata":{"trusted":true,"_uuid":"7d6741a49c75676aceaf9821a1002066b7bca2c7","_kg_hide-input":true},"cell_type":"code","source":"plt.figure(figsize=(8,10))\ndata.groupby([data['Primary Type']]).size().sort_values(ascending=True).plot(kind='barh')\nplt.title('Number of crimes by type')\nplt.ylabel('Crime Type')\nplt.xlabel('Number of crimes')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"11edcffe485ef4d4d69920428ae13bb27a813d88"},"cell_type":"markdown","source":"**Seems like Theft and Battery charges outnumbers the rest in Chicago, however it will not be 100% correct to comment that these are the only two major crimes that plague the city of Chicago as most crime incidents go unreported or unheard of , or probably some incidents got left out in the process of data collection, we will never know!**"},{"metadata":{"_uuid":"3b186bb0484c8a4e3d69b9cea9ef9ea870932486"},"cell_type":"markdown","source":"**The trends are getting pretty boring and repetitive or predictable, we all know that all crimes are not the same, some have a higher chance of occuring or be more frequent in occurence like theft/robbery than murder or homicide, it would be really cool if we could visualize some of the answers to questions like: Is theft or burglary is more likely to occur at a weekday compared to a weekend? Are they more likely to happen in the morning vs evening or late night ? are they more likely to occur in a street vs a bar? We will get to work on it with the pivot function of pandas.\n**"},{"metadata":{"trusted":true,"_uuid":"efbdbe006db3831c0ced50ef41a281066cb363b4","collapsed":true,"_kg_hide-input":true},"cell_type":"code","source":"hour_by_location = data.pivot_table(values='ID', index='Location Description', columns=data.index.hour, aggfunc=np.size).fillna(0)\nhour_by_type     = data.pivot_table(values='ID', index='Primary Type', columns=data.index.hour, aggfunc=np.size).fillna(0)\ndayofweek_by_type = data.pivot_table(values='ID', index='Primary Type', columns=data.index.dayofweek, aggfunc=np.size).fillna(0)\nlocation_by_type  = data.pivot_table(values='ID', index='Location Description', columns='Primary Type', aggfunc=np.size).fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2e9895a7f00d804e775d600401b67ac029be1806"},"cell_type":"markdown","source":"**For the purpose of plotting the heatmaps we will first do an AgglomerativeClustering on our dataset in order to group the rows in to meaningful clusters and the use those labels for the purpose of plotting our heatmaps. We will also scale the row values(z-scale) to have a mean of zero and unit variance and then use it for plotting our heatmaps.**"},{"metadata":{"trusted":true,"_uuid":"16015ee9291c26c2a8ecbe36d7953894d5fc3003","_kg_hide-input":true,"collapsed":true},"cell_type":"code","source":"from sklearn.cluster import AgglomerativeClustering as AC\n\ndef scale_df(df,axis=0):\n    return (df - df.mean(axis=axis)) / df.std(axis=axis)\n\n\ndef plot_hmap(df, ix=None, cmap='PuRd'):\n    if ix is None:\n        ix = np.arange(df.shape[0])\n    plt.imshow(df.iloc[ix,:], cmap=cmap)\n    plt.colorbar(fraction=0.03)\n    plt.yticks(np.arange(df.shape[0]), df.index[ix])\n    plt.xticks(np.arange(df.shape[1]))\n    plt.grid(False)\n    plt.show()\n    \ndef scale_and_plot(df, ix = None):\n    df_marginal_scaled = scale_df(df.T).T\n    if ix is None:\n        ix = AC(4).fit(df_marginal_scaled).labels_.argsort()\n    cap = np.min([np.max(df_marginal_scaled.as_matrix()), np.abs(np.min(df_marginal_scaled.as_matrix()))])\n    df_marginal_scaled = np.clip(df_marginal_scaled, -1*cap, cap)\n    plot_hmap(df_marginal_scaled, ix=ix)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"71e4ea2a722cd1f8545abf8361509ee1e32eec2f","_kg_hide-input":true},"cell_type":"code","source":"#CMAP = 'PuRd'\nplt.figure(figsize=(60,30))\nscale_and_plot(hour_by_location)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"8b76a0d52f041eed07392e28267ac756648c52eb"},"cell_type":"markdown","source":"**From the plot above, we can see that places like Police Facility or vehicle parking lots experience attacks mostly during the early morning(like around 3-5am, indicated by the dark purple coloring of the plot), Government buildings, Schools, College/University campus, Hospitals  all come under attack mostly during the midday timings(mostly 9-14). Fitting a regression line on such clusters would prove to be quite useful in predicting the next crime about to take place in such sites!**"},{"metadata":{"trusted":true,"_uuid":"cb43528042cb906142191631eecf53d336bbbab6","_kg_hide-input":true},"cell_type":"code","source":"#CMAP = 'OrRd'\nplt.figure(figsize=(20, 10))\nscale_and_plot(hour_by_type)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"461196cfae9b0982f1f20def46550a5b1a77aac1"},"cell_type":"markdown","source":"**Domestic violence, offense involving children, sex offense etc are all likely to occur late night at around 12 -14, burglary, theft mostly occuring early morning to mid day at around 8-12, probably because people are in their offices, children at school and broaddaylight is the least time someone would expect their house to get robbed, Homicide occuring mostly early in the morning like at around 2-3AM, probably because there wont be much people hanging around on the streets or whatever place it is at that time, leaving little to no witnesses. Next up, we will try and visualize what sort of crimes mostly occur in which places.**"},{"metadata":{"trusted":true,"_uuid":"7cc7e923e7854eaaeb8396a34da5bc42f176e380","_kg_hide-input":true},"cell_type":"code","source":"def normalize(df):\n    result = df.copy()\n    for feature_name in df.columns:\n        max_value = df[feature_name].max()\n        min_value = df[feature_name].min()\n        result[feature_name] = (df[feature_name] - min_value) / (max_value - min_value)\n    return result\n\ndf = normalize(location_by_type)\nix = AC(3).fit(df.T).labels_.argsort() \nplt.figure(figsize=(27, 12))\nplt.imshow(df.T.iloc[ix,:], cmap='Blues')\nplt.colorbar(fraction=0.03)\nplt.xticks(np.arange(df.shape[0]), df.index, rotation='vertical')\nplt.yticks(np.arange(df.shape[1]), df.columns)\nplt.title('Normalized location frequency for each crime')\nplt.grid(False)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"accc5116d9a06e530cb059e802c95b15967f00ed"},"cell_type":"markdown","source":"Notebook to be updated!"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"f156fd8d521b46a013420b8b61614054977c853d"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"850931d763ebe0d91862cc6b68c81ebd27c33e28"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}