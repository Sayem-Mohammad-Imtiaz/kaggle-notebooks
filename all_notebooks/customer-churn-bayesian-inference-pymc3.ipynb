{"cells":[{"metadata":{},"cell_type":"markdown","source":"# 1. Import required libaries\n## 1.1 Import computation libaries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n# Standard machine learning models\nfrom sklearn.linear_model import LogisticRegressionCV\n\n# Scikit-learn utilities\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score, confusion_matrix, roc_auc_score, roc_curve","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1.2 Import Bayesian related libaries"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# PyMC3 for Bayesian Inference\nimport pymc3 as pm\nprint(pm.__version__)\nimport arviz\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1.3 Import visualisation libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n%matplotlib inline\nplt.style.use('seaborn-darkgrid')\nfrom IPython.core.pylabtools import figsize\nimport matplotlib.lines as mlines\n\nimport seaborn as sns\nimport itertools\n\npd.options.mode.chained_assignment = None\n\n\nfrom warnings import filterwarnings\nfilterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Read data"},{"metadata":{"trusted":true},"cell_type":"code","source":"telcom = pd.read_csv(r\"/kaggle/input/telco-customer-churn/WA_Fn-UseC_-Telco-Customer-Churn.csv\")\n#first few rows\ntelcom.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.1 Basic data statistics"},{"metadata":{"trusted":true},"cell_type":"code","source":"print (\"Rows     : \" ,telcom.shape[0])\nprint (\"Columns  : \" ,telcom.shape[1])\nprint (\"\\nFeatures : \\n\" ,telcom.columns.tolist())\nprint (\"\\nMissing values :  \", telcom.isnull().sum().values.sum())\nprint (\"\\nUnique values :  \\n\",telcom.nunique())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Feature Engineering"},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in telcom.columns:\n    if len(telcom[i].unique())<10:\n        print(\"Column:{},Unique values:{}\".format(i,telcom[i].unique()))\n    else:\n        print(\"Column:{}Unique values:{}\".format(i,len(telcom[i].unique())))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Preparing dataframe for ML related algoithms by changing categorical variable in to dummies variable and changing \"YES\"/\"NO\" to 1/0 columns respectively"},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntelcom_dummies=pd.DataFrame()\nprint(\"Total number of rows before starting copying:{}\".format(len(telcom_dummies)))\n# len(telcom_dummies[telcom_dummies['TotalCharges'] == \" \"])\ntelcom_dummies = pd.get_dummies(telcom[['gender','PaymentMethod','Contract']], columns=['gender','PaymentMethod','Contract'])\ntelcom_dummies['SeniorCitizen'] =telcom['SeniorCitizen']\ntelcom_dummies['Partner'] = telcom['Partner'].map(lambda s :1  if s =='Yes' else 0)\ntelcom_dummies['Dependents'] = telcom['Dependents'].map(lambda s :1  if s =='Yes' else 0)\ntelcom_dummies['tenure']=telcom['tenure']\ntelcom_dummies['PhoneService'] = telcom['PhoneService'].map(lambda s :1  if s =='Yes' else 0)\ntelcom_dummies['MultipleLines'] = telcom['MultipleLines'].map(lambda s :1  if s =='Yes' else 0)\ntelcom_dummies['Has_InternetService'] = telcom['InternetService'].map(lambda s :0  if s =='No' else 1)\ntelcom_dummies['Fiber_optic'] = telcom['InternetService'].map(lambda s :1  if s =='Fiber optic' else 0)\ntelcom_dummies['DSL'] = telcom['InternetService'].map(lambda s :1  if s =='DSL' else 0)\ntelcom_dummies['OnlineSecurity'] = telcom['OnlineSecurity'].map(lambda s :1  if s =='Yes' else 0)\ntelcom_dummies['OnlineBackup'] = telcom['OnlineBackup'].map(lambda s :1  if s =='Yes' else 0)\ntelcom_dummies['DeviceProtection'] = telcom['DeviceProtection'].map(lambda s :1  if s =='Yes' else 0)\ntelcom_dummies['TechSupport'] = telcom['TechSupport'].map(lambda s :1  if s =='Yes' else 0)\ntelcom_dummies['StreamingTV'] = telcom['StreamingTV'].map(lambda s :1  if s =='Yes' else 0)\ntelcom_dummies['StreamingMovies'] = telcom['StreamingMovies'].map(lambda s :1  if s =='Yes' else 0)\ntelcom_dummies['PaperlessBilling'] = telcom['PaperlessBilling'].map(lambda s :1  if s =='Yes' else 0)\ntelcom_dummies['MonthlyCharges']=telcom['MonthlyCharges']\ntelcom_dummies['TotalCharges'] = pd.to_numeric(telcom['TotalCharges'],errors='coerce')\nprint(\"Total number of rows after  copying:{}\".format(len(telcom_dummies)))\n      #Counting number of na\nprint(\"Number of NA\")\nprint(len(telcom_dummies) - telcom_dummies.count())\ntelcom_dummies.dropna(axis=0,inplace=True)\nprint(\"Total number of rows after removing NA:{}\".format(len(telcom_dummies)))\ntelcom_dummies['Churn']=telcom['Churn'].map(lambda s :1  if s =='Yes' else 0)\ntelcom_dummies.rename(columns={\"PaymentMethod_Bank transfer (automatic)\" :\"paymnt_mthd_bank_auto\",\n\"PaymentMethod_Credit card (automatic)\"  : \"paymnt_mthd_cc_auto\",\n\"PaymentMethod_Electronic check\"   :\"paymnt_mthd_elc_check\",\n\"PaymentMethod_Mailed check\"       :\"paymnt_mthd_mailed_check\",         \n\"Contract_Month-to-month\":\"cont_mnth_to_mnth\",                    \n\"Contract_One year\"  :\"cont_1_yr\",                       \n\"Contract_Two year\"    :\"cont_2_yr\" },inplace=True)\ntelcom_dummies.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Validating the transformation"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Checking if columns are ready to apply ML algorithm\")\nfor i in telcom_dummies.columns:\n    if len(telcom_dummies[i].unique())<10:\n        print(\"Column:{},Unique values:{},Type:{}\".format(i,telcom_dummies[i].unique(),telcom_dummies[i].dtypes))\n    else:\n        print(\"Column:{}Unique values:{},Type:{}\".format(i,len(telcom_dummies[i].unique()),telcom_dummies[i].dtypes))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. Creating test train datset\nUsing test train split and creating dataframe with X_train,y_train,X_test and y_test"},{"metadata":{"trusted":true},"cell_type":"code","source":"y = telcom_dummies['Churn'].values\nX = telcom_dummies.loc[:, telcom_dummies.columns != 'Churn']\nfrom sklearn.preprocessing import MinMaxScaler\nfeatures = X.columns.values\nscaler = MinMaxScaler(feature_range = (0,1))\nscaler.fit(X)\nX = pd.DataFrame(scaler.transform(X))\nX.columns = features\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5.  Modeling "},{"metadata":{},"cell_type":"markdown","source":"## 5.1 Custom function to calculate the model metrics "},{"metadata":{},"cell_type":"markdown","source":"Defining helper function and calculating the baseline model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate the accuracy and f1 score of a model\ndef calc_metrics(predictions, y_test):\n    accuracy = np.mean(predictions == y_test)\n    f1_metric = f1_score(y_test, predictions)\n\n    print('Accuracy of Model: {:.2f}%'.format(100 * accuracy))\n    print('F1 Score of Model: {:.4f}'.format(f1_metric))\nbaseline_pred = [0 for _ in range(len(y_test))]\ncalc_metrics(baseline_pred, y_test)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"   ## 5.2 Logistic regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = LogisticRegressionCV(Cs= 20, cv = 3, scoring = 'f1', \n                          penalty = 'l2', random_state = 42)\nlr.fit(X_test, y_test)\n\n# Make predictions and evaluate\nlr_pred = lr.predict(X_test)\ncalc_metrics(lr_pred, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"  ## 5.3 Bayesian Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Build up a formula\nformula = [' %s + ' % variable for variable in X_test.columns]\nformula.insert(0, 'y ~ ')\nformula = ' '.join(''.join(formula).split(' ')[:-2])\nformula","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Intercept: {:0.4f}'.format(lr.intercept_[0]))\nfor feature, weight in zip(X_test.columns, lr.coef_[0]):\n    print('Feature: {:30} Weight: {:0.4f}'.format(feature, weight))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Appying normal non-informative prior to Intercept,MonthlyCharges and TotalCharges.Applying Uniform prior to rest of the indicator variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_with_labels = X_train.copy()\nX_with_labels['y'] = y_train\nwith pm.Model() as logistic_model:\n    priors=dict()\n    \n    for variable in X_test.columns:\n        priors[variable]=pm.Uniform.dist(0,1)\n    priors['Intercept']=pm.Normal.dist(mu=0., sigma=100.)\n    priors['MonthlyCharges']=pm.Normal.dist(mu=0., sigma=100.)\n    priors['TotalCharges'] = pm.Normal.dist(mu=0., sigma=100.)\n    # Build the model using the formula and specify the data likelihood \n    pm.GLM.from_formula(formula, data = X_with_labels, family = pm.glm.families.Binomial(),priors=priors)\n    \n    # Using the no-uturn sampler\n    sampler = pm.NUTS()\n    \n    # Sample from the posterior using NUTS\n    trace_log = pm.sample(draws=2000, step = sampler, chains=1, tune=1000, random_seed=100,init='adapt_diag')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Saving the model to pickle file(in case we don't want to run model again)\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pickle\nfileObject = open(\"all_parameters.pickle\",'wb')  \npickle.dump(trace_log, fileObject)\nfileObject.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trace_log_from_file= pickle.load(open(\"all_parameters.pickle\",'rb')  )\n#trace_log=trace_log_from_file   #Uncomment this line if we don't want to run model again","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"figsize(10, 12)\npm.forestplot(trace_log);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Intecept ,Monthly charges,Total charges,Gender_Male shows variation most variation in estimation"},{"metadata":{"trusted":true},"cell_type":"code","source":"pm.plot_posterior(trace_log);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pm.summary(trace_log)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def evaluate_trace(trace, data, print_model = False):\n    means_dict = {}\n    std_dict = {}\n    \n    for var in trace.varnames:\n        means_dict[var] = np.mean(trace[var])\n        std_dict[var] = np.std(trace[var])\n    \n    model = 'logit = %0.4f + ' % np.mean(means_dict['Intercept'])\n    \n    for var in data.columns:\n        model += '%0.4f * %s + ' % (means_dict[var], var)\n    \n    model = ' '.join(model.split(' ')[:-2])\n    if print_model:\n        print('Final Equation: \\n{}'.format(model))\n    \n    return means_dict, std_dict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"means_dict, std_dict = evaluate_trace(trace_log, X_train, print_model=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Find a single probabilty estimate using the mean value of variables in a trace\ndef find_probs(trace, data):\n    \n    # Find the means and std of the variables\n    means_dict1, std_dict = evaluate_trace(trace, data)\n          \n    probs = []\n       \n    \n    # Need an intercept term in the data\n    data['Intercept'] = 1\n    l_means_dict=dict()\n    for c in data.columns:\n        \n        l_means_dict[c]=means_dict1[c]\n    \n    data = data[list(l_means_dict.keys())]\n    mean_array = np.array(list(l_means_dict.values()))\n    # Calculate the probability for each observation in the data\n    for _, row in data.iterrows():\n        # First the log odds\n        logit = np.dot(row, mean_array)\n        # Convert the log odds to a probability\n        probability = 1 / (1 + np.exp(-logit))\n        probs.append(probability)\n        \n    return probs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"blr_probs = find_probs(trace_log, X_test.copy())\n\n# Threshold the values at 0.5\npredictions = (np.array(blr_probs) > 0.5)\ncalc_metrics(predictions, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Bayesian Logistic regression after removing insignificant features like:\nFollowing ingnisficant features are removed:\n* MonthlyCharges\n* paymnt_mthd_cc_auto\n* cont_1_yr\n* cont_2_yr\n* Partner\n* Dependents\n* Tenure\n* PhoneService\n* DSL\n* OnlineSecurity\n* OnlineBackup\n* DeviceProtection\n* TechSupport\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test2=X_test[X_test.columns.difference(['MonthlyCharges', 'paymnt_mthd_cc_auto', 'cont_1_yr', 'cont_2_yr', 'Partner', 'Dependents', 'Tenure', 'PhoneService', 'DSL', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport'])]\n# Build up a formula\nformula1 = [' %s + ' % variable for variable in X_test2.columns]\nformula1.insert(0, 'y ~ ')\nformula1 = ' '.join(''.join(formula1).split(' ')[:-2])\nformula1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with pm.Model() as logistic_model1:\n    \n    # Build the model using the formula and specify the data likelihood \n    priors=dict()\n    for variable in X_test2.columns:\n        priors[variable]=pm.Uniform.dist(0,1)\n    priors['Intercept']=pm.Normal.dist(mu=0., sigma=100.)\n    priors['MonthlyCharges']=pm.Normal.dist(mu=0., sigma=100.)\n    priors['TotalCharges'] = pm.Normal.dist(mu=0., sigma=100.)\n              \n    pm.GLM.from_formula(formula1, data = X_with_labels, family = pm.glm.families.Binomial(),priors=priors)\n    \n    # Using the no-uturn sampler\n    sampler = pm.NUTS()\n    \n    # Sample from the posterior using NUTS\n    trace_log1 = pm.sample(draws=2000, step = sampler, chains=1, tune=1000, random_seed=100,init='adapt_diag')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pm.plot_posterior(trace_log);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pm.summary(trace_log1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Saving the model to pickle file(in case we don't want to run model again)"},{"metadata":{"trusted":true},"cell_type":"code","source":"fileObject = open(\"sign_parameters.pickle\",'wb')  \npickle.dump(trace_log1, fileObject)\nfileObject.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trace_log1_frm_file= pickle.load(open(\"sign_parameters.pickle\",'rb')  )\n#trace_log1=trace_log1_frm_file #Uncomment this line if we want to load the model from static file","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"means_dict_sign, std_dict_sign = evaluate_trace(trace_log1, X_test2, print_model=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"blr1_probs = find_probs(trace_log1, X_test2)\n\n# Threshold the values at 0.5\npredictions = (np.array(blr1_probs) > 0.5)\ncalc_metrics(predictions, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**There is very marginal improvement between the two models.Second model perform better**"},{"metadata":{"trusted":true},"cell_type":"code","source":"logistic_model.name='all_parm'\nlogistic_model1.name='sign_parm'\nmodel_trace_dict = {'all_parm':trace_log,\n                   'sign_parm':trace_log1}\ndfwaic = pm.compare(model_trace_dict)\npm.compareplot(dfwaic);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dfwaicloo = pm.compare(model_trace_dict, ic='LOO')\npm.compareplot(dfwaicloo);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(dfwaic)\nprint(dfwaicloo)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusion:\nBayesian models performed more or less similar to frequentist model and with model tuning it can be made to perform better.\nFollowing are next steps:\n* With additional feature engineering Bayesian model can be fine tuned\n* Based on feature distribution can look to add more relevant prior\n* Can try different model apart from logit\n"},{"metadata":{},"cell_type":"markdown","source":"## References:\n1.\tDataset reference:\nhttps://www.kaggle.com/blastchar/telco-customer-churn\n2.\tFor EDA and problem description:\nhttps://www.kaggle.com/pavanraj159/telecom-customer-churn-prediction\n3.\tBayesian Logistic regression using PyMC3\nhttps://docs.pymc.io/notebooks/GLM-logistic.html\n4.\tBayesian model selection\nhttps://docs.pymc.io/notebooks/GLM-model-selection.html\n\n"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}