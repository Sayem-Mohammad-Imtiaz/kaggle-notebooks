{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Please dont forget to Upvote if your learned something new :)**","metadata":{}},{"cell_type":"markdown","source":"**One Notebook: Feature Engineering-->Knn Folds-->Level0:-XGBoost|CatBoost|LightGBM-->Level1:-XgBoost|RandomForest:-Submission,csv :D**","metadata":{}},{"cell_type":"markdown","source":"Thakns to Luca Massaron for sharing stratify folds based ona k-means","metadata":{}},{"cell_type":"markdown","source":"In this competition there are a lot of discussions and notebooks on hyper-parameter tuning, XGBoost and stacking, since they are the most effective techniques.\n\nYet, something is missing. For instance, there little reasoning has been done on the best way to cross-validate.\n\nActually, if you watch carefully the data, it seems like data distributions are segregated into specific portions of space, something reminiscent ot me of the Madelon dataset created by Isabelle Guyon, one of the ideators of the Support Vector Machines (see for Guyon's contribution: https://www.kdnuggets.com/2016/07/guyon-data-mining-history-svm-support-vector-machines.html for the Madelon dataset see instead: https://archive.ics.uci.edu/ml/datasets/madelon).\n\nI therefore tried to stratifiy my folds based on a k-means clustering of the non-noisy data (see https://www.kaggle.com/c/30-days-of-ml/discussion/267931) and my local cv has become more reliable (very correlated with the public leaderboard) and my models are performing much better with cv prediction.\n\nTry it and let me know, if it works also on your models!\n\nHappy Kaggling!","metadata":{}},{"cell_type":"code","source":"# Importing core libraries\nimport numpy as np\nimport pandas as pd\nimport joblib\n\n# Importing from Scikit-Learn\nfrom sklearn.model_selection import StratifiedKFold, KFold\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\n\nimport numpy as np\nimport pandas as pd\nimport scipy as sp\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold, KFold\nfrom sklearn.metrics import mean_squared_error\nfrom mlxtend.preprocessing import minmax_scaling\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor\nimport optuna\nfrom sklearn import preprocessing\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\n\nimport joblib\nimport os","metadata":{"execution":{"iopub.status.busy":"2021-08-30T04:49:27.549911Z","iopub.execute_input":"2021-08-30T04:49:27.550404Z","iopub.status.idle":"2021-08-30T04:49:31.247627Z","shell.execute_reply.started":"2021-08-30T04:49:27.550317Z","shell.execute_reply":"2021-08-30T04:49:31.246785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Loading data \nX_train_full = pd.read_csv(\"../input/30-days-of-ml/train.csv\")\nX_test_full = pd.read_csv(\"../input/30-days-of-ml/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-08-30T04:49:31.249412Z","iopub.execute_input":"2021-08-30T04:49:31.250146Z","iopub.status.idle":"2021-08-30T04:49:35.496178Z","shell.execute_reply.started":"2021-08-30T04:49:31.250097Z","shell.execute_reply":"2021-08-30T04:49:35.495174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Preparing data as a tabular matrix\nX_train=X_train_full.copy()\nX_test=X_test_full.copy()\ny_train = X_train.target\nX_train = X_train.set_index('id').drop('target', axis='columns')\nX_test = X_test.set_index('id')","metadata":{"execution":{"iopub.status.busy":"2021-08-30T04:49:35.498111Z","iopub.execute_input":"2021-08-30T04:49:35.498519Z","iopub.status.idle":"2021-08-30T04:49:35.742088Z","shell.execute_reply.started":"2021-08-30T04:49:35.498484Z","shell.execute_reply":"2021-08-30T04:49:35.74101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Pointing out categorical features\ncategoricals = [item for item in X_train.columns if 'cat' in item]","metadata":{"execution":{"iopub.status.busy":"2021-08-30T04:49:35.745466Z","iopub.execute_input":"2021-08-30T04:49:35.745824Z","iopub.status.idle":"2021-08-30T04:49:35.750487Z","shell.execute_reply.started":"2021-08-30T04:49:35.745791Z","shell.execute_reply":"2021-08-30T04:49:35.749718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Dealing with categorical data using get_dummies\ndummies = pd.get_dummies(X_train.append(X_test)[categoricals])\nX_train[dummies.columns] = dummies.iloc[:len(X_train), :]\nX_test[dummies.columns] = dummies.iloc[len(X_train): , :]\ndel(dummies)","metadata":{"execution":{"iopub.status.busy":"2021-08-30T04:49:35.751665Z","iopub.execute_input":"2021-08-30T04:49:35.752163Z","iopub.status.idle":"2021-08-30T04:49:36.508578Z","shell.execute_reply.started":"2021-08-30T04:49:35.752129Z","shell.execute_reply":"2021-08-30T04:49:36.507389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Dealing with cuniquegorical data using OrdinalEncoder (only when there are 3 or more levels)\nordinal_encoder = OrdinalEncoder()\nX_train[categoricals[3:]] = ordinal_encoder.fit_transform(X_train[categoricals[3:]]).astype(int)\nX_test[categoricals[3:]] = ordinal_encoder.transform(X_test[categoricals[3:]]).astype(int)\nX_train = X_train.drop(categoricals[:3], axis=\"columns\")\nX_test = X_test.drop(categoricals[:3], axis=\"columns\")","metadata":{"execution":{"iopub.status.busy":"2021-08-30T04:49:36.510212Z","iopub.execute_input":"2021-08-30T04:49:36.51067Z","iopub.status.idle":"2021-08-30T04:49:39.774107Z","shell.execute_reply.started":"2021-08-30T04:49:36.510625Z","shell.execute_reply":"2021-08-30T04:49:39.772877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Feature selection (https://www.kaggle.com/lucamassaron/tutorial-feature-selection-with-boruta-shap)\nimportant_features = ['cat1_A', 'cat1_B', 'cat5', 'cat8', 'cat8_C', 'cat8_E', 'cont0', \n                      'cont1', 'cont10', 'cont11', 'cont12', 'cont13', 'cont2', 'cont3', \n                      'cont4', 'cont5', 'cont6', 'cont7', 'cont8', 'cont9']\n\ncategoricals = ['cat5', 'cat8']\n\nX_train = X_train[important_features]\nX_test = X_test[important_features]","metadata":{"execution":{"iopub.status.busy":"2021-08-30T04:49:39.775406Z","iopub.execute_input":"2021-08-30T04:49:39.775697Z","iopub.status.idle":"2021-08-30T04:49:39.798529Z","shell.execute_reply.started":"2021-08-30T04:49:39.775668Z","shell.execute_reply":"2021-08-30T04:49:39.797538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Stratifying the data\n\npca = PCA(n_components=16, random_state=0)\nkm = KMeans(n_clusters=32, random_state=0)\n\npca.fit(X_train)\nkm.fit(pca.transform(X_train))\n\nprint(np.unique(km.labels_, return_counts=True))\n\ny_stratified = km.labels_","metadata":{"execution":{"iopub.status.busy":"2021-08-30T04:49:39.801397Z","iopub.execute_input":"2021-08-30T04:49:39.801715Z","iopub.status.idle":"2021-08-30T04:50:15.846678Z","shell.execute_reply.started":"2021-08-30T04:49:39.801683Z","shell.execute_reply":"2021-08-30T04:50:15.845606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating your folds for repeated use (for instance, stacking)\n#To save some time, I am using already saved Knn fold\n\"\"\"\nfolds = 10\n#seeds = [42, 0, 101]\nseeds = [42]\nfold_idxs = list()\n\nfor seed in seeds:\n    skf = StratifiedKFold(n_splits=folds,\n                          shuffle=True, \n                          random_state=seed)\n\n    fold_idxs.append(list(skf.split(X_train, y_stratified)))\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2021-08-30T04:50:15.84896Z","iopub.execute_input":"2021-08-30T04:50:15.849674Z","iopub.status.idle":"2021-08-30T04:50:15.86084Z","shell.execute_reply.started":"2021-08-30T04:50:15.849627Z","shell.execute_reply":"2021-08-30T04:50:15.859909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\n# Checking the produced folds\nfor j, fold_idxs_ in enumerate(fold_idxs):\n    print(f\"\\n--- seed={seeds[j]} ---\")\n    for k, (train_idx, validation_idx) in enumerate(fold_idxs_):\n        print(f\"fold {k} train idxs: {len(train_idx)} validation idxs: {len(validation_idx)} -> {validation_idx[:10]}\")\n\"\"\"        ","metadata":{"execution":{"iopub.status.busy":"2021-08-30T04:50:15.864438Z","iopub.execute_input":"2021-08-30T04:50:15.864774Z","iopub.status.idle":"2021-08-30T04:50:15.875832Z","shell.execute_reply.started":"2021-08-30T04:50:15.864742Z","shell.execute_reply":"2021-08-30T04:50:15.874911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Loading existing already created Fold\nsample_submission = pd.read_csv(\"../input/30-days-of-ml/sample_submission.csv\")\n# Storing into the notebook for future use\n#joblib.dump(fold_idxs, './fold_idxs.job')\n# Retrieving from the notebook\nseeds = [42]\nfold_idxs = joblib.load('../input/30daysfoldknn/fold_idxs.job')","metadata":{"execution":{"iopub.status.busy":"2021-08-30T04:50:15.878407Z","iopub.execute_input":"2021-08-30T04:50:15.880462Z","iopub.status.idle":"2021-08-30T04:50:16.176023Z","shell.execute_reply.started":"2021-08-30T04:50:15.880417Z","shell.execute_reply":"2021-08-30T04:50:16.17505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***#XGboost to create Level0 prediction\n#Parameteres are already tuned with Optuna***","metadata":{}},{"cell_type":"code","source":"\"\"\"\n# Checking the produced folds\nfinal_test_predictions = []\nfinal_valid_predictions = {}\nscores = []\nfor j, fold_idxs_ in enumerate(fold_idxs):\n    print(f\"\\n--- seed={seeds[j]} ---\")\n    for k, (train_idx, validation_idx) in enumerate(fold_idxs_):\n        print(f\"fold {k} train idxs: {len(train_idx)} validation idxs: {len(validation_idx)} -> {validation_idx[:10]}\")\n        fold=k\n        xtrain =  X_train.iloc[train_idx].reset_index(drop=True)\n        xvalid = X_train.iloc[validation_idx].reset_index(drop=True)\n        xtest = X_test.copy()\n        valid_ids = X_train_full.iloc[validation_idx]['id']\n        \n        ytrain = X_train_full.iloc[train_idx].target\n        yvalid = X_train_full.iloc[validation_idx].target\n    \n        params = {'learning_rate': 0.03097703420625469, 'reg_lambda': 0.03821095322549157, \n              'reg_alpha': 23.13181079976304, 'subsample': 0.9494442458700542, \n              'colsample_bytree': 0.11807135201147481, 'max_depth': 3,'gamma': 0, 'min_child_weight': 1}\n    \n    \n        model = XGBRegressor(\n            random_state=42,\n            n_estimators=10000,\n            **params\n        )\n        model.fit(xtrain, ytrain)\n        preds_valid = model.predict(xvalid)\n        test_preds = model.predict(xtest)\n        final_test_predictions.append(test_preds)\n        final_valid_predictions.update(dict(zip(valid_ids, preds_valid)))\n        rmse = mean_squared_error(yvalid, preds_valid, squared=False)\n        print(fold, rmse)\n        scores.append(rmse)\n\nprint(np.mean(scores), np.std(scores))\n\nfinal_valid_predictions = pd.DataFrame.from_dict(final_valid_predictions, orient=\"index\").reset_index()\nfinal_valid_predictions.columns = [\"id\", \"pred_1\"]\nfinal_valid_predictions.to_csv(\"level0_train_pred_1.csv\", index=False)\n\nsample_submission.target = np.mean(np.column_stack(final_test_predictions), axis=1)\nsample_submission.columns = [\"id\", \"pred_1\"]\nsample_submission.to_csv(\"level0_test_pred_1.csv\", index=False)\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2021-08-30T04:50:16.180083Z","iopub.execute_input":"2021-08-30T04:50:16.180419Z","iopub.status.idle":"2021-08-30T04:50:16.193452Z","shell.execute_reply.started":"2021-08-30T04:50:16.180382Z","shell.execute_reply":"2021-08-30T04:50:16.192515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**#Catboost to create Level0 prediction\n#Parameteres are already tuned with Optuna**","metadata":{}},{"cell_type":"code","source":"\"\"\"\n# Checking the produced folds\nfinal_test_predictions = []\nfinal_valid_predictions = {}\nscores = []\nfor j, fold_idxs_ in enumerate(fold_idxs):\n    print(f\"\\n--- seed={seeds[j]} ---\")\n    for k, (train_idx, validation_idx) in enumerate(fold_idxs_):\n        print(f\"fold {k} train idxs: {len(train_idx)} validation idxs: {len(validation_idx)} -> {validation_idx[:10]}\")\n        fold=k\n        xtrain =  X_train.iloc[train_idx].reset_index(drop=True)\n        xvalid = X_train.iloc[validation_idx].reset_index(drop=True)\n        xtest = X_test.copy()\n        valid_ids = X_train_full.iloc[validation_idx]['id']\n        \n        ytrain = X_train_full.iloc[train_idx].target\n        yvalid = X_train_full.iloc[validation_idx].target\n    \n        params = {'random_state':2020,'learning_rate': 0.09371412415401288, 'l2_leaf_reg': 5.708567720284058, \n              'max_bin': 355, 'subsample': 0.7102809231581142, \n              'min_data_in_leaf': 165, 'max_depth': 7}\n    \n        model = CatBoostRegressor(\n            n_estimators=25000,\n            **params\n        )\n        model.fit(xtrain, ytrain)\n        preds_valid = model.predict(xvalid)\n        test_preds = model.predict(xtest)\n        final_test_predictions.append(test_preds)\n        final_valid_predictions.update(dict(zip(valid_ids, preds_valid)))\n        rmse = mean_squared_error(yvalid, preds_valid, squared=False)\n        print(fold, rmse)\n        scores.append(rmse)\n\nprint(np.mean(scores), np.std(scores))\n\nfinal_valid_predictions = pd.DataFrame.from_dict(final_valid_predictions, orient=\"index\").reset_index()\nfinal_valid_predictions.columns = [\"id\", \"pred_2\"]\nfinal_valid_predictions.to_csv(\"level0_train_pred_2.csv\", index=False)\n\nsample_submission.target = np.mean(np.column_stack(final_test_predictions), axis=1)\nsample_submission.columns = [\"id\", \"pred_2\"]\nsample_submission.to_csv(\"level0_test_pred_2.csv\", index=False)\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2021-08-30T04:50:16.197996Z","iopub.execute_input":"2021-08-30T04:50:16.200109Z","iopub.status.idle":"2021-08-30T04:50:16.211858Z","shell.execute_reply.started":"2021-08-30T04:50:16.200063Z","shell.execute_reply":"2021-08-30T04:50:16.211039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**#LightGBM to create Level0 prediction**","metadata":{}},{"cell_type":"code","source":"\"\"\"\n# Checking the produced folds\nfinal_test_predictions = []\nfinal_valid_predictions = {}\nscores = []\nfor j, fold_idxs_ in enumerate(fold_idxs):\n    print(f\"\\n--- seed={seeds[j]} ---\")\n    for k, (train_idx, validation_idx) in enumerate(fold_idxs_):\n        print(f\"fold {k} train idxs: {len(train_idx)} validation idxs: {len(validation_idx)} -> {validation_idx[:10]}\")\n        fold=k\n        xtrain =  X_train.iloc[train_idx].reset_index(drop=True)\n        xvalid = X_train.iloc[validation_idx].reset_index(drop=True)\n        xtest = X_test.copy()\n        valid_ids = X_train_full.iloc[validation_idx]['id']\n        \n        ytrain = X_train_full.iloc[train_idx].target\n        yvalid = X_train_full.iloc[validation_idx].target\n    \n        params = {'metric': 'RMSE','feature_pre_filter': False,\n              'lambda_l1': 0.45,\n              'lambda_l2': 4.8,\n              'learning_rate': 0.008,\n              'num_trees': 50000,\n              'num_leaves': 10, \n              'feature_fraction': 0.4, \n              'bagging_fraction': 1.0, \n              'bagging_freq': 0, \n              'min_child_samples': 100,\n              'num_threads': 4}\n    \n    \n        model = LGBMRegressor(**params)\n        model.fit(xtrain, ytrain)\n        preds_valid = model.predict(xvalid)\n        test_preds = model.predict(xtest)\n        final_test_predictions.append(test_preds)\n        final_valid_predictions.update(dict(zip(valid_ids, preds_valid)))\n        rmse = mean_squared_error(yvalid, preds_valid, squared=False)\n        print(fold, rmse)\n        scores.append(rmse)\n\nprint(np.mean(scores), np.std(scores))\n\nfinal_valid_predictions = pd.DataFrame.from_dict(final_valid_predictions, orient=\"index\").reset_index()\nfinal_valid_predictions.columns = [\"id\", \"pred_3\"]\nfinal_valid_predictions.to_csv(\"level0_train_pred_3.csv\", index=False)\n\nsample_submission.target = np.mean(np.column_stack(final_test_predictions), axis=1)\nsample_submission.columns = [\"id\", \"pred_3\"]\nsample_submission.to_csv(\"level0_test_pred_3.csv\", index=False)\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2021-08-30T04:50:16.217065Z","iopub.execute_input":"2021-08-30T04:50:16.219333Z","iopub.status.idle":"2021-08-30T04:50:16.231058Z","shell.execute_reply.started":"2021-08-30T04:50:16.219264Z","shell.execute_reply":"2021-08-30T04:50:16.230227Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**#Getting level 0 prediction across all 3 models and stacking-up**","metadata":{}},{"cell_type":"code","source":"#df = pd.read_csv(\"../input/summary30daysfinalstacking/train_folds_10.csv\")\n#df_test = pd.read_csv(\"../input/30-days-of-ml/test.csv\")\ndf=X_train.copy()\ndf_test=X_test.copy()\n#df['id']=X_train.index\n#df_test['id']=X_test.index\n\nsample_submission = pd.read_csv(\"../input/30-days-of-ml/sample_submission.csv\")\n\ndf1 = pd.read_csv(\"../input/30daysfoldknn/level0_train_pred_1.csv\")\ndf2 = pd.read_csv(\"../input/30daysfoldknn/level0_train_pred_2.csv\")\ndf3 = pd.read_csv(\"../input/30daysfoldknn/level0_train_pred_3.csv\")\n\ndf_test1 = pd.read_csv(\"../input/30daysfoldknn/level0_test_pred_1.csv\")\ndf_test2 = pd.read_csv(\"../input/30daysfoldknn/level0_test_pred_2.csv\")\ndf_test3 = pd.read_csv(\"../input/30daysfoldknn/level0_test_pred_3.csv\")\n\ndf = df.merge(df1, on=\"id\", how=\"left\")\ndf = df.merge(df2, on=\"id\", how=\"left\")\ndf = df.merge(df3, on=\"id\", how=\"left\")\n\ndf_test = df_test.merge(df_test1, on=\"id\", how=\"left\")\ndf_test = df_test.merge(df_test2, on=\"id\", how=\"left\")\ndf_test = df_test.merge(df_test3, on=\"id\", how=\"left\")","metadata":{"execution":{"iopub.status.busy":"2021-08-30T04:50:16.235417Z","iopub.execute_input":"2021-08-30T04:50:16.237694Z","iopub.status.idle":"2021-08-30T04:50:17.622941Z","shell.execute_reply.started":"2021-08-30T04:50:16.23765Z","shell.execute_reply":"2021-08-30T04:50:17.621899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**#Using level 0 prediction to create level 1 prediction on XGBoost**","metadata":{}},{"cell_type":"code","source":"useful_features = [\"pred_1\", \"pred_2\",\"pred_3\"]\ndf_test = df_test[useful_features]\n\nfinal_test_predictions = []\nfinal_valid_predictions = {}\nscores = []\nfor j, fold_idxs_ in enumerate(fold_idxs):\n    print(f\"\\n--- seed={seeds[j]} ---\")\n    for k, (train_idx, validation_idx) in enumerate(fold_idxs_):\n        print(f\"fold {k} train idxs: {len(train_idx)} validation idxs: {len(validation_idx)} -> {validation_idx[:10]}\")\n        fold=k\n        xtrain =  df.iloc[train_idx].reset_index(drop=True)\n        xvalid = df.iloc[validation_idx].reset_index(drop=True)\n        xtest = df_test.copy()\n        valid_ids = xvalid.id.values.tolist()\n        \n        ytrain = X_train_full.iloc[train_idx].target\n        yvalid = X_train_full.iloc[validation_idx].target\n    \n        xtrain = xtrain[useful_features]\n        xvalid = xvalid[useful_features]\n\n        params = {\n        'random_state': 1, \n        'booster': 'gbtree',\n        'n_estimators': 7000,\n        'learning_rate': 0.03,\n        'max_depth': 2\n        }\n    \n        model = XGBRegressor(\n        n_jobs=4,\n        **params\n        )\n        model.fit(xtrain, ytrain, early_stopping_rounds=300, eval_set=[(xvalid, yvalid)], verbose=1000)\n        preds_valid = model.predict(xvalid)\n        test_preds = model.predict(xtest)\n        final_test_predictions.append(test_preds)\n        final_valid_predictions.update(dict(zip(valid_ids, preds_valid)))\n        rmse = mean_squared_error(yvalid, preds_valid, squared=False)\n        print(fold, rmse)\n        scores.append(rmse)\n\nprint(np.mean(scores), np.std(scores))\nfinal_valid_predictions = pd.DataFrame.from_dict(final_valid_predictions, orient=\"index\").reset_index()\nfinal_valid_predictions.columns = [\"id\", \"pred_1\"]\nfinal_valid_predictions.to_csv(\"level1_train_pred_1.csv\", index=False)\n\nsample_submission.target = np.mean(np.column_stack(final_test_predictions), axis=1)\nsample_submission.columns = [\"id\", \"pred_1\"]\nsample_submission.to_csv(\"level1_test_pred_1.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-08-30T04:55:00.175217Z","iopub.execute_input":"2021-08-30T04:55:00.175885Z","iopub.status.idle":"2021-08-30T04:55:07.292781Z","shell.execute_reply.started":"2021-08-30T04:55:00.175842Z","shell.execute_reply":"2021-08-30T04:55:07.29087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**#Using level 0 prediction to create level 1 prediction on Random Forest**","metadata":{}},{"cell_type":"code","source":"sample_submission = pd.read_csv(\"../input/30-days-of-ml/sample_submission.csv\")\nuseful_features = [\"pred_1\", \"pred_2\",\"pred_3\"]\ndf_test = df_test[useful_features]\n\nfinal_test_predictions = []\nfinal_valid_predictions = {}\nscores = []\nfor j, fold_idxs_ in enumerate(fold_idxs):\n    print(f\"\\n--- seed={seeds[j]} ---\")\n    for k, (train_idx, validation_idx) in enumerate(fold_idxs_):\n        print(f\"fold {k} train idxs: {len(train_idx)} validation idxs: {len(validation_idx)} -> {validation_idx[:10]}\")\n        fold=k\n        xtrain =  df.iloc[train_idx].reset_index(drop=True)\n        xvalid = df.iloc[validation_idx].reset_index(drop=True)\n        xtest = df_test.copy()\n        valid_ids = xvalid.id.values.tolist()\n        \n        ytrain = X_train_full.iloc[train_idx].target\n        yvalid = X_train_full.iloc[validation_idx].target\n    \n        xtrain = xtrain[useful_features]\n        xvalid = xvalid[useful_features]\n    \n\n        model = RandomForestRegressor(n_estimators=1000, n_jobs=-1, max_depth=3)\n        \n        model.fit(xtrain, ytrain)\n        preds_valid = model.predict(xvalid)\n        test_preds = model.predict(xtest)\n        final_test_predictions.append(test_preds)\n        final_valid_predictions.update(dict(zip(valid_ids, preds_valid)))\n        rmse = mean_squared_error(yvalid, preds_valid, squared=False)\n        print(fold, rmse)\n        scores.append(rmse)\n\nprint(np.mean(scores), np.std(scores))\nfinal_valid_predictions = pd.DataFrame.from_dict(final_valid_predictions, orient=\"index\").reset_index()\nfinal_valid_predictions.columns = [\"id\", \"pred_2\"]\nfinal_valid_predictions.to_csv(\"level1_train_pred_2.csv\", index=False)\n\nsample_submission.target = np.mean(np.column_stack(final_test_predictions), axis=1)\nsample_submission.columns = [\"id\", \"pred_2\"]\nsample_submission.to_csv(\"level1_test_pred_2.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-08-30T04:50:17.917141Z","iopub.status.idle":"2021-08-30T04:50:17.91758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Blending\n\ndf=X_train.copy()\ndf_test=X_test.copy()\n#df['id']=X_train.index\n#df_test['id']=X_test.index\nsample_submission = pd.read_csv(\"../input/30-days-of-ml/sample_submission.csv\")\n\n#df = pd.read_csv(\"../input/summary30daysfinalstacking/train_folds_10.csv\")\n#df_test = pd.read_csv(\"../input/30-days-of-ml/test.csv\")\n\ndf1 = pd.read_csv(\"level1_train_pred_1.csv\")\ndf2 = pd.read_csv(\"level1_train_pred_2.csv\")\n\ndf_test1 = pd.read_csv(\"level1_test_pred_1.csv\")\ndf_test2 = pd.read_csv(\"level1_test_pred_2.csv\")\n\ndf = df.merge(df1, on=\"id\", how=\"left\")\ndf = df.merge(df2, on=\"id\", how=\"left\")\n\ndf_test = df_test.merge(df_test1, on=\"id\", how=\"left\")\ndf_test = df_test.merge(df_test2, on=\"id\", how=\"left\")\n\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-30T04:50:17.91869Z","iopub.status.idle":"2021-08-30T04:50:17.919114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Level 1 prediction used to linearly map it to the target..LinearRegression","metadata":{}},{"cell_type":"code","source":"sample_submission = pd.read_csv(\"../input/30-days-of-ml/sample_submission.csv\")\n\nuseful_features = [\"pred_1\", \"pred_2\"]\ndf_test = df_test[useful_features]\n\nfinal_predictions = []\nscores = []\n\nfor j, fold_idxs_ in enumerate(fold_idxs):\n    print(f\"\\n--- seed={seeds[j]} ---\")\n    for k, (train_idx, validation_idx) in enumerate(fold_idxs_):\n        print(f\"fold {k} train idxs: {len(train_idx)} validation idxs: {len(validation_idx)} -> {validation_idx[:10]}\")\n        fold=k\n        xtrain =  X_train.iloc[train_idx].reset_index(drop=True)\n        xvalid = X_train.iloc[validation_idx].reset_index(drop=True)\n        xtest = X_test.copy()\n        valid_ids = X_train_full.iloc[validation_idx]['id']\n        \n        ytrain = X_train_full.iloc[train_idx].target\n        yvalid = X_train_full.iloc[validation_idx].target\n    \n\n        model = LinearRegression()\n        model.fit(xtrain, ytrain)\n        \n        preds_valid = model.predict(xvalid)\n        test_preds = model.predict(xtest)\n        final_predictions.append(test_preds)\n        rmse = mean_squared_error(yvalid, preds_valid, squared=False)\n        print(fold, rmse)\n        scores.append(rmse)","metadata":{"execution":{"iopub.status.busy":"2021-08-30T04:50:17.919948Z","iopub.status.idle":"2021-08-30T04:50:17.920379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission.target = np.mean(np.column_stack(final_predictions), axis=1)\nsample_submission.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-08-30T04:50:17.921315Z","iopub.status.idle":"2021-08-30T04:50:17.921725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}}]}