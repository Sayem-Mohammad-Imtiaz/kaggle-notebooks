{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import preprocessing\nfrom sklearn import neighbors\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix,classification_report,precision_score\nfrom sklearn.model_selection import train_test_split\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Importing Wisconsin Breast Cancer data\ndf=pd.read_csv('../input/breast-cancer-wisconsin-data/data.csv')\ndf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Understanding all the datatypes in imported data.\ndf.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Finding any missing values in the data\ndf.isna().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.drop('Unnamed: 32',axis=1,inplace=True)\ndf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# As ID columns is not required for the analysis we can drop it.\ndf.drop('id', axis=1, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Mapping 'M' and 'B' to 1 and 0 respectively\ndf['diagnosis']=df['diagnosis'].map({'M':1, 'B':0})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Counting Malignant and Benign cases.\nbenign,malignant=df['diagnosis'].value_counts()\nprint('Number of Malignant cases are', malignant)\nprint('Number of Benign cases are', benign)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Using countplot to represet the Malignant and Benign cases,\nax=sns.countplot(x='diagnosis', data=df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Using pairplot to plot the most useful features\ncols=['diagnosis','radius_mean', 'texture_mean', 'perimeter_mean',\n       'area_mean','smoothness_mean', 'compactness_mean', 'concavity_mean',\n       'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean']\nplt.figure(figsize=(20,20))\nsns.pairplot(data=df[cols],hue='diagnosis', palette='RdBu')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Finding the correlation between independent vars. so that if two or more variables are highly correlated,\n# we keep only one of them as its a duplicacy of the data\ncorr=df.corr()\ncorr","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Using heat map to graphically represnt the correlation to get better understanding of all values.\nmask=np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)]\nf, ax = plt.subplots(figsize=(20, 20))\ncmap=sns.diverging_palette(220,10, as_cmap=True)\nsns.heatmap(corr, mask=mask, cmap=\"YlGnBu\", center=0, square=True, linewidths=.5, cbar_kws={\"orientation\": \"vertical\"}, annot=True)\nplt.tight_layout()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"If we observe mean columns and worst columns in the heat map above we can observe that correlation values are very similar. So, we can remove the worst columns from the dataset for further analysis.","metadata":{}},{"cell_type":"code","source":"df.drop(['radius_worst', 'texture_worst', 'perimeter_worst', 'area_worst', 'smoothness_worst', 'compactness_worst', 'concavity_worst', \n        'concave points_worst', 'symmetry_worst', 'fractal_dimension_worst'], axis=1, inplace=True)\ndf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since SE(Standard Error) is error in measurement, they can be considered as less important for the anaylsis they can also be dropped.","metadata":{}},{"cell_type":"code","source":"df.drop(['radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se', 'compactness_se', 'concavity_se', 'concave points_se' ,\n         'symmetry_se', 'fractal_dimension_se'], axis=1, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr1=df.corr()\nmask=np.zeros_like(corr1, dtype=np.bool)\nmask[np.triu_indices_from(mask)]\nf, ax = plt.subplots(figsize=(20, 20))\ncmap=sns.diverging_palette(220,10, as_cmap=True)\nsns.heatmap(corr1, mask=mask, cmap=\"YlGnBu\", center=0, square=True, linewidths=.5, cbar_kws={\"orientation\": \"vertical\"}, annot=True)\nplt.tight_layout()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It can be observerd that radius_mean, perimeter_mean, area_mean, concavity_mean, concave points_mean have correlation values of ~0.76. So we can keep one field and drop as that would not affect model.","metadata":{}},{"cell_type":"code","source":"df.drop(['perimeter_mean', 'area_mean', 'concavity_mean', 'concave points_mean'], axis=1, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_x, train_y, test_x, test_y=train_test_split(df, df['diagnosis'],test_size=0.3, random_state=5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prediction_var=['radius_mean', 'texture_mean', 'smoothness_mean', 'compactness_mean', 'symmetry_mean', 'fractal_dimension_mean']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train, test = train_test_split(df, test_size=0.3, random_state=5)\nprint(\"train shape is \", train.shape)\nprint(\"test shape is \", test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_x=train[prediction_var]\ntest_x=test[prediction_var]\ntrain_y=train['diagnosis']\ntest_y=test['diagnosis']\nprint(\"shape of train_x is \", train_x.shape)\nprint(\"shape of train_y is \", train_y.shape)\nprint(\"shape of test_x is \", test_x.shape)\nprint(\"shape of test_y is \", test_y.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model=LogisticRegression()\nmodel.fit(train_x, train_y)\npv=model.predict(test_x)\nprint(\"accuracy score of the model is \", accuracy_score(pv, test_y))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predict=model.fit(train_x, train_y).predict(test_x)\npredict[1:6]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prediction_nominal=[0 if x<0.5 else 1 for x in predict ]\nprediction_nominal[1:6]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(test_y, prediction_nominal, digits=3))\ncfm=confusion_matrix(test_y, prediction_nominal)\ntrue_negative=cfm[0][0]\nfalse_positive=cfm[0][1]\nfalse_negative=cfm[1][0]\ntrue_positive=cfm[1][1]\n\nprint(\"Confusion Matrix: \\n\",cfm,'\\n')\nprint(\"true negative\", true_negative)\nprint(\"False Positive\", false_positive)\nprint(\"false Negative\", false_negative)\nprint(\"True Positive\", true_positive)\n\nprint(\"correct prediction\", round((true_negative+true_positive)/len(prediction_nominal) *100, 2),'%')","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}