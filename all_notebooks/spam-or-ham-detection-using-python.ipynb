{"cells":[{"metadata":{"id":"n9pAr_ra9xQy","colab_type":"text","_uuid":"eba74b3edb32435996636827b01e22d44a2a7e29"},"cell_type":"markdown","source":"# 'Spam or Ham?' classification with Python\n\n**Welcome to this kernel!**\n\n**If you like my work, please, leave an upvote: it will be really appreciated and it will motivate me in offering more content to the Kaggle community ! :)**\n\n**Context**\n\nThe SMS Spam Collection is a set of SMS tagged messages that have been collected for SMS Spam research. It contains one set of SMS messages in English of 5,574 messages, tagged acording being ham (legitimate) or spam.\nYou can find it on Kaggle at the following link: https://www.kaggle.com/uciml/sms-spam-collection-dataset\n\n**Content**\n\nThe files contain one message per line. Each line is composed by two columns: v1 contains the label (ham or spam) and v2 contains the raw text.\n\nThis corpus has been collected from free or free for research sources at the Internet:\n\n- A collection of 425 SMS spam messages was manually extracted from the Grumbletext Web site. \nThis is a UK forum in which cell phone users make public claims about SMS spam messages, most of them without reporting the very spam message received. The identification of the text of spam messages in the claims is a very hard and time-consuming task, and it involved carefully scanning hundreds of web pages. The Grumbletext Web site is http://www.grumbletext.co.uk/ . \n\n- A subset of 3,375 SMS randomly chosen ham messages of the NUS SMS Corpus (NSC), which is a dataset of about 10,000 legitimate messages collected for research at the Department of Computer Science at the National University of Singapore. The messages largely originate from Singaporeans and mostly from students attending the University. These messages were collected from volunteers who were made aware that their contributions were going to be made publicly available. \n\n- A list of 450 SMS ham messages collected from Caroline Tag's PhD Thesis available at http://etheses.bham.ac.uk/253/1/Tagg09PhD.pdf . \n\n- Finally, we have incorporated the SMS Spam Corpus v.0.1 Big. It has 1,002 SMS ham messages and 322 spam messages and it is public available at http://www.esp.uem.es/jmgomez/smsspamcorpus/ . \n\nThis corpus has been used in the following academic researches:\n\nAcknowledgements\nThe original dataset can be found here. The creators would like to note that in case you find the dataset useful, please make a reference to previous paper and the web page: http://www.dt.fee.unicamp.br/~tiago/smsspamcollection/ in your papers, research, etc.\n\nWe offer a comprehensive study of this corpus in the following paper. This work presents a number of statistics, studies and baseline results for several machine learning methods.\n\nAlmeida, T.A., Gomez Hidalgo, J.M., Yamakami, A. Contributions to the Study of SMS Spam Filtering: New Collection and Results. Proceedings of the 2011 ACM Symposium on Document Engineering (DOCENG'11), Mountain View, CA, USA, 2011."},{"metadata":{"id":"gsKmE1n--3al","colab_type":"text","_uuid":"7824570fb71129beead1024adc694a514306e3ac"},"cell_type":"markdown","source":"# Let's start!\n\nThe first action to do is to import the data."},{"metadata":{"id":"VY7VIANqyJFK","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"8663e5b0-b967-4f2d-c25d-db96610fd75b","trusted":true,"_uuid":"6c5d12230e21b1a6fc9f8bf6e973050f9ede7a2e"},"cell_type":"code","source":"import os\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"id":"6la4m9mn_Rhx","colab_type":"text","_uuid":"083fbff0dc871c3bf519c54576bb251afe68a1af"},"cell_type":"markdown","source":"Let's now import pandas and numpy and then load the csv using the pd.read_csv command."},{"metadata":{"id":"I_P20KYVyt3l","colab_type":"code","colab":{},"trusted":true,"_uuid":"da20896653abcf66f5c1f5f32dd8445127a56515"},"cell_type":"code","source":"import pandas as pd\nimport numpy as np","execution_count":null,"outputs":[]},{"metadata":{"id":"ENEECVBfywns","colab_type":"code","colab":{},"trusted":true,"_uuid":"c75ba815f29ba0ff4a16e761c4ee8f6c56475336"},"cell_type":"code","source":"# Encoding the data using only the first columns: the other seems to be an issue of the data (empty)\n\ndf = pd.read_csv('../input/spam.csv', sep=',', encoding='latin-1', usecols=lambda col: col not in [\"Unnamed: 2\",\"Unnamed: 3\",\"Unnamed: 4\"])","execution_count":null,"outputs":[]},{"metadata":{"id":"I-DDsNpQ_dHy","colab_type":"text","_uuid":"f4737da2bfedaf6dcd468abd655dc58445701014"},"cell_type":"markdown","source":"As you can see, there are a few commands added to the read_csv.\n\n- **Sep**: means that we are using as a separator the comma '','' because we are working with a csv and this is how the columns are split.\n- **Encoding**: In computer technology, encoding is the process of applying a specific code, such as letters, symbols and numbers, to data for conversion purposes. In this case, I have used latin-1.\n- **Usecols**: the dataset has a few extra columns without labels that I will not use, so I used a lambda to exclude them.\n\nLet's now use df.head(1) to review the first line of the process made above: "},{"metadata":{"id":"UYEUCaGSAG9T","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":80},"outputId":"46bca454-d8bb-45b4-cf80-aa7c73e3c5f9","trusted":false,"_uuid":"981cb9903e3482631eb83bc40e8a3141fb644fe5"},"cell_type":"code","source":"df.head(1)","execution_count":null,"outputs":[]},{"metadata":{"id":"iCDWaySjCmw4","colab_type":"text","_uuid":"b0f4c411a6928ecf6f3b8d9643e8a633f249c98b"},"cell_type":"markdown","source":"Okay, but the names of the columns are not really clear, right?\n\nLet's rename them to something more significant:"},{"metadata":{"id":"FtVPT-aL0oZM","colab_type":"code","colab":{},"trusted":false,"_uuid":"93dab9e33055f1bed5fe3f6e782af2b519eecea4"},"cell_type":"code","source":"df = df.rename(columns={\"v1\":\"label\", \"v2\":\"text\"})","execution_count":null,"outputs":[]},{"metadata":{"id":"SfTXG_NZC6Qh","colab_type":"text","_uuid":"3e20179af730687b1c4b3e12ae0beabb42c3b68b"},"cell_type":"markdown","source":"Great: let's review the result:"},{"metadata":{"id":"U05DLPWu0Hz8","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":204},"outputId":"3b99f712-2fbd-48f6-dd15-546833b94cad","trusted":false,"_uuid":"de6e088d7688eb51437a72c117c74dfa383eca49"},"cell_type":"code","source":"df.head(5)","execution_count":null,"outputs":[]},{"metadata":{"id":"QeZY1ism8c3Y","colab_type":"text","_uuid":"3a614bfd3adf710bb143098856acc966abf8ed1b"},"cell_type":"markdown","source":"# Word Counts with CountVectorizer\n\nThe CountVectorizer provides a simple way to both tokenize a collection of text documents and build a vocabulary of known words, but also to encode new documents using that vocabulary.\n\nYou can use it as follows:\n\n- Create an instance of the CountVectorizer class.\n- Call the fit() function in order to learn a vocabulary from one or more documents.\n- Call the transform() function on one or more documents as needed to encode each as a vector.\n\nAn encoded vector is returned with a length of the entire vocabulary and an integer count for the number of times each word appeared in the document.\n\nBecause these vectors will contain a lot of zeros, we call them sparse. Python provides an efficient way of handling sparse vectors in the scipy.sparse package.\n\nThe vectors returned from a call to transform() will be sparse vectors, and you can transform them back to numpy arrays to look and better understand what is going on by calling the toarray() function.\n\nSource: https://machinelearningmastery.com/prepare-text-data-machine-learning-scikit-learn/ "},{"metadata":{"id":"-NBDKYca57M-","colab_type":"code","colab":{},"trusted":false,"_uuid":"af1eb1d5944ffeadd446a5eed8e56e5876f73a32"},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nvect = CountVectorizer()","execution_count":null,"outputs":[]},{"metadata":{"id":"TQKrfPgp3otV","colab_type":"code","colab":{},"trusted":false,"_uuid":"29bf5f899f4da5798d8cb8c68eb1902758b9c110"},"cell_type":"code","source":"# Splitting the data into training and test\n\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"id":"AiXbuKaB3pdu","colab_type":"code","colab":{},"trusted":false,"_uuid":"410f70ac892c24a77b33d4ff8c81de92482f51f8"},"cell_type":"code","source":"X_train,X_test,y_train,y_test = train_test_split(df[\"text\"],df[\"label\"], test_size = 0.2, random_state = 10)","execution_count":null,"outputs":[]},{"metadata":{"id":"CED6JaV16ASR","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":119},"outputId":"33a46668-6d28-4cc2-e92e-129c981d831c","trusted":false,"_uuid":"7fdac58ecf5e5e7414521b83cfd6f9a4d590c5fe"},"cell_type":"code","source":"# Fitting the CountVectorizer using the training data\n\nvect.fit(X_train)","execution_count":null,"outputs":[]},{"metadata":{"id":"3HGgYKAd6RgG","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"9efe01fd-940c-4699-bd40-dafe688f5e5d","trusted":false,"_uuid":"329d19559744b583fe1ff9543d087dadcd100d3f"},"cell_type":"code","source":"# Transforming the dataframes\n\nX_train_df = vect.transform(X_train)\nX_test_df = vect.transform(X_test)\ntype(X_train_df)","execution_count":null,"outputs":[]},{"metadata":{"id":"VAGi6l_YDBDm","colab_type":"text","_uuid":"91c437f83c41e5678f68203d73027efd9e79169b"},"cell_type":"markdown","source":"Perfect: let's move to the machine learning part!"},{"metadata":{"id":"TXjRrQPvDGtr","colab_type":"text","_uuid":"1f4a27ac8d1569be67f7308088cdeea42df70fd3"},"cell_type":"markdown","source":"# Machine Learning\n\nLet's see if, with the simple preprocessing we did, a Logistic Regression can already fit well this dataset."},{"metadata":{"id":"DU9ecP423xTe","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":119},"outputId":"de7bd24f-7b29-49db-d2ba-c6035f596382","trusted":false,"_uuid":"7b66e6bd0458d72f7b309cc9ef10bb10649c36a2"},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nmodel = LogisticRegression()\nmodel.fit(X_train_df,y_train)","execution_count":null,"outputs":[]},{"metadata":{"id":"Ou3Xtl3tDV79","colab_type":"text","_uuid":"6341350071bdff0d0225cdaa3d91231c38b24aa6"},"cell_type":"markdown","source":"At this point, we can proceed and make our predictions.\n\nAfter the prediction, we will print the accuracy score and a classification report to review the results."},{"metadata":{"id":"p1WmTk1B6dfF","colab_type":"code","colab":{},"trusted":false,"_uuid":"4fd82088bfc40c0b2d7651878235abd8b03c1c23"},"cell_type":"code","source":"# Making predictions\n\nprediction = dict()\n\nprediction[\"Logistic\"] = model.predict(X_test_df)","execution_count":null,"outputs":[]},{"metadata":{"id":"OmMMSyRy6tGO","colab_type":"code","colab":{},"trusted":false,"_uuid":"2573128fb426f2525cbf8ade774b709734be46f5"},"cell_type":"code","source":"# Reviewing the metrics\n\nfrom sklearn.metrics import accuracy_score,confusion_matrix,classification_report","execution_count":null,"outputs":[]},{"metadata":{"id":"gLdQpA3p6v8f","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"0df95f44-39dc-4b2a-a631-56275c01c692","trusted":false,"_uuid":"3242bff9033ff66fbb00a19fed89bf6345d72e42"},"cell_type":"code","source":"accuracy_score(y_test,prediction[\"Logistic\"])","execution_count":null,"outputs":[]},{"metadata":{"id":"k61k_NFE6zD4","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":170},"outputId":"393667a5-9601-4eee-e394-9220d78e424b","trusted":false,"_uuid":"583788822cf2d7f0a7000f4663eb08d169cc3ea8"},"cell_type":"code","source":"print(classification_report(y_test,prediction[\"Logistic\"]))","execution_count":null,"outputs":[]},{"metadata":{"id":"8OwoVGQwDif_","colab_type":"text","_uuid":"74a80660deb83e148c2f2a483517b4f6cb43931e"},"cell_type":"markdown","source":"Great: so we have a classifier with a great precision but less recall on the spam class and the F1-score is quite good!"},{"metadata":{"_uuid":"39bf28884746fe5cc9b5fa6d8b20f9bb68f03201"},"cell_type":"markdown","source":"**If you like my work, please, leave an upvote: it will be really appreciated and it will motivate me in offering more content to the Kaggle community ! :)**"},{"metadata":{"id":"nuk2zgyeG9xh","colab_type":"text","_uuid":"fdcbd326ba8ae4853df4d5fb2b2660ad3e5e07b6"},"cell_type":"markdown","source":"# Thank you for your attention!"}],"metadata":{"colab":{"name":"SpamClassifier.ipynb","version":"0.3.2","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}