{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Executive Summary**:\n\n1. The SMS Spam Detection managed to achieve **98%** accuracy (**100%** precision and **84%** recall, **91%** F1 score).\n2. The text has been cleaned up before passed to the RandomForest model. The text preprocessing involves converting to lower case, tokenize, remove stopwords, lemmatize, and eventually convert to TF-IDF. The other models have been tried like SupportVectorClassifier, XGBoost, LightGBM, Logistic Regression and KNearestNeighborClassifier (PR-AUC is used for model selection and hyperparameter finetuning).\n3. Visualization the most common words in spam and ham classes and topic modelling using Latent Dirichlet Allocation (LDA).","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport re\nimport os\n\nimport nltk\n# nltk.download('stopwords')\n# nltk.download('wordnet')\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\n\nfrom wordcloud import WordCloud\nfrom gensim.models import LdaMulticore\nimport gensim.corpora as corpora\nfrom pprint import pprint\n\n# !pip install pyLDAvis\nimport pyLDAvis\nimport pyLDAvis.gensim_models\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split, cross_validate, GridSearchCV\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load data","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/sms-spam-collection-dataset/spam.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df.rename(columns={'v1':'tgt', 'v2':'text'})[['tgt','text']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"code","source":"df.tgt.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Imbalanced class even though it is not that severe","metadata":{}},{"cell_type":"code","source":"df[df.tgt=='ham'].sample(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[df.tgt=='spam'].sample(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocess data","metadata":{}},{"cell_type":"markdown","source":"Preprocess and clean up the data before we put the data into the model. The processes involves\n1. Extract only alphabetical characters and convert them into lower case\n2. Tokenize text\n2. Remove stopwords (English) + short form of stopwords\n3. Lemmatize the word\n4. Convert the word into TF-IDF vectors\n5. Process target to spam-->1, ham-->0","metadata":{}},{"cell_type":"code","source":"def clean_data(text):\n    \"\"\"\n    Extract alphabetical characters and convert them into lower case.\n    \"\"\"\n    out = re.sub('[^a-zA-Z]', ' ', text) \n    out = out.lower() \n    out = out.split()\n    out = ' '.join(out)\n    return out\n\ndef tokenize_word(text):\n    \"\"\"\n    Convert sentence into list of tokens\n    \"\"\"\n    return nltk.word_tokenize(text)\n\ndef remove_stopwords(text):\n    \"\"\"\n    Remove English stopwords from text\n    \"\"\"\n    stop_words = set(stopwords.words(\"english\")+['u','ur','r','n']) \n    filtered_text = [word for word in text if word not in stop_words]\n    return filtered_text\n\ndef lemmatize_word(text):\n    \"\"\"\n    Convert word into base form (lemmatize)\n    \"\"\"\n    lemmatizer = WordNetLemmatizer()\n    lemmas = [lemmatizer.lemmatize(word, pos ='v') for word in text]\n    return lemmas\n\ndef get_processed_tokens(text):\n    \"\"\"\n    Do all above four preprocess steps.\n    \"\"\"\n    text = clean_data(text)\n    text = tokenize_word(text)\n    text = remove_stopwords(text)\n    text = lemmatize_word(text)\n    return text","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['processed_text'] = df['text'].apply(get_processed_tokens)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create corpus of text for TF-IDF \ncorpus= []\nfor i in df[\"processed_text\"]:\n    msg = ' '.join([row for row in i])\n    corpus.append(msg)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Vectorize the corpus\nvectorizer = TfidfVectorizer()\nX = vectorizer.fit_transform(corpus).toarray()\nX.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Process label\ndf['label'] = 0\ndf.loc[df.tgt=='spam', 'label']=1\ndf.label.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = df.label.values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y.shape","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Find common keywords/topics","metadata":{}},{"cell_type":"markdown","source":"This section is separate from SMS Spam Detection. This is to model the topics in the sms","metadata":{}},{"cell_type":"code","source":"# Create a WordCloud object\nwordcloud = WordCloud(background_color=\"white\", max_words=5000, contour_width=3, contour_color='steelblue')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# All corpus\nlong_string = ' '.join(corpus)\nwordcloud.generate(long_string)\nwordcloud.to_image()","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Spam corpus\nspam_corpus= []\nfor i in df[df.tgt=='spam'][\"processed_text\"]:\n    msg = ' '.join([row for row in i])\n    spam_corpus.append(msg)\n    \nlong_string = ' '.join(spam_corpus)\nwordcloud.generate(long_string)\nwordcloud.to_image()","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Ham corpus\nham_corpus= []\nfor i in df[df.tgt=='ham'][\"processed_text\"]:\n    msg = ' '.join([row for row in i])\n    ham_corpus.append(msg)\n\nlong_string = ' '.join(ham_corpus)\nwordcloud.generate(long_string)\nwordcloud.to_image()","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"id2word = corpora.Dictionary(df[\"processed_text\"])\ntexts = df[\"processed_text\"]\ntdf = [id2word.doc2bow(text) for text in texts] #Term Document Frequency","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_topics = 8\nlda_model = LdaMulticore(corpus=tdf,id2word=id2word,num_topics=num_topics, random_state=10)\n# Print the Keyword in the 10 topics\npprint(lda_model.print_topics())\ndoc_lda = lda_model[tdf]","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualize the topics\npyLDAvis.enable_notebook()\nLDAvis_prepared = pyLDAvis.gensim_models.prepare(lda_model, tdf, id2word)\nLDAvis_prepared","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Based on the model above, most ham topics revolves around deciding when and where to go/meet, affirmation (ok), getting help, saying love/miss, etc.\nParticularly for the spam class, it involves request some action from the recipient (call, text, reply), claim some prize / free stuff / cash. The topic 2 on above visualization is particularly far from other topics on the principal components 2-D plot and seems the topic is dominated by spam keywords.","metadata":{}},{"cell_type":"markdown","source":"# Model training","metadata":{}},{"cell_type":"markdown","source":"We will split 30% of dataset for evaluating the final model performance.\nSince the dataset is quite small, we will test few models and find out the best algorithm that perform the best in cross validated dataset.\n\nWe will find the best model algorithm by using their default hyperparameter. The best model algorithm is further finetuned on its hyperparameter.\nWe will use PR-AUC / average_precision score as performance metrics to be optimized since it is imbalanced class.\n","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rfc = RandomForestClassifier(random_state=42)\nsvc = SVC(random_state=42)\nxgb = XGBClassifier(random_state=42)\nlgbm = LGBMClassifier(random_state=42)\nlr = LogisticRegression(random_state=42)\nknc = KNeighborsClassifier()\n\nmodels=[rfc,svc,xgb,lgbm,lr,knc]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_perf = pd.DataFrame()\nmetrics = ['f1','precision','recall','average_precision','roc_auc']\nfor model in models:\n    scores = cross_validate(model,X_train,y_train,scoring=metrics,n_jobs=-1,cv=3)\n    \n    tmp = {}\n    tmp['model'] = str(type(model).__name__)\n    for test_metric in ['test_'+metric for metric in metrics]:\n        tmp[test_metric] = np.mean(scores[test_metric])\n    \n    model_perf = model_perf.append(tmp,ignore_index=True)\n\nmodel_perf.sort_values('test_average_precision', ascending=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Based on the above tables, the most performing algorithm is **RandomForestClassifier**. The model is selected due to its **extremely high precision** (almost perfect precision) and **good recall**. In this case, I am keen toward to the model that has extremely high precision to ensure that we minimize the False Positive i.e. misclassify ham messages as spam. The alternative model that we can select if we try to maximize recall is **LGBMClassifier** and it has the highest F1 score. \n\nIt has been shown that tree-based algorithm performed better in this case due to its sparse feature matrix.","metadata":{}},{"cell_type":"code","source":"params = {\n    'n_estimators': [25,50,100,200,400,800],\n    'min_samples_split':[2,3,4]\n}\n\nsearch = GridSearchCV(rfc, params, scoring='average_precision', n_jobs=-1, cv=5, refit=True)\n# This process might take a while\nsearch_result = search.fit(X_train, y_train)\nprint(search_result.best_params_)\nprint(search_result.best_score_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_model = search_result.best_estimator_\n\n# best_model = RandomForestClassifier(random_state=42, min_samples_split=3, n_estimators=800)\n# best_model.fit(X_train,Y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluate model on test set","metadata":{}},{"cell_type":"code","source":"y_test_pred = best_model.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Accuracy score: %.2f' %(accuracy_score(y_test, y_test_pred)))\np,r,f,_ = precision_recall_fscore_support(y_test, y_test_pred, average='binary')\nprint(\"Precision= %.2f, Recall = %.2f, F1-score = %.2f\" % (p,r,f))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The model has perfect precision (100%) and 84% recall","metadata":{}},{"cell_type":"markdown","source":"------- End of notebook---------------------","metadata":{}}]}