{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Standard Imports of data"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Loading base libraries"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport pylab as py\nimport scipy.optimize as opt\nfrom sklearn import preprocessing\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.graph_objs as go\n%matplotlib inline\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn import tree","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Open and define the data sets using the pandas' library:\ndf=pd.read_csv(\"../input/heart-failure-clinical-data/heart_failure_clinical_records_dataset.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We use several commands to check the loaded date set:\n# Show the first 5 data to check out the data\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Show the columns names of the data sets and type of data\ndf.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Show the columns names of the data sets and type of data and amount \n# of non-null(not missed) values.\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# For a column in the data set columns show and print column name - number of \n# unique values and number of total values. If there are too many unique values \n# out of total numbers, we can easily drop out this variable.\n# But our data set have good quolity and a lots of dummy variables =>\n# we leave it unchanged for now.\ndf.info()\nfor col in df.columns:\n    print(col, df[col].nunique(), len(df))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# For each of the variables we need to check the mistakes.\n# For that we print all unique values and look for mistakes.\n# As we can see, all values are okay and no mistake exists\nfor col in df.columns:\n    print(col, df[col].unique(), len(df))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Next, let's collect descriptive statistics for each variable: \nstatistic = df.describe(include='all')\nprint(statistic)\n# Results we can see in 'variable explorer'.\n# Based on this, in the future, we will need to normalize or standardize \n# the data (optional) in order for the analysis to be more accurate.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Pearson correlation"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Next, we use correlation, in this case Pearson. We also build a correlation \n# matrix to display the result. Correlation analysis will help us determine \n# whether variables interact strongly with each other or not. If variables \n# interact strongly with each other, it will damage the analysis and make it \n# non-faithful. But as we can observe, all the variables do not \n# influence each other much.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\ncorr = df.corr(method ='pearson')\nax = sns.heatmap(\n    corr, vmin=-1, vmax=1, center=0,cmap=sns.diverging_palette(10, 200, n=200),square=True)\nax.set_xticklabels(ax.get_xticklabels(),rotation=45, horizontalalignment='right');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data visualization before analysis"},{"metadata":{},"cell_type":"markdown","source":"Looking at binaries independent variable in the same plot with the dependent variable."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig ,ax = plt.subplots(2,2,figsize=(32,32))\nax1,ax2,ax3,ax4 = ax.flatten()\nsns.countplot(data=df,x='anaemia',hue='DEATH_EVENT',palette='viridis',ax=ax1)\nsns.countplot(data=df,x='diabetes',hue='DEATH_EVENT',palette='Set1_r',ax=ax2)\nsns.countplot(data=df,x='high_blood_pressure',hue='DEATH_EVENT',palette='gist_ncar_r',ax=ax3)\nsns.countplot(data=df,x='smoking',hue='DEATH_EVENT',palette='autumn_r',ax=ax4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looking at others independent variable in the same plot with the dependent variable."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(32,16))\nsns.countplot(data=df,x='age',hue='DEATH_EVENT',palette='gist_rainbow')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(32,16))\nsns.countplot(data=df,x='serum_creatinine',hue='DEATH_EVENT',palette='YlGnBu')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(32,16))\nsns.countplot(data=df,x='serum_sodium',hue='DEATH_EVENT',palette='Oranges_r')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize= (16,4))\nsns.countplot(data=df,x='sex',hue='DEATH_EVENT',palette='Set2_r')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Make a visualisation of 'age' variable in the same plot with the dependent variable."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,ax = plt.subplots(2,2,figsize=(16,16))\nax1,ax2,ax3,ax4 = ax.flatten()\nsns.distplot(df['age'],bins=20,color='r',ax=ax1)\nsns.boxplot(y='age',x='DEATH_EVENT',data=df,ax=ax2)\nsns.pointplot(y='age',x='DEATH_EVENT',data=df,ax=ax3)\nsns.violinplot(y='age',x='DEATH_EVENT',data=df,ax=ax4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"male = df[df[\"sex\"]==1]\nfemale = df[df[\"sex\"]==0]\n\nmale_survi = male[df[\"DEATH_EVENT\"]==0]\nmale_not = male[df[\"DEATH_EVENT\"]==1]\nfemale_survi = female[df[\"DEATH_EVENT\"]==0]\nfemale_not = female[df[\"DEATH_EVENT\"]==1]\n\nlabels = ['Male - Survived','Male - Not Survived', \"Female -  Survived\", \"Female - Not Survived\"]\nvalues = [len(male[df[\"DEATH_EVENT\"]==0]),len(male[df[\"DEATH_EVENT\"]==1]),\n         len(female[df[\"DEATH_EVENT\"]==0]),len(female[df[\"DEATH_EVENT\"]==1])]\nfig = go.Figure(data=[go.Pie(labels=labels, values=values, hole=.3)])\nfig.update_layout(\n    title_text=\"Analysis on Survival - Gender\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Split data to test and train"},{"metadata":{"trusted":true},"cell_type":"code","source":"# We need to predict the chance of DEATH_EVENT => for that goal we have to use\n# logistic regression. Types of Logistic Regression:\n# 1. Binary Logistic Regression: The target variable has \n# only two possible outcomes.\n# 2. Multinomial Logistic Regression: The target variable has three or more \n# nominal categories.\n# 3. Ordinal Logistic Regression: the target variable has three or \n# more ordinal categories. \n# Hence our regression method is obviously Binary Logistic Regression.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Is to divide all variables into two groups: target variable(dependent) and others (independent) variables:\nfeature_cols = ['age', 'anaemia', 'creatinine_phosphokinase', 'diabetes', \n                'ejection_fraction', 'high_blood_pressure', 'platelets', \n                'serum_creatinine', 'serum_sodium', 'sex', 'smoking', 'time']\nX = df[feature_cols] # Features\ny = df.DEATH_EVENT # Target variable","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Understand model performance, dividing the dataset into\n# a training set and a test set\n# split X and y into training and testing sets.\n# Here, the Dataset is broken into two parts in a ratio of 80:20.\n# It means 80% data will be used for model training \n# and 20% for model testing.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Making a correlation between X train (independent) variables."},{"metadata":{"trusted":true},"cell_type":"code","source":"cor = X_train.corr()\nplt.figure(figsize=(12,6))\nsns.heatmap(cor,cmap='Set1',annot=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Standardization"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_train_st = scaler.fit_transform(X_train)\nX_test_st = scaler.fit_transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_st","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test_st","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"# First, import the Logistic Regression module and \n# create a Logistic Regression classifier object using\n# LogisticRegression() function.\n# Then, fit our model on the train set using fit() and\n# perform prediction on the test set using predict().","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression(random_state=0)\nlogreg.fit(X_train_st,y_train)\ny_pred=logreg.predict(X_test_st)\nprint(y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logreg.coef_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# A confusion matrix is a table that is used to evaluate the performance \n# of a classification model. We can also visualize the performance of an \n# algorithm. The fundamental of a confusion matrix is the number of correct\n# and incorrect predictions are summed up class-wise.\nfrom sklearn import metrics\ncnf_matrix = metrics.confusion_matrix(y_test, y_pred)\ncnf_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a matrix 2x2 with a code:\nclass_names=[0,1] # name  of classes\nfig, ax = plt.subplots()\ntick_marks = np.arange(len(class_names))\nplt.xticks(tick_marks, class_names)\nplt.yticks(tick_marks, class_names)\nsns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\nax.xaxis.set_label_position(\"top\")\nplt.tight_layout()\nplt.title('Confusion matrix', y=1.1)\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Confusion Matrix Evaluation Metrics\n# evaluate the model using model evaluation metrics \n# such as accuracy, precision, and recall\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\nprint(\"Precision:\",metrics.precision_score(y_test, y_pred))\nprint(\"Recall:\",metrics.recall_score(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ROC Curve:\n# Receiver Operating Characteristic(ROC) curve is a plot of the true \n# positive rate against the false positive rate. It shows the tradeoff \n# between sensitivity and specificity.\ny_pred_proba = logreg.predict_proba(X_test_st)[::,1]\nfpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)\nauc_logreg = metrics.roc_auc_score(y_test, y_pred_proba)\nplt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc_logreg))\nplt.legend(loc=4)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ROC Curve: 0.857\n# AUC score for the case is 0.857. AUC score 1 represents perfect classifier, \n# and 0.5 represents a worthless classifier.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Random Forest Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"help(RandomForestClassifier)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nr_clf = RandomForestClassifier(max_features=0.5, max_depth=10, random_state=0)\nr_clf.fit(X_train_st, y_train)\nr_pred = r_clf.predict(X_test_st)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(r_clf.max_depth)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(r_clf.max_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Accuracy:\",metrics.accuracy_score(y_test, r_pred))\nprint(\"Precision:\",metrics.precision_score(y_test, r_pred))\nprint(\"Recall:\",metrics.recall_score(y_test, r_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\ncnf_matrix = metrics.confusion_matrix(y_test, r_pred)\ncnf_matrix\nimport matplotlib.pyplot as plt\nclass_names=[0,1] # name  of classes\nfig, ax = plt.subplots()\ntick_marks = np.arange(len(class_names))\nplt.xticks(tick_marks, class_names)\nplt.yticks(tick_marks, class_names)\nsns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\nax.xaxis.set_label_position(\"top\")\nplt.tight_layout()\nplt.title('Confusion matrix', y=1.1)\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_proba_r_clf = r_clf.predict_proba(X_test_st)[::,1]\nfpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba_r_clf)\nauc_r_clf = metrics.roc_auc_score(y_test, y_pred_proba_r_clf)\nplt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc_r_clf))\nplt.legend(loc=4)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Decision Tree Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"help(DecisionTreeClassifier)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"decision_tree = tree.DecisionTreeClassifier(criterion='entropy', max_depth = 10, random_state=0)\ndecision_tree.fit(X_train_st, y_train)\nd_pred = decision_tree.predict(X_test_st)\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, d_pred))\nprint(\"Precision:\",metrics.precision_score(y_test, d_pred))\nprint(\"Recall:\",metrics.recall_score(y_test, d_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(decision_tree.tree_.max_depth)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\ncnf_matrix = metrics.confusion_matrix(y_test, d_pred)\ncnf_matrix\nimport matplotlib.pyplot as plt\nclass_names=[0,1] # name  of classes\nfig, ax = plt.subplots()\ntick_marks = np.arange(len(class_names))\nplt.xticks(tick_marks, class_names)\nplt.yticks(tick_marks, class_names)\nsns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\nax.xaxis.set_label_position(\"top\")\nplt.tight_layout()\nplt.title('Confusion matrix', y=1.1)\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_proba_decision_tree = decision_tree.predict_proba(X_test_st)[::,1]\nfpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba_decision_tree)\nauc_decision_tree = metrics.roc_auc_score(y_test, y_pred_proba_decision_tree)\nplt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc_decision_tree))\nplt.legend(loc=4)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Visualizing the Decision Tree"},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.display import Image as PImage\nfrom subprocess import check_call\nfrom PIL import Image, ImageDraw\nimport graphviz  \nfrom sklearn.tree import export_graphviz\n\n# Export our trained model as a .dot file\nwith open(\"tree1.dot\", 'w') as f:\n     f = export_graphviz(decision_tree, out_file=f, max_depth = 10,\n                         impurity = True, feature_names = X_train.columns,\n                         rounded = True, filled= True )\n#Convert .dot to .png to allow display in web notebook\ncheck_call(['dot','-Tpng','tree1.dot','-o','tree.png'])\n# Annotating chart with PIL\nimg = Image.open(\"tree.png\")\ndraw = ImageDraw.Draw(img)\nimg.save('sample-out.png')\nPImage(\"sample-out.png\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV,StratifiedKFold\nlogreg5 = LogisticRegression(class_weight='balanced')\nparam = {'C':[0.001,0.003,0.005,0.01,0.03,0.05,0.08, 0.1, 0.3,0.5,1,2,3,3,4,5,10,20]}\nclf = GridSearchCV(logreg5,param,scoring='roc_auc',refit=True,cv=10)\nclf.fit(X_train_st,y_train)\nprint('Best roc_auc: {:.4}, with best C: {}'.format(clf.best_score_, clf.best_params_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlogreg2 = LogisticRegression(C=0.1)\nlogreg2.fit(X_train_st,y_train)\ny_pred2=logreg2.predict(X_test_st)\nprint(y_pred2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\ncnf_matrix = metrics.confusion_matrix(y_test, y_pred2)\ncnf_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nclass_names=[0,1] # name  of classes\nfig, ax = plt.subplots()\ntick_marks = np.arange(len(class_names))\nplt.xticks(tick_marks, class_names)\nplt.yticks(tick_marks, class_names)\nsns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\nax.xaxis.set_label_position(\"top\")\nplt.tight_layout()\nplt.title('Confusion matrix', y=1.1)\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred2))\nprint(\"Precision:\",metrics.precision_score(y_test, y_pred2))\nprint(\"Recall:\",metrics.recall_score(y_test, y_pred2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_proba_logreg2 = logreg2.predict_proba(X_test_st)[::,1]\nfpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba_logreg2)\nauc_logreg2 = metrics.roc_auc_score(y_test, y_pred_proba_logreg2)\nplt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc_logreg2))\nplt.legend(loc=4)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# L1 Regularization/Lasso regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Since it provides sparse solutions, it is generally the model of choice (or some variant of this concept) for modelling cases where the \n#features are in millions or more. In such a case, getting a sparse solution is of great computational advantage as the features \n#with zero coefficients can simply be ignored.\n#It arbitrarily selects any one feature among the highly correlated ones and reduced the coefficients of the rest to zero. \n#Also, the chosen variable changes randomly with change in model parameters. This generally doesn’t work that well as compared to ridge regression.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlog_l1 = LogisticRegression(random_state=0, penalty='l1', solver='saga')\nlog_l1.fit(X_train_st,y_train)\ny_pred_l1=log_l1.predict(X_test_st)\nprint(y_pred_l1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\ncnf_matrix = metrics.confusion_matrix(y_test, y_pred_l1)\ncnf_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class_names=[0,1] # name  of classes\nfig, ax = plt.subplots()\ntick_marks = np.arange(len(class_names))\nplt.xticks(tick_marks, class_names)\nplt.yticks(tick_marks, class_names)\nsns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\nax.xaxis.set_label_position(\"top\")\nplt.tight_layout()\nplt.title('Confusion matrix', y=1.1)\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred_l1))\nprint(\"Precision:\",metrics.precision_score(y_test, y_pred_l1))\nprint(\"Recall:\",metrics.recall_score(y_test, y_pred_l1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_proba_log_l1 = log_l1.predict_proba(X_test_st)[::,1]\nfpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba_log_l1)\nauc_log_l1 = metrics.roc_auc_score(y_test, y_pred_proba_log_l1)\nplt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc_log_l1))\nplt.legend(loc=4)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"log_l1.coef_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# L2 Regularization/Ridge regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Ridge: It is majorly used to prevent overfitting. \n#Since it includes all the features, it is not very useful in case of exorbitantly high #features, say in millions, as it will pose computational challenges.\n#It generally works well even in presence of highly correlated features as it will include all of them in the model\n#but the coefficients will be distributed among them depending on the correlation.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlog_l2 = LogisticRegression(random_state=0, penalty='l2', solver='saga')\nlog_l2.fit(X_train_st,y_train)\ny_pred_l2=log_l2.predict(X_test_st)\nprint(y_pred_l2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\ncnf_matrix = metrics.confusion_matrix(y_test, y_pred_l2)\ncnf_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class_names=[0,1] # name  of classes\nfig, ax = plt.subplots()\ntick_marks = np.arange(len(class_names))\nplt.xticks(tick_marks, class_names)\nplt.yticks(tick_marks, class_names)\nsns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\nax.xaxis.set_label_position(\"top\")\nplt.tight_layout()\nplt.title('Confusion matrix', y=1.1)\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred_l2))\nprint(\"Precision:\",metrics.precision_score(y_test, y_pred_l2))\nprint(\"Recall:\",metrics.recall_score(y_test, y_pred_l2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_proba_log_l2 = log_l2.predict_proba(X_test_st)[::,1]\nfpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba_log_l2)\nauc_log_l2 = metrics.roc_auc_score(y_test, y_pred_proba_log_l2)\nplt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc_log_l2))\nplt.legend(loc=4)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"log_l2.coef_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lasso regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import Lasso\nfrom sklearn import linear_model\nlasso = Lasso()\nlasso.fit(X_train_st,y_train)\ntrain_score=lasso.score(X_train_st,y_train)\ntest_score=lasso.score(X_test_st,y_test)\ncoeff_used = np.sum(lasso.coef_!=0)\n\nprint (\"Training score:\", train_score) \nprint (\"Test score:\", test_score)\nprint (\"Number of features used:\", coeff_used)\nprint (\"____________________________________________________________________________\")\nprint()\n\n\nlasso001 = Lasso(alpha=0.01, max_iter=10e5)\nlasso001.fit(X_train_st,y_train)\ntrain_score001=lasso001.score(X_train_st,y_train)\ntest_score001=lasso001.score(X_test_st,y_test)\ncoeff_used001 = np.sum(lasso001.coef_!=0)\n\nprint (\"Training score for alpha = 0.01:\", train_score001)\nprint (\"Test score for alpha = 0.01:\", test_score001)\nprint (\"Number of features used: for alpha = 0.01:\", coeff_used001)\nprint (\"____________________________________________________________________________\")\nprint()\n\n\nlasso00001 = Lasso(alpha=0.0001, max_iter=10e5)\nlasso00001.fit(X_train_st,y_train)\ntrain_score00001=lasso00001.score(X_train_st,y_train)\ntest_score00001=lasso00001.score(X_test_st,y_test)\ncoeff_used00001 = np.sum(lasso00001.coef_!=0)\n\nprint (\"Training score for alpha = 0.0001:\", train_score00001)\nprint (\"Test score for alpha = 0.0001:\", test_score00001)\nprint (\"Number of features used: for alpha = 0.0001:\", coeff_used00001)\nprint (\"____________________________________________________________________________\")\nprint()\n\n\nlr = LogisticRegression()\nlr.fit(X_train_st,y_train)\nlr_train_score=lr.score(X_train_st,y_train)\nlr_test_score=lr.score(X_test_st,y_test)\nprint (\"Logistic Regression training score:\", lr_train_score)\nprint (\"Logistic Regression test score:\", lr_test_score)\nprint (\"____________________________________________________________________________\")\nprint()\n\nlr = LogisticRegression(random_state=0)\nlr.fit(X_train_st,y_train)\n# plot\nplt.subplot(1,2,1)\nplt.plot(lasso.coef_,alpha=0.7,linestyle='none',marker='*',markersize=6,color='red',label=r'Lasso; $\\alpha = 1$',zorder=7) # alpha here is for transparency\nplt.plot(lasso001.coef_,alpha=0.5,linestyle='none',marker='d',markersize=7,color='blue',label=r'Lasso; $\\alpha = 0.01$')\n\nplt.xlabel('Coefficient Index',fontsize=16)\nplt.ylabel('Coefficient Magnitude',fontsize=16)\nplt.legend(fontsize=13,loc=4)\nplt.subplot(1,2,2)\nplt.plot(lasso.coef_,alpha=0.7,linestyle='none',marker='*',markersize=6,color='red',label=r'Lasso; $\\alpha = 1$',zorder=7)\nplt.plot(lasso001.coef_,alpha=0.5,linestyle='none',marker='d',markersize=7,color='blue',label=r'Lasso; $\\alpha = 0.01$')\nplt.plot(lasso00001.coef_,alpha=0.8,linestyle='none',marker='v',markersize=7,color='black',label=r'Lasso; $\\alpha = 0.00001$')\nplt.plot(lr.coef_,alpha=0.7,linestyle='none',marker='o',markersize=6,color='green',label='Linear Regression',zorder=2)\nplt.xlabel('Coefficient Index',fontsize=16)\nplt.ylabel('Coefficient Magnitude',fontsize=16)\nplt.legend(fontsize=6,loc=4)\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Comparison of different Classifier Models"},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy_list = []","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy_list.append(auc_logreg)\naccuracy_list.append(auc_r_clf)\naccuracy_list.append(auc_decision_tree)\naccuracy_list.append(auc_logreg2)\naccuracy_list.append(auc_log_l1)\naccuracy_list.append(auc_log_l2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_list=['Logistic Regression','Random Forest Classifier','Decision Tree Classifier','LR validation','LR L1','LR L2']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.rcParams['figure.figsize']=20,8\nsns.set_style('darkgrid')\nax = sns.barplot(x=model_list, y=accuracy_list, palette = \"husl\", saturation =2)\nplt.xlabel('Classifier Models', fontsize = 20 )\nplt.ylabel('Value of ROC', fontsize = 20)\nplt.title('Receiver Operating Characteristic(ROC) value of different Classifier Models', fontsize = 20)\nplt.xticks(fontsize = 12, horizontalalignment = 'center', rotation = 8)\nplt.yticks(fontsize = 12)\nfor i in ax.patches:\n    width, height = i.get_width(), i.get_height()\n    x, y = i.get_xy() \n    ax.annotate(f'{round(height,4)}', (x + width/2, y + height*1.02), ha='center', fontsize = 'x-large')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy_list2 = []","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy_list2.append(metrics.accuracy_score(y_test, y_pred))\naccuracy_list2.append(metrics.accuracy_score(y_test, r_pred))\naccuracy_list2.append(metrics.accuracy_score(y_test, d_pred))\naccuracy_list2.append(metrics.accuracy_score(y_test, y_pred2))\naccuracy_list2.append(metrics.accuracy_score(y_test, y_pred_l1))\naccuracy_list2.append(metrics.accuracy_score(y_test, y_pred_l2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_list2=['Logistic Regression','Random Forest Classifier','Decision Tree Classifier','LR validation','LR L1','LR L2']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.rcParams['figure.figsize']=20,8\nsns.set_style('darkgrid')\nax = sns.barplot(x=model_list2, y=accuracy_list2, palette = \"husl\", saturation = 2)\nplt.xlabel('Classifier Models', fontsize = 20 )\nplt.ylabel('Value of accurancy', fontsize = 20)\nplt.title('Accurancy value of different Classifier Models', fontsize = 20)\nplt.xticks(fontsize = 12, horizontalalignment = 'center', rotation = 8)\nplt.yticks(fontsize = 12)\nfor i in ax.patches:\n    width, height = i.get_width(), i.get_height()\n    x, y = i.get_xy() \n    ax.annotate(f'{round(height,4)}', (x + width/2, y + height*1.02), ha='center', fontsize = 'x-large')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"my_list_coef = []","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"my_list_coef.append(logreg.coef_)\nmy_list_coef.append(logreg2.coef_)\nmy_list_coef.append(log_l1.coef_)\nmy_list_coef.append(log_l2.coef_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"my_list_coef.insert(0, 'Coef for Logistic Regression')\nmy_list_coef.insert(2, 'Coef for Logistic Regression with validation')\nmy_list_coef.insert(4, 'Coef for Logistic Regression with L1 penalty')\nmy_list_coef.insert(6, 'Coef for Logistic Regression with L2 penalty')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"my_list_coef","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import itertools\n\ncolumn_names = ['Coef for Logistic Regression','Coef for Logistic Regression with validation','Coef for Logistic Regression with L1 penalty',\n               'Coef for Logistic Regression with L2 penalty']\nvalues = [logreg.coef_, logreg2.coef_, log_l1.coef_, log_l2.coef_]\n\nL = zip(itertools.cycle(column_names), values)\n\nfor g, v in itertools.groupby(sorted(L), lambda x: x[0]):\n    print(\"{} = {}\".format(g, [i[1] for i in v]))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}