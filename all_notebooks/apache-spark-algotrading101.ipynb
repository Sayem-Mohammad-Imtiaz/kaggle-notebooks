{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Apache Spark Guide\n\nThis is an Apache Spark Guide that will be featured on the [Algotrading101 blog](https://algotrading101.com/learn/).\nIf you want the explanation of what the code does I'd suggest you to visit the blog article that will be linked [here](https://algotrading101.com/learn/pyspark-guide/) upon release.","metadata":{}},{"cell_type":"code","source":"!pip install pyspark","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pyspark.sql import SparkSession\n\n# Create the Session\nspark = SparkSession.builder\\\n    .master(\"local\")\\\n    .appName(\"PySpark Tutorial\")\\\n    .getOrCreate()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sc = spark.sparkContext\nrdd = sc.textFile('../input/stockmarketdatafrom1996to2020/Data/Data/FB/FB.csv')\nrdd.take(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pathlib import Path\ncontents = list(Path('../input/stockmarketdatafrom1996to2020/Data/Data').iterdir())\n#print(contents)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stock_1 = spark.read.csv('../input/stockmarketdatafrom1996to2020/Data/Data/AAPL/AAPL.csv', inferSchema=True, header=True)\nstock_1.show(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stock_2 = spark.read.csv('../input/stockmarketdatafrom1996to2020/Data/Data/MSFT/MSFT.csv', inferSchema=True, header=True)\nstock_2.show(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stock_1.printSchema()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stock_1.select(\"Close\").show(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pyspark.sql import functions as F\nstock_1.filter(F.col(\"Close\")>148.00).select(\"Date\",\"Close\").show(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rdd = rdd.map(lambda line: line.split(\",\"))\nrdd.top(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num = sc.parallelize([23, 1, 4, 5, 6, 7])\nnum_sum = num.reduce(lambda a,b:a+b)\nprint(num_sum)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pyspark.sql import Row\n\nheader = rdd.first()\nstock_3 = rdd.filter(lambda line: line != header)\\\n             .map(lambda line: Row(date=line[0],\n                                   open=line[1],\n                                   high=line[2],\n                                   low=line[3],\n                                   close=line[4],\n                                   adj_close=line[5],\n                                   volume=line[6])).toDF()\nstock_3.show(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pyspark.ml.feature import StandardScaler\nfrom pyspark.ml.feature import VectorAssembler\n\ninput_1 = stock_1.select(\"Adj Close\")\ninput_1.show(5)\ninput_2 = stock_2.select(\"Adj Close\")\n#######################\n\ninput_1 = input_1.withColumnRenamed(\"Adj Close\",\"label\")\ninput_2 = input_2.withColumnRenamed(\"Adj Close\",\"feature\")\n\ninput_data = input_1.join(input_2)\ninput_data.show(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"assembler = VectorAssembler(\n    inputCols=[\"feature\"],\n    outputCol=\"features\")\n\ninput_data = assembler.transform(input_data)\n\nstandardScaler = StandardScaler(inputCol=\"features\", outputCol=\"features_scaled\")\nscaler = standardScaler.fit(input_data.select(\"features\"))\n\ndf = scaler.transform(input_data)\ndf.show(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pyspark.ml.regression import LinearRegression\n\ntrain_data, test_data = df.randomSplit([.8,.2], seed=42)\n\nreg = LinearRegression(labelCol=\"label\", featuresCol=\"features_scaled\", maxIter=5)\nmodel = reg.fit(train_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Predict test_data\npredicted = model.transform(test_data)\n\n# Take predictions and the true label - zip them\npredictions = predicted.select(\"prediction\").rdd.map(lambda x: x[0])\nlabels = predicted.select(\"label\").rdd.map(lambda x: x[0])\npred_lab = predictions.zip(labels).collect()\n\n# Print out first 5  predictions\npred_lab[:5]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Model coefficients\nprint(model.coefficients)\n\n# Intercept\nprint(model.intercept)\n\n# RMSE\nprint(model.summary.rootMeanSquaredError)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}