{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Naive Bayes Classifier on IMDB Review Data set","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn import metrics\nfrom sklearn.metrics import roc_curve, auc, classification_report\nfrom nltk.stem.porter import PorterStemmer\nimport re\nfrom nltk.corpus import stopwords\nimport nltk\nfrom collections import Counter\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Read data\ndata = pd.read_csv('/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## checking the frequency of positive and negative reviewes","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data[\"sentiment\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* This is a Balanced Dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def cleanHtml(sentence):\n    cleanr = re.compile('<.*?>')\n    cleantxt = re.sub(cleanr, ' ', sentence)\n    return cleantxt\n\ndef cleanpunc(sentence):\n    cleaned = re.sub(r'[?|!|\\'|\"|#]', r'', sentence)\n    cleaned = re.sub(r'[.|,|)|(|\\|/]', r' ', cleaned)\n    return cleaned","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Text Preprocessing\n* Removing stop words from reviews\n* Removing HTML Tags and punctuations\n* Get a stem word","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\nsno = nltk.stem.SnowballStemmer(\"english\")\nstop = set(stopwords.words(\"english\"))\nall_positive_words = []\nall_negative_words = []\nfinal_string = []\nstr1 = ''\ni = 0\nfor string in data[\"review\"].values:\n    filtered_sentence = []\n    # Removes html tags from every review\n    sent = cleanHtml(string)\n    for w in sent.split():\n        # For every word in a review clean punctions\n        for cleanwords in cleanpunc(w).split():\n            # if cleaned is alphabet and length og words greater than 2 then proceed\n            if ((cleanwords.isalpha()) and len(cleanwords)>2):\n                # check weather word is stop word or not\n                if cleanwords.lower() not in stop:\n                    # If word is not stop word then append it to filtered sentence\n                    s = (sno.stem(cleanwords.lower())).encode('utf-8')\n                    filtered_sentence.append(s)\n                    if (data[\"sentiment\"].values)[i].lower() == \"positive\":\n                        all_positive_words.append(s)\n                    if (data[\"sentiment\"].values)[i].lower() == \"negative\":\n                        all_negative_words.append(s)\n                else:\n                    continue\n            else:\n                continue\n    # filtered_sentence is list contains all words of a review after preprocessing\n    # join every word in a list to get a string format of the review\n    str1 = b\" \".join(filtered_sentence)\n    #append all the string(cleaned reviews)to final_string\n    final_string.append(str1)\n    i += 1        ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Distribution of positive & Negative word Frequnecy","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axis = plt.subplots(1, 2)\nprint(len(all_positive_words))\npos_words_freq = list(Counter(all_positive_words).values())\nprint(len(all_negative_words))\nneg_words_freq = list(Counter(all_negative_words).values())\nsns.distplot(pos_words_freq, ax = axis[0])\nsns.distplot(neg_words_freq, ax = axis[1])\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Replacing the cleaned text to Data Frame\n* Replace lables with int values\n* positive = 1\n* negative = 0","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data[\"review\"] = final_string","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def conv_label(label):\n    if label.lower() == \"positive\":\n        return 1\n    elif label.lower() == \"negative\":\n        return 0\n\ndata[\"sentiment\"] = data[\"sentiment\"].map(conv_label)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"freq_pos_words = nltk.FreqDist(all_positive_words)\nfreq_neg_words = nltk.FreqDist(all_negative_words)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Here word \"good\" is present in both positive and negative reviews.\n* It's likely contains \"Not good\" in negative a review, but due to uni gram we are loosing this information.\n* So It's better to use n-grams where n>=2","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"freq_pos_words.most_common(15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"freq_neg_words.most_common(15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Bag of words vector with bi-grams\ncount_vect = CountVectorizer(ngram_range = (1, 2))\ncount_vect = count_vect.fit(data[\"review\"].values)\nbigram_wrds = count_vect.transform(data[\"review\"].values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#TF-Idf vector using bi-grams\ncount_vect_tfidf = TfidfVectorizer(ngram_range = (1, 2))\ncount_vect_tfidf = count_vect_tfidf.fit(data[\"review\"].values)\ntfidf_wrds  = count_vect_tfidf.transform(data[\"review\"].values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"bigram_wrds","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We are running our classifier on TF-Idf data here.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score \n# change X to bigram_wrds to run classifier on Bag Of Words(BoW)\nX = bigram_wrds\n# X = tfidf_wrds\nY = data[\"sentiment\"]\nx_l, x_test, y_l, y_test = train_test_split(X, Y, test_size = 0.3, random_state = 42)\nfrom sklearn.naive_bayes import MultinomialNB\nclf = MultinomialNB(alpha = 0.7)\nclf.fit(x_l, y_l)\npred = clf.predict(x_test)\nacc = accuracy_score(y_test, pred, normalize = True) * float(100)  \nprint(\"acc is on test data:\", acc)\nsns.heatmap(confusion_matrix(y_test, pred), annot = True, fmt = 'd')\ntrain_acc = accuracy_score(y_l, clf.predict(x_l), normalize = True) * float(100)\nprint(\"train accuracy is:\", train_acc)\nprint(classification_report(y_test, pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### BOW\n* alpha: 0.7\n* Test accuracy: 88.62 \n* Train accuracy: 99.77\n\n### TF-IDF\n* alpha: 0.7\n* Test accuracy: 88.98\n* Trina accuracy: 98.86","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* Here We are Testing with our own review\n* We Ran our classifier on Tf-Idf, so We use Tf-Idf to convert our reivew to vector","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"review = [\"This is a worst movie\",\"This is a good movie\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#initialize BOW vectorizer\n#we already fitted the model for train data on \"count_vect\"(means alredy found probabilities for train data)\nvectorize = CountVectorizer(vocabulary = count_vect.vocabulary_)\n#Use classifier we trained using Bag of words\npolarity = clf.predict(vectorize.transform(review))\n# count_vect_tfidf.transform(review)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(polarity)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"You can save models and deploy using following code\n* save your classifier model\n* save your word vectorizer in a pickle file","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\nimport pickle as pkl\nf = open('classifier.pickle', 'wb')\npkl.dump(clf, f)\nf.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pickle as pkl\nf = open('vectorizer.pickle', 'wb')\npkl.dump(count_vect, f)\nf.close()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Deploying Naive Bayes Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"review = \"You can take from user input\"\nwith open(\"classifier.pickle\", 'rb') as f:\n    classifier = pkl.load(f)\nwith open(\"vectorizer.pickle\", 'rb') as f:\n    vectorizer = pkl.load(f)\nfrom sklearn.feature_extraction.text import CountVectorizer\n#vectorize your review which you used in training\nvector_review = CountVectorizer(vocabulary = vectorizer.vocabulary_)\nvector_review = vector_review.transform(review)\n#predict the vectorized review using your classifier \npredict = classifier.predict(vector_review)\nprint(predict)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}