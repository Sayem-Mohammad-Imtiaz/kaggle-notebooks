{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Stars type classifier","metadata":{}},{"cell_type":"markdown","source":"This classifier uses stars type data from https://www.kaggle.com/brsdincer/star-type-classification\n\nData description:\n\nTemperature -- K\nL -- L/Lo - relative luminocity (in the model renamed to L/Lo -lumin)\n\nR -- R/Ro - relative radius (in the model renamed to R/Ro -rad)\n\nAM -- Mv - magnitude (in the model renamed to Mv -magn)\n\nColor -- General Color of Spectrum\n\nSpectral_Class -- O,B,A,F,G,K,M / SMASS - https://en.wikipedia.org/wiki/Asteroid_spectral_types\n\nType -- Red Dwarf, Brown Dwarf, White Dwarf, Main Sequence , Super Giants, Hyper Giants\n\n\nTARGET:\nType\n\nfrom 0 to 5\n\nRed Dwarf - 0\n\nBrown Dwarf - 1\n\nWhite Dwarf - 2\n\nMain Sequence - 3\n\nSuper Giants - 4\n\nHyper Giants - 5\n\n\nMATH:\n\nLo = 3.828 x 10^26 Watts (Avg Luminosity of Sun)\n\nRo = 6.9551 x 10^8 m (Avg Radius of Sun)","metadata":{}},{"cell_type":"markdown","source":"## Loading libraries and dataset","metadata":{}},{"cell_type":"code","source":"# Lets load libraries\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import classification_report,confusion_matrix\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.inspection import permutation_importance\nfrom sklearn.model_selection import cross_validate, cross_val_score, train_test_split\n\nimport pickle","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# loading dataset\nsdata = pd.read_csv('../input/star-type-classification/Stars.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Exploratory data analysis","metadata":{}},{"cell_type":"markdown","source":"### Common data","metadata":{}},{"cell_type":"code","source":"# rename columns (because I like full names :)\nsdata.rename(columns = {'L':'L/Lo -lumin','R':'R/Ro -rad','A_M':'Mv -magn'}, inplace = True)\nsdata.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sdata.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# visualize missing data\nsns.heatmap(sdata.isnull(),yticklabels=False,cbar=False,cmap='viridis')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Numeric data","metadata":{}},{"cell_type":"code","source":"# pairplots for numeric data\nsns.pairplot(sdata, kind = 'reg')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### seems Type correlates with absolute magnitude","metadata":{}},{"cell_type":"code","source":"# lets calculate pairs correlation and build a heatmap (code taken from Kaggle-user ChrisX, https://www.kaggle.com/docxian/star-type-classification)\n\nfeatures_num = ['Temperature', 'L/Lo -lumin', 'R/Ro -rad', 'Mv -magn']\n\n# calc correlation matrices\ncorr_pearson = sdata[features_num].corr(method='pearson')         # Pearson's corr - shows the linear relationship \ncorr_spearman = sdata[features_num].corr(method='spearman')       # Spearman's corr - shows monotonic relationship\n\n# and plot side by side\nplt.figure(figsize=(15,5))\nax1 = plt.subplot(1,2,1)\nsns.heatmap(corr_pearson, annot=True, cmap='RdYlGn', vmin=-1, vmax=+1)\nplt.title('Pearson Correlation')\n\nax2 = plt.subplot(1,2,2, sharex=ax1)\nsns.heatmap(corr_spearman, annot=True, cmap='RdYlGn', vmin=-1, vmax=+1)\nplt.title('Spearman Correlation')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n\n\n","metadata":{}},{"cell_type":"markdown","source":"### Categorical data","metadata":{}},{"cell_type":"code","source":"g = sns.catplot(data=sdata, x='Color', y=\"Type\")\ng.fig.set_figwidth(7)\ng.fig.set_figheight(5)\ng.set_xticklabels(rotation = 90)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Colors are a bit messy. We will replace them with numerical columns later","metadata":{}},{"cell_type":"code","source":"# Spectral class categorical plot\ng = sns.catplot(data=sdata, x='Spectral_Class', y=\"Type\")\ng.fig.set_figwidth(7)\ng.fig.set_figheight(5)\ng.set_xticklabels(rotation = 0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Categorical values to numerical","metadata":{}},{"cell_type":"code","source":"sdata.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# these are basic colors categories, we will make separated columns for them\n# the transformation done below is a controversial decision, because I reduced total numbers of colors \n# and redistributed difficult colors into these simle basic categories. This redistribution is not fully\n# based on physical approach, where we have no special spectral definition of some taken colors (like \"Pale\").\n\nbasic_colors = {'RED','ORANGE','YELLOW','GREEN','BLUE','WHITE','PALE'}\nzero_list = [0]*len(sdata)\n\nfor col in basic_colors:\n    sdata[col] =  zero_list\n\n\n# I am always lazy, so copy-pasting is my love    \nsdata.loc[sdata.Color == 'Red','RED'] = 1\nsdata.loc[sdata.Color == 'White',['WHITE']] = 1\nsdata.loc[sdata.Color == 'Blue White',['BLUE','WHITE']] = 1\nsdata.loc[sdata.Color == 'Yellowish White',['YELLOW','WHITE']] = 1\nsdata.loc[sdata.Color == 'Blue white',['BLUE','WHITE']] = 1\nsdata.loc[sdata.Color == 'Pale yellow orange',['PALE','YELLOW','ORANGE']] = 1\nsdata.loc[sdata.Color == 'Blue',['BLUE']] = 1\nsdata.loc[sdata.Color == 'Blue-white',['BLUE','WHITE']] = 1\nsdata.loc[sdata.Color == 'Whitish',['WHITE']] = 1\nsdata.loc[sdata.Color == 'yellow-white',['YELLOW','WHITE']] = 1\nsdata.loc[sdata.Color == 'Orange',['ORANGE']] = 1\nsdata.loc[sdata.Color == 'White-Yellow',['WHITE','YELLOW']] = 1\nsdata.loc[sdata.Color == 'white',['WHITE']] = 1\nsdata.loc[sdata.Color == 'yellowish',['YELLOW']] = 1\nsdata.loc[sdata.Color == 'Yellowish',['YELLOW']] = 1\nsdata.loc[sdata.Color == 'Orange-Red',['ORANGE','RED']] = 1\nsdata.loc[sdata.Color == 'Blue-White',['WHITE','BLUE']] = 1          \n          \nsdata","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# replace Spectral_Class cat to numerical\ns_class = pd.get_dummies(sdata['Spectral_Class'], drop_first = True)\nsdata = pd.concat([sdata,s_class], axis = 1)\nsdata","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# make a copy of our dataset not to reload the main if we do something wrong :)))\ntdata = sdata.drop(['Color','Spectral_Class'], axis = 1).copy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model fitting","metadata":{}},{"cell_type":"code","source":"# train data and target separation\nx_data = tdata.drop('Type', axis = 1)\ny_data = tdata['Type']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model creation\n# we will take the Gradient boosting classifier\n \ngbc = GradientBoostingClassifier(loss = 'deviance', max_depth=3, n_estimators=400, learning_rate = 0.085,\n                                 min_samples_leaf = 1, max_features = 'log2')  #GradientBoosting model\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# our dataset is not big, so as I understand, there is no reason to divide it in more than 5 folds in a k-fold validation\n# we can play with this number to see how the accuracy changes\n\nmodel = gbc\nfolds_n = 5\ncv_results = cross_val_score(model, x_data, y_data, cv = folds_n, scoring=\"accuracy\",n_jobs=-1)\nprint('min accuracy= {v}'.format(v = np.min(cv_results)))\nprint('avg accuracy= {v}'.format(v = np.mean(cv_results)))\nprint('max accuracy= {v}'.format(v = np.max(cv_results)))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Looks too optimistic (at different runs). But even if we change the number of folds to 3 (biger test, smaller train) the accuracy stays the same. I was thinking, that \"fit\" could use DataFrame index as a feature and because the initial data is sorted, it could be a data leakage, but seems not (see https://stackoverflow.com/questions/58635398/does-sklearn-use-pandas-index-as-a-feature)","metadata":{}},{"cell_type":"markdown","source":"#### So lets train the model on the full dataset and save is for future generations :)","metadata":{}},{"cell_type":"code","source":"model.fit(x_data,y_data)\n\n# save the model to disk  (see https://machinelearningmastery.com/save-load-machine-learning-models-python-scikit-learn/)\nfilename = 'star_classifier.sav'\npickle.dump(model, open(filename, 'wb'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Now lets see feature importance in the trained model (https://scikit-learn.org/stable/auto_examples/ensemble/plot_gradient_boosting_regression.html)","metadata":{}},{"cell_type":"code","source":"feature_importance = model.feature_importances_\nsorted_idx = np.argsort(feature_importance)\npos = np.arange(sorted_idx.shape[0]) + .5\nfig = plt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nplt.barh(pos, feature_importance[sorted_idx], align='center')\nplt.yticks(pos, np.array(x_data.columns)[sorted_idx])\nplt.title('Feature Importance (MDI)')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Thank you for attention :) Please judge me, but not strictly, I only study :)","metadata":{}}]}