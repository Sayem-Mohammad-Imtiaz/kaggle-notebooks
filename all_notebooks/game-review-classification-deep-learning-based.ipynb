{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Introduction\nHello people, welcome to this kernel. In this kernel I am going to classify game reviews collected from Steam. I will use deep learning based approach and tensorflow.\n\n# Table of Content\n1. Preparing Environment\n1. Data Overview and Preprocessing\n1. Building Model\n1. Loading Pre-trained Word Embeddings\n1. Training Model | Displaying Results\n1. Final Test\n1. Conclusion"},{"metadata":{},"cell_type":"markdown","source":"# Preparing Environment\n* We'll prepare our environment in this section, we'll import libraries and the data we'll use."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport re\nimport time\nimport random\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix,accuracy_score\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences \nfrom tensorflow.compat.v1.keras.layers import CuDNNGRU\n\nimport warnings as wrn\nwrn.filterwarnings('ignore')\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_set = pd.read_csv(\"/kaggle/input/game-review-dataset/train_gr/train.csv\")\ntest_set = pd.read_csv('/kaggle/input/game-review-dataset/test_gr/test.csv')\ngame_ov = pd.read_csv('/kaggle/input/game-review-dataset/train_gr/game_overview.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_set.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_set.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* As you can see, test set is useless because there are no labels in it."},{"metadata":{"trusted":true},"cell_type":"code","source":"game_ov.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Overview and Data Preprocessing\nIn this section we'll take a look at the data and then we'll process it to train a deep neural network."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_set.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* We have 17k sample.\n* Let's check class distribution."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(train_set[\"user_suggestion\"])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* We can consider our set balanced, great news guys!!!\n\nNow let's start to process our dataset, we'll follow steps below:\n\n1. Cleaning and Lowering Data\n1. Tokenizing and Padding\n1. Train Test Splitting"},{"metadata":{},"cell_type":"markdown","source":"### Step 1: Cleaning and Lowering The Data\nIn this section we'll drop redundant features from the data and define a function that clean the data."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dropping unrelevant features\nx = train_set[\"user_review\"]\ny = train_set[\"user_suggestion\"]\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def cleanTexts(texts):\n    cleaned = []\n    pattern = \"[^a-zA-Z0-9]\"\n    for text in texts:\n        clrd = re.sub(pattern,\" \",text).lower().strip()\n        cleaned.append(clrd)\n    return cleaned\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Let's check our function."},{"metadata":{"trusted":true},"cell_type":"code","source":"cleanTexts([\"If it works great, it will remove something  ()}12451235\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_cleaned = cleanTexts(x)\nx_cleaned[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Step 2: Tokenizing and Padding Data\nIn this section we'll convert our texts into sequences by matching each word with an integer. Then we'll make sure that each sequence has same length by adding 0's to the short ones and trimming long ones."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Tokenizer \ntokenizer = Tokenizer(num_words=5000)\ntokenizer.fit_on_texts(x_cleaned)\nx_tokens = tokenizer.texts_to_sequences(x_cleaned)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* And let's check our sequences."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(x_tokens[0])\nprint()\n\nprint(len(x_tokens[0]))\nprint(len(x_tokens[1]))\nprint(len(x_tokens[2]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* As you can see sequences has different shapes but neural networks works with the data that has a constant shape. Let's solve this problem by padding.\n* First we'll make an array that includes lengths of sequences and then we'll find the third quartile's value.\n* Our new sequences will have length third quartile."},{"metadata":{"trusted":true},"cell_type":"code","source":"len_arr = [len(s) for s in x_tokens]\nMAX_LEN = int(np.percentile(len_arr,.75))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Also we'll save this value to a json file in order to use in the future."},{"metadata":{"trusted":true},"cell_type":"code","source":"import json\nwith open(\"maxlen.json\",mode=\"w\") as F:\n    json.dump({\"maxlen\":MAX_LEN},F)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(MAX_LEN)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_tokens_pad = pad_sequences(x_tokens,maxlen=MAX_LEN)\nx_tokens_pad.shape\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Step 3: Train Test Splitting\nIn this section we'll split our set into train and test, we won't use test set in training."},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train,x_test,y_train,y_test = train_test_split(x_tokens_pad,np.asarray(y),test_size=0.2,random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(x_train.shape)\nprint(x_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Building Model\nIn this section I am going to build deep neural network using Tensorflow. But before start to implementation I wanna talk about GRU a bit.\n\nIn deep learning when we work with sequences (such as music is a sequence of notes and texts are sequences of words) we use Recurrent Neural Networks, because they have memories, they evaluate every part of sequences.\n\nBut when we use Recurrent Neural Networks (I'll call them RNN after this) we encounter with a big problem: **vanishing gradient**. Because of the backpropagation of neural networks we encounter with this problem.\n\nBut if we use LSTMs (Long Short Term Memories, developed version of Simple RNNs) or GRU (Gated Recurrent Units) we don't encounter with this problem, because these networks have some data filters that we named **forget gates**."},{"metadata":{"trusted":true},"cell_type":"code","source":"VOCAB_SIZE = len(tokenizer.word_index) + 1\n# We've added 1 because of padding\n\n# Each world will be 100D vector.\nVECTOR_SIZE = 100\n\ndef buildModel(MAX_LEN,embedding_weights=None):\n    \n    model = keras.Sequential()\n    if embedding_weights is not None:\n        model.add(layers.Embedding(input_dim=VOCAB_SIZE,\n                                   output_dim=VECTOR_SIZE,\n                                   input_length=MAX_LEN,\n                                   weights=[embedding_weights],\n                                   trainable=True\n                              ))\n        \n    else:\n        model.add(layers.Embedding(input_dim=VOCAB_SIZE,\n                                   output_dim=VECTOR_SIZE,\n                                   input_length=MAX_LEN\n                                  ))\n    \n    model.add(CuDNNGRU(512,return_sequences=True))\n    model.add(CuDNNGRU(1024,return_sequences=True))\n    model.add(CuDNNGRU(1024,return_sequences=False))\n    model.add(layers.Dense(1,activation=\"sigmoid\"))\n    \n    model.compile(optimizer=\"Adam\",loss=\"binary_crossentropy\",metrics=[\"accuracy\"])\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = buildModel(MAX_LEN)\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Loading Pre-trained Word Embeddings \n\nIn this section we'll load pre-trained word embeddings."},{"metadata":{"trusted":true},"cell_type":"code","source":"word2vec = {} # Trained glove model \nwith open(\"../input/glove-global-vectors-for-word-representation/glove.6B.100d.txt\",encoding=\"UTF-8\") as f:\n    for line in f:\n        values = line.split() \n        word = values[0]\n        vec = np.asarray(values[1:],dtype=\"float32\")\n        word2vec[word] = vec\n        ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* First we've read word vectors from text file and created a python dictionary."},{"metadata":{"trusted":true},"cell_type":"code","source":"# initializing as uniform\nembedding_matrix = np.random.uniform(-1,1,(VOCAB_SIZE,100))\n\nfor word,i in tokenizer.word_index.items():\n    if i<VOCAB_SIZE: \n        embedding_vector = word2vec.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[i] = embedding_vector\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Then we've created our embedding matrix and if a value in our set is in the pre trained vector we changed the value of it."},{"metadata":{"trusted":true},"cell_type":"code","source":"model = buildModel(MAX_LEN,embedding_matrix)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training Model | Displaying Results\nIn this section I am going to train our model."},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(x_train,y_train,epochs=3,validation_split=0.2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Final Test\nIn this section I am going to test our model with test set."},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = model.predict_classes(x_test)\n\naccuracy_sc = round(accuracy_score(y_pred=y_pred,y_true=y_test)*100,2)\nconf_matrix = confusion_matrix(y_pred=y_pred,y_true=y_test)\n\n\nprint(\"Accuracy score is {}% \".format(accuracy_sc))\n\nplt.subplots()\nsns.heatmap(conf_matrix,annot=True,linewidths=1.5,fmt=\".1f\")\nplt.xlabel(\"Prediction\")\nplt.ylabel(\"Actual\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Model does not have balance problem, so we can say not bad for %73."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}