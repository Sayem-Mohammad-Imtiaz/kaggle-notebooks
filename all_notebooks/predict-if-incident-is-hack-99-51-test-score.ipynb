{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Prediction if Server is Hack or not "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"This kernal(notebook) is an attempt to showcase the work done for Novartis hackathon by Hackerearth and on the topic of Imbalanced dataset, if you like my work please upvote and also provide your valuable comments on this kernal which will help me in improving further.\n\nAlthough its been 3 months for the competetion it took time for me to post the code here. \n\n**Better late than never :):)**"},{"metadata":{},"cell_type":"markdown","source":"## Problem Statement\n\nAll the countries across the globe have adapted to means of digital payments. And with the increased volume of digital payments, hacking has become common event\nwherein the hacker can try to hack your details just with your phone number linked to your bank account.\nHowever, there is data with some anonymized variables based on which one can predict that the hack is going to happen.\n\nThe problem is to build a __predictive model which can identify a pattern in these variables and suggest that a hack is going to happen So that the cyber security can somehow stop it before it happens__.\n\n![](https://raw.githubusercontent.com/VijayMukkala/Mini-Projects/master/Predict_incident-is-hack/hacker-1944688_640.jpg)"},{"metadata":{},"cell_type":"markdown","source":"- [Import Packages](#section1)<br>\n- [Exploring the data ](#section2)<br>\n- [Visualizing the data](#section3)<br>\n- [Preprocessing](#section4)<br>\n- [Model Selection](#section5)<br>\n- [Test for unseen data & output file ](#section6)<br>"},{"metadata":{},"cell_type":"markdown","source":"<a id=section1></a>\n## Import Packages"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Model libraries\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.svm import SVC\nimport xgboost as xgb\nfrom imblearn.ensemble import BalancedBaggingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\n\n#Other Libraries\nfrom sklearn.model_selection import cross_val_score, StratifiedKFold, train_test_split,RandomizedSearchCV\nfrom sklearn.metrics import recall_score,precision_score,confusion_matrix\nimport scipy.stats as stats","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=section2></a>\n## Exploring the data "},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/novartis-data/Train.csv')\ndata_test = pd.read_csv('/kaggle/input/novartis-data/Test.csv')\n\n\nprint('Shape of the data:',data.shape)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data:\n- __X_1 - X_15__ : Anonymized logging parameters\n- __Date__ : Date wof Incident occurance\n- __Incident ID__ : ID of the occurance of event\n- __Multiple Offense__ : Indicates that if the incident was hack"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info() #getting more information on the dtype","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe()  # To check on the statistics","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.isnull().sum() #checking for missing values in data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.columns #columns of the data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Visualizing the target variable"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(data['MULTIPLE_OFFENSE'].value_counts())\nplt.figure(figsize=(5,3))\nsns.countplot(data['MULTIPLE_OFFENSE'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that the data is imbalanaced & most of the mails are from hackers(suspicious)."},{"metadata":{},"cell_type":"markdown","source":"### What Is Data Imbalance?\n\nData imbalance usually reflects an unequal distribution of classes within a dataset.\n\nAs you can see from the above figure most of the mails are suspicious, if we dont fix the probelm the model will be biased. \n\nDealing with Imbalanced Datsets:\nThere are many ways of dealing with imbalanced datasets:\n- __Undersampling :__\nUndersampling is the process where you randomly\ndelete some of the observations from the majority class in order to match the numbers\nwith the minority class.\n- __Oversampling :__ \nIt is the process of generating synthetic data that tries to randomly generate a sample of the attributes from observations in the minority class.\n\nThe best kaggle kernal which I refer on the above topics :\nhttps://www.kaggle.com/janiobachmann/credit-fraud-dealing-with-imbalanced-datasets\n\nBut in this problem(kernal) I didnot use undersampling or oversampling techniques.\nI have applied on the Machine learning algorithms like XGBOOST, Ensembling methods , Balanced Bagging classifier with the required hyperparmaters which deals with imbalance data and helped achieved me a great score of __99.5 %__ on test score. We are gonna look into more detail while doing the model Building"},{"metadata":{},"cell_type":"markdown","source":"### Splitting the data into test & train before we do any preprocessing or implementing sampling techniques"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = data.drop('MULTIPLE_OFFENSE', axis=1)\ny = data['MULTIPLE_OFFENSE']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train,X_test,y_train,y_test = train_test_split(X,y,test_size =0.25, random_state = 42, stratify=y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('y_train:\\n',y_train.value_counts(normalize = True))\nprint('y_test:\\n',y_test.value_counts(normalize = True))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that the distribution of data is identical for both the varaibles __0 & 1__ in test & train data. "},{"metadata":{},"cell_type":"markdown","source":"<a id=section3></a>\n## Visualizing the data\n### Checking the distribution of Continous variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"#function to create histogram, Q-Q Plot and boxplot\n\ndef diagnostic_plots(df,variable):\n    \n    #define figure size\n    plt.figure(figsize =(16,4))\n    \n    #histogram\n    plt.subplot(1,3,1)\n    sns.distplot(df[variable], bins = 30, kde = False)\n    plt.title('Histogram')\n    \n    #Q-Q plot\n    plt.subplot(1,3,2)\n    stats.probplot(df[variable], dist = \"norm\", plot = plt)\n    plt.ylabel('RM quantiles')\n    \n    # box plot\n    plt.subplot(1,3,3)\n    sns.boxplot(y=df[variable])\n    plt.title('Boxplot')\n    \n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"diagnostic_plots(data,'X_2')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"diagnostic_plots(data,'X_3')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"diagnostic_plots(data,'X_6')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"diagnostic_plots(data,'X_7')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"diagnostic_plots(data,'X_8')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"diagnostic_plots(data,'X_10')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"diagnostic_plots(data,'X_11')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"diagnostic_plots(data.dropna(),'X_12')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"diagnostic_plots(data,'X_13')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"diagnostic_plots(data,'X_14')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"diagnostic_plots(data,'X_15')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above histogram ,Probability plot and Boxplots for continous variables in the data, we can see that there are many outliers present in the features __X_6,X_7,X_8,X_10,X_11,X_12,X_13,X_15__.\n\nOne way of dealing with outliers is to remove them from the data but this will cause information loss so we will use a technique called __Capping or Censoring capping__ in this kernal. \n\n__Capping or Censoring capping__ : means capping the maximum and /or minimum of a distribution at an arbitrary value. In other words, values bigger or smaller than the arbitrarily determined ones are __censored__.\n\nThis can be done using by a simple code and on each of the feature but in this notebook we gonna use __Winsorizer__ method from the __feature engine__ which deals with outliers using Capping method"},{"metadata":{},"cell_type":"markdown","source":"### Distribution of categorical variables "},{"metadata":{"trusted":true},"cell_type":"code","source":"f, axes = plt.subplots(ncols=4, figsize=(20,4))\nsns.countplot(data['X_1'],ax=axes[0])\nsns.countplot(data['X_4'],ax=axes[1])\nsns.countplot(data['X_5'],ax=axes[2])\nsns.countplot(data['X_9'],ax=axes[3])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The observation here is for variables X_1 and X_9 only 3 variables are giving more information and the rest of the values are very less. so instead of __onehot encoding__ technique I will use __onehot encoding of frequent categories__ .\n\nIn __One hot encoding of frequent categories__, we create dummy variables only for most frequent categories. It is equivalent to grouping all the remaining categories under a new category.We can choose if we want top 10 frequent variables or more as per the use case or data provide to us\n\nIn this kernal, we use selecting the top frequent values using __OneHotCategoricalEncoder API__. This encoder can also create binary variables for the n most popular categories"},{"metadata":{"trusted":true},"cell_type":"code","source":"### We need to install the libraries required for the preprocesing steps\n!pip install -U imbalanced-learn\n!pip install feature_engine","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=section4></a>\n\n## Preprocessing\n\n- We gonna use Pipelines for dropping the columns, doing standardization of Numerical data & for Frequent one hot encoding for categorical data\n\n\n### Why Pipelines?\n\nIn a typical machine learning workflow you will need to apply all transformations at least twice. Once when training the model and again on any new data when we want to predict. Using Scikit-learn pipelines as a tool will simplify this process.\n\nThey have several key benefits:\n- They make your workflow much easier to read and understand.\n- They enforce the implementation and order of steps in your project.\n- These in turn make your work much more reproducible."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Pipelines\nfrom sklearn.pipeline import Pipeline\nfrom imblearn.pipeline import make_pipeline as imbalanced_make_pipeline\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.pipeline import Pipeline as pl1\n\n#preprocessing methods\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, RobustScaler\n\n#preprocessing methods using feature engine\nfrom feature_engine import categorical_encoders as ce\nfrom feature_engine.outlier_removers import Winsorizer","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Preprocessing steps that need to be done before fitting the model\n\n- Missing values to be imputed for feature 'X_12'\n- Oulier treatment using capping technique.\n- Creating a pipleine with Standardization using standard scale on continous data & one hot encoding on Cagtegorical data\n- Drop the columns 'INCIDENT_ID' & 'DATE' as not much information is available\n- Fit the data and transform on both X_train & test\nThe data will be ready for using to the model after the above steps"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Segregating the data into numerical, categorical and features with outliers\nnumerical_features = ['X_2', 'X_3', 'X_6','X_7', 'X_8', 'X_10', 'X_11','X_12', 'X_13', 'X_14','X_15']\ncategorical_features = ['X_1', 'X_4', 'X_5','X_9']\noutliers_data = ['X_6', 'X_7','X_8','X_10','X_11','X_12','X_13','X_15']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Missing data imputation\nAs the X_12 feature data is skewed, the missing data can be replaced using median."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train['X_12'] = X_train['X_12'].fillna(X_train['X_12'].median())\nX_test['X_12'] = X_test['X_12'].fillna(X_test['X_12'].median())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Outlier Treatment\nWe are gonna treat the outliers for each individual variable in different ways of which tail to be considered for the capping method.\nSo in the below code we have created a pipeline"},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_features = ['X_1', 'X_4', 'X_5','X_9']\nX_train[categorical_features] = X_train[categorical_features].astype('object')\nX_test[categorical_features] = X_test[categorical_features].astype('object')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"outlier_treat =Pipeline(steps = [\n              ('outlier1', Winsorizer(distribution = 'gaussian', tail = 'right',fold = 3, variables = ['X_6', 'X_7','X_8','X_10','X_12'])),\n              ('outlier2', Winsorizer(distribution = 'gaussian', tail = 'left',fold = 3, variables = ['X_11', 'X_13'])),\n              ('outlier3', Winsorizer(distribution = 'gaussian', tail = 'both',fold = 3, variables = ['X_15']))\n                                      ])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"outlier_treat.fit(X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = outlier_treat.transform(X_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**NOTE** :The outlier treatment will be done only on the train data but not the test data."},{"metadata":{},"cell_type":"markdown","source":"### Creating pipelines with Standardization on Continous values & One Hot encoding for frequent categories using Feature engine"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Converting the categorical variables to 'object' for doing the one hot encoding operation\nX_train[categorical_features] = X_train[categorical_features].astype('object')\nX_test[categorical_features] = X_test[categorical_features].astype('object')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"numeric_transformer = Pipeline(steps = [\n              ('scaler', StandardScaler())\n                     ])\ncategorical_transformer = Pipeline(steps=[\n    ('onehot3',ce.OneHotCategoricalEncoder(top_categories = 3, variables = ['X_9','X_1'] )),\n    ('onehot4',ce.OneHotCategoricalEncoder(top_categories = 4, variables = ['X_5'] )),\n    ('onehot10',ce.OneHotCategoricalEncoder(top_categories = 9, variables = ['X_4'] ))\n])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Creating a column transformer"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.compose import ColumnTransformer\npreprocessor = ColumnTransformer(\n    transformers=[\n                 ('drop_columns', 'drop', ['INCIDENT_ID','DATE']), #dropping the columns \n                 ('num', numeric_transformer, numerical_features),\n                 ('cat', categorical_transformer,categorical_features)\n    ])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Fitting the data to the created column transformer"},{"metadata":{"trusted":true},"cell_type":"code","source":"preprocessor.fit(X_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Transforming the data on the Train & test"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = preprocessor.transform(X_train)\nX_test = preprocessor.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=section5></a>\n# Model Selection\n\nWe can try to fit the imbalanced data on below models\n\n- XGBOOST \n- Balanced Bagging classifier\n- Support vector machine\n- Random Forest\n- ADA Boost\n\n**Evaluation Metric : Recall score : TP/(TP+FN)**"},{"metadata":{},"cell_type":"markdown","source":"### 1. XGB Classifier with Hyper parameter(scale_pos_weight) \n\nscale_pos_weight hyperparamter controls the balance of positive and negative weights, useful for unbalanced classes."},{"metadata":{"trusted":true},"cell_type":"code","source":"# from sklearn.model_selection import RandomizedSearchCV\n# rs = RandomizedSearchCV(xgb_model, {\n#         'scale_pos_weight': [1,2],\n#         'learning_rate'   : [0.05,0.10,0.15,0.20,0.25,0.30],\n#         'min_child_weight': [1,3,5,7],\n#         'gamma'           : [0.0,0.1,0.2,0.3,0.4],\n#         'colsample_bytree': [0.3,0.4,0.5,0.7]\n#     }, \n#     cv=5, \n#     scoring = 'f1',\n#     return_train_score=False, \n#     n_iter=50\n# )\n# rs.fit(X_train, y_train)\n# pd.DataFrame(rs.cv_results_)[['param_scale_pos_weight','param_learning_rate','param_min_child_weight','param_gamma','param_colsample_bytree','mean_test_score']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Best parameters from the randomizedsearchcv\n{'scale_pos_weight': 1,\n 'min_child_weight': 1,\n 'learning_rate': 0.3,\n 'gamma': 0.3,\n 'colsample_bytree': 0.3}\n \n Applying the parameters in the xgb model"},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_model = xgb.XGBClassifier(scale_pos_weight= 1,min_child_weight=1,learning_rate= 0.35,gamma= 0.3,colsample_bytree= 0.3 )\nxgb_model.fit(X_train, y_train)\ny_pred_xgb = xgb_model.predict(X_test)\n\nscore = recall_score(y_test,y_pred_xgb)\nprint('Recall score :',score)\nconfusion_matrix(y_test,y_pred_xgb, labels = [1,0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2. Balanced Bagging Classifier\nBalancedBaggingClassifier uses a random undersampling strategy on the majority class within a bootstrap sample in order to balance the two classes"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create an object of the classifier.\nbbc = BalancedBaggingClassifier(base_estimator=DecisionTreeClassifier(),sampling_strategy='auto',\nreplacement=False,random_state=0)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import precision_score\nbbc.fit(X_train, y_train)\ny_pred_bbc = bbc.predict(X_test)\n\nscore = recall_score(y_test,y_pred_bbc)\nprecision = precision_score(y_test,y_pred_bbc)\nprint('recall score :',score)\nconfusion_matrix(y_test,y_pred_bbc, labels = [1,0])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3. SVM with Weighted class\n\nClass-weighted SVM is designed to deal with unbalanced data by assigning higher misclassification penalties to training instances of the minority class.\nThe paramter used : __class_weight = 'balanced'__"},{"metadata":{"trusted":true},"cell_type":"code","source":"# from sklearn.model_selection import RandomizedSearchCV\n# fold = StratifiedKFold(n_splits=5, random_state=None, shuffle=False)\n# svm = SVC()\n\n# rs = RandomizedSearchCV(SVC(class_weight = 'balanced'), {\n#         'C': [0.1, 1, 10],\n#         'kernel': ['linear', 'poly', 'rbf'],\n#         'tol' :[0.1,0.001,0.001]\n#     }, \n#     cv=fold, \n#     scoring=\"recall\", \n#     n_iter=5\n# )\n# rs.fit(X_train, y_train)\n# pd.DataFrame(rs.cv_results_)[['param_C','param_kernel','param_tol','mean_test_score']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After the randomized searchcv the best params are :\n__C = 0.1 , kernel = 'poly', tol = 0.001__"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = SVC(class_weight = 'balanced', C = 0.1 , kernel = 'poly', tol = 0.001)\nmodel.fit(X_train, y_train)\ny_pred_svm = model.predict(X_test)\n\nscore = recall_score(y_pred_svm,y_test)\nprint('recall score',score)\nconfusion_matrix(y_pred_svm,y_test, labels = [1,0])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4. Random Forest With Bootstrap Class Weighting\n\nGiven that each decision tree is constructed from a bootstrap sample (e.g. random selection with replacement), the class distribution in the data sample will be different for each tree.\n\nAs such, it might be interesting to change the class weighting based on the class distribution in each bootstrap sample, instead of the entire training dataset.\n\nThis can be achieved by setting the class_weight argument to the value ‘balanced_subsample‘."},{"metadata":{"trusted":true},"cell_type":"code","source":"model_rf = RandomForestClassifier(class_weight='balanced_subsample')\nmodel_rf.fit(X_train, y_train)\ny_pred_rf = model_rf.predict(X_test)\n\nscore = recall_score(y_pred_rf,y_test)\nprint(score)\nconfusion_matrix(y_pred_rf,y_test, labels = [1,0])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5. ADA boost"},{"metadata":{"trusted":true},"cell_type":"code","source":"ada_model = AdaBoostClassifier()\nada_model.fit(X_train, y_train)\ny_pred_ada = ada_model.predict(X_test)\n\nscore = recall_score(y_test,y_pred_ada)\nprint('Recall score :', score)\nconfusion_matrix(y_test,y_pred_ada, labels = [1,0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Conclusion :\nConsidering the recall score , XGB Classifier is having the best recall score after hyper tuning the parameters. So we will use this model for our prediction."},{"metadata":{},"cell_type":"markdown","source":"<a id=section6></a>\n## Test for unseen data & output file \n\n#### Now applying the model on the hackathon test data"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_test.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Transforming the test data same as the operations done on the train data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Replacing the missing values with the median values\ndata_test['X_12'] = data_test['X_12'].fillna(data_test['X_12'].median())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Converting the datatype to 'object' for all the categorical features for transformations\ndata_test[categorical_features] = data_test[categorical_features].astype('object')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Performing scaling , one hot encoding on the data using sklearn pipelines\ndata_test1 = preprocessor.transform(data_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Transforming the train data with all the data , previously we divide the train data again into test & train for internal evaluation and selection of algorithm"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Missing value imputation\nX['X_12'] = X['X_12'].fillna(X['X_12'].median())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Outlier treatment\nX = outlier_treat.transform(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Preprocessing \nX = preprocessor.transform(X)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now our data is ready for final prediction & the model we will be using is XBoost Classifier & ADA boost classifier"},{"metadata":{},"cell_type":"markdown","source":"### Testing the unseen data for XGB Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_model.fit(X, y)\nprediction_xgb = xgb_model.predict(data_test1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"output_xgb=pd.DataFrame({\"INCIDENT_ID\":data_test[\"INCIDENT_ID\"],\"MULTIPLE_OFFENSE\":prediction_xgb}) \noutput_xgb.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(output_xgb['MULTIPLE_OFFENSE'].value_counts())\nsns.countplot(output_xgb['MULTIPLE_OFFENSE'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Using the above approaces , I was able to achieve 99.5% on the unseen test data using XGB model with hyperparameter tuning.\n\n### I hope this notebook was useful\n\n## Thank you, kindly Upvote and Happy learning :)"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}