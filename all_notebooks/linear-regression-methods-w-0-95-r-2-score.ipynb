{"metadata":{"language_info":{"mimetype":"text/x-python","version":"3.6.1","file_extension":".py","name":"python","nbconvert_exporter":"python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3"},"kernelspec":{"name":"python3","language":"python","display_name":"Python 3"}},"cells":[{"source":"In this Kernel, we are going to visualize and then regressively predict the ground energy states of molecules based on certain quantum parameters. \n\nPackages used:\n1. pandas/numpy\n2. seaborn/matplotlib\n3. scikit learn\n\nTable of Contents\n1. Data Visualization\n2. Linear Models(Ordinary Linear Regression, Lasso, Ridge)\n3. Model Interpretation\n\nRead on if you want to learn any of these!","cell_type":"markdown","metadata":{"_cell_guid":"32b4294b-659b-46d6-b1a4-1154c92140b7","_uuid":"43bd89658d89fc1aea0b4b61ca77646d4d6ac5ad"}},{"outputs":[],"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport sklearn\nimport seaborn as sns\n\ndf = pd.read_csv('../input/roboBohr.csv')\n","cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":false,"_cell_guid":"cf676999-2f8e-4c36-9464-cec31b4699de","_kg_hide-output":false,"_uuid":"34f1eacb5afee9fd247c60a736dc1197e48d8282"}},{"source":"**VISUALIZATION**","cell_type":"markdown","metadata":{"_cell_guid":"9a0a14e9-4c7f-4094-bbe5-501102a92802","_uuid":"e449c29a32a8d07280da0c6f55184b3706a0c102"}},{"outputs":[],"source":"df.head()\nX = df.iloc[:,:1275]\ny = df.iloc[:,-1:]\n#drop unnamed id column and pubchem column\ndf = df.drop(['Unnamed: 0','pubchem_id'],axis=1)\n#rename target feature to Energy State\ndf = df.rename(columns={'Eat':'Energy_State'})\n","cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9b17fbea-bb48-44d1-bf99-602e88f12448","collapsed":true,"_uuid":"0845a3b419c3fdbce104825d960fbbeb88e454e2"}},{"outputs":[],"source":"print('We have {0} rows/training examples and {1} columns/features'.format(df.shape[0],df.shape[1]-1))\n#describe various parameters of each feature, exclude count statistic because included in rows\ndf.iloc[:,1:(len(df.columns)-1)].describe()[1:]","cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c48c5c49-8c8a-4857-bd66-082b7ab42009","_uuid":"dac7a242d76586b872d647de18713a882697c3fa"}},{"source":"Using descriptive statistics is sometimes difficult to understand, but we have too many dimensions to plot in 2 or 3 dimensions. Instead, we will selectively pick some random columns to plot. ","cell_type":"markdown","metadata":{"_cell_guid":"cccec1b0-ef5b-4bd3-bee7-a4f77f1c569a","_uuid":"7073cfe56cf1e0c1b0b9e9b4b2288006ac7c4113"}},{"outputs":[],"source":"#regression plotting b/w features and target variable\nax=sns.regplot(df['4'],df['Energy_State'])\nax=sns.regplot(df['8'],df['Energy_State'])\nax.set(xlabel='Feature Value', ylabel='Energy State')","cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a6d15290-77cb-4e7b-be63-5c07f6827446","_uuid":"337ec4a73d66ea69ad2b736b2aa10f1e82eb1ac1"}},{"outputs":[],"source":"#Violinplot some features\nax1=sns.violinplot(df[['100','200','300']],palette='muted',orient='v')\n#Note skewed distribution and outliers","cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"042c699c-9632-496c-9a26-e2967d0eb887","_uuid":"9a1e6263002d304ccfcf9f9af878d567f5603868"}},{"outputs":[],"source":"#lets try some transformations to make distributions more normal\nlogs = np.log(df[['100','200','300']]+1) # +1 because log(0) is NAN\nax1=sns.violinplot(logs,palette='muted',orient='v')","cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"482620bf-3aa8-4288-9fd6-f9c2201b363e","_uuid":"e480987f9eceb4a7dfb569cb70abb50104709140"}},{"outputs":[],"source":"#Lets try square rooting as well\nsquares = (df[['100','200','300']]+1)*0.5 # +1 because log(0) is NAN\nax1=sns.violinplot(squares,palette='muted',orient='v')","cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"347cf15d-3b62-4ff8-ada2-d618ddd08106","_uuid":"a4683785d6744f5df71acb3a153ac47fe9bc9bad"}},{"outputs":[],"source":"#violinplot of target variable\nsns.violinplot(df['Energy_State'])\n#Note that chemical bonding energy is measured from 0 negatively","cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0e870bed-5321-43ba-b561-86ecd529d143","_uuid":"82c4e1ca992a4e4eb8fb7f76a44163ad7e960989"}},{"source":"**Linear Models**","cell_type":"markdown","metadata":{}},{"outputs":[],"source":"#scales features so they have means of 0, std of 1\ndef normalization(X_train, X_test):\n    from sklearn import preprocessing\n    scaler = preprocessing.StandardScaler().fit(X_train) \n    #mean = scaler.mean_\n    #std = scaler.var_\n    X_train_std = scaler.transform(X_train)\n    X_test_std = scaler.transform(X_test)\n    return X_train_std, X_test_std, scaler\n#splits dataset in to test and train sets\ndef test_train_split(X,y,split=0.33):\n    from sklearn.model_selection import train_test_split\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=split)\n    return X_train,X_test,y_train,y_test","cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2c365a9f-f9e3-44ad-8527-e3ac40096c9b","collapsed":true,"_uuid":"8b1cb5dfbeb19d68713c6cafa2b2662448b00186"}},{"outputs":[],"source":"def lin_reg_workflow(X,y):\n    from sklearn.metrics import mean_squared_error, r2_score\n    from sklearn import linear_model\n    #split into test/train data and normalize\n    X_train, X_test, y_train, y_test = test_train_split(X,y)\n    X_train, X_test, scaler = normalization(X_train,X_test)\n    y_train,y_test = y_train.values.ravel(),y_test.values.ravel()\n    \n    regr = linear_model.LinearRegression()\n    regr.fit(X_train,y_train)\n    y_pred = regr.predict(X_test)\n    \n    print(\"Lin Reg Mean squared error: %.2f\"% mean_squared_error(y_test, y_pred))\n    print('Lin Reg Variance score aka r^2: %.2f' % r2_score(y_test, y_pred))\n    \n    coefficients = regr.coef_\n    return y_test,y_pred,coefficients\n\nOLE_y_test,OLE_y_pred,OLE_coef = lin_reg_workflow(X,y)\nprint('Linear Regression coefficients', OLE_coef)\n","cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"16c5a163-0d14-4627-a2fd-762889ccd71f","_uuid":"a084df496b499ec5854b21478deaddaaab6c7292"}},{"source":"Clearly, linear regression is a very poor predictor of Energy States. We can attribute this to the non-normal distribution of features ansd non-linear relationship between the features and the target variable.\nNow, it is also possible that some of the features were poor predictors or had low variances. We will now implement Lasso and Ridge Regression, which are feature selectors and linear regressors, and compare performances.\nNotice that some coefficients are relatively large(10^2) while some are very small(10^-2). We will see how other linear regression methods can deal with this","cell_type":"markdown","metadata":{}},{"outputs":[],"source":"#Lasso regression is L1 regression and is often used for feature selection as well\n#Note that sklearn offers built-in cross val\ndef lassoCV_reg_workflow(X,y):\n    from sklearn.metrics import mean_squared_error, r2_score\n    from sklearn.linear_model import LassoCV\n    #split into test/train and normalize\n    X_train, X_test, y_train, y_test = test_train_split(X,y)\n    X_train, X_test, scaler = normalization(X_train,X_test)\n    #sklearn has trouble with panda df, convert to matrix\n    y_train,y_test = y_train.values.ravel(),y_test.values.ravel()\n    #set of alphas we use to cross validate\n    alphas = np.logspace(-4, 4, 14) #10**start, 10**end, num_samples,\n    lasso_cv = LassoCV(max_iter=10**6,alphas=alphas)\n    lasso_cv.fit(X_train,y_train)\n    y_pred = lasso_cv.predict(X_test)\n    \n    print(\"Lasso CV Mean squared error:\", mean_squared_error(y_test, y_pred))\n    print('Lasso CV Variance score aka r^2:', r2_score(y_test, y_pred))\n    \n    coefficients = lasso_cv.coef_\n    #best alpha chosen by cv\n    alpha = lasso_cv.alpha_ \n    return y_test,y_pred,coefficients,alpha\n\nlasso_y_test,lasso_y_pred,lasso_coef,lasso_alpha = lassoCV_reg_workflow(X,y)\nprint('Lasso CV Regression coefficients', lasso_coef)\nprint('Lasso CV Regression optimal alpha', lasso_alpha)","cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4936441b-6ca5-4314-823f-42469debf72f","_uuid":"b910df5e1530b812fe0a709f75f339e4d20d05dc"}},{"outputs":[],"source":"#Ridge regression is L2 regression and tends to offer higher accuracy than L1\ndef ridgeCV_reg_workflow(X,y):\n    from sklearn.metrics import mean_squared_error, r2_score\n    from sklearn.linear_model import RidgeCV\n    #split into test/train and normalize\n    X_train, X_test, y_train, y_test = test_train_split(X,y)\n    X_train, X_test, scaler = normalization(X_train,X_test)\n    #another method to convert panda dataframes into normal arrays\n    y_train,y_test = y_train.values.ravel(),y_test.values.ravel()\n    alphas = np.logspace(-4, 4, 14) #10**start, 10**end,num_samples,\n    \n    ridge_cv = RidgeCV(alphas=alphas)\n    ridge_cv.fit(X_train,y_train)\n    y_pred = ridge_cv.predict(X_test)\n    \n    print(\"Ridge CV Mean squared error:\", mean_squared_error(y_test, y_pred))\n    print('Ridge CV Variance score aka r^2:', r2_score(y_test, y_pred))\n    \n    coef = ridge_cv.coef_\n    alpha = ridge_cv.alpha_\n    return y_test,y_pred,coef,alpha\n\nridge_y_test,ridge_y_pred,ridge_coef,ridge_alpha = ridgeCV_reg_workflow(X,y)\nprint('Ridge CV Regression coefficients', ridge_coef)\nprint('Ridge CV Regression optimal alpha', ridge_alpha)","cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"bf882a40-80f3-4e10-a261-4715c4098aa5","_uuid":"4e928e324358ffbd1863e8f04d35565514bb7193"}},{"source":"**Model Interpretation**","cell_type":"markdown","metadata":{}},{"source":"Note how Lasso Regression forces many of its coefficients to be zero, rather than arbitrarily close. This is a result of the underlying cost function, which minimizes coefficients to zero, rather than arbitrarily close to 0, as Ridge does.","cell_type":"markdown","metadata":{}},{"outputs":[],"source":"#Finding number of coefficients equal to zero\n#Ordinary Linear Regression\nplt.figure()\nax1 = sns.distplot(OLE_coef,bins=15)\nax1.set_title(\"OLE Distribution of Coefficients\")\nax1.set(xlabel=\"Distribution of Coefficient Values\")\nplt.show()\n","cell_type":"code","execution_count":null,"metadata":{}},{"outputs":[],"source":"#Lasso and Ridge CV Coefficients\n#lasso\nplt.figure()\nax2 = sns.distplot(lasso_coef,bins=15)\nax2.set_title(\"LassoCV Distribution of Coefficients\")\nax2.set(xlabel=\"Distribution of Coefficient Values\")\nplt.show()\n\n#Ridge\nplt.figure()\nax3 = sns.distplot(ridge_coef,bins=15)\nax3.set_title(\"RidgeCV Distribution of Coefficients\")\nax3.set(xlabel=\"Distribution of Coefficient Values\")\nplt.show()","cell_type":"code","execution_count":null,"metadata":{}},{"source":"Finally, lets examine the distribution of our error value i.e. the distribution of how far off we were. ","cell_type":"markdown","metadata":{}},{"outputs":[],"source":"def error_histogram(y_pred,y_test):    \n    MAE = abs(y_pred-y_test)\n    MAE = MAE[:(len(MAE)-1)]\n    print('min', y_test.min())\n    print('max', y_test.max())\n    print('MAE max', MAE.max())\n    print('MAE min', MAE.min())\n    MAE = [i for i in MAE if i<10 ]\n    return sns.distplot(MAE,bins=50)\n","cell_type":"code","execution_count":null,"metadata":{}},{"outputs":[],"source":"#Ordinary Linear Regression\n#We limit the distribution from errors of 0 to 10 because of outliers changing the graph shape\nplt.figure()\nax = error_histogram(OLE_y_pred,OLE_y_test)\nax.set(xlabel=\"Error Residual\")\nplt.show()","cell_type":"code","execution_count":null,"metadata":{}},{"outputs":[],"source":"#Lasso CV\n#Ordinary Linear Regression\n    \nplt.figure()\nax = error_histogram(lasso_y_pred,lasso_y_test)\nax.set(xlabel=\"Error Residual\")\nplt.show()\n#Notice the much lower max MAE!","cell_type":"code","execution_count":null,"metadata":{}},{"outputs":[],"source":"#Ridge CV\n    \nplt.figure()\nax = error_histogram(ridge_y_pred,ridge_y_test)\nax.set(xlabel=\"Error Residual\")\nplt.show()","cell_type":"code","execution_count":null,"metadata":{}},{"source":"In this kernel, we went over using advanced linear regression methods, such as Lasso and Ridge Linear Regression with Cross Validation via scikit learn for predicting ground state energies of molecules. If you have any questions, feel free to leave one in the comments and if you learned something from this kernel, please leave an upvote!","cell_type":"markdown","metadata":{}}],"nbformat":4,"nbformat_minor":1}