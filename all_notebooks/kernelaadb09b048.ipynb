{"cells":[{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"## Importing the required libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7af7cef5d3d3ef24b4df12fa13b429aaf93a0c1e"},"cell_type":"code","source":"## Step 1: Importing data from source\ndataset = pd.read_csv(\"../input/Pokemon.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"65947793ca800479b0743ef54a1dbb1f4f6abe1c"},"cell_type":"code","source":"## Analyzing the structure and aspects of data\nprint(dataset.head(5))\nprint(dataset.shape)\nprint(dataset.index)\nprint(dataset.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8f2e3760e741073010ab2550f81e3d2734e33597"},"cell_type":"code","source":"## Since the dataset is quite minimal, Let's try the alternative method for handling missing values\n\n## Technique 2 - This can be applied on features which has numerical data like year, values etc. This is an approximation which adds variance to the dataset but can avoid loss of data\n## It's a standard technique and for this dataset we have mix of both numerical and categorical data.\n\n## Numerical NaN \n## In the given dataset below are the features with numerical values\n\n## Features :[Total,HP,Attack,Defense,Sp.Atk,Sp.Def,Speed]\n## Note: Generation is a numerical value but those values are categorical so we are not considering it.\n\n# print(dataset['Total'].mean())\n# print(dataset['Total'].tail())\n\ndataset['Total']= dataset['Total'].fillna(dataset['Total'].mean())\n\n\n\n## Similar technique to be adopted for other numerical columns\n\n# print(dataset['HP'].mean())\n# print(dataset['HP'].tail())\n\ndataset['HP']= dataset['HP'].replace(np.NaN,dataset['HP'].mean())\n\n# print(dataset['Attack'].mean())\n# print(dataset['Attack'].tail())\n\ndataset['Attack'] = dataset['Attack'].replace(np.NaN,dataset['Attack'].mean())\n# print(dataset['Defense'].mean())\n# print(dataset['Defense'].tail())\n\ndataset['Defense'] = dataset['Defense'].replace(np.NaN,dataset['Defense'].mean())\n# print(dataset['Sp. Atk'].mean())\n# print(dataset['Sp. Atk'].tail())\n\ndataset['Sp. Atk'] = dataset['Sp. Atk'].replace(np.NaN,dataset['Sp. Atk'].mean())\n# print(dataset['Sp. Def'].mean())\n# print(dataset['Sp. Def'].tail())\n\ndataset['Sp. Def'] = dataset['Sp. Def'].replace(np.NaN,dataset['Sp. Def'].mean())\n\n# print(dataset['Speed'].mean())\n# print(dataset['Speed'].tail())\n\ndataset['Speed'] = dataset['Speed'].replace(np.NaN,dataset['Speed'].mean())\n\nprint(dataset['Speed'])\nprint(dataset.isna().any())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2307f90d324fe592fca10ddd42e058e9d985a993"},"cell_type":"code","source":"## 2. Label Encoding \n##  LabelEncoder encode labels with a value between 0 and n_classes-1 where n is the number of distinct labels. If a label repeats it assigns the same value to as assigned earlier.\n\nfrom sklearn.preprocessing import LabelEncoder\nlabel_encoder = LabelEncoder()\n\ndataset1 = dataset\nds = dataset1[['Type 1','Type 2','Generation','Legendary']]\nprint(dataset['Total'])\n#print(ds)\nX = ds.iloc[:,:4].values\nprint(X)\n#print(dataset.tail())\n\n#[:,0]=label_encoder.fit_transform(X[:,0])\n#print(X)\nX[:,1]=label_encoder.fit_transform(X[:,1].astype(str))\nX[:,2]=label_encoder.fit_transform(X[:,2])\nX[:,3]=label_encoder.fit_transform(X[:,3])\n\n##print(X[:,1])\n \ncolumns = ['Type 1','Type 2','Total','HP','Attack','Defense','Sp. Atk','Sp. Def','Speed','Generation','Legendary']\n\nType1 = pd.DataFrame(X[:,0])\nType2 = pd.DataFrame(X[:,1])\nTotal = pd.DataFrame(dataset['Total'])\nHP = pd.DataFrame(dataset['HP'])\nAttack = pd.DataFrame(dataset['Attack'])\nDefense = pd.DataFrame(dataset['Defense'])\nSpAtk = pd.DataFrame(dataset['Sp. Atk'])\nSpDef= pd.DataFrame(dataset['Sp. Def'])\nSpeed= pd.DataFrame(dataset['Speed'])\nGeneration= pd.DataFrame(X[:,2])\nLegendary= pd.DataFrame(X[:,3])\n\nencoded_dataset = pd.DataFrame()\nencoded_dataset = pd.concat([encoded_dataset,Type1,Type2,Total,HP,Attack,Defense,SpAtk,SpDef,Speed,Generation,Legendary],axis =1)\nencoded_dataset.columns = columns\nprint(encoded_dataset.columns)\n## The problem here is, since there are different numbers in the same column, \n## the model will misunderstand the data to be in some kind of order, 0 < 1 < 2. But this isnâ€™t the case at all. \n## To overcome this problem, we use One Hot Encoder.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6dda47efc6c77396ef64175c18cb6acc108b988b"},"cell_type":"code","source":"## 4. Creating dummies is another method of handling categorical data and it is somewhat similar to one hot encoding \n## Dummy Variables is one that takes the value 0 or 1 to indicate the absence or presence of some categorical effect that may be expected to shift the outcome.\n## Number of columns = number of category values\n\ndummy = pd.get_dummies(encoded_dataset['Type 1'])\nprint(dummy.columns)\ntdataset = dataset[['#', 'Name']]\ntransformed_dataset = pd.concat([tdataset,encoded_dataset],axis = 1)\ntransformed_dataset = pd.concat([transformed_dataset,dummy],axis =1)\ntransformed_dataset = transformed_dataset.drop(['Type 1'],axis = 1)\n\nprint(transformed_dataset)\n\n\n\n## 5. Sometimes, we use KNN Imputation(for Categorical variables): In this method of imputation, \n## the missing values of an attribute are imputed using the given number of attributes that are most similar to the attribute whose values are missing. \n## The similarity of two attributes is determined using a distance function, but we are going to stop our experiment only with dummies.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4d6f97fd37db1ecb1fd19e5a992380741e97161a"},"cell_type":"code","source":"# 'Bug', 'Dark', 'Dragon', 'Electric', 'Fairy', 'Fighting', 'Fire',\n#        'Flying', 'Ghost', 'Grass', 'Ground', 'Ice', 'Normal', 'Poison',\n#        'Psychic', 'Rock', 'Steel', 'Water'\nprint(transformed_dataset.columns)\n\n## Eliminating the name columns as we have '#' \nX = transformed_dataset[['#','Total','HP','Attack','Defense','Sp. Atk',\n       'Sp. Def', 'Speed', 'Generation','Legendary','Bug', 'Dark', 'Dragon',\n       'Electric', 'Fairy', 'Fighting', 'Fire', 'Flying', 'Ghost', 'Grass',\n       'Ground', 'Ice', 'Normal', 'Poison', 'Psychic', 'Rock', 'Steel',\n       'Water']]\ny = transformed_dataset[['Type 2']]\ny=y.astype('long')\nprint(X.isna().any())\nprint(y.isna().any())\n\nprint(np.where(y.values >= np.finfo(np.float64).max))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ffcfa7b2979d9c172317c9c633e9f287f105d08d"},"cell_type":"code","source":"# Import train_test_split function\nfrom sklearn.model_selection import train_test_split\n# Split dataset into training set and test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3) # 70% training and 30% test\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"468ee73bde36282ebed01b0aae2bea6149ea7c8b"},"cell_type":"code","source":"#Import Random Forest Model\nfrom sklearn.ensemble import RandomForestClassifier\n\n#Create a Gaussian Classifier\nclf=RandomForestClassifier(n_estimators=100)\n\n#Train the model using the training sets y_pred=clf.predict(X_test)\nclf.fit(X_train,y_train)\n\ny_pred=clf.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"4897258ffe1ae4e990b423db3695126db4231325"},"cell_type":"code","source":"#Import scikit-learn metrics module for accuracy calculation\nfrom sklearn import metrics\n# Model Accuracy, how often is the classifier correct?\n#print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n\n\nfrom sklearn.metrics import confusion_matrix\n\nconf_mat = confusion_matrix(y_test, y_pred)\nprint(conf_mat)\n\nconf_mat=np.matrix(conf_mat)\nFP = conf_mat.sum(axis=0) - np.diag(conf_mat)  \nFN = conf_mat.sum(axis=1) - np.diag(conf_mat)\nTP = np.diag(conf_mat)\nTN = conf_mat.sum() - (FP + FN + TP)\n\n# Sensitivity, hit rate, recall, or true positive rate\nTPR = TP/(TP+FN)\n# Specificity or true negative rate\nTNR = TN/(TN+FP) \n# Precision or positive predictive value\nPPV = TP/(TP+FP)\n# Negative predictive value\nNPV = TN/(TN+FN)\n# Fall out or false positive rate\nFPR = FP/(FP+TN)\n# False negative rate\nFNR = FN/(TP+FN)\n# False discovery rate\nFDR = FP/(TP+FP)\n\n# Overall accuracy\nACC = (TP+TN)/(TP+FP+FN+TN)\n\nprint('ACC',ACC)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a810da36730504177f0741e111e290aa22fe0e7d"},"cell_type":"code","source":"## This exercise of work is for demonstrating pre-processing techniques only, The model can give around 50% accuracy for now.\n## We got to apply some more data to make it improve it's accuracy as well hyper tuning of parameters in the algorithm.\n\n## The overall problem that the solution covers is to identify type 1 of the pokemon using other features in the dataset.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a89e265663ec03d6ae89d1b68b1366b8fb34c99d"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}