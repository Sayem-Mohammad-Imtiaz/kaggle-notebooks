{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-29T18:58:25.561904Z","iopub.execute_input":"2021-06-29T18:58:25.562317Z","iopub.status.idle":"2021-06-29T18:58:25.584336Z","shell.execute_reply.started":"2021-06-29T18:58:25.562283Z","shell.execute_reply":"2021-06-29T18:58:25.582965Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# General Idea of this code - I want to compare how similar 3 documents are.\n* Clean the text by removing  punctuation and stop words\n* Convert Text into a vector\n* Compare the vectors of these documents.\n\nThe numbers of dimensions in this vector space is the number of different words in the text. The length of the word vector will be longer if there are more occurences of that word.\n\nThis technique is called Term frequencyâ€“inverse document frequency or TF-IDF. I got most of the code from https://leantechblog.wordpress.com/2020/08/23/how-to-estimate-text-similarity-with-python/.","metadata":{}},{"cell_type":"code","source":"!pip3 install nltk\n!pip3 install gensim\nimport nltk\nnltk.download('punkt')\nnltk.download('stopwords')","metadata":{"execution":{"iopub.status.busy":"2021-06-29T18:58:25.58677Z","iopub.execute_input":"2021-06-29T18:58:25.587188Z","iopub.status.idle":"2021-06-29T18:58:39.295218Z","shell.execute_reply.started":"2021-06-29T18:58:25.587141Z","shell.execute_reply":"2021-06-29T18:58:39.293924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# How to convert text into a vector","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\nphrase_one = 'This is Sparta'\nphrase_two = 'This is New York'\nvectorizer = TfidfVectorizer ()\nX = vectorizer.fit_transform([phrase_one,phrase_two])\n\nvectorizer.get_feature_names()","metadata":{"execution":{"iopub.status.busy":"2021-06-29T18:58:39.299683Z","iopub.execute_input":"2021-06-29T18:58:39.300006Z","iopub.status.idle":"2021-06-29T18:58:39.325213Z","shell.execute_reply.started":"2021-06-29T18:58:39.299972Z","shell.execute_reply":"2021-06-29T18:58:39.324068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Cleaning the data (remove useless words and punctuation)","metadata":{}},{"cell_type":"code","source":"from string import punctuation\nfrom nltk.corpus import stopwords\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nlanguage_stopwords = stopwords.words('english')\nnon_words = list(punctuation)\n\n\"\"\"# Cleaning the data (remove useless words and punctuation)\"\"\"\n\ndef remove_stop_words(dirty_text):\n    cleaned_text = ''\n    for word in dirty_text.split():\n        if word in language_stopwords or word in non_words:\n            continue\n        else:\n            cleaned_text += word + ' '\n    return cleaned_text\n\ndef remove_punctuation(dirty_string):\n    for word in non_words:\n        dirty_string = dirty_string.replace(word, '')\n    return dirty_string\n\ndef process_file(file_name):\n    file_content = open(file_name, \"r\").read()\n    # All to lower case\n    file_content = file_content.lower()\n    # Remove punctuation and spanish stopwords\n    file_content = remove_punctuation(file_content)\n    file_content = remove_stop_words(file_content)\n    return file_content\n\ndef process_text(text):\n    text_content = text\n    # All to lower case\n    text_content = text_content.lower()\n    # Remove punctuation and spanish stopwords\n    text_content = remove_punctuation(text_content)\n    text_content = remove_stop_words(text_content)\n    return text_content\n","metadata":{"execution":{"iopub.status.busy":"2021-06-29T18:58:39.326683Z","iopub.execute_input":"2021-06-29T18:58:39.326978Z","iopub.status.idle":"2021-06-29T18:58:39.368192Z","shell.execute_reply.started":"2021-06-29T18:58:39.326949Z","shell.execute_reply":"2021-06-29T18:58:39.367056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Generate a similarity matrix\nVectorise the documents (essentially a matrix now) then compute the similarity matrix which is basically a dot product between the word vectors.\nHere article title 1,2 and 3 are all about the same thing according to the dataset labelling.\nArticle 4 is about a different subject. ","metadata":{}},{"cell_type":"code","source":"#news = pd.read_csv(\"../input/uci-news-aggregator.csv\")\narticle1 = \"Fed official says weak data caused by weather, should not slow taper\"\narticle2 = \"Fed's Charles Plosser sees high bar for change in pace of tapering\"\narticle3 = \"US open: Stocks fall after Fed official hints at accelerated tapering\"\narticle4 = \"Euro Anxieties Wane as Bunds Top Treasuries, Spain Debt Rallies\"","metadata":{"execution":{"iopub.status.busy":"2021-06-29T18:58:39.369798Z","iopub.execute_input":"2021-06-29T18:58:39.37026Z","iopub.status.idle":"2021-06-29T18:58:39.375802Z","shell.execute_reply.started":"2021-06-29T18:58:39.370207Z","shell.execute_reply":"2021-06-29T18:58:39.374654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#TF-IDF\nvectorizer = TfidfVectorizer ()\nX = vectorizer.fit_transform([article1,article2,article3,article4])\nsimilarity_matrix = cosine_similarity(X,X)\n\nprint(similarity_matrix)\n","metadata":{"execution":{"iopub.status.busy":"2021-06-29T18:58:39.377149Z","iopub.execute_input":"2021-06-29T18:58:39.377546Z","iopub.status.idle":"2021-06-29T18:58:39.397572Z","shell.execute_reply.started":"2021-06-29T18:58:39.377514Z","shell.execute_reply":"2021-06-29T18:58:39.396538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The similarity matrix shows that the documents are identical to themselves (diagonal elements). It shows that article 4 does not correlate with any of the other articles - THIS IS CORRECT. It shows correlation between articles 1, 2 and 3. (Probably because they all have the word \"Fed\" and some of them have the word \"taper\" in them)\n","metadata":{}},{"cell_type":"code","source":"!pip3 install newspaper3k\nimport newspaper\nfrom newspaper import Article","metadata":{"execution":{"iopub.status.busy":"2021-07-02T17:28:41.493236Z","iopub.execute_input":"2021-07-02T17:28:41.493828Z","iopub.status.idle":"2021-07-02T17:29:00.538291Z","shell.execute_reply.started":"2021-07-02T17:28:41.493722Z","shell.execute_reply":"2021-07-02T17:29:00.537267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"url = \"https://www.bbc.co.uk/sport/cricket/57651883\"\narticle = Article(url)\narticle.download()\narticle.parse()\narticle.nlp()\nprint(article.keywords)","metadata":{"execution":{"iopub.status.busy":"2021-06-29T19:47:18.222647Z","iopub.execute_input":"2021-06-29T19:47:18.223072Z","iopub.status.idle":"2021-06-29T19:47:19.105257Z","shell.execute_reply.started":"2021-06-29T19:47:18.223038Z","shell.execute_reply":"2021-06-29T19:47:19.103934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#print(article.text)","metadata":{"execution":{"iopub.status.busy":"2021-06-29T18:58:47.130914Z","iopub.execute_input":"2021-06-29T18:58:47.131612Z","iopub.status.idle":"2021-06-29T18:58:47.135922Z","shell.execute_reply.started":"2021-06-29T18:58:47.131557Z","shell.execute_reply":"2021-06-29T18:58:47.134891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nRowsRead = 10000 # specify 'None' if want to read whole file\n# uci-news-aggregator.csv has 422419 rows in reality, but we are only loading/previewing the first 1000 rows\nnews_df = pd.read_csv('../input/all-the-news/articles1.csv', delimiter=',', nrows = nRowsRead)\nnews_df.head(3)\narticles = news_df['content']","metadata":{"execution":{"iopub.status.busy":"2021-06-29T19:23:06.347067Z","iopub.execute_input":"2021-06-29T19:23:06.347607Z","iopub.status.idle":"2021-06-29T19:23:08.216582Z","shell.execute_reply.started":"2021-06-29T19:23:06.347569Z","shell.execute_reply":"2021-06-29T19:23:08.215269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text_list = []\nfor article in articles:\n    text = process_text(article) \n    text_list.append(text)\n    \nvectorizer = TfidfVectorizer ()\nX = vectorizer.fit_transform(text_list)\nsimilarity_matrix = cosine_similarity(X,X)\nprint(similarity_matrix)","metadata":{"execution":{"iopub.status.busy":"2021-06-29T19:23:11.483983Z","iopub.execute_input":"2021-06-29T19:23:11.484385Z","iopub.status.idle":"2021-06-29T19:23:56.92333Z","shell.execute_reply.started":"2021-06-29T19:23:11.484351Z","shell.execute_reply":"2021-06-29T19:23:56.921921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nmatrix = (similarity_matrix - np.identity(nRowsRead))+ (np.ones(nRowsRead))\nlog_matrix = np.log(similarity_matrix)\nplt.imshow(log_matrix)\nplt.colorbar()\nplt.show()\n    ","metadata":{"execution":{"iopub.status.busy":"2021-06-29T19:25:28.868159Z","iopub.execute_input":"2021-06-29T19:25:28.868533Z","iopub.status.idle":"2021-06-29T19:25:38.172006Z","shell.execute_reply.started":"2021-06-29T19:25:28.8685Z","shell.execute_reply":"2021-06-29T19:25:38.170706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Find the best article match\nresult = np.where(matrix == np.amax(matrix))\nprint(result[0])","metadata":{"execution":{"iopub.status.busy":"2021-06-29T19:26:25.976022Z","iopub.execute_input":"2021-06-29T19:26:25.976608Z","iopub.status.idle":"2021-06-29T19:26:26.479515Z","shell.execute_reply.started":"2021-06-29T19:26:25.976569Z","shell.execute_reply":"2021-06-29T19:26:26.478042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"match = news_df.loc[result[0]]\n#print(match)\nprint(match['content'])\nmatch.to_csv(r'./Match.csv', index = True)","metadata":{"execution":{"iopub.status.busy":"2021-06-29T19:32:49.355682Z","iopub.execute_input":"2021-06-29T19:32:49.356281Z","iopub.status.idle":"2021-06-29T19:32:49.374861Z","shell.execute_reply.started":"2021-06-29T19:32:49.356232Z","shell.execute_reply":"2021-06-29T19:32:49.373247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This algorithm doesn't work very well for similarity grouping.\nInstead try and determine a Vector to the whole article in terms of Category. The aim is to try and make ppls vectors become more aligned.","metadata":{}},{"cell_type":"code","source":"cnn_paper = newspaper.build('http://cnn.com')\n\nfor article in cnn_paper.articles:\n     print(article.url)\n\n\nfor category in cnn_paper.category_urls():\n     print(category)\n\n\ncnn_article = cnn_paper.articles[0]\ncnn_article.download()\ncnn_article.parse()\ncnn_article.nlp()","metadata":{"execution":{"iopub.status.busy":"2021-07-01T17:37:17.182217Z","iopub.execute_input":"2021-07-01T17:37:17.182614Z","iopub.status.idle":"2021-07-01T17:37:27.458157Z","shell.execute_reply.started":"2021-07-01T17:37:17.182572Z","shell.execute_reply":"2021-07-01T17:37:27.457166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Sentiment analysis","metadata":{}},{"cell_type":"code","source":"import nltk\nnltk.download('vader_lexicon')\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nsid = SentimentIntensityAnalyzer()\nsentence = \"I am Charlie and I like cheese.\"\nprint(sid.polarity_scores(sentence))\n\nhappy_url = \"https://edition.cnn.com/2021/07/01/health/science-of-laughter-scn-wellness/index.html\"\narticle = Article(happy_url)\narticle.download()\narticle.parse()\narticle.nlp()\n#print(article.text)\nprint(\"The happy article has scores:\")\nprint(sid.polarity_scores(article.text)['compound'])\n\nsad_url = \"https://edition.cnn.com/2021/07/02/us/miami-dade-building-collapse-friday/index.html\"\narticle2 = Article(sad_url)\narticle2.download()\narticle2.parse()\narticle2.nlp()\n#print(article.text)\nprint(\"The Sad article has scores:\")\nprint(sid.polarity_scores(article2.text)['compound'])\n\nphysics_url = \"https://edition.cnn.com/2021/07/02/world/ocean-twilight-zone-whoi-c2e-scn-spc-intl/index.html\"\narticle3 = Article(physics_url)\narticle3.download()\narticle3.parse()\narticle3.nlp()\n#print(article.text)\nprint(\"The physics article has scores:\")\nprint(sid.polarity_scores(article3.text))\n#This article has a high neutrality score\n#But the compound value is too polarised. It will be forced to pick between\n#either positive or negative values.","metadata":{"execution":{"iopub.status.busy":"2021-07-02T17:32:45.3749Z","iopub.execute_input":"2021-07-02T17:32:45.375277Z","iopub.status.idle":"2021-07-02T17:32:46.610197Z","shell.execute_reply.started":"2021-07-02T17:32:45.375246Z","shell.execute_reply":"2021-07-02T17:32:46.609161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# The problem\nThis code won't be very good. For example it doesn't take into account synonmyms of words. This is something that could be tackled with word2vec models. Loosely what we do is we say each word contains a small ammount of a different word. So the vector for the word \"King\" will also contain a small amount of the word \"Queen\" because they are related.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This is a really interesting article on a free version of GPT-3\nhttps://medium.com/mlearning-ai/text-generation-using-gpt-neo-41877ef586c7\nhttps://medium.com/mlearning-ai/a-graph-based-text-similarity-method-with-named-entity-information-in-nlp-abc7f1201d96","metadata":{}}]}