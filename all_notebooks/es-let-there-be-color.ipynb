{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport os\nimport imageio\nfrom IPython.display import Image, display\nimport matplotlib.pyplot as plt\nimport time","metadata":{"execution":{"iopub.status.busy":"2021-06-04T03:18:38.834686Z","iopub.execute_input":"2021-06-04T03:18:38.835179Z","iopub.status.idle":"2021-06-04T03:18:43.817934Z","shell.execute_reply.started":"2021-06-04T03:18:38.835074Z","shell.execute_reply":"2021-06-04T03:18:43.817141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset_path = '../input/landscape-pictures'","metadata":{"execution":{"iopub.status.busy":"2021-06-04T03:18:43.819545Z","iopub.execute_input":"2021-06-04T03:18:43.819857Z","iopub.status.idle":"2021-06-04T03:18:43.826428Z","shell.execute_reply.started":"2021-06-04T03:18:43.819831Z","shell.execute_reply":"2021-06-04T03:18:43.823043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset_len = len(os.listdir(dataset_path))","metadata":{"execution":{"iopub.status.busy":"2021-06-04T03:18:43.82836Z","iopub.execute_input":"2021-06-04T03:18:43.828782Z","iopub.status.idle":"2021-06-04T03:18:44.200464Z","shell.execute_reply.started":"2021-06-04T03:18:43.828746Z","shell.execute_reply":"2021-06-04T03:18:44.19977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset_len","metadata":{"execution":{"iopub.status.busy":"2021-06-04T03:18:44.202231Z","iopub.execute_input":"2021-06-04T03:18:44.202475Z","iopub.status.idle":"2021-06-04T03:18:44.214022Z","shell.execute_reply.started":"2021-06-04T03:18:44.202452Z","shell.execute_reply":"2021-06-04T03:18:44.213337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Dataset Preparation**","metadata":{}},{"cell_type":"code","source":"batch_size = 32\n\ndataset = tf.keras.preprocessing.image_dataset_from_directory(\n    '../input/', label_mode=None, image_size=(256, 256), batch_size=batch_size, shuffle=True, seed=16\n)\nAUTOTUNE = tf.data.AUTOTUNE\n\ndataset = dataset.cache().prefetch(buffer_size=AUTOTUNE)","metadata":{"execution":{"iopub.status.busy":"2021-06-04T03:18:44.217391Z","iopub.execute_input":"2021-06-04T03:18:44.217663Z","iopub.status.idle":"2021-06-04T03:18:51.056955Z","shell.execute_reply.started":"2021-06-04T03:18:44.217638Z","shell.execute_reply":"2021-06-04T03:18:51.056048Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# batch_size = 128\n\n# datagen = tf.keras.preprocessing.image.ImageDataGenerator()\n# dataset = datagen.flow_from_directory(\n#     '../input/flickr-image-dataset/flickr30k_images/flickr30k_images', class_mode=None, target_size=(256, 256), batch_size=batch_size\n# )\n\n# step_len = dataset_len // 128","metadata":{"execution":{"iopub.status.busy":"2021-06-04T03:18:51.059133Z","iopub.execute_input":"2021-06-04T03:18:51.059431Z","iopub.status.idle":"2021-06-04T03:18:51.066436Z","shell.execute_reply.started":"2021-06-04T03:18:51.059404Z","shell.execute_reply":"2021-06-04T03:18:51.065441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Color Space Conversion**","metadata":{}},{"cell_type":"code","source":"def rgb2ycbcr(img):\n    img_r = img[:,:,0]\n    img_g = img[:,:,1]\n    img_b = img[:,:,2]\n\n    y = 0.299*img_r + 0.587*img_g + 0.114*img_b\n    cr = (img_r - y)*0.713 + 128\n    cb = (img_b - y)*0.564 + 128\n\n    ycbcr = np.stack([y, cb, cr], axis=2).clip(min=0, max=255)\n    return ycbcr.astype('float32')\n\ndef ycbcr2rgb(img):\n    img_y = img[:,:,0]\n    img_cb = img[:,:,1]\n    img_cr = img[:,:,2]\n\n    r = img_y + 1.403*(img_cr-128)\n    g = img_y - 0.714*(img_cr-128) - 0.344*(img_cb-128)\n    b = img_y + 1.773*(img_cb-128)\n\n    rgb = np.stack([r, g, b], axis=2).clip(min=0, max=255)\n    return rgb.astype('float32')","metadata":{"execution":{"iopub.status.busy":"2021-06-04T03:18:51.06792Z","iopub.execute_input":"2021-06-04T03:18:51.068353Z","iopub.status.idle":"2021-06-04T03:18:51.07823Z","shell.execute_reply.started":"2021-06-04T03:18:51.068314Z","shell.execute_reply":"2021-06-04T03:18:51.077347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Deep Learning Model**","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras import Sequential\nfrom tensorflow.keras import Model\n\nfrom tensorflow.keras.applications import InceptionResNetV2\nfrom tensorflow.keras.applications import Xception\n\nfrom tensorflow.keras.layers import Input\nfrom tensorflow.keras.layers import Conv2D\nfrom tensorflow.keras.layers import Concatenate\nfrom tensorflow.keras.layers import Add\nfrom tensorflow.keras.layers import RepeatVector\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import Flatten\nfrom tensorflow.keras.layers import ReLU\nfrom tensorflow.keras.layers import UpSampling2D\nfrom tensorflow.keras.layers import MaxPool2D","metadata":{"execution":{"iopub.status.busy":"2021-06-04T03:18:51.079801Z","iopub.execute_input":"2021-06-04T03:18:51.080381Z","iopub.status.idle":"2021-06-04T03:18:51.08813Z","shell.execute_reply.started":"2021-06-04T03:18:51.080343Z","shell.execute_reply":"2021-06-04T03:18:51.087316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Low-Level Features Network","metadata":{}},{"cell_type":"code","source":"class low_level(tf.keras.Model):\n    def __init__(self, kernel_size=(3,3)):\n        super(low_level, self).__init__()\n        \n        self.filters = [64, 128, 128, 256, 256, 512]\n        \n        self.conv1 = Conv2D(filters=self.filters[0], kernel_size=kernel_size, strides=(2,2), padding='same')\n        self.conv2 = Conv2D(filters=self.filters[1], kernel_size=kernel_size, strides=(1,1), padding='same')\n        self.conv3 = Conv2D(filters=self.filters[2], kernel_size=kernel_size, strides=(2,2), padding='same')\n        self.conv4 = Conv2D(filters=self.filters[3], kernel_size=kernel_size, strides=(1,1), padding='same')\n        self.conv5 = Conv2D(filters=self.filters[4], kernel_size=kernel_size, strides=(2,2), padding='same')\n        self.conv6 = Conv2D(filters=self.filters[5], kernel_size=kernel_size, strides=(1,1), padding='same')\n    \n    def call(self, input_img):\n        out_s1 = ReLU()(self.conv1(input_img))\n        out = ReLU()(self.conv2(out_s1))\n        out_s2 = ReLU()(self.conv3(out))\n        out = ReLU()(self.conv4(out_s2))\n        out_s3 = ReLU()(self.conv5(out))\n        out = ReLU()(self.conv6(out_s3))\n        return out, out_s1, out_s2, out_s3","metadata":{"execution":{"iopub.status.busy":"2021-06-04T03:18:51.08946Z","iopub.execute_input":"2021-06-04T03:18:51.089879Z","iopub.status.idle":"2021-06-04T03:18:51.10328Z","shell.execute_reply.started":"2021-06-04T03:18:51.089846Z","shell.execute_reply":"2021-06-04T03:18:51.102423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Mid-Level Features Network","metadata":{}},{"cell_type":"code","source":"class mid_level(tf.keras.Model):\n    def __init__(self, kernel_size=(3,3)):\n        super(mid_level, self).__init__()\n        \n        self.filters = [512, 256]\n        \n        self.conv1 = Conv2D(filters=self.filters[0], kernel_size=kernel_size, strides=(1,1), padding='same')\n        self.conv2 = Conv2D(filters=self.filters[1], kernel_size=kernel_size, strides=(1,1), padding='same')\n    \n    def call(self, input_img):\n        out = ReLU()(self.conv1(input_img))\n        out = ReLU()(self.conv2(out))\n        return out","metadata":{"execution":{"iopub.status.busy":"2021-06-04T03:18:51.104627Z","iopub.execute_input":"2021-06-04T03:18:51.105246Z","iopub.status.idle":"2021-06-04T03:18:51.113184Z","shell.execute_reply.started":"2021-06-04T03:18:51.105211Z","shell.execute_reply":"2021-06-04T03:18:51.11218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Global-Level Features Network","metadata":{}},{"cell_type":"code","source":"class global_level(tf.keras.Model):\n    def __init__(self, kernel_size=(3,3)):\n        super(global_level, self).__init__()\n        \n        # DEFAULT\n#         self.filters = [512, 512, 512, 512, 1024, 512, 256]\n        \n#         self.conv1 = Conv2D(filters=self.filters[0], kernel_size=kernel_size, strides=(2,2), padding='same')\n#         self.conv2 = Conv2D(filters=self.filters[1], kernel_size=kernel_size, strides=(1,1), padding='same')\n#         self.conv3 = Conv2D(filters=self.filters[2], kernel_size=kernel_size, strides=(2,2), padding='same')\n#         self.conv4 = Conv2D(filters=self.filters[3], kernel_size=kernel_size, strides=(1,1), padding='same')\n#         self.fc1 = Dense(units=self.filters[4])\n#         self.fc2 = Dense(units=self.filters[5])\n#         self.fc3 = Dense(units=self.filters[6])\n\n        # VGG MANUAL\n#         self.filters = [64, 64, 128, 128, 256, 256, 256, 512, 512, 512, 1024, 512, 256]\n    \n#         self.conv1 = Conv2D(filters=self.filters[0], kernel_size=kernel_size, padding='same')\n#         self.conv2 = Conv2D(filters=self.filters[1], kernel_size=kernel_size, padding='same')\n#         self.mp1 = MaxPool2D(pool_size=(2,2),strides=(2,2))\n#         self.conv3 = Conv2D(filters=self.filters[2], kernel_size=kernel_size, padding='same')\n#         self.conv4 = Conv2D(filters=self.filters[3], kernel_size=kernel_size, padding='same')\n#         self.mp2 = MaxPool2D(pool_size=(2,2),strides=(2,2))\n#         self.conv5 = Conv2D(filters=self.filters[4], kernel_size=kernel_size, padding='same')\n#         self.conv6 = Conv2D(filters=self.filters[5], kernel_size=kernel_size, padding='same')\n#         self.conv7 = Conv2D(filters=self.filters[6], kernel_size=kernel_size, padding='same')\n#         self.mp3 = MaxPool2D(pool_size=(2,2),strides=(2,2))\n#         self.conv8 = Conv2D(filters=self.filters[7], kernel_size=kernel_size, padding='same')\n#         self.conv9 = Conv2D(filters=self.filters[8], kernel_size=kernel_size, padding='same')\n#         self.conv10 = Conv2D(filters=self.filters[9], kernel_size=kernel_size, padding='same')\n#         self.mp4 = MaxPool2D(pool_size=(2,2),strides=(2,2))\n        \n#         self.fc1 = Dense(units=self.filters[10])\n#         self.fc2 = Dense(units=self.filters[11])\n#         self.fc3 = Dense(units=self.filters[12])\n        \n        # NASNETLARGE\n        self.filters = [1024, 512, 256]\n        self.nasnet = Xception(\n            input_shape=(256,256,3),\n            include_top=False,\n            weights=\"imagenet\",\n        )\n        self.nasnet.trainable = True\n        self.fc1 = Dense(units=self.filters[0])\n        self.fc2 = Dense(units=self.filters[1])\n        self.fc3 = Dense(units=self.filters[2])\n    \n    def call(self, input_img):\n        # DEFAULT\n#         out = ReLU()(self.conv1(input_img))\n#         out = ReLU()(self.conv2(out))\n#         out = ReLU()(self.conv3(out))\n#         out = ReLU()(self.conv4(out))\n#         out = Flatten()(out)\n#         out = ReLU()(self.fc1(out))\n#         out = ReLU()(self.fc2(out))\n#         out = ReLU()(self.fc3(out))\n\n        # VGG MANUAL\n#         out = ReLU()(self.conv1(input_img))\n#         out = ReLU()(self.conv2(out))\n#         out = self.mp1(out)\n#         out = ReLU()(self.conv3(out))\n#         out = ReLU()(self.conv4(out))\n#         out = self.mp1(out)\n#         out = ReLU()(self.conv5(out))\n#         out = ReLU()(self.conv6(out))\n#         out = ReLU()(self.conv7(out))\n#         out = self.mp1(out)\n#         out = ReLU()(self.conv8(out))\n#         out = ReLU()(self.conv9(out))\n#         out = ReLU()(self.conv10(out))\n#         out = self.mp1(out)\n#         out = Flatten()(out)\n#         out = ReLU()(self.fc1(out))\n#         out = ReLU()(self.fc2(out))\n#         out = ReLU()(self.fc3(out))\n\n        # NASNET LARGE\n        out = tf.repeat(input_img, repeats=[3], axis=3)\n        out = self.nasnet(out)\n        out = Flatten()(out)\n        out = ReLU()(self.fc1(out))\n        out = ReLU()(self.fc2(out))\n        out = ReLU()(self.fc3(out))\n        \n        return out","metadata":{"execution":{"iopub.status.busy":"2021-06-04T03:18:51.115104Z","iopub.execute_input":"2021-06-04T03:18:51.115546Z","iopub.status.idle":"2021-06-04T03:18:51.126932Z","shell.execute_reply.started":"2021-06-04T03:18:51.115491Z","shell.execute_reply":"2021-06-04T03:18:51.126066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Fusion Layer","metadata":{}},{"cell_type":"code","source":"class Fusion(tf.keras.layers.Layer):\n    def __init__(self):\n        super(Fusion, self).__init__()\n    \n    def call(self, inputs):\n        input_img = inputs[0]\n        global_feature = inputs[1]\n        # print('global_feature:', global_feature.shape)\n        repeat = tf.expand_dims(global_feature, axis=1)\n        repeat = tf.repeat(repeat, repeats=[input_img.shape[1]], axis=1)\n        repeat = tf.expand_dims(repeat, axis=2)\n        repeat = tf.repeat(repeat, repeats=[input_img.shape[1]], axis=2)\n        concat = tf.concat([input_img, repeat], axis=3)\n        return concat","metadata":{"execution":{"iopub.status.busy":"2021-06-04T03:18:51.128384Z","iopub.execute_input":"2021-06-04T03:18:51.128928Z","iopub.status.idle":"2021-06-04T03:18:51.138926Z","shell.execute_reply.started":"2021-06-04T03:18:51.128846Z","shell.execute_reply":"2021-06-04T03:18:51.137909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Colorization Network","metadata":{}},{"cell_type":"code","source":"class colorization(tf.keras.Model):\n    def __init__(self, kernel_size=(3,3), skip=True):\n        super(colorization, self).__init__()\n        \n        self.filters = [256, 128, 64, 64, 32, 2]\n        self.skip = skip\n        \n        self.fusion = Fusion()\n        self.conv1 = Conv2D(filters=self.filters[0], kernel_size=kernel_size, strides=(1,1), padding='same')\n        self.upsm1 = UpSampling2D(size=(2,2))\n        self.conv2 = Conv2D(filters=self.filters[1], kernel_size=kernel_size, strides=(1,1), padding='same')\n        self.conv3 = Conv2D(filters=self.filters[2], kernel_size=kernel_size, strides=(1,1), padding='same')\n        self.conv4 = Conv2D(filters=self.filters[3], kernel_size=kernel_size, strides=(1,1), padding='same')\n        self.upsm2 = UpSampling2D(size=(2,2))\n        self.conv5 = Conv2D(filters=self.filters[4], kernel_size=kernel_size, strides=(1,1), padding='same')\n        self.conv6 = Conv2D(filters=self.filters[5], kernel_size=kernel_size, strides=(1,1), padding='same', activation='sigmoid')\n    \n    def call(self, inputs):\n        input_img = inputs[0]\n        global_feature = inputs[1]\n        skip_1 = inputs[2]\n        skip_2 = inputs[3]\n        skip_3 = inputs[4]\n        \n        out = self.fusion([input_img, global_feature])\n        out = ReLU()(self.conv1(out))\n        if self.skip:\n            out = Add()([out, skip_3])\n        \n        out = ReLU()(self.upsm1(out))\n        out = ReLU()(self.conv2(out))\n        \n        if self.skip:\n            out = Add()([out, skip_2])\n        out = ReLU()(self.conv3(out))\n        out = ReLU()(self.conv4(out))\n        \n        out = ReLU()(self.upsm2(out))\n        # if self.skip:\n        #     out = Add()([out, skip_1])\n        out = ReLU()(self.conv5(out))\n        out = self.conv6(out)\n        return out","metadata":{"execution":{"iopub.status.busy":"2021-06-04T03:18:51.140149Z","iopub.execute_input":"2021-06-04T03:18:51.140544Z","iopub.status.idle":"2021-06-04T03:18:51.155347Z","shell.execute_reply.started":"2021-06-04T03:18:51.140487Z","shell.execute_reply":"2021-06-04T03:18:51.154247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Final Model","metadata":{}},{"cell_type":"code","source":"class Colnet(tf.keras.Model):\n    def __init__(self, kernel_size=(3,3), skip=True):\n        super(Colnet, self).__init__()\n        \n        self.low_level = low_level(kernel_size=kernel_size)\n        self.mid_level = mid_level(kernel_size=kernel_size)\n        self.global_level = global_level(kernel_size=kernel_size)\n#         self.global_level = InceptionResNetV2(\n#                                 include_top=False,\n#                                 weights=\"imagenet\",\n#                                 input_shape=(256,256,3),\n#                             )\n        self.global_level.trainable = False\n        self.colorization = colorization(kernel_size=kernel_size, skip=skip)\n    \n    def call(self, input_img):\n        low, skip_1, skip_2, skip_3 = self.low_level(input_img)\n        mid = self.mid_level(low)\n        glo = self.global_level(input_img)\n        # glo = tf.image.grayscale_to_rgb(input_img)\n        # glo = self.global_level(glo)\n        col = self.colorization([mid, glo, skip_1, skip_2, skip_3])\n        out = UpSampling2D(size=(2,2))(col)\n        return out","metadata":{"execution":{"iopub.status.busy":"2021-06-04T03:18:51.156896Z","iopub.execute_input":"2021-06-04T03:18:51.157318Z","iopub.status.idle":"2021-06-04T03:18:51.167649Z","shell.execute_reply.started":"2021-06-04T03:18:51.157267Z","shell.execute_reply":"2021-06-04T03:18:51.166822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Functions","metadata":{}},{"cell_type":"code","source":"from skimage.metrics import structural_similarity as ssim\nfrom skimage.metrics import peak_signal_noise_ratio as psnr\nfrom skimage.metrics import mean_squared_error\nimport numpy as np\nfrom math import log\n\ndef convert_ycbcr2rgb(y, cbcr):\n    y = y * 255.0\n    cbcr = cbcr * 255.0\n    img = np.concatenate([y, cbcr], axis=2)\n    img = ycbcr2rgb(img)\n    return img\n\ndef show_img(step, y_img, cbcr_pred ,cbcr_img, skip):\n    # img = tf.concat([y_img, cbcr_img], axis=3)\n    # img = tf.map_fn(fn=lambda img: ycbcr2rgb(img*255.0), elems=img)\n    \n    imgs = []\n    for i, (y, cbcr_p, cbcr_o) in enumerate(zip(y_img, cbcr_pred, cbcr_img)):\n        #img_orig = convert_ycbcr2rgb(y, cbcr_o)\n        img_pred = convert_ycbcr2rgb(y, cbcr_p)\n        #img_merg = np.concatenate([img_orig, img_pred], axis=1)\n        imgs.append(img_pred)\n        if i == 15:\n            break\n    imgs = np.array(imgs).reshape(4, 4, 256, 256, 3).swapaxes(1, 2).reshape(4*256, 4*256, 3)\n    # y_img = y_img[0] * 255.0\n    # cbcr_img = cbcr_img[0] * 255.0\n\n    # img = np.concatenate([y_img, cbcr_img], axis=2)\n    # img = ycbcr2rgb(img)\n\n    imgs = tf.keras.preprocessing.image.array_to_img(imgs)\n    if skip:\n        name = \"generated_img_skip\" + str(step) + \".png\"\n    else:\n        name = \"generated_img\" + str(step) + \".png\"\n    imgs.save(os.path.join('', name))\n    display(Image(name))\n\ndef avgpsnr(psnrs):\n    return log((np.full(len(psnrs), 10) ** (np.array(psnrs) / 10)).sum() / len(psnrs), 10) * 10\n\ndef img_metrics(epoch, y_img, cbcr_pred ,cbcr_img):\n    imgs_metrics = []\n    total_mse = 0\n    total_ssim = 0\n    total_psnr = []\n    \n    for i, (y, cbcr_p, cbcr_o) in enumerate(zip(y_img, cbcr_pred, cbcr_img)):\n        img_orig = convert_ycbcr2rgb(y, cbcr_o)\n        img_pred = convert_ycbcr2rgb(y, cbcr_p)\n        \n        curr_mse = mean_squared_error(img_orig, img_pred)\n        curr_ssim = ssim(img_orig, img_pred, data_range=img_pred.max() - img_pred.min(), multichannel=True)\n        curr_psnr = psnr(img_orig, img_pred, data_range=img_pred.max() - img_pred.min())\n        \n        total_mse += curr_mse\n        total_ssim += curr_ssim\n        total_psnr.append(curr_psnr)\n        \n        metric = {\n            'mse': curr_mse,\n            'ssim': curr_ssim,\n            'psnr': curr_psnr\n        }\n        \n        imgs_metrics.append(img_pred)\n    \n    avg_mse = total_mse / len(y_img)\n    avg_ssim = total_ssim / len(y_img)\n    avg_psnr = avgpsnr(total_psnr)\n    \n    return imgs_metrics, {'mse': avg_mse, 'ssim': avg_ssim, 'psnr': avg_psnr}","metadata":{"execution":{"iopub.status.busy":"2021-06-04T03:18:51.168933Z","iopub.execute_input":"2021-06-04T03:18:51.169355Z","iopub.status.idle":"2021-06-04T03:18:51.423014Z","shell.execute_reply.started":"2021-06-04T03:18:51.169319Z","shell.execute_reply":"2021-06-04T03:18:51.422313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Training","metadata":{}},{"cell_type":"code","source":"def train_step(real_image, model):\n    ycbcr = tf.map_fn(fn=lambda img: rgb2ycbcr(img) / 255.0, elems=real_image)\n    y_img = tf.cast(tf.expand_dims(ycbcr[:,:,:,0], axis=3), tf.float32)\n    cbcr_img = tf.cast(ycbcr[:,:,:,1:], tf.float32)\n    \n    with tf.GradientTape() as tape:\n        logits = model(y_img)\n        loss = tf.keras.losses.MSE(cbcr_img, logits)\n    grads = tape.gradient(loss, model.trainable_weights)\n    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n    return loss, y_img, logits, cbcr_img\n\ndef train(epochs, dataset, model, skip):\n    metrics = []\n    metrics_detailed = []\n    for epoch in range(epochs):\n        start_epoch = time.time()\n        for step, img_batch in enumerate(dataset):\n            start_step = time.time()\n            loss, y_img, cbcr_pred, cbcr_img = train_step(img_batch, model)\n            # print('step: %d - took: %2.f seconds' % (step, time.time()-start_step))\n            # if step >= step_len:\n            #     break\n        \n        if epoch % 2 == 0:\n            metrics_detailed_single, metrics_single = img_metrics(epoch, y_img, cbcr_pred, cbcr_img)\n            print('epoch: %d - took: %2.f seconds | mse: %.4f | ssim: %.4f | psnr: %.4f' % (epoch, time.time()-start_epoch, metrics_single['mse'], metrics_single['ssim'], metrics_single['psnr']))\n            show_img(epoch, y_img, cbcr_pred, cbcr_img, skip)\n            metrics_detailed.append(metrics_detailed_single)\n            metrics.append(metrics_single)\n        \n        if skip:\n            model.save('./model-skip', save_format='tf')\n        else:\n            model.save('./model', save_format='tf')\n            \n    return metrics, metrics_detailed","metadata":{"execution":{"iopub.status.busy":"2021-06-04T03:18:51.424114Z","iopub.execute_input":"2021-06-04T03:18:51.424426Z","iopub.status.idle":"2021-06-04T03:18:51.435252Z","shell.execute_reply.started":"2021-06-04T03:18:51.424394Z","shell.execute_reply":"2021-06-04T03:18:51.434282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"colnet = Colnet(skip=False)\ncolnet_skip = Colnet(skip=True)\noptimizer = tf.keras.optimizers.Adadelta()\nepochs = 150","metadata":{"execution":{"iopub.status.busy":"2021-06-04T03:18:51.437034Z","iopub.execute_input":"2021-06-04T03:18:51.437671Z","iopub.status.idle":"2021-06-04T03:18:54.852117Z","shell.execute_reply.started":"2021-06-04T03:18:51.437635Z","shell.execute_reply":"2021-06-04T03:18:54.851216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"metrics_skip, metrics_detailed_skip = train(epochs, dataset, colnet_skip, True)","metadata":{"execution":{"iopub.status.busy":"2021-06-04T03:18:54.853491Z","iopub.execute_input":"2021-06-04T03:18:54.853903Z","iopub.status.idle":"2021-06-04T03:21:15.792096Z","shell.execute_reply.started":"2021-06-04T03:18:54.853864Z","shell.execute_reply":"2021-06-04T03:21:15.789315Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# metrics, metrics_detailed = train(epochs, dataset, colnet, False)","metadata":{"execution":{"iopub.status.busy":"2021-06-04T03:21:15.793301Z","iopub.status.idle":"2021-06-04T03:21:15.794107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\nout_path = './'\n\n# df_metrics = pd.DataFrame(metrics)\ndf_metrics_skip = pd.DataFrame(metrics_skip)\n\n# df_metrics_detailed = pd.DataFrame(metrics_detailed)\ndf_metrics_detailed_skip = pd.DataFrame(metrics_detailed_skip)\n\n# df_metrics.to_csv(out_path + 'metrics.csv', index=False)\ndf_metrics_skip.to_csv(out_path + 'metrics_skip.csv', index=False)\n# df_metrics_detailed.to_csv(out_path + 'metrics_detailed.csv', index=False)\ndf_metrics_detailed_skip.to_csv(out_path + 'metrics_detailed_skip.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-06-04T03:21:15.795296Z","iopub.status.idle":"2021-06-04T03:21:15.796004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def test(model)","metadata":{"execution":{"iopub.status.busy":"2021-06-04T03:21:15.797063Z","iopub.status.idle":"2021-06-04T03:21:15.7978Z"},"trusted":true},"execution_count":null,"outputs":[]}]}