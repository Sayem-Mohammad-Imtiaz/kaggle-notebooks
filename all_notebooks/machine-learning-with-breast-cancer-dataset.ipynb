{"cells":[{"metadata":{"_uuid":"725992e3fb1debbef7a1cc6524ac3b3eeb25acfd"},"cell_type":"markdown","source":"# Cihan Yatbaz\n###  02 / 11 / 2018\n\n\n\n1.  [Introduction:](#0)\n2. [Exploratory Data Analysis (EDA) :](#1)\n3. [Logistic Regression with Plot :](#2)\n    1. [Preparing Dataset :](#3)\n    2.  [Creating Parameters :](#4)\n    3. [Forward and Backward Propagation  :](#5)\n    4. [Updating Parameter :](#6)\n    5. [Prediction Parameter :](#7)\n    6. [ Logistic Regression :](#8)\n4. [Logistec Regression with Sklearn  :](#9)\n5. [CONCLUSION :](#10)"},{"metadata":{"_uuid":"2d83251d9b00e9ff1ab02e3a6b3148a0d8fd7170"},"cell_type":"markdown","source":"<a id=\"0\"></a> <br>\n## 1) Introduction"},{"metadata":{"_uuid":"1059038e3948ff7a668ee556203007c41a465392"},"cell_type":"markdown","source":"We will be working on this kernel Breast Cancer data. We'll introduce 80% of the cancer cells we have, and we will try to predict the remaining 20%. We will learn it whether they are 'benign' or 'malignant'. So let's start."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.linear_model import LinearRegression\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# Prepare to data\ndata = pd.read_csv(\"../input/breast-cancer.csv\")\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4053797b9ed2554de62c56b173099563dc2f22e1"},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b4a3629854b44d16608d0a043a77bda80c716f19"},"cell_type":"code","source":"# Let's wipe some columns that we won't use\ndata.drop(['id', 'Unnamed: 32'], axis=1, inplace=True)  #axis=1 tüm sütunu siler\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1e8d6e2744f3f2682af81863f484f1b10fe4e994"},"cell_type":"markdown","source":"<a id=\"1\"></a> <br>\n## 2) Exploratory Data Analysis (EDA)"},{"metadata":{"trusted":true,"_uuid":"2e4f51ed9ff3614e2c0579edfae5c0faf0ac3aa5"},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dfe585cc52969e4bb0fa7a7ef1bf8d16ffcd5e3f"},"cell_type":"code","source":"# Let's take the some columns we'll use for show data means\ndata_mean= data[['diagnosis','radius_mean','texture_mean','perimeter_mean','area_mean',\n                 'smoothness_mean','compactness_mean','concavity_mean','concave points_mean',\n                 'symmetry_mean','fractal_dimension_mean']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"246f855dcd759256f51adc71b389d721aaf0ac8a"},"cell_type":"code","source":"color_list = ['cyan' if i=='M' else 'orange' for i in data_mean.loc[:,'diagnosis']]\npd.plotting.scatter_matrix(data_mean.loc[:, data_mean.columns != 'diagnosis'],\n                           c=color_list,\n                           figsize= [15,15],\n                           diagonal='hist',\n                           alpha=0.5,\n                           s = 200,\n                           marker = '*',\n                           edgecolor= \"black\")\n                                        \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"920a7329e2233e0ce51e77f41cd91238dd7c6991"},"cell_type":"code","source":"# Values of 'Benign' and 'Malignant' cancer cells\nsns.countplot(x=\"diagnosis\", data=data)\ndata.loc[:,'diagnosis'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4a21ff23b689bfadce30fd650b16ecf3ee19f9ad"},"cell_type":"markdown","source":"<a id=\"2\"></a> <br>\n# 3) Logistic Regression with Plot\nWe are organizing the data we will use first."},{"metadata":{"_uuid":"e34960ae5efdd598ceb3425abfc0a5e99120bbfc"},"cell_type":"markdown","source":"<a id=\"3\"></a> <br>\n### A) Preparing Dataset"},{"metadata":{"trusted":true,"_uuid":"ef6d594d8690d654ab44091b9a899d72158d8156"},"cell_type":"code","source":"# Let's convert \"male\" to 1, \"female\" to 0 values\ndata.diagnosis = [ 1 if each == \"M\" else 0 for each in data.diagnosis]\ndata.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ee6032986015282b3793745226a875e28543c957"},"cell_type":"code","source":"# Let's determine the values of y and x axes\ny = data.diagnosis.values\nx_data = data.drop([\"diagnosis\"], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d22425c76735159ee9788a4fa8a82b8d45b2cacc"},"cell_type":"code","source":"# Now we are doing normalization. Because if some of our columns have very high values, they will suppress other columns and do not show much.\n# Formulel : (x- min(x)) / (max(x) - min(x))\nx = (x_data - np.min(x_data)) / (np.max(x_data) - np.min(x_data)).values\nx.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"684e3625f055fa5cd141cc3c1ce648e088b55307"},"cell_type":"code","source":"# Now we reserve 80% of the values as 'train' and 20% as 'test'.\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y, test_size = 0.2, random_state=42)\n\n# Here we will change the location of our samples and features. '(455,30) -> (30,455)' \nx_train = x_train.T   \nx_test = x_test.T\ny_train = y_train.T   \ny_test = y_test.T\n\nprint(\"x_train :\", x_train.shape)\nprint(\"x_test :\", x_test.shape)\nprint(\"y_train :\", y_train.shape)\nprint(\"y_test :\", y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fa9f436df5b7ac13c7644c85045cc4068af974f7"},"cell_type":"markdown","source":"<a id=\"4\"></a> <br>\n### B) Creating Parameters"},{"metadata":{"_uuid":"4b9eff29884921957d0c078b9d172158c7595901"},"cell_type":"markdown","source":"* Parameters are weight and bias.\n* Weights: coefficients of each pixels\n* Bias: intercept\n* z = (w.t)x + b => z equals to (transpose of weights times input x) + bias\n* In an other saying => z = b + px1w1 + px2w2 + ... + px4096*w4096\n* y_head = sigmoid(z)\n* Sigmoid function makes z between zero and one so that is probability."},{"metadata":{"trusted":true,"_uuid":"d0bfa72c66fb80bd2fba244fd3a6abd4f37f5da7"},"cell_type":"code","source":"# Now let's create the parameter and sigmoid function. Videodan nedenini yaz\ndef initialize_weights_and_bias(dimension):\n    w = np.full((dimension,1),0.01)\n    b = 0.0   # It will be float\n    return w,b\n\n# Sigmoid Function\n\n# Let's calculating z\n# z = np.dot(w.T,x_train)+b\ndef sigmoid(z):\n    y_head = 1/(1+np.exp(-z)) # sigmoid functions finding formula\n    return y_head\nsigmoid(0)  # 0 should result in 0.5","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"361d63edfde930406dd5b01eb079278eacac29d1"},"cell_type":"markdown","source":"<a id=\"5\"></a> <br>\n### C) Forward and Backward Propagation\n"},{"metadata":{"trusted":true,"_uuid":"8b77904378a716122583c528f2050fa92d8341f6"},"cell_type":"markdown","source":"Now if our cost will be error. we have to create backward propagation. Therefor let's make a backward propagation."},{"metadata":{"trusted":true,"_uuid":"c1eec62a056eea3ed37655c8da702d2a19cde969"},"cell_type":"code","source":"# In backward propagation we will use y_head that found in forward progation\n# Therefore instead of writing backward propagation method, lets combine forward propagation and backward propagation\n\ndef forward_backward_propagation(w,b,x_train,y_train):\n    \n    # forward propagation\n    z = np.dot(w.T,x_train)+b\n    y_head = sigmoid(z)\n    loss = -y_train*np.log(y_head)-(1-y_train)*np.log(1-y_head)  \n    cost =(np.sum(loss))/x_train.shape[1]         # x_train.shape[1] for scaling\n    \n    # backward propagation\n    derivative_weight = (np.dot(x_train,((y_head-y_train).T)))/x_train.shape[1]\n    derivative_bias = np.sum(y_head-y_train)/x_train.shape[1] \n    gradients = {\"derivative_weight\": derivative_weight,\"derivative_bias\": derivative_bias}\n    \n    return cost,gradients","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1327efa36398715f568aa5b16c47490833643e55"},"cell_type":"markdown","source":"<a id=\"6\"></a> <br>\n### D) Updating Parameter"},{"metadata":{"trusted":true,"_uuid":"2d36584f10fafa670e67b2910bca12920ca0c8b2"},"cell_type":"code","source":"# Now let's apply Updating Parameter\n\ndef update(w, b, x_train, y_train, learning_rate, number_of_iteration):\n    cost_list = []\n    cost_list2 = []\n    index = []\n    # Updating(learning) parameters is number_of_iteration times\n    for i in range(number_of_iteration):\n        # make forward and backward propagation and find cost gradients\n        cost,gradients = forward_backward_propagation(w,b,x_train,y_train)\n        cost_list.append(cost)\n        # lets update\n        w = w - learning_rate * gradients[\"derivative_weight\"]\n        b = b - learning_rate * gradients[\"derivative_bias\"]\n        if i % 10 == 0:\n            cost_list2.append(cost)\n            index.append(i)\n            print (\"Cost after iteration %i: %f\" %(i, cost))\n        \n        # we update(learn) parameters weights and bias\n    parameters = {\"weight\":w, \"bias\":b}\n    plt.plot(index, cost_list2)\n    plt.xticks(index, rotation='vertical')\n    plt.xlabel(\"Number of iteration\")\n    plt.ylabel(\"cost\")\n    plt.show()\n    return parameters, gradients, cost_list\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b6c0d189e9ede862859fd5fc914c4aa85370dfe0"},"cell_type":"markdown","source":"<a id=\"7\"></a> <br>\n### E) Prediction Parameter"},{"metadata":{"trusted":true,"_uuid":"1c38b7d6a1015cfaec770d782e947b86d1e964fd"},"cell_type":"markdown","source":"In prediction step we have x_test as a input and while using it, we make forward prediction. "},{"metadata":{"trusted":true,"_uuid":"14efc23bf3a4ee057a14cc2da439db8a6f39caef"},"cell_type":"code","source":"# Let's create prediction parameter\ndef predict(w,b,x_test):\n    # x_test is an input for forward propagation\n    z = sigmoid(np.dot(w.T,x_test)+b)\n    y_prediction = np.zeros((1,x_test.shape[1]))\n    # if z is bigger than 0.5, our prediction is sign one (y_head=1),\n    # if z is smaller than 0.5, our prediction is sign zero (y_head=0),\n    for i in range(z.shape[1]):\n        if z[0,i]<= 0.5:\n            y_prediction[0,i] = 0\n        else:\n            y_prediction[0,i] = 1\n\n    return y_prediction\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5c5c9758ac0aca55ddb24fcbc16b8f46fba807d1"},"cell_type":"markdown","source":"<a id=\"8\"></a> <br>\n### F) Logistic Regression"},{"metadata":{"trusted":true,"_uuid":"6901dd94914c68c5cf0783bb932463db7c98a92c"},"cell_type":"markdown","source":"Now lets put them all together."},{"metadata":{"trusted":true,"_uuid":"089b8ee8946da1ddafd3aca2a6963ed48398247c"},"cell_type":"code","source":"#Logistic Regression\n\ndef logistic_regression(x_train, y_train, x_test, y_test, learning_rate ,  num_iterations):\n    # initialize\n    dimension =  x_train.shape[0]  # that is 455\n    w,b = initialize_weights_and_bias(dimension)\n    # do not change learning rate\n    parameters, gradients, cost_list = update(w, b, x_train, y_train, learning_rate,num_iterations)\n    \n    y_prediction_test = predict(parameters[\"weight\"],parameters[\"bias\"],x_test)\n\n    # Print train/test Errors\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))\n    \nlogistic_regression(x_train, y_train, x_test, y_test,learning_rate = 0.01, num_iterations = 100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"11afaf17bbeff23aeac4a07b649e793afb3bcb87"},"cell_type":"code","source":"# We can increase the accuracy of the test by playing with learning_rate and num_iterations\nlogistic_regression(x_train, y_train, x_test, y_test,learning_rate = 5, num_iterations = 150)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"18caeae7c6b97942ec5c2299eeea0e868ca78a83"},"cell_type":"markdown","source":"<a id=\"9\"></a> <br>\n# 4) Logistec Regression with Sklearn\nWith the Sklearn library, we can find the result you found above in a much easier way."},{"metadata":{"trusted":true,"_uuid":"55deb48c2ff125ef2fbdf01ab0541063025749b4"},"cell_type":"code","source":"from sklearn import linear_model\nlgrg = linear_model.LogisticRegression(random_state=42, max_iter=150)\n\nprint(\"test accuracy: {} \".format(lgrg.fit(x_train.T, y_train.T).score(x_test.T, y_test.T)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9257f8cdfefbd3bead25eb94f40a430c5f9b4799"},"cell_type":"markdown","source":"<a id=\"10\"></a> <br>\n> # CONCLUSION                                                                                                                                                      \nThank you for your votes and comments                                                                                                                                              \n<br>**If you have any suggest, May you write for me, I will be happy to hear it.**"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}