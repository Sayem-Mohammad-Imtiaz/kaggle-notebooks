{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n%matplotlib inline","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv('../input/telco-customer-churn/WA_Fn-UseC_-Telco-Customer-Churn.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.sample(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#customer id is a waste as it is not req for our prediction so lets drop it\ndata.drop(labels = ['customerID'],axis='columns',inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.dtypes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#as we can se the totalcharges is string and monthly is number so lets \n#get our dataset in a correct dtype\n# pd.to_numeric(data.TotalCharges)\n# this will currently give error as there are somevalue which have space\n#so lets settle that out.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.to_numeric(data.TotalCharges,errors='coerce').isnull()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data[pd.to_numeric(data.TotalCharges,errors='coerce').isnull()]","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_df = data[data.TotalCharges!=' '] #lets drop that columns.\nnew_df.shape #total 11 rows are deleted. ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#now lets do the same thing\nnew_df.TotalCharges = pd.to_numeric(new_df.TotalCharges)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" new_df.dtypes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Now lets do some quick visualization.\ntenure_churn_no = new_df[new_df.Churn=='No'].tenure\ntenure_churn_yes = new_df[new_df.Churn=='Yes'].tenure","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.xlabel('Number of Customer')\nplt.ylabel('Customer Churn Prediction Visualization')\n\nplt.hist([tenure_churn_yes,tenure_churn_no],color=['green','red'],label=['Churn=Yes','Churn=No'])\nplt.legend()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#lets see for monthly charges\nmc_churn_no = new_df[new_df.Churn=='No'].MonthlyCharges\nmc_churn_yes = new_df[new_df.Churn=='Yes'].MonthlyCharges\n\nplt.xlabel('Monthly Charges')\nplt.ylabel('Number of Customers')\nplt.title('Customer Churn Prediction Visualization')\n\nblood_sugar_men = [113,85,90,150,149,88,93,115,135,80,77,82,129]\nblood_sugar_women =[67,98,89,120,133,150,84,69,89,79,120,112,100] \n\nplt.hist([mc_churn_yes,mc_churn_no],rwidth=0.95,color=['green','red'],label=['Churn=Yes','Churn=No'])\nplt.legend()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def print_unique_col_value(data):\n    for column in data:\n        if data[column].dtypes=='object':\n            print(f'{column}:{data[column].unique()}')\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print_unique_col_value(new_df) #so these are our catagorical columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Lets clean our dataset\nnew_df.replace('No internet service','No',inplace=True)\nnew_df.replace('No phone service','No',inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print_unique_col_value(new_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Now lets replace yes and no with maybe 1 and 0\nyes_no_columns = ['Partner','Dependents','PhoneService','MultipleLines',\n                 'OnlineSecurity','OnlineBackup','DeviceProtection','TechSupport',\n                 'StreamingTV','StreamingMovies','PaperlessBilling','Churn'] #all cloumn with yes and no\n\nfor col in yes_no_columns:\n    new_df[col].replace({'Yes':1,'No':0},inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for col in new_df:\n    print(f'{col}:{new_df[col].unique()}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_df['gender'].replace({'Female':1,'Male':0},inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_df['gender'].unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" new_df1 = pd.get_dummies(data=new_df,columns=['InternetService','Contract','PaymentMethod'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_df1.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_df1.sample(4) #Now our data look quite.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Now lets see the datatype... as we can see all are number which is quite great\nnew_df1.dtypes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Now lets scale our data.\n#so the columns to be scal are tenure,MonthlyCharges,TotalCharges\n#as they are nit in range 0-1\n#We will use min-max or normalization.\ncols_to_scale = ['tenure','MonthlyCharges','TotalCharges']\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nnew_df1[cols_to_scale] = scaler.fit_transform(new_df1[cols_to_scale])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Now our dataframe is scale and used for prediction\n#We are done with preprocessing,\nfor col in new_df1:\n    print(f'{col}:{new_df1[col].unique()}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def neural_net(x_train,y_train,x_test,y_test):\n    import tensorflow as tf\n    from functools import partial\n    from tensorflow import keras\n    from sklearn.metrics import confusion_matrix,classification_report\n\n\n    model = keras.Sequential(\n        [\n            keras.layers.Dense(20,input_shape=(26,),activation='relu'),\n            keras.layers.Dropout(0.3),\n            keras.layers.Dense(10,activation='relu'),\n            keras.layers.Dropout(0.3),\n            keras.layers.Dense(5,activation='relu'),\n            keras.layers.Dropout(0.3),\n            keras.layers.Dense(1,activation='sigmoid'),\n        ]\n    )\n\n    model.compile(optimizer ='adam',\n                 loss='binary_crossentropy',\n                 metrics=['accuracy'])\n\n    # model.fit(x_train,y_train,epochs=5)\n    model.fit(x_train,y_train,epochs=50,batch_size=8)\n\n    model.evaluate(x_test,y_test)\n    y_pred = model.predict(x_test)\n    y_pred_actual = []\n    for ele in y_pred:\n        if ele > 0.5:\n            y_pred_actual.append(1)\n        else :\n            y_pred_actual.append(0)\n\n    \n    print(\"Classification Reports is:\\n\",classification_report(y_test,y_pred_actual))\n    \n    return y_pred_actual","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX = new_df1.drop('Churn',axis=1)\ny = new_df1['Churn']\n\nx_train,x_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=15,stratify=y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## As We can see our dataset is imbalanced. Now see the difference between training of unbalanced dataset vs balanced dataset","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"## Training of Unbalanced Dataset :-","metadata":{}},{"cell_type":"code","source":"y_preds = neural_net(x_train,y_train,x_test,y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### lets look at confusion matrix","metadata":{}},{"cell_type":"code","source":"\nimport seaborn as sns\nimport tensorflow as tf\ncm = tf.math.confusion_matrix(labels=y_test,predictions=y_preds)\n\nplt.figure(figsize=(9,7))\nsns.heatmap(cm,annot=True,fmt='d')\nplt.xlabel('Predicted')\nplt.ylabel('Actual')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## As we can see the f1-score for 1 class is .62 lets Balance our dataset and see the difference","metadata":{}},{"cell_type":"markdown","source":"## We will use SMOTE (Over sampling by producing syntetic samples) \n**One can refer other methods of balancing :- https://towardsdatascience.com/having-an-imbalanced-dataset-here-is-how-you-can-solve-it-1640568947eb**","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"from imblearn.over_sampling import SMOTE\nsmote = SMOTE(sampling_strategy ='minority')\nx_sm, y_sm = smote.fit_resample(X,y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_sm.value_counts() #Now our dataset is balanced.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train,x_test,y_train,y_test = train_test_split(x_sm,y_sm,test_size=0.2,random_state=15,stratify=y_sm)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train.value_counts() #balanced train dataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test.value_counts() #balanced test dataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_preds = neural_net(x_train,y_train,x_test,y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Lets see confusion matrix","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\nimport tensorflow as tf\ncm = tf.math.confusion_matrix(labels=y_test,predictions=y_preds)\n\nplt.figure(figsize=(9,7))\nsns.heatmap(cm,annot=True,fmt='d')\nplt.xlabel('Predicted')\nplt.ylabel('Actual')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Conclusion :-\n**Balancing the dataset not only helps us improve f1-score of classes in classification problem but also helps improve accuracy.So tackling a unbalanced datset is must.<br>**\n<h6>*Note:- the epochs of neural network is 50 maybe more epochs may result in better result*</h6>","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}