{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Regression: Ridge Regression (interpretation)\n## Coursera University of Washington ML specialization Regression course week 5 assignment"},{"metadata":{},"cell_type":"markdown","source":"In this notebook, we will run ridge regression multiple times with different L2 penalties to see which one produces the best fit. We will revisit the example of polynomial regression as a means to see the effect of L2 regularization. In particular, we will:\n* Use a scikit-learn to run polynomial regression\n* Use matplotlib to visualize polynomial regressions\n* Use a scikit learn to run polynomial regression, this time with L2 penalty\n* Use matplotlib to visualize polynomial regressions under L2 regularization\n* Choose best L2 penalty using cross-validation.\n* Assess the final fit using test data.\n\nWe will continue to use the House data from previous notebooks.  (In the next programming assignment for this module, you will implement your own ridge regression learning algorithm using gradient descent.)"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load the data set to a data frame"},{"metadata":{"trusted":true},"cell_type":"code","source":"# dictionary with dataset column names and their corresponding data types\ndtype_dict = {'bathrooms':float, 'waterfront':int, 'sqft_above':int, 'sqft_living15':float, 'grade':int, \n              'yr_renovated':int, 'price':float, 'bedrooms':float, 'zipcode':str, 'long':float, 'sqft_lot15':float, \n              'sqft_living':float, 'floors':float, 'condition':int, 'lat':float, 'date':str, 'sqft_basement':int, \n              'yr_built':int, 'id':str, 'sqft_lot':int, 'view':int}\n\nos.chdir('/kaggle/input/polynomialregression')\nsales_df = pd.read_csv('kc_house_data.csv', dtype = dtype_dict)\nsales_df.sort_values(by=['sqft_living', 'price'], inplace=True)\nsales_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The target variable that we want to predict is price and the input features that we consider is just sqft_living"},{"metadata":{"trusted":true},"cell_type":"code","source":"sqft_living = sales_df.loc[:, 'sqft_living'].values.reshape(-1, 1)\nprice = sales_df.loc[:, 'price'].values.reshape(-1, 1)    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Use sklearn polynomial feature which accepts an array ‘feature’ in fit_transform method and a maximal ‘degree’ in constructor and returns an data frame (e.g. SFrame) with the first column equal to ‘feature’ and the remaining columns equal to ‘feature’ to increasing integer powers up to ‘degree’."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import PolynomialFeatures\n\ndef create_polynomial_features(degree_n, input_features):\n    polynomial_features = PolynomialFeatures(degree=degree_n)\n    return polynomial_features.fit_transform(input_features)    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"poly1_data = create_polynomial_features(1, sqft_living)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Write a function to:\n1. Build an polynomial data set using training_data[‘sqft_living’] as the feature and the current degree\n2. Learn a model on TRAINING data to predict ‘price’ based on your polynomial data set at the current degree\n3. Plot the predicted price on top of the scatter plot of sft-living vs price at the current degree\n4. Print the model statistics (on the training data) viz mean-squared-error aka RSS and r2-score at the current degree"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.linear_model import LinearRegression\n\ndeg_color_map = {1: 'red', 2: 'green', 3: 'blue', 4: 'cyan', 5: 'grey', 6: 'gold', 7: 'lavender', 8: 'lime', 9: 'magenta', 15: 'coral'}\n\ndef regression_degree_n(degree_n, data, regressor):\n    # data : data frame containing both input features and target variable\n    # returns : The coefficient of degree 1 i.e. theta1\n    input_features = data.loc[:, 'sqft_living'].values.reshape(-1, 1)\n    y = data.loc[:, 'price'].values.reshape(-1, 1)    \n    input_features_degree_n = create_polynomial_features(degree_n, input_features)    \n    regressor.fit(input_features_degree_n, y)\n    predicted_y = regressor.predict(input_features_degree_n)        \n    rmse_deg = np.sqrt(mean_squared_error(y, predicted_y))\n    r2_deg = r2_score(y, predicted_y)\n    plt.plot(input_features, predicted_y, color=deg_color_map[degree_n], label='degree {}'.format(degree_n))            \n    print('For model complexity of polynomial degree {}:'.format(degree_n))\n    print('The learned coefficients = {}'.format(regressor.coef_))\n    print('The root mean squared error = {}, r2_score = {} '.format(rmse_deg, r2_deg, degree_n))\n    return regressor.coef_[0][1]\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Produce a scatter plot of the training data (sqft-living vs price) and add the fitted model for polynomial degree 1 and 15"},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndef plot_sqftliving_price(sqft_living, price):\n    fig, ax = plt.subplots(figsize=(12,6))    \n    plt.plot(sqft_living, price, 'o', mfc='cyan', mec='orange')\n    xrange = np.linspace(0, 14000, 15)\n    ax.set_xticks(xrange)\n    plt.xlabel('sqft_living')\n    plt.ylabel('price')    \n\nplot_sqftliving_price(sqft_living, price)\nlinear_regressor = LinearRegression(normalize=True)\nregression_degree_n(1, sales_df, linear_regressor)    \nregression_degree_n(15, sales_df, linear_regressor)    \nplt.legend()\nplt.title('Polynomial regression')    \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Note: When we have so many features and so few data points, the solution can become highly numerically unstable, which can sometimes lead to strange unpredictable results.  Thus, rather than using no regularization, we will introduce a tiny amount of regularization (`l2_penalty=1e-5`) to make the solution numerically stable.  (In lecture, we discussed the fact that regularization can also help with numerical stability, and here we are seeing a practical example.)\n\nWith the L2 penalty specified above, fit the model and print out the learned weights."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import Ridge\n\nl2_small_penalty = 1e-5\nridge_regressor = Ridge(alpha=l2_small_penalty, normalize=True)\nplot_sqftliving_price(sqft_living, price)\nregression_degree_n(15, sales_df, ridge_regressor)    \nplt.legend()\nplt.title('Polynomial regression of degree 15 with l2 penalty')    \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Note the decrease in the value of learned coefficients with a l2 penalty applied"},{"metadata":{},"cell_type":"markdown","source":"# Observe overfitting"},{"metadata":{},"cell_type":"markdown","source":"Recall from Week 3 that the polynomial fit of degree 15 changed wildly whenever the data changed. In particular, when we split the sales data into four subsets and fit the model of degree 15, the result came out to be very different for each subset. The model had a *high variance*. We will see in a moment that ridge regression reduces such variance. But first, we must reproduce the experiment we did in Week 3."},{"metadata":{},"cell_type":"markdown","source":" Please download the provided csv files for each subset and load them with the given list of types:. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# dtype_dict same as above\nset_1 = pd.read_csv('wk3_kc_house_set_1_data.csv', dtype=dtype_dict)\nset_2 = pd.read_csv('wk3_kc_house_set_2_data.csv', dtype=dtype_dict)\nset_3 = pd.read_csv('wk3_kc_house_set_3_data.csv', dtype=dtype_dict)\nset_4 = pd.read_csv('wk3_kc_house_set_4_data.csv', dtype=dtype_dict)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next, fit a 15th degree polynomial on `set_1`, `set_2`, `set_3`, and `set_4`, using 'sqft_living' to predict prices. Print the weights and make a plot of the resulting model.\n\nThis time use l2_small_penalty=1e-9"},{"metadata":{"trusted":true},"cell_type":"code","source":"l2_smaller_penalty=1e-9\ncoeff_deg1_list = []\nridge_regressor_smaller = Ridge(alpha=l2_smaller_penalty, normalize=True)\n\ndef fit_and_plot_model(data, regressor, coef_deg1, label):\n    plot_sqftliving_price(sqft_living, price)\n    coef_deg1_1 = regression_degree_n(15, data, regressor)    \n    coef_deg1.append(coef_deg1_1)    \n    plt.legend()\n    plt.title('Polynomial regression of degree 15 with l2 penalty ({})'.format(label))    \n    plt.show()\n    \nfit_and_plot_model(set_1, ridge_regressor_smaller, coeff_deg1_list, 'set 1')    ","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"fit_and_plot_model(set_2, ridge_regressor_smaller, coeff_deg1_list, 'set 2')    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fit_and_plot_model(set_3, ridge_regressor_smaller, coeff_deg1_list, 'set 3')    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fit_and_plot_model(set_4, ridge_regressor_smaller, coeff_deg1_list, 'set 4')    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The four curves should differ from one another a lot, as should the coefficients you learned.\n\n***QUIZ QUESTION:  For the models learned in each of these training sets, what are the smallest and largest values you learned for the coefficient of feature `power_1`?***  (For the purpose of answering this question, negative numbers are considered \"smaller\" than positive numbers. So -5 is smaller than -3, and -3 is smaller than 5 and so forth.)"},{"metadata":{"trusted":true},"cell_type":"code","source":"sorted_coef_deg1 = sorted(coeff_deg1_list)\nprint(sorted_coef_deg1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Ridge regression comes to rescue"},{"metadata":{},"cell_type":"markdown","source":"Generally, whenever we see weights change so much in response to change in data, we believe the variance of our estimate to be large. Ridge regression aims to address this issue by penalizing \"large\" weights. (Weights of `model15` looked quite small, but they are not that small because 'sqft_living' input is in the order of thousands.)\n\nWith the argument `l2_penalty=1e5`, fit a 15th-order polynomial model on `set_1`, `set_2`, `set_3`, and `set_4`. Other than the change in the `l2_penalty` parameter, the code should be the same as the experiment above. "},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"l2_big_penalty = 1.23e2\ncoeff_deg1_list_2 = []\nridge_regressor_big = Ridge(alpha=l2_big_penalty, normalize=True)\n","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"fit_and_plot_model(set_1, ridge_regressor_big, coeff_deg1_list_2, 'set 1')    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fit_and_plot_model(set_2, ridge_regressor_big, coeff_deg1_list_2, 'set 2')    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fit_and_plot_model(set_3, ridge_regressor_big, coeff_deg1_list_2, 'set 3')   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fit_and_plot_model(set_4, ridge_regressor_big, coeff_deg1_list_2, 'set 4')   ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"These curves should vary a lot less, now that you applied a high degree of regularization.\n\n***QUIZ QUESTION:  For the models learned with the high level of regularization in each of these training sets, what are the smallest and largest values you learned for the coefficient of feature `power_1`?*** (For the purpose of answering this question, negative numbers are considered \"smaller\" than positive numbers. So -5 is smaller than -3, and -3 is smaller than 5 and so forth.)"},{"metadata":{"trusted":true},"cell_type":"code","source":"sorted_coef_deg1_2 = sorted(coeff_deg1_list_2)\nprint(sorted_coef_deg1_2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Selecting an L2 penalty via cross-validation"},{"metadata":{},"cell_type":"markdown","source":"Just like the polynomial degree, the L2 penalty is a \"magic\" parameter we need to select. We could use the validation set approach as we did in the last module, but that approach has a major disadvantage: it leaves fewer observations available for training. **Cross-validation** seeks to overcome this issue by using all of the training set in a smart way.\n\nWe will implement a kind of cross-validation called **k-fold cross-validation**. The method gets its name because it involves dividing the training set into k segments of roughtly equal size. Similar to the validation set method, we measure the validation error with one of the segments designated as the validation set. The major difference is that we repeat the process k times as follows:\n\nSet aside segment 0 as the validation set, and fit a model on rest of data, and evalutate it on this validation set<br>\nSet aside segment 1 as the validation set, and fit a model on rest of data, and evalutate it on this validation set<br>\n...<br>\nSet aside segment k-1 as the validation set, and fit a model on rest of data, and evalutate it on this validation set\n\nAfter this process, we compute the average of the k validation errors, and use it as an estimate of the generalization error. Notice that  all observations are used for both training and validation, as we iterate over segments of data. \n\nTo estimate the generalization error well, it is crucial to shuffle the training data before dividing them into segments. For the purpose of this assignment, let us download the csv file containing pre-shuffled rows of training and validation sets combined: wk3_kc_house_train_valid_shuffled.csv. In practice, you would shuffle the rows with a dynamically determined random seed."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_valid_shuffled = pd.read_csv('wk3_kc_house_train_valid_shuffled.csv', dtype=dtype_dict)\ntest = pd.read_csv('wk3_kc_house_test_data.csv', dtype=dtype_dict)\nprint(len(train_valid_shuffled))\nprint(len(test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Once the data is shuffled, we divide it into equal segments. Each segment should receive `n/k` elements, where `n` is the number of observations in the training set and `k` is the number of segments. Since the segment 0 starts at index 0 and contains `n/k` elements, it ends at index `(n/k)-1`. The segment 1 starts where the segment 0 left off, at index `(n/k)`. With `n/k` elements, the segment 1 ends at index `(n*2/k)-1`. Continuing in this fashion, we deduce that the segment `i` starts at index `(n*i/k)` and ends at `(n*(i+1)/k)-1`."},{"metadata":{},"cell_type":"markdown","source":"With this pattern in mind, we write a short loop that prints the starting and ending indices of each segment, just to make sure you are getting the splits right."},{"metadata":{"trusted":true},"cell_type":"code","source":"n = len(train_valid_shuffled)\nk = 10 # 10-fold cross-validation\n\nfor i in range(k):\n    start = (n*i)/k\n    end = (n*(i+1))/k-1\n    print(i, (round(start, 0), round(end, 0)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us familiarize ourselves with array slicing with SFrame. To extract a continuous slice from an SFrame, use colon in square brackets. For instance, the following cell extracts rows 0 to 9 of `train_valid_shuffled`. Notice that the first index (0) is included in the slice but the last index (10) is omitted."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_valid_shuffled[0:10] # rows 0 to 9","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let us extract individual segments with array slicing. Consider the scenario where we group the houses in the `train_valid_shuffled` dataframe into k=10 segments of roughly equal size, with starting and ending indices computed as above. Extract the fourth segment (segment 3) and assign it to a variable called `validation4`."},{"metadata":{"trusted":true},"cell_type":"code","source":"n = len(train_valid_shuffled)\nk = 10\n\ndef get_segment_indices(num_segments, num_elements):\n    k_segments = []\n    for i in range(num_segments):\n        start = (num_elements*i)/num_segments\n        end = (num_elements*(i+1))/num_segments-1\n        k_segments.append((int(round(start,0)), int(round(end,0))))\n    return k_segments\n\nk_segments = get_segment_indices(k, n)\nstart_4 = k_segments[3][0]    \nend_4 = k_segments[3][1]\nvalidation4 = train_valid_shuffled[start_4:end_4]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To verify that we have the right elements extracted, run the following cell, which computes the average price of the fourth segment. When rounded to nearest whole number, the average should be $536,234."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(int(round(validation4['price'].mean(), 0)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After designating one of the k segments as the validation set, we train a model using the rest of the data. To choose the remainder, we slice (0:start) and (end+1:n) of the data and paste them together. SFrame has `append()` method that pastes together two disjoint sets of rows originating from a common dataset. For instance, the following cell pastes together the first and last two rows of the `train_valid_shuffled` dataframe."},{"metadata":{"trusted":true},"cell_type":"code","source":"n = len(train_valid_shuffled)\nfirst_two = train_valid_shuffled[0:2]\nlast_two = train_valid_shuffled[n-2:n]\nprint(first_two.append(last_two))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Extract the remainder of the data after *excluding* fourth segment (segment 3) and assign the subset to `train4`."},{"metadata":{"trusted":true},"cell_type":"code","source":"pre_validation4 = train_valid_shuffled[0: start_4]\npost_validation4 = train_valid_shuffled[end_4+1: n]\ntrain4 = pre_validation4.append(post_validation4)\nprint(len(validation4))\nprint(len(train4))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To verify that we have the right elements extracted, run the following cell, which computes the average price of the data with fourth segment excluded. When rounded to nearest whole number, the average should be $539,450."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(int(round(train4['price'].mean(), 0)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we are ready to implement k-fold cross-validation. Write a function that computes k validation errors by designating each of the k segments as the validation set. It accepts as parameters (i) `k`, (ii) `l2_penalty`, (iii) dataframe, (iv) name of output column (e.g. `price`) and (v) list of feature names. The function returns the average validation error using k segments as validation sets.\n\n* For each i in [0, 1, ..., k-1]:\n  * Compute starting and ending indices of segment i and call 'start' and 'end'\n  * Form validation set by taking a slice (start:end+1) from the data.\n  * Form training set by appending slice (end+1:n) to the end of slice (0:start).\n  * Train a linear model using training set just formed, with a given l2_penalty\n  * Compute validation error using validation set just formed"},{"metadata":{"trusted":true},"cell_type":"code","source":"def k_fold_cross_validation(k, l2_penalty, data, output_name, features_list, degree_n):\n    n = len(data)\n    k_segments = get_segment_indices(k, n)\n    k_validation_errors = []\n    # generate polynomial features from the given input features\n    X = data.loc[:, features_list].values.reshape(-1, 1)\n    y = data.loc[:, output_name].values.reshape(-1, 1)    \n    poly_X = create_polynomial_features(degree_n, X)    \n    poly_data = np.concatenate((poly_X, y), axis=1)\n    for i, segment in enumerate(k_segments):\n        start_i = k_segments[i][0]    \n        end_i = k_segments[i][1]\n        validation_i = poly_data[start_i:end_i]\n        pre_validation_i = poly_data[0: start_i]\n        post_validation_i = poly_data[end_i+1: n]\n        train_i = np.concatenate((pre_validation_i, post_validation_i), axis=0)\n        regressor = Ridge(alpha=l2_penalty, normalize=True)\n        # exclude the last column which is the output variable y to get X\n        train_X = train_i[:, 0:-1]\n        valid_X = validation_i[:, 0:-1]\n        # the last column is y\n        train_y = train_i[:, -1]\n        valid_y = validation_i[:, -1]    \n        regressor.fit(train_X, train_y)\n        valid_predicted_y = regressor.predict(valid_X)        \n        rss = np.sum((valid_y - valid_predicted_y)**2)\n        k_validation_errors.append(rss)\n    return np.mean(k_validation_errors)\n\nresult = k_fold_cross_validation(10, 1000, train_valid_shuffled, 'price', ['sqft_living'], 15)\nprint(result)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Once we have a function to compute the average validation error for a model, we can write a loop to find the model that minimizes the average validation error. Write a loop that does the following:\n* We will again be aiming to fit a 15th-order polynomial model using the `sqft_living` input\n* For each l2_penalty in [10^3, 10^3.5, 10^4, 10^4.5, ..., 10^9] (to get this in Python, you can use this Numpy function: np.logspace(3, 9, num=13).): Run   10-fold cross-validation with l2_penalty.\n* Report which L2 penalty produced the lowest average validation error.\n\nNote: since the degree of the polynomial is now fixed to 15, to make things faster, you should generate polynomial features in advance and re-use them throughout the loop. Make sure to use `train_valid_shuffled` when generating polynomial features!"},{"metadata":{"trusted":true},"cell_type":"code","source":"penalty_rss_list = []\nfor penalty in np.logspace(3, 9, num=13):\n    result = k_fold_cross_validation(10, penalty, train_valid_shuffled, 'price', ['sqft_living'], 15)\n    penalty_rss_list.append((penalty, result))\n    print('penalty:{} --> rss:{}'.format(penalty, result))\n\nsorted_penalty_rss_list = sorted(penalty_rss_list, key=lambda item:item[1])\nprint(sorted_penalty_rss_list[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***QUIZ QUESTIONS:  What is the best value for the L2 penalty according to 10-fold validation?***"},{"metadata":{},"cell_type":"markdown","source":"Once you found the best value for the L2 penalty using cross-validation, it is important to retrain a final model on all of the training data using this value of `l2_penalty`. This way, your final model will be trained on the entire dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"regressor = Ridge(alpha=sorted_penalty_rss_list[0][0], normalize=True)\nX = sales_df.loc[:, 'sqft_living'].values.reshape(-1, 1)\ny = sales_df.loc[:, 'price'].values.reshape(-1, 1)    \npoly15_X = create_polynomial_features(15, X)    \nregressor.fit(poly15_X, y)\n# make predictions on the test dataset\nX_test = test.loc[:, 'sqft_living'].values.reshape(-1, 1)\npoly15_X_test = create_polynomial_features(15, X_test)    \ny_test = test.loc[:, 'price'].values.reshape(-1, 1)    \npredicted_y_test = regressor.predict(poly15_X_test)        \nrss_test = np.sum((y_test - predicted_y_test)**2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***QUIZ QUESTION: Using the best L2 penalty found above, train a model using all training data. What is the RSS on the TEST data of the model you learn with this L2 penalty? ***"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(rss_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":4}