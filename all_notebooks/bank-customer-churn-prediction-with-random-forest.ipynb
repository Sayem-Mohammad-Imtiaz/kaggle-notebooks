{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score,precision_score, recall_score, f1_score, confusion_matrix\n\npd.options.display.float_format = '{:,.1f}'.format","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1. Load data"},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/credit-card-customers/BankChurners.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As suggested in the dataset's description on Kaggle, we delete the last two columns. They won't be needed for churn prediction : https://www.kaggle.com/sakshigoyal7/credit-card-customers"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.iloc[:, :-2]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Explore and prepare data"},{"metadata":{},"cell_type":"markdown","source":"### 2.1 First look"},{"metadata":{},"cell_type":"markdown","source":"We start by looking at the few random rows of our table to see what the data looks like."},{"metadata":{"trusted":false},"cell_type":"code","source":"df.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print('Dataset size : '+str(df.shape))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will work with 10 127 observations and 21 variables if we include the label column."},{"metadata":{"trusted":false},"cell_type":"code","source":"df.CLIENTNUM.nunique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As it is supposed to be, for all clients we have a unique identification code. We won't be able to do much with the **CLIENTNUM** column beside identifying specific clients."},{"metadata":{},"cell_type":"markdown","source":"### 2.2 Focus on the label column"},{"metadata":{},"cell_type":"markdown","source":"The label column called **Attrition_Flag** contains the following values: *Existing Customer* and *Attrited Customer*"},{"metadata":{"trusted":false},"cell_type":"code","source":"print('Label column distribution in our dataset:')\nprint(df.Attrition_Flag.value_counts())\nsns.set(rc={'figure.figsize':(4,3)})\nsns.countplot(data=df,x = 'Attrition_Flag')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can observe that these two classes are quite unbalanced as we have around 5 times more existing customers than attrited ones."},{"metadata":{},"cell_type":"markdown","source":"For easier manipulation of the label column, we will give the value **0** to existing customers and **1** for the customers who quit the bank. "},{"metadata":{"trusted":false},"cell_type":"code","source":"df = df.replace({'Attrition_Flag' : { 'Existing Customer' : 0, 'Attrited Customer' : 1}})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.3 Missing values"},{"metadata":{"trusted":false},"cell_type":"code","source":"print('Number of missing values :')\nprint('')\nprint(df.isna().sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We don't have any missing values."},{"metadata":{},"cell_type":"markdown","source":"### 2.4 Attribute types"},{"metadata":{"trusted":false},"cell_type":"code","source":"print(df.dtypes.value_counts())\nprint(\"====================================================\")\nprint(df.info())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We count 16 variables that are numerical (label column included), the other 5 are categorical."},{"metadata":{"trusted":false},"cell_type":"code","source":"df_num = df.select_dtypes(include=['int64','float64'])\ndf_cat = df.select_dtypes(include=['object'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.5 Analysis of the numerical columns"},{"metadata":{"trusted":false},"cell_type":"code","source":"df_num.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### A few of many interesting facts we can extract from the numerical attributes:\n\n* **Customer_Age**: Most of our customers are middle aged people. 75% of them are older than 41yo. Standard deviation and range (max-min) are also relatively small. \n\n* **Dependent_count**: The number of dependents are in avarage 2.3 and never more than 5 people. The metrics indicate that most of our customers are part of small or avraged size households/families.\n\n* **Months_on_book**: Most of the customers were in the bank for about 3 years. The most recent customer was on the book for more than a year while the person who has been with the bank for the longest will be able to celabrate a 5 year old anniversary as a cutomer of the bank (unless this person is an attrited customer).\n\n* **Total_Relationship_Count**: The number of products held by the customer is usually at least 2 and in avarage close to 4 products. This is not very surprising as a product can be different kind of accounts, but also insurance contract or loan. \n\n* **Months_Inactive_12_mon**: In the last 12 months, 75% of the customers were inactive for at least two months. Also, nonone was inactive for more than 6 months.\n\n* **Contacts_Count_12_mon**: The amount of times the cutomers were in contact with the bank is rather few for 12 months, 2.5 times in avarage. Range goes from 0 to 6."},{"metadata":{},"cell_type":"markdown","source":"### 2.6 Correlation matrix "},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.figure(figsize=(20,10))\nmask = np.zeros((df_num.shape[1],df_num.shape[1]))\nmask[np.triu_indices_from(mask)] = True\nsns.heatmap(df_num.corr(), annot=True, vmin=-1, vmax=1, linewidths=.5, mask = mask)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.7 Analysis of the categorical columns"},{"metadata":{"trusted":false},"cell_type":"code","source":"df_cat.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_cat = df_cat.join(df['Attrition_Flag'])\ndf_cat['Attrition_Flag'] = df_cat['Attrition_Flag'].astype(str)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def stats_on_categ(var1,var2 = df_cat.Attrition_Flag):\n    pd.options.display.float_format = '{:,.1f}'.format\n    tab = pd.crosstab(var1,var2, margins = True)\n    #tab['Distribution of attrited custemers by index values (%)'] = (tab['1']/tab.loc['All','1'])*100\n    #tab['Part of the (\"All\") index value that attrited (%)'] = (tab['1']/tab.All)*100\n    return tab\n\ndef stats_on_categ_visual(var1):\n    sns.set(rc={'figure.figsize':(8,6)})\n    tab = stats_on_categ(var1)\n    tab = tab.drop(['All'], axis=1)\n    return tab.plot(kind='bar', stacked=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(stats_on_categ(df_cat.Gender))\nstats_on_categ_visual(df_cat.Gender)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(stats_on_categ(df_cat.Education_Level))\nstats_on_categ_visual(df_cat.Education_Level)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(stats_on_categ(df_cat.Marital_Status))\nstats_on_categ_visual(df_cat.Marital_Status)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(stats_on_categ(df_cat.Income_Category))\nstats_on_categ_visual(df_cat.Income_Category)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(stats_on_categ(df_cat.Card_Category))\nstats_on_categ_visual(df_cat.Card_Category)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Creating a validation and a train/test dataset"},{"metadata":{},"cell_type":"markdown","source":"Before injecting our data into our model, we set apart around 10% of our data that we won't use during the traing. This will allow us to test the performance of our model on completely unseen data.\n\nThe data that will be used for the model training will be split up into train and test."},{"metadata":{},"cell_type":"markdown","source":"Before these steps, we randomly shuffle our records to avoid selection bias."},{"metadata":{"trusted":false},"cell_type":"code","source":"df = df.sample(frac=1,random_state=1).reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df = df.set_index('CLIENTNUM') # customer ID won't be used for churn prediction","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_train = df.iloc[0:9000,:]\ndf_valid = df.iloc[9000:,:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"target_train = df_train.Attrition_Flag\ntarget_valid = df_valid.Attrition_Flag\n\ndata_train = df_train.drop(['Attrition_Flag'],axis = 1)\ndata_valid = df_valid.drop(['Attrition_Flag'],axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(data_train, target_train, test_size=0.20, random_state= 7)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4.Transforming categorical attributes into dummies"},{"metadata":{"trusted":false},"cell_type":"code","source":"X_train_dummy = pd.get_dummies(X_train)\nX_test_dummy = pd.get_dummies(X_test)\n\n# verify if we have the same columns in train and test dataset\nassert X_train_dummy.columns.equals(X_test_dummy.columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5. Standard scaling our data"},{"metadata":{"trusted":false},"cell_type":"code","source":"scaler = StandardScaler()\nscaler.fit(X_train_dummy)\n\nX_train_sc = pd.DataFrame(scaler.transform(X_train_dummy), columns = X_train_dummy.columns)\nX_test_sc = pd.DataFrame(scaler.transform(X_test_dummy), columns = X_train_dummy.columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 6.Training a Random Forest model"},{"metadata":{"trusted":false},"cell_type":"code","source":"%%time \n\nparam_rf = {\n    'bootstrap': [True],\n    'max_depth': [10,11,12,13],\n    'max_features': [10,11,12,13,14],\n    'n_estimators': [400,500,600,700]\n}\n\n\nclf_rf = RandomForestClassifier()\n\ngrid_rf = GridSearchCV(clf_rf, param_rf, cv = 5, n_jobs = -1)\ngrid_rf.fit(X_train_sc,y_train)\nprint(grid_rf.best_estimator_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 7. Evaluate classification performance"},{"metadata":{"trusted":false},"cell_type":"code","source":"def evaluate_perf(y_test,y_pred):\n    print(pd.crosstab(y_test, y_pred, rownames=['Actual'], colnames=['Predicted']))\n    print('====================')\n    print(\"Accuracy : \", \"{:.2f}\".format(accuracy_score(y_test,y_pred)))\n    print(\"Precision : \", \"{:.2f}\".format(precision_score(y_test,y_pred)))\n    print(\"Recall : \",\"{:.2f}\".format(recall_score(y_test,y_pred)))\n    print(\"F1 :\", \"{:.2f}\".format(f1_score(y_test,y_pred)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"y_pred_rf = grid_rf.predict(X_test_sc)\nevaluate_perf(y_test,y_pred_rf)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 8. Validation of the model performance"},{"metadata":{"trusted":false},"cell_type":"code","source":"data_valid_dummy = pd.get_dummies(data_valid)\nassert X_train_dummy.columns.equals(data_valid_dummy.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"data_valid_sc = pd.DataFrame(scaler.transform(data_valid_dummy), columns = data_valid_dummy.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"valid_pred_rf = grid_rf.predict(data_valid_sc)\nevaluate_perf(target_valid,valid_pred_rf)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 9. Explainability and feature selection"},{"metadata":{"trusted":false},"cell_type":"code","source":"pd.options.display.float_format = '{:,.3f}'.format\nfeature_imp = pd.DataFrame(data=[X_train_sc.columns.tolist(), grid_rf.best_estimator_.feature_importances_]).T\nfeature_imp.columns = ['feature','importance']\nfeature_imp.sort_values('importance',ascending=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}