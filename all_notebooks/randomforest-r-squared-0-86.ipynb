{"cells":[{"metadata":{},"cell_type":"markdown","source":" # 1. Introduction\n\n** Today, we're going to explore Washington House Price Data and will come up with a model to estimate the price of house given all the required features. We'll be doing some basic EDA followed by building multiple regression models from sckit-learn library and will evaluate their performance by comparing the predictions.**\n","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nimport folium\nimport warnings\nimport numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom folium.plugins import HeatMap\n\nfolds  = 5\n\nsns.set(color_codes=True)\n\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n\nfrom sklearn.metrics import r2_score\nfrom ml_metrics import rmse\nfrom scipy import stats\nfrom sklearn.model_selection import GridSearchCV\nscore_calc = 'neg_mean_squared_error'","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"house_price_data = pd.read_csv(\"/kaggle/input/housesalesprediction/kc_house_data.csv\")\nhouse_price_data = house_price_data.drop([\"date\",\"id\"], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Not all features are significant for regression task. We'll discard Latitude, Longitude and Zipcode to proceed with the regression models. **","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"house_price_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Feature Engineering\n\n**Now we'll add some new features to our house price data. When we actually look for a house, we keep all these features mentioned below in our mind:**\n\n* **Age of house** - Age of house extracted by choosing 2020 as a reference year.\n* **Is renovated(Categorical)** - 1 for renovated condition, 0 otherwise \n* **Total Area of house** \n* **Basement(Categorical)** - 1 if basement is available, 0 otherwise","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Adding new features\nhouse_price_data[\"Home_Age\"] = 2020 - house_price_data[\"yr_built\"]\nhouse_price_data['is_renovated'] = house_price_data[\"yr_renovated\"].where(house_price_data[\"yr_renovated\"] == 0, 1)\nhouse_price_data['Total_Area'] = house_price_data['sqft_living'] + house_price_data['sqft_lot'] + house_price_data['sqft_above'] + house_price_data['sqft_basement'] \nhouse_price_data['Basement'] = house_price_data['sqft_basement'].where(house_price_data[\"sqft_basement\"] == 0, 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Visualizing and Examining Data\n\n**This is not a very big data and we do not have too many features. Thus, we have chance to plot most of them and reach some useful analytical results. Drawing charts and examining the data before applying a model is a very good practice because we may detect some possible outliers or decide to do normalization. This is not a must but get know the data is always good. **\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# House Age Distribution \nplt.figure(figsize=(15,5))\nsns.distplot(house_price_data[\"Home_Age\"], color=\"orange\")\nplt.title(\"House Age Distribution\")\nplt.ylabel(\"Density\")\nplt.xlabel(\"House Age\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Century old houses. Hope these houses are still in living condtion. Although distribution is very skewed.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Price Distribution\nplt.figure(figsize=(15,5))\nsns.distplot(house_price_data[\"price\"], color=\"orange\")\nplt.title(\"Price Distribution\")\nplt.ylabel(\"Density\")\nplt.xlabel(\"Price\");\nsns.set(color_codes=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Prices of house are almost distributed normally with mean roughly centered around 45,000 USD. Also, We're going to ignore the impact of inflation over the years as  it is not mentioned in the data. But introducting inflation can be used to forecast the increment in prices of house for every subsequent year.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Exploring correlation between features\nplt.figure(figsize=(10,8))\ncor = house_price_data.corr()\nsns.heatmap(cor, annot=False, cmap=plt.cm.Reds)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"** Significant evidence of high correlation between Price and Area of house**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Distribution of all the parameters with respect to price. Features such as living area, lot area, grade and number bathroom are showing a linear trend with respect to price. Also the distribution below can be used to remove the possible outliers. For this analysis, we're not going to extract outliers but these plots are highly useful for removing outliers form the data**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Distribution of all parameters wrt price.\nnr_rows = 3\nnr_cols = 6\n\nfig, axs = plt.subplots(nr_rows, nr_cols, figsize=(nr_cols*4,nr_rows*4))\n\nnumerical_feats = house_price_data.dtypes[house_price_data.dtypes != \"object\"].index\n\nli_num_feats = list(numerical_feats)\nli_not_plot = ['id']\nli_plot_num_feats = [c for c in list(numerical_feats) if c not in li_not_plot]\n\nfor r in range(0,nr_rows):\n    for c in range(0,nr_cols):  \n        i = r*nr_cols+c\n        if i < len(li_plot_num_feats):\n            sns.regplot(house_price_data[li_plot_num_feats[i]], house_price_data['price'], ax = axs[r][c])\n            stp = stats.pearsonr(house_price_data[li_plot_num_feats[i]], house_price_data['price'])\n            str_title = \"r = \" + \"{0:.2f}\".format(stp[0]) + \"      \" \"p = \" + \"{0:.2f}\".format(stp[1])\n            axs[r][c].set_title(str_title,fontsize=11)\n            \nplt.tight_layout()    \nsns.set(color_codes=True)\nplt.show()   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Features having coorelation to price\nsns.set(color_codes=True)\nhouse_price_data.corrwith(house_price_data.price).plot.bar( figsize = (12, 5), title = \"Correlation with respect to Prices\", fontsize = 15, rot = 90, grid = True);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Let's make use of Lattitide, Longitude and Zipcode to explore the surrounded area. Thanks to Burhan for this neat heatmap!!**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualizing the surrounding \nmaxpr=house_price_data.loc[house_price_data['price'].idxmax()]\n\ndef generateBaseMap(default_location=[47.5112, -122.257], default_zoom_start=9.4):\n    base_map = folium.Map(location=default_location, control_scale=True, zoom_start=default_zoom_start)\n    return base_map\n\nhouse_price_data_copy = house_price_data.copy()\nhouse_price_data_copy['count'] = 1\nbasemap = generateBaseMap()\nfolium.TileLayer('cartodbpositron').add_to(basemap)\ns=folium.FeatureGroup(name='icon').add_to(basemap)\nfolium.Marker([maxpr['lat'], maxpr['long']],popup='Highest Price: $'+str(format(maxpr['price'],'.0f')),\n              icon=folium.Icon(color='green')).add_to(s)\n# add heatmap\nHeatMap(data=house_price_data_copy[['lat','long','count']].groupby(['lat','long']).sum().reset_index().values.tolist(),\n        radius=8,max_zoom=13,name='Heat Map').add_to(basemap)\nfolium.LayerControl(collapsed=False).add_to(basemap)\nbasemap","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Correlation wrt price\ncorr = house_price_data.corr()\ncorr_abs = corr.abs()\n\nnr_num_cols = len(numerical_feats)\nser_corr = corr_abs.nlargest(nr_num_cols, 'price')['price']\nprint(ser_corr)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Conclusion from EDA:**\n\n**We see that for some features like 'sqft_living' and 'grade' there is a strong linear correlation of (0.70) and (0.67) to the target.\nFor this kernel I decided to use all features for prediction that have correlation with SalePrice.\nIn future work, we can try to drop some columns that have weak correlation with the target price. \nAlso, we can remove the outliers from the data for better model building practices. **","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Creating Data Split for training and testing purpose. Also, using a scaled version of dataset for certain algorithms.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\ntarget = house_price_data[\"price\"]\nfeatures = house_price_data.drop(\"price\", axis = 1)\n\nX_train, X_test, Y_train, Y_test = train_test_split(features, target, test_size = 0.3, random_state = 1)\n\nsc = StandardScaler()\nX_train_sc = pd.DataFrame(sc.fit_transform(X_train))\nX_test_sc = pd.DataFrame(sc.transform(X_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Using Grid Search Cross Validation for choosing the best parameters of models based on obtained scores.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_best_score(grid):\n    \n    best_score = np.sqrt(-grid.best_score_)\n    print(best_score)    \n    print(grid.best_params_)\n    print(grid.best_estimator_)\n    \n    return best_score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. Model Building\n\n**Scikit-learn offers a wide variety of regression algorithms. We'll build simple to complex regression models using different algorithms and will compare the results obtained over different models with our testing dataset which contains 30% of original dataset. For tuning the model and selecting optimum parameters. We'll use GridSearch cross validation.**\n\n**We test the following Regressors from scikit-learn:**\n* LinearRegression\n* Ridge\n* Lasso\n* Stochastic Gradient Descent\n* DecisionTreeRegressor\n* RandomForestRegressor\n* KNN Regressor\n* XGBoost Regressor","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# **Linear Regression**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\n\nlinreg = LinearRegression()\nparameters = {'fit_intercept':[True,False], 'normalize':[True,False], 'copy_X':[True, False]}\ngrid_linear = GridSearchCV(linreg, parameters, cv = folds, verbose = 1 , scoring = score_calc)\ngrid_linear.fit(X_train, Y_train)\n\nsc_linear = get_best_score(grid_linear)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"LR = LinearRegression()\nLR.fit(X_train, Y_train)\npred_linreg_all = LR.predict(X_test)\npred_linreg_all[pred_linreg_all < 0] = pred_linreg_all.mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Rsquared Score for Linear Regression\nr2_score(Y_test, pred_linreg_all)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Stochastic Gradient Descent Regressor","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import SGDRegressor\n\nsgd = SGDRegressor()\nparameters = {'max_iter' :[10000], 'alpha':[1e-05], 'epsilon':[1e-02], 'fit_intercept' : [True]  }\ngrid_sgd = GridSearchCV(sgd, parameters, cv = folds, verbose = 0, scoring = score_calc)\ngrid_sgd.fit(X_train_sc, Y_train)\n\nsc_sgd = get_best_score(grid_sgd)\npred_sgd = grid_sgd.predict(X_test_sc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Rsquared Score for SGD Regressor\nr2_score(Y_test, pred_sgd)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Decision Tree Regressor","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor\n\nparam_grid = { 'max_depth' : [7,8,9,10] , 'max_features' : [11,12,13,14] ,\n               'max_leaf_nodes' : [None, 12,15,18,20] ,'min_samples_split' : [20,25,30],\n                'presort': [False,True] , 'random_state': [5] }\n\ngrid_dtree = GridSearchCV(DecisionTreeRegressor(), param_grid, cv = folds, refit = True, verbose = 0, scoring = score_calc)\ngrid_dtree.fit(X_train, Y_train)\n\nsc_dtree = get_best_score(grid_dtree)\n\npred_dtree = grid_dtree.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Rsquared Score for DTree Regression\nr2_score(Y_test, pred_dtree)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# RandomForest Regressor","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\n\nparam_grid = {'min_samples_split' : [3,4,6,10], 'n_estimators' : [70,100], 'random_state': [5] }\ngrid_rf = GridSearchCV(RandomForestRegressor(), param_grid, cv = folds, refit=True, verbose = 0, scoring = score_calc)\ngrid_rf.fit(X_train, Y_train)\n\nsc_rf = get_best_score(grid_rf)\npred_rf = grid_rf.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Rsquared Score for RandomForest\nr2_score(Y_test, pred_rf)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# KNN Regressor","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsRegressor\n\nparam_grid = {'n_neighbors' : [3,4,5,6,7,10,15] ,              \n              'weights' : ['uniform','distance'] ,\n              'algorithm' : ['ball_tree', 'kd_tree', 'brute']}\n\ngrid_knn = GridSearchCV(KNeighborsRegressor(), param_grid, cv = folds, refit=True, verbose = 0, scoring = score_calc)\ngrid_knn.fit(X_train_sc, Y_train)\nsc_knn = get_best_score(grid_knn)\npred_knn = grid_knn.predict(X_test_sc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Rsquared Score for KNN\nr2_score(Y_test, pred_knn)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# XGBoost Regressor","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBRegressor\nparam_grid = {'learning_rate' : [0.005,0.01,0.001], 'n_estimators' : [40,200], 'random_state': [5],\n              'max_depth' : [4,9]}\ngrid_xgb = GridSearchCV(XGBRegressor(), param_grid, cv = folds, refit=True, verbose = 0, scoring = score_calc)\ngrid_xgb.fit(X_train, Y_train)\n\nsc_xgb = get_best_score(grid_xgb)\npred_xgb = grid_xgb.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Rsquared Score for XGBoost Regressor\nr2_score(Y_test, pred_xgb)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Result Evaluation\n\n**Linear Regression models gave up high RMSE. Also, XGB, KNN and Dtree models performed similarly with roughly 162.5k RMS error in estimation. While RandomForest recorded the lowest RMS error.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"list_scores = [sc_linear,sc_sgd, sc_dtree, sc_rf, sc_knn, sc_xgb]\nlist_predictions = [pred_linreg_all, pred_sgd, pred_dtree, pred_rf, pred_knn, pred_xgb]\nlist_regressors = ['Linear','SGD','DTr','RF','KNN','XGB']\nfig, ax = plt.subplots()\nfig.set_size_inches(10,7)\nsns.barplot(x=list_regressors, y=list_scores, ax=ax)\nplt.ylabel('RMSE Training')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Moment of truth\n**Let's compare the overall RMS error secured by models over the testing dataset.**\n**RandomForest outformed all the models in the analysis and secured a rms value less than 150K which means we can actually use the RandomForest model in estimating the house price given all the other features.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"errors = []\nfor pred in list_predictions:\n    errors.append(rmse(Y_test, pred))\n    \nregressors = ['Linear','SGD','DTr','RF','KNN','XGB']\nfig, ax = plt.subplots()\nfig.set_size_inches(10,7)\nsns.barplot(x = regressors, y = errors, ax=ax)\nplt.ylabel('Actual Test RMSE')\nplt.xlabel('Algorithms')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = {'Linear': pred_linreg_all,\n               'SGD': pred_sgd, 'DTr': pred_dtree, 'RF': pred_rf,\n               'KNN': pred_knn, 'XGB': pred_xgb}\ndf_predictions = pd.DataFrame(data=predictions) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# How closely the predictions are related from different models\n\n**Only for Random Forest, Decision Tree and XGBoost, the results are less correlated with the other Regressors.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,8))\ncor_pred = df_predictions.corr()\nsns.heatmap(cor_pred, annot=True, cmap=plt.cm.Reds)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5. Conclusion\n\nSo far, we explored the data and added some new features into it. We did some EDA to visualize the impact of features over the traget price variable. Then we built multiple regression models and evaluated their preformance over the test dataset. RandomForest outformed other models and gave a RMSE slightly lower than 150k. We also compared correlation from different predicted estimates over the testing dataset. That's all folks!!\n\n**Future Work**: I think by tuning the parameters of XGBoost, we can achieve lower RMSE than RandomForest. I'll update the kernel later by improving the RMSE from XGBoost Regressor.\n\nIf you like this kernel than I would appreciate an **upvote** from you. \n\nThanks!!\n","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}