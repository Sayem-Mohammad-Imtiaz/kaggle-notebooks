{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data=pd.read_csv('/kaggle/input/news-summary/news_summary_more.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data1=pd.read_csv('/kaggle/input/news-summary/news_summary.csv',encoding='latin1')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"summary=pd.DataFrame()\nsummary['text']=pd.concat([data1['text'],data['text']],ignore_index=True)\nsummary['summary']=pd.concat([data1['headlines'],data['headlines']],ignore_index=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"summary.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"summary.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data1.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\nfrom tqdm import tqdm\n#Removes non-alphabetic characters:\ndef text_strip(column):\n    s=[]\n    for row in tqdm(column):\n        \n        #ORDER OF REGEX IS VERY VERY IMPORTANT!!!!!!\n        \n        row=re.sub(\"(\\\\t)\", ' ', str(row)).lower() #remove escape charecters\n        row=re.sub(\"(\\\\r)\", ' ', str(row)).lower() \n        row=re.sub(\"(\\\\n)\", ' ', str(row)).lower()\n        \n        row=re.sub(\"(__+)\", ' ', str(row)).lower()   #remove _ if it occors more than one time consecutively\n        row=re.sub(\"(--+)\", ' ', str(row)).lower()   #remove - if it occors more than one time consecutively\n        row=re.sub(\"(~~+)\", ' ', str(row)).lower()   #remove ~ if it occors more than one time consecutively\n        row=re.sub(\"(\\+\\++)\", ' ', str(row)).lower()   #remove + if it occors more than one time consecutively\n        row=re.sub(\"(\\.\\.+)\", ' ', str(row)).lower()   #remove . if it occors more than one time consecutively\n        \n        row=re.sub(r\"[<>()|&©ø\\[\\]\\'\\\",;?~*!]\", ' ', str(row)).lower() #remove <>()|&©ø\"',;?~*!\n        \n        row=re.sub(\"(mailto:)\", ' ', str(row)).lower() #remove mailto:\n        row=re.sub(r\"(\\\\x9\\d)\", ' ', str(row)).lower() #remove \\x9* in text\n        row=re.sub(\"([iI][nN][cC]\\d+)\", 'INC_NUM', str(row)).lower() #replace INC nums to INC_NUM\n        row=re.sub(\"([cC][mM]\\d+)|([cC][hH][gG]\\d+)\", 'CM_NUM', str(row)).lower() #replace CM# and CHG# to CM_NUM\n        \n        \n        row=re.sub(\"(\\.\\s+)\", ' ', str(row)).lower() #remove full stop at end of words(not between)\n        row=re.sub(\"(\\-\\s+)\", ' ', str(row)).lower() #remove - at end of words(not between)\n        row=re.sub(\"(\\:\\s+)\", ' ', str(row)).lower() #remove : at end of words(not between)\n        \n        row=re.sub(\"(\\s+.\\s+)\", ' ', str(row)).lower() #remove any single charecters hanging between 2 spaces\n        \n        #Replace any url as such https://abc.xyz.net/browse/sdf-5327 ====> abc.xyz.net\n        try:\n            url = re.search(r'((https*:\\/*)([^\\/\\s]+))(.[^\\s]+)', str(row))\n            repl_url = url.group(3)\n            row = re.sub(r'((https*:\\/*)([^\\/\\s]+))(.[^\\s]+)',repl_url, str(row))\n        except:\n            pass \n        \n\n        \n        row = re.sub(\"(\\s+)\",' ',str(row)).lower() \n        \n    \n        row=re.sub(\"(\\s+.\\s+)\", ' ', str(row)).lower() \n\n        \n        \n        s.append(row)\n    return s   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text=text_strip(summary['text'].values)\nsummary=text_strip(summary['summary'].values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text=text[:2000]\nsummary=summary[:2000]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(len(summary)):\n    summary[i]='<start> '+summary[i]+' <end>'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_t=text[:int(len(text)*0.8)]\ntest_t=text[int(len(text)*0.8):]\ntrain_s=summary[:int(len(text)*0.8)]\ntest_s=summary[int(len(text)*0.8):]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"summary_max_len=0\nfor i in summary:\n    if summary_max_len<len(i):\n        summary_max_len=len(i)\nprint(summary_max_len)        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text_lengths=[]\nsummary_lengths=[]\nfor i in tqdm(range(len(text))):\n    text_lengths.append(len(text[i]))\n    summary_lengths.append(len(summary[i]))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nsns.distplot(text_lengths)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(summary_lengths)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = Tokenizer(oov_token=\"<OOV>\", filters='')\ntokenizer1 = Tokenizer(oov_token=\"<OOV>\", filters='')\ntokenizer.fit_on_texts(train_t)\ntokenizer1.fit_on_texts(train_s)\nword_index = tokenizer.word_index\nprint(len(word_index))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_index1 = tokenizer1.word_index\nprint(len(word_index1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text_sequences_tr = tokenizer.texts_to_sequences(train_t)\ntext_padded_tr = pad_sequences(text_sequences_tr,maxlen=150,padding = 'post')\n\nprint(text_padded_tr.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"summary_sequences_tr = tokenizer1.texts_to_sequences(train_s)\nsummary_padded_tr = pad_sequences(summary_sequences_tr,maxlen=10,padding = 'post')\n\nprint(summary_padded_tr.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text_sequences_te = tokenizer1.texts_to_sequences(test_t)\ntext_padded_te = pad_sequences(text_sequences_te,maxlen=150,padding = 'post')\n\nprint(text_padded_te.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"summary_sequences_te = tokenizer.texts_to_sequences(test_s)\nsummary_padded_te = pad_sequences(summary_sequences_te,maxlen=10,padding = 'post')\n\nprint(summary_padded_te.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train=text_padded_tr\ny_train=summary_padded_tr\nx_test=text_padded_te\ny_test=summary_padded_te","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed,GRU\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import EarlyStopping\nimport tensorflow as tf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BUFFER_SIZE = len(x_train)\nBATCH_SIZE = 32\nsteps_per_epoch = len(x_train)//BATCH_SIZE\nembedding_dim = 64\nunits = 128\nvocab_inp_size = len(tokenizer.word_index)+1\nvocab_tar_size = len(tokenizer1.word_index)+1\n\ndataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(BUFFER_SIZE)\ndataset = dataset.batch(BATCH_SIZE, drop_remainder=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"example_input_batch, example_target_batch = next(iter(dataset))\nexample_input_batch.shape, example_target_batch.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nclass Encoder(tf.keras.Model):\n    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n        super(Encoder, self).__init__()\n        self.batch_sz = batch_sz\n        self.enc_units = enc_units\n        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n        self.gru = tf.keras.layers.GRU(self.enc_units,\n                                   return_sequences=True,\n                                   return_state=True,\n                                   recurrent_initializer='glorot_uniform')\n\n    def call(self, x, hidden):\n        x = self.embedding(x)\n        output, state = self.gru(x, initial_state = hidden)\n        return output, state\n\n    def initialize_hidden_state(self):\n        return tf.zeros((self.batch_sz, self.enc_units))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n\n# sample input\nsample_hidden = encoder.initialize_hidden_state()\nsample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\nprint ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\nprint ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class BahdanauAttention(tf.keras.layers.Layer):\n    def __init__(self, units):\n        super(BahdanauAttention, self).__init__()\n        self.W1 = tf.keras.layers.Dense(units)\n        self.W2 = tf.keras.layers.Dense(units)\n        self.V = tf.keras.layers.Dense(1)\n\n    def call(self, query, values):\n    \n        query_with_time_axis = tf.expand_dims(query, 1)\n\n        score = self.V(tf.nn.tanh(\n        self.W1(query_with_time_axis) + self.W2(values)))\n\n        attention_weights = tf.nn.softmax(score, axis=1)\n\n        context_vector = attention_weights * values\n        context_vector = tf.reduce_sum(context_vector, axis=1)\n\n        return context_vector, attention_weights\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"attention_layer = BahdanauAttention(10)\nattention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n\nprint(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\nprint(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Decoder(tf.keras.Model):\n    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n        super(Decoder, self).__init__()\n        self.batch_sz = batch_sz\n        self.dec_units = dec_units\n        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n        self.gru = tf.keras.layers.GRU(self.dec_units,\n                                   return_sequences=True,\n                                   return_state=True,\n                                   recurrent_initializer='glorot_uniform')\n        self.fc = tf.keras.layers.Dense(vocab_size)\n        self.attention = BahdanauAttention(self.dec_units)\n\n    def call(self, x, hidden, enc_output):\n    \n        context_vector, attention_weights = self.attention(hidden, enc_output)\n\n    \n        x = self.embedding(x)\n\n   \n        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n\n    \n        output, state = self.gru(x)\n\n    \n        output = tf.reshape(output, (-1, output.shape[2]))\n\n    \n        x = self.fc(output)\n\n        return x, state, attention_weights","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n\nsample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n                                      sample_hidden, sample_output)\n\nprint ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"optimizer = tf.keras.optimizers.Adam()\nloss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n    from_logits=True, reduction='none')\n\ndef loss_function(real, pred):\n    mask = tf.math.logical_not(tf.math.equal(real, 0))\n    loss_ = loss_object(real, pred)\n\n    mask = tf.cast(mask, dtype=loss_.dtype)\n    loss_ *= mask\n\n    return tf.reduce_mean(loss_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"checkpoint_dir = './training_checkpoints'\ncheckpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\ncheckpoint = tf.train.Checkpoint(optimizer=optimizer,\n                                 encoder=encoder,\n                                 decoder=decoder)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"@tf.function\ndef train_step(inp, targ, enc_hidden):\n    loss = 0\n    with tf.GradientTape() as tape:\n        enc_output, enc_hidden = encoder(inp, enc_hidden)\n        dec_hidden = enc_hidden\n\n        dec_input = tf.expand_dims([tokenizer1.word_index['<start>']] * BATCH_SIZE, 1)\n        for t in range(1, targ.shape[1]):\n            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n            loss += loss_function(targ[:, t], predictions)\n            dec_input = tf.expand_dims(targ[:, t], 1)\n\n    batch_loss = (loss / int(targ.shape[1]))\n\n    variables = encoder.trainable_variables + decoder.trainable_variables\n    gradients = tape.gradient(loss, variables)\n   \n    optimizer.apply_gradients(zip(gradients, variables))\n\n    return batch_loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"EPOCHS = 10\nimport time\nfor epoch in range(EPOCHS):\n    start = time.time()\n\n    enc_hidden = encoder.initialize_hidden_state()\n    total_loss = 0\n\n    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n        batch_loss = train_step(inp, targ, enc_hidden)\n        total_loss += batch_loss\n        \n\n    if batch % 100 == 0:\n        print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n                                                   batch,\n                                                   batch_loss.numpy()))\n    if (epoch + 1) % 2 == 0:\n        checkpoint.save(file_prefix = checkpoint_prefix)\n\n    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n                                      total_loss / steps_per_epoch))\n    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def evaluate(sentence):\n    #attention_plot = np.zeros((max_length_targ, max_length_inp))\n\n    #sentence = preprocess_sentence(sentence)\n\n    inputs = [tokenizer.word_index[i] for i in sentence.split(' ')]\n    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n                                                         maxlen=150,\n                                                         padding='post')\n    inputs = tf.convert_to_tensor(inputs)\n\n    result = ''\n\n    hidden = [tf.zeros((1, units))]\n    enc_out, enc_hidden = encoder(inputs, hidden)\n\n    dec_hidden = enc_hidden\n    dec_input = tf.expand_dims([tokenizer1.word_index['<start>']], 0)\n\n    for t in range(10):\n        predictions, dec_hidden , attention_weights = decoder(dec_input,\n                                                         dec_hidden,\n                                                         enc_out)\n\n    \n        #attention_weights = tf.reshape(attention_weights, (-1, ))\n        #attention_plot[t] = attention_weights.numpy()\n\n        predicted_id = tf.argmax(predictions[0]).numpy()\n\n        result += tokenizer1.index_word[predicted_id] + ' '\n\n        if tokenizer1.index_word[predicted_id] == '<end>':\n            return result, sentence\n        #, attention_plot\n\n    \n        dec_input = tf.expand_dims([predicted_id], 0)\n\n    return result, sentence\n#, attention_plot","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def translate(sentence):\n    result, sentence= evaluate(sentence)\n\n    print('Input: %s' % (sentence))\n    print('Predicted translation: {}'.format(result))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"translate(text[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"summary[0]","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}