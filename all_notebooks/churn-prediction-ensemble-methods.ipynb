{"cells":[{"metadata":{"_uuid":"e3d57f3a85babc0a6f12d37c82e8dd6e814c98e1"},"cell_type":"markdown","source":"<h2>Introduction</h2>\n\nIn this notebook we will be using ensemble methods to predict if a customer left the company in the last month. The goal of ensemble methods is to combine the predictions of several base estimators built with a given learning algorithm in order to improve robustness. There are two general categories of ensemble methods: averaging and boosting methods. Decision trees will be used as our base estimator for both."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import random\nimport pprint\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score, accuracy_score, recall_score, precision_score\nimport warnings\nwarnings.simplefilter(action='ignore', category=UserWarning)\nwarnings.simplefilter(action='ignore', category=DeprecationWarning)\ndf = pd.read_csv(\"../input/WA_Fn-UseC_-Telco-Customer-Churn.csv\")\ndf.head(3)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"234da41e00c992a849c867ef0ff90ca244a71de6"},"cell_type":"markdown","source":"<h2>1. Preprocessing</h2>\n\nSince all ensemble methods in this notebook are based on Decision Trees, we don't need feature scalling. However, we need to encode categorical features and drop a few columns. "},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"def preprocessing(df):\n    \"\"\"Preprocess df and return X (train features) and Y (target feature).\"\"\"\n    # Impute missing values on TotalCharges\n    df['TotalCharges'] = df['TotalCharges'].replace(\" \", 0).astype('float32')\n    # Drop customer id\n    df.drop(['customerID'], axis=1, inplace=True)\n    # Encode categorical features\n    cat_cols = [c for c in df.columns if df[c].dtype == 'object' or c == 'SeniorCitizen']\n    for col in cat_cols:\n        if df[col].nunique() == 2:\n            df[col], _ = pd.factorize(df[col])\n        else:\n            df = pd.get_dummies(df, columns=[col])\n    # Drop target column and some correlated features\n    drop_features = ['OnlineSecurity_No internet service', 'OnlineBackup_No internet service',\n                     'DeviceProtection_No internet service', 'TechSupport_No internet service',\n                     'StreamingTV_No internet service', 'StreamingMovies_No internet service',\n                     'PhoneService', 'Churn']\n    feats = [c for c in df.columns if c not in drop_features]\n    return df[feats], df['Churn']\nx, y = preprocessing(df)\nx.head(3)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"be7b2c61706e8544621af1e9310a8632b3e06fca"},"cell_type":"markdown","source":"<h3>Split data</h3>\n\nDivide our data in 80% for training and 20% for our final test."},{"metadata":{"trusted":true,"_uuid":"80672c4f0d474efb0c56b2e798be5537658d383d"},"cell_type":"code","source":"# Split dataset in train and test\ntrain_x, test_x, train_y, test_y = train_test_split(x, y, test_size=0.2, random_state=50)\n# Create a training and testing dataset (lightgbm api object)\ntrain_set = lgb.Dataset(data= train_x, label= train_y, silent=-1)\ntest_set = lgb.Dataset(data= test_x, label= test_y, silent=-1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"59628aca9ba6c5f3e8bee48adcd7f88a3e16ce41"},"cell_type":"markdown","source":"<h2>2. Hyperparameter tunning</h2>\n\nWe have to define many hyperparameters since we have the ensemble estimator and the base estimator. Instead of doing this by hand, the next function will randomly pick a set of parameters and test it with a KFold cross-validation scheme. The LightGBM library has a built-in function to easily implement this validation method.\n\nThe procedure can be described with the following steps:\n* Randomly pick a subset of hyperparameters\n* Train and validate our model with KFold cross-validation (second function)\n* Repeat this procedure MAX_EVALS times\n* Get the best cross-validation score and hyperparameters\n\nEarly stopping will be used to stop the algorithm when the validation score doesn't improve for 50 boosting rounds."},{"metadata":{"trusted":true,"_uuid":"718783c317645ad764d71cc3344e86d81ef1d365"},"cell_type":"code","source":"def hyperparameter_random_search(train_set, params_grid, fixed_params, max_evals,\n                                 num_folds, print_params=False):\n    \"\"\"Random search for hyperparameter optimization\"\"\"\n    # Dataframe for results\n    results = pd.DataFrame(columns = ['score', 'params', 'iteration'],\n                                  index = list(range(max_evals)))\n    \n    # Keep searching until reach max evaluations\n    for i in range(max_evals):\n        # Choose random hyperparameters\n        hyperparameters = {k: random.sample(v, 1)[0] for k, v in params_grid.items()}\n        hyperparameters.update(fixed_params)\n        if print_params:\n            print(hyperparameters)\n        # Evaluate randomly selected hyperparameters\n        eval_results = objective(train_set, hyperparameters, i, num_folds)\n        # Add results to our dataframe\n        results.loc[i, :] = eval_results\n    # Sort with best score on top\n    results.sort_values('score', ascending=False, inplace=True)\n    results.reset_index(inplace=True)\n    return results\n\ndef objective(train_set, hyperparameters, iteration, num_folds):\n    \"\"\"Objective function for grid and random search. Returns\n       the cross-validation score from a set of hyperparameters.\"\"\"\n\n     # Perform n_folds cross validation\n    hyperparameters['verbose'] = -1\n    cv_results = lgb.cv(hyperparameters, train_set, num_boost_round=10000, nfold=num_folds,\n                        early_stopping_rounds=50, metrics ='auc', seed=50)\n    score = cv_results['auc-mean'][-1]\n    estimators = len(cv_results['auc-mean'])\n    hyperparameters['n_estimators'] = estimators \n    return score, hyperparameters, iteration","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"37893528a3add8463a94c766f4a9a092e6bd78ce"},"cell_type":"markdown","source":"<h2>3. Random Forests</h2>\n\nThe ideia behind Random Forests is to build multiple decision trees and merges them together to get a more accurate and stable prediction. In each estimator, only a random subset of the features is taken into consideration for splitting a node, therefore adding additional randomness to the final model. Random Forests can be considered an averaging ensemble method."},{"metadata":{"trusted":true,"_uuid":"11f9a23bb3f081ee550a52dd451799fcec8cb830"},"cell_type":"code","source":"params_grid = {\n    'num_leaves': list(range(8, 255)),  # Control tree size\n    # Percentage (sample) of columns and rows\n    'colsample_bytree': list(np.linspace(0.4, 0.99)),\n    'subsample': list(np.linspace(0.4, 0.99)),\n    # Min data points to create a leaf\n    'min_child_samples': list(range(1, 101, 5)),\n    # Regularization\n    'reg_alpha': list(np.linspace(0, 1)),\n    'reg_lambda': list(np.linspace(0, 1)),\n}\n\nfixed_params = {'boosting_type': 'rf', 'objective': 'binary',\n                'subsample_freq': 1, 'n_jobs':4}\nres = hyperparameter_random_search(train_set, params_grid, fixed_params, 1000, 5)\nres.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"46a185805fb96e0170f24359654d6c16b981abec"},"cell_type":"markdown","source":"<h3>Testing the model</h3>"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"2a0597d9eec6d36d4b5e79fee9f2b0efa9966782"},"cell_type":"code","source":"# Create, train, test model\nmodel = lgb.LGBMClassifier(**res.loc[0, 'params'], random_state=50)\nmodel.fit(train_x.values, train_y.values)\npredict_proba = model.predict_proba(test_x.values)[:, 1]\npredict_labels = model.predict(test_x.values)\n\n# Print final results\nprint(\"Scores on test set: {:.4f} ROC AUC,  {:.4f} accuracy, {:.4f} recall, {:.4f} precision\"\n      .format(roc_auc_score(test_y, predict_proba),\n              accuracy_score(test_y, predict_labels),\n              recall_score(test_y, predict_labels), \n              precision_score(test_y, predict_labels)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"44aa1df97c04a19ec503a2bdb6b9394df23a8007"},"cell_type":"markdown","source":"<h2>4. Gradient Boosting</h2>\n\nIn gradient boosting, the predictors are not made independently, but sequentially. In each iteration, a decision tree is fit on the error from the previous round. It's usually better to use shallow trees (weak learners) and a low learning rate, so each iteration does small improvements to the overall problem. "},{"metadata":{"trusted":true,"_uuid":"e0b081ce883bf18b892d4b3ba523ac0d4d06b119"},"cell_type":"code","source":"# Possible hyperparameters (grid)\nparam_grid = {\n    'num_leaves': list(range(7, 95)),\n    'learning_rate': list(np.logspace(np.log(0.005), np.log(0.2))),\n    'subsample_for_bin': list(range(20000, 300000, 20000)),\n    'min_child_samples': list(range(20, 500, 5)),\n    'reg_alpha': list(np.linspace(0, 1)),\n    'reg_lambda': list(np.linspace(0, 1)),\n    'colsample_bytree': list(np.linspace(0.4, 1, 10)),\n    'subsample': list(np.linspace(0.5, 1, 100)),\n}\nfixed_params = {'boosting': 'gbdt', 'objective': 'binary', 'n_jobs': 4}\nres = hyperparameter_random_search(train_set, param_grid, fixed_params, 1000, 5)\nres.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b3dcf38beb6f5fa23febc6196231ca05e9dc766b"},"cell_type":"markdown","source":"<h3>Testing the model</h3>"},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"c9be81d3217e81dd828141b6775d7ae22b2de041"},"cell_type":"code","source":"# Create, train, test model\nmodel = lgb.LGBMClassifier(**res.loc[0, 'params'], random_state=50)\nmodel.fit(train_x.values, train_y.values)\npredict_proba = model.predict_proba(test_x.values)[:, 1]\npredict_labels = model.predict(test_x.values)\n\n# Print final results\nprint(\"Scores on test set: {:.4f} ROC AUC,  {:.4f} accuracy, {:.4f} recall, {:.4f} precision\"\n      .format(roc_auc_score(test_y, predict_proba),\n              accuracy_score(test_y, predict_labels),\n              recall_score(test_y, predict_labels), \n              precision_score(test_y, predict_labels)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3cc3fbd8ad1076b9c06781d73c2f69dd2b0cc7a8"},"cell_type":"markdown","source":"<h2>5. Conclusion</h2>\n\nIn this short notebook, we compared the performance of two ensemble methods in a binary classification problem. The first algorithm (Random Forests) is an averaging ensemble technique that train multiple decision trees in parallel and combine their predictions. Our second classifier was a boosting ensemble method, also using decision trees as base estimator. \n\nHyperparameters were found using random search with a KFold validation scheme over more than one thousand tries for each estimator."},{"metadata":{"trusted":true,"_uuid":"1d9ba3596ebd70c9bb2262234ad2db97456ec16c"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}