{"cells":[{"metadata":{},"cell_type":"markdown","source":"<a href=\"https://www.venturus.org.br/en/\"><img src=\"https://s3-sa-east-1.amazonaws.com/prod-jobsite-files.kenoby.com/uploads/venturus-1544703795-vnt-mainpng.png\" ></a>\n<br>\n<p >This notebook was specially made for Machine Learning students from Venturus company, located in Campinas, Brazil.\n<br>\n<a href=\"https://www.venturus.org.br/en/\">https://www.venturus.org.br/en/</a>\n</p>"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Definição do problema e Aquisição dos dados\n\nPara esse projeto o problema e os dados são nos dado pelo Kaggle. Nosso único trabalho é tratar os dados e desenvolver um modelo.\n\n<h3>Qual é o problema?</h3>\n\nO naufrágio do Titanic é um dos mais famosos da história, inclusive um grande sucesso do cinema. \nEm 15 de abril de 1912, durante sua viagem de inauguração, o Titanic afundou depois de colidir com um iceberg, matando 1502 dos 2224 passageiros e tripulação. Essa trágedia chocou o mundo e levou na melhoria de leis na navegação.\n\nUm das razões que levou a tantas mortes foi a falta de botes suficientes para passageiros e tripulação. Além disso, alguns elementos que envolvendo sorte também contribuiram para a sobrevivência, sendo que alguns grupos tinham maior chance de sobreviver, como mulheres, crianças e passageiros de alto poder aquisitivo.\n\n<b>Nessa problema, o objetivo é aplicar técnicas de Machine Learing para prever quais passageiros sobreviveriam à tragédia.</b>\n\nHabilidades práticas:\n* Classificação binária\n* Python básico\n\n<h3>Os dados</h3>\n\nOs dados são nos fornecidos em forma de planilha no formato .csv, já divididos em dois grupos: treinamento e teste.\nUsaremos os dados de treinamento para construir modelos de Machine Learning. Usaremos dados como idade, sexo e classe social para classificar quais passageiros \"deveriam\" ter sobrevivido.\n\nOs dados fornecidos são:\n\n<table>\n  <tr>\n    <th>Variável</th>\n    <th>Definição</th>\n    <th>Valor</th>\n  </tr>\n  <tr>\n    <td>survival</td>\n    <td>Sobreviveu</td>\n    <td>0 = Não e 1 = Sim</td>\n  </tr>\n  <tr>\n    <td>pclass</td>\n    <td>Classe no navio</td>\n    <td>1 = 1ª, 2 = 2ª e 3 = 3ª</td>\n  </tr>\n  <tr>\n    <td>sex</td>\n    <td>Sexo</td>\n    <td>Feminino ou Masculino</td>\n  </tr>\n  <tr>\n    <td>age</td>\n    <td>Idade</td>\n    <td></td>\n  </tr>\n  <tr>\n    <td>sibsp</td>\n    <td>Números de irmãos e conjuges a bordo do Titanic</td>\n    <td></td>\n  </tr>\n  <tr>\n    <td>parch</td>\n    <td>Número de pais ou filhos abordo do Titanic</td>\n    <td></td>\n  </tr>\n  <tr>\n    <td>ticket</td>\n    <td>Número da passagem</td>\n    <td></td>\n  </tr>\n  <tr>\n    <td>fare</td>\n    <td>Custo da passagem</td>\n    <td></td>\n  </tr>\n  <tr>\n    <td>cabin</td>\n    <td>Número da cabine</td>\n    <td></td>\n  </tr>\n  <tr>\n    <td>embarked</td>\n    <td>Porto que embarcou</td>\n    <td>C = Cherbourg, Q = Queenstown, S = Southampton</td>\n  </tr>\n</table>"},{"metadata":{},"cell_type":"markdown","source":"# Preparação dos dados\n\n1. Identificação das Variáveis\n* Limpeza de dados\n* Tratamento de valores vazios\n* Criação de Variáveis\n* Transformação de variáveis\n* <p style=\"color:red\"><del>Identificação de anomalias</del></p>"},{"metadata":{},"cell_type":"markdown","source":"# Identificação das variáveis\n\nPrimeiramente vamos carregar os dados. Para isso usamos a biblioteca Pandas, qual já é preparada para a leitura de diversos tipos de arquivos.\nPara dados tabulares o pandas nos fornece os dados em um DataFrame, uma classe qual facilita a manipulação dos dados."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_data = pd.read_csv(\"../input/titanic/train.csv\")\ntrain_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data = pd.read_csv(\"../input/titanic/test.csv\")\ntest_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"O DataFrame tem funções simples como .describe() que nos mostra estátisticas simples dos dados."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_dt = train_data.append(test_data)\ndata_dt.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Tratamento de valores vazios\n\nVamos verificar os valores vazios com a função .isna(), acrescida da função .sum() que mostra o total de valores vazios para cada variável."},{"metadata":{"trusted":true},"cell_type":"code","source":"data_dt.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Identificados as variáveis com valores vazios, temos algumas abordagens à seguir:\n1. <p style=\"color:red\"><del>Excluir linhas</del></p>\n2. Excluir colunas\n3. Preencher com a média ou mediana\n4. Preencher com média ou mediana baseado em outros atributos"},{"metadata":{},"cell_type":"markdown","source":"Vamos excluir a variável 'Cabin', visto que há uma grande quantidade de instâncias com valores vazios, dessa forma essa variável dificilmente apresenterá uma informação relevante.\nPara isso basta usar a função .drop(), com o parâmetro axis=1 digo que excluirei a coluna, já o inplace informa que o DataFrame será sobreescrito pelo resultado do .drop()"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_dt.drop(['Cabin'], axis=1, inplace=True)\n\ndata_dt.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A variável idade também tem um grande número de vazios. Entretanto, pelo histórico do naufrágio, sabemos que a idade dos passageiros pode ser uma informações importante. Por isso, não iremos descarta-lá, mas sim preenche-la com valores. Uma boa estratégia é usar a mediana da idade de todos os passageiros.\nA função .fillna() auxilia no preenchimento dos valores vazios pela mediana."},{"metadata":{"trusted":true},"cell_type":"code","source":"data_dt_proc = data_dt.copy()\n\nprint(\"Mediana de idade: {}\".format(data_dt_proc.Age.median()))\ndata_dt_proc.Age.fillna(data_dt_proc.Age.median(), inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_dt_proc.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_dt_proc.Fare.fillna(data_dt_proc.Fare.median(), inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_dt_proc.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Limpeza dos dados\n\nAlgumas variáveis tem valores unicos para cada passageiro, que podem não representar nenhuma informação relevante, por isso iremos removê-las também."},{"metadata":{"trusted":true},"cell_type":"code","source":"data_dt_proc.drop(['Ticket', 'PassengerId'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Criação de variáveis"},{"metadata":{},"cell_type":"markdown","source":"Muitas vezes podemos criar novas variáveis através de informações contidas em outras variáveis que aparentemente eram inúteis.\nUm exemplo disso é a variável 'Name', cada passageiro tem um nome único, porém acompanhado de um título ou pronome de tratamento.\nPodemos extrair esse pronome de tratamento da coluna 'Name' e utiliza-lo no treinamento de nosso modelo de Machine Learning."},{"metadata":{"trusted":true},"cell_type":"code","source":"data_dt_proc['Title'] = data_dt_proc['Name'].str.split(\", \", expand=True)[1].str.split(\".\", expand=True)[0]\n\ndata_dt_proc.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Com a função .value_counts() podemos ver a soma para cada valor único da nova coluna criada.\nComo podemos ver abaixo, existem diversas abreviações para o mesmo título. Podemos agrupar algumas dessas informações."},{"metadata":{"trusted":true},"cell_type":"code","source":"data_dt_proc['Title'].value_counts()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print( len(data_dt_proc['Title'].value_counts()) )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"O dicionário Title_Dictionary mostra um mapeamento que podemos aplicar à coluna 'Title' criada.\nVamos limitar os títulos à Officer, Royalty, Mrs, Miss, Mr e Master.\nA função .map() nos auxilia nesse processo."},{"metadata":{"trusted":true},"cell_type":"code","source":"Title_Dictionary = {\n    \"Capt\": \"Officer\",\n    \"Col\": \"Officer\",\n    \"Major\": \"Officer\",\n    \"Jonkheer\": \"Royalty\",\n    \"Don\": \"Royalty\",\n    \"Sir\" : \"Royalty\",\n    \"Dr\": \"Officer\",\n    \"Rev\": \"Officer\",\n    \"the Countess\":\"Royalty\",\n    \"Mme\": \"Mrs\",\n    \"Mlle\": \"Miss\",\n    \"Ms\": \"Mrs\",\n    \"Mr\" : \"Mr\",\n    \"Mrs\" : \"Mrs\",\n    \"Miss\" : \"Miss\",\n    \"Master\" : \"Master\",\n    \"Lady\" : \"Royalty\",\n    \"Dona\" : \"Mrs\"\n}\n\ndata_dt_proc['Title'] = data_dt_proc.Title.map(Title_Dictionary)\n\ndata_dt_proc['Title'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Também podemos criar uma variável relacionada ao tamanho da família, juntando duas variáveis: 'SibSp' (número de irmãos/conjuges) e 'Parch' (número de pais / filhos), formando a variável 'Family Size'."},{"metadata":{"trusted":true},"cell_type":"code","source":"data_dt_proc['FamilySize'] = data_dt_proc['Parch'] + data_dt_proc['SibSp'] + 1\ndata_dt_proc.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Voltando ao tratamento de valores vazios\n\nDurante o processo de tratamento de dados podemos voltar atrás e melhorar nosso processo.\nÉ possível melhorar o preenchimento de valores vazios da coluna 'Age', utilizar apenas a média parece uma forma muito simples de resolver o problema. Talvez identificar pequenos grupos e usar a média de cada um possa ser mais interessante.\nA chance de um passageiro com 'Age' vazio ter a mesma idade que um outro com as mesmas características é muito maior e real.\n\nPor isso, vamos tentar identificar pequenos grupos baseados em informações completas como sexo ('Sex'), classe no navio ('Pclass') e título ('Title'). A função .groupby() faz esse agrupamento."},{"metadata":{"trusted":true},"cell_type":"code","source":"grouped = data_dt_proc.groupby(['Sex', 'Pclass', 'Title'])\ngrouped.first()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Calculamos então a mediana de cada grupo.\nNa segunda linha de código é apenas feito um tratamento para exibir as informações que estamos analisando."},{"metadata":{"trusted":true},"cell_type":"code","source":"grouped_median = grouped.median()\ngrouped_median = grouped_median.reset_index()[['Sex', 'Pclass', 'Title', 'Age']]\n\ngrouped_median","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Vamos utilizar o DataFrame original para sobreescrever os valores vazios de idade que havíamos preenchido.\nPara isso criamos uma função que verifica se a linha processada tem o mesma valores que uma das regras, então aplica o valor de idade mediano.\nUsamos a função .apply() do DataFrame para isso. Essa função aplica uma outra função no DataFrame.\nO lambda nos ajuda a fazer linha a linha, sendo que o valor de idade será preenchido com a mediana do grupo Y para cada registro X, se a idade for vazia.\n\nPodemos ver um exemplo para o registro 65, anteriormente com a mediana ele havia recebido o valor 28.0, agora recebeu 7.0."},{"metadata":{"trusted":true},"cell_type":"code","source":"def fill_age(row, grouped_median):\n    condition = (\n        (grouped_median['Sex'] == row['Sex']) &\n        (grouped_median['Pclass'] == row['Pclass']) &\n        (grouped_median['Title'] == row['Title'])\n    )\n    return grouped_median[condition]['Age'].values[0]    \n\ndata_dt['Title'] = data_dt_proc['Title'] \n\n\nprint(\"Idade com mediana: {}\".format(data_dt_proc['Age'].iloc[65]))\n\ndata_dt_proc['Age'] = data_dt.apply(lambda row: fill_age(row,grouped_median) if np.isnan(row['Age']) else row['Age'], axis=1)\n\nprint(\"Mediana agrupada: {}\".format(data_dt_proc['Age'].iloc[65]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_dt[(data_dt['Title'] == \"Master\") & data_dt['Age'].isna() ]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Podemos ver na tabela abaixo com todos os registros com 'Title' == 'Master' que a mediana de 7 anos faz muito mais sentido."},{"metadata":{"trusted":true},"cell_type":"code","source":"data_dt[(data_dt['Title'] == \"Master\") ]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_dt.drop(['Title'], axis=1, inplace=True)\n\ndata_dt_proc.drop(['Name'], axis=1, inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Transformação de variáveis\n\nMuitas vezes é preciso transformar algumas variáveis, principalmente quando se tratam de variáveis categóricas.\nNo caso dessa base de dados, ainda temos como variáveis categóricas 'Sex', 'Embarked' e 'Title'.\nA melhor transformação para esse tipo de variável é o one hot encoding. Nessa transformação cada categoria se torna uma variável binária.\nPor exemplo, a categoria Embarked é quebrada em 3: Embarked_C, Embarked_Q e Embarked_S. Então todos os dados são transformados, se um passageiro embarcou em Southampton, sua nova classificação será para Pclass1, Embarked_C, Embarked_Q e Embarked_S, respectivamente, 0, 0 e 1."},{"metadata":{"trusted":true},"cell_type":"code","source":"data_dt_proc_pre_dummies = data_dt_proc.copy()\ndata_dt_proc = pd.get_dummies(data_dt_proc)\ndata_dt_proc.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_dt_proc['Pclass_1'] = pd.get_dummies(data_dt_proc['Pclass'])[1]\ndata_dt_proc['Pclass_2'] = pd.get_dummies(data_dt_proc['Pclass'])[2] \ndata_dt_proc['Pclass_3'] = pd.get_dummies(data_dt_proc['Pclass'])[3]\n\ndata_dt_proc.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Análise exploratória dos dados\n\nA análise dos dados também é uma etapa importante, pois permite que façamos a extração de informações importantes que podem nos auxiliar no treinamento do modelo. Informações como:\n\n* Qual métrica devo utilizar?\n* Qual o valor mínimo de acertos que o meu modelo deve ter?\n* Ainda há informações que pouca agregam no treinamento do meu modelo?\n* Posso criar mais variáveis para meu treinamento?\n* Devo usar todas as características (variáveis) no treinamento?\n\n<h3>Qual métrica devo utilizar?</h3>\n\nEm geral as métricas são baseadas na matriz de confusão.\n\n![](https://miro.medium.com/max/490/0*cBSeArnwoU_FRso7.gif)\n\nNessa caso o Kaggle já nós dá a informação: Acurácia!\n\n![](https://miro.medium.com/max/700/1*s7VB26Cfo1LdVZcLou-e0g.png)\n\n![](https://miro.medium.com/max/500/1*t1vf-ofJrJqtmam0KSn3EQ.png)\n\n* Acurácia: indica uma performance geral. Dentre todas as classificações, quantas estão certas.\n* Precisão: de todos os dados classificados como positivos, quantos realmente são positivos.\n* Recall: qual a porcentagem de dados classificados como positivos comparado com a quantidade real de positivos.\n* F1-Score: une precisão e recall em uma métrica geral.\n\n"},{"metadata":{},"cell_type":"markdown","source":"![](https://miro.medium.com/max/1346/1*s0aMRNsHq7A3bCA9gX_qXQ.png) "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import f1_score, accuracy_score, recall_score, precision_score\n\nx = np.concatenate([np.ones(25), np.ones(10), np.zeros(25), np.zeros(40)])\ny = np.concatenate( [np.ones(25), np.zeros(10), np.ones(25), np.zeros(40)])\n\nprint(\"Acurácia {:.2f}\".format(accuracy_score(y,x)))\nprint(\"Precisão {:.2f}\".format(precision_score(y,x)))\nprint(\"Recall {:.2f}\".format(recall_score(y,x)))\nprint(\"F1 {:.2f}\".format(f1_score(y,x)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_dt_proc.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nplt.figure(figsize=(10,10))\nsns.countplot(x='Survived',data=train_data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>Qual o valor mínimo de acertos que o meu modelo deve ter?</h3>\n\n* Classificador sem regra (Zero Rule): todas as saídas na label com mais ocorrências.\n* Classificador uma regra (One Rule): uso apenas uma das entradas (variáveis) para classificar a saída."},{"metadata":{"trusted":true},"cell_type":"code","source":"survived = train_data[\"Survived\"][:891]\nrate_survived = sum(survived)/len(survived) * 100\n\nprint(\"Classificador Zero Rule:\")\nprint(\"% de sobreviventes: {:.2f}\".format(rate_survived))\nprint(\"% de mortos: {:.2f}\".format(100-rate_survived))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Classificador One Rule:\")\n\n\nwomen = train_data[train_data.Sex == 'female'][\"Survived\"]\nrate_women = sum(women)/len(women)*100\n\nprint(\"% de mulheres que sobrevivem: {:.2f}\".format(rate_women))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>Ainda há informações que pouca agregam no treinamento do meu modelo?</h3>\n\nRetiramos todas!"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_dt_proc.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>Posso criar mais variáveis para meu treinamento?</h3>\n\nCriamos várias, mas com certeza ainda é possível manipular mais os dados. Mas por hora manteremos assim."},{"metadata":{},"cell_type":"markdown","source":"<h3>Devo usar todas as características no treinamento?</h3>\n\nNem sempre todas as variáveis ou características representam conhecimento valioso, muitas vezes algumas caracteristicas podem apresentar muito ruído o que acaba influenciando no desempenho do classificador. Por isso é necessário analisar a importancia de cada característica.\nUma forma de analisar isso é utilizando a correlação."},{"metadata":{"trusted":true},"cell_type":"code","source":"data_dt_proc_pre_dummies.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_dt_proc_pre_dummies['Sex'] = pd.factorize(data_dt_proc_pre_dummies['Sex'])[0]\ndata_dt_proc_pre_dummies['Embarked'] = pd.factorize(data_dt_proc_pre_dummies['Embarked'])[0]\ndata_dt_proc_pre_dummies['Title'] = pd.factorize(data_dt_proc_pre_dummies['Title'])[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,10))\nsns.heatmap(data_dt_proc_pre_dummies.corr(), annot=True, cmap=\"Blues\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(data_dt_proc.corr()['Survived'][:])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Treinamento do modelo </h1>"},{"metadata":{"trusted":true},"cell_type":"code","source":"y = train_data['Survived']\n\ndata_dt_proc.drop(['Survived'], axis=1, inplace=True)\n\nfeatures_all = [\"Pclass\", \"Age\", \"Pclass_1\",\"Pclass_2\",\"Pclass_3\", \"Fare\", \"Sex_female\",\"Sex_male\", \"SibSp\", \"Parch\", 'FamilySize', 'Embarked_C', 'Embarked_Q', 'Embarked_S', \n            'Title_Master', 'Title_Miss', 'Title_Mr', 'Title_Mrs', 'Title_Officer', 'Title_Royalty']\n\nfeatures_selected = [\"Pclass_1\",\"Pclass_2\",\"Pclass_3\", \"Sex_female\",\"Sex_male\", \"Fare\", 'Title_Master', 'Title_Miss', 'Title_Mr', 'Title_Mrs', 'Title_Officer', 'Title_Royalty']\n\nfeatures = [\"Pclass_1\",\"Pclass_2\",\"Pclass_3\", \"Sex_female\",\"Sex_male\", \"Fare\", \"Parch\", 'FamilySize', 'Embarked_C', 'Embarked_Q', 'Embarked_S']\n\n\ntrain_data_all = data_dt_proc[features_all][:891]\ntest_data_all = data_dt_proc[features_all][891:]\n\ntrain_data_selected = data_dt_proc[features_selected][:891]\ntest_data_selected = data_dt_proc[features_selected][891:]\n\ntrain_data_proc = data_dt_proc[features][:891]\ntest_data_proc = data_dt_proc[features][891:]\n\ny_test =  pd.read_csv(\"../input/titanic-leaked/titanic.csv\")\ny_test = y_test['Survived']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neural_network import MLPClassifier\n\nfrom sklearn.metrics import accuracy_score\n\ndef warn(*args, **kwargs):\n    pass\nimport warnings\nwarnings.warn = warn\n\ndef run_all_classifiers(x, y, x_test, y_test):\n    classifiers = {    \n        'knn': KNeighborsClassifier(1),\n        'svm': SVC(probability=True),\n        'decision_tree' : DecisionTreeClassifier(),\n        'random_forest' : RandomForestClassifier(n_estimators=50, max_depth=5, random_state=1),\n        'ada_boost' : AdaBoostClassifier(),\n        'gradient_boost' : GradientBoostingClassifier(),\n        'xgboost': XGBClassifier(),\n        'gaussian_nb' : GaussianNB(),\n        'linear_disc' : LinearDiscriminantAnalysis(),\n        'quadratic_disc' : QuadraticDiscriminantAnalysis(),\n        'log_regression' : LogisticRegression(verbose=False),\n        'mlp' : MLPClassifier(solver='adam', alpha=1e-3, hidden_layer_sizes=(12,1), random_state=1, max_iter=500,verbose=False)\n    }\n\n    best_acc = {'classifier' : None,\n                'accuracy' : 0.0}\n\n    all_results = {}\n\n\n    for clf in classifiers:\n        clf_model = classifiers[clf]\n        clf_model.fit(x, y)\n        y_pred = clf_model.predict(x_test)\n\n        all_results[clf] = y_pred\n\n        acc = accuracy_score(y_pred, y_test) * 100\n\n        if acc > best_acc['accuracy']:\n            best_acc['accuracy'] = acc\n            best_acc['classifier'] = clf\n\n        print(\"Acurácia de {}: {:.2f}%\".format(clf, acc))\n\n    print(\"\\nO melhor classificador foi {} com acurácia de {:.2f}%\".format(best_acc['classifier'], best_acc['accuracy']))\n    \n    return all_results, best_acc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Resultado com todas as características: \")\nall_results_all, best_acc_all = run_all_classifiers(train_data_all, y, test_data_all, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Resultado com as características selecionadas apenas via correlação: \")\nall_results_selected, best_acc_selected = run_all_classifiers(train_data_selected, y, test_data_selected, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Resultado com as características selecionadas manualmente: \")\nall_results_proc, best_acc_proc = run_all_classifiers(train_data_proc, y, test_data_proc, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"output = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': all_results_proc[best_acc_proc['classifier']] })\noutput.to_csv('my_submission.csv', index=False)\nprint(\"Arquivo de submissão gerado com sucesso!!\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1>Comparando com um classificador por genêro</h1>\n\nVamos comparar nosso resultado com o baseline que haviamos definido: classificador de uma regra - Todas as mulheres sobrevivem."},{"metadata":{"trusted":true},"cell_type":"code","source":"y_gender =  pd.read_csv(\"../input/titanic/gender_submission.csv\")\ny_gender = y_gender['Survived']\n\nacc_gender = round(accuracy_score(y_gender, y_test) * 100, 2)\n\nprint(\"Acurácia com Genero: {}\".format(acc_gender))\nprint(\"Diferença com proc de {:.2f}%\".format( best_acc_proc['accuracy'] - acc_gender ))\nprint(\"Diferença com selected de {:.2f}%\".format( best_acc_selected['accuracy'] - acc_gender ))\nprint(\"Diferença com all de {:.2f}%\".format( best_acc_all['accuracy'] - acc_gender ))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1>Opa, então quer dizer que o Gradient Boosting é o melhor?</h1>\n\n<h1 style='color: red'>NÃO!</h1>\n\n* 0.83253 com k-NN: https://www.kaggle.com/konstantinmasich/titanic-0-82-0-83/notebook\n* 0.85167 com WCG + XGBoost: https://www.kaggle.com/cdeotte/titanic-wcg-xgboost-0-84688 \n\n1.0 (100%) é impossível...\n\nManipular os dados é o segredo!\nO mesmo vale para texto e imagens!\n\n![](https://miro.medium.com/max/1018/1*umWjAXc8dY3aMdFGnor8QA.png)\n\n\n<h1>Faça você mesmo: Experimente com imagens</h1>\n    \nClassificação de flores: https://www.kaggle.com/c/tpu-getting-started\n\n* Cotas de GPU: 40h/semana\n* Cotas de TPU: 30h/semana\n\nNotebooks recomendados:\n* Primeira submissão com TPU: https://www.kaggle.com/ryanholbrook/create-your-first-submission\n* Melhorando com uma rede pré-treinada: https://www.kaggle.com/dimitreoliveira/flower-classification-with-tpus-eda-and-baseline\n* O poder do data augmentation: https://www.kaggle.com/cdeotte/rotation-augmentation-gpu-tpu-0-96\n\nZera todas as madrugadas de sexta para sabádo à meia-noite (UTC-0)"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}