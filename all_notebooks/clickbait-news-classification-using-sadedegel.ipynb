{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Introduction\n\n\n### In past couple years number of news outlets and users who consume news are skyrocketing. This has led several websites competing with each other for grabbing users attention. One of the common ways to this is called \"Clickbait\". Clickbait, a form of false advertisement, uses hyperlink text or a thumbnail link that is designed to attract attention and to entice users to follow that link and read, view, or listen to the linked piece of online content, with a defining characteristic of being deceptive, typically sensationalized or misleading. A \"teaser\" aims to exploit the \"curiosity gap\", providing just enough information to make readers of news websites curious, but not enough to satisfy their curiosity without clicking through to the linked content.(Wikipedia)\n\n### In this notebook we're going to analyze some Turkish news article titles to decide if they are clickbait or not. We're also going to build some models to build a classifier that can distinguish between actual news and what is likely to be clickbait. For this we will use \"SadedeGel\" library with it's Turkish news corpus trained on various sources.\n\n## Sadedegel:\n\n![logo](https://sadedegel.ai/dist/img/logo-2.png)"},{"metadata":{},"cell_type":"markdown","source":"###  [SadedeGel](https://github.com/GlobalMaksimum/sadedegel) is an open-source library developed during the NLP OpenHack organized by [Turkey Open Source Platform](https://www.turkiyeacikkaynakplatformu.com/). The library and its ecosystem is awarded 2nd prize in the hackathon and has been in development ever since. \n\n### \"Sadede Gel\" means \"Cut to the chase\" üèÉ‚Äç‚ôÇÔ∏è. The main idea was to perform extractive summarization of news over a chrome extension; however during development it extended to become a utility ecosystem with the building blocks, datasets, annotation tools, various tokenizers and summarizers for the task of extractive summarization. \n\n### Going forward with the development, it is now essential to accomodate other NLP tasks in Turkish. SadedeGel's building blocks in current stage are mature enough to consume and process Turkish news documents and output processed data for downstream tasks. Processed outpus may be separated sentences with an ML based SBD, tokens tokenized either by Transformers BERT-TR or a rule based tokenizer, TF-IDF vectors based on a vocabulary built on extensive Turkish news data, BERT embeddings from Turkish BERT model. Enhancements with Word2Vec, Doc2Vec, FastText and ELECTRA Turkish will be on following releases. \n\n### You can also check out the project's [MadeWithML page](https://madewithml.com/projects/2048/sadedegel-an-extraction-based-turkish-news-summarizer/)"},{"metadata":{},"cell_type":"markdown","source":"# Getting Things Ready and Loading the Data\n\n##### Here we install SadedeGel using pip installer, pretty easy!"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"# installing stadedegel package from pip\n\n!pip install sadedegel","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### We load usual stuff for NLP tasks also loading SadedeGel's building blocks."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# some basic tools\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import tqdm\n\n# loading sadedegel packages\n\nfrom sadedegel import Doc, Token\nfrom sadedegel.bblock.word_tokenizer_helper import puncts\nfrom sadedegel.bblock.util import tr_lower\n\n\n# some extra nlp packages\n\nimport nltk\nstop_word_list = nltk.corpus.stopwords.words('turkish')\nfrom collections import Counter, defaultdict\nfrom nltk.probability import FreqDist\nfrom wordcloud import WordCloud\n\n#\n\nimport random\nimport time\nimport itertools\n\n#\n\nfrom matplotlib.ticker import MaxNLocator\nimport matplotlib.gridspec as gridspec\nimport matplotlib.patches as mpatches\nplt.style.use('fivethirtyeight')\n\n#\n\nseed=42\n\nimport warnings\nwarnings.filterwarnings('ignore') ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/turkishnewstitle20000clickbaitclassified/20000_turkish_news_title.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Our data seems simple enough:\n\n- id: for identifiying specific observation row.\n- clickbait: 1 for clickbait, 0 for actual news titles.\n- site: Source of the news title.\n- title: Actual news title from various news sites."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### For this task we're not going to judge which site has more clickbait or not therefore we can include only 'title' and 'clickbait' features. It'll be enough for our classification task. There are some missing target samples in our data so we should drop them too..."},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df[['title','clickbait']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display(df.isna().sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# getting rid of nan rows\n\ndf.dropna(inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Meta Features\n\n#### In this part we'll analyse some basic meta features of our data. Like target distribution, character/word counts per title. Pretty simple stuff but can give us some insights..."},{"metadata":{},"cell_type":"markdown","source":"## Target\n\n#### Our target distribution looks nicely balanced which is good for classification tasks."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Displaying target distribution.\n\nfig, axes = plt.subplots(ncols=2, nrows=1, figsize=(18, 6), dpi=100)\nsns.countplot(df['clickbait'], ax=axes[0])\naxes[1].pie(df['clickbait'].value_counts(),\n            labels=['No Bait', 'Clickbait'],\n            autopct='%1.2f%%',\n            shadow=True,\n            explode=(0.05, 0),\n            startangle=60)\nfig.suptitle('Distribution of the Target', fontsize=24)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Character Counts\n\n### Here we observe:\n\n- Actual news titles are much longer than clickbait titles.\n- Actual news titles having more than 100 characters usually.\n- Meanwhile clickbait titles have median around 50."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating a new feature for the visualization.\n\ndf['Character Count'] = df['title'].apply(lambda x: len(str(x)))\n\n\ndef plot_dist3(df, feature, title):\n    # Creating a customized chart. and giving in figsize and everything.\n    fig = plt.figure(constrained_layout=True, figsize=(18, 8))\n    # Creating a grid of 3 cols and 3 rows.\n    grid = gridspec.GridSpec(ncols=3, nrows=3, figure=fig)\n\n    # Customizing the histogram grid.\n    ax1 = fig.add_subplot(grid[0, :2])\n    # Set the title.\n    ax1.set_title('Histogram')\n    # plot the histogram.\n    sns.distplot(df.loc[:, feature],\n                 hist=True,\n                 kde=True,\n                 ax=ax1,\n                 color='#e74c3c')\n    ax1.set(ylabel='Frequency')\n    ax1.xaxis.set_major_locator(MaxNLocator(nbins=20))\n\n    # Customizing the ecdf_plot.\n    ax2 = fig.add_subplot(grid[1, :2])\n    # Set the title.\n    ax2.set_title('Empirical CDF')\n    # Plotting the ecdf_Plot.\n    sns.distplot(df.loc[:, feature],\n                 ax=ax2,\n                 kde_kws={'cumulative': True},\n                 hist_kws={'cumulative': True},\n                 color='#e74c3c')\n    ax2.xaxis.set_major_locator(MaxNLocator(nbins=20))\n    ax2.set(ylabel='Cumulative Probability')\n\n    # Customizing the Box Plot.\n    ax3 = fig.add_subplot(grid[:, 2])\n    # Set title.\n    ax3.set_title('Box Plot')\n    # Plotting the box plot.\n    sns.boxplot(x=feature, data=df, orient='v', ax=ax3, color='#e74c3c')\n    ax3.yaxis.set_major_locator(MaxNLocator(nbins=25))\n\n    plt.suptitle(f'{title}', fontsize=24)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_dist3(df[df['clickbait'] == 0], 'Character Count',\n           'Characters Per \"Non Bait\" Title')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_dist3(df[df['clickbait'] == 1], 'Character Count',\n           'Characters Per \"Clickbait\" Title')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Word Counts\n\n### Here we observe:\n\n- It's pretty similar with character counts we seen before.\n- Bait titles having much less words in their sentences.\n- Meanwhile actual news titles are much richer in terms of word counts."},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_word_number_histogram(textno, textye):\n    \n    \"\"\"A function for comparing word counts\"\"\"\n\n    fig, axes = plt.subplots(figsize=(18, 6), sharey=True)\n    sns.kdeplot(textno.str.split().map(lambda x: len(x)),shade=True,color='#e74c3c')\n    sns.kdeplot(textye.str.split().map(lambda x: len(x)),shade=True)\n    \n    plt.xlabel('Word Count')\n    plt.ylabel('Frequency')\n    plt.legend(['Non-Bait','Bait'])\n    fig.suptitle('Words Per Title', fontsize=24, va='baseline')\n    \n    fig.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_word_number_histogram(df[df['clickbait'] == 0]['title'],\n                           df[df['clickbait'] == 1]['title'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Tokenization with Sadedegel\n\n#### SadedeGel's main building block is called 'Doc' it only needs a string format input and then turns it to SadedeGel object for future use.\n\n#### Here we loaded randomly selected news article title with SadedeGel, then tokenized sentences. You can see it's doing pretty good on Turkish syntax.\n\n#### You can choose various tokenizers built-in SadedeGel, we went with the default one for this example.\n\n#### Finally we tokenized whole dataset and stored them in corpuses for future analysis."},{"metadata":{"trusted":true},"cell_type":"code","source":"# loading sample title\n\ndocument = Doc(df.iloc[5003]['title'])\ndocument","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# tokenizing the sample using sadedegel tokenizer\n\nfor sentence in document:\n    print(sentence.tokens)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# tokenizing the clickbait data using sadedegel tokenizer\n\nbait = df[df.clickbait==1.0]['title']\nbait_corpus = []\n\nfor title in tqdm(bait):\n    d = Doc(title)\n    w = [i.tokens for i in d]\n    bait_corpus.append(list(itertools.chain.from_iterable(w)))\nbait_corpus=list(itertools.chain.from_iterable(bait_corpus))\nbait_corpus=[tr_lower(i) for i in bait_corpus]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# tokenizing the non-bait data using sadedegel tokenizer\n\nno_bait = df[df.clickbait!=1.0]['title']\nnb_corpus = []\n\nfor title in tqdm(no_bait):\n    d = Doc(title)\n    w = [i.tokens for i in d]\n    nb_corpus.append(list(itertools.chain.from_iterable(w)))\nnb_corpus=list(itertools.chain.from_iterable(nb_corpus))\nnb_corpus=[tr_lower(i) for i in nb_corpus]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Here we cleaned our corpus using SadedeGel's default puncts list and manually updated the list with some specific cases for this instance."},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\n# filtering out some tokens from bait texts for cleaner results\n\n# loading default puncts list from sadedegel and manually adding some specific terms to filter out\nspec = list(puncts)\nspec+=['‚Äô','‚Ä¶','‚Äò','bir','nin','nƒ±n','ƒ±n','in','den','dan','ten','tan','ye','ya','e','a','de','da','te','ta']\n\nfiltered_tokens = [token for token in bait_corpus if token not in stop_word_list]\nb_ht=[]\nfor i in filtered_tokens:\n    if i.startswith('#'):\n        b_ht.append(i)\n\n\nfiltered_tokens = [token for token in filtered_tokens if token not in b_ht]\nfiltered_tokens = [token for token in filtered_tokens if token not in spec]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\n# filtering out some tokens from non-bait texts for cleaner results\n\nfiltered_tokens_nb = [token for token in nb_corpus if token not in stop_word_list]\nnb_ht=[]\nfor i in filtered_tokens_nb:\n    if i.startswith('#'):\n        nb_ht.append(i)\n\nfiltered_tokens_nb = [token for token in filtered_tokens_nb if token not in spec]\nfiltered_tokens_nb = [token for token in filtered_tokens_nb if token not in b_ht]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# counting most common bait tokens\n\ncounter = Counter(filtered_tokens)\nmost = counter.most_common()\nx_b, y_b = [], []\nfor word, count in most[:20]:\n    x_b.append(word)\n    y_b.append(count)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# counting most common non-bait tokens\n\ncounter_nb = Counter(filtered_tokens_nb)\nmost_nb = counter_nb.most_common()\nx_nb, y_nb = [], []\nfor word, count in most_nb[:20]:\n    x_nb.append(word)\n    y_nb.append(count)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Most Common Words\n\n#### Here we can see there's huge difference between clickbait and actualy news titles in terms of word counts. You can easily see words like \"son, dakika\" are much more likely in clickbait titles. Where these words means something like \"Breaking News\" or \"Newsflash\"..."},{"metadata":{"trusted":true},"cell_type":"code","source":"# plotting most common tokens for bait/non_bait\n\nfig, ax = plt.subplots(1,2,figsize=(18, 6))\nsns.barplot(x=y_b, y=x_b, palette='plasma', ax=ax[1])\nsns.barplot(x=y_nb, y=x_nb, palette='plasma', ax=ax[0])\nax[0].set_title('Non_Bait')\nax[1].set_title('Bait')\nplt.suptitle('Word Counts')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# WordCloud for News Titles\n\n#### Again using our corpus which created by using SadedeGel tokenizers I wanted to visualize most common words using WordCloud. This package created by Andreas Mueller and it's pretty cool!"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def plot_wordcloud(text, title, title_size):\n    \"\"\" A function for creating wordcloud images \"\"\"\n    allwords = text\n    mostcommon = FreqDist(allwords).most_common(140)\n    wordcloud = WordCloud(\n        width=1200,\n        height=800,\n        background_color='black',\n        max_words=150,\n        scale=3,        \n        contour_width=0.1,\n        contour_color='grey',\n    ).generate(str(mostcommon))    \n\n    def grey_color_func(word,\n                        font_size,\n                        position,\n                        orientation,\n                        random_state=None,\n                        **kwargs):\n        # A definition for creating grey color shades.\n        return 'hsl(0, 0%%, %d%%)' % random.randint(60, 100)\n\n    fig = plt.figure(figsize=(18, 18), facecolor='white')\n    plt.imshow(wordcloud.recolor(color_func=grey_color_func, random_state=42),\n               interpolation='bilinear')\n    plt.axis('off')\n    plt.title(title,\n              fontdict={\n                  'size': title_size,\n                  'verticalalignment': 'bottom'\n              })\n    plt.tight_layout(pad=0)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_wordcloud(filtered_tokens_nb,\n               'Most Common Words in Non-Bait Titles',\n               title_size=30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_wordcloud(filtered_tokens,\n               'Most Common Words in Bait Titles',\n               title_size=30)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modelling\n\n#### Finally classification time! Here we going to build a model to predict given title's clickbait status."},{"metadata":{"trusted":true},"cell_type":"code","source":"# loading some packages for modelling\n\nfrom sklearn.model_selection import cross_validate, StratifiedKFold, cross_val_score, train_test_split\nimport lightgbm as lgb\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import confusion_matrix, f1_score, accuracy_score, plot_confusion_matrix\n\nfrom scipy.sparse import vstack, csr_matrix","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model With TFIDF Embeddings\n\n#### We're going to get tfidf embeddings, which is sparse representation for texts. Again we're going to use SadedeGel's built-in embedding extractors. It's pretty simple to use, we get Doc object for every news title in our data and then turn them into sparse matrix format using tfidf embeddings. Then we get average  embeddings for each sentence to get single represantation of the news title.\n\n#### In second part I just converted sparse matrix to dataframe format for showing what we did here. On x axis you can see the number of observations (number of titles) and on y axis you can see the vocabulary size which is 27744 for now..."},{"metadata":{"trusted":true},"cell_type":"code","source":"# getting tfidf embeddings using sadedegel library\n\nX_tf = df['title']\nX = []\n\nfor title in tqdm(X_tf):\n    d = Doc(title)\n    X.append(csr_matrix(d.tfidf_embeddings.mean(axis=0)))\n\n\nX = vstack(X)\nprint('Shape of Embeddings: ', X.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# converting sparse matrix to dataframe for explaining puposes (no use in training)\n\nsample=pd.DataFrame.sparse.from_spmatrix(X)\nsample.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# getting target values and train-test splitting for validation\n\ny = df['clickbait']\nX_train, X_val, y_train, y_val = train_test_split(X,y,stratify=y, test_size=0.2, random_state=seed)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Here we choose some basic classifiers to test them..."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Selecting some classifiers:\n\nlogreg = LogisticRegression(random_state=seed)\n\ndectree = DecisionTreeClassifier(random_state=seed)\n\nknclass = KNeighborsClassifier()\n\nlight = lgb.LGBMClassifier(random_state=seed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Setting 5 fold CV:\n\ncv = StratifiedKFold(5, shuffle=True, random_state=seed)\nclassifiers = [logreg,dectree, knclass, light]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def model_check(X, y, classifiers, cv):\n    \n    ''' A function for testing multiple classifiers and return several metrics. '''\n    \n    model_table = pd.DataFrame()\n\n    row_index = 0\n    for cls in classifiers:\n\n        MLA_name = cls.__class__.__name__\n        model_table.loc[row_index, 'Model Name'] = MLA_name\n        \n        cv_results = cross_validate(\n            cls,\n            X,\n            y,\n            cv=cv,\n            scoring=('accuracy','f1','roc_auc'),\n            return_train_score=True,\n            n_jobs=-1\n        )\n        model_table.loc[row_index, 'Train Roc/AUC Mean'] = cv_results[\n            'train_roc_auc'].mean()\n        model_table.loc[row_index, 'Test Roc/AUC Mean'] = cv_results[\n            'test_roc_auc'].mean()\n        model_table.loc[row_index, 'Test Roc/AUC Std'] = cv_results['test_roc_auc'].std()\n        model_table.loc[row_index, 'Train Accuracy Mean'] = cv_results[\n            'train_accuracy'].mean()\n        model_table.loc[row_index, 'Test Accuracy Mean'] = cv_results[\n            'test_accuracy'].mean()\n        model_table.loc[row_index, 'Test Acc Std'] = cv_results['test_accuracy'].std()\n        model_table.loc[row_index, 'Train F1 Mean'] = cv_results[\n            'train_f1'].mean()\n        model_table.loc[row_index, 'Test F1 Mean'] = cv_results[\n            'test_f1'].mean()\n        model_table.loc[row_index, 'Test F1 Std'] = cv_results['test_f1'].std()\n        model_table.loc[row_index, 'Time'] = cv_results['fit_time'].mean()\n\n        row_index += 1        \n\n    model_table.sort_values(by=['Test F1 Mean'],\n                            ascending=False,\n                            inplace=True)\n\n    return model_table","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Sadedegel TFIDF Results\n\n#### Alright! The results are here, lets take a look...\n\n#### They look pretty decent! I think checking f1 score on this case is more logical since we don't want many fp/fn's. Our top two f1 scorers are Logistic Regression and LGBM classifiers. But when you take a closer look you can see that LogisticRegression kinda overfitting meanwhile default LGBM looks much better, let's fit that on our train set and then test it on our validation set!"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nraw_models = model_check(X_train, y_train, classifiers, cv)\ndisplay(raw_models)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"light.fit(X_train, y_train)\ny_pred = light.predict(X_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Testing models on non-seen data\n\nprint('Accuracy:', accuracy_score(y_val, y_pred))\nprint('F1:', f1_score(y_val, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Validation looks good. Let's carry on with the confusion matrix so you can see true positive rate, false positive rate etc. easier..."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def conf_mat(X,y, classifiers):\n    \n    ''' A function for displaying confusion matrices'''\n    \n    fig, axes = plt.subplots(2,2, figsize=(12,8))\n    \n    axes = axes.flatten()\n\n    for ax, classifier in zip(axes, classifiers):\n        classifier.fit(X,y)\n        plot_confusion_matrix(classifier, X, y,\n                                         values_format = 'n',\n                                         display_labels = ['Non_Bait', 'Clickbait'],\n                                         cmap='summer_r',ax=ax)\n        ax.set_title(f'{classifier.__class__.__name__}')\n        ax.grid(False)\n        plt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"conf_mat(X_train, y_train, classifiers)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model Results Using Sklearn TFIDF Embeddings for Benchmarking\n\n#### I just wanted to check sklearn tfidf results with same models to compare it with SadedeGel's embeddings. When we check the results we can see that SadedeGel's tfidf embeddings worked better on Turkish texts almost on every classifier. Pretty cool!"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer()\nXi = vectorizer.fit_transform(df.title)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nXi_train, Xi_val, yi_train, yi_val = train_test_split(Xi,y,stratify=y, test_size=0.2, random_state=seed)\n\nraw_models = model_check(Xi_train, yi_train, classifiers, cv)\ndisplay(raw_models)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model With Sadedegel Bert Embeddings\n\n#### SadedeGel also comes with another embedding extractor which is getting popular in NLP tasks lately: 'BERT'. It's little bit slower than getting tfidf embeddings but it's much more stronger indicator for models to make predictions. For performance issues I just take 2000 random titles from our data and get BERT embeddings."},{"metadata":{"trusted":true},"cell_type":"code","source":"# randomly sampling 2000 observations\n\ndf = df.sample(2000)\n\nX_bt = df['title']\nX = []\n\n# getting embeddings\n\nfor title in tqdm(X_bt):\n    d = Doc(title)\n    X.append(csr_matrix(d.bert_embeddings.mean(axis=0)))\n\n\nX = vstack(X)\nprint('Shape of Embeddings: ', X.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = df['clickbait']\nXb_train, Xb_val, yb_train, yb_val = train_test_split(X,y,stratify=y, test_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"light.fit(Xb_train, yb_train)\nyb_pred = light.predict(Xb_val)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## BERT Embedding Results\n\n#### Oh nice! We only used 10% of the data we used for tfidf and almost got similar scores for our classifier. So if you want higher scores and have computing power you can go with SadedeGel's BERT embeddings!"},{"metadata":{"trusted":true},"cell_type":"code","source":"# bert results\n\nprint('Accuracy:', accuracy_score(yb_val, yb_pred))\nprint('F1:', f1_score(yb_val, yb_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"conf_mat(Xb_train, yb_train, classifiers)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Testing the Classifier on Randomly Selected News Titles Found on Web\n\n#### Here I gathered some titles from various news sites which posted recently. When we execute our classifier it gives us 1 for clickbait title, 0 for actual title. The results are looking promising for me, you can check them yourself too or add some other titles into 'titles' list to test them for yourself. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# some random titles from various news sites\n\ntitles = [\"√ñyle bir deƒüi≈üim ge√ßirdi ki\",\n          \"Grip a≈üƒ±sƒ±ndan bir hafta √∂nce az uyumak, a≈üƒ±nƒ±n etkisini y√ºzde 50 azaltƒ±yor\",\n          \"Son hali y√ºrek burkuyor\",\n          \"Covid-19 a≈üƒ±sƒ± bulundu\",          \n          \"Yunanistan, T√ºrkiye sƒ±nƒ±rƒ±nƒ±nda g√ºvenlik √∂nlemlerini artƒ±rƒ±yor: Duvar, kameralar ve daha √ßok sƒ±nƒ±r muhafƒ±zƒ±\",\n          \"Son Dakika | Mesut √ñzil'den Arsenal a√ßƒ±klamasƒ±!\",\n          \"Son dakika haberi: Azerbaycan ordusu Ermenistan'a aƒüƒ±r darbe vurdu! Bir tabur asker..\",\n          \"T√ºrkiye'nin 100 yƒ±llƒ±k enerjisini kar≈üƒ±layacak dev rezerv!\",\n          \"T√ºrkiye'nin en y√ºksek barajƒ±nda y√ºzde 87'lik fiziki ger√ßekle≈üme saƒülandƒ±\",\n          \"Son dakika haberi: D√ºnya bunu tartƒ±≈üƒ±yor: 'Uzun Kovid' kimleri vuruyor, uzmanlar a√ßƒ±kladƒ±!\",\n          \"Trump'ƒ±n vergi kayƒ±tlarƒ± √áin'le i≈ü baƒülantƒ±larƒ±nƒ± g√∂steriyor\"\n\n]\n\ntitle_embeds = []\nfor title in titles:\n    d = Doc(title)\n    title_embeds.append(csr_matrix(d.tfidf_embeddings.mean(axis=0)))\n\n\ntitle_embeds = vstack(title_embeds)\n\nlight.fit(X_train, y_train)\n\npreds = light.predict(title_embeds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d = {'title':titles, 'clickbait':preds}\n\npd.DataFrame(d)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Final Words\n\n### That concludes my notebook here. I wanted to present you 'SadedeGel' for Turkish text classification tasks and I'd say it did pretty good work in my first try. Hope you find it useful too.\n\n### Thanks for reading and happy coding all!"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}