{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Pre-processing for data mining\n## Introduction\nWith this notebook, you are provided an exported SQL file data_raw.sql, a record of news content from different sources.\n\nThe intial table had the following clumns: `ID`, `post_author`, `post_date`, `post_date_gmt`, `post_content`, `post_title`, `post_excerpt`, `post_status`, `comment_status`, `ping_status`, `post_password`, `post_name`, `to_ping`, `pinged`, `post_modified`, `post_modified_gmt`, `post_content_filtered`, `post_parent`, `guid`, `menu_order`, `post_type`, `post_mime_type`, `comment_count`\n\n\nBelow are two examples of the recorded data:\n- example 1:\n\n> (6189, 0, '2015-08-27 15:28:37', '2015-08-27 15:28:37', 'DHL  : amélioration de la logistique des transports (NewsMada)', 'DHL  : amélioration de la logistique des transports (NewsMada)', 'DHL  : amélioration de la logistique des transports (NewsMada)', 'inherit', 'open', 'closed', '', 'dhl-amelioration-de-la-logistique-des-transports-newsmada-2', '', '', '2015-08-27 15:28:37', '2015-08-27 15:28:37', '', 6188, 'http://example.com/wp-content/uploads/2015/08/DHL- -amélioration-de-la-logistique-des-transports-NewsMada.png', 0, 'attachment', 'image/png', 0)\n\n- example 2:\n\n\n> (6190, 1, '2015-08-26 09:19:57', '2015-08-26 09:19:57', ' [ad_1]\\r\\n<br><div id=\\\"\\\"><p style=\\\"text-align: justify;\\\">En collaboration avec la région Vakinankaratra, le Centre international de recherches agronomiques pour le développement (Cirad) et l’Institut international des sciences sociales (IISS), ayant trait à la prospective territoriale et locale, l’Agence française de développement (AFD) ont organisé récemment dans la ville d’Eaux un atelier sur la prospective territoriale participative. D’après le chef de région Mandrindra Andrianjanaka, l’objectif est d’expérimenter une nouvelle approche territoriale, en plus de l’approche sectorielle dont on avait l’habitude auparavant. 25 personnes disposant à titre personnel de connaissances complémentaires en la matière ont participé à cette rencontre.</p>\\n<p style=\\\"text-align: justify;\\\">Il s’agissait en l’occurrence d’intégrer les dynamiques démographiques dans les stratégies de développement en se projetant sur une période de 20 ans, plus exactement jusqu’en 2035. Une telle action permet ainsi d’avoir une vision globale de l’évolution future de la région Vakinankaratra et d’identifier les forces qui permettraient d’influencer son développement. Les cinq jours d’atelier ont ainsi fait ressortir différents scénarii se rapportant à la vision fixée  de 2035. Des résultats qui se veulent un outil de prise de décision pour le développement de la région.</p>\\n<p style=\\\"text-align: justify;\\\">La région Vakinankaratra est donc honorée d’avoir été choisie par l’AFD qui a déjà initié une recherche expérimentale sur plusieurs territoires ruraux d’Afrique du même genre, en promouvant une démarche participative basée sur l’implication des acteurs du territoire concerné. Mandrindra Andrianjanaka, dans son discours de clôture de l’atelier a souligné que les résolutions prises allaient être réellement prises en considération.</p>\\n<p style=\\\"text-align: right;\\\"><strong>Jeannot Ratsimbazafy</strong></p>\\n\\n<section id=\\\"text-5\\\" class=\\\"widget widget_text\\\"/><!-- #comments -->\t\t</div>\\r\\n<br>[ad_2]\\r\\n<br><a href=\\\"http://www.newsmada.com/2015/08/26/antsirabe-a-lheure-de-la-prospective-territoriale-participative/\\\">Source link </a>', 'Antsirabe : à l’heure de la prospective territoriale participative (NewsMada)', '', 'publish', 'open', 'open', '', 'antsirabe-a-lheure-de-la-prospective-territoriale-participative-newsmada', '', '', '2015-08-26 09:19:57', '2015-08-26 09:19:57', '', 0, 'http://example.com/antsirabe-a-lheure-de-la-prospective-territoriale-participative-newsmada/', 0, 'post', '', 0)\n\nIn this notebook, you will pre-process this data as part of a data mining pipeline. Your task will be completed when the required raw texts are extracted and classifed whether being in French or others.\n\n- **Preprocess** \n    - You'll extract the news content from author 1 as clean text (without HTML tags)\n    - You will create a new CSV file raw_data.csv that contains everything you collected from the previous step plus the post_date_gmt and the ```Source``` link. E.g., http://www.newsmada.com/2015/08/26/antsirabe-a-lheure-de-la-prospective-territoriale-participative in example 2. You will set missing values to None. You will also add a new column for the ```Source domain``` which is the root domain name of each source link.\n\n- **Models**\n    - You will a function process which accepts a string (text content) as input and returns a probability of content being in French.\n    - You will run process() on each clean news content from raw_data.csv.\n    - You will create a new CSV file data.csv, a copy of raw_data.csv with a new column value to specify the language being used represented by 1 when french is used and 0 overwise.\n\n- **Prediction**: \n    It is essential to note that news content containing only a few French words (names, etc.) should not be considered french news content.\n\n","metadata":{}},{"cell_type":"code","source":"# import module\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.naive_bayes import *\nfrom sklearn.metrics import *\n\nfrom io import StringIO\nimport re\n","metadata":{"execution":{"iopub.status.busy":"2021-06-11T14:31:08.641128Z","iopub.execute_input":"2021-06-11T14:31:08.641486Z","iopub.status.idle":"2021-06-11T14:31:09.415817Z","shell.execute_reply.started":"2021-06-11T14:31:08.641411Z","shell.execute_reply":"2021-06-11T14:31:09.414999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# TODO 1: Loading the data\n# 读取sql有三种方法：内置open、pymysql、pandas\nsql_file = \"/kaggle/input/news-with-french/data_raw.sql\"\ndef read_sql():\n    sql = open(sql_file, \"r\", encoding = \"utf8\")\n    sqltxt = sql.readlines()\n    sql.close()\n    print(sqltext[0]) #难以分\n# read_sql()\n# def read_sql_script_all(sql_file_path, quotechar=\"'\") -> (str, dict):\n\ndef read_sql_script_all(sql_file_path, quotechar=\"'\"):\n    insert_check=re.compile(\"insert +into +`?(\\w+?)`?\\(\", re.I|re.A)\n    with open(sql_file_path, encoding=\"utf-8\") as f:\n        sql_txt=f.read()\n    print(len(sql_txt))\n    end_pos = -1\n    df_dict = {}\n    while True:\n        match_obj = insert_check.search(sql_txt, end_pos+1)\n        print(match_obj)\n        if not match_obj: \n            break\n        table_name = match_obj.group(1)\n        start_pos = match_obj.span()[1]+1\n        end_pos = sql_txt.find(\";\", start_pos)\n        tmp = re.sub(r\"\\)( values |,)\\(\",\"\\n\",sql_txt[start_pos:end_pos])\n        tmp =re.sub(r\"[`()]\",\"\",tmp)\n        df=pd.read_csv(StringIO(tmp),quotechar=quotechar)\n        dfs=df_dict.setdefault(table_name,[])\n        dfs.append(df)\n    for table_name, dfs in df_dict.items():\n        df_dict[table_name]=pd.concat(dfs)\n    return df_dict\n\nread_sql_script_all(sql_file)","metadata":{"execution":{"iopub.status.busy":"2021-06-11T14:31:09.417182Z","iopub.execute_input":"2021-06-11T14:31:09.417517Z","iopub.status.idle":"2021-06-11T14:31:15.604121Z","shell.execute_reply.started":"2021-06-11T14:31:09.417482Z","shell.execute_reply":"2021-06-11T14:31:15.603353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# TODO 2: Generating the raw_data.csv","metadata":{"execution":{"iopub.status.busy":"2021-06-11T14:31:15.607124Z","iopub.execute_input":"2021-06-11T14:31:15.607369Z","iopub.status.idle":"2021-06-11T14:31:15.612536Z","shell.execute_reply.started":"2021-06-11T14:31:15.607344Z","shell.execute_reply":"2021-06-11T14:31:15.611769Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# TODO 3: Your model for language detection or identification\n- Some ideas about natural language processing [link](https://www.zhihu.com/question/356132676)","metadata":{}},{"cell_type":"code","source":"French_text = \"/kaggle/input/news-with-french/Paris et Londres en 1793.txt\"\nEnglish_text = \"/kaggle/input/news-with-french/A Tale of Two Cities.txt\"\nraw_french  = open(French_text,encoding='utf8').read()\nraw_english  = open(English_text,encoding='utf8').read()\nraw_english[:500]","metadata":{"execution":{"iopub.status.busy":"2021-06-11T14:31:15.614177Z","iopub.execute_input":"2021-06-11T14:31:15.61469Z","iopub.status.idle":"2021-06-11T14:31:15.657512Z","shell.execute_reply.started":"2021-06-11T14:31:15.614654Z","shell.execute_reply":"2021-06-11T14:31:15.656828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"为了解决分句的问题，我首先尝试基于ptyhon的NLTK套件来，发现默认的算法并不完善，或者说无法适应各种不同目的的需求，还是需要后续处理。比如NLTK、textstats等默认把无标点结尾而是回车符的行末统一替换为\\n，全部当作同一个句子，比如书籍前边的目录，这样的话就产生了一些不合理的超长句子。","metadata":{}},{"cell_type":"code","source":"#分词：https://zhuanlan.zhihu.com/p/242247311\ndef sentence_split_nltk(strs):\n#     import nltk\n#     nltk.download()\n\n    from nltk.tokenize import sent_tokenize\n    sent_tokenize_list = sent_tokenize(strs)\n    return sent_tokenize_list\n\ndef sentence_split(strs):\n    strs = strs.replace(\"!\",\".\").replace(\"?\",\".\").replace(\"\\t\",\" \").replace(\"\\n\",\" \")\n    return strs.split(\".\")\nsen_en = sentence_split(raw_english) #https://www.dtmao.cc/news_show_1850208.shtml\nsen_french = sentence_split(raw_french)\nx = sen_en + sen_french\ny = ['en']*len(sen_en) + ['fe']*len(sen_french)","metadata":{"execution":{"iopub.status.busy":"2021-06-11T14:31:15.659996Z","iopub.execute_input":"2021-06-11T14:31:15.660239Z","iopub.status.idle":"2021-06-11T14:31:15.673649Z","shell.execute_reply.started":"2021-06-11T14:31:15.660209Z","shell.execute_reply":"2021-06-11T14:31:15.67277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- [How to use CountVectorizer](https://blog.csdn.net/m0_37788308/article/details/80933915) 包括中文","metadata":{}},{"cell_type":"code","source":"def test_Count():\n    a =\"自然语言处理是计算机科学领域与人工智能领域中的一个重要方向。它研究能实现人与计算机之间用自然语言进行有效通信的各种理论和方法。自然语言处理是一门融语言学、计算机科学、数学于一体的科学\"\n    b = \"因此，这一领域的研究将涉及自然语言，即人们日常使用的语言，所以它与语言学的研究有着密切的联系，但又有重要的区别。自然语言处理并不是一般地研究自然语言，而在于研制能有效地实现自然语言通信的计算机系统，特别是其中的软件系统。\"\n    c =\"因而它是计算机科学的一部分。自然语言处理（NLP）是计算机科学，人工智能，语言学关注计算机和人类（自然）语言之间的相互作用的领域。\"\n    import jieba\n    all_list= ['  '.join(jieba.cut(s,cut_all = False)) for s in [a,b,c]]\n    # print((all_list)[0])\n    count_vec=CountVectorizer()\n    count_vec.fit_transform([a,b,c]).toarray()\n    print('\\nvocabulary list:\\n\\n',count_vec.get_feature_names())\n    print( '\\nvocabulary dic :\\n\\n',count_vec.vocabulary_)\n    print ('vocabulary:\\n\\n')\n    for key,value in count_vec.vocabulary_.items():\n        print(key,value)","metadata":{"execution":{"iopub.status.busy":"2021-06-11T14:31:15.676751Z","iopub.execute_input":"2021-06-11T14:31:15.677017Z","iopub.status.idle":"2021-06-11T14:31:15.684746Z","shell.execute_reply.started":"2021-06-11T14:31:15.676993Z","shell.execute_reply":"2021-06-11T14:31:15.683847Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### [The method of building the model](https://zhuanlan.zhihu.com/p/27447133)(traditional)","metadata":{}},{"cell_type":"code","source":"def remove_noise(document):\n    noise_pattern = re.compile(\"|\".join([\"http\\S+\", \"\\@\\w+\", \"\\#\\w+\"]))\n    clean_text = re.sub(noise_pattern, \"\", document)\n    return clean_text.strip()\nremove_noise(\"Trump images are now more popular than cat gifs. @trump #trends http://www.trumptrends.html\")","metadata":{"execution":{"iopub.status.busy":"2021-06-11T14:31:15.687503Z","iopub.execute_input":"2021-06-11T14:31:15.687785Z","iopub.status.idle":"2021-06-11T14:31:15.697023Z","shell.execute_reply.started":"2021-06-11T14:31:15.687759Z","shell.execute_reply":"2021-06-11T14:31:15.696075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"define model","metadata":{}},{"cell_type":"code","source":"bigram_count = CountVectorizer(ngram_range=(2,2),analyzer='char_wb',#\n    max_features=1000,  # keep the most common 1000 ngrams\n    preprocessor=remove_noise\n)\nbigram_count\n# https://www.dtmao.cc/news_show_1850208.shtml\npipline = Pipeline([(\"vectorzier\",bigram_count),(\"model\",MultinomialNB())])\npipline","metadata":{"execution":{"iopub.status.busy":"2021-06-11T14:31:15.699447Z","iopub.execute_input":"2021-06-11T14:31:15.699833Z","iopub.status.idle":"2021-06-11T14:31:15.717813Z","shell.execute_reply.started":"2021-06-11T14:31:15.699799Z","shell.execute_reply":"2021-06-11T14:31:15.71686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"train and test model","metadata":{}},{"cell_type":"code","source":"x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=1024)\npipline.fit(x_train,y_train)\ny_prd = pipline.predict(x_test)\nprint(confusion_matrix(y_test,y_prd))\nprint(classification_report(y_test,y_prd))\ndef predict_sentence(sentence:str):\n    y_prd = pipline.predict([sentence])\n    return y_prd[0]\ndef predict_article(article:str):\n    y_prd = list(pipline.predict(sentence_split(article)))\n#     print(y_prd)\n    c = max(y_prd,key=y_prd.count)\n    return c=='fe'\npredict_article(\"J'utilise souvent SQL dans mon travail, et il existe de nombreuses nuances et limitations ennuyeuses, mais en dernière analyse, c'est la pierre angulaire de l'industrie des données. Par conséquent, pour chaque travailleur dans le domaine des données, SQL est indispensable. La maîtrise de SQL est d'une grande importance.\")","metadata":{"execution":{"iopub.status.busy":"2021-06-11T14:31:15.719453Z","iopub.execute_input":"2021-06-11T14:31:15.719761Z","iopub.status.idle":"2021-06-11T14:31:17.251978Z","shell.execute_reply.started":"2021-06-11T14:31:15.719719Z","shell.execute_reply":"2021-06-11T14:31:17.250998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# TODO 4: Generating the data.csv","metadata":{"execution":{"iopub.status.busy":"2021-06-11T14:31:17.253362Z","iopub.execute_input":"2021-06-11T14:31:17.253705Z","iopub.status.idle":"2021-06-11T14:31:17.257258Z","shell.execute_reply.started":"2021-06-11T14:31:17.253669Z","shell.execute_reply":"2021-06-11T14:31:17.256401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/news-with-french/raw_data.csv\")\n# 两个清除空行方法np的isnan和df.dropna()\n# df = df[~np.isnan(df['clean_text'])] pandas的问题nan算为float\n# df['clean_text'][69].map(predict_article)\n# float报错通过找到报错项 得知[df['clean_text'][69]] nan是一个float，还不能用isnan等去去除//只能用pd的方法才能判断\ndf = df[~pd.isnull(df['clean_text'])] \ndf['Class'] = df['clean_text'].map(predict_article)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-11T14:31:17.258592Z","iopub.execute_input":"2021-06-11T14:31:17.259062Z","iopub.status.idle":"2021-06-11T14:34:47.140649Z","shell.execute_reply.started":"2021-06-11T14:31:17.259027Z","shell.execute_reply":"2021-06-11T14:34:47.139863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.to_csv(\"./data.csv\",index=None)","metadata":{"execution":{"iopub.status.busy":"2021-06-11T14:34:47.141954Z","iopub.execute_input":"2021-06-11T14:34:47.14227Z","iopub.status.idle":"2021-06-11T14:34:53.283317Z","shell.execute_reply.started":"2021-06-11T14:34:47.142236Z","shell.execute_reply":"2021-06-11T14:34:53.282329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Answer the following questions:\n\n- What is the total number of french news articles from the generated data.csv\n- How many French articles a month were published on average?\n- Make a visualization of the number of French articles published daily based on data.csv\n- Which period (lasting 15 days) had the most French articles publication?\n- Make a visualization that compares the daily article publications in French and Other languages (s) for the whole data in data.csv\n- How many unique source domains are there that in total?\n- How many unique source domains are writing content in French only.\n- Visualize the number of French articles published in the different source domains per year.","metadata":{}},{"cell_type":"markdown","source":"- What is the total number of french news articles from the generated data.csv","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"./data.csv\")\ndf_french = df[df['Class']==True]\nlen(df_french)","metadata":{"execution":{"iopub.status.busy":"2021-06-11T15:03:32.818584Z","iopub.execute_input":"2021-06-11T15:03:32.818945Z","iopub.status.idle":"2021-06-11T15:03:34.645808Z","shell.execute_reply.started":"2021-06-11T15:03:32.818891Z","shell.execute_reply":"2021-06-11T15:03:34.644881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- How many French articles a month were published on average?","metadata":{}},{"cell_type":"code","source":"# 1.parse datetime\nimport datetime as dt\nfrom datetime import datetime\n# df_french['post_data_gmt'] = df_french['post_data_gmt'].map(lambda x:x.strftime('%Y-%m-%d %H:%M:%S'))\ndef parse_ymd(s): #速度比datetime自带函数快\n    year_s, mon_s, day_s, hour_s, min_s, second_s= s.replace(\":\",\"-\").replace(\" \",\"-\").split('-')\n    return datetime(int(year_s), int(mon_s), int(day_s),int(hour_s),int(min_s),int(second_s))\ndf_french['post_data_gmt'] = df_french['post_data_gmt'].map(parse_ymd)\ndf_french = df_french.set_index('post_data_gmt')\n# 2.set a col to mark month\ndf_french['month'] = df_french.index.map(lambda x:x.month)\n# 3.get series and plot\nmonth_count = df_french.groupby('month').agg('count')['Class']\nmonth_count.plot(kind='bar')\nprint(f\"every month publish {int(month_count.mean())} on average\")","metadata":{"execution":{"iopub.status.busy":"2021-06-11T15:03:36.552888Z","iopub.execute_input":"2021-06-11T15:03:36.553242Z","iopub.status.idle":"2021-06-11T15:03:37.05297Z","shell.execute_reply.started":"2021-06-11T15:03:36.553213Z","shell.execute_reply":"2021-06-11T15:03:37.05193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Make a visualization of the number of French articles published daily based on data.csv","metadata":{}},{"cell_type":"code","source":"# set a col to mark day\ndf_french['day'] = df_french.index.map(lambda x:x.date)\n# get series and plot\nday_count = df_french.groupby('day').agg('count')['Class']\nday_count.plot()\nprint(f\"every day publish {int(day_count.mean())} on average\")","metadata":{"execution":{"iopub.status.busy":"2021-06-11T15:03:39.375372Z","iopub.execute_input":"2021-06-11T15:03:39.375705Z","iopub.status.idle":"2021-06-11T15:03:39.885087Z","shell.execute_reply.started":"2021-06-11T15:03:39.375674Z","shell.execute_reply":"2021-06-11T15:03:39.884009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Which period (lasting 15 days) had the most French articles publication?","metadata":{}},{"cell_type":"code","source":"# use between_time() to get a period\n# df_french.between_time(\"06:00\", \"22:00\") #只能是time所以不行\nmax_pub = 0\nmax_sday = df_french.index[0]\nfor i in df_french.index:\n#     days = len(df_french[i.strftime('%Y-%m-%d %H:%M:%S'):(i+dt.timedelta(days=15)).strftime('%Y-%m-%d %H:%M:%S')])\n    start = df_french.index.searchsorted(i)\n    end = df_french.index.searchsorted(i+dt.timedelta(days=15))\n    days = len(df_french[start:end])\n    if max_pub < days: \n        max_pub = days\n        max_sday = i\nprint(f\"between {max_sday} to {max_sday+dt.timedelta(days=15)} had published {max_pub} articles\")","metadata":{"execution":{"iopub.status.busy":"2021-06-11T15:03:42.530124Z","iopub.execute_input":"2021-06-11T15:03:42.530445Z","iopub.status.idle":"2021-06-11T15:03:55.879167Z","shell.execute_reply.started":"2021-06-11T15:03:42.530416Z","shell.execute_reply":"2021-06-11T15:03:55.878246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Make a visualization that compares the daily article publications in French and Other languages (s) for the whole data in data.csv","metadata":{}},{"cell_type":"code","source":"df_other = df[df['Class']==False]\ndf_other['post_data_gmt'] = df_other['post_data_gmt'].map(parse_ymd)\ndf_other = df_other.set_index('post_data_gmt')\ndf_other['day'] = df_other.index.map(lambda x:x.date)\n# get series and plot\nimport seaborn as sns; sns.set()\n%matplotlib inline\nday_count_other = df_other.groupby('day').agg('count')['Class']\nday_count.name = 'Other'\nday_count_other.name = 'French'","metadata":{"execution":{"iopub.status.busy":"2021-06-11T15:06:40.900177Z","iopub.execute_input":"2021-06-11T15:06:40.900488Z","iopub.status.idle":"2021-06-11T15:06:41.068158Z","shell.execute_reply.started":"2021-06-11T15:06:40.900459Z","shell.execute_reply":"2021-06-11T15:06:41.067155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dd=[day_count_other,day_count]\nax = sns.lineplot(data=dd)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-11T15:06:47.464482Z","iopub.execute_input":"2021-06-11T15:06:47.464818Z","iopub.status.idle":"2021-06-11T15:06:47.946643Z","shell.execute_reply.started":"2021-06-11T15:06:47.464786Z","shell.execute_reply":"2021-06-11T15:06:47.94586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}