{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom string import punctuation","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"  \n# Recommender function\ndef metadata_recommender(data,feature_names,search_name,n_recommendations=10,indexer='title'):\n    \n    # Basic data cleaning function for metadata-based recommender\n    def clean_data(x):\n        if isinstance(x, list):\n            return [str.lower(i.replace(\" \", \"\")) for i in x if not i.isdigit()]\n        else:\n            if isinstance(x, str):\n                return str.lower(x.replace(\" \", \"\"))\n            else:\n                return ''\n    \n    # Create recommender 'soup'\n    def create_soup(x):\n        soup = []\n        for feature in feature_names:\n            f = ''.join(x[feature])\n            soup.append(f)\n        return ' '.join(soup)\n     \n    # clean data iteratively\n    for feature in feature_names:\n        data[feature] = data[feature].apply(clean_data)\n    \n    # define the soup\n    data['soup'] = data.apply(create_soup,axis=1)   \n    count_vec = CountVectorizer()\n    # BOW and similarity matrix\n    count_matrix = count_vec.fit_transform(data['soup'])\n    sim_matrix = cosine_similarity(count_matrix,count_matrix)\n    \n    # mapping for the results\n    data = data.reset_index()\n    mapping = pd.Series(data.index, index=data[indexer])\n    \n    # get n recommendations\n    def extended_recommender():\n        index = mapping[search_name]\n        similarity_score = list(enumerate(sim_matrix[index]))\n        try:\n            similarity_score = sorted(similarity_score, key=lambda x: x[1],reverse=True)\n        except:\n            similarity_score = sorted(similarity_score, key=lambda x: x[0],reverse=True)\n        similarity_score = similarity_score[1:n_recommendations]\n        indices = [i[0] for i in similarity_score]\n        return data[indexer].iloc[indices]\n    \n    return extended_recommender()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/disney-plus-shows/disney_plus_shows.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"metadata_recommender(df,['plot','genre','genre','director','writer'],'Coco')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import sqlite3\n\ncnx = sqlite3.connect('../input/pitchfork-data/database.sqlite')\n\nartists = pd.read_sql_query(\"SELECT * FROM artists\", cnx)\ncontent = pd.read_sql_query(\"SELECT * FROM content\", cnx)\ngenres = pd.read_sql_query(\"SELECT * FROM genres\", cnx)\nlabels = pd.read_sql_query(\"SELECT * FROM labels\", cnx)\nreviews = pd.read_sql_query(\"SELECT * FROM reviews\", cnx)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"merged = pd.merge(reviews,content,on='reviewid').merge(genres,on='reviewid').merge(labels,on='reviewid')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"metadata_recommender(merged,['content','genre','label'],'mezzanine')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}