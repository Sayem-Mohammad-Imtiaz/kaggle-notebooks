{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"'''\nDataset : Craigslist Used Car Dataset\n\nTeam Members :    1] PES2201800116 : Aniketh D Urs \n                  2] PES2201800656 : Manoj Mahesh Patil\n                  3] PES2201800480 : Purushotham S\n                  4] PES2201800646 : Mahammad Thufail\n\nProblem Statement : To predict the price of the used car using Prediction Models that we create.\n\nIntroduction : When we want to sell used cars, one of the biggest problems is deciding reasonable selling prices for the cars. \nAn effective way to solve this problem is to use a machine-learning model that can predict car prices.\n'''\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"collapsed":true},"cell_type":"code","source":"'''Exploratory Data Analysis and Data Cleaning'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#First we import Pandas \nimport pandas as pd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Next we import the dataset \ndf_original = pd.read_csv('../input/craigslist-carstrucks-data/vehicles.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Now , let's look at the contents of our dataset \nimport numpy as np\n\ndf = df_original.copy()\n\ndf.iloc[np.r_[0:3, -3:0]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"'''Cleaning of Dataset'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"irrelevant_cols = ['id', 'url', 'region_url', 'vin', 'image_url', \\\n                   'description', 'county']\n\ndf = df.drop(columns=irrelevant_cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The \"Price\" column is our target column , so let's move it to the last of the dataset for convinience.\ncol_list = ['price']\n\nrearranged_cols = np.hstack((df.columns.difference(col_list, sort=False), col_list))\n\ndf = df.reindex(columns=rearranged_cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now we will make sure that the string values in the dataset are in lower case and there should be no spaces in between\nfor column in df.columns[1:]:\n    if df[column].dtype == 'object':\n        df[column] = df[column].str.lower().str.strip()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# As you can see above that some of the columns have been removed and the price column has been moved to the end of the dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now let us visualize these NULL values using graphs ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\nheat_map = sns.heatmap(df.isnull(), yticklabels=False, cbar=True, cmap='Blues')\n_ = heat_map.set_xticklabels(heat_map.get_xticklabels(), color='#6eafd7')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"'''\nMissing values can lead to errors in machine-learning models. To avoid these errors, we can use the following workarounds:\n         1] Remove selected rows that contain missing values.\n         2] Replace missing values with estimates by using scikit-learn imputers.\n\nAs we want our algorithms to be accurate, we must retain as much of the car data as possible.\nThis means that we will have to impute many of the missing values.\nAt the same time, we want to minimize instances of incorrect data.\nSo, we will delete selected rows as well.\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"'''What are Extra Tree Regressor and Bayesian Ridge ?\n\nExtratreesregressor divides a target dataset into smaller subsets. \nThen, it uses multiple decision trees, or extra trees, on the subsets to determine how various attributes of the dataset interrelate.\nIt combines the findings of the trees to generate an average value for each null field.\n\nUnlike Extratreesregressor, BayesianRidge uses linear regression to determine relationships between variables. \nBased on these relationships, it generates regularized values for non-null fields.'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now we import necessary modules for ExtraTreeRegressor and Bayesian Ridge\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.experimental import enable_iterative_imputer\n\nfrom sklearn import preprocessing\nfrom sklearn.impute import IterativeImputer\nfrom sklearn.linear_model import BayesianRidge\nfrom sklearn.ensemble import ExtraTreesRegressor\n\nimputers = [\n    BayesianRidge(),\n    ExtraTreesRegressor(n_estimators=10, random_state=0),\n]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now we divide the columns into 2 groups , i.e categorical and numerical \nfrom collections import Counter\n\nnumerical = ['year', 'odometer', 'lat', 'long']\n\ncategorical = list((Counter(df.columns) -\\\n                    Counter(numerical + ['manufacturer', 'model', 'price'])).elements())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# First, we will use Extratreesregressor to fill out the null fields of the numerical columns.\nsr_numerical = df[numerical]\nimp_numerical = IterativeImputer(imputers[1])\nimputed_vals = imp_numerical.fit_transform(sr_numerical)\ndf[numerical] = imputed_vals","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The numerical columns have no NULL vaues in them now \ndf.isnull().sum()[numerical]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"''' Now we will use Bayesian Ridge to remove NULL values in categorical column .\nBut the algorithm cannot understand the data as it is in string format . \nSo we have to encode it .\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def encode(data_col):\n    #A function that transforms non-null values\n    vals = np.array(data_col.dropna())\n    # Reshaping the non-null data of a column\n    reshaped_data = vals.reshape(-1,1)\n    # Encoding the reshaped data\n    encoded_data = encoder.fit_transform(reshaped_data)\n    # Assigning the encoded values to the corresponding column values\n    data_col.loc[data_col.notnull()] = np.squeeze(encoded_data)\n    return data_col","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now let us use the encode function\nsr_categorical = df[categorical]\nencoder = preprocessing.LabelEncoder()\n\n# Using a for loop to iterate through each categorical column and\n# filling out its null fields\nfor column in categorical:\n    encode(sr_categorical[column])\n    imp_categorical = IterativeImputer(BayesianRidge())\n    imputed_vals_cat = imp_categorical.fit_transform(sr_categorical[column].values.reshape(-1, 1))\n    imputed_vals_cat = imputed_vals_cat.astype('int64')\n    imputed_vals_cat = pd.DataFrame(imputed_vals_cat)\n    imputed_vals_cat = encoder.inverse_transform(imputed_vals_cat.values.reshape(-1, 1))\n    sr_categorical[column] = imputed_vals_cat\n\ndf[categorical]= sr_categorical","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We have successfully removed the null values in the categorical group also.\ndf.isnull().sum()[categorical]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now let us take a peek at our dataset \ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.loc[:, ['region', 'manufacturer', 'model']]\\\n[df.model.str.startswith(r'$500', na=False)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let us see how many unique values our dataset contains \ndf.apply(pd.Series.nunique)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The dataset is fairly clean now. Let us save it as a CSV file.\ndf.to_csv('vehicles_eda.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"'''Visualization'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We will be using  Seaborn Displot for plotting graphs .\n# Remember our Target Variable is : \"Price\"\nsns.set(color_codes=True)\nsns.set(rc={'figure.figsize':(6,3)})\n\ndef plot_histogram(col, color_val='#005c9d',\\\n                   x_label='Price [x10\\u2076 USD]', y_label='Frequency',\\\n                   title_text='Distribution of car prices'):\n    sns.distplot(col, kde=False, color=color_val)\n    \n    ax = plt.gca()\n    ax.set_xlabel(x_label)\n    ax.set_ylabel(y_label)\n    ax.set_title(title_text)\n    ax.get_xaxis().get_major_formatter().set_scientific(False)\n    ax.get_yaxis().get_major_formatter().set_scientific(False)\n\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We will also import the CSV file that we had saved in the previous step.\ndf = pd.read_csv('vehicles_eda.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now we plot histogram \nprice_mill = df.price/10**6\nplot_histogram(price_mill)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The above graph shows that the maximum number of Price values are around zero.\n# This is because we have taken the range wrong .\n# So we scale the Price range between USD 0 - 60,000 range ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_histogram(df.price[df.price<60000])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Apparently a large number of Prices are around 0 . \n# We calculate the mean , median \nprint('Mean:', df.price.mean())\nprint()\nprint('Median: ', df.price.median())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Max. price: ', df.price.max())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The min value is 0 , whereas the max value , as you can see above is huge !\n# Why is this ?\n# This is because of the presence of outliers.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's look at some attributes that are of higher values \ncols = ['region', 'year', 'manufacturer', 'model', 'price']\n\ndf.loc[:, cols][df.price>100000].sort_values(by='price', ascending=False).head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# As you can see above , the prices of the cars is in Billions , which is impossible.\n# To remove these outliers we use Inter Quartile Range (IQR)\n# But to apply IQR , the data should be uniform\n# But our target columne i.e Price columns is not uniform \n# So we use Logarithmic Function to bring uniformity and add a modified version of the Price column.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.insert(17, 'logprice', np.log1p(df['price']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# To check whether the column is uniform or not , we plot the graph for the new column i.e \"logprice\"\nplot_histogram(df.logprice) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now the data is uniform.\n# We can now remove the outlier using IQR .\n# But before that we convert the \"Price\" attribute to string , so that IQR is not applied on that , later we will once again bring it \n# to int64 format.\ndf['price'] = df.price.astype(str)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now , we will apply IQR \nQ1 = df.quantile(0.25)\nQ3 = df.quantile(0.75)\nIQR = Q3 - Q1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Q1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Q3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"IQR","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now we update the dataset , by removing the outliers using IQR \ndf = df[~((df < (Q1 - 1.5 * IQR)) | (df > (Q3 + 1.5 * IQR))).any(axis=1)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# As said earlier , we once again bring back the data type of the \"Price\" attribute to int64 format\ndf['price'] = df.price.astype(np.int64)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# As the dataset is quite clean now , we will delete all those car models that appear less than 1000 times.\n# This step will dramatically reduce the chance of unrealistic model names and manufacturer-model combinations appearing in our dataset. \n# It will also ensure that the proposed machine-learning models have enough relevant data to understand the interrelations between car attributes or characteristics and their prices.\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.groupby(\"model\").filter(lambda x: len(x) >= 1000)\ndf.reset_index(drop=True, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Next , we will fill the missing values of manufacturer column with the mode of that column i.e most occuring value.\ndf['manufacturer'] = df.groupby('model').manufacturer.transform(\n    lambda x: x.fillna(x.mode()[0])\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Next, let us sort the dataset and browse through some of its rows.\ndf.sort_values(by=['year','manufacturer', 'price'], inplace=True)\ndf.reset_index(drop=True, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.iloc[np.r_[0:3, -3:0]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The structure and standard summary statistics of the updated dataset are as follows: \ndf.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Mean , Median , Mode of the Target Variable \nprint('Mean: ', round(df.price.mean()))\nprint()\nprint('Median: ', round(df.price.median()))\nprint()\nprint('Mode: ', df.price.mode()[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The mean > median > mode \n# i.e most of the data is on the lower side \nplot_histogram(df.price)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now , plotting graph \nplt.figure(figsize=(10, 4))\nplt.xticks(rotation=90)\nsns.countplot(df.manufacturer);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Next, let us look at the counts of some of the other categorical variables.\ncateg_x = categorical.copy()\ncateg_x.remove('region')\ncateg_x.remove('state')\n\nfig, ax = plt.subplots(3, 3, figsize=(20, 15))\nfor variable, subplot in zip(categ_x, ax.flatten()):\n    sns.countplot(df[variable], ax=subplot)\n    for label in subplot.get_xticklabels():\n        label.set_rotation(90)\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Next, let us look at how prices are interrelated with various categorical variables.\nplt.figure(figsize=(10, 4))\nplt.xticks(rotation=90)\nsns.barplot(x='manufacturer', y='price', data=df);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# As the graph above indicates, Ram Trucks lead on the price front.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(3, 3, figsize=(20, 15))\nfor var, subplot in zip(categ_x, ax.flatten()):\n    sns.barplot(x=var, y='price', data=df, ax=subplot)\n    for label in subplot.get_xticklabels():\n        label.set_rotation(90)\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The graphs above show that , new cars, diesel cars, cars that have liens on them, \n#and cars with four-wheel drives have higher average prices than other types of cars in their respective categories.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let us also look at how prices vary with year of manufacture\nyear = df.year.astype(np.int64)\nprice = df.price\nplt.figure(figsize=(10, 4))\nplt.xticks(rotation=90)\nsns.barplot(year, price);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The above graph shows that prices increase fairly consistently with year.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now let us plot graphs with 3 variables \nfactor_combos = [('fuel', 'condition'), ('condition', 'size'),\\\n                 ('fuel', 'cylinders'), ('transmission', 'size'),\\\n                 ('size', 'drive'), ('drive', 'size')]\nfig, ax = plt.subplots(3, 2, figsize=(20, 15))\nfor var, subplot in zip(factor_combos, ax.flatten()):\n    sns.barplot(x=var[0], y='price', hue=var[1], data=df, ax=subplot)\n    for label in subplot.get_xticklabels():\n        label.set_rotation(90)\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# So, all the car characteristics in our dataset have some impact on the target variable, price.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Linear Regression model building and Prediction","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error as MSE\nfrom sklearn.model_selection import train_test_split as split\nimport warnings\nfrom sys import modules","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vehicles_df_to_learn = df[['odometer','year','price']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vehicles_df_to_learn = df.dropna()\nvehicles_df_to_learn.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# we split the data into \"train\" - 75% and \"test\"- 25%\nvehicles_df_train, vehicles_df_test = split(vehicles_df_to_learn, train_size=0.75, random_state=4222)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = vehicles_df_train[['odometer','year']]\ny_train = vehicles_df_train['price']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cars_lm = LinearRegression(fit_intercept=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"cars_lm.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"The model intercept is: {}\".format(cars_lm.intercept_))\nprint(\"The model coefficients are: {}\".format(cars_lm.coef_[0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train['Price_prediction'] = cars_lm.predict(X_train)\nX_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cars_train_rmse = np.sqrt(MSE(y_train, X_train['Price_prediction']))\nprint(\"RMSE = {:.2f}\".format(cars_train_rmse))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cars_lm_test = LinearRegression()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test = vehicles_df_test[['odometer','year']]\ny_test = vehicles_df_test['price']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cars_lm_test.fit(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test['price_prediction'] = cars_lm_test.predict(X_test)\nX_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cars_test_rmse = np.sqrt(MSE(y_test, X_test['price_prediction']))\nprint(\"RMSE = {:.2f}\".format(cars_test_rmse))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"'''As you can see , the RMSE value of Train and Test are amlmost same . The accuracy of our project is high.'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}