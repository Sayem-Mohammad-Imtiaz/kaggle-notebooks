{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## MNIST Dataset\n\nThe Modified National Institute of Standards and Technology ([MNIST](https://en.wikipedia.org/wiki/MNIST_database)) database is a large database of handwritten digits. It was created by mixing samples taken from American Sensor board employees and those taken from high school students.","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import necessary libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.utils import to_categorical","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load dataset\n# The path needs to be updated with the path to the data files\ndf_train = pd.read_csv('../input/mnist-in-csv/mnist_train.csv')\ndf_test = pd.read_csv('../input/mnist-in-csv/mnist_test.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The training and test dataset consists of 60,000 and 10,000 samples of handwritten digits in grayscale images, respectively. Each image has been scaled to 28 x 28 pixels. Each pixel has values in the range [0,255]. The pixels of the images have been flattened into an array of size 28 x 28 = 784. Each image in the training set has been labelled as [0-9].","metadata":{}},{"cell_type":"code","source":"print(df_train.shape)\nprint(df_test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Visualization\nLet us plot the first few images of the training dataset along with their labels. In order to do that, we need to reshape the images from 784 into 28 x 28 pixels.","metadata":{}},{"cell_type":"code","source":"train_images = np.reshape(df_train.drop(columns='label').values,(60000,28,28))\ntest_images  = np.reshape(df_test.drop(columns='label').values,(10000,28,28))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10,10))\nfor i in range (25):\n  plt.subplot(5,5,i+1)\n  plt.imshow(train_images[i,:,:])\n  plt.axis('off')\nplt.subplots_adjust(wspace=0.0, hspace=0.1) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Prepare X_Train and Y_Train for model fitting. Re-scale the pixel values from [0:255] to [0:1].","metadata":{}},{"cell_type":"code","source":"X_Train = df_train.drop(columns='label').values/255\nX_Test  = df_test.drop(columns='label').values/255\nY_Train = df_train['label'].values\nY_Test  = df_test['label'].values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Building and Compilation\n\nNow let us build the model using a feed-forward neural network consisting of 3 layers:\n* Input layer: 784 neurons corresponding to the 28 x 28 pixels\n* Second layer: hidden layer consisting of 64 neurons\n* Output layer: 10 nodes corresponding to the digits [0-9]\n\nWe would be using the Rectified Linear Units (ReLU) activation for the first two layers. For the output layer we would be using the 'softmax' activation the convert the outputs into categorical probabilities.\n\nThe model is compiled with the 'Adam' optimizer with accuracy serving as the metrics of the fitting. The categorical cross-entropy function, that is suitable for multi-class classification problems calculates the loss function. ","metadata":{}},{"cell_type":"code","source":"# Build the model\n# 3 layers, 2 layers with 64 neurons + ReLu activation function\n# l layer with 10 neuron and softmax function (Maximum entropy)\nmodel = Sequential()\nmodel.add(Dense(64,activation='relu',input_dim = 784))\nmodel.add(Dense(64,activation='relu'))\nmodel.add(Dense(10,activation='softmax'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Complie the model\nmodel.compile(\n    optimizer = 'adam',\n    loss = 'categorical_crossentropy',\n    metrics = ['accuracy']\n    )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train the model\nX_Train_fit = model.fit(\n    X_Train,\n    to_categorical(Y_Train), # Ex. 2 -> [0,0,1,0,0,0,0,0,0,0]\n    epochs = 10,\n    batch_size = 50\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training Performance","metadata":{}},{"cell_type":"code","source":"# Plot the training performance\nplt.figure(figsize=(12,4))\nplt.suptitle('Training Performance')\n\nplt.subplot(121)\nplt.plot(X_Train_fit.epoch,X_Train_fit.history['accuracy'])\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\n\nplt.subplot(122)\nplt.plot(X_Train_fit.epoch,X_Train_fit.history['loss'])\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Evaluation","metadata":{}},{"cell_type":"code","source":"# Evaluate the model\nmodel.evaluate(\n    X_Test,\n    to_categorical(Y_Test)\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The accuracy of the test set is slightly lower than that of the training set (the difference being by ~ 2%). It is possible that this can e due to overfitting of the traiing data. It might partition the training set into a validation set to further minimize this difference.","metadata":{}},{"cell_type":"markdown","source":"## Result Visualization\nLet us visualize a random set of test images along with the predicted classifications.","metadata":{}},{"cell_type":"code","source":"indices = np.random.randint(10000,size=25)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# predict on the first 5 test images\npredictions = model.predict(X_Test[indices,:])\n# Print model predictions\nprint(np.argmax(predictions, axis = 1))\nprint(Y_Test[indices])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10,10))\nj = 0\nfor i in indices:\n  plt.subplot(5,5,j+1)\n  plt.imshow(test_images[i,:,:])\n  plt.axis('off')\n  plt.title(str(np.argmax(predictions, axis = 1)[j]))\n  j = j+1\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}