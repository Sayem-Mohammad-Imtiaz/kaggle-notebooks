{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"#important libraries\nimport pandas as pd\nimport numpy as np\nimport nltk\nimport re\n#importing stopwords is optional, in this case it decreased accuracy\n#from nltk.corpus import stopwords\nimport itertools\nimport time","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start_time = time.time()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/text-emotion/text_emotion.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.stem.wordnet import WordNetLemmatizer \nlem = WordNetLemmatizer()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#comprehensive cleaning\ndef cleaning(text):\n    txt = str(text)\n    txt = re.sub(r\"http\\S+\", \"\", txt)\n    if len(txt) == 0:\n        return 'no text'\n    else:\n        txt = txt.split()\n        index = 0\n        for j in tqdm(range(len(txt))):\n            if txt[j][0] == '@':\n                index = j\n        txt = np.delete(txt, index)\n        if len(txt) == 0:\n            return 'no text'\n        else:\n            words = txt[0]\n            for k in range(len(txt)-1):\n                words+= \" \" + txt[k+1]\n            txt = words\n            txt = re.sub(r'[^\\w]', ' ', txt)\n            if len(txt) == 0:\n                return 'no text'\n            else:\n                txt = ''.join(''.join(s)[:2] for _, s in itertools.groupby(txt))\n                txt = txt.replace(\"'\", \"\")\n                txt = nltk.tokenize.word_tokenize(txt)\n                #data.content[i] = [w for w in data.content[i] if not w in stopset]\n                for j in range(len(txt)):\n                    txt[j] = lem.lemmatize(txt[j], \"v\")\n                if len(txt) == 0:\n                    return 'no text'\n                else:\n                    return txt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['content'] = data['content'].map(lambda x: cleaning(x))\nfrom tqdm import tqdm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(data['content'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data.reset_index(drop=True)\nfor i in tqdm(range(len(data))):\n    words = data.content[i][0]\n    for j in range(len(data.content[i])-1):\n        words+= ' ' + data.content[i][j+1]\n    data.content[i] = words","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(data.content)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics import classification_report\nfrom sklearn import svm\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train, x_test, y_train, y_test = train_test_split(data.content, data.sentiment, test_size=0.25, random_state=0)\n\nx_train = x_train.reset_index(drop = True)\nx_test = x_test.reset_index(drop = True)\n\ny_train = y_train.reset_index(drop = True)\ny_test = y_test.reset_index(drop = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vectorizer = TfidfVectorizer(min_df=3, max_df=0.9)\n\ntrain_vectors = vectorizer.fit_transform(x_train)\ntest_vectors = vectorizer.transform(x_test)\n\nmodel = svm.SVC(kernel='linear') \nmodel.fit(train_vectors, y_train) \npredicted_sentiment = model.predict(test_vectors)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_content = \"shut up\"\ntest_content = [test_content]\ntest_vector = vectorizer.transform(test_content)\npredicted =  model.predict(test_vector)\nprint(predicted)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"covid_data_1 = pd.read_csv('../input/coronavirus-covid19-tweets-late-april/2020-04-16 Coronavirus Tweets.CSV')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"covid_data_part_1 = covid_data_1[:1000]\ncovid_data_part_1['text'] = covid_data_part_1['text'].map(lambda x: cleaning(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install langdetect\n!pip install guess_language-spirit","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from langdetect import detect\nfrom guess_language import guess_language\ncovid_data_part_1 = covid_data_part_1.reset_index(drop=True)\nfor i in tqdm(range(len(covid_data_part_1))):\n    words = covid_data_part_1.text[i][0]\n    for j in range(len(covid_data_part_1.text[i])-1):\n        words+= ' ' + covid_data_part_1.text[i][j+1]\n    covid_data_part_1.text[i] = words","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(covid_data_part_1))\nnew_words = []\nfor i in (range(len(covid_data_part_1))):\n    if(guess_language(covid_data_part_1.text[i]) == \"en\"):\n        new_words.append(covid_data_part_1.text[i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(new_words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_data =  vectorizer.transform(new_words)\nfinal_predicted =  model.predict(final_data)\nprint(final_predicted)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction_df = pd.DataFrame({'Content':new_words, 'Emotion_predicted':final_predicted})\nprediction_df.to_csv('emotion_recognizer_svm.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"elapsed_time = time.time() - start_time\nprint (\"processing time:\", elapsed_time, \"seconds\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}