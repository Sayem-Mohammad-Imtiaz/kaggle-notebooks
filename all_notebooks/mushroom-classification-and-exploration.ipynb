{"cells":[{"metadata":{},"cell_type":"markdown","source":"Preparer-RUNLIY"},{"metadata":{},"cell_type":"markdown","source":"In this research,it is tried to learn which features are mostly indicative of a poisonous mushroom and what types of machine learning models perform best on this dataset.The Mushroom’s dataset contains 8124 mushrooms both edible and poisonous in several species.The data contains attributes are class, cap shape, cap surface, cap color, bruises,  odor, gill attachment, gill spacing, gill size, gill color, stalk shape, stalk root, stalk surface above ring, stalk surface below ring, stalk color above ring, stalk color below ring, veil type, veil color, ring number, ring type, spore print color, population, habitat. According to these attributes, it was found out which factors are effective on whether the mushroom is poisonous or not.And some classification models used to categorize the mushroom type is edible or poisonous to identify the class to which a new data will fall under."},{"metadata":{},"cell_type":"markdown","source":"**CONTENT OF THE NOTEBOOK:**\n* EXPLORATORY DATA ANALYSİS AND FEATURE ENGINEERING\n* CLASSIFICATION ALGORITHMS\n* LOGISTIC REGRESSION CLASSIFICATION\n* Confusion Matrix of Logistic Regression\n* Logistic Regression with K-Fold Cross Validation\n* Logistic Regression With Grid Search Cross Validation\n* Confusion Matrix of Logistic Regression With Grid Search CV\n* KNN (K Nearest Neighbour) CLASSIFICATION\n* Confusion Matrix of KNN\n* KNN With K-Fold Cross Validation\n* SVM (Support Vector Machine)\n* Confusion Matrix of SVM\n* SVM With K-Fold Cross Validation\n* SVM With Grid Search Cross Validation\n* SVM With Grid Search Cross Validation and K-Fold\n* NAIVE BAYES CLASSIFICATION\n* Confusion Matrix of Naive Bayes\n* Naive Bayes With K-Fold Cross Validation\n* Naive Bayes With Grid Search Cross Validation\n* Confusion Matrix of Naive Bayes With Grid Search Cross Validation\n* Naive Bayes With Grid Search Cross Validation and K-Fold\n* DECISION TREE CLASSIFICATION\n* Confusion Matrix of Decision Tree Classification\n* Decision Tree Classification With K-Fold Cross Validation\n* Decision Tree Classification With Grid Search Cross Validation\n* Decision Tree Classification With Grid Search CV and K-Fold\n* RANDOM FOREST CLASSIFICATION\n* Confusion Matrix of Random Forest Classification\n* Random Forest Classification With K-Fold Cross Validation\n* Random Forest Classification With Grid Search Cross Validation\n* Random Forest Classification With Grid Search CV and K-Fold\n* PCA (PRINCIPAL COMPONENT ANALYSIS)"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder\nfrom collections import Counter\n\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Dataset’s path is put(csv file)."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df=pd.read_csv(\"../input/mushroom-classification/mushrooms.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I don't need to use columns, dtypes, isnull and shape methods, because info() method already contains all of them."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All of features are non-null object so I have not missing values for this dataset."},{"metadata":{},"cell_type":"markdown","source":"Feature names had seperated with hyphen(-),this situation cause some syntax error later,so I changed hyphen with underscore(_) below code line."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns = df.columns.str.strip().str.replace('-', '_')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And 'class' is a keyword in python, if I don't change it, it will cause some problems.\nSo I changed with m_class that's mean mushroom class"},{"metadata":{"trusted":true},"cell_type":"code","source":"df=df.rename(columns={\"class\": \"m_class\"})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Review dataset.."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Q3-Q1=IQR\n* Q1-1.5(IQR) -> OUTLIER\n* Q3+1.5(IQR) -> OUTLIER\n* describe() already calculate these automatically"},{"metadata":{},"cell_type":"markdown","source":"For object data, the result’s index will include count, unique, top, and freq.The top is the most common value. The freq is the most common value’s frequency."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I found variety of values in features"},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in df:\n    unique=np.unique(df[i])\n    print('{}:{}'.format(i,unique))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* And I noticed that all mushrooms have the same veil-type which is 'p'(partial), none of them are 'u' (universal).So I can drop this feature, it is unnecessary.\n* axis=0 represents rows and axis=1 represents columns."},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.drop(['veil_type'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I copied data to df2, just a precaution.If df gets out of hand, I will use df2."},{"metadata":{"trusted":true},"cell_type":"code","source":"df2=df.copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Hold the feature names in list.I occur this list.Because I will use in visualizations with for loops."},{"metadata":{"trusted":true},"cell_type":"code","source":"list_df_features=[]\nfor i in df:\n    df_features=i\n    list_df_features.append(df_features)\nlist_df_features","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Hold the unique values in list."},{"metadata":{"trusted":true},"cell_type":"code","source":"#xtick olarak atamak için\nlist_unique_real=[]\nfor i in df:\n    unique=np.unique(df[i]).tolist()\n    list_unique_real.append(unique)\nlist_unique_real","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's look at our data what percentage of mushrooms are poisonous(p) and  what percentage of mushrooms are edible(e).And following visualize them."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df['m_class'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#ratio of poisonous and edible mushrooms..\n#plt.style.use('bmh')\n%matplotlib inline\n#\nf,ax=plt.subplots(1,2,figsize=(10,4))\ndf['m_class'].value_counts().plot.pie(explode=[0.1,0],autopct='%1.1f%%',ax=ax[0],shadow=True)\nax[0].set_title('m_class')\nax[0].set_ylabel('')\n#\nsns.countplot('m_class',data=df,ax=ax[1])\nax[1].set_title('m_class')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Rate of edible mushrooms are much more than ratio of poisonous by 3.6% percent difference(equal to 292 sample)."},{"metadata":{},"cell_type":"markdown","source":"* Histogram is about frequency of data with visualization.\n* bins= thinness of graphic columns and number of bar\n* figsize=magnitude of graphic frame\n\nfor i in df:\n\n   df[i].plot(kind='hist', bins=20, figsize=(5,5), color='red', grid=True)\n    \n   plt.xlabel(\"{}\".format(i), fontsize=15)\n    \n   plt.show()\n    \nI can use above codes for histogram to learn counts of values in each features but I prefer other technique.\n"},{"metadata":{},"cell_type":"markdown","source":"Two comment code cells in below(comment row because I don't want take up a lot of space in my kernel), I learnt counts of values in each features and counts of poisonous and edible mushrooms in each features.And after this code cells, I visualized them to be understandable usually six by six."},{"metadata":{"trusted":true,"_kg_hide-output":false},"cell_type":"code","source":"#for i in df:\n#    print(df[i].value_counts(),'\\n')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#for i in df:\n#    print(df.groupby([i,'m_class'])['m_class'].count())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Counts of values in each features descending from ascending..\nf,ax=plt.subplots(1,6,figsize=(19,4))\nfor i in range(0,6):\n    df[list_df_features[i]].value_counts().plot.bar(color=sns.color_palette(\"rocket\"),ax=ax[i])\n    ax[i].set_title(list_df_features[i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Counts of poisonous and edible mushrooms in each features.\n#plt.style.use('fivethirtyeight')\nf,ax=plt.subplots(1,6,figsize=(19,4))\nfor i in range(0,6):\n    sns.countplot(list_df_features[i],hue='m_class',data=df,ax=ax[i])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Inferences:**"},{"metadata":{},"cell_type":"markdown","source":"* If bruises t(true) mushrooms are edible overwhelmingly(%82 edible)\n* Mushrooms have almond(a) and anise(l) odors are completely edible (with 400 sample)\n* Mushrooms that hasn't an odor(n) are mostly edible(Only 3% are poisonous)\n* Foul(f)(2160),fishy(y)(576),spicy(s)(576),creosote(c)(192) odor types completely poisonous."},{"metadata":{"trusted":true},"cell_type":"code","source":"f,ax=plt.subplots(1,6,figsize=(19,4))\nfor i in range(0,6):\n    df[list_df_features[i+6]].value_counts().plot.bar(color=sns.color_palette(\"rocket\"),ax=ax[i])\n    ax[i].set_title(list_df_features[i+6])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f,ax=plt.subplots(1,6,figsize=(19,4))\nfor i in range(0,6):\n    sns.countplot(list_df_features[i+6],hue='m_class',data=df,ax=ax[i])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Inferences:**"},{"metadata":{},"cell_type":"markdown","source":"* If gill_spacing is crowded(w) mushrooms mostly edible(91%).\n* If gill_size is narrow(n) moshrooms are mostly poisonous(%88).\n* gill_color is brown(n)(89%=936 sample) or purple(u)(%90=444 sample),mushrooms are mostly edible.\n* gill_color is buff(b) colored, mushrooms are exactly poisonous(1728 sample)."},{"metadata":{"trusted":true},"cell_type":"code","source":"f,ax=plt.subplots(1,6,figsize=(19,4))\nfor i in range(0,6):\n    df[list_df_features[i+12]].value_counts().plot.bar(color=sns.color_palette(\"rocket\"),ax=ax[i])\n    ax[i].set_title(list_df_features[i+12])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f,ax=plt.subplots(1,6,figsize=(19,4))\nfor i in range(0,6):\n    sns.countplot(list_df_features[i+12],hue='m_class',data=df,ax=ax[i])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Inferences:**"},{"metadata":{},"cell_type":"markdown","source":"* If stalk_surface_above_ring is silky(k), mushrooms are mostly poisonous with 94% rate(2228 sample).\n* If stalk_surface_below_ring is silky(k), mushrooms are mostly poisonous with 93% rate(2160 sample).\n* If stalk_color_above_ring and stalk_color_below_ring are gray(g) colored muhrooms are completely edible.(576 sample)\n* If stalk_color_above_ring and stalk_color_below_ring are buff(b) colored mushrooms are completely poisonous(432 sample)"},{"metadata":{"trusted":true},"cell_type":"code","source":"f,ax=plt.subplots(1,4,figsize=(19,4))\nfor i in range(0,4):\n    df[list_df_features[i+18]].value_counts().plot.bar(color=sns.color_palette(\"rocket\"),ax=ax[i])\n    ax[i].set_title(list_df_features[i+18])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f,ax=plt.subplots(1,4,figsize=(19,4))\nfor i in range(0,4):\n    sns.countplot(list_df_features[i+18],hue='m_class',data=df,ax=ax[i])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Inferences:**"},{"metadata":{},"cell_type":"markdown","source":"* If ring_type is large(l),mushrooms are poisonous with 1296 samples.\n* If spore_print_color is chocolate(h),mushrooms are mostly poisonous(97% = 1584 samples).\n* If spore_print_color is brown(n),mushrooms are mostly edible(89% = 1744 samples).\n* If spore_print_color is black(k),mushrooms are mostly edible(88% = 1584 samples).\n* If habitat is path(p), mushroom are mostly poisonous(88%= 1008 samples)."},{"metadata":{},"cell_type":"markdown","source":"**Converting Object Types Into Integer Types and Label Encoder**"},{"metadata":{},"cell_type":"markdown","source":"I need numeric values for thoroughly analyze my data,but all features are in object form.I used LabelEncoder() method to transform non-numerical labels to numerical labels.\n* fit_transform() -> Fit label encoder and return encoded labels"},{"metadata":{"trusted":true},"cell_type":"code","source":"le=LabelEncoder()\nfor col in df.columns:\n    df[col] = le.fit_transform(df[col])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Checking the values.."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After that time, my works will be easier.."},{"metadata":{"trusted":true},"cell_type":"code","source":"list_unique_encoded=[]\nfor i in df:\n    unique=np.unique(df[i]).tolist()\n    list_unique_encoded.append(unique)\nlist_unique_encoded","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I compared unique values between with label encoder and without label encoder in columns to make interpretation easy."},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in df:\n    print('{}:{}'.format(col,np.unique(df[col])))\nprint(\"\\n\")\nfor i in df2:\n    print('{}:{}'.format(i,np.unique(df2[i])))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Describe() method is useful now because of the label encoder."},{"metadata":{"trusted":true},"cell_type":"code","source":"df3=df.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Let's some comments about mushrooms over relplots.. **"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.relplot(x=\"bruises\",y=\"odor\",col=\"m_class\",data=df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I can say that if there isn't bruises(0) and odor is not anise(3) or pungent(6), mushrooms are mostly poisonous."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.relplot(x=\"spore_print_color\",y=\"odor\",col=\"m_class\",data=df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In spore_print_color values are 0,1,2,3,4,8 and if odor is 5(none), mushrooms are edible."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.relplot(x=\"population\",y=\"odor\",col=\"m_class\",data=df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In all values of population, if odor is 5(none) and population isn't 1(clustered) or 4(several) mushrooms are edible.And if odor is 0(almond) and 3(anise) and population is 2(numerous),3(scattered),4(several) or 5(solitary), mushrooms are edible."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.relplot(x=\"habitat\",y=\"odor\",col=\"m_class\",data=df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In habitat value is 0(woods),1(grasses),3(meadows),4(paths) and odor is 3(anise), mushrooms are edible.And all values in habitat except 3(meadows),6(waste) if odor is 2(foul),mushrooms are poisonous."},{"metadata":{},"cell_type":"markdown","source":"**get_dummies**"},{"metadata":{},"cell_type":"markdown","source":"I used this method to convert all values into 1 and 0.It isn't necessary,sometimes it makes easier normalization but I prefer different method instead of this."},{"metadata":{"trusted":true},"cell_type":"code","source":"gd_df=pd.get_dummies(df,columns=df.columns)\ngd_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gd_df.loc[3:5, 'm_class_0':'cap_shape_5']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"corr() method uses to show the correlation between the values."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.corr()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I want to show visualization about correlation."},{"metadata":{},"cell_type":"markdown","source":"* figsize= magnitude of boxes\n* annot=visibility of numbers on boxes\n* lineWidth=distance between boxes\n* fmt=determine how many number will show in fractional part"},{"metadata":{"trusted":true},"cell_type":"code","source":"f,ax=plt.subplots(figsize=(15,15))\nsns.heatmap(df.corr(), annot=True, lineWidth=.5, fmt='.2f', ax=ax)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I see that veil-color and gill-attachment are directly proportional(0.90),the correlation is positively correlated."},{"metadata":{},"cell_type":"markdown","source":"I create new dataframe that includes only color features in df."},{"metadata":{"trusted":true},"cell_type":"code","source":"color_columns= []\nfor i in df.columns:\n    if 'color' in i:\n        color_columns.append(i)\ndf_color = df[color_columns]\ndf_color.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Hold the df_color feature names in list."},{"metadata":{"trusted":true},"cell_type":"code","source":"list_color_features=[]\nfor i in df_color:\n    color_features=i\n    list_color_features.append(color_features)\nlist_color_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f,ax=plt.subplots(figsize=(8,8))\nsns.heatmap(df_color.corr(), annot=True, lineWidth=.5, fmt='.2f',cmap=\"YlGnBu\", ax=ax)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Mushroom analysing according to only odor types**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#number of poisonous mushroom in every odor types.\nodor_list=list(df['odor'].unique())\nfor i in odor_list:\n    x=df[df['odor']==i]\n    print('{}:{}'.format(i,sum(x.m_class)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#number of edible or poisonous mushroom in every odor types\nodor_list_2=list(df['odor'].unique())\nfor i in odor_list_2:\n    x=df[df['odor']==i]\n    print('{}:{}'.format(i,x.groupby('m_class').size()))\n    print('\\n')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#number of mushrooms in every odor types\ndf.odor.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#total poisonous mushroom number\nprint(len(df[df.m_class == 1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#total edible mushroom number\nprint(len(df[df.m_class == 0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.odor.plot(kind='hist', bins=20, figsize=(8,8), color=\"gray\") \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I calculated poisonous mushroom ratio according to odors.But it is misleading.For example above histogram 4(musty odor type) count's is too few but below it seem %100 poisonous.So we can not generalize. "},{"metadata":{"trusted":true},"cell_type":"code","source":"\nodor_list=list(df['odor'].unique())\nm_class_ratio=[]\nfor i in odor_list:\n    x=df[df['odor']==i]\n    m_class_rate=sum(x.m_class)/len(x)\n    m_class_ratio.append(m_class_rate)\n\nplt.figure(figsize=(15,10))\nsns.barplot(x=odor_list, y=m_class_ratio, palette=\"Greens_d\")\nplt.xticks(rotation= 360)\nplt.xlabel('Odors')\nplt.ylabel('Poison Rate')\nplt.title('Poison Rate Given Odors')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.cap_shape.plot(kind='hist', bins=20, figsize=(8,8), color=\"gray\") \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I calculated poisonous mushroom ratio according to cap shapes.And it is misleading too with same reason."},{"metadata":{"trusted":true},"cell_type":"code","source":"cap_shape_list=list(df['cap_shape'].unique())\nm_class_ratio=[]\nfor i in cap_shape_list:\n    x=df[df['cap_shape']==i]\n    m_class_rate=sum(x.m_class)/len(x)\n    m_class_ratio.append(m_class_rate)\n\nplt.figure(figsize=(15,10))\nsns.barplot(x=cap_shape_list, y=m_class_ratio, palette = sns.cubehelix_palette(len(x)))\nplt.xticks(rotation= 360)\nplt.xlabel('Cap Shapes')\nplt.ylabel('Poison Rate')\nplt.title('Poison Rate Given Cap Shapes')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**LOGISTIC REGRESSION CLASSIFICATION**"},{"metadata":{},"cell_type":"markdown","source":"I stored m_class values in y variable and other features in x_df variable.Because I try to find next mushroom is poisonous or edible and I'll show step by step."},{"metadata":{"trusted":true},"cell_type":"code","source":"y = df.m_class.values\nx_df = df.drop([\"m_class\"],axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And normalization.."},{"metadata":{"trusted":true},"cell_type":"code","source":"x = (x_df - np.min(x_df))/(np.max(x_df)-np.min(x_df)).values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.2,random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\nlr.fit(x_train,y_train)\nprint(\"logistic regression test accuracy {}\".format(lr.score(x_test,y_test)))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Confusion Matrix of Logistic Regression**"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred =lr.predict(x_test)\ny_true = y_test\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_true,y_pred)\n#visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nf, ax = plt.subplots(figsize =(5,5))\nsns.heatmap(cm,annot = True,linewidths=0.5,linecolor=\"red\",fmt = \".0f\",ax=ax)\nplt.xlabel(\"y_pred\")\nplt.ylabel(\"y_true\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Logistic Regression With K Fold Cross Validation**"},{"metadata":{},"cell_type":"markdown","source":"* cv:Determines the cross-validation splitting strategy."},{"metadata":{},"cell_type":"markdown","source":" I want to stop Pandas showing Future Warning messages again and again so used import warnings"},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\n#K fold CV K = 10\nfrom sklearn.model_selection import cross_val_score\naccuracies = cross_val_score(lr,x,y, cv = 10)\nprint(\"average accuracy: \",np.mean(accuracies))\nprint(\"average std: \",np.std(accuracies))\n#devam","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Logistic Regression With Grid Search Cross Validation**"},{"metadata":{},"cell_type":"markdown","source":"I run below code before and found the best C and penalty values but I removed the code because it take a long time during running."},{"metadata":{},"cell_type":"markdown","source":"> from sklearn.model_selection import GridSearchCV\n\n> grid = {\"C\":np.logspace(-3,3,7),\"penalty\":[\"l1\",\"l2\"]}  # l1 = lasso ve l2 = ridge\n\n> lr = LogisticRegression()\n\n> lr_cv = GridSearchCV(lr,grid,cv = 10)\n\n> lr_cv.fit(x_train,y_train)\n\n> print(\"tuned hyperparameters: (best parameters): \",lr_cv.best_params_)\n\n> print(\"accuracy: \",lr_cv.best_score_)\n"},{"metadata":{},"cell_type":"markdown","source":"Output:\n\n* tuned hyperparameters: (best parameters):  {'C': 1000.0, 'penalty': 'l1'}\n* accuracy:  0.9663025080481659"},{"metadata":{},"cell_type":"markdown","source":"Then I apply C and penalty parameters for logistic regression."},{"metadata":{"trusted":true},"cell_type":"code","source":"lr2 = LogisticRegression(C=1000.0, penalty=\"l1\", random_state=1)\nlr2.fit(x_train,y_train)\nprint(\"logistic regression test accuracy with Grid Search Cross Validation {}\".format(lr2.score(x_test,y_test)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Confusion Matrix of Logistic Regression With Grid Search Cross Validation**"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred =lr2.predict(x_test)\ny_true = y_test\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_true,y_pred)\n#visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nf, ax = plt.subplots(figsize =(5,5))\nsns.heatmap(cm,annot = True,linewidths=0.5,linecolor=\"red\",fmt = \".0f\",ax=ax)\nplt.xlabel(\"y_pred\")\nplt.ylabel(\"y_true\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For logistic regression above codes enough.But if I calculate detailed, I write below codes."},{"metadata":{"trusted":true},"cell_type":"code","source":"# %% parameter initialize and sigmoid function\ndimension = 22\ndef initialize_weights_and_bias(dimension):\n    \n    w = np.full((dimension,1),0.01)\n    b = 0.0\n    return w,b\n\n\nw,b = initialize_weights_and_bias(22)\n\ndef sigmoid(z):\n    \n    y_head = 1/(1+ np.exp(-z))\n    return y_head\nprint(sigmoid(0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def forward_backward_propagation(w,b,x_train,y_train):\n    # forward propagation\n    z = np.dot(w.T,x_train) + b\n    y_head = sigmoid(z)\n    loss = -y_train*np.log(y_head)-(1-y_train)*np.log(1-y_head)\n    cost = (np.sum(loss))/x_train.shape[1]      # x_train.shape[1]  is for scaling\n    \n    # backward propagation\n    derivative_weight = (np.dot(x_train,((y_head-y_train).T)))/x_train.shape[1] # x_train.shape[1]  is for scaling\n    derivative_bias = np.sum(y_head-y_train)/x_train.shape[1]                 # x_train.shape[1]  is for scaling\n    gradients = {\"derivative_weight\": derivative_weight, \"derivative_bias\": derivative_bias}\n    \n    return cost,gradients","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def update(w, b, x_train, y_train, learning_rate,number_of_iterarion):\n    cost_list = []\n    cost_list2 = []\n    index = []\n    \n    # updating(learning) parameters is number_of_iterarion times\n    for i in range(number_of_iterarion):\n        # make forward and backward propagation and find cost and gradients\n        cost,gradients = forward_backward_propagation(w,b,x_train,y_train)\n        cost_list.append(cost)\n        # lets update\n        w = w - learning_rate * gradients[\"derivative_weight\"]\n        b = b - learning_rate * gradients[\"derivative_bias\"]\n        \n        cost_list2.append(cost)\n        index.append(i)\n        print (\"Cost after iteration %i: %f\" %(i, cost))\n            \n    # we update(learn) parameters weights and bias\n    parameters = {\"weight\": w,\"bias\": b}\n    plt.plot(index,cost_list2)\n    plt.xticks(index,rotation='vertical')\n    plt.xlabel(\"Number of Iterarion\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n    return parameters, gradients, cost_list","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#%%  # prediction\ndef predict(w,b,x_test):\n    # x_test is a input for forward propagation\n    z = sigmoid(np.dot(w.T,x_test)+b)\n    Y_prediction = np.zeros((1,x_test.shape[1]))\n    # if z is bigger than 0.5, our prediction is sign one (y_head=1),\n    # if z is smaller than 0.5, our prediction is sign zero (y_head=0),\n    for i in range(z.shape[1]):\n        if z[0,i]<= 0.5:\n            Y_prediction[0,i] = 0\n        else:\n            Y_prediction[0,i] = 1\n\n    return Y_prediction","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def logistic_regression(x_train, y_train, x_test, y_test, learning_rate ,  num_iterations):\n    # initialize\n    dimension =  x_train.shape[0] \n    w,b = initialize_weights_and_bias(dimension)\n    # do not change learning rate\n    parameters, gradients, cost_list = update(w, b, x_train, y_train, learning_rate,num_iterations)\n    \n    y_prediction_test = predict(parameters[\"weight\"],parameters[\"bias\"],x_test)\n\n    # Print test Errors\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))\n    \nlogistic_regression(x_train.T, y_train.T, x_test.T, y_test.T,learning_rate = 4, num_iterations = 300)    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I step up the accuracy to 95.32 from 94.95"},{"metadata":{},"cell_type":"markdown","source":"**KNN (K Neirest Neighbour) CLASSIFICATION**"},{"metadata":{},"cell_type":"markdown","source":"* p=poisonous mushroom  \n* e=edible mushromm"},{"metadata":{"trusted":true},"cell_type":"code","source":"p=df[df.m_class== 1]\ne=df[df.m_class== 0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(p.odor,p.cap_shape,color=\"red\",label=\"poisonous\")\nplt.scatter(e.odor,e.cap_shape,color=\"green\",label=\"edible\")\nplt.xlabel(\"odors\")\nplt.ylabel(\"cap_shapes\")\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(p.veil_color,p.gill_attachment,color=\"red\",label=\"poisonous\")\nplt.scatter(e.veil_color,e.gill_attachment,color=\"green\",label=\"edible\")\nplt.xlabel(\"veil_color\")\nplt.ylabel(\"gill_attachment\")\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"gill_attachment\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"veil_color\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors =3)\nknn.fit(x_train,y_train)\nprint(\" {} nn score: {} \".format(3,knn.score(x_test,y_test)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Confusion Matrix of KNN**"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred =knn.predict(x_test)\ny_true = y_test\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_true,y_pred)\n#visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nf, ax = plt.subplots(figsize =(5,5))\nsns.heatmap(cm,annot = True,linewidths=0.5,linecolor=\"red\",fmt = \".0f\",ax=ax)\nplt.xlabel(\"y_pred\")\nplt.ylabel(\"y_true\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is overfitting,algorithm memorized my datas.So I will apply K Fold Cross Validation to avoid overfitting. "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import KFold\nkf = KFold(n_splits=10, random_state=42, shuffle=False)\nfor trainkf, testkf in kf.split(x):\n    #print(\"%s %s\" % (trainkf, testkf))\n    print(\"trainkf:\",len(trainkf))\n    print(\"testkf:\",len(testkf))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**KNN With K Fold Cross Validation**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#K fold CV K = 10\nfrom sklearn.model_selection import cross_val_score\naccuracies = cross_val_score(knn,x,y, cv = 10)\nprint(accuracies)\nprint(\"average accuracy: \",np.mean(accuracies))\nprint(\"average std: \",np.std(accuracies))\n#devam","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# find k value\nscore_list = []\nfor each in range(1,50):\n    knn3 = KNeighborsClassifier(n_neighbors = each)\n    knn3.fit(x_train,y_train)\n    score_list.append(knn3.score(x_test,y_test))\n    \nplt.plot(range(1,50),score_list)\nplt.xlabel(\"k values\")\nplt.ylabel(\"accuracy\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**SVM (Support Vector Machine)**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\nsvm = SVC(random_state = 1)\nsvm.fit(x_train,y_train)\nprint(\"svm score: {} \".format(svm.score(x_test,y_test)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Confusion Matrix of SVM**"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = svm.predict(x_test)\ny_true = y_test\ncm = confusion_matrix(y_true,y_pred)\n#visualization\nf, ax = plt.subplots(figsize =(5,5))\nsns.heatmap(cm,annot = True,linewidths=0.5,linecolor=\"red\",fmt = \".0f\",ax=ax)\nplt.xlabel(\"y_pred\")\nplt.ylabel(\"y_true\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**SVM With K Fold Cross Validation**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#K fold CV K = 10\nfrom sklearn.model_selection import cross_val_score\naccuracies = cross_val_score(svm,x,y, cv = 10)\nprint(accuracies)\nprint(\"average accuracy: \",np.mean(accuracies))\nprint(\"average std: \",np.std(accuracies))\n#devam","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**SVM With Grid Search Cross Validation**"},{"metadata":{},"cell_type":"markdown","source":"I removed the below code because it take a long time during running.But I detected best parameter values with this code."},{"metadata":{},"cell_type":"markdown","source":"from sklearn.model_selection import GridSearchCV\n\ngrid = {'C': [0.1, 1, 10, 100, 1000],  \n              'gamma': [1, 0.1, 0.01, 0.001, 0.0001]}\n              \nsvm = SVC()\n\nsvm_cv = GridSearchCV(svm,grid,cv = 10)\n\nsvm_cv.fit(x_train,y_train)\n\nprint(\"tuned hyperparameters: (best parameters): \",svm_cv.best_params_)\n\nprint(\"accuracy: \",svm_cv.best_score_)"},{"metadata":{},"cell_type":"markdown","source":"*Output:\n\ntuned hyperparameters: (best parameters):  {'C': 1, 'gamma': 1}\n\naccuracy:  1.0\n"},{"metadata":{},"cell_type":"markdown","source":"**SVM With Grid Search Cross Validation and K Fold**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\nsvm2 = SVC(C=1,gamma=1)\naccuracies = cross_val_score(svm2,x,y, cv = 10)\nprint(accuracies)\nprint(\"average accuracy: \",np.mean(accuracies))\nprint(\"average std: \",np.std(accuracies))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**NAIVE BAYES CLASSIFICATION**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Naive bayes \nfrom sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\nnb.fit(x_train,y_train)\nprint(\"accuracy of naive bayes algo: \",nb.score(x_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Confusion Matrix of Naive Bayes**"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = nb.predict(x_test)\ny_true = y_test\ncm = confusion_matrix(y_true,y_pred)\n#visualization\nf, ax = plt.subplots(figsize =(5,5))\nsns.heatmap(cm,annot = True,linewidths=0.5,linecolor=\"red\",fmt = \".0f\",ax=ax)\nplt.xlabel(\"y_pred\")\nplt.ylabel(\"y_true\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Naive Bayes With K Fold Crosss Validation**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#K fold CV K = 10\nfrom sklearn.model_selection import cross_val_score\naccuracies = cross_val_score(nb,x,y, cv = 10)\nprint(accuracies)\nprint(\"average accuracy: \",np.mean(accuracies))\nprint(\"average std: \",np.std(accuracies))\n#devam","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Naive Bayes With Grid Search Cross Validation**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\ngrid = {'var_smoothing': np.logspace(0,-9, num=100)} \nnb = GaussianNB()\nnb_cv = GridSearchCV(nb,grid,cv = 10)\nnb_cv.fit(x_train,y_train)\nprint(\"tuned hyperparameters: (best parameters): \",nb_cv.best_params_)\nprint(\"accuracy: \",nb_cv.best_score_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nb2 = GaussianNB(var_smoothing=0.0023101297000831605)\nnb2.fit(x_train,y_train)\nprint(\"score: \", nb2.score(x_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Confusion Matrix of Naive Bayes With Grid Search Cross Validation**"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = nb2.predict(x_test)\ny_true = y_test\ncm = confusion_matrix(y_true,y_pred)\n#visualization\nf, ax = plt.subplots(figsize =(5,5))\nsns.heatmap(cm,annot = True,linewidths=0.5,linecolor=\"red\",fmt = \".0f\",ax=ax)\nplt.xlabel(\"y_pred\")\nplt.ylabel(\"y_true\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Naive Bayes With Grid Search Cross Validation and K Fold**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#K fold CV K = 10\nfrom sklearn.model_selection import cross_val_score\nnb2 = GaussianNB(var_smoothing=0.0023101297000831605)\naccuracies = cross_val_score(nb2,x,y, cv = 10)\nprint(accuracies)\nprint(\"average accuracy: \",np.mean(accuracies))\nprint(\"average std: \",np.std(accuracies))\n#devam","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**DECISION TREE CLASSIFICATION**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\ndt = DecisionTreeClassifier()\ndt.fit(x_train,y_train)\nprint(\"score: \", dt.score(x_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Confusion Matrix of Decision Tree Classification**"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = dt.predict(x_test)\ny_true = y_test\ncm = confusion_matrix(y_true,y_pred)\n#visualization\nf, ax = plt.subplots(figsize =(5,5))\nsns.heatmap(cm,annot = True,linewidths=0.5,linecolor=\"red\",fmt = \".0f\",ax=ax)\nplt.xlabel(\"y_pred\")\nplt.ylabel(\"y_true\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Decision Tree Classification With K Fold Cross Validation **"},{"metadata":{"trusted":true},"cell_type":"code","source":"#K fold CV K = 10\nfrom sklearn.model_selection import cross_val_score\naccuracies = cross_val_score(dt,x,y, cv = 10)\nprint(accuracies)\nprint(\"average accuracy: \",np.mean(accuracies))\nprint(\"average std: \",np.std(accuracies))\n#devam","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Decision Tree Classification With Grid Search Cross Validation**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\ngrid = {'criterion': ['gini', 'entropy'],\n             'max_depth': [1, 2, 3, 4, 5, 6, 7, 8],\n             'min_samples_split': [2, 3]}\ndt = DecisionTreeClassifier()\ndt_cv = GridSearchCV(dt,grid,cv = 10)\ndt_cv.fit(x_train,y_train)\nprint(\"tuned hyperparameters: (best parameters): \",dt_cv.best_params_)\nprint(\"accuracy: \",dt_cv.best_score_)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Decision Tree Classification With Grid Search Cross Validation and K Fold**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\ndt2 = DecisionTreeClassifier(criterion='gini', max_depth= 8, min_samples_split= 2)\naccuracies = cross_val_score(dt2,x,y, cv = 10)\nprint(accuracies)\nprint(\"average accuracy: \",np.mean(accuracies))\nprint(\"average std: \",np.std(accuracies))\n#devam","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**RANDOM FOREST CLASSIFICATION**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(random_state = 1)\nrf.fit(x_train,y_train)\nprint(\"random forest algo result: \",rf.score(x_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Confusion Matrix of Random Forest Classification**"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = rf.predict(x_test)\ny_true = y_test\ncm = confusion_matrix(y_true,y_pred)\n#visualization\nf, ax = plt.subplots(figsize =(5,5))\nsns.heatmap(cm,annot = True,linewidths=0.5,linecolor=\"red\",fmt = \".0f\",ax=ax)\nplt.xlabel(\"y_pred\")\nplt.ylabel(\"y_true\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Random Forest Classification With K Fold Cross Validation**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#K fold CV K = 10\nfrom sklearn.model_selection import cross_val_score\naccuracies = cross_val_score(rf,x,y, cv = 10)\nprint(accuracies)\nprint(\"average accuracy: \",np.mean(accuracies))\nprint(\"average std: \",np.std(accuracies))\n#devam","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Random Forest Classification With Grid Search Cross Validation**"},{"metadata":{},"cell_type":"markdown","source":"from sklearn.model_selection import GridSearchCV\n\ngrid = {'n_estimators': [100, 300, 500],\n    'criterion': ['gini', 'entropy'],\n    'bootstrap': [True, False]}  \n    \nrf = RandomForestClassifier()\n\nrf_cv = GridSearchCV(rf,grid,cv = 10)\n\nrf_cv.fit(x_train,y_train)\n\nprint(\"tuned hyperparameters: (best parameters): \",rf_cv.best_params_)\n\nprint(\"accuracy: \",rf_cv.best_score_)\n\nOutput:\n\ntuned hyperparameters: (best parameters):  {'bootstrap': True, 'criterion': 'gini', 'n_estimators': 100}\n\naccuracy:  1.0"},{"metadata":{},"cell_type":"markdown","source":"**Random Forest Classification With Grid Search Cross Validation and K Fold**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#K fold CV K = 10\nfrom sklearn.model_selection import cross_val_score\nrf2 = RandomForestClassifier(bootstrap= True, criterion='gini', n_estimators= 100)\naccuracies = cross_val_score(rf2,x,y, cv = 10)\nprint(accuracies)\nprint(\"average accuracy: \",np.mean(accuracies))\nprint(\"average std: \",np.std(accuracies))\n#devam","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**PCA (PRINCIPAL COMPONENT ANALYSIS)**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\npca = PCA(n_components = 2, whiten= True )  # whitten = normalize\npca.fit(df3)\n\ndf3_pca = pca.transform(df3)\n\nprint(\"variance ratio: \", pca.explained_variance_ratio_)\n\nprint(\"sum: \",sum(pca.explained_variance_ratio_))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* principle component=0.33699013\n* second component=0.16553877\n* I can protect my datas 0.5025289008187602 ratio."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}