{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.optim as optim","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is adapted from chapter 6 Deep learning with Pytorch book. We will look at how Pytorch nn module works."},{"metadata":{},"cell_type":"markdown","source":"## Lets get the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/celsius-to-fahrenheit/training.csv')\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t_c = list(train['Celsius'])\nt_u = list(train['Fahrenheit'])\n\nt_c[:5], t_u[:5]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\nt_u and t_c were two 1D tensors of size B. In a linear model, no neural network, Thanks\nto broadcasting, we could write our linear model as w * x + b, where w and b were\ntwo scalar parameters.\n\nThis worked because we had a single input feature: if we had\ntwo, we would need to add an extra dimension to turn that 1D tensor into a matrix\nwith samples in the rows and features in the columns.\n\nThat’s exactly what we need to do to switch to using nn.Linear. We reshape our B\ninputs to B × Nin, where Nin is 1. That is easily done with unsqueeze:"},{"metadata":{"trusted":true},"cell_type":"code","source":"t_c = torch.tensor(t_c).unsqueeze(1)\nt_u = torch.tensor(t_u).unsqueeze(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t_c[:5], t_u[:5], t_u.shape, t_c.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## training and validation set"},{"metadata":{"trusted":true},"cell_type":"code","source":"n_samples = t_u.shape[0]\nn_val = int(0.2 * n_samples)\n\nshuffled_indices = torch.randperm(n_samples)\n\ntrain_indices = shuffled_indices[:-n_val]\nval_indices = shuffled_indices[-n_val:]\n\ntrain_indices, val_indices","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t_u_train = t_u[train_indices]\nt_c_train = t_c[train_indices]\n\nt_u_val = t_u[val_indices]\nt_c_val = t_c[val_indices]\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"normalising the values"},{"metadata":{"trusted":true},"cell_type":"code","source":"t_un_train = 0.1 * t_u_train\nt_un_val = 0.1 * t_u_val","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# t_cn_train = 0.1 * t_c_train\n# t_cn_val = 0.1 * t_c_val\n\nt_cn_train = t_c_train\nt_cn_val = t_c_val","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t_cn_train, t_cn_val = t_cn_train.type(torch.FloatTensor), t_cn_val.type(torch.FloatTensor)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"type(t_cn_train), type(t_c_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t_cn_train[:5], t_un_train[:5]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# The Pytorch nn module"},{"metadata":{},"cell_type":"markdown","source":" PyTorch has a whole submodule dedicated to neural networks, called torch.nn. It\ncontains the building blocks needed to create all sorts of neural network architectures. Those building blocks are called modules in PyTorch parlance (such building\nblocks are often referred to as layers in other frameworks). A PyTorch module is a\nPython class deriving from the nn.Module base class. A module can have one or more\nParameter instances as attributes, which are tensors whose values are optimized\nduring the training process (think w and b in our linear model). A module can also\nhave one or more submodules (subclasses of nn.Module) as attributes, and it will be\nable to track their parameters as well."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import torch.nn as nn","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All PyTorch-provided subclasses of nn.Module have their __call__ method defined.\nThis allows us to instantiate an nn.Linear and call it as if it was a function, "},{"metadata":{},"cell_type":"markdown","source":"note: The submodules must be top-level attributes, not buried inside list or\ndict instances! Otherwise, the optimizer will not be able to locate the submodules (and, hence, their parameters). For situations where your model\nrequires a list or dict of submodules, PyTorch provides nn.ModuleList and\nnn.ModuleDict.\n\nAll PyTorch-provided subclasses of nn.Module have their __call__ method defined.\nThis allows us to instantiate an nn.Linear and call it as if it was a function, like so\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"linear_model = nn.Linear(1,1)\n# linear model accepts 3 : input size, output size and bias defaults to True\nlinear_model(t_un_val)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Aside : Calling an instance of nn.Module with a set of arguments ends up calling a method\nnamed forward with the same arguments. The forward method is what executes the\nforward computation, while __call__ does other rather important chores before and\nafter calling forward. So, it is technically possible to call forward directly, and it will\nproduce the same output as __call__, but this should not be done from user code:\n\n`y = model(x)` correct\n\n`y = model.forward(x)` wrong"},{"metadata":{"trusted":true},"cell_type":"code","source":"linear_model.weight","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"linear_model.bias","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## calling the module with some input\n\nlinear_model(torch.ones(1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Although PyTorch lets us get away with it, we don’t actually provide an input with the\nright dimensionality. We have a model that takes one input and produces one output,\nbut PyTorch nn.Module and its subclasses are designed to do so on multiple samples at\nthe same time. To accommodate multiple samples, modules expect the zeroth dimension of the input to be the number of samples in the batch."},{"metadata":{},"cell_type":"markdown","source":"## Batching inputs\n\nAny module in nn is written to produce outputs for a batch of multiple inputs at the\nsame time. Thus, assuming we need to run nn.Linear on 10 samples, we can create an\ninput tensor of size B × Nin, where B is the size of the batch and Nin is the number of\ninput features, and run it once through the model. For example:"},{"metadata":{"trusted":true},"cell_type":"code","source":"linear_model(torch.ones(10,1))\n\n# since there is no loss and update the values are same","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our input is B × C × H × W with a batch size of 3 (say, images of a dog, a bird, and then a car), three channel dimensions (red, green, and blue),\nand an unspecified number of pixels for height and width. As we can see, the output is a tensor of size B × Nout, where Nout is the number of output features: four, in\nthis case."},{"metadata":{},"cell_type":"markdown","source":"## Optimising batches\n\nThe reason we want to do this batching is multifaceted. One big motivation is to make\nsure the computation we’re asking for is big enough to saturate the computing\nresources we’re using to perform the computation. GPUs in particular are highly parallelized, so a single input on a small model will leave most of the computing units idle.\nBy providing batches of inputs, the calculation can be spread across the otherwise-idle\nunits, which means the batched results come back just as quickly as a single result\nwould. Another benefit is that some advanced models use statistical information from\nthe entire batch, and those statistics get better with larger batch sizes.\n\nlets pass te linear model (parameters) to optimizer:"},{"metadata":{"trusted":true},"cell_type":"code","source":"linear_model = nn.Linear(1,1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"optimizer = optim.SGD(\n    linear_model.parameters(),\n    lr=1e-2\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Usually, it was our responsibility to create parameters and pass them as the first argument to optim.SGD. Now we can use the parameters method to ask any nn.Module for\na list of parameters owned by it or any of its submodules:"},{"metadata":{"trusted":true},"cell_type":"code","source":"list(linear_model.parameters())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This call recurses into submodules defined in the module’s init constructor and\nreturns a flat list of all parameters encountered, so that we can conveniently pass it to\nthe optimizer constructor as we did previously.\n\nWe can already figure out what happens in the training loop. The optimizer is provided with a list of tensors that were defined with requires_grad = True—all Parameters\nare defined this way by definition, since they need to be optimized by gradient descent.\n\nWhen training_loss.backward() is called, grad is accumulated on the leaf nodes of the\ngraph, which are precisely the parameters that were passed to the optimizer.\n\n At this point, the SGD optimizer has everything it needs. When optimizer.step()\nis called, it will iterate through each Parameter and change it by an amount proportional to what is stored in its grad attribute. Pretty clean design.\n Let’s take a look a the training loop now:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def training_loop(n_epochs, optimizer, model, loss_fn, t_u_train, t_u_val, t_c_train, t_c_val, output_freq):\n    print(\"for t_u_train : \", t_u_train )\n    print(\"for t_c_train : \", t_c_train)\n    for epoch in range(1, n_epochs + 1):\n        t_p_train = model(t_u_train)\n        loss_train = loss_fn(t_p_train, t_c_train)\n        \n        t_p_val = model(t_u_val)\n        loss_val = loss_fn(t_p_val, t_c_val )\n        \n        optimizer.zero_grad()\n        loss_train.backward()\n        optimizer.step()\n        \n    \n        if epoch ==1 or epoch % output_freq == 0:\n            print(\"predicted t_p_train : \", t_p_train)\n            print(\"Epoch: \", epoch, \" Training loss: \", loss_train.item(), \" Val loss:\", loss_val.item())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It hasn’t changed practically at all, except that now we don’t pass params explicitly to\nmodel since the model itself holds its Parameters internally.\n There’s one last bit that we can leverage from torch.nn: the loss. Indeed, nn comes\nwith several common loss functions, among them nn.MSELoss (MSE stands for Mean\nSquare Error), which is exactly what we defined earlier as our loss_fn. Loss functions\nin nn are still subclasses of nn.Module, so we will create an instance and call it as a\nfunction. In our case, we get rid of the handwritten loss_fn and replace it."},{"metadata":{},"cell_type":"markdown","source":"### debug\n\nmy loss values were comingtoo high inf"},{"metadata":{"trusted":true},"cell_type":"code","source":"t_un_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t_cn_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"torch.ones(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"linear_model = nn.Linear(1,1)\nlinear_model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"linear_model(torch.ones(1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a = [0,1,1]\nb = [1,1,1]\n\na = torch.tensor(a).unsqueeze(1)\nb = torch.tensor(b).unsqueeze(1)\n\na,b=a.type(torch.FloatTensor),b.type(torch.FloatTensor)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# TRYING OUT LOSS FUNCTION\nt_p_train = linear_model(a)\nprint(t_p_train)\n\ntest_loss= nn.MSELoss()\ntest_loss(t_p_train, b)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"linear_model= nn.Linear(1,1)\noptimizer = optim.SGD(linear_model.parameters(), lr=1e-5)\n\ntraining_loop(\n    n_epochs = 3000,\n    optimizer = optimizer,\n    model = linear_model,\n    loss_fn = nn.MSELoss(),\n    t_u_train = t_un_train,\n    t_u_val = t_un_val,\n    t_c_train = t_cn_train,\n    t_c_val = t_cn_val,\n    output_freq = 1000\n)\n\nprint()\nprint(linear_model.weight)\nprint(linear_model.bias)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Got an answer! \n\nNote if you are getting inf as the training loss, play with learning rate, it might be too high."},{"metadata":{},"cell_type":"markdown","source":"### Replacing the model\n\nWe are going to keep everything else fixed, including the loss function, and only redefine model. Let’s build the simplest possible neural network: a linear module, followed\nby an activation function, feeding into another linear module."},{"metadata":{"trusted":true},"cell_type":"code","source":"seq_model = nn.Sequential(\n            nn.Linear(1,13),\n    # 13 was chosen arbitrarly\n            nn.Tanh(),\n            nn.Linear(13,1)\n)\n\nseq_model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"seq_model.parameters()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"[param.shape for param in seq_model.parameters()]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"These are the tensors that the optimizer will get. Again, after we call model.backward(),\nall parameters are populated with their grad, and the optimizer then updates their values accordingly during the optimizer.step() call. Not that different from our previous\nlinear model, eh? After all, they’re both differentiable models that can be trained using\ngradient descent.\n A few notes on parameters of nn.Modules. When inspecting parameters of a model\nmade up of several submodules, it is handy to be able to identify parameters by name.\nThere’s a method for that, called named_parameters:"},{"metadata":{"trusted":true},"cell_type":"code","source":"for name, param in seq_model.named_parameters():\n    print(name, param.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The name of each module in Sequential is just the ordinal with which the module\nappears in the arguments. Interestingly, Sequential also accepts an OrderedDict,in\nwhich we can name each module passed to Sequential:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import OrderedDict\n\nseq_model = nn.Sequential(OrderedDict([\n    ('hidden_linear', nn.Linear(1,8)),\n    ('hidden_activation', nn.Tanh()),\n    ('output_linear', nn.Linear(8,1))\n]))\n\nseq_model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This allows us to get more explanatory names for submodules:"},{"metadata":{"trusted":true},"cell_type":"code","source":"for name, param  in seq_model.named_parameters():\n    print(name, param.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is more descriptive; but it does not give us more flexibility in the flow of data\nthrough the network, which remains a purely sequential pass-through—the\nnn.Sequential is very aptly named. We will see how to take full control of the processing of input data by subclassing nn.Module ourselves in chapter 8.\n We can also access a particular Parameter by using submodules as attributes:\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"seq_model.output_linear.bias","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is useful for inspecting parameters or their gradients: for instance, to monitor\ngradients during training, as we did at the beginning of this chapter. Say we want to\nprint out the gradients of weight of the linear portion of the hidden layer. We can run\nthe training loop for the new neural network model and then look at the resulting\ngradients after the last epoch:\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"optimizer = optim.SGD(seq_model.parameters(), lr = 1e-3)\n\ntraining_loop(\n    n_epochs=5000,\n    optimizer=optimizer,\n    model=seq_model,\n    loss_fn=nn.MSELoss(),\n    t_u_train=t_un_train,\n    t_u_val=t_un_val,\n    t_c_train=t_cn_train,\n    t_c_val=t_cn_val,\n    output_freq = 1000\n)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Output', seq_model(t_un_val))\nprint('Answer', t_c_val)\nprint('hidden', seq_model.hidden_linear.weight.grad)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Comparing to a linear model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from matplotlib import pyplot as plt\n\nt_range = torch.arange(20.,90.).unsqueeze(1)\n\nfig = plt.figure(dpi=600)\n\nplt.xlabel(\"Fahrenheit\")\nplt.ylabel(\"Celsius\")\nplt.plot(t_u.numpy(), t_c.numpy(), 'o')\nplt.plot(t_range.numpy(), seq_model(0.1 * t_range).detach().numpy(), 'c-')\nplt.plot(t_u.numpy(), seq_model(0.1 * t_u).detach().numpy(), 'kx')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"for linear model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from matplotlib import pyplot as plt\n\nt_range = torch.arange(20.,90.).unsqueeze(1)\n\nfig = plt.figure(dpi=600)\n\nplt.xlabel(\"Fahrenheit\")\nplt.ylabel(\"Celsius\")\nplt.plot(t_u.numpy(), t_c.numpy(), 'o')\nplt.plot(t_range.numpy(), linear_model(0.1 * t_range).detach().numpy(), 'c-')\nplt.plot(t_u.numpy(), linear_model(0.1 * t_u).detach().numpy(), 'kx')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":". By now you should\nhave confidence in your understanding of what’s going on behind the scenes. Hopefully this taste of PyTorch has given you an appetite for mo"},{"metadata":{},"cell_type":"markdown","source":"# Exercises"},{"metadata":{},"cell_type":"markdown","source":"1. Experiment with the number of hidden neurons in our simple neural network\nmodel, as well as the learning rate."},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets get data\n\nimport pandas as pd\n\ntrain = pd.read_csv(\"../input/celsius-to-fahrenheit/training.csv\")\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t_c = list(train['Celsius'])\nt_u = list(train['Fahrenheit'])\n\nt_c[:5], t_u[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t_c = torch.tensor(t_c).unsqueeze(1)\nt_u = torch.tensor(t_u).unsqueeze(1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t_c[:5], t_u[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t_c.shape, t_u.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t_c, t_u = t_c.type(torch.FloatTensor), t_u.type(torch.FloatTensor)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_samples = t_u.shape[0]\nn_val = int(0.2 * n_samples)\n\nshuffled_indices = torch.randperm(n_samples)\n\ntrain_indices = shuffled_indices[:-n_val]\nval_indices = shuffled_indices[-n_val:]\n\ntrain_indices, val_indices","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# validation set creation and normalization\n\nt_un_train = t_u[train_indices] * 0.1\nt_un_val = t_u[val_indices] * 0.1\n\nt_c_train = t_c[train_indices] \nt_c_val = t_c[val_indices] ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets create a model from scratch\n\n\nseq_model = torch.nn.Sequential(\n    torch.nn.Linear(1, 17),\n    torch.nn.Tanh(),\n    torch.nn.Linear(17,1)\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"[param.shape for param in seq_model.parameters()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"optimizer = torch.optim.SGD(seq_model.parameters(), lr=1e-3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t_un_train.shape, t_c_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def training_loop(n_epochs=5000, optimizer=optimizer,model=seq_model,loss_fn=torch.nn.MSELoss(), train_t_u=t_un_train, val_t_u=t_un_val, train_t_c=t_c_train, val_t_c = t_c_val):\n    for epoch in range(1, n_epochs +1):\n        train_t_p = model(train_t_u)\n        train_loss = loss_fn(train_t_p, train_t_c)\n        \n        with torch.no_grad():\n            val_t_p = model(val_t_u)\n            val_loss = loss_fn(val_t_p, val_t_c)\n            assert val_loss.requires_grad == False\n        \n        optimizer.zero_grad()\n        train_loss.backward()\n        optimizer.step()\n        \n        if (epoch %1000 == 0):\n            print('training_loss: ', train_loss, 'validation_loss: ',val_loss)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training_loop()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}