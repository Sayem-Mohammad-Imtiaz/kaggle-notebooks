{"cells":[{"metadata":{},"cell_type":"markdown","source":"## This notebook is intended to be used as a Template notebook where one can easily load their dataset  apply preprocessing, decide on the type of embedding to use , The type of model in an instant. Feel free to download it modify it according to your need.\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Please upvote this kernel if you like it. It motivates me to produce more quality content :)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# The preprocessing part includes\n\n1. removing stopwords \n2. lowercasing \n3. removing words with len < 2\n4. removing html tags\n5. removing punctuations\n6. removing digits\n7. Lemmatization\n\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Embeddings Implemented\n1. Bagofwords \n2. Tfidf\n3. wor2vec\n4. glove \n5. tensorlfow hub ( comming soon)\n6. The Embedding layer ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# DeepLearning Models Implemented include\n1. Simple Dense + EMbedding layer\n2. Lstm DNN\n3. CNN \n4. CNN + LSTM\n5. CuDNNLSTM (comming soon)\n6. with attention layer (comming soon)\n7. Bert (comming soon)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Demo \nFor the demo I have used the Imbd dataset that contains 50K reviews \nFor embeddings I have used option 3 which is using converting tokens to sequences and using   embedding layer to produce the embedding vectors.\n\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Note:- remember to set the trainable parameter in the embedding layer = False ","execution_count":null},{"metadata":{"id":"pAjmFxmGbBMI"},"cell_type":"markdown","source":"# Imports  \n","execution_count":null},{"metadata":{"id":"kNagaveFa05R","trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Modules for data manipulation\nimport numpy as np\nimport pandas as pd\nimport re\n\n# Modules for visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sb\n\n# Tools for preprocessing input data\nfrom nltk import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk import pos_tag\n# Tools for creating ngrams and vectorizing input data\nimport gensim\n\nfrom gensim.models import Word2Vec, Phrases\n\n\n\n# Tools for building a model\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.preprocessing import text\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\n# Tools for assessing the quality of model prediction\nfrom sklearn.metrics import accuracy_score, confusion_matrix\n\n","execution_count":null,"outputs":[]},{"metadata":{"id":"t-MKfFzsdtp5","outputId":"b968e4b1-39ff-4057-8e6d-20a496eff176","trusted":true},"cell_type":"code","source":"import nltk\nnltk.download('wordnet')\nnltk.download('stopwords')\nnltk.download('punkt')\nnltk.download('averaged_perceptron_tagger')","execution_count":null,"outputs":[]},{"metadata":{"id":"IU4WiJFUbi7V"},"cell_type":"markdown","source":"# load your dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset_path = '../input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv'\ndataset= pd.read_csv(dataset_path)\ndataset.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.replace({'positive':1,'negative':0},inplace = True)\ndataset.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"I0hu5RukdB-s"},"cell_type":"markdown","source":"# Preprocessig Part\n## incldues\n<pre>\n1. removing stopwords \n2. lowercasing \n3. removing words with len < 2\n4. removing html tags\n5. removing punctuations\n6. removing digits\n7. Lemmatization\n\n\n","execution_count":null},{"metadata":{"id":"XLo1WZ0Ubi5s","outputId":"b80832eb-db93-4e7d-f7c5-5e668549079d","trusted":true},"cell_type":"code","source":"\n#helper functions for lemmatizations\ndef penn2morphy(penntag):\n    \"\"\" Converts Penn Treebank tags to WordNet. \"\"\"\n    morphy_tag = {'NN':'n', 'JJ':'a',\n              'VB':'v', 'RB':'r'}\n    try:\n        return morphy_tag[penntag[:2]]\n    except:\n        return 'n' \n\ndef clean_text(x):\n    \n    \n    \n    # remove html tags\n    regex = re.compile('<.*?>')\n    input =  re.sub(regex, '', x)\n\n    #remove punctuations, numbers.\n    input = re.sub('[!@#$%^&*()\\n_:><?\\-.{}|+-,;\"\"``~`—]|[0-9]|/|=|\\[\\]|\\[\\[\\]\\]',' ',input)\n    input = re.sub('[“’\\']','',input)   \n    \n    \n    #lemmatise \n    ls = list(wnl.lemmatize(word.lower(), pos=penn2morphy(tag)) for word, tag in pos_tag(word_tokenize(input)))\n    \n    \n    #remove stopwords\n    return_str = ''\n    for word in ls:\n       #if word its a long word with single character eg.aaaaaa remove it \n        if word not in stop_dict and len(set(word)) > 2:\n            return_str +=word.lower() + \" \"\n       \n\n    \n    #lemmatize the text.\n    \n\n    return return_str\n\n\nwnl = WordNetLemmatizer()\n\nstop_dict = stopwords.words('english')\n\ntmp_sent  = \"AAAAAA <html> <h1> run <i>running</i> ban banned dancing dance 1 2 3  4   5 5  5 !@#$%^&*(){{:><<< MMM<>?PLOKIU}} </h1> </html>\"\n\n\nclean_text(tmp_sent)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Apply The clean_text function over here","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset['review'] = dataset['review'].map(clean_text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x  = dataset['review']\ny = dataset['sentiment']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Different Embedding Types ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y.shape","execution_count":null,"outputs":[]},{"metadata":{"id":"Hnu2QVpsyHZT"},"cell_type":"markdown","source":"## 1 BagOfWords (countVectorizer) ","execution_count":null},{"metadata":{"id":"pPlnyH5qbizx","outputId":"5220814c-e9fd-48d8-b938-782a5d40d4bd","trusted":true},"cell_type":"code","source":"\ntokenizer = text.Tokenizer(num_words=1000)\ntokenizer.fit_on_texts(['sample text'])\n\nmetrix = tokenizer.texts_to_matrix(['sample text'])\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2 Using Tfidf","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntokenizer = text.Tokenizer(num_words=1000)\ntokenizer.fit_on_texts(['sample text'])\n\nmetrix = tokenizer.texts_to_matrix(['sample text'],mode = 'tfidf')\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3 Converting TO sequences  (uses the embdeing layer)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = text.Tokenizer()\ntokenizer.fit_on_texts(x)\nsequences = tokenizer.texts_to_sequences(x)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4 Using Glove Model","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 4.0 Need to create Sequences","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntokenizer = text.Tokenizer()\ntokenizer.fit_on_texts(x)\n\nsequences = tokenizer.texts_to_sequences(x)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4.1Download Pre-traied weights","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# !wget  http://nlp.stanford.edu/data/glove.6B.zip\n# !unzip glove.6B.zip    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4.2Load pretrained vectors","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nglove_embedding  = 'glove.6B.100d.txt'\n\nembeddings_index = {}\n\nfile =  open(glove_embedding,'r')\n    \n    \nfor line in file:\n    \n    word,embd = line.split(maxsplit = 1)\n  \n    embd = np.fromstring(embd,'f',sep = ' ')\n    \n    embeddings_index[word] = embd\n    \n    \n    \nfile.close()    \n    \n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4.3Prepare the weight Matrix","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_matrix = np.zeros((max_word_size,100)) \n\nprint(embedding_matrix.shape)\n\n\n\n\nfor index,word in tokenizer.index_word.items():\n    \n    embd =  embeddings_index.get(word)\n    \n    if embd is not None:\n        embedding_matrix[index] = embd\n        \n        \n# embedding_layer = Embedding(vocab_size, 150, weights=[embedding_vectors], input_length=370, trainable=True)        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5 Using Word2Vec (traning a custom wor2vec model)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Using Bigrams and Trigrams","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"bigrams = Phrases(data)\ntrigrams = Phrases(data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Traning The model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"word2vec_model = Word2Vec(\n    sentences = trigrams[data],\n    size = 300,\n    min_count=3, window=5, workers=4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Preparing the embedding matrix ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sequences = []\nfor i in tqdm.tqdm(data):\n    sent = []\n    for word in i:\n        if word in word2vec_model.wv.vocab:\n            sent.append(word2vec_model.wv.vocab[word].index)\n    sequences.append(sent)    \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Padding ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#replace your_sequences with the embedding matrix you choose form above.\nvocab_size = len(tokenizer.word_index) + 1\nX_pad =  pad_sequences(sequences,maxlen = 1000,padding = 'post',value = vocab_size - 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"","execution_count":null},{"metadata":{"id":"bfPu3KazbiyJ","outputId":"0aac65dc-ca4c-4f6a-bab3-ec1b1ec4265b","trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(\n    X_pad,\n    y,\n    test_size=0.05,\n    shuffle=True,\n    random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"id":"pFOX59TZpXLQ"},"cell_type":"markdown","source":"# DeepLearning Models","execution_count":null},{"metadata":{"id":"gIFfw6OXyK4j"},"cell_type":"markdown","source":"# Model  1 Simple model with Embed + Dense Layers","execution_count":null},{"metadata":{"id":"xjGixpqEbiwH","outputId":"4f301ec5-e1df-4535-cbfb-9bb2ff7f22f3","trusted":true},"cell_type":"code","source":"model = Sequential()\nmodel.add(Embedding(input_dim=vocab_size,output_dim=128,input_length = 1000))\nmodel.add(Flatten())\nmodel.add(Dense(50,activation = 'relu'))\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.compile(\n    loss=\"binary_crossentropy\",\n    optimizer= 'adam',\n    metrics=['accuracy'])\n\nmodel.summary()\n\nprint(model.input_shape)\n\nmodel.fit(X_train,y_train,validation_data = (X_test,y_test),epochs = 1)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model 2 with Lstm Layer","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model_lstm = Sequential()\nmodel_lstm.add(Embedding(input_dim=vocab_size,output_dim=128,input_length = 1000))\nmodel_lstm.add(LSTM(60, return_sequences = True))\nmodel_lstm.add(GlobalMaxPool1D())\n# model_lstm.add(Flatten())\nmodel_lstm.add(Dropout(0.1))\nmodel_lstm.add(Dense(50,activation = 'relu'))\nmodel_lstm.add(Dropout(0.1))\nmodel_lstm.add(Dense(1, activation='sigmoid'))\n\nmodel_lstm.compile(\n    loss=\"binary_crossentropy\",\n    optimizer= 'adam',\n    metrics=['accuracy'])\n\nmodel_lstm.summary()\n\nprint(model_lstm.input_shape)\n\nmodel_lstm.fit(X_train,y_train,validation_data = (X_test,y_test),epochs = 1)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# model 3 with CNN Layer ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model_cnn = Sequential([\n    \n    Embedding(input_dim=vocab_size,output_dim=128,input_length = 1000),\n    Conv1D(16,8,activation = 'relu'),\n    Dropout(0.5),\n    MaxPool1D(2),\n    Flatten(),\n    Dropout(0.5),\n    Dense(64,activation = 'relu'),\n    Dense(1,activation = 'sigmoid')\n    ])\n\n\nmodel_cnn.compile(\n    loss=\"binary_crossentropy\",\n    optimizer= 'adam',\n    metrics=['accuracy'])\n\nmodel_cnn.summary()\n\nprint(model_cnn.input_shape)\n\nmodel_cnn.fit(X_train,y_train,validation_data = (X_test,y_test),epochs = 1)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# model 4 with cnn + lstm","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model_cnn_lstm =  Sequential([\n    Embedding(input_dim=vocab_size,output_dim=128,input_length = 1000),\n    Conv1D(16,5,activation = 'relu',padding = 'same',strides = 1),\n    MaxPool1D(2),\n    LSTM(64,name = 'lstm_1'),\n    Dropout(0.7),\n    Dense(1,activation = 'sigmoid')\n    ])\n\n\nmodel_cnn_lstm.compile(\n    loss=\"binary_crossentropy\",\n    optimizer= 'adam',\n    metrics=['accuracy'])\n\n\n# model.fit()\nmodel_cnn_lstm.summary()\n# model_cnn_lstm.summary()\n# print(model_cnn_lstm.get_layer('lstm_1').input_shape )\n\n\nmodel_cnn_lstm.fit(X_train,y_train,validation_data = (X_test,y_test),epochs = 1)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# DL Model using CuDNNLSTM ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\n\nmodel_lstm = Sequential()\nmodel_lstm.add(Embedding(input_dim=vocab_size,output_dim=128,input_length = 1000))\nmodel_lstm.add(tf.compat.v1.keras.layers.CuDNNLSTM(60, return_sequences = True))\nmodel_lstm.add(GlobalMaxPool1D())\n# model_lstm.add(Flatten())\nmodel_lstm.add(Dropout(0.1))\nmodel_lstm.add(Dense(50,activation = 'relu'))\nmodel_lstm.add(Dropout(0.1))\nmodel_lstm.add(Dense(1, activation='sigmoid'))\n\nmodel_lstm.compile(\n    loss=\"binary_crossentropy\",\n    optimizer= 'adam',\n    metrics=['accuracy'])\n\nmodel_lstm.summary()\n\nprint(model_lstm.input_shape)\n\nmodel_lstm.fit(X_train,y_train,validation_data = (X_test,y_test),epochs = 1)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Evaluation Matrix","execution_count":null},{"metadata":{"id":"bgxdZuwQbiox","trusted":true},"cell_type":"code","source":"# def plot_confusion_matrix(y_true, y_pred, ax, class_names, vmax=None,\n#                           normed=True, title='Confusion matrix'):\n#     matrix = confusion_matrix(y_true,y_pred)\n#     if normed:\n#         matrix = matrix.astype('float') / matrix.sum(axis=1)[:, np.newaxis]\n#     sb.heatmap(matrix, vmax=vmax, annot=True, square=True, ax=ax,\n#                cmap=plt.cm.Blues_r, cbar=False, linecolor='black',\n#                linewidths=1, xticklabels=class_names)\n#     ax.set_title(title, y=1.20, fontsize=16)\n#     #ax.set_ylabel('True labels', fontsize=12)\n#     ax.set_xlabel('Predicted labels', y=1.10, fontsize=12)\n#     ax.set_yticklabels(class_names, rotation=0)","execution_count":null,"outputs":[]},{"metadata":{"id":"kAhHW5LXw5u1","outputId":"2e6a8cac-e70d-44a5-9a12-2ecb61d606c8","trusted":true},"cell_type":"code","source":"# fig, axis1 = plt.subplots(nrows=1, ncols=1)\n# plot_confusion_matrix([1,1,1,1,0,0], [1,1,0,1,0,0], ax=axis1,\n#                       title='Confusion matrix (train data)',\n#                       class_names=['Positive', 'Negative'])","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}