{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Task: What do we know about non-pharmaceutical interventions?"},{"metadata":{},"cell_type":"markdown","source":"## Install/Load Packages"},{"metadata":{},"cell_type":"markdown","source":"The first block of code is (almost) directly from kaggle"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\n# There are too many paths and printing them takes\n# up too much space so I don't do this normally\nif 1==0: \n    for dirname, _, filenames in os.walk('/kaggle/input'):\n        for filename in filenames:\n            pass\n            #print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next we installl scispacy, a repo of commands to deal with scientific documents. *Note that internet access needs to be switched on for this to work!*"},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"# Progress bar\nimport tqdm\n\n# Word2Vec\nfrom gensim.models.word2vec import Word2Vec\n\nfrom nltk.tokenize import word_tokenize \nfrom scipy.spatial.distance import cdist","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Introduction"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"Now that all our libraries are loaded we need data. We explore the full text in the files using the output generated from the following notebook:\nhttps://www.kaggle.com/xhlulu/cord-19-eda-parse-json-and-generate-clean-csv"},{"metadata":{"trusted":true},"cell_type":"code","source":"biorxiv_clean = pd.read_csv(\"../input/cord-19-eda-parse-json-and-generate-clean-csv/biorxiv_clean.csv\")\nclean_comm_use = pd.read_csv(\"../input/cord-19-eda-parse-json-and-generate-clean-csv/clean_comm_use.csv\")\nclean_noncomm_use = pd.read_csv(\"../input/cord-19-eda-parse-json-and-generate-clean-csv/clean_noncomm_use.csv\")\nclean_pmc = pd.read_csv(\"../input/cord-19-eda-parse-json-and-generate-clean-csv/clean_pmc.csv\")\n\nall_data = pd.concat([biorxiv_clean, clean_comm_use, clean_noncomm_use, clean_pmc]).reset_index(drop=True)\n\nall_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Number of Rows in Table: %i\" % len(all_data))\nprint(\"Number of Titles: %i \" % all_data['title'].count())\nprint(\"Number of Abstracts: %i \" % all_data['abstract'].count())\nprint(\"Number of Texts: %i \" % all_data['text'].count())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train Word2Vec Model\nThe input for the word2vec function is a list of documents where each document is a list of words."},{"metadata":{},"cell_type":"markdown","source":"To get our word2vec model we first get all the text from each document into a list. We do this using our `text` column from `all_data`."},{"metadata":{"trusted":true},"cell_type":"code","source":"# replace newlines with empty strings\nall_text = all_data.text.str.replace('\\n\\n', ' ')\n# remove citations\nptn = r'\\[[0-9]{1,2}\\]'\nall_text = all_text.str.replace(ptn,'')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So `all_text` is now a list of documents. However, instead of the text in each document being a string, we need the texts to be formatted as a list of words that make up the document. (Ideally, we would want real words and not numbers that stand for citations so we probably should clean this next step up eventually, but cest la vie). In other words, We are making a list of documents and then in the list of documents is lists of words in all_text_list. Just so you know the cool computational lingo, each words is also known as a token. "},{"metadata":{"trusted":true},"cell_type":"code","source":"all_text_list = list(\n    map(\n        lambda x: word_tokenize(x), all_text.values\n    ))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we can perform word2vec. Word2vec converts words to vectors using a neural networ.  Using the context of each word, it predicts a vector that represents how it is used contextually. In other words, the vectors created by word2vec are highly dependent on the texts it is trained on. These vectors for words may be very different if trained on scientific journals verses twitter data."},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Word2Vec(all_text_list, size=300, iter=10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Generate Document Vectors"},{"metadata":{},"cell_type":"markdown","source":"Ok so now `model` is like a dictionary that converts words into vectors. For each document, we convert all the words into a vector based on the model. We then make a list of these word vectors and store them in the object `word_vector`. Since each word vector is of the same length, we take the average of all the vectors in each document to get a single vector to represent the document. We set this averaged vector to the document's paper id in the dictionary `document_dict`. "},{"metadata":{"trusted":true},"cell_type":"code","source":"document_dict = {}\nfor idx, text_df in tqdm.tqdm(all_data[[\"paper_id\", \"text\"]].iterrows()):\n    text = text_df['text'].replace(\"\\n\\n\", ' ')\n    \n    word_vector = (\n        list(\n            map( # this part takes every word and converts it into a vector\n                lambda x: model.wv[x], \n                filter( #this part checks that word is in the model\n                    lambda x: x in model.wv, \n                    text.split(\" \")\n                )\n            )\n        )\n    )\n    # here we are making a dictory where the document is a key and the value\n    # is the average of the word vectors in that document\n    if len(word_vector) > 0:\n        document_dict[text_df['paper_id']] = pd.np.stack(word_vector, axis=1).mean(axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(document_dict))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(list(document_dict.values())[0].shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's convert this dictionary to a dataframe"},{"metadata":{"trusted":true},"cell_type":"code","source":"document_embeddings_df = pd.DataFrame.from_dict(document_dict, orient=\"index\")\ndocument_embeddings_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Document Cosine Similarity"},{"metadata":{},"cell_type":"markdown","source":"Now we will use costine similarity to measure how related documents are to the  a specific word. For example of how cosine similarity works see this [image](https://datascience-enthusiast.com/figures/cosine_sim.pn), Higher numbers indicate documents that are closer to the word vector of interest."},{"metadata":{"trusted":true},"cell_type":"code","source":"def cos_sim(text):\n    word_vector_list = (\n        list(\n            map( # this part takes every word and converts it into a vector\n                lambda x: model.wv[x], \n                filter( #this part checks that word is in the model\n                    lambda x: x in model.wv, \n                    text.split(\" \")\n                )\n            )\n        )\n    )\n    mean_word_vec = pd.np.stack(word_vector_list, axis=1).mean(axis=1)\n    \n\n    \n    # compute similarity\n    doc_sim = (\n        1-cdist(\n            document_embeddings_df.values,\n            [mean_word_vec],\n            'cosine'\n        )\n    )\n    # convert result to a date frame\n    document_sim_df = (\n        pd.DataFrame(doc_sim, columns=[\"cos_sim\"])\n        .assign(document_id=list(document_embeddings_df.index))\n    )\n    # sort from most similar to least\n    document_sim_df = document_sim_df.sort_values(\"cos_sim\", ascending=False)\n    \n    # perform left-join to get information about the documents\n    doc_sim_meta_df = document_sim_df.merge(all_data,\n                      how='left',\n                     left_on='document_id',\n                     right_on='paper_id')\n    return(doc_sim_meta_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.options.display.max_colwidth = 100\ncos_sim('airborne').loc[range(0,10),\"title\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.options.display.max_colwidth = 100\ncos_sim('non-pharmaceutical interventions').loc[range(0,10),\"title\"]","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}