{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"''' Import Modules '''\nimport os\nimport sys\nimport warnings\nimport numpy as np \nimport pandas as pd \nfrom pandas import set_option\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nimport seaborn as sns\n        \nif not sys.warnoptions:\n    warnings.simplefilter('ignore')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-07-25T15:56:04.70063Z","iopub.execute_input":"2021-07-25T15:56:04.701005Z","iopub.status.idle":"2021-07-25T15:56:05.901207Z","shell.execute_reply.started":"2021-07-25T15:56:04.700921Z","shell.execute_reply":"2021-07-25T15:56:05.900284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"![](https://images-wixmp-ed30a86b8c4ca887773594c2.wixmp.com/f/8cc1eeaa-4046-4c4a-ae93-93d656f68688/denywk5-2590c412-4d10-480e-be61-97f565221a25.jpg?token=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJzdWIiOiJ1cm46YXBwOjdlMGQxODg5ODIyNjQzNzNhNWYwZDQxNWVhMGQyNmUwIiwiaXNzIjoidXJuOmFwcDo3ZTBkMTg4OTgyMjY0MzczYTVmMGQ0MTVlYTBkMjZlMCIsIm9iaiI6W1t7InBhdGgiOiJcL2ZcLzhjYzFlZWFhLTQwNDYtNGM0YS1hZTkzLTkzZDY1NmY2ODY4OFwvZGVueXdrNS0yNTkwYzQxMi00ZDEwLTQ4MGUtYmU2MS05N2Y1NjUyMjFhMjUuanBnIn1dXSwiYXVkIjpbInVybjpzZXJ2aWNlOmZpbGUuZG93bmxvYWQiXX0.X_0nd81_6-gx0GEdiz8PV0amNWj3cb7DQHi9GgKFEA8)\n\n## <b>1 | <span style='color:#2DB1AB'>INTRODUCTION</span></b>\n\n#### <b><span style='color:#2DB1AB'>1.1</span> | PROBLEM DEFINITION</b>\n\n- The goal of this problem is quite model centric; predict the target variable `SPL` (Sound Pressure Level) as accurately as possible, given a set of features (independent variables). \n- The data itself is quite interesting, so it's worth to explore it & understand something about fluid flow.\n- Let's also see if we can pinpoint specific parts of the data to some noise sources (listed above). Although, this is quite a challenge, given we don't have visual data.\n\n#### <b><span style='color:#2DB1AB'>1.2</span> | SOURCES OF AIRFOIL NOISE</b>\n\n##### <b><span style='color:#5D6D7E'>RELEVANT NOISE SOURCES</span></b>\n\n`Turbulent-Boundary-Layer-Trailing-Edge Noise (TBL-TE)`<br>\nAt high Reynolds number (based on chord length), turbulent boundary layers (TBL) develop over most of the airfoil. Noise is produced as this turbulence passes over the trailing edge (TE). <br>\n\n`Laminar-Boundary-Layer-Vortex-Shedding Noise (LBL-VS)`<br>\nAt low Reynolds number, largely laminar boundary layer (LBL) develop, whose instability result in vortex shedding (VS) and associated noise from the TE. For non aoa=0, the flow can separate near the TE on the suction side of the airfoil to produce TE noise due to the shed turbulent vorticity. <br>\n\n`Separation-Stall Noise`<br>\nAt very high aoa, the separated flow near the TE gives way to large-scale separation (deep stall), causing the airfoil to radiate low-frequency noise similar to that of a bluff body in flow. <br>\nIn the work of Wagner et al. (1996), it is indicated that mildly separated flow causes\nsound radiation from the trailing edge, whereas deep stall causes radiation from the whole airfoil.\n\nSo what we might try to do is pay attention to the features `u_inf`,`l_chord` (which we have), as they are relevant to `Reynolds Number`. The other two (`density`,`viscosity`) we don't have we'll have to assume they are constant.\n\n##### <b><span style='color:#5D6D7E'>LESS RELEVANT NOISE SOURCES</span><br></b>\n\n`Trailing-Edge-Bluntness-Vortex Shedding Noise` <br>\nAnother noise source is vortex shedding occuring in the small separated flow region aft of the blunt TE\nSource is likely irrelavent as the TE is quoted to be 'very sharp' with a thickness of less than 0.05mm <br>\n\n`Tip Vortex Formation Noise` <br>\nThe last source considered here is due to the formation of the tip vortex, containing high turbulent flow, occuring near the tips of lighting wings. \n\n*Sources of Noise from Airfoil Self-Noise and Prediction, T.F.Brooks et al, 1989*\n\n#### <b><span style='color:#2DB1AB'>1.3</span> | EXPERIMENTAL DESCRIPTION</b>\n- The NASA data set comprises different size NACA 0012 airfoils at various wind tunnel speeds and angles of attack. \n- The span of the airfoil and the observer position were the same in all of the experiments. Based on the origin of this dataset ([UCI Database](https://archive.ics.uci.edu/ml/datasets/Airfoil+Self-Noise)), it's not certain whether the dataset contains tripped or non tripped cases (which is critical to know)\n- If you are interested in reading the full paper, you can find it by searching the paper idenification `NASA RP-1218`, it's best to manually extract the data from the paper & not utilise the `UCI Database`, nevertheless.\n\n#### <b><span style='color:#2DB1AB'>1.4</span> | BRIEF FEATURE DESCRIPTION</b>\n\n##### <b><span style='color:#5D6D7E'>INDEPENDENT VARIABLES</span></b>\n- <b>freq</b> : Frequency, in Hertz (Hz). <br>\n- <b>aoa</b> : Angle of attack (degrees). <br>\n- <b>l_chord</b> : Chord length (m). <br>\n- <b>u_inf</b> : Free-stream velocity. <br>\n- <b>l_sdisp</b> : Suction side displacement thickness.\n\n##### <b><span style='color:#5D6D7E'>TARGET VARIABLES</span></b>\n- <b>spl</b> : Sound Pressure Level.\n    \n***\nMost Features most likely are quite straightforward with the exception for one; <b>relevant to l_sdisp</b>\n\n- The suction side displacement thickness was determined using an expression derived from\nboundary layer experimental data from Brooks(1989) \n- (Boundary Layer) displacement thickness is the imaginary increase in thicknes of the wall; due to the effect of growing boundary layer.\nScaled Sound Pressure Level (dB)","metadata":{}},{"cell_type":"markdown","source":"## <b>2 | <span style='color:#2DB1AB'>DATASET : INITIAL IMPRESSIONS</span></b>\n\nEarly data inspection with pandas trio .info(),describe(),head(), just some very early observations.","metadata":{}},{"cell_type":"code","source":"file = '/kaggle/input/noises/airfoil_self_noise.dat'\n\n# define shortform column names\ncolumns = ['freq','aoa','l_chord','u_inf','l_sdisp','spl']\ndf = pd.read_csv(file,names=columns,header=0,delimiter='\\t')","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","execution":{"iopub.status.busy":"2021-07-25T15:56:05.902551Z","iopub.execute_input":"2021-07-25T15:56:05.902834Z","iopub.status.idle":"2021-07-25T15:56:05.921924Z","shell.execute_reply.started":"2021-07-25T15:56:05.902807Z","shell.execute_reply":"2021-07-25T15:56:05.920999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2021-07-25T15:56:05.924259Z","iopub.execute_input":"2021-07-25T15:56:05.92469Z","iopub.status.idle":"2021-07-25T15:56:05.951618Z","shell.execute_reply.started":"2021-07-25T15:56:05.924643Z","shell.execute_reply":"2021-07-25T15:56:05.950536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"set_option('precision',2)\ndf.head(13).T","metadata":{"execution":{"iopub.status.busy":"2021-07-25T15:56:05.953994Z","iopub.execute_input":"2021-07-25T15:56:05.954545Z","iopub.status.idle":"2021-07-25T15:56:05.991454Z","shell.execute_reply.started":"2021-07-25T15:56:05.954499Z","shell.execute_reply":"2021-07-25T15:56:05.990153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The dataset contains data ordered by <b>frequency</b> subgroups; here 1kHz - 16kHz","metadata":{}},{"cell_type":"code","source":"df.describe()","metadata":{"execution":{"iopub.status.busy":"2021-07-25T15:56:05.992801Z","iopub.execute_input":"2021-07-25T15:56:05.99311Z","iopub.status.idle":"2021-07-25T15:56:06.029489Z","shell.execute_reply.started":"2021-07-25T15:56:05.993077Z","shell.execute_reply":"2021-07-25T15:56:06.02845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot Correlation to Target Variable only\ndef corrMat(df,target='demand',figsize=(9,0.5),ret_id=False):\n    \n    corr_mat = df.corr().round(2);shape = corr_mat.shape[0]\n    corr_mat = corr_mat.transpose()\n    corr = corr_mat.loc[:, df.columns == target].transpose().copy() \n    \n    if(ret_id is False):\n        f, ax = plt.subplots(figsize=figsize)\n        sns.heatmap(corr,vmin=-0.3,vmax=0.3,center=0, \n                     cmap='viridis',square=False,lw=2,annot=True,cbar=False)\n        plt.title(f'Feature Correlation to {target}')\n    \n    if(ret_id):\n        return corr","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-07-25T15:56:06.030986Z","iopub.execute_input":"2021-07-25T15:56:06.031408Z","iopub.status.idle":"2021-07-25T15:56:06.038974Z","shell.execute_reply.started":"2021-07-25T15:56:06.031366Z","shell.execute_reply":"2021-07-25T15:56:06.038299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corrMat(df,'spl',figsize=(7,0.5)) # Baseline Dataframe feature correlation to Signal","metadata":{"execution":{"iopub.status.busy":"2021-07-25T15:56:06.039865Z","iopub.execute_input":"2021-07-25T15:56:06.040124Z","iopub.status.idle":"2021-07-25T15:56:06.216055Z","shell.execute_reply.started":"2021-07-25T15:56:06.040098Z","shell.execute_reply":"2021-07-25T15:56:06.214993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <b>3 | <span style='color:#2DB1AB'> EXPLORITORY DATA EXPLORATION</span></b>\n\nIn our problem, we don't have a lot of features to go by. Visual data would be very valuable to for noise identification. EDA for this problem likely going to quite a challenge given how complex and variant anything associated to turbulence is. 'The main data relation we'll keep referring to is `freq` vs `spl`, which represents the frequency domain noise spectrum.\n\n#### <b><span style='color:#2DB1AB'>3.1</span> | EDA: PAIRGRIDS</b>\nLet's look at some two feature relation plots in the form of PairGrids. They are quite loaded with information, so let's pay attention mainly at 1D histograms and `bivariate` relations for our target variable, `spl`","metadata":{}},{"cell_type":"code","source":"lst_vars = df.columns.to_list();lst_vars","metadata":{"execution":{"iopub.status.busy":"2021-07-25T15:56:06.219489Z","iopub.execute_input":"2021-07-25T15:56:06.219881Z","iopub.status.idle":"2021-07-25T15:56:06.225933Z","shell.execute_reply.started":"2021-07-25T15:56:06.219846Z","shell.execute_reply":"2021-07-25T15:56:06.224984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"''' Draw a Bivariate Seaborn Pairgrid /w KDE density w/ '''\ndef snsPairGrid(df):\n\n    sns.set(style='whitegrid')\n    g = sns.PairGrid(df,diag_sharey=False,height=4)\n    g.fig.set_size_inches(15,15)\n    g.map_diag(sns.kdeplot, lw=2)\n    g.map_lower(sns.scatterplot,s=25,edgecolor=\"k\",linewidth=0.5,alpha=0.4)\n    g.map_lower(sns.kdeplot,cmap='plasma',n_levels=6,alpha=0.5)\n    plt.tight_layout()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-07-25T15:56:06.228196Z","iopub.execute_input":"2021-07-25T15:56:06.22887Z","iopub.status.idle":"2021-07-25T15:56:06.236911Z","shell.execute_reply.started":"2021-07-25T15:56:06.228826Z","shell.execute_reply":"2021-07-25T15:56:06.236254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"snsPairGrid(df)","metadata":{"execution":{"iopub.status.busy":"2021-07-25T15:56:06.238258Z","iopub.execute_input":"2021-07-25T15:56:06.238885Z","iopub.status.idle":"2021-07-25T15:56:34.181807Z","shell.execute_reply.started":"2021-07-25T15:56:06.238842Z","shell.execute_reply":"2021-07-25T15:56:34.180653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"`Univariate Histograms` <br>\n\n- All features but `spl` contain lots of localised peaks in the histogram, `freq`,`aoa`,`l_chord`,`l_sdisp` are all heavily skewed in distribution. <br>\n- As `u_inf`,`aoa` are our experiment conditions that are varied, it would be more useful to keep them as consistent as possible. \n- We have a lot of data for lower `aoa` as opposed to higher ones, less data for medium sized `l_chord` as well. Whilst `aoa` is probably pre-adjusted, `l_chord` & `u_inf` are probably not. \n\n`Bivariate Scatters` <br>\n\nLot's of interesting data relations here, focusing on `spl`: <br>\n\n`freq|spl` is a useful relation, generally increasing and reducing, let's find out more in depth about it next. 120-130dB at a frequency of about 0-3kHz is quite populated. Looks like we have multiple peaks 140dB @ 3kHz & 140dB @ 7kHz (estimation), data above `freq>10kHz` is less populated <br>\n`aoa|spl` is quite spread out throughout the entire data range. We need to subdivide the data a bit more, lots of spread of course suggests more simpler models will probably struggle here. <br>\n`l_chord|spl` is quite random, no clear pattern emerges. Lower `l_chord` is associated with higher `SPL` reducing then going up and down again. Subdivision would prove more insightful.\n\n**OTHER COMMENTS** <br>\n- We could say our data has a few feature with a bit of outliers, skewed features & feature with multiple peaks; suggesting nonlinearity. Especially for our target variable, `spl` we have a fair bit of scattering. A powerful model & data adjustment would prove quite valuable in this problem.\n- Lets consider splitting the problem into a low & high 'aoa' prediction as angle is a significant factor in what sources of noise are created, higher angles tend to generate larger separation, we actually have `l_sdisp`, which is worth exploring a little.\n\n#### <b><span style='color:#2DB1AB'>3.2</span> | EDA: FREQUENCY DOMAIN SPL DATA ANALYSIS</b>\n- The most notable relation in noise analyses is the `freq` vs `spl` relation (frequency domain noise data)\n- Looking at the entire data migh be a little overwhelming, let's check two `l_chord` cases only for now","metadata":{}},{"cell_type":"code","source":"def plotRel(ldf):\n    sns.set(style='whitegrid')\n    g = sns.relplot(x='freq',y='spl',col='u_inf',row='l_chord',data=ldf,\n                kind='line',legend='full',hue='aoa',palette='jet_r',height=4)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-07-25T15:56:34.183293Z","iopub.execute_input":"2021-07-25T15:56:34.1837Z","iopub.status.idle":"2021-07-25T15:56:34.190142Z","shell.execute_reply.started":"2021-07-25T15:56:34.183658Z","shell.execute_reply":"2021-07-25T15:56:34.189102Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"''' Create Data Subsets for different airfoil sizes '''\nsns.set(style='whitegrid')\npdlst_lchord = [] # list of pd subset data storing all unique 'l_chord'\nlst_lchord_unique = df['l_chord'].unique().tolist();lst_lchord_unique.sort() # get unique 'l_chord'&sort()\nfor i in lst_lchord_unique:\n    ldf = df[df['l_chord']==i].copy()\n    pdlst_lchord.append(ldf)    # add all unique subsets to list","metadata":{"execution":{"iopub.status.busy":"2021-07-25T15:56:34.191612Z","iopub.execute_input":"2021-07-25T15:56:34.191964Z","iopub.status.idle":"2021-07-25T15:56:34.211254Z","shell.execute_reply.started":"2021-07-25T15:56:34.191931Z","shell.execute_reply":"2021-07-25T15:56:34.210212Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# lets plot two cases, they are ordered so smaller one on top.\nplotRel(pdlst_lchord[0])\nplotRel(pdlst_lchord[2])","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-07-25T15:56:34.212267Z","iopub.execute_input":"2021-07-25T15:56:34.212562Z","iopub.status.idle":"2021-07-25T15:56:37.722051Z","shell.execute_reply.started":"2021-07-25T15:56:34.212534Z","shell.execute_reply":"2021-07-25T15:56:37.721001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- We can notice that the level of `spl` tends to go up, reach some maximum value and then go back down with increasing `freq` <br>\n- We can also note that for a fixed `l_chord`, we have a relation `freq` vs `spl` for various `aoa` cases, which is represented by a line for each combination available to us, telling us the noise `spl` at a specific frequency `freq`, we can make some observations: \n\n`l_chord=0.0254`<br>\n- `u_inf = 31.7` & `u_inf = 39.6` curves don't actually differ very much, suggesting the flow condition `u_inf` doesn't influence the flow very much in this range.\n- for `aoa=0` & `aoa=4.8` we can note very high and wide `spl` curves, the `freq` range of generated noise is much higher than larger `aoa` cases, `u_inf=71.3` also generates an additional peak. It's likely this is associated with `TBL-TE` & `LBL-VS` noise sources.\n- As `aoa` is increased, the noise `spl` output tends to start decreasing, and `freq` with it, at a certain `aoa`, a low `freq` peak starts to emerge. These cases are likely associated with `separated-stall` noise. For different `u_inf` the formation of a low `freq` is slightly different.\n\n`l_chord=0.1016`<br>\n- 75% of `spl` has slightly increased given a larger `l_chord`, the minumum has also reduced, and occurs at `u_inf=39.6` & `aoa=15.6`, on the graphs, we can see that a similar case in `l_chord=0.0254` & `aoa=17.4` generates higher spl output\n- This reducing in SPL tendency with a reduction in `l_chord` we can visually see especially for `aoa=0`.\n- We can note a similar tendency when increasing `aoa` as we noted earlier, the `freq` range reduces and starts generating a low `freq` peak. \n- We can also note an interesting twin peak at larger `aoa` values, they tend to be be within the 2kHz `freq` range. Smaller `l_chord` also had these peaks emerging, tending to be more flat, but for the larger `l_chord` it is much more visible with actual peaks, which may be an indicator of separated flow hitting the airfoil at some point or could be the noise associated with just separated flow; `Separation-Stall Noise` <br>\n- We might note the slight plateau for the first two `u_inf` cases instead of going down, mainly for the lower angles, it seems like for the smaller `l_chord` it transitions into the clearly visible secondary peak.\n\nLet's also not forget about Reynolds number, it's associated with also `density` & `viscosity`, but we don't have the data. An increase in Re should be associated with bottom&RHS. So perhaps `LBL-VS` is the associatiated noise source for the twin peak for `aoa=0,l_chord=0.0254,u_inf=71.3`\n\n#### <b><span style='color:#2DB1AB'>3.3</span> | EDA: BOUNDARY LAYER DATA (SPL vs L_SDISP)</b>\n- The feature `l_sdisp` is an indicator of how large the local boundary layer (BL) is at the sampled point on the upper surface.\n- Let's again, look at some `l_chord` cases separately and see if we can spot anything.","metadata":{}},{"cell_type":"code","source":"sns.set(style='whitegrid')\nlst = [0,2]\ndef relplot1(id):\n    for i in id:\n        g = sns.relplot(x='spl',y='l_sdisp',col='u_inf',row='l_chord',\n                    hue='aoa',sizes='spl',palette='jet_r',\n                    kind='scatter',legend='full',data=pdlst_lchord[i])\n        g.fig.set_size_inches(13,3)\n        leg = g._legend\n        leg.set_bbox_to_anchor([1.06,1])  # coordinates of lower left of bounding box\n        leg._loc = 1  # if required you can set the loc","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-07-25T15:56:37.723354Z","iopub.execute_input":"2021-07-25T15:56:37.723631Z","iopub.status.idle":"2021-07-25T15:56:37.730982Z","shell.execute_reply.started":"2021-07-25T15:56:37.723604Z","shell.execute_reply":"2021-07-25T15:56:37.73017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"relplot1(lst)","metadata":{"execution":{"iopub.status.busy":"2021-07-25T15:56:37.732022Z","iopub.execute_input":"2021-07-25T15:56:37.732551Z","iopub.status.idle":"2021-07-25T15:56:41.29063Z","shell.execute_reply.started":"2021-07-25T15:56:37.732511Z","shell.execute_reply":"2021-07-25T15:56:41.289668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Larger `aoa` are quite consistently associated with larger `l_sdisp`, increasing quite rapidly. `spl` association to `l_sdisp` is rather tricky and as we can note is different depending on `l_chord` cases.\n- `l_sdisp` does tend to reduce as `u_inf` is increased.\n- `l_sdisp` increases depending on `l_chord` size and tends to have a rather wide `spl` output.\n- `l_sdisp` variation doesn't seem to have any clearly distinctive relation to `spl`, it seems to be quite dependent the three variables `l_chord`,`aoa` & `u_inf`\n- Larger `l_sdisp` are associated with larger separation, however the transition can be quite different for pretty much any angle in between the 0 and the maximum value. What it quite probable is the larger `aoa` cases being associated with the `separation stall noise`, which we noted have a distinctive low `freq` peak or twin peak.\n\n#### <b><span style='color:#2DB1AB'>3.4</span> | EDA: LOW FLOW SEPARATION CASES (AOA=0)</b>\n\nIt was noted that `aoa=0` has the most complete combination of `l_chord` & `u_inf` data variation, let's try to explore this subset. Keeping in mind that the associated `TBL-TE` & `LVL-VS` noise sources should be relevant here as well.\n    \n##### <b><span style='color:#5D6D7E'>EDA: L_CHORD VARIATION IN FREQUENCY SPECTRUM (FREQ/SPL)</span></b>\nlet's take a look plotting the `freq` vs `spl` relation once again with the `u_inf` columns, but `aoa` changed to `l_chord`","metadata":{}},{"cell_type":"code","source":"df_a0 = df[df['aoa']==0.0].copy() # get all aoa=0 subset data","metadata":{"execution":{"iopub.status.busy":"2021-07-25T15:56:41.291754Z","iopub.execute_input":"2021-07-25T15:56:41.292055Z","iopub.status.idle":"2021-07-25T15:56:41.297683Z","shell.execute_reply.started":"2021-07-25T15:56:41.29203Z","shell.execute_reply":"2021-07-25T15:56:41.29654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.set(style='whitegrid')\ng = sns.relplot(x='freq',y='spl',col='u_inf',hue='l_chord',data=df_a0,palette='jet_r',\n            kind='line',legend='full',size='l_chord');plt.xscale('log')\ng.fig.set_size_inches(13,3)\nleg = g._legend\nleg.set_bbox_to_anchor([1.06,1])\nleg._loc = 1 ","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-07-25T15:56:41.299009Z","iopub.execute_input":"2021-07-25T15:56:41.299553Z","iopub.status.idle":"2021-07-25T15:56:44.989404Z","shell.execute_reply.started":"2021-07-25T15:56:41.299521Z","shell.execute_reply":"2021-07-25T15:56:44.988365Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Lower <b>u_inf</b> tend to be somewhat more spread out, covering more lower `freq` values (wider frequency range)\n- Lower <b>l_chord</b> tend to have <b>spl peaks in the higher freq region</b>, and visa versa. \n- Lower <b>l_chord</b> also tend to <b>generate higher maximum spl levels</b>, notable tendency of <b>spl</b>; reducing for larger `l_chord` cases.\n- Lower `u_inf` case subgroups `u_inf=31.7,39.6` are more alike, especially with the notable low frequency peak for larger `l_chord`, a relatively constant pleuteu in the `spl=~125` region is visible, it could just be same noise source shared by larger `l_chord` cases.\n\n##### <b><span style='color:#5D6D7E'>EDA: REYNOLDS NUMBER INFLUENCE</span></b>\n\n- <b>Reynolds Number</b> is partly represented by <b>u_inf</b> & <b>l_chord</b>, other two (<b>density</b>&<b>viscosity</b>) we don't have. (Let's assume they don't change):\n- <b>TBL-TE</b>,<b>LBL-VS</b> are two noise sources associated that we can associate with <b>Reynolds Number</b> variation. One said to occur at lower <b>Reynolds Number</b>, the other at higher <b>Reynolds Number</b>","metadata":{}},{"cell_type":"code","source":"df_a0['Reynolds'] = df_a0['u_inf']*df_a0['l_chord'] ","metadata":{"execution":{"iopub.status.busy":"2021-07-25T15:56:44.990636Z","iopub.execute_input":"2021-07-25T15:56:44.990915Z","iopub.status.idle":"2021-07-25T15:56:44.998186Z","shell.execute_reply.started":"2021-07-25T15:56:44.990888Z","shell.execute_reply":"2021-07-25T15:56:44.996048Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.set(style='whitegrid')\ng = sns.relplot(x='freq',y='spl',col='u_inf',hue='Reynolds',\n            data=df_a0,kind='line',palette='viridis');plt.xscale('log')\ng.fig.set_size_inches(13,3)\n\nleg = g._legend\nleg.set_bbox_to_anchor([1.06,1])  # coordinates of lower left of bounding box\nleg._loc = 1\nfor t in leg.texts:\n    t.set_text(t.get_text()[:4])","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-07-25T15:56:44.999493Z","iopub.execute_input":"2021-07-25T15:56:44.99979Z","iopub.status.idle":"2021-07-25T15:56:48.413939Z","shell.execute_reply.started":"2021-07-25T15:56:44.999761Z","shell.execute_reply":"2021-07-25T15:56:48.412929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- We can note the twin peak is associated with the lower `Reynolds` number, which is much higher `spl` than the lower `Reynolds` peak. \n- Lower `Reynolds` number cases are also associated with higher `spl` noise, in general. <br> \n- It's not quite straightforward to differentiate the two possible sources, low & high `Reynolds` numbers are also quite subjective. \n- What we can note that the higher `Reynolds` have two distrinctive peaks, which tend to tranform into a levelled out `plateau` at higher `freq` as the `Reynolds` number is reduced. \n- If there is a clear location a model might stumble on it would be the twin peak at `u_inf=71.3` which is quite probably associated with the low `Reynolds` associated `LBL-VS` noise.\n\n#### <b><span style='color:#2DB1AB'>3.5</span> | EDA: HIGHER FLOW SEPARATION CASES (AOA>0)</b>\n\n- Whilst `aoa=0` can still have flow separation, most notably at very high `u_inf`, the flow separation is very minor \n- Larger flow separation is associated with higher <b>aoa</b> values.\n- Let's group all the higher <b>aoa</b> case we have & see if we can note anything associated to `spl` as `aoa` is increased.","metadata":{}},{"cell_type":"code","source":"sns.set(style='whitegrid')\ndf_aplus = df[df['aoa']>0.0].copy()\ng = sns.relplot(x='freq',y='spl',col='l_chord',kind='line',hue='aoa',col_wrap=3,\n            data=df_aplus,palette='jet_r');plt.minorticks_on();plt.xscale('log')\ng.fig.set_size_inches(13,8)\nleg = g._legend\nleg.set_bbox_to_anchor([1.03,1]) \nleg._loc = 1","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-07-25T15:56:48.415483Z","iopub.execute_input":"2021-07-25T15:56:48.415813Z","iopub.status.idle":"2021-07-25T15:57:04.553168Z","shell.execute_reply.started":"2021-07-25T15:56:48.415774Z","shell.execute_reply":"2021-07-25T15:57:04.552111Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- An increase in `aoa` tends to move the spectrum diagonally towards lower `freq` and `spl` simultaneously\n- The higher the `aoa` becomes, the more notable the low `freq` peaks become, quite often generating a higher local `spl` maximum at a very narrow `freq` range.\n- Medium sized `l_chord` & `aoa` tend to be associated with more freqent localised `spl` variation in the mid frequency range.","metadata":{}},{"cell_type":"markdown","source":"## <b>4 | <span style='color:#2DB1AB'>MODEL GENERATION</span></b>\n\n- Let's create a separate model for low flow separation cases `aoa=0`\n- Create a model higher angle cases `aoa>0` (Associated with larger flow separation)\n- Finish of with a hybrid of the two.  (A generalised model)\n\n##### <b><span style='color:#5D6D7E'>PHYSICS PREDICTION</span></b>\n\n- It must be noted that in depth cross validation and the concept of 'overfitting/underfitting' are aspects that one should pay close attention to in a general Machine Learning project, as we generally are concerned in integrating our model on unseen data\n- We are assuming that physics of flow phenomenon would change radically for the model to become less valid given new data.\n\n#### <b><span style='color:#2DB1AB'>4.1</span> | GAUSSIAN PROCESS REGRESSION, GPR()</b>\n\n- GP has the ability to accurately adapt to data it is provided in a high dimensional space. Which can be its strength or weakness, depending on the problem. \n- The model is actually quite versatile, it has the ability to both overpredict & underpredict. How the model behaves is related to its hyperparameters.\n- Overfitting is likely if the model is used in its automated form, often implemented by default. It's usually a good idea to pay attention to these hyperparameters in a more manual search format.\n- Each hyperparameter has it's own role in how it changes the model, with its core being the covariance matrix, which defines all of the instance relation weights in a neat matrix format. Multiple covariance matrices have to be constructed to make a prediction, including matrix inversions, yet GP is the cheaper variant of all models associated with it.\n\n##### <b><span style='color:#5D6D7E'>MODEL HYPERPARAMETERS</span></b> `theta`,`sigma`,`sigma_n`\n- Two `hyperaparameters` are associated with the `covariance function` (`kernel` if you prefer); `theta` & `sigma`, this function is used to define all weights in the `covariance matrix`. These functions are used in both `variance` & `covariance` parts of the `covariance matrix`. These functions can be set in whatever combination suits your problem.\n- The last, `sigma_n` is a hyperparameter associated with the diagonal term in the `covariance matrix`, influencing the `variance` component only. Implying how relevant the training nodes are; (noise/noiseless) assumption.\n- A brief Class explanation is aded below:\n\n##### <b><span style='color:#5D6D7E'>MODEL INSTANTIATION OPTIONS</span></b>\nIn a OOP formulation, classes need to be initialised. Having done that, activates `__init__` content: <br>\n\n**HYPERPARAMETERS** <br>\n- `self.theta` is the `covariance function` associated `hyperparameter`, similar for the other two. \n- `__init__` sets the parameters to a default value (`theta=10`,`sigma=10`,`sigma_n=1`) if not set; GPR(). \n- You can set them manually GPR(theta=1,sigma=1,sigma_n=0.01,opt=False), but `opt=False` must be present to prevent `hyperparameters` to be overwritten. <br>\n\n**OTHER OPTIONS**\n- self.opt is the previously mentioned activator for `objective function` optimisation.\n- GPR.kernel is a common class variable of GPR, defining the type of `covariance function` used. The same function is used for all weight relations of the `covariance matrix`.\n\n**TRAINING THE GPR() MODEL**, `.fit(X,y)`\n- Setting `hyperparameters` & calculating the training `covariance matrix`. \n- Hyperparameters can be both set in the manner outlined above, or tuned based on a specific `objective function`. \n- Various `objective functions` exist, the full `likelihood` & simplified variant is included, `MSE` is another alternative.\n- Scipy's `optimize.minimize` class is used together with the `L-BFGS-B` approach to find the minimum, `Nelder-Mead` is also nice. It's worth trying different approaches as minimisation approaches vary.\n\n**MAKING PREDICTIONS USING THE GPR() MODEL**, `.predict(X)`\n- The `Covariance Matrix` for Training & Test Feature Matrices needs to be calculated.\n- Commonly referred to as the `posterior mean` is the main model prediction output.","metadata":{}},{"cell_type":"code","source":"from sklearn.base import BaseEstimator,RegressorMixin\nfrom numpy.linalg import cholesky, det, lstsq, inv, eigvalsh, pinv\nfrom scipy.optimize import minimize\npi = 4.0*np.arctan(1.0)\n\n# Usage similar to any sklearn model\nclass GPR(BaseEstimator,RegressorMixin):\n\n    ''' Class Instantiation Related Variables '''\n    # With just the one class specific GPC.kernel\n    def __init__(self,kernel='rbf',theta=10.0,sigma=10.0,sigma_n=0.01,opt=True):\n        self.theta = theta      # Hyperparameter associated with covariance function\n        self.sigma = sigma      #                       ''\n        self.sigma_n = sigma_n  # Hyperparameter associated with cov.mat's diagonal component\n        self.opt = opt          # Update hyperparameters with objective function optimisation\n        GPR.kernel = kernel  # Selection of Covariance Function, class specific instantiation\n\n    ''' local covariance functions '''\n    # Covariance Functions represent a form of weight adjustor in the matrix W/K\n    # for each of the combinations present in the feature matrix\n    @staticmethod\n    def covfn(X0,X1,theta=1.0,sigma=1.0):\n\n        ''' Radial Basis Covariance Function '''\n        if(GPR.kernel == 'rbf'):\n            r = np.sum(X0**2,1).reshape(-1,1) + np.sum(X1**2,1) - \\\n                                       2 * np.dot(X0,X1.T)\n            return sigma**2 * np.exp(-0.5/theta**2*r)\n\n        ''' Matern Covariance Class of Funtions '''\n        if(GPR.kernel == 'matern'):\n            lid=2\n            r = np.sum(X0**2,1)[:,None] + np.sum(X1**2,1) - \\\n                                        2 * np.dot(X0,X1.T)\n            if(lid==1):\n                return sigma**2 * np.exp(-r/theta)\n            elif(lid==2):\n                ratio = r/theta\n                v1 = (1.0+np.sqrt(3)*ratio)\n                v2 = np.exp(-np.sqrt(3)*ratio)\n                return sigma**2*v1*v2\n            elif(lid==3):\n                ratio = r/theta\n                v1 = (1.0+np.sqrt(5)*ratio+(5.0/3.0)*ratio**2)\n                v2 = np.exp(-np.sqrt(5)*ratio)\n                return sigma**2*v1*v2\n        else:\n            print('Covariance Function not defined')\n    \n    ''' Train the GPR Model'''\n    def fit(self,X,y):\n        \n        # Two Parts Associated with base GP Model:\n        # - Hyperaparemeter; theta, sigma, sigma_n selection\n        # - Definition of Training Covariance Matrix\n        # Both are recalled in Posterior Prediction, predict()\n        \n        ''' Working w/ numpy matrices'''\n        if(type(X) is np.ndarray):\n            self.X = X;self.y = y\n        else:\n            self.X = X.values; self.y = y.values\n        self.ntot,ndim = self.X.shape\n\n        ''' Optimisation Objective Function '''\n        # Optimisation of hyperparameters via the objective funciton\n        def llhobj(X,y,noise):\n            \n            # Simplified Variant\n            def llh_dir(hypers):\n                K = self.covfn(X,X,theta=hypers[0],sigma=hypers[1]) + noise**2 * np.eye(self.ntot)\n                return 0.5 * np.log(det(K)) + \\\n                    0.5 * y.T.dot(inv(K).dot(y)).ravel()[0] + 0.5 * len(X) * np.log(2*pi)\n\n            # Full Likelihood Equation\n            def nll_full(hypers):\n                K = self.covfn(X,X,theta=hypers[0],sigma=hypers[1]) + noise**2 * np.eye(self.ntot)\n                L = cholesky(K)\n                return np.sum(np.log(np.diagonal(L))) + \\\n                    0.5 * y.T.dot(lstsq(L.T, lstsq(L,y)[0])[0]) + \\\n                    0.5 * len(X) * np.log(2*pi)\n\n            return llh_dir # return one of the two\n\n        ''' Update hyperparameters based on set objective function '''\n        if(self.opt==True):\n            # define the objective funciton\n            objfn = llhobj(self.X,self.y,self.sigma_n)\n            # search for the optimal hyperparameters based on given relation\n            res = minimize(objfn,[1,1],bounds=((1e-5,None),(1e-5, None)),method='L-BFGS-B')\n            self.theta,self.sigma = res.x # update the hyperparameters to \n\n        ''' Get Training Covariance Matrix, K^-1 '''\n        Kmat = self.covfn(self.X,self.X,self.theta,self.sigma) \\\n                 + self.sigma_n**2 * np.eye(self.ntot) # Covariance Matrix (Train/Train)\n        self.IKmat = pinv(Kmat) # Pseudo Matrix Inversion (More Stable)\n        return self  # return class & use w/ predict()\n\n    ''' Posterior Prediction;  '''\n    # Make a prediction based on what the model has learned \n    def predict(self,Xm):\n        \n        ''' Working w/ numpy matrices'''\n        if(type(Xm) is np.ndarray):\n            self.Xm = Xm\n        else:\n            self.Xm = Xm.values\n        \n        # Covariance Matrices x2 required; (Train/Train,Train/Test)\n        mtot = Xm.shape[0]  # Number of Test Matrix Instances\n        # Covariance Matrix (Train/Test)   \n        K_s = self.covfn(self.X,Xm.values,self.theta,self.sigma)             \n        # Posterior Mean Prediction\n        self.mu_s = K_s.T.dot(self.IKmat).dot(self.y)      \n        return self.mu_s # return posterior mean","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2021-07-25T15:57:04.554808Z","iopub.execute_input":"2021-07-25T15:57:04.555381Z","iopub.status.idle":"2021-07-25T15:57:04.583535Z","shell.execute_reply.started":"2021-07-25T15:57:04.555335Z","shell.execute_reply":"2021-07-25T15:57:04.582692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### <b><span style='color:#2DB1AB'>4.2</span> | LOW FLOW SEPARATION MODEL</b>\n- We can assess the accuracy of the model by plotting the model prediction values against the `spl` values\n- Smaller error is associated with more linear lines\n- let's take a look at what the model predicts as well:","metadata":{}},{"cell_type":"code","source":"def modelEval(ldf,feature='spl'):\n\n    # Given a dataframe, split feature/target variable\n    dfX = ldf.copy()\n    t = ldf[feature].copy()\n    del dfX[feature]\n    \n    model = GPR(opt=True)    # instantiate GPR model\n    model.fit(dfX,t)        # fit model to desired feature matrix\n    \n    return model,dfX,t      # return model, feature matrix and target variable","metadata":{"execution":{"iopub.status.busy":"2021-07-25T15:57:04.585119Z","iopub.execute_input":"2021-07-25T15:57:04.58572Z","iopub.status.idle":"2021-07-25T15:57:04.59953Z","shell.execute_reply.started":"2021-07-25T15:57:04.585674Z","shell.execute_reply":"2021-07-25T15:57:04.598721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del df_a0['Reynolds']            # lets remove this feature\ndf_a0['aoa'] = 0.0               # we need to readd the feature\ntdf_a0 = df_a0['freq'].copy()\nmodel_a0,X0,y0 = modelEval(df_a0)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-07-25T15:57:04.603374Z","iopub.execute_input":"2021-07-25T15:57:04.603891Z","iopub.status.idle":"2021-07-25T15:57:05.072912Z","shell.execute_reply.started":"2021-07-25T15:57:04.603847Z","shell.execute_reply":"2021-07-25T15:57:05.071693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_a0['model_spl'] = model_a0.predict(X0)\n# plot experiment_spl vs model_spl\ng = sns.relplot(x='spl',y='model_spl',col='u_inf',hue='l_chord',data=df_a0,palette='viridis',\n            kind='scatter',legend='full');plt.minorticks_on()\ng.fig.set_size_inches(13,3)\nleg = g._legend\nleg.set_bbox_to_anchor([1.06,1])  # coordinates of lower left of bounding box\nleg._loc = 1  # if required you can set the loc\n\n# plot model spl vs freq prediction\ng = sns.relplot(x='freq',y='model_spl',col='u_inf',hue='l_chord',data=df_a0,palette='viridis',\n            kind='line',legend='full');plt.minorticks_on();plt.xscale('log')\ng.fig.set_size_inches(13,3)\nleg = g._legend\nleg.set_bbox_to_anchor([1.06,1])  # coordinates of lower left of bounding box\nleg._loc = 1  # if required you can set the loc","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-07-25T15:57:05.074758Z","iopub.execute_input":"2021-07-25T15:57:05.075393Z","iopub.status.idle":"2021-07-25T15:57:11.527787Z","shell.execute_reply.started":"2021-07-25T15:57:05.075338Z","shell.execute_reply":"2021-07-25T15:57:11.52708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- We can see that the `aoa=0` model is not quite as accurate as hoped, missing some details in the spectrum here and there, especially for lower `l_chord` cases. \n- It could simply be due to insufficient data, or may not be, we can confirm this by simply reintroducing another feature we had before; `Reynolds` number.","metadata":{}},{"cell_type":"code","source":"del df_a0['model_spl']\ndf_a0['Reynolds'] = df_a0['u_inf']*df_a0['l_chord']\n\nmodel_a0B,X0B,y0B = modelEval(df_a0)\ndf_a0['model_spl'] = model_a0B.predict(X0B)","metadata":{"execution":{"iopub.status.busy":"2021-07-25T15:57:11.528967Z","iopub.execute_input":"2021-07-25T15:57:11.529402Z","iopub.status.idle":"2021-07-25T15:57:11.856791Z","shell.execute_reply.started":"2021-07-25T15:57:11.529355Z","shell.execute_reply":"2021-07-25T15:57:11.855663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot experiment_spl vs model_spl\nf = sns.relplot(x='spl',y='model_spl',col='u_inf',hue='l_chord',data=df_a0,palette='viridis',\n            kind='scatter',legend='full');plt.minorticks_on()\nf.fig.set_size_inches(13,3)\nleg = f._legend\nleg.set_bbox_to_anchor([1.06,1])  # coordinates of lower left of bounding box\nleg._loc = 1  # if required you can set the loc\n\n# plot model spl vs freq prediction\ng = sns.relplot(x='freq',y='model_spl',col='u_inf',hue='l_chord',data=df_a0,palette='viridis',\n            kind='line',legend='full');plt.minorticks_on();plt.xscale('log')\ng.fig.set_size_inches(13,3)\nleg = g._legend\nleg.set_bbox_to_anchor([1.06,1])  # coordinates of lower left of bounding box\nleg._loc = 1  # if required you can set the loc","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-07-25T15:57:11.858349Z","iopub.execute_input":"2021-07-25T15:57:11.858975Z","iopub.status.idle":"2021-07-25T15:57:18.476293Z","shell.execute_reply.started":"2021-07-25T15:57:11.858925Z","shell.execute_reply":"2021-07-25T15:57:18.47559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Looks like adding the `Reynolds` feature improved the misspredictions, especially the lower `l_chord` cases, looks quite neatly aligned now.\n\n#### <b><span style='color:#2DB1AB'>4.3</span> | HIGH FLOW SEPARATION MODEL</b>\nTaking a look at the relation model prediction vs `spl` again:","metadata":{}},{"cell_type":"code","source":"model_aplus,X1,y1 = modelEval(df_aplus)\ndf_aplus['model_spl'] = model_aplus.predict(X1)\nf = sns.relplot(x='spl',y='model_spl',col='u_inf',hue='l_chord',data=df_aplus,palette='viridis',\n            kind='scatter',legend='full');plt.minorticks_on()\nf.fig.set_size_inches(13,3)\nleg = f._legend\nleg.set_bbox_to_anchor([1.06,1])  # coordinates of lower left of bounding box\nleg._loc = 1  # if required you can set the loc\n\ndel df_aplus['model_spl']\ndf_aplus['Reynolds'] = df_aplus['u_inf']*df_aplus['l_chord']\n\nmodel_aplusB,X1B,y1B = modelEval(df_aplus)\ndf_aplus['model_spl'] = model_aplusB.predict(X1B)\n\ng = sns.relplot(x='spl',y='model_spl',col='u_inf',hue='l_chord',data=df_a0,palette='viridis',\n            kind='scatter',legend='full');plt.minorticks_on()\ng.fig.set_size_inches(13,3)\nleg = g._legend\nleg.set_bbox_to_anchor([1.06,1])  # coordinates of lower left of bounding box\nleg._loc = 1  # if required you can set the loc","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-07-25T15:57:18.477342Z","iopub.execute_input":"2021-07-25T15:57:18.477725Z","iopub.status.idle":"2021-07-25T15:57:29.193601Z","shell.execute_reply.started":"2021-07-25T15:57:18.477687Z","shell.execute_reply":"2021-07-25T15:57:29.192904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- High flow separation cases are predicted much better than the lower flow separation case when using the same hyperparameter optimisation approach. \n- Some minor variation for higher `l_chord` cases can be noted. Let's see if the addition of `Reynolds` number can help increase the accuracy of the higher `l_chord` cases.\n- `Reynolds` feature also has a positive effect on model accuracy for the higher `aoa` model.","metadata":{}},{"cell_type":"markdown","source":"#### <b><span style='color:#2DB1AB'>4.4</span> | GENERAL MODEL (BOTH LOW/HIGH FLOW SEPARATION)</b>\nThe Overall model should have similar mispredictions to both previous models, let's take a look:","metadata":{}},{"cell_type":"code","source":"model_all,X2,y2 = modelEval(df)\ndf['model_spl'] = model_all.predict(X2)\nf = sns.relplot(x='spl',y='model_spl',col='u_inf',hue='l_chord',data=df,palette='viridis',\n            kind='scatter',legend='full')\nf.fig.set_size_inches(13,3)\nleg = f._legend\nleg.set_bbox_to_anchor([1.06,1])  # coordinates of lower left of bounding box\nleg._loc = 1  # if required you can set the loc\n\ndel df['model_spl']\ndf['Reynolds'] = df['u_inf']*df['l_chord']\n\nmodel_aplusB,X1B,y1B = modelEval(df)\ndf['model_spl'] = model_aplusB.predict(X1B)\ng = sns.relplot(x='spl',y='model_spl',col='u_inf',hue='l_chord',data=df,palette='viridis',\n            kind='scatter',legend='full');plt.minorticks_on()\ng.fig.set_size_inches(13,3)\nleg = g._legend\nleg.set_bbox_to_anchor([1.06,1])  # coordinates of lower left of bounding box\nleg._loc = 1  # if required you can set the loc","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-07-25T15:57:29.194635Z","iopub.execute_input":"2021-07-25T15:57:29.195059Z","iopub.status.idle":"2021-07-25T15:57:44.906257Z","shell.execute_reply.started":"2021-07-25T15:57:29.195017Z","shell.execute_reply":"2021-07-25T15:57:44.905257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Looks like we have very similar misspredictions for the overall model (both lower and higher `l_chord` cases.\n- After add the additional feature, by the looks of it, we have made quite an accurate model for the prediction of `spl`.\n- The Gaussian based model knows can be a little too accurate for many problems, in that case you can simply adjust the hyperparameters, especially the matrix diagonal term. In this problem, we assumed that the noise hyperparameter, `sigma_n` is equal to 0.01 here.\n\n## <b>5 | <span style='color:#2DB1AB'>CONCLUSION</span></b>\n\n- In this problem, we aimed to create a model that would predict the target variable `spl`, given a set of features. \n- A brief EDA was conducted, some interesting relations were noted. Each individual spectrum has its own unique relation to `spl`, it's not exactly possible to pinpoint exact spectrum noise sources, given we don't have anything visual to go by. \n- More notably was the `Reynolds` number relation that was noted in the experiment noise sources. This feature actually helped improve the model which was quite nice, \n- In the end, our customly created <b>Gaussian Process Regression</b> model was able to quite precisely predict the `spl`, with the addition of the new feature.","metadata":{}},{"cell_type":"markdown","source":"**References** <br>\n- T.F. Brooks, D.S. Pope, and A.M. Marcolini. Airfoil self-noise and prediction. Technical report, NASA RP-1218, July 1989. \n- If you find anything useful, please consider upvoting/commenting, it would be really appreciated, thank you.","metadata":{"trusted":true}}]}