{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import tensorflow as tf\nimport keras.backend as K\nimport numpy as np\nfrom keras.layers import Dense, Bidirectional, LSTM, Input\nfrom keras.layers import Activation, Embedding, Concatenate\nfrom keras.models import Model\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import train_test_split\n\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\n\nimport unicodedata\nimport re\n\nimport os\nimport io\nimport time","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"en_dir = \"../input/english-to-french/small_vocab_en.csv\"\nfr_dir = \"../input/english-to-french/small_vocab_fr.csv\"\n\ndef make_data(Xpath, Ypath, num_examples=None):\n    X = []\n    with open(en_dir,'r', encoding='utf-8') as f:\n        for line in f:\n            line = line.rstrip()\n            line = \"<SOS> \" + line +\" <EOS>\"\n            X.append(line)\n    Y = []\n    with open(fr_dir,'r', encoding='utf-8') as f:\n        for line in f:\n            line = line.rstrip()\n            line = \"<SOS> \" + line +\" <EOS>\"\n            Y.append(line)\n    \n    return X[:num_examples], Y[:num_examples]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def tokenize(text):\n    tokenizer = Tokenizer(filters=\"\")\n    tokenizer.fit_on_texts(text)\n    sequence = tokenizer.texts_to_sequences(text) \n    pad_tensor = pad_sequences(sequence, padding='post')\n    \n    return pad_tensor, tokenizer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_data(path, num_examples=None):\n    X , Y = make_data(path[0], path[1], num_examples)\n    \n    x_tensor, x_token = tokenize(X)\n    y_tensor, y_token = tokenize(Y)\n    \n    return x_tensor, y_tensor, x_token, y_token\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_examples = 50000\nX, Y, X_token, Y_token = load_data([en_dir, fr_dir], num_examples)\nmaxX, maxY = X.shape[1], Y.shape[1]\nprint(f\"Maximum length of english sentence {maxX}\")\nprint(f\"Maximum length of french sentence {maxY}\")\nprint(X[1])\nprint(len(X))\nprint(Y[1])\nprint(len(Y))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xtrain, xvalid, ytrain, yvalid = train_test_split(X, Y, test_size=0.3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BUFFER_SIZE= len(xtrain)\nBATCH_SIZE = 35000\nsteps_per_epoch = len(xtrain)//BATCH_SIZE\nEMBEDDING_DIMS = 128\nunits = 1024\n\nvocab_en = len(X_token.word_index) + 1\nvocab_fr = len(Y_token.word_index) + 1\n\ndataset = tf.data.Dataset.from_tensor_slices((xtrain, ytrain)).shuffle(BUFFER_SIZE)\ndataset = dataset.batch(BATCH_SIZE, drop_remainder=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xtrain.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Encoder(Model):\n    def __init__(self, vocab, n_a, embedding_dims):\n        super(Encoder, self).__init__()\n        self.n_a = n_a\n        self.vocab = vocab\n        self.dims = embedding_dims\n        self.embedding = Embedding(self.vocab, self.dims)\n        \n        self.lstm = Bidirectional(LSTM(self.n_a, return_sequences=True))\n        self.concat = Concatenate()\n        \n    def call(self,X):\n        #print(f\"X shape before embedding Encoder {X.shape}\")\n        X = self.embedding(X)\n        #print(f\"X shape embedding Encoder {X.shape}\")\n        # layer output, forward a and c, backward a and c\n        a = self.lstm(X)\n        #print(f\"a shape embedding Encoder {a.shape}\")\n        \n        return a","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_a = 19\nn_s = 23\n\nx_example, y_example = next(iter(dataset))\nencoder = Encoder(vocab_en, n_a, EMBEDDING_DIMS)\n\na = encoder(xtrain)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Attention(Model):\n    def __init__(self, units):\n        super(Attention, self).__init__()\n        self.Dense1 = Dense(units, activation='tanh')\n        self.Dense2 = Dense(units, activation='tanh')\n        self.Dense3 = Dense(1)\n        self.dot = tf.keras.layers.Dot(axes=1)\n        self.Concat = Concatenate(axis=-1)\n\n        \n    def call(self, s, a):\n        \n        #print(f\"S shape before concat, {s.shape}\")\n        s = tf.keras.layers.RepeatVector(a.shape[1])(s)\n        \n        concat = self.Concat([a, s])\n        #print(f\"concat shape{concat.shape}\")\n        energies = self.Dense3(self.Dense1(concat))\n        \n        alpha = tf.nn.softmax(energies, axis=1)\n        \n        context = self.dot([alpha,a])\n        \n        #print(f\"context shape {context.shape}\")\n        context = tf.reduce_sum(context, axis=1)\n        \n        \n        return context","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"att = Attention(n_s)\ns = tf.zeros((35000,n_s))\natt_context = att(s, a)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Decoder(Model):\n    def __init__(self, vocab, n_s):\n        super(Decoder, self).__init__()\n        \n        self.embedding = Embedding(vocab_fr, 128)\n        self.lstm = LSTM(n_s,return_state=True)\n        self.output_layer = Dense(23)\n        self.attention = Attention(n_s)\n        \n    def call(self, s, c, a):\n        \n        context = self.attention(s, a)\n        #print(f\"Shape of context {context.shape} before lstm\")\n        context = tf.expand_dims(context, 1)\n        #print(context.shape)\n        s, _, c = self.lstm(inputs=context, initial_state=[s, c])\n        #print(f\"shape of s and c {s.shape}::: {c.shape}\")\n        out = self.output_layer(inputs=s)\n        #print(f\"ouput after Dense{out.shape}\")\n        out = tf.nn.softmax(out, axis=1)\n        #print(f\"output after softmax {out.shape}\")\n        #print(\"=========================================================================\")\n        return out, s ,c","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab_fr","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def dd(s, c):\n\n\n    decoder = Decoder(vocab_fr, n_s)\n    out,s ,c  = decoder(s, c, a)\n    return out , s , c\n\ns0 = tf.zeros((35000, n_s))\nc0 = tf.zeros((35000, n_s))\n\ns = s0\nc = c0\n\noutputs = []\nfor i in range(maxY):\n    out, s, c = dd(s, c)\n    print(out.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(outputs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_example.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in outputs:\n    print(Y_token.index_word[i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"decoder = Decoder(vocab_fr, n_s)\ndef model(tx, ty, m, n_a, n_s, vocab_en, vocab_fr, EMBEDDING_DIMS):\n    \n    X = Input(shape=(tx,))\n    s0 = Input(shape=(n_s,))\n    c0 = Input(shape=(n_s,))    \n    \n    s= s0\n    c= c0\n    outputs = []\n\n    a = encoder(X)\n\n    for t in range(ty):\n        \n        output,s ,c  = decoder(s0, c0, a) \n        \n        outputs.append(output)\n        \n        #print(f\"output shape = {output.shape}\")\n\n    model = Model(inputs=[X, s0, c0], outputs=outputs)\n    print(output)\n    \n    return model\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = model(maxX, maxY, BATCH_SIZE, n_a, n_s, vocab_en, vocab_fr, EMBEDDING_DIMS)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"s0 = tf.zeros((BATCH_SIZE, n_s))\nc0 = tf.zeros((BATCH_SIZE, n_s))\nmodel.fit([xtrain[:BATCH_SIZE], s0, c0], ytrain[:BATCH_SIZE],\n         epochs=5, batch_size=32)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"''''@tf.function\ndef train_step(X, Y, enc_hidden):\n    loss = 0\n    with tf.GradientTape() as tape:\n        enc_output, enc_hidden = encoder(X, enc_hidden)\n        \n        dec_hidden = enc_hidden\n        dec_input = tf.expand_dims([Y_token.word_index['<sos>']] * BATCH_SIZE, 1)\n        \n        for t in range(1, Y.shape[1]):\n            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n            loss += loss_function(Y[:, t], predictions)\n            \n            dec_input = tf.expand_dims(Y[:, t], 1)\n            \n        batch_loss = (loss/int(Y.shape[1]))\n        variables = encoder.trainable_variables + decoder.trainable_variables\n        \n        gradients = tape.gradients(loss, variables)\n        \n        optimizer.apply_gradients(zip(gradients, variables))\n        \n        return batch_loss''''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''EPOCHS = 10\n\nfor epoch in range(EPOCHS):\n    start = time.time()\n    enc_hidden = encoder.initialize_hidden()\n    total_loss = 0\n    \n    for (batch, (X, Y)) in enumerate(dataset.take(steps_per_epoch)):\n        batch_loss = train_step(X, Y, enc_hidden)\n        total_loss += batch_loss\n        \n        if batch%100 == 0:\n            print(f\"Epoch {epoch+1} Batch {batch} Loss {batch_loss.numpy():.4f}\")\n            \n        \n        print(f\"Epoch {epoch+1} Loss{total_loss/steps_per_epoch:.4f}\")\n        print(f\"Time per Epoch {time.time() - start}\")'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}