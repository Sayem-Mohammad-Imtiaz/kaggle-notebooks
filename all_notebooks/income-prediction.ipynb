{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":1,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"},"cell_type":"markdown","source":"In this analysis we will be trying to obtain the best possible model to classify based on the income whether greater than 50K or less than 50K.\n\n1. Firstly we will be analysing the given dataset by finding any discrprncies in any feature columns.\n\n2. We will study the dataset to find if there are any missing values, if found we have to analyse them to decide whether to replace them or remove them. Remember removing the missing values is not always the option. Sometimes removing the values can drastically affect the training dataset which may ultimately affect the model accuracy. Hence we need to study them closely and then decide how to deal with them.\n\n3. We need to study the correlation between all the feature values. Sometimes there are repetative feautre columns but with different names. If the correlation between the features is found to be close to 1 then we can safely remove one of them while keeping the other. As removing any one will not affect the performance drastically. This is called FEATURE REDUCTION.\n\n4. This dataset has categorical values too. The problem with most of the categorical values is repitation of the same class with different name. To ease our analysis we can combine some of the classes into single class.\n\n5. For simplicity to deal with the categorical values i have used get_dummies method of pandas dataframe. There are other various methods that you can use to deal with categorical values.\n\n6. FEATURE SELECTION is another important concept which we need to understand while analysing data. So features may not contribute to the output of our model. Butif include they can affect the result in a not so good way. Hence we need to remove such features and choose only those that drastically affect the output. Again there are different ways in which FEATURE SELECTION can be done. But here i have used Univariate feature selection and Recurssive feature elimination with cross validation.\n\nTHIS ANALYSIS IS BEEN DONE FOR BEGINEERS PERSPECTIVE. THERE IS SO MUCH MORE THAT YOU CAN DO TO GO DEEPER.\n\nHAPPY ANALYSING!!!!\n"},{"metadata":{"collapsed":true,"_cell_guid":"ea698d05-b96b-4439-a836-482be194fc27","_uuid":"a13957549a24f894305fc17fc92904b31f6a9684","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('darkgrid')\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.feature_selection import SelectKBest, chi2","execution_count":2,"outputs":[]},{"metadata":{"_cell_guid":"f907cea0-ca6d-45cc-9d9c-f16c8808aa87","_uuid":"3fa09a997536bc8e3693db638face5a7ffa59c4a","trusted":true},"cell_type":"code","source":"#Analyse the first few rows of the dataset\ndf = pd.read_csv('../input/adult.csv')\ndf.head()","execution_count":3,"outputs":[]},{"metadata":{"_cell_guid":"4b764a39-721b-4d96-9543-45113cbb2c74","_uuid":"e21101eabb696e63d9bb88fe64e47e4e39506b68","trusted":true},"cell_type":"code","source":"#Checking the shape of the dataset \ndf.shape\n\n#nrows = 48842 and ncols = 15","execution_count":4,"outputs":[]},{"metadata":{"_cell_guid":"9f2260fd-e557-4bad-a245-50985c7b7c58","_uuid":"9b4cd7f5029da3ffeec18cc8b89dadc60e0df2a7","trusted":true},"cell_type":"code","source":"#describe method provides the basic info about the dataset such as max, min value. Standard deviation,\n#mean, median etc\ndf.describe()","execution_count":5,"outputs":[]},{"metadata":{"_cell_guid":"83b6369b-de84-4ba6-834c-a4d59040e68c","_uuid":"c3bcc0e91354caaa0f125590ce07cbcb3a616333","trusted":true},"cell_type":"code","source":"#info method is another useful feature that can be used to check if any missing values are present\n# in our feature columns. It also gives the data types of our feature columns\ndf.info()","execution_count":6,"outputs":[]},{"metadata":{"_cell_guid":"db6fa1ab-c8f4-4999-a3de-d40173ffa41b","_uuid":"f40e973116c6ac7b0424aa247f01472784f5a8d1","trusted":true},"cell_type":"code","source":"#Check the percentage and number of missing values in feature columns\n\nfor i in df.columns:\n    non_value = df[i].isin(['?']).sum()\n    if non_value > 0:\n        print(i)\n        print('{}'.format(float(non_value) / (df[i].shape[0]) * 100))\n        print('\\n')","execution_count":7,"outputs":[]},{"metadata":{"_cell_guid":"255feb67-93dc-41c4-b045-cdf1aa36f2ea","_uuid":"561f283b722fbd7ea3f378793f0e2aeafcd0adf7"},"cell_type":"markdown","source":"As the percentage of '?' appearing in the three features is less we can safely remove the rows contaning '?'"},{"metadata":{"collapsed":true,"_cell_guid":"a236a93b-6076-43aa-a1ad-a254e5f755dc","_uuid":"3dc26438a5899a30be35f3b8675b5ea10a330500","trusted":true},"cell_type":"code","source":"#selecting all the rows without the '?' sign.\ndf = df[df['workclass'] != '?']\ndf = df[df['occupation'] != '?']\ndf = df[df['native-country'] != '?']","execution_count":8,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"2f4694ee-d8f8-499e-8703-c4b7087c3d5d","_uuid":"e1d7c68dd6676fe6db96cff144bd550bc45740c3","trusted":true},"cell_type":"code","source":"# This 'fnlwgt' feature does'nt seem to make any sense and also the mean value of this feature is too high we can\n#remove it\ndf = df.drop('fnlwgt', axis=1)","execution_count":9,"outputs":[]},{"metadata":{"_cell_guid":"78fb32f5-d3c2-4554-9578-bf9145f1b1a2","_uuid":"717c74b0cf9c71ffeb68e8ed12e1aaa1d5d2eb2e","trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,12))\nsns.heatmap(df.corr(), annot=True, cmap='magma', linecolor='white', linewidths=1)\nplt.show()","execution_count":10,"outputs":[]},{"metadata":{"_cell_guid":"32d2d40d-03ce-4501-b48a-1c4bb1bbd5fe","_uuid":"8414aa210efe23dad4d784fcfc2cdd8e2caf3ca4"},"cell_type":"markdown","source":"As can be observed from the above heatmap the correlation among the features does'nt seem to be strong.\nHence we cannot reduce the number of features by droping few.\nWe need to try other methods of feture selections"},{"metadata":{"_cell_guid":"daad27d2-3470-478e-b7cc-cfe2a3657eb2","_uuid":"c6603854906780a0873578fadb047c7dfefccb2b","trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,8))\nsns.countplot(df['income'], hue = df['education'], palette = 'rainbow', edgecolor = [(0,0,0), (0,0,0)])\nplt.show()","execution_count":11,"outputs":[]},{"metadata":{"_cell_guid":"a9589e4c-2ce0-454e-9e3d-8dd8fdf70504","_uuid":"58a6c9ec7e5c7a0fa8b832a7c490d1d37524f7a6"},"cell_type":"markdown","source":"The above countplot looks interesting......\n1. It shows that the number of people earning less than 50K high school grads are in greater number as compared to people with other qualifications.\n\n2. Also when it comes to people earning more tham 50K, people having bachelors degree are in greater number as compared to other qualified people.\n\n3. But overall large number of people earn less than 50K and comparitively small number of people earn more than 50K."},{"metadata":{"_cell_guid":"4299b5a2-6773-4b62-8531-8d9db03b13de","_uuid":"7c2b576ccdfeaa9ed281e743b730e17980e212cc","trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,8))\nsns.countplot(y = df['income'], hue = df['gender'], palette = 'summer', edgecolor = [(0,0,0), (0,0,0)])\nplt.show()","execution_count":12,"outputs":[]},{"metadata":{"_cell_guid":"6bcb78b5-85f1-4183-b47e-033e9886c250","_uuid":"2a66b2649a53c54f67c7bc6d069717a293cb8984"},"cell_type":"markdown","source":"The above plot shows number of people earning more or less than 50K based on gender.\n1. When it comes to earning less than 50K males dominates as compared to female counterparts.\n2. When it comes to earning more than 50K also males dominates as compared to female counterparts.\n\nThe reason that in both the categories male dominates over females can be the education. Lets see if that is the case in the next plots."},{"metadata":{"_cell_guid":"111dc872-5913-4ee2-a9d1-5e283d3c879e","_uuid":"b99a2a059ee9047a912e99a760e393dc83618097","trusted":true},"cell_type":"code","source":"print(\"The number of men with each qualification\")\nprint(df[df['gender'] == 'Male']['education'].value_counts())\nplt.figure(figsize=(12,8))\nsns.barplot(x = df[df['gender'] == 'Male']['education'].value_counts().values, y = df[df['gender'] == 'Male']['education'].value_counts().index, data = df)\nplt.show()","execution_count":15,"outputs":[]},{"metadata":{"_cell_guid":"558c50e8-d6b9-45d9-99b5-99e92f641dce","_uuid":"1aeae0a3c31aba779da93f5c30545ee5e8e944e2"},"cell_type":"markdown","source":"The above information shows the number of males having different qualifications.\nAlso the plot shows that, the number of male High school grads are highest followed by college grads, Bachelors and Masters."},{"metadata":{"_cell_guid":"5caadcb6-6798-4686-bd9f-3e7a1cf36b05","_uuid":"d85a2d3797e62260317ffde0d0bf5cb06083f995","trusted":true},"cell_type":"code","source":"(\"The number of women with each qualification\")\nprint(df[df['gender'] == 'Female']['education'].value_counts())\nplt.figure(figsize=(12,8))\nsns.barplot(x = df[df['gender'] == 'Female']['education'].value_counts().values, y = df[df['gender'] == 'Female']['education'].value_counts().index, data = df)\nplt.show()","execution_count":16,"outputs":[]},{"metadata":{"_cell_guid":"f5205ae0-e6e7-4328-9261-af3ece5a6784","_uuid":"3b612e76bb8a5388d3b0ef59ce95b0927c49ca48"},"cell_type":"markdown","source":"The above information shows the number of females having different qualifications.\nAlso the plot shows that, the number of female High school grads are highest followed by college grads, Bachelors and Masters.\nSo , the same trend is followed for both males and females.\n\nBut we have to find out why males dominates over females in both categories of earning >50K and <50K.\n\nFrom the above two plots we can see that the number of people earning less than 50K are High school grads. Also when we see the value counts of High school grads, male = 10122 and female = 4661. Clearly there are more number of male high school grads than female hence greater number of male earn less than 50K than female.\n\nSimilar, explanation can be given when it comes to earning more than 50K. Again male dominates over female. This is because, there are more number of males having bachelors degree than female. Hence larger number of men earn more than 50K than women.\n"},{"metadata":{"_cell_guid":"a5e5be7c-0988-4a3c-9bfa-ac2768b874d4","_uuid":"45ae690aaa8e93932faa8fa25ce3037b42312e88","trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,8))\nsns.countplot(df['income'], hue = df['relationship'], palette = 'autumn', edgecolor = [(0,0,0), (0,0,0)])\nplt.show()","execution_count":17,"outputs":[]},{"metadata":{"_cell_guid":"c4ff3bb1-6a43-468e-bbae-02a6a3089400","_uuid":"d60063f7e26c5316a889e21159b5145629a666f8"},"cell_type":"markdown","source":"The above plot concludes that majority of husbands earn more than 50K than other members."},{"metadata":{"_cell_guid":"1361a1e6-e061-46c8-ad9c-a93a5643d14c","_uuid":"ddb2dfb116f15b99d0f8961b05338546f21adea5","trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,8))\nsns.countplot(df['income'], hue = df['occupation'], palette = 'BuGn_d', edgecolor = [(0,0,0), (0,0,0)])\nplt.show()","execution_count":18,"outputs":[]},{"metadata":{"_cell_guid":"f8e9b254-6013-43f1-908b-37136e478c20","_uuid":"3c60cecaff7e403ac37333103c6728d980434040"},"cell_type":"markdown","source":"Another interesting plot.....\nThe above plot shows the income with respect to occupation.\n\n1. The people involved in occupation such as other service, craft repair, Adm clerical are majority in numbers earning less than 50K.\n2. A somewhat sad fact that can be interpreted from the above plot is that, people having Armed forces as occupation earn very less as compared to all the other occupations.\n3. But when it comes to people earning greater than 50K than occupation like Prof Speciality and Exec manegerial pay more than any other occupation."},{"metadata":{"_cell_guid":"dc4207da-cd5a-410e-845e-3f4863378222","_uuid":"37322720a8063656bb4911d61345b277b122204b","trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,8))\nsns.countplot(df['income'], hue = df['race'], palette = 'Set3', edgecolor = [(0,0,0), (0,0,0)])\nplt.show()","execution_count":19,"outputs":[]},{"metadata":{"_cell_guid":"1481f271-625f-4118-9525-7443d2de153e","_uuid":"a079179dd965657ac9898f1f60f2e295818ef83d"},"cell_type":"markdown","source":"There are way less black people as compared to white people for both the categories of earning. And number of people with other race are negligible."},{"metadata":{"_cell_guid":"fd2933a7-43cf-4ae5-894f-191944786aa6","_uuid":"6d3a24a63fb7651e3d38a65248d66c953984e3be","trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,8))\nsns.countplot(df['income'], hue = df['workclass'], palette = 'Dark2', edgecolor = [(0,0,0), (0,0,0)])\nplt.show()","execution_count":20,"outputs":[]},{"metadata":{"_cell_guid":"362ec301-5687-44b3-85b9-4e8926f635e9","_uuid":"6c5b6df2400808ab045168936272e7aefaaf5511"},"cell_type":"markdown","source":"1. People in private workclass earn more than all the other workclasses."},{"metadata":{"_cell_guid":"5744167d-06c3-484c-b83d-6a182f4d1794","_uuid":"690600ec4b0f9935cd1a7f25283f3a9d9ad5cf38","trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12, 10))\nsns.boxplot(x='income', y='age', data=df, hue='gender', palette = 'prism')\nplt.show()","execution_count":21,"outputs":[]},{"metadata":{"_cell_guid":"8048225d-f217-4700-992f-9c59defdcb0e","_uuid":"7bb52dbfd576f8cbe106e96e402d8ced4385e59d"},"cell_type":"markdown","source":"1. The above boxplot points out the fact that people with higher age tend to earn more than peoplt with less age.\n2. Also male and female earning less than 50K are almost of the same age.\n3. Whereas the minimum age of both the gender earning >50K is same, but as the age increases the female counterparts\n  tends to earn at a younger age as compared to male counterparts."},{"metadata":{"_cell_guid":"aed5a897-42a8-402a-bd19-36436f891201","_uuid":"5f27bef973a42f73524716b099740303d02e0fe8"},"cell_type":"markdown","source":"**LETS DEAL WITH CATEGORICAL VALUES**"},{"metadata":{"_cell_guid":"7dc9b84f-89cc-4ba2-86ab-4bef8f4aa259","_uuid":"ca7dada6a6bf69c29867beb05ea851184863c829"},"cell_type":"markdown","source":"On of the most important step in feature cleaning is reducing the repetative feature variables. The marital statue feature is too much detailed. We can reduce the variables in marital status in two variables of married and not married"},{"metadata":{"_cell_guid":"0c177aba-b573-4d72-b4a4-5d35590facc4","_uuid":"ac38b53e01002eea2f92108dd0edda971bb1e6a6","trusted":true},"cell_type":"code","source":"#Lets check the unique variables of the feature\ndf['marital-status'].unique()","execution_count":22,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"11ab51ae-12c1-4034-80d2-304a84521a51","_uuid":"ac302bc150294da9b18794d73674f8c81a003ab5","trusted":true},"cell_type":"code","source":"#Replace the unwanted variables and distribute the variables into two variables namely 'married' and 'not married'\n\ndf['marital-status'] = df['marital-status'].replace(['Never-married', 'Married-civ-spouse', 'Widowed', 'Separated', 'Divorced',\n                                  'Married-spouse-absent', 'Married-AF-spouse'], ['not married', 'married', 'not married',\n                                   'not married', 'not married', 'not married', 'married'])","execution_count":23,"outputs":[]},{"metadata":{"_cell_guid":"43d40e57-7650-47ed-854d-d70d871b6c6d","_uuid":"9ee29be67c3fc8c60c461de6539f71346d540ecd","trusted":true},"cell_type":"code","source":"#Lets chech the head again of the dataframe\ndf.head()","execution_count":24,"outputs":[]},{"metadata":{"_cell_guid":"71104de3-3ef0-4cd6-9aa7-147677bdc622","_uuid":"4b33f641b0544b8dbd89aa0a3effaf83e3dad9eb"},"cell_type":"markdown","source":"Lets convert all the catagorical features into dummy variables using get_dummies method."},{"metadata":{"collapsed":true,"_cell_guid":"ec5ea883-6b17-488c-b1ab-265fa2cadf48","_uuid":"374eaf350816ab5a906f0579ad07c6851a0eced6","trusted":true},"cell_type":"code","source":"df = pd.get_dummies(df, columns=['workclass', 'education', 'marital-status', 'occupation', 'relationship','race', 'gender',\n                           'native-country'], drop_first=True)","execution_count":25,"outputs":[]},{"metadata":{"_cell_guid":"2cd53aad-ad71-4d69-bcad-59a388a681c3","_uuid":"42fc7f0b98df47ca5165665827ed29776706f033","trusted":true},"cell_type":"code","source":"df.head()","execution_count":26,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"e57d1517-5c3e-48d3-8765-756690519145","_uuid":"fa4614bbd0a9e29641478edc183aaae938f5826c","trusted":true},"cell_type":"code","source":"# Split the dataframe into features (X) and labels(y)\nX = df.drop('income', axis=1)\ny = df['income']","execution_count":27,"outputs":[]},{"metadata":{"_cell_guid":"08ea3741-ecf7-462f-8782-6b38d1a8611a","_uuid":"9e72d796622d58f2612506113caecd4c038fe8aa","trusted":true},"cell_type":"code","source":"y = pd.get_dummies(y, columns=y, drop_first=True)\ny = y.iloc[:,-1]\ny.shape","execution_count":28,"outputs":[]},{"metadata":{"_cell_guid":"4c005adf-be3b-4fb9-bde0-1405bb268368","_uuid":"d9b60f4c44e24533029c114196cbc93e0518b9a6"},"cell_type":"markdown","source":"**TRAIN TEST SPLIT**"},{"metadata":{"collapsed":true,"_cell_guid":"d565abfe-6ec2-4ee9-b182-a3bb4061b018","_uuid":"f2a9cb18b8fb96db597fd3d5b219c7fc64cfc152","trusted":true},"cell_type":"code","source":"# Split the dataset into training and testing set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=5)","execution_count":29,"outputs":[]},{"metadata":{"_cell_guid":"1a266a37-8b1c-45d8-b798-23a8ce06215f","_uuid":"15ba89c87f0d6c53131ff58623164f9357de5206"},"cell_type":"markdown","source":"All the catagorical values are converted into dummy values.\n\nNow we can start studying for FEATURE SELECTION\n"},{"metadata":{"_cell_guid":"837b3c17-5c0a-4d37-b4d8-c796d39a72ba","_uuid":"bdbadee3349ad3e837a8c3d442780f40de613b35"},"cell_type":"markdown","source":"**FEATURE SELECTION**\n\nWell there are various methods for feature selection such as:\n\n   1. Feature selection with correlation.\n   2. Univariate feature selection. (This is what i have used in this kernel)\n   3. Recursive feature elimination (RFE)\n   4. RFE with cross validation.\n\nYou can try any one or all of these to find the optimum model and corresponding parameters of that model and also try to improve the accuracy of the model by varying the feature selection methods.\n"},{"metadata":{"_cell_guid":"3520c07b-a09b-4b6e-b522-6469a3c5cf81","_uuid":"d5f28371b7e1e694605fe0bf0d8cd3806c639b47"},"cell_type":"markdown","source":"**UNIVARIATE FEATURE SELECTION**\n\nIn univariate feature selection we select the k highest features and negelect the other ones. K is the variable indicating the features with highest impact.\n"},{"metadata":{"_cell_guid":"58003848-c557-4a8e-8219-e9b797311f83","_uuid":"e7356241a5624dbacf3cb1d4a31b2abae53efb29","trusted":true},"cell_type":"code","source":"feature_select = SelectKBest(chi2, k = 8)  #finding the top 8 best features\nfeature_select.fit(X_train, y_train)","execution_count":30,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"3d7e5062-020a-4dca-95c7-e21138743f6b","_uuid":"3cf7bfd40ee688304fed39730890959c859a8b08","trusted":true},"cell_type":"code","source":"score_list = feature_select.scores_\ntop_features = X_train.columns","execution_count":31,"outputs":[]},{"metadata":{"_cell_guid":"43e7d037-d7a7-4234-a4e3-314413cef66f","_uuid":"2dc08d69929c0b555eb70af6fde49c32bc19590b","trusted":true},"cell_type":"code","source":"uni_features = list(zip(score_list, top_features))\nprint(sorted(uni_features, reverse=True)[0:8])","execution_count":32,"outputs":[]},{"metadata":{"_cell_guid":"484acf45-8d5f-445f-ae2d-2182b088e324","_uuid":"c8f1f782f0132763776f8db84a6e73fd5982ca75"},"cell_type":"markdown","source":"This gives the top 8 features\nWe will train the model on these features and find the performance"},{"metadata":{"_cell_guid":"9390cd8a-30bf-41ab-84d4-e0bdf5739dfa","_uuid":"aa2c26bc9b1163a2f4c63df215428fe538008b28"},"cell_type":"markdown","source":"**1. RANDOM FOREST**"},{"metadata":{"_cell_guid":"55226be1-ef47-4824-9128-fea54070b773","_uuid":"ed2d470729f3684c1069042182310e13a704e224","trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix\n\nX_train_1 = feature_select.transform(X_train)\nX_test_1 = feature_select.transform(X_test)\n\n#random forest classifier with n_estimators=10 (default)\nrf_clf = RandomForestClassifier()      \nrf_clf.fit(X_train_1,y_train)\n\nrf_pred = rf_clf.predict(X_test_1)\n\naccu_rf = accuracy_score(y_test, rf_pred)\nprint('Accuracy is: ',accu_rf)\n\ncm_1 = confusion_matrix(y_test, rf_pred)\nsns.heatmap(cm_1, annot=True, fmt=\"d\")\nplt.show()","execution_count":33,"outputs":[]},{"metadata":{"_cell_guid":"d80ffec9-1c6f-430c-bb7a-399d590032dc","_uuid":"b3be1e6c12ca5be8b4c66e884a119f569703319e"},"cell_type":"markdown","source":"We got 84% of accuracy.\n\nStill lot of our predictions seem to be wrong looking at the heatmap. One thing can be done is to change the number of top features and give a trial and error method in order to improve the efficiency."},{"metadata":{"_cell_guid":"a0b58e36-18f5-493c-b89e-48c6f86c23a5","_uuid":"52102b06eaeb6e5e3143da60e89cff369b5c04c5"},"cell_type":"markdown","source":"**2. K NEAREST NEIGHBORS**"},{"metadata":{"_cell_guid":"8e6a02a5-39d8-4295-ab26-e1ab07ce5621","_uuid":"b3b59ae1f863a0cad2dfe7a6f2461a211f829077","trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix\n\nX_train_2 = feature_select.transform(X_train)\nX_test_2 = feature_select.transform(X_test)\n\n\nknn_clf = KNeighborsClassifier(n_neighbors=1)      \nknn_clf.fit(X_train_2,y_train)\n\nknn_pred = knn_clf.predict(X_test_2)\n\naccu_knn = accuracy_score(y_test, knn_pred)\nprint('Accuracy is: ',accu_knn)\n\ncm_2 = confusion_matrix(y_test, knn_pred)\nsns.heatmap(cm_2, annot=True, fmt=\"d\")\nplt.show()","execution_count":34,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"ea9d706d-7847-4b2f-a4a0-f7a25dcd8c83","_uuid":"1885acbe2145c75ba0c2ad7d06754a2309bcdd00","trusted":true},"cell_type":"code","source":"accu_score = []\n\nfor k in range(1, 50):\n    knn = KNeighborsClassifier(n_neighbors = k)\n    knn.fit(X_train_2, y_train)\n    prediction = knn.predict(X_test_2)\n    accu_score.append(accuracy_score(prediction, y_test))","execution_count":35,"outputs":[]},{"metadata":{"_cell_guid":"49be7bdf-30a7-4187-8127-c9bc1b669fba","_uuid":"08dc85fbd2cbabc5450e4b9edb92565c1d825bc8","trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nplt.figure(figsize=(12, 8))\nplt.plot(range(1, 50), accu_score)\nplt.xlabel('K values')\nplt.ylabel('Accuracy score')\nplt.show()","execution_count":36,"outputs":[]},{"metadata":{"_cell_guid":"412ab47b-e87b-4c3a-bb6f-a2e69ff80a3b","_uuid":"cd50e60b90b24099656b9003deb25a87166200b0","trusted":true},"cell_type":"code","source":"X_train_3 = feature_select.transform(X_train)\nX_test_3 = feature_select.transform(X_test)\n\n\nknn_clf_1 = KNeighborsClassifier(n_neighbors=28)      \nknn_clf_1.fit(X_train_2,y_train)\n\nknn_pred_1 = knn_clf_1.predict(X_test_2)\n\naccu_knn_1 = accuracy_score(y_test, knn_pred_1)\nprint('Accuracy is: ',accu_knn_1)\n\ncm_3 = confusion_matrix(y_test, knn_pred_1)\nsns.heatmap(cm_3, annot=True, fmt=\"d\", cmap='Dark2')\nplt.show()","execution_count":37,"outputs":[]},{"metadata":{"_cell_guid":"bf0d87e1-b8b5-4e80-a2a8-e931e2f63022","_uuid":"99f1c3b5428a11c2688669422ace49dc2aa31ea4"},"cell_type":"markdown","source":"As it can be observed from the above heat map, by finding the optimal value of k the accuracy increased from 81% to 85%"},{"metadata":{"_cell_guid":"ec564c04-851a-4024-9d09-8dee31ed64ac","_uuid":"96a92a836e0db534ed1eb89a12a6e953068a3c11"},"cell_type":"markdown","source":"**K FOLD CROSS VALIDATION**"},{"metadata":{"_cell_guid":"21155d57-9f51-4a91-b037-7c3a4d2ee3a2","_uuid":"393677f75d151587b92d6a53882aae2f5d88245c","trusted":true},"cell_type":"code","source":"from sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier()\n\nkfold = KFold(n_splits = 10, random_state = 5)\n\nresult = cross_val_score(rf, X_train_1, y_train, cv=kfold, scoring='accuracy')\n\nprint(result.mean())","execution_count":38,"outputs":[]},{"metadata":{"_cell_guid":"36399db9-bc81-4d21-9715-3d467c1c1d74","_uuid":"fe500e934f0b1ffb757ac6b4ce5c3985b056ab1f"},"cell_type":"markdown","source":"**RFE WITH CROSS-VALIDATION AND RF CLASSIFICATION**"},{"metadata":{"_cell_guid":"2f785014-a207-4407-97ff-e404d01f11c9","_uuid":"f5204241e15e2e555d902c2495fd4c5da6a1b64a","trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import RFECV\n\n# The \"accuracy\" scoring is proportional to the number of correct classifications\nclf_rf_3 = RandomForestClassifier() \nrfecv = RFECV(estimator=clf_rf_3, step=1, cv=5,scoring='accuracy')   #5-fold cross-validation\nrfecv = rfecv.fit(X_train, y_train)\n\nprint('Optimal number of features :', rfecv.n_features_)\nprint('Best features :', X_train.columns[rfecv.support_])","execution_count":39,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"a2aa2203-875d-4b96-a058-fb98ba857ee8","_uuid":"f15b80fd38e41a2917d369bd9173d3b1f078f3e2","trusted":false},"cell_type":"markdown","source":"We got optimal number of features as 3 in order to get the maximum accuracy of the model."},{"metadata":{"trusted":true,"_uuid":"af362789f2c770d3ad676ed4163ac30b5d40cc97"},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nplt.figure(figsize = (10,8))\nplt.xlabel(\"Number of features selected\")\nplt.ylabel(\"Cross validation score of number of selected features\")\nplt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\nplt.show()","execution_count":40,"outputs":[]},{"metadata":{"_uuid":"0fd873e007e32f718a9310f4e4743d40ad9e456e"},"cell_type":"markdown","source":"As can be interpreted from the above graph for number of selected features to be 3 we obtain a cross-validation score of about 86%. Which is a slight increase from the optimal K nearest neighbors.\n\nAt the end all i want to conclude is that there are many more ways and algorithms that you can use to keep on improving the accuracy of the model.\n\nBy applying minor tweaks and changing and optimising the parameters of the model you can try to improve the performance of the model\n\nPlease feel free to suggest any corrections or edits in this kernel.\n\n**THANK YOU**\n\n"},{"metadata":{"_uuid":"9aeeedc0fcd170c5d2f6d4b3b9dfc07b5ab4a6e7"},"cell_type":"markdown","source":""}],"metadata":{"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}