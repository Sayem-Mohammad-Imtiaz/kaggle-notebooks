{"cells":[{"metadata":{"_cell_guid":"d27cf591-5cd7-469e-b581-3a840e91be84","_uuid":"bfd886509a703b16606379e3263f5fb14af1ece1"},"cell_type":"markdown","source":"# Ch. 19 - LSTM for Email classification\n\nIn the last chapter we already learned about basic recurrent neural networks. In theory, simple RNN's should be able to retain even long term memories. However, in practice, this approach often falls short. This is because of the 'vanishing gradients' problem. Over many timesteps, the network has a hard time keeping up meaningful gradients. See e.g. [Learning long-term dependencies with gradient descent is difficult (Bengio, Simard and Frasconi, 1994)](http://www.iro.umontreal.ca/~lisa/pointeurs/ieeetrnn94.pdf) for details.\n\nIn direct response to the vanishing gradients problem of simple RNN's, the Long Short Term Memory layer was invented. Before we dive into details, let's look at a simple RNN 'unrolled' over time:\n\n![Unrolled RNN](https://storage.googleapis.com/aibootcamp/Week%204/assets/unrolled_simple_rnn.png)","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"877eec38-78ea-49d8-b8cc-35581d4b1698","_uuid":"b69192af10eda1a99e1cb2b2144ee1769dc11122"},"cell_type":"markdown","source":"You can see that this is the same as the RNN we saw in the previous chapter, just unrolled over time.\n\n## The Carry \nThe central addition of an LSTM over an RNN is the carry. The carry is like a conveyor belt which runs along the RNN layer. At each time step, the carry is fed into the RNN layer. The new carry gets computed in a separate operation from the RNN layer itself from the input, RNN output and old carry.\n\n![LSTM](https://storage.googleapis.com/aibootcamp/Week%204/assets/LSTM.png)","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"429b49c1-4f02-4aed-8f54-4c6b4f30d66c","_uuid":"b36f6304e6b025beb81f0866ba24fa76d53f01ee"},"cell_type":"markdown","source":"The ``Compute Carry`` can be understood as three parts:\n\nDetermine what should be added from input and state:\n\n$$i_t = a(s_t \\cdot Ui + in_t \\cdot Wi + bi)$$\n\n$$k_t = a(s_t \\cdot Uk + in_t \\cdot Wk + bk)$$\n\nwhere $s_t$ is the state at time $t$ (output of the simple rnn layer), $in_t$ is the input at time $t$ and $Ui$, $Wi$ $Uk$, $Wk$ are model parameters (matrices) which will be learned. $a()$ is an activation function.\n\nDetermine what should be forgotten from state an input:\n\n$$f_t = a(s_t \\cdot Uf) + in_t \\cdot Wf + bf)$$\n\nThe new carry is the computed as \n\n$$c_{t+1} = c_t * f_t + i_t * k_t$$\n\nWhile the standard theory claims that the LSTM layer learns what to add and what to forget, in practice nobody knows what really happens inside an LSTM. However, they have been shown to be quite effective at learning long term memory.\n\nNote that ``LSTM``layers do not need an extra activation function as they already come with a tanh activation function out of the box.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"450ef168-8832-49cb-80c7-3ae53ad6d9b4","_uuid":"cc8e1f674674e872385acc3bf2776d170f5e13a1"},"cell_type":"markdown","source":"## The Data\n\nWithout much further ado, let's dive into the task of this chapter. The [Newsgroup 20 Dataset](http://qwone.com/~jason/20Newsgroups/) is a collection of about 20,000 messages from 20 newsgroups. [Usenet Newsgroups](https://en.wikipedia.org/wiki/Usenet_newsgroup) where a form of discussion group that where quite popular in the early days of the Internet. They are technically distinct but functionally quite similar to web forums. The newsgroups where usually dedicated to a certain topic, such as cars or apple computers. We can download the newsgroup 20 dataset directly through scikit learn.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"4f486ddc-4824-4f56-9b52-1787917d1685","_uuid":"5056dcafa3e5b4587482ec2b8f72b09727fbb71f","trusted":true},"cell_type":"code","source":"!ls ../input","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"270977f6-ba10-4672-b34e-abb58897d7ce","_uuid":"bd2e3765fddd6f190239b2e2100e001d77851e64","trusted":true},"cell_type":"code","source":"from sklearn.datasets.base import get_data_home, _pkl_filepath\nimport os\nCACHE_NAME = \"20news-bydate.pkz\"\nTRAIN_FOLDER = \"20news-bydate-train\"\nTEST_FOLDER = \"20news-bydate-test\"\n\ndata_home = get_data_home()\nprint(data_home)\ncache_path = _pkl_filepath(data_home, CACHE_NAME)\nprint(cache_path)\ntwenty_home = os.path.join(data_home, \"20news_home\")\nprint(twenty_home)\n\nif not os.path.exists(data_home):\n    os.makedirs(data_home)\n    \nif not os.path.exists(twenty_home):\n    os.makedirs(twenty_home)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"5e3ed2ac-e516-4cdf-9ac6-5ff561885fd6","_uuid":"60a8ec81a1bce5b8527a5e663d1b31f3dfb02b8e","trusted":true},"cell_type":"code","source":"os.path.exists(data_home)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"4820c84d-26ad-4bf0-88ab-7e32a09fdd4b","_uuid":"8c59ff07181321c8b7c8b27c0ce86e8a09a7c466","trusted":true},"cell_type":"code","source":"!ls /tmp","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"19f6f512-cc19-4edd-81ef-c736ebe4f76a","collapsed":true,"_uuid":"4c08f90ab52ecea6f51ec66d4e941b5dfce3b0c5","trusted":true},"cell_type":"code","source":"!cp ../input/20-newsgroup-sklearn/20news-bydate_py3* /tmp/scikit_learn_data","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"40c6e4c4-563e-4132-97d1-284babd7be1c","_uuid":"7580d546aa0da65667b0d6d2e47a13d2f7c221fc","trusted":true},"cell_type":"code","source":"os.path.exists(cache_path)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"4ab47db7-0620-43cc-9b6d-f26407e9905a","collapsed":true,"_uuid":"c00bcda6bc881fa5c5aeeb2b9c4bf22abe75d700","trusted":true},"cell_type":"code","source":"from sklearn.datasets import fetch_20newsgroups\ntwenty_train = fetch_20newsgroups(subset='train', shuffle=True, download_if_missing=False)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"2c0cba52-fa7c-4c69-b523-b186564f9a65","_uuid":"99d874edb2a355bf294055ada1c7ebd9dacdaf77"},"cell_type":"markdown","source":"The posts in the newsgroup are very similar to emails. (The \\n in the text means a line break)","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"8b581426-df4d-4d58-8f7d-8ee4dde85898","_uuid":"9e4eab5a1ffa93e92ebc9b1f734c7f994fea7810","trusted":true},"cell_type":"code","source":"twenty_train.data[1]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"fa91903b-b9d6-4bf5-8ff7-8f0c57de7c54","_uuid":"a995adcf86e9cf1c9984953a95da2f0218916884"},"cell_type":"markdown","source":"From the text you might be able to judge that this text is about computer hardware. More specifically it is about Apple computers. You are not expected to have expertise in the discussions around Macs in the 90's so we can also just look at a label:","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"184f8756-b2fe-4c28-93f1-ea6fd4744c0e","_uuid":"0948fc1743fb61c868d57567fb68108b52cb90de","trusted":true},"cell_type":"code","source":"twenty_train.target_names[twenty_train.target[1]]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ca1903d1-e1fa-492b-aafc-c3ffae2ce24a","_uuid":"58a90ef14953d3c00d7d5c1f34bbad9547ecb62c"},"cell_type":"markdown","source":"## Preprocessing the data\n\nYou already learned that we have to tokenize the text before we can feed it into a neural network. This tokenization process will also remove some of the features of the original text, such as all punctuation or words that are less common.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"b3dd90f1-a67a-49c8-9a29-94c7ec09d7e8","collapsed":true,"_uuid":"15ce3cbfcae7b6af1d0f802d4c840c817b38a845","trusted":true},"cell_type":"code","source":"texts = twenty_train.data # Extract text","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"100edb6d-fcc4-4162-b0bd-d260d91e377e","collapsed":true,"_uuid":"f9de5b4d6ac617f122b13e07172aad3279359a4d","trusted":true},"cell_type":"code","source":"target = twenty_train.target # Extract target","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"1e035099-cdbe-40a5-aeb8-4e94a7457ddd","_uuid":"9d9cba4462c5391d7bdeccf6a8286c573d10969d","trusted":true},"cell_type":"code","source":"# Load tools we need for preprocessing\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"62540aeb-0b49-4171-8e7c-ac4dadce2159","_uuid":"93bd1e0a8ef83c1d36d6a3a6dbf59eaa57483027"},"cell_type":"markdown","source":"Remember we have to specify the size of our vocabulary. Words that are less frequent will get removed. In this case we want to retain the 20,000 most common words.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"7e03fe84-2d08-403b-82b0-28d117dc068e","collapsed":true,"_uuid":"5cc0f4673a4df559aad85ffb21e685f5de8fb9a8","trusted":true},"cell_type":"code","source":"vocab_size = 20000","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"84268d63-7f77-4bdd-97a2-a8279b340847","collapsed":true,"_uuid":"fd1a639cd290c49cc97bda64efb514fafd7f299e","trusted":true},"cell_type":"code","source":"tokenizer = Tokenizer(num_words=vocab_size) # Setup tokenizer\ntokenizer.fit_on_texts(texts)\nsequences = tokenizer.texts_to_sequences(texts) # Generate sequences","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"76757929-31f2-4a85-929e-011499407245","_uuid":"069169b21fdea43c6334ecd2c9307031c3460f70","trusted":true},"cell_type":"code","source":"word_index = tokenizer.word_index\nprint('Found %s unique tokens.' % len(word_index))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"4e2509e9-01d6-4039-a790-21e0ac75e15e","_uuid":"4ce7f226d4075b7e0122ba30efae47e456fd22a7"},"cell_type":"markdown","source":"Our text is now converted to sequences of numbers. It makes sense to convert some of those sequences back into text to check what the tokenization did to our text. To this end we create an inverse index that maps numbers to words while the tokenizer maps words to numbers.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"77a50c22-de13-4933-b8b8-364b04cfd3af","collapsed":true,"_uuid":"8076eb37344f7b1f04cc12913db5ddc4fc1d0f57","trusted":true},"cell_type":"code","source":"# Create inverse index mapping numbers to words\ninv_index = {v: k for k, v in tokenizer.word_index.items()}","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"adca0ca9-dcee-403c-9887-5b8102d375eb","_uuid":"99510c98db9a325e2025534b3cb1ccf9d5258238","trusted":true},"cell_type":"code","source":"# Print out text again\nfor w in sequences[1]:\n    x = inv_index.get(w)\n    print(x,end = ' ')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c726fbcc-9e0d-4860-bd55-822198f38ada","_uuid":"e09517f67a5014cfc9b44d4a74b2cf31048f4796"},"cell_type":"markdown","source":"### Measuring text length\n\nIn previous chapters, we specified a sequence length and made sure all sequences had the same length. For LSTMs this is not strictly necessary as LSTMs can work with different lengths of sequences. However, it can be a pretty good idea to restrict sequence lengths for the sake of restricting the time needed to train the network and process sequences.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"d24ad4f3-1c7e-408e-b926-f51e8f9e569b","collapsed":true,"_uuid":"ee4d7a0a9e889ee0dee4307bbc317f8f255afc80","trusted":true},"cell_type":"code","source":"import numpy as np","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"319d4933-115a-49eb-ac34-7976e2ae2b2b","_uuid":"6a2a5104a4a345c1bd93d042734b5034dba631c7","trusted":true},"cell_type":"code","source":"# Get the average length of a text\navg = sum( map(len, sequences) ) / len(sequences)\n\n# Get the standard deviation of the sequence length\nstd = np.sqrt(sum( map(lambda x: (len(x) - avg)**2, sequences)) / len(sequences))\n\navg,std","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f89e3119-5ce6-437b-8841-68e481976018","_uuid":"2043270f6cd0df8392a4368fc26a3ce702a0e123"},"cell_type":"markdown","source":"You can see, the average text is about 300 words long. However, the standard deviation is quite large which indicates that some texts are much much longer. If some user decided to write an epic novel in the newsgroup it would massively slow down training. So for speed purposes we will restrict sequence length to 100 words. You should try out some different sequence lengths and experiment with processing time and accuracy gains.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"6acc7101-3b05-4302-bad3-4fe20b53aa68","collapsed":true,"_uuid":"a7520b4c51308d4c1c2decdeff8ebfd628ce3964","trusted":true},"cell_type":"code","source":"max_length = 100","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"53020108-b7bb-4ea3-93c9-e39f59c03336","collapsed":true,"_uuid":"2278bc08b537e9c80560c08a1f34a002d079302c","trusted":true},"cell_type":"code","source":"data = pad_sequences(sequences, maxlen=max_length)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"5d5c77a4-0ef1-4602-9c2d-f41fb2ba2480","_uuid":"1b210651bbce6277e504b0ea1eb15b7df34d820b"},"cell_type":"markdown","source":"## Turning labels into One-Hot encodings\n\nLabels can quickly be encoded into one-hot vectors with Keras:","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"9bc07094-3eea-4284-870a-1c944d890709","_uuid":"4d3bb2cdd37e893a1979bdb5e605b03ca05a52f1","trusted":true},"cell_type":"code","source":"import numpy as np\nfrom keras.utils import to_categorical\nlabels = to_categorical(np.asarray(target))\nprint('Shape of data:', data.shape)\nprint('Shape of labels:', labels.shape)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"73c95137-dd9e-4a71-a6f4-e62ff2885f15","_uuid":"015296894ba4820a363af404cc0f11e7f2922949"},"cell_type":"markdown","source":"## Loading GloVe embeddings\n\nWe will use GloVe embeddings as in the chapters before. This code has been copied from previous chapters:","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"26fa63f3-bd92-4aab-9280-b270dbdee4ce","_uuid":"3b981a5596e00038ed900e76be33ba89b9561cb1","trusted":true},"cell_type":"code","source":"import os\nglove_dir = '../input/glove-global-vectors-for-word-representation' # This is the folder with the dataset\n\nembeddings_index = {} # We create a dictionary of word -> embedding\nf = open(os.path.join(glove_dir, 'glove.6B.100d.txt')) # Open file\n\n# In the dataset, each line represents a new word embedding\n# The line starts with the word and the embedding values follow\nfor line in f:\n    values = line.split()\n    word = values[0] # The first value is the word, the rest are the values of the embedding\n    embedding = np.asarray(values[1:], dtype='float32') # Load embedding\n    embeddings_index[word] = embedding # Add embedding to our embedding dictionary\nf.close()\n\nprint('Found %s word vectors.' % len(embeddings_index))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"0aad355e-a43a-4ef4-8de7-912302bf4bed","_uuid":"5d328bd3cd3ec4226555c01288be189f5a90b94f","trusted":true},"cell_type":"code","source":"# Create a matrix of all embeddings\nall_embs = np.stack(embeddings_index.values())\nemb_mean = all_embs.mean() # Calculate mean\nemb_std = all_embs.std() # Calculate standard deviation\nemb_mean,emb_std","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"8549d7bb-77bb-418d-a2bd-73838c29af8e","collapsed":true,"_uuid":"7abb5deb0c52fbf5fced70ed10f6d126746a62ef","trusted":true},"cell_type":"code","source":"embedding_dim = 100 # We use 100 dimensional glove vectors","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"8d9a3a8f-fde2-48ce-af98-71591cdbe180","collapsed":true,"_uuid":"9375f22be07d4fbba8ff316c4119e9f415ee656d","trusted":true},"cell_type":"code","source":"word_index = tokenizer.word_index\nnb_words = min(vocab_size, len(word_index)) # How many words are there actually\n\n# Create a random matrix with the same mean and std as the embeddings\nembedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embedding_dim))\n\n# The vectors need to be in the same position as their index. \n# Meaning a word with token 1 needs to be in the second row (rows start with zero) and so on\n\n# Loop over all words in the word index\nfor word, i in word_index.items():\n    # If we are above the amount of words we want to use we do nothing\n    if i >= vocab_size: \n        continue\n    # Get the embedding vector for the word\n    embedding_vector = embeddings_index.get(word)\n    # If there is an embedding vector, put it in the embedding matrix\n    if embedding_vector is not None: \n        embedding_matrix[i] = embedding_vector","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"a55fa53f-9318-4366-8725-3c8e726d4d96","_uuid":"60597b23ef8071ed117ba7add9ea90a06331e76f"},"cell_type":"markdown","source":"## Using the LSTM layer\n\nIn Keras, the LSTM layer can be used in exactly the same way as the ``SimpleRNN``layer we used earlier. It only takes the size of the layer as an input, much like a dense layer. An LSTM layer returns only the last output of the sequence by default, just like a ``SimpleRNN``. A simple LSTM network can look like this:","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"e7c4fcf9-5e44-493b-9fa7-48fe8e65faa2","collapsed":true,"_uuid":"aa15939d11df38f53119cf35bd172dddfcf3f8df","trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import LSTM, Dense, Activation, Embedding","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"1471cb15-4876-4f82-b21b-ba093c156929","_uuid":"10f9c3046fa7f9163e4b93fcca903dca84643bc1","trusted":true},"cell_type":"code","source":"model = Sequential()\nmodel.add(Embedding(vocab_size, \n                    embedding_dim, \n                    input_length=max_length, \n                    weights = [embedding_matrix], \n                    trainable = False))\nmodel.add(LSTM(128))\nmodel.add(Dense(20))\nmodel.add(Activation('softmax'))\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3be85898-a4f9-4d79-864a-85d0d315c42d","_uuid":"fa4b9dc9c744a0422589a5c1016607971b8f3989","trusted":true},"cell_type":"code","source":"model.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['acc'])\n\nmodel.fit(data,labels,validation_split=0.2,epochs=2)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ba271623-0278-4202-b727-51e2552f778c","_uuid":"c1860d218b1d657ee634b733fdbe991ee414ec75"},"cell_type":"markdown","source":"Our model achieves more than 95% accuracy on the validation set in only 2 epochs. Systems like these can be used to assign emails in customer support centers, suggest responses, or classify other forms of text like invoices which need to be assigned to an department. Let's take a look at how our model classified one of the texts:","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"b7ed3baa-25ce-4918-86cc-d4991e291619","collapsed":true,"_uuid":"ea3c5537f9c7c942f9c7e865a12f922266ac6254","trusted":true},"cell_type":"code","source":"example = data[10] # get the tokens","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f33f85c3-1abe-4d31-bba1-d401acd3202d","_uuid":"3dbca1eee1d8de8a9c4da650dfbafa31913be757","trusted":true},"cell_type":"code","source":"# Print tokens as text\nfor w in example:\n    x = inv_index.get(w)\n    print(x,end = ' ')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"6c6f3ee8-96ef-4ece-8f1e-a08ccef0524e","collapsed":true,"_uuid":"991e688ef06e9fb984786fcffee884f5c4404638","trusted":true},"cell_type":"code","source":"# Get prediction\npred = model.predict(example.reshape(1,100))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"1eae718f-f973-487c-a0ac-4358e16dc5af","_uuid":"94726bda929e3e0c6790ad3842a6606c0e717000","trusted":true},"cell_type":"code","source":"# Output predicted category\ntwenty_train.target_names[np.argmax(pred)]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79890ba4-ce2f-40bb-9559-3ed794294d5a","_uuid":"a2ae9d271d41b9ee85d3461899b3b23fd0491300"},"cell_type":"markdown","source":"## Recurrent Dropout\n\nYou have already heard of dropout. Dropout removes some elements of one layers input at random. A common and important tool in recurrent neural networks is [_recurrent dropout_](https://arxiv.org/pdf/1512.05287.pdf). Recurrent dropout does not remove any inputs between layers but inputs between _time steps_.\n\n![Recurrent Dropout](https://storage.googleapis.com/aibootcamp/Week%204/assets/recurrent_dropout.png)\n\nJust as regular dropout, recurrent dropout has a regularizing effect and can prevent overfitting. It is used in Keras by simply passing an argument to the LSTM or RNN layer. Recurrent Dropout, unlike regular dropout, does not have an own layer.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"6917d511-b9c1-4a55-820f-dc6c8d149e36","_uuid":"61ce5a02a7647a039e2cfab25a6c1c924ab09fa3","trusted":true},"cell_type":"code","source":"model = Sequential()\nmodel.add(Embedding(vocab_size, \n                    embedding_dim, \n                    input_length=max_length, \n                    weights = [embedding_matrix], \n                    trainable = False))\n\n# Now with recurrent dropout with a 10% chance of removing any element\nmodel.add(LSTM(128, recurrent_dropout=0.1)) \nmodel.add(Dense(20))\nmodel.add(Activation('softmax'))\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"8d7f888c-0cb1-4dab-aa0c-2ca5319dc231","_uuid":"597b31fdab15696140dbb1445b27504327dd3b1c","trusted":true},"cell_type":"code","source":"model.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['acc'])\n\nmodel.fit(data,labels,validation_split=0.2,epochs=2)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"22e262cf-fddb-4931-9399-7b6ab85e32f5","collapsed":true,"_uuid":"9eb8dc0cc876c0cd0730e03bbab7c71a2bfcf729"},"cell_type":"markdown","source":"## Summary\nIn this chapter you have learned about LSTMs and how to use them for email classification. You also learned about recurrent dropout. Before you head into the weekly challenge, try these exercises:\n\n## Exercises:\n- Try running the LSTM with a longer max sequence length, or no max sequence length\n- Try combining an LSTM with a Conv1D. A good idea is to first use a ``Conv1D`` layer, followed by a ``MaxPooling1D`` layer followed by an LSTM layer. This will allow you to use longer sequences at reasonable speed.\n- Try using a [``GRU``](https://keras.io/layers/recurrent/#gru). GRUs work a lot like LSTMs but are a bit faster and simpler.","outputs":[],"execution_count":null}],"metadata":{"anaconda-cloud":{},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}