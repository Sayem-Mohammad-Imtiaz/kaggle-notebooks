{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Student Grade Prediction\n\n* Name: Ikhwanul Muslimin\n\n* Dataset: [Student Grade Prediction - Kaggle](https://www.kaggle.com/dipam7/student-grade-prediction)\n\n* Dataset information:\n<p> This data approach student achievement in secondary education of two Portuguese schools. The data attributes include student grades, demographic, social and school-related features) and it was collected by using school reports and questionnaires. Two datasets are provided regarding the performance in two distinct subjects: Mathematics (mat) and Portuguese language (por). In [Cortez and Silva, 2008], the two datasets were modeled under binary/five-level classification and regression tasks. Important note: the target attribute G3 has a strong correlation with attributes G2 and G1. This occurs because G3 is the final year grade (issued at the 3rd period), while G1 and G2 correspond to the 1st and 2nd period grades. It is more difficult to predict G3 without G2 and G1, but such prediction is much more useful (see paper source for more details).</p>\n\n* Relevant papers: [P. Cortez and A. Silva. Using Data Mining to Predict Secondary School Student Performance. In A. Brito and J. Teixeira Eds., Proceedings of 5th FUture BUsiness TEChnology Conference (FUBUTEC 2008) pp. 5-12, Porto, Portugal, April, 2008, EUROSIS, ISBN 978-9077381-39-7.](http://www3.dsi.uminho.pt/pcortez/student.pdf).","metadata":{"id":"VHyQ5G8t4R8x"}},{"cell_type":"markdown","source":"# 1. Import Libraries","metadata":{"id":"k8y1zK9Sdrn-"}},{"cell_type":"code","source":"# import EDA library\nimport pandas as pd\nimport numpy as np","metadata":{"id":"_IgCWFRDdvGP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import sklearn library\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nimport sklearn.metrics as metrics\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix","metadata":{"id":"HaO-RLHNd-nP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import stats library\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor","metadata":{"id":"4uNFW2Twqw9B","outputId":"d48fce4a-f2e5-4e0f-a43d-9d416002994b"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Reading the data","metadata":{"id":"G1fVQh_5eSLh"}},{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<code>df</code> for regression and <code>df2</code> for classification.","metadata":{"id":"5xsfaLtdyqEk"}},{"cell_type":"code","source":"# read the data\ndf = pd.read_csv('/kaggle/input/student-grade-prediction/student-mat.csv')\ndf2 = pd.read_csv('/kaggle/input/student-grade-prediction/student-mat.csv')","metadata":{"id":"umWmbHgUel-X"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Exploring the data","metadata":{"id":"InWzo8hRey_Q"}},{"cell_type":"code","source":"# display the first 5 rows of the data\ndf.head()","metadata":{"id":"5xyZYhK7e0yX","outputId":"813f594f-fb95-40f2-dafc-85887677c5bc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The data has 395 rows and 33 columns without any null values.","metadata":{"id":"_Hf-yIEXf61u"}},{"cell_type":"code","source":"# simple data checking - get dataframe general information\ndf.info()","metadata":{"id":"71xw7VskfX-7","outputId":"6baef6a5-0551-4adf-dc9c-5fb3f8187d79"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To do the regression, we have to eliminate target that have the value 0 so our <code>Difference</code> is not <code>Inf</code>.","metadata":{"id":"O5f-QRrFy5nW"}},{"cell_type":"code","source":"df.drop(df[df['G3'] < 1].index, inplace = True)","metadata":{"id":"xBNRLVPMo9mQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For classification, we need the average score.","metadata":{"id":"6n1cKyBO0bDA"}},{"cell_type":"code","source":"df2['Gavg']= round((df['G1']+df['G2']+df['G3'])/3, 2)","metadata":{"id":"gURjLHdLr_Ji"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Regression","metadata":{"id":"mzuTI2uTi_8b"}},{"cell_type":"markdown","source":"## Make dummy variable","metadata":{"id":"Engv6ViTjRP1"}},{"cell_type":"code","source":"df = pd.get_dummies(df, drop_first=True)\ndf.head()","metadata":{"id":"eMdSe2OXjTJM","outputId":"1a202cb7-6991-4a08-b2d2-9cfb727a249b"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Do the regression\n\nI will choose <code>G3</code> as output variable and the others for the input.","metadata":{"id":"GnFxx2x6jfTq"}},{"cell_type":"code","source":"out = df['G3']\ninp = df.drop(['G3'], axis=1) ","metadata":{"id":"t2e9QD1TjjfL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# split the data into train and test by 80:20\nx_train, x_test, y_train, y_test = train_test_split(inp, out, test_size=0.2, random_state=29)","metadata":{"id":"83xVcaE3kIbi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load the algorithm\nmodel = LinearRegression()","metadata":{"id":"WzvnOX2XkXDn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train the data\nmodel.fit(x_train, y_train)","metadata":{"id":"IqHA4c8alDIH","outputId":"b0ce5026-14b8-4ce1-ac1e-8fba6c133122"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# predict the y using trained model\ny_train_pred = model.predict(x_train)\ny_test_pred = model.predict(x_test)","metadata":{"id":"fqQadk7ulR3Q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Check the result","metadata":{"id":"6kmAmbqqpMAU"}},{"cell_type":"code","source":"# model result\nprint('Coefficients:\\n',model.coef_)\nprint('\\n')\nprint('Intercept:',model.intercept_)","metadata":{"id":"zAivjyInpNhn","outputId":"ef2a00d7-314d-4db6-c24f-a38b8c3419da"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We get a good result, because our $R^2 \\approx 0.92$.","metadata":{"id":"qnMwrpXG0sH1"}},{"cell_type":"code","source":"# MSE and R^2\nprint(\"MSE :\", metrics.mean_squared_error(y_test,y_test_pred))\nprint(\"R squared :\", metrics.r2_score(y_test,y_test_pred))","metadata":{"id":"FbcPY_GNsc43","outputId":"5dbda208-ab09-40d8-d359-8d64b4d49310"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Check our model's performance\nI will make a new dataframe that consist of:\n* Test prediction (our result)\n* Target data (real result)\n* Difference in %","metadata":{"id":"HFqE3nkLl4FU"}},{"cell_type":"code","source":"# - Test prediction\nperformance = pd.DataFrame(y_test_pred, columns=['Prediction'])\n# - Target data\ny_test = y_test.reset_index(drop=True)\nperformance['Target'] = y_test\n# - The difference in %\nperformance['Difference (%)']= np.absolute((performance['Target'] \n                                            - performance['Prediction'])/\n                                           performance['Target']*100)\nperformance.head()","metadata":{"id":"xRGpGYLcl7Ot","outputId":"06e1515e-5181-41c1-c547-92f72b7ab5b6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Our mean difference result is only $7.69\\%$.","metadata":{"id":"Eh9Ssh5-1bzQ"}},{"cell_type":"code","source":"# check the summary statistics\nperformance.describe()","metadata":{"id":"H-KzQtD-ncxX","outputId":"b95c32b5-ca9e-4f29-c178-d29679869131"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. Classification","metadata":{"id":"XAd7dV35rGnu"}},{"cell_type":"markdown","source":"## Make target value","metadata":{"id":"MsJE6gI4ti_q"}},{"cell_type":"code","source":"# make a new target value, which is Passed\ndf2['Passed']= np.where(df2['Gavg'] > 10, 1, 0)\ndf2.head()","metadata":{"id":"hPSnJRXIrJ7S","outputId":"7a62ed1a-5a34-46eb-88e3-3d35e88df118"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Do the classification: Logistic Regression","metadata":{"id":"WWr2798Zwr-o"}},{"cell_type":"code","source":"# choose column for each x dan y\ny = df2['Passed']\nx = (df2._get_numeric_data()).drop(['Gavg','Passed'], axis=1)\n\n# split the data into test and train\nx_train, x_test, y_train, y_test= train_test_split(x, y, test_size=0.2, random_state=29)\n\n# load the algorithm\nmodel = LogisticRegression(max_iter=1000)\n\n# train the data\nmodel.fit(x_train, y_train)\n\n# predict the y using trained model\ny_train_pred = model.predict(x_train)\ny_test_pred = model.predict(x_test)","metadata":{"id":"LlDbldTywuXo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Check the result","metadata":{"id":"EoShqJaHx2_h"}},{"cell_type":"markdown","source":"We get a perfect result, because our accuracy is $100\\%$. Wait what?!","metadata":{"id":"W9dhrXuY3AqX"}},{"cell_type":"code","source":"# evaluate classification model - accuracy\naccuracy_test = metrics.accuracy_score(y_test,y_test_pred)\nprint('Accuracy Test Data: {}'.format(accuracy_test))","metadata":{"id":"94tK__W6x4jI","outputId":"393ae25f-fd6c-4e0c-bcbe-45e09b17e75a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# classification report\nprint(classification_report(y_test,y_test_pred))","metadata":{"id":"m0WciTh8oDCs","outputId":"ef497907-38ba-46cd-a721-1f8c1193f016"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Is our model correct?\n\nI think there is something wrong with our model, maybe because we are using all of the features of our data? So, there are some variables that highly correlated with each other.","metadata":{"id":"YFoyocL0qiwL"}},{"cell_type":"markdown","source":"# 6. Classification - but with Multicollinearity","metadata":{"id":"I30ogc_eq4CE"}},{"cell_type":"markdown","source":"## Multicollinearity\n\nThe reason for the absurdity of our results is multicollinearity.\n\nMulticollinearity is the occurrence of high intercorrelations among two or more independent variables in a multiple regression model. Multicollinearity can lead to skewed or misleading results when a researcher or analyst attempts to determine how well each independent variable can be used most effectively to predict or understand the dependent variable in a statistical model [(source)](https://www.investopedia.com/terms/m/multicollinearity.asp).","metadata":{"id":"jMDWTNopupbU"}},{"cell_type":"markdown","source":"## Correlation between column","metadata":{"id":"eILfL6xfxSXq"}},{"cell_type":"code","source":"corr = df2.corr()\ncorr.style.background_gradient(cmap='coolwarm').set_precision(2)","metadata":{"id":"G6Ht7M3JxUWo","outputId":"2d349312-19a3-4984-8acb-01dbb8b32a09"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Check the Variance Inflation Factor (VIF)\n\nVariance inflation factor (VIF) is a measure of the amount of multicollinearity in a set of multiple regression variables. Mathematically, the VIF for a regression model variable is equal to the ratio of the overall model variance to the variance of a model that includes only that single independent variable [(source)](https://www.investopedia.com/terms/v/variance-inflation-factor.asp).","metadata":{"id":"kmDSg265vI1z"}},{"cell_type":"code","source":"# indicate which variables to compute VIF\nnew_x = x\n\n# add intercept\nnew_x['intercept'] = 1\n\n# compute VIF\nvif = pd.DataFrame()\nvif[\"variables\"] = new_x.columns\nvif[\"VIF\"] = [variance_inflation_factor(new_x.values, i) for i in range(new_x.shape[1])]\n\n# output\nvif","metadata":{"id":"zIfcggiNq_BQ","outputId":"16a874e8-d6a9-4bf4-b819-e56b6a00af6f"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Drop the columns which have VIF > 5","metadata":{"id":"tNIYwqVlvrop"}},{"cell_type":"code","source":"# drop the columns\ndf2.drop(columns=['G2','G3'], inplace=True)\ndf2.head()","metadata":{"id":"DgdyDmGir7qx","outputId":"d75dc6c1-cbb9-422b-fc5f-dd71dc295ce1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Do the classification: Logistic Regression","metadata":{"id":"I-ZaooWvv1Xs"}},{"cell_type":"code","source":"y = df2['Passed']\nx = (df2._get_numeric_data()).drop(['Gavg','Passed'], axis=1)","metadata":{"id":"xKqY8hELsgfX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# split the data into test and train\nx_train, x_test, y_train, y_test= train_test_split(x, y, test_size=0.2, random_state=29)\n\n# load the algorithm\nmodel = LogisticRegression(max_iter=1000)\n\n# train the data\nmodel.fit(x_train, y_train)\n\n# predict the y using trained model\ny_train_pred = model.predict(x_train)\ny_test_pred = model.predict(x_test)","metadata":{"id":"RsT527Clso6_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Check the result","metadata":{"id":"1dSLvnkGv5fh"}},{"cell_type":"markdown","source":"A little bit worse than before, $92.4\\%$.","metadata":{"id":"fujHKjobxC7Y"}},{"cell_type":"code","source":"# evaluate classification model - accuracy\naccuracy_test = metrics.accuracy_score(y_test,y_test_pred)\nprint('Accuracy Test Data: {}'.format(accuracy_test))","metadata":{"id":"b_TCNq3cstfL","outputId":"e983d73a-1f91-4f89-db99-051a12e4a27a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# classification report\nprint(classification_report(y_test,y_test_pred))","metadata":{"id":"i57lJEl5sudX","outputId":"b2246c7d-a958-497f-eeb5-81d32d06049a"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conclusion\nWe already have done the linear regression with $R^2 \\approx 0.92$ and the classification twice using Logistic Regression, the first one got $100\\%$ accuracy! However, in that case we have not taken into account the multicollinearity effect. After taking that into accout, our model is performed worse than before ($100\\%$ to around $92.4\\%$, but this result has increased the reliability of our model.","metadata":{"id":"eaQotIgV6DJ2"}}]}