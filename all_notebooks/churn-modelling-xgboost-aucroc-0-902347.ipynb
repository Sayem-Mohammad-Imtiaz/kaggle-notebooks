{"cells":[{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"## I am doing improvement base on the https://www.kaggle.com/duttasd28/churn-modelling-neural-nets-smote-xgboost. ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> __Churn Modelling__ : This problem is usually concerned with predicting whether a customer\nis going to stay with a company or not. This is very useful financially for companies as it\nhelps them to target customer groups.\nThis is typically a binary classification problem. That is you need to look at the data and\npredict whether the person will be __exiting(1)__ or the person will __stay(0)__."},{"metadata":{},"cell_type":"markdown","source":"In this dataset, we are going to use multiple approaches to this problem of churn modelling"},{"metadata":{},"cell_type":"markdown","source":"# Initial Setup"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"scrolled":false,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"scrolled":false},"cell_type":"code","source":"# Import the data\ndata = pd.read_csv('/kaggle/input/churn-modelling/Churn_Modelling.csv', index_col = 'RowNumber')\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see, **Exited** is our dependent feature. Other columns are independent features"},{"metadata":{},"cell_type":"markdown","source":"Let us check how many values of __Exited__ columns are there so that we can figure out if there is class imbalance or not"},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"data['Exited'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are about 8000 examples of '0' and 2000 examples of '1'. This indicates that we have severe class imbalance.\nThis means that if we have a simple naive classifier that predicts 0 all the time, we can easily achieve 80% accuracy.\n\nSo, we need to generate additional synthetic samples for our dataset so that modelling is effective."},{"metadata":{},"cell_type":"markdown","source":"# Data Preprocessing\nIn this step, we are going to preprocess our data so that we can use it on our models.\n\nPreprocessing involves the following:\n* Checking for NaN values that is missing values in the data\n* Visualise the data so that we can derive meaningful insights\n* Split to training and test datasets\n* Fill in NaN Values\n* Convert non numeric features to numeric features so that we can do predictions\n* Scale the data \n\nLet us go ahead with the first step, __checking for NaN/missing values__"},{"metadata":{},"cell_type":"markdown","source":"# Checking for Missing Values(NaN)"},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"# check for missing values\ndata.isnull().any()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Phew! We are lucky we did not get any null values. Usually there are null values in the dataset and we need to remove them"},{"metadata":{},"cell_type":"markdown","source":"# Data Visualization\nHere we are going to plot graphs regarding the data to get a deeper insight."},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"# Import necessary plotting libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Make figures inline\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us get a list of columns in the data so that we can predict better. \nWe use the .info() method to get the datatypes too"},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Geography, Gender, Surname** are object data-types, while others are either int / float."},{"metadata":{"trusted":true,"_kg_hide-input":true,"scrolled":false},"cell_type":"code","source":"plt.figure(figsize=(8, 8))\nsns.set()\nsns.boxplot(y = 'CreditScore', x = 'Exited', data = data, palette = 'husl');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"scrolled":false},"cell_type":"code","source":"plt.figure(figsize=(8, 5))\nsns.violinplot(y = 'Exited' , x = 'Gender' , data = data, kind='boxen', palette = 'hot');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"scrolled":false},"cell_type":"code","source":"plt.figure(figsize=(8, 5))\nsns.countplot(x = 'Geography' , data = data);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us plot a heatmap of the correlations of the features with each other. That will help us discard non useful features.\nIt also gives us some idea as to what features predict dependent column best"},{"metadata":{"trusted":true,"_kg_hide-input":true,"scrolled":false},"cell_type":"code","source":"plt.figure(figsize=(10, 10))\nsns.set(style = 'white')\nsns.heatmap(data.select_dtypes(include='number').corr(), annot = True, cmap = 'magma', square = True);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Pairplot - This plots graphs between every two variables. This is useful for visualisation"},{"metadata":{"trusted":true,"_kg_hide-input":true,"scrolled":false},"cell_type":"code","source":"# Pairplot\nplt.figure(figsize=(12, 8))\nsns.pairplot(data = data, corner = True, hue = 'Exited');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Converting non numeric features to numeric features\nWe convert non numeric features to numeric features.\nAlso we drop columns which do not seem to contribute anything useful like **CustomerId**, **Surname**.\n\nBut first we will split the dataset into train and test dataset."},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"# Drop a useless feature\ndata.drop(['CustomerId', 'Surname'], axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"# Get dependent and independent features\nX = data.iloc[:, :-1]\ny = data.iloc[:, -1].astype('float')\nX.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"# Splitting to train test dataset\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.25, random_state = 1)\nlen(y_train), len(y_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"# Reset the indexes of the splitted data frames\nX_train.reset_index(drop=True, inplace=True)\nX_val.reset_index(drop=True, inplace=True)\n\ny_train.reset_index(drop=True, inplace=True)\ny_val.reset_index(drop=True, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_cols = [col for col in X_train.columns if X_train[col].dtypes == object]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"# Label encoder object\nfrom sklearn.preprocessing import LabelEncoder\nlabel_encoder = LabelEncoder()\n\n# Create two empty data frames\nX_train_categorical, X_val_categorical = pd.DataFrame(), pd.DataFrame()\n\n# Label Encode the features\nfor col in categorical_cols:\n    X_train_categorical[col] = label_encoder.fit_transform(X_train[col])\n    X_val_categorical[col] = label_encoder.transform(X_val[col])\n\n# Drop the non required columns\nX_train.drop(categorical_cols, axis = 1, inplace = True)\nX_val.drop(categorical_cols, axis = 1, inplace=True)\n\n# put new colums in dataframe\nX_train = X_train.join(X_train_categorical)\nX_val = X_val.join(X_val_categorical)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Generating new data by oversampling\nSince we have an imbalanced dataset, we will increase the number of samples by SMOTE technique"},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"from imblearn.over_sampling import SMOTE\nsmk = SMOTE()\n# Oversample training  data\nX_train, y_train = smk.fit_sample(X_train, y_train)\n\n# Oversample validation data\nX_val, y_val = smk.fit_sample(X_val, y_val)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Final check at the dataset before putting in model\nNow we take a final look at the dataset"},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"X_train.shape, X_val.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"X_train[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Scaling\nWe scale the data so that datapoints are on the same level\n\n### Note: we have labelled data, so we should not scale all the data.Otherwise meaning will be lost"},{"metadata":{"trusted":true},"cell_type":"code","source":"columns = ['CreditScore', 'Balance', 'EstimatedSalary', 'Age']  ## Columns to modify\n\n## Subtract the mean, divide by standard deviation.\nfor col in columns:\n    colMean = X_train[col].mean()\n    colStdDev = X_train[col].std()\n    X_train[col] = X_train[col].apply(lambda x : (x - colMean) / colStdDev)\n    X_val[col] = X_val[col].apply(lambda x : (x - colMean) / colStdDev)    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Models\nWe will be using the following models \n* Logistic Regression\n* Decision Tree\n* Random Forest Classifier\n* Extra Trees Classifier\n* XGBClassifier\n* ANN"},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"# metric\nfrom sklearn.metrics import f1_score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Logistic Regression"},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nmodel = LogisticRegression(solver = 'lbfgs', max_iter = 300)\n\n# fit the data\nmodel.fit(X_train, y_train)\n\n# Get predictions\ny_preds = model.predict(X_val)\n\n# Get score\nf1_score(y_preds, y_val)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Decision Tree"},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\n\nmodel = DecisionTreeClassifier()\n\n# fit the data\nmodel.fit(X_train, y_train)\n\n# Get predictions\ny_preds = model.predict(X_val)\n\n# Get score\nf1_score(y_preds, y_val)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Random Forest Classifier"},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nmodel = RandomForestClassifier()\n\n# fit the data\nmodel.fit(X_train, y_train)\n\n# Get predictions\ny_preds = model.predict(X_val)\n\n# Get score\nf1_score(y_val, y_preds)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Extra Trees Classifier"},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"from sklearn.ensemble import ExtraTreesClassifier\n\nmodel = ExtraTreesClassifier()\n\n# fit the data\nmodel.fit(X_train, y_train)\n\n# Get predictions\ny_preds = model.predict(X_val)\n\n# Get score\nf1_score(y_val, y_preds)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# XGBoost"},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"from xgboost import XGBClassifier\n\nmodel = XGBClassifier()\n\n# fit the data\nmodel.fit(X_train, y_train)\n\n# Get predictions\ny_preds = model.predict(X_val)\n\n# Get score\nf1_score(y_val, y_preds)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost as xgb\n\n\nX_train = X_train\ny_train = y_train\nX_valid = X_val\ny_valid = y_val\n\n\nparams = {\"objective\":\"binary:logistic\",'colsample_bytree': 0.9,'learning_rate': 0.05,\n                'max_depth': 7, 'eval_metric':'auc'}\n\ndtrain = xgb.DMatrix(data=X_train.values,feature_names=X_train.columns,label=y_train.values)\ndvalid = xgb.DMatrix(data=X_valid.values,feature_names=X_valid.columns,label=y_valid.values)\n\nmod = xgb.train(params=params,\n                dtrain=dtrain,\n                num_boost_round=1000,\n                early_stopping_rounds=50,\n                evals=[(dvalid,'valid'), (dtrain,'train')],\n                verbose_eval=20)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get predictions\nfrom numpy import argmax\nfrom sklearn.metrics import roc_auc_score\ny_preds = mod.predict(dvalid)\n\nthreshold = []\nf1score = []\n# Get score\nfor i in np.arange(0.1,0.4,0.005):\n    threshold.append(i)\n    #print(f1_score(y_val, y_preds>i))\n    f1score.append(f1_score(y_val, y_preds>i))\nplt.plot(threshold,f1score)\nix = argmax(f1score)\nprint('best threshold = %f' %threshold[ix])\nprint('best f1score = %f' %f1score[ix])\nxgb_roc_auc_score = roc_auc_score(y_val,y_preds)\nprint('best AUCROC = %f' %xgb_roc_auc_score)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}