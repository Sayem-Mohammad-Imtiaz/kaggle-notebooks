{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Imports"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import cufflinks as cf\nimport plotly.offline\n# cf.go_offline()\n# cf.set_config_file(offline=False, world_readable=True)\nplotly.offline.init_notebook_mode(connected = True)\n# pd.options.plotting.backend = \"plotly\"\n\nimport matplotlib.pyplot as plt\n\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import plot_confusion_matrix, classification_report\nfrom sklearn.metrics import f1_score, hinge_loss, roc_curve, auc\n\nfrom sklearn.preprocessing import label_binarize\nfrom itertools import cycle\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# df = pd.read_csv('/kaggle/input/corpus-of-russian-news-articles-from-lenta/lenta-ru-news.csv')\n# tags = ['Футбол','Бокс и ММА','Летние виды','Зимние виды', 'Хоккей', 'Английский футбол']\n# sport = df[(df.tags.isin(tags)) | (df.topic == 'Спорт')]\n# sport.to_csv('sport_news.csv')\n# sport.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\n                #'/kaggle/working/sport_news.csv', \n                '/kaggle/input/for-sport-ru-news/sport_news.csv',\n                 index_col=0, parse_dates=['date'])\ndf.reset_index(drop=True, inplace=True)\ndf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Tags - targets"},{"metadata":{"trusted":true},"cell_type":"code","source":"(df.tags.value_counts()/df.shape[0]).plot.bar()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## URL"},{"metadata":{},"cell_type":"markdown","source":"Разобьем ссылку на составляющие"},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess_url(x):\n    return x.url.split('/')\n\nurl = df.apply(preprocess_url, axis=1, result_type='expand')\nurl.iloc[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Нас может заинтересовать здесь последняя составляющая (время уже учтено в наборе данных)"},{"metadata":{"trusted":true},"cell_type":"code","source":"url = url[[7]]\nurl","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Объединим тэги из ссылок с тематиками статей"},{"metadata":{"trusted":true},"cell_type":"code","source":"url = url.merge(df.tags, left_index=True, right_index=True)\nurl.to_csv('urt_to_tag.csv')\nurl","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# частотность тэгов\nurl[7].value_counts().sort_values().reset_index(drop=True).plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(url[7].value_counts() > 33).sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Большинство тэгов встречаются по небольшому количеству раз. И только чуть больше 200 тэгов являются часто употребляемыми \n\nВизуализиурем частотные слова по тематикам"},{"metadata":{"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud\n\nwcs, tags = [], []\nfor tag, gr in url.groupby('tags'):\n    wcs.append(WordCloud(width = 800, height = 800, \n                background_color ='white', colormap='Blues',\n                min_font_size = 10).generate(\" \".join(gr[7].values)))\n    tags.append(tag)\n    \ntags[1] = 'Англ. футбол'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pickle\n\npickle.dump(wcs, open('wcs.pickle', 'wb'), )\npickle.dump(tags, open('tags.pickle', 'wb'), )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(4,2, figsize=(15,30))\nfont = {'family': 'IBM Plex Sans',\n        'color':  'black',\n        'weight': 'normal',\n        'size': 60,\n        }\nfor ax, wc, tag in zip(axes.ravel(), wcs, tags):\n#     plt.figure(figsize = (10, 10), facecolor = 'white', edgecolor='blue') \n    ax.imshow(wc) \n    ax.axis(\"off\") \n    ax.set_title(tag, fontdict=font)\n    plt.tight_layout(pad = 0.5) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Для всех видов спорта частотными словами являются названия видов спорта или имена спорсменов.Но для летних видом спорта частотными словами являются - НХЛ, КХЛ, Овечкин итд (тематика хоккей). Видимо, при загрузке статей на сайт были допущены ошибки и часть статей, тема которых явно соответсвует хоккею, были опубликованы под тематикой летних видов спорта\nПроверим, что тэг действительно соответсвует тематике текста"},{"metadata":{"trusted":true},"cell_type":"code","source":"# тэг khl и тематика - Летние виды\ndf[(df.url.str.find('khl') != -1) & (df.tags == 'Летние виды')].text.iloc[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# кликабельная ссылка, чтобы удедиться, что информация соотвеует информации на сайте\ndf[(df.url.str.find('khl') != -1) & (df.tags == 'Летние виды')].iloc[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Заченим тематики в ручную -  статьи с основными хоккейными тегами будут с тематикой Хоккей"},{"metadata":{"trusted":true},"cell_type":"code","source":"to_hockey = ['khl', 'nhl', 'hockey', \n             'ovechkin', 'ovi', 'ove', \n             'malkin', 'tarasenko', 'kovalchuk', 'voynov',\n            'fetisov', 'radulov', 'jagr', 'datsyuk', 'bobrovsky',\n            'bryzgalov', 'znarok', 'markov', 'nabokov', 'anisimov',\n            'yakupov','gagarin', 'gonchar', 'khudobin', 'varlamov',\n            'mutko',]\n\ndf_ch = df.copy()\ndf_ch.tags[(url[7].isin(to_hockey))& ((df_ch.tags != 'Все')|(df_ch.tags != 'Все'))] = 'Хоккей'\n\nurl_ch = df_ch.apply(preprocess_url, axis=1, result_type='expand')\nurl_ch.drop([0,1,2,4,5,6, 8], axis=1, inplace=True)\nurl_ch = url_ch.merge(df_ch.tags, left_index=True, right_index=True)\nurl_ch.to_csv('urt_to_tag_changed.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wc = WordCloud(width = 800, height = 800, \n                background_color ='white',\n                min_font_size = 10).generate(\" \".join(url_ch[url_ch.tags=='Летние виды'][7].values))\nfont = {'family': 'serif',\n        'color':  'black',\n        'weight': 'normal',\n        'size': 16,\n        }\nplt.figure(figsize=(10,10))\nplt.imshow(wc) \nplt.axis(\"off\") \nplt.title('Летние виды', fontdict=font);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\n\nt = pd.concat((df.tags.value_counts()/df.shape[0], df_ch.tags.value_counts()/df.shape[0])).to_frame()\nt['type'] = 'changed'\nt['type'].iloc[:df.tags.value_counts().shape[0]] = 'ini'\n\nsns.barplot(data=t.reset_index(), x='index', y='tags', hue='type')\nplt.xticks(rotation=45);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.tags.value_counts().to_csv('tags_count.csv', encoding='windows-1251' )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df[(~df.tags.isna()) & (df.tags != 'Все')]\ndf_ch = df.copy()\ndf_ch.tags[(url[7].isin(to_hockey))& (df_ch.tags != 'Все')&(df_ch.tags != 'Футбол')] = 'Хоккей'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Import for nlp"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install pymystem3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\nnltk.download(\"stopwords\")\n\nfrom nltk.corpus import stopwords\nfrom pymystem3 import Mystem\nfrom string import punctuation\nfrom string import digits\n\nmystem = Mystem() \nrussian_stopwords = stopwords.words(\"russian\")\nrussian_stopwords","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess_text(text):\n    tokens = mystem.lemmatize(text.lower())\n    tokens = ' '.join(tokens).split()\n    tokens = [token for token in tokens\\\n              if token not in russian_stopwords\\\n              and token != \" \" \\\n              and token.strip(' ') not in digits\\\n              and token.strip(' ') not in punctuation]\n    \n    text = \" \".join(tokens)\n    return text","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preprocessing title and model"},{"metadata":{"trusted":true},"cell_type":"code","source":"import IPython\ndef display(*dfs):\n    for df in dfs:\n        IPython.display.display(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Проверим, что тоектизация и лемматизация выполняется корректно"},{"metadata":{"trusted":true},"cell_type":"code","source":"display(df.iloc[:10].title.apply(preprocess_text), df.iloc[:10].title )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"title = df.title.apply(preprocess_text)\ntitle.to_csv('prepocess_title.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Создадим мешок слов для признаков. Тематики пронумерум. И создадим hold out  выборку для финальной проверки"},{"metadata":{"trusted":true},"cell_type":"code","source":"# counter = TfidfVectorizer(ngram_range=(1,1))\ncounter = CountVectorizer(ngram_range=(1,1))\ntitle_preproc = counter.fit_transform(title)\nassert title_preproc.shape[0] == df.shape[0]\n\nle = LabelEncoder()\ny = le.fit_transform(df.tags)\nint_to_class = {i: cl for i,cl in enumerate(le.classes_)}\n\nx_train ,x_test, y_train, y_test = train_test_split(title_preproc, y,\n                                                    random_state=2020, stratify=y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nmodel = SGDClassifier(loss='log', random_state=2020, n_jobs=-1, class_weight='balanced',\n                     alpha=1*10e-5)\nmodel.fit(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test, model.predict(x_test)))\nprint(f1_score(y_test, model.predict(x_test), average='macro'))\nplot_confusion_matrix(model, x_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(confusion_matrix(y_test, model.predict(x_test))).to_csv('cm_title.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Используя только названия статьей уже можно получить хороший бейзлайн в 0.8 для f1"},{"metadata":{},"cell_type":"markdown","source":"# Preprocessing text and model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# %%time\n# mystem = Mystem() \n# text = df.text.apply(preprocess_text)\n# text.to_csv('text.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text = pd.read_csv('/kaggle/input/for-sport-ru-news/text.csv', index_col=0)\n\nle = LabelEncoder()\ny = le.fit_transform(df.tags)\nx_train, x_test, y_train, y_test = train_test_split(text.text, y,\n                                                    random_state=2020, stratify=y)\n\ncounter = TfidfVectorizer(ngram_range=(1,1), min_df=3)\nx_train_tf = counter.fit_transform(x_train.values)\nx_test_tf = counter.transform(x_test)\nx_train_tf.shape\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Попробуем уменьшить пространство признаков с помощью PCA, а заодно визуализируем датасет по тематикам"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import SparsePCA, TruncatedSVD\nimport plotly.express as px\n\n\npca = TruncatedSVD(n_components=1000)\npca.fit(x_train_tf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.Series(pca.explained_variance_ratio_).plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.Series(pca.explained_variance_ratio_).diff().plot(xlim=(0,50))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Первые 20 компонент имеют наибольшую дисперсию"},{"metadata":{"trusted":true},"cell_type":"code","source":"pca = TruncatedSVD(n_components=20)\n\nx_pca = pca.fit_transform(x_train_tf)\nx_pca.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"to_pca = pd.DataFrame(x_pca).merge(pd.Series(y_train,name='class'), left_index=True, right_index=True)\nto_pca['class'] = to_pca['class'].map(int_to_class)\nto_pca.to_csv('text_pca20.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"px.scatter_3d(to_pca, x=1, y=0, z=2, color='class', \n              title=f'Total Explained Variance: {pca.explained_variance_ratio_.sum() * 100:.2f}%',\n              labels={'0': 'PC 1', '1': 'PC 2', '2': 'PC 3'},  \n              category_orders={'class': np.sort(to_pca['class'].unique()).tolist()},\n              color_discrete_sequence= px.colors.sequential.Plasma_r\n              )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Тематика Бокса и ММА имеет явное отличие в пространсве признако и поэтому её будет проще всего предсказать. Остальные тематики смешаны друг относительно друга и находятся в одной плоскости "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_validate\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nx_train, x_test, y_train, y_test = train_test_split(text.text, y,\n                                                    random_state=2020, stratify=y)\npipeline = Pipeline([('counter', TfidfVectorizer(ngram_range=(1,2), min_df=10)),\n#                     ('pca', TruncatedSVD(n_components=20)),\n                    ('model', SGDClassifier(loss='log', random_state=2020, n_jobs=-1, class_weight='balanced',\n                     alpha=0.5*10e-6))\n#                      ('model', SGDClassifier( random_state=2020, n_jobs=-1, class_weight='balanced',\n#                      alpha=0.5*10e-5))\n#                      ('model', RandomForestClassifier(n_estimators=150, class_weight='balanced', n_jobs=1,\n#                                                       max_depth=10,\n#                                                       min_samples_split=3, #max_features='log2'\n#                                                      ))                     \n                    ])\n\nresults = cross_validate(pipeline, x_train, y_train,\n                    scoring='f1_macro', cv=3,\n                    return_train_score=True, n_jobs=-1)\nresults, results['test_score'].mean(), results['train_score'].mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Лучший результат модели на кросс валидации - 0.891. Проверим итоговое значение метрики на отложенной выборке"},{"metadata":{"trusted":true},"cell_type":"code","source":"pipeline.fit(x_train, y_train)\npred = pipeline.predict(x_test)\nprint(classification_report(y_test, pred))\nprint(f1_score(y_test, pred, average='macro'))\nplot_confusion_matrix(pipeline, x_test, y_test);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"int_to_class","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cm = pd.DataFrame(confusion_matrix(y_test, pipeline.predict(x_test)))\ncm.columns = cm.columns.map(int_to_class)\ncm.index = cm.index.map(int_to_class)\ncm.to_csv('cm_text.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Missclassification study\nИзучим по подробней статьи на которых классификатор ошибается"},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = pipeline.predict(x_test)\nmiss_class_x = x_test[y_test != pred]\nmiss_class_y = y_test[y_test != pred]\nmiss_class_pred = pred[y_test != pipeline.predict(x_test)]\nidx = y_test != pipeline.predict(x_test)\n\nprediction = pd.concat((df.loc[miss_class_x.index][['title', 'text']].reset_index(drop=True), pd.Series(miss_class_y).map(int_to_class), pd.Series(miss_class_pred).map(int_to_class)), axis=1)\nprediction.rename(columns={0: 'true', 1:'predict'}, inplace=True)\nprediction.to_csv('prediction.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(10,5))\nfig.suptitle('The probability distribution of main class ')\nax = plt.subplot(121)\npd.Series(pipeline.predict_proba(x_test.loc[idx]).max(axis=1)).plot(kind='hist', bins=20, ax=ax, \n                                                                    title='in miss-classification sample')\n\nax = plt.subplot(122)\npd.Series(pipeline.predict_proba(x_test.loc[~idx]).max(axis=1)).plot(kind='hist', bins=20, ax=ax,\n                                                                    title='in right classification samples')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Для правильно классифицированных статей характерны высокие значения вероятности. "},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_miss = pd.Series(pipeline.predict_proba(x_test.loc[idx]).max(axis=1))\npred_right = pd.Series(pipeline.predict_proba(x_test.loc[~idx]).max(axis=1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"threshold = 0.90\nprint(f'classificate as right in missclassification:\\t {(pred_miss > threshold).sum() / pred_miss.shape[0] * 100}')\nprint(f'not sure for right classification:\\t\\t {(pred_right < threshold).sum() / pred_right.shape[0] * 100}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def highlight_max(s):\n    '''\n    highlight the maximum in a Series yellow.\n    '''\n    is_max = s == s.max()\n    return ['background-color: yellow' if v else '' for v in is_max]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"int_to_class","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"i = pd.Series(pred_miss)[(0.80 < pred_miss) ].index\nt = pd.concat((x_test.iloc[idx].iloc[i].reset_index(), \n               pd.DataFrame(pipeline.predict_proba(x_test.iloc[idx].iloc[i])),\n               pd.Series(y_test[idx][i], name='true')), \n         axis=1)\n\nt['pred'] = t.iloc[:, -8:-1].idxmax(1).map(int_to_class)\nt['true'] = t['true'].map(int_to_class)\nt['prob'] = t.iloc[:, -10:-3].max(1)\nt[['text', 'pred', 'true', 'prob']].sort_values('prob', ascending=False).style.format({'text': '{:.200s}'})\n# t.style.apply(highlight_max, subset=[0,1,2,3,4,5,6], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"p = pipeline.predict_proba(x_test)\np[p.max(axis=1) < 0.90].shape[0],p[p.max(axis=1) < 0.90].shape[0]/p.shape[0], p.shape[0] - p[p.max(axis=1) < 0.90].shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t = pd.concat((x_test.reset_index(), pd.DataFrame(p), pd.Series(y_test, name='true')), \n              axis=1)\nt['max_prob'] = t.iloc[:, -8:-1].max(1)\nt = t[t.max_prob < 0.90]\nt['pred'] = t.iloc[:, -9:-2].idxmax(1).map(int_to_class)\nt['true'] = t['true'].map(int_to_class)\nt.max_prob.plot(kind='hist', bins=20)\n# t[['text', 'pred', 'true']].style.format({'text': '{:.200s}'})\n# t.style.apply(highlight_max, subset=[0,1,2,3,4,5,6], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# With changed data"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_ch = df.copy()\ndf_ch.tags[(url[7].isin(to_hockey))& ((df_ch.tags != 'Все')&(df_ch.tags != 'Футбол'))] = 'Хоккей'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_ch.tags.value_counts() - df.tags.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"le = LabelEncoder()\ny = le.fit_transform(df_ch.tags)\nx_train, x_test, y_train, y_test = train_test_split(text.text, y,\n                                                    random_state=2020, stratify=y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\npipeline = Pipeline([('counter', TfidfVectorizer(ngram_range=(1,2), min_df=10)),\n#                     ('pca', TruncatedSVD(n_components=20)),\n                    ('model', SGDClassifier(loss='log', random_state=2020, n_jobs=-1, class_weight='balanced',\n                     alpha=0.1*10e-6))\n#                      ('model', RandomForestClassifier(n_estimators=150, class_weight='balanced', n_jobs=1,\n#                                                       max_depth=10,\n#                                                       min_samples_split=3, #max_features='log2'\n#                                                      ))                     \n                    ])\n\nresults = cross_validate(pipeline, x_train, y_train,\n                    scoring='f1_macro', cv=3,\n                    return_train_score=True, n_jobs=-1)\nresults, results['test_score'].mean(), results['train_score'].mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pipeline.fit(x_train, y_train)\npred = pipeline.predict(x_test)\nprint(classification_report(y_test, pred))\nprint(f1_score(y_test, pred, average='macro'))\nplot_confusion_matrix(pipeline, x_test, y_test);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Сущетсвеннхы изменений в классификации нет и чистка лейблов не дает преимущества"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"int_to_class","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}