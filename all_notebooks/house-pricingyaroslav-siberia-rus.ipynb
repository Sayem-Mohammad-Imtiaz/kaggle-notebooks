{"cells":[{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom scipy.stats import norm\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy import stats\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df_train = pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv')\ndf_test = pd.read_csv('../input/house-prices-advanced-regression-techniques/test.csv')\nids = df_test['Id'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Объединим датасеты, пометив тренировочный и тестовый."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['Train'] = 1  # помечаем где у нас трейн\ndf_test['Train'] = 0  # помечаем где у нас тест\n\ndf = df_train.append(df_test, sort=False).reset_index(drop=True)\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Изучим целевую переменную"},{"metadata":{"trusted":true},"cell_type":"code","source":"def num_describer(df,param,bins=20):\n    nulls=round(df[param].isnull().sum()/len(df[param]),4)\n    IQR = df[param].quantile(\n        0.75) - df[param].quantile(0.25)\n    perc25 = df[param].quantile(0.25)\n    perc75 = df[param].quantile(0.75)\n    #blowout=len(df[(df[param]<perc25 - 1.5*IQR)|(df[param]>perc75 + 1.5*IQR)])\n    print('доля пропусков : {} \\n\\r'.format(nulls),\n          'min : {} \\n\\r'.format(df[param].min()),\n          'std : {} \\n\\r'.format(df[param].std()),         \n          'среднее: {} \\n\\r'.format(df[param].mean()),\n          'max : {} \\n\\r'.format(df[param].max()),\n          '25-й перцентиль: {} \\n\\r'.format(perc25),\n          'медиана: {} \\n\\r'.format(df[param].median()),\n          '75-й перцентиль: {} \\n\\r'.format(perc75),\n          \"IQR: {} \\n\\r\".format(IQR),\n          \"Границы выбросов: [{f}, {l}] \\n\\r\".format(f=perc25 - 1.5*IQR, l=perc75 + 1.5*IQR),\n         )#'Количество выбросов: \\n\\r'.format(blowout))\n    fig, (ax1, ax2) = plt.subplots(\n    nrows=1, ncols=2,\n    figsize=(8, 4))\n    sns.distplot(df[param],ax=ax1)\n    #ax1.hist(df[param],label = param,bins=bins)\n    ax1.set_title('Распределение значений')\n    ax1.legend()\n    sns.distplot(np.log(df[param]+1),ax=ax2)\n    #ax2.hist(np.log(df[param]+1),label = param,bins=bins)\n    ax2.set_title('Логорифмированное Распределение значений')\n    ax2.legend()\n    #sns.distplot(df[param])\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_describer(df[df['Train'] == 1],'SalePrice')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Из графикоы четко видно что целевая переменная имеет логнормальное распределение, т.к. целевая логарифмировать не буду)"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(np.log(df[df['Train'] == 1]['SalePrice']))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Для ускорения работы напишем функцию(готова была заранее), которая ускорит первичное исследование линейных переменных.\nЗ.Ы. Теоретически если категорийная переменная будет иметь числовой формат она тоже сюда попадет но это увидим"},{"metadata":{"trusted":true},"cell_type":"code","source":"def describe_without_plots_num_collumns(d_df,columns):\n    #numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    num_df = d_df[columns]\n    list_of_names = list(num_df.columns)\n    temp_dict = {}\n    temp_dict['имя признака'] = list_of_names\n    temp_dict['тип'] = num_df.dtypes\n    temp_dict['# пропусков(NaN)'] = num_df.isnull().sum().values \n    temp_dict['# уникальных'] = num_df.nunique().values\n    temp_dict['минимум'] = num_df.describe(include='all').loc['min']\n    temp_dict['среднее'] = num_df.describe(include='all').loc['mean']\n    temp_dict['медиана'] = num_df.describe(include='all').loc['50%']\n    temp_dict['макс'] = num_df.describe(include='all').loc['max']\n    temp_dict['std'] = pd.DataFrame(num_df.std().values,columns=['std']).loc[:,'std']\n    temp_df = pd.DataFrame.from_dict(temp_dict, orient='index')\n    display(temp_df.T)\n    \n    return\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Удобство такого метода, как я считаю позволит нам потом искать параметры по нужным характеристикам.\nПо отношению дисперсии и мат ожидания, по количеству значений(см.выше), по разбросу значений, разницу среднего и медианы"},{"metadata":{"trusted":true},"cell_type":"code","source":"num_cols=list(df[df['Train'] == 1].describe().columns)\ndescribe_without_plots_num_collumns(df[df['Train'] == 1],num_cols)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Параметров много все расписывать не буду.\nТут вот видим категорийные признаки по количеству значений\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"def describe_by_plots_num_collumns(d_df,columns,font_size=12):\n    fig, axes = plt.subplots(len(columns), 4, figsize=(8, 5*len(columns)))\n    for i in range(0,len(columns)):\n        axes[i,0].hist(d_df[columns[i]],label=str(columns[i]))\n        axes[i,0].set_title(str(columns[i]),fontsize=font_size)\n        axes[i,1].boxplot(d_df[columns[i]])\n        axes[i,1].set_title(str(columns[i]),fontsize=font_size)\n        axes[i,2].hist(np.log(d_df[columns[i]]+1),label=str(columns[i]))\n        axes[i,2].set_title('log_'+str(columns[i]),fontsize=font_size)\n        axes[i,3].boxplot(np.log(d_df[columns[i]]+1))\n        axes[i,3].set_title('log_'+str(columns[i]),fontsize=font_size)\n    return","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Рассмотрим все категорийные и бинарные признаки разом"},{"metadata":{"trusted":true},"cell_type":"code","source":"describe_by_plots_num_collumns(df[df['Train'] == 1],num_cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Обнаружили большое число признаков с выбросами","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import math\n\ndef borders_of_outliers(d_df,columns, log = False):\n    num_df = d_df[columns]\n    list_of_names = list(num_df.columns)\n    temp_dict = {}\n    temp_dict['имя признака'] = list_of_names\n    left_board=[]\n    right_board=[]\n    left_outliers=[]\n    right_outliers=[]\n    path_left_outliers=[]\n    path_right_outliers=[]\n    if log:\n        log_left_board=[]\n        log_right_board=[]\n        log_left_outliers=[]\n        log_right_outliers=[]\n        log_path_left_outliers=[]\n        log_path_right_outliers=[]\n    for i in columns:\n        IQR = num_df[i].quantile(0.75) - num_df[i].quantile(0.25)\n        perc25 = num_df[i].quantile(0.25)\n        perc75 = num_df[i].quantile(0.75)\n        left_border = num_df[i].quantile(0.25) - 1.5*IQR\n        right_border = perc75 + 1.5*IQR\n        left_board.append(left_border)\n        right_board.append(right_border)\n        left_outliers.append((num_df[i]<left_border).sum())\n        right_outliers.append((num_df[i]>right_border).sum())\n        path_left_outliers.append((num_df[i]<left_border).sum()/len(num_df[i]))\n        path_right_outliers.append((num_df[i]>right_border).sum()/len(num_df[i]))\n        if log:\n            num_df[i]=num_df[i].apply(lambda x: math.log(x+1))\n            IQR = num_df[i].quantile(0.75) - num_df[i].quantile(0.25)\n            perc25 = num_df[i].quantile(0.25)\n            perc75 = num_df[i].quantile(0.75)\n            left_border = num_df[i].quantile(0.25) - 1.5*IQR\n            right_border = perc75 + 1.5*IQR\n            log_left_board.append(left_border)\n            log_right_board.append(right_border)\n            log_left_outliers.append((num_df[i]<left_border).sum())\n            log_right_outliers.append((num_df[i]>right_border).sum())\n            log_path_left_outliers.append((num_df[i]<left_border).sum()/len(num_df[i]))\n            log_path_right_outliers.append((num_df[i]>right_border).sum()/len(num_df[i]))\n        \n    temp_dict['левая граница'] = left_board\n    temp_dict['правая граница'] = right_board\n    temp_dict['выбросов слева'] = left_outliers\n    temp_dict['выбросов справа'] = right_outliers\n    temp_dict['доля выбросов слева'] = path_left_outliers\n    temp_dict['доля выбросов справа'] = path_right_outliers\n    \n    if log:\n        temp_dict['левая граница с логарифмом'] = log_left_board\n        temp_dict['правая граница с логарифмом'] = log_right_board\n        temp_dict['выбросов слева с логарифмом'] = log_left_outliers\n        temp_dict['выбросов справа с логарифмом'] = log_right_outliers\n        temp_dict['доля выбросов слева с логарифмом'] = log_path_left_outliers\n        temp_dict['доля выбросов справа с логарифмом'] = log_path_right_outliers\n    \n    temp_df = pd.DataFrame.from_dict(temp_dict, orient='index')\n    display(temp_df.T)\n    \n    return","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Пройдемся конкретно по выбросам.\nПосмотрим что мо идее стоит логарифмировать а что нет.\nКак и что стоит чистить"},{"metadata":{"trusted":true},"cell_type":"code","source":"borders_of_outliers(df[df['Train'] == 1],num_cols,log=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def describe_without_plots_cat_bin_collumns(d_df,columns):\n    cat_bin_df = d_df[columns]\n    list_of_names = list(cat_bin_df.columns)\n    temp_dict = {}\n    temp_dict['имя признака'] = list_of_names\n    temp_dict['тип'] = cat_bin_df.dtypes\n    temp_dict['# пропусков(NaN)'] = cat_bin_df.isnull().sum().values \n    temp_dict['# уникальных'] = cat_bin_df.nunique().values\n    temp_dict['# мода'] = cat_bin_df.mode()\n    temp_df = pd.DataFrame.from_dict(temp_dict, orient='index')\n    display(temp_df.T)\n    \n    return","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_bin=df[df['Train'] == 1].select_dtypes('object').columns.to_list()\ndescribe_without_plots_cat_bin_collumns(df[df['Train'] == 1],cat_bin)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ncorrmat = df[df['Train'] == 1][[num_cols]].corr()\nplt.subplots(figsize=(12,9))\nsns.heatmap(corrmat, vmax=1, square=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Избавляемся от пропусков"},{"metadata":{},"cell_type":"markdown","source":"Посмотрим и долю пропусков и абсолютное количество\nСчитаю что много пропусков это 5% и более."},{"metadata":{"trusted":true},"cell_type":"code","source":"total = df[df['Train'] == 1].isnull().sum().sort_values(ascending=False)\npercent = (df[df['Train'] == 1].isnull().sum()/df[df['Train'] == 1].isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(20)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Дадим скидку, 5.5% трогать не будем)"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.drop((missing_data[missing_data['Total'] > 81]).index,1)\ndf = df.apply(lambda x:x.fillna(x.value_counts().index[0]))\ndf.isnull().sum().max()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df=df.drop(\"Id\", axis = 1)\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\ncols = df.select_dtypes(include='object').columns\n\nfor c in cols:\n    lbl = LabelEncoder() \n    lbl.fit(list(df[c].values)) \n    df[c] = lbl.transform(list(df[c].values))\n\nprint('Shape all_data: {}'.format(df.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = df.query('Train == 1')\ntrain=train.drop('Train',axis=1)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = df.query('Train == 0')\ntest=test.drop(['Train','SalePrice'],axis=1)\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train=train.drop('SalePrice',1)\ny_train=train.SalePrice.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"indexes = x_train[(train['GrLivArea']>4000) & (train['SalePrice']<300000)].index \n\nx_train = x_train.drop(indexes)\ny_train = np.delete(y_train, indexes)\n\ny_train.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Построение модели"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb\nimport lightgbm as lgb\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_xgb = xgb.XGBRegressor(n_estimators=2200)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nn_folds = 5\n\ndef rmsle_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train.values)\n    rmse = np.sqrt(-cross_val_score(model, x_train, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)\n\ndef rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import GridSearchCV\nparams = {'n_estimators':[300,500,700,900,1100],\n          'criterion':['mse'],\n          'max_depth':[10,20,30,40,50,60,70],\n          'random_state':[42],\n          'n_jobs':[-1]}\nmodel = RandomForestRegressor()\n\ngrid = GridSearchCV(model, params)\n\ngrid.fit(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(grid.best_params_)\nbest_forest=grid.best_estimator_\nbest_forest.fit(x_train, y_train)\nforest_train_pred = best_forest.predict(x_train)\nprint(rmsle(y_train, forest_train_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"Тааак, не туда свернули"},{"metadata":{"trusted":true},"cell_type":"code","source":"lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=42))\nlasso_score = rmsle_cv(lasso)\nprint(\"\\nLasso score: {:.4f} ({:.4f})\\n\".format(lasso_score.mean(), lasso_score.std()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_xgb = xgb.XGBRegressor(reg_lambda=0.8571, n_estimators=2200, nthread = -1)\nmodel_xgb.fit(x_train, y_train)\nxgb_train_pred = model_xgb.predict(x_train)\nxgb_pred = model_xgb.predict(test)\nprint(rmsle(y_train, xgb_train_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.linear_model import ElasticNet, Lasso\nimport lightgbm as lgb\n\nclass StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, base_models, meta_model, n_folds=5):\n        self.base_models = base_models\n        self.meta_model = meta_model\n        self.n_folds = n_folds\n   \n    # We again fit the data on clones of the original models\n    def fit(self, X, y):\n        self.base_models_ = [list() for x in self.base_models]\n        self.meta_model_ = clone(self.meta_model)\n        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156)\n        \n        # Train cloned base models then create out-of-fold predictions\n        # that are needed to train the cloned meta-model\n        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n        for i, model in enumerate(self.base_models):\n            for train_index, holdout_index in kfold.split(X, y):\n                instance = clone(model)\n                self.base_models_[i].append(instance)\n                instance.fit(X[train_index], y[train_index])\n                y_pred = instance.predict(X[holdout_index])\n                out_of_fold_predictions[holdout_index, i] = y_pred\n                \n        # Now train the cloned  meta-model using the out-of-fold predictions as new feature\n        self.meta_model_.fit(out_of_fold_predictions, y)\n        return self\n   \n    #Do the predictions of all base models on the test data and use the averaged predictions as \n    #meta-features for the final prediction which is done by the meta-model\n    def predict(self, X):\n        meta_features = np.column_stack([\n            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n            for base_models in self.base_models_ ])\n        return self.meta_model_.predict(meta_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"KRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)\n\nGBoost = GradientBoostingRegressor(n_estimators=3000, random_state =42)\n\nENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005,random_state=42))\nlasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=42))\n\nstacked_averaged_models = StackingAveragedModels(base_models = (ENet, GBoost, KRR),\n                                                 meta_model = lasso)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_xgb = xgb.XGBRegressor(n_estimators=2200, nthread = -1)\nmodel_xgb.fit(x_train, y_train)\n\nmodel_lgb = lgb.LGBMRegressor(objective='regression',n_estimators=720)\nmodel_lgb.fit(x_train, y_train)\n\nstacked_averaged_models.fit(x_train.values, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stacked_pred = stacked_averaged_models.predict(test.values)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Так я и не понимал что за дела и как люди додавили то такой микроскопической ошибки в 0.00044\nНу потом побольше полазил по чужим нетбукам и нашел)))\nДругой датасет с такими же данными но там тренировочнаяи тестовые части отличаются потомууууу...сравним попарно.На самом деле просто скопипастил"},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm import tqdm\n\ntrain = pd.read_csv('../input/ames-housing-dataset/AmesHousing.csv')\ntrain.drop(['PID'], axis=1, inplace=True)\n\norigin = pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv')\ntrain.columns = origin.columns\n\ntest = pd.read_csv('../input/house-prices-advanced-regression-techniques/test.csv')\n\nsubmission = pd.read_csv('../input/house-prices-advanced-regression-techniques/sample_submission.csv')\n\nprint('Train:{}   Test:{}'.format(train.shape,test.shape))\n\nmissing = test.isnull().sum()\nmissing = missing[missing>0]\ntrain.drop(missing.index, axis=1, inplace=True)\ntrain.drop(['Electrical'], axis=1, inplace=True)\n\ntest.dropna(axis=1, inplace=True)\ntest.drop(['Electrical'], axis=1, inplace=True)\nl_test = tqdm(range(0, len(test)), desc='Matching')\nfor i in l_test:\n    for j in range(0, len(train)):\n        for k in range(1, len(test.columns)):\n            if test.iloc[i,k] == train.iloc[j,k]:\n                continue\n            else:\n                break\n        else:\n            submission.iloc[i, 1] = train.iloc[j, -1]\n            break\nl_test.close()\n\nsubmission.to_csv('result-best.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}