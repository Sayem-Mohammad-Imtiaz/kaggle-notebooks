{"cells":[{"metadata":{},"cell_type":"markdown","source":"### Table of Contents\n\n1. [**Importing necessary Libraries**](# 1.-Importing-necessary-Libraries)   \n2. [**Loading Data**](#2.-Loading-Data)  \n3. [**Data Preprocessing**](#3.-Data-Preprocessing)  \n    3.a [**Checking for missing values**](#3.a-Checking-for-missing-values)  \n    3.b [**Checking for Class Imbalance**](#3.b-Checking-for-Class-Imbalance)  \n    3.c [**Handling class imbalance**](#3.c-Handling-class-imbalance)  \n4. [**Feature Selection**](#4.-Feature-Selection)  \n    4.a [**Forward Propagation**](#4.a-Forward-Propagation)  \n    4.b [**Splitting the data in a 80:20 ratio**](#4.b-Splitting-the-data-in-a-80:20-ratio)  \n5. [**Fitting a Logistic Regression model**](#5.-Fitting-a-Logistic-Regression-model)  \n    5.a [**Hyper-parameter tuning**](#5.a-Hyper-parameter-tuning)  \n    5.b [**predictions against test data**](#5.b-predictions-against-test-data)  \n6. [**Using AutoML**](#6.-Using-AutoML)  "},{"metadata":{},"cell_type":"markdown","source":"**Additional NOTE**\n\nIf you are interested in learning or exploring more about importance of feature selection in machine learning, then refer to my below blog offering.\n\nhttps://www.analyticsvidhya.com/blog/2020/10/a-comprehensive-guide-to-feature-selection-using-wrapper-methods-in-python/"},{"metadata":{},"cell_type":"markdown","source":"### 1. Importing necessary Libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd\n\nfrom sklearn.utils import resample\n\nimport matplotlib.pyplot as plt\nimport plotly.figure_factory as ff\nimport plotly.graph_objects as go\n\nfrom mlxtend.feature_selection import SequentialFeatureSelector as SFS\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.metrics import accuracy_score,f1_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2. Loading Data"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/insurance-churn-prediction-weekend-hackathon/Insurance_Churn_ParticipantsData/Train.csv\")\nprint(\" Dataset size : \", df.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3. Data Preprocessing\n\n#### 3.a Checking for missing values "},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"=\"*15,\"Total no. of missing values in each column\",\"=\"*15)\nprint(df.isna().sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 3.b Checking for Class Imbalance"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Frequency of churns and No churns : \\n\", df[\"labels\"].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = go.Figure([go.Bar(x=df[\"labels\"].value_counts().index, y=df[\"labels\"].value_counts().values)])\nfig['layout'].update(title={\"text\" : 'Distribution of churn labels','y':0.9,'x':0.5,'xanchor': 'center','yanchor': 'top'}, xaxis_title=\"label\",yaxis_title=\"count\")\nfig.update_layout(width=500,height=500)\nfig","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 3.c Handling class imbalance\n\nThere are multiple Sampling approaches to deal with class imbalance problem. Most commonly preferred are \n1. Under(down) Sampling\n2. Over(up) Sampling\n3. SMOTE\n\nAlternatively, we can also use the algorithms(like Logistic Regression, SVM etc.) in a balanced mode by balancing the class weights."},{"metadata":{},"cell_type":"markdown","source":"### 4. Feature Selection\n\n#### 4.a Forward Propagation"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.drop(\"labels\", axis=1)\ny = df[[\"labels\"]]\n\nsfs = SFS(LogisticRegression(class_weight = \"balanced\"),\n           k_features=10,\n           forward=True,\n           floating=False,\n           scoring = 'f1',\n           cv = 0)\n\nsfs.fit(X,y)\nprint(\"Top 10 features selected using Forward Propagation\",sfs.k_feature_names_ )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from mlxtend.plotting import plot_sequential_feature_selection as plot_sfs\nimport matplotlib.pyplot as plt\nfig1 = plot_sfs(sfs.get_metric_dict(), kind='std_dev')\nplt.title('Sequential Forward Selection (w. StdErr)')\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Above plot indicates that the performance measure(F1-score here) becomes stationary after the use of first 8 features. So let's use these 8 features in model training."},{"metadata":{},"cell_type":"markdown","source":"#### 4.b Splitting the data in a 80:20 ratio"},{"metadata":{"trusted":true},"cell_type":"code","source":"train,val = train_test_split(df, test_size = 0.2, random_state = 42, stratify = df['labels'])\nX_train = train[['feature_0', 'feature_2', 'feature_3', 'feature_4', 'feature_6', 'feature_9', 'feature_11', 'feature_12']]\ny_train = train[[\"labels\"]]\nX_val = val[['feature_0', 'feature_2', 'feature_3', 'feature_4', 'feature_6', 'feature_9', 'feature_11', 'feature_12']]\ny_val = val[[\"labels\"]]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5. Fitting a Logistic Regression model"},{"metadata":{"trusted":true},"cell_type":"code","source":"log_clf = LogisticRegression(class_weight = \"balanced\")\nlog_clf.fit(X_train, y_train)\ntrain_pred = log_clf.predict(X_train)\nprint(\"Training data accuracy : \", accuracy_score(train_pred,y_train))\nprint(\"Training data F1-score : \", f1_score(train_pred,y_train))\n\n\nval_pred = log_clf.predict(X_val)\nprint(\"Validation data accuracy : \", accuracy_score(val_pred,y_val))\nprint(\"Validation data F1-score : \", f1_score(val_pred,y_val))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 5.a Hyper-parameter tuning"},{"metadata":{"trusted":true},"cell_type":"code","source":"c = [10 ** x for x in range(-5, 2)]\nf1_score_array=[]\nfor i in c:\n    clf = LogisticRegression(C =i, class_weight = 'balanced')\n    clf.fit(X_train, y_train)\n    predict_y = clf.predict(X_val)\n    f1_score_array.append(f1_score(y_val, predict_y))\n    print('For values of alpha = ', i, \"The F1 - score is:\",f1_score(y_val, predict_y))\n    \nprint(\"\\nThe maximum value of f1_score is {} for C = {}\".format(max(f1_score_array), c[f1_score_array.index(max(f1_score_array))]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"log_clf = LogisticRegression(C = 0.001,class_weight = \"balanced\")\nlog_clf.fit(X_train, y_train)\ntrain_pred = log_clf.predict(X_train)\nprint(\"Training data accuracy : \", accuracy_score(train_pred,y_train))\nprint(\"Training data F1-score : \", f1_score(train_pred,y_train))\n\n\nval_pred = log_clf.predict(X_val)\nprint(\"Validation data accuracy : \", accuracy_score(val_pred,y_val))\nprint(\"Validation data F1-score : \", f1_score(val_pred,y_val))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 5.b predictions against test data"},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv(\"/kaggle/input/insurance-churn-prediction-weekend-hackathon/Insurance_Churn_ParticipantsData/Test.csv\")\npredictions = log_clf.predict(submission[['feature_0', 'feature_1', 'feature_3', 'feature_4', 'feature_6', 'feature_9', 'feature_10', 'feature_11']])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 6. Using AutoML "},{"metadata":{"trusted":true},"cell_type":"code","source":"import h2o\nfrom h2o.automl import H2OAutoML\nh2o.init(max_mem_size='16G')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Data loading\ndf = pd.read_csv(\"/kaggle/input/insurance-churn-prediction-weekend-hackathon/Insurance_Churn_ParticipantsData/Train.csv\")\ndf[\"labels\"] = df[\"labels\"].map({0: \"No\", 1:\"Yes\"})\nX = h2o.H2OFrame(df)\nX.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Splitting the data\nsplits = X.split_frame(ratios=[0.8],seed=1)\ntrain = splits[0]\nval = splits[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = \"labels\"\nx_train = train.columns\nx_train.remove(y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fitting the model\naml = H2OAutoML(max_runtime_secs=300, seed=1,keep_cross_validation_predictions = True,balance_classes = True, max_after_balance_size = 7934)\naml.train(x = x_train, y = y, training_frame = train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lb = aml.leaderboard\nlb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# making the predictions on validation data \npred = aml.predict(val.drop(\"labels\",axis=1))\n\n#computing accuracy\nprint(\"Accuracy on validation data = \",accuracy_score(pred.as_data_frame()[\"predict\"], val[\"labels\"].as_data_frame()[\"labels\"]))\n\n#computing f1-score \nprint(\"F1- score = \",f1_score(pred.as_data_frame()[\"predict\"], val[\"labels\"].as_data_frame()[\"labels\"],pos_label = \"Yes\" ))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}