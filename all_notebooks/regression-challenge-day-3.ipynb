{"metadata":{"kernelspec":{"name":"python3","language":"python","display_name":"Python 3"},"language_info":{"name":"python","nbconvert_exporter":"python","version":"3.6.3","codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","pygments_lexer":"ipython3"}},"cells":[{"metadata":{"_cell_guid":"1da05bb5-a89d-42c9-8f86-f3a9f8ecf3ff","_uuid":"aa5fa3edfaca1ac1591a7fb9abbaa7547f115833"},"cell_type":"markdown","source":"## Your turn!\n___\n\nNow it's your turn to come up with a model and interpret it!\n\n1. Pick a question to answer to using the Cameras dataset. Pick a variable to predict and one variable to use to predict it.\n2. Fit a GLM model of the appropriate family. (Check out [Monday's challenge](https://www.kaggle.com/rtatman/regression-challenge-day-1) if you need a refresher).\n3. *Optional but recommended:* Plot diagnostic plots for your model. Does it seem like your model is a good fit for your data? If you're fitting a linear or Poisson model, are the residuals normally distributed (no patterns in the first plot and the points in the second plot are all in a line)? Are there any influential outliers?\n4. Check out your model using the summary() function. Does your input variable have a strong relationship to the output variable you're predicting?\n5. Write a couple sentences describing what you've learned from your model. (It could just be that it's not a very good model!)\n5. Plot your two variables & use \"geom_smooth\" and the appropriate family to fit and plot a model. Does this confirm what you learned from examining your model?\n6. *Optional:* If you want to share your analysis with friends or to ask for help, you’ll need to make it public so that other people can see it.\n    * Publish your kernel by hitting the big blue “publish” button. (This may take a second.)\n    * Change the visibility to “public” by clicking on the blue “Make Public” text (right above the “Fork Notebook” button).\n    * Tag your notebook with 5daychallenge"},{"execution_count":null,"metadata":{"collapsed":true,"_cell_guid":"9cca4d4d-2c04-4089-9b32-b61bd14c8c29","_uuid":"07eeb52df68f7ac14740d7774ca945086feb1a58"},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n# from sklearn import linear_model\n# from sklearn.metrics import mean_squared_error, r2_score\n# GFX\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input/1000-cameras-dataset\"]).decode(\"utf8\"))","outputs":[]},{"execution_count":null,"metadata":{"collapsed":true,"_cell_guid":"6e56e3de-393f-4d69-a7cd-1df86a799d00","_uuid":"5b5a0f573e7ffd61d32277bc16cd8e7ba2b827d1"},"cell_type":"code","source":"cameras = pd.read_csv(\"../input/1000-cameras-dataset/camera_dataset.csv\")\ncameras.head(5).transpose()\nprint(\"Shape: %s rows, %s columns\" % (cameras.shape[0], cameras.shape[1]))\n\n# nas = cameras.isnull().sum(axis=0)\n\ndf = pd.concat([cameras.isnull().sum(axis=0), \\\n                cameras.applymap(lambda x: True if x==0 else False).sum(axis=0), \\\n                cameras.dtypes], axis = 1)\ndf.columns = [\"#NA\", \"#0s\", \"dtypes\"]\nprint(df)","outputs":[]},{"metadata":{"_cell_guid":"aa963793-5a27-481c-a19b-5463d24009c5","_uuid":"0e285f0f2be9bb6f70d7f754180e1494237a328b"},"cell_type":"markdown","source":"Not that many NAs but a suspicious # of 0s... Let's take a closer look at the NAs."},{"execution_count":null,"metadata":{"collapsed":true,"_cell_guid":"67d644da-9f92-49fc-854e-cc273b21780c","_uuid":"b48faed2769bddd32d32568fd4b822abf3e3e3a2"},"cell_type":"code","source":"na_idxs = cameras[\"Macro focus range\"].loc[cameras[\"Macro focus range\"].isnull()].index.values[0]\nprint(cameras[(na_idxs-2):(na_idxs+2)])\ncameras_orig = cameras\ncameras.dropna(axis=0, how=\"any\",inplace=True)","outputs":[]},{"metadata":{"_cell_guid":"f7677810-5244-4dd0-bd49-5cb69ad94719","_uuid":"78e3e7586fdbac936fafe86437c21c419919a9bc"},"cell_type":"markdown","source":"It appears that only 2 of the cameras (of 1038) have NA fields, let's drop them for now.\n\nNow we'll plot some histograms to get a feel for the variable distributions:"},{"execution_count":null,"metadata":{"collapsed":true,"_cell_guid":"520a0c06-698c-4f41-8688-3d75ecc7931c","_uuid":"a94df71cfa14cd064929c4b8fc5632abc274777d"},"cell_type":"code","source":"num_features = cameras.columns[1:]\nfig, axs = plt.subplots(4,3,figsize=(15,13))\naxs = axs.flatten()\n\ni = 0\nfor feature in num_features:\n    sns.distplot(cameras[feature], ax = axs[i], \\\n                 color=(sns.color_palette()[i % len(sns.color_palette())]));\n    i += 1","outputs":[]},{"metadata":{"_cell_guid":"d7f96d9a-030e-4b58-ac85-8e07881caa6e","_uuid":"1c1f5f54ceac243c7809277ce5ae595cf5cc617f"},"cell_type":"markdown","source":"Some observations:\n* Most of the cameras are pretty new in the dataset (see Release date)\n* Features with a lot of 0s: Zooms (W) and (T), Dimensions\n* Features with positive outliers: Zoom tele (T), Macro range, Storage, Weight, Price\n\nLet's now check out the correlation between variables so we can pick out a dependent variable for our regression. Starting with a Pearson (clustered) correlation matrix:"},{"execution_count":null,"metadata":{"collapsed":true,"_cell_guid":"2c46ad10-cd39-49ef-a10a-3d1d58210bdc","_uuid":"79d1863c070bd6468774076716d77f25e5562c79"},"cell_type":"code","source":"corr = cameras[num_features].corr()\ncg = sns.clustermap(corr, cmap=\"YlGnBu\");\nplt.show();","outputs":[]},{"metadata":{"_cell_guid":"08d5f643-a282-4e2a-bcab-2a5cad02b4dc","_uuid":"82a1bc9b5cb2139c892c921b2a815ce8c86393fd"},"cell_type":"markdown","source":"Interestingly, most of the technical parameters (e.g. resolution) don't seem very correlated with price, the strongest positive correlations are Weight and Dimensions.<br>\nIn fact, Zooms are negatively correlated.<br>\nFor completeness, let's examine the Spearman correlation matrix:"},{"execution_count":null,"metadata":{"collapsed":true,"_cell_guid":"4166eb79-853e-4e7a-b2fc-28f88d4bee87","_uuid":"ac85e88c15e3edc78c6a62522f799d8a0f0fb8a9"},"cell_type":"code","source":"corr = cameras[num_features].corr(\"spearman\")\ncg = sns.clustermap(corr, cmap=\"RdYlGn\");\nplt.show();\ncorr_feats = ['Price', 'Weight (inc. batteries)', 'Dimensions', 'Zoom tele (T)', 'Zoom wide (W)', 'Macro focus range']","outputs":[]},{"metadata":{"_cell_guid":"17ada97b-d978-44f8-8b78-81036fe196fe","_uuid":"369de2d9fadc44fa1d282e9ceb403635e9a2c788"},"cell_type":"markdown","source":"We can examine some pairplots to get a better feel for the interplay between variables:"},{"execution_count":null,"metadata":{"collapsed":true,"_cell_guid":"c589139b-cf0a-4fed-8198-7be5c72343e8","_uuid":"e631abc61a5b710098fe7faaefec9d69e7dc90e0"},"cell_type":"code","source":"ax = sns.pairplot(cameras[corr_feats], palette = \"Spectral\", hue=\"Price\");\nax._legend.remove()\nplt.show();","outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"d33c996d-4a7c-4dbe-8203-00606649dbe6","_uuid":"20da0f8b7985eaa839d12b5cb06c156f596048be"},"cell_type":"markdown","source":"It's hard to see a clear relationship with price, but there also appear to be many outliers. Let's see how the relationships for a few of the features would look if we got rid of price outliers."},{"execution_count":null,"metadata":{"collapsed":true,"_cell_guid":"06c3987f-288a-40da-b633-5929b94961d8","_uuid":"7c7b84a47c548d27ff5edefbf0632ba75681aad6"},"cell_type":"code","source":"def nol(df, feature, m=2):\n    if m==0:\n        return df\n    x = df[feature]\n    mask = abs(x - np.mean(x)) < m*np.std(x)\n    return df.loc[mask]\n\nreg_feats = list(corr_feats[i] for i in [1,2,4,5])\nm_=1.5 # No. of std deviations - cutoff\ncameras_nol = nol(cameras, 'Price', m=m_)\n\nfig, axs = plt.subplots(2,2,figsize=(15,10))\naxs = axs.flatten()\ni=0\nfor feature in reg_feats:\n    df = cameras_nol\n    ax = sns.regplot(x=feature, y='Price', data=df, ax=axs[i])\n    left, right = min(df[feature]) - 0.3*df[feature].std(),\\\n        max(df[feature] + 0.3*df[feature].std())\n    ax.set_xlim(left, right)\n    i+=1","outputs":[]},{"metadata":{"_cell_guid":"ec69d07e-50f6-4bad-b676-7dadc73d4dae","_uuid":"ad39e0b9b93b4fbca4ab5a184ce572383969d1ed"},"cell_type":"markdown","source":"There are a lot of 0s for the dependent variable which I suspect are NAs (barring the existence weightless cameras). Let's see how the regression plots would look like with 0s removed for dependent variables:"},{"execution_count":null,"metadata":{"collapsed":true,"_cell_guid":"4ad7fcda-5032-463e-9165-58c447966c99","_uuid":"ed88fed0b786b0aa721e692d6bc46d252cd0322d"},"cell_type":"code","source":"def nozero(df, feature):\n    x = df[feature]\n    mask = x != 0\n    return df.loc[mask]\n\nreg_feats = list(corr_feats[i] for i in [1,2,4,5])\nm_=1.5 # No. of std deviations - cutoff\n\nfig, axs = plt.subplots(2,2,figsize=(15,10))\naxs = axs.flatten()\ni=0\ncameras_nol = nol(cameras, 'Price', m=m_)\nfor feature in reg_feats:\n    df = nozero(cameras_nol, feature)\n    ax = sns.regplot(x=feature, y='Price', data=df, ax=axs[i], marker='.')\n    left, right = min(df[feature]) - 0.3*df[feature].std(),\\\n        max(df[feature] + 0.3*df[feature].std())\n    ax.set_xlim(left, right)\n    i+=1","outputs":[]},{"metadata":{"_cell_guid":"bc94bd94-a09c-4430-9fe0-497860cffaad","_uuid":"a25a981601e2e79874145950eb2c4526cf3bdba9"},"cell_type":"markdown","source":"It's starting to look a little better. The prices are clearly clustered. We can try to separate them into a few groups with KMeans:"},{"execution_count":null,"metadata":{"collapsed":true,"_cell_guid":"91bf62b4-e0a3-4dbe-9a69-75840fb4e737","_uuid":"91b7b890f16c9405bcc99d3a67923feda060e6f6"},"cell_type":"code","source":"from sklearn.cluster import KMeans\nprices = np.array(cameras['Price'])\ncolors = [\"blue\", \"green\", \"yellow\", \"orange\"]\nfig, ax = plt.subplots(figsize=(15,6))\n# optional - logscale (really saving for later projs)\n# ax = plt.subplot(111)\n# ax.set_yscale(\"log\")\nfor k in range(2,5):\n    km = KMeans(n_clusters=k).fit(prices.reshape(-1,1))\n    cluster = km.labels_\n    for i in range(0,k):\n        y=prices[cluster==i]\n        plt.scatter(x=[k]*len(y)+np.random.normal(0,0.01,(len(y))), \\\n                    y=y, c=sns.color_palette()[i], marker='.', alpha=0.8);\nax.set(xlabel=\"# clusters\", xticks=[2,3,4], xticklabels=[2,3,4],\\\n       ylabel=\"Price\", title=\"KMeans price clusters\")\nplt.show()\nprint(\"Look at k=4 for filtering:\")\nkm = KMeans(n_clusters=4).fit(prices.reshape(-1,1))\ncameras['Cluster'] = km.labels_\nprint(\"Group %s: %s\\n\"*4 % sum(tuple((i+1, (km.labels_ == i).sum()) for i in range(4)),()))","outputs":[]},{"metadata":{"_cell_guid":"e9742fcf-12af-40ed-9c92-297a88ac6ec1","_uuid":"87323d23d2fdef1299e9fd1dc31c70bc7f1b8235"},"cell_type":"markdown","source":"Let's see our regression with only the cheapest cluster, the most populated one, considered:"},{"execution_count":null,"metadata":{"collapsed":true,"_cell_guid":"a7eba96f-b230-491b-9b26-a464b680a01a","_uuid":"f06ab8183e4e5dd2fdcd34f55f4469973189dfa2"},"cell_type":"code","source":"fig, axs = plt.subplots(2,2,figsize=(15,10))\naxs = axs.flatten()\ni=0\nk=0\nfor feature in reg_feats:\n    df = nozero(cameras, feature)\n    df = df.loc[df['Cluster']==k]\n    ax = sns.regplot(x=feature, y='Price', data=df, ax=axs[i], marker='.')\n    left, right = min(df[feature]) - 0.3*df[feature].std(),\\\n        max(df[feature] + 0.3*df[feature].std())\n    ax.set_xlim(left, right)\n    i+=1\nprint(\"cluster filtering looks good, but at this zoom the linear relationships look fairly flat...\")","outputs":[]},{"metadata":{"_cell_guid":"c455bbb0-706e-4c6e-9803-d6ba3327a6ec","_uuid":"76c7181acd9823b54c61b26b317f521515a6db32"},"cell_type":"markdown","source":"We've successfully massaged the data so the prices appear to be in one cohort, but the linear relationships with the variables still appear flat. <br>\nLet's see how regressions with some of the other variables (that were prevoiusly shown to have low correlations with price) would look in this cluster."},{"execution_count":null,"metadata":{"collapsed":true,"_cell_guid":"e37a3748-09de-4c44-aca6-08bf3bd831ca","_uuid":"93b74a97a5f11603240ac7f56370921c02bd4676"},"cell_type":"code","source":"other_reg_feats = ['Max resolution', 'Effective pixels', 'Storage included', 'Normal focus range']\nfig, axs = plt.subplots(2,2,figsize=(15,10))\naxs = axs.flatten()\ni=0\nk=0\nfor feature in other_reg_feats:\n    df = nozero(cameras, feature)\n    df = df.loc[df['Cluster']==k]\n    ax = sns.regplot(x=feature, y='Price', data=df, ax=axs[i], marker='.')\n    left, right = min(df[feature]) - 0.3*df[feature].std(),\\\n        max(df[feature] + 0.3*df[feature].std())\n    ax.set_xlim(left, right)\n    i+=1","outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"b23ed4ba-cf6d-406b-abf0-a0f034a12a6f","_uuid":"16f9a0c9759e950c05b3dcd49b8e77d8dd475f51"},"cell_type":"markdown","source":"Superficially, Max resolution seems to have a (slightly) stronger effect on price than the others.<br>\nFor the sake of completing the exercise, let's choose Max res as the explanatory variable.<br>\nLet's check out first if Max res also fared well in the other relatively populous cluster:"},{"execution_count":null,"metadata":{"collapsed":true,"_cell_guid":"8f50f6aa-a1de-4d71-a8af-7018ba7041d2","_uuid":"70999b71cce59335ccdd1e4395d59fba165a1290"},"cell_type":"code","source":"feature = 'Max resolution'\nfig, axs = plt.subplots(1,2,figsize=(15,5))\naxs = axs.flatten()\ni = 0\nfor k in [0,1]:\n    df = nozero(cameras, feature)\n    df = df.loc[df['Cluster']==k]\n    ax = sns.regplot(x=feature, y='Price', data=df, ax=axs[i], marker='.')\n    left, right = min(df[feature]) - 0.3*df[feature].std(),\\\n        max(df[feature] + 0.3*df[feature].std())\n    ax.set_xlim(left, right)\n    i += 1","outputs":[]},{"metadata":{"_cell_guid":"195f6d90-06cc-4b76-9bcc-aa4fb7494568","_uuid":"91ba27ab3f6073aff5c4d0f27a26a763494b643c"},"cell_type":"markdown","source":"If we have any luck here with a univariate linear model, it will be limited to the cheapest price cluster.<br>\nLet's run a regression:"},{"execution_count":null,"metadata":{"collapsed":true,"_cell_guid":"02091a0d-fd96-4e24-abf4-12b8f425da4b","_uuid":"3df709451e00fd27c9b6f68848b32ddb48ea8e60"},"cell_type":"code","source":"import statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\ndf = nozero(cameras, feature)\ndf = df.loc[df['Cluster'] == 0]\nX = df[feature]\nX = sm.tools.add_constant(X, prepend=True)\nY = df['Price']\nmodel = sm.OLS(Y,X)\nresults = model.fit()\nprint(results.summary())","outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"1da886f0-d0eb-4e6a-bd53-86eac8ef1d68","_uuid":"3d264bf60367ee7cafb1b155b458d60f3c0e4b1b"},"cell_type":"markdown","source":"Observations:\n* Very high intercept estimate, in value and deviation.\n* Rather low slope coefficient for our independent variable, especially because it's not of a completely different order than Price.\n* Jarque-Bera test for normality of residuals also doesn't look great, but lets plot the residuals to confirm."},{"execution_count":null,"metadata":{"collapsed":true,"_cell_guid":"b51d3b43-036d-4e76-bde7-f6096dc9149e","_uuid":"d88edb5a9a878da6a7fc2cf68b303897320ea048"},"cell_type":"code","source":"n = len(results.resid)\nax = plt.figure(figsize = (12,6))\nplt.scatter(x=np.linspace(0,n,n), y=results.resid, marker='.');\nl = plt.axhline(0, color=\"green\", ls=\"--\");","outputs":[]},{"execution_count":null,"metadata":{"collapsed":true,"_cell_guid":"aa790577-0aad-494e-8b4f-a88ea0782f5f","_uuid":"9766afc94890fb933b3fa749aba93e247ecd0b27"},"cell_type":"code","source":"from scipy.stats import probplot\nprint(\"QQ plot:\")\nax = plt.figure(figsize = (12,6))\nprobplot(results.resid, plot=plt)\nplt.show()","outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"20515ba0-5d7f-497a-87b9-c76cbade08d5","_uuid":"cfb3f970890642535c56f904de32aa9fcaa21e4b"},"cell_type":"markdown","source":"TBC... (?)"}],"nbformat":4,"nbformat_minor":1}