{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#importing the required Libraries\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nimport xgboost as xg\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nimport warnings\nwarnings.simplefilter(action='ignore', category=Warning)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Data Collection\ntrain = pd.read_csv(\"/kaggle/input/novartis-data/Train.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"shape of data set: \", train.shape)\ntrain.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Checking for Null Values","metadata":{}},{"cell_type":"code","source":"nullcnt = train.isnull().sum()\nNullValues=train.isnull().sum()/len(train)\nframe = {'Number of Null values': nullcnt, 'percentage of null values': NullValues}\npd.DataFrame(frame)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Handling Data Types","metadata":{}},{"cell_type":"code","source":"train.dtypes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#convert date dtype from object to date datatype\ntrain.DATE = pd.to_datetime(train.DATE)\nprint(\"Dtype of date column: \", train.DATE.dtypes)\nprint(train.DATE.head(10))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating an additional columns with date for analyss\ndef datetimesplit(df):\n    df['date'] = df['DATE'].dt.month\n    df['DAY'] = df['DATE'].dt.day\n    df['YEAR'] = df['DATE'].dt.year\n    df['DAYOFWEEK'] = df['DATE'].dt.dayofweek\n    df['WEEKEND'] = np.where(df['DATE'].dt.day_name().isin(['Sunday','Saturday']),1,0)\n    df = df.drop('DATE', axis = 1)\n    return df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = datetimesplit(train)\ntrain.head(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Handling the null values","metadata":{}},{"cell_type":"markdown","source":"##### X_12 contains 0.7% null values. We can handle it by considering X_12 column as independent column and rest are dependent column\n","metadata":{}},{"cell_type":"code","source":"train_miss = train[train.X_12.isnull()== False].drop(['X_12','INCIDENT_ID'], axis = 1)\ntrain_y = train[train.X_12.isnull()== False].X_12\npred_miss = train[train.X_12.isnull()== True].drop(['X_12','INCIDENT_ID'], axis = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"shape: \", train_miss.shape)\nprint(\"shape: \", train_y.shape)\nprint(\"shape: \", pred_miss.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clf = GradientBoostingClassifier()\nclf.fit(train_miss, train_y)\ny_pred= clf.predict(pred_miss)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_miss = train[train.X_12.isnull()== True]\npred_miss.X_12 = y_pred","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.set_option('display.max_columns', None)\npred_miss\n\ntrain_miss = train[train.X_12.isnull()== False]\n\ntrain_file = train_miss.append(pred_miss).sort_index()\nnull_col = [i for i in train_file.columns if train_file[i].isnull().sum() > 0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_file.head(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## EDA (Exploratory data Analysis)\n- In statistics, exploratory data analysis (EDA) is an approach to analyzing data sets to summarize their main characteristics, often with visual methods. A statistical model can be used or not, but primarily EDA is for seeing what the data can tell us beyond the formal modeling or hypothesis testing task.","metadata":{}},{"cell_type":"code","source":"import plotly.express as px\nimport plotly.graph_objects as go\nmultiple_offence_tab = train_file.groupby('MULTIPLE_OFFENSE').agg({'INCIDENT_ID':'count'}).reset_index()\ncolours = ['#00CC96','#636EFA']\nfig = go.Figure(data=[go.Bar(x = multiple_offence_tab.MULTIPLE_OFFENSE, y = multiple_offence_tab.INCIDENT_ID, marker_color = colours)])\nfig.update_layout(title_text='Multple Offense are very high',autosize=False, width=550,height=450)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"weekcount = train_file.groupby(['WEEKEND','MULTIPLE_OFFENSE']).agg({'INCIDENT_ID':'count'}).reset_index()\nweekcount['name'] = np.where(weekcount.WEEKEND == 0 , 'Weekday','Weekend')\nweekcount['Percentage'] = (weekcount['INCIDENT_ID']/sum(weekcount['INCIDENT_ID']) *100).round(2).astype(str) + '%'\nfig = px.bar(weekcount, x = 'MULTIPLE_OFFENSE', y = 'INCIDENT_ID',  barmode = 'group', color = 'name', text = 'Percentage',width=550,height=450)\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = train_file.groupby(['YEAR']).agg({'MULTIPLE_OFFENSE':'sum'}).reset_index()\nfig = px.line(df, x='YEAR', y='MULTIPLE_OFFENSE', width=550,height=450)\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = train_file.groupby('DAYOFWEEK').agg({'MULTIPLE_OFFENSE':'sum'}).reset_index()\ndf['weekname'] = df.DAYOFWEEK.map({0:'Monday', 1:'Tuesday',2:'Wednesday',3:'Thursday',4:'Friday',\n                                  5:'Saturday',6:'Sunday',})\nfig = px.pie(df, values = 'MULTIPLE_OFFENSE', names = 'weekname')\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nNumerical_features=['X_1','X_2','X_3','X_4','X_5','X_6','X_7','X_8','X_9','X_10','X_11','X_12', 'X_13','X_14','X_15']\nfig, ax = plt.subplots(5, 3, figsize=(20, 10))\nfor variable, subplot in zip(Numerical_features, ax.flatten()):\n    sns.distplot(train_file[variable], ax=subplot)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## SMOTE UPSAMPLING\n#### Dataset is imbalanced, so i'm using SMOTE upsamping techniques to make it balanced\n- SMOTE first selects a minority class instance a at random and finds its k nearest minority class neighbors. The synthetic instance is then created by choosing one of the k nearest neighbors b at random and connecting a and b to form a line segment in the feature space. The synthetic instances are generated as a convex combination of the two chosen instances a and b.","metadata":{}},{"cell_type":"code","source":"train_Ads = train_file.drop(['INCIDENT_ID','MULTIPLE_OFFENSE'], axis = 1)\ntrain_y = train_file['MULTIPLE_OFFENSE']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#!pip install imblearn","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from imblearn.over_sampling import SMOTE\noversample = SMOTE()\ntrain_Ads, train_y = oversample.fit_resample(train_Ads, train_y)\nlen(train_Ads),len(train_y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Spliting data for training the model. Splitting the data will be done at the begining of feature seletion phase\n\nX_train, X_test, y_train, y_test = train_test_split(train_Ads, train_y,test_size=0.20, random_state= 42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.head(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature Scaling\n- Scaled the data for each metrics by using feature scaling techniques to reduce the bias, to normalize the data within a range and speeding up the calculation while training the model. After applying the Standard Scaler, data range is in between -3 to 3\n","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\ndef scaling_train(df):\n    StandardScale = StandardScaler()\n    global X_train_ADS\n    X_train_ADS = pd.DataFrame(StandardScale.fit_transform(df),columns =  X_train.columns)\n    return X_train_ADS\n\ndef scaling_test(df):\n    StandardScale = StandardScaler()\n    global X_test_ADS\n    X_test_ADS = pd.DataFrame(StandardScale.fit_transform(df),columns =  X_train.columns)\n    return X_test_ADS","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scaling_train(X_train)\nscaling_test(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature Selection\n\n### Removing the Multicolinearity in dataset","metadata":{}},{"cell_type":"code","source":"heat = X_train_ADS.corr()\nf, ax = plt.subplots(figsize=(16, 12))\nsns.heatmap(heat, annot = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## CHI Square method to select features\n#### By using \"CHI Square test\", selecting the features with less P-values and high F-values i.e dependent feature is more dependent on those independent features  ","metadata":{}},{"cell_type":"code","source":"# Dropping th multi colinearity columns\nX_train_ADS = X_train_ADS.drop(['X_3','X_7','X_12','DAYOFWEEK'],axis = 1)\nfrom sklearn.feature_selection import chi2\nf_p_values=chi2(X_train[X_train_ADS.columns],y_train)\nf_p_values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Seecting the Top 10 columns ehich having less P values and more F value. Those columns we connsider for training the data\nk = pd.Series(f_p_values[1])\nk.index  = X_train_ADS.columns\nl = pd.Series(f_p_values[0])\nl.index  = X_train_ADS.columns\nframe = {'Pvalue':k, 'Fvalue':l}\nTop10cols = pd.DataFrame(frame).sort_values('Pvalue', ascending = True).head(10).index\nTop10cols","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_ADS = X_train_ADS[Top10cols]\nX_test_ADS = X_test_ADS[Top10cols]\nX_train_ADS.shape, X_test_ADS.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Creating a pipeline for Machine Learining models which requires feature scaling","metadata":{}},{"cell_type":"code","source":"model_names = {\n\n    \"Decision Tree Classfier\":\n    {\n        \"model\": DecisionTreeClassifier()\n    },\n    \"Random Forest Classfier\":\n    {\n        \"model\": RandomForestClassifier()\n    },\n    \"XgBoost Classfier\":\n    {\n        \"model\": GradientBoostingClassifier()\n    },\n    \"Logestic Regresssion\":\n    {\n        \"model\": LogisticRegression()\n    },\n    \"KNN Classfier\":\n    {\n        \"model\": KNeighborsClassifier()\n    },\n    \"SVC\":\n    {\n        \"model\" : SVC()\n    }\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Model_Name = []\nModel_Acc = []\nfor model_name, mn in model_names.items():\n    model = mn['model']\n    model.fit(X_train_ADS, y_train)\n    y_pred= model.predict(X_test_ADS)\n    print(model_name)\n    print(classification_report(y_test, y_pred))\n    print(\"Confusin Matrix: \\n\",confusion_matrix(y_test, y_pred))\n    print(\"Accuracy: \\t\",accuracy_score(y_test, y_pred))\n    print('\\n \\n')\n    Model_Name.append(model_name)\n    Model_Acc.append(accuracy_score(y_test, y_pred))\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ModelName_Accuracy = dict(zip(Model_Name,Model_Acc))\nModelName_Accuracy","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- We are obtaining good accuracy with KNN Classifier and Random Forest. If you are giving more importance to False Positive we can go with Ranfom Forest Classifier else KNN Classifier is preferred\n- If you like the notebook, Please Upvote add up your comments/Suggestions to this nootebook ..... Happy coding :)\n","metadata":{}}]}