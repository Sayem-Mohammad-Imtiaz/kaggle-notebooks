{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#This notebook was entirely writen by Rodrigo RamÃ­rez\n#The purpose of this notebook is to create an Artificial Neural Network in order to predict if a person is vulnerable to suffer a heart stroke\n\n\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-05-19T21:34:03.326874Z","iopub.execute_input":"2021-05-19T21:34:03.327197Z","iopub.status.idle":"2021-05-19T21:34:03.34234Z","shell.execute_reply.started":"2021-05-19T21:34:03.327117Z","shell.execute_reply":"2021-05-19T21:34:03.341287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**\nThis Notebook is not finished yet.**\n\n","metadata":{}},{"cell_type":"code","source":"#Reading and overview of Healthcare stroke dataset\n\ndataset  = pd.read_csv('../input/stroke-prediction-dataset/healthcare-dataset-stroke-data.csv')\ndataset.dropna()\ndataset.head(6)","metadata":{"execution":{"iopub.status.busy":"2021-05-19T21:34:03.344063Z","iopub.execute_input":"2021-05-19T21:34:03.34434Z","iopub.status.idle":"2021-05-19T21:34:03.394426Z","shell.execute_reply.started":"2021-05-19T21:34:03.344314Z","shell.execute_reply":"2021-05-19T21:34:03.393398Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Dataset info.\ndataset.info","metadata":{"execution":{"iopub.status.busy":"2021-05-19T21:34:03.395883Z","iopub.execute_input":"2021-05-19T21:34:03.396165Z","iopub.status.idle":"2021-05-19T21:34:03.41282Z","shell.execute_reply.started":"2021-05-19T21:34:03.396128Z","shell.execute_reply":"2021-05-19T21:34:03.41173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Splitting the dataframe: X\n\nX = dataset.iloc[:,1:-1]\nX.head()","metadata":{"execution":{"iopub.status.busy":"2021-05-19T21:34:03.414179Z","iopub.execute_input":"2021-05-19T21:34:03.414464Z","iopub.status.idle":"2021-05-19T21:34:03.437375Z","shell.execute_reply.started":"2021-05-19T21:34:03.414436Z","shell.execute_reply":"2021-05-19T21:34:03.436347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Splitting the dataframe: Y (label)\n\nY = dataset.iloc[:,-1]\nY.head()","metadata":{"execution":{"iopub.status.busy":"2021-05-19T21:34:03.438624Z","iopub.execute_input":"2021-05-19T21:34:03.438888Z","iopub.status.idle":"2021-05-19T21:34:03.450633Z","shell.execute_reply.started":"2021-05-19T21:34:03.438863Z","shell.execute_reply":"2021-05-19T21:34:03.449399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Encoding of Gender column using the Label Encoder from SKLearn\n\nfrom sklearn.preprocessing import LabelEncoder\nlabelEncoder = LabelEncoder()\nX.iloc[:,0]  = labelEncoder.fit_transform(X.iloc[:,0])\nX.head()","metadata":{"execution":{"iopub.status.busy":"2021-05-19T21:34:03.451856Z","iopub.execute_input":"2021-05-19T21:34:03.452203Z","iopub.status.idle":"2021-05-19T21:34:03.793938Z","shell.execute_reply.started":"2021-05-19T21:34:03.452175Z","shell.execute_reply":"2021-05-19T21:34:03.792857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Encoding of Ever_married column using the Label Encoder from SKLearn\n\nX.iloc[:,4]  = labelEncoder.fit_transform(X.iloc[:,4])\nX.head()","metadata":{"execution":{"iopub.status.busy":"2021-05-19T21:34:03.795333Z","iopub.execute_input":"2021-05-19T21:34:03.795744Z","iopub.status.idle":"2021-05-19T21:34:03.815394Z","shell.execute_reply.started":"2021-05-19T21:34:03.795701Z","shell.execute_reply":"2021-05-19T21:34:03.814394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Encoding of Residence_type column using the Label Encoder from SKLearn\n\nX.iloc[:,6].unique()\nX.iloc[:,6]  = labelEncoder.fit_transform(X.iloc[:,6])\nX.head()","metadata":{"execution":{"iopub.status.busy":"2021-05-19T21:34:03.818103Z","iopub.execute_input":"2021-05-19T21:34:03.818538Z","iopub.status.idle":"2021-05-19T21:34:03.840981Z","shell.execute_reply.started":"2021-05-19T21:34:03.818504Z","shell.execute_reply":"2021-05-19T21:34:03.839901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#For columns 'work_type' and 'smoking_status' we weill use OneHotEncoder from SKLearn\n\n\n#from sklearn.compose import ColumnTransformer\n#from sklearn.preprocessing import OneHotEncoder\n\n#columnTransformer = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [5])], remainder='passthrough')\n#X = columnTransformer.fit_transform(X)\n\n\n#columnTransformer = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [-1])], remainder='passthrough')\n#X = columnTransformer.fit_transform(X)\n\n#X\n\nX  = X.drop(['work_type', 'smoking_status'], axis=1) #Drop columns: only momentarily","metadata":{"execution":{"iopub.status.busy":"2021-05-19T21:34:03.842422Z","iopub.execute_input":"2021-05-19T21:34:03.8427Z","iopub.status.idle":"2021-05-19T21:34:03.853727Z","shell.execute_reply.started":"2021-05-19T21:34:03.842673Z","shell.execute_reply":"2021-05-19T21:34:03.85274Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Splitting dataframe into X_train, X_test, y_train and y_test\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2, random_state = 0)","metadata":{"execution":{"iopub.status.busy":"2021-05-19T21:34:03.855244Z","iopub.execute_input":"2021-05-19T21:34:03.855595Z","iopub.status.idle":"2021-05-19T21:34:03.884122Z","shell.execute_reply.started":"2021-05-19T21:34:03.855563Z","shell.execute_reply":"2021-05-19T21:34:03.883181Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Delete any outliers in the data using StandardScaler from SKLearn\n\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","metadata":{"execution":{"iopub.status.busy":"2021-05-19T21:34:03.885383Z","iopub.execute_input":"2021-05-19T21:34:03.885692Z","iopub.status.idle":"2021-05-19T21:34:03.896718Z","shell.execute_reply.started":"2021-05-19T21:34:03.885647Z","shell.execute_reply":"2021-05-19T21:34:03.895792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Import Tensorflow and create a Sequential Model to add layer for the ANN\n\nimport tensorflow as tf\nann = tf.keras.models.Sequential()","metadata":{"execution":{"iopub.status.busy":"2021-05-19T21:34:03.897806Z","iopub.execute_input":"2021-05-19T21:34:03.898075Z","iopub.status.idle":"2021-05-19T21:34:05.67116Z","shell.execute_reply.started":"2021-05-19T21:34:03.898048Z","shell.execute_reply":"2021-05-19T21:34:05.670152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Add the first layer with 6 units and the activation function ReLU\n\nann.add(tf.keras.layers.Dense(units=6, activation='relu'))","metadata":{"execution":{"iopub.status.busy":"2021-05-19T21:34:05.681609Z","iopub.execute_input":"2021-05-19T21:34:05.682213Z","iopub.status.idle":"2021-05-19T21:34:05.697493Z","shell.execute_reply.started":"2021-05-19T21:34:05.682169Z","shell.execute_reply":"2021-05-19T21:34:05.696764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Add the second and last layer with 1 unit and the activation function Sigmoid\n# I use the a Sigmoid Function because I want my prediction to be classified into 0 an 1 (Yes and No)\n\nann.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))","metadata":{"execution":{"iopub.status.busy":"2021-05-19T21:34:05.698414Z","iopub.execute_input":"2021-05-19T21:34:05.698761Z","iopub.status.idle":"2021-05-19T21:34:05.711614Z","shell.execute_reply.started":"2021-05-19T21:34:05.698734Z","shell.execute_reply":"2021-05-19T21:34:05.710729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Compiling the ANN using ADAM optimizer.\n\nann.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2021-05-19T21:34:05.713056Z","iopub.execute_input":"2021-05-19T21:34:05.713596Z","iopub.status.idle":"2021-05-19T21:34:05.738049Z","shell.execute_reply.started":"2021-05-19T21:34:05.713565Z","shell.execute_reply":"2021-05-19T21:34:05.736789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Train the ANN with 100 epochs.\n\nann.fit(X_train, y_train, batch_size = 32, epochs = 100)","metadata":{"execution":{"iopub.status.busy":"2021-05-19T21:34:05.739068Z","iopub.execute_input":"2021-05-19T21:34:05.739514Z","iopub.status.idle":"2021-05-19T21:34:18.512016Z","shell.execute_reply.started":"2021-05-19T21:34:05.73948Z","shell.execute_reply":"2021-05-19T21:34:18.510868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#I set the threshold for the predictions. In this case, my threshold is 0.5 (this value can be modified).\n\ny_pred = ann.predict(X_test)\ny_pred = (y_pred > 0.5)\n","metadata":{"execution":{"iopub.status.busy":"2021-05-19T21:34:18.515322Z","iopub.execute_input":"2021-05-19T21:34:18.515676Z","iopub.status.idle":"2021-05-19T21:34:18.629506Z","shell.execute_reply.started":"2021-05-19T21:34:18.515646Z","shell.execute_reply":"2021-05-19T21:34:18.62847Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# A confusion matrix helps to evaluate the accuracy of the predictions made. \n\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\naccuracy_score(y_test, y_pred)","metadata":{"execution":{"iopub.status.busy":"2021-05-19T21:34:18.630918Z","iopub.execute_input":"2021-05-19T21:34:18.63131Z","iopub.status.idle":"2021-05-19T21:34:18.64596Z","shell.execute_reply.started":"2021-05-19T21:34:18.631266Z","shell.execute_reply":"2021-05-19T21:34:18.644849Z"},"trusted":true},"execution_count":null,"outputs":[]}]}