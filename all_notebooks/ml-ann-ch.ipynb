{"cells":[{"metadata":{},"cell_type":"markdown","source":"人工神经网络的实现例子。这里介绍ANN 的两个应用：回归和分类。首先建立一个基本的回归模型；之后加入 early stopping 功能提高模型的性能；最后介绍用于分类的基本模型。"},{"metadata":{},"cell_type":"markdown","source":"# **加载数据和软件包**"},{"metadata":{},"cell_type":"markdown","source":"以下加载我们需要用到的软件包。作为例子的数据'fuel'已经加载到目录 '../input/dl-course-data/fuel.csv' 下面。"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Setup plotting\nimport matplotlib.pyplot as plt\nplt.style.use('seaborn-whitegrid')\n# Set Matplotlib defaults\nplt.rc('figure', autolayout=True)\nplt.rc('axes', labelweight='bold', labelsize='large',\n       titleweight='bold', titlesize=18, titlepad=10)\nplt.rc('animation', html='html5')\n\n# Setup feedback system\nfrom learntools.core import binder\nbinder.bind(globals())\nfrom learntools.deep_learning_intro.ex6 import *","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import callbacks\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.compose import make_column_transformer, make_column_selector\nfrom sklearn.model_selection import GroupShuffleSplit","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **ANN 用于回归 (Regression)**"},{"metadata":{},"cell_type":"markdown","source":"加载 fuel 的数据作为例子，首先加载数据，划分特征值和目标值。这里数据的列 'FE' 作为目标值。同时对数据进行标准的处理：scaling, encoding。"},{"metadata":{"trusted":true},"cell_type":"code","source":"fuel = pd.read_csv('../input/dl-course-data/fuel.csv')\n\nX = fuel.copy()\n# Remove target\ny = X.pop('FE')\n\npreprocessor = make_column_transformer(\n    (StandardScaler(),\n     make_column_selector(dtype_include=np.number)),\n    (OneHotEncoder(sparse=False),\n     make_column_selector(dtype_include=object)),\n)\n\nX = preprocessor.fit_transform(X)\ny = np.log(y) # log transform target instead of standardizing\n\ninput_shape = [X.shape[1]]\nprint(\"Input shape: {}\".format(input_shape))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"一个基本的 ANN 模型建立分为几个步骤：\n- 初始化模型\n- 追加模型的层 （layer)\n- 编译模型\n- 用数据对模型进行训练\n- 模型对新数据预测"},{"metadata":{},"cell_type":"markdown","source":"以下对模型进行初始化，并建立三个隐藏层 (hidden layer)，最后建立输出层。\n\n三个隐藏层的神经元数目分别为 128， 128， 64。最后输出的是实数，因此输出层的神经元数目为 1。"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = keras.Sequential([\n    layers.Dense(128, activation='relu', input_shape=input_shape),\n    layers.Dense(128, activation='relu'),    \n    layers.Dense(64, activation='relu'),\n    layers.Dense(1),\n])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"之后对模型编译，即，定义优化方法 optimizer 和损失函数 loss。"},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(\n    optimizer='adam',\n    loss='mae'\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"模型建立以后，用数据对木星进行训练。训练的结果是一个基于数据和 loss function 的最优的模型。这里我们定义 epoch=200, bach_size = 218。"},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(\n    X, y,\n    batch_size=128,\n    epochs=200\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"最后检查模型的性能，画出 loss funcion 的曲线。"},{"metadata":{"trusted":true},"cell_type":"code","source":"history_df = pd.DataFrame(history.history)\n\n# Start the plot at epoch 5. You can change this to get a different view.\nhistory_df.loc[5:, ['loss']].plot();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **改进的回归模型**"},{"metadata":{},"cell_type":"markdown","source":"以上介绍了一个基本的 ANN 回归模型。这里介绍两种基本的模型改进方法：\n- train/validation 划分\n- early stopping callback"},{"metadata":{},"cell_type":"markdown","source":"以下的例子加载数据 'spottify' 作为示范。"},{"metadata":{"trusted":true},"cell_type":"code","source":"spotify = pd.read_csv('../input/dl-course-data/spotify.csv')\n\nX = spotify.copy().dropna()\ny = X.pop('track_popularity')\nartists = X['track_artist']\n\nfeatures_num = ['danceability', 'energy', 'key', 'loudness', 'mode',\n                'speechiness', 'acousticness', 'instrumentalness',\n                'liveness', 'valence', 'tempo', 'duration_ms']\nfeatures_cat = ['playlist_genre']\n\npreprocessor = make_column_transformer(\n    (StandardScaler(), features_num),\n    (OneHotEncoder(), features_cat),\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"经过标准的处理以后，把数据划分为 train(75%) 和 validation(25%)。"},{"metadata":{"trusted":true},"cell_type":"code","source":"# We'll do a \"grouped\" split to keep all of an artist's songs in one\n# split or the other. This is to help prevent signal leakage.\ndef group_split(X, y, group, train_size=0.75):\n    splitter = GroupShuffleSplit(train_size=train_size)\n    train, test = next(splitter.split(X, y, groups=group))\n    return (X.iloc[train], X.iloc[test], y.iloc[train], y.iloc[test])\n\nX_train, X_valid, y_train, y_valid = group_split(X, y, artists)\n\nX_train = preprocessor.fit_transform(X_train)\nX_valid = preprocessor.transform(X_valid)\ny_train = y_train / 100 # popularity is on a scale 0-100, so this rescales to 0-1.\ny_valid = y_valid / 100\n\ninput_shape = [X_train.shape[1]]\nprint(\"Input shape: {}\".format(input_shape))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"在建立模型以前首先定义 early stopping。以下的定义中有两个主要的参数：\n- patience: 定义了一个epoch的数目，当经过连续的 epoch 之后模型没有改进，模型训练停止\n- min_delta: 是一个阈值，当 loss 的绝对变化小于这个阈值，我们认为模型没有改进，从而停止训练"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define early stopping callack\nearly_stopping = callbacks.EarlyStopping(\n    patience=5,\n    min_delta=0.001,\n    restore_best_weights=True,\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"定义了 early stopping 之后，开始建立模型：初始化一个三层的模型；编译模型；用数据训练模型。\n\n在训练模型的步骤中，加入了之前定义的 early_stopping 函数。"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = keras.Sequential([\n    layers.Dense(128, activation='relu', input_shape=input_shape),\n    layers.Dense(64, activation='relu'),\n    layers.Dense(1)\n])\nmodel.compile(\n    optimizer='adam',\n    loss='mae',\n)\n\nhistory = model.fit(\n    X_train, y_train,\n    validation_data=(X_valid, y_valid),\n    batch_size=512,\n    epochs=50,\n    callbacks=[early_stopping]\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"最后观测模型的性能。"},{"metadata":{"trusted":true},"cell_type":"code","source":"history_df = pd.DataFrame(history.history)\nhistory_df.loc[:, ['loss', 'val_loss']].plot()\nprint(\"Minimum Validation Loss: {:0.4f}\".format(history_df['val_loss'].min()));","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **ANN 用于分类 （classification)**"},{"metadata":{},"cell_type":"markdown","source":"首先加载数据。这里，以 'hotel' 的数据为例进行演示。"},{"metadata":{"trusted":true},"cell_type":"code","source":"hotel = pd.read_csv('../input/dl-course-data/hotel.csv')\n\nX = hotel.copy()\ny = X.pop('is_canceled')\n\nX['arrival_date_month'] = \\\n    X['arrival_date_month'].map(\n        {'January':1, 'February': 2, 'March':3,\n         'April':4, 'May':5, 'June':6, 'July':7,\n         'August':8, 'September':9, 'October':10,\n         'November':11, 'December':12}\n    )\n\nfeatures_num = [\n    \"lead_time\", \"arrival_date_week_number\",\n    \"arrival_date_day_of_month\", \"stays_in_weekend_nights\",\n    \"stays_in_week_nights\", \"adults\", \"children\", \"babies\",\n    \"is_repeated_guest\", \"previous_cancellations\",\n    \"previous_bookings_not_canceled\", \"required_car_parking_spaces\",\n    \"total_of_special_requests\", \"adr\",\n]\nfeatures_cat = [\n    \"hotel\", \"arrival_date_month\", \"meal\",\n    \"market_segment\", \"distribution_channel\",\n    \"reserved_room_type\", \"deposit_type\", \"customer_type\",\n]\n\ntransformer_num = make_pipeline(\n    SimpleImputer(strategy=\"constant\"), # there are a few missing values\n    StandardScaler(),\n)\ntransformer_cat = make_pipeline(\n    SimpleImputer(strategy=\"constant\", fill_value=\"NA\"),\n    OneHotEncoder(handle_unknown='ignore'),\n)\n\npreprocessor = make_column_transformer(\n    (transformer_num, features_num),\n    (transformer_cat, features_cat),\n)\n\n# stratify - make sure classes are evenlly represented across splits\nX_train, X_valid, y_train, y_valid = \\\n    train_test_split(X, y, stratify=y, train_size=0.75)\n\nX_train = preprocessor.fit_transform(X_train)\nX_valid = preprocessor.transform(X_valid)\n\ninput_shape = [X_train.shape[1]]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"分类和回归问题的一个主要不同点是，在分类中，普通的模型性能度量不能用做 loss function, 因为在模型优化的算法中，SGD 要求 loss function 是光滑函数(便于求导）。因此，cross-entropy 函数用来作为分类问题的 loss function。\n\n同时，我们用 sigmoid activation 将输出的实数值转化为离散的数值 (即分类问题的标签)。"},{"metadata":{},"cell_type":"markdown","source":"以下初始化模型，并加入 hidden layers。不同于之前的模型，我们加入了两个函数：\n- batchnormalization: 一种将每一层的输入特征值进行标准化的技术，其结果是训练的结果更为稳定，并更快地收敛到最优结果。\n- dropout: 在层之间的数据传输中，摒弃部分数据，增加模型的随机性，避免过度拟合。"},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow import keras\nfrom tensorflow.keras import layers\n\n# YOUR CODE HERE: define the model given in the diagram\nmodel = keras.Sequential([\n    layers.BatchNormalization(input_shape=input_shape),\n    layers.Dense(256, activation='relu'),\n    layers.BatchNormalization(),\n    layers.Dropout(0.3),\n    layers.Dense(256, activation='relu'),\n    layers.BatchNormalization(),\n    layers.Dropout(0.3),\n    layers.Dense(1, activation='sigmoid'),\n])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"编译模型，注意这里的 loss function 类型是 binary_crossentrpy。"},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(\n    optimizer='adam',\n    loss='binary_crossentropy',\n    metrics=['binary_accuracy'],\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"如前定义 early stopping，并对模型进行训练。"},{"metadata":{"trusted":true},"cell_type":"code","source":"early_stopping = keras.callbacks.EarlyStopping(\n    patience=5,\n    min_delta=0.001,\n    restore_best_weights=True,\n)\nhistory = model.fit(\n    X_train, y_train,\n    validation_data=(X_valid, y_valid),\n    batch_size=512,\n    epochs=200,\n    callbacks=[early_stopping],\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"最后观测模型的性能。"},{"metadata":{"trusted":true},"cell_type":"code","source":"history_df = pd.DataFrame(history.history)\nhistory_df.loc[:, ['loss', 'val_loss']].plot(title=\"Cross-entropy\")\nhistory_df.loc[:, ['binary_accuracy', 'val_binary_accuracy']].plot(title=\"Accuracy\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}