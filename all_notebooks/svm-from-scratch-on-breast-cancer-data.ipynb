{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-28T05:15:59.846851Z","iopub.execute_input":"2021-07-28T05:15:59.847245Z","iopub.status.idle":"2021-07-28T05:15:59.855717Z","shell.execute_reply.started":"2021-07-28T05:15:59.847211Z","shell.execute_reply":"2021-07-28T05:15:59.854773Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport sklearn\nimport random\nimport math\nimport sys","metadata":{"execution":{"iopub.status.busy":"2021-07-28T05:48:59.333685Z","iopub.execute_input":"2021-07-28T05:48:59.334075Z","iopub.status.idle":"2021-07-28T05:48:59.338311Z","shell.execute_reply.started":"2021-07-28T05:48:59.334046Z","shell.execute_reply":"2021-07-28T05:48:59.33764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, recall_score, precision_score\nfrom sklearn.utils import shuffle\nimport statsmodels.api as sm\n\n\ndef remove_correlated_features(X):\n    corr_threshold = 0.9\n    corr = X.corr()\n    drop_columns = np.full(corr.shape[0], False, dtype=bool)\n    for i in range(corr.shape[0]):\n        for j in range(i + 1, corr.shape[0]):\n            if corr.iloc[i, j] >= corr_threshold:\n                drop_columns[j] = True\n    columns_dropped = X.columns[drop_columns]\n    X.drop(columns_dropped, axis=1, inplace=True)\n    return columns_dropped\n\n\ndef remove_less_significant_features(X, Y):\n    sl = 0.05\n    regression_ols = None\n    columns_dropped = np.array([])\n    print(X.shape)\n    for itr in range(0, len(X.columns)):\n        regression_ols = sm.OLS(Y, X).fit()\n        max_col = regression_ols.pvalues.idxmax()\n        max_val = regression_ols.pvalues.max()\n        if max_val > sl:\n            X.drop(max_col, axis='columns', inplace=True)\n            columns_dropped = np.append(columns_dropped, [max_col])\n        else:\n            break\n    regression_ols.summary()\n    return columns_dropped\n\ndef preprocess_data(df):\n    # Applying Label values\n    label_map = {'M': 1.0, 'B': -1.0}\n    df['diagnosis'] = df.diagnosis.apply(label_map.get)\n    \n    # Remove unnecessary columns\n    df.drop(['id', 'Unnamed: 32'], axis=1, inplace=True)\n    \n    # Separate label and training data\n    X = df.iloc[:, 1:]\n    Y = df.loc[:, 'diagnosis']\n    \n    # Normalize data\n    sc = MinMaxScaler()\n    X_norm = sc.fit_transform(X)\n    X = pd.DataFrame(X_norm)\n    \n    remove_correlated_features(X)\n    remove_less_significant_features(X, Y)\n    \n    # Do train test split\n    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n    return X_train, X_test, y_train, y_test","metadata":{"execution":{"iopub.status.busy":"2021-07-28T07:26:00.499776Z","iopub.execute_input":"2021-07-28T07:26:00.500369Z","iopub.status.idle":"2021-07-28T07:26:01.191714Z","shell.execute_reply.started":"2021-07-28T07:26:00.500336Z","shell.execute_reply":"2021-07-28T07:26:01.190842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path = '/kaggle/input/breast-cancer-wisconsin-data/data.csv'\ndf = pd.read_csv(path)\nX_train, X_test, y_train, y_test = preprocess_data(df) \nlist(map(lambda x: len(x), [X_train, X_test, y_train, y_test]))","metadata":{"execution":{"iopub.status.busy":"2021-07-28T07:26:03.771559Z","iopub.execute_input":"2021-07-28T07:26:03.771923Z","iopub.status.idle":"2021-07-28T07:26:03.876338Z","shell.execute_reply.started":"2021-07-28T07:26:03.771895Z","shell.execute_reply":"2021-07-28T07:26:03.874503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.dtypes","metadata":{"execution":{"iopub.status.busy":"2021-07-28T06:34:08.459684Z","iopub.execute_input":"2021-07-28T06:34:08.460042Z","iopub.status.idle":"2021-07-28T06:34:08.467456Z","shell.execute_reply.started":"2021-07-28T06:34:08.460011Z","shell.execute_reply":"2021-07-28T06:34:08.466784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# SVM Model Implementation\n- https://towardsdatascience.com/svm-implementation-from-scratch-python-2db2fc52e5c2#d7d8","metadata":{}},{"cell_type":"markdown","source":"#### Cost Function\n\nsvm loss = 0.5 * ||W||^2  + loss\n\nsvm optimization function = y*(X.W) >= 1 ==> (1 - y*(X.W)) <= 0\n\n\n#### Cost Gradient\n\ngrad = w -> when max(0, yi * (w * xi) = 0\n\nw - lambda * yi * xi --> when not zero\n\n#### SGD\nupdate w = w - learning_rate * grad","metadata":{}},{"cell_type":"code","source":"learning_rate = 0.000001\nlmda = 10000\n\ndef compute_cost(W, X, Y):\n    # Calculate hinge losss = \n    N = X.shape[0]\n    distance = 1 - Y * (np.dot(X, W))\n    distance[distance < 0] = 0.0\n    hinge_loss = lmda * np.sum(distance) / N\n    \n    # Cost\n    cost = 0.5 * np.dot(W, W) + hinge_loss\n    return cost\n\ndef cost_gradient(W, X_batch, Y_batch):\n    if type(Y_batch) == np.float64:\n        Y_batch = np.array([Y_batch])\n        X_batch = np.array([X_batch])\n    \n    dist = 1 - (Y_batch * np.dot(X_batch, W))\n    dw = np.zeros(len(W))\n\n    for indx, d in enumerate(dist):\n        if max(0, d) == 0:\n            di = W\n        else:\n            di = W - (lmda * Y_batch[indx] * X_batch[indx])\n        dw += di\n    \n    dw /= len(Y_batch)\n        \n    return dw\n\ndef sgd(features, \n        output, \n        num_epoch):\n    \n    weights = np.zeros(features.shape[1])\n    \n    n = 0\n    prev_cost = float(\"inf\")\n    cost_threshold = 0.01\n    for epoch in range(num_epoch):\n        X, Y = shuffle(features, output)\n        for ind, x in enumerate(X):\n            ascent = cost_gradient(weights, x, Y[ind])\n            weights = weights - (learning_rate * ascent)\n        \n        # stopping condition\n        if epoch == 2 ** n or epoch == num_epoch - 1:\n            cost = compute_cost(weights, X, Y)\n            cost_diff = abs(prev_cost - cost)\n            print(f'epoch: {epoch}, prev_cost: {prev_cost}, new_cost: {cost}, diff: {cost_diff}')\n            \n            if cost_diff < cost_threshold * prev_cost:\n                return weights\n            \n            prev_cost = cost\n            n += 1\n            \n    return weights\n\ndef test_model(X_test, y_test, W):\n    # testing the model on test set\n    y_test_predicted = np.array([])\n    for i in range(X_test.shape[0]):\n        yp = np.sign(np.dot(W, X_test.to_numpy()[i])) #model\n        y_test_predicted = np.append(y_test_predicted, yp)\n    print(\"accuracy on test dataset: {}\".format(accuracy_score(y_test.to_numpy(), y_test_predicted)))\n    print(\"recall on test dataset: {}\".format(recall_score(y_test.to_numpy(), y_test_predicted)))\n    print(\"precision on test dataset: {}\".format(precision_score(y_test.to_numpy(), y_test_predicted)))","metadata":{"execution":{"iopub.status.busy":"2021-07-28T07:26:08.196448Z","iopub.execute_input":"2021-07-28T07:26:08.196804Z","iopub.status.idle":"2021-07-28T07:26:08.213341Z","shell.execute_reply.started":"2021-07-28T07:26:08.196776Z","shell.execute_reply":"2021-07-28T07:26:08.212286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Running Model","metadata":{}},{"cell_type":"code","source":"weights = sgd(X_train.to_numpy(), y_train.to_numpy(), 5000)","metadata":{"execution":{"iopub.status.busy":"2021-07-28T07:26:10.775449Z","iopub.execute_input":"2021-07-28T07:26:10.775988Z","iopub.status.idle":"2021-07-28T07:26:34.477501Z","shell.execute_reply.started":"2021-07-28T07:26:10.775944Z","shell.execute_reply":"2021-07-28T07:26:34.476326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_model(X_test, y_test, weights)","metadata":{"execution":{"iopub.status.busy":"2021-07-28T07:26:34.47892Z","iopub.execute_input":"2021-07-28T07:26:34.47923Z","iopub.status.idle":"2021-07-28T07:26:34.491315Z","shell.execute_reply.started":"2021-07-28T07:26:34.479202Z","shell.execute_reply":"2021-07-28T07:26:34.490413Z"},"trusted":true},"execution_count":null,"outputs":[]}]}