{"nbformat_minor":1,"nbformat":4,"cells":[{"source":"# Experiment\n\nThis is a little experiment to see if I have understood the principles of machine learning as layed out in Andrew Ng's specialization in Coursera. If anyone can give me some pointers on things that can be further improved or that I have misunderstood while trying to make sense of this data please do so! ","cell_type":"markdown","metadata":{"_uuid":"ec5b9f682a5fc8cc048870ac0c9f8522d9e37a7a","_cell_guid":"a62afa81-99d1-4221-9372-9eaed2e7d29d"}},{"outputs":[],"execution_count":null,"source":"from subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n","cell_type":"code","metadata":{"_uuid":"9c7095f8d631d990e03445ea5dbdf5afc63c52eb","_cell_guid":"dd0688d3-1b30-40f9-9e60-97d12bfe19e7"}},{"source":"# Import dependencies","cell_type":"markdown","metadata":{"_uuid":"54f51274fc49a007c835c15258beae543e8629fd","_cell_guid":"eb5ffeaf-8fe1-48bd-afdf-41ffac37308f"}},{"outputs":[],"execution_count":null,"source":"import numpy as np \nimport pandas as pd \nimport math\nimport matplotlib.pyplot as plt # we will use this for plotting\nfrom sklearn.model_selection import train_test_split # very handy way of selecting sets\nimport tensorflow as tf","cell_type":"code","metadata":{"collapsed":true,"_uuid":"323f194fa698e8c31faf324a6508a7cb523ed308","_cell_guid":"2e3d6cda-6053-4d92-8cf4-e43cfb01c971"}},{"source":"# Load data","cell_type":"markdown","metadata":{"_uuid":"44d5b9e7951aa046c0ff008cc9c24b30447b722e","_cell_guid":"c65623dd-afb3-4d20-8e05-c21af8b2d050"}},{"outputs":[],"execution_count":null,"source":"data = pd.read_csv('../input/Wine.csv', header=None)\ndata.columns = ['name'\n                ,'alcohol'\n                ,'malicAcid'\n                ,'ash'\n                ,'ashalcalinity'\n                 ,'magnesium'\n                ,'totalPhenols'\n                 ,'flavanoids'\n                 ,'nonFlavanoidPhenols'\n                 ,'proanthocyanins'\n                ,'colorIntensity'\n                 ,'hue'\n                 ,'od280_od315'\n                 ,'proline'\n                ]\n\ndata.isnull().sum() # Check if there are any missing values","cell_type":"code","metadata":{"collapsed":true,"_uuid":"10b73ce6a986291856a81c8f11998c28235a00c7","_cell_guid":"04d5497a-8845-4d34-8dfc-e54892852d79"}},{"source":"# Extract features\n\nI have learned by looking at this kernel: https://www.kaggle.com/abhikaggle8/wine-classification/notebook\nand trying out what the author has done, that excluding correlations is important as I won over 30 percentile points of accuracy doing so! I'm gonna show the heatmap that the author or that kernel has produced: ","cell_type":"markdown","metadata":{"_uuid":"c63d95fd92b5515a1b567c93cbe847e878d10d20","_cell_guid":"34b132f6-b500-4d73-b4ee-8f0fee22bb3e"}},{"outputs":[],"execution_count":null,"source":"import seaborn as sns\ncorrelations = data[data.columns].corr(method='pearson')\nsns.heatmap(correlations, cmap=\"YlGnBu\", annot = True)","cell_type":"code","metadata":{"collapsed":true,"_uuid":"0fcd62676d3d028296821ff52e40c5411130d82d","_cell_guid":"a42406f5-3e94-42a3-955a-a33b1fa1ac85"}},{"source":"But is kind of difficult for me to make sense out of the table... the easiest way to find weak correlations is to get the abs sum of each column (I think)\n","cell_type":"markdown","metadata":{"_uuid":"a18b32dcf646ecfca5701a1bb05714ea00d6290f","_cell_guid":"b03eadc5-d12d-41f3-9959-4ae379cbefca"}},{"outputs":[],"execution_count":null,"source":"import heapq\n\nprint('Absolute overall correlations')\nprint('-' * 30)\ncorrelations_abs_sum = correlations[correlations.columns].abs().sum()\nprint(correlations_abs_sum, '\\n')\n\nprint('Weakest correlations')\nprint('-' * 30)\nprint(correlations_abs_sum.nsmallest(3))","cell_type":"code","metadata":{"collapsed":true,"_uuid":"09676b61e0d8a80ba6023564a9a41617960751c6","_cell_guid":"a38844ed-9df9-4724-acb2-d0a1bb2d315a"}},{"outputs":[],"execution_count":null,"source":"# From this we learn that we could drop these 3 parameters and improve our algorithm...possibly?\n# We also need to drop 'name' as that is our label vector in fact!\n#X_data = data.drop(['name','ash', 'magnesium', 'colorIntensity'], axis=1)\n#X_data = data.drop(['name','ash', 'magnesium'], axis=1)\nX_data = data.drop(['name','ash'], axis=1)\n\nY_data = data.iloc[:,:1] # take all the names (see pandas reference for iloc vs loc)\nclasses = Y_data.name.unique()\nnum_classes = len(classes)\nprint('Class names: ', classes)\nprint('Number of classes: ', num_classes)","cell_type":"code","metadata":{"collapsed":true,"_uuid":"1b737a0865cb7ad8625537f9edf485312773e9f0","_cell_guid":"3d5ac018-b7e3-4393-aa0c-296e637d5763"}},{"source":"# Convert data\n\nFor convenience, we transform the dataframe data from pandas to numpy arrays","cell_type":"markdown","metadata":{"_uuid":"8462683d681fab55a7198ddd9a122eaab6916116","_cell_guid":"05435043-bf96-4e19-9387-2a36da37085a"}},{"outputs":[],"execution_count":null,"source":"X = X_data.values \nY = Y_data.values\nprint('Data types: ', type(X), type(Y))","cell_type":"code","metadata":{"collapsed":true,"_uuid":"fb596ec3d212e61e9e746c81bf26abdd94fbb3e7","_cell_guid":"e6d2d7e3-bd14-4598-bce5-b29d96e47cb4"}},{"source":"# Create labels\n\nWe create a 3 label array for our class representation","cell_type":"markdown","metadata":{"_uuid":"1e626138d4b734132a0e0a711cbad9bfba46c3ca","_cell_guid":"83e38b1c-831f-4405-a0a1-710a4dc390e4"}},{"outputs":[],"execution_count":null,"source":"def labelMaker(val):\n    if val == 1:\n        return [1, 0, 0]\n    elif val == 2:\n        return [0, 1, 0]\n    else: \n        return [0, 0, 1]\n\nY = np.array([labelMaker(i[0]) for i in Y])\nprint(Y.shape)","cell_type":"code","metadata":{"collapsed":true,"_uuid":"595e6a5013f45faa486c1b5e1915d0184b2bacc0","_cell_guid":"0421d834-28d5-423c-a536-8aaf3f6827e8"}},{"source":"# Create training and testing sets\n\nThis data set is quite small (it has 178 elements, nowadays a size up to 10000 data points is considered to be small),  so we can  split our data in the classic 70% vs 30% scheme with confidence that is an acceptable distribution.","cell_type":"markdown","metadata":{"_uuid":"402823073a6cbb536ce2dfd378d8a95849036dc1","_cell_guid":"fecf9f2e-06be-456b-8aec-71d0230d97f7"}},{"outputs":[],"execution_count":null,"source":"from sklearn.model_selection import train_test_split # very handy way of selecting training and testing sets\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=0)\n\ntraining_size = X_train.shape[0]\nnum_parameters = X_train.shape[1]\nnum_classes = Y_train.shape[1]\nprint('Training size: ', training_size)\nprint('Number of parameters: ', num_parameters)\nprint('Number of classes: ', num_classes)\n","cell_type":"code","metadata":{"collapsed":true,"_uuid":"e0e6732600901cdbad0ec57e62bd36f5378af16e","_cell_guid":"60f12d6e-22c8-4345-87fe-ec0be406936a"}},{"source":"# Checking our shapes are correct\n\nIt is important to notice that TensorFlow expects shapes of: \n- X(input_size, number of examples)  \n- Y(classes, number of examples)\n\nSo we are going to transpose the matrices to correspond to expectation:","cell_type":"markdown","metadata":{"_uuid":"ed747b5a190c82ac59d952d9407d31694cad3c85","_cell_guid":"8b54f272-3031-4779-9f18-184be3582b5d"}},{"outputs":[],"execution_count":null,"source":"# (n_x: input size, m : number of examples in the train set)\n# (n_y : output size, m: number of examples)\nX_train = X_train.transpose()\nY_train = Y_train.transpose()\nX_test = X_test.transpose()\nY_test = Y_test.transpose()\n\nprint(X_train.shape)\nprint(Y_train.shape)\nprint(X_test.shape)\nprint(Y_test.shape)","cell_type":"code","metadata":{"collapsed":true,"_uuid":"1dc067bc40f6d9c051f90c390ad2ce5ed977499b","_cell_guid":"c28067ec-93be-472c-929a-d94b275b9a1b"}},{"source":"# Create our prediction model\n\nI will be using TensorFlow for this notebook. It is important to first understand the problem we are tackling before choosing a loss function for TensorFlow. In this case, we are trying to build a classifier that can determine what is the name (class) of the wine based on the given parameters. So a loss function that works well with a classifier is the right decision.\n\nSince we don't have binary labeling problem, but a multilabeling problem (since there are many names and not just 2) we will need to use a softmax() function for the output layer so that we can have a variable range of probabilities for each one of the labels. \n\nI will be now using Andrew Ng's functions to create the model to see if I have understood how to set my data to fit the model.","cell_type":"markdown","metadata":{"_uuid":"1b6e25c759a5b3fbb9a321497d484c5fcbe184a7","_cell_guid":"a15a2ff8-63e1-4502-9f2c-dd60fac00eea"}},{"outputs":[],"execution_count":null,"source":"# n_x = num__input_features\n# n_y = expected output (num classes)\ndef create_placeholders(n_x, n_y):\n    X = tf.placeholder(tf.float32, [n_x, None], name=\"X\")\n    Y = tf.placeholder(tf.float32, [n_y, None], name=\"Y\")\n    return X, Y\n\ndef initialize_parameters(num_input_features=12):\n    \"\"\"\n    Initializes parameters to build a neural network with tensorflow. The shapes are:\n                        W1 : [num_hidden_layer, num_input_features]\n                        b1 : [num_hidden_layer, 1]\n                        W2 : [num_output_layer_1, num_hidden_layer]\n                        b2 : [num_output_layer_1, 1]\n                        W3 : [num_output_layer_2, num_output_layer_1]\n                        b3 : [num_output_layer_2, 1]\n    \n    Returns:\n    parameters -- a dictionary of tensors containing W1, b1, W2, b2, W3, b3\n    \"\"\" \n    tf.set_random_seed(1)           \n    W1 = tf.get_variable(\"W1\", [10, num_input_features], initializer = tf.contrib.layers.xavier_initializer(seed=1))\n    b1 = tf.get_variable(\"b1\", [10, 1], initializer = tf.zeros_initializer())\n    W2 = tf.get_variable(\"W2\", [5, 10], initializer = tf.contrib.layers.xavier_initializer(seed=1))\n    b2 = tf.get_variable(\"b2\", [5, 1], initializer = tf.zeros_initializer())\n    W3 = tf.get_variable(\"W3\", [3, 5], initializer = tf.contrib.layers.xavier_initializer(seed=1))\n    b3 = tf.get_variable(\"b3\", [3, 1], initializer = tf.zeros_initializer())\n    \n    parameters = {\"W1\": W1,\n                  \"b1\": b1,\n                  \"W2\": W2,\n                  \"b2\": b2,\n                  \"W3\": W3,\n                  \"b3\": b3}\n    \n    return parameters\n\ndef forward_propagation(X, parameters):\n    \"\"\"\n    Implements the forward propagation for the model: \n    LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SOFTMAX\n    \n    Arguments:\n    X -- input dataset placeholder, of shape (input size, number of examples)\n    parameters -- python dictionary containing your parameters \n    \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\"\n    the shapes are given in initialize_parameters\n\n    Returns:\n    Z3 -- the output of the last LINEAR unit\n    \"\"\"\n    \n    # Retrieve the parameters from the dictionary \"parameters\" \n    W1 = parameters['W1']\n    b1 = parameters['b1']\n    W2 = parameters['W2']\n    b2 = parameters['b2']\n    W3 = parameters['W3']\n    b3 = parameters['b3']\n    \n    ### START CODE HERE ### (approx. 5 lines)              # Numpy Equivalents:\n    Z1 = tf.add(tf.matmul(W1, X), b1)                      # Z1 = np.dot(W1, X) + b1\n    A1 = tf.nn.relu(Z1)                                    # A1 = relu(Z1)\n    Z2 = tf.add(tf.matmul(W2, A1), b2)                     # Z2 = np.dot(W2, a1) + b2\n    A2 = tf.nn.relu(Z2)                                    # A2 = relu(Z2)\n    Z3 = tf.add(tf.matmul(W3, A2), b3)                     # Z3 = np.dot(W3,Z2) + b3\n    ### END CODE HERE ###\n    \n    '''\n     It is important to note that the forward propagation stops at z3. \n     The reason is that in tensorflow the last linear layer output is \n     given as input to the function computing the loss. \n     Therefore, you don't need a3!\n    '''\n    return Z3\n\ndef compute_cost(Z3, Y):\n    \"\"\"\n    Computes the cost\n    \n    Arguments:\n    Z3 -- output of forward propagation (output of the last LINEAR unit), of shape (3, number of examples)\n    Y -- \"true\" labels vector placeholder, same shape as Z3\n    \n    Returns:\n    cost - Tensor of the cost function\n    \"\"\"\n    \n    # to fit the tensorflow requirement for tf.nn.softmax_cross_entropy_with_logits(...,...)\n    logits = tf.transpose(Z3)\n    labels = tf.transpose(Y)\n   \n    #cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n    # The newer recommended function in Tensor flow\n    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=labels))\n    return cost","cell_type":"code","metadata":{"collapsed":true,"_uuid":"22108a41363292395534f2d0f4336c7a8c67bf67","_cell_guid":"21fe3d03-6765-4cff-a41d-af29474e63f2"}},{"source":"# Testing our functions","cell_type":"markdown","metadata":{"_uuid":"d6da0a321f79736905d80cbe9bb5a5c9e76ce1f6","_cell_guid":"b32da590-6b5c-4ee1-9064-28c8e2b7ba5f"}},{"outputs":[],"execution_count":null,"source":"num__input_features = 12\nnum_output_features = 3 # (num classes)\n\ntf.reset_default_graph()\n\nwith tf.Session() as sess:\n    X, Y = create_placeholders(num__input_features, num_output_features)\n    parameters = initialize_parameters(num__input_features)\n    Z3 = forward_propagation(X, parameters)\n    cost = compute_cost(Z3, Y)\n    print(\"cost = \" + str(cost))","cell_type":"code","metadata":{"collapsed":true,"_uuid":"6052e1bb4cd0aa0f2f2240dc0f31db89f2dbe3f6","_cell_guid":"ff257e0b-977e-43bf-a54d-1ddd51683d6d"}},{"source":"# Building the model\n\nIn Andrew Ng's model, there's a function to compute random minibatches. Let's create that function:","cell_type":"markdown","metadata":{"_uuid":"a5f2d9bf3b17a14a295ccee9b4e9534fdc5b11fa","_cell_guid":"a8958fb3-0400-4cf5-b633-edb295fe9b2b"}},{"outputs":[],"execution_count":null,"source":"def random_mini_batches(X, Y, mini_batch_size = 64, seed = 0):\n    \"\"\"\n    Creates a list of random minibatches from (X, Y)\n    \n    Arguments:\n    X -- input data, of shape (input size, number of examples)\n    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n    mini_batch_size - size of the mini-batches, integer\n    seed -- this is only for the purpose of grading, so that you're \"random minibatches are the same as ours.\n    \n    Returns:\n    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n    \"\"\"\n    \n    m = X.shape[1]                  # number of training examples\n    mini_batches = []\n    np.random.seed(seed)\n    \n    # Step 1: Shuffle (X, Y)\n    permutation = list(np.random.permutation(m))\n    shuffled_X = X[:, permutation]\n    shuffled_Y = Y[:, permutation].reshape((Y.shape[0],m))\n\n    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n    num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n    for k in range(0, num_complete_minibatches):\n        mini_batch_X = shuffled_X[:, k * mini_batch_size : k * mini_batch_size + mini_batch_size]\n        mini_batch_Y = shuffled_Y[:, k * mini_batch_size : k * mini_batch_size + mini_batch_size]\n        mini_batch = (mini_batch_X, mini_batch_Y)\n        mini_batches.append(mini_batch)\n    \n    # Handling the end case (last mini-batch < mini_batch_size)\n    if m % mini_batch_size != 0:\n        mini_batch_X = shuffled_X[:, num_complete_minibatches * mini_batch_size : m]\n        mini_batch_Y = shuffled_Y[:, num_complete_minibatches * mini_batch_size : m]\n        mini_batch = (mini_batch_X, mini_batch_Y)\n        mini_batches.append(mini_batch)\n    \n    return mini_batches\n","cell_type":"code","metadata":{"collapsed":true,"_uuid":"b0d1361e605f9af29cafe03af6c8d05cc23543e1","_cell_guid":"0f5576f4-c5ff-42fe-94d9-d46600ac55c2"}},{"source":"Now we can use the following model:","cell_type":"markdown","metadata":{"_uuid":"22db4d9cf70e20195b821a65ce32f7c1e99ca2a0","_cell_guid":"0904c3cc-988f-470b-b564-56b4744a6de2"}},{"outputs":[],"execution_count":null,"source":"from tensorflow.python.framework import ops\n\ndef model(X_train, Y_train, X_test, Y_test, learning_rate = 0.0001,\n          num_epochs = 1500, minibatch_size = 32, print_cost = True):\n    \n    ops.reset_default_graph()                         # to be able to rerun the model without overwriting tf variables\n    tf.set_random_seed(1)                             # to keep consistent results\n    seed = 3                                          # to keep consistent results\n    (n_x, m) = X_train.shape                          # (n_x: input size, m : number of examples in the train set)\n    n_y = Y_train.shape[0]                            # n_y : output size\n    costs = []                                        # To keep track of the cost\n    \n    # Create Placeholders of shape (n_x, n_y)\n    X, Y = create_placeholders(n_x, n_y)\n\n    # Initialize parameters\n    parameters = initialize_parameters(12)\n    \n    # Forward propagation: Build the forward propagation in the tensorflow graph\n    Z3 = forward_propagation(X, parameters)\n    \n    # Cost function: Add cost function to tensorflow graph\n    cost = compute_cost(Z3, Y)\n    \n    # Backpropagation: Define the tensorflow optimizer. Use an AdamOptimizer.\n    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n    \n    # Initialize all the variables\n    init = tf.global_variables_initializer()\n\n    # Start the session to compute the tensorflow graph\n    with tf.Session() as sess:\n        \n        # Run the initialization\n        sess.run(init)\n        \n        # Do the training loop\n        for epoch in range(num_epochs):\n\n            epoch_cost = 0.                       # Defines a cost related to an epoch\n            num_minibatches = int(m / minibatch_size) # number of minibatches of size minibatch_size in the train set\n            seed = seed + 1\n            minibatches = random_mini_batches(X_train, Y_train, minibatch_size, seed)\n\n            for minibatch in minibatches:\n\n                # Select a minibatch\n                (minibatch_X, minibatch_Y) = minibatch\n                \n                # IMPORTANT: The line that runs the graph on a minibatch.\n                # Run the session to execute the \"optimizer\" and the \"cost\", the feedict should contain a minibatch for (X,Y).\n                ### START CODE HERE ### (1 line)\n                _ , minibatch_cost = sess.run([optimizer, cost], feed_dict={X: minibatch_X, Y: minibatch_Y})\n                ### END CODE HERE ###\n                \n                epoch_cost += minibatch_cost / num_minibatches\n\n            # Print the cost every epoch\n            if print_cost == True and epoch % 100 == 0:\n                print (\"Cost after epoch %i: %f\" % (epoch, epoch_cost))\n            if print_cost == True and epoch % 5 == 0:\n                costs.append(epoch_cost)\n                \n        # plot the cost\n        plt.plot(np.squeeze(costs))\n        plt.ylabel('cost')\n        plt.xlabel('iterations (per tens)')\n        plt.title(\"Learning rate =\" + str(learning_rate))\n        plt.show()\n\n        # lets save the parameters in a variable\n        parameters = sess.run(parameters)\n        print(\"Parameters have been trained!\")\n\n        # Calculate the correct predictions\n        correct_prediction = tf.equal(tf.argmax(Z3), tf.argmax(Y))\n\n        # Calculate accuracy on the test set\n        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n\n        print(\"Train Accuracy:\", accuracy.eval({X: X_train, Y: Y_train}))\n        print(\"Test Accuracy:\", accuracy.eval({X: X_test, Y: Y_test}))\n        \n        return parameters","cell_type":"code","metadata":{"collapsed":true,"_uuid":"c0f77cdf3fbc0adf6fde9cc5cb45bda128e4c109","_cell_guid":"23617d79-6d7c-4b4b-b9f4-312905b5ad0b"}},{"source":"# Test it","cell_type":"markdown","metadata":{"_uuid":"9e45a56d129dc2d6447d025c310d09bbedb65419","_cell_guid":"4aa6a26a-b5d6-44c2-ba30-5c54f2811a83"}},{"outputs":[],"execution_count":null,"source":"#parameters = model(X_train, Y_train, X_test, Y_test) # This is before I played with dev set\nparameters = model(X_train, Y_train, X_test, Y_test, minibatch_size = 2) # This is the optimal I found\n","cell_type":"code","metadata":{"collapsed":true,"_uuid":"e32e21b9cea0a065928d8001304a0f2cd33b89ce","_cell_guid":"46a29330-ebbe-453c-92ae-c631235deae7"}},{"source":"# Optimizing\n\nNow, we can try to improve our model by defining our data differently. We can create a development set and a training set.","cell_type":"markdown","metadata":{"_uuid":"18d4eec28f2a8ced044575f5666363f84382df89","_cell_guid":"c86e8b8f-1720-4c89-a96a-d48db37e3319"}},{"outputs":[],"execution_count":null,"source":"'''\nUse train_test_split from sklearn.model_selection to split the data in dev and test sets\nWe will use a distribution of 60% for training, and 20% for both dev and test\n'''\n\nX = X_data.values \nY = Y_data.values\n\n# Recreate labels as hot-encoded using our function\nY = np.array([labelMaker(i[0]) for i in Y])\n\n# Split first 60% vs 40%\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.6, random_state=0)\n\n# Split then that 40% of the test set in 50/50\nX_dev, X_test, Y_dev, Y_test = train_test_split(X_test, Y_test, test_size=0.5, random_state=0)\n\n# We transpose all the matrixes to compile with expected shapes\nX_train = X_train.transpose()\nX_test = X_test.transpose()\nX_dev = X_dev.transpose()\nY_train = Y_train.transpose()\nY_test = Y_test.transpose()\nY_dev = Y_dev.transpose()\n\nprint(X_train.shape, X_test.shape, X_dev.shape)\nprint(Y_train.shape, Y_test.shape, Y_dev.shape)\n\n# We feed our data \nparameters = model(X_train, Y_train, X_dev, Y_dev, learning_rate = 0.0001, num_epochs = 1500, minibatch_size = 2)\n\n","cell_type":"code","metadata":{"collapsed":true,"_uuid":"262ce40f554a0fa7c6ba7b91dfce948d5dc4beba","_cell_guid":"268f29ed-d695-42dd-b220-807869587d76"}},{"source":"It seems that a smaller mini batch size helps in this case. Let see with the test set?","cell_type":"markdown","metadata":{"_uuid":"b8592aba6b36ef8643eecf9d25086fb857444dc1","_cell_guid":"a32eef74-4428-4fc5-93a8-e0b54a0e4f08"}},{"outputs":[],"execution_count":null,"source":"# We feed our data \nparameters = model(X_train, Y_train, X_test, Y_test, learning_rate = 0.0001, num_epochs = 1500, minibatch_size = 2)\n","cell_type":"code","metadata":{"collapsed":true,"_uuid":"1fb7558f35f4763e81c50eff449f9241e0588199","_cell_guid":"6571a3e8-9e84-4f06-80ef-a5aff454c8d9"}},{"source":"# Conclusions\n\n1) I have learned that is very important to check for correlations in the data (thanks abhikaggle8!) <br>\n2) I have, after using the dev set, determine that perhaps a smaller batch size than what I initially had helps <br>\n3) I have replaced the initial batch size I had (32) for the best I could find after a few attempts (2) <br>\n4) The model increased it accuracy in about 30%! <br>\n\nI wonder what could be a better approach? Suggestions please! \n","cell_type":"markdown","metadata":{"_uuid":"fc6b8aeb6a98b054c58e20281db1d85c373a48ad","_cell_guid":"23530ab3-2f03-4772-b080-31d34811ef81"}},{"outputs":[],"execution_count":null,"source":"","cell_type":"code","metadata":{"collapsed":true,"_uuid":"98bb82de588c9d90bac53a450774ac056d9b841a","_cell_guid":"e8d03b8a-f2ba-4d72-aee1-150acf23440d"}}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3","language":"python"},"language_info":{"file_extension":".py","pygments_lexer":"ipython3","name":"python","version":"3.6.4","nbconvert_exporter":"python","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3}}}}