{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ‚õîÔ∏è Traffic Signs Detection with YOLO v3, OpenCV and Keras","metadata":{}},{"cell_type":"markdown","source":"* Firstly, trained model in Darknet framework **detects Traffic Signs among 4 categories** by OpenCV dnn library.\n* Then, trained model in Keras **classifies** cut fragmets of Traffic Signs into one of **43 classes**.\n* Results are experimental, but can be used for further improvements.","metadata":{}},{"cell_type":"markdown","source":"# üì∞ Related Papers\n\n1. Sichkar V. N. **Real time detection and classification of traffic signs based on YOLO version 3 algorithm.** *Scientific and Technical Journal of Information Technologies, Mechanics and Optics*, 2020, vol. 20, no. 3, pp. 418‚Äì424. DOI: 10.17586/2226-1494-2020-20-3-418-424 (Full-text available on ResearchGate here: [Real time detection and classification of traffic signs based on YOLO version 3 algorithm](https://www.researchgate.net/publication/342638954_Real_time_detection_and_classification_of_traffic_signs_based_on_YOLO_version_3_algorithm)\n\n1. Sichkar V. N. **Effect of various dimension convolutional layer filters on traffic sign classification accuracy.** *Scientific and Technical Journal of Information Technologies, Mechanics and Optics*, 2019, vol. 19, no. 3, pp. 546‚Äì552. DOI: 10.17586/2226-1494-2019-19-3-546-552 (Full-text available on ResearchGate here: [Effect of various dimension convolutional layer filters on traffic sign classification accuracy](https://www.researchgate.net/publication/334074308_Effect_of_various_dimension_convolutional_layer_filters_on_traffic_sign_classification_accuracy)","metadata":{}},{"cell_type":"markdown","source":"# üéì Related course for detection tasks","metadata":{}},{"cell_type":"markdown","source":"**Training YOLO v3 for Objects Detection with Custom Data.** *Build your own detector by labelling, training and testing on image, video and in real time with camera.* **Join here:** [https://www.udemy.com/course/training-yolo-v3-for-objects-detection-with-custom-data/](https://www.udemy.com/course/training-yolo-v3-for-objects-detection-with-custom-data/?referralCode=A283956A57327E37DDAD)\n\nExample of detections on video are shown below. **Trained weights** can be found in the course mentioned above.\n\n![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F3400968%2Fbcdae0b57021d6ac3e86a9aa2e8c4b08%2Fts_detections.gif?generation=1581700736851192&alt=media)","metadata":{}},{"cell_type":"markdown","source":"# üéì Related course for classification tasks","metadata":{}},{"cell_type":"markdown","source":"**Design**, **Train** & **Test** deep CNN for Image Classification.\n\n**Join** the course & enjoy new opportunities to get deep learning skills:\n\n\n[https://www.udemy.com/course/convolutional-neural-networks-for-image-classification/](https://www.udemy.com/course/convolutional-neural-networks-for-image-classification/?referralCode=12EE0D74A405BF4DDE9B)\n\n\n![](https://github.com/sichkar-valentyn/1-million-images-for-Traffic-Signs-Classification-tasks/blob/main/images/slideshow_classification.gif?raw=true)","metadata":{}},{"cell_type":"markdown","source":"# üì• Importing needed libraries","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport cv2\nimport time\nfrom timeit import default_timer as timer\nimport matplotlib.pyplot as plt\nimport pickle\n\nfrom keras.models import load_model\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('../input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\nprint(os.listdir('../input'))\n\n# Any results we write to the current directory are saved as output\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-14T17:23:15.355799Z","iopub.execute_input":"2021-06-14T17:23:15.356158Z","iopub.status.idle":"2021-06-14T17:23:15.362924Z","shell.execute_reply.started":"2021-06-14T17:23:15.356107Z","shell.execute_reply":"2021-06-14T17:23:15.361698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# üìÇ Loading *labels*","metadata":{}},{"cell_type":"code","source":"# Reading csv file with labels' names\n# Loading two columns [0, 1] into Pandas dataFrame\nlabels = pd.read_csv('../input/traffic-signs-preprocessed/label_names.csv')\n\n# Check point\n# Showing first 5 rows from the dataFrame\nprint(labels.head())\nprint()\n\n# To locate by class number use one of the following\n# ***.iloc[0][1] - returns element on the 0 column and 1 row\nprint(labels.iloc[0][1])  # Speed limit (20km/h)\n# ***['SignName'][1] - returns element on the column with name 'SignName' and 1 row\nprint(labels['SignName'][1]) # Speed limit (30km/h)\n","metadata":{"execution":{"iopub.status.busy":"2021-06-14T17:23:18.28474Z","iopub.execute_input":"2021-06-14T17:23:18.285119Z","iopub.status.idle":"2021-06-14T17:23:18.330355Z","shell.execute_reply.started":"2021-06-14T17:23:18.285044Z","shell.execute_reply":"2021-06-14T17:23:18.329152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# üìÇ Loading trained Keras CNN model for Classification","metadata":{}},{"cell_type":"code","source":"# Loading trained CNN model to use it later when classifying from 4 groups into one of 43 classes\nmodel = load_model('../input/traffic-signs-classification-with-cnn/model-23x23.h5')\n\n# Loading mean image to use for preprocessing further\n# Opening file for reading in binary mode\nwith open('../input/traffic-signs-preprocessed/mean_image_rgb.pickle', 'rb') as f:\n    mean = pickle.load(f, encoding='latin1')  # dictionary type\n    \nprint(mean['mean_image_rgb'].shape)  # (3, 32, 32)\n","metadata":{"execution":{"iopub.status.busy":"2021-06-14T17:23:20.383493Z","iopub.execute_input":"2021-06-14T17:23:20.383846Z","iopub.status.idle":"2021-06-14T17:23:24.192913Z","shell.execute_reply.started":"2021-06-14T17:23:20.383794Z","shell.execute_reply":"2021-06-14T17:23:24.192144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# üí† Loading YOLO v3 network by OpenCV dnn library","metadata":{}},{"cell_type":"markdown","source":"## Loading *trained weights* and *cfg file* into the Network","metadata":{}},{"cell_type":"code","source":"# Trained weights can be found in the course mentioned above\npath_to_weights = '../input/trained-traffic-signs-detector-based-on-yolo-v3/yolov3_ts_train_5000.weights'\npath_to_cfg = '../input/traffic-signs-dataset-in-yolo-format/yolov3_ts_test.cfg'\n\n# Loading trained YOLO v3 weights and cfg configuration file by 'dnn' library from OpenCV\nnetwork = cv2.dnn.readNetFromDarknet(path_to_cfg, path_to_weights)\n\n# To use with GPU\nnetwork.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV)\nnetwork.setPreferableTarget(cv2.dnn.DNN_TARGET_OPENCL_FP16)\n","metadata":{"execution":{"iopub.status.busy":"2021-06-14T17:23:26.892469Z","iopub.execute_input":"2021-06-14T17:23:26.892821Z","iopub.status.idle":"2021-06-14T17:23:29.710403Z","shell.execute_reply.started":"2021-06-14T17:23:26.892765Z","shell.execute_reply":"2021-06-14T17:23:29.709605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Getting *output layers* where detections are made","metadata":{}},{"cell_type":"code","source":"# Getting names of all YOLO v3 layers\nlayers_all = network.getLayerNames()\n\n# Check point\n# print(layers_all)\n\n# Getting only detection YOLO v3 layers that are 82, 94 and 106\nlayers_names_output = [layers_all[i[0] - 1] for i in network.getUnconnectedOutLayers()]\n\n# Check point\nprint()\nprint(layers_names_output)  # ['yolo_82', 'yolo_94', 'yolo_106']\n","metadata":{"execution":{"iopub.status.busy":"2021-06-14T17:23:32.130663Z","iopub.execute_input":"2021-06-14T17:23:32.131018Z","iopub.status.idle":"2021-06-14T17:23:32.138189Z","shell.execute_reply.started":"2021-06-14T17:23:32.130969Z","shell.execute_reply":"2021-06-14T17:23:32.137177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Setting *probability*, *threshold* and *colour* for bounding boxes","metadata":{}},{"cell_type":"code","source":"# Minimum probability to eliminate weak detections\nprobability_minimum = 0.2\n\n# Setting threshold to filtering weak bounding boxes by non-maximum suppression\nthreshold = 0.2\n\n# Generating colours for bounding boxes\n# randint(low, high=None, size=None, dtype='l')\ncolours = np.random.randint(0, 255, size=(len(labels), 3), dtype='uint8')\n\n# Check point\nprint(type(colours))  # <class 'numpy.ndarray'>\nprint(colours.shape)  # (43, 3)\nprint(colours[0])  # [25  65 200]\n","metadata":{"execution":{"iopub.status.busy":"2021-06-14T17:23:34.586999Z","iopub.execute_input":"2021-06-14T17:23:34.587425Z","iopub.status.idle":"2021-06-14T17:23:34.596613Z","shell.execute_reply.started":"2021-06-14T17:23:34.587369Z","shell.execute_reply":"2021-06-14T17:23:34.595335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# üé¨ Reading input video","metadata":{}},{"cell_type":"code","source":"# Reading video from a file by VideoCapture object\n# video = cv2.VideoCapture('../input/traffic-signs-dataset-in-yolo-format/traffic-sign-to-test.mp4')\n# video = cv2.VideoCapture('../input/videofortesting/ts_video_1.mp4')\nvideo = cv2.VideoCapture('../input/videofortesting/ts_video_6.mp4')\n\n# Writer that will be used to write processed frames\nwriter = None\n\n# Variables for spatial dimensions of the frames\nh, w = None, None\n","metadata":{"execution":{"iopub.status.busy":"2021-06-14T17:23:36.981436Z","iopub.execute_input":"2021-06-14T17:23:36.981817Z","iopub.status.idle":"2021-06-14T17:23:37.06198Z","shell.execute_reply.started":"2021-06-14T17:23:36.981767Z","shell.execute_reply":"2021-06-14T17:23:37.061166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ‚ûø Processing frames in the loop","metadata":{}},{"cell_type":"code","source":"%matplotlib inline\n\n# Setting default size of plots\nplt.rcParams['figure.figsize'] = (3, 3)\n\n# Variable for counting total amount of frames\nf = 0\n\n# Variable for counting total processing time\nt = 0\n\n# Catching frames in the loop\nwhile True:\n    # Capturing frames one-by-one\n    ret, frame = video.read()\n\n    # If the frame was not retrieved\n    if not ret:\n        break\n       \n    # Getting spatial dimensions of the frame for the first time\n    if w is None or h is None:\n        # Slicing two elements from tuple\n        h, w = frame.shape[:2]\n\n    # Blob from current frame\n    blob = cv2.dnn.blobFromImage(frame, 1 / 255.0, (416, 416), swapRB=True, crop=False)\n\n    # Forward pass with blob through output layers\n    network.setInput(blob)\n    start = time.time()\n    output_from_network = network.forward(layers_names_output)\n    end = time.time()\n\n    # Increasing counters\n    f += 1\n    t += end - start\n\n    # Spent time for current frame\n    print('Frame number {0} took {1:.5f} seconds'.format(f, end - start))\n\n    # Lists for detected bounding boxes, confidences and class's number\n    bounding_boxes = []\n    confidences = []\n    class_numbers = []\n\n    # Going through all output layers after feed forward pass\n    for result in output_from_network:\n        # Going through all detections from current output layer\n        for detected_objects in result:\n            # Getting 80 classes' probabilities for current detected object\n            scores = detected_objects[5:]\n            # Getting index of the class with the maximum value of probability\n            class_current = np.argmax(scores)\n            # Getting value of probability for defined class\n            confidence_current = scores[class_current]\n\n            # Eliminating weak predictions by minimum probability\n            if confidence_current > probability_minimum:\n                # Scaling bounding box coordinates to the initial frame size\n                box_current = detected_objects[0:4] * np.array([w, h, w, h])\n\n                # Getting top left corner coordinates\n                x_center, y_center, box_width, box_height = box_current\n                x_min = int(x_center - (box_width / 2))\n                y_min = int(y_center - (box_height / 2))\n\n                # Adding results into prepared lists\n                bounding_boxes.append([x_min, y_min, int(box_width), int(box_height)])\n                confidences.append(float(confidence_current))\n                class_numbers.append(class_current)\n                \n\n    # Implementing non-maximum suppression of given bounding boxes\n    results = cv2.dnn.NMSBoxes(bounding_boxes, confidences, probability_minimum, threshold)\n\n    # Checking if there is any detected object been left\n    if len(results) > 0:\n        # Going through indexes of results\n        for i in results.flatten():\n            # Bounding box coordinates, its width and height\n            x_min, y_min = bounding_boxes[i][0], bounding_boxes[i][1]\n            box_width, box_height = bounding_boxes[i][2], bounding_boxes[i][3]\n            \n            \n            # Cut fragment with Traffic Sign\n            c_ts = frame[y_min:y_min+int(box_height), x_min:x_min+int(box_width), :]\n            # print(c_ts.shape)\n            \n            if c_ts.shape[:1] == (0,) or c_ts.shape[1:2] == (0,):\n                pass\n            else:\n                # Getting preprocessed blob with Traffic Sign of needed shape\n                blob_ts = cv2.dnn.blobFromImage(c_ts, 1 / 255.0, size=(32, 32), swapRB=True, crop=False)\n                blob_ts[0] = blob_ts[0, :, :, :] - mean['mean_image_rgb']\n                blob_ts = blob_ts.transpose(0, 2, 3, 1)\n                # plt.imshow(blob_ts[0, :, :, :])\n                # plt.show()\n\n                # Feeding to the Keras CNN model to get predicted label among 43 classes\n                scores = model.predict(blob_ts)\n\n                # Scores is given for image with 43 numbers of predictions for each class\n                # Getting only one class with maximum value\n                prediction = np.argmax(scores)\n                # print(labels['SignName'][prediction])\n\n\n                # Colour for current bounding box\n                colour_box_current = colours[class_numbers[i]].tolist()\n\n                # Drawing bounding box on the original current frame\n                cv2.rectangle(frame, (x_min, y_min),\n                              (x_min + box_width, y_min + box_height),\n                              colour_box_current, 2)\n\n                # Preparing text with label and confidence for current bounding box\n                text_box_current = '{}: {:.4f}'.format(labels['SignName'][prediction],\n                                                       confidences[i])\n\n                # Putting text with label and confidence on the original image\n                cv2.putText(frame, text_box_current, (x_min, y_min - 5),\n                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, colour_box_current, 2)\n\n\n    # Initializing writer only once\n    if writer is None:\n        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n\n        # Writing current processed frame into the video file\n        writer = cv2.VideoWriter('result.mp4', fourcc, 25,\n                                 (frame.shape[1], frame.shape[0]), True)\n\n    # Write processed current frame to the file\n    writer.write(frame)\n\n\n# Releasing video reader and writer\nvideo.release()\nwriter.release()\n","metadata":{"execution":{"iopub.status.busy":"2021-06-14T17:23:40.947537Z","iopub.execute_input":"2021-06-14T17:23:40.947891Z","iopub.status.idle":"2021-06-14T17:27:21.400568Z","shell.execute_reply.started":"2021-06-14T17:23:40.94784Z","shell.execute_reply":"2021-06-14T17:27:21.399678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## üèÅ FPS results","metadata":{}},{"cell_type":"code","source":"print('Total number of frames', f)\nprint('Total amount of time {:.5f} seconds'.format(t))\nprint('FPS:', round((f / t), 1))\n","metadata":{"execution":{"iopub.status.busy":"2021-06-14T17:27:31.089978Z","iopub.execute_input":"2021-06-14T17:27:31.090385Z","iopub.status.idle":"2021-06-14T17:27:31.096001Z","shell.execute_reply.started":"2021-06-14T17:27:31.09033Z","shell.execute_reply":"2021-06-14T17:27:31.094945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Saving locally without committing\nfrom IPython.display import FileLink\n\nFileLink('result.mp4')\n","metadata":{"execution":{"iopub.status.busy":"2021-06-14T17:27:38.740782Z","iopub.execute_input":"2021-06-14T17:27:38.741163Z","iopub.status.idle":"2021-06-14T17:27:38.74889Z","shell.execute_reply.started":"2021-06-14T17:27:38.741109Z","shell.execute_reply":"2021-06-14T17:27:38.74791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# üéà Reading input images","metadata":{}},{"cell_type":"code","source":"# Reading image with OpenCV library\n# In this way image is opened already as numpy array\n# WARNING! OpenCV by default reads images in BGR format\n# image_BGR = cv2.imread('../input/videofortesting/traffic_sign.jpg')\n# image_BGR = cv2.imread('../input/videofortesting/ts_video_1_1.png')\n# image_BGR = cv2.imread('../input/videofortesting/ts_video_1_2.png')\n# image_BGR = cv2.imread('../input/videofortesting/ts_video_1_3.png')\n# image_BGR = cv2.imread('../input/videofortesting/ts_video_1_4.png')\n# image_BGR = cv2.imread('../input/videofortesting/ts_video_6_1.png')\n# image_BGR = cv2.imread('../input/videofortesting/ts_video_6_2.png')\n# image_BGR = cv2.imread('../input/videofortesting/ts_video_6_3.png')\nimage_BGR = cv2.imread('../input/videofortesting/ts_final_1.png')\n\n# Check point\n# Showing image shape\nprint('Image shape:', image_BGR.shape)  # tuple of (731, 1092, 3)\n\n# Getting spatial dimension of input image\nh, w = image_BGR.shape[:2]  # Slicing from tuple only first two elements\n\n# Check point\n# Showing height an width of image\nprint('Image height={0} and width={1}'.format(h, w))  # 731 1092\n","metadata":{"execution":{"iopub.status.busy":"2021-06-14T17:28:00.020111Z","iopub.execute_input":"2021-06-14T17:28:00.020499Z","iopub.status.idle":"2021-06-14T17:28:00.058144Z","shell.execute_reply.started":"2021-06-14T17:28:00.020446Z","shell.execute_reply":"2021-06-14T17:28:00.057331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# üßø Processing single image","metadata":{}},{"cell_type":"code","source":"# Variable for counting total processing time\nt = 0\n\n# Blob from current frame\nblob = cv2.dnn.blobFromImage(image_BGR, 1 / 255.0, (416, 416), swapRB=True, crop=False)\n\n# Forward pass with blob through output layers\nnetwork.setInput(blob)\nstart = time.time()\noutput_from_network = network.forward(layers_names_output)\nend = time.time()\n\n# Time\nt += end - start\nprint('Total amount of time {:.5f} seconds'.format(t))\n\n# Lists for detected bounding boxes, confidences and class's number\nbounding_boxes = []\nconfidences = []\nclass_numbers = []\n\n# Going through all output layers after feed forward pass\nfor result in output_from_network:\n    # Going through all detections from current output layer\n    for detected_objects in result:\n        # Getting 80 classes' probabilities for current detected object\n        scores = detected_objects[5:]\n        # Getting index of the class with the maximum value of probability\n        class_current = np.argmax(scores)\n        # Getting value of probability for defined class\n        confidence_current = scores[class_current]\n\n        # Eliminating weak predictions by minimum probability\n        if confidence_current > probability_minimum:\n            # Scaling bounding box coordinates to the initial frame size\n            box_current = detected_objects[0:4] * np.array([w, h, w, h])\n\n            # Getting top left corner coordinates\n            x_center, y_center, box_width, box_height = box_current\n            x_min = int(x_center - (box_width / 2))\n            y_min = int(y_center - (box_height / 2))\n\n            # Adding results into prepared lists\n            bounding_boxes.append([x_min, y_min, int(box_width), int(box_height)])\n            confidences.append(float(confidence_current))\n            class_numbers.append(class_current)\n                \n\n# Implementing non-maximum suppression of given bounding boxes\nresults = cv2.dnn.NMSBoxes(bounding_boxes, confidences, probability_minimum, threshold)\n\n# Checking if there is any detected object been left\nif len(results) > 0:\n    # Going through indexes of results\n    for i in results.flatten():\n        # Bounding box coordinates, its width and height\n        x_min, y_min = bounding_boxes[i][0], bounding_boxes[i][1]\n        box_width, box_height = bounding_boxes[i][2], bounding_boxes[i][3]\n            \n            \n        # Cut fragment with Traffic Sign\n        c_ts = image_BGR[y_min:y_min+int(box_height), x_min:x_min+int(box_width), :]\n        # print(c_ts.shape)\n            \n        if c_ts.shape[:1] == (0,) or c_ts.shape[1:2] == (0,):\n            pass\n        else:\n            # Getting preprocessed blob with Traffic Sign of needed shape\n            blob_ts = cv2.dnn.blobFromImage(c_ts, 1 / 255.0, size=(32, 32), swapRB=True, crop=False)\n            blob_ts[0] = blob_ts[0, :, :, :] - mean['mean_image_rgb']\n            blob_ts = blob_ts.transpose(0, 2, 3, 1)\n            # plt.imshow(blob_ts[0, :, :, :])\n            # plt.show()\n\n            # Feeding to the Keras CNN model to get predicted label among 43 classes\n            scores = model.predict(blob_ts)\n\n            # Scores is given for image with 43 numbers of predictions for each class\n            # Getting only one class with maximum value\n            prediction = np.argmax(scores)\n            print(labels['SignName'][prediction])\n\n\n            # Colour for current bounding box\n            colour_box_current = colours[class_numbers[i]].tolist()\n            \n            # Green BGR\n            colour_box_current = [0, 255, 61]\n            \n            # Yellow BGR\n#             colour_box_current = [0, 255, 255]\n\n            # Drawing bounding box on the original current frame\n            cv2.rectangle(image_BGR, (x_min, y_min),\n                              (x_min + box_width, y_min + box_height),\n                              colour_box_current, 6)\n\n#             # Preparing text with label and confidence for current bounding box\n#             text_box_current = '{}: {:.4f}'.format(labels['SignName'][prediction],\n#                                                    confidences[i])\n            \n#             # Putting text with label and confidence on the original image\n#             cv2.putText(image_BGR, text_box_current, (x_min, y_min - 15),\n#                             cv2.FONT_HERSHEY_SIMPLEX, 0.9, colour_box_current, 2)\n            \n            if prediction == 5:\n                # Preparing text with label and confidence for current bounding box\n                text_box_current = '{}: {:.4f}'.format('Speed limit 60', confidences[i])\n\n                # Putting text with label and confidence on the original image\n                cv2.putText(image_BGR, text_box_current, (x_min - 110, y_min - 10),\n                                cv2.FONT_HERSHEY_SIMPLEX, 0.9, colour_box_current, 2)\n                \n            elif prediction == 9:            \n                # Preparing text with label and confidence for current bounding box\n                text_box_current = '{}: {:.4f}'.format('No overtaking', confidences[i])\n\n                # Putting text with label and confidence on the original image\n                cv2.putText(image_BGR, text_box_current, (x_min - 110, y_min + box_height + 30),\n                                cv2.FONT_HERSHEY_SIMPLEX, 0.9, colour_box_current, 2)\n\n#             elif prediction == 17:            \n#                 # Preparing text with label and confidence for current bounding box\n#                 text_box_current = '{}: {:.4f}'.format('No entry', confidences[i])\n\n#                 # Putting text with label and confidence on the original image\n#                 cv2.putText(image_BGR, text_box_current, (x_min - 170, y_min - 15),\n#                                 cv2.FONT_HERSHEY_SIMPLEX, 0.9, colour_box_current, 2)\n                \n                \n# Saving image\ncv2.imwrite('result.png', image_BGR)\n","metadata":{"execution":{"iopub.status.busy":"2021-06-14T17:28:11.25118Z","iopub.execute_input":"2021-06-14T17:28:11.251577Z","iopub.status.idle":"2021-06-14T17:28:11.701424Z","shell.execute_reply.started":"2021-06-14T17:28:11.251528Z","shell.execute_reply":"2021-06-14T17:28:11.700687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ü¶û Showing processed image","metadata":{}},{"cell_type":"code","source":"%matplotlib inline\n\nplt.rcParams['figure.figsize'] = (35.0, 35.0) # Setting default size of plots\n\nimage_BGR = cv2.imread('/kaggle/working/result.png')\n\n# Showing image shape\nprint('Image shape:', image_BGR.shape)  # tuple of (800, 1360, 3)\n\n# Getting spatial dimension of input image\nh, w = image_BGR.shape[:2]  # Slicing from tuple only first two elements\n\n# Showing height an width of image\nprint('Image height={0} and width={1}'.format(h, w))  # 800 1360\n\nplt.imshow(cv2.cvtColor(image_BGR, cv2.COLOR_BGR2RGB))\nplt.axis('off')\n# plt.title('Keras Visualization', fontsize=18)\n\n# Showing the plot\nplt.show()\n\nplt.close()\n","metadata":{"execution":{"iopub.status.busy":"2021-06-14T17:29:43.810122Z","iopub.execute_input":"2021-06-14T17:29:43.810504Z","iopub.status.idle":"2021-06-14T17:29:44.676631Z","shell.execute_reply.started":"2021-06-14T17:29:43.810452Z","shell.execute_reply":"2021-06-14T17:29:44.675579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Saving locally without committing\nfrom IPython.display import FileLink\n\nFileLink('result.png')\n","metadata":{"execution":{"iopub.status.busy":"2021-06-14T17:30:06.810643Z","iopub.execute_input":"2021-06-14T17:30:06.810993Z","iopub.status.idle":"2021-06-14T17:30:06.818201Z","shell.execute_reply.started":"2021-06-14T17:30:06.810943Z","shell.execute_reply":"2021-06-14T17:30:06.817207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# üîé Example of the result","metadata":{}},{"cell_type":"markdown","source":"![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F3400968%2Fa57f58b38e3caab6fbf72169895f5074%2Fresult.gif?generation=1585955236302060&alt=media)","metadata":{}}]}