{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport operator\n\n# Correlation matrixes\nimport scipy.stats as ss\nimport itertools\nimport math\nfrom collections import Counter\n\n# SelectKBest\nfrom functools import partial\nfrom sklearn.feature_selection import SelectKBest, chi2\n\n# Modeling\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, classification_report, f1_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import svm, tree\nimport graphviz \n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get data from csv\nraw_data = pd.read_csv('../input/wmo-hurricane-survival-dataset/World_MO_Hurricane_Survival.csv')\n\n# Check the df\nraw_data.head()","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Preparation","metadata":{}},{"cell_type":"markdown","source":"**For the start need to clean the data from NaNs**","metadata":{}},{"cell_type":"code","source":"# Delete rows with nan because we have not much nans and we can juat clean them from the data\nprint(\"NaNs in the features:\")\nprint(pd.isnull(raw_data).sum())\n\nprint(\"\\nNumber of rows in data the before cleaning :\")\nprint(len(raw_data.index))\n\n# As we can see, the total number of rows is much bigger than number of missing values \n# and cleaning the NaN contain rows will not dramatically influence on the result\n# So I decided to delete the rows containing NaNs\n\nraw_data = raw_data.dropna()\nprint(\"\\nNumber of rows in data the after cleaning :\")\nprint(len(raw_data.index))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Next,check the types of the features**","metadata":{}},{"cell_type":"code","source":"# Get info about features type\nraw_data.dtypes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**As we can see, we got almost all the data types as \"object\". So we convert the types \"object\" to proper types.**","metadata":{}},{"cell_type":"code","source":"############ Converting the objects to proper data types ############\n\n# Does not need the ID for prediction\nraw_data = raw_data.drop(columns=['ID'])\n\n# DOB to Age: I don't want the date of birth of a person, but I want to know the age of the person\nraw_data['DOB'] = pd.to_datetime('today').year - pd.to_datetime(raw_data['DOB'], format='%m/%d/%Y').dt.year\n\n#Create column for age category\nbins= [20,30,40,50,60,70]\nlabels = ['21-30','31-40','41-50','51-60','61-70']\nraw_data['AgeGroup'] = pd.cut(raw_data['DOB'], bins=bins, labels=labels, right=False)\n\n# I used this guide: https://pbpython.com/categorical-encoding.html\n# Object columns to category: I want to categorize the string data to understand what I have in my hands\nfor col in ['M_STATUS', 'SALARY', 'EDU_DATA', 'EMP_DATA', 'REL_ORIEN', 'FAV_TV', 'PREF_CAR', 'GENDER',\n           'FAV_CUIS', 'FAV_MUSIC', 'ENDU_LEVEL', 'FAV_SPORT', 'FAV_COLR', 'NEWS_SOURCE', 'DIST_FRM_COAST',\n           'MNTLY_TRAVEL', 'GEN_MOVIES', 'FAV_SUBJ', 'ALCOHOL', 'FAV_SUPERHERO']:\n    raw_data[col] = raw_data[col].astype('category')\n\n# I don't understand that feature from its values and we don't have description of this feature in the origin dataset\n# On the other hand we have 'DIST_FRM_COAST' feature giving us the data about distance from the coast. So I decided to remove that feature.\nraw_data = raw_data.drop(columns=['Dist_Coast'])\n\n# Class label to boolean: survived = true, not survived = false\nmapping_bool_dict = {'x': True, 'y': False}\nraw_data['Class'] = raw_data['Class'].map(mapping_bool_dict)\n\n# Rename the features to more suitable names\nraw_data.rename(columns = {'DOB': 'Age', 'M_STATUS': 'Marital_Status', 'SALARY': 'Salary', 'EDU_DATA': 'Education',\n                          'EMP_DATA': 'Employment', 'REL_ORIEN': 'Religion', 'FAV_TV': 'Fav_TV_Show',\n                          'PREF_CAR': 'Fav_Car', 'GENDER': 'Gender', 'FAV_CUIS': 'Fav_Cuisine', 'FAV_MUSIC': 'Fav_Music',\n                          'ENDU_LEVEL': 'Endurance', 'FAV_SPORT': 'Fav_Sport', 'FAV_COLR': 'Fav_Color',\n                          'NEWS_SOURCE': 'Fav_News_Source', 'DIST_FRM_COAST': 'Distance_From_Coast', \n                          'MNTLY_TRAVEL': 'Monthly_Travel_Distance', 'GEN_MOVIES': 'Fav_Movies_Genre',\n                          'FAV_SUBJ': 'Fav_Subject', 'ALCOHOL': 'Fav_Alcohol', 'FAV_SUPERHERO': 'Fav_Superhero',\n                          'Class': 'Is_Survived'}, inplace = True)\n\n\nraw_data.dtypes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check if the class is balanced\nraw_data.groupby('Is_Survived').describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**As we can see, we have positive values are approximately same as negative values, so the data is balanced**","metadata":{}},{"cell_type":"code","source":"#### Label Encoding ####\nlabel_encoding_raw_data = raw_data\nfor col in label_encoding_raw_data:\n    if label_encoding_raw_data[col].dtype.name == 'category':\n        label_encoding_raw_data[col] = label_encoding_raw_data[col].cat.codes\nlabel_encoding_raw_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"###### Feature Statistics ######\nplt.figure(figsize=(25,10))\n\nplt.subplot(3, 3, 1)\nsns.countplot(x='AgeGroup', hue='Is_Survived', data=label_encoding_raw_data)\n\nplt.subplot(3, 3, 2)\nsns.countplot(x='Marital_Status', hue='Is_Survived', data=label_encoding_raw_data)\n\nplt.subplot(3, 3, 3)\nsns.countplot(x='Salary', hue='Is_Survived', data=label_encoding_raw_data)\n\nplt.subplot(3, 3, 4)\nsns.countplot(x='Education', hue='Is_Survived', data=label_encoding_raw_data)\n\nplt.subplot(3, 3, 5)\nsns.countplot(x='Employment', hue='Is_Survived', data=label_encoding_raw_data)\n\nplt.subplot(3, 3, 6)\nsns.countplot(x='Religion', hue='Is_Survived', data=label_encoding_raw_data)\n\nplt.subplot(3, 3, 7)\nsns.countplot(x='Fav_TV_Show', hue='Is_Survived', data=label_encoding_raw_data)\n\nplt.subplot(3, 3, 8)\nsns.countplot(x='Fav_Car', hue='Is_Survived', data=label_encoding_raw_data)\n\nplt.subplot(3, 3, 9)\nsns.countplot(x='Gender', hue='Is_Survived', data=label_encoding_raw_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"###### Feature Statistics ######\nplt.figure(figsize=(25,10))\n\nplt.subplot(3, 3, 1)\nsns.countplot(x='Fav_Cuisine', hue='Is_Survived', data=label_encoding_raw_data)\n\nplt.subplot(3, 3, 2)\nsns.countplot(x='Fav_Music', hue='Is_Survived', data=label_encoding_raw_data)\n\nplt.subplot(3, 3, 3)\nsns.countplot(x='Endurance', hue='Is_Survived', data=label_encoding_raw_data)\n\nplt.subplot(3, 3, 4)\nsns.countplot(x='Fav_Sport', hue='Is_Survived', data=label_encoding_raw_data)\n\nplt.subplot(3, 3, 5)\nsns.countplot(x='Fav_Color', hue='Is_Survived', data=label_encoding_raw_data)\n\nplt.subplot(3, 3, 6)\nsns.countplot(x='Fav_News_Source', hue='Is_Survived', data=label_encoding_raw_data)\n\nplt.subplot(3, 3, 7)\nsns.countplot(x='Distance_From_Coast', hue='Is_Survived', data=label_encoding_raw_data)\n\nplt.subplot(3, 3, 8)\nsns.countplot(x='Monthly_Travel_Distance', hue='Is_Survived', data=label_encoding_raw_data)\n\nplt.subplot(3, 3, 9)\nsns.countplot(x='Fav_Movies_Genre', hue='Is_Survived', data=label_encoding_raw_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"###### Feature Statistics ######\nplt.figure(figsize=(25,10))\n\nplt.subplot(1, 3, 1)\nsns.countplot(x='Fav_Subject', hue='Is_Survived', data=label_encoding_raw_data)\n\nplt.subplot(1, 3, 2)\nsns.countplot(x='Fav_Alcohol', hue='Is_Survived', data=label_encoding_raw_data)\n\nplt.subplot(1, 3, 3)\nsns.countplot(x='Fav_Superhero', hue='Is_Survived', data=label_encoding_raw_data)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Sources:\n# https://towardsdatascience.com/the-search-for-categorical-correlation-a1cf7f1888c9\n# https://stackoverflow.com/questions/51859894/how-to-plot-a-cramer-s-v-heatmap-for-categorical-features\n# https://github.com/shakedzy/dython/blob/master/dython/nominal.py\n\n############ Getting Correclation Matrixes: Cramér’s V and Theil’s U ############\n# It's very a big Correclation Matrix with dummies, \n# so I found how to get the Correclation Matrixes with category types\n\n# Save the all the scores in the dictionary \nall_scores = {}\n\n# Correclation Matrix with Cramér’s V\ndef cramers_v(x, y):\n    confusion_matrix = pd.crosstab(x,y)\n    chi2 = ss.chi2_contingency(confusion_matrix)[0]\n    n = confusion_matrix.sum().sum()\n    phi2 = chi2/n\n    r,k = confusion_matrix.shape\n    phi2corr = max(0, phi2-((k-1)*(r-1))/(n-1))\n    rcorr = r-((r-1)**2)/(n-1)\n    kcorr = k-((k-1)**2)/(n-1)\n    return np.sqrt(phi2corr/min((kcorr-1),(rcorr-1)))\n\n# Correclation Matrix with Theil’s U\ndef conditional_entropy(x,\n                        y,\n                        nan_strategy='replace',\n                        nan_replace_value=0.0,\n                        log_base: float = math.e):\n    if nan_strategy == 'replace':\n        x = np.array([v if v == v and v is not None else value for v in x])\n        y = np.array([v if v == v and v is not None else value for v in y])\n    elif nan_strategy == 'drop':\n        x, y = remove_incomplete_samples(x, y)\n    y_counter = Counter(y)\n    xy_counter = Counter(list(zip(x, y)))\n    total_occurrences = sum(y_counter.values())\n    entropy = 0.0\n    for xy in xy_counter.keys():\n        p_xy = xy_counter[xy] / total_occurrences\n        p_y = y_counter[xy[1]] / total_occurrences\n        entropy += p_xy * math.log(p_y / p_xy, log_base)\n    return entropy\n\ndef theils_u(x, y):\n    s_xy = conditional_entropy(x,y)\n    x_counter = Counter(x)\n    total_occurrences = sum(x_counter.values())\n    p_x = list(map(lambda n: n/total_occurrences, x_counter.values()))\n    s_x = ss.entropy(p_x)\n    if s_x == 0:\n        return 1\n    else:\n        return (s_x - s_xy) / s_x\n\ncols = list(raw_data) \ncorrM_cramers_v = np.zeros((len(cols),len(cols)))\ncorrM_theils_u = np.zeros((len(cols),len(cols)))\n\nfor col1 in cols: \n    idx1 = cols.index(col1) \n    corrM_cramers_v[idx1, idx1] = cramers_v(raw_data[col1], raw_data[col1])\n    corrM_theils_u[idx1, idx1] = theils_u(raw_data[col1], raw_data[col1])\n\nfor col1, col2 in itertools.combinations(cols, 2): \n    idx1, idx2 = cols.index(col1), cols.index(col2)\n    score_cramers_v = cramers_v(raw_data[col1], raw_data[col2])\n    score_theils_u = theils_u(raw_data[col1], raw_data[col2]) \n    corrM_cramers_v[idx1, idx2] = score_cramers_v\n    corrM_cramers_v[idx2, idx1] = corrM_cramers_v[idx1, idx2]\n    corrM_theils_u[idx1, idx2] = score_theils_u\n    corrM_theils_u[idx2, idx1] = corrM_theils_u[idx1, idx2]\n    \n    if col1 == 'Is_Survived':\n        all_scores[col2] = [score_cramers_v, score_theils_u]\n    elif col2 == 'Is_Survived':\n        all_scores[col1] = [score_cramers_v, score_theils_u]\n\n\ncorr_cramers_v = pd.DataFrame(corrM_cramers_v, index=cols, columns=cols)\ncorr_theils_u = pd.DataFrame(corrM_theils_u, index=cols, columns=cols)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print Cramer V correlation matrix\nfig, ax = plt.subplots(figsize=(25, 25)) \nax = sns.heatmap(corr_cramers_v, annot=True, ax=ax)\nax.set_title(\"Cramer V Correlation between Variables\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print Theil’s U correlation matrix\nfig, ax = plt.subplots(figsize=(25, 25)) \nax = sns.heatmap(corr_theils_u, annot=True, ax=ax)\nax.set_title(\"Theil’s U Correlation between Variables\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"############ Select best features with SelectKBest ############\n\ndef select_k_best_with_plot(X, y, k_plot):\n    selector = SelectKBest(chi2, k='all').fit(X, y)\n    \n    # normalization\n    scores = -np.log10(selector.pvalues_)\n    scores /= scores.max()\n\n    indices = np.argsort(scores)[::-1]\n\n    # To get your top 15 feature names\n    features = []\n    for i in range(k_plot):\n        features.append(X.columns[indices[i]])\n        all_scores[X.columns[indices[i]]].append(scores[indices[i]])\n\n    # Now plot\n    f, ax = plt.subplots(figsize=(25,10)) # set the size that you'd like (width, height)\n    plt.bar(features[:10], scores[indices[range(10)]], align='center')\n    ax.legend(fontsize = 14)\n    plt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# SelectKBest with Lable Encoding data\ny = label_encoding_raw_data['Is_Survived']\nX = label_encoding_raw_data.drop(columns=['Is_Survived'])\nselect_k_best_with_plot(X, y, 21)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##### Get score to the feature #####\n\n# Sum all the scores from Correlation Matrixes and SelectKBest\nfinal_scores = {}\nfor feature in all_scores:\n    final_scores[feature] = sum(all_scores[feature])\n\n# sort the features by scores\nfinal_scores = dict(sorted(final_scores.items(), key=operator.itemgetter(1),reverse=True))\n\n# The best 10 features\nlist(final_scores)[:10]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#### Relevant Features ####\n\n# features I will not use to modelling\nto_delete_features = list(final_scores)[10:]\n\n# Leave the features from the articles in final data\nto_delete_features.remove('Gender')\nto_delete_features.remove('Endurance')\n\n# get relevant features to df\ndata = raw_data.drop(columns=to_delete_features)\n\n# print final features\nfeatures = data.drop(columns=['Is_Survived'])\nlist(features.columns) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#### One Hot Encoding ####\n\ncategory_cols = list(data)\ncategory_cols.remove('Age')\ncategory_cols.remove('Is_Survived')\n\ndata = pd.get_dummies(data, columns=category_cols, drop_first=True)\ndata.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modeling and Evaluation","metadata":{}},{"cell_type":"code","source":"# Prepare train test\ny = data['Is_Survived']\nX = data.drop(columns=['Is_Survived'])\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##### Random Forest Gini #####\nrf = RandomForestClassifier(n_estimators = 1000,\n                                    max_depth=115,\n                                    min_samples_split=4,\n                                    min_samples_leaf=1,\n                                    criterion='gini'\n                                   )\nrf.fit(X_train, y_train)\nrf_pred = rf.predict(X_test)\n\nprint(\"Random Forest Gini\")\nprint(\"Accuracy: \", accuracy_score(y_test, rf_pred))\nprint(\"Recall: \", recall_score(y_test, rf_pred))\nprint(\"Precision: \", precision_score(y_test, rf_pred))\nprint(\"F-Score: \", f1_score(y_test, rf_pred))\n\n# Confusion Matrix\nplt.figure(figsize=[7, 6])\nplt.title('Random Forest Gini', fontsize = 15) \n\ncm = confusion_matrix(y_test,rf_pred)\nsns.heatmap(cm, annot=cm, fmt='g', xticklabels=['Predicted: No','Predicted: Yes'], yticklabels=['Actual: No','Actual: Yes'], cmap='copper')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##### Random Forest Entropy #####\nrf = RandomForestClassifier(n_estimators = 1000,\n                                    max_depth=115,\n                                    min_samples_split=4,\n                                    min_samples_leaf=1,\n                                    criterion='entropy'\n                                   )\nrf.fit(X_train, y_train)\nrf_pred = rf.predict(X_test)\n\nprint(\"Random Forest Entropy\")\nprint(\"Accuracy: \", accuracy_score(y_test, rf_pred))\nprint(\"Recall: \", recall_score(y_test, rf_pred))\nprint(\"Precision: \", precision_score(y_test, rf_pred))\nprint(\"F-Score: \", f1_score(y_test, rf_pred))\n\n# Confusion Matrix\nplt.figure(figsize=[7, 6])\nplt.title('Random Forest Entropy', fontsize = 15) \n\ncm = confusion_matrix(y_test,rf_pred)\nsns.heatmap(cm, annot=cm, fmt='g', xticklabels=['Predicted: No','Predicted: Yes'], yticklabels=['Actual: No','Actual: Yes'], cmap='copper')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##### Logistic Regression #####\n\nlr = LogisticRegression()\nlr.fit(X_train, y_train)\n\nlr.fit(X_train, y_train)\nlr_pred = lr.predict(X_test)\n\nprint(\"Logistic Regression\")\nprint(\"Accuracy: \", accuracy_score(y_test, lr_pred))\nprint(\"Recall: \", recall_score(y_test, lr_pred))\nprint(\"Precision: \", precision_score(y_test, lr_pred))\nprint(\"F-Score: \", f1_score(y_test, lr_pred))\n\n# Confusion Matrix\nplt.figure(figsize=[7, 6])\nplt.title('Logistic Regression', fontsize = 15)\n\ncm = confusion_matrix(y_test,lr_pred)\nsns.heatmap(cm, annot=cm, fmt='g', xticklabels=['Predicted: No','Predicted: Yes'], yticklabels=['Actual: No','Actual: Yes'], cmap='copper')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##### SVM #####\n\nsvm_model = svm.LinearSVC(C=0.01)\nsvm_model.fit(X_train, y_train)\n\nsvm_model_pred = svm_model.predict(X_test)\n\nprint(\"SVM\")\nprint(\"Accuracy: \", accuracy_score(y_test, svm_model_pred))\nprint(\"Recall: \", recall_score(y_test, svm_model_pred))\nprint(\"Precision: \", precision_score(y_test, svm_model_pred))\nprint(\"F-Score: \", f1_score(y_test, svm_model_pred))\n\n# Confusion Matrix\nplt.figure(figsize=[7, 6])\nplt.title('SVM', fontsize = 15)\n\ncm = confusion_matrix(y_test,svm_model_pred)\nsns.heatmap(cm, annot=cm, fmt='g', xticklabels=['Predicted: No','Predicted: Yes'], yticklabels=['Actual: No','Actual: Yes'], cmap='copper')\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##### Decision Tree #####\n\ndt = tree.DecisionTreeClassifier()\ndt.fit(X_train, y_train)\n\ndt_pred = dt.predict(X_test)\n\nprint(\"Decision Tree\")\nprint(\"Accuracy: \", accuracy_score(y_test, dt_pred))\nprint(\"Recall: \", recall_score(y_test, dt_pred))\nprint(\"Precision: \", precision_score(y_test, dt_pred))\nprint(\"F-Score: \", f1_score(y_test, dt_pred))\n\n# Confusion Matrix\nplt.figure(figsize=[7, 6])\nplt.title('Decision Tree', fontsize = 15)\n\ncm = confusion_matrix(y_test,dt_pred)\nsns.heatmap(cm, annot=cm, fmt='g', xticklabels=['Predicted: No','Predicted: Yes'], yticklabels=['Actual: No','Actual: Yes'], cmap='copper')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##### Decision Tree plot #####\ndot_data = tree.export_graphviz(dt, out_file=None, \n                      feature_names=list(X),   \n                      filled=True, rounded=True,  \n                      special_characters=True)  \ngraph = graphviz.Source(dot_data)  \ngraph.render(\"df_tree\",view=True)\nf = open(\"df_tree\",\"w+\")\nf.write(dot_data)\nf.close()\ngraph","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}