{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"In this notebook, we will attempt to predict the price of diamonds after analysing the effect of different physical variables that influence the price. We will use different regression techniques to model the price and evaluate their performance.\n"},{"metadata":{},"cell_type":"markdown","source":"### Importing the libraries and dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd \nimport numpy as np \nimport matplotlib.pyplot as plt\nimport seaborn as sns ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/diamonds/diamonds.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The dataset contains the prices and other attributes of almost 54,000 diamonds. The columns are as follows:\n\n - carat weight of the diamond (0.2--5.01)\n\n - cut quality of the cut (Fair, Good, Very Good, Premium, Ideal)\n\n - color diamond colour, from J (worst) to D (best)\n\n - clarity a measurement of how clear the diamond is (I1 (worst), SI2, SI1, VS2, VS1, VVS2, VVS1, IF (best))\n\n - x length in mm (0--10.74)\n\n - y width in mm (0--58.9)\n\n - z depth in mm (0--31.8)\n\n - depth total depth percentage = z / mean(x, y) = 2 * z / (x + y) (43--79)\n\n - table width of top of diamond relative to widest point (43--95)\n\n - price (dependent variable)\n\n We will use regression methods to model the price according to the different features."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop('Unnamed: 0',axis=1, inplace=True)\ndf = df.reindex(columns=[\"carat\", \"cut\", \"color\", \"clarity\", \"depth\", \"table\", \"x\", \"y\", \"z\", \"price\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### EDA"},{"metadata":{"trusted":true},"cell_type":"code","source":"len(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check for missing values \ndf.isnull().any()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us look at the distribution of the target variable"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(df['price'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that the target variable is right-skewed. We can take the log transform this variable so that it becomes normally distributed. A normally distributed target variable helps in better modelling the relationship of the target variable with the independent variables."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Skewness \nprint(\"The skewness of the Price in the dataset is {}\".format(df['price'].skew()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us now log-transform this variable and see if the distribution can get any more closer to normal "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Transforming the target variable\ntarget = np.log(df['price'])\nprint(\"Skewness: {}\".format(target.skew()))\nsns.distplot(target)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us now examine each of the independent variables"},{"metadata":{},"cell_type":"markdown","source":"#### Carat"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['carat'].hist()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that most of the diamond carats range from 0.2-1.2\n"},{"metadata":{},"cell_type":"markdown","source":"#### Cut"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['cut'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x='cut', data=df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can infer that majority of the cuts are of \"Ideal\" or \"Premium\" type, whereas there are very few \"Fair\" cuts in the data."},{"metadata":{},"cell_type":"markdown","source":"#### Color"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['color'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x='color', data=df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Clarity "},{"metadata":{"trusted":true},"cell_type":"code","source":"df['clarity'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(df['clarity'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here, we can infer that most of the diamonds have claritites of 'SI1' or 'VS2'\n"},{"metadata":{},"cell_type":"markdown","source":"#### Depth and Table"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(2, figsize=(10,10))\ndf['depth'].hist(ax=ax[0])\ndf['table'].hist(ax=ax[1])\nax[0].set_title(\"Distribution of depth\")\nax[1].set_title(\"Distribution of table\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### x,y,z"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(3, figsize=(10,10))\ndf['x'].hist(ax=ax[0])\ndf['y'].hist(ax=ax[1])\ndf['z'].hist(ax=ax[2])\nax[0].set_title(\"Distribution of x\")\nax[1].set_title(\"Distribution of y\")\nax[2].set_title(\"Distribution of z\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Price"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['price'].hist()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature Selection"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Using Pearson Correlation \nplt.figure(figsize=(12,10))\ncor = df.corr()\nsns.heatmap(cor, annot=True,cmap=plt.cm.Reds)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# correlation with output variable \ncor_target = abs(cor[\"price\"])\n\n# Selecting highly correlated features \nrelevent_features = cor_target[cor_target>0.5]\nrelevent_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop(['depth', 'table'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## REGRESSION"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Encoding the categorical data \n# Encoding the independent variables\ndummy_cut = pd.get_dummies(df['cut'],drop_first=True)   # drop_first to avoid the dummy variable trap\ndf = pd.concat([df, dummy_cut], axis=1)\ndf = df.drop('cut',axis=1)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dummy_color = pd.get_dummies(df['color'], drop_first=True)   \ndf = pd.concat([df, dummy_color], axis=1)\ndf = df.drop('color',axis=1)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dummy_clarity = pd.get_dummies(df['clarity'], drop_first=True)\ndf = pd.concat([df, dummy_clarity], axis=1)\ndf = df.drop('clarity', axis=1)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Splitting the data into training and test sets "},{"metadata":{"trusted":true},"cell_type":"code","source":"order = df.columns.to_list()\norder","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"order = ['carat',\n 'x',\n 'y',\n 'z',\n 'Good',\n 'Ideal',\n 'Premium',\n 'Very Good',\n 'E',\n 'F',\n 'G',\n 'H',\n 'I',\n 'J',\n 'IF',\n 'SI1',\n 'SI2',\n 'VS1',\n 'VS2',\n 'VVS1',\n 'VVS2',\n  'price']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df[order]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.iloc[:,:-1].values\ny = df.iloc[:,21].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Multiple Linear Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nfrom sklearn import model_selection\nfrom sklearn.metrics import r2_score, mean_squared_error\n\nregressor = LinearRegression()\nregressor.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# making predictions\ny_pred = regressor.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mlr_score = regressor.score(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Support Vector Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import preprocessing, svm\n\nX_svm = X.copy()\nX_svm = preprocessing.scale(X_svm)\n\nX_svm_train, X_svm_test, y_svm_train, y_svm_test = train_test_split(X_svm, y, test_size=0.2, random_state=0)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = svm.SVR(kernel='linear')\nclf.fit(X_svm_train, y_svm_train)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svr_score = clf.score(X_svm_test,y_svm_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Decision Tree Regression "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor\nregressor_dt = DecisionTreeRegressor(random_state=0)\nregressor_dt.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"regressor_dt.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dt_score = regressor_dt.score(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Random Forest Regression\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nregressor_rf = RandomForestRegressor(n_estimators=100, random_state=0)\nregressor_rf.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_score = regressor_rf.score(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Best Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Multiple Linear Regression accuracy:', mlr_score)\nprint('SVR score: ', svr_score)\nprint('Decision Tree Regression score: ', dt_score)\nprint('Random Forest Regression score: ', rf_score)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can conclude that the Random Forest Regression model performed the best with an accuracy of 97.4%"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}