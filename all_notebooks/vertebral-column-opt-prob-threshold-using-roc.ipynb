{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt # plotting\nimport numpy as np # linear algebra\nimport os # accessing directory structure\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"path = \"/kaggle/input/vertebralcolumndataset/\"\ndf1 = pd.read_csv(path+'column_2C.csv', delimiter=',')\nnRow, nCol = df1.shape\nprint(f'There are {nRow} rows and {nCol} columns')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Bivariate relationship between the features"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(df1, hue=\"class\", size=3, diag_kind=\"kde\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Modeling"},{"metadata":{"trusted":true},"cell_type":"code","source":"df1['class'] = df1['class'].map({'Normal': 0, 'Abnormal': 1})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn import model_selection\nfrom sklearn.metrics import accuracy_score\nX = df1[['pelvic_incidence','pelvic_tilt','lumbar_lordosis_angle','sacral_slope', 'pelvic_radius','degree_spondylolisthesis']]\nY = df1['class']\n# split data into train and test sets\nseed = 2020\ntest_size = 0.33\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=test_size, random_state=seed)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"XGBoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost as xgb\n\n# fit model no training data\nmodel = xgb.XGBClassifier()\nmodel.fit(X_train, y_train)\n# save model to file\nmodel.save_model(\"model.bst\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Running this example summarizes the performance of the model on the test set"},{"metadata":{"trusted":true},"cell_type":"code","source":"# make predictions for test data\ny_pred = model.predict(X_test)\npredictions = [round(value) for value in y_pred]\n# evaluate predictions\naccuracy = accuracy_score(y_test, predictions)\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# make predictions proba for test data\ny_pred_prob = model.predict_proba(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, auc\n\nfalse_pos_rate, true_pos_rate, proba = roc_curve(y_test, y_pred_prob[:, -1])\nplt.figure()\nplt.plot([0,1], [0,1], linestyle=\"--\") # plot random curve\nplt.plot(false_pos_rate, true_pos_rate, marker=\".\", label=f\"AUC = {roc_auc_score(y_test, predictions)}\")\nplt.title(\"ROC Curve\")\nplt.ylabel(\"True Positive Rate\")\nplt.xlabel(\"False Positive Rate\")\nplt.legend(loc=\"lower right\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_classes = 2\n\nfpr = dict()\ntpr = dict()\nthresholds = dict()\nroc_auc = dict()\n# Compute False Positive and True Positive Rates for each class\nfor i in range(num_classes):\n    fpr[i], tpr[i], thresholds[i] = roc_curve(y_test, y_pred_prob[:, -1], drop_intermediate=False)\n    roc_auc[i] = auc(fpr[i], tpr[i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"J_stats = [None]*num_classes\nopt_thresholds = [None]*num_classes\n\n# Compute Youden's J Statistic for each class\nfor i in range(num_classes):\n    J_stats[i] = tpr[i] - fpr[i]\n    opt_thresholds[i] = thresholds[i][np.argmax(J_stats[i])]\n    print('Optimum threshold for classe ',i,': '+str(opt_thresholds[i]))\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Obtain Optimal Probability Thresholds with ROC Curve \n\nIn this notebook, we will be using the Youden's J statistic, that is the distance between the ROC curve and the \"chance line\" - the ROC curve of a classifier that guesses randomly. The optimal threshold is that which maximises the J Statistic. We will be using the Youden's J statistic to obtain the optimal probability threshold and this method gives equal weights to both false positives and false negatives.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"optimal_proba_cutoff = sorted(list(zip(np.abs(true_pos_rate - false_pos_rate), y_pred_prob[:, -1])), key=lambda i: i[0], reverse=True)[0][1]\nroc_predictions = [1 if i >= optimal_proba_cutoff else 0 for i in y_pred_prob[:,-1]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"optimal_proba_cutoff","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Accuracy Score Before Thresholding: {}\".format(accuracy_score(y_test, predictions)))\nprint(\"Precision Score Before Thresholding: {}\".format(precision_score(y_test, predictions)))\nprint(\"Recall Score Before Thresholding: {}\".format(recall_score(y_test, predictions)))\nprint(\"F1 Score Before Thresholding: {}\".format(f1_score(y_test, predictions)))\nprint(\"ROC AUC Score: {}\".format(roc_auc_score(y_test, y_pred_prob[:, -1])))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Accuracy Score Before and After Thresholding: {}, {}\".format(accuracy_score(y_test, predictions), accuracy_score(y_test, roc_predictions)))\nprint(\"Precision Score Before and After Thresholding: {}, {}\".format(precision_score(y_test, predictions), precision_score(y_test, roc_predictions)))\nprint(\"Recall Score Before and After Thresholding: {}, {}\".format(recall_score(y_test, predictions), recall_score(y_test, roc_predictions)))\nprint(\"F1 Score Before and After Thresholding: {}, {}\".format(f1_score(y_test, predictions), f1_score(y_test, roc_predictions)) )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Confusion Matrix of Model (After Thresholding) \n\nWe can see that the new predictions have fewer false positives in the process. Recall score have improved."},{"metadata":{"trusted":true},"cell_type":"code","source":"y_actual = pd.Series(y_test, name='Actual')\ny_predict_tf = pd.Series(roc_predictions, name='Predicted')\ndf_confusion = pd.crosstab(y_actual, y_predict_tf, rownames=['Actual'], colnames=['Predicted'], margins=True)\nprint (df_confusion)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"End Notebook"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}