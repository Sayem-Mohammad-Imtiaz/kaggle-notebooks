{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-08T17:34:49.030021Z","iopub.execute_input":"2021-08-08T17:34:49.030385Z","iopub.status.idle":"2021-08-08T17:34:49.039766Z","shell.execute_reply.started":"2021-08-08T17:34:49.030351Z","shell.execute_reply":"2021-08-08T17:34:49.03849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#!pip install missingno\n#!pip install pylib","metadata":{"execution":{"iopub.status.busy":"2021-08-08T17:35:03.237435Z","iopub.execute_input":"2021-08-08T17:35:03.237785Z","iopub.status.idle":"2021-08-08T17:35:03.243363Z","shell.execute_reply.started":"2021-08-08T17:35:03.237748Z","shell.execute_reply":"2021-08-08T17:35:03.242443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This is to filter any warnings\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2021-08-08T17:35:04.49733Z","iopub.execute_input":"2021-08-08T17:35:04.497701Z","iopub.status.idle":"2021-08-08T17:35:04.502104Z","shell.execute_reply.started":"2021-08-08T17:35:04.49767Z","shell.execute_reply":"2021-08-08T17:35:04.50101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Import the dataset into a dataframe\nimport pandas as pd\n\ndata = pd.read_csv(\"/kaggle/input/pima-indians-diabetes-database/diabetes.csv\")\ndata.head()\n","metadata":{"execution":{"iopub.status.busy":"2021-08-08T17:35:06.331239Z","iopub.execute_input":"2021-08-08T17:35:06.331589Z","iopub.status.idle":"2021-08-08T17:35:06.35271Z","shell.execute_reply.started":"2021-08-08T17:35:06.331558Z","shell.execute_reply":"2021-08-08T17:35:06.35155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print the column types (All the datatypes should be numeric)\ndata.info()","metadata":{"execution":{"iopub.status.busy":"2021-08-08T17:35:07.869469Z","iopub.execute_input":"2021-08-08T17:35:07.869816Z","iopub.status.idle":"2021-08-08T17:35:07.886212Z","shell.execute_reply.started":"2021-08-08T17:35:07.869771Z","shell.execute_reply":"2021-08-08T17:35:07.884912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Find if there are any missing data (There are no missing values)\nimport missingno as msno\nax = msno.bar(data,color='tab:blue')\nax.set_title(\"Missing data report for Pima Indian Diabates dataset\")\nax.set_xlabel(\"Features\")\ntx = ax.set_ylabel(\"Importance\")\n","metadata":{"execution":{"iopub.status.busy":"2021-08-08T17:35:09.560411Z","iopub.execute_input":"2021-08-08T17:35:09.56075Z","iopub.status.idle":"2021-08-08T17:35:10.278356Z","shell.execute_reply.started":"2021-08-08T17:35:09.560718Z","shell.execute_reply":"2021-08-08T17:35:10.276672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Draw a correlation heatmap to understand the relationshipo between featrures and target variable outcome\nimport seaborn as sns\nax = sns.heatmap(data.corr(),cmap='Accent',annot=True)\n#Glucose , BMI and age has a slight positive corrlation with the output","metadata":{"execution":{"iopub.status.busy":"2021-08-08T17:35:14.393816Z","iopub.execute_input":"2021-08-08T17:35:14.394153Z","iopub.status.idle":"2021-08-08T17:35:15.107253Z","shell.execute_reply.started":"2021-08-08T17:35:14.394124Z","shell.execute_reply":"2021-08-08T17:35:15.106065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Check if the data classes are imbalanced\nax = sns.countplot(data[\"Outcome\"],color=\"tab:blue\")\ntx = ax.set_title(\"Distribution of data by classes\")\n# Clases are bit imbalanced and can be used without oversampling or undersampling","metadata":{"execution":{"iopub.status.busy":"2021-08-08T17:35:17.650901Z","iopub.execute_input":"2021-08-08T17:35:17.651241Z","iopub.status.idle":"2021-08-08T17:35:17.769107Z","shell.execute_reply.started":"2021-08-08T17:35:17.651212Z","shell.execute_reply":"2021-08-08T17:35:17.768068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split the dataset as X and y variable using train test split\nfrom sklearn.model_selection import train_test_split\nX = data.iloc[:,:-1]\ny = data.iloc[:,-1]\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=0)","metadata":{"execution":{"iopub.status.busy":"2021-08-08T17:35:21.359891Z","iopub.execute_input":"2021-08-08T17:35:21.360218Z","iopub.status.idle":"2021-08-08T17:35:21.370148Z","shell.execute_reply.started":"2021-08-08T17:35:21.36019Z","shell.execute_reply":"2021-08-08T17:35:21.368975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We try to find the feature importance by fitting a random forest classifier\nfrom sklearn.ensemble import RandomForestClassifier\nimport matplotlib.pyplot as plt\n\nfimp_model = RandomForestClassifier()\nfimp_model = fimp_model.fit(X_train,y_train)\nfeatures = X_train.columns\nimportance= fimp_model.feature_importances_\nfimpdata = pd.DataFrame()\nfimpdata[\"Features\"] = features\nfimpdata[\"Importance\"] = fimp_model.feature_importances_ *100\nfimpdata.sort_values(by='Importance',ascending=False,inplace=True)\nax = sns.barplot(y=\"Features\",x=\"Importance\",data=fimpdata,color=\"tab:blue\")\ntx = ax.set_title(\"Feature Importance of all columnns\")\n\n#It seems the classification is not confined to few columns, \n#so we are not reducing any columns part of this experiment","metadata":{"execution":{"iopub.status.busy":"2021-08-08T17:35:24.086959Z","iopub.execute_input":"2021-08-08T17:35:24.087294Z","iopub.status.idle":"2021-08-08T17:35:24.474154Z","shell.execute_reply.started":"2021-08-08T17:35:24.087267Z","shell.execute_reply":"2021-08-08T17:35:24.472939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We would train three models part of this experiment and compare the accuracy scores\n# 1. Logistic regression --> sklearn.linear_model\n# 2. Decison Tree --> sklearn.tree\n# 3. Random forest classifier --> sklearn.ensemble","metadata":{"execution":{"iopub.status.busy":"2021-08-08T17:35:27.691695Z","iopub.execute_input":"2021-08-08T17:35:27.692097Z","iopub.status.idle":"2021-08-08T17:35:27.697254Z","shell.execute_reply.started":"2021-08-08T17:35:27.692062Z","shell.execute_reply":"2021-08-08T17:35:27.696161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Training model using Logistic regression\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score,classification_report\nmodel1 = LogisticRegression(max_iter=1000)\nmodel1 = model1.fit(X_train,y_train)\ny_pred1 = model1.predict(X_test)\nacc_scores = dict()\nacc_scores[\"Logistic regression\"] = accuracy_score(y_test,y_pred1)\nprint(\"Classification report - Logistic regression \")\nprint(\"============================================\")\nprint(classification_report(y_test,y_pred1))","metadata":{"execution":{"iopub.status.busy":"2021-08-08T17:36:33.207994Z","iopub.execute_input":"2021-08-08T17:36:33.208339Z","iopub.status.idle":"2021-08-08T17:36:33.257556Z","shell.execute_reply.started":"2021-08-08T17:36:33.20831Z","shell.execute_reply":"2021-08-08T17:36:33.256428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Training model using Decision tree classifier \nfrom sklearn.tree import DecisionTreeClassifier\nmodel2 = DecisionTreeClassifier()\nmodel2 = model2.fit(X_train,y_train)\ny_pred2 = model2.predict(X_test)\nacc_scores[\"Decision tree\"] = accuracy_score(y_test,y_pred2)\nprint(\"Classification report - Decision tree\")\nprint(\"=====================================\")\nprint(classification_report(y_test,y_pred2))\n","metadata":{"execution":{"iopub.status.busy":"2021-08-08T17:36:57.223895Z","iopub.execute_input":"2021-08-08T17:36:57.224243Z","iopub.status.idle":"2021-08-08T17:36:57.244483Z","shell.execute_reply.started":"2021-08-08T17:36:57.224212Z","shell.execute_reply":"2021-08-08T17:36:57.243049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Training model using Random forest classifier \nfrom sklearn.ensemble import RandomForestClassifier\nmodel3 = RandomForestClassifier()\nmodel3 = model3.fit(X_train,y_train)\ny_pred3 = model3.predict(X_test)\nacc_scores[\"Random forest\"] = accuracy_score(y_test,y_pred3)\nprint(\"Classification report - Random Forest \")\nprint(\"======================================\")\nprint(classification_report(y_test,y_pred3))\n","metadata":{"execution":{"iopub.status.busy":"2021-08-08T17:37:18.331688Z","iopub.execute_input":"2021-08-08T17:37:18.332063Z","iopub.status.idle":"2021-08-08T17:37:18.534805Z","shell.execute_reply.started":"2021-08-08T17:37:18.332028Z","shell.execute_reply":"2021-08-08T17:37:18.533887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Plot to compare accuracies\naccscore_df = pd.DataFrame()\naccscore_df[\"Model\"] = acc_scores.keys()\naccscore_df[\"Accuracy\"] = acc_scores.values()\nax = sns.barplot(y=\"Model\",x=\"Accuracy\",data=accscore_df)\ntx = ax.set_title(\"Comparion of accuracy Logistioc regression vs Decision Tree vs Random Forest\")","metadata":{"execution":{"iopub.status.busy":"2021-08-08T17:37:22.565905Z","iopub.execute_input":"2021-08-08T17:37:22.566235Z","iopub.status.idle":"2021-08-08T17:37:22.729399Z","shell.execute_reply.started":"2021-08-08T17:37:22.566208Z","shell.execute_reply":"2021-08-08T17:37:22.728432Z"},"trusted":true},"execution_count":null,"outputs":[]}]}