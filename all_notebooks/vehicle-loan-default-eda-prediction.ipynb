{"cells":[{"metadata":{},"cell_type":"markdown","source":"This notebook contains Exploratory Data Analysis followed by Credit Score Model to predict the loans which are going to default."},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport pandas_datareader as pdr\nimport datetime as dt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"../input/lt-vehicle-loan-default-prediction/train.csv\")\ndf=data\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, we will convert object datatype into an appropriate data type."},{"metadata":{"trusted":true},"cell_type":"code","source":"#converting object datatype into appropriate datatype\n\n\ndf['Employment.Type'] = data['Employment.Type'].astype('category')\ndf['PERFORM_CNS.SCORE.DESCRIPTION'] = data['PERFORM_CNS.SCORE.DESCRIPTION'].astype('category')\n\ndef dateconv(x,format):\n    year = pd.datetime.today().year\n    dob = pd.to_datetime(x,format = format)\n    dob.loc[dob.dt.year.gt(year)] -= pd.DateOffset(years=100)\n    return dob\n\ndf['Date.of.Birth'] = dateconv(data['Date.of.Birth'], '%d-%m-%y')\ndf['DisbursalDate'] = dateconv(data['DisbursalDate'], '%d-%m-%y')\n\ndf['CREDIT.HISTORY.LENGTH'] = data['CREDIT.HISTORY.LENGTH'].str.split(\"yrs\",expand=True)[0].astype(np.int) * 12 + data['CREDIT.HISTORY.LENGTH'].str.split(\"yrs\",expand=True)[1].str.split(\"mon\",expand=True)[0].astype(np.int)\ndf['AVERAGE.ACCT.AGE'] = data['AVERAGE.ACCT.AGE'].str.split(\"yrs\",expand=True)[0].astype(np.int) * 12 + data['AVERAGE.ACCT.AGE'].str.split(\"yrs\",expand=True)[1].str.split(\"mon\",expand=True)[0].astype(np.int)\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, we will remove id columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"#dropping unnecessary columns\ndf = df.drop(['UniqueID','branch_id','supplier_id','manufacturer_id','Current_pincode_ID','Employee_code_ID'],axis=1)\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Columns with missing values\ndf.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, we will check if there are missing values in the dataset. Those missing values will be plugged using **Mode Replacement**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Replacing NA with mode replacement\ndf = df.apply(lambda x:x.fillna(x.value_counts().index[0]))\ndf.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Below are some EDA charts and analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"#single variable analysis\nprint(df.loan_default.value_counts())\ndf.loan_default.value_counts().plot.bar()\nplt.title('Default Count')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df['Employment.Type'].value_counts())\ndf['Employment.Type'].value_counts().plot.bar()\nplt.title('Employment Type')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df['PERFORM_CNS.SCORE.DESCRIPTION'].value_counts())\ndf['PERFORM_CNS.SCORE.DESCRIPTION'].value_counts().plot.bar()\nplt.title('CNS Score Description')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['State_ID'].value_counts().plot.bar()\nplt.title('State')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.State_ID.value_counts(normalize=True).plot.bar()\nplt.title('State - Relative count')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['disbursed_amount'].plot.hist(bins=100)\nplt.title('Disbursed Amount Histogram')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.boxplot('disbursed_amount')\nplt.title('Disbursed Amount BoxPlot')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['ltv'].plot.hist()\nplt.title('LTV Histogram')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.boxplot('ltv')\nplt.title('LTV BoxPlot')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['age'] = pd.DatetimeIndex(data['DisbursalDate']).year - pd.DatetimeIndex(data['Date.of.Birth']).year\ndf['age'].plot.hist()\nplt.title('Age Histogram')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.boxplot('age')\nplt.title('Age BoxPlot')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x='loan_default', y='age',data=df)\nplt.title('Age BoxPlot')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x='loan_default', y='ltv',data=df)\nplt.title('LTV BoxPlot')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x='Employment.Type',hue='loan_default',data=df)\nplt.title('Employment Bar Graph')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Employment_default= pd.crosstab(df['Employment.Type'],df['loan_default'],normalize='index')\nEmployment_default.plot.bar()\nplt.title('Normalized Employment Bar Graph')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x='Employment.Type', y='ltv',data=df)\nplt.title('Employment Type BoxPlot')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score_default= pd.crosstab(df['PERFORM_CNS.SCORE.DESCRIPTION'],df['loan_default'],normalize='index')\nscore_default.plot.bar()\nplt.title('CNS Score BoxPlot')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr = df.corr()\nsns.heatmap(corr)\nplt.title('Heat Map')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we will start with model creation. Before we start, we have to convert categorical data into codes and preprocess by dropping some variables and reducing the number of states by keeping only top 6 highest frequency states and combining all other into others and then creating dummy variables of all those states."},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Employment.Type'] = df['Employment.Type'].cat.reorder_categories(['Self employed', 'Salaried'])\ndf['Employment.Type']  = df['Employment.Type'].cat.codes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Preprocessing\n\n#dropping unnecessary columns\n# MobileNo_Avl_Flag - All values are 1\n# Date.of.Birth , DisbursalDate -  Already used to compute age\n# PERFORM_CNS.SCORE.DESCRIPTION - Score is already in dataset\n\ndf = df.drop(['MobileNo_Avl_Flag','Date.of.Birth','DisbursalDate','PERFORM_CNS.SCORE.DESCRIPTION'],axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.State_ID.value_counts()/len(df)\n\n#70% of the data is in 1st 6 states and hence we will change all other state's value as 'other'\ndef stid(i):\n        switcher={\n                4              :'4',\n                3              :'3',\n                6              :'6',\n                13             :'13',\n                9              :'9',\n                8              :'8'\n               \n             }\n        return switcher.get(i,'Other')\ndf['State_ID_new']=df['State_ID'].apply (stid) \ndf = df.drop(['State_ID'], axis = 1)     ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_new = pd.get_dummies(df,drop_first=True) \nprint(df_new.columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, we will first standardize the data and then split the data into train and test using stratify option."},{"metadata":{"trusted":true},"cell_type":"code","source":"#train test split\nX = df_new.drop('loan_default',axis=1)\ny = df_new['loan_default']\n\nfrom sklearn.preprocessing import StandardScaler\n\nzs = StandardScaler()\nzs.fit(X)\n\n\nXt_z = pd.DataFrame(zs.transform(X), columns =X.columns)\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(Xt_z, y, stratify=y, random_state=5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Below are 5 default models and 5 Fine Tuned Models."},{"metadata":{},"cell_type":"markdown","source":"### Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression()\nfit=logreg.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_list = list(X_train)\nimportance = pd.DataFrame(index=feature_list, data=np.transpose(logreg.coef_, axes=None), columns=[\"feature coefficient\"])\nimportance","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred= logreg.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nfrom sklearn import metrics\nprint(confusion_matrix(y_pred,y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cm = confusion_matrix(y_pred,y_test)\naccuracy_lr = (cm[0,0]+cm[1,1])/(cm[0,0]+cm[0,1]+cm[1,0]+cm[1,1])\nprint(\"accuracy\" ,accuracy_lr)\n#sensitivity(TPR) = (TP)/(TP+FN)\nsensitivity_lr = (cm[1,1])/(cm[0,1]+cm[1,1])\nprint(\"sensitivity\" ,sensitivity_lr)\n#Specificity(TNR) = (TN)/(TN+FP)\nSpecificity_lr = (cm[0,0])/(cm[0,0]+cm[1,0])\nprint(\"Specificity\" ,Specificity_lr)\n#FPR = (FP)/(FP+TN)\nFPR_lr = (cm[1,0])/(cm[0,0]+cm[1,0])\nprint(\"FPR\" ,FPR_lr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_curve\n\ny_pred_prob = logreg.predict_proba(X_test)[:,1]\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\nplt.plot(fpr, tpr, label='Logistic Regression')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Logistic Regression ROC Curve')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\nroc_lr = roc_auc_score(y_test, y_pred_prob) #used to check different models such as logistic, decision tree\nroc_lr","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Logistic Regression Fine Tuned"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Hyper Parameters - Internal parameter to a model\n#GridSearch CV\nmax_iter=[100,250,500,750]\nC = [0.01,0.1,1.0]\nparam_grid = dict(max_iter=max_iter,C=C)\nparam_grid","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nlr = LogisticRegression()\ngrid = GridSearchCV(estimator=lr, param_grid=param_grid)\ngrid_result = grid.fit(X_train, y_train)\nprint(\"Best Score: \",grid_result.best_score_)\nprint(\"Best Score: \",grid_result.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred= grid.predict(X_test)\ncm_ft = confusion_matrix(y_pred,y_test)\ncm_ft","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy_lrft = (cm_ft[0,0]+cm_ft[1,1])/(cm_ft[0,0]+cm_ft[0,1]+cm_ft[1,0]+cm_ft[1,1])\nprint(\"accuracy\" ,accuracy_lrft)\n#sensitivity(TPR) = (TP)/(TP+FN)\nsensitivity_lrft = (cm_ft[1,1])/(cm_ft[0,1]+cm_ft[1,1])\nprint(\"sensitivity\" ,sensitivity_lrft)\n#Specificity(TNR) = (TN)/(TN+FP)\nSpecificity_lrft = (cm_ft[0,0])/(cm_ft[0,0]+cm_ft[1,0])\nprint(\"Specificity\" ,Specificity_lrft)\n#FPR = (FP)/(FP+TN)\nFPR_lrft = (cm_ft[1,0])/(cm_ft[0,0]+cm_ft[1,0])\nprint(\"FPR\" ,FPR_lrft)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_curve\n\ny_pred_prob = grid.predict_proba(X_test)[:,1]\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\nplt.plot(fpr, tpr, label='Logistic Regression')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Logistic Regression Fine Tuned ROC Curve')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\nroc_lrft = roc_auc_score(y_test, y_pred_prob) #used to check different models such as logistic, decision tree\nroc_lrft","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Decision Tree"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\nDT = DecisionTreeClassifier(random_state=42)\nDT.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_importances = DT.feature_importances_\nfeature_list = list(X_train)\nimportance = pd.DataFrame(index=feature_list, data=feature_importances, columns=[\"feature importance\"])\nimportance.sort_values(by='feature importance',ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred= DT.predict(X_test)\ncm_dt = confusion_matrix(y_pred,y_test)\ncm_dt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#accuracy = (TN+TP)/(ALL)\naccuracy_dt = (cm_dt[0,0]+cm_dt[1,1])/(cm_dt[0,0]+cm_dt[0,1]+cm_dt[1,0]+cm_dt[1,1])\nprint(\"accuracy\" ,accuracy_dt)\n#sensitivity(TPR) = (TP)/(TP+FN)\nsensitivity_dt = (cm_dt[1,1])/(cm_dt[0,1]+cm_dt[1,1])\nprint(\"sensitivity\" ,sensitivity_dt)\n#Specificity(TNR) = (TN)/(TN+FP)\nSpecificity_dt = (cm_dt[0,0])/(cm_dt[0,0]+cm_dt[1,0])\nprint(\"Specificity\" ,Specificity_dt)\n#FPR = (FP)/(FP+TN)\nFPR_dt = (cm_dt[1,0])/(cm_dt[0,0]+cm_dt[1,0])\nprint(\"FPR\" ,FPR_dt)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_curve\n\ny_pred_prob = DT.predict_proba(X_test)[:,1]\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\nplt.plot(fpr, tpr, label='Logistic Regression')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Decision Tree ROC Curve')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\nroc_dt = roc_auc_score(y_test, y_pred_prob) #used to check different models such as logistic, decision tree\nroc_dt","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Decision Tree Fine Tuned"},{"metadata":{"trusted":true},"cell_type":"code","source":"#gridsearch\nfrom sklearn.model_selection import GridSearchCV\nmax_depth = [i for i in range(5,10,1)]\nmin_samples_leaf = [i for i in range(500,2500,500)]\nmax_features = [10,15,20,25,30]\nparam_grid = dict(max_depth=max_depth,min_samples_leaf=min_samples_leaf,max_features=max_features)\nparam_grid","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\nDT = DecisionTreeClassifier()\ngrid = GridSearchCV(estimator=DT, param_grid=param_grid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_result = grid.fit(X_train, y_train)\nprint(\"Best Score: \",grid_result.best_score_)\nprint(\"Best Score: \",grid_result.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred= grid.predict(X_test)\ncm_dt_ft = confusion_matrix(y_pred,y_test)\ncm_dt_ft","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#accuracy = (TN+TP)/(ALL)\naccuracy_dt_ft = (cm_dt_ft[0,0]+cm_dt_ft[1,1])/(cm_dt_ft[0,0]+cm_dt_ft[0,1]+cm_dt_ft[1,0]+cm_dt_ft[1,1])\nprint(\"accuracy\" ,accuracy_dt_ft)\n#sensitivity(TPR) = (TP)/(TP+FN)\nsensitivity_dt_ft = (cm_dt_ft[1,1])/(cm_dt_ft[0,1]+cm_dt_ft[1,1])\nprint(\"sensitivity\" ,sensitivity_dt_ft)\n#Specificity(TNR) = (TN)/(TN+FP)\nSpecificity_dt_ft = (cm_dt_ft[0,0])/(cm_dt_ft[0,0]+cm_dt_ft[1,0])\nprint(\"Specificity\" ,Specificity_dt_ft)\n#FPR = (FP)/(FP+TN)\nFPR_dt_ft = (cm_dt_ft[1,0])/(cm_dt_ft[0,0]+cm_dt_ft[1,0])\nprint(\"FPR\" ,FPR_dt_ft)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_curve\n\ny_pred_prob = grid.predict_proba(X_test)[:,1]\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\nplt.plot(fpr, tpr, label='Logistic Regression')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Decision Tree Fine Tuned ROC Curve')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\nroc_dt_ft = roc_auc_score(y_test, y_pred_prob) #used to check different models such as logistic, decision tree\nroc_dt_ft","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## BaggingClassifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\nDT = DecisionTreeClassifier(random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import BaggingClassifier\nBC = BaggingClassifier(base_estimator=DT,oob_score=True)\nBC.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred= BC.predict(X_test)\ncm_dt_bg = confusion_matrix(y_pred,y_test)\ncm_dt_bg","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#accuracy = (TN+TP)/(ALL)\naccuracy_dt_bg = (cm_dt_bg[0,0]+cm_dt_bg[1,1])/(cm_dt_bg[0,0]+cm_dt_bg[0,1]+cm_dt_bg[1,0]+cm_dt_bg[1,1])\nprint(\"accuracy\" ,accuracy_dt_bg)\n#sensitivity(TPR) = (TP)/(TP+FN)\nsensitivity_dt_bg = (cm_dt_bg[1,1])/(cm_dt_bg[0,1]+cm_dt_bg[1,1])\nprint(\"sensitivity\" ,sensitivity_dt_bg)\n#Specificity(TNR) = (TN)/(TN+FP)\nSpecificity_dt_bg = (cm_dt_bg[0,0])/(cm_dt_bg[0,0]+cm_dt_bg[1,0])\nprint(\"Specificity\" ,Specificity_dt_bg)\n#FPR = (FP)/(FP+TN)\nFPR_dt_bg = (cm_dt_bg[1,0])/(cm_dt_bg[0,0]+cm_dt_bg[1,0])\nprint(\"FPR\" ,FPR_dt_bg)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_curve\n\ny_pred_prob = BC.predict_proba(X_test)[:,1]\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\nplt.plot(fpr, tpr, label='Logistic Regression')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Bagging Classifier ROC Curve')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\nroc_dt_bg = roc_auc_score(y_test, y_pred_prob) #used to check different models such as logistic, decision tree\nroc_dt_bg","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Bagging Classifier Fine Tuned"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nmax_features = [15,20,25,30]\nn_estimators = [i for i in range(8,15,2)]\nparam_grid = dict(max_features=max_features,\n                n_estimators=n_estimators )\nparam_grid","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\nDT = DecisionTreeClassifier()\nfrom sklearn.ensemble import BaggingClassifier\nBC = BaggingClassifier(base_estimator=DT,oob_score=True,random_state =42)\ngrid = GridSearchCV(estimator=BC, param_grid=param_grid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_result = grid.fit(X_train, y_train)\nprint(\"Best Score: \",grid_result.best_score_)\nprint(\"Best Score: \",grid_result.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred= grid.predict(X_test)\ncm_dt_bg_ft = confusion_matrix(y_pred,y_test)\ncm_dt_bg_ft","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#accuracy = (TN+TP)/(ALL)\naccuracy_bg_ft = (cm_dt_bg_ft[0,0]+cm_dt_bg_ft[1,1])/(cm_dt_bg_ft[0,0]+cm_dt_bg_ft[0,1]+cm_dt_bg_ft[1,0]+cm_dt_bg_ft[1,1])\nprint(\"accuracy\" ,accuracy_bg_ft)\n#sensitivity(TPR) = (TP)/(TP+FN)\nsensitivity_bg_ft = (cm_dt_bg_ft[1,1])/(cm_dt_bg_ft[0,1]+cm_dt_bg_ft[1,1])\nprint(\"sensitivity\" ,sensitivity_bg_ft)\n#Specificity(TNR) = (TN)/(TN+FP)\nSpecificity_bg_ft = (cm_dt_bg_ft[0,0])/(cm_dt_bg_ft[0,0]+cm_dt_bg_ft[1,0])\nprint(\"Specificity\" ,Specificity_bg_ft)\n#FPR = (FP)/(FP+TN)\nFPR_bg_ft = (cm_dt_bg_ft[1,0])/(cm_dt_bg_ft[0,0]+cm_dt_bg_ft[1,0])\nprint(\"FPR\" ,FPR_bg_ft)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_curve\n\ny_pred_prob = grid.predict_proba(X_test)[:,1]\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\nplt.plot(fpr, tpr, label='Logistic Regression')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Bagging Classifier Fine Tuned ROC Curve')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\nroc_bg_ft = roc_auc_score(y_test, y_pred_prob) #used to check different models such as logistic, decision tree\nroc_bg_ft","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## RandomForest Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nRF = RandomForestClassifier(oob_score=True)\nRF.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred= RF.predict(X_test)\ncm_rf = confusion_matrix(y_pred,y_test)\ncm_rf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#accuracy = (TN+TP)/(ALL)\naccuracy_rf = (cm_rf[0,0]+cm_rf[1,1])/(cm_rf[0,0]+cm_rf[0,1]+cm_rf[1,0]+cm_rf[1,1])\nprint(\"accuracy\" ,accuracy_rf)\n#sensitivity(TPR) = (TP)/(TP+FN)\nsensitivity_rf = (cm_rf[1,1])/(cm_rf[0,1]+cm_rf[1,1])\nprint(\"sensitivity\" ,sensitivity_rf)\n#Specificity(TNR) = (TN)/(TN+FP)\nSpecificity_rf = (cm_rf[0,0])/(cm_rf[0,0]+cm_rf[1,0])\nprint(\"Specificity\" ,Specificity_rf)\n#FPR = (FP)/(FP+TN)\nFPR_rf = (cm_rf[1,0])/(cm_rf[0,0]+cm_rf[1,0])\nprint(\"FPR\" ,FPR_rf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_curve\n\ny_pred_prob = RF.predict_proba(X_test)[:,1]\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\nplt.plot(fpr, tpr, label='Logistic Regression')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Random Forest ROC Curve')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\nroc_rf = roc_auc_score(y_test, y_pred_prob) #used to check different models such as logistic, decision tree\nroc_rf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Random Forest Classifier Fine Tuned"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nmax_depth = [i for i in range(5,10,2)]\nmin_samples_leaf = [i for i in range(500,2500,1000)]\nmax_features = [20,25,30]\nn_estimators = [i for i in range(10,15,2)]\nparam_grid = dict(max_depth=max_depth,min_samples_leaf=min_samples_leaf,max_features=max_features,\n                n_estimators=n_estimators )\nparam_grid","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nRF = RandomForestClassifier(oob_score=True,random_state =42)\ngrid = GridSearchCV(estimator=RF, param_grid=param_grid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_result = grid.fit(X_train, y_train)\nprint(\"Best Score: \",grid_result.best_score_)\nprint(\"Best Score: \",grid_result.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred= grid.predict(X_test)\ncm_rf_ft = confusion_matrix(y_pred,y_test)\ncm_rf_ft","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#accuracy = (TN+TP)/(ALL)\naccuracy_rf_ft = (cm_rf_ft[0,0]+cm_rf_ft[1,1])/(cm_rf_ft[0,0]+cm_rf_ft[0,1]+cm_rf_ft[1,0]+cm_rf_ft[1,1])\nprint(\"accuracy\" ,accuracy_rf_ft)\n#sensitivity(TPR) = (TP)/(TP+FN)\nsensitivity_rf_ft = (cm_rf_ft[1,1])/(cm_rf_ft[0,1]+cm_rf_ft[1,1])\nprint(\"sensitivity\" ,sensitivity_rf_ft)\n#Specificity(TNR) = (TN)/(TN+FP)\nSpecificity_rf_ft = (cm_rf_ft[0,0])/(cm_rf_ft[0,0]+cm_rf_ft[1,0])\nprint(\"Specificity\" ,Specificity_rf_ft)\n#FPR = (FP)/(FP+TN)\nFPR_rf_ft = (cm_rf_ft[1,0])/(cm_rf_ft[0,0]+cm_rf_ft[1,0])\nprint(\"FPR\" ,FPR_rf_ft)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_curve\n\ny_pred_prob = grid.predict_proba(X_test)[:,1]\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\nplt.plot(fpr, tpr, label='Logistic Regression')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Random Forest Fine Tuned ROC Curve')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\nroc_rf_ft = roc_auc_score(y_test, y_pred_prob) #used to check different models such as logistic, decision tree\nroc_rf_ft","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## GradientBoostingClassifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier\nclf = GradientBoostingClassifier(random_state=100)\nclf.fit(X_train, y_train)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred= clf.predict(X_test)\ncm_gbf = confusion_matrix(y_pred,y_test)\ncm_gbf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#accuracy = (TN+TP)/(ALL)\naccuracy_gbf = (cm_gbf[0,0]+cm_gbf[1,1])/(cm_gbf[0,0]+cm_gbf[0,1]+cm_gbf[1,0]+cm_gbf[1,1])\nprint(\"accuracy\" ,accuracy_gbf)\n#sensitivity(TPR) = (TP)/(TP+FN)\nsensitivity_gbf = (cm_gbf[1,1])/(cm_gbf[0,1]+cm_gbf[1,1])\nprint(\"sensitivity\" ,sensitivity_gbf)\n#Specificity(TNR) = (TN)/(TN+FP)\nSpecificity_gbf = (cm_gbf[0,0])/(cm_gbf[0,0]+cm_gbf[1,0])\nprint(\"Specificity\" ,Specificity_gbf)\n#FPR = (FP)/(FP+TN)\nFPR_gbf = (cm_gbf[1,0])/(cm_gbf[0,0]+cm_gbf[1,0])\nprint(\"FPR\" ,FPR_gbf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_curve\n\ny_pred_prob = clf.predict_proba(X_test)[:,1]\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\nplt.plot(fpr, tpr, label='Logistic Regression')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Gradient Boosting ROC Curve')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\nroc_gbf = roc_auc_score(y_test, y_pred_prob) #used to check different models such as logistic, decision tree\nroc_gbf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## GradientBoostingClassifier Fine Tuned"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nmax_depth = [i for i in range(3,6,2)]\nmin_samples_leaf = [i for i in range(500,2500,1000)]\nmax_features = [20,25,30]\nn_estimators = [i for i in range(10,15,2)]\nparam_grid = dict(max_depth=max_depth,min_samples_leaf=min_samples_leaf,max_features=max_features,\n                n_estimators=n_estimators )\nparam_grid","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier\nclf = GradientBoostingClassifier(random_state=100)\ngrid = GridSearchCV(estimator=clf, param_grid=param_grid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_result = grid.fit(X_train, y_train)\nprint(\"Best Score: \",grid_result.best_score_)\nprint(\"Best Score: \",grid_result.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred= grid.predict(X_test)\ncm_gbf_ft = confusion_matrix(y_pred,y_test)\ncm_gbf_ft","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#accuracy = (TN+TP)/(ALL)\naccuracy_gbf_ft = (cm_gbf_ft[0,0]+cm_gbf_ft[1,1])/(cm_gbf_ft[0,0]+cm_gbf_ft[0,1]+cm_gbf_ft[1,0]+cm_gbf_ft[1,1])\nprint(\"accuracy\" ,accuracy_gbf_ft)\n#sensitivity(TPR) = (TP)/(TP+FN)\nsensitivity_gbf_ft = (cm_gbf_ft[1,1])/(cm_gbf_ft[0,1]+cm_gbf_ft[1,1])\nprint(\"sensitivity\" ,sensitivity_gbf_ft)\n#Specificity(TNR) = (TN)/(TN+FP)\nSpecificity_gbf_ft = (cm_gbf_ft[0,0])/(cm_gbf_ft[0,0]+cm_gbf_ft[1,0])\nprint(\"Specificity\" ,Specificity_gbf_ft)\n#FPR = (FP)/(FP+TN)\nFPR_gbf_ft = (cm_gbf_ft[1,0])/(cm_gbf_ft[0,0]+cm_gbf_ft[1,0])\nprint(\"FPR\" ,FPR_gbf_ft)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_curve\n\ny_pred_prob = grid.predict_proba(X_test)[:,1]\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\nplt.plot(fpr, tpr, label='Logistic Regression')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Gradient Boosting Fine Tuned ROC Curve')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\nroc_gbf_ft = roc_auc_score(y_test, y_pred_prob) #used to check different models such as logistic, decision tree\nroc_gbf_ft","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ALL MODEL COMPUTATION"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = pd.DataFrame({'Logistic Regression': {'Accuracy': \"{0:.2f}%\".format(accuracy_lr * 100), 'Sensitivity': \"{0:.2f}%\".format(sensitivity_lr * 100), 'Specificity': \"{0:.2f}%\".format(Specificity_lr * 100), 'FPR': \"{0:.2f}%\".format(FPR_lr * 100), 'ROC': \"{0:.2f}%\".format(roc_lr * 100)},\n                      'Logistic Regression Fine Tuned': {'Accuracy': \"{0:.2f}%\".format(accuracy_lrft * 100), 'Sensitivity': \"{0:.2f}%\".format(sensitivity_lrft * 100), 'Specificity': \"{0:.2f}%\".format(Specificity_lrft * 100), 'FPR': \"{0:.2f}%\".format(FPR_lrft * 100), 'ROC': \"{0:.2f}%\".format(roc_lrft * 100)},\n                      'Decision Tree': {'Accuracy': \"{0:.2f}%\".format(accuracy_dt * 100), 'Sensitivity': \"{0:.2f}%\".format(sensitivity_dt * 100), 'Specificity': \"{0:.2f}%\".format(Specificity_dt * 100), 'FPR': \"{0:.2f}%\".format(FPR_dt * 100), 'ROC': \"{0:.2f}%\".format(roc_dt * 100)},\n                      'Decision Tree Fine Tuned': {'Accuracy': \"{0:.2f}%\".format(accuracy_dt_ft * 100), 'Sensitivity': \"{0:.2f}%\".format(sensitivity_dt_ft * 100), 'Specificity': \"{0:.2f}%\".format(Specificity_dt_ft * 100), 'FPR': \"{0:.2f}%\".format(FPR_dt_ft * 100), 'ROC': \"{0:.2f}%\".format(roc_dt_ft * 100)},\n                      'Bagging Classifier': {'Accuracy': \"{0:.2f}%\".format(accuracy_dt_bg * 100), 'Sensitivity': \"{0:.2f}%\".format(sensitivity_dt_bg * 100), 'Specificity': \"{0:.2f}%\".format(Specificity_dt_bg * 100), 'FPR': \"{0:.2f}%\".format(FPR_dt_bg * 100), 'ROC': \"{0:.2f}%\".format(roc_dt_bg * 100)},\n                      'Bagging Classifier Fine Tuned': {'Accuracy': \"{0:.2f}%\".format(accuracy_bg_ft * 100), 'Sensitivity': \"{0:.2f}%\".format(sensitivity_bg_ft * 100), 'Specificity': \"{0:.2f}%\".format(Specificity_bg_ft * 100), 'FPR': \"{0:.2f}%\".format(FPR_bg_ft * 100), 'ROC': \"{0:.2f}%\".format(roc_bg_ft * 100)},\n                      'Random Forest': {'Accuracy': \"{0:.2f}%\".format(accuracy_rf * 100), 'Sensitivity': \"{0:.2f}%\".format(sensitivity_rf * 100), 'Specificity': \"{0:.2f}%\".format(Specificity_rf * 100), 'FPR': \"{0:.2f}%\".format(FPR_rf * 100), 'ROC': \"{0:.2f}%\".format(roc_rf * 100)},\n                      'Random Forest Fine Tuned': {'Accuracy': \"{0:.2f}%\".format(accuracy_rf_ft * 100), 'Sensitivity': \"{0:.2f}%\".format(sensitivity_rf_ft * 100), 'Specificity': \"{0:.2f}%\".format(Specificity_rf_ft * 100), 'FPR': \"{0:.2f}%\".format(FPR_rf_ft * 100), 'ROC': \"{0:.2f}%\".format(roc_rf_ft * 100)},\n                      'Gradient Boosting': {'Accuracy': \"{0:.2f}%\".format(accuracy_gbf * 100), 'Sensitivity': \"{0:.2f}%\".format(sensitivity_gbf * 100), 'Specificity': \"{0:.2f}%\".format(Specificity_gbf * 100), 'FPR': \"{0:.2f}%\".format(FPR_gbf * 100), 'ROC': \"{0:.2f}%\".format(roc_gbf * 100)},\n                      'Gradient Boosting Fine Tuned': {'Accuracy': \"{0:.2f}%\".format(accuracy_gbf_ft * 100), 'Sensitivity': \"{0:.2f}%\".format(sensitivity_gbf_ft * 100), 'Specificity': \"{0:.2f}%\".format(Specificity_gbf_ft * 100), 'FPR': \"{0:.2f}%\".format(FPR_gbf_ft * 100), 'ROC': \"{0:.2f}%\".format(roc_gbf_ft * 100)}})\nmodel.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since the objective of the model is to predict the number of loans which are going to default, we will focus on **Sensitivity** rather than accuracy of the model. To compare sensitivity and specificity, we will use ROC values of different model to check.\n\nBased on above table, we can observe that **Gradient Boosting** has the highest ROC value and hence, we will select Gradient Boosting as our model for prediction.\n\n\nDue to default threshold value of the model being 0.5 for dividing the predicted probabilities into Default or Not Default, we are getting such a low Sensitivity value. To get optimal Sensitivity value, we will plot Sensitivty and Specificty for different threshold values."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Gradient Boosting is best\n\nfrom sklearn.ensemble import GradientBoostingClassifier\nclf = GradientBoostingClassifier(random_state=100)\nclf.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking optimal threshold value to get decent sensitivity value\n\nfrom sklearn.metrics import roc_curve\n\ny_pred_prob = clf.predict_proba(X_test)[:,1]\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\nplt.plot(thresholds, tpr, label='Sensitivty')\nplt.xlabel('Thresholds')\nplt.ylabel('Rate')\nplt.plot(thresholds, 1-fpr, label='Specificity')\nplt.title('Gradient Boosting ROC Curve')\nplt.axvline(0.23, c='black', ls='dashed') # trail and error\nplt.xlim(0,1)\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We need to get threshold value for which we can get high Sensitivity value but at the same time we cannot compromise with Sepecitivty value also. From above graph, it is evident that Sensitivity is a downward sloping curve whereas Specificity is a upward sloping curve and at the intersection point, we can get highest value of Sensitivty and specificity simulatneously.\n\nHence, at **threshold = 0.23**, the two curve interects and hence we will select 0.23 as threshold value for deciding if a loan is going to default or not."},{"metadata":{"trusted":true},"cell_type":"code","source":"#As per the graph at Threshold 0.23, we get maximum Sensitivity and Specificity\ny_pred= (clf.predict_proba(X_test)[:,1]>=0.23).astype(int)\ncm_final = confusion_matrix(y_pred,y_test)\ncm_final","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#accuracy = (TN+TP)/(ALL)\naccuracy_final = (cm_final[0,0]+cm_final[1,1])/(cm_final[0,0]+cm_final[0,1]+cm_final[1,0]+cm_final[1,1])\nprint(\"accuracy\" ,accuracy_final)\n#sensitivity(TPR) = (TP)/(TP+FN)\nsensitivity_final = (cm_final[1,1])/(cm_final[0,1]+cm_final[1,1])\nprint(\"sensitivity\" ,sensitivity_final)\n#Specificity(TNR) = (TN)/(TN+FP)\nSpecificity_final = (cm_final[0,0])/(cm_final[0,0]+cm_final[1,0])\nprint(\"Specificity\" ,Specificity_final)\n#FPR = (FP)/(FP+TN)\nFPR_final = (cm_final[1,0])/(cm_final[0,0]+cm_final[1,0])\nprint(\"FPR\" ,FPR_final)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From above values, we can observe that accuracy of the model may have gone down but the model is giving very good sensitivty as well as specificty values."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}