{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data = pd.read_csv(os.path.join(dirname, filename))\ndata.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.columns = ['country', 'year', 'sex', 'age', 'suicides_no', 'population',\n       'suicides/100kpop', 'country-year', 'HDI_for_year',\n       'gdp_for_year_dollars', 'gdp_per_capita_dollars', 'generation']\ndata.columns.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del data['country-year']\n#del data['HDI_for_year'], silmek yerine NaN değerleri ortalama değeri ile dolduracağız","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['gdp_for_year_dollars'] = data['gdp_for_year_dollars'].str.replace(',','').astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#½20 test için ayırdık\nfrom sklearn.model_selection import StratifiedShuffleSplit\n\nsplit = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state = 1)\nfor train_index, test_index in split.split(data, data['generation']):\n    train = data.loc[train_index]\n    test = data.loc[test_index]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = train.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_labels = train['suicides/100kpop']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#categorical sütunları numerical olarak çeviriyoruz:\nfrom sklearn.preprocessing import LabelEncoder\n\ncategory_features = data[[column for column in data.columns if data[column].dtype == 'object']]\nle = LabelEncoder()\n\ndata_category = category_features.apply(lambda col: le.fit_transform(col))\ndata_category.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#overfitting önlemek için ayrı ayrı sütunlar üreterek binary şeklinde getiriyoruz;\ndata_category_dummies = pd.get_dummies(data, columns=category_features.columns, drop_first=True )\ndata_category_dummies\n#117 sütun elde ettik","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#imputer kütüphanesi tek seferde tüm sütunlardaki missing değeri doldurmamıza yardımcı olur.\n#tek NaN değerleri sütun HDI_for_year olmasına rağmen yine de bu yöntemi uygulayalım\n\n#from sklearn.preprocessing import Imputer ---> versiondan dolayı hata aldım bu yüzden simpleimputer kullandım.\nfrom sklearn.impute import SimpleImputer \nsimple_imputer = SimpleImputer(strategy='median')\n\nnumerical_features = data[data.columns[data.dtypes != 'object']]\n\n#imputer fonksiyonunu numerical sütunlarda uygulayacağız bu yüzden categorical olanları drop edeilm\ndata_numerical = simple_imputer.fit_transform(numerical_features)\ndata_numerical = pd.DataFrame(data_numerical,columns=data.columns[data.dtypes != 'object'])\ndata_numerical['HDI_for_year'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nscaled_data_numerical = scaler.fit_transform(data_numerical)\n\nscaled_data_numerical = pd.DataFrame(scaled_data_numerical,columns=data.columns[data.dtypes != 'object'])\n\nscaled_data_numerical","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import LabelEncoder\n\nnumerical_pipeline = Pipeline([\n    ('imputer', SimpleImputer(strategy='median')),\n    ('scaler', StandardScaler(with_mean=False))\n])\n\ncategorical_pipeline = Pipeline([\n    ('encoder', OneHotEncoder(handle_unknown='ignore')),\n    ('scaler', StandardScaler(with_mean=False))\n])\n\nfull_pipeline = ColumnTransformer([\n    ('numerical_pipeline', numerical_pipeline, list(numerical_features.columns)),\n    ('categorical_pipeline', categorical_pipeline, list(category_features.columns)),\n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_prepared = full_pipeline.fit_transform(data)\ntype(data_prepared)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_pipeline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor\nfrom sklearn.metrics import mean_squared_error\ndr = DecisionTreeRegressor(random_state=0)\ndr.fit(data_prepared,data_labels)\ndr_predictions = dr.predict(data_prepared)\n\ndrmse = np.sqrt(mean_squared_error(data_labels,dr_predictions))\ndrmse","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\n\nscores_1 = cross_val_score(dr, data_prepared, data_labels, scoring = \"neg_mean_squared_error\", cv = 10)\ntree_scores = np.sqrt(-scores_1)\ntree_scores.mean()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}