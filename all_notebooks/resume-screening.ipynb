{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Resume Screening ","metadata":{}},{"cell_type":"markdown","source":"Hiring the right talent is a challenge for all businesses. This challenge is magnified by the high volume of applicants if the business is labour-intensive, growing, and facing high attrition rates.\nTypically, large companies do not have enough time to open each CV, so they use machine learning algorithms for the Resume Screening task which we are gonna try to implement in this project","metadata":{}},{"cell_type":"code","source":"# importing libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn import metrics\nfrom sklearn.metrics import accuracy_score\nfrom pandas.plotting import scatter_matrix\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import metrics\n\n# reading our dataset\ndf = pd.read_csv('../input/updatedresumedataset/UpdatedResumeDataSet.csv',encoding='utf8')\ndf['cleaned_resume'] = ''\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-16T05:50:33.925715Z","iopub.execute_input":"2021-06-16T05:50:33.926244Z","iopub.status.idle":"2021-06-16T05:50:33.975502Z","shell.execute_reply.started":"2021-06-16T05:50:33.926197Z","shell.execute_reply":"2021-06-16T05:50:33.974365Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape","metadata":{"execution":{"iopub.status.busy":"2021-06-16T05:50:33.977547Z","iopub.execute_input":"2021-06-16T05:50:33.977845Z","iopub.status.idle":"2021-06-16T05:50:33.983294Z","shell.execute_reply.started":"2021-06-16T05:50:33.977818Z","shell.execute_reply":"2021-06-16T05:50:33.982497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Exploration","metadata":{}},{"cell_type":"markdown","source":"our dataset has 962 rows and 3 columns","metadata":{}},{"cell_type":"code","source":"print (\"Displaying the distinct categories of resume :\")\nprint (df['Category'].unique())","metadata":{"execution":{"iopub.status.busy":"2021-06-16T05:50:33.984799Z","iopub.execute_input":"2021-06-16T05:50:33.98535Z","iopub.status.idle":"2021-06-16T05:50:34.000892Z","shell.execute_reply.started":"2021-06-16T05:50:33.985308Z","shell.execute_reply":"2021-06-16T05:50:33.999755Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print (\"Displaying the distinct categories of resume and the number of records belonging to each category\")\nprint(df['Category'].value_counts())","metadata":{"execution":{"iopub.status.busy":"2021-06-16T05:50:34.002175Z","iopub.execute_input":"2021-06-16T05:50:34.002458Z","iopub.status.idle":"2021-06-16T05:50:34.018868Z","shell.execute_reply.started":"2021-06-16T05:50:34.002431Z","shell.execute_reply":"2021-06-16T05:50:34.017708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# visualising the distinct categories\nplt.figure(figsize=(15,7))\nsns.countplot(y='Category',data=df)","metadata":{"execution":{"iopub.status.busy":"2021-06-16T05:50:34.021937Z","iopub.execute_input":"2021-06-16T05:50:34.022385Z","iopub.status.idle":"2021-06-16T05:50:34.406693Z","shell.execute_reply.started":"2021-06-16T05:50:34.022353Z","shell.execute_reply":"2021-06-16T05:50:34.405647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import plotly.express as px\nCount = df['Category'].value_counts()\nnames  = df['Category'].unique()\nfig = px.pie(df, values=Count, names=names, title='Distribution of Categories')\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-16T05:50:34.41006Z","iopub.execute_input":"2021-06-16T05:50:34.410393Z","iopub.status.idle":"2021-06-16T05:50:36.909997Z","shell.execute_reply.started":"2021-06-16T05:50:34.410364Z","shell.execute_reply":"2021-06-16T05:50:36.908952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Text Cleaning","metadata":{}},{"cell_type":"markdown","source":"Now we will try to remove the URLs, hashtags, mentions, special letters, and punctutations present in our dataset","metadata":{}},{"cell_type":"code","source":"import re\ndef cleanResume(resumeText):\n    resumeText = re.sub('http\\S+\\s*', ' ', resumeText)  # remove URLs\n    resumeText = re.sub('RT|cc', ' ', resumeText)  # remove RT and cc\n    resumeText = re.sub('#\\S+', '', resumeText)  # remove hashtags\n    resumeText = re.sub('@\\S+', '  ', resumeText)  # remove mentions\n    resumeText = re.sub('[%s]' % re.escape(\"\"\"!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\"\"\"), ' ', resumeText)  # remove punctuations\n    resumeText = re.sub(r'[^\\x00-\\x7f]',r' ', resumeText) \n    resumeText = re.sub('\\s+', ' ', resumeText)  # remove extra whitespace\n    return resumeText\n    \ndf['cleaned_resume'] = df.Resume.apply(lambda x: cleanResume(x))\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-16T05:50:36.91124Z","iopub.execute_input":"2021-06-16T05:50:36.911524Z","iopub.status.idle":"2021-06-16T05:50:37.38882Z","shell.execute_reply.started":"2021-06-16T05:50:36.911495Z","shell.execute_reply":"2021-06-16T05:50:37.387742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nltk\nfrom nltk.corpus import stopwords\nimport string\nfrom wordcloud import WordCloud\n\noneSetOfStopWords = set(stopwords.words('english')+['``',\"''\"])\ntotalWords =[]\nSentences = df['cleaned_resume'].values\ncleanedSentences = \"\"\nfor i in range(0,160):\n    cleanedText = cleanResume(Sentences[i])\n    cleanedSentences += cleanedText\n    requiredWords = nltk.word_tokenize(cleanedText)\n    for word in requiredWords:\n        if word not in oneSetOfStopWords and word not in string.punctuation:\n            totalWords.append(word)\n    \nwordfreqdist = nltk.FreqDist(totalWords)\nmostcommon = wordfreqdist.most_common(50)\nprint(mostcommon)","metadata":{"execution":{"iopub.status.busy":"2021-06-16T05:50:37.390468Z","iopub.execute_input":"2021-06-16T05:50:37.390871Z","iopub.status.idle":"2021-06-16T05:50:38.453033Z","shell.execute_reply.started":"2021-06-16T05:50:37.39083Z","shell.execute_reply":"2021-06-16T05:50:38.452224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wc = WordCloud().generate(cleanedSentences)\nplt.figure(figsize=(15,15))\nplt.imshow(wc, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-16T05:50:38.453963Z","iopub.execute_input":"2021-06-16T05:50:38.454237Z","iopub.status.idle":"2021-06-16T05:50:39.102934Z","shell.execute_reply.started":"2021-06-16T05:50:38.454211Z","shell.execute_reply":"2021-06-16T05:50:39.101821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# converting the words into categorical values\nfrom sklearn.preprocessing import LabelEncoder\n\nvar_mod = ['Category']\nle = LabelEncoder()\nfor i in var_mod:\n    df[i] = le.fit_transform(df[i])","metadata":{"execution":{"iopub.status.busy":"2021-06-16T05:50:39.104205Z","iopub.execute_input":"2021-06-16T05:50:39.104486Z","iopub.status.idle":"2021-06-16T05:50:39.111381Z","shell.execute_reply.started":"2021-06-16T05:50:39.10446Z","shell.execute_reply":"2021-06-16T05:50:39.110222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training our model","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom scipy.sparse import hstack\n\nrequiredText = df['cleaned_resume'].values\nrequiredTarget = df['Category'].values\n\nword_vectorizer = TfidfVectorizer(\n    sublinear_tf=True,\n    stop_words='english',\n    max_features=1500)\nword_vectorizer.fit(requiredText)\nWordFeatures = word_vectorizer.transform(requiredText)\n\nprint (\"Feature completed\")\n\nX_train,X_test,y_train,y_test = train_test_split(WordFeatures,requiredTarget,random_state=0, test_size=0.2)\nprint(X_train.shape)\nprint(X_test.shape)","metadata":{"execution":{"iopub.status.busy":"2021-06-16T05:50:39.112688Z","iopub.execute_input":"2021-06-16T05:50:39.112969Z","iopub.status.idle":"2021-06-16T05:50:39.969312Z","shell.execute_reply.started":"2021-06-16T05:50:39.112943Z","shell.execute_reply":"2021-06-16T05:50:39.968403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clf = OneVsRestClassifier(KNeighborsClassifier())\nclf.fit(X_train, y_train)\nprediction = clf.predict(X_test)\nprint('Accuracy of KNeighbors Classifier on training set: {:.2f}'.format(clf.score(X_train, y_train)))\nprint('Accuracy of KNeighbors Classifier on test set: {:.2f}'.format(clf.score(X_test, y_test)))\n\nprint(\"\\n Classification report for classifier %s:\\n%s\\n\" % (clf, metrics.classification_report(y_test, prediction)))","metadata":{"execution":{"iopub.status.busy":"2021-06-16T05:50:39.972803Z","iopub.execute_input":"2021-06-16T05:50:39.973121Z","iopub.status.idle":"2021-06-16T05:50:42.557905Z","shell.execute_reply.started":"2021-06-16T05:50:39.973089Z","shell.execute_reply":"2021-06-16T05:50:42.556907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}