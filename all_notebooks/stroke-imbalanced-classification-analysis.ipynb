{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Stroke predictions and analysis"},{"metadata":{},"cell_type":"markdown","source":"**Table of Contents:**\n\n1. [Load Data and Analyse](#load)\n2. [EDA](#EDA)\n3. [Data Preparation and Preprocessing](#data-preprocessing)\n4. [Model exploration](#model-exploration) \n5. [Oversampling our minority features](#oversampling-data)\n6. [Test set predictions](#test-predictions)"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install pydotplus","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import gc\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport pandas as pd\nimport seaborn as sns\n\nfrom catboost import CatBoostRegressor, cv, Pool\nfrom collections import defaultdict\n\nfrom imblearn.over_sampling import SMOTE\nfrom IPython.display import Image\nfrom pydotplus import graph_from_dot_data\n        \nfrom sklearn.decomposition import PCA\nfrom sklearn.tree import DecisionTreeClassifier, export_graphviz\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, BaggingClassifier\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\nfrom sklearn.model_selection import train_test_split, KFold, cross_val_score, cross_val_predict\nfrom sklearn.model_selection import cross_validate, StratifiedKFold\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, roc_curve, \\\n                            recall_score, confusion_matrix, classification_report, \\\n                            auc, precision_recall_curve\nfrom sklearn.linear_model import LinearRegression, Lasso, Ridge\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression, Perceptron, RidgeClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.svm import SVC","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"load\"></a>\n## 1. Load Data and inspect top level features"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_dir = '/kaggle/input/stroke-prediction-dataset'\ndata_df = pd.read_csv(os.path.join(data_dir, 'healthcare-dataset-stroke-data.csv'))\ndata_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_df.nunique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Check for missing values within our data:"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_df.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It appears our BMI column is missing a fair number of values within the dataset. We'll have to implement some means of either imputing approximate values, or dealing with the missing values in some way."},{"metadata":{"trusted":true},"cell_type":"code","source":"data_df['bmi'].hist()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_df[data_df.isnull().any(axis=1)]['stroke'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Its probably reasonable to simply impute all missing BMI values as the median. Conversely, if we wanted something more sophisticated than this we could create a model to predict the BMI missing values using regression."},{"metadata":{"trusted":true},"cell_type":"code","source":"data_df['bmi'] = data_df['bmi'].fillna(data_df['bmi'].median())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_df.isna().sum().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Good, we've dealt with all missing values within our dataset. Now lets proceed onwards to some exploratory data analysis."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"EDA\"></a>\n## 2. Exploratory Data Analysis (EDA)"},{"metadata":{},"cell_type":"markdown","source":"### 2.1 Analysis of Categorical Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"def custom_countplot(data_df, col_name, ax=None):\n    \"\"\" Plot seaborn countplot for selected dataframe col \"\"\"\n    c_plot = sns.countplot(x=col_name, data=data_df, ax=ax)\n    for g in c_plot.patches:\n        c_plot.annotate(f\"{g.get_height()}\",\n                        (g.get_x()+g.get_width()/3,\n                         g.get_height()+60))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"custom_countplot(data_df=data_df, col_name='stroke')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets do similar plots for all of our categorical columns:"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_cols = data_df.loc[:, data_df.dtypes == object].columns.values\ncat_cols","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Note that all our categorical variables have not been identified above - we have additional ones that are already encoded as numerical features, such as hypertension and heart_disease. We'll add these to our list so we've got them all..."},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_cols = np.append(cat_cols, ['hypertension', 'heart_disease', 'stroke'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n = len(cat_cols)\n\nfig, axs = plt.subplots(2, 4, figsize=(14,7))\naxs = axs.flatten()\n\n# iterate through each col and plot\nfor i, col_name in enumerate(cat_cols):\n    custom_countplot(data_df, col_name, ax=axs[i])\n    axs[i].set_xlabel(f\"{col_name}\", weight = 'bold')\n    axs[i].set_ylabel('Count', weight='bold')\n    \n    if (i != 0 and i != 5):\n        axs[i].set_ylabel('')\n        \nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have a fairly good balance of categorical feature values across our data, with exception to one or two minorities. For example, we have a very low number of cases of hypertension (1) and heart_disease (1). In addition, our target variable (stroke) seems to be extremely imbalanced. As such, we should consider applying some techniques to help work with imbalanced data, otherwise we will have poor generlisation in our models."},{"metadata":{"trusted":true},"cell_type":"code","source":"data_df['stroke'].value_counts(normalize=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As shown, we have less than 5% data that is for those that had a stroke. For instances like this, we need to be careful on the training and evaluation of models, due to the severe data imbalance that we have."},{"metadata":{},"cell_type":"markdown","source":"Lets convert our hypertension and heart_disease variables into objects so that our data preprocessor handles them accordingly later."},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_map_dict = { x : 'object' for x in ['hypertension', 'heart_disease'] }\ndata_df = data_df.astype(cat_map_dict)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This seems pointless (and it kind of is), since I have done this purely for convenience, so that we identify our categorical columns as those that are object dtypes later during preprocessing."},{"metadata":{},"cell_type":"markdown","source":"### 2.2 Analysis of Numerical Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_df['age'].hist()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_variables = ['age', 'avg_glucose_level', 'bmi']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def target_boxplot(y_val_col, x_val_col, data_df, figsize=(9,6), ax=None, name=\"Boxplot\"):\n    \"\"\" Custom boxplot function - plot a chosen value against target x col \"\"\"\n    \n    fig, ax = plt.subplots(figsize=figsize)\n    b_plot = sns.boxplot(x=x_val_col, y=y_val_col, data=data_df, ax=ax)\n    \n    medians = data_df.groupby(x_val_col)[y_val_col].median()\n    vert_offset = data_df[y_val_col].median() * 0.05 \n    \n    for xtick in b_plot.get_xticks():\n        b_plot.text(xtick, medians[xtick] + vert_offset, medians[xtick], \n                horizontalalignment='center',size='small',color='w',weight='semibold')\n        \n    plt.title(f\"{name}\", weight='bold')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target_boxplot('age', 'stroke', data_df, \n               name='Age of those for each stroke output')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target_boxplot('avg_glucose_level', 'stroke', data_df, \n               name='Avg glucode level of those for each stroke output')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target_boxplot('bmi', 'stroke', data_df, \n               name='BMI of those for each stroke output')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets look at the relationship and correlation of numerical variables against our target stroke variable:"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_cols = ['age', 'avg_glucose_level', 'bmi']\nsns.pairplot(data_df.loc[:, plot_cols], height=4, plot_kws={'alpha':0.2})\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# find the correlation between our variables\ncorr = data_df.loc[:, plot_cols].corr()\n\nplt.figure(figsize=(12,8))\nsns.heatmap(corr, annot=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have a few fairly strong correlations between our variables, however they do not resemble anywhere near perfect correlation (1.0), and so we should generally keep all of our variables in this case. If we had a huge number of features, with many being very highly correlated, it could be worth eliminating some to reduce redundancy. In this case we have nothing to worry about, since we have a few low number of variables in the dataset."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"data-preprocessing\"></a>\n## 3. Data Preprocessing"},{"metadata":{},"cell_type":"markdown","source":"Its important that we handle our numerical and categorical features appropriately prior to producing our models.\n\nWe'll put together some preprocessing functions to encode our categorical features and standardise our numerical features. Whilst doing this, we'll also add support for producing additional dimensionality-reduced features (using PCA) to our dataset.\n\nThese extra features will allow us to experiment and tune to find the best combinations of feature engineering to perform for this problem."},{"metadata":{"trusted":true},"cell_type":"code","source":"class DataProcessor(object):\n    def __init__(self):\n        self.encoder = None\n        self.standard_scaler = None\n        self.num_cols = None\n        self.cat_cols = None\n        \n    def preprocess(self, data_df, train=True, one_hot_encode=False,\n                   add_pca_feats=False):\n        \"\"\" Preprocess train / test as required \"\"\"\n        \n        # if training, fit our transformers\n        if train:\n            self.train_ids = data_df.loc[:, 'id']\n            train_cats = data_df.loc[:, data_df.dtypes == object]\n            self.cat_cols = train_cats.columns\n            \n            # if selected, one hot encode our cat features\n            if one_hot_encode:\n                self.encoder = OneHotEncoder(handle_unknown='ignore')\n                oh_enc = self.encoder.fit_transform(train_cats).toarray()\n                train_cats_enc = pd.DataFrame(oh_enc, columns=self.encoder.get_feature_names())\n                self.final_cat_cols = list(train_cats_enc.columns)\n            \n            # otherwise just encode our cat feats with ints\n            else:\n                # encode all of our categorical variables\n                self.encoder = defaultdict(LabelEncoder)\n                train_cats_enc = train_cats.apply(lambda x: \n                                                  self.encoder[x.name].fit_transform(x))\n                self.final_cat_cols = list(self.cat_cols)\n            \n            \n            # standardise all numerical columns\n            train_num = data_df.loc[:, data_df.dtypes != object].drop(columns=['stroke', 'id'])\n            self.num_cols = train_num.columns\n            self.standard_scaler = StandardScaler()\n            train_num_std = self.standard_scaler.fit_transform(train_num)\n            \n            # add pca reduced num feats if selected, else just combine num + cat feats\n            if add_pca_feats:\n                pca_feats = self._return_num_pca(train_num_std)\n                self.final_num_feats = list(self.num_cols)+list(self.pca_cols)\n                \n                \n                X = pd.DataFrame(np.hstack((train_cats_enc, train_num_std, pca_feats)), \n                        columns=list(self.final_cat_cols)+list(self.num_cols)+list(self.pca_cols))\n            else:   \n                self.final_num_feats = list(self.num_cols)\n                X = pd.DataFrame(np.hstack((train_cats_enc, train_num_std)), \n                        columns=list(self.final_cat_cols)+list(self.num_cols))\n        \n        # otherwise, treat as test data\n        else:\n            # transform categorical and numerical data\n            self.test_ids = data_df.loc[:, 'id']\n            cat_data = data_df.loc[:, self.cat_cols]\n        \n            if one_hot_encode:\n                oh_enc = self.encoder.transform(cat_data).toarray()\n                cats_enc = pd.DataFrame(oh_enc, columns=self.encoder.get_feature_names())\n            else:\n                cats_enc = cat_data.apply(lambda x: self.encoder[x.name].transform(x))\n                \n            # transform test numerical data\n            num_data = data_df.loc[:, self.num_cols]\n            num_std = self.standard_scaler.transform(num_data)\n            \n            if add_pca_feats:\n                pca_feats = self._return_num_pca(num_std, train=False)\n                \n                X = pd.DataFrame(np.hstack((cats_enc, num_std, pca_feats)), \n                        columns=list(self.final_cat_cols)+list(self.num_cols)+list(self.pca_cols))\n            \n            else:\n                X = pd.DataFrame(np.hstack((cats_enc, num_std)), \n                        columns=list(self.final_cat_cols)+list(self.num_cols)) \n        return X\n    \n    def _return_num_pca(self, num_df, n_components=0.85, train=True):\n        \"\"\" return dim reduced numerical features using PCA \"\"\"\n        if train:\n            self.pca = PCA(n_components=n_components)\n            num_rd = self.pca.fit_transform(num_df)\n            \n            # create new col names for our reduced features\n            self.pca_cols = [f\"pca_{x}\" for x in range(num_rd.shape[1])]\n            \n        else:\n            num_rd = self.pca.transform(num_df)\n        \n        return pd.DataFrame(num_rd, columns=self.pca_cols)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets transform our data into a form suitable for training various models. This includes encoding our categorical variables, and standardising our numerical variables.\n\nWe can either encode our categorical feature values, or one-hot encode them. Our preprocessing function supports whichever we want, through simply setting the one_hot_encode argument as true (one-hot encoding) or false (simple numerical encoding). We obtain a larger number of feature columns if we one-hot encode, and therefore introduce more complexity. However, many models perform better with one-hot encoding, so it is worth trying both techniques for our range of models.\n\nWe'll be using mainly tree-based methods in this notebook, and as such one-hot encoding and simple encoding of features does not actually make any noticeable difference (as demonstrated through years of empirical research and comparisons). Therefore, we'll keep our dataset simpler and just use categorical encoding."},{"metadata":{"trusted":true},"cell_type":"code","source":"data_proc = DataProcessor()\n\n# simply preprocess without one-hot encoding or pca feats\n#X = data_proc.preprocess(data_df)\n\n# advanced preprocessing- include pca feats + one hot encoding\nX = data_proc.preprocess(data_df, one_hot_encode=True, add_pca_feats=True)\ny = data_df.loc[:, 'stroke']\n\nprint(f\"X: {X.shape} \\ny: {y.shape}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_full, X_test, y_train_full, y_test = train_test_split(X, y, test_size=0.2, random_state=12, stratify=y)\nprint(f\"X_train_full: {X_train_full.shape} \\ny_train_full: {y_train_full.shape} \\nX_val: {X_test.shape}, \\ny_val: {y_test.shape}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_full.value_counts(normalize=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test.value_counts(normalize=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As shown above, we need to ensure we stratify our split on y, since we have such an imbalanced dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size=0.2, random_state=12, stratify=y_train_full)\nprint(f\"X_train: {X_train.shape} \\ny_train_full: {y_train.shape} \\nX_val: {X_val.shape}, \\ny_val: {y_val.shape}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"model-exploration\"></a>\n## 4. Exploring our dataset with the use of different models"},{"metadata":{"trusted":true},"cell_type":"code","source":"def show_tree_graph(tree_model, feature_names):\n    \"\"\" Output a decision tree to notebook \"\"\"\n    draw_data = export_graphviz(tree_model, filled=True, \n                                rounded=True, feature_names=feature_names, \n                                out_file=None, rotate=True, class_names=True)\n    graph = graph_from_dot_data(draw_data)\n\n    return Image(graph.create_png())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_clf = RandomForestClassifier(n_estimators=100, max_depth=3)\nrf_clf.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A random forest uses a large number of individual decision trees, and averages these to create an overall model that is more robust and better performing. We can inspect the underlying decision trees within our random forest model using the estimators_ array, like so:"},{"metadata":{"trusted":true},"cell_type":"code","source":"show_tree_graph(rf_clf.estimators_[0], list(X_train.columns))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A key feature of Random Forest models is the ability to ability to compute feature importances. We can grade this feature importance in multiple ways. The popular way (used by Scikit-Learn) is to measure how much a feature reduces the impurity on average across all trees in the forest. For our model, this would equate to measuring on average how strongly each feature reduces the cost function across all our tree models."},{"metadata":{"trusted":true},"cell_type":"code","source":"def feature_importances(rf_model, dataframe):\n    \"\"\" Return dataframe of feat importances from random forest model \"\"\"\n    return pd.DataFrame({'columns' : dataframe.columns, \n                         'importance' : rf_model.feature_importances_}\n                       ).sort_values('importance', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"importances = feature_importances(rf_clf, X_train)\n\nplt.figure(figsize=(10,6))\nsns.barplot(x=\"columns\", y=\"importance\", data=importances)\nplt.ylabel(\"Feature Importances\", weight='bold')\nplt.xlabel(\"Features\", weight='bold')\nplt.title(\"Random Forest Feature Importances\", weight='bold')\nplt.xticks(rotation=90)\nplt.show()\nprint(importances)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_clf = RandomForestClassifier(n_estimators=100)\nrf_clf.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"val_preds = rf_clf.predict(X_val)\nval_acc = accuracy_score(val_preds, y_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Random Forest accuracy on validation set: {val_acc}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(val_preds, y_val))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"These metrics are hard to appreciate from the values alone, however they do highlight a severe limitation of our model. Lets plot a confusion matrix, which will help illustrate what this is."},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_confusion_matrix(true_y, pred_y, title='Confusion Matrix', figsize=(8,6)):\n    \"\"\" Custom function for plotting a confusion matrix for predicted results \"\"\"\n    conf_matrix = confusion_matrix(true_y, pred_y)\n    conf_df = pd.DataFrame(conf_matrix, columns=np.unique(true_y), index = np.unique(true_y))\n    conf_df.index.name = 'Actual'\n    conf_df.columns.name = 'Predicted'\n    plt.figure(figsize = figsize)\n    plt.title(title)\n    sns.set(font_scale=1.4)\n    sns.heatmap(conf_df, cmap=\"Blues\", annot=True, \n                annot_kws={\"size\": 16}, fmt='g')\n    plt.show()\n    return","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_confusion_matrix(y_val, val_preds)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Oh dear, our model doesnt even predict one instance correct for when our data corresponds to a stroke. This highlights the severe data imbalance problem highlighted during our exploratory data analysis earlier.\n\nThis is why accuracy is such a poor metric for imbalanced classification problems such as this one - we score 95% accuracy by simply predicting 0 everytime. Since we've made this so easy for our model with such imbalanced data, you cant blame it for taking the easy option and simply predicting 0 all the time.\n\nTo overcome this we should always make use of better metrics for such problems, such as precision, recall, f1 score, and various other available metrics."},{"metadata":{"trusted":true},"cell_type":"code","source":"def multi_model_cross_validation(clf_tuple_list, X, y, K_folds=10, score_type='accuracy', random_seed=0):\n    \"\"\" Find cross validation scores, and print and return results \"\"\"\n    \n    model_names, model_scores = [], []\n    \n    for name, model in clf_list:\n        k_fold = StratifiedKFold(n_splits=K_folds, shuffle=True, random_state=random_seed)\n        cross_val_results = cross_val_score(model, X, y, cv=k_fold, scoring=score_type, n_jobs=-1)\n        model_names.append(name)\n        model_scores.append(cross_val_results)\n        print(\"{0:<40} {1:.5f} +/- {2:.5f}\".format(name, cross_val_results.mean(), cross_val_results.std()))\n    \n    results_dict = { x : y for x in model_names for y in model_scores } \n    \n    return pd.melt(pd.DataFrame(results_dict), var_name='model', value_name='results')\n\n\ndef boxplot_comparison(melted_df, figsize=(12, 6), score_type=\"Accuracy\",\n                       title=\"Boxplot Model Results\"):\n    \"\"\" Boxplot comparison of a range of models using Seaborn and matplotlib \"\"\"\n    \n    model_names = list(melted_df['model'].unique())\n    \n    fig = plt.figure(figsize=figsize)\n    fig.suptitle(title, fontsize=18)\n    ax = fig.add_subplot(111)\n    sns.boxplot(x='model', y='results', data=melted_df)\n    ax.set_xticklabels(model_names)\n    ax.set_xlabel(\"Model\", fontsize=16) \n    ax.set_ylabel(\"Model Score ({})\".format(score_type), fontsize=16)\n    ax.set_xticklabels(ax.get_xticklabels(), rotation=60)\n    plt.show()\n    return","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# list of classifiers to compare - use some additional models this time\nclf_list = [(\"Perceptron\", Perceptron(eta0=0.1)),\n            (\"Logistic Regression\", LogisticRegression(C=10.0, max_iter=200)),\n            (\"Support Vector Machine\", SVC(kernel='linear', C=1.0)),\n            (\"Decision Tree\", DecisionTreeClassifier()),\n            (\"Random Forest\", RandomForestClassifier(n_estimators=300)),\n            (\"Ridge Classifier\", RidgeClassifier()),\n            (\"Naive Bayes Classifier\", GaussianNB()),\n            (\"Gradient Boosting\", GradientBoostingClassifier())]\n\n# calculate cross-validation scores and print / plot for each model accordingly\nresults_df = multi_model_cross_validation(clf_list, X_train_full, y_train_full, score_type='f1')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Terrible f1 scores, as expected! Lets try and improve this through some application of imbalanced data techniques."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"oversampling-data\"></a>\n## 5. Oversampling of minority features using SMOTE"},{"metadata":{"trusted":true},"cell_type":"code","source":"smote = SMOTE(sampling_strategy='minority', random_state=42)\nX_train_os, y_train_os = smote.fit_resample(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_os.value_counts().plot.bar(color=\"k\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Good, we've now got a balanced number of samples. Lets see how our models now perform, in comparison to previously."},{"metadata":{"trusted":true},"cell_type":"code","source":"# calculate cross-validation scores and print / plot for each model accordingly\nresults_df = multi_model_cross_validation(clf_list, X_train_os, y_train_os, score_type='f1')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Much better results.... HOWEVER - through performing k-folds cross validation like we have in this regard we have actually introduced a huge amount of DATA LEAKAGE.\n\nSince we've duplicated a huge number of stroke instances in the dataset, when we form training and validation splits as part of kfolds cross validation we are actually including a large number of duplicates in both training and validation partitions. This means our model can just memorise instances from the training set, and score perfectly on these same instances in the validation set. \n\nTherefore, the scores obtained above need to be taken with a massive pinch of salt! To get a much better idea of the generalisation performance of our models, we should perform manual kfolds cross validation that oversamples the training splits using SMOTE on each fold. This ensures no data leakage between our training and validation splits whilst evaluating performance."},{"metadata":{},"cell_type":"markdown","source":"We can overcome this issue partially through simply training on the oversampled training set, and making predictions on the non-oversampled validation set. If our performance on this is still terrible, it gives us an idea of how effective our models actually are.\n\nLets try the random forest model again, and evaluate our predictions on the validation split:"},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_clf = RandomForestClassifier(n_estimators=1000)\nrf_clf.fit(X_train_os, y_train_os)\nval_preds = rf_clf.predict(X_val)\nval_acc = accuracy_score(val_preds, y_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(val_preds, y_val))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_confusion_matrix(y_val, val_preds)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Still not amazing, but definitely a bit better... \n\nIn contrast to the KFolds cross validation scores obtained earlier after SMOTE oversampling, this model performs much worse. Hopefully this highlights how performing incorrect KFolds cross validation with oversampled data can provide over-optimistic estimations on how well our model will perform.\n\nInstead, we should always perform SMOTE within each fold of cross validation, rather than beforehand on the entire training split."},{"metadata":{},"cell_type":"markdown","source":"Lets reassess our importances and see if they have changed after over-sampling our data:"},{"metadata":{"trusted":true},"cell_type":"code","source":"importances = feature_importances(rf_clf, X_train_os)\n\nplt.figure(figsize=(10,6))\nsns.barplot(x=\"columns\", y=\"importance\", data=importances)\nplt.ylabel(\"Feature Importances\", weight='bold')\nplt.xlabel(\"Features\", weight='bold')\nplt.title(\"Random Forest Feature Importances\", weight='bold')\nplt.xticks(rotation=90)\nplt.show()\nprint(importances)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Age is still by far the most important feature, with BMI closely behind."},{"metadata":{"trusted":true},"cell_type":"code","source":"gb_clf = GradientBoostingClassifier()\ngb_clf.fit(X_train_os, y_train_os)\nval_preds = gb_clf.predict(X_val)\nval_acc = accuracy_score(val_preds, y_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(val_preds, y_val))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_confusion_matrix(y_val, val_preds)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Analysing the ROC AUC"},{"metadata":{},"cell_type":"markdown","source":"Much better, and still undoubtedly much more room for improvement through hyper-parameter tuning. We could do this quite simply through the use of grid-search or bayesian optimisation techniques, to find the optimal set(s) of hyper-parameters."},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_roc_curve(y_train, y_train_probs, y_val, y_val_probs, figsize=(8,8)):\n    \"\"\" Helper function to plot the ROC AUC from given labels \"\"\"\n    # obtain true positive and false positive rates for roc_auc\n    fpr, tpr, thresholds = roc_curve(y_train, y_train_probs[:, 1], pos_label=1)\n    roc_auc = auc(fpr, tpr)\n\n    # obtain true positive and false positive rates for roc_auc\n    val_fpr, val_tpr, val_thresholds = roc_curve(y_val, y_val_probs[:, 1], pos_label=1)\n    val_roc_auc = auc(val_fpr, val_tpr)\n\n    plt.figure(figsize=figsize)\n    plt.plot(fpr, tpr, label=f\"Train ROC AUC = {roc_auc}\", color='blue')\n    plt.plot(val_fpr, val_tpr, label=f\"Val ROC AUC = {val_roc_auc}\", color='red')\n    plt.plot([0,1], [0, 1], label=\"Random Guessing\", \n             linestyle=\":\", color='grey', alpha=0.6)\n    plt.plot([0, 0, 1], [0, 1, 1], label=\"Perfect Performance\", \n             linestyle=\"--\", color='black', alpha=0.6)\n    plt.xlabel(\"False Positive Rate\")\n    plt.ylabel(\"True Positive Rate\")\n    plt.title(\"Receiver Operating Characteristic\", weight='bold')\n    plt.legend(loc='best')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# obtain prediction probabilities for trg and val\ny_val_probs = gb_clf.predict_proba(X_val)\ny_trg_probs = gb_clf.predict_proba(X_train_os)\n\n# plot our ROC curve\nplot_roc_curve(y_train_os, y_trg_probs, y_val, y_val_probs)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Not too shabby, but we could certainly improve this further before settling on a final model.\n\nLets also see how our precision recall curve looks using these predictions on the validation set:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_prec_rec_curve(y_train, y_train_probs, y_val, y_val_probs, figsize=(14,6)):\n    \"\"\" Helper function to plot the ROC AUC from given labels \"\"\"\n    # obtain true positive and false positive rates for roc_auc\n    prec, rec, thresholds = precision_recall_curve(y_train, \n                                                   y_train_probs[:, 1], \n                                                   pos_label=1)\n    prec_rec_auc = auc(rec, prec)\n\n    # obtain true positive and false positive rates for roc_auc\n    val_prec, val_rec, val_thresholds = precision_recall_curve(y_val, \n                                                               y_val_probs[:, 1], \n                                                               pos_label=1)\n    val_prec_rec_auc = auc(val_rec, val_prec)\n\n    plt.figure(figsize=figsize)\n    plt.plot(prec, rec, \n             label=f\"Train Precision-Recall AUC = {prec_rec_auc}\", color='blue')\n    plt.plot(val_prec, val_rec, \n             label=f\"Val Precision-Recall AUC = {val_prec_rec_auc}\", color='red')\n    plt.plot([0,1], [0, 1], label=\"Random Guessing\", \n             linestyle=\":\", color='grey', alpha=0.6)\n    plt.plot([0, 0, 1], [0, 1, 1], label=\"Perfect Performance\", \n             linestyle=\"--\", color='black', alpha=0.6)\n    plt.xlabel(\"Recall\")\n    plt.ylabel(\"Precision\")\n    plt.title(\"Precision-Recall Curve\", weight='bold')\n    plt.legend(loc='best')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot our precision recall curve\nplot_prec_rec_curve(y_train_os, y_trg_probs, y_val, y_val_probs)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All things considered, the model performs terribly in terms of recall. Even through adjustment of thresholds, there is no easy way to increase its performance in reducing false negatives. We can only achieve a high recall by completely diminishing precision, which is not ideal and eliminates any intelligence from our model.\n\nIt seems to improve this we need to significantly tune the model, or choose a more appropriate model for this problem."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"test-predictions\"></a>\n## 6. Predictions on the test sets "},{"metadata":{"trusted":true},"cell_type":"code","source":"test_preds = gb_clf.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(test_preds, y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_confusion_matrix(y_test, test_preds)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Overall, our model is still poor in terms of recall, which we could improve further through modification of our thresholds for predictions. Since we would much rather spot someone who might have a stroke (minimise false negatives) rather than trying to avoid inadvertently classifying a healthy person as a risk of stroke (minimising false positives), we should prioritise our recall, rather than precision.\n\nWe can do this type of analysis better through the use of precision-recall curves and Reciever Operator Characteristic Area Under the Curve (ROC-AUC)."},{"metadata":{"trusted":true},"cell_type":"code","source":"# obtain prediction probabilities for trg and val\ny_test_probs = gb_clf.predict_proba(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_roc_curve(y_train_os, y_trg_probs, y_test, y_test_probs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_prec_rec_curve(y_train_os, y_trg_probs, y_test, y_test_probs)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Well, not too shabby... but then again, definitely room for improvement. There are a huge range of additional techniques we can apply to improve this performance for imbalanced binary classification, but we'll leave these for another time.\n\nMany thanks for reading through this notebook! I hope you enjoyed and found some use from it!"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}