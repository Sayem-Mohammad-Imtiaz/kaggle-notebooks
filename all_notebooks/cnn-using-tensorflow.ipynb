{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report,confusion_matrix\nfrom sklearn.preprocessing import OneHotEncoder\nimport tensorflow.compat.v1 as tf\ntf.compat.v1.disable_v2_behavior()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Loading Data**"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = pd.read_csv('/kaggle/input/sign-language-mnist/sign_mnist_train.csv')\ntest_data = pd.read_csv('/kaggle/input/sign-language-mnist/sign_mnist_test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.shape, test_data.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Data Visualisation**"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,10))\nsns.countplot(train_data['label'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Classes that we have to predict are pretty balanced in data."},{"metadata":{},"cell_type":"markdown","source":"**Previewing Few Images**"},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(2,5) \nf.set_size_inches(10, 10)\nk = 0\nfor i in range(2):\n    for j in range(5):\n        ax[i,j].imshow(x_train[k].reshape(28, 28) , cmap = \"gray\")\n        k += 1\n    plt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Data Preprocessing**"},{"metadata":{"trusted":true},"cell_type":"code","source":"encoder = OneHotEncoder() # encoding target variable i.e. label\n\nx_train = (train_data.iloc[:, 1:]/255).values # normalizing train images to standard 0-1 pixel values.\ny_train = encoder.fit_transform(train_data['label'].values.reshape(-1,1)).toarray()\n\nx_test = (test_data.iloc[:, 1:]/255).values # normalizing test images to standard 0-1 pixel values.\ny_test = encoder.fit_transform(test_data['label'].values.reshape(-1,1)).toarray()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train.shape, y_train.shape, x_test.shape, y_test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Initializing Weights And Units In Layers**"},{"metadata":{"trusted":true},"cell_type":"code","source":"input_width = 28\ninput_height = 28\ninput_channel = 1\ninput_pixels = 784\n\nn_conv1 = 64\nn_conv2 = 128\nstride_conv1 = 1\nstride_conv2 = 1\nfilter1_k = 5\nfilter2_k = 5\nmaxpool1_k = 2\nmaxpool2_k = 2\n\nn_hidden = 1024\nn_out = 24\n\ninput_size_to_hidden_layer = ((input_width//(maxpool1_k*maxpool2_k)) * (input_height//(maxpool1_k*maxpool2_k)) * n_conv2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"weights = {\n    'wc1' : tf.Variable(tf.random_normal([filter1_k, filter1_k, input_channel, n_conv1])), # weight corresponding to convolutional layer1.\n    'wc2' : tf.Variable(tf.random_normal([filter2_k, filter2_k, n_conv1, n_conv2])), # weight corresponding to convolutional layer2.\n    'wh' : tf.Variable(tf.random_normal([input_size_to_hidden_layer, n_hidden])),  # weight corresponding to hidden layer.\n    'wo' : tf.Variable(tf.random_normal([n_hidden, n_out])) # weight corresponding to output layer.\n}\n\nbiases = {\n    'bc1' : tf.Variable(tf.random_normal([n_conv1])), # biases corresponding to convolutional layer1.\n    'bc2' : tf.Variable(tf.random_normal([n_conv2])), # biases corresponding to convolutional layer2.\n    'bh' : tf.Variable(tf.random_normal([n_hidden])), # biases corresponding to hidden layer.\n    'bo' : tf.Variable(tf.random_normal([n_out])) # biases corresponding to output layer.\n}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Forward Propagation**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# function to get the output from a convolutional layer.\ndef conv(x, weights, bias, stride = 1):\n    output = tf.nn.conv2d(x, weights, padding='SAME', strides=[1, stride, stride, 1])\n    output = tf.nn.bias_add(output, bias)\n    output = tf.nn.relu(output) # applying activation function.\n    return output","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# function which return output of pooling layer used to decrease image size so that we have to train less weights and biases.\ndef maxpooling(x, k):\n    return tf.nn.max_pool(x, padding='SAME', ksize=[1, k, k, 1], strides=[1, k, k, 1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def forward_propagation(x, weights, biases):\n    x = tf.reshape(x, shape = [-1, input_width, input_height, input_channel])\n    \n    conv1 = conv(x, weights['wc1'], biases['bc1'], stride_conv1)\n    conv1_pool = maxpooling(conv1, maxpool1_k)\n    \n    conv2 = conv(conv1_pool, weights['wc2'], biases['bc2'], stride_conv2)\n    conv2_pool = maxpooling(conv2, maxpool2_k)\n    \n    hidden_layer_input = tf.reshape(conv2_pool, shape = [-1, input_size_to_hidden_layer])\n    hidden_layer_output = tf.nn.relu(tf.add(tf.matmul(hidden_layer_input, weights['wh']), biases['bh']))\n    \n    output = tf.add(tf.matmul(hidden_layer_output, weights['wo']), biases['bo'])\n    return output","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Defining Variables**"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = tf.placeholder(tf.float32, [ None, input_pixels], name='x')\nY = tf.placeholder(tf.int32, [ None, n_out], name='y')\npred = forward_propagation(X, weights, biases)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=pred, labels=Y))\n\n# using adam optimizer on the cost.\noptimizer = tf.train.AdamOptimizer(learning_rate=0.011)\noptimize = optimizer.minimize(cost)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# creating a new session of tensorflow.\nsess = tf.Session()\nsess.run(tf.global_variables_initializer())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Batch Gradient Descent To Train**"},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size=64\na = 0\nfor i in range(10):\n    num_batches = int(len(x_train)/batch_size)\n    total_cost = 0\n    for j in range(num_batches):\n        batch_x = x_train[a: a+batch_size]\n        batch_y = y_train[a: a+batch_size]\n        c, _ = sess.run([cost, optimize], feed_dict={X:batch_x, Y:batch_y})\n        total_cost += c\n        a += batch_size\n    a = 0\n    print('total cost at',i+1,'iteration:',total_cost)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Testing Our Model**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# testing model with training data.\npredictions = tf.argmax(pred, axis=1)\ncorrect_labels = tf.argmax(Y, axis=1)\naccuracy = tf.equal(predictions, correct_labels)\npredictions, labels, accuracy = sess.run([predictions, correct_labels, accuracy], feed_dict={X:x_train, Y:y_train})\naccuracy.sum()/len(x_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# testing model with testing data.\npredictions = tf.argmax(pred, axis=1)\ncorrect_labels = tf.argmax(Y, axis=1)\naccuracy = tf.equal(predictions, correct_labels)\npredictions, labels, accuracy = sess.run([predictions, correct_labels, accuracy], feed_dict={X:x_test, Y:y_test})\naccuracy.sum()/len(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}