{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"demo = pd.read_csv(\"/kaggle/input/pima-indians-diabetes-database/diabetes.csv\")\ndemo.head()","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from matplotlib import pyplot as plt\nimport seaborn as sns\nsns.pairplot(demo,hue=\"Outcome\",diag_kind='kde')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Feature Selection\nimport warnings\nwarnings.filterwarnings('ignore')\nX=demo.drop('Outcome',axis=1)\ny=demo[['Outcome']]\nfrom sklearn.ensemble import GradientBoostingClassifier\ngb = GradientBoostingClassifier()\ngb.fit(X,y)\ngb.feature_importances_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_selection import SelectKBest,f_classif\nskb = SelectKBest(f_classif,8)\nskb.fit_transform(X,y)\nskb.pvalues_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plasdiabetic = demo[demo['Outcome']==1]['Glucose']\nplasnondiabetic = demo[demo['Outcome']==0]['Glucose']\nimport scipy.stats as stats\nprint(stats.ttest_ind(plasdiabetic,plasnondiabetic))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"demo.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"demo.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import PowerTransformer\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20,random_state=20)\n#With Pipeline\nfrom sklearn.pipeline import Pipeline\npipe = Pipeline((\n(\"pt\",PowerTransformer()),\n(\"lr\", LogisticRegression()),\n))\npipe.fit(X_train,y_train)\nprint(\"Testing Accuracy\")\nprint(pipe.score(X_test,y_test))\nprint(\"Training Accuracy\")\nprint(pipe.score(X_train,y_train))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pipe.named_steps['lr'].coef_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"demo = pd.read_csv(\"/kaggle/input/pima-indians-diabetes-database/diabetes.csv\")\ndemo['Glucose'].replace(0,np.nan,inplace=True)\ndemo['BloodPressure'].replace(0,np.nan,inplace=True)\ndemo['SkinThickness'].replace(0,np.nan,inplace=True)\ndemo['Insulin'].replace(0,np.nan,inplace=True)\ndemo['BMI'].replace(0,np.nan,inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"demo.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Missing Values with Iterative Imputer\nfrom sklearn.experimental import enable_iterative_imputer  # noqa\nfrom sklearn.impute import IterativeImputer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler,MinMaxScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import PowerTransformer\nX=demo.drop('Outcome',axis=1)\ny=demo['Outcome']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20,random_state=20)\nit=IterativeImputer()\nitXtrain = pd.DataFrame(it.fit_transform(X_train))\nitXtest = pd.DataFrame(it.transform(X_test))\npt = PowerTransformer()\npowerxtrain = pt.fit_transform(itXtrain)\npowerxtest = pt.transform(itXtest)\nsc=StandardScaler()\nscaledxtrain = sc.fit_transform(powerxtrain)\nscaledxtest = sc.transform(powerxtest)\nlr = LogisticRegression()\nlr.fit(scaledxtrain,y_train)\nprint(\"What is the Testing Accuracy\")\nprint(lr.score(scaledxtest,y_test))\nprint(\"What is the Training Accuracy\")\nprint(lr.score(scaledxtrain,y_train))","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"demo=demo.drop('Insulin',axis=1)\ndemo.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#With Pipeline\nX=demo.drop('Outcome',axis=1)\ny=demo['Outcome']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20,random_state=20)\nfrom sklearn.pipeline import Pipeline\npipe = Pipeline((\n(\"it\", IterativeImputer()),\n(\"pt\",PowerTransformer()),\n(\"sc\", StandardScaler()),\n(\"lr\", LogisticRegression()),\n))\npipe.fit(X_train,y_train)\nprint(\"Testing Accuracy\")\nprint(pipe.score(X_test,y_test))\nprint(\"Training Accuracy\")\nprint(pipe.score(X_train,y_train))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Including RFE\n#With Pipeline\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.feature_selection import RFE\npipe = Pipeline((\n(\"it\", IterativeImputer()),\n(\"pt\",PowerTransformer()),\n(\"sc\", StandardScaler()),\n(\"fs\",RFE(estimator = LogisticRegression(),n_features_to_select=3, step=1)),\n(\"lr\", LogisticRegression()),\n))\npipe.fit(X_train,y_train)\nprint(\"Testing Accuracy\")\nprint(pipe.score(X_test,y_test))\nprint(\"Training Accuracy\")\nprint(pipe.score(X_train,y_train))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pipe.named_steps['lr'].coef_","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Including SelectKBest\n#With Pipeline\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_classif\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.feature_selection import RFE\npipe = Pipeline((\n(\"it\", IterativeImputer()),\n(\"pt\",PowerTransformer()),\n(\"sc\", StandardScaler()),\n(\"skb\",SelectKBest(f_classif,k=3)),\n(\"lr\", LogisticRegression()),\n))\npipe.fit(X_train,y_train)\nprint(\"Testing Accuracy\")\nprint(pipe.score(X_test,y_test))\nprint(\"Training Accuracy\")\nprint(pipe.score(X_train,y_train))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predicted = pipe.predict(X_test)\nfrom sklearn.metrics import confusion_matrix,classification_report,recall_score,precision_score,f1_score\nprint(confusion_matrix(y_test,predicted))\nprint(classification_report(y_test,predicted))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(recall_score(y_test,predicted,average=None))\nprint(precision_score(y_test,predicted,average=None))\nprint(f1_score(y_test,predicted,average=None))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f1_score(y_test,predicted,average=None)[1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Evaluating models using Cross Validation\nfrom sklearn.model_selection import cross_val_score\nscoreslr = cross_val_score(pipe, X_train, y_train, cv=10, scoring='accuracy')\nprint(scoreslr)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nprint(\"Average Accuracy of my model\")\nprint(np.mean(scoreslr))\nprint(\"SD of accuracy of the model\")\nprint(np.std(scoreslr,ddof=1))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 95% Confidence Interval of Accuracy\nimport scipy.stats\nxbar = np.mean(scoreslr)\nn=10\ns = np.std(scoreslr,ddof=1)\nse = s/np.sqrt(n)\nstats.t.interval(0.95,df=n-1,loc=xbar,scale=se)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Learning Curve Demo\ndemo = pd.read_csv(\"/kaggle/input/pima-indians-diabetes-database/diabetes.csv\")\nprint(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.model_selection import ShuffleSplit\n\n\ndef plot_learning_curve(estimator, title, X, y, axes=None, ylim=None, cv=None,\n                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n    \n    if axes is None:\n        _, axes = plt.subplots(1, 3, figsize=(20, 5))\n\n    axes[0].set_title(title)\n    if ylim is not None:\n        axes[0].set_ylim(*ylim)\n    axes[0].set_xlabel(\"Training examples\")\n    axes[0].set_ylabel(\"Score\")\n\n    train_sizes, train_scores, test_scores, fit_times, _ = \\\n        learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs,\n                       train_sizes=train_sizes,\n                       return_times=True)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    fit_times_mean = np.mean(fit_times, axis=1)\n    fit_times_std = np.std(fit_times, axis=1)\n\n    # Plot learning curve\n    axes[0].grid()\n    axes[0].fill_between(train_sizes, train_scores_mean - train_scores_std,\n                         train_scores_mean + train_scores_std, alpha=0.1,\n                         color=\"r\")\n    axes[0].fill_between(train_sizes, test_scores_mean - test_scores_std,\n                         test_scores_mean + test_scores_std, alpha=0.1,\n                         color=\"g\")\n    axes[0].plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n                 label=\"Training score\")\n    axes[0].plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n                 label=\"Cross-validation score\")\n    axes[0].legend(loc=\"best\")\n\n    # Plot n_samples vs fit_times\n    axes[1].grid()\n    axes[1].plot(train_sizes, fit_times_mean, 'o-')\n    axes[1].fill_between(train_sizes, fit_times_mean - fit_times_std,\n                         fit_times_mean + fit_times_std, alpha=0.1)\n    axes[1].set_xlabel(\"Training examples\")\n    axes[1].set_ylabel(\"fit_times\")\n    axes[1].set_title(\"Scalability of the model\")\n\n    # Plot fit_time vs score\n    axes[2].grid()\n    axes[2].plot(fit_times_mean, test_scores_mean, 'o-')\n    axes[2].fill_between(fit_times_mean, test_scores_mean - test_scores_std,\n                         test_scores_mean + test_scores_std, alpha=0.1)\n    axes[2].set_xlabel(\"fit_times\")\n    axes[2].set_ylabel(\"Score\")\n    axes[2].set_title(\"Performance of the model\")\n\n    return plt\n\n\nfig, axes = plt.subplots(3, 2, figsize=(10, 15))\n\nX=demo.drop('Outcome',axis=1)\ny=demo['Outcome']\n\ntitle = \"Learning Curves (GradientBoosting)\"\n# Cross validation with 10 iterations to get smoother mean test and train\n# score curves, each time with 20% data randomly selected as a validation set.\ncv = ShuffleSplit(n_splits=50, test_size=0.2, random_state=0)\n\nestimator = GradientBoostingClassifier()\nplot_learning_curve(estimator, title, X, y, axes=axes[:, 0], ylim=(0.7, 1.01),\n                    cv=cv, n_jobs=4)\n\ntitle = r\"Learning Curves (LogisticRegression)\"\n# SVC is more expensive so we do a lower number of CV iterations:\ncv = ShuffleSplit(n_splits=50, test_size=0.2, random_state=0)\nestimator = LogisticRegression()\nplot_learning_curve(estimator, title, X, y, axes=axes[:, 1], ylim=(0.7, 1.01),\n                    cv=cv, n_jobs=4)\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}