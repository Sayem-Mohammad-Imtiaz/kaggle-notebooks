{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Import libraries"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns \n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Analysis / EDA"},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.options.display.float_format = '{:,.2f}'.format\n\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"../input/breast-cancer-wisconsin-data/data.csv\", delimiter=',')\ndf.drop(['Unnamed: 32'], axis = 1, inplace = True)  #unnecessary column\ndf.head(10)   # Showing first 10 rows ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()   # Give information about dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = sns.countplot(df.diagnosis,label=\"Count\") \nB, M = df.diagnosis.value_counts()\nprint('Number of Benign: ',B)\nprint('Number of Malignant : ',M)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_features = [\"radius_mean\", \"texture_mean\", \"perimeter_mean\", \"area_mean\", \"smoothness_mean\", \"compactness_mean\", \"concavity_mean\", \n                 \"symmetry_mean\", \"fractal_dimension_mean\"]\n\nfor column in mean_features:\n    plt.figure(figsize = (10,5))\n    feature = df[column]\n    sns.distplot(feature, hist=True, kde=True, \n                    bins=int(180/5), color = 'blue',\n                     hist_kws={'edgecolor':'black'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.subplots(figsize=(10,8))\nsns.heatmap(df[mean_features].corr(),cmap='coolwarm',annot=True);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"se_features = [\"radius_se\", \"texture_se\", \"perimeter_se\", \"area_se\", \"smoothness_se\", \"compactness_se\", \"concavity_se\", \n                 \"symmetry_se\", \"fractal_dimension_se\"]\n\nfor column in se_features:\n    plt.figure(figsize = (10,5))\n    feature = df[column]\n    sns.distplot(feature, hist=True, kde=True, \n                    bins=int(180/5), color = 'red',\n                     hist_kws={'edgecolor':'black'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.subplots(figsize=(10,8))\nsns.heatmap(df[se_features].corr(),cmap='coolwarm',annot=True);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"worst_features = [\"radius_worst\", \"texture_worst\", \"perimeter_worst\", \"area_worst\", \"smoothness_worst\", \"compactness_worst\", \"concavity_worst\", \n                 \"symmetry_worst\", \"fractal_dimension_worst\"]\n\nfor column in worst_features:\n    plt.figure(figsize = (10,5))\n    feature = df[column]\n    sns.distplot(feature, hist=True, kde=True, \n                    bins=int(180/5), color = 'yellow',\n                     hist_kws={'edgecolor':'black'});\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.subplots(figsize=(10,8))\nsns.heatmap(df[worst_features].corr(),cmap='coolwarm',annot=True);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for x_column in mean_features:\n    for y_cloumn in se_features:\n        sns.jointplot(x= x_column,y=y_cloumn,data=df,kind='scatter');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for x_column in mean_features:\n    for y_cloumn in worst_features:\n        sns.jointplot(x= x_column,y=y_cloumn,data=df,kind='kde');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for x_column in se_features:\n    for y_cloumn in worst_features:\n        sns.jointplot(x= x_column,y=y_cloumn,data=df,kind='hex');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.concat([df.diagnosis,df[mean_features]],axis=1)\ndata = pd.melt(data,id_vars=\"diagnosis\",\n                    var_name=\"features\",\n                    value_name='value')\nplt.figure(figsize=(20,10))\nsns.swarmplot(x=\"features\", y=\"value\", hue=\"diagnosis\", data=data);\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.concat([df.diagnosis,df[se_features]],axis=1)\ndata = pd.melt(data,id_vars=\"diagnosis\",\n                    var_name=\"features\",\n                    value_name='value')\nplt.figure(figsize=(20,10))\nsns.swarmplot(x=\"features\", y=\"value\", hue=\"diagnosis\", data=data);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.concat([df.diagnosis,df[worst_features]],axis=1)\ndata = pd.melt(data,id_vars=\"diagnosis\",\n                    var_name=\"features\",\n                    value_name='value')\nplt.figure(figsize=(20,10))\nsns.swarmplot(x=\"features\", y=\"value\", hue=\"diagnosis\", data=data);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We have target feature = 'diagnosis' in order to classification so we split this feature\n\ny = df.diagnosis \nx = df.drop('diagnosis', axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = sns.countplot(y,label=\"Count\") \nB, M = y.value_counts()\nprint('Number of Benign: ',B)\nprint('Number of Malignant : ',M)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = df.diagnosis.values\nx_df = df.drop([\"diagnosis\"],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# X Dataframe should be normalized to avoid dominance among numerical values because it has several features and \n#model success becomes more realistic if numbers are drawn between 0-1.\n\nx = (x_df - np.min(x_df))/(np.max(x_df)-np.min(x_df)).values           # Formula of normalization","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Train and test split "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.2,random_state=42)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"x_train: \",x_train.shape)\nprint(\"x_test: \",x_test.shape)\nprint(\"y_train: \",y_train.shape)\nprint(\"y_test: \",y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Transforming arrays to transpoze in order to avoid getting shape error\nx_train = x_train.T\nx_test = x_test.T\ny_train = y_train.T\ny_test = y_test.T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"x_train: \",x_train.shape)\nprint(\"x_test: \",x_test.shape)\nprint(\"y_train: \",y_train.shape)\nprint(\"y_test: \",y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"name_list = ['Logistic Regression', 'Knn(n=8)', 'SVM', 'Native Bayes','Decision Tree', 'Random Foerest']\npred_score = []    # In order to compare all pred scores","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1. First Way to Logistic Regresion Using by Deep Learning Formulas"},{"metadata":{"trusted":true},"cell_type":"code","source":"# We determite to parameter initialize values for sigmoid function \n# Giving dimension value to funciton and change weight and bias values each step\n\ndef change_weights_and_bias(dimension):\n    \n    w = np.full((dimension,1),0.01)\n    b = 0.0\n    return w,b\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating sigmoid function \n\ndef sigmoid(z):\n    \n    y_head = 1/(1+ np.exp(-z))    # Sigmoid function formula\n    \n    return y_head\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Making forward and backward iteration formula functions and changing values each step by step\n# We need x_train, y_train and z, loss, cost formula \n# Backward interation contains derivate weight and bias values step by step and return this values with gradients dictionary \n# Evaluating cost and gradients value after these interations\n\ndef forward_backward(w,b,x_train,y_train):\n    \n    z = np.dot(w.T,x_train) + b   #  Sigmoid function formula\n    y_head = sigmoid(z)           # Creating pred values to return sigmoid funciton\n    \n    loss = ((-1)* y_train * np.log(y_head)) + ((-1) * (1 - y_train) * np.log(1- y_head))    # Evaluating loss value within this formula \n    \n    #loss = -y_train*np.log(y_head)-(1-y_train)*np.log(1-y_head)\n    \n    cost = (np.sum(loss))/x_train.shape[1]                         # Evaluating cost value within this formula \n\n\n    derivative_weight = (np.dot(x_train,((y_head-y_train).T)))/x_train.shape[1] \n    derivative_bias = np.sum(y_head-y_train)/x_train.shape[1]\n    \n    gradients = {\"derivative_weight\": derivative_weight, \"derivative_bias\": derivative_bias}\n    \n    return cost,gradients\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Each step, updating learning parametres automatically\n# Giving learning rate and number of interation value because they are hyperparametre\n# Saving cost values in cost_list after confuse cost values step by step\n\ndef update(w, b, x_train, y_train, learning_rate, number_iteration):\n    \n    cost_list = []\n    \n    for i in range(number_iteration):\n        \n        cost,gradients = forward_backward(w,b,x_train,y_train)   # Calling previous function\n        cost_list.append(cost)\n        \n        w = w - learning_rate * gradients[\"derivative_weight\"]\n        b = b - learning_rate * gradients[\"derivative_bias\"]\n        \n        # Updating values \n        \n    parameters = {\"weight\": w,\"bias\": b}\n    \n    # Show all of them\n    \n    plt.plot(index,cost_list)\n    plt.xticks(index,rotation='vertical')\n    plt.xlabel(\"Number of Iteration\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n    \n    return parameters, gradients, cost_list\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating predict values and need weight, bias and x_test value\n# x_test is a input for forward iteration\n# Using sigmoid functions : if z is bigger than 0.5, y_head = 1 or if z is smaller than 0.5, y_head = 0 \n\ndef predict(w,b,x_test):\n\n    z = sigmoid(np.dot(w.T,x_test)+b)\n    y_prediction = np.zeros((1,x_test.shape[1])),\n    for i in range(z.shape[1]):\n        if z[0,i]<= 0.5:\n            y_prediction[0,i] = 0\n        else:\n            y_prediction[0,i] = 1\n\n    return y_prediction","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We use all funcitons for logistic regression \n# We need x and y splits, learning rate and number of iterations\n# Initializing value of dimension is x_train.shape[0] \n\ndef logistic_regression(x_train, y_train, x_test, y_test, learning_rate, number_iterations):\n    # initialize\n    dimension =  x_train.shape[0]  # that is 30\n    w,b = change_weights_and_bias(dimension)     # Calling change of this values funciton\n    \n    parameters, gradients, cost_list = update(w, b, x_train, y_train, learning_rate,number_iterations)  # Calling update function\n    \n    y_prediction_test = predict(parameters[\"weight\"],parameters[\"bias\"],x_test)  # Calling predict function\n    \n    # Print accuracy value \n    \n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Then use logistic regression funciton for example learning rate is 1 and number of iterations are 250\n\n\n# logistic_regression(x_train, y_train, x_test, y_test,learning_rate = 1, number_iterations = 250) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Second Way Logistic Regression with SKLearn"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr.fit(x_train.T,y_train.T)\n\nprint(\"accuracy of logistic regression is:  {}\".format(lr.score(x_test.T,y_test.T)))\n\npred_score.append(lr.score(x_test.T,y_test.T))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. KNN Algorithm and Visualization"},{"metadata":{},"cell_type":"markdown","source":"##### We change a lo of things on datasets so again importing original version dataset and using KNN algorithm"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"../input/breast-cancer-wisconsin-data/data.csv\", delimiter=',')\ndf.drop([\"id\",\"Unnamed: 32\"],axis=1,inplace=True)\n\ndf.tail(10)   # Controlling last 10 rows","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Target Feature is diagnosis wthin M and B class\n\nM = df[df.diagnosis == \"M\"]\nB = df[df.diagnosis == \"B\"]\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(M.perimeter_mean,M.texture_mean,color=\"blue\",label=\"M class diagnosis\")\nplt.scatter(B.radius_mean,B.texture_mean,color=\"red\",label=\"B class diagnosis\")\n\nplt.xlabel(\"radius_mean\")\nplt.ylabel(\"perimeter_mean\")\nplt.legend()\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(M.radius_mean,M.area_mean,color=\"green\",label=\"M class diagnosis\")\nplt.scatter(B.radius_mean,B.area_mean,color=\"yellow\",label=\"B class diagnosis\")\n\nplt.xlabel(\"radius_mean\")\nplt.ylabel(\"area_mean\")\nplt.legend()\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(M.smoothness_mean,M.compactness_mean,color=\"cyan\",label=\"M class diagnosis\")\nplt.scatter(B.smoothness_mean,B.compactness_mean,color=\"black\",label=\"B class diagnosis\")\n\nplt.xlabel(\"smoothness_mean\")\nplt.ylabel(\"compactness_mean\")\nplt.legend()\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.jointplot(x='smoothness_mean',y='compactness_mean',data=df,kind='scatter');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.jointplot(x='radius_mean',y='perimeter_mean',data=df,kind='scatter');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(df.corr());\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We assign 1 value if diagnosis is M and we assing 0 value if diagnosis is B  \n\ndf.diagnosis = [1 if each == \"M\" else 0 for each in df.diagnosis]\n\ny = df.diagnosis.values\nx_df = df.drop([\"diagnosis\"],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# normalization \nx = (x_df - np.min(x_df))/(np.max(x_df)-np.min(x_df))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train and test split\n\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Using KNN Model\n\nfrom sklearn.neighbors import KNeighborsClassifier\nn_neighbors = 5  # as an example\n\nknn = KNeighborsClassifier(n_neighbors = n_neighbors) # n_neighbors = k\n\nknn.fit(x_train,y_train)\n\nprediction = knn.predict(x_test)\nprint(\" {} nn score: {} \".format(n_neighbors, knn.score(x_test,y_test)))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Using KNN Model\n\nn_neighbors = 8  # as an example\n\nknn = KNeighborsClassifier(n_neighbors = n_neighbors) # n_neighbors = k\n\nknn.fit(x_train,y_train)\n\nprediction = knn.predict(x_test)\nprint(\" {} nn score: {} \".format(n_neighbors, knn.score(x_test,y_test)))\n\npred_score.append(knn.score(x_test,y_test))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Finding optimal  k value\n\nscore_list = []\n\nfor each in range(1,15):\n    knn2 = KNeighborsClassifier(n_neighbors = each)\n    knn2.fit(x_train,y_train)\n    score_list.append(knn2.score(x_test,y_test))\n    \nplt.plot(range(1,15),score_list)\nplt.xlabel(\"k values\")\nplt.ylabel(\"accuracy\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#####  As you seen, optimal k values to 6 from 12. \n"},{"metadata":{},"cell_type":"markdown","source":"## 4. Support Vector Machines "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\n \nsvm = SVC(random_state = 1)\nsvm.fit(x_train,y_train)\n \nprint(\"accuracy of svm value is: \",svm.score(x_test,y_test))\n\npred_score.append(svm.score(x_test,y_test))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5. Native Bayes Classification"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\nnb.fit(x_train,y_train)\n \nprint(\"accuracy of naive bayes : \",nb.score(x_test,y_test))\n\npred_score.append(nb.score(x_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 6. Decision Tree"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\ndt = DecisionTreeClassifier()\ndt.fit(x_train,y_train)\n\nprint(\"accuracy of decision tree: \", dt.score(x_test,y_test))\n\npred_score.append(dt.score(x_test,y_test))\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 7. Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nn_estimators = 100  # as an example number of trees\n\nrf = RandomForestClassifier(n_estimators = n_estimators,random_state = 1)\n\nrf.fit(x_train,y_train)\n\nprint(\"accuracy of random forest (100): \",rf.score(x_test,y_test))\n\npred_score.append(rf.score(x_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Evaluating Confusion Matrix"},{"metadata":{},"cell_type":"markdown","source":"##### As an example Random Forests predictions values"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = rf.predict(x_test)\ny_true = y_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_true,y_pred)\n\n# visualization with heatmap\nf, ax = plt.subplots(figsize =(5,5))\nsns.heatmap(cm,annot = True,linewidths=0.5,fmt = \".0f\",ax = ax)\n\nplt.xlabel(\"y_pred\")\nplt.ylabel(\"y_true\")\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy = pd.DataFrame({'algorithmas' : name_list, 'accuracy_value': pred_score})\n\nplt.figure(figsize=(12,6))\n\nplt.plot(accuracy.algorithmas,accuracy.accuracy_value)\n\nplt.title(\"Comparison of Accuracy Values\")\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}