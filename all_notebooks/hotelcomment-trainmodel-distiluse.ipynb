{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Import Libraries**","metadata":{}},{"cell_type":"code","source":"import os\nimport gc\nimport sys\nimport time\nimport shutil\n\nimport random\nimport pickle\n\nfrom tqdm import tqdm as print_progress\nfrom glob import glob\n\nimport dask.dataframe as dd\nimport pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nfrom IPython.display import display, HTML","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.environ['TF_KERAS'] = '1'\n\nimport tensorflow as tf\n\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.utils import Sequence\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.metrics import TopKCategoricalAccuracy\nfrom tensorflow.keras.losses import categorical_crossentropy\nfrom tensorflow.keras.callbacks import EarlyStopping, TensorBoard, ModelCheckpoint, ReduceLROnPlateau, LearningRateScheduler, Callback\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.regularizers import l2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.layers import (\n    Layer, \n    Input, InputLayer, Embedding, \n    Dropout, Dense, \n    Dot, Concatenate, Average, Add,\n    Bidirectional, LSTM,\n    Lambda, Reshape\n)\nfrom tensorflow.keras.activations import softmax, sigmoid\nfrom tensorflow.keras.initializers import Identity, GlorotNormal\nfrom tensorflow.keras.utils import plot_model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # detect and init the TPU\n# tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n# tf.config.experimental_connect_to_cluster(tpu)\n# tf.tpu.experimental.initialize_tpu_system(tpu)\n\n# # instantiate a distribution strategy\n# tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install stellargraph","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install gradient-centralization-tf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Load data**","metadata":{}},{"cell_type":"code","source":"datasets_path = '../input/hotel-comment-splitted'\nsample_dfs = dict()\nfor dataset in ['training', 'valuating', 'testing']:\n    print(f'\\n\\n\\nProcessing {dataset} ...')\n    sample_dfs[dataset] = dd.read_csv(\n        os.path.join(datasets_path, f'{dataset}_data*.csv')).compute()\n    print(f\"{dataset}-set contains {len(sample_dfs[dataset])} samples\")\n    print(sample_dfs[dataset].sample(n=3))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"filename = os.path.join(datasets_path, 'label_encoder.pkl')\nlabel_encoder = pickle.load(open(filename, 'rb'))\nlabels = list(label_encoder.classes_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Pretrained Sentence-Transformer**","metadata":{}},{"cell_type":"code","source":"pip install sentence-transformers","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer\n\nmodel_version = '../input/sentence-transformers/distilUSE'\nembedder = SentenceTransformer(model_version)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels_vector = embedder.encode(labels, convert_to_numpy=True, output_value='token_embeddings')\nlabels_vector = [np.mean(l.cpu().numpy(), axis=0) for l in labels_vector]\nlabels_matrix = np.vstack(labels_vector)\nlabels_matrix = np.expand_dims(labels_matrix, axis=0)\n# np.save(os.path.join(datasets_path, 'labels_embeddings.npy'), labels_matrix)\nlabels_matrix.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Data Generator**","metadata":{}},{"cell_type":"code","source":"import sklearn\nfrom ast import literal_eval\nfrom tensorflow.keras.utils import Sequence, to_categorical\n\nclass DataGenerator(Sequence):\n\n    def __init__(self, data_df: pd.DataFrame,\n                       word_embedder,\n                       labels_fixed: np.array,\n                       batch_size: int = 64, \n                       shuffle: bool = True):\n        self.data_df = data_df\n        if len(labels_fixed.shape) == 2:\n            labels_fixed = np.expand_dims(labels_fixed, axis=0)\n        elif len(labels_fixed.shape) != 3:\n            raise ValueError(\"Shape of `labels_fixed` must be 2D or 3D\")\n        self.labels_fixed = labels_fixed\n        self.word_embedder = word_embedder\n        self.embedding_dim = list(word_embedder.parameters())[-2].shape[-1]\n        self.max_seq_length = word_embedder.max_seq_length\n        self.batch_size = batch_size\n        self.shuffle = shuffle\n        self.indices = np.array(list(self.data_df.index))\n        self.on_epoch_end()\n\n    def __len__(self):\n        \" Denotes the number of batches per epoch \"\n        return int(len(self.data_df) // self.batch_size)\n\n    def __getitem__(self, index):\n        \" Generate single batch of data \"\n        # Generate indexes of the batch\n        start_index = self.batch_size * index\n        end_index = self.batch_size * (index+1)\n        indices = self.indices[start_index:end_index]\n\n        # Generate data\n        samples = self.data_df.loc[indices, ['Comment', 'label_encoder']].copy()\n        labels = samples.label_encoder.values.tolist()\n        texts = samples.Comment.values.tolist()\n        embeddings = self.word_embedder.encode(texts, \n                                               batch_size=4,\n                                               output_value='token_embeddings',\n                                               convert_to_numpy=True,\n                                               show_progress_bar=False)\n        # 0-padding for word embeddings\n        embeddings_padded = np.zeros((len(samples), self.max_seq_length, self.embedding_dim))\n        for e_i, e in enumerate(embeddings):\n            embeddings_padded[e_i, :len(e), :self.embedding_dim] = e.cpu().numpy()\n        embeddings_padded = np.array(embeddings_padded)\n        \n        # Encoding multi-class labels\n        mClss_labels = []\n        for l in labels:\n            l = literal_eval(l) if ',' in l else [int(ch) for ch in l[1:-1].split()]\n                \n            # Build multi-class labels\n            mtc = np.sum(to_categorical(l, num_classes=self.labels_fixed.shape[-2]), axis=0)\n            mClss_labels += [self.smooth_labels(mtc)]\n        mClss_labels = np.array(mClss_labels)\n\n        del samples, labels, texts, embeddings\n        _ = gc.collect()\n        return [embeddings_padded, self.labels_fixed], mClss_labels\n\n    def smooth_labels(self, labels, factor=0.1):\n        \" Smooth the labels \"\n        labels *= (1 - factor)\n        labels += (factor / labels.shape[-1])\n        return labels\n\n    def on_epoch_end(self):\n        \" Update indices after each epoch \"\n        if self.shuffle:\n            self.indices = sklearn.utils.shuffle(self.indices)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_generator = dict()\nfor dataset, dset in sample_dfs.items():\n    data_generator[dataset] = DataGenerator(dset, \n                                            word_embedder=embedder, \n                                            labels_fixed=labels_matrix, \n                                            batch_size=64, \n                                            shuffle=True if dataset=='training' else False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(data_generator['training']), len(data_generator['valuating'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = data_generator['valuating'][0]\nprint(X[0][0].shape)\nprint(X[0][1].shape)\nprint(X[1].shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Load Model**","metadata":{}},{"cell_type":"code","source":"class CyclicLR(Callback):\n    \"\"\"\n    This callback implements a cyclical learning rate policy (CLR).\n    The method cycles the learning rate between two boundaries with some constant frequency, \n        as detailed in this paper (https://arxiv.org/abs/1506.01186).\n    The amplitude of the cycle can be scaled on a per-iteration or per-cycle basis.\n    \n    This class has three built-in policies, as put forth in the paper.\n    \"triangular\":\n        A basic triangular cycle w/ no amplitude scaling.\n    \"halving\":\n        A basic triangular cycle that scales initial amplitude by half each cycle.\n    \"exponential\":\n        A cycle that scales initial amplitude by gamma**(cycle iterations) at each \n        cycle iteration.\n\n    For more detail, please read the paper.\n    \n    # Example\n        ```python\n            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n                                step_size=2000., mode='triangular')\n            model.fit(X_train, Y_train, callbacks=[clr])\n        ```\n    \n    Class also supports custom scaling functions:\n        ```python\n            clr_fn = lambda x: 0.5*(1+np.sin(x*np.pi/2.))\n            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n                                step_size=2000., scale_fn=clr_fn,\n                                scale_mode='cycle')\n            model.fit(X_train, Y_train, callbacks=[clr])\n        ```    \n    # Arguments\n        base_lr: initial learning rate which is the\n            lower boundary in the cycle.\n        max_lr: upper boundary in the cycle. Functionally,\n            it defines the cycle amplitude (max_lr - base_lr).\n            The lr at any cycle is the sum of base_lr\n            and some scaling of the amplitude; therefore \n            max_lr may not actually be reached depending on\n            scaling function.\n        step_size: number of training iterations per\n            half cycle. Authors suggest setting step_size\n            2-8 x training iterations in epoch.\n        mode: one of {original, halving, exponential}.\n            Default 'original'.\n            Values correspond to policies detailed above.\n            If scale_fn is not None, this argument is ignored.\n        gamma: constant in 'exp_range' scaling function:\n            gamma**(cycle iterations)\n        scale_fn: Custom scaling policy defined by a single\n            argument lambda function, where \n            0 <= scale_fn(x) <= 1 for all x >= 0.\n            mode paramater is ignored \n        scale_mode: {'cycle', 'iterations'}.\n            Defines whether scale_fn is evaluated on \n            cycle number or cycle iterations (training\n            iterations since start of cycle). Default is 'cycle'.\n    \"\"\"\n    def __init__(self, base_lr=0.001, max_lr=0.1, step_size=2000., mode='original',\n                 gamma=1., scale_fn=None, scale_mode='cycle'):\n        super(CyclicLR, self).__init__()\n\n        self.base_lr = base_lr\n        self.max_lr = max_lr\n        self.step_size = step_size\n        self.mode = mode\n        self.gamma = gamma\n        if scale_fn == None:\n            if self.mode == 'halving':\n                self.scale_fn = lambda x: 1/(2.**(x-1))\n                self.scale_mode = 'cycle'\n            elif self.mode == 'exponential':\n                self.scale_fn = lambda x: gamma**(x)\n                self.scale_mode = 'iterations'\n            else:\n                self.scale_fn = lambda x: 1.\n                self.scale_mode = 'cycle'\n        else:\n            self.scale_fn = scale_fn\n            self.scale_mode = scale_mode\n        self.clr_iterations = 0.\n        self.trn_iterations = 0.\n        self.history = {}\n\n        self._reset()\n\n    def _reset(self, new_base_lr=None, new_max_lr=None, new_step_size=None):\n        \"\"\"\n        Resets cycle iterations.\n            Optional boundary/step size adjustment.\n        \"\"\"\n        if new_base_lr is not None:\n            self.base_lr = new_base_lr\n        if new_max_lr is not None:\n            self.max_lr = new_max_lr\n        if new_step_size is not None:\n            self.step_size = new_step_size\n        self.clr_iterations = 0.\n        \n    def clr(self):\n        cycle = np.floor(1+self.clr_iterations/(2*self.step_size))\n        x = np.abs(self.clr_iterations/self.step_size - 2*cycle + 1)\n        if self.scale_mode == 'cycle':\n            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(cycle)\n        else:\n            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(self.clr_iterations)\n        \n    def on_train_begin(self, logs={}):\n        logs = logs or {}\n        if self.clr_iterations == 0:\n            K.set_value(self.model.optimizer.lr, self.base_lr)\n        else:\n            K.set_value(self.model.optimizer.lr, self.clr())        \n            \n    def on_batch_end(self, epoch, logs=None):        \n        logs = logs or {}\n        self.trn_iterations += 1\n        self.clr_iterations += 1\n\n        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n        self.history.setdefault('iterations', []).append(self.trn_iterations)\n\n        for k, v in logs.items():\n            self.history.setdefault(k, []).append(v)\n        \n        new_lr = self.clr()\n        K.set_value(self.model.optimizer.lr, new_lr)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Adjacency(Layer):\n\n    def __init__(self, nodes=1, weights=None, init_method='identity'):\n        super(Adjacency, self).__init__()\n\n        self.shape = (1, nodes, nodes)\n\n        if weights is not None:\n            assert weights.shape==(nodes, nodes), \\\n                f'Adjacency Matrix must have shape ({nodes}, {nodes})' + \\\n                f' while its shape is {weights.shape}'\n            w_init = tf.convert_to_tensor(weights)\n        else:\n            init_method = init_method.lower()\n            if init_method == 'identity':\n                initializer = tf.initializers.Identity()\n            elif init_method in ['xavier', 'glorot']:\n                initializer = tf.initializers.GlorotNormal()\n            w_init = initializer(shape=(nodes, nodes))\n\n        self.w = tf.Variable(\n            initial_value=tf.expand_dims(w_init, axis=0), \n            dtype=\"float32\", trainable=True\n        )\n\n    def call(self, inputs):\n        return tf.convert_to_tensor(self.w)\n\n    def compute_output_shape(self):\n        return self.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras import activations, initializers, constraints, regularizers\nfrom tensorflow.keras.layers import Layer, Dropout, LeakyReLU\n\n\nclass GraphAttention(Layer):\n    \"\"\"\n    Graph Attention (GAT) layer. The base implementation is taken from\n            https://github.com/danielegrattarola/keras-gat,\n    with some modifications added for ease of use.\n    Based on the original paper: Graph Attention Networks. P. Veličković et al. ICLR 2018 https://arxiv.org/abs/1710.10903\n    Notes:\n      - The inputs are tensors with a batch dimension of 1:\n        Keras requires this batch dimension, and for full-batch methods\n        we only have a single \"batch\".\n      - There are two inputs required, the node features,\n        and the graph adjacency matrix\n      - This does not add self loops to the adjacency matrix, you should preprocess\n        the adjacency matrix to add self-loops\n    .. seealso:: :class:`.GAT` combines several of these layers, and :class:`.GraphAttentionSparse` supports a sparse adjacency matrix.\n    \n    Args:\n        F_out (int): dimensionality of output feature vectors\n        attn_heads (int or list of int): number of attention heads\n        attn_heads_reduction (str): reduction applied to output features of each attention head, ``concat`` or ``average``.\n            ``average`` should be applied in the final prediction layer of the model (Eq. 6 of the paper).\n        in_dropout_rate (float): dropout rate applied to features\n        attn_dropout_rate (float): dropout rate applied to attention coefficients\n        activation (str): nonlinear activation applied to layer's output to obtain output features (eq. 4 of the GAT paper)\n        final_layer (bool): Deprecated, use ``tf.gather`` or :class:`.GatherIndices`\n        use_bias (bool): toggles an optional bias\n        saliency_map_support (bool): If calculating saliency maps using the tools in\n            stellargraph.interpretability.saliency_maps this should be True. Otherwise this should be False (default).\n        kernel_initializer (str or func, optional): The initialiser to use for the head weights.\n        kernel_regularizer (str or func, optional): The regulariser to use for the head weights.\n        kernel_constraint (str or func, optional): The constraint to use for the head weights.\n        bias_initializer (str or func, optional): The initialiser to use for the head bias.\n        bias_regularizer (str or func, optional): The regulariser to use for the head bias.\n        bias_constraint (str or func, optional): The constraint to use for the head bias.\n        attn_kernel_initializer (str or func, optional): The initialiser to use for the attention weights.\n        attn_kernel_regularizer (str or func, optional): The regulariser to use for the attention weights.\n        attn_kernel_constraint (str or func, optional): The constraint to use for the attention weights.\n    \"\"\"\n\n    def __init__(\n        self,\n        units,\n        attn_heads=1,\n        attn_heads_reduction=\"concat\",  # {'concat', 'average'}\n        in_dropout_rate=0.0,\n        attn_dropout_rate=0.0,\n        activation=\"relu\",\n        use_bias=True,\n        final_layer=None,\n        saliency_map_support=False,\n        kernel_initializer=\"glorot_uniform\",\n        kernel_regularizer=None,\n        kernel_constraint=None,\n        bias_initializer=\"zeros\",\n        bias_regularizer=None,\n        bias_constraint=None,\n        attn_kernel_initializer=\"glorot_uniform\",\n        attn_kernel_regularizer=None,\n        attn_kernel_constraint=None,\n        **kwargs,\n    ):\n\n        if attn_heads_reduction not in {\"concat\", \"average\"}:\n            raise ValueError(\n                \"{}: Possible heads reduction methods: concat, average; received {}\".format(\n                    type(self).__name__, attn_heads_reduction\n                )\n            )\n\n        self.units = units  # Number of output features (F' in the paper)\n        self.attn_heads = attn_heads  # Number of attention heads (K in the paper)\n        self.attn_heads_reduction = attn_heads_reduction  # Eq. 5 and 6 in the paper\n        self.in_dropout_rate = in_dropout_rate  # dropout rate for node features\n        self.attn_dropout_rate = attn_dropout_rate  # dropout rate for attention coefs\n        self.activation = activations.get(activation)  # Eq. 4 in the paper\n        self.use_bias = use_bias\n        if final_layer is not None:\n            raise ValueError(\n                \"'final_layer' is not longer supported, use 'tf.gather' or 'GatherIndices' separately\"\n            )\n\n        self.saliency_map_support = saliency_map_support\n\n        # Populated by build()\n        self.kernels = []  # Layer kernels for attention heads\n        self.biases = []  # Layer biases for attention heads\n        self.attn_kernels = []  # Attention kernels for attention heads\n\n        if attn_heads_reduction == \"concat\":\n            # Output will have shape (..., K * F')\n            self.output_dim = self.units * self.attn_heads\n        else:\n            # Output will have shape (..., F')\n            self.output_dim = self.units\n\n        self.kernel_initializer = initializers.get(kernel_initializer)\n        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n        self.kernel_constraint = constraints.get(kernel_constraint)\n        self.bias_initializer = initializers.get(bias_initializer)\n        self.bias_regularizer = regularizers.get(bias_regularizer)\n        self.bias_constraint = constraints.get(bias_constraint)\n        self.attn_kernel_initializer = initializers.get(attn_kernel_initializer)\n        self.attn_kernel_regularizer = regularizers.get(attn_kernel_regularizer)\n        self.attn_kernel_constraint = constraints.get(attn_kernel_constraint)\n\n        super().__init__(**kwargs)\n\n    def get_config(self):\n        \"\"\"\n        Gets class configuration for Keras serialization\n        \"\"\"\n        config = {\n            \"units\": self.units,\n            \"attn_heads\": self.attn_heads,\n            \"attn_heads_reduction\": self.attn_heads_reduction,\n            \"in_dropout_rate\": self.in_dropout_rate,\n            \"attn_dropout_rate\": self.attn_dropout_rate,\n            \"activation\": activations.serialize(self.activation),\n            \"use_bias\": self.use_bias,\n            \"saliency_map_support\": self.saliency_map_support,\n            \"kernel_initializer\": initializers.serialize(self.kernel_initializer),\n            \"kernel_regularizer\": regularizers.serialize(self.kernel_regularizer),\n            \"kernel_constraint\": constraints.serialize(self.kernel_constraint),\n            \"bias_initializer\": initializers.serialize(self.bias_initializer),\n            \"bias_regularizer\": regularizers.serialize(self.bias_regularizer),\n            \"bias_constraint\": constraints.serialize(self.bias_constraint),\n            \"attn_kernel_initializer\": initializers.serialize(\n                self.attn_kernel_initializer\n            ),\n            \"attn_kernel_regularizer\": regularizers.serialize(\n                self.attn_kernel_regularizer\n            ),\n            \"attn_kernel_constraint\": constraints.serialize(\n                self.attn_kernel_constraint\n            ),\n        }\n        base_config = super().get_config()\n        return {**base_config, **config}\n\n    def compute_output_shape(self, input_shapes):\n        \"\"\"\n        Computes the output shape of the layer.\n        Assumes the following inputs:\n        Args:\n            input_shapes (tuple of int)\n                Shape tuples can include None for free dimensions, instead of an integer.\n        Returns:\n            An input shape tuple.\n        \"\"\"\n        feature_shape, *As_shapes = input_shapes\n\n        batch_dim = feature_shape[0]\n        out_dim = feature_shape[1]\n\n        return batch_dim, out_dim, self.output_dim\n\n    def build(self, input_shapes):\n        \"\"\"\n        Builds the layer\n        Args:\n            input_shapes (list of int): shapes of the layer's inputs (node features and adjacency matrix)\n        \"\"\"\n        feat_shape = input_shapes[0]\n        input_dim = int(feat_shape[-1])\n\n        # Variables to support integrated gradients\n        self.delta = self.add_weight(\n            name=\"ig_delta\", shape=(), trainable=False, initializer=initializers.ones()\n        )\n        self.non_exist_edge = self.add_weight(\n            name=\"ig_non_exist_edge\",\n            shape=(),\n            trainable=False,\n            initializer=initializers.zeros(),\n        )\n\n        # Initialize weights for each attention head\n        for head in range(self.attn_heads):\n            # Layer kernel\n            kernel = self.add_weight(\n                shape=(input_dim, self.units),\n                initializer=self.kernel_initializer,\n                regularizer=self.kernel_regularizer,\n                constraint=self.kernel_constraint,\n                name=\"kernel_{}\".format(head),\n            )\n            self.kernels.append(kernel)\n\n            # # Layer bias\n            if self.use_bias:\n                bias = self.add_weight(\n                    shape=(self.units,),\n                    initializer=self.bias_initializer,\n                    regularizer=self.bias_regularizer,\n                    constraint=self.bias_constraint,\n                    name=\"bias_{}\".format(head),\n                )\n                self.biases.append(bias)\n\n            # Attention kernels\n            attn_kernel_self = self.add_weight(\n                shape=(self.units, 1),\n                initializer=self.attn_kernel_initializer,\n                regularizer=self.attn_kernel_regularizer,\n                constraint=self.attn_kernel_constraint,\n                name=\"attn_kernel_self_{}\".format(head),\n            )\n            attn_kernel_neighs = self.add_weight(\n                shape=(self.units, 1),\n                initializer=self.attn_kernel_initializer,\n                regularizer=self.attn_kernel_regularizer,\n                constraint=self.attn_kernel_constraint,\n                name=\"attn_kernel_neigh_{}\".format(head),\n            )\n            self.attn_kernels.append([attn_kernel_self, attn_kernel_neighs])\n        self.built = True\n\n    def call(self, inputs):\n        \"\"\"\n        Creates the layer as a Keras graph.\n        Note that the inputs are tensors with a batch dimension of 1:\n        Keras requires this batch dimension, and for full-batch methods\n        we only have a single \"batch\".\n        There are two inputs required, the node features,\n        and the graph adjacency matrix\n        Notes:\n            This does not add self loops to the adjacency matrix.\n        Args:\n            inputs (list): list of inputs with 3 items:\n            node features (size 1 x N x F),\n            graph adjacency matrix (size N x N),\n            where N is the number of nodes in the graph,\n                  F is the dimensionality of node features\n                  M is the number of output nodes\n        \"\"\"\n        X = inputs[0]  # Node features (1 x N x F)\n        A = inputs[1]  # Adjacency matrix (1 X N x N)\n        N = K.int_shape(A)[-1]\n\n        batch_dim, n_nodes, _ = K.int_shape(X)\n        if batch_dim != 1:\n            raise ValueError(\"Currently full-batch methods only support a batch dimension of one\")\n        else:\n            # Remove singleton batch dimension\n            X = K.squeeze(X, 0)\n            A = K.squeeze(A, 0)\n\n        outputs = []\n        for head in range(self.attn_heads):\n            kernel = self.kernels[head]  # W in the paper (F x F')\n            attention_kernel = self.attn_kernels[head]  # Attention kernel a in the paper (2F' x 1)\n\n            # Compute inputs to attention network\n            features = K.dot(X, kernel)  # (N x F')\n\n            # Compute feature combinations\n            # Note: [[a_1], [a_2]]^T [[Wh_i], [Wh_2]] = [a_1]^T [Wh_i] + [a_2]^T [Wh_j]\n            attn_for_self = K.dot(\n                features, attention_kernel[0]\n            )  # (N x 1), [a_1]^T [Wh_i]\n            attn_for_neighs = K.dot(\n                features, attention_kernel[1]\n            )  # (N x 1), [a_2]^T [Wh_j]\n\n            # Attention head a(Wh_i, Wh_j) = a^T [[Wh_i], [Wh_j]]\n            dense = attn_for_self + K.transpose(\n                attn_for_neighs\n            )  # (N x N) via broadcasting\n\n            # Add nonlinearity\n            dense = LeakyReLU(alpha=0.2)(dense)\n\n            # Mask values before activation (Vaswani et al., 2017)\n            #   YT: this only works for 'binary' A, not for 'weighted' A!\n            #   YT: if A does not have self-loops, the node itself will be masked, so A should have self-loops\n            #   YT: this is ensured by setting the diagonal elements of A tensor to 1 above\n            if not self.saliency_map_support:\n                mask = -10e9 * (1.0 - A)\n                dense += mask\n                dense = K.softmax(dense)  # (N x N), Eq. 3 of the paper\n            else:\n                # dense = dense - tf.reduce_max(dense)\n                # GAT with support for saliency calculations\n                W = (self.delta * A) * K.exp(\n                    dense - K.max(dense, axis=1, keepdims=True)\n                ) * (1 - self.non_exist_edge) + self.non_exist_edge * (\n                    A + self.delta * (tf.ones((N, N)) - A) + tf.eye(N)\n                ) * K.exp(\n                    dense - K.max(dense, axis=1, keepdims=True)\n                )\n                dense = W / K.sum(W, axis=1, keepdims=True)\n\n            # Apply dropout to features and attention coefficients\n            dropout_feat = Dropout(self.in_dropout_rate)(features)  # (N x F')\n            dropout_attn = Dropout(self.attn_dropout_rate)(dense)  # (N x N)\n\n            # Linear combination with neighbors' features [YT: see Eq. 4]\n            node_features = K.dot(dropout_attn, dropout_feat)  # (N x F')\n\n            if self.use_bias:\n                node_features = K.bias_add(node_features, self.biases[head])\n\n            # Add output of attention head to final output\n            outputs.append(node_features)\n\n        # Aggregate the heads' output according to the reduction method\n        if self.attn_heads_reduction == \"concat\":\n            output = K.concatenate(outputs)  # (N x KF')\n        else:\n            output = K.mean(K.stack(outputs), axis=0)  # N x F')\n\n        # Nonlinear activation function\n        output = self.activation(output)\n\n        # Add batch dimension back if we removed it\n        if batch_dim == 1:\n            output = K.expand_dims(output, 0)\n\n        return output","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def buil_MAGNET(n_labels,\n                embedding_dim: int,\n                sequence_length: int=512, \n                lstm_units: int=64,\n                dropout_rates=[0.2, 0.3],\n                attention_heads=[4, 2],\n                adjacency_matrix=None,\n                adjacency_generation='xavier', # 'identity' or 'xavier' or 'glorot'\n                feed_text_embeddings=True, # if False, add additional Embedding layer\n                text_embeddings_matrix=None, # initialized weights for text Embedding layer\n                feed_label_embeddings=True, # if False, add additional Embedding layer\n                label_embeddings_matrix=None, # initialized weights for label Embedding layer\n                ) -> Model:\n\n    if isinstance(attention_heads, int):\n        attention_heads = [attention_heads, attention_heads]\n    if not isinstance(attention_heads, (list, tuple)):\n        raise ValueError('`attention_heads` must be INT, LIST or TUPLE')\n\n    # 1. Sentence Representation\n    if feed_text_embeddings:\n        sentence_model = Sequential(name='sentence_model')\n        sentence_model.add(Dropout(dropout_rates[0], input_shape=(sequence_length, embedding_dim), name='word_embeddings'))\n        word_inputs, word_embeddings = sentence_model.inputs, sentence_model.outputs\n    else:\n        word_inputs = Input(shape=(sequence_length, ), name='word_inputs')\n        embedding_args = {\n            'input_dim': sequence_length,\n            'output_dim': embedding_dim,\n            'name': 'word_embeddings'\n        }\n        if text_embeddings_matrix is not None \\\n            and text_embeddings_matrix.shape==(sequence_length, embedding_dim):\n            embedding_args['weights'] = [text_embeddings_matrix]\n        word_embeddings = Embedding(**embedding_args)(word_inputs)\n        word_embeddings = Dropout(dropout_rates[0], name='WE_dropout')(word_embeddings)\n\n    forward_rnn = LSTM(units=lstm_units, return_sequences=True, name='forward_rnn')\n    backward_rnn = LSTM(units=lstm_units, return_sequences=True, name='backward_rnn', go_backwards=True)\n    bidir_rnn = Bidirectional(layer=forward_rnn, backward_layer=backward_rnn, merge_mode=\"concat\", name='bidir_rnn')\n    \n    sentence_repr = bidir_rnn(word_embeddings)\n    sentence_repr = K.mean(sentence_repr, axis=1)\n    # print(f\"sentence_repr: {K.int_shape(sentence_repr)}\")\n\n    # 2. Labels Representation\n    if feed_label_embeddings:\n        label_inputs = Input(batch_shape=(1, n_labels, embedding_dim), name='label_embeddings')\n        label_embeddings = label_inputs\n    else:\n        label_inputs = Input(batch_shape=(1, n_labels), name='label_inputs')\n        embedding_args = {'input_dim': n_labels,\n                          'output_dim': embedding_dim,\n                          'name': 'label_embeddings'}\n        if label_embeddings_matrix is not None \\\n            and label_embeddings_matrix.shape==(n_labels, embedding_dim):\n            embedding_args['weights'] = [label_embeddings_matrix]\n        label_embeddings = Embedding(**embedding_args)(label_inputs)\n        label_embeddings = Dropout(rate=dropout_rates[0], name='LE_dropout')(label_embeddings)\n    label_embeddings = Dense(units=embedding_dim//4, name='label_embeddings_reduced')(label_embeddings)\n    # print(f\"label_inputs: {K.int_shape(label_inputs)}\")\n\n    label_correlation = Adjacency(nodes=n_labels, \n                                  weights=adjacency_matrix,\n                                  init_method=adjacency_generation)(label_embeddings)\n    # print(f\"label_correlation: {K.int_shape(label_correlation)}\")\n\n    label_attention = GraphAttention(units=embedding_dim//4//attention_heads[0],\n                                     activation='tanh',\n                                     attn_heads=attention_heads[0],\n                                     in_dropout_rate=dropout_rates[1],\n                                     attn_dropout_rate=dropout_rates[1], )([label_embeddings, label_correlation])\n    # print(f\"label_attention: {K.int_shape(label_attention)}\")\n\n    label_residual = Add(name='label_residual')([label_attention, label_embeddings])\n    # print(f\"label_residual: {K.int_shape(label_residual)}\")\n\n    label_repr = GraphAttention(units=2*lstm_units,\n                                activation='tanh',\n                                attn_heads_reduction='average',\n                                attn_heads=attention_heads[1],\n                                in_dropout_rate=dropout_rates[1],\n                                attn_dropout_rate=dropout_rates[1], )([label_residual, label_correlation])\n\n    label_repr = K.sum(label_repr, axis=0, keepdims=False)\n    # print(f\"label_repr: {K.int_shape(label_repr)}\")\n\n    # 3. Prediction\n    prediction = tf.einsum('Bk,Nk->BN', sentence_repr, label_repr)\n    prediction = sigmoid(prediction)\n    # print(f\"prediction: {K.int_shape(prediction)}\")\n\n    return Model(inputs=[word_inputs, label_inputs], outputs=prediction, name='MAGNET')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def weighted_cross_entropy(y_true, y_pred, pos_weight=1.618):\n    losses = y_true * -K.log(y_pred) * pos_weight + (1-y_true) * -K.log(1-y_pred)\n    losses = K.clip(losses, 0.0, 9.7)\n    return K.mean(losses)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gctf\nfrom stellargraph.utils import plot_history\n\nclass MAGNET:\n\n    def __init__(self, n_labels: int, embedding_dim: int):\n\n        self.embedding_dim = embedding_dim\n\n        # Build model(s)\n        print(f\"\\n\\n\\nBuilding MAGNET ...\\n\\n\\n\")\n        self.model = buil_MAGNET(n_labels, embedding_dim=embedding_dim, sequence_length=512, lstm_units=32)\n        self.model.summary()\n\n    def compile(self, model_saved: str, logs_path: str, schedule_step: int, verbose: int=1):\n                    \n        # Compile optimizer, loss & metric functions\n        print(f\"Compiling MAGNET using \\n\\tgrad-centralized ADAM, \\n\\ttop-k Accuracy, \\n\\tweighted Cross-Entropy \\n...\")\n        self.model.compile(optimizer=gctf.optimizers.adam(learning_rate=0.001), \n                           # optimizer=Adam(learning_rate=0.001), \n                           metrics=[\"accuracy\", TopKCategoricalAccuracy(k=3)],\n                           loss=weighted_cross_entropy)\n\n        # Define Callbacks\n        return [\n            # TensorBoard(log_dir=logs_path),\n            # ReduceLROnPlateau(monitor='loss', factor=0.1, patience=3, verbose=verbose),\n            CyclicLR(mode='exponential', base_lr=1e-7, max_lr=1e-3, step_size=schedule_step),\n            ModelCheckpoint(filepath=model_saved, monitor='accuracy', save_weights_only=True, save_best_only=False, save_freq='epoch'),\n            # LearningRateScheduler(noam_scheme),\n            # EarlyStopping(monitor='val_accuracy', mode='max', restore_best_weights=True, min_delta=1e-7, patience=7, verbose=verbose),\n        ]\n\n    def finetune(self, train_generator, val_generator, model_saved: str, logs_path: str, n_loops: int=3, verbose: int=1):\n        # Compile\n        schedule_step = len(train_generator) // 2\n        custom_callbacks = self.compile(model_saved, logs_path, schedule_step, verbose)\n\n        # Define part(s) of layers for fine-tuning\n        graph_layers = ['adjacency', 'graph_attention', 'graph_attention_1']\n        word_layers = ['bidir_rnn', 'label_embeddings_reduced']\n        train_histories = []\n\n        ######################################\n        #             FINE-TUNING            #\n        ######################################\n\n        print(f\"[Fine-tuning MAGNET]\")\n        train_args = {\n            'generator': train_generator,\n            'steps_per_epoch': len(train_generator)//3,\n            'validation_data': val_generator,\n            'validation_steps': len(val_generator),\n            'callbacks': custom_callbacks\n        }\n        for l in range(n_loops):\n            \n            print(f\"Training loop {l+1}\")\n\n            # Step 1: Train ALL layers\n            for layer in self.model.layers:\n                layer.trainable = True\n\n            print(f\"\\tStep 1: Training ALL layers ...\")\n            train_history = self.model.fit_generator(initial_epoch=l*5, epochs=l*5+2, **train_args)\n            train_histories.append(train_history)\n\n            # Step 2: Train GRAPH layers\n            for layer in self.model.layers:\n                layer.trainable = True if layer.name in graph_layers else False\n\n            print(f\"\\tStep 2: Training GRAPH layers ...\")\n            train_history = self.model.fit_generator(initial_epoch=l*5+2, epochs=l*5+3, **train_args)\n            train_histories.append(train_history)\n\n            # Step 3: Train EMBEDDING layers\n            for layer in self.model.layers:\n                layer.trainable = True if layer.name in word_layers else False\n\n            print(f\"\\tStep 3: Training EMBEDDING layers ...\")\n            train_history = self.model.fit_generator(initial_epoch=l*5+3, epochs=l*5+4, **train_args)\n            train_histories.append(train_history)\n\n            # Step 4: Train ALL layers\n            for layer in self.model.layers:\n                layer.trainable = True\n\n            print(f\"\\tStep 4: Training ALL layers ...\")\n            train_history = self.model.fit_generator(initial_epoch=l*5+4, epochs=l*5+5, **train_args)\n            train_histories.append(train_history)\n\n            # Reduce learning rate\n            # custom_callbacks[0].base_lr /= 1.69\n            # custom_callbacks[0].max_lr /= 1.69\n\n        return train_histories\n\n    def train(self, train_generator, val_generator, \n                    model_saved: str, logs_path: str,\n                    max_epochs: int=50, verbose: int=1):\n        # Compile\n        schedule_step = len(train_generator) // 2\n        custom_callbacks = self.compile(model_saved, logs_path, schedule_step, verbose)\n\n        # Training\n        train_history = self.model.fit_generator(generator=train_generator,\n                                                 steps_per_epoch=len(train_generator),\n                                                 validation_data=val_generator,\n                                                 validation_steps=len(val_generator),\n                                                 callbacks=custom_callbacks, \n                                                 epochs=max_epochs,\n                                                 initial_epoch=0)\n        return train_history\n\n    def load_weights(self, weight_path: str):\n        self.model.load_weights(weight_path)\n\n    def predict(self, label_embeddings: np.array, sent_embeddings: np.array):\n        sent_embeddings = np.reshape(sent_embeddings, (1, 512, self.embedding_dim))\n        preds = self.model.predict([sent_embeddings, label_embeddings]).tolist()\n        return preds[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"N_LABELS = labels_matrix.shape[1]\nembedding_dim = list(embedder.parameters())[-2].shape[-1]\n\nmodel = MAGNET(n_labels=N_LABELS, embedding_dim=embedding_dim)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def noam_scheme(global_step, init_lr, warmup_steps=16):\n    \"\"\"\n    Noam scheme learning rate decay\n        init_lr: (scalar) initial learning rate. \n        global_step: (scalar) current training step\n        warmup_steps: (scalar) During warmup_steps, learning rate increases until it reaches init_lr.\n    \"\"\"\n    step = tf.cast(global_step+1, dtype=tf.float32, name=\"global_step\")\n    return init_lr * (warmup_steps**0.5) * tf.minimum(step*(warmup_steps**-1.5), step**-0.5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Train**","metadata":{}},{"cell_type":"code","source":"models_path = '/kaggle/working/models'\nif not os.path.isdir(models_path):\n    os.makedirs(models_path)\n\nlogs_path = '/kaggle/working/logs'\nif not os.path.isdir(logs_path):\n    os.makedirs(logs_path)\n    \npred_dir = '/kaggle/working/predictions'\nif not os.path.isdir(pred_dir):\n    os.makedirs(pred_dir)\n    \nmodel_format = 'ep={epoch:03d}_acc={accuracy:.3f}_val_acc={val_accuracy:.3f}_topk={top_k_categorical_accuracy:.3f}_val_topk={val_top_k_categorical_accuracy:.3f}.h5'\nmodel_saved =  os.path.join(models_path, model_format)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_history = model.finetune(data_generator['training'], \n                               data_generator['valuating'], \n                               model_saved=model_saved, \n                               logs_path=logs_path, \n                               n_loops=3, \n                               verbose=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_history(train_history)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.chdir(r'/kaggle/working')\ndir_path = '/kaggle/working/'\nshutil.make_archive(dir_path+\"data\", 'zip', dir_path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}