{"cells":[{"metadata":{"_uuid":"4806c15a-64a6-46eb-be13-e71949c26855","_cell_guid":"278c13a0-9c29-4071-a3f4-e8e52eb93d4f","trusted":true},"cell_type":"markdown","source":"## <center>Sarcasm detection with logistic regression\n    \nWe'll be using the dataset from the [paper](https://arxiv.org/abs/1704.05579) \"A Large Self-Annotated Corpus for Sarcasm\" with >1mln comments from Reddit, labeled as either sarcastic or not. A processed version can be found on Kaggle in a form of a [Kaggle Dataset](https://www.kaggle.com/danofer/sarcasm)."},{"metadata":{"_uuid":"7f02164e-62e7-485d-9f49-92890cb3185c","_cell_guid":"f9be9d06-e6e8-49ee-a993-3efdf27898fc","trusted":true},"cell_type":"code","source":"!ls ../input/sarcasm/","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1805915b-d61f-4341-9243-20ef9be72206","_cell_guid":"36134f9f-8646-4cde-b07e-e483a27c83d7","trusted":true},"cell_type":"code","source":"# some necessary imports\nimport os\nimport string\nimport numpy as np\nimport pandas as pd\nfrom sklearn import preprocessing, naive_bayes,linear_model, metrics\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nimport seaborn as sns\ncolor = sns.color_palette()\n\nfrom matplotlib import pyplot as plt\nfrom plotly import tools\nimport plotly.graph_objs as go\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c4d19e76-d38f-48ec-b7f1-7ec3aa8aa60c","_cell_guid":"4576c487-d989-46d0-ac73-7d3e64f63b54","trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('../input/sarcasm/train-balanced-sarcasm.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cd8ea7b0-c4e2-4feb-a00f-f478b23e965b","_cell_guid":"74fd0b92-90c5-4e93-be5c-1cb29b55ffd9","trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"50b479c7-5232-4e50-adbc-c4566645bc4c","_cell_guid":"5b054980-12a0-41b8-a343-a8b1cf78c982","trusted":true},"cell_type":"code","source":"train_df.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9580b9df-a5a2-4b5b-a62b-242c9f2f606a","_cell_guid":"d383b774-7265-41b6-9550-6bb438b596da","trusted":true},"cell_type":"markdown","source":"Some comments are missing, so we drop the corresponding rows."},{"metadata":{"_uuid":"3780bcf7-6111-4bfc-a8d2-4a96cec8692d","_cell_guid":"8a58b252-fb43-4262-b339-80a2e2d76737","trusted":true},"cell_type":"code","source":"train_df.dropna(subset=['comment'], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"48b2408d-9649-4960-8d2f-95720c316cba","_cell_guid":"4662a14a-6ae7-4485-9c5f-b8cbe25f1694","trusted":true},"cell_type":"code","source":"train_df.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"924fe3ff-40a4-4eb0-9ae8-381b2283dec3","_cell_guid":"396d394d-4362-4ca3-9a70-452b662cd275","trusted":true},"cell_type":"markdown","source":"We notice that the dataset is indeed balanced"},{"metadata":{"_uuid":"cfd6b7df-273d-4851-8da9-a76a94652ded","_cell_guid":"baf06749-693d-4f1e-bfb1-3af9d6b927a1","trusted":true},"cell_type":"code","source":"train_df['label'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f559bf2e-7751-4bf7-b848-9e4439d6e5cf","_cell_guid":"39239b33-24f5-49ce-a839-47ef08b2aa1f","trusted":true},"cell_type":"markdown","source":"We split data into training and validation parts."},{"metadata":{"_uuid":"23a35522-a2f6-4ff7-a2f7-c29108fa49a8","_cell_guid":"f60c47fd-6e52-4459-a273-a1ed35a738e2","trusted":true},"cell_type":"code","source":"train_texts, valid_texts, y_train, y_valid = \\\n        train_test_split(train_df['comment'], train_df['label'], random_state=17, train_size=.7)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dc493e6c-a0d2-4d0b-ae4d-40988ffaaa15","_cell_guid":"b60342eb-263c-4ba3-b513-404c04fb526a","trusted":true},"cell_type":"markdown","source":"**Init code**"},{"metadata":{"_uuid":"99f9fc8b-9ad5-4974-b879-32676d76b6b0","_cell_guid":"934cc43d-1773-46f6-bf50-a1b53929bed9","trusted":true},"cell_type":"code","source":"#cm = plt.cm.get_cmap('RdYlBu_r')\n\n#n, bins, patches = plt.hist(train_df['label'], density = True)\n#    then normalize\n#col = (n - n.min())/(n.max() - n.min())\n#print(col)\n#for c, p in zip(col, patches):\n #   plt.setp(p, 'facecolor', cm(c))\ntrgt_count = train_df['label'].value_counts()\n\nlabels = '0', '1'\nsizes = np.array(trgt_count / trgt_count.sum() * 100)\n#_, axes = plt.subplots(1, 2, sharey = True, figsize=(12, 8))    \nsns.countplot(x ='label', data = train_df)\n\n   ## target distribution ##\nplt.pie(sizes, labels = labels)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"02120b74-fb26-43ed-831f-94cad806329c","_cell_guid":"4ebd3702-ff17-47e4-b889-4b6fda3a703f","trusted":true},"cell_type":"code","source":"#target count#\ntrgt_counts = train_df['label'].value_counts()\ntrace = go.Bar(\n    x=trgt_counts.index, \n    y = trgt_counts.values,\n    marker=dict(\n        color=trgt_counts.values,\n        colorscale='Picnic',\n        reversescale=True\n    ),\n)\nlayout = go.Layout(\n    title='Target Count',\n    font=dict(size=18),\n    width = 400, \n    height =500,\n)\ndata=[trace]\nfig=go.Figure(data=data,layout=layout)\npy.iplot(fig,filename='TargetCount')\n\n#target distribution#\n\nlabels = np.array(trgt_counts.index)\nsizes = np.array(trgt_counts /trgt_counts.sum() * 100)\n\ntrace = go.Pie(labels=labels,values=sizes)\nlayout = go.Layout(\n    title='Target Distribution',\n    font=dict(size=18),\n    width=600,\n    height=600,\n)\ndata=[trace]\nfig=go.Figure(data=data,layout=layout)\npy.iplot(fig,filename='shit')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a03fb690-b169-4b6d-86da-d22cdef05165","_cell_guid":"ef118207-330b-4716-9cb6-016fad3c73b8","trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud, STOPWORDS\n\n# Thanks : https://www.kaggle.com/aashita/word-clouds-of-various-shapes ##\ndef plot_wordcloud(text, mask=None, max_words=200, max_font_size=100, figure_size=(24.0,16.0), \n                   title = None, title_size=40, image_color=False):\n    stopwords = set(STOPWORDS)\n    more_stopwords = {'one', 'br', 'Po', 'th', 'sayi', 'fo', 'Unknown'}\n    stopwords = stopwords.union(more_stopwords)\n\n    wordcloud = WordCloud(background_color='black',\n                    stopwords = stopwords,\n                    max_words = max_words,\n                    max_font_size = max_font_size, \n                    random_state = 42,\n                    width = 800, \n                    height = 400,\n                    mask = mask)\n    wordcloud.generate(str(text))\n    \n    plt.figure(figsize = figure_size)\n    if image_color:\n        image_colors = ImageColorGenerator(mask);\n        plt.imshow(wordcloud.recolor(color_func = image_colors), interpolation=\"bilinear\");\n        plt.title(title, fontdict={'size': title_size,  \n                                  'verticalalignment': 'bottom'})\n    else:\n        plt.imshow(wordcloud);\n        plt.title(title, fontdict={'size': title_size, 'color': 'black', \n                                  'verticalalignment': 'bottom'})\n    plt.axis('off');\n    plt.tight_layout()  \n    \nplot_wordcloud(train_texts, title='Word Cloud of Comments')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"800800fe-2efe-4a60-9340-0dad92dfe403","_cell_guid":"bd4a6c72-4a94-4999-8ab4-aa42163247b6","trusted":true},"cell_type":"markdown","source":"## Word Frequency plot of sarcasm & non-sarcasm comments:"},{"metadata":{"_uuid":"e71e2de3-8d5d-4980-968d-1b6537f56abd","_cell_guid":"0afd333b-e50c-4c43-8748-a1947258faa7","trusted":true},"cell_type":"code","source":"from collections import defaultdict\n\ntrain1_df = train_texts[y_train == 1]\ntrain0_df = train_texts[y_train == 0]\n\n## let's generate some ngrams ##\ndef generate_ngrams(text, n_gram = 1):\n    token = [token for token in text.lower().split(' ') if token != '' if token not in STOPWORDS]\n    ngrams = zip(*[token[i:] for i in range(n_gram)])\n    return [' '.join(ngram) for ngram in ngrams]\n\n# custom function for horizontal bar chart ##\ndef horizontal_bar_chart(df, color):\n    trace = go.Bar(\n        y = df['word'].values[::-1],\n        x = df['wordcount'].values[::-1],\n        showlegend=False,\n        orientation='h',\n        marker=dict(color=color),\n    )\n    return trace\n\n# Get the bar chart for non-sarcasm comments #\nfreq_dict = defaultdict(int)\nfor sent in train0_df:\n    for word in generate_ngrams(sent):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(),key=lambda x:x[1])[::-1])\nfd_sorted.columns = ['word','wordcount']\ntrace0 = horizontal_bar_chart(fd_sorted.head(50),'blue')\n\n# Get the bar chart for sarcasm comments #\nfreq_dict = defaultdict(int)\nfor sent in train1_df:\n    for word in generate_ngrams(sent):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(),key=lambda x:x[1])[::-1])\nfd_sorted.columns = ['word','wordcount']\ntrace1 = horizontal_bar_chart(fd_sorted.head(50),'red')\n\n#create two subplots\nfig = tools.make_subplots(rows=1,cols=2,vertical_spacing=0.04,\n                         subplot_titles=['Frequent words of non-sarcasm comments',\n                                        'Frequent words of sarcasm comments'])\nfig.append_trace(trace0,1,1)\nfig.append_trace(trace1,1,2)\nfig['layout'].update(height=1200, width=900,paper_bgcolor='rgb(233,233,233)',title='Word count sarcasm plots')\npy.iplot(fig, filename='Word_count_plots')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"650f2db3-8a0a-4eff-9556-be0a5b5d9ebc","_cell_guid":"06078071-5c6c-42f2-9f07-a67678e212a0","trusted":true},"cell_type":"markdown","source":"## Then get some bigrams:"},{"metadata":{"_uuid":"2c0cf56f-41d1-4d95-98ee-3f02ec201574","_cell_guid":"267521ac-6182-4f23-93e1-784d74f24d56","trusted":true},"cell_type":"code","source":"freq_dict = defaultdict(int)\nfor sent in train0_df:\n    for word in generate_ngrams(sent, 2):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(),key=lambda x:x[1])[::-1])\nfd_sorted.columns=['word','wordcount']\ntrace0=horizontal_bar_chart(fd_sorted.head(50),'yellow')\n\n#Get the bar chart from sarcasm comments ##\nfreq_dict =defaultdict(int)\nfor sent in train1_df:\n    for word in generate_ngrams(sent, 2):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(),key=lambda x:x[1])[::-1])\nfd_sorted.columns=['word','wordcount']\ntrace1=horizontal_bar_chart(fd_sorted.head(50),'orange')\n\n#create two subplots\nfig = tools.make_subplots(rows=1,cols=2,vertical_spacing=0.04,\n                         subplot_titles=['Frequent bigrams of non-sarcasm comments',\n                                        'Frequent bigrams of sarcasm comments'])\nfig.append_trace(trace0,1,1)\nfig.append_trace(trace1,1,2)\nfig['layout'].update(height=1200, width=900,paper_bgcolor='rgb(233,233,233)',title='Bigrams sarcasm plots')\npy.iplot(fig, filename='word_count_plots')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"abef57f3-c230-4cc4-8103-3b8ea3367852","_cell_guid":"2bd063b5-363d-487f-bd39-afc8a0814bb8","trusted":true},"cell_type":"markdown","source":"## Now look at the trigrams:"},{"metadata":{"_uuid":"d06beabd-b689-4e7c-b4b6-44a77aab0fba","_cell_guid":"35ba102e-dc37-4210-bb82-3980b2f4fda5","trusted":true},"cell_type":"code","source":"freq_dict = defaultdict(int)\nfor sent in train0_df:\n    for word in generate_ngrams(sent, 3):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(),key=lambda x:x[1])[::-1])\nfd_sorted.columns=['word','wordcount']\ntrace0=horizontal_bar_chart(fd_sorted.head(50),'green')\n \n# Get the bar chart from sarcasm comments #\nfreq_dict =defaultdict(int)\nfor sent in train1_df:\n    for word in generate_ngrams(sent, 3):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(),key=lambda x:x[1])[::-1])\nfd_sorted.columns = ['word','wordcount']\ntrace1=horizontal_bar_chart(fd_sorted.head(50),'blue')\n\n#create two subplots\nfig = tools.make_subplots(rows=1,cols=2,vertical_spacing=0.04,\n                         subplot_titles=['Frequent trigrams of non-sarcasm comments',\n                                        'Frequent trigrams of sarcasm comments'])\nfig.append_trace(trace0,1,1)\nfig.append_trace(trace1,1,2)\nfig['layout'].update(height=1200, width=1500,paper_bgcolor='rgb(233,233,233)',title='Trigram sarcasm plots')\npy.iplot(fig, filename='word_count_plots')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"type(train_texts)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_texts = train_texts.to_frame('comment')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"valid_texts=valid_texts.to_frame('comment')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_texts['label'] = y_train\nvalid_texts['label'] = y_valid","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4669977d-2a3d-4994-b572-8d26ce931a39","_cell_guid":"7f714261-27d7-4a60-8c54-d7305447e68a","trusted":true},"cell_type":"code","source":"# creating some extra features for better prediction accuracy\n\ntrain_texts['num_words'] = train_texts['comment'].apply(lambda x: len(str(x).split()))\nvalid_texts['num_words'] = valid_texts['comment'].apply(lambda x: len(str(x).split()))\n\ntrain_texts['num_unique_words'] = train_texts['comment'].apply(lambda x: len(set(str(x).split())))  # for each comment\nvalid_texts['num_unique_words'] = valid_texts['comment'].apply(lambda x: len(set(str(x).split())))\n\ntrain_texts['num_chars'] = train_texts['comment'].apply(lambda x: len(str(x))) # for each comment\nvalid_texts['num_chars'] = valid_texts['comment'].apply(lambda x: len(str(x)))\n\ntrain_texts['num_stopwords'] = train_texts['comment'].apply(lambda x: len([w for w in str(x).lower().split() if w in STOPWORDS]))\nvalid_texts['num_stopwords'] = train_texts.apply(lambda x: len([w for w in str(x).lower().split() if w in STOPWORDS]))\n\ntrain_texts['num_punctuations'] = train_texts['comment'].apply(lambda x: len([p for p in str(x) if p in string.punctuation]))\nvalid_texts['num_punctuations'] = valid_texts['comment'].apply(lambda x: len([p for p in str(x) if p in string.punctuation]))\n\ntrain_texts['num_words_upper'] = train_texts['comment'].apply(lambda x: len([u for u in str(x) if u.isupper()]))\nvalid_texts['num_words_upper'] = valid_texts['comment'].apply(lambda x: len([u for u in str(x) if u.isupper()]))\n\ntrain_texts['num_words_title'] = train_texts['comment'].apply(lambda x: len([t for t in str(x) if t.istitle()]))\nvalid_texts['num_words_title'] = valid_texts['comment'].apply(lambda x: len([t for t in str(x) if t.istitle()]))\n\ntrain_texts['mean_word_len'] = train_texts['comment'].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\nvalid_texts['mean_word_len'] = valid_texts['comment'].apply(lambda x: np.mean([len(w) for w in str(x).split()]))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cc3c9d53-1774-4fa0-9488-db9edde8b5ec","_cell_guid":"14099317-3776-4d3a-b0c4-79692a8ea75d","trusted":true},"cell_type":"code","source":"# Truncate some extreme values for better visuals ##\ntrain_texts['num_words'].loc[train_texts['num_words']>60] = 60\ntrain_texts['num_punctuations'].loc[train_texts['num_punctuations']>10] = 10\ntrain_texts['num_chars'].loc[train_texts['num_chars']>350] = 350\n\nf, axes = plt.subplots(3, 1, figsize=(10,20))\nsns.boxplot(x='label', y='num_words', data=train_texts,ax=axes[0])\naxes[0].set_xlabel('Label', fontsize=12)\naxes[0].set_title('Number of words in each class', fontsize=15)\n\nsns.boxplot(x='label', y='num_chars', data=train_texts,ax=axes[1])\naxes[1].set_xlabel('Label', fontsize=12)\naxes[1].set_title('Number of characters in each class', fontsize=15)\n\nsns.boxplot(x='label', y='num_punctuations', data=train_texts,ax=axes[2])\naxes[2].set_xlabel('Label', fontsize=12)\naxes[2].set_title('Number of punctuations in each class', fontsize=15)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2e51c7ec-a8a6-44f9-8b35-470e3c37139b","_cell_guid":"242d7045-1f36-4950-b5af-30f31c35a1da","trusted":true},"cell_type":"code","source":"# get the tfidf vectors #\ntfidf_vec = TfidfVectorizer(stop_words ='english', ngram_range=(1,3))\ntfidf_vec.fit_transform(train_texts['comment'].values.tolist() + valid_texts['comment'].values.tolist())\ntrain_tfidf = tfidf_vec.transform(train_texts['comment'].values.tolist())\ntest_tfidf = tfidf_vec.transform(valid_texts['comment'].values.tolist())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_tfidf","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"130a747b-bf21-44c0-9b33-ef2568cc2e33","_cell_guid":"2c8d34e5-dd21-40c0-acfc-7c53ff3624d5","trusted":true},"cell_type":"markdown","source":"## Let's build a model for that:"},{"metadata":{"_uuid":"0cae11a0-a808-4cda-b67c-22bececfe594","_cell_guid":"759ffb82-5177-4205-981c-d56e2eb2d9d1","trusted":true},"cell_type":"code","source":"train_y = train_texts['label'].values\n\ndef runModel(train_X, train_y, test_X, test_y, test_X2):\n    logreg = linear_model.LogisticRegression(C=5, solver = 'sag')\n    logreg.fit(train_X, train_y)\n    pred_test_y=logreg.predict_proba(test_X)[:,1]\n    pred_test_y2=logreg.predict_proba(test_X2)[:,1]\n    return pred_test_y, pred_test_y2, logreg\n\ncv_scores =[]\npred_full_test=0\npred_train=np.zeros([train_df.shape[0]])\n\nkf=KFold(n_splits=5,shuffle=True,random_state=2020)\nfor dev_index,val_index in kf.split(train_texts):\n    dev_X, val_X = train_tfidf[dev_index],train_tfidf[val_index]\n    dev_y, val_y = train_y[dev_index], train_y[val_index]\n    pred_val_y, pred_test_y, model = runModel(dev_X, dev_y, val_X, val_y, test_tfidf)\n    pred_full_test=pred_full_test+pred_test_y\n    pred_train[val_index]=pred_val_y\n    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n    break","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"83ab3b64-00e2-4f9e-b092-ed45fec28eba","_cell_guid":"e7d93b5c-a397-480a-a09d-98d56aacad79","trusted":true},"cell_type":"code","source":"for thresh in np.arange(0.3, 0.401, 0.01):\n    thresh = np.round(thresh, 2)\n    print(\"F1 score at threshold {0} is {1}\".format(thresh, metrics.f1_score(val_y, (pred_val_y>thresh).astype(int))))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f670ab7b-ac75-4038-a729-e547f5120889","_cell_guid":"19430221-a622-4ac7-9a12-4a949081f883","trusted":true},"cell_type":"code","source":"import eli5\neli5.show_weights(model, vec=tfidf_vec, top=100, feature_filter=lambda x: x != '<BIAS>')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}