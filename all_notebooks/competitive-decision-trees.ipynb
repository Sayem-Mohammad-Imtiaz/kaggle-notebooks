{"cells":[{"metadata":{},"cell_type":"markdown","source":"## <center>  Decision trees with a toy task and the Telecom dataset \n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We would be performing Decision Tree Algorithms on 1.Built in Dataset  2. Telecom Churn Dataset 2.1 without Hyperparameter Tuning 2.2 Hyperparameter Tuning with GridSearch Stratified K-Fold Cross Validation 2.3 Without GridSearch Stratified K-Fold Cross Validation.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Loading all necessary libraries.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pip install pydotplus","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"%matplotlib inline\nfrom matplotlib import pyplot as plt\nplt.rcParams['figure.figsize'] = (10, 8)\nimport seaborn as sns\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nimport collections\nfrom sklearn.model_selection import GridSearchCV, StratifiedKFold\nfrom sklearn import preprocessing\nfrom sklearn.tree import DecisionTreeClassifier, export_graphviz\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom ipywidgets import Image\nfrom io import StringIO\nimport pydotplus \nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom tqdm import tqdm_notebook","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Part 1. Toy dataset \"Will They? Won't They?\"","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Your goal is to figure out how decision trees work by walking through a toy problem. While a single decision tree does not yield outstanding results, other performant algorithms like gradient boosting and random forests are based on the same idea. That is why knowing how decision trees work might be useful.\nWe'll go through a toy example of binary classification - Person A is deciding whether they will go on a second date with Person B. It will depend on their looks, eloquence, alcohol consumption (only for example), and how much money was spent on the first date.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"\nCreating the dataset\n","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#Creating dataframe with dummy variables\ndef df(dic,features):\n    out=pd.DataFrame(dic)\n    out=pd.concat([out,pd.get_dummies(out[features])],axis=1)\n    out.drop(features,axis=1,inplace=True)\n    return out\n\n#Intersecting features in train and train as absent in each.\ndef int_features(train,test):\n    com_feat=list(set(train.keys()) & set(test.keys())) \n    return train[com_feat],test[com_feat]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feat=['Looks', 'Alcoholic_beverage','Eloquence','Money_spent']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Train data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = {}\ndf_train['Looks'] = ['handsome', 'handsome', 'handsome', 'repulsive',\n                         'repulsive', 'repulsive', 'handsome'] \ndf_train['Alcoholic_beverage'] = ['yes', 'yes', 'no', 'no', 'yes', 'yes', 'yes']\ndf_train['Eloquence'] = ['high', 'low', 'average', 'average', 'low',\n                                   'high', 'average']\ndf_train['Money_spent'] = ['lots', 'little', 'lots', 'little', 'lots',\n                                  'lots', 'lots']\ndf_train['Will_go'] = LabelEncoder().fit_transform(['+', '-', '+', '-', '-', '+', '+'])\n\ndf_train = df(df_train, feat)\ndf_train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Test Data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test = {}\ndf_test['Looks'] = ['handsome', 'handsome', 'repulsive'] \ndf_test['Alcoholic_beverage'] = ['no', 'yes', 'yes']\ndf_test['Eloquence'] = ['average', 'high', 'average']\ndf_test['Money_spent'] = ['lots', 'little', 'lots']\ndf_test = df(df_test, feat)\ndf_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#features present in train but not in test need to be taken care of\ny=df_train['Will_go']\ndf_train.pop('Will_go')\ndf_train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"What is the entropy  S0  of the initial system? By system states, we mean values of the binary feature \"Will_go\" - 0 or 1 - two states in total.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"S0 = -3/4*log2(3/4) - 1/4*log(1/4) =0.311+0.5=0.811","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Train decision tree using sklearn classifier**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"tree=DecisionTreeClassifier(criterion='entropy', random_state=12) \ntree.fit(df_train, y) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### The \"Telecom\" dataset","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Each row represents a customer; each column contains customerâ€™s attributes. The datasets have the following attributes or features:\nState: string\nAccount length: integer\nArea code: integer\nInternational plan: string\nVoice mail plan: string\nNumber vmail messages: integer\nTotal day minutes: double\nTotal day calls: integer\nTotal day charge: double\nTotal eve minutes: double\nTotal eve calls: integer\nTotal eve charge: double\nTotal night minutes: double\nTotal night calls: integer\nTotal night charge: double\nTotal intl minutes: double\nTotal intl calls: integer\nTotal intl charge: double\nCustomer service calls: integer\nChurn: string\n\n\nThe dataset contains 667 rows (customers) and 20 columns (features).\n\nThe \"Churn\" column is the target to predict.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Reading the dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/edadata/telecom_churn.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Slight preprocessing**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Labelling for the International plan & Voice mail Plan.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data['International plan']= data['International plan'].map({'Yes':1,'No':0})\n\ndata['Voice mail plan']= data['Voice mail plan'].map({'Yes':1,'No':0})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Convet churn variable into 0s & 1s","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Churn']=data['Churn'].astype('int')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Primary Data Analysis of the data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Check for missing values in training set.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Removing target variable &  saving State as a series.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"state=data.pop('State')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Split the dataframe into x matrix and y target vector.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"x,y = data.drop(['Churn'], axis=1),data['Churn']\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x.shape,y.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The matrix and the vector have the same number of instances with all the columns in the matrix.\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Split x & y into train and test data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train,x_valid,y_train,y_valid=train_test_split(x,y,test_size=0.3,random_state=19)\nx_train.shape,x_valid.shape,y_train.shape,y_valid.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Building the classifier.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"tk=DecisionTreeClassifier(random_state=19)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Fit the train data into the classifier","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"tk.fit(x_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Model learning on validation set for prediction.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pre_valid=tk.predict(x_valid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pre_valid.shape,y_valid.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy_score(pre_valid,y_valid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y.value_counts(normalize=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n\nChurn depicting 1 says the percentage of clients about to churn out. 14.4 % is much bad as compared to 8.7 % which is determined by Decision Tree Classifier.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"HYPERPARAMETER TUNING using GridSearch Cross validation","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Set the combination of parameters for grid creation.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"param = {'max_depth': np.arange(2,11),'min_samples_leaf': np.arange(1,11)}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For CV, we require number of splits and shuffling after each also known as K Fold Stratified CV.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Kfold= StratifiedKFold(n_splits=5,shuffle=True,random_state=19)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Creation of Grid","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"best = GridSearchCV(estimator=tk,param_grid=param,cv=Kfold,n_jobs=-1,verbose=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Model fit with training.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"best.fit(x_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"81 candidate refer to 9 sets of permutations for max_depth with 10 sets of min_samples_leaf","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Cross Validation Assesment on model quality.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Our best set of parameters.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"best.best_params_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our best estimator.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"best.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our CV score for 'best' tree.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"best.best_score_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Validation Assesment**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Time to check accuracy of the model on validation set.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_val=best.predict(x_valid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy_score(pred_val,y_valid)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**So now our accuracy has increased from 91.3% to 94.2 %. This is always a better result.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Tree visuals","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dot_data=StringIO()\nexport_graphviz(decision_tree=best.best_estimator_,out_file=dot_data,filled=True,feature_names=data.drop(['Churn'], axis=1).columns)\ngraph=pydotplus.graph_from_dot_data(dot_data.getvalue())\nImage(value=graph.create_png())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The tree we see here is problematic for the naked eye, let us reduce the max_depth to 3 just for visualization purpose.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"tk9=DecisionTreeClassifier(random_state=19,max_depth=3).fit(x_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dot2_data=StringIO()\nexport_graphviz(decision_tree=tk9,out_file=dot2_data,filled=True,feature_names=data.drop(['Churn'], axis=1).columns)\ngraph=pydotplus.graph_from_dot_data(dot2_data.getvalue())\nImage(value=graph.create_png())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So, orange boxes resemble optimism ,i.e. there is scope of retention of more clients and vice versa for blue. Above figure is the threshold for split. gini again like entropy is worse if tends towards 1. Out of 2333 clients 1984 are loyal and rest would churn out. And accordingly we go down the tree depth via thresholds and splits.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"HYPERPARAMETER TUNING using hands on CV without any GRID SEARCH.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Kfold2= StratifiedKFold(n_splits=5,shuffle=True,random_state=19)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Lists of CV accuracies and Validation accuracies\n\nacc_depth , valid_acc=[],[]\nmax_depth_val= np.arange(2,10)\n#for each value of max_depth\nfor new_max_depth in tqdm_notebook(max_depth_val):\n    new=DecisionTreeClassifier(random_state=19,max_depth=new_max_depth)\n    \n    #performing cross validation\n    val=cross_val_score(estimator=tk,X=x_train,y=y_train,cv=Kfold2)\n    \n    #Appending all CV scores after each split to get their mean.\n    acc_depth.append(val.mean())\n    \n    #Asses the model on validation set\n    new.fit(x_train,y_train)\n    val2=new.predict(x_valid)\n    valid_acc.append(accuracy_score(val2,y_valid))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"acc_depth,valid_acc","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For sets of 8 max_depth parameter we observe the respective accuracy scores of cross validation and validation sets.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}