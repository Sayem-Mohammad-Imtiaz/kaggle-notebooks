{"cells":[{"metadata":{},"cell_type":"markdown","source":"In this notebook,I have done some Exploratory Data Analysis(EDA) on the data and also, I used different classifier models to predict the quality of the wine.\n1.\tLogistic Regression\n2.\tKNeighborsClassifier\n3.\tSVC\n4.\tDecisionTree Classifier\n5.\tRandomForest Classifier\nAnd also, I used cross validation evaluation technique to optimize the model performance.\n\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom matplotlib.ticker import FormatStrFormatter\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import accuracy_score,confusion_matrix,classification_report\nfrom sklearn.tree import DecisionTreeClassifier ,export_graphviz \nimport graphviz\nfrom IPython.display import Image  # To plot decision tree.\nfrom sklearn.externals.six import StringIO","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/red-wine-quality-cortez-et-al-2009/winequality-red.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display(np.round(df.describe()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.groupby('quality').count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"null_columns=df.columns[df.isnull().any()]\nprint(null_columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Finding the correlation bewteen the Features.\nplt.figure(figsize=(10,5))\nheatmap = sns.heatmap(df.corr(), annot=True, fmt=\".1f\", cmap=\"Reds\")\nheatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=12)\nheatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize=12)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#relationship between the some of features\ncols_sns = ['residual sugar', 'chlorides', 'density', 'pH', 'alcohol', 'quality']\nsns.set(style=\"ticks\")\nsns.pairplot(df[cols_sns],hue=\"quality\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#BoxPlot for different features.\nfeatures = [\"fixed acidity\", \"volatile acidity\", \"citric acid\", \"residual sugar\", \"chlorides\",\n            \"free sulfur dioxide\", \"total sulfur dioxide\", \"density\", \"pH\", \"sulphates\", \"alcohol\"]\n\nfig = plt.figure(figsize=(16,8))\nfor i in range(len(features)):\n    ax1 = fig.add_subplot(3, 4, i+1)\n    ax1.xaxis.set_major_formatter(FormatStrFormatter('%.2f'))\n    ax1.yaxis.set_major_formatter(FormatStrFormatter('%.2f'))\n    sns.boxplot(x='quality', y=features[i], data=df,palette=\"Set3\")\n    i = i + 1\nplt.subplots_adjust(hspace = 0.5)\nplt.subplots_adjust(wspace = 0.5)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize = (10,6))\nsns.barplot(x='quality', y='sulphates',data=df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nbins = (2, 5.5, 8)\ngroup_names = ['bad', 'good']\ndf['quality'] = pd.cut(df['quality'], bins = bins, labels = group_names)\n\nlabel_quality = LabelEncoder()\ndf['quality'] = label_quality.fit_transform(df['quality'])\ndf['quality'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.drop('quality', axis=1)\ny = df['quality']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y,test_size = 0.2)\n\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.fit_transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"X_train {0} , X_test {1} \" .format(X_train.shape , X_test.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#------------------------######### LogisticRegression #######-----------------\nlrg_classifier = LogisticRegression(solver='newton-cg',tol= 0.0001,C= 0.5,)\nlrg_classifier.fit(X_train, y_train.ravel())\n\ncv_lr = cross_val_score(estimator = lrg_classifier, X = X_train, y = y_train.ravel(), cv = 10)\nprint(\"CV: \", cv_lr.mean())\n\nlt_y_pred_train = lrg_classifier.predict(X_train)\naccuracy_lr_train = accuracy_score(y_train, lt_y_pred_train)\nprint(\"Training set accuracy for Logistic Regression: \", accuracy_lr_train)\n\ny_pred_lr_test = lrg_classifier.predict(X_test)\naccuracy_lr_test = accuracy_score(y_test, y_pred_lr_test)\nprint(\"Test set accuracy for Logistic Regression: \", accuracy_lr_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lrg_classifier.fit(X_train, y_train)\nlr_y_pred_test = lrg_classifier.predict(X_test)\n\nprint(classification_report(y_test, lr_y_pred_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"confusion_matrix(y_test, lr_y_pred_test)\n\ntp_lr = confusion_matrix(y_test, lr_y_pred_test)[0,0]\nfp_lr = confusion_matrix(y_test, lr_y_pred_test)[0,1]\ntn_lr = confusion_matrix(y_test, lr_y_pred_test)[1,1]\nfn_lr = confusion_matrix(y_test, lr_y_pred_test)[1,0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#------------------------#########  KNeighbors #######-----------------\nfrom sklearn.neighbors import KNeighborsClassifier\nknn_classifier = KNeighborsClassifier(leaf_size = 1, metric = 'minkowski', n_neighbors = 28, weights = 'distance')\nknn_classifier.fit(X_train, y_train.ravel())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predicting Cross Validation Score\nknn_cv = cross_val_score(estimator = knn_classifier, X = X_train, y = y_train.ravel(), cv = 10)\nprint(\"CV: \", knn_cv.mean())\n\nknn_y_pred_train = knn_classifier.predict(X_train)\nknn_accuracy_train = accuracy_score(y_train, knn_y_pred_train)\nprint(\"Training set accuracy for KNN: \", knn_accuracy_train)\n\nknn_y_pred_test = knn_classifier.predict(X_test)\nknn_accuracy_test = accuracy_score(y_test, knn_y_pred_test)\nprint(\"Test set accuracy for KNN: \", knn_accuracy_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"confusion_matrix(y_test, knn_y_pred_test)\n\ntp_knn = confusion_matrix(y_test, knn_y_pred_test)[0,0]\nfp_knn = confusion_matrix(y_test, knn_y_pred_test)[0,1]\ntn_knn = confusion_matrix(y_test, knn_y_pred_test)[1,1]\nfn_knn = confusion_matrix(y_test, knn_y_pred_test)[1,0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#------------------------#########  SVC #######-----------------\nfrom sklearn.svm import SVC\nsvm_linear_classifier = SVC(kernel = 'linear')\nsvm_linear_classifier.fit(X_train, y_train.ravel())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svm_linear_cv = cross_val_score(estimator = svm_linear_classifier, X = X_train, y = y_train.ravel(), cv = 10)\nprint(\"CV: \", svm_linear_cv.mean())\n\nsvm_linear_train_y_pred = svm_linear_classifier.predict(X_train)\n\nsvm_linear_accuracy_train = accuracy_score(y_train, svm_linear_train_y_pred)\nprint(\"Training set accuracy for SVC: \", svm_linear_accuracy_train)\n\nsvm_linear_y_pred_test = svm_linear_classifier.predict(X_test)\nsvm_linear_accuracy_test = accuracy_score(y_test, svm_linear_y_pred_test)\nprint(\"Test set accuracy for SVC: \", svm_linear_accuracy_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"confusion_matrix(y_test, svm_linear_y_pred_test)\n\ntp_svm_linear = confusion_matrix(y_test, svm_linear_y_pred_test)[0,0]\nfp_svm_linear = confusion_matrix(y_test, svm_linear_y_pred_test)[0,1]\ntn_svm_linear = confusion_matrix(y_test, svm_linear_y_pred_test)[1,1]\nfn_svm_linear = confusion_matrix(y_test, svm_linear_y_pred_test)[1,0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#------------------------#########  DecisionTree Classifier #######-----------------\n\ndt_classifier = DecisionTreeClassifier(criterion = 'gini', max_features=6, max_leaf_nodes=400, random_state = 33, max_depth=4)\ndt_classifier.fit(X_train, y_train.ravel())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dt_cv = cross_val_score(estimator = dt_classifier, X = X_train, y = y_train.ravel(), cv = 10)\nprint(\"CV: \", dt_cv.mean())\n\ndt_y_pred_train = dt_classifier.predict(X_train)\ndt_accuracy_train = accuracy_score(y_train, dt_y_pred_train)\nprint(\"Training set accuracy for DecisionTree: \", dt_accuracy_train)\n\ndt_y_pred_test = dt_classifier.predict(X_test)\ndt_accuracy_test = accuracy_score(y_test, dt_y_pred_test)\nprint(\"Test set accuracy for DecisionTree: \", dt_accuracy_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"confusion_matrix(y_test, dt_y_pred_test)\n\ntp_dt = confusion_matrix(y_test, dt_y_pred_test)[0,0]\nfp_dt = confusion_matrix(y_test, dt_y_pred_test)[0,1]\ntn_dt = confusion_matrix(y_test, dt_y_pred_test)[1,1]\nfn_dt = confusion_matrix(y_test, dt_y_pred_test)[1,0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = export_graphviz(dt_classifier,out_file=None,feature_names=list(X.columns.values),class_names=None,   \n                         filled=True, rounded=True,  \n                         special_characters=True,proportion=True)\n\ngraph = graphviz.Source(data)\ngraph","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#------------------------#########  RandomForest Classifier #######-----------------\nfrom sklearn.ensemble import RandomForestClassifier\nrf_classifier = RandomForestClassifier(criterion = 'entropy', max_features = 4, n_estimators = 800, random_state=33)\nrf_classifier.fit(X_train, y_train.ravel())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predicting Cross Validation Score\nrf_cv = cross_val_score(estimator = rf_classifier, X = X_train, y = y_train.ravel(), cv = 10)\nprint(\"CV: \", rf_cv.mean())\n\nrf_y_pred_train = rf_classifier.predict(X_train)\nrf_accuracy_train = accuracy_score(y_train, rf_y_pred_train)\nprint(\"Training set: \", rf_accuracy_train)\n\nrf_y_pred_test = rf_classifier.predict(X_test)\nrf_accuracy_test = accuracy_score(y_test, rf_y_pred_test)\nprint(\"Test set: \", rf_accuracy_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#..--------Important Features of Random Forest\nfeature_importances = pd.DataFrame(rf_classifier.feature_importances_,\n                                   index = X.columns,columns=['importance']).sort_values('importance', ascending=False)\nprint(feature_importances)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"importance = pd.DataFrame({'Importance': rf_classifier.feature_importances_}, index=X.columns)\nimportance.sort_values('Importance', axis=0, ascending=True).plot(kind='barh', color='r')\nplt.xlabel('Variable importance')\nplt.legend(loc='lower right')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"confusion_matrix(y_test, rf_y_pred_test)\n\ntp_rf = confusion_matrix(y_test, rf_y_pred_test)[0,0]\nfp_rf = confusion_matrix(y_test, rf_y_pred_test)[0,1]\ntn_rf = confusion_matrix(y_test, rf_y_pred_test)[1,1]\nfn_rf = confusion_matrix(y_test, rf_y_pred_test)[1,0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#------------\n###-----------Comparsion between the different models of Accuracy and Cross Validation.\n\nmodels = [('Logistic Regression', tp_lr, fp_lr, tn_lr, fn_lr, accuracy_lr_train, accuracy_lr_test, cv_lr.mean()),\n          ('K-Nearest Neighbors (KNN)', tp_knn, fp_knn, tn_knn, fn_knn, knn_accuracy_train, knn_accuracy_test, knn_cv.mean()),\n          ('SVM', tp_svm_linear, fp_svm_linear, tn_svm_linear, fn_svm_linear, svm_linear_accuracy_train, svm_linear_accuracy_test, svm_linear_cv.mean()),\n          ('Decision Tree Classification', tp_dt, fp_dt, tn_dt, fn_dt, dt_accuracy_train, dt_accuracy_test, dt_cv.mean()),\n          ('Random Forest Tree Classification', tp_rf, fp_rf, tn_rf, fn_rf, rf_accuracy_train, rf_accuracy_test, rf_cv.mean())\n         ]\n\npredict = pd.DataFrame(data = models, columns=['Model', 'True Positive', 'False Positive', 'True Negative','False Negative', 'Accuracy(training)', 'Accuracy(test)',\n                                               'Cross-Validation'])\npredict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f, axes = plt.subplots(2,1, figsize=(14,10))\npredict.sort_values(by=['Accuracy(training)'], ascending=False, inplace=True)\nsns.barplot(x='Accuracy(training)', y='Model', data = predict, palette='Blues_d', ax = axes[0])\n#axes[0].set(xlabel='Region', ylabel='Charges')\naxes[0].set_xlabel('Accuracy (Training)', size=16)\naxes[0].set_ylabel('Model')\naxes[0].set_xlim(0,1.0)\naxes[0].set_xticks(np.arange(0, 1.1, 0.1))\npredict.sort_values(by=['Accuracy(test)'], ascending=False, inplace=True)\nsns.barplot(x='Accuracy(test)', y='Model', data = predict, palette='Reds_d', ax = axes[1])\n#axes[0].set(xlabel='Region', ylabel='Charges')\naxes[1].set_xlabel('Accuracy (Test)', size=16)\naxes[1].set_ylabel('Model')\naxes[1].set_xlim(0,1.0)\naxes[1].set_xticks(np.arange(0, 1.1, 0.1))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After comparing all the models, we can observe that RF classifier has produced better results..\n\nPlease leave in comments in case of any questions, concerns, and feedback! Thank you."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}