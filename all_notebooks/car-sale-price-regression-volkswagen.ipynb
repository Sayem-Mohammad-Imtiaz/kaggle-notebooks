{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"***Car sale price regression for Volkswagen***\n\nThis was my first notebook that I made myself from scratch. Very interesting dataset.\n\nPlease comment on what you would improve or try out further? Suggestions or improvements would be interesting and appreciated!","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load packages\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n# Load all data:\nall_data = pd.read_csv('/kaggle/input/used-car-dataset-ford-and-mercedes/vw.csv')\n# Change order of data\ncol_index_new = [0,1,3,4,5,6,7,8,2] # Third column should become last one \nall_data = all_data.reindex(columns = all_data.columns.values[col_index_new])\nall_data.head(5), all_data.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot of distribution as well as density\nplt.figure(figsize=(10,10))\nh2 = sns.histplot(data= all_data, x = 'price', hue ='model',kde=True, linewidth=0)\nprint('Skewness = %f and Kurtosis = %f' % (all_data['price'].skew(),all_data['price'].kurt()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Take-aways:**\n* The overall distribution seems to consist of different peaks. This could be possible explained by the different types of cars (in the industry different classes/segments). \n* Most of the cars on sale are of type: Polo, closely followed by Golf.\n* Golf has a second peak? GTI spec?  \n\nLet's now look for correlations:","metadata":{}},{"cell_type":"code","source":"# EDA: Correlations, scatter plots etc.\ncorr = all_data.corr()\nmask = np.triu(np.ones_like(corr))\ncmap=sns.diverging_palette(20, 220, n=200),\nh1 = sns.heatmap(corr,mask=mask,cmap=sns.diverging_palette(20, 220, n=200), square=True, annot = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***Take-aways:***\n* The year the car is made is most strongly correlated with the sale price. Newer = More expensive\n* More milage = cheaper car evidently.\n* Tax and engine size is not as correlated as I expected. Tax calculation might be based on factors such as weight as well, which is not included in the data.\n* Interestingly mpg is also quite strongly negatively correlated. There might be a reason for that gasoline = lower mpg but cheaper!\n\nLets dive (even) deeper in these last points! ","metadata":{}},{"cell_type":"code","source":"f,ax = plt.subplots(1,2,figsize=(12,5))\nsns.scatterplot(data=all_data,x='tax',y='engineSize',hue='fuelType',ax=ax[0])\nsns.scatterplot(data=all_data,x='mpg',y='price',hue='fuelType',ax=ax[1])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Seems like that last thought was indeed correct! Furthermore, the hybrids further strengthen this negative correlation!","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(12,12))\np1 = sns.pairplot(data=all_data,hue='model')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***Take-aways***\n* Clearly not all variables have a linear relationship with the target (price) variable. Linear models might struggle with this dataset because of this reason. Non-linear models required? ","metadata":{}},{"cell_type":"code","source":"# Check for missing data:\nall_data.isna().sum()>0\n# Dummy variables\nall_dataWithDummy = pd.get_dummies(all_data)\n# Predictor and target variables\nX_data = all_dataWithDummy.drop(['price'],axis=1)\ny_data = all_dataWithDummy['price']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***Take-aways***\n* None of the data is missing! \n\nLets get modeling!","metadata":{}},{"cell_type":"code","source":"# Import modeling packages\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.tree import DecisionTreeRegressor\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split train and test set\nX_train, X_test, y_train, y_test = train_test_split(X_data,y_data,test_size = 0.2, random_state = 8)\n# Create linear regression model\nLR = LinearRegression()\n# Fit linear regression model\nLR.fit(X_train,y_train)\n# Use linear regression to predict test set\ny_pred = LR.predict(X_test)\n# Quantify prediction\nRMSE = np.sqrt(mean_squared_error(y_test,y_pred))\nprint('First 5 real values to predict: ' +  str(np.round(y_test[0:5].values,1)))\nprint('First 5 predicted values: ' +  str(np.round(y_pred[0:5],1)))\nprint('Average error in price units: ' + str(np.round(RMSE,1)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"RFR = RandomForestRegressor()\nDTR = DecisionTreeRegressor()\nLR = LinearRegression()\n\nmodels = [RFR,DTR,LR]\n\nfor model in models:\n    model.fit(X_train,y_train)\n    y_pred = model.predict(X_test)\n    RMSE = np.sqrt(mean_squared_error(y_test,y_pred))\n    print(str(model),': ',np.round(RMSE,1))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***Take-aways:***\n* Random forest outperforms other models with default parameters\n\nNote: This is definitely NOT the most optimal model to use. There are more advanced/other models to try!\n","metadata":{}}]}