{"cells":[{"metadata":{},"cell_type":"markdown","source":"Notebook em complemento aos meus outros trabalhos, todos então disponíveis em meu github. Esse é um dos desafios disponíveis no Kaggle sobre o covid-19, e tentarei durante o trabalho realizar algumas das taferas.\n\nhttps://github.com/LeopoldoZanellato/python\n\nhttps://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge/tasks\n\n\nO dataset disponível hoje, tem um total de mais de 13GB e 120 mil artigos, no que eu trabalharei possui cerca de 3GB e 20 mil artigos.\nUm dos motivos pelo qual não utilizarei o de 13GB é de que utilizaria muita memória e tomaria muito tempo, e esse não é o propósito. O propósito é de apresentar pequenas soluções e demonstrar como o uso do machine learning poderá ajudar os profissionais de saúdes no combate ao covid-19\n\nEsse trabalho será divido em 2 partes:\n\nPrimeira parte a de exploração de dados dos artigos científicos, onde buscarei algumas respostas pelos desafios e como de uma maneira facil poderemos responder todos eles\n\nSegunda parte é a simulação de predições com a base de dados disponível pelo ministério da saúde. \n\n\nComeçaremos pelo tratamento dos dados:","execution_count":null},{"metadata":{"_kg_hide-output":true,"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"!pip install scispacy","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"!pip install pmdarima","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.4/en_core_sci_md-0.2.4.tar.gz","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"!python -m spacy download en","execution_count":null,"outputs":[]},{"metadata":{"id":"5pXNRuVOUp6d","outputId":"0ac0d89a-7c23-423d-fcb5-49b656163b12","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import glob, json, zipfile, en_core_sci_md\n\"\"\"from bokeh.models import ColumnDataSource, HoverTool, LinearColorMapper, CustomJS\nfrom bokeh.palettes import Category20\nfrom bokeh.transform import linear_cmap\nfrom bokeh.io import output_file, show\nfrom bokeh.transform import transform\nfrom bokeh.io import output_notebook\nfrom bokeh.plotting import figure\nfrom bokeh.layouts import column\nfrom bokeh.models import RadioButtonGroup\nfrom bokeh.models import TextInput\nfrom bokeh.layouts import gridplot\nfrom bokeh.models import Div\nfrom bokeh.models import Paragraph\nfrom bokeh.layouts import column, widgetbox\"\"\"\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport spacy, scispacy, operator\nimport matplotlib.pyplot as plt\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom IPython.core.display import HTML\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import MiniBatchKMeans\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom statsmodels.tools.eval_measures import rmse, mse\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom statsmodels.graphics.tsaplots import plot_acf,plot_pacf\nfrom statsmodels.tsa.statespace.tools import diff\nfrom pmdarima import auto_arima\nfrom statsmodels.tsa.stattools import adfuller\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"id":"OO2pAE-1U__9","outputId":"7edbe5da-ecf1-43f9-f02e-5c2b76648258","trusted":false,"_kg_hide-input":true},"cell_type":"code","source":"\"\"\"from google.colab import drive\ndrive.mount(\"/content/gdrive\")\"\"\";","execution_count":null,"outputs":[]},{"metadata":{"id":"qjD7XfpnVO8T","trusted":false,"_kg_hide-input":true},"cell_type":"code","source":"\"\"\"import zipfile\npath = '/content/gdrive/My Drive/CORD-19-research-challenge.zip'\nzip_object = zipfile.ZipFile(file = path, mode = 'r')\nzip_object.extractall('./')\nzip_object.close()\"\"\";","execution_count":null,"outputs":[]},{"metadata":{"id":"l0bANbPfVWWI","trusted":false,"_kg_hide-input":true},"cell_type":"code","source":"corona_features = {'paper_id': [], 'title': [],\n                   'abstract': [], 'text': []}","execution_count":null,"outputs":[]},{"metadata":{"id":"VFKgGg7LVYeW","trusted":false,"_kg_hide-input":true},"cell_type":"code","source":"corona_df = pd.DataFrame.from_dict(corona_features)","execution_count":null,"outputs":[]},{"metadata":{"id":"kFpSRzenVa9f","trusted":false,"_kg_hide-input":true},"cell_type":"code","source":"json_filenames = glob.glob(f'{\"./\"}//**/*.json', recursive = True)","execution_count":null,"outputs":[]},{"metadata":{"id":"QrEY6secVc_v","trusted":false,"_kg_hide-input":true},"cell_type":"code","source":"def return_corona_df(json_filenames, df):\n    \"Função para ler os arquivos json\"\n    for file_name in json_filenames:\n        row = {'paper_id': None, 'title': None,\n           'abstract': None,'text': None}\n    \n        with open(file_name) as json_data:\n            try:\n                if file_name == \"./sample_data/anscombe.json\":\n                    continue\n      \n                data = json.load(json_data)\n\n                row['paper_id'] = data['paper_id'].strip()\n                row['title'] = data['metadata']['title'].strip()\n\n                abstract_list = [abstract['text'] for abstract in data['abstract']]\n                abstract = '\\n '.join (abstract_list)\n                row['abstract'] = abstract.strip()\n\n                text_list = [text['text'] for text in data['body_text']]\n                text = '\\n '.join(text_list)\n                row['text'] = text.strip()\n\n                df = df.append(row, ignore_index = True)\n            except:\n                pass \n    return df\n","execution_count":null,"outputs":[]},{"metadata":{"id":"Ttc266NPVhdZ","trusted":false,"_kg_hide-input":true},"cell_type":"code","source":"corona_df = return_corona_df(json_filenames, corona_df)","execution_count":null,"outputs":[]},{"metadata":{"id":"Exo2Q0VvVifI","outputId":"4c60954c-6ed0-441e-b798-9063360be672","trusted":false,"_kg_hide-input":true},"cell_type":"code","source":"corona_df.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"XaOOUYO0VkQx","trusted":false,"_kg_hide-input":true},"cell_type":"code","source":"corona_df = corona_df[corona_df['title']!= \"\"]\ncorona_df = corona_df[corona_df['abstract']!= \"\"]","execution_count":null,"outputs":[]},{"metadata":{"id":"ZlhjmAGnV3RI","trusted":false,"_kg_hide-input":true},"cell_type":"code","source":"corona_df.drop_duplicates(['abstract','text','title'], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"id":"WTISNI6VWASE","outputId":"80088c95-ab7d-464a-e7ec-b75fc2a502bd","trusted":false,"_kg_hide-input":true},"cell_type":"code","source":"corona_df.shape","execution_count":null,"outputs":[]},{"metadata":{"id":"NlHS-uPhWBVp","trusted":false,"_kg_hide-input":true},"cell_type":"code","source":"corona_df.to_csv(\"pre_processado.csv\")","execution_count":null,"outputs":[]},{"metadata":{"id":"e1HqDyI-Wt9D","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"nlp = en_core_sci_md.load(disable = ['tagger', 'parser', 'ner'])\nnlp.max_length = 2000000","execution_count":null,"outputs":[]},{"metadata":{"id":"TbCK-GEsXF52","trusted":false,"_kg_hide-input":true},"cell_type":"code","source":"new_stop_words = ['et', 'al','doi','cppyright','http', \n                  'https', 'fig','table','result','show']\nfor word in new_stop_words:\n    nlp.vocab[word].is_stop = True","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Durante o tratamento dos dados realizaremos a lemmatização, e a função abaixo servirá para isso.\n\n\nA lematização é uma técnica, geralmente utilizada por buscador de palavras em sites, para abranger a quantidade de opções de palavras relacionadas a palavra buscada, ignorando o tempo verbal caso seja um verbo, o gênero da palavra, o plural e etc.\n\nPor exemplo, as palavras pensamento, pensando, e derivadas delas serão trocadas todas por \"pensar\", facilitando assim a \"padronização\" do texto.\n\nSerão retirados também todos os números (que estão sozinhos), stop words (palavras de ligação que são irrelevantes ao conjunto dos resultados), pontuações e espaços indevidos ","execution_count":null},{"metadata":{"id":"HmkLbq4oXZJo","trusted":true},"cell_type":"code","source":"def spacy_tokenizer(sentence):\n    sentence = sentence.lower()\n    lista = []\n    lista = [word.lemma_ for word in nlp(sentence) if not (word.is_stop or\n                                                         word.like_num or\n                                                         word.is_punct or \n                                                         word.is_space or\n                                                         len(word)==1)]\n    lista = ' '.join([str(element) for element in lista])\n    return lista","execution_count":null,"outputs":[]},{"metadata":{"id":"U5M4PCLPXs-E","trusted":false,"_kg_hide-input":true},"cell_type":"code","source":"corona_df['text'] = corona_df['text'].apply(spacy_tokenizer)","execution_count":null,"outputs":[]},{"metadata":{"id":"0jt5qXT4XygQ","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true,"collapsed":true},"cell_type":"code","source":"corona_df.to_csv(\"pos_processamento.csv\", encoding = \"UTF-8\")","execution_count":null,"outputs":[]},{"metadata":{"id":"40OK-smcPiGa","trusted":true},"cell_type":"code","source":"corona_df_completo = pd.read_csv(\"../input/coronavirus/pos_processamento.csv\", index_col = 0)\n#corona_df_completo = pd.read_csv(\"../input/CORD-19-research-challenge/metadata.csv\", index_col=0)","execution_count":null,"outputs":[]},{"metadata":{"id":"vzX9qPDqc-57","trusted":true},"cell_type":"code","source":"corona_df_completo.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Após o tratamento de dados podemos ver como ficou um dos textos","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(corona_df_completo.iloc[0][\"text\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"corona_df_completo.dropna(inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"id":"gKqrJ3n-dNy9","outputId":"203f3b17-41f8-4690-f3cc-4cee42c09c03","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"corona_df_completo.shape","execution_count":null,"outputs":[]},{"metadata":{"id":"qW81rMVxdPdS","outputId":"6093dd63-0bd2-4d61-f082-9a5981d9bf92","trusted":true},"cell_type":"code","source":"corona_df_completo.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Aqui estão todos os artigos, cada um com o seu respectivo ID, título , abstract e o texto","execution_count":null},{"metadata":{"id":"lKqkk-WHdkxx","trusted":true},"cell_type":"code","source":"dataset_texts = corona_df_completo['text'].tolist()","execution_count":null,"outputs":[]},{"metadata":{"id":"Rb1CtZcmeEVF","outputId":"88e55d1d-4acb-4304-abdf-83509eb0be30","trusted":true},"cell_type":"code","source":"len(dataset_texts)","execution_count":null,"outputs":[]},{"metadata":{"id":"9Mpt-F3ReFAl","outputId":"fbdc59ea-f80b-4401-ac23-6a71e07950df","trusted":true},"cell_type":"code","source":"tfidf = TfidfVectorizer(max_features=2**12) #utilização desse vetor para limitação do tamanho da matriz esparsa\nvectorized = tfidf.fit_transform(dataset_texts)\nvectorized","execution_count":null,"outputs":[]},{"metadata":{"id":"kHzsXYNIj4u9"},"cell_type":"markdown","source":"Redução de dimensionalidade para separação dos artigos em classes:","execution_count":null},{"metadata":{"id":"la0LA7aSj6b1","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"\npca = PCA(n_components = 2)\nX_pca = pca.fit_transform(vectorized.toarray())","execution_count":null,"outputs":[]},{"metadata":{"id":"aAn8MknwqfHm","outputId":"a4483cf3-5622-4ed5-9bc4-8161add1b6c2","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"components = pca.explained_variance_ratio_\ncomponents","execution_count":null,"outputs":[]},{"metadata":{"id":"Z4NMKjGArBuz"},"cell_type":"markdown","source":"A seguir será feita a definição dos clusters, apesar do gráfico demonstar que seriam interessantes 15 clusters, apenas para fins de demonstração, farei apenas com 5 grupos para uma melhor visualização, o interessante que conforme passamos o mouse pelos pontos, podemos então ler cada um dos títulos e o contexto do artigo","execution_count":null},{"metadata":{"id":"BxMbBlW1r6W9","outputId":"742f5a2c-aab5-4b80-ad71-de4c4effde84","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"wcss = []\nfor i in range(1,21):\n    kmeans = MiniBatchKMeans(n_clusters = i, random_state = 0)\n    kmeans.fit(vectorized)\n    wcss.append(kmeans.inertia_)\nplt.plot(range(1,21), wcss)\nplt.xlabel('Number of Clusters')\nplt.ylabel(\"WCSS\")","execution_count":null,"outputs":[]},{"metadata":{"id":"EDuVsXndsZiB","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"k = 5\nkmeans = MiniBatchKMeans(n_clusters=k, random_state = 16)\ny_pred = kmeans.fit_predict(vectorized)","execution_count":null,"outputs":[]},{"metadata":{"id":"5kvxTuw-tBAW","outputId":"8d443fae-b1a6-4680-e596-b5d2a0289f4e","trusted":true},"cell_type":"code","source":"np.unique(y_pred)","execution_count":null,"outputs":[]},{"metadata":{"id":"mqn7eXJmPFFL","outputId":"86decbc5-c52c-4496-e48f-448a6564d99e","trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"# Baseado em: https://www.kaggle.com/maksimeren/covid-19-literature-clustering\n\n\noutput_notebook()\ny_labels = y_pred\n\n# data sources\nsource = ColumnDataSource(data=dict(\n    x= X_pca[:,0], \n    y= X_pca[:,1],\n    x_backup = X_pca[:,0],\n    y_backup = X_pca[:,1],\n    desc= y_labels, \n    titles= corona_df_completo['title'],\n    abstract = corona_df_completo['abstract'],\n    labels = [\"C-\" + str(x) for x in y_labels]\n    ))\n\n# hover over information\nhover = HoverTool(tooltips=[\n    (\"Title\", \"@titles{safe}\"),\n    (\"Abstract\", \"@abstract{safe}\"),\n],\n                 point_policy=\"follow_mouse\")\n\n# map colors\nmapper = linear_cmap(field_name='desc', \n                     palette=Category20[9],\n                     low=min(y_labels) ,high=max(y_labels))\n\n# prepare the figure\np = figure(plot_width=800, plot_height=800, \n           tools=[hover, 'pan', 'wheel_zoom', 'box_zoom', 'reset'], \n           title=\"Covid-19 Papers\", \n           toolbar_location=\"right\")\n\n# plot\np.scatter('x', 'y', size=5, \n          source=source,\n          fill_color=mapper,\n          line_alpha=0.3,\n          line_color=\"black\",\n          legend = 'labels')\n\n# add callback to control \ncallback = CustomJS(args=dict(p=p, source=source), code=\"\"\"\n            \n            var radio_value = cb_obj.active;\n            var data = source.data; \n            \n            x = data['x'];\n            y = data['y'];\n            \n            x_backup = data['x_backup'];\n            y_backup = data['y_backup'];\n            \n            labels = data['desc'];\n            \n            if (radio_value == '5') {\n                for (i = 0; i < x.length; i++) {\n                    x[i] = x_backup[i];\n                    y[i] = y_backup[i];\n                }\n            }\n            else {\n                for (i = 0; i < x.length; i++) {\n                    if(labels[i] == radio_value) {\n                        x[i] = x_backup[i];\n                        y[i] = y_backup[i];\n                    } else {\n                        x[i] = undefined;\n                        y[i] = undefined;\n                    }\n                }\n            }\n\n\n        source.change.emit();\n        \"\"\")\n\n\n# option\noption = RadioButtonGroup(labels=[\"C-0\", \"C-1\", \"C-2\",\n                                  \"C-3\", \"C-5\",\n                                   \"All\"], \n                          active=5, callback=callback)\n\n#header\nheader = Div(text=\"\"\"<h1>Covid-19 Papers</h1>\"\"\")\n\n# show\nshow(column(header, widgetbox(option),p))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.display import Image\nImage(filename='../input/coronavirus/imagem3.jpg', width=500, height=500)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Como a função acima trás um gráfico panoramico com algumas funcionalidades, na hora de postar no github deu máximo de tamanho permitido, para solucionar esse problema, tive importar o print da imagem de como o gráfico funciona. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,8))\nsns.scatterplot(X_pca[:,0], X_pca[:,1], hue=y_pred,palette=\"bright\")\nplt.title(\"Covid-19 Papers\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Até agora foi utilizado o dataset completo, a partir desse ponto, utilizarei apenas uma amostra de 500 artigos para que não demore muito tempo. O propósito é demonstrar as ferramentas e como podem ser utilizadas","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"corona_df = corona_df_completo.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corona_df = corona_df.sample(n = 500, random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corona_df","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"!pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.0/en_core_web_sm-2.2.0.tar.gz","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text = str(corona_df['text'][25534][:1000])\n","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"nlp_ent = spacy.load(\"en_core_web_sm\")\nnlp_ent.max_length = 2000000","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"doc = nlp_ent(text)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Utilizarei a biblioteca Spacy para fazer a leitura de linguagem natural. Trouxe uma definição de linguagem natural segundo o site do wikipedia:\n\nLíngua natural (língua humana, língua idiomática, ou somente língua ou idioma) é qualquer linguagem desenvolvida naturalmente pelo ser humano, de forma não premeditada, como resultado da facilidade inata para a linguagem possuída pelo intelecto humano. Vários exemplos podem ser dados como as línguas faladas e as línguas de sinais. A linguagem natural é normalmente utilizada para a comunicação. As línguas naturais são diferentes das línguas construídas e das línguas formais, tais como a linguística computacional, a língua escrita, a linguagem animal e as linguagens usadas no estudo formal da lógica, especialmente da lógica matemática.\n\n\nUm exemplo disso é que o próprio algoritmo ja define o que é localização, pessoas, nacionalidade, nomes próprio etc... Assim, podemos verificar os países que são mais citados nos textos\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Abaixo podemos ver como o modelo se comporta quando separa as entidades, pessoas, datas, localizações etc...","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from spacy import displacy\ndisplacy.render(doc,style = 'ent', jupyter = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gpe = []\nfor index, row in corona_df.iterrows():\n    text = row['text']\n    doc = nlp_ent(text)\n    for entity in doc.ents:\n        if entity.label_ =='GPE':\n            gpe.append(str(entity.text))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"values_gpe,counts_gpe = np.unique(np.array(gpe), return_counts = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"gpe_df = pd.DataFrame({'value': values_gpe, 'counts': counts_gpe})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"gpe_df = gpe_df.sort_values(by='counts', ascending=False).head(8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def autolabel(rects):\n    \"\"\"Attach a text label above each bar in *rects*, displaying its height.\"\"\"\n    for rect in rects:\n        height = round(float(rect.get_height()),2)\n        ax.annotate('{}'.format(height),\n                    xy=(rect.get_x() + rect.get_width() / 2, height),\n                    xytext=(0, 3),  # 3 points vertical offset\n                    textcoords=\"offset points\",\n                    ha='center', va='bottom')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(10,8))\nrect1 = ax.bar(x=gpe_df[\"value\"], height=gpe_df[\"counts\"])\nplt.title(\"Citações dos países nos 500 textos\")\nplt.ylabel('Citações de cada País / localização')\nplt.xlabel('País/Localização')\nautolabel(rect1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Agora começarei a parte de como explorar os textos!! Faremos a busca por palavras e nos retornará o texto, abstract e o título do texto","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def find_all_texts(input_str, search_str, number_of_words):\n    text_list = []\n    index = 0\n    number_of_words = number_of_words\n    while index < len(input_str):\n        i = input_str.find(search_str, index)\n        if i == -1:\n            return text_list\n    \n        if input_str[i-number_of_words:i] == '':\n            start = 0\n        else:\n            start = i - number_of_words\n    \n        text_list.append(input_str[start:i] + input_str[i:i + number_of_words])\n        index = i + i\n    return text_list","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nlp = en_core_sci_md.load(disable = ['tagger', 'parser', 'ner'])\nnlp.max_length = 2000000","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def spacy_tokenizer(sentence):\n    sentence = sentence.lower()\n    lista = []\n    lista = [word.lemma_ for word in nlp(sentence) if not (word.is_stop or\n                                                         word.like_num or\n                                                         word.is_punct or \n                                                         word.is_space or\n                                                         len(word)==1)]\n    lista = ' '.join([str(element) for element in lista])\n    return lista","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Até agora foi utilizado a opção de de busca de apenas 1 palavra, onde aparece o Título, ID e quantas vezes a palavra desejada apareceu no artigo, facilitando assim a busca por conteúdo.\n\nA seguir, será feita a busca por mais de uma palavra, conforme o desejo do especialista de saúde","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"search_strings = [\"traveler\"]\ntokens_list = [nlp(spacy_tokenizer(item)) for item in search_strings]\ntokens_list","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from spacy.matcher import PhraseMatcher\nmatcher = PhraseMatcher(nlp.vocab)\nmatcher.add(\"SEARCH\", None, *tokens_list)\nnumber_of_words = 50","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"corona_df_200 = corona_df.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corona_df_200 = corona_df_200[:200]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfor index, row in corona_df_200.iterrows():\n    marked_text = \"\"\n    doc = nlp(row[\"text\"])\n    paper_id = row[\"paper_id\"]\n    title=row['title']\n    matches = matcher(doc)\n    if matches == []:\n        continue\n        \n    print(f\"\\n \\n \\nWords: {search_strings}\\n\")\n    print(f\"Title: {title}\\n\")\n    print(f\"Paper ID: {paper_id}\\n\")\n    print(f\"Matches: {len(matches)}\\n\")\n    \n    \n    for i in matches:\n        start=i[1] - number_of_words\n        if start<0:\n            start=0\n        for j in range(len(tokens_list)):\n            if doc[i[1]:i[2]].similarity(tokens_list[j]) ==1:\n                search_text = str(tokens_list[j])\n                market_text = str(doc[start:i[2] + number_of_words]).replace(search_text, search_text)\n                print(f\"TEXTO: {market_text}\")\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"O próximo desafio é verificar a similaridade de alguns textos com os artigos, obtendo aqueles artigos que possuem uma maior similaridade\n\nUm dos textos propostos é: 'Guidance on ways to scale up NPIs in a more coordinated way (e.g., establish funding, infrastructure and authorities to support real time, authoritative (qualified participants) collaboration with all states to gain consensus on consistent guidance and to mobilize resources to geographic areas where critical shortfalls are identified) to give us time to enhance our health care delivery system capacity to respond to an increase in cases. Rapid design and execution of experiments to examine and compare NPIs currently being implemented. DHS Centers for Excellence could potentially be leveraged to conduct these experiments'","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"texts = corona_df['text'].tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"tfidf = TfidfVectorizer()\nvectorized = tfidf.fit_transform(texts)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"search_string = 'Guidance on ways to scale up NPIs in a more coordinated way (e.g., establish funding, infrastructure and authorities to support real time, authoritative (qualified participants) collaboration with all states to gain consensus on consistent guidance and to mobilize resources to geographic areas where critical shortfalls are identified) to give us time to enhance our health care delivery system capacity to respond to an increase in cases. Rapid design and execution of experiments to examine and compare NPIs currently being implemented. DHS Centers for Excellence could potentially be leveraged to conduct these experiments'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"search_string = spacy_tokenizer(search_string)\nprint(search_string)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"search_string_vectorized = tfidf.transform([search_string])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"similarity = cosine_similarity(search_string_vectorized, vectorized)\nsimilarity","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"scores_dict = {}\nfor i in range(len(similarity[0])):\n    scores_dict[i] = similarity[0][i]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"sorted_scores = sorted(scores_dict.items(), key=operator.itemgetter(1), reverse = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Aqui então temos por ordem de importância, aqueles artigos onde foi encontrado uma maior similaridade com a frase pesquisada, lembrando que aqui estamos utilizando apenas em 500 artigos, porém pode ser utilizado em todos os outros do banco de dados","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in sorted_scores[:5]:\n    df=corona_df.iloc[i[0]]\n    print(f\"Title: {df['title']}\")\n    print(f\"Paper ID: {df['paper_id']}\")\n    print(f\"Score: {i[1]}\")\n    print(f\"Abstract: {str(df['abstract'])[0:500]}\")\n    print(\"--------------------------------------------------------------------------------------------------------------------------------------\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A seguir será realizado alguns testes para saber se é possível prever com algum modelo o número de mortes pelo covid aqui no Brasil, e sempre lembrando que esse trabalho é apenas para fins teóricos e de aprendizado. Explicarei também um pouco do modelo que será utilizado para as previsões. A intenção não é contar mortes!!\nO primeiro dataset que estarei trabalhando, a data irá do dia 17/03 (dia do primeiro caso de morte) até o dia 28/6, e então farei previsões utilizando modelos estatísticos e de machine learning com a finalidade de tentar prever os próximos dias","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_brasil = pd.read_csv(\"../input/coronavirus/brazil_covid19_macro.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"brasil = df_brasil[['date', 'deaths']].groupby('date').sum().reset_index()\nbrasil = brasil[brasil['deaths'] >0]\nbrasil['date'] = pd.to_datetime(brasil['date'])\nbrasil.set_index('date', inplace=True)\nbrasil.index.freq = \"D\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"brasil","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plt.figure(figsize=(8,5))\nplt.title(\"Covid-19 no Brasil\")\nplt.plot(brasil.values,label=\"nº Mortes\")\nplt.xlabel(\"Dias\")\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_acf(brasil['deaths'], lags=40);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_pacf(brasil['deaths'], lags=40);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"seasonal = seasonal_decompose(brasil['deaths'], model='aditive');\nseasonal.plot();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Podemos então dividir o primeiro gráfico entre a tendência, a sazonalidade e o residuo\n\nA tendência é em que forma que o gráfico está se aprensentando? Em alta? Em queda?\n\nSazonalidade é os períodos que se repetem, por exemplo, no inverno as pessoas costumam viajar mais do que no verão, e isso se repete todo ano\n\nResiduo é tudo aquilo que não pode ser explicado pela tendência e pela sazonalidade\n\nA seguir podemos aproximar a sazonalidade e afirmar que existe uma sazonalidade semanal aos finais de semana, isso poderia ser explicado pelo atraso dos dados aos sábados e domingos","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plt.figure(figsize=(12,5))\nseasonal.seasonal.plot();\nplt.title(\"Seasonal\")\nplt.xlabel('Date')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Abaixo temos o gráfico de tendência, como ja poderíamos supor, existe uma tendência de alta nos números de mortes","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plt.figure(figsize=(12,5))\nseasonal.trend.plot()\nplt.title(\"Tendência\")\nplt.xlabel('Date')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Abaixo vemos o gráfico de resíduos, que é tudo aquilo que não pode ser explicado nem pela tendência e nem pela sazonalidade","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plt.figure(figsize=(12,5))\nseasonal.resid.plot()\nplt.title(\"Residual\")\nplt.xlabel('Date')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A seguir devemos então verificar se a série é estacionária ou não, para isso é utilizaremos o teste do adfuller\n\nO seguinte teste retornará uma tupla de estatísticas do teste do ADF, como Estatística do teste Valor-P; Número de defasagens usadas; Número de observações usadas para a regressão do ADF e um dicionário de Valores críticos.\n\nResumindo, se o valor-p encontrado pelo teste for < 0,05, a série é estacionária, ja se o valor for acima de 0,05 a série não é estacionária e por esse motivo devemos então normalizala.\n\nUtilizei uma função para simplificar a visualização dos valores, e o valor p encontrado foi superior a 0,05, ou seja, a série não é estacionaria e para realizar as previsões devemos transforma-la em estacionária","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def adf_test(series,title=''):\n    \"\"\"\n    Pass in a time series and an optional title, returns an ADF report\n    \"\"\"\n    print(f'Augmented Dickey-Fuller Test: {title}')\n    result = adfuller(series.dropna(),autolag='AIC') # .dropna() handles differenced data\n    \n    labels = ['ADF test statistic','p-value','# lags used','# observations']\n    out = pd.Series(result[0:4],index=labels)\n\n    for key,val in result[4].items():\n        out[f'critical value ({key})']=val\n        \n    print(out.to_string())          # .to_string() removes the line \"dtype: float64\"\n    \n    if result[1] <= 0.05:\n        print(\"Strong evidence against the null hypothesis\")\n        print(\"Reject the null hypothesis\")\n        print(\"Data has no unit root and is stationary\")\n    else:\n        print(\"Weak evidence against the null hypothesis\")\n        print(\"Fail to reject the null hypothesis\")\n        print(\"Data has a unit root and is non-stationary\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"adf_test(brasil[\"deaths\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Para transformar em uma série estacionária, foi necessária a realização de 2 diferenciações. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df1 = brasil.copy()\nplt.figure(figsize=(12,5))\ndf1['d2'] = diff(brasil['deaths'],k_diff=2)\ndf1['d2'][2:].plot();\nplt.title(\"Stacionary timeseries\")\nplt.ylabel(\"Date\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"adf_test(df1['d2'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"como podemos ver, agora se tornou uma série estacionária!!","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"MODELO SARIMA\n\nDe uma forma resumida, o ARIMA model é composto por:\n\nAR(p) = Autoregression model, prevemos utilizando a combinação de valores passados da próprima variável. Gera modelos lineares.\n\nRepresentado pela letra P MA(d) = é o modelo de média móvel. ARMA(p,q) = A junção dos dois acima - representado pela letra Q \n\nARIMA(p,q,d) = O mesmo processo que ocorre para o ARMA + aplicação da diferenciação para tornar a série estacionária.\n\nTemos então o SARIMAX (termo genérico), além dos parâmetros (p,q,d) aceita também o (P,D,Q)m, descrevendo os componentes sazonais. P,D e Q representa a regressão sazonal, diferenciação, e média movel, m representa o número de pontos para cada ciclo.\n\nO X representa a variável exógena, como não utilizaremos ela, não entrarei em maiores detalhes.\n\nPara a definição dos melhores parâmetros existe a forma manual e a forma automática, onde a função definirá os melhores parâmetros para nós...\n\nFunção: Auto_arima, devo informar ainda o meu dataset, definir os pontos de start do \"p\" e do \"q\", definir a sazonalidade e o período de sazonalidade, no caso são de 7 dias.\n\nO objetivo é achar os melhores parâmetros em base no valor AIC, que deve ser o menor possível, com a menor complexidade\n\nEssa ferramenta impede que cometemos erros na hora de analisar os gráficos e realizar transformações nos dados, assim ele nos informa os melhores parâmetros!!\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_set = brasil[:90]\ntest_set = brasil[90:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sarima = auto_arima(brasil['deaths'],start_p=0, start_q=0,seasonal=True,trace=True, m=7)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sarima","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sarima.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start = len(train_set)\nend = len(train_set) + len(test_set) - 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = SARIMAX(train_set['deaths'], order=(2, 1, 3),seasonal_order=(0, 1, 1, 7)).fit()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = model.predict(start,end,typ=\"levels\").rename(\"SARIMAX(2, 1, 3)x(0, 1, 1, 7)\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"predictions","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Nos gráficos abaixo podemos ver que as predições foram muito próximas aos valores de teste!! O que faremos depois é prever por mais uma semana e depois conferir com os dados**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test_set['deaths'].plot(label=\"test set\", legend=True)\ntrain_set['deaths'].plot(legend=True, label=\"train set\")\npredictions.plot(label=\"prediction\", legend=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_set['deaths'].plot(label=\"test set\", legend=True)\npredictions.plot(label=\"prediction\", legend=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rmse(test_set['deaths'], predictions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start = len(train_set)\nend = len(train_set) + len(test_set) + 6","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = model.predict(start,end,typ=\"levels\").rename(\"SARIMAX(2, 1, 3)x(0, 1, 1, 7)\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Previsões feitas até o dia 05/07/2020, agora vamos conferir com os dados oficiais**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"brasil_covid = pd.read_csv(\"../input/coronavirus/brasil_covid.csv\", index_col=1, parse_dates=True,dayfirst=True, sep=\";\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"comparacao = brasil_covid[-21:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,5))\ntrain_set['deaths'].plot(legend=True, label=\"Dataset de Treino\")\ncomparacao[\"obitosAcumulado\"].plot(legend=True, label=\"Dataset de Test\")\npredictions.plot(legend=True, label=\"Predictions\")\nplt.title(\"Predictions\")\nplt.ylabel(\"Mortes\")\nplt.xlabel(\"Data\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Novamente os valores foram muito próximos aos reais, chegando ao dia 5 com erro de 7 mortes no total acumulado","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,5))\ncomparacao[\"obitosAcumulado\"].plot(legend=True, label=\"Dataset de Test\")\npredictions.plot(legend=True, label=\"Predictions\")\nplt.title(\"Predictions\")\nplt.ylabel(\"Mortes\")\nplt.xlabel(\"Data\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataframe = pd.DataFrame({\"predictions\": predictions.values,\n                          \"mortes\":comparacao['obitosAcumulado']})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataframe['diferença'] = dataframe['predictions'] - dataframe['mortes']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataframe['diferença'] = dataframe['diferença'].astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataframe","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**CONCLUSÃO:**\n\nPodemos ver de uma forma simplificada, como a utilização do machine learning e técnicas de Inteligência Artificial, podem contribuir para a soluções de alguns problemas e pode também automatizar tarefas que poderiam levar muito tempo. Podemos concluir que com a utilização de modelos preditivos, é possível prever com uma certa precisão os números de mortes para que as governantes possam tomar atitudes acertivas.\n\nAlgumas considerações: \n\nAs previsões foram feitas apenas com relação as datas, mas outros fatores podem inteferir nos números.\n\nQualquer tipo de erro que eu cometi, podem entrar em contado, sempre gosto de críticas e sugestões\n\nPara a exploração dos textos foi utilizado apenas 500 textos de forma 'aleatória' pois caso utilizasse os 20 mil, tomaria muito tempo de processamento.\n\nAlgumas das funções utilizadas tomei como base de outros trabalhos, sendo assim, citei quando necessário.\n\n\n","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}