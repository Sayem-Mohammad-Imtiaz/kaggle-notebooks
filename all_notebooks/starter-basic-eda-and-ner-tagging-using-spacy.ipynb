{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Objective : This code make use of complaint data against financial companies to perform following tasks on the data\n\n    1. Identify company names using NER tagging\n    \n    2. Create Topic models to classify comlplaints in various categories\n  \n    \n## About Data :\nThe dataset comprises of Consumer Complaints on Financial products and weâ€™ll see how to classify consumer complaints text into these categories: Debt collection, Consumer Loan, Mortgage, Credit card, Credit reporting, Student loan, Bank account or service, Payday loan, Money transfers, Other financial service, Prepaid card.\nAlso we will try to identify the companies from the dataset\n    ","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport pandas as pd\n%matplotlib inline\nfrom matplotlib import pyplot as plt\nimport nltk \nimport spacy ### For NER tagging\nimport seaborn as sns\nimport pickle\nimport wordcloud\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#### Read the compalints data csv\ncomplaint_data = pd.read_csv(\"../input/consumer-complaints-financial-products/Consumer_Complaints.csv\",low_memory = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Convert the columns names so that they don't have space and are more readable\ncomplaint_data.columns = [i.lower().replace(\" \",\"_\").replace(\"-\",\"_\") for i in complaint_data.columns]\ncomplaint_data.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Let us do basic description of the data\nprint (\"The shape of data is \",complaint_data.shape)\nprint (\"The data types for our data are as follows \")\nprint (complaint_data.info())\n\nprint (complaint_data.describe(include= 'object'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### All the varables are text - which may correspond to categories and other variables\nprint (\" The number of unique values in each column is as follows\")\n### Lets do a describe with including objects\ncomplaint_data.describe(include = 'object').T.reset_index()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"######  From above description we see that only 114704 rows have complaint text and as we are interested in only those row which have complaint text. We wll drop all rows where complaint narrative is NA or Blanks","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#### Keep only the consumer complaints is not null\ncomplaint_data = complaint_data[~complaint_data['consumer_complaint_narrative'].isna()]\n#### Create a distirbution of length of customers complaints. We have very left skew in length of complaints\n### Which is expected as most compalints can be written in less than 500 words\ncomplaint_data['consumer_complaint_narrative'].apply(len).plot(kind = 'hist',title = 'Histogram by length of compalints text')\nplt.xlabel(\"Number of complaints\")\n### Keep the length columns as a new column\ncomplaint_data['comp_length'] = complaint_data['consumer_complaint_narrative'].apply(len)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Distribution of complaints by length of text has Right Skew. This is intuitive as most complaints have text less than 1000 words","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"### Lets look at complaints distribution by product type\nfig,ax = plt.subplots(figsize=(24,6))\ncomplaint_data['product'].value_counts().plot(kind = 'bar',title = 'Complaints By Product')\nplt.xlabel(\"Product\")\nplt.ylabel(\"Number of complaints\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Products which have maximum complaints are related to lending is some or other ways. \n1. Debt Collection\n2. Mortgage\n3. Credit Reporting\n4. Credit Card\n5. Bank account or service","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"### Lets look at the distribution of every product by distputed or not\npd.crosstab(complaint_data['product'],complaint_data['consumer_disputed?']).reset_index().set_index('product').sort_values('No',ascending = False).plot(kind='bar',title = 'Distribution of compalints by labels')\nprint (pd.crosstab(complaint_data['product'],complaint_data['consumer_disputed?']).reset_index().set_index('product').sort_values('No',ascending = False))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Company Distribution by number of complaints\ncomplaint_data['company'].value_counts()[0:25].plot(kind= 'bar',title = ' Top 25 companies by number of complaints')\nplt.xlabel(\"Company Name\")\nplt.ylabel(\"Number of Complaints\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Let create word cloud for each product category. This will help us to get an idea of most frequents occuring words in each category","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"### Create a unqiue list fo each products\nproduct_list = complaint_data['product'].unique()\n### Iterate through each products category \nfor i in product_list:\n    ### Convert the text to lower case and subset only text for product of interest\n    text = \" \".join(review.lower() for review in complaint_data[complaint_data['product'] == i]['consumer_complaint_narrative'])\n    ### Import the redefine stopwords list\n    stopwords = set(STOPWORDS)\n    ### Extend the predefine stop words list \n    stopwords.update([\"xxxx\", \"xx\", \"xxxx\", \"xxxxx\",'said','told','phone','trying','ask','asked',\"call\",\"called\"])\n\n    # Generate a word cloud image\n    wordcloud = WordCloud(stopwords=stopwords, background_color=\"white\").generate(text)\n\n    # Display the generated image:\n    # the matplotlib way:\n    print (\"Producing Word Cloud for :\", i)\n    plt.imshow(wordcloud, interpolation='bilinear')\n    plt.axis(\"off\")\n    plt.show() \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Objective 1 :  For learning purpose we will assume we don't have any company names and try to assign company name to each complaint with text using NLP","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"### Lets do some cleaning on the data. Mainly we will remove the stopwords and XXXX marks from our data\n### In this data some word are masked due to sensitivity of the data\n\ncomplaint_data['consumer_complaint_narrative'] =complaint_data['consumer_complaint_narrative'].str.replace(r'[^\\w\\s]',\"\")\ncomplaint_data['consumer_complaint_narrative'] = complaint_data['consumer_complaint_narrative'].str.replace(r\"XX+\\s\",\"\")\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Define a function which applies NER tagging and return a list of ORG found in data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#### Defined extract entities names and store it in a list \ndef extract_org_list(str1):\n    ''' This will take a str1 and extract the list of organization. This will be stored as a list of organisations'''\n    ### We are using the predefined ner parser tagging\n    docs = nlp(str1)\n    \n    ### We will return a list \n    return ( [str(i) for i in docs.ents if i.label_ == 'ORG'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Before applying the above function lets test the function out on a subset of data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cmp = complaint_data.iloc[0:10]\nnlp = spacy.load('en_core_web_sm')\ncmp['organisation_list'] =cmp['consumer_complaint_narrative'].apply(extract_org_list)\nprint (cmp[['consumer_complaint_narrative','organisation_list']].head(1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### The function is working apply it to complete datasets","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"### apply it and store its as org_list_spacy\nimport time\nstart = time.process_time()\ncomplaint_data['org_list_spacy'] = complaint_data['consumer_complaint_narrative'].apply(extract_org_list)\n### Store the output in text file so that we don't have to run the model again\nprint(\" Time taken to extract org list from data is \",time.process_time() - start)\ncomplaint_data.to_csv(\"text.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### After looking at the list of Organization we need to perform a few things of the data\n\n   #### 1. 40K claims model is not able to identify Orgs, on manual QA it seems the compalints dont have company name \n   #### 2. As it is free text, each organisation can be written in multiple ways for example Capital One is written as following\n        [\"Cap One\",\"Capital One Bank\",\"Cap One Corp\",'the Capital One Bank', 'Capital One' ] etc\n   #### 3. We need to create a method of normalizing the company names to the required list","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Problem 1 - Suggested Solution :\n   #### 1. Create a matching score between every company names in the stored dictionary. We can use fuzzy wuzzy matching for the same\n   #### 2. Based on fuzzy wuzzy matching scoring algorithm metric, create cluster. As we are not sure of how many clsuters be used we will use a Affinity clustering algorithm\n   #### 3. Once the cluster is created, assign names based which has highest similarity score to everyone else in the cluster\n    \n## Problem 2 - Suggested Solutions:\n   #### 1. Hypothesis : The freq of these noise terms would be very much less than the freqeuncy of comapny names\n   #### 2. We should do this on cluster frequency rather than the individual words as individual frequency can be biased and not reliable due to different names used","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# If you like the Kernel and dataset, please do a **upvote**","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}