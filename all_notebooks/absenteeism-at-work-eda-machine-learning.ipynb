{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Problem\n- As a company with tight financial. The company wants to provide health-mental wellness program and health examination program for employees that TENDS to absent. But it is hard for the company to indentify which employees that needs to be treated in the program.","metadata":{}},{"cell_type":"markdown","source":"# Goals\n- Identifying the employee that TENDS to absent\n- find the factors based on the data that affecting the healthiness of employee\n- the manager is ready enough to handle if their employee are tends go absent","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom dython.nominal import associations, cramers_v, correlation_ratio, theils_u\n\n\npd.options.display.max_columns = 999\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/absenteeism-at-work-uci-ml-repositiory/Absenteeism_at_work.csv', sep = ';')\ndf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Cleaning","metadata":{}},{"cell_type":"code","source":"bsDesc = []\n\nfor i in df.columns:\n    bsDesc.append([\n        i,\n        df[i].dtypes,\n        df[i].isna().sum(),\n        round((((df[i].isna().sum() )/ len(df)) * 100),2),\n        df[i].nunique(),\n        df[i].drop_duplicates().sample(2).values\n    ])\npd.DataFrame(data = bsDesc,\n            columns = [\n                'Features',\n                'DataType',\n                'Null',\n                'NullPercentage',\n                'Unique',\n                'Unique Sample'\n            ])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Data is clean","metadata":{}},{"cell_type":"code","source":"df.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.dtypes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.drop(columns = 'ID', inplace = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"outliers = []\ndef outlier_check(data):\n    sorted(data)\n    q1, q3 = np.percentile(data, [25,75])\n    iqr = q3-q1\n    lower_fence = q1 - (1.5 * iqr)\n    upper_fence = q3 + (1.5 * iqr)\n    for i in data:\n        if i < lower_fence or i > upper_fence:\n            outliers.append(i)\n    return outliers","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"outliers = []\nlen(outlier_check(df['Age']))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_outlierage = df[df['Age'].isin(outliers)]\ndf_outlierage","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10,8))\nsns.boxplot(data=df['Age'])\nplt.title(\"Amount of outliers in Age\")\nplt.xticks(rotation=45)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- data is clean\n- no missing value\n- all rows are numeric values\n- data target is absenteeism time in hours -> continous numeric -> we are going to change the target into 1 (absent) and 0 (not absent) later on for machine learning purposes.\n- based on df.describe() data is well prepared, the min and max value is according to its description. And there are no outliers that we need to drop","metadata":{}},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize = (10,8))\nsns.distplot(df['Age'],label = 'Skewness : %.2f'%(df['Age'].skew()))\nplt.title('distribution of age data')\nplt.legend()\n\nplt.plot()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The distribution of age is moderately skewed with skewness score = 0.7. Also positive skew which means that the mode is  smaller than the median or mean. Based on the data the majority of employee's age is around 20 to 60. ","metadata":{}},{"cell_type":"code","source":"mean_reason = df[['Reason for absence', 'Absenteeism time in hours']].groupby('Reason for absence').mean().reset_index().sort_values('Absenteeism time in hours', ascending = False)\nplt.figure(figsize = (20,5))\nsns.barplot(mean_reason['Reason for absence'],   mean_reason['Absenteeism time in hours'])\nplt.title('mean absent hours based on reason of absence')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The highest mean hours of absensce is because of disease of circulatory system. It is related to heart disease, based on the data heart disease took longer time to recover, it is 18 hours longer than the top 2 absent reason which is neoplasm. ","metadata":{}},{"cell_type":"code","source":"reason_counts = df[['Reason for absence', 'Absenteeism time in hours']].groupby('Reason for absence').count().reset_index().sort_values('Absenteeism time in hours', ascending = False)\nreason_counts.columns = ['Reason for absence', 'Absenteeism count']\nplt.figure(figsize = (20,5))\nsns.barplot(reason_counts['Reason for absence'],   reason_counts['Absenteeism count'])\nplt.title('absenteeism count based on reason for absence')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reason_sum = df[['Reason for absence', 'Absenteeism time in hours']].groupby('Reason for absence').sum().reset_index().sort_values('Absenteeism time in hours', ascending = False)\nreason_sum.columns = ['Reason for absence', 'Total numbers of hours']\nplt.figure(figsize = (20,5))\nsns.barplot(reason_sum['Reason for absence'],   reason_sum['Total numbers of hours'])\nplt.title('total absent hours based on reason of absence')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"top 3 total number of hours :\n- Diseases of the musculoskeletal system and connective tissue  \n  because of limitation of data (we dont know that type of this company is) so basecdon the disease we assume that lots of the company are manual labour (hard worker) that use its muscle to work. it leads to the highest hours of absent. \n- Symptoms, signs and abnormal clinical and laboratory findings, not elsewhere classified  \n- medical consultation\n  medical consultation is common in reason of abscenses in company.","metadata":{}},{"cell_type":"code","source":"bd_mass = df[['Reason for absence', 'Body mass index']].groupby('Reason for absence').mean().reset_index().sort_values('Body mass index', ascending = False)\nplt.figure(figsize = (20,5))\nsns.barplot(x = bd_mass['Reason for absence'], y = bd_mass['Body mass index'])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Underweight = <18.5\nNormal weight = 18.5–24.9\nOverweight = 25–29.9\nObesity = BMI of 30 or greater\n- the mean body mass index in the company is 26.6 -> overweight\n\n\nbased on the data, obesity employees (bmi >= 30) has reason of absence :\n- unjustified reason\n- mental and behavioral disorder\n- blood donation\n- surprisingly, the person that donates blood came from obesity person. based on article ,there is no upper weight limit as long as your weight is not higher than the weight limit of the donor bed or lounge you are using.\n- well unjustified reason needs to be describe more specificly to know that does obesity leads to serious problem.\n- mental and behavioral disorder are from obese person -> company needs to provide mental wellness care\n- surprisingly the person who are healthy (no absent) are from overweight class. As long as the person can maintain its healthy-life balance they has lower probability of sickness","metadata":{}},{"cell_type":"code","source":"month = df[['Month of absence', 'Absenteeism time in hours']].groupby('Month of absence').sum().reset_index()\nplt.figure(figsize = (10,5))\nsns.lineplot(x = month['Month of absence'], y= month['Absenteeism time in hours'])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In march and july employee tends to go absent. Company should take a step to lower the number of absentees in this month. Higher hours of absentees can leads to lower productivity to the company. ","metadata":{}},{"cell_type":"code","source":"day = df[['Day of the week', 'Absenteeism time in hours']].groupby('Day of the week').sum().reset_index()\nday\nplt.figure(figsize = (10,5))\nsns.lineplot(x = day['Day of the week'], y= day['Absenteeism time in hours'])\nplt.plot()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There were no data that recorded on monday (0 records).\n- The more closer to weekend, the lower the total absenteeism hours","metadata":{}},{"cell_type":"code","source":"season = df[['Seasons', 'Absenteeism time in hours']].groupby('Seasons').sum().reset_index()\nplt.figure(figsize = (10,5))\nsns.lineplot(x = season['Seasons'], y = season['Absenteeism time in hours'])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Despite having the highest total hours of absence in march, if we see from the graph above. Season 1 (Q1) from jaunary to march is not the highest total hours of absence.\nThe highest is from season s3(Q3) from july to september. ","metadata":{}},{"cell_type":"code","source":"sns.scatterplot(df['Transportation expense'], df['Absenteeism time in hours'])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"If we take a look at the scatter plot, the were no linear relationship between transporation expense and absenteeism time in hours","metadata":{}},{"cell_type":"code","source":"sns.heatmap(df[['Transportation expense', 'Absenteeism time in hours']].corr('spearman'), annot = True)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Based on confusion matrix, there are positive correlation but the number is quite low (weak correlation).","metadata":{}},{"cell_type":"code","source":"sns.scatterplot(df['Distance from Residence to Work'], df['Absenteeism time in hours'])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.heatmap(df[['Distance from Residence to Work', 'Absenteeism time in hours']].corr('spearman'), annot = True)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- If we take a look at the scatter plot, the were no  linear relationship between distance and absenteeism time in hours. Based on confusion matrix, there are positive correlation but the number is indicating that it is very weak because the number is very close to 0. We could conclude that distance is not the factors that made the employee absent.","metadata":{}},{"cell_type":"code","source":"sns.scatterplot(df['Service time'], df['Absenteeism time in hours'])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.heatmap(df[['Service time', 'Absenteeism time in hours']].corr('spearman'), annot = True)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- If we take a look at the scatter plot, the were no  linear relationship between service time and absenteeism time in hours. Based on confusion matrix, there are negative correlation but the number is indicating that it is very weak because the number is very close to 0. We could conclude that service time  is not the factors that made the employee absent.","metadata":{}},{"cell_type":"code","source":"sns.scatterplot(df['Age'], df['Absenteeism time in hours'])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.heatmap(df[['Age', 'Absenteeism time in hours']].corr('spearman'), annot = True)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- If we take a look at the scatter plot, the were no  linear relationship between age and absenteeism time in hours. Based on confusion matrix, there are negative correlation but the number is indicating that it is very weak because the number is very close to 0. We could conclude that age is not the factors that made the employee absent. Employee that is on its senior age does not mean that they are more prone to sickness, also employee that are still young does not mean they're healthy.","metadata":{}},{"cell_type":"code","source":"sns.scatterplot(df['Work load Average/day '], df['Absenteeism time in hours'])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.heatmap(df[['Work load Average/day ', 'Absenteeism time in hours']].corr('spearman'), annot = True)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- If we take a look at the scatter plot, the were no  linear relationship between workload and absenteeism time in hours. Based on confusion matrix, there are positive correlation but the number is indicating that it is very weak because the number is very close to 0. We could conclude that workload is not the factors that made the employee absent. Usually higher workload leads to more stress for the employee, but based on this data higher workload does not affecting their mental health. ","metadata":{}},{"cell_type":"code","source":"sns.scatterplot(df['Hit target'], df['Absenteeism time in hours'])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.heatmap(df[['Hit target', 'Absenteeism time in hours']].corr('spearman'), annot = True)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- If we take a look at the scatter plot, the were no  linear relationship between Hit target and absenteeism time in hours. Based on confusion matrix, there are positive correlation but the number is indicating that it is very weak because the number is very close to 0. We could conclude that hit target is not the factors that made the employee absent.","metadata":{}},{"cell_type":"code","source":"df[['Disciplinary failure', 'Absenteeism time in hours']].groupby('Disciplinary failure').sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- The employee that are absent does not because of indisciplinary action\n- the indisiciplinary action is not because of they are absent with no reason. but it is more indiciplinary action in work environment","metadata":{}},{"cell_type":"code","source":"df[['Education', 'Absenteeism time in hours']].groupby('Education').count()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"edu= df[['Education', 'Absenteeism time in hours']].groupby('Education').sum().reset_index()\nsns.barplot(edu['Education'], edu['Absenteeism time in hours'])\nplt.plot()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"correlation_ratio(df['Education'], df['Absenteeism time in hours'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- the majority education of employee is on highschool level and the majority of absenteeism are from high-school education - employee.\n- based on correlation ratio, higher education does not mean they are healthier.","metadata":{}},{"cell_type":"code","source":"son= df[['Son', 'Absenteeism time in hours']].groupby('Son').sum().reset_index()\nsns.barplot(son['Son'], son['Absenteeism time in hours'])\nplt.plot()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[['Son', 'Absenteeism time in hours']].corr('spearman')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- surprisingly, the more number of son they have the less hours they are absence\n- based on correlation ratio, more son does not affecting absence hours.","metadata":{}},{"cell_type":"code","source":"drink= df[['Social drinker', 'Absenteeism time in hours']].groupby('Social drinker').sum().reset_index()\nsns.barplot(drink['Social drinker'], son['Absenteeism time in hours'])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Social drinker category does not leads to more absenteism time in hours","metadata":{}},{"cell_type":"code","source":"smoke= df[['Social smoker', 'Absenteeism time in hours']].groupby('Social smoker').sum().reset_index()\nsns.barplot(smoke['Social smoker'], son['Absenteeism time in hours'])\nplt.plot()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- surprisingly smoking does not leads to more absenteeism\n- based on correlation ratio, smoking does not affecting the hours of absenteeism","metadata":{}},{"cell_type":"code","source":"pet= df[['Pet', 'Absenteeism time in hours']].groupby('Pet').sum().reset_index()\nsns.barplot(pet['Pet'], pet['Absenteeism time in hours'])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- based on the data, employee that have pet are having less in absenteeism time in hours. Having a pet its one of source of hapiness for some person, it could boost immunity if the person is happy.","metadata":{}},{"cell_type":"code","source":"bmi= df[['Body mass index', 'Absenteeism time in hours']].groupby('Body mass index').sum().reset_index()\nsns.barplot(bmi['Body mass index'], bmi['Absenteeism time in hours'])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[['Body mass index', 'Absenteeism time in hours']].corr('spearman')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- bmi does not affecting to absenteesim time in hours based on the barplot and correlation (-0.06)","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\nfrom sklearn.metrics import classification_report, recall_score, accuracy_score, f1_score, confusion_matrix, precision_score\nfrom sklearn.svm import SVC\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import RobustScaler, PowerTransformer, PolynomialFeatures, OneHotEncoder, StandardScaler\nfrom sklearn.compose import ColumnTransformer\n\nfrom xgboost import XGBClassifier, XGBRegressor","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"b = []\nfor i in df['Absenteeism time in hours']:\n    if i > 0 :\n        b.append(1)\n    else:\n        b.append(0)\ndf['Absent'] = b #classification target","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.drop(columns = ['Reason for absence', 'Absenteeism time in hours'], inplace = True) #it will be obvious if we include the reason for absence and absenteeism time\ndf.drop(columns = ['Month of absence', 'Day of the week'], inplace = True) #it will be obvious if we include the month of absence and day of week. We use seasons instead (season more inmpactful to person's health)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = df.drop(columns = 'Absent')\ny = df['Absent']\nX_train, X_test, y_train, y_test = train_test_split(X,y, stratify = y , test_size = 0.3, random_state =42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['Absent'].value_counts(normalize = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Minority = not absent\nmajority = absent\n1 = absent (not present)\n0 = not absent (present)\nTrue positive = Actual absent, prediction absent\nTrue negative = Actual not absent, prediction not absent\nFalse positive =  Actual not absent (present), but the prediction is absent\nFalse negative = Actual absent but the prediction is present -> main concern \n\nMAIN CONCERN\n- our main concern is to supress the number of false negative while also having least number of false positive. (balancing)\n- Why we want to balance out the false positive and false negative. \n  Case:\n  After detecting the employee that tends to go sick. the company apporach is to examine the employees health to mental-wellness-program and healthy-program. Also taking several health examination. If we have big false positive then company have to pay more for the program and examination (more people that detected tends to go absent but the actual he is healthy). Also we need to supress the false negative so that our prediction is more likely to be accurate in predicting people that tends to go absent.\n  \nBecause the data is imbalance so we focus on tuning the minority class which is 0 (not absent) to get more accurate prediction","metadata":{}},{"cell_type":"code","source":"# Base model\nLR = LogisticRegression()\nLR.fit(X_train,y_train)\ny_LR_test = LR.predict(X_test)\nprint(classification_report(y_test, y_LR_test))\ncm_LR_test = confusion_matrix(y_test, y_LR_test, labels = [1,0])\ndf_LR_test = pd.DataFrame(data=cm_LR_test, columns=['Pred 1', 'Pred 0'], index=['Akt 1', 'Akt 0'])\nsns.heatmap(df_LR_test, annot=True)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nKNN = KNeighborsClassifier()\nKNN.fit(X_train,y_train)\ny_KNN_test = KNN.predict(X_test)\nprint(classification_report(y_test, y_KNN_test))\ncm_KNN_test = confusion_matrix(y_test, y_KNN_test, labels = [1,0])\ndf_KNN_test = pd.DataFrame(data=cm_KNN_test, columns=['Pred 1', 'Pred 0'], index=['Akt 1', 'Akt 0'])\nsns.heatmap(df_KNN_test, annot=True)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SVM = SVC()\nSVM.fit(X_train,y_train)\ny_SVM_test = SVM.predict(X_test)\nprint(classification_report(y_test, y_SVM_test))\ncm_SVM_test = confusion_matrix(y_test, y_SVM_test, labels = [1,0])\ndf_SVM_test = pd.DataFrame(data=cm_SVM_test, columns=['Pred 1', 'Pred 0'], index=['Akt 1', 'Akt 0'])\nsns.heatmap(df_SVM_test, annot=True)\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DT = DecisionTreeClassifier()\nDT.fit(X_train,y_train)\ny_DT_test = DT.predict(X_test)\nprint(classification_report(y_test, y_DT_test))\ncm_DT_test = confusion_matrix(y_test, y_DT_test, labels = [1,0])\ndf_DT_test = pd.DataFrame(data=cm_KNN_test, columns=['Pred 1', 'Pred 0'], index=['Akt 1', 'Akt 0'])\nsns.heatmap(df_DT_test, annot=True)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"RF = RandomForestClassifier()\nRF.fit(X_train,y_train)\ny_RF_test = RF.predict(X_test)\nprint(classification_report(y_test, y_RF_test))\ncm_RF_test = confusion_matrix(y_test, y_RF_test, labels = [1,0])\ndf_RF_test = pd.DataFrame(data=cm_RF_test, columns=['Pred 1', 'Pred 0'], index=['Akt 1', 'Akt 0'])\nsns.heatmap(df_RF_test, annot=True)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conclusion on base model\n- Random forest has the best recall  in the MINORITY CLASS.\n- Decision Tree 2nd best but it has higher false positive\n- KNN and SVM recall-precision score is far from accurate\n- Logreg still can be improved","metadata":{}},{"cell_type":"code","source":"pipeline_LR = Pipeline([\n    ('scale', StandardScaler()),\n    ('algo', LogisticRegression())\n])\npipeline_KNN = Pipeline([\n    ('scale', StandardScaler()),\n    ('algo', KNeighborsClassifier())\n])\npipeline_SVM = Pipeline([\n    ('scale', StandardScaler()),\n    ('algo', SVC())\n])\npipeline_RF = Pipeline([\n    ('scale', StandardScaler()),\n    ('algo', RandomForestClassifier())\n])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pipeline_LR.fit(X_train, y_train)\ny_LRT_test = pipeline_LR.predict(X_test)\nprint(classification_report(y_test, y_LRT_test))\ncm_LRT_test = confusion_matrix(y_test, y_LRT_test, labels = [1,0])\ndf_LRT_test = pd.DataFrame(data=cm_LRT_test, columns=['Pred 1', 'Pred 0'], index=['Akt 1', 'Akt 0'])\nsns.heatmap(df_LRT_test, annot=True)\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pipeline_KNN.fit(X_train, y_train)\ny_KNNT_test = pipeline_KNN.predict(X_test)\nprint(classification_report(y_test, y_KNNT_test))\ncm_KNNT_test = confusion_matrix(y_test, y_KNNT_test, labels = [1,0])\ndf_KNNT_test = pd.DataFrame(data=cm_KNNT_test, columns=['Pred 1', 'Pred 0'], index=['Akt 1', 'Akt 0'])\nsns.heatmap(df_KNNT_test, annot=True)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pipeline_SVM.fit(X_train, y_train)\ny_SVM_test = pipeline_SVM.predict(X_test)\nprint(classification_report(y_test, y_SVM_test))\ncm_SVM_test = confusion_matrix(y_test, y_SVM_test, labels = [1,0])\ndf_SVM_test = pd.DataFrame(data=cm_SVM_test, columns=['Pred 1', 'Pred 0'], index=['Akt 1', 'Akt 0'])\nsns.heatmap(df_SVM_test, annot=True)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pipeline_RF.fit(X_train, y_train)\ny_RF2_test = pipeline_RF.predict(X_test)\nprint(classification_report(y_test, y_RF2_test))\ncm_RF2_test = confusion_matrix(y_test, y_RF2_test, labels = [1,0])\ndf_RF2_test = pd.DataFrame(data=cm_RF2_test, columns=['Pred 1', 'Pred 0'], index=['Akt 1', 'Akt 0'])\nsns.heatmap(df_RF2_test, annot=True)\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# conclusion using scalling\n- well all of the algoritm seems to have great score after scalling. recall = 0.85","metadata":{}},{"cell_type":"markdown","source":"# Hyperparameter Tuning","metadata":{}},{"cell_type":"code","source":"param_KNN = {\n    'algo__n_neighbors': np.arange(1,51,2),\n    \"algo__p\" : [1,2],\n    'algo__weights' : ['uniform', 'distance']\n}\nskf = StratifiedKFold(n_splits = 3)\nKNN_GS = GridSearchCV(pipeline_KNN, param_KNN, cv = skf, scoring = 'f1', n_jobs= -1, verbose = 1)\nKNN_GS.fit(X_train,y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"KNN_GS.best_params_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"KNN_Tuned = KNN_GS.best_estimator_\ny_KNN2_test = KNN_Tuned.predict(X_test)\nprint(classification_report(y_test, y_KNN2_test))\ncm_KNN2_test = confusion_matrix(y_test, y_KNN2_test, labels = [1,0])\ndf_KNN2_test = pd.DataFrame(data=cm_KNN2_test, columns=['Pred 1', 'Pred 0'], index=['Akt 1', 'Akt 0'])\nsns.heatmap(df_SVM_test, annot=True)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"param_SVM = {\n    'algo__C' : np.logspace(-3,3,7),\n    'algo__gamma' : np.arange(10, 101, 10),\n    'algo__class_weight': [{0 : 1- x, 1 : x} for x in [.1, .2, .3, .4, .5]]\n}\nSVM_GS = GridSearchCV(pipeline_SVM, param_SVM, cv = skf, scoring = 'recall', n_jobs= -1, verbose = 1)\nSVM_GS.fit(X_train,y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SVM_Tuned = SVM_GS.best_estimator_\ny_SVMT_test = SVM_Tuned.predict(X_test)\nprint(classification_report(y_test, y_SVMT_test))\ncm_SVMT_test = confusion_matrix(y_test, y_SVMT_test, labels = [1,0])\ndf_SVMT_test = pd.DataFrame(data=cm_SVMT_test, columns=['Pred 1', 'Pred 0'], index=['Akt 1', 'Akt 0'])\nsns.heatmap(df_SVMT_test, annot=True)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"param_RF = {\n    \"n_estimators\" : np.arange(100, 501, 100), \n    \"max_depth\" : [None, 5, 10, 15], \n    \"min_samples_leaf\" : np.arange(1, 17, 5), \n    \"max_features\" : [0.3, 0.5, 0.7, 0.8],\n    \"class_weight\" : [{0 : 1- x, 1 : x} for x in [.1, .2, .3, .4, .5]]\n}\nRF_T = GridSearchCV(RandomForestClassifier(), param_RF, cv=skf, n_jobs=-1, verbose=1, scoring='recall')\nRF_T.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"RF_Tuned = RF_T.best_estimator_\ny_RF3_test = RF_Tuned.predict(X_test)\nprint(classification_report(y_test, y_RF3_test))\ncm_RF3_test = confusion_matrix(y_test, y_RF3_test, labels = [1,0])\ndf_RF3_test = pd.DataFrame(data=cm_RF3_test, columns=['Pred 1', 'Pred 0'], index=['Akt 1', 'Akt 0'])\nsns.heatmap(df_RF3_test, annot=True)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It seems like the recall score still 0.85 (our best recall score), so we use another metrics called ROC AUC. AUC - ROC curve is a performance measurement for the classification problems at various threshold settings. ROC is a probability curve and AUC represents the degree or measure of separability. It tells how much the model is capable of distinguishing between classes. Higher the AUC, the better the model is at predicting 0s as 0s and 1s as 1s. By analogy, the Higher the AUC, the better the model is at distinguishing between patients with the disease and no disease.","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import roc_auc_score, roc_curve, auc","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_proba = pipeline_LR.predict_proba(X_test)\npred_1 = y_proba[:, 1]\nroc_curve(y_test, pred_1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fpr, tpr, threshold = roc_curve(y_test, pred_1)\n# AUC Score = Area Under Curve\nauc_score = round((auc(fpr, tpr)), 2)\nplt.figure(figsize=(10, 6))\nplt.plot(fpr, tpr, 'b', label = f\"AUC = {auc_score}\")\nplt.plot([0, 1], [0, 1], 'r-.')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.xlabel('False Positive Rate (FPR)')\nplt.ylabel('True Positive Rate (TPR)')\nplt.title('Receiver Operating Characteristic Curve of Logistic regression after feature engineering')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_proba = KNN_Tuned.predict_proba(X_test)\npred_1 = y_proba[:, 1]\nroc_curve(y_test, pred_1)\nfpr, tpr, threshold = roc_curve(y_test, pred_1)\nauc_score = round((auc(fpr, tpr)), 2)\nplt.figure(figsize=(10, 6))\nplt.plot(fpr, tpr, 'b', label = f\"AUC = {auc_score}\")\nplt.plot([0, 1], [0, 1], 'r-.')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.xlabel('False Positive Rate (FPR)')\nplt.ylabel('True Positive Rate (TPR)')\nplt.title('Receiver Operating Characteristic Curve of KNN hyperparameter tuning')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_proba = DT.predict_proba(X_test)\npred_1 = y_proba[:, 1]\nroc_curve(y_test, pred_1)\nfpr, tpr, threshold = roc_curve(y_test, pred_1)\nauc_score = round((auc(fpr, tpr)), 2)\nplt.figure(figsize=(10, 6))\nplt.plot(fpr, tpr, 'b', label = f\"AUC = {auc_score}\")\nplt.plot([0, 1], [0, 1], 'r-.')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.xlabel('False Positive Rate (FPR)')\nplt.ylabel('True Positive Rate (TPR)')\nplt.title('Receiver Operating Characteristic Curve of DecisionTree ')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_proba = RF_Tuned.predict_proba(X_test)\npred_1 = y_proba[:, 1]\nroc_curve(y_test, pred_1)\nfpr, tpr, threshold = roc_curve(y_test, pred_1)\nauc_score = round((auc(fpr, tpr)), 2)\nplt.figure(figsize=(10, 6))\nplt.plot(fpr, tpr, 'b', label = f\"AUC = {auc_score}\")\nplt.plot([0, 1], [0, 1], 'r-.')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.xlabel('False Positive Rate (FPR)')\nplt.ylabel('True Positive Rate (TPR)')\nplt.title('Receiver Operating Characteristic Curve of random forest after Hyperparameter tuning')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conclusion\n- the best algoritm for this classification problem is using randomforest algoritm with area under the curve (AUC = 0.95) and recall score 0.85\n- the recall score random forest before tuned and after tuned is the same also the AUC score\n- tuning does not improve the model recall score","metadata":{}},{"cell_type":"markdown","source":"# EDA Conclusion\n- bmi does not affecting to absenteesim time in hours based on the barplot and correlation (-0.06)\n- based on the data, employee that have pet are having less in absenteeism time in hours. Having a pet its one of source of hapiness for some person, it could boost immunity if the person is happy.\n- surprisingly smoking does not leads to more absenteeism \n- Social drinker category leads to more absenteism time in hours\n- surprisingly, the more number of son they have the less hours they are absence. based on correlation ratio, more son does not affecting absence hours.\n- the majority education of employee is on highschool level and the majority of absenteeism are from high-school education - employee. based on correlation ratio, higher education does not mean they are healthier.\n- The employee that are absent does not because of indisciplinary action\n- the indisiciplinary action is not because of they are absent with no reason. but it is more indiciplinary action in work environment\n-  We could conclude that hit target is not the factors that made the employee absent.\n-  We could conclude that workload is not the factors that made the employee absent. Usually higher workload leads to more stress for the employee, but based on this data higher workload does not affecting their mental health. \n- We could conclude that age is not the factors that made the employee absent. Employee that is on its senior age does not mean that they are more prone to sickness, also employee that are still young does not mean they're healthy.\n- We could conclude that service time  is not the factors that made the employee absent\n- We could conclude that distance is not the factors that made the employee absent.\n- If we take a look at the scatter plot, the were no linear relationship between transporation expense and absenteeism time in hours, weak correlation\n- Despite having the highest total hours of absence in march, if we see from the graph above. Season 1 (Q1) from jaunary to march is not the highest total hours of absence.The highest is from season s3(Q3) from july to september.\n- The more closer to weekend, the lower the total absenteeism hours\n- In march and july employee tends to go absence. Company should take a step to lower the number of absentees in this month. Higher hours of absentees can leads to lower productivity to the company. \n- The highest mean hours of absensce is because of disease of circulatory system. It is related to heart disease, based on the data heart disease took longer time to recover, it is 18 hours longer than the top 2 absent reason which is neoplasm \n- blood donation has the highest number of employee that absent, followed by dental consultation, physioteraphy, and musculoskeletal system. Well the employee is really care about humanity lots of them donates blood. Blood donation average hour absence is 2.8 hours per employee it is relatively low (ranked 4 from bottom of mean absence hours) it does not contribute much to the absent hours of employee per employee. if the duration of the absence is short, the work can still be done without disturbing the timeline of the project. But if the duration is high then the employee needs to find someone as a backup or the backup person might do 2 task in the same time which is leading to more stressed to the backup employee. So health is really a concern to maintain productivity of employee.\n- Unjustified reason needs to be more specific because the number mean hours of absentees is 6.3 (higher than the median).\n\nbased on the data, obesity employees (bmi >= 30) has reason of absence :\n- unjustified reason\n- mental and behavioral disorder\n- blood donation\n- surprisingly, the person that donates blood came from obesity person. based on article ,there is no upper weight limit as long as your weight is not higher than the weight limit of the donor bed or lounge you are using.\n- well unjustified reason needs to be describe more specificly to know that does obesity leads to serious problem.\n- mental and behavioral disorder are from obese person -> company needs to provide mental wellness care\n- surprisingly the person who are healthy (no absent) are from overweight class. As long as the person can maintain its healthy-life balance they has lower probability of sickness","metadata":{}},{"cell_type":"markdown","source":"# Machine learning conclusion and recommendation\nMinority = not absent\nmajority = absent\n1 = absent (not present)\n0 = not absent (present)\nTrue positive = Actual absent, prediction absent\nTrue negative = Actual not absent, prediction not absent\nFalse positive =  Aktualnya tidak absen (masuk), tapi diprediksi absen (tidak masuk)\nFalse negative = Aktual nya dia absen (tidak masuk) tapi prediksi nya ga absen (masuk) -> main concern \n\nMAIN CONCERN\n- our main concern is to supress the number of false negative while also having least number of false positive. (balancing)\n- Why we want to balance out the false positive and false negative. \n  Case:\n  After detecting the employee that tends to go sick. the company apporach is to examine the employees health to mental-wellness-program and healthy-program. Also taking several health examination. If we have big false positive then company have to pay more for the program and examination (more people that detected tends to go absent but the actual he is healthy). Also we need to supress the false negative so that our prediction is more likely to be accurate in predicting people that tends to go absent.\n  \nBecause the data is imbalance so we focus on tuning the minority class which is 0 (not absent) to get more accurate prediction\n\n\n- The recall score for base model is already high, so we do not need to do imbalance data handling\n- the best algoritm for this classification problem is using randomforest algoritm with area under the curve (AUC = 0.95) and recall score 0.85\n- the recall score random forest before tuned and after tuned is the same also the AUC score\n- tuning does not improve the model recall score\nI recommend the company to gather more health data about the employee such as (glucose level, cholestrol, uric acid, blood pressure and so on) and the data needs to be updated every terms. So that it gave more accurate readings of the prediction. Healthy employee leads to increase in productivity and low absenteeism.\n\nHandling people that tends to go absent \n- mental wellness program\n- health care program\n- health examination\n\nAssumption\n- cost of medical program per employee = 10 USD\n- if we provide medical program for all employee : 740 * 10 USD = 7400 USD\n- if we use machine learning as the predictor, the company just pay based on the prediction (not paying for all employee)\n- For example : using machine learning it predicts that there are 120 people tends to go absent.\n    120 x 10 USd = 1200 USD\n    7400 -1200  = 6200 USD -> SAVED.\n\n- in longrun it will increase the productivity","metadata":{}}]}