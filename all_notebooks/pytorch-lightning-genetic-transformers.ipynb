{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install --pre torch==1.7.0.dev20200701+cu101 torchvision==0.8.0.dev20200701+cu101 -f https://download.pytorch.org/whl/nightly/cu101/torch_nightly.html","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n!python pytorch-xla-env-setup.py --apt-packages libomp5 libopenblas-dev","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install pytorch-lightning","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pytorch_lightning as plit\nimport torch_xla.core.xla_model as xm\nimport torch_xla.distributed.parallel_loader as pl\nimport torch_xla.distributed.xla_multiprocessing as xmp\nimport transformers\nimport torch.nn as nn\nfrom transformers import AdamW\nfrom transformers import get_linear_schedule_with_warmup","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom scipy.special import softmax\nimport torch\nimport torch.nn as nn\nfrom tqdm.notebook import tqdm\n%matplotlib inline\nsns.set()\nimport os\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_values = pd.read_csv('/kaggle/input/genetic-engineering-attribution-challenge/train_values.csv',index_col='sequence_id')\ntrain_labels = pd.read_csv('/kaggle/input/genetic-engineering-attribution-challenge/train_labels.csv', index_col='sequence_id')\ntest_values = pd.read_csv('/kaggle/input/genetic-engineering-attribution-challenge/test_values.csv', index_col='sequence_id')\nsubmission_format = pd.read_csv('/kaggle/input/genetic-engineering-attribution-challenge/submission_format_3TFRxH6.csv', index_col='sequence_id')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm.notebook import tqdm\ndef get_kmers(df, size=5,stride=3):\n    sequence_list = list(df.sequence.values)\n    kmers = []\n    for item in tqdm(range(len(sequence_list))):\n        #kmers.append([sequence_list[item][x:x+size].lower() for x in range(0,len(sequence_list[item]) - size + 1,stride)])\n        kmers.append([sequence_list[item][x:x+size].lower() for x in range(0,512 - size + 1,stride)])\n    print(len(kmers),df.shape[0])\n  \n    for idx in tqdm(range(len(kmers))):\n        kmers[idx] = ' '.join(kmers[idx])\n\n    return kmers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lab_ids = pd.DataFrame(train_labels.idxmax(axis=1), columns=['lab_id'])\nlab_ids.reset_index(inplace=True)\nlab_ids.columns = lab_ids.columns.get_level_values(0)\nkmers_train = get_kmers(train_values,5,3)\nkmers_test = get_kmers(test_values,5,3)\ntrain_values.reset_index(inplace=True)\ntest_values.reset_index(inplace=True)\ntrain = train_values[['sequence_id']]\ntest = test_values[['sequence_id']]\ntrain['kmers'] = kmers_train\ntest['kmers'] = kmers_test\ntrain = pd.merge(train,lab_ids,on='sequence_id',how='left')\ntrain.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['kmers'].apply(len).max()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.memory_usage().sum()/(1024**2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_rare0 = train[train['lab_id'] == 'ON9AXMKF'].sample(frac=2,replace=True)\ntrain_rare1 = train[train['lab_id'] == '0L3Y6ZB2'].sample(frac=2,replace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.concat([train,train_rare0,train_rare1])\ntrain.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"le = LabelEncoder()\ntrain['lab_id'] = le.fit_transform(train['lab_id'])\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = train[['kmers','lab_id']]\ndata.rename(columns={'kmers':'text','lab_id':'label'},inplace=True)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_labels = data['label'].nunique()\nnum_labels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import transformers\nimport tokenizers\n\nMAX_LEN = 128\nTRAIN_BATCH_SIZE = 8\nVALID_BATCH_SIZE = 4\nEPOCHS = 3\n#BERT_PATH = \"/kaggle/input/bert-pytorch/\"\n#BERT_PATH = \"/kaggle/input/bert-base-uncased/\"\n#MODEL_PATH = \"bert-large-uncased-pytorch_model.bin\"\n#TRAINING_FILE = \"../input/imdb.csv\"\nTOKENIZER_BERT = transformers.BertTokenizer.from_pretrained(\n    'bert-base-uncased',\n    do_lower_case=True\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch.nn.functional as F\n\nclass BERTDataset:\n    def __init__(self,text,label):\n        self.text = text\n        self.label=label\n        self.tokenizer = TOKENIZER_BERT\n        self.max_len = MAX_LEN\n            \n    def __len__(self):\n        return len(self.text)\n    \n    def __getitem__(self, item):\n        text = str(self.text[item])\n        #text = \" \".join(text.split())\n                \n        inputs = self.tokenizer.encode_plus(\n            text,\n            None,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            pad_to_max_length=True,\n            return_token_type_ids=True\n        )\n\n        ids = inputs[\"input_ids\"]\n        mask = inputs[\"attention_mask\"]\n        token_type_ids = inputs[\"token_type_ids\"]\n\n        return {\n            'ids': torch.tensor(ids, dtype=torch.long),\n            'mask': torch.tensor(mask, dtype=torch.long),\n            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n            'label': torch.tensor(self.label[item], dtype=torch.long),\n                       \n        }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class BertDataModule(plit.LightningDataModule):\n    def __init__(self, hparams, data):\n        super().__init__()\n        self.hparams = hparams\n        self.data = data\n        \n    def prepare_data(self):\n        pass\n\n    def setup(self, stage=None):\n\n        train_df,val_df = train_test_split(self.data,test_size=0.20,random_state=42,stratify=self.data['label'])\n        \n        self.train_dataset = BERTDataset(text=train_df.text.values,label=train_df.label.values)\n        self.valid_dataset = BERTDataset(text=val_df.text.values,label=val_df.label.values)\n\n    def train_dataloader(self):\n        train_loader = torch.utils.data.DataLoader(\n            self.train_dataset,\n            batch_size=8,\n            num_workers=2,\n            shuffle=True,\n        )\n        return train_loader\n\n    def val_dataloader(self):\n        valid_loader = torch.utils.data.DataLoader(\n            self.valid_dataset,\n            batch_size=4,\n            num_workers=2,\n            shuffle=False,\n        )\n\n        return valid_loader\n\n    def test_dataloader(self):\n        return None\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class BERTBaseUncased(nn.Module):\n    def __init__(self):\n        super(BERTBaseUncased, self).__init__()\n        self.bert = transformers.BertModel.from_pretrained('bert-base-uncased')\n        self.bert_drop = nn.Dropout(0.3)\n        self.out = nn.Linear(768, 1314)\n    \n    def forward(self, ids, mask,token_type_ids):\n        _, o2 = self.bert(\n            ids, \n            attention_mask=mask,\n            token_type_ids=token_type_ids\n        )\n        bo = self.bert_drop(o2)\n        output = self.out(bo)\n        return output","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class BertLit(plit.LightningModule):\n    def __init__(self, hparams, model):\n        super(BertLit, self).__init__()\n        self.hparams = hparams\n        self.model = model\n        self.loss_fn = nn.CrossEntropyLoss() \n        \n    def forward(self, ids,mask,token_type_ids):\n        return self.model(ids,mask,token_type_ids)\n        \n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n                                                               patience=3, threshold=0.00001, mode=\"min\", verbose=True)\n        return ([optimizer],[{'scheduler': scheduler, 'interval': 'epoch', 'monitor': 'valid_loss'}])\n    \n    def training_step(self, batch, batch_idx):\n        ids = batch['ids']\n        mask = batch['mask']\n        token_type_ids = batch['token_type_ids']\n        label = batch['label']\n        out = self(ids,mask,token_type_ids)\n        loss = self.loss_fn(out, label)\n        \n        logs = {'train_loss': loss}\n        \n        return {'loss': loss, 'log': logs, 'progress_bar': logs}\n    \n    def training_epoch_end(self, outputs):\n        avg_loss = torch.stack([x['loss'] for x in outputs]).mean()\n        logs = {'train_loss': avg_loss}\n        return {'log': logs, 'progress_bar': logs}\n\n    def validation_step(self, batch, batch_idx):\n        ids = batch['ids']\n        mask = batch['mask']\n        token_type_ids = batch['token_type_ids']\n        label = batch['label']\n        out = self(ids,mask,token_type_ids)\n        loss = self.loss_fn(out, label)\n        logs = {'valid_loss': loss}\n        \n        return {'loss': loss, 'log': logs, 'progress_bar': logs}\n    \n    def validation_epoch_end(self, outputs):\n        avg_loss = torch.stack([x['loss'] for x in outputs]).mean()\n        logs = {'valid_loss': avg_loss}\n        return {'log': logs, 'progress_bar': logs}\n                \n   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainer = plit.Trainer(tpu_cores=8,precision=16,max_epochs=3,weights_summary='full')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = BERTBaseUncased()\nmodel = BertLit(hparams={}, model=model)\ndm = BertDataModule(hparams={}, data=data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainer.fit(model, dm)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}