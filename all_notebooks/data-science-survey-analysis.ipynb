{"nbformat":4,"nbformat_minor":1,"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3","language":"python"},"language_info":{"nbconvert_exporter":"python","file_extension":".py","pygments_lexer":"ipython3","name":"python","mimetype":"text/x-python","version":"3.6.3","codemirror_mode":{"name":"ipython","version":3}}},"cells":[{"source":"yourFilePath=\"../input/multipleChoiceResponses.csv\"\ncurencyConverion=\"../input/conversionRates.csv\"\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport squarify\nplt.style.use('seaborn-paper')\nimport warnings\nwarnings.filterwarnings('ignore')\nimport numpy as np\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nimport base64\nimport io\nfrom scipy.misc import imread\nimport codecs\nfrom IPython.display import HTML\nfrom matplotlib_venn import venn2\nfrom subprocess import check_output\n\nfrom __future__ import print_function\nfrom ipywidgets import interact, interactive, fixed, interact_manual\nfrom IPython.display import display\nimport ipywidgets as widgets\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.metrics import f1_score, precision_score, recall_score , accuracy_score, roc_curve, auc\nfrom sklearn import tree\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier \n\nimport operator\nimport os,sys\nimport matplotlib.pyplot as plt\nfrom sklearn.feature_selection import VarianceThreshold\nfrom pandas.api.types import CategoricalDtype\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import LabelEncoder, LabelBinarizer\n\nfrom sklearn_pandas import DataFrameMapper\n\ndef f(x):\n    return x\n\ndef loadData(yourFilePath):\n    df=pd.read_csv(yourFilePath, low_memory=False,encoding='ISO-8859-1')\n    return df\n\ndef summaryColumn(columnName):\n    column_summary=df_raw[[columnName]].groupby([columnName]).size().reset_index(name='counts')\n    column_summary=column_summary.sort_values(['counts'],ascending=[0])\n    column_category=list(column_summary[columnName])\n    category_counts=list(column_summary['counts'])\n#     plt.rcdefaults()\n    fig, ax = plt.subplots()\n    y_pos=np.arange(len(column_category))\n    ax.barh(y_pos, category_counts, align='center',\n        color='green')\n    ax.set_yticks(y_pos)\n    ax.set_yticklabels(column_category)\n    ax.invert_yaxis()  # labels read top-to-bottom\n    ax.set_xlabel('Count')\n    ax.set_title(columnName)\n    plt.show()\n    return column_summary","execution_count":null,"metadata":{"_cell_guid":"f1947144-8cfa-490b-a118-c372e2464108","_uuid":"d77bb5c36f63c692a12dd781a1b689baf4058cda","collapsed":true},"outputs":[],"cell_type":"code"},{"source":"Loading the data from files using pandas reav_csv api call. We set the encoding to ISO-8859-1 and the low memory option to False.","metadata":{"_cell_guid":"2ab86bc0-41fa-460d-94cc-fffca88bce0f","_uuid":"0286f7b2786629a05faadbb135f31a82eeb78074"},"cell_type":"markdown"},{"source":"data = loadData(yourFilePath)\nrates = loadData(curencyConverion)\ndf_raw=loadData(yourFilePath)\ncolumnList=list(df_raw.columns)","execution_count":null,"metadata":{"_cell_guid":"77b6f83c-12c4-4435-8626-cfc755bc0458","_uuid":"6029fa2ee2208f03632f441485a22f808337d2fa","collapsed":true},"outputs":[],"cell_type":"code"},{"source":"## First step is to visualize all the data elements.\nWe used the IPython interactive display panel to allow users to chose the attribute and visualize the distribution for that attribute.","metadata":{"_cell_guid":"09e7f2a9-6da7-4c49-b178-aeff00794b61","_uuid":"9712d8df00264832e6194eef23e0fd5946aba9d0"},"cell_type":"markdown"},{"source":"result=interactive(summaryColumn,columnName=columnList);\ndisplay(result)","execution_count":null,"metadata":{"_cell_guid":"1d9c05f6-f461-4098-adcb-1d97ced56a17","_uuid":"37d3b19fbc0b7ad674353f8ab9a55d63df91326c","collapsed":true},"outputs":[],"cell_type":"code"},{"source":"Based on the understanding the we gained by looking at data distribution in each attribute, we then proceed to do exploratory analysis across attributes.","metadata":{"_cell_guid":"c976f6b1-ecfa-4361-aef6-19f4b01d707c","_uuid":"f6780c6b06e2def377a5fa93fd1bc38853a1fc4d"},"cell_type":"markdown"},{"source":"## Second we tried to see if there is correlation between current job and the major of the respondent","metadata":{"_cell_guid":"8a97de56-3fdf-4d66-8622-b8c2954df2a7","_uuid":"0b4a0c57b8fc8fc2cd4125efb2fc2340ebd8ac1f"},"cell_type":"markdown"},{"source":"# Major and CurrentJobTitle Relation   \nf,ax=plt.subplots(1,2,figsize=(20,10))                                                                                  \nsns.countplot(y = data['MajorSelect'],ax=ax[0],order= data['MajorSelect'].value_counts().index)                         \nax[0].set_title('Major')                                                                                                \nax[0].set_ylabel('')                                                                                                    \nsns.countplot(y= data['CurrentJobTitleSelect'],ax=ax[1],order=data['CurrentJobTitleSelect'].value_counts().index)       \nax[1].set_title('Current Job')                                                                                          \nax[1].set_ylabel('')                                                                                                    \nplt.subplots_adjust(wspace=0.8)                                                                                         \nplt.show()   ","execution_count":null,"metadata":{"_cell_guid":"b03a03a0-a691-4bb0-be60-d1c911b73827","_uuid":"37e0cfa2c49c1d81c6e6af026a12c234ef072269","collapsed":true},"outputs":[],"cell_type":"code"},{"source":"We notice that most data scientists majored in computer science","metadata":{"_cell_guid":"349d0729-d4c3-4643-9273-1374f29f1ce5","_uuid":"e11aaf1ca16b86dda0806e8fc03f1d3240e1000a"},"cell_type":"markdown"},{"source":"#### We analyse the most important work langauges. See if there are any intresting observations to be made there. Lets us see if we can get some insights of usage between python and R.\n\nShow the work language  which are frequently used. We find python and R are top two.  ","metadata":{"_cell_guid":"042becab-ae37-4a39-b8f4-b7bba5ce1447","_uuid":"3489d3fa8c020e6295b3b306437dac0ebf7e0a13"},"cell_type":"markdown"},{"source":"  \nwork_tools = data['WorkToolsSelect'].dropna().str.split(',')              \ntools = []                                                                \nfor wktools in work_tools:                                                \n    for tool in wktools:                                                  \n        tools.append(tool)                                                \nresult = pd.Series(tools).value_counts()[:10]                             \nplt.subplots(figsize=(10,10))                                             \nsns.barplot(result.values,result.index)                                   \nplt.title('Work Tools')                                                   \nplt.show()              ","execution_count":null,"metadata":{"_cell_guid":"cd06067b-cee3-4fd1-8421-e46aa976d8a0","_uuid":"c834e1c53746681bfa1b19f9e1cb45e50987a139","collapsed":true},"outputs":[],"cell_type":"code"},{"source":"We looked at the type of tools Data scientist use at work. Its mostly Python, R and SQL ","metadata":{"_cell_guid":"02e3862e-46d6-4272-b4a2-4d1f594cd3cb","_uuid":"9af42716992f2ac996ec57bb4c91f060780f72ea"},"cell_type":"markdown"},{"source":"resp = data.dropna(subset=['WorkToolsSelect'])                                                                \nresp = pd.merge(resp,rates,left_on='CompensationCurrency',right_on='originCountry',how='left')                \npython = resp[(resp['WorkToolsSelect'].str.contains('Python'))&(~resp['WorkToolsSelect'].str.contains('R'))]  \nR = resp[(~resp['WorkToolsSelect'].str.contains('Python'))&(resp['WorkToolsSelect'].str.contains('R'))]       \nboth = resp[(resp['WorkToolsSelect'].str.contains('Python'))&(resp['WorkToolsSelect'].str.contains('R'))] \n# python and R users recommendations:                                                \np_reconmd = python['LanguageRecommendationSelect'].value_counts()[:2]                \nr_reconmd = R['LanguageRecommendationSelect'].value_counts()[:2]                     \nlabels1 = p_reconmd.index                                                            \nvalues1 = p_reconmd.values                                                           \nlabels2 = r_reconmd.index                                                            \nvalues2 = r_reconmd.values                                                           \nf,ax = plt.subplots(1,2,figsize=(10,10))                                             \nax[0].pie(values1, labels = labels1,autopct='%1.1f%%', shadow=False, startangle=90)  \nax[0].axis('equal')                                                                  \nax[0].set_title('Python Users Recommendation')                                       \nax[1].pie(values2, labels = labels2,autopct='%1.1f%%', shadow=False, startangle=90)  \nax[1].axis('equal')                                                                  \nax[1].set_title('R Users Recommendation')                                            \nplt.show()      ","execution_count":null,"metadata":{"_cell_guid":"29af1478-138f-422a-83d1-582762f4a4d2","_uuid":"6f7ddcc8dc763326869d7ea9a2690fd5e4164852","collapsed":true},"outputs":[],"cell_type":"code"},{"source":"On further analysis we find that majority of python users use only python and a few use C++. But quite a few R users also use python. This reinforced the idea that python is the predominant lanaguage of choice for data scientists.","metadata":{"_cell_guid":"4ab9112b-ccad-4027-a45e-0f1c3f11dbf9","_uuid":"d6885a4e610bf17e5f8cfc066746f2e33116d9ae"},"cell_type":"markdown"},{"source":"#  python and R salary compare:  \npy_sal=(pd.to_numeric(python['CompensationAmount'].dropna(),errors='coerce')*python['exchangeRate']).dropna()\npy_avr_sal = pd.Series(py_sal).median()\n\nR_sal=(pd.to_numeric(R['CompensationAmount'].dropna(),errors='coerce')*R['exchangeRate']).dropna()\nR_avr_sal = pd.Series(R_sal).median()\n\nboth_sal=(pd.to_numeric(both['CompensationAmount'].dropna(),errors='coerce')*both['exchangeRate']).dropna()\nboth_avr_sal = pd.Series(both_sal).median()\nprint ('Median Salary For Individual using Python:',py_avr_sal)\nprint ('Median Salary For Individual using R:',R_avr_sal)\nprint ('Median Salary For Individual knowing both languages:',both_avr_sal)","execution_count":null,"metadata":{"_cell_guid":"fbf53c81-6328-4fa8-84fc-e778568c6b95","_uuid":"76a39b36d31183631346b7adc5550a7a0755748a","collapsed":true},"outputs":[],"cell_type":"code"},{"source":"Clearly the salary for Python is higher. Note that this is a median salary across the word. So a $1000 difference could be significant based on which part of the world you are in.","metadata":{"_cell_guid":"9892cfa2-19c9-4fd4-a528-665e819f13cd","_uuid":"340f202fd59bccc3856db2308be3c2150d7695e6"},"cell_type":"markdown"},{"source":"#### Next we start looking at job titles and see how it effects there compensation and inclination to switch their jobs.","metadata":{"_cell_guid":"6833efde-0a1b-4839-b5b4-e5e30076dc40","_uuid":"854d7f18d5dac2044dbf6d83aed9f541e782a3f0"},"cell_type":"markdown"},{"source":"                                                                      \nsalary=data[['CompensationAmount','CompensationCurrency','GenderSelect','Country','CurrentJobTitleSelect']].dropna()   \nsalary=pd.merge(salary,rates,left_on='CompensationCurrency',right_on='originCountry',how='left')                       \n                                                                                                                       \nsalary['Salary'] = pd.to_numeric(salary['CompensationAmount'],errors='coerce') * salary['exchangeRate'].dropna()       \nsalary_null = pd.isnull(salary['Salary'])                                                                              \n                                                                                                                       \nsalary_null_false= salary['Salary'][salary_null == False][salary['Salary'] >= 0]                                       \n\n#Compensation By Job Title   \nsal_job = salary.groupby('CurrentJobTitleSelect')['Salary'].median().to_frame().sort_values(by='Salary',ascending=False)\nplt.subplots(figsize=(10,10))                                                                                           \nsns.barplot(sal_job.Salary,sal_job.index)                                                                               \nplt.title('Compensation By Job Title')                                                                                  \nplt.show()                   ","execution_count":null,"metadata":{"_cell_guid":"9d62656d-e836-4823-b8a9-6c349645efe1","_uuid":"24b08486707428457d11a69cb1487e952c00601a","collapsed":true},"outputs":[],"cell_type":"code"},{"source":"Next we see who many of the respondents who are employed write code regularly\n#### Does Data Scientist always need to write code?","metadata":{"_cell_guid":"42f9f174-7ab5-42b1-8f8b-9faa67f30ece","_uuid":"1eed5e6f8d7314f73c58f2e8617a168b6bce1795"},"cell_type":"markdown"},{"source":"\nemployed=['Employed full-time','Independent contractor, freelancer, or self-employed','Employed part-time']\ndf_employed=df_raw[df_raw['EmploymentStatus'].isin(employed)]\ndf_employed[['CodeWriter']].groupby(['CodeWriter']).size().reset_index(name='counts')","execution_count":null,"metadata":{"_cell_guid":"ba977ad9-0b8a-4603-85f0-c704992b6b2e","_uuid":"02cca2cea7b762674a8d058c4e315b1df8e69591","collapsed":true},"outputs":[],"cell_type":"code"},{"source":"80% of the respondents who are employed write code. \n\nWe can now check how many of the respondents are looking to switch their jobs.","metadata":{"_cell_guid":"89c01ee2-d330-4b86-af37-dc06d307b842","_uuid":"3ca1e3a57287bbcca5dba5cd95dc9ef72994aab4"},"cell_type":"markdown"},{"source":"switcher=df_raw[df_raw['CareerSwitcher']=='Yes']\nswitcher[['CurrentJobTitleSelect']].groupby(['CurrentJobTitleSelect']).size().reset_index(name='counts').sort_values(['counts'],ascending=[0])","execution_count":null,"metadata":{"_cell_guid":"4e449e4e-b8e8-48bf-b380-ecc66a4eafb2","_uuid":"3746b493fb4d00ce4999c1989ae455bc162b9f9c","collapsed":true},"outputs":[],"cell_type":"code"},{"source":"Software Developer, programmer and Business analyst are the major population who are seeking for new opportunities which reflects: \na. Great mobility of these roles. There are tons of opportunities for people with these skills.\nb. High expectation of future career development and better compensation\n\n","metadata":{"_cell_guid":"9379743d-5577-4705-b2c6-9c6f5e82ef8a","_uuid":"cf7a4fa876648982644a5675c537da50aa8f90fc"},"cell_type":"markdown"},{"source":"df_raw['TitleFit_Score']=df_raw['TitleFit'].apply(lambda x: 5 if x=='Perfectly' else (3 if x=='Fine' else 1))\ntitleFit_idx=df_raw['TitleFit'].isnull()\ndf_withTitleFit=df_raw[~titleFit_idx]\nprint(df_withTitleFit.groupby(['CurrentJobTitleSelect'])['TitleFit_Score'].mean())","execution_count":null,"metadata":{"_cell_guid":"bbbe599c-01ac-4d78-8190-7c1f107f17e9","_uuid":"6d05f0cd074c1f864c28fdd8d50b659869ada67a","collapsed":true},"outputs":[],"cell_type":"code"},{"source":"Business Analyst and Engineer overall have the lowest 'title fit' rate which might because that their work has too diversed responsibility which are hard to be summazied with one title.\n\nGiven that we have analyzed respondents opinions on on the langauges, tools and their titles. Given how often people switch jobs and the opportunities they have, the next logical question to ask would be,\n\nWhat are the indicators of Job Satisfaction and can we predict how job satisfaction of an employee based on the the environmnet they are in. \n\nJob Satisfaction is a attribute on the responses. We use that as the category label.\n\n### Predicting JobSatisfaction ","metadata":{"_cell_guid":"8dae16ec-c027-45de-8e2b-2b86c0b9ddd7","_uuid":"2af1c7a68a541b00fba16dfeaccc19a337381ef7"},"cell_type":"markdown"},{"source":"df_raw_pre=loadData(yourFilePath)\nobj_df1 = df_raw_pre.select_dtypes(include=['object']).copy()\nobj_df1[obj_df1.isnull().any(axis=1)]\n\nnonCatlistColumns = ['Age','LearningCategorySelftTaught','LearningCategoryOnlineCourses','LearningCategoryWork','LearningCategoryUniversity',\n     'LearningCategoryKaggle','LearningCategoryOther','TimeGatheringData',\n     'TimeModelBuilding','TimeProduction','TimeVisualizing','TimeFindingInsights','TimeOtherSelect']\n\ndef convertToInt(amt):\n    try:\n        if ',' in amt:\n            return float(amt.replace(',',''))\n        elif amt != '-':\n            return float(amt)\n    except:\n        return float(amt)\n\n        \ndef convertToString(str):\n    if '' == str:\n        return ''\n    else:\n        return str\n    \ndef groupSatifaction(val):\n    if 0 < val and val < 7:\n        return 1\n    #elif 5<= val and 7 > val:\n     #   return 2\n    elif 7<= val:\n        return 2\n    else:\n        return 0\n    \ncompensation = ['CompensationAmount','CompensationCurrency']\ncomp = obj_df1['CompensationAmount']\ncompCur = obj_df1['CompensationCurrency']\n\ncomp = comp.apply(convertToInt )\ncompCur = compCur.apply(convertToString)\nobj_df1['CompensationAmount'] = comp\n    \ncolumns = list(df_raw_pre.columns)\nfor column in columns:\n    if column in nonCatlistColumns:\n        continue\n    obj_df1[column] = obj_df1[column].astype('category').cat.codes","execution_count":null,"metadata":{"_cell_guid":"30adb530-6ba9-4d56-bc49-1144984ecec8","_uuid":"8f833d350370b33f7b9a74275f6fb8fbbeb74743","collapsed":true},"outputs":[],"cell_type":"code"},{"source":"def featurize():\n    return DataFrameMapper(obj_df1.columns)\n\n\npipeline = Pipeline([('featurize', featurize()), ('forest', RandomForestClassifier())])\n\ny = obj_df1['JobSatisfaction'].apply(groupSatifaction)\nX = obj_df1[obj_df1.columns.drop('JobSatisfaction')]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\nfrom sklearn.ensemble import RandomForestClassifier\nclf_rfc = RandomForestClassifier(max_depth=2, random_state=0)\nclf_rfc.fit(X_train, y_train)\n\ni=0\ncolumns = list(X.columns)\nfor val in np.nditer(clf_rfc.feature_importances_):\n    if val > 0.01 :\n        print (val, columns[i])\n    i = i+1    \n\n","execution_count":null,"metadata":{"_cell_guid":"a6c18088-5907-4e77-9f94-92e66702272d","_uuid":"f3a0f845f2573fb7fc68400b2b5c31daf90ef99c","collapsed":true},"outputs":[],"cell_type":"code"},{"source":"print(clf_rfc.score(X_test, y_test))","execution_count":null,"metadata":{"_cell_guid":"108df4ee-b6cf-44e5-a99c-2c5be4c03d4f","_uuid":"83d77a2f410cd52306eda7bedebf2a435ee8d359","collapsed":true},"outputs":[],"cell_type":"code"},{"source":"We can see the we can predict if an employee is satisfied based on other attributes. We belive that this is very useful for companies to determine employee satisfaction. The features that are needed for prediction would already be avaliable with companies. This model gives us the ability to predict satisfaction based on the know attributes about a company.","metadata":{"_cell_guid":"e044da1c-1579-4e0a-aef4-db8ddef632b8","_uuid":"01f3b7f4577e1be421c65cfdea272deca09d5e34"},"cell_type":"markdown"},{"source":"Another intresting observation is that WorktoolSelect is the most important attribute for employee satisfaction. It means that people are passionate about their work. The tools they use matters to them. So can we help identify the learning paltforms that people find useful. This will help people pick better work tools and thus lead to greater employee job satisfaction.","metadata":{"_cell_guid":"3d5898a6-1e16-46d1-a74d-4f1a8ab2d73e","_uuid":"0c71ad36e142fce37474c452fceae385ad4d348d"},"cell_type":"markdown"},{"source":"df_raw[['MLToolNextYearSelect']].groupby(['MLToolNextYearSelect']).size().reset_index(name='counts').sort_values(['counts'],ascending=[0])","execution_count":null,"metadata":{"_cell_guid":"b858f1c1-df13-4b41-b7f7-a86378b5508b","_uuid":"37bef61a1099112a4b60623c805580d888c741f6","collapsed":true},"outputs":[],"cell_type":"code"},{"source":"Tool for the future, TensorFlow, Python, R, Spark, Hadoop -- very clear trend of future","metadata":{"_cell_guid":"83cb4fff-48b5-4120-8496-9a4d77e02069","_uuid":"2a49d75db829174b1783b97e6ffa8109a481932d"},"cell_type":"markdown"},{"source":"df_raw[['MLMethodNextYearSelect']].groupby(['MLMethodNextYearSelect']).size().reset_index(name='counts').sort_values(['counts'],ascending=[0])","execution_count":null,"metadata":{"_cell_guid":"b291a346-4268-47d7-bafd-d879e4128c73","_uuid":"97e182dddacc5d2fd5ec10f6fdba3de8a1677699","collapsed":true},"outputs":[],"cell_type":"code"},{"source":"Algos for the future, Deep learning, Neural Nets, Time Series Analysis, Bayesian Methods, Text mining -- very clear trend of future.\n\nLets look at learning platform usefulness for data scientists.","metadata":{"_cell_guid":"d44b7d06-6427-4d61-8307-8846ba88a086","_uuid":"f492455b77cf829e13580eed112dca787330885f"},"cell_type":"markdown"},{"source":"learningPlatformSurvey=['LearningPlatformUsefulnessArxiv','LearningPlatformUsefulnessBlogs'\n                        ,'LearningPlatformUsefulnessCollege','LearningPlatformUsefulnessCompany'\n                        ,'LearningPlatformUsefulnessConferences','LearningPlatformUsefulnessFriends'\n                        ,'LearningPlatformUsefulnessKaggle','LearningPlatformUsefulnessNewsletters'\n                        ,'LearningPlatformUsefulnessCommunities'\n                        ,'LearningPlatformUsefulnessDocumentation','LearningPlatformUsefulnessCourses'\n                        ,'LearningPlatformUsefulnessProjects','LearningPlatformUsefulnessPodcasts'\n                        ,'LearningPlatformUsefulnessSO','LearningPlatformUsefulnessTextbook'\n                        ,'LearningPlatformUsefulnessTradeBook','LearningPlatformUsefulnessTutoring'\n                        ,'LearningPlatformUsefulnessYouTube']\nfor surveyTarget in learningPlatformSurvey:\n    df_raw['{}_Score'.format(surveyTarget)]=df_raw[surveyTarget].apply(lambda x: 5 if x=='Very useful' else (3 if x=='Somewhat useful' else 1))\n\n    \n# for DataScientist\ndf_raw_DataScientist=df_raw[df_raw['CurrentJobTitleSelect']=='Data Scientist']\nfor surveyTarget in learningPlatformSurvey:\n    print ('{} average rate is {}'.format(surveyTarget,df_raw_DataScientist['{}_Score'.format(surveyTarget)].mean()))","execution_count":null,"metadata":{"_cell_guid":"6fee07f7-7744-4358-908b-e14ba848a076","_uuid":"786e7ab1cee3a6eac81e32d3db3a293e661a298c","collapsed":true},"outputs":[],"cell_type":"code"},{"source":"For Data Scientist group, we find almost all the ratings of the platforms are higher than the whole population which show the strong willingness to learn new skills for this community and also it shows that data science area is an actively moving industry. Stack Overflow are highly recommended in this community and very interestingly college is not highly rated as by the whole population","metadata":{"_cell_guid":"ea293717-5c50-41a1-9d32-5d125284ec1f","_uuid":"64ac0b77df4f9c55b07e4c43b4cbf4fde6bc2dad"},"cell_type":"markdown"},{"source":"# for Software engineer\ndf_raw_SoftwareEngineer=df_raw[df_raw['CurrentJobTitleSelect']=='Software Developer/Software Engineer']\nfor surveyTarget in learningPlatformSurvey:\n    print ('{} average rate is {}'.format(surveyTarget,df_raw_SoftwareEngineer['{}_Score'.format(surveyTarget)].mean()))","execution_count":null,"metadata":{"_cell_guid":"20d4303b-8367-42ef-8cf2-a2f4554dec4a","_uuid":"f0a8e4abfda6e5dd783269a2a1966693544e6ed0","collapsed":true},"outputs":[],"cell_type":"code"},{"source":"Comparatively, Software engineer is a more mature industry even thought the techniques and tools are still actively updated everyday. They are not as passionate as data scientist in learning. And also we see a drop in the college rating for this community which is very interesting. And surprisingly, SO also gets a lower rate.\n\nThere are a lot of tools and each has its own usefulness based on job title, the usefulness is very close. Can we do pair analysis and see which two learning platforms together would be most useful.","metadata":{"_cell_guid":"08fcdded-1112-4ade-83d5-a5e07bf58ffe","_uuid":"cdb1971e87d39b2bbb22ce6de1bebc38c23b0d22"},"cell_type":"markdown"},{"source":"from apyori import apriori\nrules = apriori(platformPick, min_support = 0.003, min_confidence = 0.2, min_lift = 3, min_length = 2)\nresults=list(rules)[:3]\nprint(results)","metadata":{"_cell_guid":"f163d1a8-b894-4a51-8fa8-c3325619668c","_uuid":"1016cc38af1eb1d9843178cefc9e6960768fd689","collapsed":true},"cell_type":"markdown"},{"source":"def load_dataset():\n    \"Load the sample dataset.\"\n    return basketList\n\n\ndef createC1(dataset):\n    \"Create a list of candidate item sets of size one.\"\n    c1 = []\n    for transaction in dataset:\n        for item in transaction:\n            if not [item] in c1:\n                c1.append([item])\n    c1.sort()\n    #frozenset because it will be a ket of a dictionary.\n    return map(frozenset, c1)\n\n\ndef scanD(dataset, candidates, min_support):\n    \"Returns all candidates that meets a minimum support level\"\n    sscnt = {}\n    for tid in dataset:\n        for can in candidates:\n            if can.issubset(tid):\n                sscnt.setdefault(can, 0)\n                sscnt[can] += 1\n\n    num_items = float(len(dataset))\n    retlist = []\n    support_data = {}\n    for key in sscnt:\n        support = sscnt[key] / num_items\n        if support >= min_support:\n            retlist.insert(0, key)\n        support_data[key] = support\n    return retlist, support_data\n\n\ndef aprioriGen(freq_sets, k):\n    \"Generate the joint transactions from candidate sets\"\n    retList = []\n    lenLk = len(freq_sets)\n    for i in range(lenLk):\n        for j in range(i + 1, lenLk):\n            L1 = list(freq_sets[i])[:k - 2]\n            L2 = list(freq_sets[j])[:k - 2]\n            L1.sort()\n            L2.sort()\n            if L1 == L2:\n                retList.append(freq_sets[i] | freq_sets[j])\n    return retList\n\n\ndef apriori(dataset, minsupport=0.5):\n    \"Generate a list of candidate item sets\"\n    C1 = createC1(dataset)\n    D = map(set, dataset)\n    L1, support_data = scanD(D, C1, minsupport)\n    L = [L1]\n    k = 2\n    while (len(L[k - 2]) > 0):\n        Ck = aprioriGen(L[k - 2], k)\n        Lk, supK = scanD(D, Ck, minsupport)\n        support_data.update(supK)\n        L.append(Lk)\n        k += 1\n\n    return L, support_data","metadata":{"_cell_guid":"32c82070-0290-4eb3-b46d-6e1cf4bbebdc","_uuid":"025080ca1b6a1c665ba366a1fe1a11432ebd5922","collapsed":true},"cell_type":"markdown"},{"source":"apriori(platformPick,0.4)","metadata":{"_cell_guid":"73d87f12-2182-4cec-8ac4-e3a67ebd5cef","_uuid":"21a4f6e83640504d40c373fab2b8ff7c316312f9","collapsed":true},"cell_type":"markdown"},{"source":"","execution_count":null,"metadata":{"_cell_guid":"7e3f4b23-bf9c-4e50-a2f5-4cbfae7ac1bb","_uuid":"08410fd1325dc3122333aed0a53569b33b3c1ff6","collapsed":true},"outputs":[],"cell_type":"code"}]}