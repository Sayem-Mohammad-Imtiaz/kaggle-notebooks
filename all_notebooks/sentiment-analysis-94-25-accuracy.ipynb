{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#Import Statements\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport spacy\n\nfrom xgboost import XGBClassifier\nfrom sklearn.svm import LinearSVC","metadata":{"execution":{"iopub.status.busy":"2021-07-03T19:37:03.269792Z","iopub.execute_input":"2021-07-03T19:37:03.270128Z","iopub.status.idle":"2021-07-03T19:37:05.490345Z","shell.execute_reply.started":"2021-07-03T19:37:03.2701Z","shell.execute_reply":"2021-07-03T19:37:05.489555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Extract reddit data\nreddit_data = pd.read_csv('../input/twitter-and-reddit-sentimental-analysis-dataset/Reddit_Data.csv')\nreddit_data.rename(columns = {'clean_comment': 'text'}, inplace = True)","metadata":{"execution":{"iopub.status.busy":"2021-07-03T19:37:05.491726Z","iopub.execute_input":"2021-07-03T19:37:05.492161Z","iopub.status.idle":"2021-07-03T19:37:05.754716Z","shell.execute_reply.started":"2021-07-03T19:37:05.49213Z","shell.execute_reply":"2021-07-03T19:37:05.753904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Extract twitter data\ntwitter_data = pd.read_csv('../input/twitter-and-reddit-sentimental-analysis-dataset/Twitter_Data.csv')\ntwitter_data.rename(columns = {'clean_text': 'text'}, inplace = True)","metadata":{"execution":{"iopub.status.busy":"2021-07-03T19:37:05.755721Z","iopub.execute_input":"2021-07-03T19:37:05.756103Z","iopub.status.idle":"2021-07-03T19:37:06.469714Z","shell.execute_reply.started":"2021-07-03T19:37:05.756075Z","shell.execute_reply":"2021-07-03T19:37:06.468909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Combine both dataframes into one master dataframe\ndata = pd.concat([reddit_data, twitter_data], ignore_index = True)","metadata":{"execution":{"iopub.status.busy":"2021-07-03T19:37:13.918705Z","iopub.execute_input":"2021-07-03T19:37:13.919061Z","iopub.status.idle":"2021-07-03T19:37:13.936508Z","shell.execute_reply.started":"2021-07-03T19:37:13.919032Z","shell.execute_reply":"2021-07-03T19:37:13.935288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Check for any null values\ndata.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2021-07-03T19:37:21.72189Z","iopub.execute_input":"2021-07-03T19:37:21.72247Z","iopub.status.idle":"2021-07-03T19:37:21.756183Z","shell.execute_reply.started":"2021-07-03T19:37:21.722434Z","shell.execute_reply":"2021-07-03T19:37:21.755296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Drop rows with null values\ndata.dropna(axis = 0, inplace = True)","metadata":{"execution":{"iopub.status.busy":"2021-07-03T19:37:28.91935Z","iopub.execute_input":"2021-07-03T19:37:28.919741Z","iopub.status.idle":"2021-07-03T19:37:28.987911Z","shell.execute_reply.started":"2021-07-03T19:37:28.919712Z","shell.execute_reply":"2021-07-03T19:37:28.987082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Checking the shape of the data to ensure nothing is broken\ndata.shape","metadata":{"execution":{"iopub.status.busy":"2021-07-03T19:37:36.505621Z","iopub.execute_input":"2021-07-03T19:37:36.505984Z","iopub.status.idle":"2021-07-03T19:37:36.511468Z","shell.execute_reply.started":"2021-07-03T19:37:36.505953Z","shell.execute_reply":"2021-07-03T19:37:36.510552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Getting Stopwords\nfrom spacy.lang.en.stop_words import STOP_WORDS\nstopwords = list(STOP_WORDS)\n\n#Getting a list of punctuations\nfrom string import punctuation\npunct = list(punctuation)\n\nprint(\"Length of punctuations:\\t {} \\nLength of stopwords:\\t {}\".format(len(punct), len(stopwords)))","metadata":{"execution":{"iopub.status.busy":"2021-07-03T19:37:43.248003Z","iopub.execute_input":"2021-07-03T19:37:43.248362Z","iopub.status.idle":"2021-07-03T19:37:43.26797Z","shell.execute_reply.started":"2021-07-03T19:37:43.248332Z","shell.execute_reply":"2021-07-03T19:37:43.267202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig , ax = plt.subplots(figsize = (10,10))\nax = data['category'].value_counts().plot(kind = 'bar')\n\nplt.xticks(rotation = 0, size = 14)\nplt.yticks(size = 14, color = 'white')\nplt.title('Distribution of Sentiment', size = 20)\n\nax.annotate(text = data['category'].value_counts().values[0], xy = (-0.13,88079), size = 18)\nax.annotate(text = data['category'].value_counts().values[1], xy = (0.87,68253), size = 18)\nax.annotate(text = data['category'].value_counts().values[2], xy = (1.87,43786), size = 18)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-03T19:37:51.582176Z","iopub.execute_input":"2021-07-03T19:37:51.582659Z","iopub.status.idle":"2021-07-03T19:37:51.803171Z","shell.execute_reply.started":"2021-07-03T19:37:51.582626Z","shell.execute_reply":"2021-07-03T19:37:51.802469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer,HashingVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport numpy as np\n\n# Create a Vectorizer Object using default parameters\nhash_vectorizer = HashingVectorizer()\n\n# Convert a collection of text documents to a matrix of token counts\ntoken_count_matrix=hash_vectorizer.fit_transform(data['text'])\nprint(f'The size of the count matrix for the texts = {token_count_matrix.get_shape()}')\nprint(f'The sparse count matrix is as follows:')\nprint(token_count_matrix)\n\n# Create a tf_idf object using default parameters\ntf_idf_transformer=TfidfTransformer(use_idf=True, smooth_idf=True, sublinear_tf=False) \n\n# Fit to the count matrix, then transform it to a normalized tf-idf representation\ntf_idf_matrix = tf_idf_transformer.fit_transform(token_count_matrix)\n\nprint(f'The size of the tf_idf matrix for the texts = {tf_idf_matrix.get_shape()}')\nprint(f'The sparse tf_idf matrix is as follows:')\nprint(tf_idf_matrix)","metadata":{"execution":{"iopub.status.busy":"2021-07-03T19:38:07.084908Z","iopub.execute_input":"2021-07-03T19:38:07.085393Z","iopub.status.idle":"2021-07-03T19:38:12.5219Z","shell.execute_reply.started":"2021-07-03T19:38:07.085363Z","shell.execute_reply":"2021-07-03T19:38:12.520888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Getting X and y\n\nX = tf_idf_matrix\ny = data.category\n\n#Splitting the data into training and testing\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y,\n                                                    test_size=0.1)","metadata":{"execution":{"iopub.status.busy":"2021-07-03T19:38:22.460669Z","iopub.execute_input":"2021-07-03T19:38:22.461027Z","iopub.status.idle":"2021-07-03T19:38:22.502074Z","shell.execute_reply.started":"2021-07-03T19:38:22.460998Z","shell.execute_reply":"2021-07-03T19:38:22.500961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Creating, fitting and scoring classifier\nclassifier = LinearSVC()\nclassifier.fit(X_train, y_train)\nprint(f\"Accuracy: {classifier.score(X_test, y_test) * 100:.3f}%\", )","metadata":{"execution":{"iopub.status.busy":"2021-07-03T19:38:33.310983Z","iopub.execute_input":"2021-07-03T19:38:33.311329Z","iopub.status.idle":"2021-07-03T19:38:40.862703Z","shell.execute_reply.started":"2021-07-03T19:38:33.3113Z","shell.execute_reply":"2021-07-03T19:38:40.861547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}