{"cells":[{"metadata":{"_uuid":"ee5af6b5746b7efc6721e544d405c89c940c15a9","_cell_guid":"f0223808-7b84-241d-baf7-8b3033f5e78c"},"cell_type":"markdown","source":"# Analysing Wine\n_Started on 10 October 2017_\n\n_Reviewed on 15 December 2017_\n* The purpose of this kernel is to explore several ML classifiers on the wine datasets."},{"metadata":{"collapsed":true,"_uuid":"403542149c8c0fed069c48269d86e2fc76503f70","_cell_guid":"adcfb11a-ba70-e71b-1793-a5ec23d8397f","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pandas.plotting import scatter_matrix\n\n# Modelling Helpers\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n# Modelling Algorithms\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC, LinearSVC","execution_count":1,"outputs":[]},{"metadata":{"_uuid":"7ec01ae2d72a6412577789edf9a80be94aa577ab","_cell_guid":"7bd7598b-5cb3-2751-23a5-e80e52d24726"},"cell_type":"markdown","source":"# Load data"},{"metadata":{"collapsed":true,"_uuid":"65fd03825969dcf645ed6ec6594b07d4ad476315","_cell_guid":"4abdff95-884d-1e32-f980-8e2133f65c95","trusted":true},"cell_type":"code","source":"redWine = pd.read_csv('../input/winequality-red.csv')\nwhiteWine = pd.read_csv('../input/winequality-white.csv')","execution_count":2,"outputs":[]},{"metadata":{"_uuid":"d24030a0558ddbf84179579b8cabc73091318e70","_cell_guid":"cdcbd673-673b-da21-acb1-14285ecfc162","trusted":true},"cell_type":"code","source":"redWine.describe()","execution_count":3,"outputs":[]},{"metadata":{"_uuid":"77534fcb0501b1cb7cdc43b88935ccf4c1ded5b4","_cell_guid":"c1befa6f-8692-fec4-d93e-aa17bef48a9c","trusted":true},"cell_type":"code","source":"whiteWine.describe()","execution_count":4,"outputs":[]},{"metadata":{"_uuid":"2f4badba50e1319445340ef4f7a847765809342c","_cell_guid":"f4e4627d-9e71-59ca-26ef-844b8bb5b129","trusted":true},"cell_type":"code","source":"#Checking for duplicates\nprint(\"Number of duplicates in red wine: \"+ str(np.sum(np.array(redWine.duplicated()))))\nprint(\"Number of duplicates in white wine:  \"+ str(np.sum(np.array(whiteWine.duplicated()))))","execution_count":5,"outputs":[]},{"metadata":{"_uuid":"e1d5773e12d08637be8bebddabfb00708ce074f7","_cell_guid":"59f28c8d-c2c6-7c5a-a1ce-a7fa684b746f"},"cell_type":"markdown","source":"The red wine data has 240 duplicated rows whereas white wine has 937. While duplicated rows may cause biases in analysis and inference, I think the duplicated rows here look more like several wine tasters rating the same wine similarly. Hence, it will be relevant to keep all the observations as this can add more information."},{"metadata":{"_uuid":"d63f14324fd9065cf9be88c73f21ee7250a43df9","_cell_guid":"6e605e93-d54b-7ec6-81a2-6e2ccdbed722","trusted":true},"cell_type":"code","source":"# Combining the red and white wine data\nwine_df = redWine.append(whiteWine)\nprint(wine_df.shape)","execution_count":6,"outputs":[]},{"metadata":{"_uuid":"2866091eb83001647ac991840fedb86488b04888","_cell_guid":"21929716-d6ac-f6e3-04fc-5f879d8c2bab"},"cell_type":"markdown","source":"### Summary Statistics"},{"metadata":{"_uuid":"005b74d54300f82cdc301f564f362c26e974b75f","_cell_guid":"fb9fe5a6-f9b6-98f9-bdc8-311376c5fdd9","trusted":true},"cell_type":"code","source":"wine_df.describe()","execution_count":7,"outputs":[]},{"metadata":{"_uuid":"e4d45a71a0809c0b3abd25b06a0bdca2a171dea2","_cell_guid":"c9f88159-d3c2-881b-b9b9-2cadf6b89f97"},"cell_type":"markdown","source":"# Exploratory data analysis (EDA)"},{"metadata":{"_uuid":"44a7d6d6b7655bdd0b669adec7b3c42516db4844","_cell_guid":"a61556b2-30f7-b712-30df-20fafdd85cd6","trusted":true},"cell_type":"code","source":"# Features for the wine data\nsns.set()\npd.DataFrame.hist(wine_df, figsize = [15,15], color='green')\nplt.show()","execution_count":8,"outputs":[]},{"metadata":{"_uuid":"d78e861cde1da198eb96e7b3922eeb7c9e2df8c9","_cell_guid":"c447c9d5-9b3c-c21e-05d2-89a5c66996fa"},"cell_type":"markdown","source":"#### Range of predictor variables for wine based on the histogram above\n* Free sulfur dioxide from 0 to 120; total sulfur dioxide from 0 to 300.\n* Volatile acidity from 0 to 1.1; sulphates from 0.2 to 1.3; chlorides from 0 to 0.4.\n* The scale of the features are quite different. Hence, using algorithms that are based on distance, e.g. kNN, may focus unfairly on these features. To use such classifiers, the data would have to be normalized.\n* Wine quality rating are discrete ranging from 3 to 9, with large proportion in the category 5, 6 & 7."},{"metadata":{"_uuid":"fbcb42484db54d9e8a579b665776787c8567c844","_cell_guid":"5b64698d-ca56-def3-5a6a-cc8bf06bceb1"},"cell_type":"markdown","source":"### Correlation Heatmap\n\nThis is to generate some correlation plots of the features to see how related one feature is to the next. The Seaborn plotting package which allows us to plot heatmaps very conveniently can be used as follow."},{"metadata":{"_uuid":"67b270ba5c923f48782604b2145bc586631d17bd","_cell_guid":"32c9b556-d10e-a10f-29b7-b38783e4376e","trusted":true},"cell_type":"code","source":"colormap = plt.cm.viridis\nplt.figure(figsize=(12,12))\nplt.title('Correlation of Features', y=1.05, size=15)\nsns.heatmap(wine_df.astype(float).corr(),linewidths=0.1,vmax=1.0, square=True, cmap=colormap, linecolor='white', annot=True)","execution_count":9,"outputs":[]},{"metadata":{"_uuid":"704bdabff2572588d8341885b470359e9155312b","_cell_guid":"6c39686d-b863-d5f1-a630-3b6b48b157fd"},"cell_type":"markdown","source":"#### Observations from the Plots\nOne thing that the correlation plot tells us is that there are very few features strongly correlated with one another. From the point of view of feeding these features into our ML models,  this means that there isn't much redundant data as each feature carries with it some unique information. \n\nFrom the plot, the two most correlated features free sulfur dioxide and total sulfur dioxide. Another point is that alcohol correlates most with quality than any of the other features at 0.44."},{"metadata":{"_uuid":"64d1d43bd65e388f3398de6b397976fb9792fb56","_cell_guid":"a026cd8e-f5cd-4e11-be44-ccd856c340f4"},"cell_type":"markdown","source":"### Trend of the features grouped quality\nLet's examine the plot. We will scaled the features first to have a better visualization."},{"metadata":{"_uuid":"8604ce8df148beb65f21a9b6bcff71b29c3bb572","_cell_guid":"863de555-e373-4e0d-a7a6-3dd59c9fb209","trusted":true},"cell_type":"code","source":"cols_to_scale = wine_df.columns.tolist()\ncols_to_scale.remove('quality')\nscaled_wine_df = wine_df\nscaler = MinMaxScaler()\nscaled_wine_df[cols_to_scale] = scaler.fit_transform(scaled_wine_df[cols_to_scale])\nscaled_wine_df.groupby('quality').mean().plot(kind='bar', figsize=(15,5))\nplt.xlim(-1,9)","execution_count":10,"outputs":[]},{"metadata":{"_uuid":"0b71a0eb743436ca1034064f2efd55db875868b0","_cell_guid":"2bf1068b-e42d-42e7-b005-26d89894d3eb"},"cell_type":"markdown","source":"#### Observations from the Plots\nFrom this plot, it looks like alcohol, volatile acidity, citric acid, chloride, density are correlated to wine quality. These all sounds like \"french\" to me, but perhaps to the wine connoiseurs, the observations from the above plot may mean something."},{"metadata":{"_uuid":"80c042f1fc5335c501d79d4d9db59b51743707be","_cell_guid":"25a643f6-92e9-2224-b6c2-a798ff6c7fd1"},"cell_type":"markdown","source":"# Wine quality analysis"},{"metadata":{"_uuid":"51b2f78e9c1b438c701677e9bb084525e60ac88d","_cell_guid":"7e8c1094-0f23-d2eb-21e0-0216184bf947"},"cell_type":"markdown","source":"#### Investigating how different chemical levels affect the quality of wine\n* View quality of wine as a continuous variable from 0 to 10. This view yields a regression problem.\n* Define good wine to be a wine having quality larger than or equal to 6. This yields a binary classification problem of separating good wine from not so good wine.\n\n#### I will focus on the classification subtask for this notebook\n* In classification, the goal is to minimize classification error, which is (1 - accuracy). "},{"metadata":{"collapsed":true,"_uuid":"0f45cfaa108ca5b435f35829ba6af5352a8b3eff","_cell_guid":"3bc91c18-b23b-4eb5-8266-3f9070093406","trusted":true},"cell_type":"code","source":"y = wine_df['quality']\nX = wine_df.drop('quality', axis=1)","execution_count":11,"outputs":[]},{"metadata":{"_uuid":"febffa376841f17c3c843e952ed069d9cfe387b3","_cell_guid":"d36296ac-adf2-b39d-a5d7-0286f627b626","trusted":true},"cell_type":"code","source":"y1 = y > 5 # is the rating > 5? \n# plot histograms of original target variable (quality)\n# and aggregated target variable\n\nplt.figure(figsize=(20,5))\nplt.subplot(121)\nplt.hist(y, color='black')\nplt.title('Wine Quality Distribution')\nplt.xlabel('original target value')\nplt.ylabel('count')\n\nplt.subplot(122)\nplt.title('Wine Quality Distribution')\nplt.hist(y1, color='black')\nplt.xlabel('aggregated target value')\nplt.show()","execution_count":12,"outputs":[]},{"metadata":{"_uuid":"1aab961a63fc1072d17b7326062a2b93d65d5cfc","_cell_guid":"fe08a206-d982-f615-0de9-89cdc761922e"},"cell_type":"markdown","source":"The second subplot shows the count distribution of good and bad wine."},{"metadata":{"_uuid":"75e467e7d943f66f1e42a2f3cfbf3d46f7853d5e","_cell_guid":"791ebbfb-457d-858b-bc33-0f2a416e14ad"},"cell_type":"markdown","source":"## Algorithm Evaluation\nProcedure:\n* Separate out a validation dataset.\n* Set-up the test to use 10-fold cross validation.\n* Build four common classification models to predict quality from other wine features.\n* Select the best model."},{"metadata":{"_uuid":"110b1c509c943c6d33ac504a79dc208200de4a26","_cell_guid":"7b5461df-de8b-23ce-482c-fc1d9347fd3b"},"cell_type":"markdown","source":"### Split into training and test data"},{"metadata":{"collapsed":true,"_uuid":"edfae98fdc0c0da29061f57d71b1f3e922b47bbf","_cell_guid":"d55d65ac-9842-06ec-d166-1745d4044d9c","trusted":true},"cell_type":"code","source":"seed = 8 # for reproducibility\nX_train,X_test,y_train,y_test = train_test_split(X, y1, test_size=0.2, random_state=seed)","execution_count":13,"outputs":[]},{"metadata":{"_uuid":"9f6c2eea446d6dc6dadb7b6c99ebfc48a9a3b857","_cell_guid":"c24cbc7d-3dd8-de65-c504-fc107d402c1f"},"cell_type":"markdown","source":"The split sets aside 20% of the data as test set for evaluating the model."},{"metadata":{"_uuid":"1da0d55534f855c97f11c9f6d8c0c520ce3d7b87","_cell_guid":"1c8bb1ac-efa8-53f4-3b3f-4f9a7ea1d174","trusted":true},"cell_type":"code","source":"X_train.shape, y_train.shape, X_test.shape, y_test.shape ","execution_count":14,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"40f3397f60029546f959044bdc9051b6f11f1284","_cell_guid":"7c40c33a-6c83-78df-1db8-e969f4e63739"},"cell_type":"markdown","source":"### Evaluating common classification models"},{"metadata":{"collapsed":true,"_uuid":"a6212c6d064d3721dfadd7f3a5d8b0802d27ec4a","_cell_guid":"daeb4de9-98b4-4172-86f3-d784ab68871f","trusted":true},"cell_type":"code","source":"# Spot Check Algorithms\nmodels = []\nmodels.append(('LR', LogisticRegression()))\nmodels.append(('RF', RandomForestClassifier()))\nmodels.append(('DT', DecisionTreeClassifier()))\nmodels.append(('SVM_rbf', SVC()))\nmodels.append(('SVM_linear', SVC(kernel='linear')))","execution_count":15,"outputs":[]},{"metadata":{"_uuid":"e28b6aae51e9fe68097aba19a2c8a857577399c2","_cell_guid":"7f7752a1-2bc9-442f-ad61-68bed0ec6426","trusted":true},"cell_type":"code","source":"# Evaluate each model in turn\ntrain_results = []\ntest_results = []\nnames = []\nfor name, model in models:\n    cv_train_results = cross_val_score(model, X_train, y_train, \n                                       cv=10, scoring='accuracy')\n    train_results.append(cv_train_results)\n    clf = model.fit(X_train, y_train)\n    cv_test_results = accuracy_score(y_test, clf.predict(X_test))\n    test_results.append(cv_test_results)\n    names.append(name)\n    result = \"%s: %f (%f) %f\" % (name, cv_train_results.mean(), cv_train_results.std(), \n                                cv_test_results)\n    print(result)","execution_count":16,"outputs":[]},{"metadata":{"_uuid":"6c9305d2127aa126d45c70979cce285aa645206a","_cell_guid":"7766d69f-f205-fa08-31b7-4d95df939a93"},"cell_type":"markdown","source":"Random Forest Classifier has the highest training & test accuracy of about 80% & 82%, followed by Decision Trees with training & test accuracy of about 76% & 78% accuracy. \n\nLet's create a plot of the model evaluation results and compare the spread and the mean accuracy of each model. This is a population of accuracy measures for each algorithm as each algorithm was evaluated 5 times (5-fold cross validation)."},{"metadata":{"_uuid":"5d56e252bf62184eaf9138e59dc089c526afce81","_cell_guid":"438b90cc-f751-7ff9-64e2-f6267c369bfb","trusted":true},"cell_type":"code","source":"fig = plt.figure()\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(111)\nplt.boxplot(train_results)\nax.set_xticklabels(names)\nplt.show()","execution_count":17,"outputs":[]},{"metadata":{"_uuid":"80ca9471d83054e704e0d2b409b8342fb514e5cf","_cell_guid":"cd63c13c-6f47-afaf-f4b3-075695658f47"},"cell_type":"markdown","source":"Random Forest has the highest cv score of about 80%."},{"metadata":{"_uuid":"4a803b06aae5089b5fa744d213ce3a5f26d5033b","_cell_guid":"81474488-d8af-ce3f-fbcf-e71d2bc0a5b6"},"cell_type":"markdown","source":"### Feature Importances"},{"metadata":{"_uuid":"7af50bf063788ed40ac566e78eab4594d1f794ec","_cell_guid":"01678235-1a16-4452-9036-26d9f3fc0287"},"cell_type":"markdown","source":"I will now focus on the Random Forest Classifier, tune its hyperparameters and see if the accuracy can be improved. But first, let's investigate the feature importance in this model."},{"metadata":{"_uuid":"1e503b7d2d333008f00bafe759853a40e360fe99","_cell_guid":"32fbeefa-334a-415f-8cd4-d7e4a7c5dd3a","trusted":true},"cell_type":"code","source":"RF = RandomForestClassifier(random_state=seed)\nRF.fit(X_train, y_train)","execution_count":18,"outputs":[]},{"metadata":{"_uuid":"4c34d0c917e13f7754015854f594cfb599ad1732","_cell_guid":"a3901c75-6051-455b-a326-ffebe24ffb9a","trusted":true},"cell_type":"code","source":"names = list(X_train.columns.values)\nimportances = RF.feature_importances_\n# Plot the feature importances of the forest\nplt.figure(figsize=(10,5))\nplt.title(\"Feature Importances\")\ny_pos = np.arange(len(names))\nplt.bar(y_pos, importances, align='center')\nplt.xticks(y_pos, names, rotation=90)\nplt.show()","execution_count":19,"outputs":[]},{"metadata":{"_uuid":"5f1570df40d231ec132f32586bdc5a17d1257e86","_cell_guid":"46fd1b66-9241-4957-abc9-3ed2047df633"},"cell_type":"markdown","source":"The features which contribute more to the quality of the wine include alcohol (highest) followed by volatile acidity and density. This is similar to what we observed earlier on."},{"metadata":{"_uuid":"39c7c6e098526b39fdab198494839df16b6816ea","_cell_guid":"569bf72f-4382-bd74-f9ba-537953e2befb"},"cell_type":"markdown","source":"## Declare hyperparameters to tune\n\nWithin each decision tree, the algorithm can empirically decide where to create branches based on the metrics that it is optimising on. Therefore the actual branch locations and where to split are model parameters. However, the algorithm doesn't decide how many trees to include in the forest, what is the max_depth for the trees etc. These are hyperparameters that the user must set & tune. For this purpose, GridSearchCV can be used."},{"metadata":{"_uuid":"fc8f7696cdb9a05bcf1e7c9e96a9e45df31f59ba","_cell_guid":"45f909d9-b544-4b8a-ad81-38bc298408e0"},"cell_type":"markdown","source":"## Tune model using GridSearchCV"},{"metadata":{"_uuid":"a3f5cdea61c0e74c3756dd2153373eda819464fa","_cell_guid":"a0922060-6620-4bb8-b3de-41bf7f1b6f13","trusted":true},"cell_type":"code","source":"clf = RandomForestClassifier()\ngrid_values = {'max_features':['auto','sqrt','log2'],'max_depth':[None, 10, 5, 3, 1],\n              'min_samples_leaf':[1, 5, 10, 20, 50]}\nclf","execution_count":20,"outputs":[]},{"metadata":{"_uuid":"f320fdd257b8eab1e8aebafff6f64bbb3964f909","_cell_guid":"dd978a85-bdce-ad16-62ec-f0145ac1e9fe","trusted":true},"cell_type":"code","source":"grid_clf = GridSearchCV(clf, param_grid=grid_values, cv=10, scoring='accuracy')\ngrid_clf.fit(X_train, y_train) # fit and tune model","execution_count":21,"outputs":[]},{"metadata":{"_uuid":"fb4260d32b9bb68aa7db955338710afa64f6d724","_cell_guid":"8b382c0e-5c02-a0fe-09ab-b667d51c130b","trusted":true},"cell_type":"code","source":"grid_clf.best_params_","execution_count":22,"outputs":[]},{"metadata":{"_uuid":"502fad00ad8a6cf36eb8a9a815c1772f2136ccea","_cell_guid":"d8da0527-f19c-4860-8927-14519da82a29"},"cell_type":"markdown","source":"#### It looks like mostly the default hyperparameters are recommended."},{"metadata":{"collapsed":true,"_uuid":"a7aef0087ed15ceb01cf1f6c7176149f32b31efb","_cell_guid":"a5b0f144-2999-aa23-305f-3ea6be81e242","trusted":true},"cell_type":"code","source":"clf = RandomForestClassifier().fit(X_train, y_train)","execution_count":23,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"9e4e25b431ee6846d25f255006c55c1e18d2e953","_cell_guid":"79f5927c-1b1c-787f-8738-19d901db7cbb"},"cell_type":"markdown","source":"## Evaluate model accuracy on test data"},{"metadata":{"collapsed":true,"_uuid":"866ec0b75f6b26595b5b94a0d78690d4c2fda8fd","_cell_guid":"6f1c4cb2-944e-4b8f-82a7-216ad38d8e59","trusted":true},"cell_type":"code","source":"y_pred = clf.predict(X_test)","execution_count":24,"outputs":[]},{"metadata":{"_uuid":"1ae2a0f0ef35901b46c63e7da3264179d218bbf2","_cell_guid":"822e8e39-a105-4a60-a109-9eafc7a03a83"},"cell_type":"markdown","source":"### Accuracy and confusion matrix"},{"metadata":{"_uuid":"04704cb3447719cdf9508d5b0baf7820714fc4ab","_cell_guid":"da453a4a-eb85-4881-a096-ea88e555cdad","trusted":true},"cell_type":"code","source":"print('Training Accuracy :: ', accuracy_score(y_train, clf.predict(X_train)))\nprint('Test Accuracy :: ', accuracy_score(y_test, y_pred))","execution_count":25,"outputs":[]},{"metadata":{"_uuid":"3f939b01d8c2f8287709879031cf503b678e7839","_cell_guid":"a84fd158-5a00-47f8-a30a-f0bd107d76e6","trusted":true},"cell_type":"code","source":"print(confusion_matrix(y_test, y_pred))","execution_count":26,"outputs":[]},{"metadata":{"_uuid":"4f6c04ea3962aa587752488e5f264b5f52c7d368","_cell_guid":"83deafa2-114c-8bda-daf9-1dd2c0003946"},"cell_type":"markdown","source":"# End notes\nTips and comments are welcomed. Thank you in advance.\n\nRhodium Beng...... \n"},{"metadata":{"collapsed":true,"_uuid":"9165a1cd66f9f2ae9422a2ad050b92f4b17ed964","_cell_guid":"b9dfd1c7-64ca-4d8a-b1a3-7388d647b2e7","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"_change_revision":0,"_is_fork":false,"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}