{"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3","language":"python"},"language_info":{"version":"3.6.3","nbconvert_exporter":"python","file_extension":".py","pygments_lexer":"ipython3","mimetype":"text/x-python","name":"python","codemirror_mode":{"version":3,"name":"ipython"}}},"nbformat_minor":1,"cells":[{"cell_type":"code","metadata":{"_uuid":"79475dd1bfc865e4f12828265acbb2472ba11012","_cell_guid":"ae210268-5760-4b77-aed0-e66fe567b899"},"execution_count":null,"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.","outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"_uuid":"f5135a77cc043e332e404114e994802e9d144917","_cell_guid":"247a4f7e-1a2a-4d8c-adfb-00a5e7f56293"},"execution_count":null,"source":"data = pd.read_csv(\"../input/mushrooms.csv\")","outputs":[]},{"cell_type":"code","metadata":{},"execution_count":null,"source":"from sklearn.preprocessing import LabelEncoder\nlabelencoder=LabelEncoder()\nfor col in data.columns:\n    data[col] = labelencoder.fit_transform(data[col])\n \ndata.head()","outputs":[]},{"cell_type":"code","metadata":{"_uuid":"913da4c6c82e4209eb92f23c6e716c55cb6910d1","_cell_guid":"efeb524f-b10b-4c40-a66f-3e9d84260864"},"execution_count":null,"source":"print(data.shape)\ndata =np.array(data)\n#print(data.head)\n_length = 8000\ntrain, test = data[0:_length,], data[_length:,]\nXtrain, ytrain = train[:,1:],train[:,0]\nXtest, ytest = test[:,1:], test[:,0]\nprint(Xtrain.shape)\nprint(ytrain .shape)\nprint(Xtest.shape)\nprint(ytest.shape)\n","outputs":[]},{"cell_type":"code","metadata":{"_uuid":"d74fa867b9e49708ab3b4d7f31e1a1c2f8901f14","_cell_guid":"3f211513-bb0e-4179-9358-92754f281689"},"execution_count":null,"source":"#Import Library\nfrom sklearn import svm\n#model = svm.SVC(kernel='linear', c=1, gamma=1) \nmodel = svm.SVC(kernel='linear', gamma=1) \n# there is various option associated with it, like changing kernel, gamma and C value.\nmodel.fit(Xtrain, ytrain)\nscr = model.score(Xtrain, ytrain)\nprint(scr)\n#Predict Output\npredicted= model.predict(Xtest)\nprint(predicted)\n\ndif = predicted - ytest\nprint(dif)\n\n","outputs":[]},{"cell_type":"code","metadata":{},"execution_count":null,"source":"from keras.models import *\nfrom keras.layers import *\n\nbatch_size = 1\nmlp_neurons = 5\nneurons = 5\nbi_neurons = 5\nrepeats = 5\nnb_epochs = 5\n","outputs":[]},{"cell_type":"code","metadata":{"collapsed":true},"execution_count":null,"source":"## mlp\ndef mlp_model(train, batch_size, nb_epoch, neurons):\n    X,y = train[:,1:], train[:,0]\n    #X = X.reshape(X.shape[0],1,X.reshape[1])\n    model = Sequential()\n    model.add(Dense(neurons,input_dim = X.shape[1],init = 'normal', activation = 'relu'))\n    model.add(Dropout(0.2))\n    model.add(Dense(neurons, init = 'normal', activation = 'relu'))\n    model.add(Dropout(0.2))\n    model.add(Dense(neurons, init = 'normal', activation = 'relu'))\n    model.add(Dense(1, activation = 'linear'))\n    model.compile(loss = 'mse', optimizer = 'adam', metrics = ['acc'])\n    for i in range(nb_epoch):\n        model.fit(X,y, epochs = 1, batch_size = batch_size, verbose = 2, shuffle = False)\n        model.reset_states()\n    return model","outputs":[]},{"cell_type":"code","metadata":{"collapsed":true},"execution_count":null,"source":"def forecast_mlp(model, batch_size, row):\n    X = row\n    X = X.reshape(1,len(X))\n    yhat = model.predict(X, batch_size = batch_size)\n    #return yhat[0,0]\n    return yhat","outputs":[]},{"cell_type":"code","metadata":{},"execution_count":null,"source":"mlp_RNN = mlp_model(train, batch_size, nb_epochs, mlp_neurons)","outputs":[]},{"cell_type":"code","metadata":{"collapsed":true},"execution_count":null,"source":"def simulated_mlp(model, train, batch_size, nb_epochs, neurons):\n    n1 = len(Xtest)\n    n2 = repeats\n    predictions1 = np.zeros((n1,n2), dtype = float)\n    for r in range(repeats):\n        predictions2 = list()\n        for i in range(len(Xtest)):\n            if(i == 0):\n               y = forecast_mlp(model, batch_size, Xtest[i,:])\n               Xtest[i+1,:-1] = Xtest[i,1:]\n               Xtest[i+1,-1] = y\n               predictions2.append(y)\n            else:\n               y = forecast_mlp(model, batch_size, Xtest[i-1,:])\n               Xtest[i,:-1] = Xtest[i-1,1:]\n               Xtest[i,-1] = y\n               predictions2.append(y)\n        predictions1[:,r] = predictions2\n    return np.mean(predictions1, axis = 1)","outputs":[]},{"cell_type":"code","metadata":{},"execution_count":null,"source":"print(ytest)\nprint(\"===================== mlp ==================================\")\nprint(simulated_mlp(mlp_RNN, train, batch_size, nb_epochs, mlp_neurons))","outputs":[]}],"nbformat":4}