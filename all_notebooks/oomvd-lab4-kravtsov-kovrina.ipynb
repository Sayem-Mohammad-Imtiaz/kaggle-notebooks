{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\ndataset = pd.read_csv(\"../input/reddit-vaccine-myths/reddit_vm.csv\")\ndataset","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom subprocess import check_output\nfrom wordcloud import WordCloud, STOPWORDS\n\n\nmpl.rcParams['font.size']=12               \nmpl.rcParams['savefig.dpi']=100            \nmpl.rcParams['figure.subplot.bottom']=.1 \n\n\nstopwords = set(STOPWORDS)\ndata = pd.read_csv(\"../input/reddit-vaccine-myths/reddit_vm.csv\")\n\nwordcloud = WordCloud(\n                          background_color='white',\n                          stopwords=stopwords,\n                          max_words=200,\n                          max_font_size=40, \n                          random_state=42\n                         ).generate(str(data['title']))\n\nprint(wordcloud)\nfig = plt.figure(1)\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.show()\nfig.savefig(\"word1.png\", dpi=900)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\npip install --upgrade --force-reinstall nltk==3.4 ##pip install --upgrade --force-reinstall nltk==3.2.4","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pkg_resources\npkg_resources.require(\"nltk==3.2.4\")\nimport nltk \n\nfrom nltk.util import pad_sequence\nfrom nltk.util import bigrams\nfrom nltk.util import ngrams\nfrom nltk.util import everygrams\nfrom nltk.lm.preprocessing import pad_both_ends\nfrom nltk.lm.preprocessing import flatten\nfrom nltk import word_tokenize, sent_tokenize \nfrom nltk.lm.preprocessing import padded_everygram_pipeline\nfrom nltk.lm import MLE\n\nheadlines = list(dataset['title'].apply(nltk.word_tokenize))\nn = 3\ntrain_data, padded_sents = padded_everygram_pipeline(n, headlines)\nheadlines_model = MLE(n)\nheadlines_model.fit(train_data, padded_sents)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.tokenize.treebank import TreebankWordDetokenizer\n\ndetokenize = TreebankWordDetokenizer().detokenize\n\ndef generate_sentense(model, num_words, random_seed=42):\n    content = []\n    for token in model.generate(num_words, random_seed=random_seed):\n        if token == '<s>':\n            continue\n        if token == '</s>':\n            break\n        content.append(token)\n    return detokenize(content)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"generate_sentense(headlines_model, 30, random_seed=69420)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}