{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Importing the necessary libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom keras import Model\nfrom keras.applications.mobilenet import MobileNet, preprocess_input\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, Callback\nfrom keras.layers import Conv2D, Reshape\nfrom keras.utils import Sequence\nfrom keras.backend import epsilon\nimport tensorflow as tf\n\nfrom PIL import Image\n\nimport os\nimport random\n\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nfrom PIL import Image\nimport numpy as np\nimport cv2\n\nnp.random.seed(1)","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","execution":{"iopub.status.busy":"2021-07-24T22:43:19.651125Z","iopub.execute_input":"2021-07-24T22:43:19.6515Z","iopub.status.idle":"2021-07-24T22:43:24.925645Z","shell.execute_reply.started":"2021-07-24T22:43:19.651445Z","shell.execute_reply":"2021-07-24T22:43:24.92479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Reading the training data from train.csv file","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv(\"../input/racoon-detection/train_labels_.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-07-24T22:43:24.92746Z","iopub.execute_input":"2021-07-24T22:43:24.927865Z","iopub.status.idle":"2021-07-24T22:43:24.953191Z","shell.execute_reply.started":"2021-07-24T22:43:24.927833Z","shell.execute_reply":"2021-07-24T22:43:24.952142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-24T22:43:24.954711Z","iopub.execute_input":"2021-07-24T22:43:24.955107Z","iopub.status.idle":"2021-07-24T22:43:24.976384Z","shell.execute_reply.started":"2021-07-24T22:43:24.955066Z","shell.execute_reply":"2021-07-24T22:43:24.975401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.shape","metadata":{"execution":{"iopub.status.busy":"2021-07-24T22:43:24.977944Z","iopub.execute_input":"2021-07-24T22:43:24.97828Z","iopub.status.idle":"2021-07-24T22:43:24.983845Z","shell.execute_reply.started":"2021-07-24T22:43:24.978244Z","shell.execute_reply":"2021-07-24T22:43:24.982936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Create a list variable known as 'path' which has all the path for all the training images\n* Create an array 'coords' which has the resized coordinates of the bounding box for the training images","metadata":{}},{"cell_type":"code","source":"IMAGE_SIZE = 128","metadata":{"execution":{"iopub.status.busy":"2021-07-24T22:43:24.987893Z","iopub.execute_input":"2021-07-24T22:43:24.988328Z","iopub.status.idle":"2021-07-24T22:43:24.992359Z","shell.execute_reply.started":"2021-07-24T22:43:24.988283Z","shell.execute_reply":"2021-07-24T22:43:24.991398Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"coords=train[[\"width\",\"height\",\"xmin\",\"ymin\",\"xmax\",\"ymax\"]]\n\ncoords[\"xmin\"] = coords[\"xmin\"] *IMAGE_SIZE/coords[\"width\"]\ncoords[\"xmax\"] = coords[\"xmax\"]*IMAGE_SIZE /coords[\"width\"]\ncoords[\"ymin\"] = coords[\"ymin\"] *IMAGE_SIZE/coords[\"height\"]\ncoords[\"ymax\"] = coords[\"ymax\"] *IMAGE_SIZE/coords[\"height\"]\n\ncoords.drop([\"width\",\"height\"],axis =1,inplace=True)\ncoords.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-24T22:43:24.994766Z","iopub.execute_input":"2021-07-24T22:43:24.995252Z","iopub.status.idle":"2021-07-24T22:43:25.042194Z","shell.execute_reply.started":"2021-07-24T22:43:24.995212Z","shell.execute_reply":"2021-07-24T22:43:25.04116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"paths = train[\"filename\"]\nlen(paths)","metadata":{"execution":{"iopub.status.busy":"2021-07-24T22:43:25.043758Z","iopub.execute_input":"2021-07-24T22:43:25.044173Z","iopub.status.idle":"2021-07-24T22:43:25.052784Z","shell.execute_reply.started":"2021-07-24T22:43:25.04413Z","shell.execute_reply":"2021-07-24T22:43:25.050421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"images = \"../input/racoon-detection/Racoon Images/images/\"\n\nbatch_images = np.zeros((len(paths), IMAGE_SIZE, IMAGE_SIZE,3), dtype=np.float32)\n\nfor i, f in enumerate(paths):\n  #print(f)\n  img = Image.open(images+f)\n  img = img.resize((IMAGE_SIZE, IMAGE_SIZE))\n  img = img.convert('RGB')\n  batch_images[i] = preprocess_input(np.array(img, dtype=np.float32))","metadata":{"execution":{"iopub.status.busy":"2021-07-24T22:43:25.054706Z","iopub.execute_input":"2021-07-24T22:43:25.055219Z","iopub.status.idle":"2021-07-24T22:43:27.686749Z","shell.execute_reply.started":"2021-07-24T22:43:25.055138Z","shell.execute_reply":"2021-07-24T22:43:27.685848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Building\n* Building the model using transfer learning","metadata":{}},{"cell_type":"code","source":"ALPHA = 1.0\n\nmodel = MobileNet(input_shape=(IMAGE_SIZE,IMAGE_SIZE,3), include_top=False, alpha=ALPHA)","metadata":{"execution":{"iopub.status.busy":"2021-07-24T22:43:27.690198Z","iopub.execute_input":"2021-07-24T22:43:27.690483Z","iopub.status.idle":"2021-07-24T22:43:31.074708Z","shell.execute_reply.started":"2021-07-24T22:43:27.690439Z","shell.execute_reply":"2021-07-24T22:43:31.073875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for layers in model.layers:\n  layers.trainable = False\n\nx = model.layers[-1].output\nx = Conv2D(4, kernel_size = 4, name=\"coords\")(x)\nx = Reshape((4,))(x)\n\nmodel = Model(inputs = model.inputs, outputs = x)\n","metadata":{"execution":{"iopub.status.busy":"2021-07-24T22:43:31.076796Z","iopub.execute_input":"2021-07-24T22:43:31.077053Z","iopub.status.idle":"2021-07-24T22:43:31.1154Z","shell.execute_reply.started":"2021-07-24T22:43:31.077028Z","shell.execute_reply":"2021-07-24T22:43:31.114767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2021-07-24T23:06:13.963625Z","iopub.execute_input":"2021-07-24T23:06:13.963938Z","iopub.status.idle":"2021-07-24T23:06:13.998412Z","shell.execute_reply.started":"2021-07-24T23:06:13.963907Z","shell.execute_reply":"2021-07-24T23:06:13.996514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Define a custom loss function IoU which calculates Intersection Over Union","metadata":{}},{"cell_type":"code","source":"def loss(gt,pred):\n    intersections = 0\n    unions = 0\n    diff_width = np.minimum(gt[:,0] + gt[:,2], pred[:,0] + pred[:,2]) - np.maximum(gt[:,0], pred[:,0])\n    diff_height = np.minimum(gt[:,1] + gt[:,3], pred[:,1] + pred[:,3]) - np.maximum(gt[:,1], pred[:,1])\n    intersection = diff_width * diff_height\n    \n    # Compute union\n    area_gt = gt[:,2] * gt[:,3]\n    area_pred = pred[:,2] * pred[:,3]\n    union = area_gt + area_pred - intersection\n\n#     Compute intersection and union over multiple boxes\n    for j, _ in enumerate(union):\n        if union[j] > 0 and intersection[j] > 0 and union[j] >= intersection[j]:\n            intersections += intersection[j]\n            unions += union[j]\n\n    # Compute IOU. Use epsilon to prevent division by zero\n    iou = np.round(intersections / (unions + epsilon()), 4)\n    iou = iou.astype(np.float32)\n    return iou\n\ndef IoU(y_true, y_pred):\n    iou = tf.py_function(loss, [y_true, y_pred], tf.float32)\n    return iou","metadata":{"execution":{"iopub.status.busy":"2021-07-24T22:43:31.117501Z","iopub.execute_input":"2021-07-24T22:43:31.118037Z","iopub.status.idle":"2021-07-24T22:43:31.129987Z","shell.execute_reply.started":"2021-07-24T22:43:31.118Z","shell.execute_reply":"2021-07-24T22:43:31.129044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Compiling the model","metadata":{}},{"cell_type":"code","source":"gt = coords\n\nPATIENCE=10\n\nmodel.compile(optimizer = \"Adam\", loss = \"mse\", metrics = [IoU])\n\nstop = EarlyStopping(monitor='val_iou', patience=PATIENCE, mode=\"max\" )\n\nreduce_lr = ReduceLROnPlateau(monitor='val_iou',factor=0.2,patience=PATIENCE, min_lr=1e-7, verbose=1, mode=\"max\" )\n\nmodel.fit(batch_images, gt, epochs=100,callbacks=[stop,reduce_lr], verbose = 2)","metadata":{"execution":{"iopub.status.busy":"2021-07-24T22:43:31.131267Z","iopub.execute_input":"2021-07-24T22:43:31.131675Z","iopub.status.idle":"2021-07-24T22:44:07.438753Z","shell.execute_reply.started":"2021-07-24T22:43:31.131637Z","shell.execute_reply":"2021-07-24T22:44:07.438003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Pick a test image from the given data","metadata":{}},{"cell_type":"code","source":"print(paths)\nprint(random.choice(paths))","metadata":{"execution":{"iopub.status.busy":"2021-07-24T22:53:20.221457Z","iopub.execute_input":"2021-07-24T22:53:20.221829Z","iopub.status.idle":"2021-07-24T22:53:20.230333Z","shell.execute_reply.started":"2021-07-24T22:53:20.221795Z","shell.execute_reply":"2021-07-24T22:53:20.229026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_img = random.choice(paths)","metadata":{"execution":{"iopub.status.busy":"2021-07-24T22:44:07.440287Z","iopub.execute_input":"2021-07-24T22:44:07.440667Z","iopub.status.idle":"2021-07-24T22:44:07.44557Z","shell.execute_reply.started":"2021-07-24T22:44:07.440628Z","shell.execute_reply":"2021-07-24T22:44:07.444673Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"filename = images+ test_img\nprint(filename)\nunscaled = cv2.imread(filename)\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\n\n# Create figure and axes\nfig,ax = plt.subplots(1)\n#ax.show(unscaled)\n#plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-24T23:03:57.158328Z","iopub.execute_input":"2021-07-24T23:03:57.158647Z","iopub.status.idle":"2021-07-24T23:03:57.270806Z","shell.execute_reply.started":"2021-07-24T23:03:57.158619Z","shell.execute_reply":"2021-07-24T23:03:57.269987Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preprocessing of Test Image\nResizing the image to 128 * 128 and preprocess the image for the MobileNet model","metadata":{}},{"cell_type":"code","source":"image_height, image_width, _ = unscaled.shape\nimage = cv2.resize(unscaled,(IMAGE_SIZE,IMAGE_SIZE))\nfeat_scaled = preprocess_input(np.array(image, dtype=np.float32))","metadata":{"execution":{"iopub.status.busy":"2021-07-24T22:44:07.478451Z","iopub.execute_input":"2021-07-24T22:44:07.47881Z","iopub.status.idle":"2021-07-24T22:44:07.487761Z","shell.execute_reply.started":"2021-07-24T22:44:07.478774Z","shell.execute_reply":"2021-07-24T22:44:07.486791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Predict the coordinates of the bounding box for the given test image","metadata":{}},{"cell_type":"code","source":"region = model.predict(x = np.array([feat_scaled]))[0]","metadata":{"execution":{"iopub.status.busy":"2021-07-24T22:44:07.489176Z","iopub.execute_input":"2021-07-24T22:44:07.48959Z","iopub.status.idle":"2021-07-24T22:44:07.975948Z","shell.execute_reply.started":"2021-07-24T22:44:07.489553Z","shell.execute_reply":"2021-07-24T22:44:07.975151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.models import model_from_json\nimport numpy\nmodel_json=model.to_json();\nwith open(\"model.json\",\"w\") as json_file:\n    json_file.write(model_json)\n    \nmodel.save_weights(\"model.h5\")    ","metadata":{"execution":{"iopub.status.busy":"2021-07-24T23:23:27.706451Z","iopub.execute_input":"2021-07-24T23:23:27.706775Z","iopub.status.idle":"2021-07-24T23:23:27.867605Z","shell.execute_reply.started":"2021-07-24T23:23:27.706744Z","shell.execute_reply":"2021-07-24T23:23:27.866842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Scaling the BBox","metadata":{}},{"cell_type":"code","source":"x0 = int(region[0] * image_width / IMAGE_SIZE) \ny0 = int(region[1] * image_height / IMAGE_SIZE)\n\nx1 = int((region[2]) * image_width / IMAGE_SIZE)\ny1 = int((region[3]) * image_height / IMAGE_SIZE)","metadata":{"execution":{"iopub.status.busy":"2021-07-24T22:44:07.97869Z","iopub.execute_input":"2021-07-24T22:44:07.979042Z","iopub.status.idle":"2021-07-24T22:44:07.985777Z","shell.execute_reply.started":"2021-07-24T22:44:07.979005Z","shell.execute_reply":"2021-07-24T22:44:07.984685Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Plotting the predicted bounding box","metadata":{}},{"cell_type":"code","source":"# Create figure and axes\nfig,ax = plt.subplots(1)\n\n# Display the image\nax.imshow(unscaled)\n\n# Create a Rectangle patch\nrect = patches.Rectangle((x0, y0), (x1 - x0) , (y1 - y0) , linewidth=2, edgecolor='r', facecolor='none')\n\n# Add the patch to the Axes\nax.add_patch(rect)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-24T22:44:07.987243Z","iopub.execute_input":"2021-07-24T22:44:07.987609Z","iopub.status.idle":"2021-07-24T22:44:08.203201Z","shell.execute_reply.started":"2021-07-24T22:44:07.987573Z","shell.execute_reply":"2021-07-24T22:44:08.202271Z"},"trusted":true},"execution_count":null,"outputs":[]}]}