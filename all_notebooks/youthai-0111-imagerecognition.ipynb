{"cells":[{"metadata":{},"cell_type":"markdown","source":"# 线性回归"},{"metadata":{},"cell_type":"markdown","source":"导入numpy、matplotlib库"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np # 导入numpy库\nimport matplotlib.pyplot as plt # 导入matplotlib库","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"生成需要拟合的线性数据"},{"metadata":{"trusted":true},"cell_type":"code","source":"x = np.array([1, 2, 3])\ny = 2 * x + 1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"第一步：确定模型，定义一个一次函数(即只有一个参数的线性函数)作为预测函数\n\n线性模型 y = wx + b\n\n需要学习的参数是 w 和 b，初始化都为 0"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(x, y, 'x-')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"定义预测函数"},{"metadata":{"trusted":true},"cell_type":"code","source":"w = 0\nb = 0\ndef predict(x):\n    return w * x + b","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"第二步：定义损失函数，衡量预测值与真实值之间的误差\n\n求该线性回归模型的损失函数，这里使用均方误差除以2，即 $\\frac{1}{2N}\\sum_{i=1}^N{(y\\_predict-y)^2}$"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_predict = predict(x)\n\ndef MSE(y_predict, y):\n    return 0.5 * np.mean((y_predict - y) ** 2)# 提示：np.mean的作用就是求平均，这里是把所有的平方差加起来求平均","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"输出当前 w 和 b 下损失函数的值，并尝试不同的 w 和 b"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"MSE(predict(x), y)  # 输出一下当前w和b下误差是多少，尝试不同的 w和 b ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"loss = $\\frac{1}{2N}(\\boldsymbol{y}-\\boldsymbol{y\\_{predict})^\\top} (\\boldsymbol{y}-\\boldsymbol{y\\_{predict}})=\\frac{1}{2N}(\\boldsymbol{y}-(w\\boldsymbol{x}+b))^\\top (\\boldsymbol{y}-(w\\boldsymbol{x}+b))\\quad\\quad$ //向量形式\n\nloss =$\\frac{1}{2N}\\sum_{i=1}^N{(y-y\\_predict)^2}=\\frac{1}{2N}\\sum_{i=1}^N{(y-(wx+b))^2}\\quad\\quad $    //标量形式\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"w = 0\nb = 0\nlearning_rate = .01               # 学习率\nloss_list = []                    # 保存训练过程中每次更新后的loss\nfor i in range(50):\n    d_w = (y - predict(x)) @ -x   # w的梯度\n    d_b = np.sum(predict(x) - y)  # b的梯度\n    w -= learning_rate * d_w      # 更新w(为了降低loss则减去梯度，增加loss则加上梯度)\n    b -= learning_rate * d_b      # 更新b\n    loss = MSE(predict(x), y)     # 求出新的loss\n    loss_list.append(loss)        # 记录loss，以便后面可视化\n    print(w, b, loss)             # 输出更新后的w,b以及loss","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"可视化loss每一步变化"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(loss_list)                ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 梯度下降"},{"metadata":{"trusted":true},"cell_type":"code","source":"def f(x):                    # 目标(损失)函数\n    return x ** 2\n\ndef d(x):                    # 梯度 d(x²) = 2x\n    return 2 * x\n\nx = np.linspace(-5, 5, 100)     # 画出损失函数，在[-5，5]上取100个点, 用numpy函数\ny = f(x)\nplt.plot(x, y)\nplt.title(\"loss\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"可视化梯度下降过程"},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.display import display, clear_output\nx_start = -3              # 初始化参数\nlearning_rate = 0.1       # 学习率 1.1发散原理讲解\nstep = 10                 # 迭代步数\nfor i in range(step):\n    x_start = x_start - learning_rate * d(x_start)         # 用梯度更新参数\n    plt.title(\"x: %.4f, y: %.4f\" % (x_start, f(x_start)))  # 标题 x, y保留四位小数\n    plt.plot(x, y)                                         # 画出损失函数\n    plt.plot(x_start, f(x_start), 'ro')                    # 画出当前的参数值以及对应的损失函数 \n    plt.show()\n    clear_output(wait=True)                                # 等到下一张图来了再删除上一张\n    plt.pause(0.5)                                         # 每张图停留0.5s","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"定义损失函数并画出"},{"metadata":{"trusted":true},"cell_type":"code","source":"def f(x):                    # 目标(损失)函数               \n    return x ** 4 + x ** 3 - 20 * x ** 2 + x + 1\n\ndef d(x):                    # 梯度 d(x) \n    return 4 * x ** 3 + 3 * x ** 2 - 40 * x + 1\n\nx = np.linspace(-5, 5, 100)  # 画出损失函数                \ny = f(x)\nplt.plot(x, y)\nplt.title(\"loss\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"可视化梯度下降过程"},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.display import display, clear_output   # 尝试不同学习率的结果            \nx_start = -2             # 初始化参数 尝试 1, -2 \nlearning_rate = 0.01      # 学习率 0.01\nstep = 10                 # 迭代步数\nfor i in range(step):\n    x_start = x_start - learning_rate * d(x_start)         # 用梯度更新参数\n    plt.title(\"x: %.4f, y: %.4f\" % (x_start, f(x_start)))  # 标题 x, y保留四位小数\n    plt.plot(x, y)                                         # 画出损失函数\n    plt.plot(x_start, f(x_start), 'ro')                    # 画出当前的参数值以及对应的损失函数 \n    plt.show()\n    clear_output(wait=True)                                # 等到下一张图来了再删除上一张\n    plt.pause(0.5)  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Keras 搭建 MLP(多层感知机/神经网络)"},{"metadata":{},"cell_type":"markdown","source":"首先我们导入一些必要的库，我们主要会用到keras库。\n- Sequential: keras线性模型框架，可以理解为积木的模板\n- 神经网络中的一些常用层\n    - Dense: 全连接层\n    - Activation：激活层\n    - Flatten: 平铺层（二维转一维）\n    - Dropout：随机失活层\n- 优化器optimizers：优化网络参数"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"from keras.models import Sequential                               # 序列模型，线性逐层叠加\nfrom keras.layers import Dense, Activation, Flatten, Dropout      # 导入全连接层、激活函数层、二维转一维、Dropout等神经网络常用层\nfrom keras.optimizers import SGD                                  # 导入随机梯度下降优化器\n\nimport numpy as np # 导入numpy库\nimport matplotlib.pyplot as plt # 导入matplotlib库","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"模型构建"},{"metadata":{"trusted":true},"cell_type":"code","source":"x = np.array([\n    [0, 0],\n    [0, 1],\n    [1, 0],\n    [1, 1]\n])\ny = np.array([1, 0, 0, 1])\n\nmodel = Sequential()                  # 创建一个序列模型的对象          \nmodel.add(Dense(2))                   # 添加一个全连接层，2->2\nmodel.add(Activation('sigmoid'))      # 添加一个sigmoid的激活函数\nmodel.add(Dense(1))                   # 继续添加一个全连接层, 2->1\nmodel.add(Activation('sigmoid'))      # 再添加一个sigmoid的激活函数","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"编译模型"},{"metadata":{},"cell_type":"markdown","source":"搭建完卷积神经网络后，我们定义一个优化器，用来找到使损失函数最小的权重，这里我们使用SGD优化器。\n最后我们使用二分类的交叉熵作为损失函数，使用准确率作为度量指标，并完成模型的搭建。"},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(optimizer = SGD(lr = 1),      # 选用sgd的优化器，学习率设为1\n              loss = 'binary_crossentropy', # 损失(目标)函数采用二分类交叉熵损失函数\n              metrics = ['accuracy'])       # 用精度作为性能评价指标","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"模型训练"},{"metadata":{},"cell_type":"markdown","source":"keras提供了fit函数来进行训练，将训练的输入与输出x,y传给fit函数，指定训练轮数为500轮。"},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(x, y, epochs=1000)    # 模型训练，history记录了训练过程中的一些中间信息","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"模型训练过程可视化"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(history.history[\"loss\"])       # 画出训练过程中每一步精度的变化\nplt.plot(history.history[\"accuracy\"])        # 画出训练过程中每一步精度的变化","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"输出模型每层权重"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"for layer in model.layers:        # 遍历模型的所有层\n    print(layer.get_weights())    # 输出每层的权重(即需要训练的参数)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 用 MLP 识别 MNIST 数据集"},{"metadata":{},"cell_type":"markdown","source":"数据读取 (*这里数据读取时，需要注意路径名是否正确，当添加多个数据集时，input会为每一个数据集建一个文件夹*)"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd                                                  # 导入pandas库用于读取数据\nimport numpy as np  # 导入numpy库\n# mnist_dir = \"youthai/data/mnist-in-csv/\"\nmnist_dir = \"../input/youthaiimageclassification/\"\nmnist_train = pd.read_csv(mnist_dir + \"mnist_train.csv\")   #数据读取，当你的input只有mnist数据集时，目录应改为‘../input/mnist_train.csv’\nmnist_test = pd.read_csv(mnist_dir + \"mnist_test.csv\")     #具体路径最好通过 os.listdir('../input/') 看下\n# mnist_train = pd.read_csv(\"../input/mnist-in-csv/mnist_train.csv\")   # 数据读取，当你的input只有mnist数据集时，目录应改为‘../input/mnist_train.csv’\n# mnist_test = pd.read_csv(\"../input/mnist-in-csv/mnist_test.csv\")     # 具体路径最好通过 os.listdir('../input/') 看下\nx_train = np.array(mnist_train.iloc[:, 1:]).reshape(-1, 28, 28)      # 去除训练数据第一列的标签数据，并将数据reshape成 N×h×w\ny_train = np.array(mnist_train.iloc[:, 0])                           # 提取训练数据标签\nx_test = np.array(mnist_test.iloc[:, 1:]).reshape(-1, 28, 28)        # 去除测试数据第一列的标签数据，并将数据reshape成 N×h×w\ny_test = np.array(mnist_test.iloc[:, 0])                             # 提取测试数据标签\n\nnum_classes = 10    # 识别手写数字这个问题中一共有0~9个数字，共10类","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"从x_train和y_train的形状可以看到训练数据中包含了60000个数据点，其中输入x是60000张28x28=784像素组成的图像，由于MNIST数据集是灰度图，所以每个像素仅由一个数字表示；输出y是60000个数字，代表了每一张图像对应的数字几。测试数据x_test和y_test中则包含了10000个数据点。"},{"metadata":{},"cell_type":"markdown","source":"数据预处理"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential                               # 序列模型，线性逐层叠加\nfrom keras.layers import Dense, Activation, Flatten, Dropout      # 导入全连接层、激活函数层、二维转一维、Dropout等神经网络常用层\nfrom keras.optimizers import SGD                                  # 导入随机梯度下降优化器\nimport matplotlib.pyplot as plt # 导入matplotlib库\nfrom keras.utils import to_categorical\n\nx_train = x_train / 255        # 数据归一化\nx_test = x_test / 255          \ny_train = to_categorical(y_train, num_classes)  # 将标签变成独热编码，方便后面的交叉熵计算\ny_test = to_categorical(y_test, num_classes)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"原本的输入数据中每个像素是0-255的整数，但是对于神经网络的输入，我们一般希望将输入转化到0-1左右的较小的数字，所以我们将输入数据除以255。另外对于输出数据，我们不再简单的用一个数字来表示。对于多分类问题，我们往往采用独热编码作为输出。keras提供了一个方便的函数to_categorical来完成这个变换。"},{"metadata":{},"cell_type":"markdown","source":"模型构建"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()                           # 创建一个序列模型的对象\nmodel.add(Flatten(input_shape = (28, 28)))     # 对于每个数据(这里指图片)全连接输入必输为一个向量，因此使用Flatten层起到了reshape的作用\nmodel.add(Dense(20, activation = 'relu'))      # 加上全连接层 784->20\nmodel.add(Dense(20, activation = 'relu'))      # 加上全连接层 20->20\nmodel.add(Dense(num_classes, activation = 'softmax')) # 添加全连接层，然后加上softmax的激活函数","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()                           # 创建一个序列模型的对象\nmodel.add(Flatten(input_shape = (28, 28)))     # 对于每个数据(这里指图片)全连接输入必输为一个向量，因此使用Flatten层起到了reshape的作用\nmodel.add(Dense(512, activation = 'relu'))     # 加上全连接层 784->512\nmodel.add(Dropout(0.2))                        # dropout层在每一个batchsize训练中随机使网络中一些节点失效(0.2的概率)，可以起到解耦合，防止过拟合等一系列作用\nmodel.add(Dense(512, activation = 'relu'))     # 加上全连接层 512->512\nmodel.add(Dropout(0.2))                        # 添加dropout层\nmodel.add(Dense(num_classes, activation = 'softmax')) # 添加全连接层，然后加上softmax的激活函数","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"模型各层详细信息可视化"},{"metadata":{},"cell_type":"markdown","source":"keras提供了summary函数，方便查看模型每一层的结构，以及参数个数。"},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()  # 输出模型各层详细信息，可以看到各层参数状况","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"模型编译"},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(loss='categorical_crossentropy',   # 损失函数使用多类交叉熵损失函数\n              optimizer=\"rmsprop\",               # 优化器采用rmsprop\n              metrics=['accuracy'])              # 用精度作为性能评估指标","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"模型训练、评估"},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 32        # 每次输入32张图片,前向传播求出损失函数平均值，然后反向传播一次更新梯度\nepochs = 5             # 保证所有训练数据被输入网络五次\nhistory = model.fit(x_train, y_train, # 训练数据\n                    batch_size=batch_size,\n                    epochs=epochs,\n                    verbose=1,   # 越大，训练过程中显示的信息越详细  查bing\n                    validation_data=(x_test, y_test))  # 验证集\nscore = model.evaluate(x_test, y_test, verbose=0)      # 模型评估，返回模型的loss和metric\nprint('Test loss:', score[0])     # 测试集上模型损失\nprint('Test accuracy:', score[1]) # 测试集上模型精度","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"模型预测"},{"metadata":{"trusted":true},"cell_type":"code","source":"model.predict(x_test[0:1]) # 模型预测，输出预测的标签信息","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 用 MLP 识别 CIFAR-10 数据集"},{"metadata":{},"cell_type":"markdown","source":"加载CIFAR-10数据集"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pickle\nwith open(\"../input/youthaiimageclassification/cifar10.pkl\", \"rb\") as f:\n    (x_train, y_train), (x_test, y_test) = pickle.load(f)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"数据预处理"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential                               # 序列模型，线性逐层叠加\nfrom keras.layers import Dense, Activation, Flatten, Dropout      # 导入全连接层、激活函数层、二维转一维、Dropout等神经网络常用层\nfrom keras.optimizers import SGD                                  # 导入随机梯度下降优化器\nimport matplotlib.pyplot as plt # 导入matplotlib库\nfrom keras.utils import to_categorical\n\n# (x_train, y_train), (x_test, y_test) = cifar10.load_data()   # 用keras提供的api读取数据\nprint(x_train.shape)\nprint(y_train.shape)\nprint(x_test.shape)\nprint(y_test.shape)\nx_train = x_train / 255  # 数据归一化\nx_test = x_test / 255\nnum_classes = 10         # 数据一共有10类\ny_train = to_categorical(y_train, num_classes) # 将训练数据的标签独热编码\ny_test = to_categorical(y_test, num_classes)   # 将测试数据的标签独热编码","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"模型构建"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()                          # 创建序列模型的对象\nmodel.add(Flatten(input_shape=(32, 32, 3)))   # 讲解shape，用Flatten层将数据reshape成batchsize×（32*32*3）\nmodel.add(Dense(512, activation='relu'))      # 添加全连接层，使用relu作为激活函数3072->512\nmodel.add(Dense(512, activation='relu'))      # 添加全连接层，使用relu作为激活函数512->512\nmodel.add(Dense(num_classes, activation='softmax'))# 添加全连接层，激活函数为softmax 512->10\n\nmodel.compile(loss='categorical_crossentropy',  # 多类交叉熵损失函数\n              optimizer=\"rmsprop\",              # 优化器使用rmsprop\n              metrics=['accuracy'])             # 评估指标：精度","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"模型训练、评估"},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 32               # 每次输入32张图片,前向传播求出损失函数平均值，然后反向传播一次更新梯度\nepochs = 5                    # 保证所有训练数据被输入网络五次\nhistory = model.fit(x_train, y_train,                   # 训练数据\n                    batch_size=batch_size,\n                    epochs=epochs,\n                    verbose=1,                          # 越大，训练过程中显示的信息越详细             \n                    validation_data=(x_test, y_test))   # 验证集\nscore = model.evaluate(x_test, y_test, verbose=0)       # 模型评估，返回模型的loss和metric\nprint('Test loss:', score[0])                           # 测试集上模型损失\nprint('Test accuracy:', score[1])                       # 测试集上模型精度","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"model = Sequential()                          # 创建序列模型的对象\nmodel.add(Flatten(input_shape=(32, 32, 3)))   # 讲解shape，用Flatten层将数据reshape成batchsize×（32*32*3）\nmodel.add(Dense(512, activation='relu'))      # 添加全连接层，使用relu作为激活函数3072->512\nmodel.add(Dropout(0.2))                       # 添加dropout层，dropout层在每一个batchsize训练中随机使网络中一些节点失效(0.2的概率)，可以起到解耦合，防止过拟合等一系列作用\nmodel.add(Dense(512, activation='relu'))      # 添加全连接层，使用relu作为激活函数512->512\nmodel.add(Dropout(0.2))                       # 添加dropout层\nmodel.add(Dense(num_classes, activation='softmax'))# 添加全连接层，激活函数为softmax 512->10\n\nmodel.compile(loss='categorical_crossentropy',  # 多类交叉熵损失函数\n              optimizer=\"rmsprop\",              # 优化器使用rmsprop\n              metrics=['accuracy'])             # 评估指标：精度\nbatch_size = 32               # 每次输入32张图片,前向传播求出损失函数平均值，然后反向传播一次更新梯度\nepochs = 5                    # 保证所有训练数据被输入网络五次\nhistory = model.fit(x_train, y_train,                   # 训练数据\n                    batch_size=batch_size,\n                    epochs=epochs,\n                    verbose=1,                          # 越大，训练过程中显示的信息越详细             \n                    validation_data=(x_test, y_test))   # 验证集\nscore = model.evaluate(x_test, y_test, verbose=0)       # 模型评估，返回模型的loss和metric\nprint('Test loss:', score[0])                           # 测试集上模型损失\nprint('Test accuracy:', score[1])                       # 测试集上模型精度","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 用卷积神经网络 CNN 识别 CIFAR-10 数据集"},{"metadata":{},"cell_type":"markdown","source":"模型构建"},{"metadata":{},"cell_type":"markdown","source":"首先我们导入一些必要的库，我们主要会用到keras库。\n- Sequential: keras线性模型框架，可以理解为积木的模板\n- 卷积神经网络中的一些常用层\n    - Dense: 全连接层\n    - Flatten: 平铺层（二维转一维）\n    - Conv2D: 二维卷积层\n    - MaxPooling2D: 二维池化层"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pickle\nwith open(\"../input/youthaiimageclassification/cifar10.pkl\", \"rb\") as f:\n    (x_train, y_train), (x_test, y_test) = pickle.load(f)\n\n    \nfrom keras.models import Sequential                               # 序列模型，线性逐层叠加\nfrom keras.layers import Dense, Activation, Flatten, Dropout      # 导入全连接层、激活函数层、二维转一维、Dropout等神经网络常用层\nfrom keras.optimizers import SGD                                  # 导入随机梯度下降优化器\nimport matplotlib.pyplot as plt # 导入matplotlib库\nfrom keras.utils import to_categorical\n\n# 数据预处理\nprint(x_train.shape)\nprint(y_train.shape)\nprint(x_test.shape)\nprint(y_test.shape)\nx_train = x_train / 255  # 数据归一化\nx_test = x_test / 255\nnum_classes = 10         # 数据一共有10类\ny_train = to_categorical(y_train, num_classes) # 将训练数据的标签独热编码\ny_test = to_categorical(y_test, num_classes)   # 将测试数据的标签独热编码\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"模型搭建"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.layers import Conv2D, MaxPooling2D  # 从keras导入卷积层和最大池化层\n\nmodel = Sequential()\nmodel.add(Conv2D(16, (5, 5), padding='same',    # 添加卷积层；16：卷积核的个数;（5，5）:卷积核大小；padding=’same‘：图片卷积后大小不变\n                 input_shape=x_train.shape[1:]))# 第一个卷基层需要告诉它输入图片大小，以方便网络推导后面所需参数\nmodel.add(Activation('relu'))                   # 使用relu作为激活函数\nmodel.add(Conv2D(32, (5, 5)))                   # 添加卷积层\nmodel.add(Activation('sigmoid'))                # 使用sigmoid作为激活函数\nmodel.add(MaxPooling2D(pool_size=(2, 2)))       # 最大池化层，在2*2的区域中选取最大的数\nmodel.add(Dropout(0.25))                        # 添加dropout层，dropout层在每一个batchsize训练中随机使网络中一些节点失效(0.25的概率)\n\nmodel.add(Conv2D(64, (5, 5), padding='same'))   # 添加卷积层；64：卷积核的个数;（5，5）:卷积核大小；padding=’same‘：图片卷积后大小不变\nmodel.add(Activation('relu'))                   # 使用relu作为激活函数\nmodel.add(MaxPooling2D(pool_size=(2, 2)))       # 最大池化层，在2*2的区域中选取最大的数\nmodel.add(Dropout(0.25))                        # 添加dropout层，dropout层在每一个batchsize训练中随机使网络中一些节点失效(0.25的概率)\n\nmodel.add(Flatten())\nmodel.add(Dense(100))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(num_classes))\nmodel.add(Activation('softmax'))\n\n# 模型编译\nmodel.compile(loss='categorical_crossentropy',  # 损失函数使用多类交叉熵损失函数\n              optimizer=\"adam\",                 # 优化器采用adam\n              metrics=['accuracy'])             # 用精度作为性能评价指标","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.layers import Conv2D, MaxPooling2D  # 从keras导入卷积层和最大池化层\n\nmodel = Sequential()\nmodel.add(Conv2D(32, (3, 3), padding='same',    # 添加卷积层；32：卷积核的个数;（3，3）:卷积核大小；padding='same'：图片卷积后大小不变\n                 input_shape=x_train.shape[1:]))# 第一个卷基层需要告诉它输入图片大小，以方便网络推导后面所需参数\nmodel.add(Activation('relu'))                   # 使用relu作为激活函数\nmodel.add(Conv2D(32, (3, 3)))                   # 添加卷积层\nmodel.add(Activation('relu'))                   # 使用relu作为激活函数\nmodel.add(MaxPooling2D(pool_size=(2, 2)))       # 最大池化层，在2*2的区域中选取最大的数\nmodel.add(Dropout(0.25))                        # 添加dropout层，dropout层在每一个batchsize训练中随机使网络中一些节点失效(0.25的概率)\n\nmodel.add(Conv2D(64, (3, 3), padding='same'))   # 添加卷积层；64：卷积核的个数;（3，3）:卷积核大小；padding='same'：图片卷积后大小不变\nmodel.add(Activation('relu'))                   # 使用relu作为激活函数\nmodel.add(Conv2D(64, (3, 3)))                   # 添加卷积层；64：卷积核的个数;（3，3）\nmodel.add(Activation('relu'))                   # 使用relu作为激活函数\nmodel.add(MaxPooling2D(pool_size=(2, 2)))       # 最大池化层，在2*2的区域中选取最大的数\nmodel.add(Dropout(0.25))                        # 添加dropout层，dropout层在每一个batchsize训练中随机使网络中一些节点失效(0.25的概率)\n\nmodel.add(Flatten())\nmodel.add(Dense(512))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(num_classes))\nmodel.add(Activation('softmax'))\n\n# 模型编译\nmodel.compile(loss='categorical_crossentropy',  # 损失函数使用多类交叉熵损失函数\n              optimizer=\"adam\",                 # 优化器采用adam\n              metrics=['accuracy'])             # 用精度作为性能评价指标","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"模型训练"},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 32               # 每次输入32张图片,前向传播求出损失函数平均值，然后反向传播一次更新梯度\nepochs = 5                    # 保证所有训练数据被输入网络五次\nhistory = model.fit(x_train, y_train,                   # 训练数据\n                    batch_size=batch_size,\n                    epochs=epochs,\n                    verbose=1,                          # 越大，训练过程中显示的信息越详细             \n                    validation_data=(x_test, y_test))   # 验证集\nscore = model.evaluate(x_test, y_test, verbose=0)       # 模型评估，返回模型的loss和metric\nprint('Test loss:', score[0])                           # 测试集上模型损失\nprint('Test accuracy:', score[1])                       # 测试集上模型精度","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 图像增强"},{"metadata":{},"cell_type":"markdown","source":"有一种观点认为神经网络是靠数据喂出来的，如果能够增加训练数据，提供海量数据进行训练，则能够有效提升算法的准确率，因为这样可以避免过拟合，从而可以进一步增大、加深网络结构。而当训练数据有限时，可以通过一些变换从已有的训练数据集中生成一些新的数据，以快速地扩充训练数据。这种方法被成为“数据增强”。\n\n对图像数据进行变换的方式有很多，最常用的包括：\n\n- 旋转图像\n- 平移图像\n- 水平或竖直翻转图像"},{"metadata":{},"cell_type":"markdown","source":"keras提供了ImageDataGenerator模块方便我们进行图像数据增强，仅需几行代码就可以完成。我们使用cifar-10数据集做例子，对原始数据进行数据增强。"},{"metadata":{},"cell_type":"markdown","source":"首先导入一些必要的库\n\n- cifar10： keras库中包含的一个方便我们加载CIFAR-10数据集的类\n- ImageDataGenerator：keras中用于进行图像数据增强的类\n- pyplot：python中最常用的画图工具"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.image import ImageDataGenerator\nfrom matplotlib import pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"加载cifar10数据集并进行数据预处理"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pickle\nwith open(\"../input/youthaiimageclassification/cifar10.pkl\", \"rb\") as f:\n    (x_train, y_train), (x_test, y_test) = pickle.load(f)\n\n    \nfrom keras.models import Sequential                               # 序列模型，线性逐层叠加\nfrom keras.layers import Dense, Activation, Flatten, Dropout      # 导入全连接层、激活函数层、二维转一维、Dropout等神经网络常用层\nfrom keras.optimizers import SGD                                  # 导入随机梯度下降优化器\nimport matplotlib.pyplot as plt # 导入matplotlib库\nfrom keras.utils import to_categorical\n\n# 数据预处理\nprint(x_train.shape)\nprint(y_train.shape)\nprint(x_test.shape)\nprint(y_test.shape)\nx_train = x_train / 255  # 数据归一化\nx_test = x_test / 255\nnum_classes = 10         # 数据一共有10类\ny_train = to_categorical(y_train, num_classes) # 将训练数据的标签独热编码\ny_test = to_categorical(y_test, num_classes)   # 将测试数据的标签独热编码\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"在ImageDataGenerator中定义图像增强的方式，这里我们采取了下列几种方式\n\n- rotation_range=30，随机旋转不超过30度\n- horizontal_flip=True, 随机进行水平翻转\n- vertical_flip=True, 随机进行竖直反转\n- width_shift_range=5, 随机进行不超过5像素的水平平移\n- height_shift_range=5, 随机进行不超过5像素的竖直平移"},{"metadata":{"trusted":true},"cell_type":"code","source":"datagen = ImageDataGenerator(\n    rotation_range=30,\n    horizontal_flip=True,\n    vertical_flip=True,\n    width_shift_range=5,\n    height_shift_range=5\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"我们选取训练集中的一张卡车的图片，用这个ImageDataGenerator对图像进行五次随机变换并将它们画出来。"},{"metadata":{"trusted":true},"cell_type":"code","source":"origin_image = x_train[1] # 选取原图\n\n# 将原图画出来\nplt.imshow(origin_image) \nplt.show()\n\n# 对图像作五次随机变换并画出来\nfig, ax = plt.subplots(1, 5, figsize=(15, 3))\nax = ax.flatten()\nfor i in range(5):\n    ax[i].imshow(datagen.random_transform(origin_image)) # 使用datagen对图像作随机变换\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"要使用图像增强后的数据代替原数据进行训练也非常简单，只需要更改一行代码。回想我们如何使用model.fit函数指定训练数据对模型进行训练。"},{"metadata":{"trusted":false},"cell_type":"code","source":"model.fit(x_train, y_train, batch_size=32, epochs=10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"如果我们希望启用图像增强，keras提供了一个fit_generator函数，用法如下。"},{"metadata":{"trusted":false},"cell_type":"code","source":"model.fit_generator(datagen.flow(x_train, y_train, batch_size=32), epochs=10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"fit_generator接收的第一个参数是datagen.flow(x_train, y_train, batch_size=32), 这句话的意思是使用定义好的datagen对原始的输入数据进行变换，生成新的训练数据，每批次生成32个。其他的步骤没有变化，我们就可以像之前一样训练模型了。不要忘记在训练模型之前需要对数据进行预处理。"},{"metadata":{},"cell_type":"markdown","source":"除了上面提到的几种变换方式，keras还提供了亮度变换、缩放等其他变换，另外还可以自己编写变换函数，对图片进行自定义的变化。"},{"metadata":{},"cell_type":"markdown","source":"## 样例图片生成"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.imshow(origin_image)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"顺时针旋转30度"},{"metadata":{"trusted":true},"cell_type":"code","source":"transformed_image = datagen.apply_transform(origin_image, {\n    \"theta\": 30\n})\nplt.imshow(transformed_image)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"向x，y正方向平移5个单位"},{"metadata":{"trusted":true},"cell_type":"code","source":"transformed_image = datagen.apply_transform(origin_image, {\n    \"tx\": 5,\n    \"ty\": 5\n})\nplt.imshow(transformed_image)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"垂直翻转，即沿x轴翻转"},{"metadata":{"trusted":true},"cell_type":"code","source":"transformed_image = datagen.apply_transform(origin_image, {\n    \"flip_vertical\": True\n})\nplt.imshow(transformed_image)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"水平翻转，即沿y轴翻转"},{"metadata":{"trusted":true},"cell_type":"code","source":"transformed_image = datagen.apply_transform(origin_image, {\n    \"flip_horizontal\": True\n})\nplt.imshow(transformed_image)\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.7"}},"nbformat":4,"nbformat_minor":1}