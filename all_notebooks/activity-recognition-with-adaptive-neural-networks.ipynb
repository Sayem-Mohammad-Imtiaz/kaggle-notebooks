{"cells":[{"metadata":{"id":"drO4xxydmf9i"},"cell_type":"markdown","source":"## Introduction\n\nCurrent deep neural architectures for processing sensor data are mainly designed for data coming from a fixed set of sensors, with a fixed sampling rate. Changing the dimensions of the input data causes considerable accuracy loss, unnecessary computations, or application failures. To address this problem, we introduce a **dimension-adaptive pooling (DAP)** layer that makes deep architectures robust to temporal changes in sampling rate and in sensor availability. DAP operates on convolutional filter maps of variable dimensions and produces an input of fixed dimensions suitable for feedforward and recurrent layers. Building on this architectural improvement, we propose a **dimension-adaptive training (DAT)** procedure to generalize over the entire space of feasible data dimensions at the inference time. DAT comprises the random selection of dimensions during the forward passes and optimization with accumulated gradients of several backward passes. We then can combine DAP and DAT to transform existing non-adaptive deep architectures into a **Dimension-Adaptive Neural Architecture (DANA)** without altering other architectural aspects. Our solution does not need up-sampling or imputation, thus reduces unnecessary computations at inference time. Experimental results on public datasets show that DANA prevents losses in classification accuracy of the state-of-the-art deep architectures, under dynamic sensor availability and varying sampling rates.\n\nPaper: https://arxiv.org/abs/2008.02397\nCode: https://github.com/mmalekzadeh/dana","execution_count":null},{"metadata":{"id":"GZMbLzCyFcx5"},"cell_type":"markdown","source":"### Import Libraries \n\nFirs things first :)","execution_count":null},{"metadata":{"executionInfo":{"elapsed":4029,"status":"ok","timestamp":1596730174157,"user":{"displayName":"a cup of data","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GghBghr_Yn6r8UH5P2egK3SaSAe2gEQgbVSenKq=s64","userId":"12973428829118435411"},"user_tz":-60},"id":"mCliN3wnFYPa","outputId":"d7cd0b62-d32c-46a3-b948-42ea58659cb7","trusted":true},"cell_type":"code","source":"import numpy as np\nimport time\nimport requests\nfrom io import BytesIO\nimport matplotlib.pyplot as plt\nfrom sklearn.utils import class_weight\n\nimport tensorflow as tf\nimport tensorflow.keras as keras\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.models import Model\nprint(\"TensorFlow Version: \", tf.__version__)","execution_count":null,"outputs":[]},{"metadata":{"id":"KMDZiSW0h2dS"},"cell_type":"markdown","source":"We need a TensorFlow version 2.1.X or above.","execution_count":null},{"metadata":{"id":"4e30Voq_Fhzu"},"cell_type":"markdown","source":"### Import Dataset\n\nHere we use UCI-HAR dataset: \"Human Activity Recognition database built from the recordings of 30 subjects performing activities of daily living (ADL) while carrying a waist-mounted smartphone with embedded inertial sensors\"\n\nMore info: https://archive.ics.uci.edu/ml/datasets/human+activity+recognition+using+smartphones ","execution_count":null},{"metadata":{"executionInfo":{"elapsed":5501,"status":"ok","timestamp":1596730175636,"user":{"displayName":"a cup of data","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GghBghr_Yn6r8UH5P2egK3SaSAe2gEQgbVSenKq=s64","userId":"12973428829118435411"},"user_tz":-60},"id":"UQHBj2ty9GEn","outputId":"3a141fc6-bec3-49bc-e9f1-6f03e14d60a6","trusted":true},"cell_type":"code","source":"## In this dataset Each person performed six activities, while\n## wearing a smartphone (Samsung Galaxy S II) on the waist.\ndataset_name = \"UCIHAR\" \ndata_class_names = [\"Walking\", \"Sairs-Up\", \"Sairs-Down\", \"Sitting\", \"Standing\", \"Lying\"]\n\n## A pre-processd version of this dataset, splitted into training and testing,\n## and transformed into consequtive time windows of lenght 2.5 seconds.\n## Here we choose the first 6 sensor streams that shows \"accelerometer (x,y,z)\"  and \"gyroscope (x,y,z)\". \n## The remaing 3 streams show the estimated body acceleration which is a dervied data (see the Main Source above)\ndataset_address = \"https://raw.githubusercontent.com/mmalekzadeh/dana/master/dana/datasets/\"+dataset_name+\"/\"\nX_train = np.load(BytesIO(requests.get(dataset_address + \"X_train.npy\").content))[:,:,:6]\nY_train = np.load(BytesIO(requests.get(dataset_address + \"y_train.npy\").content)).argmax(1)   \nX_test = np.load(BytesIO(requests.get(dataset_address + \"X_test.npy\").content))[:,:,:6] \nY_test = np.load(BytesIO(requests.get(dataset_address + \"y_test.npy\").content)).argmax(1) \n\n## Shufflig the training set\nrnd_seed = 0\ntf.random.set_seed(rnd_seed)\nindices = np.random.RandomState(seed=rnd_seed).permutation(len(X_train))\nX_train = X_train[indices]\nY_train = Y_train[indices]        \n\n## Computing the class weight for each label (in case that dataset is not balanced)\ndata_class_weights = class_weight.compute_class_weight('balanced',range(len(data_class_names)),Y_train)\ndata_class_weights = dict(zip(range(len(data_class_weights)),data_class_weights.round(2)))\n\nprint(\"- Data Shape:\\n -- Training:  Data {} Labels {} \\n -- Testing: Data {} Labels {}\".\n      format(X_train.shape, Y_train.shape, X_test.shape, Y_test.shape))\nprint(\"- Activity Weights\",dict(zip(data_class_names,data_class_weights.values())))","execution_count":null,"outputs":[]},{"metadata":{"id":"0XgDi935linT"},"cell_type":"markdown","source":"#### Visualization\nLet us plot some of this time windows to get an idea about the data.","execution_count":null},{"metadata":{"executionInfo":{"elapsed":4074,"status":"ok","timestamp":1596731020900,"user":{"displayName":"a cup of data","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GghBghr_Yn6r8UH5P2egK3SaSAe2gEQgbVSenKq=s64","userId":"12973428829118435411"},"user_tz":-60},"id":"QjiHDmfJiE0Q","outputId":"0b069e31-cd91-44f0-c590-43dd2e148a22","trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(nrows=5, ncols=3, figsize=(24,18))\nfor r in range(5):\n    for c in range(3):        \n        ind = np.random.choice(len(X_train))\n        ax[r,c].plot(X_train[ind])\n        ax[r,c].set_title(data_class_names[Y_train[ind]], size=15)\n        if r == 3:\n            ax[r,c].set_xlabel(\"Sample Points (in 50Hz)\", size=12)\n        ax[r,c].set_ylabel(\"Sensor Redaing\", size=12)\nfig.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"id":"VBVnoYUULkAw"},"cell_type":"markdown","source":"### The Original Model","execution_count":null},{"metadata":{"id":"FPqdISGQlxfD"},"cell_type":"markdown","source":"First, we build and train on of the sate-of-the-art models for activity recogniton. We call this tehe original model which is trained on the dataset on with a fixed sampling rate 50Hz and both sensors: accelerometer and gyroscope.","execution_count":null},{"metadata":{"executionInfo":{"elapsed":2149,"status":"ok","timestamp":1596729704722,"user":{"displayName":"a cup of data","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GghBghr_Yn6r8UH5P2egK3SaSAe2gEQgbVSenKq=s64","userId":"12973428829118435411"},"user_tz":-60},"id":"Hwl4L20RIENM","outputId":"deb2b931-ac08-4742-8d0b-994f0a3598b1","trusted":true},"cell_type":"code","source":"def Ordonez2016DeepOriginal(inp_shape, out_shape):   \n    \"\"\"\n    @article{ordonez2016deep,\n        title={Deep convolutional and {LSTM} recurrent neural networks for multimodal wearable activity recognition},\n        author={Ord{\\'o}{\\~n}ez, Francisco and Roggen, Daniel},\n        journal={Sensors},\n        volume={16},\n        number={1},\n        pages={115},\n        year={2016},\n        publisher={Multidisciplinary Digital Publishing Institute}\n    }\n    \"\"\"   \n    nb_filters = 64 \n    drp_out_dns = .5 \n    nb_dense = 128 \n    \n    inp = Input(inp_shape)\n\n    x = Conv2D(nb_filters, kernel_size = (5,1),\n              strides=(1,1), padding='valid', activation='relu')(inp)    \n    x = Conv2D(nb_filters, kernel_size = (5,1),\n              strides=(1,1), padding='valid', activation='relu')(x)\n    x = Conv2D(nb_filters, kernel_size = (5,1), \n              strides=(1,1), padding='valid', activation='relu')(x)\n    x = Conv2D(nb_filters, kernel_size = (5,1), \n              strides=(1,1), padding='valid', activation='relu')(x)    \n    x = Reshape((x.shape[1],x.shape[2]*x.shape[3]))(x)\n    act = LSTM(nb_dense, return_sequences=True, activation='tanh', name=\"lstm_1\")(x)        \n    act = Dropout(drp_out_dns, name= \"dot_1\")(act)\n    act = LSTM(nb_dense, activation='tanh', name=\"lstm_2\")(act)        \n    act = Dropout(drp_out_dns, name= \"dot_2\")(act)\n    out_act = Dense(out_shape, activation='softmax',  name=\"act_smx\")(act)\n    \n    model = keras.models.Model(inputs=inp, outputs=out_act)\n    return model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here I set `num_epochs` to be fixed for both of our experiments.\n\nNote that training an adaptive model needs time. \n\nSo, usually we have to choose a large value for `num_epochs`.\n\nI could see a convergence for a `num_epochs ~ 500` which takes about an hour using one GPU.\n\nFell free to reduce it for faster experiments.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"### Let's fix the number of epochs.\nnum_epochs = 500 \n\nX_train = np.expand_dims(X_train,3)\nX_test = np.expand_dims(X_test,3)\n\nw = X_train.shape[1]\nh = X_train.shape[2]\norg_model = Ordonez2016DeepOriginal((w, h, 1), len(np.unique(Y_train)))\norg_model.summary()","execution_count":null,"outputs":[]},{"metadata":{"executionInfo":{"elapsed":6397,"status":"ok","timestamp":1596717914310,"user":{"displayName":"a cup of data","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GghBghr_Yn6r8UH5P2egK3SaSAe2gEQgbVSenKq=s64","userId":"12973428829118435411"},"user_tz":-60},"id":"lNj35tQeKzRY","trusted":true},"cell_type":"code","source":"def standard_training(model, X_train, Y_train, X_val, Y_val, data_class_weights,\n                      batch_size=128, num_epochs=128, save_dir=None):\n    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n    optimizer = tf.keras.optimizers.Adam()\n    best_val_accuracy = 0.\n    for epoch in range(num_epochs):  \n\n        ## Training\n        train_dataset = tf.data.Dataset.from_tensor_slices((X_train, Y_train))\n        train_dataset = iter(train_dataset.shuffle(len(X_train)).batch(batch_size))\n        n_iterations_per_epoch = len(X_train)//(batch_size)\n        epoch_loss_avg = tf.keras.metrics.Mean()           \n        for i in range(n_iterations_per_epoch):                \n            with tf.GradientTape() as tape:\n                X, Y = next(train_dataset)\n                sample_weight = [data_class_weights[y] for y in Y.numpy()]                        \n                logits = model(X)               \n                loss_value = loss_fn(Y, logits, sample_weight)\n            gradients = tape.gradient(loss_value, model.trainable_weights)\n            optimizer.apply_gradients(zip(gradients, model.trainable_weights))                        \n            epoch_loss_avg.update_state(loss_value)\n\n        ## Validation\n        val_accuracy = tf.keras.metrics.Accuracy()\n        logits = model(X_val, training=False)\n        prediction = tf.argmax(logits, axis=1, output_type=tf.int32)\n        val_accuracy(prediction, Y_val)\n        current_val_acc = val_accuracy.result()\n        \n        if current_val_acc > best_val_accuracy:\n            best_val_accuracy = current_val_acc\n            if save_dir:\n                model.save_weights(save_dir)\n                \n        print (\"Epoch {} -- Training Loss = {:.4f} -- Validation Accuracy {:.4f}\".format(\n            epoch,\n            epoch_loss_avg.result(),\n            current_val_acc))\n    \n    if save_dir:\n        model.load_weights(save_dir)\n    print(\"Best Validation Accuracy {:.4f}\".format(best_val_accuracy))\n    print(\"Training Finished! \\n------------------\\n\")\n    return model","execution_count":null,"outputs":[]},{"metadata":{"executionInfo":{"elapsed":1074044,"status":"ok","timestamp":1596718981962,"user":{"displayName":"a cup of data","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GghBghr_Yn6r8UH5P2egK3SaSAe2gEQgbVSenKq=s64","userId":"12973428829118435411"},"user_tz":-60},"id":"3asrqlIRUGX8","outputId":"23a04290-45d7-4764-b9ae-3daf8cbe30c8","trusted":true},"cell_type":"code","source":"org_model = standard_training(org_model, \n                  X_train[len(X_train)//10:], Y_train[len(X_train)//10:],\n                  X_train[:len(X_train)//10], Y_train[:len(X_train)//10],\n                  data_class_weights,\n                  batch_size=128, num_epochs=num_epochs,\n                  save_dir = \"saved_models/standard/Ordonez2016DeepOriginal\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Test Accuracy for standard Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"## Testing\ntest_accuracy = tf.keras.metrics.Accuracy()\nlogits = org_model(X_test, training=False)\nprediction = tf.argmax(logits, axis=1, output_type=tf.int32)\ntest_accuracy(prediction, Y_test)\nprint(\"Test Accuracy {:.4f}\".format(test_accuracy.result()))","execution_count":null,"outputs":[]},{"metadata":{"id":"jy1kMUiLdWRd"},"cell_type":"markdown","source":"## Dimension-Adaptive Pooling  (DAP) Layer","execution_count":null},{"metadata":{"id":"qSTQqcp_nqok"},"cell_type":"markdown","source":"DAP layer provides a fixed-sized output and preserves temporal correlation among consecutive data samples. This is important as RNNs are more efficient in processing temporal data than FNNs. Moreover, DAP can handle situations when one or more sensors may be unavailable at inference time. The flexibility of DAP not only makes DNNs adaptive to changes in the dimension of data, but allows us to efficiently train the DNN such that it provides reliable performance across several combinations of data dimensions.\n\n![alt text](https://raw.githubusercontent.com/mmalekzadeh/dana/master/dana.jpg)\n","execution_count":null},{"metadata":{"executionInfo":{"elapsed":827,"status":"ok","timestamp":1596729723077,"user":{"displayName":"a cup of data","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GghBghr_Yn6r8UH5P2egK3SaSAe2gEQgbVSenKq=s64","userId":"12973428829118435411"},"user_tz":-60},"id":"7QzNue75dd_6","trusted":true},"cell_type":"code","source":"from tensorflow.keras import layers\nfrom tensorflow.python.framework import tensor_shape\n# Parts of the code is taken from this repo: https://github.com/yhenon/keras-spp          \nclass DimensionAdaptivePooling(layers.Layer):\n    \"\"\" Dimension Adaptive Pooling layer for 2D inputs.\n    # Arguments\n        pool_list: a tuple (W,H)\n            List of pooling regions to use. The length of the list is the number of pooling regions,\n            each tuple in the list is the number of regions in that pool. For example [(8,6),(4,3)] would be 2\n            regions with 1, 8x6 and 4x3 max pools, so 48+12 outputs per feature map.\n        forRNN: binary\n            Determines wheterh the layer after this is a recurrent layer (LSTM) or not (it is Dense)\n        operation: string\n            Either `max` or `avg`.\n    # Input shape\n        4D tensor with shape: `(samples, w, h, M)` .\n    # Output shape\n        2D or 3D tensor with shape: `(samples,  W*H*M)` or `(samples,  W, H*M)`.\n    \"\"\"\n    def __init__(self, pooling_parameters, forRNN=False, operation=\"max\", name=None, **kwargs):\n        super(DimensionAdaptivePooling, self).__init__(name=name, **kwargs)\n        self.pool_list = np.array(pooling_parameters)\n        self.forRNN = forRNN\n        self.W = self.pool_list[0]\n        self.H = self.pool_list[1]\n        self.num_outputs_per_feature_map =  self.W * self.H\n        if operation == \"max\":\n            self.operation = tf.math.reduce_max\n        elif operation == \"avg\":\n            self.operation = tf.math.reduce_mean\n       \n    def build(self, input_shape):        \n        self.M = input_shape[3]      \n\n    def compute_output_shape(self, input_shape):\n        input_shape = tensor_shape.TensorShape(input_shape).as_list()        \n        if self.forRNN:\n            return tensor_shape.TensorShape([input_shape[0], self.W, self.H * self.M])\n        else:\n            return tensor_shape.TensorShape([input_shape[0], self.W * self.H * self.M])\n\n    def get_config(self):\n        config = {'dap pooling parameters': self.pool_list, 'forRNN': self.forRNN}\n        base_config = super(DimensionAdaptivePooling, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))    \n\n\nclass DimensionAdaptivePoolingForSensors(DimensionAdaptivePooling):\n    def __init__(self, pooling_parameters, forRNN=False, operation=\"max\", name=None, **kwargs):\n        super(DimensionAdaptivePoolingForSensors, self).__init__(pooling_parameters=pooling_parameters, \n                                                                forRNN=forRNN, \n                                                                operation=operation,\n                                                                name=name, **kwargs)      \n    def call(self, xp, mask=None):\n        xp_dtype = xp.dtype                \n        input_shape = tf.shape(xp)\n        wp = input_shape[1] ## This is the number of sample points in each time-window (w')\n        hp = input_shape[2] ## This is the number of sensor channels (h')\n        \n        xpp = tf.identity(xp)\n        try:\n            A = tf.cast(tf.math.maximum(tf.math.ceil((self.H-hp)/3),0), dtype=xp_dtype)\n            for ia in range(tf.cast(A, tf.int32)):\n                xpp = tf.concat([xpp, xp],2)                                                                         \n            xpp = xpp[:, :wp, :tf.math.maximum(hp,self.H), :]\n        except:\n            A = tf.Variable(0,dtype=xp_dtype)        \n        p_w = tf.cast(wp / self.W, dtype=xp_dtype)\n        p_h = tf.cast(hp / self.H, dtype=xp_dtype)\n        Zp = []\n        for iw in range(self.W):                \n            for ih in range(self.H):\n                r1 = tf.cast(tf.math.round(iw * p_w), tf.int32)\n                r2 = tf.cast(tf.math.round((iw+1) * p_w), tf.int32)\n                if A == 0:\n                    c1 = tf.cast(tf.math.round(ih *p_h), tf.int32)\n                    c2 = tf.cast(tf.math.round((ih+1)*p_h), tf.int32)\n                else:\n                    c1 = tf.cast(tf.math.round(ih * tf.math.floor((A+1)*p_h)), tf.int32)\n                    c2 = tf.cast(tf.math.round((ih+1) * tf.math.floor((A+1)*p_h)), tf.int32)\n                try:                                               \n                    Zp.append(self.operation(xpp[:, r1:r2, c1:c2, :], axis=(1, 2)))\n                except:\n                    Zp = []\n        Zp = tf.concat(Zp, axis=-1)\n        if self.forRNN:\n            Zp = tf.reshape(Zp,(input_shape[0], self.W, self.H * self.M))\n        else:\n            Zp = tf.reshape(Zp,(input_shape[0], self.W * self.H * self.M))\n        return Zp","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Build the Adaptive (DANA) Model","execution_count":null},{"metadata":{"executionInfo":{"elapsed":2788,"status":"ok","timestamp":1596729727707,"user":{"displayName":"a cup of data","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GghBghr_Yn6r8UH5P2egK3SaSAe2gEQgbVSenKq=s64","userId":"12973428829118435411"},"user_tz":-60},"id":"VodIWm93j3ZN","outputId":"8a503328-cf37-4f36-e5f7-ff7c3b66c9d7","trusted":true},"cell_type":"code","source":"def Ordonez2016DeepWithDAP(inp_shape, out_shape, pool_list=(4,6)):      \n    nb_filters = 64 \n    drp_out_dns = .5 \n    nb_dense = 128 \n\n    inp = Input(inp_shape)\n\n    x = Conv2D(nb_filters, kernel_size = (5,1),\n              strides=(1,1), padding='same', activation='relu')(inp)    \n    x = Conv2D(nb_filters, kernel_size = (5,1),\n              strides=(1,1), padding='same', activation='relu')(x)\n    x = Conv2D(nb_filters, kernel_size = (5,1), \n              strides=(1,1), padding='same', activation='relu')(x)\n    x = Conv2D(nb_filters, kernel_size = (5,1), \n              strides=(1,1), padding='same', activation='relu')(x)    \n    #### DAP Layer    \n    x = DimensionAdaptivePoolingForSensors(pool_list, operation=\"max\", name =\"DAP\", forRNN=True)(x)    \n\n    act = LSTM(nb_dense, return_sequences=True, activation='tanh', name=\"lstm_1\")(x)        \n    act = Dropout(drp_out_dns, name= \"dot_1\")(act)\n    act = LSTM(nb_dense, activation='tanh', name=\"lstm_2\")(act)        \n    act = Dropout(drp_out_dns, name= \"dot_2\")(act)\n    out_act = Dense(out_shape, activation='softmax',  name=\"act_smx\")(act)\n\n    model = keras.models.Model(inputs=inp, outputs=out_act)\n    return model\n\ndana_model = Ordonez2016DeepWithDAP((None, None, 1), len(np.unique(Y_train)))\ndana_model.summary()","execution_count":null,"outputs":[]},{"metadata":{"id":"slWvpDoLo9vj"},"cell_type":"markdown","source":"## Dimension-Adaptive Training (DAT)\n\nTo enable DNNs that use DAP to generalize at inference time over the set of feasible dimensions, we propose a dimension-adaptive training~(DAT) procedure, which incorporates dimension randomization and optimization with accumulated gradients. In each forward pass, DAT re-samples a batch of time widows to a new rate and may also cope with removed streams  from some sensors. Then, gradients from several batches are accumulated before updating the parameters. Combining DAP and DAT, we show how to transform an existing DNN into an adaptive architecture, while keeping the same size and accuracy, and improving the inference time. Beside allowing adaptive sampling rate and sensor selection in a unified solution, DANA also enables power-limited devices to take advantage of convolutional layers capability in reducing the performed computations according to the dimensions of the sampled data.\n\n![alt text](https://raw.githubusercontent.com/mmalekzadeh/dana/master/dat.jpg)\n","execution_count":null},{"metadata":{"executionInfo":{"elapsed":1075515,"status":"ok","timestamp":1596718983447,"user":{"displayName":"a cup of data","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GghBghr_Yn6r8UH5P2egK3SaSAe2gEQgbVSenKq=s64","userId":"12973428829118435411"},"user_tz":-60},"id":"swPiMhQpeSzR","trusted":true},"cell_type":"code","source":"def dimension_adaptive_training(model, X_train, Y_train, X_val, Y_val, data_class_weights,\n                      batch_size=128, num_epochs=128, save_dir=None,\n                      W_combinations=None, H_combinations=None,\n                      n_batch_per_train_setp=1):\n    \n    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n    optimizer = tf.keras.optimizers.Adam()\n    best_val_accuracy = 0.\n    \n    for epoch in range(num_epochs):  \n\n        ## Training\n        train_dataset = tf.data.Dataset.from_tensor_slices((X_train, Y_train))\n        train_dataset = iter(train_dataset.shuffle(len(X_train)).batch(batch_size))\n        n_iterations_per_epoch = len(X_train)//(batch_size*n_batch_per_train_setp)\n        epoch_loss_avg = tf.keras.metrics.Mean()           \n\n        for i in range(n_iterations_per_epoch):\n            rnd_order_H = np.random.permutation(len(H_combinations))\n            rnd_order_W = np.random.permutation(len(W_combinations))\n            n_samples = 0.\n            with tf.GradientTape() as tape:\n                accum_loss = tf.Variable(0.)\n                for j in range(n_batch_per_train_setp):\n                    try:\n                        X, Y = next(train_dataset)\n                    except:\n                        break\n                    X = X.numpy()\n                    sample_weight = [data_class_weights[y] for y in Y.numpy()]\n                    \n                    ### Dimension Randomization \n                    ####### Random Sensor Selection\n                    rnd_H = H_combinations[rnd_order_H[j%len(rnd_order_H)]]                    \n                    X = X[:,:,rnd_H,:] \n                    ####### Random Sampling Rate Selection  \n                    rnd_W = W_combinations[rnd_order_W[j%len(rnd_order_W)]]\n                    X = tf.image.resize(X, (rnd_W, len(rnd_H)))    \n\n                    logits =  model(X)               \n                    accum_loss = accum_loss + loss_fn(Y, logits, sample_weight)\n                    n_samples = n_samples + 1.\n            gradients = tape.gradient(accum_loss, model.trainable_weights)\n            gradients = [g*(1./n_samples) for g in gradients]\n            optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n            epoch_loss_avg.update_state(accum_loss*(1./n_samples))\n\n        ## Validation\n        accuracy_record = np.zeros((len(W_combinations),len(H_combinations)))\n        for w, W_comb in enumerate(W_combinations):\n            for h, H_comb in enumerate(H_combinations):                    \n                val_accuracy = tf.keras.metrics.Accuracy()\n\n                X = X_val.copy()                                    \n                X = X[:,:,H_comb,:]\n                X = tf.image.resize(X, (W_comb, len(H_comb)))\n\n                logits = model(X, training=False)\n                prediction = tf.argmax(logits, axis=1, output_type=tf.int32)\n                val_accuracy(prediction, Y_val)\n                accuracy_record[w,h] = val_accuracy.result()\n        \n        current_val_acc = accuracy_record\n        if np.mean(current_val_acc) > np.mean(best_val_accuracy):\n            best_val_accuracy = current_val_acc\n            if save_dir:\n                model.save_weights(save_dir)\n\n        print (\"Epoch {} -- Training Loss = {:.4f} -- Validation Mean Accuracy {:.4f}\".format(\n            epoch,\n            epoch_loss_avg.result(),\n            np.mean(current_val_acc)))\n    \n    if save_dir:\n        model.load_weights(save_dir)\n    print(\"Best Validation Accuracy {}\".format(best_val_accuracy.round(4)))\n    print(\"Training Finished! \\n------------------\\n\")\n    return model\n\n\n### These are a subset of feasible situations in both dimensions\nW_combinations = list(np.arange(16,129,16))\nH_combinations = [[0,1,2], [3,4,5], [0,1,2,3,4,5], [3,4,5,0,1,2]]\nn_batch_per_train_setp = 5 ## This is B","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training the Adaptive Model","execution_count":null},{"metadata":{"executionInfo":{"elapsed":4335020,"status":"ok","timestamp":1596722242956,"user":{"displayName":"a cup of data","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GghBghr_Yn6r8UH5P2egK3SaSAe2gEQgbVSenKq=s64","userId":"12973428829118435411"},"user_tz":-60},"id":"HbpOO7hxizVQ","outputId":"90ae7dd7-cc02-43bb-88fa-2584024aff44","trusted":true},"cell_type":"code","source":"dana_model = dimension_adaptive_training(dana_model, \n                  X_train[len(X_train)//10:], Y_train[len(X_train)//10:],\n                  X_train[:len(X_train)//10], Y_train[:len(X_train)//10],\n                  data_class_weights,\n                  batch_size=128, num_epochs=num_epochs,\n                  save_dir = \"saved_models/dana/Ordonez2016DeepWithDAP\",\n                  W_combinations = W_combinations,\n                  H_combinations = H_combinations,\n                  n_batch_per_train_setp=n_batch_per_train_setp\n                  )","execution_count":null,"outputs":[]},{"metadata":{"executionInfo":{"elapsed":13382,"status":"ok","timestamp":1596729760261,"user":{"displayName":"a cup of data","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GghBghr_Yn6r8UH5P2egK3SaSAe2gEQgbVSenKq=s64","userId":"12973428829118435411"},"user_tz":-60},"id":"pSbm4fqEpgP7","outputId":"2a75558c-95a3-4490-f581-a0f0a80b151c","trusted":true},"cell_type":"code","source":"dana_model.load_weights(\"saved_models/dana/Ordonez2016DeepWithDAP\")\naccuracy_record = np.zeros((len(W_combinations),len(H_combinations)))\nfor w, W_comb in enumerate(W_combinations):\n    for h, H_comb in enumerate(H_combinations):                    \n        test_accuracy = tf.keras.metrics.Accuracy()\n\n        X = X_test.copy()                                    \n        X = X[:,:,H_comb,:]\n        X = tf.image.resize(X, (W_comb, len(H_comb)))\n\n        logits = dana_model(X, training=False)\n        prediction = tf.argmax(logits, axis=1, output_type=tf.int32)\n        test_accuracy(prediction, Y_test)\n        accuracy_record[w,h] = test_accuracy.result()\n\nprint(\"Test  Mean Accuracy {:.4f}\".format(np.mean(accuracy_record)))\nprint(\"Test Accuracies {}\".format((accuracy_record.round(4))))","execution_count":null,"outputs":[]},{"metadata":{"id":"Ve8mSPgTplcn"},"cell_type":"markdown","source":"## Comparing DANA with the Original model\n\nThe original model only works on input data that have fixed dimensions. So, we cannot test it on the sampling rate lower than 50Hz, unless we perform upsampling.\n\nMore importantly, when it misses a sensor, we have to imput the data of the missed sensor with either zeros or with the currently available data of another sensor.","execution_count":null},{"metadata":{"executionInfo":{"elapsed":3950,"status":"ok","timestamp":1596729766862,"user":{"displayName":"a cup of data","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GghBghr_Yn6r8UH5P2egK3SaSAe2gEQgbVSenKq=s64","userId":"12973428829118435411"},"user_tz":-60},"id":"12eCWV8MgGle","outputId":"788a9c86-b99e-4fe8-d927-905f031dec5b","trusted":true},"cell_type":"code","source":"def test_model(model, X, Y):\n    test_accuracy = tf.keras.metrics.Accuracy()\n    logits = model(X, training=False)\n    prediction = tf.argmax(logits, axis=1, output_type=tf.int32)\n    test_accuracy(prediction, Y)\n    return test_accuracy.result()\n\norg_model.load_weights(\"saved_models/standard/Ordonez2016DeepOriginal\")\norg_both = test_model(org_model, X_test, Y_test)\norg_accl = test_model(org_model, np.concatenate((X_test[:,:,:3,:], np.zeros_like(X_test[:,:,:3,:])),axis=2), Y_test)\norg_gyro = test_model(org_model, np.concatenate((X_test[:,:,3:,:], np.zeros_like(X_test[:,:,3:,:])),axis=2), Y_test)\nprint(\"Original Model Test Accuracy \\n Both: {:.4f} \\n Only Accl: {:.4f} \\n Only Gyro: {:.4f}\".format(org_both, org_accl, org_gyro))","execution_count":null,"outputs":[]},{"metadata":{"executionInfo":{"elapsed":2886,"status":"ok","timestamp":1596729771358,"user":{"displayName":"a cup of data","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GghBghr_Yn6r8UH5P2egK3SaSAe2gEQgbVSenKq=s64","userId":"12973428829118435411"},"user_tz":-60},"id":"HwM2qAFvcRs5","outputId":"5252dd0e-ba4a-45c6-fa6a-40e692559258","trusted":true},"cell_type":"code","source":"def test_model(model, X, Y):\n    test_accuracy = tf.keras.metrics.Accuracy()\n    logits = model(X, training=False)\n    prediction = tf.argmax(logits, axis=1, output_type=tf.int32)\n    test_accuracy(prediction, Y)\n    return test_accuracy.result()\n\norg_model.load_weights(\"saved_models/standard/Ordonez2016DeepOriginal\")\norg_both = test_model(org_model, X_test, Y_test)\norg_accl = test_model(org_model, np.concatenate((X_test[:,:,:3,:],X_test[:,:,:3,:]),axis=2), Y_test)\norg_gyro = test_model(org_model, np.concatenate((X_test[:,:,3:,:],X_test[:,:,3:,:]),axis=2), Y_test)\nprint(\"Original Model Test Accuracy \\n Both: {:.4f} \\n Only Accl: {:.4f} \\n Only Gyro: {:.4f}\".format(org_both, org_accl, org_gyro))","execution_count":null,"outputs":[]},{"metadata":{"executionInfo":{"elapsed":1785,"status":"ok","timestamp":1596729839855,"user":{"displayName":"a cup of data","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GghBghr_Yn6r8UH5P2egK3SaSAe2gEQgbVSenKq=s64","userId":"12973428829118435411"},"user_tz":-60},"id":"ldLi9gZ1XlBl","outputId":"f51e1ae7-5d0c-4239-aebe-0a42aac20bf9","trusted":true},"cell_type":"code","source":"labels = [\"DANA: Only Accl\", \"DANA: Only Gyro\",\n         \"DANA: Both Sensors\"]\nmarkers = [\"-or\",\"--sb\",\"-.*g\"]\nfig, ax = plt.subplots(nrows=1, ncols=1, figsize=(20,12))\nfig.tight_layout()\n\n\nplt.plot(50,org_accl*100, \"^k\", label=\"Non-Adaptive Original: Only Accl\", markersize= 40, linewidth=5,\n          markerfacecolor=\"None\",  markeredgewidth=5)\nplt.plot(50,org_gyro*100, \">k\", label=\"Non-Adaptive Original: Only Gyro\", markersize= 40, linewidth=5,\n          markerfacecolor=\"None\",  markeredgewidth=5)\nplt.plot(50,org_both*100, \"vk\", label=\"Non-Adaptive Original: Both Sensors\", markersize= 40, linewidth=5,\n          markerfacecolor=\"None\",  markeredgewidth=5)\n\nfor i in range(len(H_combinations)-1):\n    y_data = accuracy_record[:,i]*100\n    x_data = np.floor(np.array(W_combinations)/(2.56))\n    \n    ax.plot(x_data, y_data, markers[i], label=labels[i], markersize= 40, linewidth=5,\n            markerfacecolor=\"None\",  markeredgewidth=3)    \n\n\nax.set_xticks(x_data)\nplt.setp(ax.get_xticklabels(), fontsize=35)    \nax.set_yticks(np.arange(0,101,10))\nplt.setp(ax.get_yticklabels(), fontsize=35)\n\nfig.text(0.4, -0.05, \"Sampling Rate (Hz)\", size = 40)\nfig.text(-0.05, 0.5, \"Classification Accuracy (%)\", va='center', rotation='vertical', size = 40)\nax.legend(loc= \"lower center\",prop={'size': 30}, ncol=2)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"OPXmKFe1qX94"},"cell_type":"markdown","source":"#### I hope you enjoyed it!. You can find more info. and examples in this link: https://github.com/mmalekzadeh/dana","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}