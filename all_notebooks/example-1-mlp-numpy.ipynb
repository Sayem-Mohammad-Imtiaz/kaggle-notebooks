{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# to do OHE labels\nfrom keras.utils import to_categorical\n# to show numbers\nimport matplotlib.pyplot as plt\n# the only python lib we really need\nimport numpy as np","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"# read data\nimport pandas as pd\nmnist_test = pd.read_csv(\"../input/mnist-in-csv/mnist_test.csv\")\nmnist_train = pd.read_csv(\"../input/mnist-in-csv/mnist_train.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mnist_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# do numpy arrays\nXtrain = mnist_train.drop(['label'], axis=1).values\nYtrain =  mnist_train.loc[:, 'label'].values\nXtest = mnist_test.drop(['label'], axis=1).values\nYtest =  mnist_test.loc[:, 'label'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(Xtrain.shape)\nprint(Ytrain.shape)\nprint(Xtest.shape)\nprint(Ytest.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"number_example = Xtrain[0].reshape(28, 28)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.imshow(number_example, cmap='gray')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# one hot encoded Y\nYtrain_ohe = to_categorical(Ytrain) \nYtest_ohe = to_categorical(Ytest) \nprint(Ytrain.shape)\nprint(Ytrain_ohe.shape)\nprint(Xtest.shape)\nprint(Ytest_ohe.shape)\nprint(Ytrain[0])\nprint(Ytrain_ohe[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>Network math</h>\n<br>\n$$\n\\begin{aligned}\nz^{(1)} & = xW^{(1)} + b^{(1)} \\\\\na^{(1)} & = \\tanh(z^{(1)}) \\\\\nz^{(2)} & = a^{(1)}W^{(2)} + b^{(2)} \\\\\na^{(2)} & = \\hat{y} = \\mathrm{softmax}(z^{(2)})\n\\end{aligned}\n$$"},{"metadata":{"trusted":true},"cell_type":"code","source":"# layers size\ninput_layer = 784\nhidden_layer = 100\noutput_layer = 10","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# initial weights and bias\nW1 = np.random.randn( input_layer, hidden_layer ) #W1\nb1 = np.random.randn( 1, hidden_layer ) #b1\nW2 = np.random.randn( hidden_layer, output_layer ) #W2\nb2 = np.random.randn( 1, output_layer ) #b2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"W1.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>Activations functions</h3>\n<br>\nhyperbolic tangent - tanh<br>\n$$\n\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}\n$$\n<br>\nSoftmax<br>\n$$\ns(x)_i = \\frac{e^{x_i}}{\\sum_{j=1}^{C} e^{x_j}}\n$$\n<br>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# activations functions\ndef tanh(x):\n    return np.tanh(x)\n\n# for tyhe last layer (output)\ndef softmax(x):\n    exp_scores = np.exp(x)\n    return exp_scores / np.sum(exp_scores, axis=1, keepdims=True) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# first layer\nZ1 = Xtrain.dot(W1) + b1\nprint(Z1.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# first layer after activation\nA1 = tanh(Z1)\nprint(A1.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# second layer\nZ2 = A1.dot(W2) + b2\nprint(Z2.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# second layer after activation - networkoutput (yhat)\nA2 = softmax(Z2)\nprint(A2.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# predict \npredictions = np.argmax(A2, axis=1)\nprint(predictions.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>Loss function used in multicalssifiaction</h3>\n<br>\nCross entropy<br>\n$$\nL(y,\\hat{y}) = -\\sum_{j=1}^{C} y_j\\log\\hat{y}_j\n$$\n<br>\nCross entropy loss<br>\n$$\nJ(W, b) = - \\frac{1}{N}\\sum_{j=1}^{C} L(y,\\hat{y}) = - \\frac{1}{N} \\sum_{i=1}^{n} \\sum_{j=1}^{C} y_{i,j} \\log\\hat{y}_{i,j}\n$$"},{"metadata":{"trusted":true},"cell_type":"code","source":"# forward propagation\ndef forward(X):\n    Z1 = X.dot(W1) + b1\n    A1 = tanh(Z1)\n    Z2 = A1.dot(W2) + b2\n    A2 = softmax(Z2)\n    return A2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def loss(y, X):\n    N = len(y)\n    yhat = forward(X)\n    logs = np.sum(np.log(yhat[range(N), y]))\n    return -1.0/N * logs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def loss_alt(y, X):\n    yhat = forward(X)\n    return - np.mean( np.log( yhat[ range(len(yhat)), y ] ) )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check our loss\nprint(loss(Ytrain, Xtrain))\nprint(loss_alt(Ytrain, Xtrain))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>Backpropagation and fitting<h3><br>\ntanh derivative<br><br>\n$$\n\\frac{\\mathrm d}{\\mathrm d x} \\tanh x = (1 - \\tanh^2x)\n$$\n<br>\nLoss fuction grandient for weights and bias computations:<br><br>\n$$\n\\begin{aligned}\n& \\delta^{(2)} = \\frac{\\hat{y} - y}{ m } \\\\\n\\end{aligned}\n$$\n<BR>\n$$\n\\begin{aligned}\n& \\delta^{(1)} = \\delta^{(2)}W^{(2)T} \\circ \\frac{\\mathrm d}{\\mathrm d x} \\tanh z^{(1)}  \\\\\n\\end{aligned}\n$$\n<BR>\n$$\n\\begin{aligned}\n& \\frac{\\partial{J}}{\\partial{W^{(2)}}} = a^{(1)T} \\delta^{(2)} \\hspace{10mm} \\frac{\\partial{J}}{\\partial{b^{(2)}}} = \\delta^{(2)}\\\\ \n\\end{aligned}\n$$\n<BR>\n$$\n\\begin{aligned}  \n& \\frac{\\partial{J}}{\\partial{W^{(1)}}} = x^T \\delta^{(1)} \\hspace{10mm} \\frac{\\partial{J}}{\\partial{b^{(1)}}} = \\delta^{(1)} \\\\\n\\end{aligned}\n$$"},{"metadata":{"trusted":true},"cell_type":"code","source":"# tanh derivative\ndef tanh_dev(x):\n    return 1.0-np.tanh(x)**2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# learning rate\nlearning_rate = 0.1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#backpropagation\n# delta 2\ndelta2 = (A2-Ytrain_ohe)/len(Ytrain)\n\n# to compute delta1 we need \ndZ1 = tanh_dev(Z1)\n\n# delta 1\ndelta1 = delta2.dot(W2.T) * dZ1\n\n# partial derivatives for weighs\ndev_W2 = A1.T.dot(delta2)\ndev_W1 = Xtrain.T.dot(delta1)\n\n# partial derivatives for bias\ndev_b2 = np.sum( delta2, axis=0, keepdims=True )\ndev_b1 = np.sum( delta1, axis=0, keepdims=True )\n\n# update waights and bias\nW1 -= (learning_rate * dev_W1)\nb1 -= (learning_rate * dev_b1)\nW2 -= (learning_rate * dev_W2)\nb2 -= (learning_rate * dev_b2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(loss(Ytrain, Xtrain))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}