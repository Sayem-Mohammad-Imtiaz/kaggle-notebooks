{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# NumPy\nimport numpy as np\n\n# Dataframe operations\nimport pandas as pd\n\n# Data visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Scalers\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.utils import shuffle\n\n# Models\nfrom sklearn.linear_model import LogisticRegression #logistic regression\nfrom sklearn.linear_model import Perceptron\nfrom sklearn import svm #support vector Machine\nfrom sklearn.ensemble import RandomForestClassifier #Random Forest\nfrom sklearn.neighbors import KNeighborsClassifier #KNN\nfrom sklearn.naive_bayes import GaussianNB #Naive bayes\nfrom sklearn.tree import DecisionTreeClassifier #Decision Tree\nfrom sklearn.model_selection import train_test_split #training and testing data split\nfrom sklearn import metrics #accuracy measure\nfrom sklearn.metrics import confusion_matrix #for confusion matrix\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.neural_network import MLPClassifier\n\n# Cross-validation\nfrom sklearn.model_selection import KFold #for K-fold cross validation\nfrom sklearn.model_selection import cross_val_score #score evaluation\nfrom sklearn.model_selection import cross_val_predict #prediction\nfrom sklearn.model_selection import cross_validate\n\n# GridSearchCV\nfrom sklearn.model_selection import GridSearchCV\n\n#Common Model Algorithms\nfrom sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\n\n#Common Model Helpers\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn import feature_selection\nfrom sklearn import model_selection\nfrom sklearn import metrics\n\n#Visualization\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport matplotlib.pylab as pylab\nimport seaborn as sns\n#from pandas.tools.plotting import scatter_matrix\nfrom pandas import plotting ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(\"../input/titanic/train.csv\")\ntest_df = pd.read_csv(\"../input/titanic/test.csv\")\ndata_df = train_df.append(test_df) # The entire data: train + test.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_df['Title'] = data_df['Name']\n# Cleaning name and extracting Title\nfor name_string in data_df['Name']:\n    data_df['Title'] = data_df['Name'].str.extract('([A-Za-z]+)\\.', expand=True)\n\n# Replacing rare titles with more common ones\nmapping = {'Mlle': 'Miss', 'Major': 'Mr', 'Col': 'Mr', 'Sir': 'Mr', 'Don': 'Mr', 'Mme': 'Miss',\n          'Jonkheer': 'Mr', 'Lady': 'Mrs', 'Capt': 'Mr', 'Countess': 'Mrs', 'Ms': 'Miss', 'Dona': 'Mrs'}\ndata_df.replace({'Title': mapping}, inplace=True)\ntitles = ['Dr', 'Master', 'Miss', 'Mr', 'Mrs', 'Rev']\nfor title in titles:\n    age_to_impute = data_df.groupby('Title')['Age'].median()[titles.index(title)]\n    data_df.loc[(data_df['Age'].isnull()) & (data_df['Title'] == title), 'Age'] = age_to_impute\n    \n# Substituting Age values in TRAIN_DF and TEST_DF:\ntrain_df['Age'] = data_df['Age'][:891]\ntest_df['Age'] = data_df['Age'][891:]\n\n# Dropping Title feature\ndata_df.drop('Title', axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_df['Family_Size'] = data_df['Parch'] + data_df['SibSp']\n\n# Substituting Age values in TRAIN_DF and TEST_DF:\ntrain_df['Family_Size'] = data_df['Family_Size'][:891]\ntest_df['Family_Size'] = data_df['Family_Size'][891:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_df['Last_Name'] = data_df['Name'].apply(lambda x: str.split(x, \",\")[0])\ndata_df['Fare'].fillna(data_df['Fare'].mean(), inplace=True)\n\nDEFAULT_SURVIVAL_VALUE = 0.5\ndata_df['Family_Survival'] = DEFAULT_SURVIVAL_VALUE\n\nfor grp, grp_df in data_df[['Survived','Name', 'Last_Name', 'Fare', 'Ticket', 'PassengerId',\n                           'SibSp', 'Parch', 'Age', 'Cabin']].groupby(['Last_Name', 'Fare']):\n    \n    if (len(grp_df) != 1):\n        # A Family group is found.\n        for ind, row in grp_df.iterrows():\n            smax = grp_df.drop(ind)['Survived'].max()\n            smin = grp_df.drop(ind)['Survived'].min()\n            passID = row['PassengerId']\n            if (smax == 1.0):\n                data_df.loc[data_df['PassengerId'] == passID, 'Family_Survival'] = 1\n            elif (smin==0.0):\n                data_df.loc[data_df['PassengerId'] == passID, 'Family_Survival'] = 0\n\nprint(\"Number of passengers with family survival information:\", \n      data_df.loc[data_df['Family_Survival']!=0.5].shape[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for _, grp_df in data_df.groupby('Ticket'):\n    if (len(grp_df) != 1):\n        for ind, row in grp_df.iterrows():\n            if (row['Family_Survival'] == 0) | (row['Family_Survival']== 0.5):\n                smax = grp_df.drop(ind)['Survived'].max()\n                smin = grp_df.drop(ind)['Survived'].min()\n                passID = row['PassengerId']\n                if (smax == 1.0):\n                    data_df.loc[data_df['PassengerId'] == passID, 'Family_Survival'] = 1\n                elif (smin==0.0):\n                    data_df.loc[data_df['PassengerId'] == passID, 'Family_Survival'] = 0\n                        \nprint(\"Number of passenger with family/group survival information: \" \n      +str(data_df[data_df['Family_Survival']!=0.5].shape[0]))\n\n# # Family_Survival in TRAIN_DF and TEST_DF:\ntrain_df['Family_Survival'] = data_df['Family_Survival'][:891]\ntest_df['Family_Survival'] = data_df['Family_Survival'][891:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_df['Fare'].fillna(data_df['Fare'].median(), inplace = True)\n\n# Making Bins\ndata_df['FareBin'] = pd.qcut(data_df['Fare'], 5)\n\nlabel = LabelEncoder()\ndata_df['FareBin_Code'] = label.fit_transform(data_df['FareBin'])\n\ntrain_df['FareBin_Code'] = data_df['FareBin_Code'][:891]\ntest_df['FareBin_Code'] = data_df['FareBin_Code'][891:]\n\ntrain_df.drop(['Fare'], 1, inplace=True)\ntest_df.drop(['Fare'], 1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_df['Embarked'].fillna('s', inplace = True)\nlabel = LabelEncoder()\ndata_df['Embarked_Code'] = label.fit_transform(data_df['Embarked'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_df.head(50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['Embarked_Code'] = data_df['Embarked_Code'][:891]\ntest_df['Embarked_Code'] = data_df['Embarked_Code'][891:]\n\ntrain_df.drop(['Embarked'], 1, inplace=True)\ntest_df.drop(['Embarked'], 1, inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_df['AgeBin'] = pd.qcut(data_df['Age'], 4)\n\nlabel = LabelEncoder()\ndata_df['AgeBin_Code'] = label.fit_transform(data_df['AgeBin'])\n\ntrain_df['AgeBin_Code'] = data_df['AgeBin_Code'][:891]\ntest_df['AgeBin_Code'] = data_df['AgeBin_Code'][891:]\n\ntrain_df.drop(['Age'], 1, inplace=True)\ntest_df.drop(['Age'], 1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['Sex'].replace(['male','female'],[0,1],inplace=True)\ntest_df['Sex'].replace(['male','female'],[0,1],inplace=True)\n\n#train_df.drop(['Name', 'PassengerId', 'SibSp', 'Parch', 'Ticket', 'Cabin',\n#               'Embarked'], axis = 1, inplace = True)\n#test_df.drop(['Name','PassengerId', 'SibSp', 'Parch', 'Ticket', 'Cabin',\n#              'Embarked'], axis = 1, inplace = True)\ntrain_df.drop(['Name', 'PassengerId', 'SibSp', 'Parch', 'Ticket', 'Cabin'], axis = 1, inplace = True)\ntest_df.drop(['Name','PassengerId', 'SibSp', 'Parch', 'Ticket', 'Cabin'], axis = 1, inplace = True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head(3)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.tail(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#X = train_df.drop('Survived', 1)\n#y = train_df['Survived']\n#X_test = test_df.copy()\ny = train_df.pop('Survived')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#light GBM\nimport lightgbm as lgbm\n# Take a hold out set randomly\nX_train, X_test, y_train, y_test = train_test_split(train_df, y, test_size=0.2, random_state=42)\n# Create an LGBM dataset for training\ncategorical_features = ['Pclass', 'Family_Size','Family_Survival', 'FareBin_Code','Embarked_Code']\ntrain_data = lgbm.Dataset(data=X_train, label=y_train, categorical_feature=categorical_features, free_raw_data=False)\n# Create an LGBM dataset from the test\ntest_data = lgbm.Dataset(data=X_test, label=y_test, categorical_feature=categorical_features, free_raw_data=False)\n# Finally, create a dataset for the FULL training data to give us maximum amount of data to train on after \n# performance has been calibrate\nfinal_train_set = lgbm.Dataset(data=train_df, label=y, \n                               categorical_feature=categorical_features, free_raw_data=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgbm_params = {\n    'boosting': 'dart',          # dart (drop out trees) often performs better\n    'application': 'binary',     # Binary classification\n    'learning_rate': 0.1,       # Learning rate, controls size of a gradient descent step\n    'min_data_in_leaf': 40,      # Data set is quite small so reduce this a bit\n    'feature_fraction': 0.8,     # Proportion of features in each boost, controls overfitting\n    'num_leaves': 41,            # Controls size of tree since LGBM uses leaf wise splits\n    'metric': 'binary_logloss',  # Area under ROC curve as the evaulation metric\n    'drop_rate': 0.0\n              }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"evaluation_results = {}\nclf = lgbm.train(train_set=train_data,\n                 params=lgbm_params,\n                 valid_sets=[train_data, test_data], \n                 valid_names=['Train', 'Test'],\n                 evals_result=evaluation_results,\n                 num_boost_round=2000,\n                 early_stopping_rounds=10,\n                 verbose_eval=20\n                )\noptimum_boost_rounds = clf.best_iteration","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axs = plt.subplots(1, 2, figsize=[15, 4])\n\n# Plot the log loss during training\naxs[0].plot(evaluation_results['Train']['binary_logloss'], label='Train')\naxs[0].plot(evaluation_results['Test']['binary_logloss'], label='Test')\naxs[0].set_ylabel('Log loss')\naxs[0].set_xlabel('Boosting round')\naxs[0].set_title('Training performance')\naxs[0].legend()\n\n# Plot feature importance\nimportances = pd.DataFrame({'features': clf.feature_name(), \n                            'importance': clf.feature_importance()}).sort_values('importance', ascending=False)\naxs[1].bar(x=np.arange(len(importances)), height=importances['importance'])\naxs[1].set_xticks(np.arange(len(importances)))\naxs[1].set_xticklabels(importances['features'])\naxs[1].set_ylabel('Feature importance (# times used to split)')\naxs[1].set_title('Feature importance')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = np.round(clf.predict(X_test))\nprint('Accuracy score = \\t {}'.format(accuracy_score(y_test, preds)))\nprint('Precision score = \\t {}'.format(precision_score(y_test, preds)))\nprint('Recall score =   \\t {}'.format(recall_score(y_test, preds)))\nprint('F1 score =      \\t {}'.format(f1_score(y_test, preds)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_final = lgbm.train(train_set=final_train_set,\n                      params=lgbm_params,\n                      #num_boost_round=optimum_boost_rounds,\n                      verbose_eval=0\n                      )\n\ny_pred = np.round(clf_final.predict(test_df)).astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp = pd.DataFrame(pd.read_csv(\"../input/titanic/test.csv\")['PassengerId'])\ntemp['Survived'] = pd.DataFrame({'Survived': y_pred})\ntemp.to_csv(\"../working/submission-lgbm4.csv\", index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_passenger_ids = test_df.pop('PassengerId')\noutput_df = pd.DataFrame({'PassengerId': test_passenger_ids, 'Survived': y_pred})\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"std_scaler = StandardScaler()\nX = std_scaler.fit_transform(X)\nX_test = std_scaler.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_neighbors = [6,7,8,9,10,11,12,14,16,18,20,22]\n#n_neighbors = [16,17,18,19,20]#18\nalgorithm = ['auto']\nweights = ['uniform', 'distance']\nleaf_size = list(range(1,50,5)) #26\n#leaf_size = [20,21,22,23,24,25,26,27,28,29,30]\n\nhyperparams = {'algorithm': algorithm, 'weights': weights, 'leaf_size': leaf_size, \n               'n_neighbors': n_neighbors}\n\ngd=GridSearchCV(estimator = KNeighborsClassifier(), param_grid = hyperparams, verbose=True, \n                cv=10, scoring = \"roc_auc\")\ngd.fit(X, y)\nprint(gd.best_score_)\nprint(gd.best_estimator_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gd.best_estimator_.fit(X, y)\ny_pred = gd.best_estimator_.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#knn = KNeighborsClassifier(algorithm='auto', leaf_size=26, metric='minkowski', \nknn = KNeighborsClassifier(algorithm='brute', leaf_size=26, metric='minkowski', \n                           metric_params=None, n_jobs=1, n_neighbors=18, p=3, \n                           weights='uniform')\nknn.fit(X, y)\ny_pred = knn.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp = pd.DataFrame(pd.read_csv(\"../input/titanic/test.csv\")['PassengerId'])\ntemp['Survived'] = y_pred\ntemp.to_csv(\"../working/submission-p3.csv\", index = False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}