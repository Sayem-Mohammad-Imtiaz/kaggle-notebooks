{"cells":[{"metadata":{"id":"lrenm-nk5nhb"},"cell_type":"markdown","source":"# Data Mining Term Project - Board Game Geek Rating Prediction¶\n"},{"metadata":{"id":"-ZFH348g51Yl"},"cell_type":"markdown","source":"# **Abhishek Shinde**\n# **Student ID - 1001754842**\n\n"},{"metadata":{},"cell_type":"markdown","source":"# **MY LINKS**\n\n## **Link for my live prediction:  https://myridgeclassifierlive.herokuapp.com/**\n\n## **Link for my GitHub : https://github.com/Warlord3097/My-Ridge-Classifier**\n\n## **Link to the dataset used : https://www.kaggle.com/jvanelteren/boardgamegeek-reviews**\n"},{"metadata":{"id":"oPtik9z5Bs-6"},"cell_type":"markdown","source":"# Reference Links\n1. https://monkeylearn.com/text-classification/\n2. https://www.geeksforgeeks.org/applying-multinomial-naive-bayes-to-nlp-problems/\n3. https://github.com/jushih/Sentiment-Analysis\n4. http://www.site.uottawa.ca/~stan/csi5387/DMNB-paper.pdf\n5. https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html\n6. https://scikit-learn.org/stable/modules/svm.html\n7. https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeClassifier.html\n"},{"metadata":{"id":"96OwfemC6Gsl"},"cell_type":"markdown","source":"This is the term project for my Data Mining Class.\n\nDue to the large size of the data set, I have divided the dataset into different smaller subsets and have developed my model on those subsets of data as my laptop was not able to handle the computations that came along with the big dataset."},{"metadata":{"id":"7a-oe9xz6POG"},"cell_type":"markdown","source":"## Overview\n"},{"metadata":{"id":"xGQtmGml6Pgn"},"cell_type":"markdown","source":"Text Classification can be defined as the simple process of assigning tags or categorising data according to its content. One of the most crutial tasks  in this process is Natual Language Processing (NLP) which is widely used in Sentiment Analysis, spam detection and intent detection.\n\n\nIn today's day and age, there is an abundance of data. Data has become an economy and the more smartly one can use it, the more successful they become. This data can be of any format, either textual, numerical, images, etc. Out of these, text data is often considered to be the most rich source of information.\n\nBut it is equally challenging and and time consuming to extract valuable information out of all text data. All major businesses are in need of smart decision making algorithms which can help them predict the information needed for them to make a profit for themselves.\n\nWhich is why, I am implementing this project as a prime example of how textual data can be, how we perform NLP and other operations, and using different models to predict the accuracy. \nAnd to also find which model is having better accuracy among the others. \n\nWe shall also cover what improvements can be made to the existing logic so as to improve our data prediction accuracy\n"},{"metadata":{"id":"oG6zbm1d6PdJ"},"cell_type":"markdown","source":"# Purpose of this project\n\nThe main and foremost objective of this project is to predict the rating for a game, given a textual review.\n\nOther objectives include understanding the mechanisms of text cleaning and NLP. \n\nAlso to understand the working of different classification models.\n\nAmother objective is to develop our presentation skills and documentation skills.\n"},{"metadata":{"id":"wzUXwmEsI-0V"},"cell_type":"markdown","source":"# **So without any further delay, lets begin**"},{"metadata":{"id":"NOBzFZlbJ_SZ"},"cell_type":"markdown","source":"***An understanding of our dataset***\n\nThe original Board Game Dataset is a csv file which consists of 1,31,70,703 rows of data. \n\nThis data is a combination of Review, Rating, Users, ID and Game Name.\n\nFor the purpose of our project, since our goal is to predict the **Rating** using the given **Review** we will not focus on any other data.\n\n\n"},{"metadata":{"id":"_XlLUU7aJ_Mi"},"cell_type":"markdown","source":"**Note:-** As it is very difficult and out of my system's capabilities to work on such a large dataset at once, we will be dividing our dataset into smaller samples.\n\nThey will be explained below."},{"metadata":{"id":"CHzHfRpAJ_HI"},"cell_type":"markdown","source":"I have used 3 models for comparison and prediction of our data. They are : \n\n# **1. Multinomial Naive Bayes Classifier**\n\nThe Naive Bayes classifier is a very straightforward probabilistic classifier which has been based on the Bayes Theorem. This classifier assumes strong and very naive independence assumptions.\n\nBayes theorem calculates probability P(c|x) where c is the class of the possible outcomes and x is the given instance which has to be classified, representing some certain features.\n\n**P(c|x) = P(x|c) * P(c) / P(x)**\n\n\nNaive Bayes is one of the most common and basic text classification techniques with various applications in Spam Detection, Disaster Tweet Detection, Sentiment Analysis etc. \n\nWorking:\nWe are given a dataset and we have 11 possible classifications, i.e. The rating of our review can lie between 0 and 10.\n\nTherefore, for a given review,\nwe first perform various NLP operations such as standardizing the text by converting it all to lower case, removing punctuations, numerical data, special characters, hyperlinks and html tags.\n\nFor a better cleaning, we use the NLTK library to remove all the stopwords present in our data.\n\nStopwords are common words such as \"it, is, able, else, and, that\" ,etc.\n\nAfter performing all these text cleaning operations,\nwe tokenize the words and to find the probability of every word for every rating, starting from 0 to 10.\n\nWe use CountVectorizer. It provides a simple and efficient method to tokenize a collection of text documents and to build a vocabulary of known words, while encoding the new documents using that vocabulary.\n\nThe CountVectorizer returns a matrix of words and their corresponding occurrences in our dataset, i.e for a word \"game\" it would also display how many times the word \"game\" has occurred in the dataset.\n\n\nWe use TfIdfTransformer. It returns a normalized tf-idf representation for a count matrix. It is a common weighing scheme in information retrieval. The goal of using Tf-Idf  instead of the raw frequencies of occurrence of a token in a given document is to scale down the impact of tokens that occur very frequently in a given corpus and that are hence empirically less informative than features that occur in a small fraction of the training corpus. Tf–idf, short for term frequency–inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus.\n\nAfter we get our collection or corpus of words, and have identified their numerical count in all ratings, we use the conditional probability formula of Naive Bayes to Calculate the Probability of a Review by multiplying the probabilities of all the words in the review.\n\nThe benefit of using Multinomial Naive-Bayes Classifier is that it can efficiently make predictions when there are multiple possibilities for a class prediction.\n\n\n\n\n\n# 2. Linear SVM using SGD Training\nThe SGD Classifier is a Linear Classifier for SVM or Logistic Regression with a Stochastic Gradient Descent Training ( SGD Training )\nThis trainer helps in implementing regularized linear models with a stochastic gradient descent learning. \n\nThis implementation works with the data represented as dense or sparse arrays of floating point values for the features. The model it fits can be controlled with the loss parameter, while by default, it fits a Linear Support Vector machine (Linear SVM).\n\nSVM or Support Vector Machine is a linear model for classification and regression problems. It can solve linear and non-linear problems and work well for many practical problems. The idea of SVM is simple: The algorithm creates a line or a hyperplane which separates the data into classes.  SVM is an algorithm that takes the data as an input and outputs a line that separates those classes if possible. **A hyperplane in an n-dimensional Euclidean space is a flat, n-1 dimensional subset of that space that divides the space into two disconnected parts**. \n\n# 3. Ridge Classifier\nThis classifier uses Ridge Regression for its implementation. This classifier first converts the target values into {-1,1} and then treats the problem as a regression task. It performs a multi-output regression in our case as we have multiple classes to predict.\n"},{"metadata":{"id":"KseixJB2I-tt"},"cell_type":"markdown","source":"This documentation will explain every code block in a detailed and concise manner"},{"metadata":{"id":"q4wX88pSI-oq"},"cell_type":"markdown","source":"## **Now that we have a better understanding of the different models used in this project, lets start with the programming part of our project**"},{"metadata":{"id":"Zlsq94xAWOvj"},"cell_type":"markdown","source":"**Importing Libraries**\n\nThis is where we import all the important libraries which will be needed by us later in the program. \n\nThere is no compulsion to have all your import statements together in one block. It is simply a personal preference if you would like to keep all your import statements together or if you would like to keep them wherever."},{"metadata":{"id":"TIsAyZbApWys","trusted":true},"cell_type":"code","source":"import math\nimport pandas as pd\nimport numpy as np\nimport random as rnd\nimport matplotlib.pyplot as plt\nimport time\nimport pickle\nimport string\nimport sys\nimport nltk\nfrom nltk.stem import PorterStemmer\nfrom nltk.tokenize import word_tokenize\nimport re\nimport os\nfrom sklearn.metrics import accuracy_score\nfrom nltk.corpus import stopwords\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import RidgeClassifier\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.metrics import accuracy_score","execution_count":null,"outputs":[]},{"metadata":{"id":"ynmxYJDs-jBM"},"cell_type":"markdown","source":"**Reading Dataset File**\n\nHere, after mounting our drive and importing our essential libraries, we read our dataset from our mounted google drive using Pandas, as we want to read the data into our dataframe."},{"metadata":{"id":"XD0wLSqIW8RQ"},"cell_type":"markdown","source":"We use the .head() function to display the top 5 records of our dataset. \n\nAs you can see below, we have 5 rows of data, and the column names are also available."},{"metadata":{"id":"JGncT4PloAmH","outputId":"e8bc6d95-5397-4b83-b090-4d69d7c95bdf","trusted":true},"cell_type":"code","source":"# df = pd.read_csv('/content/drive/My Drive/Colab_Data_set/bgg-13m-reviews.csv')\n# df.head()\n\ndf =  pd.read_csv('../input/boardgamegeek-reviews/bgg-13m-reviews.csv', index_col=0)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"ycDtePkdXKvy"},"cell_type":"markdown","source":"As you know, the scope of our project is to predict the rating, given a review. Therefore, we are only concerned with the columns \"rating\" and \"comment\". \n\nTherefore, we drop the rest of the columns as they would not fit any purpose for our data analysis.\nHence, here, we drop the \"ID\",\"user\",\"name\" below."},{"metadata":{"id":"6A5btEuz-t6s"},"cell_type":"markdown","source":"Dropping Column \"ID\", \"user\", \"name\""},{"metadata":{"id":"LG-dadpjLGlb","trusted":true},"cell_type":"code","source":"dataset = df.drop(columns=\"ID\")\ndataset = dataset.drop(columns=\"user\")\ndataset = dataset.drop(columns=\"name\")\n\n","execution_count":null,"outputs":[]},{"metadata":{"id":"JNNRLmVo_C0m"},"cell_type":"markdown","source":"**Current DataSet**\n\nHere, we can see the modified dataset with only the review and rating column, as desired."},{"metadata":{"id":"aexh1kZw_E-o","outputId":"a680837b-a1a2-4d6b-ed27-607c9c9cd4ac","trusted":true},"cell_type":"code","source":"dataset.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"oWIRLTqv-3ow"},"cell_type":"markdown","source":"**Removing all the comments which are empty**\n\nOur dataset consists of 13 million reviews. But many of them(approx. 80%) of them contain empty comments. We can see those empty comments as \"NaN\" in the \"comment\" column above.\n\nSince this data is also not essential to us and we wont be able to use it to perform any sort of analysis or prediction,we delete all the rows which have no column value."},{"metadata":{"id":"yJ4t_Yfg_ApG","trusted":true},"cell_type":"code","source":"#Removing Empty Comments\ndataset = dataset.dropna(subset=['comment'])","execution_count":null,"outputs":[]},{"metadata":{"id":"nbDrxRTx_P9e"},"cell_type":"markdown","source":"**Dataset after removing empty comments**\n\nTherefore, after removing all the comments with a \"NaN\" value, we get the following dataset."},{"metadata":{"id":"r29xZLvJ_UH_","outputId":"0002af1e-b9f7-42bd-e5ef-5e653f9a7e38","trusted":true},"cell_type":"code","source":"dataset.head()\n","execution_count":null,"outputs":[]},{"metadata":{"id":"EMTBRpUqZfLv"},"cell_type":"markdown","source":"For better exploration of data, this histogram displays the distribution of all the ratings in our dataset againt the number of reviews for each rating."},{"metadata":{"id":"tMFwxG5GZt5Z","outputId":"6941a8f4-22b5-46af-c0c8-e1a1e5d06122","trusted":true},"cell_type":"code","source":"#plot histogram of ratings\nnum_bins = 500\nplt.hist(dataset.rating, num_bins, facecolor='blue', alpha=10)\n\n#plt.xticks(range(9000))\nplt.title('Histogram of Ratings')\nplt.xlabel('Ratings')\nplt.ylabel('Count')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"Y6IlRn9taCFU"},"cell_type":"markdown","source":"According to the above histogram, we can see that our dataset is heavily unbalanced, i.e. it contains a large majority of reviews which have a sentimentally positive rating that lies between 6 and 8. "},{"metadata":{"id":"jgldzkOm_aTI"},"cell_type":"markdown","source":"\n# **Splitting the Dataset into Train and Test Subsets**\nSince the Total Dataset is still huge, we split it into Train And Test Set in a 75:25 ratio.\n\nFor this operation, we use train_test_split library from Sklearn package.\n\nThe train_test_split function is for splitting a single dataset for two different purposes: training and testing. The training subset is for building your model. The testing subset is for using the model on unknown data to evaluate the performance of the model. \nThis function makes random partitions for the two subsets."},{"metadata":{"id":"JbjEmBr5OcM8","trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ntrain,test = train_test_split(dataset,test_size = 0.25,random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"id":"ANZWTaQK_jaX"},"cell_type":"markdown","source":"Train Set"},{"metadata":{"id":"NuqBAA_N_lKg","outputId":"27fa48a2-07a5-4462-a5f6-86a018de844c","trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"zh91Qk1Gbwz8"},"cell_type":"markdown","source":"Our Train Set has a total of 1978317 records"},{"metadata":{"id":"rPaFb4dKb2mV"},"cell_type":"markdown","source":"Our Test Set has a total of 659439 records"},{"metadata":{"id":"fsMFDAz4b9oN"},"cell_type":"markdown","source":"We make a copy of our training set as we are going to make two samples of our training dataset and make development sets from these samples for calculating our model accuracy and performance."},{"metadata":{"id":"mfK2dwlBOxIa","trusted":true},"cell_type":"code","source":"temp_train= train.copy()","execution_count":null,"outputs":[]},{"metadata":{"id":"Yp8C0UqjAUux"},"cell_type":"markdown","source":"The Maximum value of ratings : "},{"metadata":{"id":"Is_HrmCQZKl3","outputId":"a6d58de7-3957-4836-a771-2ce7b90c017b","trusted":true},"cell_type":"code","source":"temp_train['rating'].max()","execution_count":null,"outputs":[]},{"metadata":{"id":"_Vh5s-76AbVe"},"cell_type":"markdown","source":"The Minimum value of ratings :"},{"metadata":{"id":"GO7Ri3ggAMn-","outputId":"98d218ab-fcd0-415d-ba22-576e9479602f","trusted":true},"cell_type":"code","source":"temp_train['rating'].min()","execution_count":null,"outputs":[]},{"metadata":{"id":"poFPW6gYt2E0","outputId":"c5cd8eac-d465-418d-ac11-f3fb8076758d","trusted":true},"cell_type":"code","source":"temp_train.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"lVlCpd1kO9iH"},"cell_type":"markdown","source":"# **For Ease of Computation and High Performance, we further divide our training dataset into two samples**\n\nHere, we split the training set into 2 Sample sets.\n\nThese sample sets each have 50 % of the Main Training Set"},{"metadata":{"id":"j3s7FtecO6rj","trusted":true},"cell_type":"code","source":"sample_1,sample_2 = train_test_split(temp_train,test_size=0.5,random_state = 2)","execution_count":null,"outputs":[]},{"metadata":{"id":"GeVoyDSAA_ne"},"cell_type":"markdown","source":"Sample Set 1 :"},{"metadata":{"id":"ipdKBLMrPF4e","outputId":"161b27ab-6fc6-43a1-f427-5f2372ef6ba9","trusted":true},"cell_type":"code","source":"sample_1.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"kbbCjiQXBF0Z"},"cell_type":"markdown","source":"Sample Set 2 :"},{"metadata":{"id":"n06Lr1w4AwxT","outputId":"25ba3491-1b7e-4fd7-88bc-6635f70f0f22","trusted":true},"cell_type":"code","source":"sample_2.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"C1D--zVtoOTV"},"cell_type":"markdown","source":"# **We now Start Cleaning Our Samples so that we can then use the refined dataset to fit into our models**"},{"metadata":{"id":"ex4LyRyJcgY9"},"cell_type":"markdown","source":"In the cleaning process for our text data, we:\n1. Remove all Punctuations that are present in our text data.\n2. Convert all text data into a standardized LowerCase Text.\n3. Removing all the Stopwords from our text data.\n\nStopwords are a set of commonly used words, irrespective of the language. The main reason for removing Stopwords from our text data is so that if we remove the common words, we will be able to focus on the important words instead."},{"metadata":{"id":"Gj_DzYNEdkWo"},"cell_type":"markdown","source":"To import the list of stopwords which we can use to remove them easily, we need to use nltk and download it to our system one time so that we can perform text cleaning without errors."},{"metadata":{"id":"P6j4KtblpENT"},"cell_type":"markdown","source":"Cleaning Sample Set 1"},{"metadata":{"id":"eHXiDevdpGPS","outputId":"13182cac-0e04-498c-d911-90e23c9cc7c4","trusted":true,"collapsed":true},"cell_type":"code","source":"#lowercase and remove punctuation\nsample_1['comment'] = sample_1['comment'].str.lower().apply(lambda x:''.join([i for i in x if i not in string.punctuation]))\n\n# stopword list to use\nstopwords_list = stopwords.words('english')\n\n\"\"\"\nSince this is a game dataset review, and since i ran the data cleaning process once before,\ni identified a few extra words which can also be added to our stopword list\n\n\"\"\"\nstopwords_list.extend(('game','play','played','players','player','people','really','board','games','one','plays','cards','would')) \n#remove stopwords\nsample_1['comment'] = sample_1['comment'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stopwords_list)]))\n","execution_count":null,"outputs":[]},{"metadata":{"id":"f0zZLlm_eNUf"},"cell_type":"markdown","source":"Here we see that 'sample_1' has been cleaned and has been converted to lowercase. \n\nIt also does not have any special characters or any punctuations."},{"metadata":{"id":"XoUrhBQ2tTLU","outputId":"7dd88c5e-ead0-449c-fa5a-0eae1a21b168","trusted":true},"cell_type":"code","source":"sample_1.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"ZfnR8S_7ecN8"},"cell_type":"markdown","source":"Cleaning Sample Set 2"},{"metadata":{"id":"Rsfukip8vDsQ","outputId":"5f8fbe5f-10f1-4063-cc35-2e1d7900ac76","trusted":true},"cell_type":"code","source":"#lowercase and remove punctuation\nsample_2['comment'] = sample_2['comment'].str.lower().apply(lambda x:''.join([i for i in x if i not in string.punctuation]))\n\n#remove stopwords\nsample_2['comment'] = sample_2['comment'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stopwords_list)]))\n","execution_count":null,"outputs":[]},{"metadata":{"id":"7YXB2CsTegYy"},"cell_type":"markdown","source":"Here is the cleaned 'sample_2'"},{"metadata":{"id":"9WDIOgyNpoht","outputId":"8c79296d-5e52-4cc2-cd64-eb2d5de9b319","trusted":true},"cell_type":"code","source":"sample_2.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"1Bgntee_PPci"},"cell_type":"markdown","source":"We now divide each sample into a train and a development set so that we can test the accuracy of our models\n\nWe divide the sample set into a ratio of 80:20"},{"metadata":{"id":"1Mvin66LPJAF","trusted":true},"cell_type":"code","source":"#Splitting Sample 1 \nsample_1_train, sample_1_dev = train_test_split(sample_1,test_size=0.2,random_state = 0)\n\n#Splitting Sample 2\nsample_2_train, sample_2_dev = train_test_split(sample_2,test_size=0.2,random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{"id":"4zqAHsb4CNqZ"},"cell_type":"markdown","source":"Train Set for Sample 1"},{"metadata":{"id":"6MGLOZ6BPUHS","outputId":"9ced511d-362e-406b-90d3-d39df8cfbbb4","trusted":true},"cell_type":"code","source":"sample_1_train.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"22i1sjufCeK1"},"cell_type":"markdown","source":"Development Set for Sample 1"},{"metadata":{"id":"OcP1gr1UCg9m","outputId":"5aa3fa0a-12a9-4636-d772-e1c75d1039a8","trusted":true},"cell_type":"code","source":"sample_1_dev.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"rlUtvdjmCizm"},"cell_type":"markdown","source":"Train Set for Sample 2"},{"metadata":{"id":"eik0ju30CnXY","outputId":"795fc3bd-c13f-4a74-9aa3-0428750f1dfc","trusted":true},"cell_type":"code","source":"sample_2_train.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"RzkZJgn0CtVS"},"cell_type":"markdown","source":"Development Set for Sample 2"},{"metadata":{"id":"uGR6Io32CwQo","outputId":"e4b0bc4b-a45c-4912-a615-f84bf1bac946","trusted":true},"cell_type":"code","source":"sample_2_dev.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"qnfR6fZVDfMf"},"cell_type":"markdown","source":"## **We make x and y train sets for both our samples**"},{"metadata":{"id":"3Xr4V_jmhEDa"},"cell_type":"markdown","source":"Creating Train X, Train Y ,  Dev X and Dev Y for Sample 1"},{"metadata":{"id":"Z8jk2_UjPYXY","trusted":true},"cell_type":"code","source":"#Training X and Y for Sample 1\ntrain_1x=[]\nfor i in sample_1_train['comment']:\n    train_1x.append(i)\n\ntrain_1y=[]\nfor i in sample_1_train['rating']:\n    train_1y.append(i)\n\ndev_1x=[]\nfor i in sample_1_dev['comment']:\n    dev_1x.append(i)\n\ndev_1y=[]\nfor i in sample_1_dev['rating']:\n    dev_1y.append(i)","execution_count":null,"outputs":[]},{"metadata":{"id":"VvOGYLWwhRqt"},"cell_type":"markdown","source":"Creating Train X, Train Y ,  Dev X and Dev Y for Sample 2"},{"metadata":{"id":"UWA9xpnTEl3D","trusted":true},"cell_type":"code","source":"#Training X and Y for Sample 2\ntrain_2x=[]\nfor i in sample_2_train['comment']:\n    train_2x.append(i)\n\ntrain_2y=[]\nfor i in sample_2_train['rating']:\n    train_2y.append(i)\n\ndev_2x=[]\nfor i in sample_2_dev['comment']:\n    dev_2x.append(i)\n\ndev_2y=[]\nfor i in sample_2_dev['rating']:\n    dev_2y.append(i)","execution_count":null,"outputs":[]},{"metadata":{"id":"7r5YDuRrEw2o"},"cell_type":"markdown","source":"Therefore, our current Train and Development Sets for Sample 1 (first 5 records)  are:"},{"metadata":{"id":"Ndpr5QBeCvtg","outputId":"31bf2937-5d25-45f5-aa7e-b951c0b5c7c9","trusted":true},"cell_type":"code","source":"train_1x[:5]","execution_count":null,"outputs":[]},{"metadata":{"id":"suUAU2AoFMSu","outputId":"8b7a1457-0def-4338-cd2d-19993a476820","trusted":true},"cell_type":"code","source":"train_1y[:5]","execution_count":null,"outputs":[]},{"metadata":{"id":"jbIpKxJVFPhZ","outputId":"3cf210b8-3dd9-4ad7-874e-5a2c13b0f5a6","trusted":true},"cell_type":"code","source":"dev_1x[:5]","execution_count":null,"outputs":[]},{"metadata":{"id":"i7f6lEveFPoL","outputId":"0f9c9f31-1be9-4765-ffa1-b7559b319f89","trusted":true},"cell_type":"code","source":"dev_1y[:5]","execution_count":null,"outputs":[]},{"metadata":{"id":"pt0PQdjQFk_k"},"cell_type":"markdown","source":"And our current Train and Development Sets for Sample 2 (first 5 records) are:"},{"metadata":{"id":"OmUf_0eWFqg8","outputId":"5b4c7e39-3973-4f42-ec76-8a5ae6ba144d","trusted":true},"cell_type":"code","source":"train_2x[:5]","execution_count":null,"outputs":[]},{"metadata":{"id":"HiQGk0rbFqj7","outputId":"7b5fd652-4981-45d8-e01f-4e7b704c0d5b","trusted":true},"cell_type":"code","source":"train_2y[:5]","execution_count":null,"outputs":[]},{"metadata":{"id":"J7QuwTpyFqsB","outputId":"0814024c-29ea-40e7-c429-65eb7a43abf0","trusted":true},"cell_type":"code","source":"dev_2x[:5]","execution_count":null,"outputs":[]},{"metadata":{"id":"7hEw1TAdFqp0","outputId":"20143e21-7561-4f1b-93a2-a980752ae1c3","trusted":true},"cell_type":"code","source":"dev_2y[:5]","execution_count":null,"outputs":[]},{"metadata":{"id":"cyynLNWxF_Zg"},"cell_type":"markdown","source":"Here we perform Text Cleaning Again. This is an optional Step. We aim to also clean out any and all numerical values present in our data samples as well.\n\n\n**This is an optional step**\n\nThe Functions below remove all special characters and numerical values along with html tags and line spaces and numerical values. We also standardize the comments by converting all text to lowercase."},{"metadata":{"id":"wpjaeeS4QYPt","trusted":true},"cell_type":"code","source":"#Text Cleaning\ndef TextClean(data):\n    \n    txt = []\n    for T in data:\n        T = re.sub(r'@[A-Za-z0-9_]+','',T)\n        T = re.sub(r\"http\\S+\", \"\", T)\n        T = T.replace('<br />', '')\n        T = T.replace(\"\\'\",\"\")\n        T = T.replace(\"?'\",\"\")\n        T = T.replace(\"*\", \"\")\n        T = T.replace(\"/\", \"\")\n        T = T.replace(\"\\ \", \"\")\n        T = T.replace(\".\", \"\")\n        T = T.replace(\"(\", \"\")\n        T = T.replace(\")\", \"\")\n        T = T.replace(\":\", \"\")\n        T = T.replace('\"', \"\")\n        T = T.replace(\",\", \"\")\n        T = T.replace(\"!\", \"\")\n        T = T.replace(\"'\", \"\")\n        T = T.replace(\"&\", \"\")\n        T = re.sub(r\"[0-9]*\", \"\", T)\n        T = re.sub(r\"(”|“|-|\\+|`|#|,|;|\\|/|\\\\|)*\",\"\", T)\n        T = re.sub(r\"&amp\",\"\", T)\n        T = T.lower()\n        txt.append(T)\n    return txt\n\n\n#Removing Special Characters\ndef Remove_SC(text):\n    alphabet = []\n    alpha = 'a'\n    for i in range(0, 26): \n        alphabet.append(alpha) \n        alpha = chr(ord(alpha) + 1)\n    l = []\n    for i in text:\n        txt = []\n        t = i.split(' ')\n        for j in t:\n            m = j\n            for k in m:\n                if k not in alphabet:\n                    m = m.replace(k, '')\n            if m != '':\n                txt.append(m)\n        #l.append(txt)\n        s = ''\n        for j in txt:\n            s = s + j + ' '\n        l.append(s)\n    return l","execution_count":null,"outputs":[]},{"metadata":{"id":"0mN208JdhmPv"},"cell_type":"markdown","source":"Cleaning the Train and Development Sets for Sample 1"},{"metadata":{"id":"SN3oDtpuQkl-","trusted":true},"cell_type":"code","source":"#Cleaning Sample 1 Train Sets and Development Sets\n#Execution takes 120 seconds\n\n#Clean Text and Remove Numerical Values \ntrain_1x = TextClean(train_1x)\ndev_1x   = TextClean(dev_1x)\n#Remove Special Characters\ntrain_1x = Remove_SC(train_1x)\ndev_1x = Remove_SC(dev_1x)","execution_count":null,"outputs":[]},{"metadata":{"id":"_MhnLFf3i8eU"},"cell_type":"markdown","source":"### As we can see now, we do not have any numerical value in our dataset as well. This means that our data is now completely clean and ready to be fit into the models."},{"metadata":{"id":"tJask2XdjWW0"},"cell_type":"markdown","source":"After Cleaning and Removing Special Characters, the Train and Development Sets for Sample 1 (first 5 records) are : "},{"metadata":{"id":"XSbf3w72wihL","outputId":"f41a6edc-b9d9-4fab-f788-6430e66dcd5e","trusted":true},"cell_type":"code","source":"train_1x[:5]","execution_count":null,"outputs":[]},{"metadata":{"id":"pMonopPAHvPf","outputId":"30992b87-0f29-4126-d8ae-e22b8d278cbf","trusted":true},"cell_type":"code","source":"dev_1x[:5]","execution_count":null,"outputs":[]},{"metadata":{"id":"SXrd5gtIHksK","trusted":true},"cell_type":"code","source":"#Cleaning Sample 2 Train Sets and Development Sets\n#Execution takes 120 seconds\n#Clean Text\ntrain_2x = TextClean(train_2x)\ndev_2x   = TextClean(dev_2x)\n#Remove Special Characters\ntrain_2x = Remove_SC(train_2x)\ndev_2x = Remove_SC(dev_2x)","execution_count":null,"outputs":[]},{"metadata":{"id":"svuybjXCJKpT"},"cell_type":"markdown","source":"After Cleaning and Removing Special Characters, the Train and Development Sets for Sample 2 (first 5 records) are : "},{"metadata":{"id":"3YuA4WLlIqMy","outputId":"90e8c8b1-b248-4cf2-973e-d1ca819e98c6","trusted":true},"cell_type":"code","source":"train_2x[:5]","execution_count":null,"outputs":[]},{"metadata":{"id":"0W1lQbJpIqSh","outputId":"045fe999-fa10-4c9f-ff71-8d76a37ab544","trusted":true},"cell_type":"code","source":"dev_2x[:5]","execution_count":null,"outputs":[]},{"metadata":{"id":"045D1VdFkUw2"},"cell_type":"markdown","source":"# **Rounding off Rating Values**"},{"metadata":{"id":"p-WvsQC8jvpa"},"cell_type":"markdown","source":"**In our dataset, we know that there are ratings in the float format, i.e. they have decimal values as well.**\n\n**If we use these decimal values as they are for prediction, we will end up with more than 10 classes as it will consider every unique rating as a seperate class. Therefore, we will round off all rating values in our sample sets so that we can identity the classes easily, as we will get all the values in whole numbers, without any decimal value.**"},{"metadata":{"id":"YV6IuNyxSjAE","trusted":true},"cell_type":"code","source":"#Rounding Rating Values for : \n#Training And Development Set for Sample 1\n\ntrain_1y  = [round(num) for num in train_1y]\ndev_1y    = [round(num) for num in dev_1y]\n\n#Training And Development Set for Sample 2\n\ntrain_2y  = [round(num) for num in train_2y]\ndev_2y    = [round(num) for num in dev_2y]","execution_count":null,"outputs":[]},{"metadata":{"id":"_wqvuRuIlByQ"},"cell_type":"markdown","source":"\n\n---\n\n\n\n---\n\n"},{"metadata":{"id":"8xzpO8tIUYem"},"cell_type":"markdown","source":"Since, we now have both the cleaned data and the rounded ratings, we will now fit our models with the training data and calculate the accuracy."},{"metadata":{"id":"hwbDyPW2KGM0"},"cell_type":"markdown","source":"## We now fit our training data into the Multinomial NaiveBayes Classifier and Predict the Development Set"},{"metadata":{"id":"RZN80s6u_Px9"},"cell_type":"markdown","source":"Calculating Accuracy for the Development Set of Sample 1\n"},{"metadata":{"id":"rxjZt_qkTffW","outputId":"fb2167ca-1a6b-4af2-9cb3-11628b1f90ff","trusted":true},"cell_type":"code","source":"#Takes 30 seconds to execute\nnb_1 = Pipeline([('vect', CountVectorizer()),('tfidf', TfidfTransformer()),('clf', MultinomialNB()),])\n#Fitting Training Set to Model\nnb_1.fit(train_1x,train_1y)\n#Prediction\ny_pred = nb_1.predict(dev_1x)\n#Predicting For Development Set 1\nprint(\"Accuracy of Multinomial Naive-Bayes Classifier for Sample 1 :\", accuracy_score(dev_1y, y_pred)*100,\" %\")\n","execution_count":null,"outputs":[]},{"metadata":{"id":"TWGvwAuTLAFp"},"cell_type":"markdown","source":"Calculating Accuracy for the Development Set of Sample 2"},{"metadata":{"id":"ZCqmUHfVLAV9","outputId":"64e3241f-5479-4cc2-b209-5117c19da626","trusted":true},"cell_type":"code","source":"#Takes 30 seconds to execute\nnb_2 = Pipeline([('vect', CountVectorizer()),('tfidf', TfidfTransformer()),('clf', MultinomialNB()),])\n#Fitting Training Set to Model\nnb_2.fit(train_2x,train_2y)\n#Prediction\ny_pred = nb_2.predict(dev_2x)\n#Predicting For Development Set 2\nprint(\"Accuracy of Multinomial Naive-Bayes Classifier for Sample 2 :\", accuracy_score(dev_2y, y_pred)*100,\" %\")","execution_count":null,"outputs":[]},{"metadata":{"id":"p5cOmNRnlwyM"},"cell_type":"markdown","source":"So we have got the accuracy of our Multinomial Naive Bayes Classifier to be around 30 % which is not bad at all, given that we are using 20% of the entire dataset and are using a subset of that dataset for our model execution."},{"metadata":{"id":"TzUr2bIwLfGO"},"cell_type":"markdown","source":"## We now fit our training data into the Linear SVM Classifier and Predict the Development Set"},{"metadata":{"id":"aLaVWLjPVcSd","outputId":"53624f06-700c-43c5-b7bd-9a58edc75f9b","trusted":true},"cell_type":"code","source":"#Takes 1 minute to execute\nsgd_clf_1 = Pipeline([('vect', CountVectorizer()),('tfidf', TfidfTransformer()),('clf',\n  SGDClassifier(penalty='l2',alpha=1e-3, random_state=42,max_iter=5, tol=None)),])\n#Fitting Model \nsgd_clf_1.fit(train_1x,train_1y)\n#Prediction\ny_pred = sgd_clf_1.predict(dev_1x)\n#Predicting For Development Set 1\nprint(\"Accuracy of Linear SVM Classifier for Sample 1 :\", accuracy_score(dev_1y, y_pred)*100,\" %\")","execution_count":null,"outputs":[]},{"metadata":{"id":"SUI3zcP2N6Sf"},"cell_type":"markdown","source":"Calculating Accuracy for the Development Set of Sample 2"},{"metadata":{"id":"u9Ms5AastwPE","outputId":"b2c69405-40b3-445c-bdc6-4490a57ec569","trusted":true},"cell_type":"code","source":"#Takes 1 minute to execute\nsgd_clf_2 = Pipeline([('vect', CountVectorizer()),('tfidf', TfidfTransformer()),('clf', \n  SGDClassifier(penalty='l2',alpha=1e-3, random_state=42,max_iter=5, tol=None)),])\n#Fitting Model \nsgd_clf_2.fit(train_2x,train_2y)\n#Prediction\ny_pred = sgd_clf_2.predict(dev_2x)\n#Predicting For Development Set 2\nprint(\"Accuracy of Linear SVM Classifier for Sample 2 :\", accuracy_score(dev_2y, y_pred)*100,\" %\")","execution_count":null,"outputs":[]},{"metadata":{"id":"CpHpSnTImeXt"},"cell_type":"markdown","source":"For our Linear SVM Classifier, we have achieved an accuracy of ~26% which is not too bad. But it proves that Naive Bayes is the better among the two."},{"metadata":{"id":"TU3LSsr28hWE"},"cell_type":"markdown","source":"## We now fit our training data into the Ridge Classifier and Predict the Development Set"},{"metadata":{"id":"K2RJTS9z8V0s"},"cell_type":"markdown","source":"Calculating Accuracy for the Development Set of Sample 1"},{"metadata":{"id":"z6XoHSBZ8apA","outputId":"276060a9-e5b6-4248-c437-4e56f2bd95af","trusted":true},"cell_type":"code","source":"#This Code takes 230 seconds to execute\ntridge_clf_1 = Pipeline([('vect', CountVectorizer()),('tfidf', TfidfTransformer()),('clf',RidgeClassifier()),])\ntridge_clf_1.fit(train_1x,train_1y)\n\ny_pred = tridge_clf_1.predict(dev_1x)\n#Predicting For Development Set 1\nprint(\"Accuracy of Ridge Classifier for Sample 1  :\", accuracy_score(dev_1y, y_pred)*100,\" %\")","execution_count":null,"outputs":[]},{"metadata":{"id":"ELes7rTl91ip"},"cell_type":"markdown","source":"Calculating Accuracy for the Development Set of Sample 2"},{"metadata":{"id":"z3WQfwkR97Pb","outputId":"97169ff5-d10b-4b83-80c8-5ecce41ff697","trusted":true},"cell_type":"code","source":"#This Code takes 230 seconds to execute\ntridge_clf_2 = Pipeline([('vect', CountVectorizer()),('tfidf', TfidfTransformer()),('clf',RidgeClassifier()),])\ntridge_clf_2.fit(train_2x,train_2y)\n\ny_pred = tridge_clf_2.predict(dev_2x)\n#Predicting For Development Set 2\nprint(\"Accuracy of Ridge Classifier for Sample 2  :\", accuracy_score(dev_2y, y_pred)*100,\" %\")","execution_count":null,"outputs":[]},{"metadata":{"id":"k0eRk0o6n_nn"},"cell_type":"markdown","source":"### **This is Interesting!! Our Ridge Classifier has a Higher Accuracy than Naive Bayes. It may only be by a margin of 1% but is a detail that must be tracked and made note of.**"},{"metadata":{"id":"2m05RusiOZU-"},"cell_type":"markdown","source":"# To Summarize the accuracies:\n\nFor Multinomial Naive-Bayes\n\n1.   Accuracy of Multinomial Naive-Bayes Classifier for \n     Sample 1 : 30.30702818553116  %\n\n2.   Accuracy of Multinomial Naive-Bayes Classifier for \n     Sample 2 : 30.147296696186665  %\n\n\nFor Linear SVM\n\n1.   Accuracy of Linear SVM Classifier for \n     Sample 1 : 25.813316349225605  %\n\n2.   Accuracy of Linear SVM Classifier for \n     Sample 2 : 26.562942294472077  %\n\nFor Ridge Classifier\n\n1.   Accuracy of Ridge Classifier for \n     Sample 1 : 31.3134376642808  %\n\n2.   Accuracy of Ridge Classifier for \n     Sample 2 : 31.268955477374742  %\n\n\n\n"},{"metadata":{"id":"PrhnKhkZR5xN"},"cell_type":"markdown","source":"# Contribution -  For a more accurate result, we will add a smoothing value to our accuracy calculation.\n\nWe know that there are 2.6 million reviews each of which has a rating between 0 and 10.These ratings also had values which had significance to the 3rd decimal as well, e.g. 3.234. Therefore, it is possible that, if a review has a rating of 10, but is predicted to be 9, then we should also consider the prediction value \"9\" to be accurate as well.But if the same review is predicted to be a rating of \"8\" then that is an inaccurate prediction.\n\nHence, to add this consideration while calculating accuracy, we create a new function which checks the value of the prediction, and if it is off by only some decimal value which is less than 1, then it predicts it to be the correct prediction value.\n\nFor eg., if a review has a rating of 10 or 9.5 or 9.6 or some value in the decimal of 9 , but is predicted as 9, we will classify this as an accurate prediction instead of an inaccurate prediction."},{"metadata":{"id":"MQjOysjTR8oJ","trusted":true},"cell_type":"code","source":"#Function for calculating accuracy with a smoothing factor\ndef smooth_acc(yr,yp):\n  c=0\n  for i in range(len(yr)):\n    if(yr[i] == yp[i] or yr[i]==(yp[i]+1) or yr[i]==(yp[i]-1) ):\n      c=c+1\n    \n  return c/len(yr)","execution_count":null,"outputs":[]},{"metadata":{"id":"IO0LupnupSVV"},"cell_type":"markdown","source":"We now calculate the accuracies for our models using the new accuracy function. We should have some increase in the accuracy of our models.\n\nLets Observe :)"},{"metadata":{"id":"DmppX8Z1Suxo"},"cell_type":"markdown","source":"## Using Multinomial Naive-Bayes"},{"metadata":{"id":"9D9PJOMFS-R8"},"cell_type":"markdown","source":"**Calculating Accuracy for Sample 1**"},{"metadata":{"id":"yWXvNtFRTL60","outputId":"8c571bc8-cf13-4971-e89b-1fac77ee392f","trusted":true},"cell_type":"code","source":"nb_1 = Pipeline([('vect', CountVectorizer()),('tfidf', TfidfTransformer()),('clf', MultinomialNB()),])\n#Fitting Training Set to Model\nnb_1.fit(train_1x,train_1y)\n#Prediction\ny_pred = nb_1.predict(dev_1x)\n#Predicting For Development Set 1\nprint(\"Accuracy of Multinomial Naive-Bayes Classifier for Sample 1 :\", smooth_acc(dev_1y, y_pred)*100,\" %\")","execution_count":null,"outputs":[]},{"metadata":{"id":"zfR4pW0OX2N3"},"cell_type":"markdown","source":"**Calculating Accuracy for Sample 2**"},{"metadata":{"id":"1qrA9QYBX4rn","outputId":"cd402551-811b-4655-d7fd-d4988cf8a92f","trusted":true},"cell_type":"code","source":"nb_2 = Pipeline([('vect', CountVectorizer()),('tfidf', TfidfTransformer()),('clf', MultinomialNB()),])\n#Fitting Training Set to Model\nnb_2.fit(train_2x,train_2y)\n#Prediction\ny_pred = nb_2.predict(dev_2x)\n#Predicting For Development Set 2\nprint(\"Accuracy of Multinomial Naive-Bayes Classifier for Sample 1 :\", smooth_acc(dev_2y, y_pred)*100,\" %\")","execution_count":null,"outputs":[]},{"metadata":{"id":"sjo3VOV1poTc"},"cell_type":"markdown","source":"WOW!! Thats a big improvement in the accuracy from a mere 30% to a WHOPPING 66%. That is a really good prediction.\n\nLets see how are the results for our other classifiers :D"},{"metadata":{"id":"QNSuAggrYDAR"},"cell_type":"markdown","source":"## Using Linear SVM"},{"metadata":{"id":"huMju3k4YYpo"},"cell_type":"markdown","source":"**Calculating Accuracy for Sample 1**"},{"metadata":{"id":"CkEnGBk_YI3k","outputId":"98b2ff42-4f21-465e-cc31-f4081b0f4946","trusted":true},"cell_type":"code","source":"sgd_clf_1 = Pipeline([('vect', CountVectorizer()),('tfidf', TfidfTransformer()),('clf', \n  SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42,max_iter=5, tol=None)),])\n#Fitting Model \nsgd_clf_1.fit(train_1x,train_1y)\n#Prediction\ny_pred = sgd_clf_1.predict(dev_1x)\n#Predicting For Development Set 1\nprint(\"Accuracy of Linear SVM Classifier for Sample 1 :\", smooth_acc(dev_1y, y_pred)*100,\" %\")","execution_count":null,"outputs":[]},{"metadata":{"id":"zGsg1VtUYaWl"},"cell_type":"markdown","source":"**Calculating Accuracy for Sample 2**"},{"metadata":{"id":"xxNfcy-AYbx8","outputId":"109fce25-9d02-4e5b-b709-2799282ed4a1","trusted":true},"cell_type":"code","source":"sgd_clf_2 = Pipeline([('vect', CountVectorizer()),('tfidf', TfidfTransformer()),('clf', \n  SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42,max_iter=5, tol=None)),])\n#Fitting Model \nsgd_clf_2.fit(train_2x,train_2y)\n#Prediction\ny_pred = sgd_clf_2.predict(dev_2x)\n#Predicting For Development Set 2\nprint(\"Accuracy of Linear SVM Classifier for Sample 2 :\", smooth_acc(dev_2y, y_pred)*100,\" %\")","execution_count":null,"outputs":[]},{"metadata":{"id":"_NvpESXip-Zv"},"cell_type":"markdown","source":"Great! Our SVM Classifier also has an increase in accuracy in comparison to before when we did not have any smoothing for our accuracy.\n\nIt still has a lower accuracy than our Naive Bayes so it still comes second to it."},{"metadata":{"id":"UDNy24TGB-v-"},"cell_type":"markdown","source":"## Using Ridge Classifier"},{"metadata":{"id":"lQL6vqopCDQU"},"cell_type":"markdown","source":"**Calculating Accuracy for Sample 1**"},{"metadata":{"id":"DhvXhlXGCJGK","outputId":"577f50d5-6b99-426b-c125-1afd9100041d","trusted":true},"cell_type":"code","source":"#This Code takes 230 seconds to execute\ntridge_clf_1 = Pipeline([('vect', CountVectorizer()),('tfidf', TfidfTransformer()),('clf',RidgeClassifier()),])\ntridge_clf_1.fit(train_1x,train_1y)\n\ny_pred = tridge_clf_1.predict(dev_1x)\n#Predicting For Development Set 1\nprint(\"Accuracy of Ridge Classifier for Sample 1  :\", smooth_acc(dev_1y, y_pred)*100,\" %\")","execution_count":null,"outputs":[]},{"metadata":{"id":"T6B-qg_oCKxt"},"cell_type":"markdown","source":"**Calculating Accuracy for Sample 2**"},{"metadata":{"id":"jpTt-_7uCLBX","outputId":"5f158286-79f8-4744-8bb5-aa96a75e3aad","trusted":true},"cell_type":"code","source":"#This Code takes 230 seconds to execute\ntridge_clf_2 = Pipeline([('vect', CountVectorizer()),('tfidf', TfidfTransformer()),('clf',RidgeClassifier()),])\ntridge_clf_2.fit(train_2x,train_2y)\n\ny_pred = tridge_clf_2.predict(dev_2x)\n#Predicting For Development Set 2\nprint(\"Accuracy of Ridge Classifier for Sample 2  :\", smooth_acc(dev_2y, y_pred)*100,\" %\")","execution_count":null,"outputs":[]},{"metadata":{"id":"5eaFobwWqWy1"},"cell_type":"markdown","source":"Amazing! Our Ridge Classifier has had the highest accuracy so far after applying the smoothing accuracy function. \n\nAnd thats a big jump from a 31% accuracy to a 69% accuracy. \n\n\nThis reinforces our assumption and helps us conclude with confidence that our Smoothing Function is providing a good improvement in accuracy."},{"metadata":{"id":"P8SB46VOaPWM"},"cell_type":"markdown","source":"# To Summarize the Accuracies using our Smoothing Accuracy Calculating Function:\n\n**Using Smoothing Function for Calculating Accuracy for Model**\n\nFor Multinomial Naive-Bayes\n\n1.   Accuracy of Multinomial Naive-Bayes Classifier for \n     Sample 1 : 66.01763112135549  %\n2.   Accuracy of Multinomial Naive-Bayes Classifier for \n     Sample 2 : 65.79067087225525  %\n\nFor Linear SVM \n\n1.   Accuracy of Linear SVM Classifier for \n     Sample 1 : 63.035302681062724  %\n2.   Accuracy of Linear SVM Classifier for \n     Sample 2 : 63.0494561041692  %\n\nFor Ridge Classifier\n\n1.   Accuracy of Ridge Classifier for \n     Sample 1 : 69.1101540701201  %\n2.   Accuracy of Ridge Classifier for \n     Sample 2 : 69.2319746047151  %\n\n\n\n\n"},{"metadata":{"id":"WaZdSWgzsUUw"},"cell_type":"markdown","source":"### **Therefore, we have attained the maximum accuracy attainable using our smoothing function for calculating accuracy.**\n\nBut there are other approaches that can also be used to improve the rating prediction.\n\nThey are mentioned below. :)"},{"metadata":{"id":"YfbJw3sosIri"},"cell_type":"markdown","source":"\n\n---\n\n\n\n---\n\n"},{"metadata":{"id":"U6eSo25KsTZb"},"cell_type":"markdown","source":"# **Contribution - Rescale the Ratings to a scale of 0 to 5 from the original scale 0 to 10**"},{"metadata":{"id":"rul4lnY7HV3T"},"cell_type":"markdown","source":"Another Approach to Improving the accuracy would be to rescale the ratings from 0-10 to 0-5.\n\nOur samplesets have ratings ranging from 0 to 10. That means that there are just as many reviews which lie on a broader spectrum of ratings. But if we were to change the scale in such a way that we can condense the scale of ratings, then it should, theoretically, increase the base accuracy of our models, without the smoothing function as well.\n\nSo, lets just jump into it. :)"},{"metadata":{"id":"brqYpttiI-AZ"},"cell_type":"markdown","source":"## Since the textual data will remain the same, we will use the same datasets that we have created above"},{"metadata":{"id":"zQLk068QtxrP"},"cell_type":"markdown","source":"Train And Development Sets for Sample 1"},{"metadata":{"id":"JfGG-FgGWNcb","outputId":"c73a67e7-6581-4c4a-debb-36dd08d47a63","trusted":true},"cell_type":"code","source":"train_1x[:5]","execution_count":null,"outputs":[]},{"metadata":{"id":"FmNtf7h6WSmh","outputId":"b12689f6-6a7a-4445-f912-a87789352735","trusted":true},"cell_type":"code","source":"dev_1x[:5]","execution_count":null,"outputs":[]},{"metadata":{"id":"SSRsxRZ8t2_D"},"cell_type":"markdown","source":"Train And Development Sets for Sample 2"},{"metadata":{"id":"NgjpgTqjWVyi","outputId":"c2d14680-5bbf-48fc-cdb3-6ec546b38046","trusted":true},"cell_type":"code","source":"train_2x[:5]","execution_count":null,"outputs":[]},{"metadata":{"id":"hHPX79pCWXX0","outputId":"ba271d76-c76b-408f-c2a9-9d9ef84e0dec","trusted":true},"cell_type":"code","source":"dev_2x[:5]","execution_count":null,"outputs":[]},{"metadata":{"id":"Y_U8saQEuKb4"},"cell_type":"markdown","source":"## **Before we rescale the ratings, let us first have a look at the originally rounded off ratings.**\n\n**Since they are still in a scale of 0 to 10, the maximum value should be 10 and the minimum value should be 0.**\n"},{"metadata":{"id":"z3tk0_EauyJw"},"cell_type":"markdown","source":"For Sample 1"},{"metadata":{"id":"HlZcTbytufdi","outputId":"4931cb91-ae52-4d08-c6f8-ce9e7c612529","trusted":true},"cell_type":"code","source":"print(\"For Training Set ( Sample 1 ) \")\nprint(\"Max value : \",max(train_1y), \" Min Rating : \",min(train_1y))\n\nprint(\"For Development Set ( Sample 1 ) \")\nprint(\"Max value : \",max(dev_1y), \" Min Rating : \",min(dev_1y))","execution_count":null,"outputs":[]},{"metadata":{"id":"7IrDMjJpu0AC"},"cell_type":"markdown","source":"For Sample 2"},{"metadata":{"id":"gj12ahyvu1xI","outputId":"ff29a77a-47f1-4838-97c4-c57b922e8fdb","trusted":true},"cell_type":"code","source":"print(\"For Training Set ( Sample 2 ) \")\nprint(\"Max value : \",max(train_2y), \" Min Rating : \",min(train_2y))\n\nprint(\"For Development Set ( Sample 2 ) \")\nprint(\"Max value : \",max(dev_2y), \" Min Rating : \",min(dev_2y))","execution_count":null,"outputs":[]},{"metadata":{"id":"AQ0UmQxLMnIT"},"cell_type":"markdown","source":"**As we can see, our sample sets rating values are from 0 - 10.**\n\n**Where 10 is the maximum and 0 is the minimum, just as we predicted.**\n\nWe have 4 sets of ratings, as we have two samples and each have a train and a development set."},{"metadata":{"id":"_KloHU2yvQri"},"cell_type":"markdown","source":"Reading Rating Values for Both Sample Sets and appending to List"},{"metadata":{"id":"LItV-NjUWjby","trusted":true},"cell_type":"code","source":"#Train and Development for Sample 1\nnew_train_1y=[]\nfor i in sample_1_train['rating']:\n    new_train_1y.append(i)\n\nnew_dev_1y=[]\nfor i in sample_1_dev['rating']:\n    new_dev_1y.append(i)\n\n#Train and Development for Sample 2\nnew_train_2y=[]\nfor i in sample_2_train['rating']:\n    new_train_2y.append(i)\n\nnew_dev_2y=[]\nfor i in sample_2_dev['rating']:\n    new_dev_2y.append(i)\n","execution_count":null,"outputs":[]},{"metadata":{"id":"ga4s9lB8XKZH"},"cell_type":"markdown","source":"Since we are using the same samples as we used before, we already have the cleaned dataset of comments. So we now only need to rescale the rating values"},{"metadata":{"id":"Ym7APQ4uZhfY"},"cell_type":"markdown","source":"Re-Scaling for Sample 1"},{"metadata":{"id":"UHkVWQG8JKJk","outputId":"fbbb69e3-dc79-4323-bc20-d10bf4bfbf29","trusted":true},"cell_type":"code","source":"#Re-Scaling Values\n#Training And Development Set for Sample 1\nnew_train_1y  = [round(num/2) for num in new_train_1y]\nnew_dev_1y    = [round(num/2) for num in new_dev_1y]\n\nprint(\"For Training Set ( Sample 1 ) \")\nprint(\"Max value : \",max(new_train_1y), \" Min Rating : \",min(new_train_1y))\n\nprint(\"For Development Set ( Sample 1 ) \")\nprint(\"Max value : \",max(new_dev_1y), \" Min Rating : \",min(new_dev_1y))","execution_count":null,"outputs":[]},{"metadata":{"id":"eTaJEAstZjLG"},"cell_type":"markdown","source":"Re-Scaling for Sample 2"},{"metadata":{"id":"_spnYslxX8n6","outputId":"69759587-89a0-4e40-f26f-b20755291d27","trusted":true},"cell_type":"code","source":"#Re-Scaling Values\n#Training And Development Set for Sample 2\nnew_train_2y  = [round(num/2) for num in new_train_2y]\nnew_dev_2y    = [round(num/2) for num in new_dev_2y]\n\nprint(\"For Training Set ( Sample 2 ) \")\nprint(\"Max value : \",max(new_train_2y), \" Min Rating : \",min(new_train_2y))\n\nprint(\"For Development Set ( Sample 2 ) \")\nprint(\"Max value : \",max(new_dev_2y), \" Min Rating : \",min(new_dev_2y))","execution_count":null,"outputs":[]},{"metadata":{"id":"Kb89-a0nJ9Pr"},"cell_type":"markdown","source":"As we can see, the maximum rating value in all of the ratings is now 5 and the minimum value is 0. \n\n## This is what we wanted to achieve, so now lets again calculate the accuracy using these new scaled rating values"},{"metadata":{"id":"op7M3IFbYYn9"},"cell_type":"markdown","source":"Calculating Accuracy for Naive Bayes using Scaled Ratings"},{"metadata":{"id":"dy_F2TzYJ7w8","outputId":"1865b5e7-ac61-4766-9b5c-5a4717a6e260","trusted":true},"cell_type":"code","source":"nb_n1 = Pipeline([('vect', CountVectorizer()),('tfidf', TfidfTransformer()),('clf', MultinomialNB()),])\n#Fitting Training Set to Model\nnb_n1.fit(train_1x,new_train_1y)\n#Prediction\ny_pred = nb_n1.predict(dev_1x)\n#Predicting For Development Set 1\nprint(\"Accuracy of Multinomial Naive-Bayes Classifier for Sample 1 :\", accuracy_score(new_dev_1y, y_pred)*100,\" %\")\n\nnb_n2 = Pipeline([('vect', CountVectorizer()),('tfidf', TfidfTransformer()),('clf', MultinomialNB()),])\n#Fitting Training Set to Model\nnb_n2.fit(train_2x,new_train_2y)\n#Prediction\ny_pred = nb_n2.predict(dev_2x)\n#Predicting For Development Set 2\nprint(\"Accuracy of Multinomial Naive-Bayes Classifier for Sample 2 :\", accuracy_score(new_dev_2y, y_pred)*100,\" %\")","execution_count":null,"outputs":[]},{"metadata":{"id":"AlqnLpVdv-d6"},"cell_type":"markdown","source":"**Great! Our Assumption and expectation is successful. We have an improved base accuracy of 55%.**\n\n**Initially, when we did not rescale our ratings, we had achieved an accuracy of 30%, but now using the scaled ratings, we have a huge jump to 55% in the base accuracy, which is very good.**\n\n\nLets now see how our other models have fared this new change."},{"metadata":{"id":"ScHwyVf3Y-u1"},"cell_type":"markdown","source":"Calculating Accuracy for Linear SVM using Scaled Ratings"},{"metadata":{"id":"4nhHA7-6YjY_","outputId":"e816e47b-1822-4741-9501-c46245d34b3c","trusted":true},"cell_type":"code","source":"sgd_clf_n1 = Pipeline([('vect', CountVectorizer()),('tfidf', TfidfTransformer()),('clf', \n  SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42,max_iter=5, tol=None)),])\n#Fitting Model \nsgd_clf_n1.fit(train_1x,new_train_1y)\n#Prediction\ny_pred = sgd_clf_n1.predict(dev_1x)\n#Predicting For Development Set 1\nprint(\"Accuracy of Linear SVM Classifier for Sample 1 :\", accuracy_score(new_dev_1y, y_pred)*100,\" %\")\n\nsgd_clf_n2 = Pipeline([('vect', CountVectorizer()),('tfidf', TfidfTransformer()),('clf', \n  SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42,max_iter=5, tol=None)),])\n#Fitting Model \nsgd_clf_n2.fit(train_2x,new_train_2y)\n#Prediction\ny_pred = sgd_clf_n2.predict(dev_2x)\n#Predicting For Development Set 2\nprint(\"Accuracy of Linear SVM Classifier for Sample 2 :\", accuracy_score(new_dev_2y, y_pred)*100,\" %\")","execution_count":null,"outputs":[]},{"metadata":{"id":"x2Z5O6xKwjMx"},"cell_type":"markdown","source":"Yay! Although lower than Naive Bayes, our SVM has still achieved an accuracy boost which is almost twice as much as when the ratings were not scaled. SVM had an accuracy of ~26% before scaling, but it now achieved 54% accuracy, which is amazing."},{"metadata":{"id":"8Y0GfhB8bLZm"},"cell_type":"markdown","source":"Calculating Accuracy for Ridge Classifier using Scaled Ratings"},{"metadata":{"id":"3GlVSXXzbLm0","outputId":"7ad7248a-92fd-42c0-eded-0f63f6b7c601","trusted":true},"cell_type":"code","source":"#This Code takes 230 seconds to execute\ntridge_clf_n1 = Pipeline([('vect', CountVectorizer()),('tfidf', TfidfTransformer()),('clf',RidgeClassifier()),])\ntridge_clf_n1.fit(train_1x,new_train_1y)\n\ny_pred = tridge_clf_n1.predict(dev_1x)\n#Predicting For Development Set 1\nprint(\"Accuracy of Ridge Classifier for Sample 1  :\", accuracy_score(new_dev_1y, y_pred)*100,\" %\")\n\n#This Code takes 230 seconds to execute\ntridge_clf_n2 = Pipeline([('vect', CountVectorizer()),('tfidf', TfidfTransformer()),('clf',RidgeClassifier()),])\ntridge_clf_n2.fit(train_2x,new_train_2y)\n\ny_pred = tridge_clf_n2.predict(dev_2x)\n#Predicting For Development Set 2\nprint(\"Accuracy of Ridge Classifier for Sample 2  :\", accuracy_score(new_dev_2y, y_pred)*100,\" %\")","execution_count":null,"outputs":[]},{"metadata":{"id":"ML5J4JvtxEzj"},"cell_type":"markdown","source":"**Wow!! The Accuracy of our Ridge Classifier is by far the most high among all other classifiers, and with respect to its previous accuracy as well.**\n\n\n**Ridge Classifier showed an accuracy of 31% without scaling the ratings but now has achieved an accuracy of 59% which is a significant growth.**\n"},{"metadata":{"id":"K7ThNAB8lo0O"},"cell_type":"markdown","source":"# To Summarize the Accuracies of our models after scaling the ratings from (0 to 10) to ( 0 to 5)\n\nFor Multinomial Naive-Bayes\n\n1. Accuracy of Multinomial Naive-Bayes Classifier for \n   Sample 1 : 55466254195478996  %\n2. Accuracy of Multinomial Naive-Bayes Classifier for \n   Sample 2 : 55.331796675967496  %\n\nFor Linear SVM \n\n1. Accuracy of Linear SVM Classifier for \n   Sample 1 : 54.846030975777424  %\n2. Accuracy of Linear SVM Classifier for \n   Sample 2 : 54.76717618989851  %\n\nFor Ridge Classifier\n\n1. Accuracy of Ridge Classifier for \n   Sample 1  : 59.35743459096607  %\n2. Accuracy of Ridge Classifier for \n   Sample 2  : 59.299809939746865  %\n"},{"metadata":{"id":"Ene_X8fHbLx3"},"cell_type":"markdown","source":"# We will now further try to achieve a higher accuracy by combining this approach, i.e. rescaling the interval approach, with the smooothed accuracy function that we have created above "},{"metadata":{"id":"P0Uoqs22gs5c"},"cell_type":"markdown","source":"Calculating Accuracy for Naive Bayes using Scaled Ratings and Smoothing Accuracy Function"},{"metadata":{"id":"Pzkld3JhbL73","outputId":"3ab9e4d1-2568-452d-c1c1-a7e76a9f3f01","trusted":true},"cell_type":"code","source":"nb_n1 = Pipeline([('vect', CountVectorizer()),('tfidf', TfidfTransformer()),('clf', MultinomialNB()),])\n#Fitting Training Set to Model\nnb_n1.fit(train_1x,new_train_1y)\n#Prediction\ny_pred = nb_n1.predict(dev_1x)\n#Predicting For Development Set 1\nprint(\"Accuracy of Multinomial Naive-Bayes Classifier for Sample 1 :\", smooth_acc(new_dev_1y, y_pred)*100,\" %\")\n\nnb_n2 = Pipeline([('vect', CountVectorizer()),('tfidf', TfidfTransformer()),('clf', MultinomialNB()),])\n#Fitting Training Set to Model\nnb_n2.fit(train_2x,new_train_2y)\n#Prediction\ny_pred = nb_n2.predict(dev_2x)\n#Predicting For Development Set 2\nprint(\"Accuracy of Multinomial Naive-Bayes Classifier for Sample 2 :\", smooth_acc(new_dev_2y, y_pred)*100,\" %\")","execution_count":null,"outputs":[]},{"metadata":{"id":"sEOkuoa9ySAj"},"cell_type":"markdown","source":"Amazing. We have achieved an accuracy of 82% on our Naive Bayes Classifier by combining our Rescaled Ratings and Our Smoothing Accuracy Function.\n\nWe have thus Succeeded in changing the overall accuracy of our Naive Bayes Model from 30% to a whole 82%\n\nLet us look at the other competition :)"},{"metadata":{"id":"ir4K_Lnig86M"},"cell_type":"markdown","source":"Calculating Accuracy for Linear SVM using Scaled Ratings and Smoothing Accuracy Function\n"},{"metadata":{"id":"kISw6726hHPe","outputId":"b153c4d0-ddeb-4e06-ddb3-bed11ddd370d","trusted":true},"cell_type":"code","source":"sgd_clf_n1 = Pipeline([('vect', CountVectorizer()),('tfidf', TfidfTransformer()),('clf', \n  SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42,max_iter=5, tol=None)),])\n#Fitting Model \nsgd_clf_n1.fit(train_1x,new_train_1y)\n#Prediction\ny_pred = sgd_clf_n1.predict(dev_1x)\n#Predicting For Development Set 1\nprint(\"Accuracy of Linear SVM Classifier for Sample 1 :\", smooth_acc(new_dev_1y, y_pred)*100,\" %\")\n\nsgd_clf_n2 = Pipeline([('vect', CountVectorizer()),('tfidf', TfidfTransformer()),('clf', \n  SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42,max_iter=5, tol=None)),])\n#Fitting Model \nsgd_clf_n2.fit(train_2x,new_train_2y)\n#Prediction\ny_pred = sgd_clf_n2.predict(dev_2x)\n#Predicting For Development Set 2\nprint(\"Accuracy of Linear SVM Classifier for Sample 2 :\", smooth_acc(new_dev_2y, y_pred)*100,\" %\")","execution_count":null,"outputs":[]},{"metadata":{"id":"GD8hPTjnyrKt"},"cell_type":"markdown","source":"Great, this shows that our approach to changing the scale and accuracy function is a correct approach as we can see the significant improvement in our accuracies.\n\nSVM has jumped from an accuracy of 26% to ~82 %.\n\n"},{"metadata":{"id":"flt8Fuaahhuk"},"cell_type":"markdown","source":"Calculating Accuracy for Ridge Classifier using Scaled Ratings and Smoothing Accuracy Function"},{"metadata":{"id":"rga-1s2Shlyl","outputId":"6f76c784-6492-430d-9829-20aa7d0e97ea","trusted":true},"cell_type":"code","source":"#This Code takes 230 seconds to execute\ntridge_clf_n1 = Pipeline([('vect', CountVectorizer()),('tfidf', TfidfTransformer()),('clf',RidgeClassifier()),])\ntridge_clf_n1.fit(train_1x,new_train_1y)\n\ny_pred = tridge_clf_n1.predict(dev_1x)\n#Predicting For Development Set 1\nprint(\"Accuracy of Ridge Classifier for Sample 1  :\", smooth_acc(new_dev_1y, y_pred)*100,\" %\")\n\n#This Code takes 230 seconds to execute\ntridge_clf_n2 = Pipeline([('vect', CountVectorizer()),('tfidf', TfidfTransformer()),('clf',RidgeClassifier()),])\ntridge_clf_n2.fit(train_2x,new_train_2y)\n\ny_pred = tridge_clf_n2.predict(dev_2x)\n#Predicting For Development Set 2\nprint(\"Accuracy of Ridge Classifier for Sample 2  :\", smooth_acc(new_dev_2y, y_pred)*100,\" %\")","execution_count":null,"outputs":[]},{"metadata":{"id":"POLSZla9zBAG"},"cell_type":"markdown","source":"WOW!!\nSo this should officially prove that our Ridge Classifier is more superior than all other classifiers as we have found the rescaling and smoothing to be big contributing factors.\n\nWe have jumped from an initial accuracy of 31% to a big 88% accuracy.\n\n"},{"metadata":{"id":"xLjZv4mhqYwQ"},"cell_type":"markdown","source":"# To Summarise the Accuracy of all Classifiers after : \n\n**Rescaling Rating Intervals from 0 - 10 to 0 - 5**\n\n**AND**\n\n**Using Smoothing Function for Calculating Accuracy for Model**\n\n\nFor Multinomial Naive-Bayes\n\n1. Accuracy of Multinomial Naive-Bayes Classifier for \n   Sample 1 : 82.80864571960048  %\n2. Accuracy of Multinomial Naive-Bayes Classifier for \n   Sample 2 : 82.6347608071495  %\n\nFor Linear SVM \n\n1. Accuracy of Linear SVM Classifier for \n   Sample 1 : 81.92506773423915  %\n2. Accuracy of Linear SVM Classifier for \n   Sample 2 : 81.81841158154394  %\n\nFor Ridge Classifier\n\n1. Accuracy of Ridge Classifier for \n   Sample 1  : 88.37650127380809  %\n2. Accuracy of Ridge Classifier for \n   Sample 2  : 88.27338347688948  %\n"},{"metadata":{"id":"O9fQJ1-CZQoy"},"cell_type":"markdown","source":"# Therefore, we will use our Ridge Classifier model with the scaled values of the rating for the testing of our test set"},{"metadata":{"id":"uQirPJ3k0KVO"},"cell_type":"markdown","source":"Cleaning the Data for Train and Test Set"},{"metadata":{"id":"C66WpnJXsHaX","outputId":"7c19443d-5694-47f4-d89e-01f19ba9376d","trusted":true},"cell_type":"code","source":"#lowercase and remove punctuation\ntrain['comment'] = train['comment'].str.lower().apply(lambda x:''.join([i for i in x if i not in string.punctuation]))\n\n#remove stopwords\ntest['comment'] = test['comment'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stopwords_list)]))","execution_count":null,"outputs":[]},{"metadata":{"id":"e0xfA0oB1Pt_"},"cell_type":"markdown","source":"Building X nd Y List for Train and Test Set1\n"},{"metadata":{"id":"CUPv_mzlZWwx","trusted":true},"cell_type":"code","source":"#Training X and Y\nx_train=[]\nfor i in train['comment']:\n    x_train.append(i)\n\ny_train=[]\nfor i in train['rating']:\n  y_train.append(i)\n\n#Test X and Y\nx_test=[]\nfor i in test['comment']:\n    x_test.append(i)\n\ny_test=[]\nfor i in test['rating']:\n    y_test.append(i)","execution_count":null,"outputs":[]},{"metadata":{"id":"__LZO5dj1XEU"},"cell_type":"markdown","source":"Cleaning Once Again to Remove all the Numerical Values "},{"metadata":{"id":"immqWhyEZ_hN","trusted":true},"cell_type":"code","source":"#Cleaning Train Sets and Test Sets\n\n#Execution takes 300 seconds\n#Clean Text\nx_train = TextClean(x_train)\nx_test   = TextClean(x_test)\n#Remove Special Characters\nx_train = Remove_SC(x_train)\nx_test = Remove_SC(x_test)","execution_count":null,"outputs":[]},{"metadata":{"id":"FTKJwlbY1xeT"},"cell_type":"markdown","source":"Rounding and Rescaling the Interval to 0 ot 5"},{"metadata":{"id":"K4M23FNodHal","trusted":true},"cell_type":"code","source":"#Training And Test Set for Sample 1\ny_train  = [round(num/2) for num in y_train]\ny_test   = [round(num/2) for num in y_test]\n","execution_count":null,"outputs":[]},{"metadata":{"id":"umJlyyFPb-_o"},"cell_type":"markdown","source":"Fitting Train Data into Ridge Classifier Model for Test Set Accuracy Calculation"},{"metadata":{"id":"yktma4OFahzl","trusted":true},"cell_type":"code","source":"#This Code takes 230 seconds to execute\ntridge_clf_n1 = Pipeline([('vect', CountVectorizer()),('tfidf', TfidfTransformer()),('clf',RidgeClassifier()),])\n#Fitting Training Set to Model\ntridge_clf_n1.fit(train_1x,new_train_1y)\n","execution_count":null,"outputs":[]},{"metadata":{"id":"5LtAPTo6viuX"},"cell_type":"markdown","source":"**Using Normal Accuracy Function**"},{"metadata":{"id":"cwoTmb9Dvg7D","outputId":"5eddaddb-8d91-46df-ae27-d4c1f74ab444","trusted":true},"cell_type":"code","source":"#Prediction\ny_pred = tridge_clf_n1.predict(x_test)\n#Predicting For Development Set 1\nprint(\"Accuracy of Ridge Classifier for Test Set  :\", accuracy_score(y_test, y_pred)*100,\" %\")\n","execution_count":null,"outputs":[]},{"metadata":{"id":"6aSCeBl_unwd"},"cell_type":"markdown","source":"Using Smooth Accuracy Function"},{"metadata":{"id":"DFX4LmWMur3G","outputId":"2d5e305e-135e-4337-ce29-6659624cb75f","trusted":true},"cell_type":"code","source":"#Prediction\ny_pred = tridge_clf_n1.predict(x_test)\n#Predicting For Development Set 1\nprint(\"Accuracy of Ridge Classifier for Test Set  :\", smooth_acc(y_test, y_pred)*100,\" %\")\n","execution_count":null,"outputs":[]},{"metadata":{"id":"Ka4Brz5u4uxJ"},"cell_type":"markdown","source":"# Great!!\n\n# For our Final Test Set,\n# We get an accuracy of 57.63% if we do not apply any smoothing, while we get an accuracy of 86.65% while we use our smoothing accuracy function. \n\n\n# Thus we conclude that our Ridge Classifier is the most accurate model as compared to our other models. And we will put this model for hosting to perform live prediction."},{"metadata":{"id":"ffP37bbarp0V"},"cell_type":"markdown","source":"# Challenges :\nOne of the major challenges in this project, for me was the managing of the huge dataset. Since i have no prior experience in the field of data mining in general, it took me quite some time to adapt to the different techniques and their workings. Using all the pre-existing knowledge that i had, i tried my best and was able to make a comparison of the 3 models and was able to perform some optimizations on the accuracy of our project. From coming up with various ideas for the project to being able to actually implement them was a mammoth task for me as well. \n\nAnother challenge which i am very happy about that i was able to implement was the extra credit part of our project, i.e. to deploy our model on a live web server using Flask Web App.\n\nThis was a difficult project but i have been able to learn quite alot from this challenge and i feel that my grasp over the various topics in the domain of Data Mining has strengthened. I also feel very comfortable with Jupyter Notebook, Kaggle and Google Colab. \n\nI also put a lot of effort and plenty of hours into the Web Hosting Part of our assignment as it would not allow a dataset of 1 gb to be uploaded, hence i had to use Pickle to save my model by serializing it, and then deserialize it and run it. After putting almost 5 to 6 hours into understanding Flask, i am now successfully able to deploy my mmodel on a live web server using Heroku."},{"metadata":{"id":"vPgjTVTKuYVk"},"cell_type":"markdown","source":"# Contribution\nThere are 2 contributions in this project that i can confidently say are my own thinking and my own approach. I have already mentioned them in the Document as and when they come up. \n\nOne of the contributions are Applying a Smoothing Parameter for our accuracy calculation, where we keep a buffer of almost one rating and if the predicted rating is within the range of that one rating with respect to the actual rating, then we consider the predicted rating to be accurate as well.\nThis helped increase the Accuracy of my Model from 30% to 55% in case of normal rounded off ratings. And it boosted the accuracy from 66% to 88% while using the rescaled rating values.\n\nWhich brings me to my second contribution, the approach of rescaling our rating values by dividing all rating values by 2 and rounding them off to get all values within a range of 0 to 5. This majorly boosted our base accuracy to 55% and our Smoothed Accuracy to 88%. That has been my major contribution in this project along with detailed comparison of accuracy for each of the three models."}],"metadata":{"colab":{"name":"DataMiningProject_Version_1.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":4}