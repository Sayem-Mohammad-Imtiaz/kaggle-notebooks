{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"markdown","source":"Importing main libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nimport tensorflow as tf","metadata":{"execution":{"iopub.status.busy":"2021-05-26T12:36:17.566522Z","iopub.execute_input":"2021-05-26T12:36:17.567029Z","iopub.status.idle":"2021-05-26T12:36:23.536188Z","shell.execute_reply.started":"2021-05-26T12:36:17.566926Z","shell.execute_reply":"2021-05-26T12:36:23.535421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we just read our dataframe and delete the first column which is useless in training phase","metadata":{}},{"cell_type":"code","source":"dataframe = pd.read_csv('/kaggle/input/iba-ml1-final-project/train.csv')\ndataframe.drop([\"Id\"], axis = 1, inplace = True)\ndataframe","metadata":{"execution":{"iopub.status.busy":"2021-05-26T12:36:23.537397Z","iopub.execute_input":"2021-05-26T12:36:23.537844Z","iopub.status.idle":"2021-05-26T12:36:23.761222Z","shell.execute_reply.started":"2021-05-26T12:36:23.537815Z","shell.execute_reply":"2021-05-26T12:36:23.760549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"When training the model you can see that the overfitting happening easily, it is because we have little data for the NLP task, and we know that one of the best ways of dealing against overfitting is adding the new data to our dataframe. That is why I have added new data, basically united two dataframe (our competition data, and another data which available in Kaggle). After training the model with this united dataframe, the model gives very very good results.","metadata":{}},{"cell_type":"markdown","source":"Basically in this cell we read that dataframe, change appropriate columns and append it to our main dataframe.However due to new requirement,  this cell was commented, and it will not used in model fitting","metadata":{}},{"cell_type":"code","source":"#dataframe_extra = pd.read_csv('/kaggle/input/womens-ecommerce-clothing-reviews/Womens Clothing E-Commerce Reviews.csv')\n#dataframe_extra.drop(['Unnamed: 0', 'Clothing ID'], axis = 1, inplace = True)\n#dataframe_extra = dataframe_extra.rename(columns={'Title': 'Review_Title',\n#                                                  'Review Text': 'Review',\n#                                                  'Recommended IND':'Recommended',\n#                                                  'Positive Feedback Count':'Pos_Feedback_Cnt',\n#                                                  'Division Name':'Division',\n#                                                  'Department Name':'Department',\n#                                                  'Class Name':'Product_Category'})\n#dataframe = dataframe.append(dataframe_extra).reset_index().drop(\"index\",axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-05-26T12:36:23.762869Z","iopub.execute_input":"2021-05-26T12:36:23.763144Z","iopub.status.idle":"2021-05-26T12:36:23.766249Z","shell.execute_reply.started":"2021-05-26T12:36:23.763117Z","shell.execute_reply":"2021-05-26T12:36:23.765597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we just one hot encode the Rating target value with using get_dummies method in pandas.","metadata":{}},{"cell_type":"code","source":"one_hot_Rating = pd.get_dummies(dataframe.Rating,prefix=\"Rating\")\ndataframe = dataframe.drop('Rating',axis = 1)\ndataframe = dataframe.join(one_hot_Rating)\ndataframe","metadata":{"execution":{"iopub.status.busy":"2021-05-26T12:36:23.767249Z","iopub.execute_input":"2021-05-26T12:36:23.767648Z","iopub.status.idle":"2021-05-26T12:36:23.809579Z","shell.execute_reply.started":"2021-05-26T12:36:23.767619Z","shell.execute_reply":"2021-05-26T12:36:23.808601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's find and drop the records which do not have both Review and Review title, because if both of them are null at the same time, there is no way that we can know what is our target values","metadata":{}},{"cell_type":"code","source":"dataframe.drop(dataframe[dataframe.Review_Title.isnull() & dataframe.Review.isnull()].index, inplace = True)\ndataframe = dataframe.reset_index().drop('index',axis =1)\ndataframe","metadata":{"execution":{"iopub.status.busy":"2021-05-26T12:36:23.810725Z","iopub.execute_input":"2021-05-26T12:36:23.810981Z","iopub.status.idle":"2021-05-26T12:36:23.848459Z","shell.execute_reply.started":"2021-05-26T12:36:23.810954Z","shell.execute_reply":"2021-05-26T12:36:23.847385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we write our new function which just converts the pos_tag which nltk found for the word to wordnet pos_tag. We will need it for lemmatization.","metadata":{}},{"cell_type":"code","source":"from nltk.corpus import wordnet\n\ndef get_pos(word):\n    tag = nltk.pos_tag([word])[0][1][0]\n    tag_dict = {\"J\": wordnet.ADJ,\n                \"N\": wordnet.NOUN,\n                \"V\": wordnet.VERB,\n                \"R\": wordnet.ADV}\n    element = tag_dict.get(tag)\n    if(element == \"J\" or element==\"N\" or element==\"V\" or element==\"R\"):\n        return element\n    return 'n' #it is just like for not getting the error, we pass 'n' if the pos tag couldn't found or is not in the dictionary.","metadata":{"execution":{"iopub.status.busy":"2021-05-21T16:07:05.525035Z","iopub.execute_input":"2021-05-21T16:07:05.525633Z","iopub.status.idle":"2021-05-21T16:07:06.979808Z","shell.execute_reply.started":"2021-05-21T16:07:05.52558Z","shell.execute_reply":"2021-05-21T16:07:06.979031Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In this cell, First we filling missin values with None, and then we apply the classical NLP techniques to the Review and Review_Title columns. First we make all of them lower string, later we remove the stop words, and then lemmatize the review or review_title.","metadata":{}},{"cell_type":"code","source":"import re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\n\n\ndataframe['Review_Title'] = dataframe['Review_Title'].fillna('None')\ndataframe['Review'] = dataframe['Review'].fillna('None')\npd.set_option('mode.chained_assignment', None)\ndataframe.Review = dataframe.Review.str.lower()\ndataframe.Review_Title = dataframe.Review_Title.str.lower()\nfor i in range(0,len(dataframe)):\n    review = re.sub('[^a-zA-z]', ' ', dataframe['Review'][i])\n    review = review.split()\n    wl = WordNetLemmatizer()\n    all_stopwords = stopwords.words('english')\n    all_stopwords.remove('not')    \n    review = [wl.lemmatize(word, pos = get_pos(word)) for word in review if word not in set(all_stopwords)]\n    review = ' '.join(review)\n    dataframe['Review'][i] = review\n    \n    review_title = re.sub('[^a-zA-z]', ' ', dataframe['Review_Title'][i])\n    review_title = review_title.split()\n    review_title = [wl.lemmatize(word, pos = get_pos(word)) for word in review_title if word not in set(all_stopwords)]\n    review_title = ' '.join(review_title)\n    dataframe['Review_Title'][i] = review_title","metadata":{"execution":{"iopub.status.busy":"2021-05-21T16:07:09.844162Z","iopub.execute_input":"2021-05-21T16:07:09.84473Z","iopub.status.idle":"2021-05-21T16:12:33.970477Z","shell.execute_reply.started":"2021-05-21T16:07:09.844679Z","shell.execute_reply":"2021-05-21T16:12:33.96937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataframe[['Review_Title','Review']]","metadata":{"execution":{"iopub.status.busy":"2021-05-21T16:16:06.602016Z","iopub.execute_input":"2021-05-21T16:16:06.602462Z","iopub.status.idle":"2021-05-21T16:16:06.623783Z","shell.execute_reply.started":"2021-05-21T16:16:06.602427Z","shell.execute_reply":"2021-05-21T16:16:06.622494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's split our data to train test split","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_dataframe = dataframe.iloc[:,:-6]\ny_dataframe = dataframe.iloc[:,-6:]\n\nX_train, X_test, y_train, y_test = train_test_split(X_dataframe,y_dataframe, stratify = y_dataframe.Recommended)","metadata":{"execution":{"iopub.status.busy":"2021-05-21T16:16:08.974162Z","iopub.execute_input":"2021-05-21T16:16:08.974868Z","iopub.status.idle":"2021-05-21T16:16:09.025833Z","shell.execute_reply.started":"2021-05-21T16:16:08.974812Z","shell.execute_reply":"2021-05-21T16:16:09.024742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will use functional API for that problem, that is why we wil have 3 different input, and two output. For inputs one for the Review_Title, another for Review, and the last one for other columns. And for outputs one for Recommended and the other for Review","metadata":{}},{"cell_type":"code","source":"X_train_review_title_input = X_train[['Review_Title']]\nX_train_review_input = X_train[['Review']]\nX_train_other_input = X_train[['Age','Pos_Feedback_Cnt','Division','Department','Product_Category']]\n\nX_test_review_title_input = X_test[['Review_Title']]\nX_test_review_input = X_test[['Review']]\nX_test_other_input = X_test[['Age','Pos_Feedback_Cnt','Division','Department','Product_Category']]\n\ny_train_rating_output = y_train[['Rating_1','Rating_2','Rating_3','Rating_4','Rating_5']]\ny_train_recommended_output = y_train[['Recommended']]\n\n\ny_test_rating_output = y_test[['Rating_1','Rating_2','Rating_3','Rating_4','Rating_5']]\ny_test_recommended_output = y_test[['Recommended']]","metadata":{"execution":{"iopub.status.busy":"2021-05-21T16:16:11.376048Z","iopub.execute_input":"2021-05-21T16:16:11.376407Z","iopub.status.idle":"2021-05-21T16:16:11.396547Z","shell.execute_reply.started":"2021-05-21T16:16:11.376378Z","shell.execute_reply":"2021-05-21T16:16:11.395604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let's do some preprocessing steps for the other columns train input. For it, we will create a pipeline and do some appropriate steps (scaling, imputing) for numerical and categorical columns.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\nfrom sklearn.compose import ColumnTransformer\n\n\nprep_other_pipe = Pipeline(steps=[\n    ('preprocessing',ColumnTransformer(transformers=[\n        ('numeric', Pipeline(steps=[\n           ('impute', SimpleImputer(strategy='mean')),\n            ('scaler', RobustScaler())\n        ]), ['Age']),\n        ('categorical', Pipeline(steps=[\n            ('impute', SimpleImputer(strategy='most_frequent')),\n            ('one_hot_encoding', OneHotEncoder(handle_unknown = 'ignore', sparse = False))\n        ]), ['Pos_Feedback_Cnt','Division','Department','Product_Category'])\n    ]))\n])\nX_train_other_input = prep_other_pipe.fit_transform(X_train_other_input)\nX_test_other_input = prep_other_pipe.transform(X_test_other_input)","metadata":{"execution":{"iopub.status.busy":"2021-05-21T16:16:13.789345Z","iopub.execute_input":"2021-05-21T16:16:13.789843Z","iopub.status.idle":"2021-05-21T16:16:14.112287Z","shell.execute_reply.started":"2021-05-21T16:16:13.789799Z","shell.execute_reply":"2021-05-21T16:16:14.110886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_other_input","metadata":{"execution":{"iopub.status.busy":"2021-05-21T16:16:16.607582Z","iopub.execute_input":"2021-05-21T16:16:16.607986Z","iopub.status.idle":"2021-05-21T16:16:16.61542Z","shell.execute_reply.started":"2021-05-21T16:16:16.60795Z","shell.execute_reply":"2021-05-21T16:16:16.614102Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let's Vectorize our Review and Review_Title columns. I will use 500  Vocabulary size for Review Title and 10000 Vocabulary size for Review","metadata":{}},{"cell_type":"code","source":"Vocab_Size_Review_Title = 5000\nencoder_RTitle = tf.keras.layers.experimental.preprocessing.TextVectorization(max_tokens=Vocab_Size_Review_Title)\nencoder_RTitle.adapt(np.array(X_train_review_title_input))\n#encoder_RTitle.get_vocabulary()","metadata":{"execution":{"iopub.status.busy":"2021-05-21T16:17:22.072184Z","iopub.execute_input":"2021-05-21T16:17:22.072595Z","iopub.status.idle":"2021-05-21T16:17:22.379898Z","shell.execute_reply.started":"2021-05-21T16:17:22.072561Z","shell.execute_reply":"2021-05-21T16:17:22.378921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Vocab_Size_Review = 10000\nencoder_Review = tf.keras.layers.experimental.preprocessing.TextVectorization(max_tokens=Vocab_Size_Review)\nencoder_Review.adapt(np.array(X_train_review_input))\n#encoder_Review.get_vocabulary()","metadata":{"execution":{"iopub.status.busy":"2021-05-21T16:17:32.508241Z","iopub.execute_input":"2021-05-21T16:17:32.50859Z","iopub.status.idle":"2021-05-21T16:17:33.090008Z","shell.execute_reply.started":"2021-05-21T16:17:32.508562Z","shell.execute_reply":"2021-05-21T16:17:33.089023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"markdown","source":"Now let's create our model. As I said, we will use function api for it.","metadata":{}},{"cell_type":"code","source":"from tensorflow import keras\nfrom tensorflow.keras import layers\n\ninput_review_title = keras.Input(shape=(None,), name=\"review_title\")\ninput_review = keras.Input(shape=(None,), name=\"review\")\ninput_other_columns = keras.Input(shape=(X_train_other_input.shape[1],), name=\"other_columns\")\n\nembedding_review_title = layers.Embedding(len(encoder_RTitle.get_vocabulary()),500)(input_review_title)\nembedding_review = layers.Embedding(len(encoder_Review.get_vocabulary()),500)(input_review)\n\nlstm_review_title = layers.LSTM(16,name = 'lstm_review_title')(embedding_review_title)\nlstm_review = layers.LSTM(32,name = 'lstm_review')(embedding_review)\ndense_other_columns = layers.Dense(2, name='dense_other_columns')(input_other_columns)\n\nconcat_layer = layers.concatenate([lstm_review_title,lstm_review,dense_other_columns])\n\nrating_pred = layers.Dense(5,activation ='softmax',name=\"rating\")(concat_layer)\nrecommended_pred = layers.Dense(1,activation ='sigmoid',name=\"recommended\")(concat_layer)\n\nmodel = keras.Model(\n    inputs=[input_review_title, input_review,input_other_columns],\n    outputs=[rating_pred, recommended_pred],\n)","metadata":{"execution":{"iopub.status.busy":"2021-05-21T16:17:34.846295Z","iopub.execute_input":"2021-05-21T16:17:34.846658Z","iopub.status.idle":"2021-05-21T16:17:35.465503Z","shell.execute_reply.started":"2021-05-21T16:17:34.846627Z","shell.execute_reply":"2021-05-21T16:17:35.464542Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's visualize our model","metadata":{}},{"cell_type":"code","source":"keras.utils.plot_model(model,show_shapes=True)","metadata":{"execution":{"iopub.status.busy":"2021-05-21T16:17:36.64563Z","iopub.execute_input":"2021-05-21T16:17:36.64602Z","iopub.status.idle":"2021-05-21T16:17:37.139755Z","shell.execute_reply.started":"2021-05-21T16:17:36.645986Z","shell.execute_reply":"2021-05-21T16:17:37.13893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will use adam optimizer, and as a loss functions we will use categorical_crossentropy for rating and binary_crossentropy for recommended. And as a metric we will use accuracy score for both of them","metadata":{}},{"cell_type":"code","source":"model.compile(optimizer='adam', loss = {\"rating\": \"categorical_crossentropy\",\n                                        \"recommended\": \"binary_crossentropy\"},\n                                metrics={'rating': 'accuracy', \n                                         'recommended': 'accuracy'})","metadata":{"execution":{"iopub.status.busy":"2021-05-21T16:17:39.237765Z","iopub.execute_input":"2021-05-21T16:17:39.238174Z","iopub.status.idle":"2021-05-21T16:17:39.260803Z","shell.execute_reply.started":"2021-05-21T16:17:39.238137Z","shell.execute_reply":"2021-05-21T16:17:39.25982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will have two callback, one for loss and other for validation rating accuracy with patience 3. And for fitting the mmodel we will have 25 epoch with batch size 32","metadata":{}},{"cell_type":"code","source":"callback_loss = tf.keras.callbacks.EarlyStopping(monitor='loss',patience = 5)\ncallback_accuracy = tf.keras.callbacks.EarlyStopping(monitor='val_rating_accuracy', patience = 3)\nhistory = model.fit({\"review_title\": encoder_RTitle(X_train_review_title_input),\n                     \"review\": encoder_Review(X_train_review_input),\n                     \"other_columns\": X_train_other_input}, \n                    {\"rating\": y_train_rating_output,\n                     \"recommended\":y_train_recommended_output}, \n                    epochs=30,batch_size=128, verbose=0, \n                    validation_data = ({\"review_title\": encoder_RTitle(X_test_review_title_input),\n                                        \"review\": encoder_Review(X_test_review_input),\n                                        \"other_columns\": X_test_other_input},\n                                         {\"rating\": y_test_rating_output,\n                                           \"recommended\":y_test_recommended_output}),\n                   callbacks=[callback_loss, callback_accuracy])","metadata":{"execution":{"iopub.status.busy":"2021-05-21T16:17:40.930102Z","iopub.execute_input":"2021-05-21T16:17:40.930634Z","iopub.status.idle":"2021-05-21T16:22:39.705529Z","shell.execute_reply.started":"2021-05-21T16:17:40.930601Z","shell.execute_reply":"2021-05-21T16:22:39.704599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Interpretation","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nprint(history.history.keys())\n\nfig, axs = plt.subplots(2, 2, figsize = (20,10))\naxs[0, 0].plot(history.history['rating_loss'])\naxs[0, 0].set_title('rating_loss')\naxs[0, 1].plot(history.history['recommended_loss'], 'tab:orange')\naxs[0, 1].set_title('recommended_loss]')\naxs[1, 0].plot(history.history['val_rating_loss'], 'tab:green')\naxs[1, 0].set_title('val_rating_loss')\naxs[1, 1].plot(history.history['val_recommended_loss'], 'tab:red')\naxs[1, 1].set_title('val_recommended_loss')","metadata":{"execution":{"iopub.status.busy":"2021-05-21T14:22:47.37427Z","iopub.execute_input":"2021-05-21T14:22:47.374658Z","iopub.status.idle":"2021-05-21T14:22:47.393681Z","shell.execute_reply.started":"2021-05-21T14:22:47.374618Z","shell.execute_reply":"2021-05-21T14:22:47.392131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Applying to the test set","metadata":{}},{"cell_type":"markdown","source":"In these following 5 columns, we will do the same EDA processes for our test set","metadata":{}},{"cell_type":"code","source":"test_dataframe = pd.read_csv('/kaggle/input/iba-ml1-final-project/test.csv')\ntest_dataframe_Id = test_dataframe[\"Id\"]\ntest_dataframe.drop([\"Id\"], axis = 1, inplace = True)\ntest_dataframe","metadata":{"execution":{"iopub.status.busy":"2021-05-20T00:12:28.73343Z","iopub.execute_input":"2021-05-20T00:12:28.733777Z","iopub.status.idle":"2021-05-20T00:12:28.810906Z","shell.execute_reply.started":"2021-05-20T00:12:28.733747Z","shell.execute_reply":"2021-05-20T00:12:28.80968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Applying the same NLP techniques","metadata":{}},{"cell_type":"code","source":"test_dataframe['Review_Title'] = test_dataframe['Review_Title'].fillna('None')\ntest_dataframe['Review'] = test_dataframe['Review'].fillna('None')\n#Now\ntest_dataframe.Review = test_dataframe.Review.str.lower()\ntest_dataframe.Review_Title = test_dataframe.Review_Title.str.lower()\nfor i in range(0,len(test_dataframe)):\n    review = re.sub('[^a-zA-z]', ' ', test_dataframe['Review'][i])\n    review = review.split()\n    wl = WordNetLemmatizer()\n    all_stopwords = stopwords.words('english')\n    all_stopwords.remove('not')    \n    review = [wl.lemmatize(word, pos = get_pos(word)) for word in review if word not in set(all_stopwords)]\n    review = ' '.join(review)\n    test_dataframe['Review'][i] = review\n    \n    review_title = re.sub('[^a-zA-z]', ' ', test_dataframe['Review_Title'][i])\n    review_title = review_title.split()\n    review_title = [wl.lemmatize(word, pos = get_pos(word)) for word in review_title if word not in set(all_stopwords)]\n    review_title = ' '.join(review_title)\n    test_dataframe['Review_Title'][i] = review_title","metadata":{"execution":{"iopub.status.busy":"2021-05-20T00:12:30.99795Z","iopub.execute_input":"2021-05-20T00:12:30.998311Z","iopub.status.idle":"2021-05-20T00:13:43.997636Z","shell.execute_reply.started":"2021-05-20T00:12:30.998269Z","shell.execute_reply":"2021-05-20T00:13:43.996315Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Seperate columns for input","metadata":{}},{"cell_type":"code","source":"test_df_review_title_input = test_dataframe[['Review_Title']]\ntest_df_review_input = test_dataframe[['Review']]\ntest_df_other_input = test_dataframe[['Age','Pos_Feedback_Cnt','Division','Department','Product_Category']]","metadata":{"execution":{"iopub.status.busy":"2021-05-20T00:13:43.999523Z","iopub.execute_input":"2021-05-20T00:13:43.999847Z","iopub.status.idle":"2021-05-20T00:13:44.008156Z","shell.execute_reply.started":"2021-05-20T00:13:43.999816Z","shell.execute_reply":"2021-05-20T00:13:44.007223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Transform them (imputing, scaling) with using the same object of main dataframe","metadata":{}},{"cell_type":"code","source":"test_df_other_input = prep_other_pipe.transform(test_df_other_input)","metadata":{"execution":{"iopub.status.busy":"2021-05-20T00:13:44.009661Z","iopub.execute_input":"2021-05-20T00:13:44.01017Z","iopub.status.idle":"2021-05-20T00:13:44.05089Z","shell.execute_reply.started":"2021-05-20T00:13:44.010137Z","shell.execute_reply":"2021-05-20T00:13:44.050146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let's predict our model","metadata":{}},{"cell_type":"code","source":"test_pred = model.predict({\"review_title\": encoder_RTitle(test_df_review_title_input),\n                           \"review\": encoder_Review(test_df_review_input),\n                           \"other_columns\": test_df_other_input})","metadata":{"execution":{"iopub.status.busy":"2021-05-20T00:13:44.060948Z","iopub.execute_input":"2021-05-20T00:13:44.06129Z","iopub.status.idle":"2021-05-20T00:13:49.544432Z","shell.execute_reply.started":"2021-05-20T00:13:44.061258Z","shell.execute_reply":"2021-05-20T00:13:49.543034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Change the probabilities to appropriate numbers.","metadata":{}},{"cell_type":"code","source":"rating_answer = np.argmax(np.array(test_pred[0]),axis=1)+1\nrecom_answer = (test_pred[1] > 0.5).astype(int).flatten()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T00:13:49.546005Z","iopub.execute_input":"2021-05-20T00:13:49.54704Z","iopub.status.idle":"2021-05-20T00:13:49.55305Z","shell.execute_reply.started":"2021-05-20T00:13:49.546939Z","shell.execute_reply":"2021-05-20T00:13:49.552143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's put them to the pandas dataframe with proper columns","metadata":{}},{"cell_type":"code","source":"df_answer = pd.DataFrame({\"Id\":test_dataframe_Id,\n                          \"Rating\":rating_answer,\n                          \"Recommended\": recom_answer})\ndf_answer","metadata":{"execution":{"iopub.status.busy":"2021-05-20T00:13:49.554466Z","iopub.execute_input":"2021-05-20T00:13:49.555042Z","iopub.status.idle":"2021-05-20T00:13:49.578475Z","shell.execute_reply.started":"2021-05-20T00:13:49.555003Z","shell.execute_reply":"2021-05-20T00:13:49.577095Z"},"trusted":true},"execution_count":null,"outputs":[]}]}