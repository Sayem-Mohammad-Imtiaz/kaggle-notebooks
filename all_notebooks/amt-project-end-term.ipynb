{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Vehicle Loan EMI Default Prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\n#Scikit learn librairies\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import scale\n\nimport pandas as pd\nimport numpy as np\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df= pd.read_csv('../input/ltfs-av-data/train.csv')\ndftmp= pd.read_csv('../input/ltfs-av-data/train.csv')\ndftmp2= pd.read_csv('../input/ltfs-av-data/train.csv')\n#dfe= pd.read_csv('test.csv')\n#dftest= pd.read_csv('loan_car_short.csv')\n#df=dftest\n#print(dftest.shape,df.shape,df.columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# # Data Structure"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We plot the distribution of the target"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"fig11=plt.figure()\nax11=plt.axes()\nthe_target = dftmp['loan_default']\nthe_target.replace(to_replace=[1,0], value= ['YES','NO'], inplace = True)\nplt.title('Target repartition')\nax11 = ax11.set(xlabel='Default proportion', ylabel='Number of people')\nthe_target.value_counts().plot.bar()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We want to plot the correlation matrix of the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Correlation Matrix calculation\ncorr_mat = df.corr()\n\nfig2=plt.figure()\nsns.set(rc={'figure.figsize':(20,15)})\nk = 10\ncols = corr_mat.nlargest(k, 'loan_default')['loan_default'].index\ncm = np.corrcoef(df[cols].values.T)\nsns.set(font_scale=1.25)\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\nplt.title('Correlation Matrix')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_mat['loan_default'].sort_values(ascending = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Some features have to high or to low values, we want to delete them to help our model on regular values.\nWe are targeting columns that have disproportionated values. We are dropping to high value to concentrate more on mid range value.\nCustumers with extreme values (good or bad) are easy to predict, we want to improve our model on average custumer"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Delete too high or too low values\ndef delete_absurd_values(df_transformed,cols,max_value,percentage):\n        \n        \n        for col in cols:\n            if (df_transformed[col].dtypes !='object'):\n                       \n                q99=df_transformed[col].quantile(q=percentage)\n                q01=df_transformed[col].quantile(q=(1-percentage))\n                for i in df_transformed.index:\n                    \n                    if (df_transformed.loc[i,col]> max_value*q99 or df_transformed.loc[i,col]< q01/max_value):\n                        df_transformed=df_transformed.drop(index=i)\n        \n        return df_transformed\n\ncols=['disbursed_amount', 'asset_cost', 'PRI.NO.OF.ACCTS', 'PRI.ACTIVE.ACCTS','PRI.OVERDUE.ACCTS','PRI.CURRENT.BALANCE', 'PRI.SANCTIONED.AMOUNT','PRI.SANCTIONED.AMOUNT',\n       'PRI.DISBURSED.AMOUNT', 'SEC.NO.OF.ACCTS', 'SEC.ACTIVE.ACCTS','SEC.CURRENT.BALANCE', 'SEC.SANCTIONED.AMOUNT',\n       'SEC.DISBURSED.AMOUNT', 'PRIMARY.INSTAL.AMT', 'SEC.INSTAL.AMT',\n       'NEW.ACCTS.IN.LAST.SIX.MONTHS', 'DELINQUENT.ACCTS.IN.LAST.SIX.MONTHS',\n       'AVERAGE.ACCT.AGE', 'CREDIT.HISTORY.LENGTH', 'NO.OF_INQUIRIES']\ndf=delete_absurd_values(df,cols,5,0.999)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's plot the repartition of the target"},{"metadata":{"trusted":true},"cell_type":"code","source":"#The repartition of the target\nfig7=plt.figure()\nax7=plt.axes()\nthe_target = dftmp2['loan_default']\nthe_target.replace(to_replace=[1,0], value= ['YES','NO'], inplace = True)\nplt.title('Target repartition')\nax7 = ax7.set(xlabel='Default proportion')\nthe_target.value_counts().plot.pie()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Printing the types of the features\ndf.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"MISSING VALUES"},{"metadata":{"trusted":true},"cell_type":"code","source":"def nan_count_df(df_to_print):\n    \n    nan_count = df_to_print.isnull().sum()\n\n    nan_percentage = (nan_count / len(df))*100\n\n    nan_df=pd.concat([nan_percentage], axis=1)\n    nan_df=nan_df.rename(columns={0:'Percentage'})\n    nan_df=nan_df[nan_df.Percentage != 0]\n    nan_df = nan_df.sort_values(by='Percentage',ascending=False)\n    return nan_df\n\nnan_count_df(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.fillna(df.mode().iloc[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So there is only one column with missing values: \"Employment Type\""},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"#Number of unique values\ndf.nunique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" Changing Column Names"},{"metadata":{"trusted":true},"cell_type":"code","source":"df=df.rename(columns={'Date.of.Birth': 'Date_of_Birth','Employment.Type': 'Employment_Type', 'PERFORM_CNS.SCORE.DESCRIPTION': 'PERFORM_CNS_SCORE_DESCRIPTION'})\n\ndf.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Changing Date_Of_Birth to Age ( age of customer )"},{"metadata":{"trusted":true},"cell_type":"code","source":"now = pd.Timestamp('now')\ndf['Date_of_Birth'] = pd.to_datetime(df['Date_of_Birth'], format='%d-%m-%y')\ndf['Date_of_Birth'] = df['Date_of_Birth'].where(df['Date_of_Birth'] < now, df['Date_of_Birth'] -  np.timedelta64(100, 'Y'))\ndf['Age'] = (now - df['Date_of_Birth']).astype('<m8[Y]')\ndf=df.drop('Date_of_Birth',axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(df['Age'], color = 'blue')\nplt.title('Distribution of Age')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Label Encoding and One-Hot Encoding"},{"metadata":{},"cell_type":"markdown","source":"We want to create a function for encoding the two categories variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating a function for encoding 2 categories features\ndef two_cat_encoding(df_to_transf):\n    le = LabelEncoder()\n\n    for cols in df_to_transf:\n        if df_to_transf[cols].dtype == 'object':\n            if len(list(df_to_transf[cols].unique())) == 2:\n                le.fit(df_to_transf[cols])\n                df_to_transf[cols] = le.transform(df_to_transf[cols])\n    return df_to_transf\ndf=two_cat_encoding(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Reducing 20 different categories of 'PERFORM_CNS_SCORE_DESCRIPTION' to 15 categories."},{"metadata":{"trusted":true},"cell_type":"code","source":"df['PERFORM_CNS_SCORE_DESCRIPTION'].replace(to_replace=['Not Scored: More than 50 active Accounts found', 'Not Scored: No Activity seen on the customer (Inactive)','Not Scored: No Updates available in last 36 months','Not Enough Info available on the customer','Not Scored: Only a Guarantor','Not Scored: Sufficient History Not Available','Not Scored: Not Enough Info available on the customer'], value= 'Not Scored', inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Dropping unwanted columns."},{"metadata":{"trusted":true},"cell_type":"code","source":"columns_to_drop = ['UniqueID','MobileNo_Avl_Flag','DisbursalDate','AVERAGE.ACCT.AGE','CREDIT.HISTORY.LENGTH','SEC.OVERDUE.ACCTS']\ndf=df.drop(columns=columns_to_drop)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Encoding the features."},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.get_dummies(df)\ndf.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The dataset after encoding have metric values."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.dtypes.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Spliting the data between training and testing"},{"metadata":{"trusted":true},"cell_type":"code","source":"X =df.drop('loan_default',axis=1)\ny = df['loan_default']  \n\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"  # Logistic Regression Implementation"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlogisticRegr = LogisticRegression(C=1.0, class_weight=None,fit_intercept=True,max_iter=100)\nlogisticRegr.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#ERROR\nerror = (1 - logisticRegr.score(X_test, y_test))*100\nprint('Score  = ',logisticRegr.score(X_test, y_test)*100, '%','\\nErreur = %f' % error, '%')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Confusion Matrix\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix\npredictions = logisticRegr.predict(X_test)\n\n#print(classification_report(y_test,predictions))\n#print('\\n')\nprint(confusion_matrix(y_test,predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Success rate by model:\\n\\nLogistic Regression:',logisticRegr.score(X_test, y_test)*100,'%')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.0"}},"nbformat":4,"nbformat_minor":4}