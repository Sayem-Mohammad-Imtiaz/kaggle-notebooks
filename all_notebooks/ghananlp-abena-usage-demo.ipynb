{"cells":[{"metadata":{},"cell_type":"markdown","source":"# [GhanaNLP](https://ghananlp.org) - ABENA Usage DEMO\n\nThis is a short demo on how to use our ABENA (and BAKO) family of transformer-based language models for Akuapem and Asante Twi"},{"metadata":{},"cell_type":"markdown","source":"## Fill-in-the-blanks"},{"metadata":{},"cell_type":"markdown","source":"First, Use the [Hugging Face](https://huggingface.co/) transformers *pipelines* API to fill-in-the blanks for a sample sentence using the Akan/Twi BERT Model ABENA"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Pick any one of the following Twi models\n#MODEL = \"Ghana-NLP/abena-base-akuapem-twi-cased\" # (Akuapem ABENA) mBERT fine-tuned on JW300 Akuapem Twi, cased\n#MODEL = \"Ghana-NLP/abena-base-asante-twi-uncased\" # (Asante ABENA) Akuapem ABENA fine-tuned on Asante Twi Bible, uncased\nMODEL = \"Ghana-NLP/distilabena-base-akuapem-twi-cased\" # (Akuapem DistilABENA) DistilmBERT fine-tuned on JW300 Akuapem Twi, cased\n#MODEL = \"Ghana-NLP/distilabena-base-v2-akuapem-twi-cased\" # (Akuapem DistilABENA V2) DistilmBERT fine-tuned on JW300 Akuapem Twi with Twi-only tokenizer trained from scratch, cased\n#MODEL = \"Ghana-NLP/distilabena-base-asante-twi-uncased\" # (Asante DistilABENA) Akuapem DistilABENA fine-tuned on Asante Bible, uncased\n#MODEL = \"Ghana-NLP/distilabena-base-v2-asante-twi-uncased\" # (Asante DistilABENA V2) Akuapem DistilABENA V2 fine-tuned on Asante Bible, uncased\n#MODEL = \"Ghana-NLP/robako-base-akuapem-twi-cased\" # (Akuapem RoBAKO) RoBERTa trained from scratch on JW300 Akuapem Twi, cased [note - use <mask> not [MASK] to represent blank in sentence]\n#MODEL = \"Ghana-NLP/robako-base-asante-twi-uncased\" # (Asante RoBAKO) Akuapem RoBAKO fine-tuned on Asante Twi Bible, uncased [note - use <mask> not [MASK] to represent blank in sentence]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from transformers import pipeline\n\nfill_mask = pipeline(\n    \"fill-mask\",\n    model=MODEL,\n    tokenizer=MODEL\n)\n\nprint(fill_mask(\"kwame yɛ panyin [MASK].\")) # if using ABENA\n\n#print(fill_mask(\"Saa tebea yi maa me papa <mask>.\")) # if using BAKO\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Another sentence:"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(fill_mask(\" Mayɛ basaa, da mu no nyinaa mede [MASK] nenam \")) # if using ABENA\n\n#print(fill_mask(\"Eyi de ɔhaw kɛse baa <mask> hɔ.\")) # if using BAKO","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Encode a Sentence With Tokenizer"},{"metadata":{},"cell_type":"markdown","source":"This is how you would load the model and tokenizer for further use by [transformers](https://github.com/huggingface/transformers):"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForMaskedLM\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL)\nmodel = AutoModelForMaskedLM.from_pretrained(MODEL)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For instance, let us tokenize and encode a sentence:"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"input_ids = tokenizer(\"Mayɛ basaa, da mu no nyinaa mede awerɛhow nenam\", return_tensors=\"pt\")[\"input_ids\"] # these are indices of tokens in the vocabulary\nprint(input_ids)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To see what exactly these tokens are, decode them:"},{"metadata":{"trusted":true},"cell_type":"code","source":"decoded_tokens = [tokenizer.decode(int(el)) for el in input_ids[0]]\nprint(decoded_tokens)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can clearly see the subword nature of the tokenization from this."},{"metadata":{},"cell_type":"markdown","source":"## Extract Vector Representation For Sentence"},{"metadata":{},"cell_type":"markdown","source":"This function will extract an average of pretrained vectors of all tokens in a sentence (variable `average_vec`). You can modify this function to get embeddings for all individual tokens before the average back (`embedding_vecs`) or the CLS (always first in BERT-type models) token used in BERT for average representation of the entire sequence (`CLS_embedding_vec`). Depending on the application, one of these might work better than others. "},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport numpy as np\n\ndef get_embedding(in_str,model):\n    input_ids = torch.tensor(tokenizer.encode(in_str)).unsqueeze(0)  # Batch has size 1\n    outputs = model(input_ids)\n    last_hidden_states = outputs[1]  # The embedding vectors are a tuple of length equal to number of layers\n    embedding_vecs = last_hidden_states[-1].detach().numpy()[0] # these vectors are n_tokens by 768 in size\n    CLS_embedding_vec = embedding_vecs[0] # the CLS token is usually used as the average representation for classification\n    average_vec = np.average(embedding_vecs[1:],axis=0) # averaging remaining vectors instead for similarity task yields slightly better results\n    return average_vec\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To use this function, you need to make sure the model outputs the hidden states, which are the representation vectors we are looking for"},{"metadata":{"trusted":true},"cell_type":"code","source":"from transformers import AutoConfig\nconfig = AutoConfig.from_pretrained(MODEL)\nconfig.output_hidden_states=True\nmodel = AutoModelForMaskedLM.from_pretrained(MODEL,config=config)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So now, let's actually use this function to get an average vector representation for a twi sentence."},{"metadata":{"trusted":true},"cell_type":"code","source":"vec = get_embedding(\"Eyi de ɔhaw kɛse baa fie hɔ\",model)\nprint(\"The vector representation of the sentence is:\")\nprint(vec)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"The shape of the vector is:\")\nprint(vec.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Classification Example\n\nWe built a simple sentiment analysis dataset of just 20 samples and want to see if we can use the features shown above to classify them.\n\nFirst, let's load our dataset and display it."},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas\n\ndata_df = pandas.read_csv(\"../input/twi-sentiment-analysis-unit-dataset/sentiment_analysis_unit_dataset.csv\")\ndata_df = data_df.sample(frac=1) # shuffle\nprint(data_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = data_df[:14][\"Sentence\"].values # use 14 out of the 20 as training, i.e., val ratio of 30%\ntrain_labels = data_df[:14][\"Label (1 is +ve)\"].values\ntest_data = data_df[14:][\"Sentence\"].values # use 6 out of the 20 as testing\ntest_labels = data_df[14:][\"Label (1 is +ve)\"].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(test_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Checking testing data:\")\nprint(test_data)\nprint(test_labels)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, vectorize the dataset, fit and test a simple nearest neighbour classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_list = [get_embedding(sent,model) for sent in train_data] # vectorize/generate features for training\nX_train = np.asarray(X_train_list)\ny_train = train_labels\nprint(\"Training data shape is:\")\nprint(X_train.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test_list = [get_embedding(sent,model) for sent in test_data] # vectorize/generate features for testing\nX_test = np.asarray(X_test_list)\ny_test = test_labels\nprint(\"Testing data shape is:\")\nprint(X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, we are ready to train the classifier and test it... First train:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier # use a simple sklearn nearest neighbor classifier\n\nCLF = KNeighborsClassifier(n_neighbors=1)\nCLF.fit(X_train, y_train)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Then test:"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = CLF.predict(X_test)\nprint(y_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Compute Accuracy:"},{"metadata":{"trusted":true},"cell_type":"code","source":"np.average(y_test==y_pred)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}