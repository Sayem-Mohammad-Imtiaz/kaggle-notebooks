{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Power Quality Classification using Muti Layer Perceptron (Dataset 1)","metadata":{}},{"cell_type":"markdown","source":"This notebook focusses on developing a Multi Layer perceptron which classifies a particular power signal into its respective power quality condition. The dataset used here contains signals which belong to one of the 5 classes(power quality condition). The sampling rate of this data is 128. This means that each signal is characterized by 128 data points. Here the signals provided are in time domain.\n\nThe power quality condition with respect to the output class value is as follows: <br>\n1 - Normal<br>\n2 - 3rd harmonic wave<br>\n3 - 5th harmonic wave<br>\n4 - Voltage dip<br>\n5 - transient<br>","metadata":{}},{"cell_type":"code","source":"#importing the required libraries\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\nimport datetime\nfrom scipy.fft import fft,fftfreq\nfrom sklearn.preprocessing import StandardScaler\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Activation\nfrom tensorflow.keras.optimizers import Adam","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#loading the dataset using pandas\ndata = pd.read_csv(\"../input/powerqualitydistributiondataset1/PowerQualityDistributionDataset1.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#The dataset is already preprocessed\ndata.drop(data.columns[[0]],axis=1,inplace=True)\ndata.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#here we are constructing the array which will finally contain the column names\nheader =[]\nfor i in range(1,129):\n    header.append(\"Col\"+str(i))\ndata_out = data['output']    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.drop(['output'],axis=1,inplace=True)\ndata_arr = data.to_numpy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_arr.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data transformation","metadata":{}},{"cell_type":"markdown","source":"The data transformation steps employed here are as follows:<br>\n\n1) Fourier Transform<br>\n2) Normalization\n","metadata":{}},{"cell_type":"code","source":"#In this segment we are plotting one wave from each class after applying fourier transformation \nw1 = data_arr[0][0:128]\nw1[0:128] = np.abs(fft(w1[0:128]))\nxf = fftfreq(128,1/128)\nplt.plot(xf, w1)\nplt.show()\nprint(\"class\",data_out[0], \"Normal wave\")\n\nw2 = data_arr[1][0:128]\nw2[0:128] = np.abs(fft(w2[0:128]))\nxf = fftfreq(128,1/128)\nplt.plot(xf, w2)\nplt.show()\nprint(\"class\",data_out[1], \"3rd harmonic wave\")\n\nw3 = data_arr[3][0:128]\nw3[0:128] = np.abs(fft(w3[0:128]))\nxf = fftfreq(128,1/128)\nplt.plot(xf, w3)\nplt.show()\nprint(\"class\",data_out[3], \"5th harmonic wave\")\n\nw4 = data_arr[6][0:128]\nw4[0:128] = np.abs(fft(w4[0:128]))\nxf = fftfreq(128,1/128)\nplt.plot(xf, w4)\nplt.show()\nprint(\"class\",data_out[6], \"Voltage dip\")\n\nw5 = data_arr[8][0:128]\nw5[0:128] = np.abs(fft(w5[0:128]))\nxf = fftfreq(128,1/128)\nplt.plot(xf, w5)\nplt.show()\nprint(\"class\",data_out[8], \"Transient wave\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#here we are overwritting the dataframe with the waves which we obtained after doing fourier transformation\nn = data_arr.shape[0]\nfor i in range(0,n):\n    data_arr[i][0:128] = np.abs(fft(data_arr[i][0:128]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_arr.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#here we are performing normalization\ntransform = StandardScaler()\ndata_arr = transform.fit_transform(data_arr)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#converting the numpy array back to data frame\ndata = pd.DataFrame(data_arr,columns=header)\ndata['output'] = data_out","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model creation and training","metadata":{}},{"cell_type":"code","source":"#here we are splitting the dataset in the ratio of 60%,20%,20% (training set,validation set, test set)\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(data.loc[:,data.columns != 'output'],data['output'],test_size=0.2)\nx_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.25, random_state=42)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get_dummies function is used here to perform one hot encoding of the y_* numpy arrays\ny_train_hot = pd.get_dummies(y_train)\ny_test_hot = pd.get_dummies(y_test)\ny_val_hot = pd.get_dummies(y_val)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Training\",x_train.shape)\nprint(y_train_hot.shape)\nprint(\"Validation\",x_val.shape)\nprint(y_val_hot.shape)\nprint(\"Test\",x_test.shape)\nprint(y_test_hot.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Sequential()\nmodel.add(Dense(64, input_shape=(128,), activation = 'relu'))\nmodel.add(Dense(32, activation = 'relu'))\nmodel.add(Dense(16, activation = 'relu'))\nmodel.add(Dense(5, activation = 'softmax'))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"log_dir = \"logs1/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\ntensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n\nmodel.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit(x_train, y_train_hot, batch_size=64, epochs=30, validation_data=(x_val, y_val_hot), callbacks=[tensorboard_callback])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%load_ext tensorboard\n%tensorboard --logdir logs1/fit","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(model.metrics_names)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model evaluation","metadata":{}},{"cell_type":"code","source":"np.mean(history.history['val_accuracy']) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_acc = model.evaluate(x_test,y_test_hot)\nprint(\"Test accuracy is {}\".format(pred_acc))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_test_arr = x_test.to_numpy()\npredict = model.predict(x_test_arr[0:10][:])\npredict_class = np.argmax(predict, axis=1)\npredict_class = np.array(predict_class.tolist())\nprint(predict_class+1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}