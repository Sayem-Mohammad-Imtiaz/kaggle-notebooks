{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom collections import Counter\nfrom pandas import Timestamp\nimport xgboost\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom sklearn.feature_selection import SelectKBest, chi2, f_regression, mutual_info_regression, VarianceThreshold, RFE\nfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.linear_model import LinearRegression, RidgeCV, LassoCV, Ridge, Lasso, LogisticRegression\nfrom mlxtend.feature_selection import SequentialFeatureSelector\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Dataset: https://www.kaggle.com/hmavrodiev/london-bike-sharing-dataset","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In this notebook Exploratory Data Analysis (EDA) was performed using the *London bike sharing dataset* from Kaggle with the goal of extracting useful information from the data in order to implement a Machine Learning model capable of predicting the number of bike rentals for specific days."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_df=pd.read_csv('..//input/london-bike-sharing-dataset/london_merged.csv')\n\n#convert timestamp type to datetime\ndata_df['timestamp'] = pd.to_datetime(data_df['timestamp'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Details of the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_df.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Engineering"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#add column representing the hour of the timestamp\ndata_df.loc[:, 'hour'] = data_df['timestamp'].dt.hour\n\n#add column representing the month of the timestamp\ndata_df.loc[:, 'month'] = data_df['timestamp'].dt.month\n\n#add column representing the day of the week\ndata_df['day_of_week'] = data_df['timestamp'].dt.dayofweek\n\n#add column indicating if it is work day or not\ndata_df['work_day'] = [row['is_holiday'] == 0 and row['is_weekend'] == 0 for i, row in data_df.iterrows()]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Analysis of the **timestamp** column"},{"metadata":{"trusted":true},"cell_type":"code","source":"time_diffs = np.diff(data_df['timestamp'])\ntime_diffs = time_diffs / np.timedelta64(1, 'h')\n\nprint(\"Unique values: \", np.unique(time_diffs))\n\nCounter(time_diffs)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This way we can conclude that the time interval between each sample is 1 hour most of the time, although there are some exceptions"},{"metadata":{"trusted":true},"cell_type":"code","source":"display(min(data_df['timestamp']))\ndisplay(max(data_df['timestamp']))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Data goes from 2015-01-04 to 2017-01-03"},{"metadata":{},"cell_type":"markdown","source":"## Analysis of the **t1**  and **t2** columns"},{"metadata":{},"cell_type":"markdown","source":"Variation of the temperature t1 and t2 in just the fist week: from **2015-01-04 00:00** to **2015-01-11 00:00**"},{"metadata":{"trusted":true},"cell_type":"code","source":"n_days = 7\nplt.figure(figsize=(10,10))\nplt.plot(data_df['timestamp'][:n_days * 24],data_df['t1'][:n_days * 24], label=\"t1\")\nplt.plot(data_df['timestamp'][:n_days * 24],data_df['t2'][:n_days * 24], label=\"t2\")\n\nplt.legend()\nplt.show()\n\nprint('From: {}\\nTo: {}'.format(data_df['timestamp'][0], data_df['timestamp'][n_days * 24 - 1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Variation of the temperature t1 and t2 for the entire data time interval: from **2015-01-04 00:00** to **2017-01-03 23:00**. Tipically the \"feeling like\" temperature is lower than the real one"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfig, ax = plt.subplots(2, 1, figsize=(20,15))\n\nax[0].plot(data_df['timestamp'],data_df['t1'], label=\"t1\")\nax[1].plot(data_df['timestamp'],data_df['t2'], label=\"t2\")\n\nax[0].title.set_text('T1 Feature')\nax[1].title.set_text('T2 Feature')\n\nax[0].set_xlabel('Timestamp')\nax[0].set_ylabel(u'C°')\n\nax[1].set_xlabel('Timestamp')\nax[1].set_ylabel(u'C°')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"By analysing the entire data timestamp, it is possible to see some seasonality over time regarding the t1."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Analysis of the correlation of the temperatue in each year","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_2015 = data_df.loc[(data_df['timestamp'] > Timestamp('2015-01-01 0:0:0')) & (data_df['timestamp'] <= Timestamp('2015-12-31 23:59:59'))]\ndata_2016 = data_df.loc[(data_df['timestamp'] > Timestamp('2016-01-01 0:0:0')) & (data_df['timestamp'] <= Timestamp('2016-12-31 23:59:59'))]\n\n\nfig = plt.figure(figsize=(20, 15))\ngrid = plt.GridSpec(3, 2)\n\n\ntop_ax = fig.add_subplot(grid[0, 0:])\ntop_ax.plot(data_2015['timestamp'], data_2015['t1'], label=\"t1 2015\")\ntop_ax.plot(data_2016['timestamp'], data_2016['t1'], label=\"t1 2016\")\ntop_ax.set_ylabel(u'C°')\n\ntop_ax.legend()\n\n\nmid_ax = fig.add_subplot(grid[1, 0:], sharex = top_ax)\nplt.setp(mid_ax.get_xticklabels(), visible=False)\nmid_ax.plot(data_2015['timestamp'], data_2015['t2'], label=\"t2 2015\")\nmid_ax.plot(data_2016['timestamp'], data_2016['t2'], label=\"t2 2016\")\nmid_ax.invert_yaxis()\nmid_ax.set_ylabel(u'C°')\n\nmid_ax.legend()\n\nleft_ax = plt.subplot(grid[2, 0]);\nleft_ax.plot([i for i in range(0, len(data_2015))], data_2015['t2'], label=\"t2\")\nleft_ax.plot([i for i in range(0, len(data_2016))], data_2016['t2'], label=\"t1\")\n\nright_ax = plt.subplot(grid[2, 1]);\nright_ax.plot([i for i in range(0, len(data_2015))], data_2015['t2'], label=\"t2\")\nright_ax.plot([i for i in range(0, len(data_2016))], data_2016['t2'], label=\"t1\")\n\ntop_ax.title.set_text('T1 and T2 over time')\nleft_ax.title.set_text('T1 Feature')\nright_ax.title.set_text('T2 Feature')\n\nleft_ax.set_xlabel('Timestamp')\nleft_ax.set_ylabel(u'C°')\nleft_ax.legend()\n\nright_ax.set_xlabel('Timestamp')\nright_ax.set_ylabel(u'C°')\nright_ax.legend()\n\n\n\ndisplay('Correlation between T1 and T2 over the years', data_df[['t1', 't2']].corr())\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As can be observed, the T1 and T2 features are highly correlated. Furthermore, the temperature evolution over the year follows a specific pattern, as it is identical in 2015 and 2016"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Analysis of the influence of *weather code*"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(data_df['weather_code'].unique())\n\ndata_df.loc[data_df['weather_code'] == 26, 'weather_code'] = 5\ndata_df.loc[data_df['weather_code'] == 10, 'weather_code'] = 6\ndata_df.loc[data_df['weather_code'] == 94, 'weather_code'] = 8\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"By changing these specific codes, the new weather codes are ordered sequentially in time"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Analysis of the influence of Weather in the number of bikes shares: *t1, t2, hum, wind_speed, weather_code, season*"},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef plot_sensors_hist(df, s_columns):\n\n    fig = plt.figure(figsize=(14, 7))\n    fig.subplots_adjust(wspace=0.4)\n\n    \n    for i, s in enumerate(s_columns):\n\n        ax = plt.subplot(2, 3, i+1)\n        \n        vals = df[s]\n\n        vals.hist()\n        \n        ax.set_ylabel(s)\n        ax.set_xlabel('Time')\n\n    plt.show()\n\n        \nplot_sensors_hist(data_df, ['t1', 't2', 'hum', 'wind_speed', 'weather_code', 'season'])\n\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the plots, the histogram of *t1, t2, hum and wind_speed* are within the expected. However, *weather_code* and *season* feature show strange values distribution. These assymetrical distributions are due the fact that the values follow a specific coding system, namelly:\n\n**weather_code** (updated codes):\n1 = Clear ; mostly clear but have some values with haze/fog/patches of fog/ fog in vicinity\n2 = scattered clouds / few clouds\n3 = Broken clouds\n4 = Cloudy\n5 = snowfall\n6 = rain with thunderstorm\n7 = Rain/ light Rain shower/ Light rain\n8 = Freezing Fog\n\n**season**:\n0-spring ; 1-summer; 2-fall; 3-winter."},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\ndef plot_bar_cnt(df, s_columns, custom_bins):\n    \n    fig = plt.figure(figsize=(25, 10))\n    fig.subplots_adjust(wspace=0.2)\n\n    t_df = df.copy()\n    for i, s in enumerate(s_columns):\n\n        ax = plt.subplot(2, 3, i+1)\n\n        custom_bins[i] = np.round(custom_bins[i], 1)\n        \n        t_df['bins'] = pd.cut(x=t_df[s], bins=custom_bins[i])\n        \n        t_df.groupby('bins').mean()['cnt'].plot.bar(rot=0)\n        \n        ax.set_ylabel('Mean of Nº of bike shares')\n        ax.set_xlabel(s)\n\n    plt.show()\n\n        \ncustom_bins = [np.linspace(min(data_df['t1']), max(data_df['t1']), num=5),\n        np.linspace(min(data_df['t2']), max(data_df['t2']), num=5),\n        np.linspace(min(data_df['hum']), max(data_df['hum']), num=5),\n        np.linspace(min(data_df['wind_speed']), max(data_df['wind_speed']), num=5),\n        np.append(-1, np.linspace(min(data_df['weather_code']), max(data_df['weather_code']), num=len(data_df['weather_code'].unique()))),\n        np.append(-1, np.linspace(min(data_df['season']), max(data_df['season']), num=len(data_df['season'].unique()))),\n       ]\n\nplot_bar_cnt(data_df, ['t1', 't2', 'hum', 'wind_speed', 'weather_code', 'season'], custom_bins)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Some conclusions can be drawn from these plots. Regarding the temperature (t1 and t2), the probability of people riding a rented bike is higher as the temperature increases. The opposite happens with the humidity, this means that people are more likely to rent a bike when the humidity is lower. By observing the wind speed, we can conclude that it does not have a significal impact on the number of bike rides, as the number of bike shares is regular even at different wind speeds. Finally, by analysing the weather_code and the season, we can conclude that the bike rentals are higher when there are fewer clouds (weather_code = 2) in the summer (season = 1)"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef plot_hourly_cnt(df, label):\n    df.groupby(df['hour']).mean()['cnt'].plot(label=label)\n    \n    \n    \nfig, ax = plt.subplots(figsize=(15,10))\n\nplot_hourly_cnt(data_df.loc[(data_df['is_holiday'] == 0) & (data_df['is_weekend'] == 0), :], 'Work Days')\nplot_hourly_cnt(data_df.loc[data_df['is_holiday'] == 1, :], 'Weekend')\nplot_hourly_cnt(data_df.loc[data_df['is_weekend'] == 1, :], 'Holiday')\n\nax.set_ylabel('Nº of bike shares')\nax.title.set_text('Mean of the number of bikes shares along the day')\nax.set_xlabel('Hour of the day (h)')\n\nplt.legend()\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In this graph, we can see that during work days, people tend to ride the rental bikes more during the morning (possibly on their way to work) and in the late afternoon (when leaving their work). During the weekends and holidays, the bike sharing trend is different, as people tend to ride more between the 10am and about 5pm. "},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Analysis of the number of bike sharing, depending the type of the day (work day, weekend, holiday)"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_cnt(df, label, ax):\n    #df.groupby(df['timestamp'].dt.month).mean()['cnt'].plot.bar(label=label, rot=0)\n\n    df.boxplot(column='cnt', by='hour', ax=ax[0])\n    df.boxplot(column='cnt', by='month', ax=ax[1])\n    ax[0].title.set_text('Mean of the number of bikes shares hourly - ' + label)\n    ax[1].title.set_text('Mean of the number of bikes shares monthly - ' + label)\n    ax[0].set_ylabel('Nº of bike shares')\n    ax[1].set_ylabel('Nº of bike shares')\n    \n    \n\nfig, ax = plt.subplots(2, 3, figsize=(30, 15))\n\nplot_cnt(data_df.loc[(data_df['is_holiday'] == 0) & (data_df['is_weekend'] == 0), :], 'Work Days', ax[:, 0])\nplot_cnt(data_df.loc[data_df['is_holiday'] == 1, :], 'Weekend', ax[:, 1])\nplot_cnt(data_df.loc[data_df['is_weekend'] == 1, :], 'Holiday', ax[:, 2])\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As can be observed, the conclusions made previously regarding the number of bike rentals for each type of day (work day, weekend and holiday) are verified by these graphs.\nRegarding the number of bike shares in each month, we can conclude that during the summer people rent more bikes, probably due to the favourable weather."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Analysis of the **correlation** between the different variables with respect to column **cnt**."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plot the data features, in groups of 2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(data_df[['cnt', 't1', 't2', 'hum', 'wind_speed', 'weather_code', 'is_holiday', 'is_weekend', 'season', 'hour', 'month']])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndata_df[['cnt', 't1', 't2', 'hum', 'wind_speed', 'weather_code', 'is_holiday', 'is_weekend', 'season']].corr()['cnt']\n\n\nplt.figure(figsize=(15,15))\n\nsns.set(font_scale=1.1)\n\nax = sns.heatmap(\n    data_df[['cnt', 't1', 't2', 'hum', 'wind_speed', 'weather_code', 'is_holiday', 'is_weekend', 'season']].corr(), \n    vmin=-1, vmax=1, center=0,\n    cmap=sns.diverging_palette(240, 240, l=35, n=20),\n    square=True,\n    annot=True, annot_kws={\"size\": 15}\n)\nax.set_xticklabels(\n    ax.get_xticklabels(),\n    rotation=45,\n    horizontalalignment='center'\n);\n\n           \nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As can be concluded, there are no variables highly correlated with the **cnt column**.\n\nFurthermore, the variables with higher correlation are the **humidity** (negative correlation) and the **temperature** (t1 and t2)\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Prediction of number of bike shares\n\nThe goal is to create a Machine Learning model capable of predicting the number of bikes needed in a certain day"},{"metadata":{},"cell_type":"markdown","source":"#### Detection of Missing values"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_df.isnull().sum().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Split in Train and Test dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(data_df[data_df.columns.difference(['timestamp', 'cnt'])], data_df['cnt'], test_size = 0.2, random_state = 0)\ny_true = data_df['cnt']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Feature Selection"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Filter Methods - univariate selection"},{"metadata":{"trusted":true},"cell_type":"code","source":"#1. K best\n#Regression: f_regression, mutual_info_regression, chi2 (only positive number)\n\nselector = SelectKBest(f_regression, k=4)\nselector = SelectKBest(mutual_info_regression, k=4)\nX_train_new = selector.fit_transform(X_train, y_train)\n\nrelevant_features_f1 = X_train.columns[selector.get_support(indices=True)].values\nprint(\"Feature Selected: \", X_train.columns[selector.get_support(indices=True)].values)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#2. Variance Threshold\n\n#Features with a training-set variance lower than this threshold will be removed. O means removing only constant features\n\nthresholder = VarianceThreshold(threshold=15)\nX_train_new_2 = thresholder.fit_transform(X_train)\n\nrelevant_features_f2 = X_train.columns[selector.get_support(indices=True)].values\nprint(\"Feature Selected: \", X_train.columns[thresholder.get_support(indices=True)].values)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#3. Correlation\n\nentire_X_train = X_train.copy()\nentire_X_train['cnt'] = y_train\n\ncor = entire_X_train[['cnt', 't1', 't2', 'hum', 'wind_speed', 'weather_code', 'is_holiday', 'is_weekend', 'season']].corr()\ncor_target = abs(cor[\"cnt\"])\n\nrelevant_features_f3 = cor_target[cor_target>0.3]\nprint(\"Feature Selected: \", relevant_features_f3.index.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Wrapper Methods"},{"metadata":{"trusted":true},"cell_type":"code","source":"#1. Forward: Adding feature with better results, in each iteration. The Random Forest model will be used\n\nfeature_selector = SequentialFeatureSelector(LinearRegression(),\n           k_features=4,\n           forward=True,\n           verbose=1,\n           scoring='neg_mean_squared_error',\n           cv=3)\n\nmdl = feature_selector.fit(X_train, y_train)\n\nrelevant_features_w1 = X_train.columns[list(mdl.k_feature_idx_)]\nprint(\"Feature Selected: \", relevant_features_w1.values)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#2. Backward: Removing the feature with worse results, in each iteration. The Random Forest model will be used\n\nfeature_selector = SequentialFeatureSelector(LinearRegression(n_jobs=-1),\n           k_features=5,\n           forward=False,\n           verbose=1,\n           scoring='neg_mean_squared_error',\n           cv=3)\n\nmdl = feature_selector.fit(X_train, y_train)\n\nrelevant_features_w2 = X_train.columns[list(mdl.k_feature_idx_)]\nprint(\"Feature Selected: \", relevant_features_w2.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#3. RFE:\n\nmodel = LinearRegression()\nrfe = RFE(model, 5)\nX_rfe = rfe.fit_transform(X_train, y_train)  \n\nmodel.fit(X_rfe,y_train)\n\nrelevant_features_w3 = X_train.columns[rfe.support_]\n#print(rfe.ranking_)\nprint(\"Feature Selected: \", relevant_features_w3.values)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Embedded Methods"},{"metadata":{"trusted":true},"cell_type":"code","source":"#1. Lasso\n\n#1.1 With cross validation to determine the best alpha\n\nreg = LassoCV(cv = 3)\nreg.fit(X_train, y_train)\n\ncoef = pd.Series(reg.coef_, index = X_train.columns)\nrelevant_features_l1 = [i for i in coef.index if coef[i] == 0.0]\n\nprint(\"Feature Selected: \", [i for i in coef.index if coef[i] == 0.0])\n\n\n\n#1.2.1 Simple Lasso (without cv)\n\nreg = Lasso(alpha = 7)\nreg.fit(X_train, y_train)\n\ncoef = pd.Series(reg.coef_, index = X_train.columns)\nrelevant_features_l1\n\nprint(\"Feature Selected: \", [i for i in coef.index if coef[i] == 0.0])\n\n\n\n#1.2.2 Lasso with Gradient Descent: only for classification\n\n#2. Tree based: RF or XGBoost do not work for Regression\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = X_train[relevant_features_w1]\nX_test = X_test[relevant_features_w1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Different types of feature selection models were implemented. The method chosen to be applied was the **Forward Selection**, where the Linear Regression was used for evaluating the different features set performance"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Identification of Categorical Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def convert_to_categorical(df, column_names, y_true):\n    df = pd.concat([X_train, X_test], sort=False)\n    \n    for col in column_names:\n        if col in X_train.columns:\n            df[col] = df[col].astype('category')\n            df = pd.concat([df, pd.get_dummies(df[col], prefix=col, drop_first=True)], axis=1, sort=False)\n            df.drop([col], axis=1, inplace=True)\n    \n    return train_test_split(df, y_true, test_size = 0.2, random_state = 0)\n\n\n#cat_columns = ['weather_code']\n#X_train, X_test, y_train, y_test = convert_to_categorical(data_df, cat_columns, y_true)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The columns *hour*, *month*, *day_of_week*, although they are of type int or float and have a set of possible values, they should not be represented as categorical features, because there is some relation between the values, ex: 1h is closer to 3h than 8h."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Data Normalization"},{"metadata":{"trusted":true},"cell_type":"code","source":"continuous_columns = ['t1', 'hum']\n\nsc = StandardScaler()\nX_train[continuous_columns] = sc.fit_transform(X_train[continuous_columns])\nX_test[continuous_columns] = sc.transform(X_test[continuous_columns])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Application of a ML model"},{"metadata":{"trusted":true},"cell_type":"code","source":"#XGBOOST\n\nparameters = {\n    'max_depth': [1, 5],\n    'n_estimators': [400, 800],\n    'learning_rate': [0.01, 0.05]\n}\n#'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 800\n\n\n#Cross Validation\ngrid_search = GridSearchCV(\n    estimator=xgboost.XGBRegressor(objective ='reg:squarederror'),\n    param_grid=parameters,\n    n_jobs = 10,\n    cv = 10\n)\n\n\ngrid_search.fit(X_train, y_train)\nprint(grid_search.best_params_)\nbest_model = grid_search.best_estimator_\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#sgb = xgboost.XGBRegressor(objective ='reg:squarederror', learning_rate= 0.01, max_dept= 1, n_estimators= 800)\n#best_model = sgb.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Results\n\nresults = best_model.predict(X_test)\n\nprint(\"MSE: \", np.round(mean_squared_error(y_test, results), 2))\nprint(\"MAE: \", np.round(mean_absolute_error(y_test, results), 2))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\nA simple XGBoost model was applied with cross validation for parameter tuning. In the end, the obtained results are interesting, although they can still be improved.\n\nPossible improvements could be the applicatioin of other type of Machine Learning methods, also performing parameter tuning in the ML model, as well as in the chosen method for feature selection.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.3"}},"nbformat":4,"nbformat_minor":1}