{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# -*- coding: utf-8 -*-\n\"\"\"Heart_failure_prediction.ipynb\n\nAutomatically generated by Colaboratory.\n\nOriginal file is located at\n    \n\"\"\"\n\nimport pandas as pd\nimport numpy as np\nimport sklearn as skt\nfrom fancyimpute import KNN\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import chi2_contingency\n\ndf = pd.read_csv(\"/content/drive/MyDrive/All datasets/heart_failure_clinical_records_dataset.csv\")\ndf\n\ndf.dtypes\n\ndf.describe()\n\ndf.isnull().sum()\n\ng=sns.countplot(df.DEATH_EVENT)\n\ndf.keys()\n\nhf=df.copy()\n\ncat_names=['anaemia','diabetes','high_blood_pressure','sex', 'smoking','DEATH_EVENT']\n\nfor i in cat_names:\n  print(i)\n  hf.loc[:,i]=hf.loc[:,i].round()\n  hf.loc[:,i]=hf.loc[:,i].astype('object')\n\nhf.dtypes\n\nlis = []\nfor i in range(0,hf.shape[1]):\n  #print(i)\n  if(hf.iloc[:,i].dtype==\"object\"):\n    hf.iloc[:,i]=pd.Categorical(hf.iloc[:,i])\n    hf.iloc[:,i]=hf.iloc[:,i].cat.codes\n    lis.append(hf.columns[i])\n\nhf.head(5)\n\n# outlier analysis ( Detecting & replacing )\nhf2=hf.copy()\n\ncnames=['age','creatinine_phosphokinase','ejection_fraction', 'platelets','serum_creatinine', 'serum_sodium','time']\n\nfor i in cnames:\n  print(i)\n  q75,q25=np.percentile(hf2.loc[:,i],[75,25])\n  iqr=q75-q25\n  min=q25-(1.5*iqr)\n  max=q75+(1.5*iqr)\n  print(min)\n  print(max)\n  hf2=hf2.replace((hf2[hf2.loc[:,i]<min].index),np.nan)\n  hf2=hf2.replace((hf2[hf2.loc[:,i]>max].index),np.nan)\n\nhf2.head(5)\n\nhf2.isnull().sum()\n\nmissing_values2=pd.DataFrame(hf2.isnull().sum())\nmissing_values2\n\nmissing_values2 = missing_values2.reset_index()\n\nmissing_values2.columns = [\"variables\",\"missing_sum\"]\n\nmissing_values2[\"missing%\"]=missing_values2[\"missing_sum\"]/len(hf2)\nmissing_values2.sort_values(\"missing%\",axis=0,ascending=False).head(5)\n\n# missing_value_imputation\n\nhf2=pd.DataFrame(KNN(k=3).fit_transform(hf2),columns=hf2.columns)\n\nhf2.isnull().sum()\n\nfor i in lis:\n  hf2.loc[:,i]=hf2.loc[:,i].round()\n  hf2.loc[:,i]=hf2.loc[:,i].astype('object')\n\nhf3=hf2.copy()\n\n# Checking_multicolinarity with Corelation\n\n#cnames=['age','creatinine_phosphokinase','ejection_fraction', 'platelets','serum_creatinine', 'serum_sodium','time']\n\nhf3_corr = hf3.loc[:,cnames]\ncorr = hf3_corr.corr()\nhighly_corelated = corr[corr>0.95]\nhighly_corelated\n\n#feature Selection_VIF\n\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nhf3_VIF = hf3.loc[:,cnames]\nvif_data = pd.DataFrame()\nvif_data[\"feature\"] = hf3_VIF.columns\n\n# calculating VIF for each feature\nvif_data[\"VIF\"] = [variance_inflation_factor(hf3_VIF.values, i)\n                          for i in range(len(hf3_VIF.columns))]\n\nprint(vif_data)\n\n# Removing Categorical Variables\n#cat_names=['anaemia','diabetes','high_blood_pressure','sex', 'smoking','DEATH_EVENT']\n\nfor i in cat_names:\n  print(i)\n  chi2,p,dof,ex = chi2_contingency(pd.crosstab(hf3.DEATH_EVENT,hf3[i]))\n  print(chi2,p,dof,ex)\n\n# Dropping Columns with corelation & high P value\n\nhf3=hf3.drop(['age','ejection_fraction','platelets','serum_sodium','anaemia','diabetes','high_blood_pressure','sex','smoking'],axis=1)\n\nhf3.head(5)\n\nhf4=hf3.copy()\n\n#normality check\nplt.hist(hf4.creatinine_phosphokinase,bins=\"auto\")\n\nhf4 = hf4.drop(['DEATH_EVENT'],axis=1)\n\n#Normalisation\nfrom sklearn import preprocessing\nmin_max_scaler = preprocessing.MinMaxScaler()\nhf4= hf4.values #returns a numpy array\nx_scaled = min_max_scaler.fit_transform(hf4)\nhf4 = pd.DataFrame(x_scaled)\nhf4.head(5)\n\nhf4.columns = ['creatinine_phosphokinase','serum_creatinine','time']\n\nhf4.keys()\n\nhf4['DEATH_EVENT']=hf3['DEATH_EVENT']\nhf4.shape\n\n#sampling\ng = sns.countplot(hf4.DEATH_EVENT)\nplt.show()\n\n# Oversampling\n\nclass_count_0,class_count_1=hf4['DEATH_EVENT'].value_counts()\nprint(class_count_0,class_count_1)\nclass_0 = hf4[hf4['DEATH_EVENT'] == 0]\nclass_1 = hf4[hf4['DEATH_EVENT'] == 1]# print the shape of the class\nprint('class 0:', class_0.shape)\nprint('class 1:', class_1.shape)\n\n# Data is limited_Use Oversampling\n\nclass_1_over = class_1.sample(class_count_0,replace=True)\nclass_1_over.shape\ntest_over = pd.concat([class_1_over,class_0],axis=0)\n\nprint(test_over['DEATH_EVENT'].value_counts())\n\ng1 = sns.countplot(test_over['DEATH_EVENT'])\nplt.show()\n\nhf5 = test_over.copy()\n\nhf5.keys()\n\nfrom sklearn.model_selection import train_test_split\n\n# Model Development\n\nX_over = hf5.values[:,0:3]\nY_over = hf5.values[:,3]\nY_over=Y_over.astype('int')\nX_over_train,X_over_test,Y_over_train,Y_over_test = train_test_split(X_over,Y_over, test_size = 0.3, random_state = 42)\n\n# logistic regression\nfrom sklearn.linear_model import LogisticRegression\nlr_over = LogisticRegression()\n\nlogit_model= lr_over.fit(X_over_train,Y_over_train)\n\nY_Over_Pred = lr_over.predict(X_over_test)\n\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\n\nprint(recall_score(Y_over_test,Y_Over_Pred))\nprint(accuracy_score(Y_over_test,Y_Over_Pred))\nprint(confusion_matrix(Y_over_test,Y_Over_Pred))\n\nCM_Crosstab = pd.crosstab(Y_over_test,Y_Over_Pred)\nCM_Crosstab.columns=[\"No\",\"Yes\"]\nCM_Crosstab.index = [\"No\",\"Yes\"]\nCM_Crosstab\n\nFNR = (CM_Crosstab.iloc[1,0]/(CM_Crosstab.iloc[1,1]+CM_Crosstab.iloc[1,0]))*100\nFNR\n\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import RocCurveDisplay\nfrom sklearn import metrics\nfpr, tpr, thresholds = roc_curve(Y_over_test,Y_Over_Pred)\nroc_auc = metrics.auc(fpr, tpr)\ndisplay = RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc, estimator_name='Logistic Regression')\ndisplay.plot()  \nplt.show()\n\n# Decision Tree\nfrom sklearn import tree\ndt = tree.DecisionTreeClassifier(criterion=\"entropy\",max_depth=3, random_state=1234) #DecisionTreeClassifier(max_depth=3, random_state=1234)\n\nc50_model = dt.fit(X_over_train,Y_over_train)\nY_Over_Pred_C50 = dt.predict(X_over_test)\n\nfrom sklearn import tree\n\n#clf = tree.DecisionTreeClassifier(max_leaf_nodes=n)\n#clf_ = clf.fit(X, data_y)\nX_over_train_1=pd.DataFrame(X_over_train)\nX_over_train_1.columns = ['creatinine_phosphokinase', 'serum_creatinine', 'time']\nfeature_names = X_over_train_1.columns\nclass_name = c50_model.classes_.astype(int).astype(str)\n\ndef output_pdf(c50_model, name):\n    from sklearn import tree\n    from sklearn.externals.six import StringIO\n    import pydot_ng as pydot\n    dot_data = StringIO()\n    tree.export_graphviz(c50_model, out_file=dot_data,\n                         feature_names=feature_names,\n                         class_names=class_name,\n                         filled=True, rounded=True,\n                         special_characters=True,\n                          node_ids=1,)\n    graph = pydot.graph_from_dot_data(dot_data.getvalue())\n    graph.write_pdf(\"%s.pdf\"%name)\n\noutput_pdf(c50_model, name='filename%s')\n\nprint(recall_score(Y_over_test,Y_Over_Pred_C50))\nprint(accuracy_score(Y_over_test,Y_Over_Pred_C50))\nconfusion_matrix(Y_over_test,Y_Over_Pred_C50)\n\nCM_Crosstab_1 = pd.crosstab(Y_over_test,Y_Over_Pred_C50)\nCM_Crosstab_1.columns=[\"No\",\"Yes\"]\nCM_Crosstab_1.index = [\"No\",\"Yes\"]\nCM_Crosstab_1\n\nFNR = (CM_Crosstab_1.iloc[1,0]/(CM_Crosstab_1.iloc[1,1]+CM_Crosstab_1.iloc[1,0]))*100\nFNR\n\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import RocCurveDisplay\nfrom sklearn import metrics\nfpr, tpr, thresholds = roc_curve(Y_over_test,Y_Over_Pred_C50)\nroc_auc = metrics.auc(fpr, tpr)\ndisplay = RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc, estimator_name='Decision Tree')\ndisplay.plot()  \nplt.show()\n\n# Random Forest\nfrom sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(n_estimators=500)\n\nrf_model = rf.fit(X_over_train,Y_over_train)\nY_Over_Pred_rf = rf.predict_proba(X_over_test) # predict_proba for predicting probalities\n\nY_Over_Pred_rf_1=pd.DataFrame(Y_Over_Pred_rf)\n\nY_Over_Pred_rf_1.head(5)\n\nY_over_test_1=pd.DataFrame(Y_over_test)\n\nY_over_test_1.head(5)\n\n#result = left.join(right)\nY_NEW = pd.concat([Y_over_test_1,Y_Over_Pred_rf_1],axis=1)\n\nY_NEW.head(5)\n\nrf_model = rf.fit(X_over_train,Y_over_train)\nY_Over_Pred_rff = rf.predict(X_over_test)\nprint(recall_score(Y_over_test,Y_Over_Pred_rff))\nprint(accuracy_score(Y_over_test,Y_Over_Pred_rff))\n\nCM_Crosstab_2 = pd.crosstab(Y_over_test,Y_Over_Pred_rff)\nCM_Crosstab_2.columns=[\"No\",\"Yes\"]\nCM_Crosstab_2.index = [\"No\",\"Yes\"]\nCM_Crosstab_2\n\nFNR = (CM_Crosstab_2.iloc[1,0]/(CM_Crosstab_2.iloc[1,1]+CM_Crosstab_2.iloc[1,0]))*100\nFNR\n\nfrom sklearn.metrics import roc_auc_score\nprint(roc_auc_score(Y_over_test,Y_Over_Pred))\nprint(roc_auc_score(Y_over_test,Y_Over_Pred_C50))\nprint(roc_auc_score(Y_over_test,Y_Over_Pred_rff))\n\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import RocCurveDisplay\nfrom sklearn import metrics\nfpr, tpr, thresholds = roc_curve(Y_over_test,Y_Over_Pred_rff)\nroc_auc = metrics.auc(fpr, tpr)\ndisplay = RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc, estimator_name='RandomForest')\ndisplay.plot()  \nplt.show()","metadata":{"_uuid":"f3d9bbf0-fab6-4d67-9802-8a2a2fb8b954","_cell_guid":"908b3ee0-42bb-4572-b151-d2c549dd8069","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}