{"cells":[{"metadata":{},"cell_type":"markdown","source":"**IMPORT PACKAGES**"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-output":false},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n# %reset -f\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nimport gc\nimport os\n\nimport matplotlib.pyplot as plt\nfrom numba import jit\n\nfrom sklearn import preprocessing\nimport pywt\n\nimport tsfresh\nfrom tsfresh import extract_features\nfrom tsfresh.utilities.dataframe_functions import impute\nfrom tsfresh.feature_extraction import ComprehensiveFCParameters, MinimalFCParameters, EfficientFCParameters\nimport pickle\nfrom pathlib import Path\n\nfrom scipy.stats import kurtosis\nfrom scipy.stats import skew\nfrom scipy import stats\nimport scipy.signal as sg\nimport multiprocessing as mp\nfrom scipy.signal import hann\nfrom scipy.signal import hilbert\nfrom scipy.signal import convolve\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import StandardScaler\n\nimport lightgbm as lgb\nfrom sklearn.model_selection import KFold\n\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom sklearn.metrics import roc_auc_score, roc_curve\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\n\n\nfrom tqdm import tqdm\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n\nprint(os.listdir(\"../input/\"))\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**USEFUL FUNCTIONS**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# REDUCE MEMORY USAGE - COMMON CODE SHARED ON KAGGLE KERNELS\n\n# FAST AUC CALCULATION\ndef fast_auc(y_true, y_prob):\n    \"\"\"\n    fast roc_auc computation: https://www.kaggle.com/c/microsoft-malware-prediction/discussion/76013\n    \"\"\"\n    y_true = np.asarray(y_true)\n    y_true = y_true[np.argsort(y_prob)]\n    nfalse = 0\n    auc = 0\n    n = len(y_true)\n    for i in range(n):\n        y_i = y_true[i]\n        nfalse += (1 - y_i)\n        auc += y_i * nfalse\n    auc /= (nfalse * (n - nfalse))\n    return auc\n\n\nclass CustomUnpickler(pickle.Unpickler):\n\n    def find_class(self, module, name):\n        if name == 'Manager':\n            from settings import Manager\n            return Manager\n        return super().find_class(module, name)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**PART 1 - READING ALL DATA PROVIDED**\n\nFor files in safety.zip:\n1. extract all the files in \"features\" folder & concatenate them to pandas format\n2. import data in \"labels\" folder"},{"metadata":{},"cell_type":"markdown","source":"*a. Import **train** dataset*"},{"metadata":{"trusted":true,"_kg_hide-output":false},"cell_type":"code","source":"inp = '../input/safeornot/safety/'\npathlist = Path(inp + 'features').glob('**/*.csv')\nchunks=[]\n\nfor path in pathlist:\n    path_in_str = str(path)\n    chunks.append(pd.read_csv(path_in_str))\n    print('Done: ', str(path))\n    \ndata1 = pd.concat(chunks, axis=0, ignore_index=True,sort=False)\nprint('Done: FEATURES')\n\ndel chunks\ngc.collect()\n\npathlist2 = Path(inp + 'labels').glob('**/*.csv')\nchunks=[]\nfor path in pathlist2:\n    path_in_str = str(path)\n    chunks.append(pd.read_csv(path_in_str))\n    print('Done: ', str(path))\nlabels = pd.concat(chunks, axis=0, ignore_index=True,sort=False)\nprint('Done: LABELS')\n\ndel chunks","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*b. Import **test** dataset*"},{"metadata":{"trusted":true},"cell_type":"code","source":"inp = '../input/safeornot/safety_test/'\npathlist = Path(inp + 'features').glob('**/*.csv')\nchunks=[]\n\nfor path in pathlist:\n    path_in_str = str(path)\n    chunks.append(pd.read_csv(path_in_str))\n    print('Done: ', str(path))\n    \ndata2 = pd.concat(chunks, axis=0, ignore_index=True,sort=False)\nprint('Done: FEATURES')\n\ndel chunks\ngc.collect()\n\npathlist2 = Path(inp + 'labels').glob('**/*.csv')\nchunks=[]\nfor path in pathlist2:\n    path_in_str = str(path)\n    chunks.append(pd.read_csv(path_in_str))\n    print('Done: ', str(path))\ntest_labels = pd.concat(chunks, axis=0, ignore_index=True,sort=False)\nprint('Done: LABELS')\n\ndel chunks","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**PART 2 - QUICK CHECK ON THE TRAINING DATA**\n\nObs: Realised that there are duplicates in label data - meaning they are bookingID's wth more than 1 type of label.\nSol: Remove these samples from training dataset\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# ensure no duplicates\nlabels = labels.drop_duplicates()\nprint('Features Data bookingID Row #     :' , len(data1))\nprint('Features Data Unique bookingID #  :' , len(data1['bookingID'].unique()))\n\nprint('Label Data bookingID     #        :' , len(labels))\nprint('Label Data bookingID Row #        :' , len(labels.drop_duplicates().bookingID.unique()))\n\nprint('Some duplicated labels')\nprint(labels[labels.duplicated(['bookingID'], keep=False)].sort_values(by=['bookingID']).head())\n\nto_rm = labels[labels.duplicated(['bookingID'], keep=False)].sort_values(by=['bookingID']).drop('label',1)\n# to_rm.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**PART 3 - DATA PREPROCESSING**\n\nFor simplicity, we concat the train and hold-out data here for preprocessing purpose.\n"},{"metadata":{"trusted":true,"_kg_hide-input":false,"_kg_hide-output":true},"cell_type":"code","source":"# data \n\n# sort the data by bookingID and second\ndata =  pd.concat([data1, data2], axis=0)\n\ndel data1\ndel data2\ngc.collect()\n\ndata = data.sort_values(by=['bookingID','second']).reset_index(drop=True)\n\n# create diff to match - to flag not continuous part\ndata = data.assign(diff =  data['second'] - data['second'].shift(1) ).fillna(0)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Accuracy']=data.Accuracy.astype('float')\ndata['Bearing']=data.Bearing.astype('float')\ndata['acceleration_x']=data.acceleration_x.astype('float')\ndata['acceleration_y']=data.acceleration_y.astype('float')\ndata['acceleration_z']=data.acceleration_z.astype('float')\ndata['gyro_x']=data.gyro_x.astype('float')\ndata['gyro_y']=data.gyro_y.astype('float')\ndata['gyro_z']=data.gyro_z.astype('float')\ndata['Speed']=data.Speed.astype('float')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Remove some inconsistent data: \n1. Duplicated labels\n2. Extreme difference from previous,i.e.: bookingID ran through more than 1 day"},{"metadata":{"trusted":true},"cell_type":"code","source":"# REMOVE BOOKINGID WITH DUPLICATED LABELS\nprint(\"ORI ROW #                            : \", len(data))\ndata = data[~data.bookingID.isin(to_rm.bookingID)]\n\nprint(\"AFTER REMOVING DUPLICATED LABEL ROW #: \", len(data))\n\ndata = data.loc[data['diff'] <= 86_400]  #don't make sense to have same bookingID more than 1 day diff from previous rows\n\nprint(\"AFTER EXTREME DIFF REMOVAL      ROW #: \", len(data))\n\nprint(\"UNIQUE COUNT BOOKING ID:\", len(data.bookingID.unique()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['acc'] = np.sqrt(np.power(data['acceleration_x'], 2) + np.power(data['acceleration_y'], 2) + np.power(data['acceleration_z'], 2))\ndata['gyr'] = np.sqrt(np.power(data['gyro_x'], 2) + np.power(data['gyro_y'], 2) + np.power(data['gyro_z'], 2))\n\n# recreate data-set with average for every 2 secs or try to make it continuous \n# a portion of data is using \ndata = data.assign(skip = data['second']//2)\n# data = reduce_mem_usage(data)\n\ndata_gp = data.groupby([\"bookingID\",\"skip\"])\n\ndata_max_sec = data_gp['second'].max().reset_index()\n# data_max_sec = reduce_mem_usage(data_max_sec)\nprint(data_max_sec.head())\n\ndata_mean = data_gp['acc','gyr','Speed','Accuracy','Bearing'].mean().sort_values(by=['bookingID','skip']).reset_index()\n# data_mean = reduce_mem_usage(data_mean)\nprint(data_mean.head())\n\nlabel_y = labels[labels.bookingID.isin(data_mean.bookingID)].sort_values(by=['bookingID']).reset_index()\nprint(label_y.head())\n\ngc.collect()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Feature selection**\n\n1. Features from **TSFresh** - have been pre-selected using RFE (RandomForest & Lightgbm) and Anova Test to drop unnecessary features; \n2. Afterwards run a 10-folds Lightgbm in order to obtain the top important features,i.e.: features more important than the variables generated with random numbers."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"p_bearing = {'absolute_sum_of_changes': None,\n'fft_coefficient': [{'coeff': 38, 'attr': 'abs'},\n{'coeff': 70, 'attr': 'abs'}]}\n        \np_speed = {'count_below_mean': None,\n'fft_aggregated': [{'aggtype': 'centroid'}],\n'fft_coefficient': [{'coeff': 40, 'attr': 'abs'},\n{'coeff': 73, 'attr': 'abs'}],\n'longest_strike_below_mean': None,\n'number_crossing_m': [{'m': 1}],\n'range_count': [{'max': 1, 'min': -1}]}\n \np_acc = {'agg_linear_trend': [{'f_agg': 'mean',\n'chunk_len': 5,\n'attr': 'stderr'},\n{'f_agg': 'min', 'chunk_len': 50, 'attr': 'rvalue'}],\n'change_quantiles': [{'f_agg': 'mean', 'isabs': True, 'qh': 1.0, 'ql': 0.6}],\n'count_below_mean': None,\n'energy_ratio_by_chunks': [{'num_segments': 10, 'segment_focus': 1}],\n'fft_coefficient': [{'coeff': 16, 'attr': 'abs'},\n{'coeff': 49, 'attr': 'real'}]}\n \np_accuracy = {'count_below_mean': None}\n \np_gyr =  {'fft_aggregated': [{'aggtype': 'variance'}, {'aggtype': 'centroid'}],\n  'number_peaks': [{'n': 1}],\n  'length': None,\n  'count_above_mean': None,\n  'count_below_mean': None,\n  'range_count': [{'max': 1000000000000.0, 'min': 0}]}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Helper Functions\ndef mean_change_of_abs_change(x):\n    return np.mean(np.diff(np.abs(np.diff(x))))\n    \ndef _kurtosis(x):\n    return kurtosis(x)\n\ndef CPT5(x):\n    den = len(x)*np.exp(np.std(x))\n    return sum(np.exp(x))/den\n\ndef skewness(x):\n    return skew(x)\n\ndef SSC(x):\n    x = np.array(x)\n    x = np.append(x[-1], x)\n    x = np.append(x,x[1])\n    xn = x[1:len(x)-1]\n    xn_i2 = x[2:len(x)]    # xn+1 \n    xn_i1 = x[0:len(x)-2]  # xn-1\n    ans = np.heaviside((xn-xn_i1)*(xn-xn_i2),0)\n    return sum(ans[1:]) \n\ndef wave_length(x):\n    x = np.array(x)\n    x = np.append(x[-1], x)\n    x = np.append(x,x[1])\n    xn = x[1:len(x)-1]\n    xn_i2 = x[2:len(x)]    # xn+1 \n    return sum(abs(xn_i2-xn))\n    \ndef norm_entropy(x):\n    tresh = 3\n    return sum(np.power(abs(x),tresh))\n\ndef SRAV(x):    \n    SRA = sum(np.sqrt(abs(x)))\n    return np.power(SRA/len(x),2)\n\ndef mean_abs(x):\n    return sum(abs(x))/len(x)\n\ndef zero_crossing(x):\n    x = np.array(x)\n    x = np.append(x[-1], x)\n    x = np.append(x,x[1])\n    xn = x[1:len(x)-1]\n    xn_i2 = x[2:len(x)]    # xn+1\n    return sum(np.heaviside(-xn*xn_i2,0))\n\n\ndef add_trend_feature(arr, abs_values=False):\n    idx = np.array(range(len(arr)))\n    if abs_values:\n        arr = np.abs(arr)\n    lr = LinearRegression()\n    lr.fit(idx.reshape(-1, 1), arr)\n    return lr.coef_[0]\n\ndef classic_sta_lta(x, length_sta, length_lta):\n    sta = np.cumsum(x ** 2)\n    # Convert to float\n    sta = np.require(sta, dtype=np.float)\n    # Copy for LTA\n    lta = sta.copy()\n    # Compute the STA and the LTA\n    sta[length_sta:] = sta[length_sta:] - sta[:-length_sta]\n    sta /= length_sta\n    lta[length_lta:] = lta[length_lta:] - lta[:-length_lta]\n    lta /= length_lta\n    # Pad zeros\n    sta[:length_lta - 1] = 0\n    # Avoid division by zero by setting zero values to tiny float\n    dtiny = np.finfo(0.0).tiny\n    idx = lta < dtiny\n    lta[idx] = dtiny\n    return sta / lta\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Extra features\n\ndef feat_eng_acc(data):\n    \n    df = pd.DataFrame()\n\n    \n    for col in data.columns:\n        if col in ['bookingID','skip']:\n            continue\n            \n        df[col + '_abs_min'] = data.groupby(['bookingID'])[col].apply(lambda x: np.min(np.abs(x)))\n        df[col + '_q95'] = data.groupby(['bookingID'])[col].quantile(0.95)\n\n    return df\n\ndef feat_eng_accuracy(data):\n    \n    df = pd.DataFrame()\n\n    \n    for col in data.columns:\n        if col in ['bookingID','skip']:\n            continue\n            \n        df[col + '_mad'] = data.groupby(['bookingID'])[col].mad()\n        df[col + '_median'] = data.groupby(['bookingID'])[col].median()\n        df[col + '_q75'] = data.groupby(['bookingID'])[col].quantile(0.75)\n        df[col + '_q95'] = data.groupby(['bookingID'])[col].quantile(0.95)\n        df[col + '_range'] = data.groupby(['bookingID'])[col].max() - data.groupby(['bookingID'])[col].min()\n        df[col + '_SSC'] = data.groupby(['bookingID'])[col].apply(SSC) \n\n    return df\n\ndef feat_eng_bearing(data):\n    \n    df = pd.DataFrame()\n\n    \n    for col in data.columns:\n        if col in ['bookingID','skip']:\n            continue\n            \n        df[col + '_iqr'] = data.groupby(['bookingID'])[col].quantile(0.75) - data.groupby(['bookingID'])[col].quantile(0.25)\n        df[col + '_mad'] = data.groupby(['bookingID'])[col].mad()\n        df[col + '_q95'] = data.groupby(['bookingID'])[col].quantile(0.95)\n        df[col + '_std'] = data.groupby(['bookingID'])[col].std()\n\n    return df\n\ndef feat_eng_gyr(data):\n    \n    df = pd.DataFrame()\n\n    \n    for col in data.columns:\n        if col in ['bookingID','skip']:\n            continue\n            \n        df[col + '_max'] = data.groupby(['bookingID'])[col].max()\n        df[col + '_min'] = data.groupby(['bookingID'])[col].min()\n        df[col + '_q75'] = data.groupby(['bookingID'])[col].quantile(0.75)\n        df[col + '_q95'] = data.groupby(['bookingID'])[col].quantile(0.95)\n        df[col + '_wave_length'] = data.groupby(['bookingID'])[col].apply(wave_length)\n\n    return df\n\ndef feat_eng_speed(data):\n    \n    df = pd.DataFrame()\n\n    \n    for col in data.columns:\n        if col in ['bookingID','skip']:\n            continue\n            \n        df[col + '_max'] = data.groupby(['bookingID'])[col].max()\n        df[col + '_mean'] = data.groupby(['bookingID'])[col].mean()\n        df[col + '_mean_abs_chg'] = data.groupby(['bookingID'])[col].apply(lambda x: np.mean(np.abs(np.diff(x))))\n        df[col + '_mean_change_of_abs_change'] = data.groupby('bookingID')[col].apply(mean_change_of_abs_change)\n        df[col + '_median'] = data.groupby(['bookingID'])[col].median()\n        df[col + '_norm_entropy'] = data.groupby(['bookingID'])[col].apply(norm_entropy)\n        df[col + '_q25'] = data.groupby(['bookingID'])[col].quantile(0.25)\n        df[col + '_q75'] = data.groupby(['bookingID'])[col].quantile(0.75)\n        df[col + '_q95'] = data.groupby(['bookingID'])[col].quantile(0.95)\n        df[col + '_range'] = data.groupby(['bookingID'])[col].max() - data.groupby(['bookingID'])[col].min()\n        df[col + '_skew'] = data.groupby(['bookingID'])[col].skew()\n        df[col + '_sla_tla_10_40_mean'] = data.groupby(['bookingID'])[col].apply(lambda x: classic_sta_lta(x, 10, 40).mean()) \n        df[col + '_sla_tla_10_50_mean'] = data.groupby(['bookingID'])[col].apply(lambda x: classic_sta_lta(x, 10, 50).mean()) \n        df[col + '_trend'] = data.groupby(['bookingID'])[col].apply(lambda x: add_trend_feature(x)) \n        df[col + '_wave_length'] = data.groupby(['bookingID'])[col].apply(wave_length)\n\n    return df\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"extract_from = data_mean[['bookingID','skip','Accuracy']]\ngp_accuracy  = feat_eng_accuracy(extract_from)\ngp_accuracy['bookingID'] = gp_accuracy.index\n\n\next_features = tsfresh.extract_features(extract_from,\n                                          column_id='bookingID',\n                                          column_sort='skip',\n                                          n_jobs = 5, impute_function= impute,\n                                          default_fc_parameters = p_accuracy)\next_features['bookingID'] = ext_features.index\n\naccuracy_var = pd.merge(ext_features, gp_accuracy, on = 'bookingID', how = 'left')\n\ndel extract_from\ndel ext_features\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"extract_from = data_mean[['bookingID','skip','acc']]\ngp_acc  = feat_eng_acc(extract_from)\ngp_acc['bookingID'] = gp_acc.index\n\next_features_2 = tsfresh.extract_features(extract_from,\n                                          column_id='bookingID',\n                                          column_sort='skip',\n                                          n_jobs = 5, impute_function= impute,\n                                          default_fc_parameters = p_acc)\next_features_2['bookingID'] = ext_features_2.index\nacc_var = pd.merge(ext_features_2, gp_acc, on = 'bookingID', how = 'left')\n\ndel extract_from\ndel ext_features_2\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"extract_from = data_mean[['bookingID','skip','Bearing']]\ngp_bearing  = feat_eng_bearing(extract_from)\ngp_bearing['bookingID'] = gp_bearing.index\n\next_features_3 = tsfresh.extract_features(extract_from,\n                                          column_id='bookingID',\n                                          column_sort='skip',\n                                          n_jobs = 5, impute_function= impute,\n                                          default_fc_parameters = p_bearing)\next_features_3['bookingID'] = ext_features_3.index\nbearing_var = pd.merge(ext_features_3, gp_bearing, on = 'bookingID', how = 'left')\n\ndel extract_from\ndel ext_features_3\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"extract_from = data_mean[['bookingID','skip','Speed']]\ngp_speed  = feat_eng_speed(extract_from)\ngp_speed['bookingID'] = gp_speed.index\n\next_features_4 = tsfresh.extract_features(extract_from,\n                                          column_id='bookingID',\n                                          column_sort='skip',\n                                          n_jobs = 5, impute_function= impute,\n                                          default_fc_parameters = p_speed)\next_features_4['bookingID'] = ext_features_4.index\nspeed_var = pd.merge(ext_features_4, gp_speed, on = 'bookingID', how = 'left')\n\ndel extract_from\ndel ext_features_4\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"extract_from = data_mean[['bookingID','skip','gyr']]\ngp_gyr  = feat_eng_gyr(extract_from)\ngp_gyr['bookingID'] = gp_gyr.index\n\next_features_5 = tsfresh.extract_features(extract_from,\n                                          column_id='bookingID',\n                                          column_sort='skip',\n                                          n_jobs = 5, impute_function= impute,\n                                          default_fc_parameters = p_gyr)\next_features_5['bookingID'] = ext_features_5.index\ngyr_var = pd.merge(ext_features_5, gp_gyr, on = 'bookingID', how = 'left')\n\ndel extract_from\ndel ext_features_5\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_all = pd.merge(accuracy_var, acc_var, on = 'bookingID', how = 'left')\ndata_all = pd.merge(data_all, bearing_var, on = 'bookingID', how = 'left')\ndata_all = pd.merge(data_all, speed_var, on = 'bookingID', how = 'left')\ndata_all = pd.merge(data_all, gyr_var, on = 'bookingID', how = 'left')\ndata_all = pd.merge(data_all, labels, on = 'bookingID', how = 'left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(data_all.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_all.to_pickle('data_all.pkl')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_all.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = data_all[data_all.label.notnull()]\ntest  = data_all[data_all.label.isnull()]\n\ntest = test.drop(['label'],axis = 1)\ntest = pd.merge(test, test_labels, on = 'bookingID', how = 'left')\n\nfeatures = train.drop(['label','bookingID'],axis = 1).columns\n\n# train\nX = train[features]\ny = train.label\n\n# test\nX2 = test[features]\ny2 = test.label\nX2_id = pd.DataFrame(test['bookingID'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**MODEL EXECUTEION - LIGHTGBM WITH 10 FOLDS CROSS VALIDATION**\n\nTo reduce the sampling bias, 10 lgb models have been produced on different splits of data. Parameters have been chosen beforehand via Bayesian Optimisation."},{"metadata":{"trusted":true},"cell_type":"code","source":"num_round = 10000\nkfold = 10\nfolds = StratifiedKFold(n_splits=kfold, shuffle=False, random_state=42)\noof = np.zeros(len(X))\npredictions = np.zeros(len(X2))\nimportanceDF = pd.DataFrame()\n\n# Parameter chosen from Bayesian Optimisation \nparams =  {\n    'learning_rate': 0.035, \n    'boosting': 'gbdt', \n    'objective': 'binary', \n    'metric': 'auc',\n    'is_training_metric': True, \n    'seed': 999,   \n    'bagging_fraction': 0.6473097703475733,\n    'feature_fraction': 0.8559684085310095,\n    'lambda_l1': 0.716766437045232,\n    'lambda_l2': 2.8340067511487517,\n    'max_depth': 5,\n    'min_child_weight': 21.903773119545132,\n    'min_split_gain': 0.027191005598358072,\n    'num_leaves': 35}\n\n\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(X.values,y.values)):\n    print(\"Fold {}\".format(fold_))\n    trn_data = lgb.Dataset(X.iloc[trn_idx], label=y.iloc[trn_idx])\n    val_data = lgb.Dataset(X.iloc[val_idx], label=y.iloc[val_idx])\n    clf = lgb.train(params, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=500, early_stopping_rounds = 50)\n    \n    feature_imp = pd.DataFrame(sorted(zip(clf.feature_importance(),X.columns)), columns=['Value','Feature'])\n    k = sum(feature_imp.Value)\n    feature_imp['Value'] = feature_imp['Value']*100/k\n    feature_imp.sort_values(by=\"Value\", ascending=False)\n    \n    importanceDF = pd.concat([importanceDF, feature_imp], axis=0)\n    \n    oof[val_idx] = clf.predict(X.iloc[val_idx], num_iteration=clf.best_iteration)\n    predictions += clf.predict(X2, num_iteration=clf.best_iteration) / folds.n_splits","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Feature Importance**"},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_imp = pd.DataFrame(importanceDF.groupby(['Feature'])['Value'].apply(lambda x: x.mean()) )\nfeature_imp  = feature_imp.sort_values(by=[\"Value\"], ascending = False)\nfeature_imp =  feature_imp.reset_index(level=['Feature'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Plot top 15 important features*\n\nMost of the are coming from speed-related variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nsns.barplot(y=\"Feature\", x=\"Value\",data = feature_imp.head(15))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Combine the test data set with the predictions**"},{"metadata":{"trusted":true},"cell_type":"code","source":"type(predictions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_prediction = X2_id.copy()\ntest_prediction['prediction'] = predictions\ntest_prediction = pd.merge(test_prediction, test_labels, on = 'bookingID', how = 'left')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Calculate the AUC of the test dataset**"},{"metadata":{"trusted":true},"cell_type":"code","source":"fast_auc(test_prediction.label, test_prediction.prediction)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}