{"cells":[{"metadata":{},"cell_type":"markdown","source":"<p style=\"text-align: justify; line-height: 2.0; font-size: 14px; padding-right: 100px;\">\n    <b>Author:</b> Yap Jheng Khin\n</p>\n<p style=\"text-align: justify; line-height: 2.0; font-size: 14px; padding-right: 100px;\">\n    Note that this is the continuation from Part 1, which was done in <a href=\"https://github.com/polarBearYap/speeddating_AI\">here</a>.\n    I have also discover many mistakes from part I, and part II will serve as an <b>improvement</b> or postmortem.\n</p>\n<p style=\"text-align: justify; line-height: 2.0; font-size: 14px; padding-right: 100px;\">\n    List of mistakes that I have made in part I are:\n</p>\n<ol>\n    <li style=\"line-height: 2.0; font-size: 14px;\">Preprocess on whole dataset, which cause train-test contamination.</li>\n    <li style=\"line-height: 2.0; font-size: 14px;\">Perform cross validation instead of nested cross validation.</li>\n</ol>\n<p style=\"text-align: justify; line-height: 2.0; font-size: 14px; padding-right: 100px;\">\n    My learning expection in Part II are:\n</p>\n<ol>\n    <li style=\"line-height: 2.0; font-size: 14px;\">Discover various ways to detect correlated features.</li>\n    <li style=\"line-height: 2.0; font-size: 14px;\">Perform feature selection to reduce model complexity.</li>\n    <li style=\"line-height: 2.0; font-size: 14px;\">Apply nested cross validation on areas like hyperarameter tuning.</li>\n    <li style=\"line-height: 2.0; font-size: 14px;\">Discover XAI techniques that can be used in explaining black box models.</li>\n</ol>"},{"metadata":{},"cell_type":"markdown","source":"<p style=\"text-align: justify; line-height: 2.0; font-size: 14px; padding-right: 100px;\">\n    List of dependencies used:\n</p>\n<ol>\n    <li style=\"line-height: 2.0; font-size: 14px;\">tqdm</li>\n    <li style=\"line-height: 2.0; font-size: 14px;\">catboost</li>\n    <li style=\"line-height: 2.0; font-size: 14px;\">xgboost</li>\n    <li style=\"line-height: 2.0; font-size: 14px;\">seaborn 0.11.0</li>\n    <li style=\"line-height: 2.0; font-size: 14px;\">alibi</li>\n</ol>"},{"metadata":{"trusted":false},"cell_type":"code","source":"!pip install tqdm\n!pip install 'seaborn == 0.11.0'\n!pip install xgboost\n!pip install catboost\n!pip install alibi","execution_count":null,"outputs":[]},{"metadata":{"executionInfo":{"elapsed":5607,"status":"ok","timestamp":1599298622547,"user":{"displayName":"PolarBear Yap","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh6xY6wDRASo9ifgNeDcwMeBd3TCQFVGw14HLGtCg=s64","userId":"07391035732959922581"},"user_tz":-480},"id":"xjdzFReXcIO-","lines_to_next_cell":2,"outputId":"f0834459-1dd6-45a4-94f9-216bd3ac6b33","trusted":true},"cell_type":"code","source":"import time\nfrom itertools import product\nfrom math import ceil\n\nimport ast\nimport numpy as np\nimport pandas as pd\nimport pickle\nimport re\nimport seaborn as sns\nimport warnings\nfrom catboost import CatBoostClassifier\nfrom matplotlib import pyplot as plt\nfrom scipy.cluster import hierarchy\nfrom scipy.stats import spearmanr\nfrom sklearn.base import BaseEstimator, TransformerMixin, clone\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.ensemble import (\n    AdaBoostClassifier,\n    BaggingClassifier,\n    ExtraTreesClassifier,\n    GradientBoostingClassifier,\n    RandomForestClassifier,\n    StackingClassifier,\n    VotingClassifier,\n)\nfrom sklearn.exceptions import ConvergenceWarning\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.inspection import permutation_importance\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import (\n    RocCurveDisplay,\n    auc,\n    average_precision_score,\n    precision_recall_curve,\n    roc_auc_score,\n    roc_curve,\n)\nfrom sklearn.model_selection import (\n    RandomizedSearchCV,\n    StratifiedKFold,\n    StratifiedShuffleSplit,\n    cross_val_predict,\n    cross_validate,\n)\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.svm import LinearSVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom tqdm import tqdm\nfrom xgboost import XGBClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"RANDOM_SEED = 42\n\n# Set the default font size of all matplotlib plots\nplt.rcParams.update({'font.size': 12})\n\n# Set the display option of pandas objects\npd.set_option('display.max_rows', 150)\npd.set_option('display.max_columns', 150)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The functions below are needed to pickle objects later on to cut down total execution time."},{"metadata":{"trusted":true},"cell_type":"code","source":"def dump_objects(file_name, *objects):\n    with open(f'{file_name}.sav', 'wb') as file:\n        for obj in objects:\n            pickle.dump(obj, file)\n\n\ndef load_objects(file_name, num_objects=1):\n    objects = []\n    with open(f'../input/pickles/speeddating_pickles/{file_name}.sav', 'rb') as file:\n        while num_objects > 0:\n            objects.append(pickle.load(file))\n            num_objects -= 1\n    return objects","execution_count":null,"outputs":[]},{"metadata":{"id":"V8yVBoUxNq1H"},"cell_type":"markdown","source":"# Get Data"},{"metadata":{},"cell_type":"markdown","source":"<p style=\"text-align: justify; line-height: 2.0; text-indent: 5%; font-size: 14px; padding-right: 100px;\">\nThe dataset we have chosen is SpeedDating from <a href=https://www.openml.org/d/40536>openml</a> which focused on experimental speed dating information, which included the answers of 8,378 participants between 2002 and 2004. Each participant had a 4-minute \"first date\" with the opposite sex. Once they completed the short-term date, the participants were asked to rate their likelihood of seeing their partner again (between 0 and 10). Everyone is also asked to rate their partners on 6 subjective attributes. According to our preliminary analysis, these subjective attributes and ratings of self-perception, actual age and whether they match other people will form the basis for my inquiry. The attributes are Attractiveness, Sincerity, Intelligence, Fun, Ambition, and Shared Interests.\n</p>"},{"metadata":{"id":"hAax9YHibqLa","trusted":false},"cell_type":"code","source":"BANK_DATA_URL = 'https://raw.githubusercontent.com/polarBearYap/speeddating_AI/main/datasets/speed_dating.csv'\nFILE_PATH = '../input/speed-dating/speeddating.csv'\n\n\ndef fetch_data_from_website(path):\n    return pd.read_csv(path, low_memory=False)","execution_count":null,"outputs":[]},{"metadata":{"id":"R_kFejV3b9h8","trusted":false},"cell_type":"code","source":"dating = fetch_data_from_website(FILE_PATH)\ndating.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"064tAdLdZspr"},"cell_type":"markdown","source":"# Data Exploration and Problem Understanding"},{"metadata":{},"cell_type":"markdown","source":"<p style=\"text-align: justify; line-height: 2.0; text-indent: 5%; font-size: 14px; padding-right: 100px;\">\nWe have analyzed a dataset which consists of 8378 samples gathered from participants in experimental speed dating events from 2002 until 2004. The dataset output is to determine whether the given partners would match with the users. There are 122 inputs and 1 output which in summation 123 attributes in the dataset. The dataset consists of 7 attributes which datatype are in integer type and the remaining 116 attributes are all in object type.\n</p>"},{"metadata":{"executionInfo":{"elapsed":8846,"status":"ok","timestamp":1599298625877,"user":{"displayName":"PolarBear Yap","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh6xY6wDRASo9ifgNeDcwMeBd3TCQFVGw14HLGtCg=s64","userId":"07391035732959922581"},"user_tz":-480},"id":"Fp4m9zkVZsK3","outputId":"29344e1f-75c3-40c1-a4af-11f132a54b8e","trusted":false},"cell_type":"code","source":"# List of all attributes\nlist(dating.columns.values)","execution_count":null,"outputs":[]},{"metadata":{"executionInfo":{"elapsed":8619,"status":"ok","timestamp":1599298625877,"user":{"displayName":"PolarBear Yap","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh6xY6wDRASo9ifgNeDcwMeBd3TCQFVGw14HLGtCg=s64","userId":"07391035732959922581"},"user_tz":-480},"id":"Jg20dAwVZ04_","outputId":"eb92900a-9c42-4248-bc95-786288b6781d","trusted":false},"cell_type":"code","source":"dating.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p style=\"text-align: justify; line-height: 2.0; text-indent: 5%; font-size: 14px; padding-right: 100px;\">\nThere are totally 56 preprocessed features which have undergone the data preprocessing in the dataset such as 'd_importance_same_race' which represents the various types of age difference for the users and given partners. Therefore, all the features with heading 'd_' will show the particular attributes in discrete type which should be filtered out from the raw dataset while performing data preprocessing. However, we do find want to keep one of the features which is 'd_age', since difference in age is quite important in dating in our opinion.\n</p>"},{"metadata":{"executionInfo":{"elapsed":8504,"status":"ok","timestamp":1599298625880,"user":{"displayName":"PolarBear Yap","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh6xY6wDRASo9ifgNeDcwMeBd3TCQFVGw14HLGtCg=s64","userId":"07391035732959922581"},"user_tz":-480},"id":"BVZ-kIo3aFwt","outputId":"9a5b374f-9b1c-41c6-9a61-320f438b8359","trusted":false},"cell_type":"code","source":"preprocessed_features = [feature for feature in dating.columns if feature.lower()[\n    :2] == 'd_']\npreprocessed_features.remove('d_age')\nprint(\n    f'Amount of remaining preproccessed features: {len(preprocessed_features)}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p style=\"text-align: justify; line-height: 2.0; text-indent: 5%; font-size: 14px; padding-right: 100px;\">\nThe dataset also exists several irrelevant features such as 'has null' which represents whether the particular sample consisting null values. Several features with 'expected_' means the expectations of the users towards partners and also all the users' self interest features such as sports, movies and others could be considered as subjective features and should also be dropped from the datasets. The fields of study of the users would also not be considered in the dataset. Moreover, the attributes which consist of '_o' represent the opinions of the given partners which are also irrelated in the dataset. For instance, 'pref_o_attractive' means the importance rated by the partners towards the attractiveness of the participants.\tHence, features selection should be performed during the data preprocessing in order to filter out all irrelevant attributes.\n</p>"},{"metadata":{"id":"B4PGJD-jj9cp","trusted":false},"cell_type":"code","source":"irrelevant_features = ['has_null',\n                       'wave',\n                       'expected_happy_with_sd_people',\n                       'expected_num_interested_in_me',\n                       'expected_num_matches',\n                       'field',\n                       'decision']\n\nself_interest_feature = ['sports',\n                         'tvsports',\n                         'exercise',\n                         'dining',\n                         'museums',\n                         'art',\n                         'hiking',\n                         'gaming',\n                         'clubbing',\n                         'reading',\n                         'tv',\n                         'theater',\n                         'movies',\n                         'concerts',\n                         'music',\n                         'shopping',\n                         'yoga']\n\npartner_features = ['age_o',\n                    'race_o',\n                    'pref_o_attractive',\n                    'pref_o_sincere',\n                    'pref_o_intelligence',\n                    'pref_o_funny',\n                    'pref_o_ambitious',\n                    'pref_o_shared_interests',\n                    'attractive_o',\n                    'sinsere_o',\n                    'intelligence_o',\n                    'funny_o',\n                    'ambitous_o',\n                    'shared_interests_o']","execution_count":null,"outputs":[]},{"metadata":{"id":"jOsg2u0FaxdI"},"cell_type":"markdown","source":"# Data Cleaning & Preprocessing"},{"metadata":{},"cell_type":"markdown","source":"<p style=\"text-align: justify; line-height: 2.0; font-size: 14px; padding-right: 100px;\">\n    First, we are going to drop <b>decision</b> and <b>decision_o</b>, since these two features will cause\n    <a href=\"https://www.kaggle.com/alexisbcook/data-leakage\">data leakage</a> for our model.\n</p>"},{"metadata":{"trusted":false},"cell_type":"code","source":"dating = dating.drop(columns=['decision', 'decision_o'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p style=\"text-align: justify; line-height: 2.0; font-size: 14px; padding-right: 100px;\">\n    Next, we are going to remove preprocessed features since it has high correlation with raw features,\n    which complicates the final model.\n</p>"},{"metadata":{"trusted":false},"cell_type":"code","source":"dating = dating.drop(columns=preprocessed_features, axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p style=\"text-align: justify; line-height: 2.0; font-size: 14px; padding-right: 100px;\">\n    Thirdly, we are going to drop has_null feature since it is irrelevant in the prediction.\n</p>"},{"metadata":{"trusted":false},"cell_type":"code","source":"dating = dating.drop(columns='has_null', axis=1)","execution_count":null,"outputs":[]},{"metadata":{"id":"FIFU-cknNwui"},"cell_type":"markdown","source":"## Data cleaning pipeline"},{"metadata":{"id":"FZieDiQQQqjS","trusted":false},"cell_type":"code","source":"# Use a custom transformer for data preprocessing\nclass DataCleaner(BaseEstimator, TransformerMixin):\n\n    def __init__(self, y_feature):\n        self.y_feature = y_feature\n        self.features_with_wrong_data_type = []\n        self.numerical_features = []\n        self.categorical_features = []\n        self.features_with_invalid_value = []\n        self.one_hot_features = []\n        self.invalid_values = set()\n\n    # Getter for numerical features\n    def getNumericalFeatures(self):\n        return self.numerical_features\n\n    # Getter for categorical features\n    def getCategoricalFeatures(self):\n        return self.categorical_features\n\n    # Getter for collected invalid values\n    def getInvalidValues(self):\n        return self.invalid_values\n\n    # Detect integer value in data using regex/regular expression\n    def detect_int_value(self, data):\n        return np.any(data.astype(str).str.contains('^\\d+$', regex=True))\n\n    # Detect integer value in data using regex/regular expression\n    def detect_float_value(self, data):\n        return np.any(data.astype(str).str.contains('^-?\\d+\\.\\d+$|^\\d+$', regex=True))\n\n    # Detect invalid integer value in data using regex/regular expression\n    def get_invalid_int_value(self, data):\n        return ', '.join(data[~data.astype(str).str.contains('^\\d+$', regex=True)]\n                         .value_counts().index.to_list())\n\n    # Detect invalid float value in data using regex/regular expression\n    def get_invalid_float_value(self, data):\n        return ', '.join(data[~data.astype(str).str.contains('^-?\\d+\\.\\d+$|^\\d+$', regex=True)]\n                         .value_counts().index.to_list())\n\n    def drop_rows_with_unknow_values(self, data, feature):\n        return data[~data[feature].isna()]\n\n    def find_invalid_values(self, data):\n        # Iterates all columns in the dating dataset and detect data types automatically\n        for feature in data.columns.values:\n\n            # Check if the features casted as object should be casted with float\n            if data[feature].dtype == 'object':\n                # If the features should be casted with float, flag the feature as 'features_with_wrong_data_type'\n                if self.detect_float_value(data[feature]):\n                    data[feature] = data[feature].astype(\n                        'float64', errors='ignore')\n                    invalid_value = self.get_invalid_float_value(data[feature])\n                    # If invalid values are found, flag the feature as 'features_with_invalid_value'\n                    if invalid_value != '':\n                        self.invalid_values.add(invalid_value)\n                        self.features_with_invalid_value.append(feature)\n                    self.features_with_wrong_data_type.append(feature)\n                # If the feature is actually categorical, flag the feature as 'categorical_features'\n                else:\n                    self.categorical_features.append(feature)\n\n            # Check for invalid integer value in numerical columns with 'int64' datatype\n            if data[feature].dtype == 'int64':\n                invalid_value = self.get_invalid_int_value(data[feature])\n                if invalid_value != '':\n                    self.invalid_values.add(invalid_value)\n                    self.features_with_invalid_value.append(feature)\n                data[feature] = data[feature].astype('float64', errors='raise')\n                self.numerical_features.append(feature)\n\n            # Check for invalid integer value in numerical columns with 'float64' datatype\n            elif data[feature].dtype == 'float64':\n                invalid_value = self.get_invalid_float_value(data[feature])\n                if invalid_value != '':\n                    self.invalid_values.add(invalid_value)\n                    self.features_with_invalid_value.append(feature)\n                self.numerical_features.append(feature)\n\n    def fit(self, data, y=None):\n\n        # Detect any numerical features casted with 'object' data type and with invalid values\n        self.find_invalid_values(data)\n\n        return self\n\n    def transform(self, data, y=None):\n\n        # Replace '?' value with NaN\n        data = data.replace('^\\?$', np.NaN, regex=True)\n\n        # Change numerical features with 'object' data type and change to 'float64'\n        for feature in self.features_with_invalid_value:\n            data[feature] = data[feature].astype('float64', errors='raise')\n\n        # Add the fixed features back to numerical features\n        self.numerical_features += self.features_with_invalid_value\n\n        # Remove unwanted quotes: change values like ''Example'' to 'Example'\n        for feature in self.categorical_features:\n            for value in data[feature].value_counts().index:\n                if re.search('^\\'.+\\'$', value.replace(' ', '')):\n                    index = data[data[feature] == value].index\n                    data.loc[index, feature] = value[1:-1]\n\n        return data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p style=\"text-align: justify; line-height: 2.0; font-size: 14px; padding-right: 100px;\">\n    We are going to clean our data. The data cleaning pipeline automatically converts the\n    columns into suitable data type depending on the majority of the values, respectively.\n    Note that the dataset contains unknown values labelled by\n    the value of '?' and is replaced with <i>np.NaN</i>.\n</p>"},{"metadata":{"trusted":false},"cell_type":"code","source":"cleaner = DataCleaner('match')\ndating1 = cleaner.fit_transform(dating.copy())","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(f'Invalid values found: {cleaner.getInvalidValues()}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p style=\"text-align: justify; line-height: 2.0; font-size: 14px; padding-right: 100px;\">\n    Below are the lists of numerical and categorical features contained in the dataset.\n    The <i>match</i> feature is omitted since it is the outcome we want to predict.\n</p>"},{"metadata":{"trusted":false},"cell_type":"code","source":"print('List of numerical features:')\nnum_attr = cleaner.getNumericalFeatures()\nnum_attr.remove('match')\nnum_attr","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print('List of categorical features:')\ncat_attr = cleaner.getCategoricalFeatures()\ncat_attr","execution_count":null,"outputs":[]},{"metadata":{"id":"KXpQiDgQbbxS"},"cell_type":"markdown","source":"## Train-test split"},{"metadata":{},"cell_type":"markdown","source":"<p style=\"text-align: justify; line-height: 2.0; text-indent: 5%; font-size: 14px; padding-right: 100px;\">\n    <b>StratifiedShuffleSplit</b> is used instead of sklearn.model_selection.train_test_split that implementing randomized splitting since the dataset is imbalanced, that is, 83.53% of negative (not match) and 15.47% of positive (match) samples, respectively. As a result, the percentage of the samples is maintained based on the proportion of 'match' after splitting.\n</p>"},{"metadata":{"id":"DYyuvkzElwR6","trusted":false},"cell_type":"code","source":"def split_data(X, y, n_splits=1, test_size=0.2, random_state=RANDOM_SEED):\n\n    # split using stratified sampling\n    split = StratifiedShuffleSplit(n_splits=n_splits, test_size=test_size,\n                                   random_state=random_state)\n\n    train_index, test_index = next(split.split(X, y))\n\n    X_train, X_test, y_train, y_test = X.iloc[train_index], X.iloc[test_index], y[train_index], y[test_index]\n\n    return X_train, X_test, y_train, y_test","execution_count":null,"outputs":[]},{"metadata":{"executionInfo":{"elapsed":12989,"status":"ok","timestamp":1599298630931,"user":{"displayName":"PolarBear Yap","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh6xY6wDRASo9ifgNeDcwMeBd3TCQFVGw14HLGtCg=s64","userId":"07391035732959922581"},"user_tz":-480},"id":"VBvrjgfoAYLs","outputId":"046c0b86-9bad-4917-a3ab-d52680ff7dbe","trusted":false},"cell_type":"code","source":"Y_FEATURE = 'match'\n\nX = dating1.copy().drop(Y_FEATURE, axis=1)\ny = dating1[Y_FEATURE]\n\nX_train, X_test, y_train, y_test = split_data(X, y, test_size=0.15)\n\nprint(f'Traning dataset shape:')\nprint(f'train X   : {X_train.shape}')\nprint(f'train y   : {y_train.shape}')\nprint(f'train X   : {X_test.shape}')\nprint(f'train y   : {y_test.shape}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data preprocessor Pipeline"},{"metadata":{},"cell_type":"markdown","source":"The data preprocessor pipeline is created as follows to allow easy integration with\n any models to form a complete training pipeline."},{"metadata":{"trusted":false},"cell_type":"code","source":"def generate_1_hot_attr(X=X, cat_attr=cat_attr):\n    # Generate one-hot-encoded feature's names\n    index = np.any(pd.isnull(X[cat_attr]), axis=1)\n    X_cat = X.loc[~index, cat_attr].copy()\n    one_hot_enc = OneHotEncoder(handle_unknown='ignore')\n    one_hot_enc.fit(X_cat)\n    return one_hot_enc.categories_, \\\n        one_hot_enc.get_feature_names(cat_attr)\n\n\ndef make_preprocess_pipeline(num_attr=num_attr, cat_attr=cat_attr):\n\n    one_hot_attrs, _ = generate_1_hot_attr(cat_attr=cat_attr)\n\n    # Impute the null values for categorical attribute\n    # Label encode using OneHotEncoder\n    categorical_pipleline = make_pipeline(\n        SimpleImputer(strategy='most_frequent'),\n        OneHotEncoder(categories=one_hot_attrs)\n    )\n\n    # Add mean value to the missing values for numerical attribute\n    numerical_pipeline = make_pipeline(\n        SimpleImputer(strategy='mean'),\n        StandardScaler()\n    )\n\n    # Combine numerical_pipeline and categorical_pipleline\n    preprocess_pipeline = make_column_transformer(\n        (numerical_pipeline, num_attr),\n        (categorical_pipleline, cat_attr),\n        remainder='passthrough')\n\n    return preprocess_pipeline\n\n\ndef make_training_pipeline(ml_model, num_attr=num_attr, cat_attr=cat_attr):\n\n    training_pipeline = make_pipeline(\n        make_preprocess_pipeline(num_attr, cat_attr),\n        ml_model\n    )\n\n    return training_pipeline","execution_count":null,"outputs":[]},{"metadata":{"id":"A0MuOGICHieJ"},"cell_type":"markdown","source":"# Prepare Classifiers"},{"metadata":{"id":"m1nbVJJTcJmg","lines_to_next_cell":2},"cell_type":"markdown","source":"List of classifiers chosen for model selection\n- Logistic Classifier\n- Linear Support Vector Classifier\n- K Neighbors Classifier\n- Multi-layer Perceptron classifier\n- Decision Tree\n- Random forest Classifier\n- Extra Tree Classifier\n- AdaBoost Classifier\n- Gradient Boosting Classifier\n- Bagging Classifier\n- CatBoost Classifier\n- XGB Classifier'"},{"metadata":{"id":"n7wdBK88HmMW","trusted":false},"cell_type":"code","source":"RANDOM_SEED = 42\n\nshort_names = ['log_reg', 'linear_svm', 'k_neighbors', 'neural_network',\n               'decision_tree', 'rand_forest', 'extra_tree', 'ada_boost_cf',\n               'gradient_b_cf', 'bagging_cf', 'catboost_cf', 'xg_boost']\n\nnames = ['Logistic Classifier', 'Linear Support Vector Classifier',\n         'K Neighbors Classifier', 'Multi-layer Perceptron classifier',\n         'Decision Tree', 'Random forest Classifier', 'Extra Tree Classifier',\n         'AdaBoost Classifier', 'Gradient Boosting Classifier',\n         'Bagging Classifier', 'CatBoost Classifier', 'XGBClassifier']\n\nfunctions = [\n    LogisticRegression(random_state=RANDOM_SEED, n_jobs=-1, max_iter=1000),\n    LinearSVC(C=1, loss=\"hinge\", random_state=RANDOM_SEED),\n    KNeighborsClassifier(n_neighbors=20, n_jobs=-1),\n    MLPClassifier(random_state=RANDOM_SEED, early_stopping=True),\n    DecisionTreeClassifier(),\n    RandomForestClassifier(random_state=RANDOM_SEED, n_jobs=-1),\n    ExtraTreesClassifier(random_state=RANDOM_SEED, n_jobs=-1),\n    AdaBoostClassifier(random_state=RANDOM_SEED),\n    GradientBoostingClassifier(random_state=RANDOM_SEED),\n    BaggingClassifier(random_state=RANDOM_SEED, n_jobs=-1),\n    CatBoostClassifier(random_seed=RANDOM_SEED, silent=True),\n    XGBClassifier(random_state=RANDOM_SEED, n_jobs=-1)\n]\n\nclassifiers_idx = {}\nclassifiers = {}\n\n# Zip all classfiers together into a dictionary for convenient access\nfor idx, s_name, name, func in zip(range(len(names)), short_names, names, functions):\n    classifiers_idx[idx] = {'name': name, 'func': func}\n    classifiers[s_name] = {'name': name, 'func': func}","execution_count":null,"outputs":[]},{"metadata":{"id":"y1EG8yIk4m0o"},"cell_type":"markdown","source":"# Model Selection"},{"metadata":{"id":"1OI-tv4o45AS"},"cell_type":"markdown","source":"## Phase 1: Performance Score"},{"metadata":{},"cell_type":"markdown","source":"<p style=\"text-align: justify; line-height: 2.0; text-indent: 5%; font-size: 14px; padding-right: 100px;\">\nWe are going to select a few top machine learning algorithm of different types by evaluating\nthe performance score measued in nested cross validated training sets. Nested cross validation is\nto used such that the scores are better estimates for generalization error. Scoring metrics used\nare f1 score, precision, recall and auc of roc.\n</p>"},{"metadata":{"id":"lILuUaOwLwjj","trusted":false},"cell_type":"code","source":"def get_models_performance(models, X, y, n_splits,\n                           scoring_metrics, num_attr=num_attr,\n                           cat_attr=cat_attr, random_state=RANDOM_SEED):\n\n    X = X.reset_index(drop=True)\n    y = y.reset_index(drop=True)\n\n    cv = StratifiedKFold(n_splits=n_splits,\n                         shuffle=True, random_state=random_state)\n\n    mean_cols = []\n    std_cols = []\n    for name in ['train', 'test']:\n        mean_cols += [f'{name}_{metric}' for metric in scoring_metrics]\n        std_cols += [f'{name}_{metric}_std' for metric in scoring_metrics]\n    cols = mean_cols + std_cols\n\n    results = {'model_name': [], 'duration': []}\n\n    for col in cols:\n        results[col] = []\n\n    # Loop through all models\n    for idx in range(len(models)):\n        cf_name = models[idx]['name']\n\n        print(f'{cf_name} has started...')\n        # Count time to get the duration of the models\n        start = time.time()\n\n        ml_pipeline = make_training_pipeline(\n            clone(models[idx]['func']), num_attr, cat_attr)\n        # cross_validate returns both train_score and test_score by setting return_train_score to True\n        cv_scores = cross_validate(ml_pipeline, X, y,\n                                   scoring=scoring_metrics, cv=cv,\n                                   return_train_score=True)\n\n        end = time.time()\n        duration = end - start\n        print(f'{cf_name} ended in {duration} seconds.\\n')\n\n        updateRecord(results, cv_scores, mean_cols, std_cols,\n                     cf_name, duration, scoring_metrics)\n\n    # Return as DataFrame instead of dictionary\n    return pd.DataFrame(results)\n\n# Append values to the dictionary based on key_name passed into the function\n\n\ndef updateRecord(df, scores, mean_cols, std_cols, model_name, duration, scoring_metrics):\n    df['model_name'].append(model_name)\n    df['duration'].append(duration)\n    for mean_col, std_col in zip(mean_cols, std_cols):\n        df[mean_col].append(np.mean(scores[mean_col]))\n        df[std_col].append(np.std(scores[mean_col]))\n\n\ndef sortValues(df, cols, sort_idx, ascending=False):\n    df = df.copy()\n    try:\n        cols.remove('model_name')\n        cols.remove('duration')\n    except ValueError:\n        pass\n    regex = '(?:^.+)(_after|_before)$'\n    for col in cols:\n        match = re.search(regex, col)\n        if not match:\n            col_std = f'{col}_std'\n        elif match.group(1) == '_before':\n            col_std = f'{col[:-7]}_std_before'\n        else:\n            col_std = f'{col[:-6]}_std_after'\n        df[col] = df[col].astype('float64')\n        df[col_std] = df[col_std].astype('float64')\n        def display(row): return f'{row[0]:.4f} +/-{row[1]:.4f}'\n        df[col] = df[[col, col_std]].apply(display, axis=1)\n    cols = np.array(cols)\n    sort_cols = list(cols[sort_idx]) if isinstance(sort_idx, list) \\\n        else [cols[sort_idx]]\n    df = df[['model_name'] +\n            list(cols)].sort_values(sort_cols, ascending=ascending)\n    return df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### First Round: Train Model with 100% Features"},{"metadata":{"trusted":false},"cell_type":"code","source":"scoring_metrics = ['f1', 'roc_auc', 'precision', 'recall']\nwarnings.filterwarnings(action='ignore', category=ConvergenceWarning)\nresults = get_models_performance(\n    classifiers_idx, X_train, y_train, 5, scoring_metrics)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p style=\"text-align: justify; line-height: 2.0; text-indent: 5%; font-size: 14px; padding-right: 100px;\">\nBased on the table below, all the classifiers were able to achieve more than 0.70 recall rate. Ensemble methods which are Random forest, Extra Trees Classifier, AdaBoost Classifier are clearly overfits the training set, while Complement Naive Bayes, Quadratic Discriminant Analysis performs very poor on precision.\n</p>"},{"metadata":{},"cell_type":"markdown","source":"<p style=\"text-align: justify; line-height: 2.0; font-size: 14px; padding-right: 100px;\">\ntest_results shows the mean test score of each models after undergo 5 folds cross validation.\n</p>"},{"metadata":{"trusted":false},"cell_type":"code","source":"roc = ['train_roc_auc', 'test_roc_auc']\nsortValues(results, roc, 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"f1 = ['train_f1', 'test_f1']\nsortValues(results, f1, 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"precision_recall = ['train_precision',\n                    'test_precision', 'train_recall', 'test_recall']\nsortValues(results, precision_recall, [1, 3])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sortValues(results, precision_recall, [3, 1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Second Round: Train Model with Features Selection"},{"metadata":{},"cell_type":"markdown","source":"Now, our model is very complicated, see if we can cut down any further without sacrificing too much\nclassification accuracy.\n\n<p style=\"text-align: justify; line-height: 2.0; font-size: 14px; padding-right: 100px;\">\nIn general, instead of using dimensionality reduction techniques like PCA, we\nare going to manually discard unimportant features by visualizing using the techniques as mentioned below:\n</p>\n\n1. Impurity based importances\n2. Permutation importances\n3. Spearman rank-order correlation coefficient"},{"metadata":{"trusted":false},"cell_type":"code","source":"def plot_impurity_based_importances(training_pipeline, X, y, feature_names, n_splits,\n                                    fig_w, fig_h, random_state=RANDOM_SEED):\n\n    X = X.reset_index(drop=True)\n    y = y.reset_index(drop=True)\n\n    feature_names = pd.Index(feature_names)\n\n    avg_feature_importances = np.zeros(len(feature_names))\n\n    cv = StratifiedKFold(n_splits=n_splits,\n                         shuffle=True, random_state=random_state)\n\n    with tqdm(total=100) as pbar:\n        progress_unit = 100/(n_splits)\n        for train_ix, _ in cv.split(X, y):\n            X_train = X.loc[train_ix]\n            y_train = y[train_ix]\n            cur_model = clone(training_pipeline)\n            cur_model.fit(X_train, y_train)\n            avg_feature_importances += cur_model[-1].feature_importances_\n            pbar.update(progress_unit)\n\n    tree_feature_importances = avg_feature_importances / n_splits\n    sorted_idx = tree_feature_importances.argsort()\n\n    y_ticks = np.arange(0, len(feature_names))\n    fig, ax = plt.subplots()\n    ax.barh(y_ticks, tree_feature_importances[sorted_idx])\n    ax.set_yticklabels(feature_names[sorted_idx])\n    ax.set_yticks(y_ticks)\n    ax.set_title(\"Random Forest Feature Importances (MDI)\")\n    fig.set_size_inches(fig_w, fig_h)\n    ax.title.set_fontsize(16)\n    plt.show()\n\n\ndef plot_permutation_importances(training_pipeline, X, y, feature_names, n_splits,\n                                 n_repeats, plot_title, fig_w, fig_h,\n                                 n_jobs=-1, random_state=RANDOM_SEED):\n\n    X = X.reset_index(drop=True)\n    y = y.reset_index(drop=True)\n\n    avg_importances_mean = np.zeros(len(feature_names))\n    avg_importances = np.zeros((len(feature_names), n_repeats))\n\n    cv = StratifiedKFold(n_splits=n_splits,\n                         shuffle=True, random_state=random_state)\n\n    with tqdm(total=100) as pbar:\n        progress_unit = 100/(n_splits)\n        for train_ix, test_ix in cv.split(X, y):\n            X_train = X.loc[train_ix]\n            y_train = y[train_ix]\n            X_test = X.loc[test_ix]\n            y_test = y[test_ix]\n            cur_model = clone(training_pipeline)\n            cur_model.fit(X_train, y_train)\n            result = permutation_importance(cur_model, X_test, y_test,\n                                            n_repeats=n_repeats, scoring='roc_auc',\n                                            random_state=random_state, n_jobs=n_jobs)\n            avg_importances_mean += result.importances_mean\n            avg_importances += result.importances\n            pbar.update(progress_unit)\n\n    avg_importances_mean /= n_splits\n    avg_importances /= n_splits\n    sorted_idx = avg_importances_mean.argsort()\n\n    fig, ax = plt.subplots()\n    ax.boxplot(avg_importances[sorted_idx].T,\n               vert=False, labels=feature_names)\n    ax.set_title(plot_title)\n    fig.set_size_inches(fig_w, fig_h)\n    ax.title.set_fontsize(16)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since the list of X columns after data preprocessing is not available, we have to code by our own."},{"metadata":{"trusted":false},"cell_type":"code","source":"_, one_hot_attrs = generate_1_hot_attr()\nX_preprocessed_attr = list(num_attr) + list(one_hot_attrs)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seems that field features are not important in the prediction.\nBut still, we have to use other means to confirm this hypothesis."},{"metadata":{"trusted":false},"cell_type":"code","source":"rand_forest_cf_pipeline = make_training_pipeline(\n    classifiers['rand_forest']['func'])\n\nplot_impurity_based_importances(rand_forest_cf_pipeline, X_train, y_train,\n                                X_preprocessed_attr, 5, 8, 80)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p style=\"text-align: justify; line-height: 2.0; text-indent: 5%; font-size: 14px; padding-right: 100px;\">\nAccording to <a href=\"https://christophm.github.io/interpretable-ml-book/feature-importance.html\">Christoph Molnar (2020)</a>,\n    permutation importances will not yield accurate measurement for features\nwith high correlation. It is because even if one of the features are removed, information from other\ncorrelated features still can cover the loss of information of that removed feature. As a result, we need to use\ncorrelation metrics such as Spearman rank-order correlation coefficient to decide whether a feature is really not\nimportant or having high correlation with other features based on the results from permutation importances.\n</p>\n<p style=\"text-align: justify; line-height: 2.0; font-size: 14px; padding-right: 100px;\">\nBased on the plot below, the lists of unimportant features by types are:\n</p>\n<table style=\"width:35%; float: left; display: inline-block;\">\n  <tr>\n    <th>gender</th>\n    <th>Example</th>\n  </tr>\n  <tr>\n    <td>age-related features</td>\n    <td>age, age_o, d_age</td>\n  </tr>\n  <tr>\n    <td>unknown feature</td>\n    <td>wave</td>\n  </tr>\n   <tr>\n    <td>field</td>\n    <td>field_sociology, field_money</td>\n  </tr>\n  <tr>\n    <td>interest-related features</td>\n    <td>shopping, music</td>\n  </tr>\n  <tr>\n    <td>partner-related features</td>\n    <td>intelligence_partner, funny_partner</td>\n  </tr>\n  <tr>\n    <td>race-related features</td>\n    <td>race, importance_same_race</td>\n  </tr>\n  <tr>\n    <td>features about partner's preference</td>\n    <td>pref_o_intelligence, pref_o_ambitious</td>\n  </tr>\n  <tr>\n    <td>features about partner's rating on self</td>\n    <td>intelligence_o, funny_o</td>\n  </tr>\n  <tr>\n    <td>features about self's preference</td>\n    <td>ambition_important, funny_important</td>\n  </tr>\n  <tr>\n    <td>features about self's rating on herself/himself</td>\n    <td>funny, intelligence</td>\n  </tr>\n</table>"},{"metadata":{"trusted":false},"cell_type":"code","source":"plot_permutation_importances(rand_forest_cf_pipeline, X_train, y_train,\n                             X_train.columns, 5, 8,\n                             'Permutation Importances (nested cross validated)',\n                             8, 85, RANDOM_SEED)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p style=\"text-align: justify; line-height: 2.0; font-size: 14px; padding-right: 100px;\">\nWe are going to visualize Spearman rank-order correlation coefficient using dendogram and heatmap, respectively.\nThe code is inspired from this <a href=\"https://scikit-learn.org/stable/auto_examples/inspection/plot_permutation_importance_multicollinear.html#sphx-glr-auto-examples-inspection-plot-permutation-importance-multicollinear-py\">sklearn guide</a>.\n</p>"},{"metadata":{"trusted":false},"cell_type":"code","source":"def plot_dendro_corr(X, feature_names, fig_w, fig_h,\n                     orientation='top', font_size=15,\n                     rotation=90):\n    fig, ax = plt.subplots(figsize=(fig_w, fig_h))\n    corr = spearmanr(X).correlation\n    corr_linkage = hierarchy.ward(corr)\n    dendro = hierarchy.dendrogram(\n        corr_linkage, labels=feature_names, ax=ax, leaf_rotation=rotation,\n        leaf_font_size=font_size, orientation=orientation\n    )\n    fig.tight_layout()\n    plt.show()\n\n\ndef plot_heatmap_corr_full(X, X_features, fig_w, fig_h, annot=False, enable_mask=True):\n\n    fig, ax = plt.subplots(figsize=(fig_w, fig_h))\n\n    corr = X.corr(method='spearman')\n    corr.index\n\n    # triu\n    if enable_mask:\n        mask = np.tril(np.ones_like(corr, dtype=bool))\n    else:\n        mask = False\n    sns.heatmap(corr, linewidths=0.1, linecolor='white',\n                square=True, annot=annot, mask=mask,\n                vmin=-1, vmax=1, center=0, ax=ax,\n                xticklabels=True,\n                yticklabels=True)\n\n    fig.tight_layout()\n    plt.tick_params(axis='both', which='minor', labelsize=15)\n    plt.show()\n\n\ndef plot_heatmap_corr(X, X_features, selected_features,\n                      fig_w, fig_h, annot=False):\n\n    fig, ax = plt.subplots(figsize=(fig_w, fig_h))\n\n    corr = X.corr(method='spearman')[\n        selected_features].drop(index=selected_features)\n    non_selected_features = corr.index\n    x_axis = selected_features\n    y_axis = non_selected_features\n    if len(y_axis) < len(x_axis):\n        corr = corr.T\n        xticklabels = non_selected_features\n        yticklabels = selected_features\n    else:\n        xticklabels = selected_features\n        yticklabels = non_selected_features\n\n    sns.heatmap(corr, linewidths=0.1, linecolor='white',\n                square=True, annot=annot,\n                vmin=-1, vmax=1, center=0, ax=ax,\n                xticklabels=True,\n                yticklabels=True)\n\n    ax.set_xticklabels(xticklabels, rotation='vertical')\n    ax.set_yticklabels(yticklabels, rotation='horizontal')\n    fig.tight_layout()\n    plt.tick_params(axis='both', which='minor', labelsize=15)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since we have 100++ features, we are certainly not going to visualize it as a whole. We going to chop down and\nanalyze piece by piece.\n```\nplot_heatmap_corr(X_train_imputed, X_preprocessed_attr, 80, 80)\n```"},{"metadata":{},"cell_type":"markdown","source":"*impute_pipe* is used to preprocess *X_train* before calculating the Spearman correlation."},{"metadata":{"trusted":false},"cell_type":"code","source":"def make_impute_pipeline(num_attr=num_attr, cat_attr=cat_attr):\n\n    # Impute the null values for categorical attribute\n    # Label encode using OneHotEncoder\n    categorical_pipleline = make_pipeline(\n        SimpleImputer(strategy='most_frequent'),\n        OneHotEncoder()\n    )\n\n    # Add mean value to the missing values for numerical attribute\n    numerical_pipeline = make_pipeline(\n        SimpleImputer(strategy='mean'),\n        StandardScaler()\n    )\n\n    # Combine numerical_pipeline and categorical_pipleline\n    impute_pipeline = make_column_transformer(\n        (numerical_pipeline, num_attr),\n        (categorical_pipleline, cat_attr),\n        remainder='passthrough')\n\n    return impute_pipeline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**field**"},{"metadata":{},"cell_type":"markdown","source":"<p style=\"text-align: justify; line-height: 2.0; font-size: 14px; padding-right: 100px;\">\nWe suspect that <i>field</i> might not be significant in predicting whether there is a match, therefore let's see\nif the one-hot encoded <i>field</i> values contains any correlations with other features or not.\n</p>"},{"metadata":{"trusted":false},"cell_type":"code","source":"_, one_hot_attrs = generate_1_hot_attr(X_train, cat_attr)\nX_preprocessed_attr = list(num_attr) + list(one_hot_attrs)\n\nX_train_imputed = make_impute_pipeline(\n    num_attr, cat_attr).fit_transform(X_train.copy())\nX_train_imputed = pd.DataFrame(\n    X_train_imputed.toarray(), columns=X_preprocessed_attr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plot_heatmap_corr(X_train_imputed, X_preprocessed_attr,\n                  one_hot_attrs, 20, 100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p style=\"text-align: justify; line-height: 2.0; font-size: 14px; padding-right: 100px;\">\nBased on the plot above, we can drop <i>field</i> since the it has low permutation importances score and\nlow correlation with other features.\n</p>"},{"metadata":{"trusted":false},"cell_type":"code","source":"dating2 = dating1.drop(columns='field', axis=1)\nY_FEATURE = 'match'\n\nX2 = dating2.copy().drop(Y_FEATURE, axis=1)\ny2 = dating2[Y_FEATURE]\n\nX2_train, X2_test, y2_train, y2_test = split_data(X2, y2, test_size=0.15)\n\nprint(f'Traning dataset shape:')\nprint(f'train X   : {X2_train.shape}')\nprint(f'train y   : {y2_train.shape}')\nprint(f'train X   : {X2_test.shape}')\nprint(f'train y   : {y2_test.shape}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**interest-related features**"},{"metadata":{},"cell_type":"markdown","source":"<p style=\"text-align: justify; line-height: 2.0; font-size: 14px; padding-right: 100px;\">\nFrom the permutation importances plot, we do notice that scores for interest-related features are\nquite low. Let's see if we can drop them.\n</p>"},{"metadata":{"trusted":false},"cell_type":"code","source":"cat_attr2 = cat_attr[:-1]\n_, one_hot_attrs2 = generate_1_hot_attr(X2_train, cat_attr2)\nX2_preprocessed_attr = list(num_attr) + list(one_hot_attrs2)\n\nX2_train_imputed = make_impute_pipeline(\n    num_attr, cat_attr2).fit_transform(X2_train.copy())\nX2_train_imputed = pd.DataFrame(X2_train_imputed, columns=X2_preprocessed_attr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"interest_features = ['sports', 'tvsports', 'exercise', 'dining', 'museums',\n                     'art', 'hiking', 'gaming',\n                     'clubbing', 'reading', 'tv', 'theater', 'movies',\n                     'concerts', 'music', 'shopping', 'yoga',\n                     'interests_correlate'\n                     ]\n\nplot_heatmap_corr(X2_train_imputed, X2_preprocessed_attr,\n                  interest_features, 30, 80, True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p style=\"text-align: justify; line-height: 2.0; font-size: 14px; padding-right: 100px;\">\nBased on the plot above, we can drop <i>interest-related features</i> since the most of the\nfeatures has  low permutation importances score and low or unmeaningful correlation with other features.\n</p>"},{"metadata":{"trusted":false},"cell_type":"code","source":"dating3 = dating2.drop(columns=interest_features, axis=1)\nY_FEATURE = 'match'\n\nX3 = dating3.copy().drop(Y_FEATURE, axis=1)\ny3 = dating3[Y_FEATURE]\n\nX3_train, X3_test, y3_train, y3_test = split_data(X3, y3, test_size=0.15)\n\nprint(f'Traning dataset shape:')\nprint(f'train X   : {X3_train.shape}')\nprint(f'train y   : {y3_train.shape}')\nprint(f'train X   : {X3_test.shape}')\nprint(f'train y   : {y3_test.shape}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"cat_attr3 = cat_attr[:-1]\nnum_attr3 = num_attr[:36] + num_attr[54:]\n_, one_hot_attrs3 = generate_1_hot_attr(X3_train, cat_attr3)\nX3_preprocessed_attr = list(num_attr3) + list(one_hot_attrs3)\n\nX3_train_imputed = make_impute_pipeline(\n    num_attr3, cat_attr3).fit_transform(X3_train.copy())\nX3_train_imputed = pd.DataFrame(X3_train_imputed, columns=X3_preprocessed_attr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plot_heatmap_corr_full(\n    X3_train_imputed, X3_preprocessed_attr, 25, 25, enable_mask=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p style=\"text-align: justify; line-height: 2.0; font-size: 14px; padding-right: 100px;\">\nAfter painstakingly observe every samples, below are the features that have\nlow permutation importances scores and low correlation with other features at\nthe same time. Therefore, we can confidently remove these features.\n</p>\n\n- wave\n- d_age\n- age\n- age_o\n- pref_o_intelligence\n- pref_o_funny\n- intellicence_important\n- funny_important\n\n<p style=\"text-align: justify; line-height: 2.0; text-indent: 5%; font-size: 14px; padding-right: 100px;\">\nBesides, we also can remove race-related features. It is because for self's race (identified\n    by the prefix <i>race_</i>) only negatively correlates to itself. It means that if a person is\n    Asian-American then that person is not African American. The same applies for partner's race.\n    Furthermore, there is visibly no correlation (1) between self's and partner's race, and\n    (2) between both races and importance_same_race and importance_same_religion.\n</p>"},{"metadata":{"trusted":false},"cell_type":"code","source":"age_related_features = ['d_age', 'age', 'age_o']\n\nrace_related_features = ['race', 'race_o', 'samerace',\n                         'importance_same_race', 'importance_same_religion']\n\nfeatures_to_be_dropped = ['wave', 'pref_o_intelligence',\n                          'pref_o_funny', 'intellicence_important',\n                          'funny_important'] + age_related_features + race_related_features\n\ndating4 = dating3.drop(columns=features_to_be_dropped, axis=1)\nY_FEATURE = 'match'\n\nX4 = dating4.copy().drop(Y_FEATURE, axis=1)\ny4 = dating4[Y_FEATURE]\n\nX4_train, X4_test, y4_train, y4_test = split_data(X4, y4, test_size=0.15)\n\nprint(f'Traning dataset shape:')\nprint(f'train X   : {X4_train.shape}')\nprint(f'train y   : {y4_train.shape}')\nprint(f'train X   : {X4_test.shape}')\nprint(f'train y   : {y4_test.shape}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"cols4 = list(dating4.columns)\ncat_attr4 = [cols4[0]]\nnum_attr4 = cols4[1:-1]\n_, one_hot_attrs4 = generate_1_hot_attr(X4_train, cat_attr4)\nX4_preprocessed_attr = list(num_attr4) + list(one_hot_attrs4)\n\nX4_train_imputed = make_impute_pipeline(\n    num_attr4, cat_attr4).fit_transform(X4_train.copy())\nX4_train_imputed = pd.DataFrame(X4_train_imputed, columns=X4_preprocessed_attr)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p style=\"text-align: justify; line-height: 2.0; text-indent: 5%; font-size: 14px; padding-right: 100px;\">\nNow, let's visualize the remaining features with full heatmap and dendrogram. Looks good to me ;)\nHowever, as a perfectionist, I think we can still simplify our models while sacrificing as little\nperformance as possible. We going to try <b>recursive feature elimination</b>. I still hope sklearn\ndeveloper will introduce <b>genetic algorithm</b> in feature selection though :(\n</p>"},{"metadata":{"trusted":false},"cell_type":"code","source":"plot_heatmap_corr_full(\n    X4_train_imputed, X4_preprocessed_attr, 12, 12, enable_mask=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plot_dendro_corr(X4_train_imputed, X4_preprocessed_attr,\n                 18, 10, orientation='top', font_size=15, rotation=90)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p style=\"text-align: justify; line-height: 2.0; line-indent: 5.0%; font-size: 14px; padding-right: 100px;\">\nBut first, let's check the model's performance of current subset of features with models that are trained with\nfull set of features just now.\n</p>"},{"metadata":{"trusted":false},"cell_type":"code","source":"results1 = get_models_performance(classifiers_idx, X4_train, y4_train,\n                                  5, scoring_metrics, num_attr4, cat_attr4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p style=\"text-align: justify; line-height: 2.0; line-indent: 5.0%; font-size: 14px; padding-right: 100px;\">\nTo effectively compare results, we are going to merge two results together into one single dataframe.\n</p>"},{"metadata":{"trusted":false},"cell_type":"code","source":"sum_results = results.merge(\n    results1, on='model_name', suffixes=('_before', '_after'))\nsum_results.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"roc2 = list(map(lambda elem: elem+'_before', roc))\nroc2 += list(map(lambda elem: elem+'_after', roc))\nroc2","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sortValues(sum_results, roc2, [1, 3])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p style=\"text-align: justify; line-height: 2.0; line-indent: 5.0%; font-size: 14px; padding-right: 100px;\">\nAfter comparing the results, we do not notice any significant improvement or deterioration in the model's\nperformance. It is due to the removed features are not important in prediciting the match outcome.\n</p>"},{"metadata":{"trusted":false},"cell_type":"code","source":"f1_2 = list(map(lambda elem: elem+'_before', f1))\nf1_2 += list(map(lambda elem: elem+'_after', f1))\nf1_2","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sortValues(sum_results, f1_2, [1, 3])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"try:\n    precision_recall.remove('train_precision')\n    precision_recall.remove('train_recall')\nexcept ValueError:\n    pass\nprecision_recall_2 = list(map(lambda elem: elem+'_before', precision_recall))\nprecision_recall_2 += list(map(lambda elem: elem+'_after', precision_recall))\nprecision_recall_2","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sortValues(sum_results, precision_recall_2, [1, 3])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sortValues(sum_results, precision_recall_2, [3, 1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Recursive feature elimination"},{"metadata":{"trusted":false},"cell_type":"code","source":"def recursive_feature_elimination(model, X, y, n_splits, scoring_metrics,\n                                  num_attr, cat_attr, preprocessed_attr,\n                                  random_state=RANDOM_SEED):\n\n    X = X.reset_index(drop=True)\n    y = y.reset_index(drop=True)\n\n    cv = StratifiedKFold(n_splits=n_splits,\n                         shuffle=True, random_state=random_state)\n\n    rfecv = RFECV(clone(model['func']), step=1, cv=cv,\n                  scoring=scoring_metrics, n_jobs=-1)\n    ml_pipeline = make_training_pipeline(rfecv, num_attr, cat_attr)\n    ml_pipeline.fit(X, y)\n\n    print('Optimal number of features : {}\\nDropped features: {}'.format(\n        rfecv.n_features_,\n        ', '.join(np.array(preprocessed_attr)[~rfecv.support_]))\n    )\n    # Plot number of features VS. cross-validation scores\n    plt.figure()\n    plt.xlabel(\"Number of features selected\")\n    plt.ylabel(\"Cross validation score (nb of correct classifications)\")\n    plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\n    plt.show()\n\n    return rfecv","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p style=\"text-align: justify; line-height: 2.0; text-indent: 5%; font-size: 14px; padding-right: 100px;\">\nHmmm, the results are quite inconsistent across different models. We can drop quite many features for\nXGB Classfier but we can't drop any feature for Random Forest Classifier. Well, we have done our best,\nI think :)\n</p>"},{"metadata":{"trusted":false},"cell_type":"code","source":"rfecv1 = recursive_feature_elimination(classifiers['xg_boost'],\n                                       X4_train, y4_train, 5, 'roc_auc',\n                                       num_attr4, cat_attr4, X4_preprocessed_attr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"rfecv2 = recursive_feature_elimination(classifiers['log_reg'],\n                                       X4_train, y4_train, 5, 'roc_auc',\n                                       num_attr4, cat_attr4, X4_preprocessed_attr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"rfecv3 = recursive_feature_elimination(classifiers['rand_forest'],\n                                       X4_train, y4_train, 5, 'roc_auc',\n                                       num_attr4, cat_attr4, X4_preprocessed_attr)","execution_count":null,"outputs":[]},{"metadata":{"id":"N7vZuUut4pIH"},"cell_type":"markdown","source":"## Phase 2: Precision-Recall Curve"},{"metadata":{"trusted":false},"cell_type":"code","source":"def plot_precision_vs_recall(classifier, cf_name, X, y, ax, method, n_splits,\n                             num_attr, cat_attr, label=False, random_state=RANDOM_SEED):\n\n    X = X.reset_index(drop=True)\n    y = y.reset_index(drop=True)\n    # get accurate y_scores using cross_val_predict, not from overfitted models\n    # y_scores are generate using 'predict_proba' method of each models,\n    # therefore probabilities of each class (total of 2) are returned\n    ml_pipeline = make_training_pipeline(classifier, num_attr, cat_attr)\n    cv = StratifiedKFold(n_splits=n_splits,\n                         shuffle=True, random_state=random_state)\n    y_scores_cv = cross_val_predict(ml_pipeline, X, y,\n                                    cv=cv, method=method, n_jobs=-1)\n\n    # Get the last columns of the y_scores only if more than one columns are detected\n    if y_scores_cv.ndim > 1:\n        y_scores_cv = y_scores_cv[:, -1]\n\n    precisions, recalls, thresholds = precision_recall_curve(y, y_scores_cv)\n    auc_score = average_precision_score(y, y_scores_cv)\n\n    # Adjust settings for the plot (eg. set title of the plot)\n    if label:\n        label_name = cf_name\n    else:\n        label_name = None\n    ax.plot(recalls, precisions, label=label_name)\n    ax.set(xlabel='recall', ylabel='precision',\n           title=f'PR Curve for {cf_name}')\n    ax.title.set_fontsize(16)\n    ax.grid()\n\n    return precisions, recalls, thresholds, auc_score\n\n\ndef create_subplots(num_subplots, num_cols_per_row, fig_w, fig_h):\n    num_rows = ceil(num_subplots / num_cols_per_row)\n    indexes = list(product(range(num_rows), range(num_cols_per_row)))\n    fig, axs = plt.subplots(num_rows, num_cols_per_row)\n    fig.set_size_inches(fig_w, num_rows * fig_h)\n    return num_rows, indexes, axs, fig\n\n\ndef plot_pr_curves(classifiers, X, y, n_splits, num_attr, cat_attr,\n                   num_cols_per_row=4, fig_w=8, fig_h=6,\n                   sameplot=False, random_state=RANDOM_SEED):\n    num_rows, indexes, axs, fig = create_subplots(1 if sameplot else len(classifiers),\n                                                  num_cols_per_row, fig_w, fig_h)\n    auc_pr_curves = []\n\n    with tqdm(total=100) as pbar:\n        progress_unit = 100/len(classifiers)\n\n        models_with_no_p_proba = ['Linear Support Vector Classifier']\n        for classifier in classifiers.values():\n            method = 'predict_proba' if classifier['name'] not in models_with_no_p_proba else 'predict'\n            ax = axs if num_cols_per_row == 1 or sameplot else axs[\n                idx] if num_rows == 1 else axs[indexes[idx][0]][indexes[idx][1]]\n            _, _, _, auc_score = plot_precision_vs_recall(classifier['func'], classifier['name'],\n                                                          X, y, ax, method, n_splits, num_attr,\n                                                          cat_attr, label=True)\n            auc_pr_curves.append(\n                {'name': classifier['name'], 'auc_score': auc_score})\n            pbar.update(progress_unit)\n\n        ax.set(xlabel='recall', ylabel='precision',\n               title=f'PR Curve for All Models')\n        ax.title.set_fontsize(20)\n        fig.legend(loc='upper right')\n        fig.tight_layout()\n\n    return auc_pr_curves","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p style=\"text-align: justify; line-height: 2.0; text-indent: 5%; font-size: 14px; padding-right: 100px;\">\nBased on the cross-validated precision-recall curve below, Complement Naive Bayes, Quadratic Discriminant Analysis are discarded since both performed even worse than a purely random dummy classifier, that is, the area under curve is less than 5.0.\n</p>"},{"metadata":{},"cell_type":"markdown","source":"<p style=\"text-align: justify; line-height: 2.0; text-indent: 5%; font-size: 14px; padding-right: 100px;\">\nCombining all cross-validated precision recall curve under one figure as shown below, we can clearly observe that Decision Tree Classifier and K Neighbours Classifier have the lowest area under curve as compared to the less of the models. Therefore, these models are later discarded in the phase 4.\n</p>"},{"metadata":{"trusted":false},"cell_type":"code","source":"auc_pr_curves = plot_pr_curves(\n    classifiers, X4_train, y4_train, 5, num_attr4, cat_attr4, 1, 12, 8, sameplot=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p style=\"text-align: justify; line-height: 2.0; font-size: 14px; padding-right: 100px;\">\n<b>Linear Support Vector Classifier</b> and <b>Decision Tree</b> have a very low auc for precision-recall curve.\n</p>"},{"metadata":{"trusted":false},"cell_type":"code","source":"auc_pr_curves = pd.DataFrame(auc_pr_curves)\nauc_pr_curves.sort_values('auc_score', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"id":"jEDmmU-k4gFd"},"cell_type":"markdown","source":"## Phase 3: ROC Curve"},{"metadata":{"trusted":false},"cell_type":"code","source":"def plot_roc_curves(classifiers, X, y, n_splits, num_attr, cat_attr, num_cols_per_row=4,\n                    fig_w=8, fig_h=6, sameplot=False, random_state=RANDOM_SEED):\n    num_rows, indexes, axs, fig = create_subplots(1 if sameplot else len(classifiers),\n                                                  num_cols_per_row, fig_w, fig_h)\n\n    roc_scores = []\n    X = X.reset_index(drop=True)\n    y = y.reset_index(drop=True)\n\n    with tqdm(total=100) as pbar:\n        progress_unit = 100/len(classifiers)\n\n        models_with_no_p_proba = ['Linear Support Vector Classifier']\n        # Iterate all classifiers to plot on the same axis\n        for classifier in classifiers.values():\n\n            new_cf = clone(classifier['func'])\n            method = 'predict_proba' if classifier['name'] not in models_with_no_p_proba else 'predict'\n            ax = axs if num_cols_per_row == 1 or sameplot else axs[\n                idx] if num_rows == 1 else axs[indexes[idx][0]][indexes[idx][1]]\n\n            # get cross_validated y_score of training set from cross_val_predict,\n            # without having to fit the whole training set or use test set\n            cv = StratifiedKFold(n_splits=n_splits,\n                                 shuffle=True, random_state=random_state)\n            ml_pipeline = make_training_pipeline(new_cf, num_attr, cat_attr)\n            y_score_cv = cross_val_predict(\n                ml_pipeline, X, y, cv=cv, method=method)\n\n            # Get the last columns of the y_scores only if more than one columns are detected\n            if y_score_cv.ndim > 1:\n                y_score_cv = y_score_cv[:, -1]\n\n            # Plot the ROC curve\n            fpr, tpr, threshold = roc_curve(y_train, y_score_cv)\n            roc_auc = auc(fpr, tpr)\n\n            graph = RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc,\n                                    estimator_name=classifier['name'])\n            graph.plot(ax=ax)\n\n            roc_scores.append(\n                {'name': classifier['name'], 'auc_score': roc_auc})\n            pbar.update(progress_unit)\n\n        ax.set(xlabel='False positive rate', ylabel='True positive rate',\n               title=f'ROC Curve with cross validation')\n        ax.title.set_fontsize(20)\n        ax.legend(loc=\"lower right\")\n        fig.tight_layout()\n        plt.show()\n\n        return roc_scores","execution_count":null,"outputs":[]},{"metadata":{"executionInfo":{"elapsed":128923,"status":"ok","timestamp":1599298747311,"user":{"displayName":"PolarBear Yap","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh6xY6wDRASo9ifgNeDcwMeBd3TCQFVGw14HLGtCg=s64","userId":"07391035732959922581"},"user_tz":-480},"id":"FAS988yNNFp_","incorrectly_encoded_metadata":"_kg_hide-output=true","outputId":"91ba4b24-6479-4efb-f59a-6cd9c564791d","trusted":false},"cell_type":"code","source":"roc_auc_scores = plot_roc_curves(\n    classifiers, X4_train, y4_train, 5, num_attr4, cat_attr4, 1, 12, 8, sameplot=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p style=\"text-align: justify; line-height: 2.0; font-size: 14px; padding-right: 100px;\">\n<b>Linear Support Vector Classifier</b> and <b>Decision Tree</b> also have a very low auc for ROC curve.\nWe can discard them later.\n</p>"},{"metadata":{"trusted":false},"cell_type":"code","source":"roc_auc_scores = pd.DataFrame(roc_auc_scores)\nroc_auc_scores.sort_values('auc_score', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"id":"IJrbS4pQVJH7"},"cell_type":"markdown","source":"## Phase 4: Final Selection"},{"metadata":{},"cell_type":"markdown","source":"<ol>\n<li>Logistic Classifier</li>\n<li>K Neighbors Classifier</li>\n<li>Multi-layer Perceptron classifier</li>\n<li>Ensemble Tree</li>\n\n<ul>\n    <li>Random forest Classifier</li>\n    <li>Extra Tree Classifier</li>\n</ul>\n\n<li>Bagging Classifier</li>\n<li>Boosting algorithm</li>\n\n<ul>\n  <li>AdaBoost Classifier</li>\n  <li>Gradient Boosting Classifier</li>\n  <li>CatBoost Classifier</li>\n  <li>XGBClassifier</li>\n</ul>\n\n<li>Linear Support Vector Classifier</li>\n<li>Decision Tree</li>\n</ol>"},{"metadata":{"id":"HsitEAc8VP3U"},"cell_type":"markdown","source":"<p style=\"text-align: justify; line-height: 2.0; text-indent: 5%; font-size: 14px; padding-right: 100px;\">\nWe are going to select models that have decent performance and each belonging to\ndifferent families of algorithm. I am also going to choose <b>XGB classifier</b> since it\nhave relatively better f1 score. I am also going to choose <b>Random Forest Classifier, Logistic Classifier,\nK Neighbors Classifier, and Multi-layer Perceptron classifier</b>. The reason I choose so many models so that I can build\nvoting and stacking classifier after model tuning to improve the robustness of the final model.\n</p>"},{"metadata":{"trusted":false},"cell_type":"code","source":"roc_scores = roc_auc_scores.merge(\n    auc_pr_curves, on='name', suffixes=('_roc', '_pr'))\nroc_scores.sort_values(['auc_score_roc', 'auc_score_pr'], ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"id":"11C2RaonLrIQ","trusted":false},"cell_type":"code","source":"selected_cfs = {}\n\nfor key in ['log_reg', 'k_neighbors', 'neural_network', 'rand_forest', 'xg_boost']:\n    selected_cfs[key] = classifiers[key]","execution_count":null,"outputs":[]},{"metadata":{"executionInfo":{"elapsed":128861,"status":"ok","timestamp":1599298747317,"user":{"displayName":"PolarBear Yap","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh6xY6wDRASo9ifgNeDcwMeBd3TCQFVGw14HLGtCg=s64","userId":"07391035732959922581"},"user_tz":-480},"id":"_3iW6QhNVNrK","outputId":"056c935c-70a4-4db8-ac78-cac40a7d0ece","trusted":false},"cell_type":"code","source":"print('List of Choosen Models')\n\nfor idx, classifier in enumerate(selected_cfs.values()):\n    print(f'{idx+1} - {classifier[\"name\"]}')","execution_count":null,"outputs":[]},{"metadata":{"id":"BDPxcecd5S8b"},"cell_type":"markdown","source":"# Model Tuning\n\n"},{"metadata":{},"cell_type":"markdown","source":"<p style=\"text-align: justify; line-height: 2.0; text-indent: 5%; font-size: 14px; padding-right: 100px;\">\nWe are going to use randomized search instead of grid search since it takes too much time to go through all possibilities.\n</p>\n<p style=\"text-align: justify; line-height: 2.0; text-indent: 5%; font-size: 14px; padding-right: 100px;\">\nWe are going to choose the best combination of hyperparameters sorted by the <b>num_trials, mean_cv_score and mean_test_score.</b>\nAssuming <i>X_train</i> and <i>y_train</i> is the intial training set we feed into the algorithm, the algorithm will perform nested cross validated\nrandomized search by splitting <i>X_train</i> and <i>y_train</i> into k1-outer-fold <i>X_outer_train</i> and <i>y_outer_train</i>. Then, the algorithm will split\n<i>X_outer_train</i> and <i>y_outer_train</i> into k2-inner-fold <i>X_inner_train</i> and <i>y_inner_train</i>.\n<b>num_trials</b> is the number of times a combination of hyparameters explored by the randomized search during the k2-inner-fold sets.\n<b>mean_test_score</b> is the average test score for each k2-inner-fold sets for each hyperparameters combination.\n<b>mean_cv_score</b> is the average cross validated score across all k1-outer-fold sets for each hyperparameters combination.\n</p>\n<p style=\"text-align: justify; line-height: 2.0; text-indent: 5%; font-size: 14px; padding-right: 100px;\">\nTo put it simply, k2-inner-fold sets are used to find the hyperparameter's combination of the best estimator,\nwhile the k1-outer fold sets are used to evaluate that best estimator with that hyperparameter's combination.\nThe purpose is to prevent the randomized search from producing overly optimistic results, which cause the model to overfit\nthe original training set and does not generalize well to real-world data with different variations and distributions.\n</p>"},{"metadata":{"trusted":false},"cell_type":"code","source":"def nested_cv_param_search(model, param_grid, X, y, num_attr, cat_attr,\n                           n_iter, scoring, n_outer_splits, n_inner_spits,\n                           random_state=RANDOM_SEED):\n\n    cv_outer = StratifiedKFold(n_splits=n_outer_splits, shuffle=False)\n    cv_inner = StratifiedKFold(\n        n_splits=n_inner_spits, shuffle=True, random_state=random_state)\n\n    outer_roc_score = list()\n    inner_roc_score = list()\n\n    X = X.reset_index(drop=True)\n    y = y.reset_index(drop=True)\n\n    with tqdm(total=100) as pbar:\n        progress_unit = 75/(n_outer_splits)\n\n        for train_ix, test_ix in cv_outer.split(X, y):\n            X_train, X_test = X.loc[train_ix, :], X.loc[test_ix, :]\n            y_train, y_test = y[train_ix], y[test_ix]\n\n            estimator = make_training_pipeline(\n                clone(model), num_attr, cat_attr)\n            search = RandomizedSearchCV(estimator, param_grid, n_iter=n_iter,\n                                        scoring=scoring, cv=cv_inner, refit=True)\n            result = search.fit(X_train, y_train)\n\n            inner_roc_score.append(result.cv_results_)\n\n            best_model = result.best_estimator_\n            y_test_pred = best_model.predict(X_test)\n            roc_score = roc_auc_score(y_test, y_test_pred)\n            outer_roc_score.append(roc_score)\n\n            pbar.update(progress_unit)\n\n        features = ['params', 'mean_test_score', 'std_test_score']\n        base = pd.DataFrame()\n\n        for roc_score in inner_roc_score:\n            roc_score = pd.DataFrame(roc_score)[features]\n            base = base.append(roc_score, ignore_index=True)\n\n        base['params'] = base['params'].astype('str')\n        agg_mean = base.groupby('params')['mean_test_score']\n        new_df = {'mean_test_score': agg_mean.mean(),\n                  'std_test_score': agg_mean.std(), 'num_trials': agg_mean.count()}\n        param_result = pd.DataFrame(new_df).reset_index()\n\n        param_result['mean_cv_score'] = 0\n        param_result['std_cv_score'] = 0\n\n        params = list(param_result['params'].value_counts().index)\n        progress_unit = 25/(n_outer_splits * len(params))\n\n        for param in params:\n            outer_roc_score = []\n            for train_ix, test_ix in cv_outer.split(X, y):\n                X_train, X_test = X.loc[train_ix, :], X.loc[test_ix, :]\n                y_train, y_test = y[train_ix], y[test_ix]\n\n                estimator = make_training_pipeline(\n                    clone(model), num_attr, cat_attr)\n                estimator.set_params(**ast.literal_eval(param))\n                result = estimator.fit(X_train, y_train)\n\n                y_test_pred = estimator.predict(X_test)\n                roc_score = roc_auc_score(y_test, y_test_pred)\n                outer_roc_score.append(roc_score)\n\n                pbar.update(progress_unit)\n\n            mean = np.mean(outer_roc_score)\n            std = np.std(outer_roc_score)\n            index = list(param_result['params'] == param).index(True)\n            param_result.loc[index, 'mean_cv_score'] = mean\n            param_result.loc[index, 'std_cv_score'] = std\n\n    outer_roc_score = pd.DataFrame(\n        outer_roc_score, columns=[f'{scoring}_score'])\n    return param_result","execution_count":null,"outputs":[]},{"metadata":{"id":"4N9HcSyFdxYi"},"cell_type":"markdown","source":"## Model 1: Logistic Regression"},{"metadata":{},"cell_type":"markdown","source":"We are going to tune C and l1_ratio for Logistic Regression."},{"metadata":{"trusted":false},"cell_type":"code","source":"log_reg = LogisticRegression(n_jobs=-1, max_iter=7000, random_state=RANDOM_SEED,\n                             solver='saga', penalty='elasticnet')\n\nlog_reg_param_grid = {'logisticregression__C': [0.01, 0.1, 1, 10, 100, 1000],\n                      'logisticregression__l1_ratio': [0, 0.25, 0.50, 0.75, 1]}","execution_count":null,"outputs":[]},{"metadata":{"id":"i9F6BoPnCpvp"},"cell_type":"markdown","source":"**Note**: Results of the randomized search is loaded from the pickle file because it takes time to run. Please run the code below if you insist.\n\n```python\nlog_param_result = nested_cv_param_search(log_reg, log_reg_param_grid,\n                                          X4_train, y4_train, num_attr4,\n                                          cat_attr4, 10, 'roc_auc', 5, 4)\n\ndump_objects('log_reg_cv_rand_search', log_param_result)\n```"},{"metadata":{},"cell_type":"markdown","source":"Here's the nested cross validated scores from randomized search for each explored hyperparameters' combinations for Logistic Regression."},{"metadata":{"trusted":true},"cell_type":"code","source":"[log_param_result] = load_objects(file_name='log_reg_cv_rand_search')\nlog_param_result","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"log_param_ranked = log_param_result.sort_values(\n    ['num_trials', 'mean_cv_score', 'mean_test_score'], ascending=False).reset_index(drop=True)\nlog_param_ranked.loc[:10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The best combination of hyperparameters for Logistic Regression."},{"metadata":{"trusted":false},"cell_type":"code","source":"log_best_param = log_param_ranked.loc[0, 'params']\nlog_best_param","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model 2: K-Nearest Neighbors Classifier"},{"metadata":{},"cell_type":"markdown","source":"We are going to tune n_neighbors for  K-Nearest Neighbors Classifier."},{"metadata":{"trusted":false},"cell_type":"code","source":"k_nearest_neigh = KNeighborsClassifier(weights='distance', n_jobs=-1)\n\nk_nearest_neigh_param_grid = {'kneighborsclassifier__n_neighbors': [\n    10, 20, 30, 40, 50, 60, 70, 80, 90, 100]}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Note**: Results of the randomized search is loaded from the pickle file because it takes time to run. Please run the code below if you insist.\n\n```python\nknn_param_result = nested_cv_param_search(k_nearest_neigh, k_nearest_neigh_param_grid,\n                                          X4_train, y4_train, num_attr4,\n                                          cat_attr4, 10, 'roc_auc', 5, 4)\n\ndump_objects('knn_cv_rand_search', knn_param_result)\n```"},{"metadata":{},"cell_type":"markdown","source":"Here's the nested cross validated scores from randomized search for each explored hyperparameters' combinations for K-Nearest Neighbors Classifier."},{"metadata":{"trusted":false},"cell_type":"code","source":"[knn_param_result] = load_objects(file_name='knn_cv_rand_search')\nknn_param_result","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"knn_param_ranked = knn_param_result.sort_values(\n    ['num_trials', 'mean_cv_score', 'mean_test_score'], ascending=False).reset_index(drop=True)\nknn_param_ranked.loc[:5]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The best combination of hyperparameters for K-Nearest Neighbors Classifier."},{"metadata":{"trusted":false},"cell_type":"code","source":"knn_best_param = knn_param_ranked.loc[0, 'params']\nknn_best_param","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model 3: Multi-layer Perceptron Classifier"},{"metadata":{},"cell_type":"markdown","source":"We are going to tune alpha, beta_1, and beta_2 for Multi-layer Perceptron Classifier."},{"metadata":{"trusted":false},"cell_type":"code","source":"neural_network = MLPClassifier(early_stopping=True, random_state=RANDOM_SEED)\n\nneural_network_param_grid = {\n    'mlpclassifier__alpha': [0.0001, 0.001, 0.1, 1, 10],\n    'mlpclassifier__beta_1': [0.1, 0.3, 0.6, 0.9],\n    'mlpclassifier__beta_2': [0.1, 0.3, 0.6, 0.9]\n}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Note**: Results of the randomized search is loaded from the pickle file because it takes time to run. Please run the code below if you insist.\n\n```python\nneural_network_param_result = nested_cv_param_search(neural_network, neural_network_param_grid,\n                                                     X4_train, y4_train, num_attr4,\n                                                     cat_attr4, 10, 'roc_auc', 5, 4)\n\ndump_objects('neural_network_cv_rand_search', neural_network_param_result)\n```"},{"metadata":{},"cell_type":"markdown","source":"Here's the nested cross validated scores from randomized search for each explored hyperparameters' combinations for Multi-layer Perceptron Classifier."},{"metadata":{"trusted":false},"cell_type":"code","source":"[neural_network_param_result] = load_objects(\n    file_name='neural_network_cv_rand_search')\nneural_network_param_result","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"neural_network_param_ranked = neural_network_param_result.sort_values(\n    ['num_trials', 'mean_cv_score', 'mean_test_score'], ascending=False).reset_index(drop=True)\nneural_network_param_ranked.loc[:10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The best combination of hyperparameters for Multi-layer Perceptron Classifier."},{"metadata":{"trusted":false},"cell_type":"code","source":"neural_network_best_param = neural_network_param_ranked.loc[0, 'params']\nneural_network_best_param","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model 4: Random Forest Classifier"},{"metadata":{},"cell_type":"markdown","source":"We are going to tune min_samples_split, min_samples_leaf, and max_samples for Random Forest Classifier."},{"metadata":{"trusted":false},"cell_type":"code","source":"rand_forest = RandomForestClassifier(\n    bootstrap=True, random_state=RANDOM_SEED, n_jobs=-1)\n\nrand_forest_param_grid = {\n    'randomforestclassifier__min_samples_split': [0.01, 0.05, 0.10, 0.15],\n    'randomforestclassifier__min_samples_leaf': [0.01, 0.05, 0.10, 0.15],\n    'randomforestclassifier__max_samples': [0.70, 0.80, 0.90],\n}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Note**: Results of the randomized search is loaded from the pickle file because it takes time to run. Please run the code below if you insist.\n\n```python\nrand_forest_param_result = nested_cv_param_search(rand_forest, rand_forest_param_grid,\n                                                  X4_train, y4_train, num_attr4,\n                                                  cat_attr4, 15, 'roc_auc', 5, 4)\n\ndump_objects('rand_forest_cv_rand_search', rand_forest_param_result)\n```"},{"metadata":{},"cell_type":"markdown","source":"Here's the nested cross validated scores from randomized search for each explored hyperparameters' combinations for Random Forest Classifier."},{"metadata":{"trusted":false},"cell_type":"code","source":"[rand_forest_param_result] = load_objects(\n    file_name='rand_forest_cv_rand_search')\nrand_forest_param_result","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"rand_forest_param_ranked = rand_forest_param_result.sort_values(\n    ['num_trials', 'mean_cv_score', 'mean_test_score'], ascending=False).reset_index(drop=True)\nrand_forest_param_ranked.loc[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"rand_forest_param_ranked = rand_forest_param_result.sort_values(\n    ['mean_cv_score', 'num_trials', 'mean_test_score'], ascending=False).reset_index(drop=True)\nrand_forest_param_ranked.loc[:10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The best combination of hyperparameters for Random Forest Classifier."},{"metadata":{"trusted":false},"cell_type":"code","source":"rand_forest_best_param = rand_forest_param_ranked.loc[1, 'params']\nrand_forest_best_param","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model 5: XGB Classifier"},{"metadata":{},"cell_type":"markdown","source":"We are going to tune learning_rate, min_child_weight, subsample and colsample_bytree for XGB Classifier."},{"metadata":{"trusted":false},"cell_type":"code","source":"xgb_cf = XGBClassifier(random_state=RANDOM_SEED, n_jobs=-1)\n\nxgb_cf_param_grid = {\n    'xgbclassifier__learning_rate': [0.01, 0.05, 0.12, 0.20],\n    'xgbclassifier__min_child_weight': [3, 6, 9],\n    'xgbclassifier__subsample': [0.5, 0.7, 0.9],\n    'xgbclassifier__colsample_bytree': [0.5, 0.7, 0.9],\n}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Note**: Results of the randomized search is loaded from the pickle file because it takes time to run. Please run the code below if you insist.\n\n```python\nxgb_cf_param_result = nested_cv_param_search(xgb_cf, xgb_cf_param_grid,\n                                             X4_train, y4_train, num_attr4,\n                                             cat_attr4, 20, 'roc_auc', 5, 4)\n\ndump_objects('xgb_cf_cv_rand_search', xgb_cf_param_result)\n```"},{"metadata":{},"cell_type":"markdown","source":"Here's the nested cross validated scores from randomized search for each explored hyperparameters' combinations for XGB Classifier."},{"metadata":{"trusted":false},"cell_type":"code","source":"[xgb_cf_param_result] = load_objects(file_name='xgb_cf_cv_rand_search')\nxgb_cf_param_result","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"xgb_cf_param_ranked = xgb_cf_param_result.sort_values(\n    ['num_trials', 'mean_cv_score', 'mean_test_score'], ascending=False).reset_index(drop=True)\nxgb_cf_param_ranked.loc[:10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The best combination of hyperparameters for XGB Classifier."},{"metadata":{"trusted":false},"cell_type":"code","source":"xgb_cf_best_param = xgb_cf_param_ranked.loc[0, 'params']\nxgb_cf_best_param","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Combining Models"},{"metadata":{},"cell_type":"markdown","source":"<p style=\"text-align: justify; line-height: 2.0; font-size: 14px; padding-right: 100px;\">\nLet's see if combining the models together into voting and stacking classifiers will improve the model performance or not.\n</p>"},{"metadata":{"trusted":false},"cell_type":"code","source":"optimized_models = []\nmodels_name = ['log_reg', 'knn', 'neural_network', 'rand_forest', 'xgb_cf']\nbest_params = [log_best_param, knn_best_param,\n               neural_network_best_param, rand_forest_best_param, xgb_cf_best_param]\n\nfor cf, param in zip(selected_cfs.values(), best_params):\n    new_cf = clone(cf['func'])\n    param = re.sub('(?<=\\').+__(?=.+\\')', '', param)\n    new_cf.set_params(**ast.literal_eval(param))\n    optimized_models.append(new_cf)\n\noptimized_models = list(zip(models_name, optimized_models))\noptimized_models","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def get_stack_cf_cv_scores(model, X, y, num_attr, cat_attr,\n                           n_outer_splits, n_inner_spits,\n                           random_state=RANDOM_SEED):\n\n    cv_outer = StratifiedKFold(n_splits=n_outer_splits, shuffle=False)\n    cv_inner = StratifiedKFold(\n        n_splits=n_inner_spits, shuffle=True, random_state=random_state)\n\n    model.set_params(cv=cv_inner)\n\n    outer_roc_score = list()\n\n    X = X.reset_index(drop=True)\n    y = y.reset_index(drop=True)\n\n    with tqdm(total=100) as pbar:\n        progress_unit = 100/(n_outer_splits)\n\n        for train_ix, test_ix in cv_outer.split(X, y):\n            X_train, X_test = X.loc[train_ix, :], X.loc[test_ix, :]\n            y_train, y_test = y[train_ix], y[test_ix]\n\n            estimator = make_training_pipeline(\n                clone(model), num_attr, cat_attr)\n            estimator.fit(X_train, y_train)\n\n            y_test_pred = estimator.predict(X_test)\n            roc_score = roc_auc_score(y_test, y_test_pred)\n            outer_roc_score.append(roc_score)\n\n            pbar.update(progress_unit)\n\n    outer_roc_score = pd.DataFrame(outer_roc_score, columns=[f'roc_score'])\n    return outer_roc_score\n\n\ndef get_voting_cf_cv_scores(model, X, y, num_attr, cat_attr,\n                            n_outer_splits, n_inner_spits, scoring='roc_auc',\n                            random_state=RANDOM_SEED):\n\n    cv_outer = StratifiedKFold(n_splits=n_outer_splits, shuffle=False)\n    cv_inner = StratifiedKFold(\n        n_splits=n_inner_spits, shuffle=True, random_state=random_state)\n\n    roc_scores = pd.DataFrame()\n\n    X = X.reset_index(drop=True)\n    y = y.reset_index(drop=True)\n\n    with tqdm(total=100) as pbar:\n        progress_unit = 100/(n_outer_splits)\n\n        for train_outer_ix, test_outer_ix in cv_outer.split(X, y):\n            X_train, X_test = X.loc[train_outer_ix, :], X.loc[test_outer_ix, :]\n            y_train, y_test = y[train_outer_ix], y[test_outer_ix]\n\n            estimator = make_training_pipeline(\n                clone(model), num_attr, cat_attr)\n            cv_scores = cross_validate(estimator, X_train, y_train,\n                                       scoring=scoring, cv=cv_inner, n_jobs=-1)\n\n            inner_roc_scores = cv_scores['test_score']\n\n            estimator.fit(X_train, y_train)\n            y_test_pred = estimator.predict(X_test)\n            outer_roc_score = roc_auc_score(y_test, y_test_pred)\n\n            new_row = {'outer_roc_score': outer_roc_score,\n                       'inner_roc_score_mean': np.mean(inner_roc_scores),\n                       'inner_roc_score_std': np.std(inner_roc_scores)}\n            roc_scores = roc_scores.append(new_row, ignore_index=True)\n\n            pbar.update(progress_unit)\n\n    return roc_scores","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p style=\"text-align: justify; line-height: 2.0; font-size: 14px; padding-right: 100px;\">\n<b>roc_score</b> in the <i>outer_roc_score_stack</i> means the AUC of ROC calculated for each k1-outer-fold split of <i>X4_train, y4_train</i>.\nThe stacking classifier uses the k2-inner-fold split of splitted <i>X4_train, y4_train</i>. We use k1 = 5 and k2 = 4 to measure the\nnested cross-validated scores.\n</p>"},{"metadata":{"trusted":false},"cell_type":"code","source":"stack_cf = StackingClassifier(\n    optimized_models, stack_method='auto', n_jobs=-1, passthrough=False)\nouter_roc_score_stack = get_stack_cf_cv_scores(\n    stack_cf, X4_train, y4_train, num_attr4, cat_attr4, 5, 4)\nouter_roc_score_stack","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p style=\"text-align: justify; line-height: 2.0; font-size: 14px; padding-right: 100px;\">\n<b>outer_roc_score</b> in the <i>outer_roc_score_voting</i> means the AUC of ROC calculated for each k1-outer-fold split of <i>X4_train, y4_train</i>.\nWhile <b>inner_roc_score_mean</b> means the average of the AUC of ROC calculated for k2-inner-fold split of splitted <i>X4_train, y4_train</i>.\nWe use k1 = 5 and k2 = 4 to measure the nested cross-validated scores.\n</p>"},{"metadata":{"trusted":false},"cell_type":"code","source":"voting_cf = VotingClassifier(optimized_models, voting='soft', n_jobs=-1)\nouter_roc_score_voting = get_voting_cf_cv_scores(\n    voting_cf, X4_train, y4_train, num_attr4, cat_attr4, 5, 4)\nouter_roc_score_voting","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p style=\"text-align: justify; line-height: 2.0; font-size: 14px; padding-right: 100px;\">\nUnfortunately, voting and stacking classifiers don't yield any significant improvement :(\n</p>"},{"metadata":{"id":"ErXvDRJn7PIS"},"cell_type":"markdown","source":"# Conclusion"},{"metadata":{},"cell_type":"markdown","source":"<p style=\"text-align: justify; line-height: 2.0; text-indent: 5%; font-size: 14px; padding-right: 100px;\">\nWe are not going to test our models since the nested cross-validated performance score already tell a lot about our overall model performance.\nRegardless, I definitely going to do another improvement on the models in the coming future. However, we do achieve\nquite a lot in this part. We are able to prevent train-test contamination by only preprocess the train set instead of\nthe whole dataset. Besides, we are also able to perform feature selection without sacrificing too much on model performance.\nFurthermore, I also have apply nested cross-validation on many areas especially in hyperparameter tuning.\n</p>\n<p style=\"text-align: justify; line-height: 2.0; text-indent: 5%; font-size: 14px; padding-right: 100px;\">\nIn part III, we are going to understand why the models suck at doing their jobs. I suspect that outlier might have\nsomething to do with the poor performance as shown in the plot below. However, we cannot deny other possible factors like insufficient\nrelevant features or samples, which is something that I can't fulfill :)\n</p>\n<p style=\"text-align: justify; line-height: 2.0; font-size: 14px; padding-right: 100px;\">\nIf you guys have any feedbacks or suggestions please feel free to share,\n    I started to learn data science &#60; 6 months ago  and still have many more to learn :)\n</p>"},{"metadata":{"trusted":false},"cell_type":"code","source":"def create_subplots(cols, num_cols_per_row, fig_w, fig_h):\n    num_rows = ceil(len(cols) / num_cols_per_row)\n    indexes = list(product(range(num_rows), range(num_cols_per_row)))\n    fig, axs = plt.subplots(num_rows, num_cols_per_row)\n    fig.set_size_inches(fig_w, num_rows * fig_h)\n    return num_rows, indexes, axs\n\n\ndef plot_boxplot(df, cols, num_cols_per_row=4, fig_w=16, fig_h=7):\n    num_rows, indexes, axs = create_subplots(\n        cols, num_cols_per_row, fig_w, fig_h)\n\n    for idx, col in enumerate(cols):\n        ax = axs[idx] if num_rows == 1 else axs[indexes[idx][0]][indexes[idx][1]]\n        sns.boxplot(y=df[col], ax=ax)\n        ax.set(title=col, ylabel=None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plot_boxplot(dating4, num_attr4)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}