{"cells":[{"metadata":{},"cell_type":"markdown","source":"Data Mining e Machine Learning II\nInstituição: Centro Universitário IESB\n\nCurso: Pós Graduação Ciência de Dados - Campus BSB/Asa Sul\n\nDisciplina: Data Mining e Machine Learning II\n\nOrientador: Marcos Vinicius Guimarães\n\nAluna:LÍDIA MARA AGUIAR BEZERRA"},{"metadata":{},"cell_type":"markdown","source":"Esse banco \"HMEQ\" é composto de informações sobre o crédito do consumidor de um banco para verificar se a prova ou reprova empréstimos. Assim, o objetivo desse trabalho foi realizar uma análise exploratória dos dados, bem como, desenvolver modelos preditivos para se descobrir a chance de um cliente ser um inadimplente ou não."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import KFold\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import confusion_matrix\nfrom imblearn.over_sampling import SMOTE,ADASYN\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom sklearn.model_selection import StratifiedKFold\nfrom collections import Counter\nimport itertools\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"###Carregando a base e verificando a forma da base. \ndf = pd.read_csv ('/kaggle/input/hmeq-data/hmeq.csv')\n##Verificando as dimensões da base\ndf.shape\n###Observa-se que a base possui 5960 observações e 13 variáveis.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"###Verificando os nomes das colunas\ndf.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"###Verificando as variáveis na forma transposta\ndf.head().T\n###É possivel observar as 13 variáveis e as 4 primeiras observações. \n##E já se observa a presença de NAN","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"###Verificando de forma aleatória para verificar outros detalhes\ndf.sample(5).T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"###Coletando informações dos dados\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# columns of dataset\ndf.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"####Nesses códigos abaixo é possível observar os valores de alguns casos \ndef rstr(df): return df.shape, df.apply(lambda x: [x.unique()])\nprint(rstr(df))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"###VERIFICANDO SE AS VARIÁVEIS DUMMIES FORAM CRIADAS\ndf.shape\n# Observa-se um aumento no múmero de variáveis de 13 para 19.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##Agora verifiacando as variáveis dummies na base\ndf.sample(5).T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"####AGORA, VAMOS VERIFICAR A ESTATÍSTICA DESCRITIVA DAS VARIÁVEIS \nprint(df.describe().T)\n## vale ressaltar que as variáveis qualitativas estão por padrão nessa análise.\n#então a média e desvios-padrão não são adequados.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"###############talvez retirar\n###Verificando a estatística descritiva das variáveis qualitativas\n# categorical features\ncategorical_cols = [cname for cname in df.columns if\n                    df[cname].dtype in ['object']]\ncat = df[categorical_cols]\ncat.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##Verificando se existem MISSING VALUES. Foi também calculado o percentual de missing cases\n\nfeat_missing = []\n\nfor f in df.columns:\n    missings = df[f].isnull().sum()\n    if missings > 0:\n        feat_missing.append(f)\n        missings_perc = missings/df.shape[0]\n        \n###Verificando o percentual de missing\n        print('Variable {} has {} records ({:.2%}) with missing values'.format(f, missings, missings_perc))\n    \n###Verificando quantas variáveis apresentam os casos faltosos\n\nprint()\nprint('In total, there are {} variables with missing values'.format(len(feat_missing)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#dropping rows that have missing data\ndf.dropna(axis=0, how='any', inplace=True)\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##Verificando se existem MISSING VALUES. Foi também calculado o percentual de missing cases\n###Confirmando se restou valor faltoso\nfeat_missing = []\n\nfor f in df.columns:\n    missings = df[f].isnull().sum()\n    if missings > 0:\n        feat_missing.append(f)\n        missings_perc = missings/df.shape[0]\n        \n###Verificando o percentual de missing\n        print('Variable {} has {} records ({:.2%}) with missing values'.format(f, missings, missings_perc))\n    \n###Verificando quantas variáveis apresentam os casos faltosos\n\nprint()\nprint('In total, there are {} variables with missing values'.format(len(feat_missing)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"###Criando uma nova coluna para iniciar a previsão de forma mais acurada\ndf['VALUE_MORTDUE'] = df['VALUE'] - df['MORTDUE']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"###Agora verificando novamente a estatística descritiva das variáveis quantitativas\nprint(df.describe().T)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"######DEFININDO A \"TARGET\" \"BAD\"\ny=df.BAD","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"####\nimport matplotlib.pyplot as plt\nax = sns.countplot(y='BAD', data=df).set_title(\"Clientes inadimplentes ou não\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"####Verificando a estatística das variáveis qualitativas\ny = y.astype(object)\ncount = pd.crosstab(index = y, columns=\"count\")\npercentage = pd.crosstab(index = y, columns=\"frequency\")/pd.crosstab(index = y, columns=\"frequency\").sum()\npd.concat([count, percentage], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"###Verificando as outras variáveis categóricas\ncategorical_cols = [cname for cname in df.columns if\n                    df[cname].dtype in ['object']]\ncat = df[categorical_cols]\ncat.columns\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"###Iportando bibliotecas para o modelo\nimport pandas\nfrom pandas.plotting import scatter_matrix\nfrom matplotlib import pyplot","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# GRáficos de contagem de cada categoria\nsns.set( rc = {'figure.figsize': (5, 5)})\nfcat = ['REASON','JOB']\n\nfor col in fcat:\n    plt.figure()\n    sns.countplot(x=cat[col], data=cat, palette=\"Set3\")\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"###Criando variáveis dummies\ndf_dum = pd.get_dummies(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"###Verificando as novas variáveis dummies na base transposta\ndf_dum.sample(5).T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"###Verificando as características das variáveis numéricas\nnumerical_cols = [cname for cname in df.columns if\n                 df[cname].dtype in ['float']]\nnum = df[numerical_cols]\nnum.columns\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"###Analisando os histogramas das variáveis quantitativas\n##Os histogramas abaixo demonstram distribuições assimétricas.\nf, axes = plt.subplots(3,3, figsize=(20,20))\nsns.distplot( df_dum[\"LOAN\"] , color=\"skyblue\", bins=15, kde=False, ax=axes[0, 0])\nsns.distplot( df_dum[\"DEBTINC\"] , color=\"olive\", bins=15, kde=False, ax=axes[0, 1])\nsns.distplot( df_dum[\"MORTDUE\"] , color=\"orange\", bins=15, kde=False, ax=axes[0, 2])\nsns.distplot( df_dum[\"YOJ\"] , color=\"yellow\", bins=15, kde=False, ax=axes[1, 0])\nsns.distplot( df_dum[\"VALUE\"] , color=\"pink\", bins=15, kde=False, ax=axes[1, 1])\nsns.distplot( df_dum[\"CLAGE\"] , color=\"gold\", bins=15, kde=False, ax=axes[1, 2])\nsns.distplot( df_dum[\"CLNO\"] , color=\"teal\", bins=15, kde=False, ax=axes[2, 1])\nsns.distplot( df_dum['DEROG'], color=\"blue\", bins=15, kde=False, ax=axes[2, 2])\nsns.distplot( df_dum['DELINQ'], color=\"green\", bins=15, kde=False, ax=axes[2, 0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#VALIDAÇÃO CRUZADA ESTRATIFICADA + REAMOSTRAGEM\n###Uma abordagem para lidar com conjuntos de dados desequilibrados é sobreamostrar a classe minoritária.\n###A abordagem mais simples envolve a duplicação de exemplos na classe minoritária, embora esses exemplos não adicionem novas informações ao modelo. \n###Em vez disso, novos exemplos podem ser sintetizados a partir dos exemplos existentes.\n###Esse é um tipo de aumento de dados para a classe minoritária e é chamado de Técnica de superamostragem por minoria sintética, ou SMOTE, para abreviar\n##Synthetic Minority Oversampling Technique = SMOT\ny = y.astype('int') \nsmo = SMOTE(random_state=0)\nX_resampled, y_resampled = smo.fit_resample(df_dum, y)\nprint(sorted(Counter(y_resampled).items()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dividindo o DataFrame\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##### Treino e teste\ntrain, test = train_test_split(df_dum, test_size=0.15, random_state=42)\n\n# Veificando o tanho dos DataFrames\ntrain.shape, test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#######\ndf_dum.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#####features\nfeats = [c for c in df_dum.columns if c not in ['BAD']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_dum.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"###Cross validation\nfrom sklearn.model_selection import cross_val_score\n\nscores = cross_val_score(rf, train[feats], train['BAD'], n_jobs=-1, cv=5)\n\nscores, scores.mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"####Dividindo o dataset\n\nX_train, X_valid, y_train, y_valid = train_test_split(X_resampled, y_resampled, train_size=0.8, test_size=0.2,\n                                                                random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#####features\nfeats = [c for c in df_dum.columns if c not in ['BAD']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"GERANDO A MATRIZ DE CONFUSÃO\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"MODELOS DA LINHA DE BASE"},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"###Gerando modelos da linha de base\n##oPÇÕES DE TESTE E MÉTRICA DA AVALIAÇÃO\n##Algoritmos de verificação\n\nmodels = []\nmodels.append(('LogisticRegression', LogisticRegression(random_state=0)))\nmodels.append(('Bagging', BaggingClassifier(random_state=0)))\nmodels.append(('RandomForest', RandomForestClassifier(random_state=0)))\nmodels.append(('AdaBoost', AdaBoostClassifier(random_state=0)))\nmodels.append(('GBM', GradientBoostingClassifier(random_state=0)))\nmodels.append(('XGB', XGBClassifier(random_state=0)))\nresults_t = []\nresults_v = []\nnames = []\nscore = []\nskf = StratifiedKFold(n_splits=5)\nfor (name, model) in models:\n    param_grid = {}\n    my_model = GridSearchCV(model,param_grid,cv=skf)\n    my_model.fit(X_train, y_train)\n    predictions_t = my_model.predict(X_train) \n    predictions_v = my_model.predict(X_valid)\n    accuracy_train = accuracy_score(y_train, predictions_t) \n    accuracy_valid = accuracy_score(y_valid, predictions_v) \n    results_t.append(accuracy_train)\n    results_v.append(accuracy_valid)\n    names.append(name)\n    f_dict = {\n        'model': name,\n        'accuracy_train': accuracy_train,\n        'accuracy_valid': accuracy_valid,\n    }\n   #computando a matrix de confusão para o algoritmo acima\n    cnf_matrix = confusion_matrix(y_valid, predictions_v)\n    np.set_printoptions(precision=2)\n\n#Gráfico da matriz de confusão não-normalizado\n    plt.figure()\n    plot_confusion_matrix(cnf_matrix, classes=[\"BAD\"], title=\"Confusion Matrix - \"+str(name))\n    score.append(f_dict)\nplt.show()    \nscore = pd.DataFrame(score, columns = ['model','accuracy_train', 'accuracy_valid'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"###Gerando as pontuações\nprint(score)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> MODELOS DE BASES ESCALONADAS"},{"metadata":{"trusted":true},"cell_type":"code","source":"###Modelos de base escalonadas\n# Algoritmos de verificação pontual com conjunto de dados padronizado\npipelines = []\npipelines.append(('Scaled_LogisticRegression', Pipeline([('Scaler', StandardScaler()),('LogisticRegression', LogisticRegression(random_state=0))])))\npipelines.append(('Scaled_Bagging', Pipeline([('Scaler', StandardScaler()),('Bagging', BaggingClassifier(random_state=0))])))\npipelines.append(('Scaled_RandomForest', Pipeline([('Scaler', StandardScaler()),('RandomForest', RandomForestClassifier(random_state=0))])))\npipelines.append(('Scaled_AdaBoost', Pipeline([('Scaler', StandardScaler()),('AdaBoost', AdaBoostClassifier(random_state=0))])))\npipelines.append(('Scaled_GBM', Pipeline([('Scaler', StandardScaler()),('GBM', GradientBoostingClassifier(random_state=0))])))\npipelines.append(('Scaled_XGB', Pipeline([('Scaler', StandardScaler()),('XGB', XGBClassifier(random_state=0))])))\npipelines.append(('Scaled_NeuralNetwork', Pipeline([('Scaler', StandardScaler()),('NeuralNetwork', MLPClassifier(random_state=0))])))\nresults_t = []\nresults_v = []\nnames = []\nscore_sd = []\nskf = StratifiedKFold(n_splits=5)\nfor (name, model) in pipelines:\n    param_grid = {}\n    my_model = GridSearchCV(model,param_grid,cv=skf)\n    my_model.fit(X_train, y_train)\n    predictions_t = my_model.predict(X_train) \n    predictions_v = my_model.predict(X_valid)\n    accuracy_train = accuracy_score(y_train, predictions_t) \n    accuracy_valid = accuracy_score(y_valid, predictions_v) \n    results_t.append(accuracy_train)\n    results_v.append(accuracy_valid)\n    names.append(name)\n    f_dict = {\n        'model': name,\n        'accuracy_train': accuracy_train,\n        'accuracy_valid': accuracy_valid,\n    }\n    # Computing Confusion matrix for the above algorithm\n    cnf_matrix = confusion_matrix(y_valid, predictions_v)\n    np.set_printoptions(precision=2)\n\n    # Plot non-normalized confusion matrix\n    plt.figure()\n    plot_confusion_matrix(cnf_matrix, classes=[\"BAD\"], title=\"Confusion Matrix - \"+str(name))\n    score_sd.append(f_dict)\nplt.show()   \nscore_sd = pd.DataFrame(score_sd, columns = ['model','accuracy_train', 'accuracy_valid'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"###Verificando os scores dos modelos\nprint(score_sd)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**####Aplicando do método ADASYN\n##ADASYN: Método de amostragem sintética adaptável para dados desequilibrados\n## Trata-se de uma técnica de sobreamostragem para classes minoritárias\n#que é usado principalmente para resolver o problema de classificação desequilibrada nos casos de uso do Machine Learning\n#Tem como objetivo ajustar um modelo que aprende um limite de decisão muito complexo, \n#resultando em uma classificação bem-sucedida das classes, mas o resultado é um modelo que sofre de alta variância \n#e baixa condição de viés, no caso OVERFITING***"},{"metadata":{"trusted":true},"cell_type":"code","source":"####Aplicando do método ADASYN\ny = y.astype('int') \nada = ADASYN(random_state=0)\nX_resampled_, y_resampled_ = ada.fit_resample(df_dum, y)\nprint(sorted(Counter(y_resampled_).items()))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"DIVIDINDO O DATA SET\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Separando o treino e validação dos dados de treino\n\nX_train_, X_valid_, y_train_, y_valid_ = train_test_split(X_resampled_, y_resampled_, train_size=0.8, test_size=0.2,\n                                                                random_state=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"MODELOS DE LINHA DE BASE"},{"metadata":{"trusted":true},"cell_type":"code","source":"###MOdelos de linha de base\n# Opções de teste e métrica da avaliação\n#Algoritmos de verificação pontual\nmodels = []\nmodels.append(('LogisticRegression', LogisticRegression(random_state=0)))\nmodels.append(('Bagging', BaggingClassifier(random_state=0)))\nmodels.append(('RandomForest', RandomForestClassifier(random_state=0)))\nmodels.append(('AdaBoost', AdaBoostClassifier(random_state=0)))\nmodels.append(('GBM', GradientBoostingClassifier(random_state=0)))\nmodels.append(('XGB', XGBClassifier(random_state=0)))\nresults_t = []\nresults_v = []\nnames = []\nscore = []\nskf = StratifiedKFold(n_splits=5)\nfor (name, model) in models:\n    param_grid = {}\n    my_model = GridSearchCV(model,param_grid,cv=skf)\n    my_model.fit(X_train_, y_train_)\n    predictions_t = my_model.predict(X_train_) \n    predictions_v = my_model.predict(X_valid_)\n    accuracy_train = accuracy_score(y_train_, predictions_t) \n    accuracy_valid = accuracy_score(y_valid_, predictions_v) \n    results_t.append(accuracy_train)\n    results_v.append(accuracy_valid)\n    names.append(name)\n    f_dict = {\n        'model': name,\n        'accuracy_train': accuracy_train,\n        'accuracy_valid': accuracy_valid,\n    }\n    #Matriz de confusão do algoritmo acima \n    cnf_matrix = confusion_matrix(y_valid_, predictions_v)\n    np.set_printoptions(precision=2)\n\n  #Gráfico não normalizado da matriz \n    plt.figure()\n    plot_confusion_matrix(cnf_matrix, classes=[\"BAD\"], title=\"Confusion Matrix - \"+str(name))\n    score.append(f_dict)\nplt.show()    \nscore = pd.DataFrame(score, columns = ['model','accuracy_train', 'accuracy_valid'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##Verificando os scores\nprint(score)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"MODELOS DE BASE ESCALONADOS"},{"metadata":{"trusted":true},"cell_type":"code","source":"####MODELOS DE LINHA DE BASE ESCALONADOS\n# Algoritmos de verificação pontual com conjunto de dados padronizado\npipelines = []\npipelines.append(('Scaled_LogisticRegression', Pipeline([('Scaler', StandardScaler()),('LogisticRegression', LogisticRegression(random_state=0))])))\npipelines.append(('Scaled_Bagging', Pipeline([('Scaler', StandardScaler()),('Bagging', BaggingClassifier(random_state=0))])))\npipelines.append(('Scaled_RandomForest', Pipeline([('Scaler', StandardScaler()),('RandomForest', RandomForestClassifier(random_state=0))])))\npipelines.append(('Scaled_AdaBoost', Pipeline([('Scaler', StandardScaler()),('AdaBoost', AdaBoostClassifier(random_state=0))])))\npipelines.append(('Scaled_GBM', Pipeline([('Scaler', StandardScaler()),('GBM', GradientBoostingClassifier(random_state=0))])))\npipelines.append(('Scaled_XGB', Pipeline([('Scaler', StandardScaler()),('XGB', XGBClassifier(random_state=0))])))\npipelines.append(('Scaled_NeuralNetwork', Pipeline([('Scaler', StandardScaler()),('NeuralNetwork', MLPClassifier(random_state=0))])))\nresults_t = []\nresults_v = []\nnames = []\nscore_sd = []\nskf = StratifiedKFold(n_splits=5)\nfor (name, model) in pipelines:\n    param_grid = {}\n    my_model = GridSearchCV(model,param_grid,cv=skf)\n    my_model.fit(X_train_, y_train_)\n    predictions_t = my_model.predict(X_train_) \n    predictions_v = my_model.predict(X_valid_)\n    accuracy_train = accuracy_score(y_train_, predictions_t) \n    accuracy_valid = accuracy_score(y_valid_, predictions_v) \n    results_t.append(accuracy_train)\n    results_v.append(accuracy_valid)\n    names.append(name)\n    f_dict = {\n        'model': name,\n        'accuracy_train': accuracy_train,\n        'accuracy_valid': accuracy_valid,\n    }\n    #Matriz de confusão do algoritmo acima \n    cnf_matrix = confusion_matrix(y_valid_, predictions_v)\n    np.set_printoptions(precision=2)\n\n    #Gráfico não normalizado da matriz\n    plt.figure()\n    plot_confusion_matrix(cnf_matrix, classes=[\"BAD\"], title=\"Confusion Matrix - \"+str(name))\n    score_sd.append(f_dict)\nplt.show()   \nscore_sd = pd.DataFrame(score_sd, columns = ['model','accuracy_train', 'accuracy_valid'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Verificando os scores\nprint(score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"PARTINDO PARA ANÁLISE DE CLUSTER\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importando o k-means\n# Determinando a quantidade de clusters\n\n# Importando o k-means\nfrom sklearn.cluster import KMeans\n\n# Selecionando as variaveis para utilizar no modelo.\nX= df_dum[['MORTDUE','LOAN', 'YOJ']]\n\n# Cálculo do SSE - Sum of Squared Erros\nsse = []\n\nfor k in range (1, 12):\n    kmeans = KMeans(n_clusters=k, random_state=42).fit(X)\n    sse.append(kmeans.inertia_)\nprint(sse)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"###Agora verificando a sugestão de quantos cluster deve ser formados \nimport matplotlib.pyplot as plt\n\nplt.plot(range(1, 12), sse, 'bx-')\nplt.title('Elbow Method')\nplt.xlabel('Quantidade de Clusters')\nplt.ylabel('SSE')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Sobre o gráfico de cotovelo, é sugerido formar 4 grupos (clusters)"},{"metadata":{"trusted":true},"cell_type":"code","source":"###Formando os cluster\nkmeans = KMeans(n_clusters=4, init='k-means++', random_state=42)\ncluster_id = kmeans.fit_predict(X)\ncluster_id","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##Salvando os resultados do dataset e  conferindo o tamanho\n\nX['cluster_id'] = cluster_id\n\n\nX.sample(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##Gráfico dos cluster e os centróides\nfig = plt.figure(figsize=(14,10))\n\nplt.scatter(X.values[:,0], X.values[:,1], c=kmeans.labels_)\nplt.scatter(kmeans.cluster_centers_[:,0] ,kmeans.cluster_centers_[:,1], color='blue', marker=\"x\", s=200)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Após a construção do gráfico de cluster, observa-se os centróides marcados de cor azul. A distância entre esses centróides pode ser calculada pelo método de Euclides para gerar uma média."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{},"cell_type":"raw","source":""},{"metadata":{},"cell_type":"markdown","source":""}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}