{"cells":[{"metadata":{},"cell_type":"markdown","source":"# SMS Spam Detection using Naive Bayes classifier along with sophisticated features (0.99% accuracy achieved)"},{"metadata":{},"cell_type":"markdown","source":"Goal of this notebook is to test Multinomial Naive-Bayes classifier in combination with a plethora of sophisticated features and see how it perform on the given dataset"},{"metadata":{},"cell_type":"markdown","source":"## Import Libraries\n"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# Ignoring unnecessory warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")  \n# Specialized container datatypes\nimport collections\n# For data vizualization \nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n# For large and multi-dimensional arrays\nimport numpy as np\n# For data manipulation and analysis\nimport pandas as pd\n# Natural language processing library\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer \nfrom nltk.stem import SnowballStemmer\nfrom nltk.tokenize import word_tokenize\nfrom nltk.util import ngrams\n# For basic cleaning and data preprocessing \nimport re\nimport string \n# Machine learning libary\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report,confusion_matrix\nfrom sklearn.metrics import precision_recall_fscore_support as score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\n# For wordcloud generating \nfrom wordcloud import WordCloud","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Exploratory Data Analysis (EDA)\n\n"},{"metadata":{},"cell_type":"markdown","source":"Read the data using pandas' read_csv method and let's look at the dataset info to see if everything is alright"},{"metadata":{"trusted":true},"cell_type":"code","source":"DATAPATH = '../input/sms-spam-collection-dataset/spam.csv'\ndf = pd.read_csv(DATAPATH, encoding='latin')\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Shape of our data\ndf.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Have a look on the first 5 rows of the dataset\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now drop \"unnamed\" columns and rename v1 and v2 to \"label\" and \"message\" respectively.  Also drop any row which has any NaN value"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop(['Unnamed: 2','Unnamed: 3','Unnamed: 4'], axis=1, inplace=True)\ndf.rename(columns = {'v1':'label','v2':'message'}, inplace=True)\ndf.dropna(inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see the first 5 rows of the dataset again"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Target count for data\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['label'].value_counts().plot(kind = 'barh', color = ['blue','red'], figsize = (8, 6))\nplt.title('Horizontal Bar Chart for Data Distribution', fontsize = 20)\nplt.ylabel('Spam vs Ham')\nplt.xlabel('Number of messages')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Vizualize data distribution using pie chart\n"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"df['label'].value_counts().plot(kind = 'pie', colors = ['blue','red'], explode = [0, 0.1], figsize = (6, 6), autopct = '%1.1f%%', shadow = True)\nplt.title('Pie Chart for Data Distribution', fontsize = 20)\nplt.ylabel('Spam vs Ham')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It's crystal clear that 'ham' sms are 6.5 times more than the 'spam' ones. \n- We have to be very careful when we gonna split our data set into train and test set or when we gonna use cross-validation. Otherwise we have a chance of our training model being skewed towards normal messages. That's will happen if the sample we choose to train our model consists majorly of 'ham' sms. In this case, it's very propable to end up predicting everything as 'ham'.\n- Also precision is very important as we don't want to predict any 'ham' sms as 'spam'. Actually, we don't mind if we miss any odd 'spam' sms."},{"metadata":{},"cell_type":"markdown","source":"Let's have a look into the data grouped by into labels 'ham' or 'spam'\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.groupby('label').describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see how a ham and a spam sms looks like"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Pick at random a ham sms \ndf.loc[df['label'] == 'ham'].sample()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Pick at random a spam sms \ndf.loc[df['label'] == 'spam'].sample()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let's see 30 most frequent occuring words "},{"metadata":{"trusted":true},"cell_type":"code","source":"cv = CountVectorizer()\nwords = cv.fit_transform(df.message)\n\nsum_words = words.sum(axis=0)\n\nwords_freq = [(word, sum_words[0, i]) for word, i in cv.vocabulary_.items()]\nwords_freq = sorted(words_freq, key = lambda x: x[1], reverse = True)\n\nfrequency = pd.DataFrame(words_freq, columns=['word', 'freq'])\n\nfrequency.head(30).plot(x='word', y='freq', kind='bar', figsize=(15, 7), color = 'orange')\nplt.title(\"Most Frequently Occuring Words - Top 30\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It's time to generate wordclouds for both 'spam' and 'ham' sms to have a rough estimate of the words that has the highest frequency in the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# First define a function to find all words (excluding numbers and stopwords) of each category\ndef getWords(label):\n    temp_words = ' '.join(list(df.loc[df['label'] == label]['message'])) \n    lst_words = []\n    words = [word.lower() for word in word_tokenize(temp_words) \n             if word.lower() not in stopwords.words(\"english\") and word.lower().isalpha()]\n    lst_words = lst_words + words\n    return lst_words","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get both spam and ham words\nspam_words = getWords('spam')\nham_words = getWords('ham')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def generate_wordcloud(words):\n    # exclude stop words \n    wordcloud = WordCloud(max_words=1000,width=840, height=540).generate(words)\n    plt.imshow(wordcloud)\n    plt.axis('off')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# generate wordcloud for 'spam' sms\ngenerate_wordcloud(' '.join(spam_words))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# generate wordcloud for 'ham' sms\ngenerate_wordcloud(' '.join(ham_words))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we see words like 'free', 'text', 'call', 'reply', 'mobile' and 'now' appear very often in 'spam' sms. Respectively the most frequent words in 'ham' sms are 'ok', 'will', 'now', 'got', 'gt', 'lt' and etc"},{"metadata":{},"cell_type":"markdown","source":"Let's get more precise and find out what are the 20 most frequent words in each category"},{"metadata":{"trusted":true},"cell_type":"code","source":"# 20 most frequent spam words\nCounter = collections.Counter(spam_words)\nmost_occur_spam_words = Counter.most_common(20)\ndf_most_occur_spam_words = pd.DataFrame(most_occur_spam_words, columns=['word','frequency'])\ndf_most_occur_spam_words","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_most_occur_spam_words.plot(x='word', y='frequency', kind='bar', figsize=(15, 7), color = 'red')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 20 most frequent ham words\nCounter = collections.Counter(ham_words)\nmost_occur_ham_words = Counter.most_common(20)\ndf_most_occur_ham_words = pd.DataFrame(most_occur_ham_words, columns=['word','frequency'])\ndf_most_occur_ham_words","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_most_occur_ham_words.plot(x='word', y='frequency', kind='bar', figsize=(15, 7), color = 'blue')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In many cases bigrams is also an important tool in data analysis. So let's see if there some words which occur more frequently than the other ones for each category"},{"metadata":{"trusted":true},"cell_type":"code","source":"# top 20 bigrams in spam_words\nspam_bigrams = ngrams(spam_words, 2)\nspam_bigrams_freq = collections.Counter(spam_bigrams)\nmost_freq_spam_bigrams = spam_bigrams_freq.most_common(20)\ndf_most_freq_spam_bigrams = pd.DataFrame(most_freq_spam_bigrams, columns=['bigram','frequency'])\ndf_most_freq_spam_bigrams","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_most_freq_spam_bigrams.plot(x='bigram', y='frequency', kind='bar', figsize=(15, 7), color = 'red')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# top 20 bigrams in ham_words\nham_bigrams = ngrams(ham_words, 2)\nham_bigrams_freq = collections.Counter(ham_bigrams)\nmost_freq_ham_bigrams = ham_bigrams_freq.most_common(20)\ndf_most_freq_ham_bigrams = pd.DataFrame(most_freq_ham_bigrams, columns=['bigram','frequency'])\ndf_most_freq_ham_bigrams","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_most_freq_ham_bigrams.plot(x='bigram', y='frequency', kind='bar', figsize=(15, 7), color = 'blue')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Preprocessing "},{"metadata":{},"cell_type":"markdown","source":"Now define our text precessing function. It will remove any punctuation and stopwords. Also it will convert all letters to lowercase and perform stemming aswell."},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess_text(text):\n    # remove all punctuation\n    text = re.sub(r'[^\\w\\d\\s]', ' ', text)\n    # collapse all white spaces\n    text = re.sub(r'\\s+', ' ', text)\n    # convert to lower case\n    text = re.sub(r'^\\s+|\\s+?$', '', text.lower())\n    # remove stop words and perform stemming\n    stop_words = nltk.corpus.stopwords.words('english')\n    lemmatizer = WordNetLemmatizer() \n    return ' '.join(\n        lemmatizer.lemmatize(term) \n        for term in text.split()\n        if term not in set(stop_words)\n    )\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['processed_text'] = df.message.apply(lambda row : preprocess_text(row))\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Engineering"},{"metadata":{},"cell_type":"markdown","source":"As we have seen from the EDA step, both 1-gram and 2-gram  play a significant role when it comes to classify a message as spam or ham. So our role is to transform these 1-gram and 2-gram into a feature vector. To do that, we gonna use TfidfVectorizer"},{"metadata":{"trusted":true},"cell_type":"code","source":"tfidf_vec = TfidfVectorizer(ngram_range=(1, 2)).fit_transform(df.processed_text)\ntfidf_data = pd.DataFrame(tfidf_vec.toarray())\ntfidf_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have now a data with 38084 columns where each column represent a different 1/2-gram contained in the current message"},{"metadata":{},"cell_type":"markdown","source":"#### Message length"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['message_length'] = df.message.apply(lambda row : len(row))\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.violinplot(df['message_length'], df['label'])\nplt.title('Distribution of message length')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.hist(column='message_length', by='label', bins=50, figsize=(12,4))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Punctuation "},{"metadata":{"trusted":true},"cell_type":"code","source":"def count_punct(text):\n    count = sum([1 for char in text if char in string.punctuation])\n    return round(count/(len(text)), 3)*100","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"df['punct%'] = df.message.apply(lambda row : count_punct(row))\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.violinplot(df['punct%'], df['label'])\nplt.title('Distribution of punctuation')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.hist(column='punct%', by='label', bins=50, figsize=(12,4))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Http address"},{"metadata":{"trusted":true},"cell_type":"code","source":"def count_https(text):\n    http_regex = re.compile(r'(http[s]?\\S+)|(\\w+\\.[A-Za-z]{2,4}\\S*)')\n    count = sum([1 for word in text.split() if http_regex.match(word)])\n    return round(count/(len(text)), 3)*100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['httpaddr%'] = df.message.apply(lambda row : count_https(row))\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.violinplot(df['httpaddr%'], df['label'])\nplt.title('Distribution of https')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.hist(column='httpaddr%', by='label', bins=50, figsize=(12,4))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Money symbol"},{"metadata":{"trusted":true},"cell_type":"code","source":"def count_money_symb(text):\n    money_symb = ['£','$','€']\n    count = sum([1 for char in text if char in money_symb])\n    return round(count/(len(text)), 3)*100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['money_symb%'] = df.message.apply(lambda row : count_money_symb(row))\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.violinplot(df['money_symb%'], df['label'])\nplt.title('Distribution of money symbols')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.hist(column='money_symb%', by='label', bins=50, figsize=(12,4))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Phone number"},{"metadata":{"trusted":true},"cell_type":"code","source":"def count_phone_numbers(text):\n    phone_regex = re.compile(r'\\b(\\+\\d{1,2}\\s)?\\d?[\\-(.]?\\d{3}\\)?[\\s.-]?\\d{3}[\\s.-]?\\d{4}\\b')\n    count = sum([1 for word in text.split() if phone_regex.match(word)])\n    return round(count/(len(text)), 3)*100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['phone_numb%'] = df.message.apply(lambda row : count_phone_numbers(row))\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.violinplot(df['phone_numb%'], df['label'])\nplt.title('Distribution of phone numbers')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.hist(column='phone_numb%', by='label', bins=50, figsize=(12,4))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Numbers "},{"metadata":{"trusted":true},"cell_type":"code","source":"def count_numbers(text):\n    count = sum([1 for word in text.split() if word.isdigit()])\n    return round(count/(len(text)), 3)*100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['numbers%'] = df.message.apply(lambda row : count_numbers(row))\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.violinplot(df['numbers%'], df['label'])\nplt.title('Distribution of numbers')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.hist(column='numbers%', by='label', bins=50, figsize=(12,4))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we see, only phone number and money symbol are trustworthy indicators of a spam sms. So our final data will contain money_symb%, phone_numb% and tfidf_data"},{"metadata":{"trusted":true},"cell_type":"code","source":"final_data = pd.concat([df['money_symb%'], df['phone_numb%'], tfidf_data], axis=1)\nfinal_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training and evaluating the model"},{"metadata":{},"cell_type":"markdown","source":"Split data set into train and test set "},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(final_data, df['label'], test_size=.2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Fit classifier and make predictions on the test set"},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = MultinomialNB(alpha=0.2)\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)\nprint(classification_report(y_test,y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Accuracy score: {}\".format(round(accuracy_score(y_test,y_pred),3)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's use a confusion matrix to take a peek at what types of mistakes the classifier is making."},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(\n    confusion_matrix(y_test, y_pred),\n    index=[['actual', 'actual'], ['spam', 'ham']],\n    columns=[['predicted', 'predicted'], ['spam', 'ham']]\n)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":1}