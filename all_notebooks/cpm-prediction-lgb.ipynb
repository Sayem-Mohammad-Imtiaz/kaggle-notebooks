{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom itertools import combinations\nimport lightgbm as lgb\nfrom sklearn.metrics import mean_squared_error\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"d_full = df = pd.read_csv(\n    '/kaggle/input/real-time-advertisers-auction/Dataset.csv',\n    parse_dates=[\"date\"]\n)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1. Common info"},{"metadata":{"trusted":true},"cell_type":"code","source":"d_full.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d_full.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d_full.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feats_const = []\nfeats_empty = []\nfeats_rare = []\nthr = int(d_full.shape[0]*.05)\nfor col in d_full:\n    v_counts = dict(d_full[col].value_counts())\n    v_min = min(v_counts.values())\n    v_max = max(v_counts.values())\n    if len(v_counts)==0:\n        feats_empty.append(col)\n    elif len(v_counts)==1:\n        feats_const.append(col)\n    if v_max<thr:\n        feats_rare.append(col)\n    print(f\"{col}: {len(v_counts)}\")\n#     elif len(v_counts)<=20:\n#         feats_cat.append(col)\n#         print(f\"{col}: {v_counts}\")\nprint(f\"rare={feats_rare}\")\nprint(f\"empty={feats_empty}\")\nprint(f\"const={feats_const}\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d_full.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Data preparation"},{"metadata":{"trusted":true},"cell_type":"code","source":"# target computation\ntarget = \"CPM\"\n\n#calculating CPM\n#calculating the value that the Advertisers Bid for the month of June\n# CPM(the value which was the winning bid value) = \n#((revenue of the publisher*100)/revenue_share_percentage)/measurable_impressions)*1000\n\ndef weird_division(n, d):\n    return n / d if d else 0\n\nd_full['CPM'] = d_full.apply(\n    lambda x: weird_division(((x['total_revenue']*100)),x['measurable_impressions'])*1000 , \n    axis=1\n)\n\nd_full = d_full[d_full.CPM >= 0].reset_index(drop=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d_full[\"wday\"] = d_full.date.dt.weekday","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feats_cat = [\n    'ad_type_id',\n    'ad_unit_id',\n    'advertiser_id',\n    'device_category_id',\n    'geo_id',\n    'integration_type_id',\n    'line_item_type_id',\n    'os_id',\n    'site_id',\n    \"wday\"\n]\nfeats_num = [\n    'measurable_impressions', \n    'total_impressions', \n    'viewable_impressions',            \n]\n\n\nfor arg1, arg2 in combinations(feats_num, 2):\n    col_trg = f\"{arg1}/{arg2}\"\n    d_full[col_trg] = d_train[arg1]/d_train[arg2]\n    feats_num.append(col_trg)\n    \n    col_trg = f\"{arg1}*{arg2}\"\n    d_full[col_trg] = d_train[arg1]*d_train[arg2]\n    feats_num.append(col_trg)\n\nfor num, den in combinations(feats_cat, 2):\n    col_trg = f\"{num}*{den}\"\n    d_full[col_trg] = d_train[num]*d_train[den]\n    feats_cat.append(col_trg)\n\n\nfeats = feats_cat + feats_num","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d_train = d_full[d_full.date < '2019-06-22'].reset_index(drop=True)\nd_test = d_full[d_full.date >= '2019-06-22'].reset_index(drop=True)\n\nd_train = d_train[d_train.CPM<d_train.CPM.quantile(.95)].reset_index(drop=True)\nd_test = d_test[d_test.CPM<d_test.CPM.quantile(.95)].reset_index(drop=True)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"prm_lgb = {\n    'n_estimators': 300, \n    'learning_rate': 0.07, \n    'num_leaves': 60,\n    \n    'reg_alpha': 0.5,\n    'reg_lambda': 0.5, \n\n    'objective': 'tweedie', \n    'tweedie_variance_power': 1.25,\n}\n\nest = lgb.LGBMRegressor(**prm_lgb)\n\nest.fit(\n    d_train[feats], d_train[target], \n    eval_metric=['mse'], \n    categorical_feature=feats_cat\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_squared_error(\n    y_true=d_test[target],\n    y_pred=est.predict(d_test[feats]), \n)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}