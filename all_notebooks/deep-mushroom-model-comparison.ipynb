{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"602c63c3-ecf8-9ada-eab4-4c50a6451974"},"source":"# Deep mushroom: model comparison\n\nAuthor: Luis Bronchal<br>Date: April 27, 2017\n\n\n## Summary\n\nThere are several \"classic\" models which fit well with this dataset and achieve a great accuracy. We have compared some of them (*Logistic Regression, KNN, Trees, Naive Bayes, SVM and Random Forest*)\n\nWe have also implemented a neural network with Keras and experimented obtaining the values of the hidden layer for each input. We have used t-SNE to project this data into a two dimension plot to evaluate the ability of the neural network to transform raw input data into useful features."},{"cell_type":"markdown","metadata":{"_cell_guid":"0920f009-2e0b-014e-b872-f7ac90465a82"},"source":"##Â Analysis"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c6ab3261-af03-285e-7e4c-b62f7f78f378"},"outputs":[],"source":"%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nimport math"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"39b6c1c8-7444-02a5-b63c-0f0ca8dfe0a4"},"outputs":[],"source":"data = pd.read_csv(\"../input/mushrooms.csv\")"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6f7688e2-2730-689d-d440-215b2fe63a92"},"outputs":[],"source":"data.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"93ca8a70-fb7d-56ba-39b9-82585fbea984"},"outputs":[],"source":"data.info()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b8d81d6d-af33-fa11-c2f5-388f35311b77"},"outputs":[],"source":"data['stalk-root'].value_counts()"},{"cell_type":"markdown","metadata":{"_cell_guid":"1cfe9ea2-ae8e-10a1-101d-0aa134f6042c"},"source":"More than 30% of the values of **stalk-root** are missing values. That's to much missing data to do imputation (if you are playing with poison mushrooms). We are going to remove the feature (we are going to lose some signal and noise with this). Let's see if we can achieve good accuracy despite this."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f209adc7-1f41-0592-5deb-0b07e00b45c5"},"outputs":[],"source":"100*len(data.loc[data['stalk-root']=='?']) / sum(data['stalk-root'].value_counts())"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2e195102-07aa-28c6-a22c-8b0f761c1322"},"outputs":[],"source":"data = data.drop('stalk-root', 1)"},{"cell_type":"markdown","metadata":{"_cell_guid":"7c76d9d5-f20b-ec93-66b6-d24e46793787"},"source":"The variable to predict (class) is very balanced:"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"70bd33e3-ba78-f672-9e3d-d931775ee947"},"outputs":[],"source":"data['class'].value_counts()"},{"cell_type":"markdown","metadata":{"_cell_guid":"22cce55f-ac5d-d571-7f70-c062e94ddb05"},"source":"We prepare the data to be used in the neural network model:"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6e798914-a2c8-3402-9ec6-3fda51432103"},"outputs":[],"source":"Y = pd.get_dummies(data.iloc[:,0],  drop_first=False)\nX = pd.DataFrame()\nfor each in data.iloc[:,1:].columns:\n    dummies = pd.get_dummies(data[each], prefix=each, drop_first=False)\n    X = pd.concat([X, dummies], axis=1)"},{"cell_type":"markdown","metadata":{"_cell_guid":"4a008f81-d910-9d45-564d-499e68d30a67"},"source":"## Modeling"},{"cell_type":"markdown","metadata":{"_cell_guid":"77306270-067a-b183-6ce5-f5f28b023e50"},"source":"### Classic models\n\nWe are going to compare the performance of some \"classic\" machine learning models: *Logistic Regression, KNN, Trees, Naive Bayes, Suport Vector Machines and Random Forest*.\n\nWe are going to use cross validation and the AUC metric."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3e400c3d-b6db-b38d-9c37-2b17ceee4bad"},"outputs":[],"source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=0)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7c5633c7-9882-3b97-d306-4b31c2268294"},"outputs":[],"source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\n\nmodels = []\nmodels.append(('LR', LogisticRegression()))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('CART', DecisionTreeClassifier()))\nmodels.append(('NB', GaussianNB()))\nmodels.append(('SVM', SVC(probability=True)))\nmodels.append(('RF', RandomForestClassifier()))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"414b7889-762d-50d7-bb0f-9b053f59a89b"},"outputs":[],"source":"from sklearn.model_selection import cross_val_score, KFold\n\nseed = 321\n\n# evaluate each model in turn\nresults = []\nnames = []\nfor name, model in models:\n    kfold = KFold(n_splits=10, random_state=seed)\n    cv_results = cross_val_score(model, X_train, y_train.iloc[:,1], cv=kfold, scoring='roc_auc')    \n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)"},{"cell_type":"markdown","metadata":{"_cell_guid":"d1bc5d92-d4a4-23f0-4329-23e7cbaa7b9f"},"source":"The cross validation show a good performance. Let's see in a plot:"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5a2a214e-f7bb-f1ce-3f04-3113cfe7de48"},"outputs":[],"source":"# Compare Algorithms\nfig = plt.figure(figsize=(16, 8))\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(111)\nplt.boxplot(results, vert=False)\nax.set_yticklabels(names)\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"a6efd1a7-7d54-a3d3-d4aa-130bb6e2af40"},"source":"Let's see the performance of each model with the test data. They works really well:"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"280fbc7c-aa29-f83d-522c-358342efa70e"},"outputs":[],"source":"from collections import defaultdict\nfrom sklearn.metrics import roc_auc_score\n\nmodel_predictions = defaultdict()\nmodel_score = defaultdict(np.float)\nfor name, model in models:\n    model.fit(X_train, y_train.iloc[:,1])\n    my_pred = model.predict(X_test)\n    model_predictions[name] = my_pred\n    model_score[name] = roc_auc_score(y_test.iloc[:,1], my_pred)\n\n    msg = \"%s: %f\" % (name, model_score[name])\n    print(msg) "},{"cell_type":"markdown","metadata":{"_cell_guid":"acdc6dd4-194f-91f2-4bc0-b2474e08c7ea"},"source":"We are going to see how the different models are correlated between them. They are highly correlated although NB is a little less."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d49fd392-c3a7-e000-0b2f-7b576c1716ad"},"outputs":[],"source":"model_predicions_df = pd.DataFrame(model_predictions)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1fbe7768-b74e-c8ba-18e6-4b4d99d23c0e"},"outputs":[],"source":"corrmat = model_predicions_df.corr()\ncorrmat"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e4dad2d9-b6a1-43da-1cc3-3f42231db44d"},"outputs":[],"source":"sns.heatmap(corrmat)\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"a933f169-7172-5fce-3f85-41a27a8d0b2b"},"source":"### Neural Network model\n\nWe are going to build a neural network model with Keras. We'll check its accuracy, but our main objective here is to inspect the values of\nthe hidden layer and to project them into a two dimension plot to see how the neural network identify by itself the different groups"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"22c2729a-7f4e-bf8b-a2d0-c83eeebaf4b9"},"outputs":[],"source":"\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Dropout\nfrom keras.optimizers import SGD\nfrom keras.callbacks import EarlyStopping\nfrom sklearn.model_selection import cross_val_score\nfrom keras import backend as K\n\nseed = 123456 \n\ndef create_model():\n    model = Sequential()\n    model.add(Dense(20, input_dim=X.shape[1], kernel_initializer='uniform', activation='relu'))\n    model.add(Dropout(0.1))\n    model.add(Dense(2, activation='softmax'))\n    sgd = SGD(lr=0.01, momentum=0.7, decay=0, nesterov=False)\n    model.compile(loss='binary_crossentropy' , optimizer='sgd', metrics=['accuracy'])\n    return model"},{"cell_type":"markdown","metadata":{"_cell_guid":"68765185-183a-2e30-e237-af7d4d722f78"},"source":"We train the model and get the associated training graphs:"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e93a707a-6532-785b-c861-9806dde8b085"},"outputs":[],"source":"model = create_model()\nhistory = model.fit(X.values, Y.values, validation_split=0.20, epochs=300, batch_size=100, verbose=0)\n\n\n# list all data in history\nprint(history.history.keys())\n# summarize history for accuracy\nplt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9bc7af90-31fd-2c22-f76f-a5d1e0579c39"},"outputs":[],"source":"print(\"Training accuracy: %.2f%% / Validation accuracy: %.2f%%\" % \n      (100*history.history['acc'][-1], 100*history.history['val_acc'][-1]))"},{"cell_type":"markdown","metadata":{"_cell_guid":"7226b25f-2124-c2af-1c80-38c1b61b171b"},"source":"We are going to obtain the values of the layer previous to the output layer:"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"43c47c35-f7ed-199c-4c97-0b37fcb6a889"},"outputs":[],"source":"from keras import backend as K\nimport numpy as np\n\nlayer_of_interest=0\nintermediate_tensor_function = K.function([model.layers[0].input],[model.layers[layer_of_interest].output])\nintermediate_tensor = intermediate_tensor_function([X.iloc[0,:].values.reshape(1,-1)])[0]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8021a601-66b6-89ae-19ab-80d224c4ccfe"},"outputs":[],"source":"intermediates = []\ncolor_intermediates = []\nfor i in range(len(X)):\n    output_class = np.argmax(Y.iloc[i,:].values)\n    intermediate_tensor = intermediate_tensor_function([X.iloc[i,:].values.reshape(1,-1)])[0]\n    intermediates.append(intermediate_tensor[0])\n    if(output_class == 0):\n        color_intermediates.append(\"#0000ff\")\n    else:\n        color_intermediates.append(\"#ff0000\")"},{"cell_type":"markdown","metadata":{"_cell_guid":"a197a930-cd16-9c2e-2011-84fa4f8c9d07"},"source":"The penultimate layer has 20 neurons. We are going to build a t-SNE projection:"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"93338989-2de6-4fdc-856a-ba75612edec6"},"outputs":[],"source":"from sklearn.manifold import TSNE\ntsne = TSNE(n_components=2, random_state=0)\nintermediates_tsne = tsne.fit_transform(intermediates)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"232934dc-c67b-c1c6-10b1-ac5598ada68d"},"outputs":[],"source":"plt.figure(figsize=(8, 8))\nplt.scatter(x = intermediates_tsne[:,0], y=intermediates_tsne[:,1], color=color_intermediates)\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"59452356-950c-de04-accd-c82899d6f0c4"},"source":"## Conclusion\n\nWe have obtained a clear image where the different classes are very identificable (poison and edible mushrooms)"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}