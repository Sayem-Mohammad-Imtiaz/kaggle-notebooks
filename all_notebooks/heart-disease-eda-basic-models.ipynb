{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 1. Packages","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\n\nimport warnings\nwarnings.simplefilter(action='ignore', category = FutureWarning)\n\nsns.set_style(\"darkgrid\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-30T15:40:53.099163Z","iopub.execute_input":"2021-06-30T15:40:53.099888Z","iopub.status.idle":"2021-06-30T15:40:53.938409Z","shell.execute_reply.started":"2021-06-30T15:40:53.09976Z","shell.execute_reply":"2021-06-30T15:40:53.937511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split, KFold, cross_val_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nimport lightgbm as lgb\nimport xgboost as xgb","metadata":{"execution":{"iopub.status.busy":"2021-06-30T15:40:53.939834Z","iopub.execute_input":"2021-06-30T15:40:53.940126Z","iopub.status.idle":"2021-06-30T15:40:55.675785Z","shell.execute_reply.started":"2021-06-30T15:40:53.9401Z","shell.execute_reply":"2021-06-30T15:40:55.674963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. General Infos About Data","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"../input/heart-disease-uci/heart.csv\")\ndf","metadata":{"execution":{"iopub.status.busy":"2021-06-30T15:40:55.677238Z","iopub.execute_input":"2021-06-30T15:40:55.677692Z","iopub.status.idle":"2021-06-30T15:40:55.727725Z","shell.execute_reply.started":"2021-06-30T15:40:55.677646Z","shell.execute_reply":"2021-06-30T15:40:55.727029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[df.duplicated()]","metadata":{"execution":{"iopub.status.busy":"2021-06-30T15:40:55.72918Z","iopub.execute_input":"2021-06-30T15:40:55.729469Z","iopub.status.idle":"2021-06-30T15:40:55.75814Z","shell.execute_reply.started":"2021-06-30T15:40:55.729442Z","shell.execute_reply":"2021-06-30T15:40:55.757124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Total Observations: \" + str(df.shape[0]))\n\ndf = df.drop_duplicates()\n\nprint(\"Total Observations After Removing Duplicates: \" + str(df.shape[0]))","metadata":{"execution":{"iopub.status.busy":"2021-06-30T15:40:55.759666Z","iopub.execute_input":"2021-06-30T15:40:55.760056Z","iopub.status.idle":"2021-06-30T15:40:55.771259Z","shell.execute_reply.started":"2021-06-30T15:40:55.760015Z","shell.execute_reply":"2021-06-30T15:40:55.770104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2021-06-30T15:40:55.772929Z","iopub.execute_input":"2021-06-30T15:40:55.773702Z","iopub.status.idle":"2021-06-30T15:40:55.793089Z","shell.execute_reply.started":"2021-06-30T15:40:55.773657Z","shell.execute_reply":"2021-06-30T15:40:55.792017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe()","metadata":{"execution":{"iopub.status.busy":"2021-06-30T15:40:55.794224Z","iopub.execute_input":"2021-06-30T15:40:55.794506Z","iopub.status.idle":"2021-06-30T15:40:55.857171Z","shell.execute_reply.started":"2021-06-30T15:40:55.794479Z","shell.execute_reply":"2021-06-30T15:40:55.855983Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Our categorical features had encoded with using label encoder or had been considered as ordinal feature. I will remap them with meanings and I will re-encode them with using one-hot encoder.\n","metadata":{}},{"cell_type":"markdown","source":"# 2.1 Features","metadata":{}},{"cell_type":"markdown","source":"**age**: age in years\n\n**sex**: sex (1 = male; 0 = female)\n\n**cp**: chest pain type\n    Value 1: typical angina\n    Value 2: atypical angina\n    Value 3: non-anginal pain\n    Value 4: asymptomatic\n    \n**trestbps**: resting blood pressure (in mm Hg on admission to the hospital)\n\n**chol**: serum cholestoral in mg/dl\n\n**fbs**: (fasting blood sugar > 120 mg/dl) (1 = true; 0 = false)\n\n**restecg**: resting electrocardiographic results\n    \n    Value 0: normal\n    Value 1: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV)\n    Value 2: showing probable or definite left ventricular hypertrophy by Estes' criteria\n    \n**thalach**: maximum heart rate achieved\n\n**exang**: exercise induced angina (1 = yes; 0 = no)\n\n**oldpeak** = ST depression induced by exercise relative to rest\n\n**slope**: the slope of the peak exercise ST segment\n    \n    Value 1: upsloping\n    Value 2: flat\n    Value 3: downsloping\n    \n**ca**: number of major vessels (0-3) colored by flourosopy\n\n**thal**: 3 = normal; 6 = fixed defect; 7 = reversable defect\n\nhttps://archive.ics.uci.edu/ml/datasets/Heart+Disease","metadata":{}},{"cell_type":"code","source":"df.loc[:, \"slope\"] = df.loc[:, \"slope\"].map({0: \"downsloping\", 1: \"flat\", 2: \"upsloping\"})\ndf.loc[:, \"thal\"] = df.loc[:, \"thal\"].map({1: \"fixed_effect\", 2: \"normal\", 3: \"reversable_defect\", 0: \"else\"})","metadata":{"execution":{"iopub.status.busy":"2021-06-30T15:40:55.86106Z","iopub.execute_input":"2021-06-30T15:40:55.861503Z","iopub.status.idle":"2021-06-30T15:40:55.875064Z","shell.execute_reply.started":"2021-06-30T15:40:55.861457Z","shell.execute_reply":"2021-06-30T15:40:55.873754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Target Distribution","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize = (8, 8))\nax.pie(df.target.value_counts(), labels=[\"0\", \"1\"], autopct='%1.2f%%', startangle=180)\nax.set_title(\"target\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-30T15:40:55.877549Z","iopub.execute_input":"2021-06-30T15:40:55.878033Z","iopub.status.idle":"2021-06-30T15:40:56.022611Z","shell.execute_reply.started":"2021-06-30T15:40:55.877989Z","shell.execute_reply":"2021-06-30T15:40:56.021686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Target value's distribution is 54.3% - 45.7%. It is balanced. So, we don't have to use stratification techniques for cross validation and splitting the data, or we don't need to applying sampling to the data.","metadata":{}},{"cell_type":"code","source":"cat_cols = [\"sex\", \"cp\", \"fbs\", \"restecg\", \"exang\", \"slope\", \"ca\", \"thal\"]\nnum_cols = [\"age\", \"trestbps\", \"chol\", \"thalach\", \"oldpeak\"]","metadata":{"execution":{"iopub.status.busy":"2021-06-30T15:40:56.023952Z","iopub.execute_input":"2021-06-30T15:40:56.024239Z","iopub.status.idle":"2021-06-30T15:40:56.029052Z","shell.execute_reply.started":"2021-06-30T15:40:56.024212Z","shell.execute_reply":"2021-06-30T15:40:56.028026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. EDA","metadata":{}},{"cell_type":"markdown","source":"## 4.1 Categorical Features","metadata":{}},{"cell_type":"code","source":"def count_percentage(df, col, hue):\n\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(22, 6))\n    order = sorted(df[col].unique())\n    \n    sns.countplot(col, data = df, hue = hue, ax = ax1, order = order).set_title(\"Counts For Feature:\\n\" + col)\n\n    df_temp = df.groupby(col)[hue].value_counts(normalize = True).\\\n    rename(\"percentage\").\\\n    reset_index()\n    \n    fig = sns.barplot(x = col, y = \"percentage\", hue = hue, data = df_temp, ax = ax2, order = order)\n    fig.set_ylim(0,1)\n    \n    fontsize = 14 if len(order) <= 10 else 10\n    for p in fig.patches:\n        \n        txt = \"{:.1f}\".format(p.get_height() * 100) + \"%\"\n        txt_x = p.get_x() \n        txt_y = p.get_height()\n        fig.text(txt_x + 0.125, txt_y + 0.02,txt, fontsize = fontsize)\n\n    ax2.set_title(\"Percentages For Feature: \\n\" + col)","metadata":{"execution":{"iopub.status.busy":"2021-06-30T15:40:56.030478Z","iopub.execute_input":"2021-06-30T15:40:56.030825Z","iopub.status.idle":"2021-06-30T15:40:56.040813Z","shell.execute_reply.started":"2021-06-30T15:40:56.030793Z","shell.execute_reply":"2021-06-30T15:40:56.039913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for col in cat_cols:\n    count_percentage(df, col, \"target\")","metadata":{"execution":{"iopub.status.busy":"2021-06-30T15:40:56.042086Z","iopub.execute_input":"2021-06-30T15:40:56.042413Z","iopub.status.idle":"2021-06-30T15:40:59.201111Z","shell.execute_reply.started":"2021-06-30T15:40:56.042382Z","shell.execute_reply":"2021-06-30T15:40:59.200053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.1.1 Takeaways - Categorical Features","metadata":{}},{"cell_type":"markdown","source":"**sex**: Really effective feature. Male's target value is 75%, female's target value is 45%\n\n**cp**: If a person doesn't have asymptomatic chest pain (encoded as 0), target value is at least 70%\n\n**fbs**: This feature looks like ineffective. Target's values are 55% and 51% for two option.\n\n**restecg**: Target values are 46%, 63% and 25%. This variable could be useful\n\n**exang**: Really effective feature. 69% - 23%\n\n**slope**: 43, 35 and 75 percent. It could be also useful.\n\n**thal**: 50-33-78-24. It could be also useful.","metadata":{}},{"cell_type":"markdown","source":"# 4.2 Numerical Features","metadata":{}},{"cell_type":"markdown","source":"## 4.2.1 Numerical vs Target","metadata":{}},{"cell_type":"code","source":"def feature_dist_clas(df, col, hue):\n    fig, axes = plt.subplots(1, 4, figsize = (25, 5))\n    order = sorted(df[hue].unique())\n\n    sns.histplot(x = col, hue = hue, data = df, ax = axes[0])\n    sns.kdeplot(x = col, hue = hue, data = df, fill = True, ax = axes[1])\n    sns.boxplot(y = col, hue = hue, data = df, x = [\"\"] * len(df), ax = axes[2])\n    sns.violinplot(y = col, hue = hue, data = df, x = [\"\"] * len(df), ax = axes[3])\n    \n    fig.suptitle(\"For Feature:  \" + col)\n    axes[0].set_title(\"Histogram For Feature \" + col)\n    axes[1].set_title(\"KDE Plot For Feature \" + col)   \n    axes[2].set_title(\"Boxplot For Feature \" + col)   \n    axes[3].set_title(\"Violinplot For Feature \" + col)   ","metadata":{"execution":{"iopub.status.busy":"2021-06-30T15:40:59.202672Z","iopub.execute_input":"2021-06-30T15:40:59.203073Z","iopub.status.idle":"2021-06-30T15:40:59.21238Z","shell.execute_reply.started":"2021-06-30T15:40:59.203032Z","shell.execute_reply":"2021-06-30T15:40:59.21128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for col in num_cols:\n    feature_dist_clas(df, col, \"target\")","metadata":{"execution":{"iopub.status.busy":"2021-06-30T15:40:59.213603Z","iopub.execute_input":"2021-06-30T15:40:59.213903Z","iopub.status.idle":"2021-06-30T15:41:03.775707Z","shell.execute_reply.started":"2021-06-30T15:40:59.213878Z","shell.execute_reply":"2021-06-30T15:41:03.774961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def feature_distribution(df, col):\n    \n    skewness = np.round(df[col].skew(), 3)\n    kurtosis = np.round(df[col].kurtosis(), 3)\n\n    fig, axes = plt.subplots(1, 3, figsize = (18, 6))\n\n    sns.kdeplot(data = df, x = col, fill = True, ax = axes[0], color = \"orangered\")\n    sns.boxplot(data = df, y = col, ax = axes[1], color = \"orangered\")\n    stats.probplot(df[col], plot = axes[2])\n\n    axes[0].set_title(\"Distribution \\nSkewness: \" + str(skewness) + \"\\nKurtosis: \" + str(kurtosis))\n    axes[1].set_title(\"Boxplot\")\n    axes[2].set_title(\"Probability Plot\")\n    fig.suptitle(\"For Feature:  \" + col)","metadata":{"execution":{"iopub.status.busy":"2021-06-30T15:41:03.776708Z","iopub.execute_input":"2021-06-30T15:41:03.777094Z","iopub.status.idle":"2021-06-30T15:41:03.784427Z","shell.execute_reply.started":"2021-06-30T15:41:03.777065Z","shell.execute_reply":"2021-06-30T15:41:03.783676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for col in num_cols:\n    feature_distribution(df, col)","metadata":{"execution":{"iopub.status.busy":"2021-06-30T15:41:03.78563Z","iopub.execute_input":"2021-06-30T15:41:03.786021Z","iopub.status.idle":"2021-06-30T15:41:06.27889Z","shell.execute_reply.started":"2021-06-30T15:41:03.785993Z","shell.execute_reply":"2021-06-30T15:41:06.278136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.2.2 Takeaways - Numerical Features","metadata":{}},{"cell_type":"markdown","source":"Age, thalachh and oldpeak would be useful variables.\n\ntrtbps, col and oldpeak have a few outliers. thalachh has just an outlier.\n\nOur numerical variables generally have normal distribution except oldpeak. Also, outliers at chol feature are problem for this feature's normality.","metadata":{}},{"cell_type":"code","source":"def heatmap(df):\n    \n    fig, ax = plt.subplots(figsize = (15, 15))\n    \n    sns.heatmap(df.corr(), cmap = \"coolwarm\", annot = True, fmt = \".2f\", annot_kws = {\"fontsize\": 9},\n                vmin = -1, vmax = 1, square = True, linewidths = 0.8, cbar = False)\n    \nheatmap(df)","metadata":{"execution":{"iopub.status.busy":"2021-06-30T15:41:06.279865Z","iopub.execute_input":"2021-06-30T15:41:06.280237Z","iopub.status.idle":"2021-06-30T15:41:07.092925Z","shell.execute_reply.started":"2021-06-30T15:41:06.280208Z","shell.execute_reply":"2021-06-30T15:41:07.092203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. Preprocessing","metadata":{}},{"cell_type":"markdown","source":"One hot encoding for two feature, \n\nSplitting the data, \n\nDefining cross validations, \n\nScaling data with using Standard Scaler","metadata":{}},{"cell_type":"code","source":"encode_cols = [\"slope\", \"thal\"]\n\ndummies = pd.get_dummies(df[encode_cols], drop_first = True)\n\nfin = pd.concat([df, dummies], axis = 1).drop(encode_cols, axis = 1)\nfin","metadata":{"execution":{"iopub.status.busy":"2021-06-30T15:41:07.093921Z","iopub.execute_input":"2021-06-30T15:41:07.094286Z","iopub.status.idle":"2021-06-30T15:41:07.126416Z","shell.execute_reply.started":"2021-06-30T15:41:07.094258Z","shell.execute_reply":"2021-06-30T15:41:07.125512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have 302 observations, it means we have a small data. 25-75 or 30-70 is ideal for train test proportion.","metadata":{}},{"cell_type":"code","source":"target = \"target\"\npredictors = [col for col in fin.columns if col != target]\n\nX_train, X_test, y_train, y_test = train_test_split(fin[predictors],\n                                                    fin[target],\n                                                    test_size = 0.25,\n                                                    random_state = 42)\n\ncv3 = KFold(n_splits = 3, shuffle = True, random_state = 42)\ncv5 = KFold(n_splits = 5, shuffle = True, random_state = 42)\ncv10 = KFold(n_splits = 10, shuffle = True, random_state = 42)\n\ndef cv_model(model, X = X_train, y = y_train, cv = cv5):\n    return cross_val_score(model, X, y, scoring = \"accuracy\", cv = cv, n_jobs = -1).mean()","metadata":{"execution":{"iopub.status.busy":"2021-06-30T15:41:07.127915Z","iopub.execute_input":"2021-06-30T15:41:07.128307Z","iopub.status.idle":"2021-06-30T15:41:07.140275Z","shell.execute_reply.started":"2021-06-30T15:41:07.128266Z","shell.execute_reply":"2021-06-30T15:41:07.139533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for col in num_cols:   \n    scaler = StandardScaler()\n\n    X_train[col] = scaler.fit_transform(X_train[col].values.reshape(-1, 1))\n    X_test[col] = scaler.transform(X_test[col].values.reshape(-1, 1))","metadata":{"execution":{"iopub.status.busy":"2021-06-30T15:41:07.141683Z","iopub.execute_input":"2021-06-30T15:41:07.142341Z","iopub.status.idle":"2021-06-30T15:41:07.155583Z","shell.execute_reply.started":"2021-06-30T15:41:07.142254Z","shell.execute_reply":"2021-06-30T15:41:07.154721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I just remove the variable **fbs**. It is a categorical feature that has two possibility(55% - 51%). It has lowest correlation as we can from heatmap.","metadata":{}},{"cell_type":"code","source":"X_train2 = X_train.drop(\"fbs\", axis = 1)\nX_test2 = X_test.drop(\"fbs\", axis = 1)","metadata":{"execution":{"iopub.status.busy":"2021-06-30T15:41:07.156792Z","iopub.execute_input":"2021-06-30T15:41:07.157113Z","iopub.status.idle":"2021-06-30T15:41:07.164997Z","shell.execute_reply.started":"2021-06-30T15:41:07.157084Z","shell.execute_reply":"2021-06-30T15:41:07.164016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 6. Models","metadata":{}},{"cell_type":"markdown","source":"I just use basic classification algorithms with their default parameters.\n\nI also use Voting Classifier to construct ensemble models with choosing a couple of them. I won't tune hyperparameters.","metadata":{}},{"cell_type":"code","source":"logreg = LogisticRegression(random_state = 42)\nsvc = SVC(random_state=42, probability = True)\ngnb = GaussianNB()\nrfc = RandomForestClassifier(random_state = 42)\nknnc = KNeighborsClassifier(n_jobs = -1)\nlgbc = lgb.LGBMClassifier(random_state = 42, n_jobs = -1)\ndtc = DecisionTreeClassifier(random_state = 42)\nxgbc = xgb.XGBClassifier(random_state = 42, n_jobs = -1, use_label_encoder = False, eval_metric = \"logloss\")\n\nvc_logreg_svc_rfc_knn = VotingClassifier([(\"logreg\", logreg), (\"svc\", svc), (\"rfc\", rfc), (\"knn\", knnc)],\n                                         voting = \"soft\")\nvc_logreg_svc_knn = VotingClassifier([(\"logreg\", logreg), (\"svc\", svc), (\"knn\", knnc)],\n                                         voting = \"soft\")\nvc_logreg_svc = VotingClassifier([(\"logreg\", logreg), (\"svc\", svc)],\n                                         voting = \"soft\")\nvc_all = VotingClassifier([(\"logreg\", logreg), (\"svc\", svc), (\"gnb\", gnb), ( \"rfc\", rfc), (\"knn\", knnc),\n                           (\"lgb\", lgbc), (\"dtc\", dtc), (\"xgb\", xgbc)],\n                          voting = \"soft\")\n\ntrain_accuracy = {}\ntest_accuracy = {}\ncv_score3 = {}\ncv_score5 = {}\ncv_score10 = {}\n\nmodels = {\n    \"LogisticRegression\": logreg,\n    \"SupportVectorMachine\": svc,\n    \"GaussianNaiveBayes\": gnb,\n    \"RandomForest\": rfc,\n    \"KNN\": knnc,\n    \"LightGBM\": lgbc,\n    \"DecisionTree\": dtc,\n    \"XGBoost\": xgbc,\n    \"VotingClassifier (All Models)\": vc_all,\n    \"VotingClassifier (Logreg-SVC)\": vc_logreg_svc,\n    \"VotingClassifier (Logreg-SVC-KNN)\": vc_logreg_svc_knn,\n    \"VotingClassifier (Logreg-SVC-RFC-KNN)\": vc_logreg_svc_rfc_knn   \n}\n\nfor name, model in models.items():\n    \n    model.fit(X_train2, y_train)\n    train_preds = model.predict(X_train2)\n    test_preds = model.predict(X_test2)\n    \n    train_accuracy[name] = accuracy_score(train_preds, y_train).round(4)\n    test_accuracy[name] = accuracy_score(test_preds, y_test).round(4)\n    cv_score3[name] = cv_model(model, X_train2, y_train, cv = cv3).round(4)\n    cv_score5[name] = cv_model(model, X_train2, y_train, cv = cv5).round(4)\n    cv_score10[name] = cv_model(model, X_train2, y_train, cv = cv10).round(4)\n    \nscores = pd.DataFrame([train_accuracy, test_accuracy, cv_score3, cv_score5, cv_score10], \n                      index = [\"TrainAccuracy\", \"TestAccuracy\", \"3FoldCVScore\", \"5FoldCVScore\", \"10FoldCVScore\"]).T","metadata":{"execution":{"iopub.status.busy":"2021-06-30T15:41:07.166348Z","iopub.execute_input":"2021-06-30T15:41:07.166737Z","iopub.status.idle":"2021-06-30T15:46:01.198071Z","shell.execute_reply.started":"2021-06-30T15:41:07.166709Z","shell.execute_reply":"2021-06-30T15:46:01.197061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scores","metadata":{"execution":{"iopub.status.busy":"2021-06-30T15:46:01.201261Z","iopub.execute_input":"2021-06-30T15:46:01.201599Z","iopub.status.idle":"2021-06-30T15:46:01.217711Z","shell.execute_reply.started":"2021-06-30T15:46:01.201553Z","shell.execute_reply":"2021-06-30T15:46:01.216934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 6.1. Takeaways - Models","metadata":{}},{"cell_type":"markdown","source":"If we look at above table;\n\nLinear classifiers achieves better results i.e Logistic Regression, SVM\n\nTree based algorithms have overfitting problem because we didn't tune hyperparameters.\n\nCross validation scores are unstable. Our cv scores are changing between 0.79 - 0.83 but our scores on testing data are 0.88 - 0.90\n\nFor individual models, **Logistic Regression** has best cv scores with avg **0.82** accuracy.\n\nAlso, **Support Vector Classifier** has best test accuracy with nearly **90%**\n\nFor ensemble models,\n\n   If we use all models, overfitted models hurt our model's performance.\n   \n   If we build Voting Classifier with using Logistic Regression, SVM, and KNN, we get almost **91% test set accuracy** and average 83% cv score.","metadata":{}}]}