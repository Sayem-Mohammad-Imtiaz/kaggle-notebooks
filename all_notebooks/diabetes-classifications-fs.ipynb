{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom sklearn import preprocessing\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\nfrom sklearn.metrics import roc_curve\nfrom keras.utils import to_categorical\nimport random\n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import classification_report","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def decode(datum):\n    return np.argmax(datum)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dic={'Male':1,'Female':0,'Yes':1,'No':0,'Positive':1,'Negative':0}","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# We are reading our data\ndf = pd.read_csv(\"../input/early-stage-diabetes-risk-prediction-datasets/diabetes_data_upload.csv\")\nprint(df.head(),df.shape)\n\ndf.boxplot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# First 5 rows of our data\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_col=df.columns\n\nfor i in range(df.shape[1]):\n    df.replace\nprint(all_col[-1],len(all_col))\nfor col in range(1,len(all_col)):\n    df=df.replace({all_col[col]: dic})\nprint(df.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['class'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x=\"class\", data=df, palette=\"bwr\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"countNo = len(df[df['class'] == 0])\ncountYes = len(df[df['class'] == 1])\nprint(\"Percentage of NO: {:.2f}%\".format((countNo / (len(df['class']))*100)))\nprint(\"Percentage of Yes: {:.2f}%\".format((countYes / (len(df['class'])*100))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = df['class'].values\nx_data = df.drop(['class'], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Normalize\nx = (x_data - np.min(x_data)) / (np.max(x_data) - np.min(x_data)).values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.2,random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#transpose matrices\nx_train = x_train.T\ny_train = y_train.T\nx_test = x_test.T\ny_test = y_test.T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#initialize\ndef initialize(dimension):\n    \n    weight = np.full((dimension,1),0.01)\n    bias = 0.0\n    return weight,bias","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def sigmoid(z):\n    \n    y_head = 1/(1+ np.exp(-z))\n    return y_head","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def forwardBackward(weight,bias,x_train,y_train):\n    # Forward\n    \n    y_head = sigmoid(np.dot(weight.T,x_train) + bias)\n    loss = -(y_train*np.log(y_head) + (1-y_train)*np.log(1-y_head))\n    cost = np.sum(loss) / x_train.shape[1]\n    \n    # Backward\n    derivative_weight = np.dot(x_train,((y_head-y_train).T))/x_train.shape[1]\n    derivative_bias = np.sum(y_head-y_train)/x_train.shape[1]\n    gradients = {\"Derivative Weight\" : derivative_weight, \"Derivative Bias\" : derivative_bias}\n    \n    return cost,gradients","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def update(weight,bias,x_train,y_train,learningRate,iteration) :\n    costList = []\n    index = []\n    \n    #for each iteration, update weight and bias values\n    for i in range(iteration):\n        cost,gradients = forwardBackward(weight,bias,x_train,y_train)\n        weight = weight - learningRate * gradients[\"Derivative Weight\"]\n        bias = bias - learningRate * gradients[\"Derivative Bias\"]\n        \n        costList.append(cost)\n        index.append(i)\n\n    parameters = {\"weight\": weight,\"bias\": bias}\n    \n    print(\"iteration:\",iteration)\n    print(\"cost:\",cost)\n\n    plt.plot(index,costList)\n    plt.xlabel(\"Number of Iteration\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n\n    return parameters, gradients","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict(weight,bias,x_test):\n    z = np.dot(weight.T,x_test) + bias\n    y_head = sigmoid(z)\n\n    y_prediction = np.zeros((1,x_test.shape[1]))\n    \n    for i in range(y_head.shape[1]):\n        if y_head[0,i] <= 0.5:\n            y_prediction[0,i] = 0\n        else:\n            y_prediction[0,i] = 1\n    return y_prediction","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def logistic_regression(x_train,y_train,x_test,y_test,learningRate,iteration):\n    dimension = x_train.shape[0]\n    weight,bias = initialize(dimension)\n    \n    parameters, gradients = update(weight,bias,x_train,y_train,learningRate,iteration)\n\n    y_prediction = predict(parameters[\"weight\"],parameters[\"bias\"],x_test)\n    \n    print(\"Manuel Test Accuracy: {:.2f}%\".format((100 - np.mean(np.abs(y_prediction - y_test))*100)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logistic_regression(x_train,y_train,x_test,y_test,1,100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Sklearn Logistic Regression**"},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracies = {}\n\nlr = LogisticRegression()\nlr.fit(x_train.T,y_train.T)\nacc = lr.score(x_test.T,y_test.T)*100\n\naccuracies['Logistic Regression'] = acc\nprint(\"Test Accuracy {:.2f}%\".format(acc))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"K-Nearest Neighbour (KNN) Classification"},{"metadata":{"trusted":true},"cell_type":"code","source":"# KNN Model\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors = 2)  # n_neighbors means k\nknn.fit(x_train.T, y_train.T)\nprediction = knn.predict(x_test.T)\n\nprint(\"{} NN Score: {:.2f}%\".format(2, knn.score(x_test.T, y_test.T)*100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# try ro find best k value\nscoreList = []\nfor i in range(1,20):\n    knn2 = KNeighborsClassifier(n_neighbors = i)  # n_neighbors means k\n    knn2.fit(x_train.T, y_train.T)\n    scoreList.append(knn2.score(x_test.T, y_test.T))\n    \nplt.plot(range(1,20), scoreList)\nplt.xticks(np.arange(1,20,1))\nplt.xlabel(\"K value\")\nplt.ylabel(\"Score\")\nplt.show()\n\nacc = max(scoreList)*100\naccuracies['KNN'] = acc\nprint(\"Maximum KNN Score is {:.2f}%\".format(acc))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Support Vector Machine (SVM) Algorithm"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svm = SVC(random_state = 1,kernel='linear')\nsvm.fit(x_train.T, y_train.T)\n\nacc = svm.score(x_test.T,y_test.T)*100\naccuracies['SVM'] = acc\nprint(\"Test Accuracy of SVM Algorithm: {:.2f}%\".format(acc))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Naive Bayes Algorithm"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\nnb.fit(x_train.T, y_train.T)\n\nacc = nb.score(x_test.T,y_test.T)*100\naccuracies['Naive Bayes'] = acc\nprint(\"Accuracy of Naive Bayes: {:.2f}%\".format(acc))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Decision Tree Algorithm"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\ndtc = DecisionTreeClassifier()\ndtc.fit(x_train.T, y_train.T)\n\nacc = dtc.score(x_test.T, y_test.T)*100\naccuracies['Decision Tree'] = acc\nprint(\"Decision Tree Test Accuracy {:.2f}%\".format(acc))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Random Forest Classification"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Random Forest Classification\nfrom sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(n_estimators = 2, random_state = 1)\nrf.fit(x_train.T, y_train.T)\n\nacc = rf.score(x_test.T,y_test.T)*100\naccuracies['Random Forest'] = acc\nprint(\"Random Forest Algorithm Accuracy Score : {:.2f}%\".format(acc))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Comparing Models"},{"metadata":{"trusted":true},"cell_type":"code","source":"colors = [\"purple\", \"green\", \"orange\", \"magenta\",\"#CFC60E\",\"#0FBBAE\"]\n\nsns.set_style(\"whitegrid\")\nplt.figure(figsize=(16,5))\nplt.yticks(np.arange(0,100,10))\nplt.ylabel(\"Accuracy %\")\nplt.xlabel(\"Algorithms\")\nsns.barplot(x=list(accuracies.keys()), y=list(accuracies.values()), palette=colors)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Confusion Matrix, F1 Score"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predicted values\ny_head_lr = lr.predict(x_test.T)\nknn3 = KNeighborsClassifier(n_neighbors = 3)\nknn3.fit(x_train.T, y_train.T)\ny_head_knn = knn3.predict(x_test.T)\ny_head_svm = svm.predict(x_test.T)\ny_head_nb = nb.predict(x_test.T)\ny_head_dtc = dtc.predict(x_test.T)\ny_head_rf = rf.predict(x_test.T)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import f1_score\nf1_lr = f1_score(y_test,y_head_lr)\nf1_knn = f1_score(y_test,y_head_knn)\nf1_svm = f1_score(y_test,y_head_svm)\nf1_nb = f1_score(y_test,y_head_nb)\nf1_dtc = f1_score(y_test,y_head_dtc)\nf1_rf = f1_score(y_test,y_head_rf)\n\nprint('Logistic Regression F1 score:', f1_lr)\nprint('K Nearest Neighbors F1 score:', f1_knn)\nprint('Support Vector Machine F1 score:', f1_svm)\nprint('Naive Bayes F1 score:', f1_nb)\nprint('Decision Tree s F1 score:', f1_dtc)\nprint('Random Forest F1 score:', f1_rf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import recall_score\nrc_lr = recall_score(y_test,y_head_lr)\nrc_knn = recall_score(y_test,y_head_knn)\nrc_svm = recall_score(y_test,y_head_svm)\nrc_nb = recall_score(y_test,y_head_nb)\nrc_dtc = recall_score(y_test,y_head_dtc)\nrc_rf = recall_score(y_test,y_head_rf)\n\nprint('Logistic Regression recall:', rc_lr)\nprint('K Nearest Neighbors recall:', rc_knn)\nprint('Support Vector Machine recall:', rc_svm)\nprint('Naive Bayes recall:', rc_nb)\nprint('Decision Tree recall:', rc_dtc)\nprint('Random Forest recall:', rc_rf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import precision_score\nprecision_lr = precision_score(y_test,y_head_lr)\nprecision_knn = precision_score(y_test,y_head_knn)\nprecision_svm = precision_score(y_test,y_head_svm)\nprecision_nb = precision_score(y_test,y_head_nb)\nprecision_dtc = precision_score(y_test,y_head_dtc)\nprecision_rf = precision_score(y_test,y_head_rf)\n\nprint('Logistic Regression precision:', precision_lr)\nprint('K Nearest Neighbors precision:', precision_knn)\nprint('Support Vector Machine precision:', precision_svm)\nprint('Naive Bayes precision:', precision_nb)\nprint('Decision Tree precision:', precision_dtc)\nprint('Random Forest precision:', precision_rf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\n\ncm_lr = confusion_matrix(y_test,y_head_lr)\ncm_knn = confusion_matrix(y_test,y_head_knn)\ncm_svm = confusion_matrix(y_test,y_head_svm)\ncm_nb = confusion_matrix(y_test,y_head_nb)\ncm_dtc = confusion_matrix(y_test,y_head_dtc)\ncm_rf = confusion_matrix(y_test,y_head_rf)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_roc_curve(false_positive_rate, true_positive_rate, label=None):\n    plt.plot(false_positive_rate, true_positive_rate, linewidth=2, label=label)\n    plt.plot([0, 1], [0, 1], 'r', linewidth=4)\n    plt.axis([0, 1, 0, 1])\n    plt.xlabel('False Positive Rate (FPR)', fontsize=16)\n    plt.ylabel('True Positive Rate (TPR)', fontsize=16)\n    plt.legend","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_curve, auc\nfpr_lr, tpr_lr, thresholds_lr = roc_curve(y_test, y_head_lr)\nfpr_knn, tpr_knn, thresholds_knn = roc_curve(y_test, y_head_knn)\nfpr_svm, tpr_svm, thresholds_svm = roc_curve(y_test, y_head_svm)\nfpr_nb, tpr_nb, thresholds_nb = roc_curve(y_test, y_head_nb)\nfpr_dtc, tpr_dtc, thresholds_dtc = roc_curve(y_test, y_head_dtc)\nfpr_rf, tpr_rf, thresholds_rf = roc_curve(y_test, y_head_rf)\n\nroc_auc_lr = auc(fpr_lr, tpr_lr)\nroc_auc_knn = auc(fpr_knn, tpr_knn)\nroc_auc_svm = auc(fpr_svm, tpr_svm)\nroc_auc_nb = auc(fpr_nb, tpr_nb)\nroc_auc_dtc = auc(fpr_dtc, tpr_dtc)\nroc_auc_rf = auc(fpr_rf, tpr_rf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(14, 7))\n\nplt.suptitle(\"ROC\",fontsize=24)\nplt.subplots_adjust(wspace = 0.4, hspace= 0.4)\n\n\nplt.subplot(2,3,1)\nplt.title(\"Logistic Regression ROC\")\nplot_roc_curve(fpr_lr, tpr_lr)\nplt.plot(fpr_lr, tpr_lr, label='AUC = %0.4f'% roc_auc_lr)\nplt.legend(loc='lower right')\n\nplt.subplot(2,3,2)\nplt.title(\"K Nearest Neighbors ROC\")\nplot_roc_curve(fpr_knn, tpr_knn)\nplt.plot(fpr_knn, tpr_knn, label='AUC = %0.4f'% roc_auc_knn)\nplt.legend(loc='lower right')\n\nplt.subplot(2,3,3)\nplt.title(\"Support Vector Machine ROC\")\nplot_roc_curve(fpr_svm, tpr_svm)\nplt.plot(fpr_svm, tpr_svm, label='AUC = %0.4f'% roc_auc_svm)\nplt.legend(loc='lower right')\n\nplt.subplot(2,3,4)\nplt.title(\"Naive Bayes ROC\")\nplot_roc_curve(fpr_nb, tpr_nb)\nplt.plot(fpr_nb, tpr_nb, label='AUC = %0.4f'% roc_auc_nb)\nplt.legend(loc='lower right')\n\nplt.subplot(2,3,5)\nplt.title(\"Decision Tree Classifier ROC\")\nplot_roc_curve(fpr_dtc, tpr_dtc)\nplt.plot(fpr_dtc, tpr_dtc, label='AUC = %0.4f'% roc_auc_dtc)\nplt.legend(loc='lower right')\n\nplt.subplot(2,3,6)\nplt.title(\"Random Forest\")\nplot_roc_curve(fpr_rf, tpr_rf)\nplt.plot(fpr_rf, tpr_rf, label='AUC = %0.4f'% roc_auc_rf)\nplt.legend(loc='lower right')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(24,12))\n\nplt.suptitle(\"Confusion Matrixes\",fontsize=24)\nplt.subplots_adjust(wspace = 0.4, hspace= 0.4)\n\nplt.subplot(2,3,1)\nplt.title(\"Logistic Regression Confusion Matrix\")\nsns.heatmap(cm_lr,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False, annot_kws={\"size\": 24})\n\nplt.subplot(2,3,2)\nplt.title(\"K Nearest Neighbors Confusion Matrix\")\nsns.heatmap(cm_knn,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False, annot_kws={\"size\": 24})\n\nplt.subplot(2,3,3)\nplt.title(\"Support Vector Machine Confusion Matrix\")\nsns.heatmap(cm_svm,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False, annot_kws={\"size\": 24})\n\nplt.subplot(2,3,4)\nplt.title(\"Naive Bayes Confusion Matrix\")\nsns.heatmap(cm_nb,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False, annot_kws={\"size\": 24})\n\nplt.subplot(2,3,5)\nplt.title(\"Decision Tree Classifier Confusion Matrix\")\nsns.heatmap(cm_dtc,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False, annot_kws={\"size\": 24})\n\nplt.subplot(2,3,6)\nplt.title(\"Random Forest Confusion Matrix\")\nsns.heatmap(cm_rf,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False, annot_kws={\"size\": 24})\n\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}