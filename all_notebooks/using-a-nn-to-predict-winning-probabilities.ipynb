{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Hello! This is my first notebook and upload of a Dataset - as such any constructive feedback you have, I would love to hear. \n\nIn future I want the form stats to loop over seasons for teams (this is doable, but runtime might be a bit of an issue) as well as getting data as to which players are playing for each team, thus we get more of a sense of how individual players affect team performance explicitly in the dataframe.","metadata":{}},{"cell_type":"code","source":"# Here are the packages I loaded in for testing this on a Neural Network\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.callbacks import EarlyStopping\nimport matplotlib.pyplot as plt\nimport os\n\n#Here is how we put all the seasons from 2014-2019 from the Premier League into one dataframe\n#- so that we can use it as training data\nteam_csvs = []\nTeam_1420_dfs = []\nyears = ['/2014', '/2015', '/2016', '/2017', '/2018', '/2019', '/2020']\nfor year in years[:-1]:\n    year = year[1:]\n    for entry in os.scandir('../input/football-soccor-teams-stats-20142021/Understat_Team_Data/EPL_teams' + '/{}'.format(year)):\n        if entry.is_file():\n            df = pd.read_csv('../input/football-soccor-teams-stats-20142021/Understat_Team_Data/EPL_teams' + '/{}'.format(year) + '/{}'.format(entry.name))\n            for col in list(df.columns):\n                if 'Unnamed' in col:\n                    del df[col]       \n            Team_1420_dfs.append(df)\nbig_team_df = pd.concat(Team_1420_dfs, ignore_index=True)\nbig_team_df.to_csv('All_Epl_dfs_1420.csv')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-03T15:39:34.556089Z","iopub.execute_input":"2021-08-03T15:39:34.556511Z","iopub.status.idle":"2021-08-03T15:39:38.577762Z","shell.execute_reply.started":"2021-08-03T15:39:34.556475Z","shell.execute_reply":"2021-08-03T15:39:38.576852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next, I shift the dataset so that we try to measure the probability of winning from the PREVIOUS results of both teams, hence we split our dataset in two halves, opponent dataset and previous game dataset, to get a result. Before that, I put in a dictionary so we can rename columns to show data from the previous game.","metadata":{}},{"cell_type":"code","source":"OG_cols = ['Home/Away','xG','xGA','non-penalty xG','non-penalty xG Against','Deep','Deep Allowed','scored','conceded',\n 'Expected Points','Result','date','Wins','Draws','Loses','Points','Non-penalty xG difference','ppda attack','ppda defence',\n 'ppda allowed att','ppda allowed def','Wins cum','Loses cum','Draws cum','Points cum','npxG difference cum','xG cum','xGA cum',\n 'npxG cum','npxGA cum','scored cum', 'conceded cum', 'Expected Points cum','Opponent','Prob Win',\n 'Prob Draw','Prob Lose','Wins in last 5','Loses in last 5','Draws in last 5','Points in last 5','npxG difference last 5',\n           'xG last 5','xGA last 5','npxG last 5','npxGA last 5','scored last 5','conceded last 5','Expected Points last 5']\nprev_game_cols = []\nfor col in OG_cols:\n    prev_game_cols.append('Prev game ' + col)","metadata":{"execution":{"iopub.status.busy":"2021-08-03T15:47:35.894804Z","iopub.execute_input":"2021-08-03T15:47:35.895216Z","iopub.status.idle":"2021-08-03T15:47:35.902643Z","shell.execute_reply.started":"2021-08-03T15:47:35.895182Z","shell.execute_reply":"2021-08-03T15:47:35.901579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here - we make the training set","metadata":{}},{"cell_type":"code","source":"big_team_df = pd.read_csv('All_Epl_dfs_1420.csv')\n#Make a df with JUST opponent data as well as the probabilities of win/lose/draw\nfor col in list(big_team_df.columns):\n    if ('Opp' not in col) and ('Prob ' not in col):\n        del big_team_df[col]\n        \n#Make the next one, where we keep the Team stat side of the dataframe, moving it to be the previous game.\nbig_team_df_2 = pd.DataFrame(index = [0], columns=big_team_df.columns)\nbig_team_df_2 = big_team_df_2.fillna(0)\nfor dataframe in Team_1420_dfs:\n    jeff = []\n    #Make the first line 0s \n    df_ = pd.DataFrame(index = [0], columns=big_team_df.columns)\n    df_ = df_.fillna(0)\n    #Get the above with the other lines of the dataframe\n    jeff.append(df_)\n    jeff.append(dataframe.drop(index = 37))\n    #put all of these together\n    jeff_concat = pd.concat(jeff, ignore_index = True)\n    big_team_df_2 = pd.concat([big_team_df_2, jeff_concat], ignore_index = True)\nbig_team_df_2 = big_team_df_2.drop(index = 0)\n\n#Delete all of the opponent columns/ probability columns\nfor col in list(big_team_df_2.columns):\n    if ('Opp' in col) or ('Prob ' in col):\n        del big_team_df_2[col]\n\nprev_game_dict = {OG_cols[i]: prev_game_cols[i] for i in range(len(OG_cols))}\ndel prev_game_dict['Home/Away']\nB_df_2 = big_team_df_2.rename(columns = prev_game_dict, inplace = False)\nB_df_3 = pd.concat([B_df_2, big_team_df], axis = 1)\n\nB_df_3 = B_df_3.dropna()\n\nB_df_3['% Win'] = 100*B_df_3['Prob Win']\nB_df_3['% Draw']= 100*B_df_3['Prob Draw']\nB_df_3['% Lose']= 100*B_df_3['Prob Lose']\n#Making sure the dataframe's columns are suitable for the model \n#- specifically assigning numerical values to indicate whether a team was home or away\nh_a_dict = { 'h': 1, 'a' : -1}\ndef h_a_numbs(loc):\n    return h_a_dict[loc]\nB_df_3['Home/Away'] = B_df_3['Home/Away'].apply(h_a_numbs)","metadata":{"execution":{"iopub.status.busy":"2021-08-03T15:48:04.974136Z","iopub.execute_input":"2021-08-03T15:48:04.974553Z","iopub.status.idle":"2021-08-03T15:48:08.643996Z","shell.execute_reply.started":"2021-08-03T15:48:04.97451Z","shell.execute_reply":"2021-08-03T15:48:08.6429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we are going to use the 20/21 Arsenal Season to validate our data. You could use any dataset you want from the 20/21 season to test a Neural Network with.","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('../input/football-soccor-teams-stats-20142021/Understat_Team_Data/EPL_teams/2020/Arsenal.csv')\nfor col in list(df.columns):\n    if ('Opp' not in col) and ('Prob ' not in col):\n        del df[col]\n#Previous Game data\ndf_1 = pd.read_csv('../input/football-soccor-teams-stats-20142021/Understat_Team_Data/EPL_teams/2020/Arsenal.csv')\ndf_ = pd.DataFrame(index = [0], columns=df.columns)\ndf_ = df_.fillna(0)\ndf_2 = pd.concat([df_, df_1.drop(index = 37)], ignore_index=True)\nfor col in list(df_2.columns):\n    if ('Opp' in col) or ('Prob ' in col) or ('Unnamed' in col):\n        del df_2[col]\nprev_game_dict = {OG_cols[i]: prev_game_cols[i] for i in range(len(OG_cols))}\ndel prev_game_dict['Home/Away']\ndf_2 = df_2.rename(columns = prev_game_dict, inplace = False)\n#Combining and making probabilities into percentages\nArsenal_2021_df = pd.concat([df_2, df], axis = 1)\nArsenal_2021_df = Arsenal_2021_df.drop(index = 0)\nArsenal_2021_df['% Win'] = 100*Arsenal_2021_df['Prob Win']\nArsenal_2021_df['% Draw']= 100*Arsenal_2021_df['Prob Draw']\nArsenal_2021_df['% Lose']= 100*Arsenal_2021_df['Prob Lose']\nArsenal_2021_df['Home/Away'] = Arsenal_2021_df['Home/Away'].apply(h_a_numbs)","metadata":{"execution":{"iopub.status.busy":"2021-08-03T15:59:48.618504Z","iopub.execute_input":"2021-08-03T15:59:48.618885Z","iopub.status.idle":"2021-08-03T15:59:48.743499Z","shell.execute_reply.started":"2021-08-03T15:59:48.618853Z","shell.execute_reply":"2021-08-03T15:59:48.742446Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Get rid of the columns that are strings and define our train/test split. I've not adjusted all columns to numerical values yet - only the home/away column as seen above.","metadata":{}},{"cell_type":"code","source":"#Predicting 2020/21 season results from 2014-2020 results\ncols = []\nfor col in list(B_df_3.columns):\n    if type(B_df_3[col][2]) != str:\n        cols.append(col)\ncols.remove(cols[43])\ncols.remove(cols[43])\ncols.remove(cols[43])\ncols = cols[:-3]\n#cols.remove(cols[-2])\n#cols.remove(cols[-2])\n#cols.remove(cols[-2])\npred = ['% Win', '% Draw', '% Lose']\ntrain_y = B_df_3['% Win']\ntrain_X = B_df_3[cols]\nval_y = Arsenal_2021_df['% Win']\nval_X = Arsenal_2021_df[cols]","metadata":{"execution":{"iopub.status.busy":"2021-08-03T16:05:47.811334Z","iopub.execute_input":"2021-08-03T16:05:47.81181Z","iopub.status.idle":"2021-08-03T16:05:47.836817Z","shell.execute_reply.started":"2021-08-03T16:05:47.811773Z","shell.execute_reply":"2021-08-03T16:05:47.835763Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Scaling the model by using the Standard Scaler function as well as Principal Component Analysis (PCA)","metadata":{}},{"cell_type":"code","source":"#Applying Scaling to the model, preprocessing the data\nsc = StandardScaler()\nsc.fit(train_X)\nscaled_train_X = sc.transform(train_X)\nscaled_val_X = sc.transform(val_X)","metadata":{"execution":{"iopub.status.busy":"2021-08-03T16:06:47.392807Z","iopub.execute_input":"2021-08-03T16:06:47.393248Z","iopub.status.idle":"2021-08-03T16:06:47.424086Z","shell.execute_reply.started":"2021-08-03T16:06:47.393207Z","shell.execute_reply":"2021-08-03T16:06:47.422939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pca = PCA(n_components = 30)\npca.fit(scaled_train_X)\nX_pca = pca.transform(scaled_train_X)\nX_pca_val = pca.transform(scaled_val_X)","metadata":{"execution":{"iopub.status.busy":"2021-08-03T16:10:08.491615Z","iopub.execute_input":"2021-08-03T16:10:08.492006Z","iopub.status.idle":"2021-08-03T16:10:08.57892Z","shell.execute_reply.started":"2021-08-03T16:10:08.49197Z","shell.execute_reply":"2021-08-03T16:10:08.577817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finally - we get on to a Neural Network! Batch Normalization and Dropout are applied here to eliminate a previous problem of overfitting. Other numbers I optimised through listing through certain values and seeing how that affected the results. Our Loss function ends up being valued at around 22-25 each time on the evaulation stage on the test data.","metadata":{}},{"cell_type":"code","source":"# Let's try making a Neural Network! (With PCA)\nearly_stopping = EarlyStopping(\n    min_delta=0.001,\n    patience=20, \n    restore_best_weights=True,\n)\n\nNN_model_pca = keras.Sequential([\n    layers.Dense(8, activation='relu', input_shape=[30]),\n    layers.Dropout(0.2),\n    layers.BatchNormalization(),\n    layers.Dense(8, activation='relu'),\n    layers.Dropout(0.2),\n    layers.BatchNormalization(),\n    layers.Dense(8, activation='relu'),\n    layers.Dropout(0.2),\n    layers.BatchNormalization(),\n    layers.Dense(1),\n])\n\nNN_model_pca.compile(\n    optimizer='adam',\n    loss='mae',\n)\n\nhistory = NN_model_pca.fit(\n    X_pca, train_y,\n    validation_data=(X_pca_val, val_y),\n    batch_size=190,\n    epochs=500,\n    verbose = 1,\n    shuffle = True\n)","metadata":{"execution":{"iopub.status.busy":"2021-08-03T16:12:15.50935Z","iopub.execute_input":"2021-08-03T16:12:15.509954Z","iopub.status.idle":"2021-08-03T16:13:08.405747Z","shell.execute_reply.started":"2021-08-03T16:12:15.509902Z","shell.execute_reply":"2021-08-03T16:13:08.404666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finally - we graph the history!","metadata":{}},{"cell_type":"code","source":"history_df = pd.DataFrame(history.history)\nhistory_df.loc[:, ['loss', 'val_loss']].plot();","metadata":{"execution":{"iopub.status.busy":"2021-08-03T16:13:47.360989Z","iopub.execute_input":"2021-08-03T16:13:47.361403Z","iopub.status.idle":"2021-08-03T16:13:47.599282Z","shell.execute_reply.started":"2021-08-03T16:13:47.361366Z","shell.execute_reply":"2021-08-03T16:13:47.59808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Thank you very much for reading, I would love to know your thoughts for how to optimise/ improve things in the comments! I am currently a student doing this in his spare time so I am open to any advice and feedback.","metadata":{}}]}