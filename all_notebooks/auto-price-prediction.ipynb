{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Car Price Prediction:","metadata":{"id":"fMLHD8rRF1md"}},{"cell_type":"markdown","source":"Download dataset from this link:\n\nhttps://www.kaggle.com/hellbuoy/car-price-prediction","metadata":{"id":"OF9OdlmlF1me"}},{"cell_type":"markdown","source":"# Problem Statement::","metadata":{"id":"DEXBp_v-F1mf"}},{"cell_type":"markdown","source":"A Chinese automobile company Geely Auto aspires to enter the US market by setting up their manufacturing unit there and producing cars locally to give competition to their US and European counterparts.\n\nThey have contracted an automobile consulting company to understand the factors on which the pricing of cars depends. Specifically, they want to understand the factors affecting the pricing of cars in the American market, since those may be very different from the Chinese market. The company wants to know:\n\nWhich variables are significant in predicting the price of a car\nHow well those variables describe the price of a car\nBased on various market surveys, the consulting firm has gathered a large data set of different types of cars across the America market.\n\n# task::\nWe are required to model the price of cars with the available independent variables. It will be used by the management to understand how exactly the prices vary with the independent variables. They can accordingly manipulate the design of the cars, the business strategy etc. to meet certain price levels. Further, the model will be a good way for management to understand the pricing dynamics of a new market.","metadata":{"id":"sqbmOQUuF1mf"}},{"cell_type":"markdown","source":"# WORKFLOW ::","metadata":{"id":"WQlK0ECoF1mf"}},{"cell_type":"markdown","source":"1.Load Data\n\n2.Check Missing Values ( If Exist ; Fill each record with mean of its feature )\n\n3.Split into 50% Training(Samples,Labels) , 30% Test(Samples,Labels) and 20% Validation Data(Samples,Labels).\n\n4.Model : input Layer (No. of features ), 3 hidden layers including 10,8,6 unit & Output Layer with activation function relu/tanh (check by experiment).\n\n5.Compilation Step (Note : Its a Regression problem , select loss , metrics according to it)\n6.Train the Model with Epochs (100) and validate it\n\n7.If the model gets overfit tune your model by changing the units , No. of layers , activation function , epochs , add dropout layer or add Regularizer according to the need .\n\n8.Evaluation Step\n\n9.Prediction","metadata":{"id":"TttJ28M5F1mg"}},{"cell_type":"markdown","source":"# 1. Load Data\nLoading and immediately shuffeling Data frame.","metadata":{"id":"PrYIeESrF1mg"}},{"cell_type":"code","source":"# Prediction Model developed by:\n# Khurram Nazir\n# khurram.deutsch@yahoo.com\n# Munich,Germany.\n#Comments and suggestions are welcomed.\n#\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\n#\nimport io\n#from google.colab import files\n#uploaded_file = files.upload()\n\ndf = pd.read_csv('../input/car-price-prediction/CarPrice_Assignment.csv')\n#df = pd.read_csv(\"C:/Users/khurr/Documents/GitHub/AI-Engineering/PIAIC/Quarter-2/DeepLearning/Car_Price_Prediction_Assignment/CarPrice_Assignment.csv\",sep=',')\n#df = pd.DataFrame(pd.read_csv(io.BytesIO(uploaded_file['CarPrice_Assignment.csv']),sep=','))","metadata":{"id":"Sjv1jv86F4qX","outputId":"1d2ecf6f-0254-480d-c92c-b45e60afcc7f","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndf = df.sample(frac=1) #Shuffeling DF.\ndf","metadata":{"id":"Pmp96Z8YF1mg","outputId":"7f73b5c3-9b2d-4a05-906d-4ad16c9b4343","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def df_lookslike(v_df):\n    #---------------------------------------How dataframe looks like?\n    v_df.info()\n    print(v_df.head(5))\n    total_cells=np.product(v_df.shape)\n    num_col = [i for i in v_df.columns if (v_df[i].dtype=='int64' or v_df[i].dtype=='float64')]\n    print(v_df[num_col].describe().loc[['min','max', 'mean','50%'],:]) #How big is Messy data?\n    missing_Values=v_df.isnull().sum()\n    print(missing_Values)\n    total_missing=missing_Values.sum()\n\n    #Percent of Missing data\n    print(\"Percent of data is missing:\",((total_missing/total_cells) * 100))","metadata":{"id":"g-1lIkPBF1mh","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Check Missing Values\n( If Exist ; Fill each record with mean of its feature )\n\nI found there is NO missing/NULL data.","metadata":{"id":"wL4RDLrTF1mh"}},{"cell_type":"code","source":"df_lookslike(df)# How DF looks like?","metadata":{"id":"keDUgcetF1mi","outputId":"c6e08452-9ee2-40e7-f847-c85878404bcc","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preparing Tensor / Vectorization","metadata":{"id":"uzNlmZtLF1mi"}},{"cell_type":"code","source":"#Following libarary is performing OneHotEncoding. BEST thing: Its also modifying data types of target columns.\nfrom sklearn.preprocessing import LabelEncoder\nlabel_enc=LabelEncoder()\n\n\n#Encoding scheme HotEncoding by using SKlearn. Let's seee further Heatmap to drop unneeded columns.\ndf['CarName']=label_enc.fit_transform(df['CarName'])\ndf['fueltype']=label_enc.fit_transform(df['fueltype'])\ndf['aspiration']=label_enc.fit_transform(df['aspiration']) \ndf['doornumber']=label_enc.fit_transform(df['doornumber'])\ndf['carbody']=label_enc.fit_transform(df['carbody'])\ndf['drivewheel']=label_enc.fit_transform(df['drivewheel'])\ndf['enginelocation']=label_enc.fit_transform(df['enginelocation'])\ndf['enginetype']=label_enc.fit_transform(df['enginetype'])\ndf['cylindernumber']=label_enc.fit_transform(df['cylindernumber'])\ndf['fuelsystem']=label_enc.fit_transform(df['fuelsystem'])\n\ndf_lookslike(df)# Post OnehotEncoding let see, how DF looks like?\n","metadata":{"id":"gBjpbJvHF1mi","outputId":"a6d05924-7b82-4969-b058-dc72ae4c611d","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\ncorrmat = df.corr(method='pearson',min_periods=5) #standard correlation coefficient,\ntop_corr_features = corrmat.index\nplt.figure(figsize=(30,30))\ng=sns.heatmap(df[top_corr_features].corr(),annot=True,cmap=\"RdYlGn\")","metadata":{"id":"gQXrzJWSF1mi","outputId":"4fc28895-4ca9-4cec-9b22-46a40c437aaf","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#I'm going to drop few features/Indirect variable which has -ve correlation/association with Price.\ndf.drop(columns=['highwaympg','citympg','car_ID','CarName','symboling','enginelocation'], inplace=True)\ndf.head(10)","metadata":{"id":"BKdNG0XkF1mj","outputId":"a3289e52-3ee4-4e90-b1b4-ef1f7a41cb48","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Split Data.\n\n50% Training(Samples,Labels) , 30% Test(Samples,Labels) and 20% Validation Data(Samples,Labels).\n","metadata":{"id":"oExDMNaYF1mj"}},{"cell_type":"code","source":"\nfrom sklearn.model_selection import train_test_split\n\nY=(df.loc[:,df.columns=='price']) #Lets take Dependent Variable/Target in a serpate df i.e X.\nX=(df.loc[:,df.columns!='price']) #Lets take Independent Variables in a serpate df i.e Y.\n\n\nx_train_50,X_remaining,y_train_50,Y_remaining=train_test_split(X,Y,test_size=0.5,random_state=0)\nx_test_30,x_valid_20,y_test_30,y_valid_20=train_test_split(X_remaining,Y_remaining,test_size=0.7,random_state=0)\n\nprint(\"Training Data-X [50%]:\\t\", x_train_50.shape)\nprint(\"Test Data-X [30%]:\\t\", x_test_30.shape)\nprint(\"Validate Data-X [20%]:\\t\", x_valid_20.shape)","metadata":{"id":"eKB-0tT4F1mj","outputId":"fd9c4e75-b7bd-41bd-c463-f0a6af2b553f","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Data Normalization\n","metadata":{"id":"-b8NLkIYD43U"}},{"cell_type":"code","source":"\n#Training data normalization manually.\n#Its not encouraged to Normalize all DF. Rather, to only qualified columns. Need more discussion/comments.\n\nmean = x_train_50.iloc[: , 0:14].mean(axis=0)\nx_train_50.iloc[: , 0:14] -= mean\nstd = x_train_50.iloc[:, 0:14].std(axis=0)\nx_train_50.iloc[: , 0:14] /= std\nx_train_50\n\n","metadata":{"id":"yy02bpmohqHr","outputId":"3aeaccb5-f39e-49d4-fb93-372c9a205436","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Validation data normalization\nval_mean = x_valid_20.iloc[:, 0:14].mean(axis=0)\nval_std = x_valid_20.iloc[:, 0:14].std(axis=0)\nx_valid_20.iloc[:, 0:14] -= val_mean\nx_valid_20.iloc[:, 0:14] /= val_std\nx_valid_20","metadata":{"id":"-lq0m_5WIsD_","outputId":"0cf097a1-9233-433e-aff4-244a319a6e52","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Test data normalization\ntest_mean = x_test_30.iloc[:, 0:14].mean(axis=0)\ntest_std =  x_test_30.iloc[:, 0:14].std(axis=0)\nx_test_30.iloc[:, 0:14] -= test_mean\nx_test_30.iloc[:, 0:14] /= test_std\nx_test_30","metadata":{"id":"vrKYN6WZIvc4","outputId":"33ca7921-d43c-4047-8dfe-532c40bf2599","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. Model :\n\nInput Layer (No. of features ), 3 hidden layers including 10,8,6 unit & Output Layer with activation function relu/tanh (check by experiment)","metadata":{"id":"q5mkYxH3F1mj"}},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras.layers import Dropout\n\nnetwork = Sequential()\n\n\n\nnetwork.add(layers.Dense(16, activation='relu', kernel_regularizer =regularizers.l2(0.02),   input_shape=(x_train_50.shape[1],)))\nnetwork.add(layers.Dense(14, activation='relu', kernel_regularizer =regularizers.l2(0.02)))\nnetwork.add(layers.Dense(8, activation='relu', kernel_regularizer =regularizers.l2(0.02)))\nnetwork.add(layers.Dense(6, activation='relu', kernel_regularizer =regularizers.l2(0.002)))\n\n\n\n#I'm passing 01-Neuron but do not specify Activation function as its Regression.\nnetwork.add(layers.Dense(1))\n","metadata":{"id":"h2f-qnryF1mj","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 6.Compilation\nSteps(Note : Its a Regression problem , select loss , metrics according to it)\n","metadata":{"id":"7xva6cVtF1mk"}},{"cell_type":"code","source":"from tensorflow import keras\n#Preparing parameters for Optimizer.\nopt = keras.optimizers.RMSprop() #I want lowest learning rate as higher accuracy required.\nnetwork.compile(optimizer=opt, loss='mean_absolute_error', metrics='mae')","metadata":{"id":"80DpAsScL9Dw","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 7.Train Model\nwith Epochs (100) and validate it\n\n","metadata":{"id":"KlkVDQQRF1mk"}},{"cell_type":"code","source":"history = network.fit(x_train_50,y_train_50, batch_size=16, verbose=0, epochs=300, validation_data=(x_valid_20, y_valid_20))","metadata":{"id":"GY4w2zCZgToI","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs = range(1, 301)\ntrain_mae = history.history['mae']\nval_mae = history.history['val_mae']","metadata":{"id":"MlC6ldAVHU-6","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(epochs, train_mae, 'r', label='Training MAE')\nplt.plot(epochs, val_mae, 'g', label='Validation MAE')\nplt.title('Training vs Validation Loss ')\nplt.xlabel('Epochs')\nplt.ylabel('MAE')\nplt.legend()\nplt.show()","metadata":{"id":"BwUDcl7dHVKA","outputId":"2b39054f-9fcc-43d2-ac4f-58426fd83c62","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_loss = history.history['loss']\nval_loss = history.history['val_loss']","metadata":{"id":"BUQlhgL3ImEE","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(epochs, train_loss, 'r', label='Training Loss')\nplt.plot(epochs, val_loss, 'g', label='Validation Loss')\nplt.title('Training vs Validation loss ')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","metadata":{"id":"uR5gPp9KImGy","outputId":"0c0180b4-2996-4d73-c081-e0962a1eccf9","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Predicted Price:\\n',network.predict(x_test_30))","metadata":{"id":"DdE_UcZ0ImJs","outputId":"46e48378-bcd8-4f93-95c1-de5f3e8bc021","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 8.Evaluation Step","metadata":{"id":"npBbpkT1F1ml"}},{"cell_type":"code","source":"test_mse_score, test_mae_score  =network.evaluate(x_test_30, y_test_30)","metadata":{"id":"Mc_ipjFyJIU-","outputId":"bc33de1f-55eb-4fc2-9966-2a66d6f753a9","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Please comment specially w.r.t Normalization.\n#khurram.deutsch@yahoo.com","metadata":{"id":"VUDDQoEUn4y6","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}