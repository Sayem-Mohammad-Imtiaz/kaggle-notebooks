{"cells":[{"metadata":{"_uuid":"eda0664a628827ca84fddb602fbeb645abb70078","_cell_guid":"fb160fd6-9e23-47a5-9c36-4adf8378fd7d"},"cell_type":"markdown","source":"# Introduction\n- Intuitively we know there is a significant cost with employee turnover. The basic ideas behind this project are: can we accurately predict employee attrition, what are the most important features that contribute to attrition, and is there a method of reducing the cost of employee turnover? \n    - Structure:\n        - Cleaning and EDA\n        - Survival Analysis of an employee's tenure with the company\n        - Machine Learning for predicting attrition\n        - Developing a cost function for our models\n\n### Main Goals\n- Find useful information through Exploratory Analysis\n    - Using survival analysis to speculate on important features for attrition\n- Predict employee attrition\n    - Target: Yes (16.2%) vs. No (83.88%)\n    - Benchmark (predicting every employee as No Attrition): 83.88% accuracy\n    - Metrics: Accuracy, False Negative Rate\n- Preemptively combat attrition and try to minimize cost\n\n### Findings\n- We were able to find the most important features for attrition (all of them make intuitive sense)\n- Predicted employee attrition with 91% accuracy\n- By giving a bonus for employees that were at risk of attrition, we were able to save IBM a significant amount of money\n    - The cost function used very conservative estimates (as a worst case scenario) and we we're still able to show savings","outputs":[],"execution_count":null},{"metadata":{"_uuid":"67cb129d1bbcb81ae2a279fba6915fd1a86c21cd","_cell_guid":"6fcfa776-dec0-4af5-8f40-f049712267ff"},"cell_type":"markdown","source":"# Importance of keeping quality employees\n- Cost of advertising job opening, interview process, training, lost productivity, etc.\n    - Studies show it can take up 2 years for a new hire to reach the same productivity level as the old employee for certain postitions\n- Total cost for losing an employee can be anywhere between 6-24 months of their salary (keep this in mind when we try to reduce cost for the company)\n    - e.g. an employee making \\$50,000/year will cost \\$25,000 - \\$100,000 to replace\n        - More research on the cost of replacing IBM employees will need further analysis","outputs":[],"execution_count":null},{"metadata":{"_uuid":"438de84a019d700a26dcf002ba6a75abef1fce9e","_cell_guid":"5944cfb2-6382-48ab-b65e-14ed64a720db"},"cell_type":"markdown","source":"### Disclaimer\n- It is an ARTIFICIAL dataset\n    - However, we can still gain insights and I believe the process used is repeatable with a real world dataset\n- I am still new to the Data Science world (this is my first contribution to Kaggle), so any feedback is much appreciated!","outputs":[],"execution_count":null},{"metadata":{"collapsed":true,"_uuid":"13da8d1b7e45431a2247a2ea64e2f906fb9618e0","_cell_guid":"fef34065-1cfc-4ba3-9470-01fd5bf881e1","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport seaborn as sns\n\nfrom IPython.display import display\nfrom scipy import stats\n\nimport warnings\n%matplotlib inline \nnp.random.seed(42)\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"3a3feb07b184a344de9adcc7e6a7bbdf40e4145f","_cell_guid":"89258ccd-ef37-4638-ad5c-1f7fb3f6f0ad","trusted":true},"cell_type":"code","source":"hr_df = pd.read_csv('../input/WA_Fn-UseC_-HR-Employee-Attrition.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"707a47cabff7f7731e38beb0591a447c5f9946ff","_cell_guid":"b371b61c-2319-4559-8954-4689c1ef2774","trusted":true},"cell_type":"code","source":"hr_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2fd756536a26dbeb8103b2884ba711fd53b007a3","_cell_guid":"c9f92429-5d6f-472b-acd0-b95f96aec9b7"},"cell_type":"markdown","source":"> # Exploratory Analysis","outputs":[],"execution_count":null},{"metadata":{"_uuid":"f76283c29f54048def8cd0ee70b86ab45a890999","_cell_guid":"1ca11e69-f79e-46aa-90ee-b7f1f9df21e2","trusted":true},"cell_type":"code","source":"hr_df.info()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"1e4436cd35354b03e067f3227437e8e67a185acd","_cell_guid":"057c4691-5f70-4619-bbb8-d98f1e4b6a88"},"cell_type":"markdown","source":"### There are no missing values and most of the features are the right data type\n### Changing some numerical data to categorical\n- Education: 1 = 'Below College', 2 = 'College', 3 = 'Bachelor', 4 = 'Master', 5 = 'Doctor'\n- EnvironmentSatisfaction: 1 = 'Low', 2 = 'Medium', 3 = 'High', 4 = 'Very High'\n- JobInvolvement: 1 = 'Low', 2 = 'Medium', 3 = 'High', 4 = 'Very High'\n- JobSatisfaction: 1 = 'Low', 2 = 'Medium', 3 = 'High', 4 = 'Very High'\n- PerformanceRating: 1 = 'Low', 2 = 'Good', 3 = 'Excellent', 4 'Outstanding'\n- RelationshipSatisfaction: 1 = 'Low', 2 = 'Medium', 3 = 'High', 4 = 'Very High'\n- WorkLifeBalance: 1 = 'Bad', 2 = 'Good', 3 = 'Better', 4 = 'Best'","outputs":[],"execution_count":null},{"metadata":{"collapsed":true,"_uuid":"bba0df1844e3103d7721d87c333a07d02a6e3fad","_cell_guid":"6984c415-2d11-4c4a-9563-db6753da4f4b","trusted":true},"cell_type":"code","source":"# Changing numeric values to corresponding categorical values\nhr_df['Education'] = hr_df['Education'].map({1: 'Below College', 2: 'College', 3: 'Bachelor', 4: 'Masters', 5: 'Doctor'})\nhr_df['EnvironmentSatisfaction'] = hr_df['EnvironmentSatisfaction'].map({1: 'Low', 2: 'Medium', 3: 'High', 4: 'Very High'})\nhr_df['JobInvolvement'] = hr_df['JobInvolvement'].map({1: 'Low', 2: 'Medium', 3: 'High', 4: 'Very High'})\nhr_df['JobSatisfaction'] = hr_df['JobSatisfaction'].map({1: 'Low', 2: 'Medium', 3: 'High', 4: 'Very High'})\nhr_df['RelationshipSatisfaction'] = hr_df['RelationshipSatisfaction'].map({1: 'Low', 2: 'Medium', 3: 'High', 4: 'Very High'})\nhr_df['PerformanceRating'] = hr_df['PerformanceRating'].map({1: 'Low', 2: 'Good', 3: 'Excellent', 4: 'Outstanding'})\nhr_df['WorkLifeBalance'] = hr_df['WorkLifeBalance'].map({1: 'Bad', 2: 'Good', 3: 'Better', 4: 'Best'})","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c895589f7593e16aeaae773d2c0cf1ff0145e5e5","_cell_guid":"d6677791-d5bc-44f5-ab44-a6662ff2c9cc"},"cell_type":"markdown","source":"#### We will drop EmployeeCount, StandardHours, and EmployeeNumber because they are not informative","outputs":[],"execution_count":null},{"metadata":{"collapsed":true,"_uuid":"96c413a039ed60253e0a3389e5f33988b29ffab3","_cell_guid":"169277b6-6402-4d87-9fa4-4b4d1d95cfc7","trusted":true},"cell_type":"code","source":"hr_df = hr_df.drop(['EmployeeCount', 'StandardHours', 'EmployeeNumber', 'Over18'], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f8cbd731ed3e5e7f25aa9f5eef65b99083cb40a9","_cell_guid":"7ca22a1b-68df-49e6-879b-b9104483bb6b"},"cell_type":"markdown","source":"### We will make data frames for both numerical and categorical data to make EDA easier","outputs":[],"execution_count":null},{"metadata":{"collapsed":true,"_uuid":"fb6c5b98e2649edccae28ce1966a0e87b752d4d4","_cell_guid":"18032afa-4c92-48c5-9ae4-b6f5a9141450","trusted":true},"cell_type":"code","source":"#making categorical and numerical data frames\nhr_categorical = []\nhr_numerical = []\nfor column in hr_df:\n    if type(hr_df[column][1]) == str:\n        hr_categorical.append(column)\n    \n    else:\n        hr_numerical.append(column)\n        \nnumerical_df = hr_df[hr_numerical]\ncategorical_df = hr_df[hr_categorical]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"aabd76c77e23deb855de5f7599c393f13a6c1d38","_cell_guid":"02afb716-2275-4226-bb40-5acafd0bb862","trusted":true},"cell_type":"code","source":"# histograms of the numerical data\nfig = plt.figure(figsize = (15,30))\ni = 0\n\nfor column in numerical_df:\n    i += 1\n    fig.add_subplot(6,3,i)\n    plt.hist(numerical_df[column])\n    plt.title(column)\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"bc4deee811a122a74b220f6e0602055cfb74db13","_cell_guid":"bd315de9-3365-463f-b497-8bb2e2fe2b39","trusted":true},"cell_type":"code","source":"hr_df['Attrition'] = hr_df['Attrition'].map({'Yes': 1, 'No': 0})","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1a7f74dec6bd306e987757c96c83b9d6d02e96a0","_cell_guid":"61c7b317-8665-455e-a623-84ed8fb70375","trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize = (30,40))\ni = 0\n\nfor col in categorical_df:\n    i += 1\n    fig.add_subplot(5,3,i)\n    sns.countplot(categorical_df[col])\n    plt.xticks(rotation=35, fontsize = 20)\n    plt.title(col, fontsize = 20)\n    \nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e0dba031fc9cb17a960ce4d9c0ee26bf53bdf6b3","_cell_guid":"7b107887-f315-4ae1-9681-5d0692dc7783"},"cell_type":"markdown","source":"### We will now look at the correlations between the numerical features","outputs":[],"execution_count":null},{"metadata":{"_uuid":"ccd65933f5e242d9ddcff6969472a53e7ffda436","_cell_guid":"c8af446c-4445-4c9f-b457-c8d37aff492a","trusted":true},"cell_type":"code","source":"cor = numerical_df.corr()\nplt.figure(figsize = (15,15))\nsns.heatmap(cor, annot = True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2f6854c7e08573b917f9b1fb24793918eaf81e66","_cell_guid":"7a95adfa-86b3-4442-be1d-284bb24f850d"},"cell_type":"markdown","source":"### Since that looks clumsy, we will show a heat map with the highest correlations highlighted","outputs":[],"execution_count":null},{"metadata":{"_uuid":"bff79c8ae34767a2a33c07590c0b5fe24375f108","_cell_guid":"22d40fb4-5018-427f-8c01-7089045ed3b8","trusted":true},"cell_type":"code","source":"plt.figure(figsize = (10,8))\nbig_cor = cor.where(abs(cor) > .6)\nsns.heatmap(big_cor.replace(np.nan, 0))\nplt.title('Correlation Heat Map with High Correlations Highlighted')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"faf1517c9bf66b20b8da7a184b387d57e47e5b4e","_cell_guid":"85e4dfdf-9ac8-4995-9df3-0950074c5965"},"cell_type":"markdown","source":"# Summary of Data\n- Very clean dataset\n    - No missing values and everything is the right data type\n- Not a whole lot of high correlations across the board\n    - Things that deal with \"Years\" are obviously correlated\n- A lot of the numerical features are skewed right\n- There are only two classes of performances of employees (Excellent vs. Outstanding)\n    - We can reasonably assume that the reason behind their attrition is VOLUNTARILY leaving IBM\n        - Meaning there will be a cost of losing an employee\n- Class inbalances in Attrition","outputs":[],"execution_count":null},{"metadata":{"_uuid":"dc42c4544c7a10667a954e99ff0b38176d107d92","_cell_guid":"b079320f-a176-432f-944d-22d1706afb40"},"cell_type":"markdown","source":"### Moving forward\n- We will use survival analysis to speculate on important features for attrition\n\n## Survival Analysis\n- Analyzing the expected duration of time until one or more events happen\n    - Typical example: Use this analysis in clinical trials to test the effect of a certain treatment on survival time\n- Kaplan-Meier Estimator: $\\hat S(t) = \\prod _{t_{i} \\leq t} \\big ( 1 - \\frac{d_i}{n_i} \\big )$ \n    - Where $d_i$ is the number of exits and $n_i$ is the total number of individuals still at the company","outputs":[],"execution_count":null},{"metadata":{"collapsed":true,"_uuid":"becd5ae6d0b00024a81c6c0a1fb2a472fc5fd751","_cell_guid":"5518212b-33a4-4a5a-b8a9-bc0de3143f17"},"cell_type":"markdown","source":"### We saw that there were some outliers in YearsAtCompany\n- Let's explore this further","outputs":[],"execution_count":null},{"metadata":{"_uuid":"e206286d50248403c92bab78b7db0a1ca296f9fe","_cell_guid":"cc468e8c-b99a-44a6-a254-acbef1260647","trusted":true},"cell_type":"code","source":"sns.boxplot(hr_df['YearsAtCompany'])\nplt.title('Box Plot for Years at Company')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"d8ce43d439ac9ef5c48cbc31a77a0aa2ba7ad8c8","_cell_guid":"1fc2d102-84c5-4056-a3d2-a8fb4bd4ec5c","trusted":true},"cell_type":"code","source":"threshold = np.std(hr_df['YearsAtCompany']) * 3 # 3 std above mean\nhr_df = hr_df[hr_df['YearsAtCompany'] < np.mean(hr_df['YearsAtCompany']) + threshold]","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"7b15acc7c6071f0630832af2904bdb8a9a44f0dc","_cell_guid":"ff9c5b11-79ea-4dfc-abad-e35b4f78ee38"},"cell_type":"markdown","source":"### 25 observations are above 3 standard deviations above the mean\n- ~1.7% of data\n- I am comfortable removing these observations","outputs":[],"execution_count":null},{"metadata":{"_uuid":"0d45dafbf1e2d93c27c72299c09c6055d0492991","_cell_guid":"69cb5b43-2dca-461a-a8a6-b16dd48eea0c","trusted":true},"cell_type":"code","source":"from lifelines import KaplanMeierFitter","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0c35fe9a77ffefa50b2d8ab0338e237f5fba8751","_cell_guid":"e1180d3b-f7ed-47ca-8d22-1a51a5777c2f"},"cell_type":"markdown","source":"### Could not import the lifelines package...\n- Any help would be appreciated\n- If you'd like to see this analysis, it is on my GitHub at https://github.com/eprentice30/hr_analytics/blob/master/IBM_hr_analytics/01_survival_analysis.ipynb\n\n### Anyways, the important stuff is the likely important features for attrition\n\n    - Overtime\n    - JobSatisfaction\n    - MaritalStatus\n    - Age\n    - MonthlyIncome\n    - EnvironmentSatisfaction\n    - StockOptionLevel\n    - NumCompaniesWorked\n- We will keep these features in mind while we try to validate the most important features for attrition","outputs":[],"execution_count":null},{"metadata":{"collapsed":true,"_uuid":"56b0c28c9f58e9147719cc42feb633a61965aff2","_cell_guid":"9fba925c-c390-4081-a5e5-22d62eeeec5f"},"cell_type":"markdown","source":"## Predicting Attrition","outputs":[],"execution_count":null},{"metadata":{"_uuid":"28626195c79885677143ca57e41963f7b7f0d763","_cell_guid":"998c64c1-a5f6-454f-9f0c-8bf5d7a32949","trusted":true},"cell_type":"code","source":"sns.countplot(hr_df['Attrition'])\nplt.text(x = -.15, y = 800, s = str(np.round(1233/1470.0, 4) * 100) + '%', fontsize = 16)\nplt.text(x = .85, y = 100, s = str(np.round(237/1470.0, 4) * 100) + '%', fontsize = 16)\nplt.xticks(np.arange(2),('No', 'Yes'))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"3ebed586ea3552184470d623e9cae1753ed1b728","_cell_guid":"37923639-4fac-4459-b67b-8faca129c702","trusted":true},"cell_type":"code","source":"from sklearn.metrics import auc, roc_curve, roc_auc_score, classification_report, confusion_matrix, \\\n                        precision_recall_fscore_support","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a696ef45f9e7d5ba4d1588edf2c8a3f85b8aedd3","_cell_guid":"154ea487-7a58-4681-80cf-68ac146aa422"},"cell_type":"markdown","source":"### Metrics to Consider\n- Accuracy and False Negative Rate\n    - False negative rate because this will be most costly for the company (predicting no attrition when there is attrition)\n- Benchmark accuracy score\n    - Accuracy = 83.88% (predicting every employee as leaving)","outputs":[],"execution_count":null},{"metadata":{"_uuid":"ed713edbe0a2d5e1e503f982cc914eb6b9e73f34","_cell_guid":"d7253a47-2a30-475d-8246-f821e672b2a1"},"cell_type":"markdown","source":"## Preprocessing\n### Getting Dummy Variables","outputs":[],"execution_count":null},{"metadata":{"collapsed":true,"_uuid":"4a4efddcf733a0b14ac0abaeabb6a20f17f0acb1","_cell_guid":"e4ac60c4-3998-40ed-b3d8-14666c8dbe21","trusted":true},"cell_type":"code","source":"hr_df = pd.get_dummies(hr_df, drop_first = True) #to avoid multicolinearity","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"301191431f8de36c1c15a57ffcade162d1c2b946","_cell_guid":"eb2bc15d-4583-4001-ae16-8c8e1e28b8e7"},"cell_type":"markdown","source":"### Setting Up Feature and Target Matrix","outputs":[],"execution_count":null},{"metadata":{"collapsed":true,"_uuid":"5d7b29cb4fd45bbb0e0d5150e9c171dadb6e2fdd","_cell_guid":"0b97ce54-5a6b-419a-9ad0-9a78355cf066","trusted":true},"cell_type":"code","source":"X = hr_df.drop('Attrition', axis = 1)\ny = hr_df['Attrition']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6d0d27ec6e0b9b23a2fdd5d24b7a531c380aca67","_cell_guid":"12be844f-28dd-4e63-948c-b2336a13c4ff"},"cell_type":"markdown","source":"### Scaling","outputs":[],"execution_count":null},{"metadata":{"_uuid":"192927824cee4d8f43b6ba68bf510b99e787e404","_cell_guid":"bed88b63-3202-4b31-9d0e-0a0dd920402d","trusted":true},"cell_type":"code","source":"from sklearn.cross_validation import train_test_split\nfrom sklearn.preprocessing import StandardScaler","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"753d7a38db4f09075808ceca61762394f9168ee7","_cell_guid":"178721d0-a8ac-4699-a403-0056cacf95ed"},"cell_type":"markdown","source":" ### Train-test split and scaling data","outputs":[],"execution_count":null},{"metadata":{"collapsed":true,"_uuid":"e2600cd3e58dfe86a782530395c109254adea8ab","_cell_guid":"505fdd1c-7e6b-4a5d-952a-36df2006136f","trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42)\n\n#scale the data\nscaler = StandardScaler()\n# Fit_transform\nX_train_scaled = scaler.fit_transform(X_train)\n# transform\nX_test_scaled = scaler.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"95af38cb60fe08c7dfeea71fd4f6dada3afbe3d9","_cell_guid":"ccfdc5df-665b-416b-8fd7-401194bd87c4"},"cell_type":"markdown","source":"### Now we will fit a bunch of classification models and see which performs the best","outputs":[],"execution_count":null},{"metadata":{"collapsed":true,"_uuid":"4a714bfa342e9e04b42dc14ff523c12e38920808","_cell_guid":"b3ee684f-c5d5-4126-990e-208f9ca8828d","trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import GridSearchCV","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e55baf43b1cb3ce7578a724b2879105a19346881","_cell_guid":"56803564-77c4-4de5-a69c-24d5b2afcd17"},"cell_type":"markdown","source":"## For the sake of space I will only show our top 2 performing models\n- XGBoost and an Artificial Neural Network\n\n### XGBoost","outputs":[],"execution_count":null},{"metadata":{"_uuid":"1623a34731f1bc657ec4020f9196eb305ecd7e98","_cell_guid":"e19ab3b0-eaa3-430e-b070-a8ea3d28f749","trusted":true},"cell_type":"code","source":"xgbparams = {\n    'max_depth':[1,3,5],\n    'learning_rate':[.1,.5,.7,.8],\n    'n_estimators':[25,50,100]\n}\n\nxgb_gs = GridSearchCV(XGBClassifier(), param_grid = xgbparams, cv=5, n_jobs=-1, verbose = 1)\nxgb_gs.fit(X_train_scaled, y_train)\nxgb_gs.best_params_","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9c329d97da5d6541879eb18e4c78b493e57916cc","_cell_guid":"d6ac93ae-ea75-4d58-a02a-c65663717c8e","trusted":true},"cell_type":"code","source":"print('Train acc =', xgb_gs.score(X_train_scaled, y_train))\nprint('Test acc = ', xgb_gs.score(X_test_scaled, y_test))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f247f46c24b02ec0e27413ceaab902771e00250","_cell_guid":"e83485f7-2d8b-402c-afb1-0737281bc357"},"cell_type":"markdown","source":"### Testing accuracy of 90%\n- Below is the confusion matrix and other metrics","outputs":[],"execution_count":null},{"metadata":{"_uuid":"1c87e4b020cf27e83a61473a59c0f72a9210c594","_cell_guid":"88cc5eda-6eaa-4520-94ef-a32926ae9721","trusted":true},"cell_type":"code","source":"y_pred = xgb_gs.predict(X_test_scaled)\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\nprint(classification_report(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"46bf3ea9a79274f8af17216ee2e72df34a1a6491","_cell_guid":"873e1143-97e8-41f2-a9f3-391073559521"},"cell_type":"markdown","source":"### Artificial Neural Network","outputs":[],"execution_count":null},{"metadata":{"collapsed":true,"_uuid":"89c8da312da2c767d31c475b40ea5fb9855c6f9b","_cell_guid":"65b4372d-f4ba-4b79-8e65-a10f5784b2ca","trusted":true},"cell_type":"code","source":"from sklearn.neural_network import MLPClassifier","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1660b61c6ab7979a75b1173ec17a1a33dde6d7ba","_cell_guid":"45881978-f5aa-4197-b4eb-7f1943fadfd6","trusted":true},"cell_type":"code","source":"mlpparams = {\n            'learning_rate': [\"constant\", \"invscaling\", \"adaptive\"],\n            'hidden_layer_sizes': [(30,), (60,), (50,), (40,)],\n            'alpha': [.1],\n            'activation': [\"logistic\", \"relu\", \"tanh\"]\n            }\n\nmlp_gs = GridSearchCV(MLPClassifier(), param_grid = mlpparams, cv = 5, verbose = 1)\nmlp_gs.fit(X_train_scaled, y_train)\nmlp_gs.best_params_","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e63f2ff9d4ab437f4be2e4368332c980afafb5cf","_cell_guid":"6585d1b5-ca36-4b92-a7bb-e0e762a07d08","trusted":true},"cell_type":"code","source":"print('Train acc =', mlp_gs.score(X_train_scaled, y_train))\nprint('Test acc =', mlp_gs.score(X_test_scaled, y_test))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3cdea8cd3805f7a22fcf955a9d89cccc6cd6ea84","_cell_guid":"65e010ab-1111-45c4-87b0-422b378305c2"},"cell_type":"markdown","source":"### Testing accuracy of 90.6%\n- Below is the confusion matrix and other metrics","outputs":[],"execution_count":null},{"metadata":{"_uuid":"d96e7a09461ea54e0e145216fd4b75413924900b","_cell_guid":"59cca10f-ccb2-4ca7-9c4b-43c0d12e26d5","trusted":true},"cell_type":"code","source":"y_pred = mlp_gs.predict(X_test_scaled)\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\nprint(classification_report(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6a7f6ff4386864d697da06bceb00ff0ebacc5f63","_cell_guid":"56096314-642a-4370-a62d-87a56ca94a84"},"cell_type":"markdown","source":"### Comparing Models","outputs":[],"execution_count":null},{"metadata":{"_uuid":"b9de67f54d503d6a35313d7e79da2825337a7d72","_cell_guid":"98fbc7be-6cb5-4d65-98c1-a02c2561f302"},"cell_type":"markdown","source":"| Model                            | Parameters                                                                                     | False Negative Rate | Train Acc | Test Acc |\n|----------------------------------|------------------------------------------------------------------------------------------------|---------------------|-------------|------------|\n| ANN                              | activation = logistic, alpha = 0.1, hidden_layer_sizes = (60,)  | 43%                 | 0.8957      | 0.9061      |\n| XGBoost                          | learning_rate = 0.8, max_depth = 1, n_estimates = 50                                             | 60%                 | 0.9058      | 0.9006     |\n| Logistic Regression (1st Deg poly) | c = 0.1, penalty = 'l2'                                                                         | 58%                 | 0.8938      | 0.8723     |\n| Logistic Regression (2nd Deg poly) | c  = 0.1, penalty = 'l2'                                                                        | 60%                 | 0.9437      | 0.8777     |\n| KNN                              | n_neighbors = 5                                                                                | 85%                 | 0.8675      | 0.8890     |\n| Random Forrest                   | max_depth = None, max_features = auto, n_estimates = 100                                         | 92%                 | 1.000       | 0.8696     |","outputs":[],"execution_count":null},{"metadata":{"_uuid":"38f014249e9225ab6cef7d263fd5a0e9d80321a3","_cell_guid":"0c1ddd07-6c44-419a-a1ae-a5661d7c9841"},"cell_type":"markdown","source":"### The top two models were our Artificial Neural Network and our XGBoost\n\n### Feature Importance","outputs":[],"execution_count":null},{"metadata":{"_uuid":"be9df13bff0d51b74da062e5bd51a575ca27a46f","_cell_guid":"da3cd01b-a345-4eba-bbe9-88264a8bc827","trusted":true},"cell_type":"code","source":"xgb = XGBClassifier(max_depth = 1, learning_rate = .8, n_estimators = 50)\nxgb.fit(X_train_scaled, y_train)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"f4334d04cb9396f721820998e0b021b160e2d474","_cell_guid":"e173c105-744a-4c5b-ae98-2cbeead24fcf","trusted":true},"cell_type":"code","source":"zipped = list(zip(X.columns, xgb.feature_importances_))\nzipped = pd.DataFrame(zipped)\nzipped = zipped.sort_values(by = 1)\nzipped = zipped.iloc[27:]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d0d45c4288c0490b0699bc0682a81d6e1314691f","_cell_guid":"a347afb8-50b8-480f-8fc3-d7e4637b5fbe","trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,10))\nplt.barh(np.arange(30), zipped[1],)\nplt.yticks(np.arange(30), (list(zipped[0])))\nplt.ylabel('Feature', fontsize=15)\nplt.xlim(xmin = .015, xmax = .14)\nplt.xlabel('Importance')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"11849185da99c5c04bc8c6c9213a66f70f7dcdc2","_cell_guid":"11ed9315-e541-4bf3-be62-de16affbe01d"},"cell_type":"markdown","source":"### We see that there is a lot of overlap with the important features found with Survival Analysis and XGBoost\n- We can be fairly confident in our findings of important features (especially since they all make intuitive sense)","outputs":[],"execution_count":null},{"metadata":{"_uuid":"da12c759014aed8247f3c074d1719208aff33199","_cell_guid":"9db40731-9c55-46b7-8fe3-e3eabdc219f6"},"cell_type":"markdown","source":"# Ok so we know what features are important for attrition and we are fairly accurate with our predictions (91%)\n\n# So now what?\n\n## We can do nothing and view attrition as an overhead cost","outputs":[],"execution_count":null},{"metadata":{"_uuid":"67aee8683e5cd423de238a22dae900378c4fe424","_cell_guid":"25ff1846-6bd4-41d4-968a-ab15fb17e7bd"},"cell_type":"markdown","source":"### Confusion Matrix\n|            | Predicted No | Predicted Yes |\n|------------|--------------|---------------|\n| Actual No  | True Negative       |    False Positive    |\n| Actual Yes |    False Negative   |     True Positive    |\n\n### Cost Matrix\n\n|            | Predicted No | Predicted Yes |\n|------------|--------------|---------------|\n| Actual No  | c = 0     |    c = 0    |\n| Actual Yes |    c = Salary*.5  |     Salary*.5    |\n\n- Basically, if the employee is still with IBM, there is no cost invovled\n- But if the employee leaves we will take the CONSERVATIVE estimate that it costs half of their yearly salary to replace them\n    - Assuming an average salary of \\$50,000 per year (also conservative)\n\n$$\\Rightarrow C = TN_n\\cdot 0 + FP_n \\cdot 0 + FN_n \\cdot Salary \\cdot 0.5+ TP_n \\cdot Salary \\cdot 0.5$$\n$$\\Rightarrow C = FN_n \\cdot Salary \\cdot 0.5 + TP_n \\cdot Salary \\cdot 0.5= \\boxed{\\$ 1,375,000}$$\n\n- So we have an overhead cost of attrition of around 1.375 Million dollars per year (using the \"testing\" employees: 55 employees leaving and 307 staying)\n\n## Or we could take preemptive action against attrition\n- What was the most important feature for attrition?\n    - INCOME (obviously)\n- Let's give employees that are at risk for attrtition a 10% bonus in the hopes that they will stay\n- Assumptions\n    - Bonus will prevent an employee from leaving a percentage ($p$) of the time\n    - Cost of losing an employee = $.5 \\cdot Salary = \\$25,000$\n    - Bonus = $0.1 \\cdot Salary = \\$5,000$\n    \n### New cost function if we implement this bonus program\n|            | Predicted No | Predicted Yes |\n|------------|--------------|---------------|\n| Actual No  | c = 0     |    c = 5,000    |\n| Actual Yes |    c = 25,000  |   p $\\cdot$ 5,000 + (1 - p) $\\cdot$ 25,000   |\n\n- So if we predict no attirtion and the employee will be staying we incure a cost of \\$0\n- But if we predict attrition when there is not attrition, we take a hit of the bonus (\\$5,000)\n- If we don't predict attrition when there is, we lose half of the employee's salary (\\$25,000)\n- Now if we predict attrition when the employee was planning on leaving, there's a probability that the bonus will keep them and the subsequent opposite probabilty that it doesn't work, so we can come up with an expected cost this way\n\n- Here's our new cost function\n$\\Rightarrow C = TN_n\\cdot 0 + FP_n \\cdot 5000 + FN_n \\cdot 25000+ TP_n (p \\cdot 5000+ (1 - p) \\cdot 25000)$ \n\n### We will now try to tune our models to minimize this cost function\n","outputs":[],"execution_count":null},{"metadata":{"collapsed":true,"_uuid":"4e62c06edf070c7b853061bb1865d21fb7cec9b1","_cell_guid":"1db6caff-85f9-43af-bb6a-0f60165bbace","trusted":true},"cell_type":"code","source":"from sklearn.metrics import precision_score, recall_score","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"73953eef3f14e6c1b24d2c4b45404bca23a9a9cf","_cell_guid":"7fce2b57-3828-43f9-ad1f-236afcc754fb","trusted":true},"cell_type":"code","source":"mlp = MLPClassifier(activation = 'logistic', hidden_layer_sizes = (60,), alpha = .1, \\\n                    learning_rate = 'adaptive')\nmlp.fit(X_train_scaled, y_train)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"6135f45bb2cba9b73295d5d4ba3984cf8ffcedb3","_cell_guid":"c6d02e3e-9327-4088-b93b-c08c0d4da7c6","trusted":true},"cell_type":"code","source":"def best_threshold (model, steps, X, y, p):\n    salary = 50000.0\n    bonus = 5000.0\n    TN_cost = 0\n    TP_cost = p*bonus + (1-p)*.5*salary\n    FP_cost = bonus\n    FN_cost = .5*salary\n    \n    cost = []\n    threshold = 0\n    \n    m = model\n    #train test split\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42)\n    #scale the data\n    scaler = StandardScaler()\n    # Fit_transform\n    X_train_scaled = scaler.fit_transform(X_train)\n    # transform\n    X_test_scaled = scaler.transform(X_test)\n    \n    m.fit(X_train_scaled, y_train)\n    \n    for i in range(steps + 1):\n        y_pred_train = (model.predict_proba(X_train_scaled)[:,1] > threshold)\n        y_pred_test = (model.predict_proba(X_test_scaled)[:,1] > threshold)\n        \n        cm = confusion_matrix(y_test, y_pred_test)\n        TN = cm[0,0]\n        TP = cm[1,1]\n        FP = cm[0,1]\n        FN = cm[1,0]\n        \n        total_cost = TN_cost*TN + TP_cost*TP + FP_cost*FP + FN_cost*FN\n        results_dict = {\n                'threshold' : threshold,\n                'cost' : total_cost,\n                'precision_score_test': precision_score(y_test, y_pred_test),\n                'recall_score_test': recall_score(y_test, y_pred_test),\n                'TN': TN,\n                'FP': FP,\n                'FN': FN,\n                'TP': TP,\n                        }\n        cost.append(results_dict)\n        threshold += (1.0/steps) \n    \n    thresh_results = pd.DataFrame(cost, columns=['cost', 'threshold', 'precision_score_test',\\\n                        'recall_score_test','FN', 'FP','TN','TP'])\n    return thresh_results","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"959bc2519021a525af8fc0b35e8e2a7792285d1b","_cell_guid":"dbdeb485-2b16-4575-8450-0e6c27c81194","trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize = (15,90))\nj = 0\nprobabilities = -(np.sort(-np.linspace(0,1,11)))\n\nfor i in probabilities:\n    df1 = best_threshold(xgb,20,X,y, i)\n    df2 = best_threshold(mlp,20,X,y, i)\n    \n    j += 1\n    fig.add_subplot(12,1,j)\n    plt.plot(df2['threshold'], df2['cost'], label = 'ANN')\n    plt.plot(df1['threshold'], df1['cost'], label = 'XGBoost')\n    plt.plot(np.linspace(0,1,9), 1375000*np.ones(9), label = 'Overhead Cost')\n    plt.ylabel('Cost')\n    plt.xlabel('Probability Threshold')\n    plt.title('Comparing Models to Minimze Cost')\n    plt.text(x = .325, y = 1500000, s = 'Probability that bonus is successful = ' + str(i), fontsize = 14)\n\n    m = df2['cost'].min()\n    profit = 1375000 - m\n    plt.text(x = .325, y = 1400000, s = 'Savings = $' + str(profit), fontsize = 14)\n    plt.legend()\nplt.tight_layout()\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8ced6ff67b4b7ad4ae6ef57469ae44d4bb2f5562","_cell_guid":"620686c7-68b0-4407-9ca1-ad53636a5357"},"cell_type":"markdown","source":"## Best case scenario (works 100% of the time)\n- We can expect savings of \\$590,000 (from the testing set)\n\n## Worst Case (Works 10% of the time)\n- We can expect savings of \\$37,000 (from the testing set)","outputs":[],"execution_count":null},{"metadata":{"collapsed":true,"_uuid":"a48db27ad9179cf5f65e8c81ea08102951d304dc","_cell_guid":"76b1900d-f1ba-4d7f-bb72-d90f491a0da1"},"cell_type":"markdown","source":"### So when we come up with a confident answer for how often the bonus actually works\n- We can see where the cost functon is minimized and say \"Hey this employee has probability (p > threshold) of leaving, let's give them a bonus\" and we can expect significant savings\n    - For example, if the bonus program only works 50% of the time, we will use our ANN model to come up with a probability of the employee leaving and if that probability is greater than 0.45 we give them a bonus. We can then expect savings of \\$260,000\n\n### It's pretty clearly a bad idea to give everyone a bonus\n- And if nobody gets the bonus, the cost function converges to the \"overhead cost\"\n    - This also means that even if the bonus NEVER works it is not any worse than doing nothing to combat it\n- But those middle grounds are the important metrics\n    - Also keep in mind these are based on conservative estimates (only costs half of the employees salary to lose them)","outputs":[],"execution_count":null},{"metadata":{"collapsed":true,"_uuid":"4efcf8cfabcbfa6aa54cd46ba5573faee4a49144","_cell_guid":"f40d4540-fb3c-41fc-8b45-93ae10ae68fa"},"cell_type":"markdown","source":"# Conclusions\n- Yes, it is an artificial dataset\n    - But for the most part a lot of these metrics are recorded by companies\n        - Even if they are not recorded, it would not be to difficult to obtain this data\n- We were able to reliably predict whether an employee will leave the company (91% accuracy vs. benchmark accuracy of 84%)\n- We are confident in which attributes are most important to attrition\n  - MonthlyIncome\n  - Overtime\n  - DailyRate\n  - DistanceFromHome\n  - EnvironmentSatisfaction\n  - RelationshipSatisfaction\n  - NumCompaniesWorked\n  - StockOptionLevel\n  - TotalWorkingYears\n  - JobRole\n- More importantly we looked at whether we should do nothing about employee turnover or try to combat it\n    - We used conservative estimates for how much it will cost IBM to replace an employee\n    - We used a conservative estimate for the average employee's salary at IBM (\\$50,000 vs the actual median of \\$58,000)\n    - We looked at a wide range of possibities of how often the bonus program will work\n        - Even if it works 0% of the time there is no cost involved","outputs":[],"execution_count":null},{"metadata":{"collapsed":true,"_uuid":"9a5a06caa2b30ff364c75c6d0ef1a7ba494da75e","_cell_guid":"121159e3-81fb-4ad9-b829-8ce06c2a382d"},"cell_type":"markdown","source":"## Further Work\n- Actually validating the assumptions of our cost function\n    - How often a bonus will keep an employee at IBM\n    - How much it actually costs to lose an employee\n - Rather than taking the an \"average\" salary for employees\n     - Change the cost function to reflect everyone's individual salary, among other metrics\n- Continue tuning models to more accurately reflect an employee's probability of attrition\n     - Look into model stacking\n     - Look into methods of dealing with a class inbalance   ","outputs":[],"execution_count":null}],"metadata":{"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}