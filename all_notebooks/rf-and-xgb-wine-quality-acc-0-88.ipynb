{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1-Import the dataset\n\nThe first step is obviously to import the dataset into Python. The column names will not be changed since they are representative of the data in its respective column."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df=pd.read_csv('../input/red-wine-quality-cortez-et-al-2009/winequality-red.csv')\ndf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2-Explore and clean the dataset\n\nThe second step is to explore and clean the dataset. I will check first of all the types of variable that we have for each column."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see clearly that we are only dealing with continuous numerical features because their dtype is float64. However, for the response variable, we are dealing with a categorical numerical variable, which is the quality of the wine, on a scale of 10. \n\nLet's check now the summary statistics of each variable in the dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"First, we can see that the count for each variable in the dataset is 1599, which means that we will not have to deal with missing value. This is a great news since we  want as less as possible to impute the missing values by the mean, median or mode because they can be inacurate with respect to the observation that we have.\n\nFrom this table, there does not seem to be any abnormal values. \n\nLet's now count the number of observations in each category for the quality of the wine."},{"metadata":{"trusted":true},"cell_type":"code","source":"df['quality'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from matplotlib import pyplot as plt\nimport seaborn as sns\n\nprint(sns.distplot( df[\"quality\"],color='red' ))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see, there are a lot of observations in the 5 and 6 range, and a few observations in the 3, 4, 7 and 8 range. Their seems to be no value in the other ranges, which will simplify the problem. \n\nLet's now analyze the outliers of the numerical features of the dataset with boxplots."},{"metadata":{"trusted":true},"cell_type":"code","source":"numerical_features=[\"fixed acidity\",\"volatile acidity\",\"citric acid\",\"residual sugar\",\"chlorides\",\"free sulfur dioxide\",\"total sulfur dioxide\",\"density\",\"pH\",\"sulphates\", \"alcohol\"]\n\nfor numerical_feature in numerical_features:\n    plt.figure()\n    df.boxplot(column=[numerical_feature],grid= False )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can notice, from the boxplots, that there are many outliers in the dataset. However, I will try to  delete the most exagerated ones. For example, for the volatile acidity, I will delete only the values that are over 1.3 since there are too many values that are close to 1. I will then keep all the values that are between 1 and 1.3 even if they are outliers. I will repeat the same process for the other variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"df=df[(df['volatile acidity']<1.1) & (df['citric acid']<0.9) & (df['residual sugar']<10.0) & (df['chlorides']<0.3) & (df['free sulfur dioxide']<45) & (df['total sulfur dioxide']<250.0) & (df['pH']<3.7) & (df['sulphates']<1.5)]\ndf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see, there are still 1512 observations remaining in the dataset after having removed the outliers compared to 1599 observations before, which is reasonable to build a machine learning model.\n\nMoreover, to simplify the prediction, I will put the wine quality in a binary classification. 0 will be a bad wine (3,4 and 5) and 1 will be a good wine (6,7,8)."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.loc[(df['quality']<7) , 'quality'] = 0\ndf.loc[(df['quality']>=7) , 'quality'] = 1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3-Features selection\n\nIn this step, I will choose the most important features that I will include in my machine learning model. Since my response variable is categorical (3+ categories) and all the other variables are continuous, I will perform an ANOVA test with my independent (feature) and dependent (response) variables. If the p-value is under 0.01, that means that there is a relationship between the response variable and the feature, which means that I will keep that feature in my model. Otherwise, I will not keep that feature in my model."},{"metadata":{"trusted":true},"cell_type":"code","source":"import scipy.stats as stats\n\nnumerical_features=[\"fixed acidity\",\"volatile acidity\",\"citric acid\",\"residual sugar\",\"chlorides\",\"free sulfur dioxide\",\"total sulfur dioxide\",\"density\",\"pH\",\"sulphates\", \"alcohol\"]\n\nfor numerical_feature in numerical_features:\n    p_value= stats.ttest_ind(df[numerical_feature][df['quality'] == 0],\n               df[numerical_feature][df['quality'] == 1], equal_var=False\n                  ).pvalue\n    if p_value<0.05:\n        print('We keep the', numerical_feature, 'in the model', p_value)\n    else:\n        print('We do not keep the', numerical_feature, 'in the model', p_value)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From my ANOVA tests, we can clearly see that I will keep all my features except the residual sugar in my model to predict the wine quality."},{"metadata":{},"cell_type":"markdown","source":"# 4-Build the Random Forest model\n\nNow that I have chosen all the features to predict the wine quality, I will build my machine learning model. I will first set x1 as my features that I chose earlier and y as my response variable (wine quality). I will thereafter split my dataset into two sets and scale my features to get a better accuracy."},{"metadata":{"trusted":true},"cell_type":"code","source":"X1=df.drop(columns=['quality'])\ny=df['quality']\n\nfrom sklearn.model_selection import train_test_split\n\nX_train,X_test,y_train,y_test=train_test_split(X1,y,test_size=0.3,random_state=0)\n\nfrom sklearn.preprocessing import StandardScaler\n\nX_train=StandardScaler().fit_transform(X_train)\nX_test=StandardScaler().fit_transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, it is time to fit the model. To get the best accuracy as possible, I will test different number of estimators to see which one gives the best accuracy. I will then choose that number of estimators to predict and to calculate my final accuracy."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\n\nRF = RandomForestClassifier()\nRF_reg_parameters = { 'n_estimators': [1,5,10,50,100,200,500] }\ngrid_RF_acc = GridSearchCV(RF, param_grid = RF_reg_parameters,cv=10)\ngrid_RF_acc.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"My optimal number of estimators, as it is mentioned below, is 200."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(grid_RF_acc.best_estimator_.n_estimators)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It is now the time to calculate the accuracy score."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\n\ny_pred = grid_RF_acc.predict(X_test)\nprint('RF Accuracy =', metrics.accuracy_score(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"By optimizing my number of estimators, I get an accuracy of 87%. However, I will try to improve the model by removing the less important features."},{"metadata":{"trusted":true},"cell_type":"code","source":"RF.fit(X_train,y_train)\n\nfeature_imp = pd.Series(RF.feature_importances_,index=[\"fixed acidity\",\"volatile acidity\",\"citric acid\",\"residual sugar\",\"chlorides\",\"free sulfur dioxide\",\"total sulfur dioxide\",\"density\",\"pH\",\"sulphates\", \"alcohol\"]).sort_values(ascending=False)\nfeature_imp","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can notice above, free sulfur dioxide, ph and chlorides are the lest important features to predict the wine quality with the RF model. I will then repeat the steps above to calculate my accuracy with my new Random Forest model."},{"metadata":{"trusted":true},"cell_type":"code","source":"X2=df.drop(columns=['quality','free sulfur dioxide','pH','chlorides'])\ny=df['quality']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train,X_test,y_train,y_test=train_test_split(X2,y,test_size=0.3,random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"RF = RandomForestClassifier()\nRF_reg_parameters = { 'n_estimators': [1,5,10,50,100,200,500] }\ngrid_RF_acc = GridSearchCV(RF, param_grid = RF_reg_parameters,cv=10)\ngrid_RF_acc.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(grid_RF_acc.best_estimator_.n_estimators)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = grid_RF_acc.predict(X_test)\nprint('RF Accuracy =', metrics.accuracy_score(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see, the accuracy is better when I remove the least important features in my model. I will however try to improve my prediction by choosing another Machine Learning model."},{"metadata":{},"cell_type":"markdown","source":"# 5-Build the XGBoost model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBClassifier\n\nX1=df.drop(columns=['quality'])\ny=df['quality']\n\nX_train,X_test,y_train,y_test=train_test_split(X1,y,test_size=0.3,random_state=0)\n\n\nXGB = XGBClassifier()\nXGB_reg_parameters = { 'n_estimators': [1,5,10,50,100,200,500] }\ngrid_XGB_acc = GridSearchCV(RF, param_grid = XGB_reg_parameters,cv=10)\ngrid_XGB_acc.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(grid_XGB_acc.best_estimator_.n_estimators)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = grid_XGB_acc.predict(X_test)\nprint('XGB Accuracy =', metrics.accuracy_score(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"XGB.fit(X_train,y_train)\n\nfeature_imp = pd.Series(XGB.feature_importances_,index=[\"fixed acidity\",\"volatile acidity\",\"citric acid\",\"residual sugar\",\"chlorides\",\"free sulfur dioxide\",\"total sulfur dioxide\",\"density\",\"pH\",\"sulphates\", \"alcohol\"]).sort_values(ascending=False)\nfeature_imp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X2=df.drop(columns=['quality','citric acid','pH','chlorides','residual sugar'])\ny=df['quality']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train,X_test,y_train,y_test=train_test_split(X2,y,test_size=0.3,random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"XGB = XGBClassifier()\nXGB_reg_parameters = { 'n_estimators': [1,5,10,50,100,200,500] }\ngrid_XGB_acc = GridSearchCV(RF, param_grid = XGB_reg_parameters,cv=10)\ngrid_XGB_acc.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(grid_XGB_acc.best_estimator_.n_estimators)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = grid_XGB_acc.predict(X_test)\nprint('XGB Accuracy =', metrics.accuracy_score(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"We can see here that it is slightly better to keep all the features in the model since it gives a better accuracy."},{"metadata":{},"cell_type":"markdown","source":"# 6-Conclusion\n\nEven though every tested model gave a good accuracy, the random forest one with a reduced number of components stays the best option for this dataset."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}