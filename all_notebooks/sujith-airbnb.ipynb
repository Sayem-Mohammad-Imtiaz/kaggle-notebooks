{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"ls","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Reading the data\ntraining_data = pd.read_csv('/kaggle/input/airbnb-price-prediction/train.csv')\ntraining_data.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(training_data.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training_data.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# unique Id\nlen(set(training_data.id))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## remove id column\ntraining_data.drop('id', axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Missing values\nmissPer = training_data.apply(lambda x: sum(x.isnull())/training_data.shape[0]*100, axis=0)\nmissPer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# separating numeric and categorical data\ncolnames = training_data.columns\nnumcolnames = training_data._get_numeric_data().columns\ncat_data = training_data[list(set(colnames) - set(numcolnames))]\ncat_data.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## numerical data exploration\nnumeric_data = training_data[numcolnames]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Skews\nskew = numeric_data.skew()\nskew","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# varience\nvar = numeric_data.var()\nvar","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# correlation\ncor = numeric_data.corr()\ncor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# For vif\nnumeric_data.drop('cleaning_fee', axis=1, inplace=True)\nnumeric_data.drop('log_price', axis=1, inplace=True)\nnumeric_data.dropna(inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# For each X, calculate VIF and save in dataframe\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nvif = pd.DataFrame()\nvif[\"VIF Factor\"] = [variance_inflation_factor(numeric_data.values, i) for i in range(numeric_data.shape[1])]\nvif[\"features\"] = numeric_data.columns\nvif","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"###### excluding the variables\n# From the correlation values, we can drop few variables which has no correlation\ntraining_data.drop(['latitude', 'longitude', 'number_of_reviews',\n                    'review_scores_rating'], axis=1, inplace=True)\n# we have city we can drop Zipcode\ntraining_data.drop(['zipcode'], axis=1, inplace=True)\n# first_review,last_review, host_since are dates and 21% of the data are missing\ntraining_data.drop(['first_review', 'last_review', 'host_since'],\n                   axis=1, inplace=True)\n# description, name, thumbnail_url are almost unique\ntraining_data.drop(['description', 'name', 'thumbnail_url'],\n                   axis=1, inplace=True)\n# host details are not giving much relation with the target variable\ntraining_data.drop(['host_has_profile_pic', 'host_response_rate'],\n                   axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## =============================================================================\n## Feature engineering\n## =============================================================================\n## deriving new features with amenities","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"amenities_list = []\nfor i in range(0, training_data.shape[0]):\n    am = training_data['amenities'][i].split(',')\n    for j in am:\n        amenities_list.append(j.replace('\"', '').replace('}', '').replace('{', ''))\namenities_set = set(amenities_list)\nlen(amenities_set)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"amenities_dict = {}\nfor am in set(amenities_list):\n    #print(str(am) +' : ' +str(amenities_list.count(am)))\n    amenities_dict.update({str(am) : amenities_list.count(am)})\nsorted(amenities_dict.items(), key=lambda kv: (kv[1], kv[0]), reverse=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Keep only top 20\n# 'Wireless Internet', 71265),\n#  ('Kitchen', 67526),\n#  ('Heating', 67073),\n#  ('Essentials', 64005),\n#  ('Smoke detector', 61727),\n#  ('Air conditioning', 55210),\n#  ('TV', 52458),\n#  ('Shampoo', 49465),\n#  ('Hangers', 49173),\n#  ('Carbon monoxide detector', 47190),\n#  ('Internet', 44648),\n#  ('Laptop friendly workspace', 43703),\n#  ('Hair dryer', 43330),\n#  ('Washer', 43169),\n#  ('Dryer', 42711),\n#  ('Iron', 41687),\n#  ('Family/kid friendly', 37026),\n#  ('Fire extinguisher', 30724),\n#  ('First aid kit', 27532),\n#  ('translation missing: en.hosting_amenity_50', 25291),\n#  ('Cable TV', 24253),\n#  ('Free parking on premises', 23639),\n#  ('translation missing: en.hosting_amenity_49', 20427),\n#  ('24-hour check-in', 19015),\n#  ('Lock on bedroom door', 17983),\n#  ('Buzzer/wireless intercom', 17033),\n#  ('Safety card', 11513),","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"amenities_array = []\nfor i in range(0, training_data.shape[0]):\n    array = np.zeros(shape=(len(amenities_set)))\n    am = training_data['amenities'][i].split(',')\n    for j in am:\n        item = j.replace('\"', '').replace('}', '').replace('{', '')\n        res = list(amenities_dict.keys()).index(item)\n        array[res] = 1\n    amenities_array.append(array.tolist())\n\namenities_df = pd.DataFrame(amenities_array, columns=amenities_dict.keys())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"amenities_df.apply(pd.Series.value_counts)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"amenities_df = amenities_df[['Wireless Internet','Kitchen','Heating','Essentials','Smoke detector','Air conditioning','TV','Shampoo','Hangers',\n'Carbon monoxide detector']]\n#                              ,'Internet','Laptop friendly workspace', 'Hair dryer','Washer','Dryer','Iron',\n# 'Family/kid friendly', 'Fire extinguisher', 'First aid kit','translation missing: en.hosting_amenity_50']]\n#                              ,'Cable TV',\n# 'Free parking on premises']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"amenities_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"amenities_df = amenities_df.astype('category')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training_data.drop(['amenities'], axis=1, inplace=True)\ntraining_data = pd.concat([training_data, amenities_df], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from fastai.imports import *\n#from fastai.structured import *\nfrom fastai.tabular import * ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"colnames = training_data.columns\nnumcolnames = training_data._get_numeric_data().columns\ncat_data = training_data[list(set(colnames) - set(numcolnames))]\ncat_data.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"numcolnames[1:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dep_var = 'log_price'\n\ncat_names = cat_data.columns.tolist()\n\ncont_names = numcolnames[1:].tolist()\n\n# Transformations\nprocs = [FillMissing, Categorify, Normalize]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Start index for creating a validation set from train_data\nstart_indx = len(training_data) - int(len(training_data) * 0.2)\n\n#End index for creating a validation set from train_data\nend_indx = len(training_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#TabularList for Validation\nval = (TabularList.from_df(training_data.iloc[start_indx:end_indx].copy(), cat_names=cat_names, cont_names=cont_names))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train Data Bunch\ndata = (TabularList.from_df(training_data, path='.', cat_names=cat_names, cont_names=cont_names, procs=procs)\n                        .split_by_idx(list(range(start_indx,end_indx)))\n                        .label_from_df(cols = dep_var)\n                        .databunch())\n\ndata.show_batch(rows=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create deep learning model\nlearn = tabular_learner(data, layers=[1000, 200,15], metrics= [rmse,r2_score], emb_drop=0.1, callback_fns=ShowGraph)\n\n# select the appropriate learning rate\nlearn.lr_find(start_lr = 1e-05,end_lr = 1e+05, num_it = 100)\n\n# we typically find the point where the slope is steepest\nlearn.recorder.plot()\n\n# Fit the model based on selected learning rate\nlearn.fit_one_cycle(15)\n\n# Analyse our model\nlearn.model\nlearn.recorder.plot_losses()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training_data.iloc[0,:].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.predict(training_data.iloc[0,:])[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}