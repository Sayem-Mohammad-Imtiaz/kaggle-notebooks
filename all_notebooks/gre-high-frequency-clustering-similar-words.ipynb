{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Intro:\n#### When I started with my GRE preparation, after going through many resources (for the vocab section) I found that some words appear more frequently in the exam, and Barron’s, Manhattan's and, Magoosh's word lists are the most renowned resources that contain the high-frequency GRE words. For this project, I picked Barron’s 333, Manhattan's 500 and, Magoosh's 1000 wordlists. The next challenge was learning these words so I came up with a plan. If I could somehow group similar words it would make the learning process much easier. But how to do that? Manually grouping these words would be way more challenging than simply learning the words as they are. After pondering for some time, it occurred to me why not let the machine do all the hard work! I think with a capability of above one million million floating-point operations per second it is much better for these types of tasks than I am so let’s get started and see how to build a model that can cluster similar words together.\n\n#### I've used python for the project and the topics covered are Exploratory Data Analysis (EDA), Natural Language Processing (NLP), Word Embedding generation using Global Vectors (GloVe), Hierarchical Clustering, t-distributed Stochastic Neighbor Embedding (T-SNE)."},{"metadata":{},"cell_type":"markdown","source":"![pic](https://images.unsplash.com/photo-1558021212-51b6ecfa0db9?ixid=MXwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHw%3D&ixlib=rb-1.2.1&auto=format&fit=crop&w=1522&q=80)"},{"metadata":{},"cell_type":"markdown","source":"## 📊 EDA of High Frequency GRE vocabulary words."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's begin by importing all the necessary libraries\nfrom scipy.cluster import hierarchy\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nimport plotly.express as px\nsns.set()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's import the 3 word lists and merge them to greate a single list. \n# These lists contain multiple columns, for this task, I've only considered 'word' and 'definition' columns.\n# The merged dataframe also has a column named 'word_list' that represents one of the 3 the word lists the word corresponds to.\nfolder = '../input/gre-high-frequency-vocabulary-word-lists/'\nmagoosh = pd.read_csv(folder+'magoosh_1000.csv')[['word', 'definition']]\nmagoosh['word_list'] = ['magoosh_1000']*magoosh.shape[0]\nmanhattan = pd.read_csv(folder+'manhattan_500.csv')[['word', 'definition']]\nmanhattan['word_list'] = ['manhattan_500']*manhattan.shape[0]\nbarron = pd.read_csv(folder+'barron_333.csv')[['word', 'definition']]\nbarron['word_list'] = ['barrons_333']*barron.shape[0]\n\ndf = pd.concat([manhattan, magoosh, barron]).dropna().drop_duplicates(subset=['word'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# This function is for calculating the Alphabetical frequency of words in the lists.\ndef alphabetical_frequency(df, wordlist='all'):\n  counts = {}\n  if wordlist=='manhattan':\n    df = df[df['word_list']=='manhattan_500']\n  elif wordlist=='barrons':\n    df = df[df['word_list']=='barrons_333']\n  elif wordlist=='magoosh':\n    df = df[df['word_list']=='magoosh_1000']    \n  for i in list('abcdefghijklmnopqrstuvwxyz'):\n    k=1\n    for j in df.word:\n      if j[0]==i:\n        k+=1\n    counts[i] = k\n  dd = pd.DataFrame()\n  dd['alphabet'] = counts.keys()\n  dd['frequency'] = counts.values()\n  fig = px.bar(dd, x='alphabet', y='frequency')\n  fig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Alphabetical frequency of words in all the word lists."},{"metadata":{"trusted":true},"cell_type":"code","source":"alphabetical_frequency(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Alphabetical frequency of words in Manhattan's 500 word list."},{"metadata":{"trusted":true},"cell_type":"code","source":"alphabetical_frequency(df, wordlist='manhattan')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Alphabetical frequency of words in Magoosh's 1000 word lists."},{"metadata":{"trusted":true},"cell_type":"code","source":"alphabetical_frequency(df, wordlist='magoosh')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Alphabetical frequency of words in Barron's 333 word lists."},{"metadata":{"trusted":true},"cell_type":"code","source":"alphabetical_frequency(df, wordlist='barrons')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### While scraping the Barron's 333 word list, I found a meta feature 'frequency' that represents the frequency of occurance of a word in the list."},{"metadata":{"trusted":true},"cell_type":"code","source":"barron = pd.read_csv(folder+'barron_333.csv')\nbarron.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Let's plot a box plot of the frequency distribution."},{"metadata":{"trusted":true},"cell_type":"code","source":"# box plot of the word frequencies in barron's 333 word list\nfig = px.box(barron, y=\"frequency\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Let's look at the highest and lowest frequent GRE words"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Collecting words with frequency over 10k\ndef top_frequency(barron, n=30):\n  high_frequency = barron[['word', 'frequency']].sort_values(by='frequency', ascending=False)\n  word_list = {}\n  for w,f in high_frequency.values[:n]:\n    word_list[w] = int(f)\n  wordcloud = WordCloud(background_color=\"black\").generate_from_frequencies(word_list)\n  plt.figure(figsize=(15, 6))\n  plt.imshow(wordcloud)\n  plt.axis(\"off\")\n  plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Top 30 highest occurring GRE words as per Barron's list"},{"metadata":{"trusted":true},"cell_type":"code","source":"top_frequency(barron, n=30)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Top 30 lowest occurring GRE words as per Barron's list"},{"metadata":{"trusted":true},"cell_type":"code","source":"top_frequency(barron, n=-30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# words common in Magoosh and Barron lists\ncommon_words_1 = set.intersection(set(magoosh['word']), set(barron['word']))\nlen(common_words_1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# words common in Magoosh and Manhattan lists\ncommon_words_2 = set.intersection(set(magoosh['word']), set(manhattan['word']))\nlen(common_words_2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# words common in Manhattan and Barron lists\ncommon_words_3 = set.intersection(set(manhattan['word']), set(barron['word']))\nlen(common_words_3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# words common in Manhattan, Barron and Magoosh lists\ncommon_words_all = set.intersection(set(manhattan['word']), set(barron['word']), set(magoosh['word']))\nlen(common_words_all)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from matplotlib_venn import venn3, venn3_circles\n\nplt.figure(figsize=(16,9))\nvd3=venn3([set(manhattan['word']), set(barron['word']), set(magoosh['word'])],\n           set_labels = ('Manhattan\\'s list', 'Barron\\'s list', 'Magoosh\\'s list'),\n           set_colors=('#c4e6ff', '#F4ACB7', '#9D8189'), \n           alpha = 0.8)\n\nvenn3_circles([set(manhattan['word']), set(barron['word']), set(magoosh['word'])],\n              linestyle='-.', linewidth=2, color='grey')\n\nfor text in vd3.set_labels:\n    text.set_fontsize(16);\nfor text in vd3.subset_labels:\n    text.set_fontsize(16)\n    \nplt.title('Venn Diagram for the 3 lists', fontname='Times New Roman', fontweight='bold', fontsize=20,\n           pad=30, backgroundcolor='#cbe7e3', color='black', style='italic')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Clustering similar words\n#### In this section, we'll look at how to cluster similar words based on their Vector representations.\n#### The w2v representation I've used for each word is GloVe (Global Vectors). It is an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space. More information on GloVe can be found on [this link.](https://nlp.stanford.edu/projects/glove/)\n\n#### Let's begin by downloading and extracting the word embeddings. The embedding I'll be using contains data for about 2.2 million case sensitive words with each embedding vector of dimension 300."},{"metadata":{"trusted":true},"cell_type":"code","source":"# !wget http://nlp.stanford.edu/data/glove.840B.300d.zip\n# !unzip glove.840B.300d.zip","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Creating a dictionary with our GRE words as keys and their corresponding embedding vectors from GloVe as values."},{"metadata":{"trusted":true},"cell_type":"code","source":"# file = open('glove.840B.300d.txt')\n# dic = {}\n# for line in tqdm(file):\n#   w = line.split()[0]\n#   if w in df.word.values:\n#     m = line.split()[1:]\n#     dic[w] = m\n# file.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# w2v = []\n# for w in df.word.values:\n#   if w in dic.keys():\n#     w2v.append(np.asarray(dic[w]).astype(np.float))\n#   else:\n#     w2v.append(np.nan)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Adding the collected word embeddings to the word meaning data frame as a new feature 'embeddings'\nI've also saved the dataframe as a csv file so that it can be used directly."},{"metadata":{"trusted":true},"cell_type":"code","source":"# df['embeddings'] = w2v\n# df = df.dropna()[]\n# for i,e in enumerate(w2v.T):\n#   c = f'embedding_{i+1}'\n#   df[c] = e\n\n# # df.to_csv('words_meaning_embeddings.csv', index=False)\n# df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This function is for collecting the embeddings from the dataframe such that if the parameter 'wordlist' is:\n1. manhattan_500: returns the embeddings of the words in Manhattan's 500 word list.\n2. magoosh_1000: returns the embeddings of the words in Magoosh's 1000 word list.\n3. barron_333: returns the embeddings of the words in Barron's 333 word list.\n4. all: returns the embeddings of the words in from the word lists (set-wise union)."},{"metadata":{"trusted":true},"cell_type":"code","source":"# def get_w2v(df, wordlist='all'):\n#     if wordlist=='all':\n#         w2v = np.row_stack(df.embeddings.values)\n#     else:\n#         w2v = np.row_stack(df[df['word_list']==wordlist].embeddings.values)\n#     return w2v","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # since wordlist is 'all', the function returns the embeddings of all the words (in all the 3 word-lists)\n# w2v = get_w2v(df, wordlist='all')\n# w2v.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### I've already prepared and saved a dataframe containing the data. We can simply use that and skip the above steps."},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(folder+'words_meaning_embeddings.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"w2v = df[df.columns[3:]]\nw2v.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Now it's time to cluster the word-embeddings using Hierarchical Clustering.\n#### Hierarchical Clustering is a type of unsupevised learning technique that uses groups similar objects into groups called clusters. The endpoint is a set of clusters, where each cluster is distinct from each other cluster, and the objects within each cluster are broadly similar to each other. For this data, I have used Agglomerative Hierarchical Clustering also known as AGNES (Agglomerative Nesting). The algorithm starts by treating each object as a singleton cluster. Next, pairs of clusters are successively merged until all clusters have been merged into one big cluster containing all objects. The result is a tree-based representation of the objects, named dendrogram.\n#### For implementing the algorithm, I have used sciPy library which comes with buildin functionalities like calculating the wands or linkage of the datapoints based on the distance to consider between those points and the distance metric to use. These factors are passed as a attributes while initializing the clustering method. Finally, using these wands, the data is clustered.\n#### You can read more about sciPy's hierarchical clustering on [this blog](https://www.datanovia.com/en/lessons/agglomerative-hierarchical-clustering/). \n\n![clustering](https://dashee87.github.io/images/hierarch.gif)"},{"metadata":{"trusted":true},"cell_type":"code","source":"threshold = 0.7\nZ = hierarchy.linkage(w2v, \"average\", metric=\"cosine\",)\nC = hierarchy.fcluster(Z, threshold, criterion=\"distance\")\nn = len(np.unique(C))\nprint(f'Number of clusters created: {n}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# embedding the clusters to dataframe for extracting word clusters\ndata = df[['word', 'definition']].copy()\ndata['labels'] = C","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Let's check the number of words in each cluster"},{"metadata":{"trusted":true},"cell_type":"code","source":"dd = pd.DataFrame(np.asarray(np.unique(C, return_counts=True)).T)\ndd.columns = ['group_id', 'number of words']\nfig = px.bar(dd, x='group_id', y='number of words')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Let's see some of the word groups generated by clustering algorithm"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_grp = data.groupby('labels')\ndf_grp.get_group(81)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_grp.get_group(90)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_grp.get_group(79)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_grp.get_group(83)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Finally let's visualize the word embeddings in the form of a scatter plot using T-SNE. But first, let's quickly understand T-SNE.\n#### t-SNE (t-Distributed Stochastic Neighbor Embedding) is a non-linear dimensionality reduction algorithm used for exploring high-dimensional data. It maps multi-dimensional data to two or more dimensions such that each embedding in the lower dimension represents the value in higher dimension. Also, these embeddings are placed in the lower dimension in such a manner that the distance between neighborhood points is preserved. So, t-SNE preserves the local structure of the data as well.\n#### For a given point in n-dimensional hyperspace, it calculates the distance of that point from all the other points and converts these distributions of distances to student’s t-distribution. This is done for all the points such that in the end, each point has its own t-distribution of distances from all the other points. Now the points are randomly scattered in the lower dimensional space and each point is displaced by some distance such that after the displacement of all the points is done if we recalculate the t-distribution of distances of each point from the remaining points (this time this is done in the lower dimensional space), the distribution would be the same as what we obtained in n-dimensional hyperspace.\n#### There are 2 main hyperparameters in t-SNE-\n#### Perplexity: Instead of calculating the distance from all the other points, we can use only ‘k’ nearest points. This value of ‘k’ is called the perplexity value.\n#### Iterations: The number of iterations for which we want t-SNE to update the points in lower-dimensional space.\n#### Due to stochasticity, the algorithm may perform differently for different perplexity values so as a good practice, it is preferred to run t-SNE for different perplexity values and different numbers of iterations. To know more about t-SNE, check out [this awesome blog](https://distill.pub/2016/misread-tsne/), it has t-SNE very well explained with interactive visualization."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.manifold import TSNE\ntransform = TSNE\ntrans = transform(n_components=2, perplexity=10, n_iter=1000, metric='cosine')\nembeddings_2d = trans.fit_transform(w2v)\n\ndff = data[['word', 'definition', 'labels']].copy()\ndff['x'] = embeddings_2d[:,0]\ndff['y'] = embeddings_2d[:,1]\n\nfig = px.scatter(dff, x=\"x\", y=\"y\", color=\"labels\",\n                 hover_data=['word', 'definition'])\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Below is the dendogram (a diagram that shows the hierarchical relationship between objects) of word clusters.\nYou can view it by right clicking, selecting 'open image in new tab' and, zooming in."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(128, 72))\ndn = hierarchy.dendrogram(Z)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### That was it about this project. If you found the work useful, please upvote⬆ the notebook📓 and leave your feedback🗣 in the comment section below👇🏼.\n### Thanks for reading!"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}