{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt \nimport seaborn as sns \nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/drug-classification/drug200.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploratory Data Analysis","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"With .head() we see that there are ordinal categorical features which we can encode using sklearn's LabelEncoder","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We use .value_counts() to find the distribution of values that our categorical features take on. We can then use sklearn's LabelEncoder to encode all 'object' type features.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"le = LabelEncoder()\nfor i in list(df.columns):\n    if df[i].dtype=='object':\n        df[i] = le.fit_transform(df[i])\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using .info() we see that there are 200 non-null values so there is no need to fill in missing values","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the heatmap below we see that Na_to_K has a very low correlation with the final drug prediction","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(df.corr())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Further investigating the pairplot below, we see that Na_to_K is heavily right skewed, so we will apply a log transformation to scale the values after creating a Random Forest model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(df, hue='Drug')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.loc[:, df.columns != 'Drug']\ny = df.loc[:, 'Drug']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\nmodel1 = RandomForestRegressor().set_params(random_state=23)\nmodel1.fit(X_train, y_train)\npreds1 = model1.predict(X_test)\n\naccuracy_score(y_test, preds1.astype(int))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Before we fit a Logistic Regression model, we scale the Na_to_K column given that it is heavily right skewed","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['log_Na_to_K'] = np.log(df['Na_to_K'] + 1)\ndf.drop('Na_to_K', axis=1, inplace=True)\nsns.pairplot(df, hue='Drug')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.loc[:, df.columns != 'Drug']\ny = df.loc[:, 'Drug']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\nmodel2 = LogisticRegression(max_iter=10000)\nmodel2.fit(X_train, y_train)\npreds2 = model2.predict(X_test)\n\naccuracy_score(y_test, preds2.astype(int))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can do better if we use the Random Forest Classifier given that this is ultimately a classification problem - we end up with 100% accuracy","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"rfc = RandomForestClassifier()\nrfc.fit(X_train,y_train)\npred_2 = rfc.predict(X_test)\nscore_2 = accuracy_score(y_test,pred_2.astype(int))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score_2","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}