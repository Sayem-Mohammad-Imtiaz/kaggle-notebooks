{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport re\nfrom tqdm import tqdm\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/first-gop-debate-twitter-sentiment/Sentiment.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preprocessing the data\n\nOnly keeping 'text', 'sentiment' coulmns and we only classifying Positive or Negative sentiments"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = df[['text','sentiment']]\ndata = data[data.sentiment != 'Neutral']\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**make the text only lower case & remove the special characters**"},{"metadata":{"trusted":true},"cell_type":"code","source":"data['text'] = data['text'].apply(lambda x: x.lower())\ndata['text'] = data['text'].apply((lambda x: re.sub('[^a-z0-9\\s]','',x)))\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**removing \"rt\" notation which means retweeted , but it doesn't give any information to the data**"},{"metadata":{"trusted":true},"cell_type":"code","source":"for idx,row in tqdm(data.iterrows()):\n    row[0] = row[0].replace('rt ',' ')\n\ndata.head()   ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our data distribution has a huge class imbalance"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Positive: ',data[ data['sentiment'] == 'Positive'].size)\nprint('Negative: ',data[ data['sentiment'] == 'Negative'].size)\nx = data['text']\ny = data['sentiment']\nplt.hist(y,bins=3)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Lets try Machine Learning first\n##### We are going to use TfidfVectorizer as a preprocessing step\n\nWhat is a TfidfVectorizer?\nTF (Term Frequency): The number of times a word appears in a document. A higher value means a term appears more often than others.\n\nIDF (Inverse Document Frequency): Words that occur many times a document, but also occur many times in many others, may be irrelevant. IDF is a measure of how significant a term is in the entire corpus.\n\nThe TfidfVectorizer converts a collection of raw documents into a matrix of TF-IDF features."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import PassiveAggressiveClassifier\nfrom sklearn.metrics import accuracy_score\n\n\ny = LabelEncoder().fit_transform(y)\nx_train,x_test,y_train,y_test = train_test_split(x,y,random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vectorizer = TfidfVectorizer(stop_words='english',max_df=0.8)\nv_train=vectorizer.fit_transform(x_train) \nv_test=vectorizer.transform(x_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Using PassiveAggresiveClassifier as our model**\n\nWhat is a PassiveAggressiveClassifier? Passive Aggressive algorithms are online learning algorithms. Such an algorithm remains passive for a correct classification outcome, and turns aggressive in the event of a miscalculation, updating and adjusting. Unlike most other algorithms, it does not converge. Its purpose is to make updates that correct the loss, causing very little change in the norm of the weight vector."},{"metadata":{"trusted":true},"cell_type":"code","source":"pac=PassiveAggressiveClassifier(C=0.01,random_state=42)\npac.fit(v_train,y_train)\n\ny_pred=pac.predict(v_test)\nscore=accuracy_score(y_test,y_pred)\nprint(f'Accuracy: {round(score*100,2)}%')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Turns out that PassiveAggressiveClassifier deals well with our class imbalance dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"pos_cnt, neg_cnt, pos_correct, neg_correct = 0, 0, 0, 0\nfor i in tqdm(range(len(x_test))):\n   \n    if y_pred[i] == y_test[i]:\n        if y_test[i] == 1:\n            neg_correct += 1\n        else:\n            pos_correct += 1\n       \n    if y_pred[i] == 1:\n        neg_cnt += 1\n    else:\n        pos_cnt += 1\n\n\n\nprint(f\"Positive Accuracy {round(pos_correct/pos_cnt*100,2)} %\")\nprint(f\"Negative Accuracy {round(neg_correct/neg_cnt*100,2)} %\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cmnt = [\"I hate summer because it is so sweaty\",\n        \"I love walking in the park at sunset\"]\ncmnt = vectorizer.transform(cmnt)\npred = pac.predict(cmnt)\nfor p in pred:\n    if p == 0:\n        print('Negative')\n    else:\n        print('Positive')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Now lets try DeepLearning"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Embedding, LSTM\nfrom keras.utils.np_utils import to_categorical","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We are using Tokenizer with top 3000 words in our dataset then padding our text with zeros to data have the same length"},{"metadata":{"trusted":true},"cell_type":"code","source":"max_fatures = 3000\ntokenizer = Tokenizer(num_words=max_fatures)\ntokenizer.fit_on_texts(data['text'].values)\nX = tokenizer.texts_to_sequences(data['text'].values)\nX = pad_sequences(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, Y_train, Y_test = train_test_split(X,y, random_state = 42)\nprint(X_train.shape,Y_train.shape)\nprint(X_test.shape,Y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using Embedding layer to extract some meanings of our data > LSTM layer as our model > Desne Layer as a classifying unit"},{"metadata":{"trusted":true},"cell_type":"code","source":"embed_dim = 128\nlstm_out = 128\n\nmodel = Sequential()\nmodel.add(Embedding(max_fatures, embed_dim,input_length = X.shape[1]))\nmodel.add(LSTM(lstm_out))\nmodel.add(Dense(1,activation='sigmoid'))\nmodel.compile(loss = 'binary_crossentropy', optimizer='adam',metrics = ['accuracy'])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 64\nepochs = 10\nmodel.fit(X_train, Y_train, epochs = epochs, batch_size=batch_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score,acc = model.evaluate(X_test, Y_test, verbose = 2, batch_size = batch_size)\nprint(\"score: %.2f\" % (score))\nprint(\"acc: %.2f\" % (acc*100),\"%\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### It doesn't work well with our class imbalance"},{"metadata":{"trusted":true},"cell_type":"code","source":"pos_cnt, neg_cnt, pos_correct, neg_correct = 0, 0, 0, 0\nfor i in tqdm(range(len(X_test))):\n    \n    result = model.predict(X_test[i].reshape(1,X_test.shape[1]),batch_size=1,verbose = 2)[0]\n   \n    if round(result[0]) == Y_test[i]:\n        if Y_test[i] == 0:\n            neg_correct += 1\n        else:\n            pos_correct += 1\n       \n    if Y_test[i] == 0:\n        neg_cnt += 1\n    else:\n        pos_cnt += 1\n\n\n\nprint(f\"Positive Accuracy {round(pos_correct/pos_cnt*100,2)} %\")\nprint(f\"Negative Accuracy {round(neg_correct/neg_cnt*100,2)} %\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cmnt = [\"I hate summer because it is so sweaty\",\n        \"I love walking in the park at sunset\"]\n\ncmnt = tokenizer.texts_to_sequences(cmnt)\ncmnt = pad_sequences(cmnt, maxlen=29, dtype='int32', value=0)\n\nfor p in cmnt:\n    sentiment = model.predict(np.expand_dims(p,axis=0),batch_size=1,verbose = 2)[0]\n    if(np.round(sentiment[0]) == 0):\n        print(\"negative\")\n    elif (np.round(sentiment[0]) == 1):\n        print(\"positive\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}