{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport functools\nimport time\nimport pickle\nfrom datetime import datetime, timedelta\n\nimport tensorflow as tf # 2.5.0\nfrom tensorflow import keras\n\nimport torch # 1.8.1\n\nimport mxnet as mx # 1.8.0\n\nfrom sklearn.metrics import r2_score\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import cross_val_score\n\n\nINPUT_SIZE_STR = 'input_size_str'\ngroup_time = 60000\nsteps = 5\nirrelevant = 2\n\n\ndef read_csv(fname):\n    df = pd.read_csv(fname)\n    df.index = pd.to_datetime(df['time'])\n    df.drop(columns=['time'], inplace=True)\n    return df\n\n\ndef min_max_normalization(df, df_min=None, df_max=None):\n    if df_min is None:\n        df_min = df.min(axis=0)\n    if df_max is None:\n        df_max = df.max(axis=0)\n\n    print(df)\n    df_norm = (df - df_min) / (df_max - df_min)\n\n    if 'time' in df.columns:\n        df_norm['time'] = df['time']  # pd.concat([df['time'], df_norm], axis=1)\n\n    return df_norm, df_min, df_max\n\n\ndef reconstruct_train_data(df):\n    \"\"\"inputs每分钟一行，临近5min数据拼一行，每行数据时间上要求是顺序相连的。\n       如果当前时间是t，则拼成的一行包含[t-5, t-1] 的所有数据.\n       取出y，删除t-1, t-2 的y，和时间列。如果这一行有空值数据则删除此行。\n    \"\"\"\n    if len(df) < steps + 1:\n        return None, None\n\n    if 'time' in df.columns:\n        convert_strtime_datetime = functools.partial(datetime.strptime)\n        time_column = list(map(lambda x: convert_strtime_datetime(x, '%Y-%m-%d %H:%M'), df['time']))\n\n    labels = []\n    data = None\n\n    for t in range(len(df) - 1, steps - 1, -1):\n        time_difference_seconds = int(group_time / 1000)\n        item = np.empty(0)\n        for j in range(1, steps + 1):\n            if (df.index[t] - df.index[t - j]).seconds != time_difference_seconds * j:\n                break\n        else:\n            for i in range(1, steps + 1):\n                item = np.hstack((item, df.iloc[t - i, :-1].values)) \\\n                    if i <= irrelevant else np.hstack((item, df.iloc[t - i, :].values))\n\n            data = np.vstack((data, item)) if data is not None else item\n            labels.append(df.iloc[t, -1])\n    return data, np.asarray(labels).reshape((-1, 1))\n\n\ndef build_lstm_model(X, learning_rate):\n    model = keras.Sequential()\n    model.add(keras.layers.LSTM(64, input_shape=(X.shape[1], X.shape[2])))\n    # keras.layers.LSTM(64, batch_input_shape=(1, 1, X.shape[2]), stateful=True) # keep the cell status\n    model.add(keras.layers.Dense(1))\n    model.add(keras.layers.Dropout(0.01))\n    model.add(keras.layers.Dense(1))\n\n    model.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n                  loss='mae',\n                  metrics=['accuracy'])\n    return model\n\n\ndef train_keras(train_x, train_y, batch_size, num_epochs, learning_rate, validation_x=None, validation_y=None):\n    model = build_lstm_model(train_x, learning_rate)\n    model.summary()\n\n    if validation_x is None:\n        history = model.fit(train_x, train_y,\n                            epochs=num_epochs,\n                            batch_size=batch_size,\n                            verbose=0,\n                            shuffle=False)\n    else:\n        history = model.fit(train_x, train_y,\n                            epochs=num_epochs,\n                            batch_size=batch_size,\n                            validation_data=(validation_x, validation_y),\n                            verbose=0,\n                            shuffle=False)\n    model.save_weights('./keras.h5')\n\n\ndef load_keras(X, Y, learning_rate):\n    start = time.time()\n    lstm = build_lstm_model(X, learning_rate)\n    lstm.load_weights('./keras.h5')\n\n    loaded = time.time()\n    print(f'keras load weights cost: {loaded - start}s')\n\n    Y_hat = lstm.predict(X)\n    print(f'keras predict one average cost: {(time.time() - loaded) / Y_hat.size}s')\n    print(f'keras r2 score: {r2_score(Y_hat, Y)}')\n    print('keras percentage error: {:.4f}%'.format(((Y_hat - Y) / Y).mean() * 100))\n\n\nclass LSTMTORCH(torch.nn.Module):\n    \"\"\"\n        input:   (1148, 1, 43)\n          h0/c0: [1, 1, 64]\n        LSTM:    (43, 64) -> (1148, 1, 64)\n        dense:            -> (1148, 1, 1)\n        decoder:          -> (1148, 1, 1)\n    \"\"\"\n    def __init__(self, num_hiddens, input_size):\n        super(LSTMTORCH, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = num_hiddens\n        self.num_layers = 1\n        self.encoder = torch.nn.LSTM(self.input_size, self.hidden_size, self.num_layers)\n        self.middle = torch.nn.Linear(64, 1)\n        self.drop = torch.nn.Dropout(p=0.01)\n        self.decoder = torch.nn.Linear(1, 1)\n\n    def forward(self, inputs):\n        batch_size = inputs.shape[1]\n        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size)\n        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size)\n        output, hidden = self.encoder(inputs, (h0, c0))\n        outs = self.decoder(self.drop(self.middle(output)))\n        first_dim = outs.size(0)\n        return outs.reshape(first_dim, 1)\n\n\ndef train_torch(train_x, train_y, batch_size, num_epochs, lr, validation_x=None, validation_y=None):\n    start = time.time()\n    train_x_t = torch.tensor(train_x)\n    train_y_t = torch.tensor(train_y)\n    \n    if validation_x is not None:\n        validation_x = torch.tensor(validation_x)\n        validation_y = torch.tensor(validation_y)\n\n    input_size = train_x.shape[2]\n    net = LSTMTORCH(64, input_size)\n    criterion = torch.nn.MSELoss()\n    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n\n    for epoch in range(num_epochs):\n        train_loss_sum = 0.0\n        optimizer.zero_grad()\n        outputs = net(train_x_t)\n        loss = criterion(outputs, train_y_t.float())\n        loss.backward()\n        optimizer.step()\n\n        train_loss_sum += loss.data.item() * train_y_t.size(0)\n\n        if validation_x is not None:\n            validation_loss = criterion(net(validation_x), validation_y.float())\n            validation_loss /= batch_size\n            msg = 'epoch %d, train_loss %.4f, validation_loss %.4f, %.4fs/epoch' % (\n            epoch + 1, loss.mean().sqrt(), validation_loss, (time.time() - start) / (epoch + 1))\n        else:\n            msg = 'epoch %d, train_loss %.4f, %.4fs/epoch' % (epoch + 1, train_loss_sum / batch_size, (time.time() - start) / (epoch + 1))\n        if epoch & 63 == 0:\n            print(msg)\n\n    torch.save(net.state_dict(), 'torch.pt')\n\ndef load_torch(validation_x, validation_y):\n    start = time.time()\n    input_size = validation_x.shape[2]\n    net = LSTMTORCH(64, input_size)\n    net.load_state_dict(torch.load('torch.pt'))\n    net.eval()\n\n    loaded = time.time()\n    print(f'torch load weights cost: {loaded - start}s')\n\n    Y_hat = net(torch.tensor(validation_x))\n    print(f'torch predict one average cost: {(time.time() - loaded) / Y_hat.size().numel()}s')\n\n    Y_hat = Y_hat.detach().numpy()\n    print(f'torch r2 score: {r2_score(Y_hat, validation_y)}')\n    print('torch percentage error: {:.4f}%'.format(((Y_hat - validation_y) / validation_y).mean() * 100))\n\n\n\nclass LSTMNet(mx.gluon.nn.Block):\n    \"\"\"\n        input:   (1077, 1, 51)\n        LSTM:    (51, 64) -> (1077, 1, 64)\n        dense:            -> (1077, 1)\n        decoder:          -> (1077, 1)\n    \"\"\"\n    def __init__(self, num_hiddens, input_size):\n        super(LSTMNet, self).__init__()\n        self.encoder = mx.gluon.rnn.LSTM(hidden_size=num_hiddens, input_size=input_size)\n        self.middle = mx.gluon.nn.Dense(1)\n        self.drop = mx.gluon.nn.Dropout(0.01)\n        self.decoder = mx.gluon.nn.Dense(1)\n\n    def forward(self, inputs):\n        outputs = self.encoder(inputs)\n        outs = self.decoder(self.drop(self.middle(outputs)))\n        return outs\n\n\nclass LSTMNetBetter(mx.gluon.nn.Block):\n    \"\"\"\n        input:     (1436, 1, 43)\n        LSTM1:     (43, 32) -> (1436, 1, 32)\n        LSTM2:     (32, 64) -> (1436, 1, 64)\n        decoder:            -> (1436, 1)\n    \"\"\"\n    def __init__(self, num_hiddens, input_size):\n        \"\"\"\n        :param num_hiddens: 64 is optimized in some algorighm\n        :param input_size:\n        \"\"\"\n        super(LSTMNetBetter, self).__init__()\n        half_hiddens = int(num_hiddens * 0.5)\n        self.encoder = mx.gluon.rnn.LSTM(hidden_size=half_hiddens, input_size=input_size)\n        self.drop = mx.gluon.nn.Dropout(0.01)\n        self.encoder2 = mx.gluon.rnn.LSTM(hidden_size=num_hiddens, input_size=half_hiddens)\n        self.drop2 = mx.gluon.nn.Dropout(0.01)\n        self.decoder = mx.gluon.nn.Dense(1)\n\n    def forward(self, inputs):\n        outputs1 = self.drop(self.encoder(inputs))\n        outputs = self.drop2(self.encoder2(outputs1))\n        outs = self.decoder(outputs)\n        return outs\n\n\ndef training_procedure(features, labels, net, num_epochs, trainer, loss, validation_x=None, validation_y=None):\n    start = time.time()\n    for epoch in range(num_epochs):\n        train_l_sum, n = 0.0, 0\n\n        with mx.autograd.record():\n            y_hat = net(features)\n            l = loss(y_hat, labels)\n\n        l.backward()\n        trainer.step(labels.shape[0])\n\n        train_l_sum += l.sum().asscalar()\n        n += l.size\n\n        if validation_x is not None:\n            validation_loss = loss(net(validation_x), validation_y).mean().asscalar()\n            msg = 'epoch %d, train_loss %.4f, validation_loss %.4f, %.4fs/epoch' % (\n            epoch + 1, train_l_sum / n, validation_loss, (time.time() - start) / (epoch + 1))\n        else:\n            msg = 'epoch %d, train_loss %.4f, %.4fs/epoch' % (epoch + 1, train_l_sum / n, (time.time() - start) / (epoch + 1))\n        if epoch & 63 == 0:\n            print(msg)\n\n\ndef save_mxnet_model(net, fname, input_size):\n    params = net._collect_params_with_prefix()\n    model = {key: val._reduce() for key, val in params.items()}\n    model[INPUT_SIZE_STR] = input_size\n    with open(fname, 'wb') as fd:\n        pickle.dump(model, fd)\n\n\ndef load_mxnet_model(fname):\n    with open(fname, 'rb') as fd:\n        model = pickle.load(fd)\n    input_size = model.pop(INPUT_SIZE_STR)\n\n    net = LSTMNet(64, input_size)\n    params = net._collect_params_with_prefix()\n    for name in model:\n        if name in params:\n            params[name]._load_init(model[name], mx.cpu(), cast_dtype=False, dtype_source='current')\n\n    return net\n\n\ndef run_mxnet(train_x, train_y, num_epochs, learning_rate, validation_x=None, validation_y=None):\n    input_size = train_x.shape[2]\n    net = LSTMNet(64, input_size)\n    net.initialize(mx.init.Xavier())\n    net.hybridize()\n\n    trainer = mx.gluon.Trainer(net.collect_params(), 'adam', {'learning_rate': learning_rate})\n    training_procedure(mx.nd.array(train_x), mx.nd.array(train_y), net,\n        num_epochs, trainer,\n        mx.gluon.loss.L2Loss(),\n        mx.nd.array(validation_x),\n        mx.nd.array(validation_y))\n    save_mxnet_model(net, './mxnet.net', input_size) # net.save_parameters('./mxnet.net')\n\n\ndef load_mxnet(X, Y):\n    start = time.time()\n    net = load_mxnet_model('./mxnet.net')\n    # net = LSTMNetBetter(64, X.shape[2])\n    # net.load_parameters('./mxnet.net')\n    net.hybridize()\n\n    loaded = time.time()\n    print(f'mxnet load weights cost: {loaded - start}s')\n\n    Y_hat = net(mx.nd.array(X)).asnumpy()\n    print(f'mxnet predict one average cost: {(time.time() - loaded) / Y_hat.size}s')\n    print(f'mxnet r2 score: {r2_score(Y_hat, Y)}')\n    print('mxnet percentage error: {:.4f}%'.format(((Y_hat - Y) / Y).mean() * 100))\n\n\ndef linear_model_way(train_x, train_y):\n    lr = LinearRegression()\n    scores = cross_val_score(lr, train_x, train_y, cv=3, scoring='r2')\n    print(scores) # [0.88790513 0.78219812 0.75124809]\n\n\ndef main():\n    df = read_csv('../input/data-lstm-example/datas.csv')\n    df_norm, df_min, df_max = min_max_normalization(df)\n    datas, labels = reconstruct_train_data(df_norm)\n\n    print(df_norm.shape, datas.shape, labels.shape)  # (1441, 9) (1436, 43) (1436, 1)\n    datas = datas.astype('float32')\n\n    validation_factor = int(0.8 * datas.shape[0])\n    train_x = datas[:validation_factor, :]\n    train_y = labels[:validation_factor]\n    validation_x = datas[validation_factor:, :]\n    validation_y = labels[validation_factor:]\n\n    linear_model_way(train_x, train_y)\n\n    batch_size = 1\n    m, n = train_x.shape\n    train_x = train_x.reshape((m, batch_size, n))\n    m, n = validation_x.shape\n    validation_x = validation_x.reshape((m, batch_size, n))\n\n    print(train_x.shape, train_y.shape, validation_x.shape,\n          validation_y.shape)  # (1077, 1, 43) (1077, 1) (359, 1, 43) (359, 1)\n\n    s = time.time()\n\n    # keras load weights cost: 0.18310260772705078s\n    # keras predict one average cost: 0.000997248466300433s\n    # keras r2 score: 0.6448245046829568\n    # keras percentage error: 2.8355%\n    # whole time: 161.03141260147095\n    learning_rate = 0.0001\n    num_epochs = 500\n    train_keras(train_x, train_y, batch_size, num_epochs, learning_rate, validation_x, validation_y)\n    load_keras(validation_x, validation_y, learning_rate)\n    print(f'keras whole time: {time.time() - s}')\n\n    s = time.time()\n    learning_rate = 0.004\n    num_epochs = 500\n    train_torch(train_x, train_y, batch_size, num_epochs, learning_rate, validation_x, validation_y)\n    load_torch(validation_x, validation_y)\n    print(f'torch whole time: {time.time() - s}')\n    \n    # mxnet load weights cost: 0.004607439041137695s\n    # mxnet predict one average cost: 8.711907856975757e-06\n    # mxnet r2 score: 0.32217135154990173\n    # mxnet percentage error: 3.436%\n    # whole time: 1.1297211647033691\n    s = time.time()\n    learning_rate = 0.002\n    num_epochs = 500\n    run_mxnet(train_x, train_y, num_epochs, learning_rate, validation_x, validation_y)\n    load_mxnet(validation_x, validation_y)\n\n    print(f'mxnet whole time: {time.time() - s}')\n\n\n\nmain()\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]}]}