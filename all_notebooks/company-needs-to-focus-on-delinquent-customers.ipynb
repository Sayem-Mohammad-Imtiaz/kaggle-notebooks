{"cells":[{"metadata":{},"cell_type":"markdown","source":"#    AV Janata Hack - Credit Card Payment Default Prediction Hackathon              ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Executive Summary\n\n\nAs a participant in the AnalyticsVidhya Janata Hack - Credit Card Payment Default Prediction Hackathon, I analyzed the data and information on the credit card payment defaults by the customers of a Taiwan based company. Based on extensive data analysis, I wish to present the following summary of my findings, and proposed changes going forward:\n\n1. As per the available data, 22% of the customers shall be defaulting on their payments next month. This is an extremely high number of defaulting customers. The global benchmark for payment default is less than 2%.\n\n2. Even though the problem statement mentions that the payment defaults (ie. customers not paying their bills) was the primary concern that the company was trying to address, the analysis of the data shows that actually the problem is the customers paying less than the \"Minimum Billing Amount\" resulting in large \"Pending Amounts\" against such \"Delinquent\" customers month-after-month.\n\n3. The company gave a credit of 3.5 billion NT dollars to its customers. \n\n4. The total payment pending from delinquent customers is 5 billion NT dollars. \n\n5. The total payment pending from defulting customers is 1.2 billion dollars.\n\n6. The pending amount keeps increasing by an order of magnitude as the customers keep getting delinquent month after month. This is also because there is a regular increase in the number of customers who are getting delinquent multiple times.\n\n7. At the same time, the pending amount keeps decreasing by an order of magnitude as the customers keep defaulting month after month. This is also because there is a regular decrease in the number of customers who are defaulting multiple times.\n\n**We can clearly establish that certain basic \"Credit Controls\" have not been observed by the company. Obviously, they have allowed customers to continue to use cards beyond the authorized credit limits.**\n\n**The reason is very simple - the entire focus has been on customers who shall not pay next month, rather than on customers who are paying less than the \"Minimum Amounts\" and falling into \"Delinquent\" status month-after-month.**\n\n**My recommendation is that the focus of the company should shift to predicting customers who shall pay less than the Minimum Amount to keep their cards \"Active\", rather than on customers who make \"No Payments.\"**\n\n**This change in strategy and focus will deliver far greater financial and bottom line benefits to the company.**\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Problem Statement\n\n\n\nPredicting accurately which customers are most probable to default represents a significant business opportunity for all banks. Bank cards are the most common credit card type in Taiwan, which emphasizes the impact of risk prediction on both the consumers and banks. \n\nThis would inform the bank’s decisions on criteria to approve a credit card application and also decide upon what credit limit to provide.\n\nThis dataset contains information on default payments, demographic factors, credit data, history of payment, and bill statements of credit card clients in Taiwan from April 2005 to September 2005. \n\nUsing the information given, predict the probability of a customer defaulting in the next month.\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Data Dictionary\n\nBelow is a thorough description of the 25 features/variables:\n\n1. **ID :** Customer ID\n\n2. **LIMIT_BAL:** Amount of the given credit (NT dollar): it includes both the individual consumer credit and his/her family (supplementary) credit.\n\n3. **SEX :** Gender (1 = male; 2 = female).\n\n4. **EDUCATION:** Education (1 = graduate school; 2 = university; 3 = high school; 4 = others).\n\n5. **MARRIAGE :** Marital status (1 = married; 2 = single; 3 = others).\n\n6. **AGE :** Age (year).\n\n7. **PAY_0 - PAY_6:** History of past payments. The past monthly payment records (from April to September, 2005) are as follows: \n\n* PAY_0 = the repayment status in September, 2005 \n* PAY_2 = the repayment status in August, 2005; . . .;\n* PAY_6 = the repayment status in April, 2005. \n\nThe measurement scale for the repayment status is: -1 = pay duly; 1 = payment delay for one month; 2 = payment delay for two months; . . .; 8 = payment delay for eight months; 9 = payment delay for nine months and above.\n\n\n8. **BILL_AMT1 - BILL_AMT6:** Amount of bill statement (NT dollar) as follows : \n\n* BILL_AMT1 = amount of bill statement in September, 2005 \n* BILL_AMT2 = amount of bill statement in August, 2005; . . .; \n* BILL_AMT6 = amount of bill statement in April, 2005.\n\n9. **PAY_AMT1 - PAY_AMT6:** Amount of previous payment (NT dollar) as follows : \n\n* PAY_AMT1 = amount paid in September, 2005; \n* PAY_AMT2 = amount paid in August, 2005; . . .;\n* PAY_AMT6 = amount paid in April, 2005.\n\n10. **default_payment_next_month:** Target variable - Default Payment (Yes = 1, No = 0)\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#                               Hypothesis Generation\n\n\nBased on the information provided about the data, and general understanding of the credit card business, let us try to list out all the possible factors that can affect the outcome (identifying who will default on payment) :\n\n1. It should not matter whether customer is a male or a female when it comes to defaulting on payments. We assume equal probability of defaulters being males or females.\n\n2. Married customers are less likely to default than single customers.\n\n3. Younger customers (less than 25 years of age) are more likely to default, than senior customers.\n\n4. Customers who are senior in age, education and job profile should have higher billing, than the younger customers who are still studying or are unemployed. \n\n5. Lower the billing, lower should be the probability of a payment default. Lower the billing, higher should be the probability of making full payment on time, and hence lower the probability of payment default.\n\n6. Higher the billing, higher should be the probability of payment default irrespective of credit limit.  Higher the billing, lower should be the probability of making full payment on time, and hence higher the probability of payment default.\n\n7. Higher the billing, higher the credit limit, and hence higher the probability that customer shall not make regular payments. Lower the billing, lower the credit limit, hence lower the probability that customer shall make regular payments.\n\n8. We should expect about 2% delinquency, which is a normal and acceptable delinquency rate in the cards industry.\n\n9. Not paying even the Minimum Amount required to be paid to keep the card from getting blocked should point to increasing probability of payment default in next cycle.\n\n10. Paying only the Minimum Amount month after month should result in higher probability of eventual payment default.\n\nAs Data Scientists, we should discuss these hypotheses with the business and have full concurrance with them. We should be ready to make any changes to these so that there are no surprises when we actually present the analysis facts to the business.\n\nAlso, it is a well known fact that in any data science / machine learning project, more than 70% of the time gets spent on data gathering, data exploration, data cleaning, data transformation, data enhancement and detailed data analysis.\n\nOnly when the data is understood clearly do we proceed to apply ML modeling algorithms.\n\nSo.....let us spend the necessary 70% of the time in understanding our data.\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Loading Packages and data.\n\n\nFor this Janata Hack problem, we have been given three CSV files: train, test and sample submission.\n\nTrain file will be used for training the model, i.e. our model will learn from this file. It contains all the independent variables and the target variable.\n\nTest file contains all the independent variables, but not the target variable. We will apply the model to predict the target variable for the test data.\n\nSample submission file contains the format in which we have to submit our predictions.\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd \nimport numpy as np \nimport seaborn as sns \nimport matplotlib.pyplot as plt \n%matplotlib inline \n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reading data\n\ntrain_df=pd.read_csv(\"../input/av-janata-hack-payment-default-prediction/train_20D8GL3.csv\") \ntest_df=pd.read_csv(\"../input/av-janata-hack-payment-default-prediction/test_O6kKpvt.csv\")\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let’s make a copy of train and test data so that even if we have to make any changes in these datasets we would not lose the original datasets.\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"original_train_df=train_df.copy() \noriginal_test_df=test_df.copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#                        Understanding the Data\n\n\nWe will look at the structure of the train and test datasets. \n\nFirstly, we will check the features present in our data and then we will look at their data types.\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have 24 independent variables and 1 target variable, i.e. 'default_payment_next_month' in the train dataset. \n\nWe will predict the Payment Default next month using the model built using the train data.\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that all our data is already in numeric (integer) format.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.shape, test_df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have 21000 rows and 25 columns in the train dataset and 9000 rows and 24 columns in test dataset.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##       Proposed Enhancements to the data provided\n\nFrom the meta-data provided, and looking at the first few rows of the training dataset (and hence the test dataset), we find the following serious anamolies in the dataset :\n\n1. The PAY_0 column-name should actually be PAY_1. We shall rename it.\n\n2. The columns PAY_1 to PAY_6 are said to be capturing the \"Payment delay in months\". For example, PAY_6 represents \"the repayment status in April, 2005\", and value of 7 in that column would mean \"payment delay for seven months\", whereas a value of -1 in that column would mean \"paid duly\". We find that many of the values in PAY_1 to PAY_6 are zeros and -2. and the meta data has not specified any meanings to 0 and -2 in these columns.\n\nNow when it comes to credit card payments, the following types of payments are generally captured by such organizations :\n\n1.  **\"No Payment\"**  indicating that the customer refused to make any payment against the bill.\n\n2.  **\"Minimum Payment\"**  representing about 5-10% against the total billed amount which the customer has to pay within the credit period applicable to the customer to keep the card current and active.\n\n3. **\"Full Payment\"**  representing full payment against the total billed amount which the customer has to pay within the credit period applicable.\n\n4. **\"Part Payment\"**  representing any fraction of the payment against the total billed amount which the customer may decide to pay.\n\n5. Such **\"Part Payment\"**  can be less than the **\"Minimum Payment\"**, equal to the minimum payment or more than the minimum payment but less than the total amount.\n\n6. All such **\"Pending Payments\"** are then carried forward to the next bill, with additional applicable finance and interest charges on such pending amount.\n\n7. Customer can keep paying **\"Part Payments\"** over many months and keep accumulating his **\"Total Pending Amount\"** till his **\"Credit Balance Limit\"** is reached, in the process incurring additional monthly finance and interest charges.\n\nBased on the common understanding of the credit card billing cycle, we shall utilize the data available in the BILL_AMT1 to BILL_AMT6 columns, PAY_AMT1 to PAY_AMT6 columns, and LIMIT_BAL column to create the following additional variables to enrich our predictive model  :\n\nEven though it is not explicitly mentioned anywhere, we are assuming that all the bills have a 1-month credit period in which customers need to make payments.\n\n1.  **MIN_AMT_1 to MIN_AMT_6 :**\n\nThese values shall represent the \"Minimum Billing\" amount that needs to be paid within the next 1-month credit period for the card to be kept active. We are taking 10% as the Minimum amount that needs to be paid to keep the card live. This means that MIN_AMT_1 shall represent the minimum amount (10%) of the total pending amount till August-2005 to be paid in September-2005.\n\n2.  **PENDING_AMT_1 to PENDING_AMT_6 :**\n\nThese integer variables shall represent the \"Total Pending Amount\" for the previous 6 bills. This is because we have the data only for the 6 months. Hence PENDING_AMT_1 shall represent total pending amount in September-2005 - which is the accumulated unpaid amounts for the last 6 months (ie. since April-2005), but PENDING_AMT_2 shall represent total pending amount in August-2005 since April-2005 and so on.\n\n3.  **DELINQ_1 to DELINQ_6 :**\n\nThese indicators shall represent how many times the customer has been classified into \"Delinquent\" state. As per the credit card business, customers move to the delinquency state when they fail to pay the Minimum Billing Amount necessary to keep the card alive. Assuming 1-month credit period for making the payment, this would mean that if the customer does not pay more than the MIN_AMT bill of August-2005 by September-2005, the customer would be classified as \"Delinquent\". \n\nPlease note that any amount paid which is less than the required MIN_AMT is considered as no payment, and customer is moved to \"Delinquent\" state.\n\nHence, DELINQ_1 would represent how many times customer has been in delinquent state since April-2005.\n\n4. **NO_PMNT_1 to NO_PMNT_6 :**\n\nThese indicators shall capture how many times the customer made \"No Payments\". This would mean that if the customer does not pay any amount (0 value) against the bill of August-2005 by September-2005, the customer payment status would be moved to \"No Payment\" state.\n\nHence, NO_PMNT1 would represent how many times customer made \"No Payments\" since April-2005.\n\n5. **AVG_6MTH_BAL :**\n\nThis value shall represent mean (average) value of PENDING_AMT_1 over a 6 month period.\n\n6. **CREDIT_UTILIZATION_RATIO :**\n\nAverage 6 month balance (AVG_6MTH_BAL) divided by the individual’s credit limit (LIMIT_BAL). As per the credit card industry, anything <= .3 is considered good, whereas anything closer to 1 is considered very risky.\n\nFrom the data given, we have been asked to predict customers who are likely to default in their payments next month. \n\nConsidering that the last bill data in the dataset is for September-2005, we have been asked to predict customers who shall default in Oct-2005.\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Create additional variables for the enhancement of the data\n\n\nFirst of all, we shall proceed to create the additional variables from the existing data as described in detail above.\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.rename(columns = {'PAY_0':'PAY_1'}, inplace = True)\ntest_df.rename(columns = {'PAY_0':'PAY_1'}, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We now proceed to create the additional variables that were explained earlier.\n\nLet us start with creating MIN_AMT6 variable which shall represent the minimum amount of the April-2005 bill to be paid by May-2005.\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['MIN_AMT6']=train_df['BILL_AMT6']*0.1\ntest_df['MIN_AMT6']=test_df['BILL_AMT6']*0.1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\nLet us create PENDING_AMT6 variable which shall represent (April-2005 Bill Amount - April-2005 Payment Amount). \n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['PENDING_AMT6']=train_df['BILL_AMT6'] - train_df['PAY_AMT6']\ntest_df['PENDING_AMT6']=test_df['BILL_AMT6'] - test_df['PAY_AMT6']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us create MIN_AMT5 variable which shall represent the minimum of the pending amount till May-2005 to be paid by June-2005.\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['MIN_AMT5']=train_df['PENDING_AMT6']*0.1\ntest_df['MIN_AMT5']=test_df['PENDING_AMT6']*0.1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us create DELINQ_5 variable which shall represent whether the customer has become 'Delinquent' because customer has failed to pay more than the Minimum amount.\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['DELINQ_5'] = np.where((train_df['PAY_AMT5']>0) & (train_df['PAY_AMT5']<train_df['MIN_AMT6']),1,0)\ntest_df['DELINQ_5'] = np.where((test_df['PAY_AMT5']>0) & (test_df['PAY_AMT5']<test_df['MIN_AMT6']),1,0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us create NO_PMNT5 variable which shall represent whether the customer did not make any payment in May-2005.\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['NO_PMNT5']=np.where(train_df['PAY_AMT5'] == 0,1,0)\ntest_df['NO_PMNT5']=np.where(test_df['PAY_AMT5'] == 0,1,0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us create PENDING_AMT5 variable. \n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['PENDING_AMT5'] = (train_df['BILL_AMT5']+train_df['BILL_AMT6']) - (train_df['PAY_AMT5']+train_df['PAY_AMT6'])\ntest_df['PENDING_AMT5'] = (test_df['BILL_AMT5']+test_df['BILL_AMT6']) - (test_df['PAY_AMT5']+test_df['PAY_AMT6'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us create all other derived variables in a similar manner.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['MIN_AMT4']=train_df['PENDING_AMT5']*0.1\ntest_df['MIN_AMT4']=test_df['PENDING_AMT5']*0.1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['DELINQ_4'] = np.where((train_df['PAY_AMT4']>0) & (train_df['PAY_AMT4']<train_df['MIN_AMT5']),1,0) + train_df['DELINQ_5']\ntest_df['DELINQ_4'] = np.where((test_df['PAY_AMT4']>0) & (test_df['PAY_AMT4']<test_df['MIN_AMT5']),1,0) + test_df['DELINQ_5']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['NO_PMNT4']=np.where(train_df['PAY_AMT4'] == 0,1,0) + train_df['NO_PMNT5']\ntest_df['NO_PMNT4']=np.where(test_df['PAY_AMT4'] == 0,1,0) + test_df['NO_PMNT5']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['PENDING_AMT4'] = (train_df['BILL_AMT4']+train_df['BILL_AMT5']+train_df['BILL_AMT6']) - (train_df['PAY_AMT4']+train_df['PAY_AMT5']+train_df['PAY_AMT6'])\ntest_df['PENDING_AMT4'] = (test_df['BILL_AMT4']+test_df['BILL_AMT5']+test_df['BILL_AMT6']) - (test_df['PAY_AMT4']+test_df['PAY_AMT5']+test_df['PAY_AMT6'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['MIN_AMT3']=train_df['PENDING_AMT4']*0.1\ntest_df['MIN_AMT3']=test_df['PENDING_AMT4']*0.1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['DELINQ_3'] = np.where((train_df['PAY_AMT3']>0) & (train_df['PAY_AMT3']<train_df['MIN_AMT4']),1,0) + train_df['DELINQ_4']\ntest_df['DELINQ_3'] = np.where((test_df['PAY_AMT3']>0) & (test_df['PAY_AMT3']<test_df['MIN_AMT4']),1,0) + test_df['DELINQ_4']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['NO_PMNT3']=np.where(train_df['PAY_AMT3'] == 0,1,0) + train_df['NO_PMNT4']\ntest_df['NO_PMNT3']=np.where(test_df['PAY_AMT3'] == 0,1,0) + test_df['NO_PMNT4']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['PENDING_AMT3'] = (train_df['BILL_AMT3']+train_df['BILL_AMT4']+train_df['BILL_AMT5']+train_df['BILL_AMT6']) - (train_df['PAY_AMT3']+train_df['PAY_AMT4']+train_df['PAY_AMT5']+train_df['PAY_AMT6'])\ntest_df['PENDING_AMT3'] = (test_df['BILL_AMT3']+test_df['BILL_AMT4']+test_df['BILL_AMT5']+test_df['BILL_AMT6']) - (test_df['PAY_AMT3']+test_df['PAY_AMT4']+test_df['PAY_AMT5']+test_df['PAY_AMT6'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['MIN_AMT2']=train_df['PENDING_AMT3']*0.1\ntest_df['MIN_AMT2']=test_df['PENDING_AMT3']*0.1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['DELINQ_2'] = np.where((train_df['PAY_AMT2']>0) & (train_df['PAY_AMT2']<train_df['MIN_AMT3']),1,0) + train_df['DELINQ_3']\ntest_df['DELINQ_2'] = np.where((test_df['PAY_AMT2']>0) & (test_df['PAY_AMT2']<test_df['MIN_AMT3']),1,0) + test_df['DELINQ_3']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['NO_PMNT2']=np.where(train_df['PAY_AMT2'] == 0,1,0) + train_df['NO_PMNT3']\ntest_df['NO_PMNT2']=np.where(test_df['PAY_AMT2'] == 0,1,0) + test_df['NO_PMNT3']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['PENDING_AMT2'] = (train_df['BILL_AMT2']+train_df['BILL_AMT3']+train_df['BILL_AMT4']+train_df['BILL_AMT5']+train_df['BILL_AMT6']) - (train_df['PAY_AMT2']+train_df['PAY_AMT3']+train_df['PAY_AMT4']+train_df['PAY_AMT5']+train_df['PAY_AMT6'])\ntest_df['PENDING_AMT2'] = (test_df['BILL_AMT2']+test_df['BILL_AMT3']+test_df['BILL_AMT4']+test_df['BILL_AMT5']+test_df['BILL_AMT6']) - (test_df['PAY_AMT2']+test_df['PAY_AMT3']+test_df['PAY_AMT4']+test_df['PAY_AMT5']+test_df['PAY_AMT6'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['MIN_AMT1']=train_df['PENDING_AMT2']*0.1\ntest_df['MIN_AMT1']=test_df['PENDING_AMT2']*0.1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['DELINQ_1'] = np.where((train_df['PAY_AMT1']>0) & (train_df['PAY_AMT1']<train_df['MIN_AMT2']),1,0) + train_df['DELINQ_2']\ntest_df['DELINQ_1'] = np.where((test_df['PAY_AMT1']>0) & (test_df['PAY_AMT1']<test_df['MIN_AMT2']),1,0) + test_df['DELINQ_2']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['NO_PMNT1']=np.where(train_df['PAY_AMT1'] == 0,1,0) + train_df['NO_PMNT2']\ntest_df['NO_PMNT1']=np.where(test_df['PAY_AMT1'] == 0,1,0) + test_df['NO_PMNT2']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['PENDING_AMT1'] = (train_df['BILL_AMT1']+train_df['BILL_AMT2']+train_df['BILL_AMT3']+train_df['BILL_AMT4']+train_df['BILL_AMT5']+train_df['BILL_AMT6']) - (train_df['PAY_AMT1']+train_df['PAY_AMT2']+train_df['PAY_AMT3']+train_df['PAY_AMT4']+train_df['PAY_AMT5']+train_df['PAY_AMT6'])\ntest_df['PENDING_AMT1'] = (test_df['BILL_AMT1']+test_df['BILL_AMT2']+test_df['BILL_AMT3']+test_df['BILL_AMT4']+test_df['BILL_AMT5']+test_df['BILL_AMT6']) - (test_df['PAY_AMT1']+test_df['PAY_AMT2']+test_df['PAY_AMT3']+test_df['PAY_AMT4']+test_df['PAY_AMT5']+test_df['PAY_AMT6'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us create AVG_6MTH_BAL variable which shall represent mean / average value of Amount owed (PENDING_AMT1) by the customer over a 6 month period.\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['AVG_6MTH_BAL'] = train_df['PENDING_AMT1']/6\ntest_df['AVG_6MTH_BAL'] = test_df['PENDING_AMT1']/6","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us create CREDIT_UTIL_RATIO variable which shall represent (average 6 month balance divided by the individual’s credit limit)\n\nPlease note that a Credit utilization ratio <= .3 is considered good, whereas anything close to 1 or more is considered very risky.\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['CREDIT_UTIL_RATIO'] = train_df['AVG_6MTH_BAL']/train_df['LIMIT_BAL']\ntest_df['CREDIT_UTIL_RATIO'] = test_df['AVG_6MTH_BAL']/test_df['LIMIT_BAL']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looking the Age distribution of the customers, we have decided to divide the customers according to their Age Bins as follows.\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"bins=[0,20,30,40,50,60,70,80] \ngroup=['VERY_YOUNG','YOUNG','MIDDLE','SENIOR','VERY_SENIOR','RETIRED','ELDERLY'] \n\ntrain_df['AGE_BIN']=pd.cut(train_df['AGE'],bins,labels=group)\ntest_df['AGE_BIN']=pd.cut(test_df['AGE'],bins,labels=group)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"original_columns = train_df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# delete un-necessary columns\n\ncolumns_to_delete = ['ID','AGE']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_columns = list(set(original_columns)-set(columns_to_delete))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us create final version of the training data set.\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"final_train_df = train_df[final_columns]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The test_df does not contain the target variable, so we will make appropriate change for the test_df.\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"original_columns = test_df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_columns = list(set(original_columns)-set(columns_to_delete))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_test_df = test_df[final_columns]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"With the above, we have created all the required variables and now we can proceed to Data Exploration & Analysis stage, and gather some Insights about this business.\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#        Data Exploration & Data Analysis\n\n\nNow, we will check the final features present in our data, look at their range of values, distributions, missing values, any outliers etc.\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"final_train_df.shape, final_test_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let us look at the datatypes of the training data columns.\n\nfinal_train_df.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let us look at the datatypes of the test data columns.\n\nfinal_test_df.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that all the columns are of numeric type except column AGE_BIN. \n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"##   Check missing data\n\n\nLet's check if there is any missing data.\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"total = final_train_df.isnull().sum().sort_values(ascending = False)\npercent = (final_train_df.isnull().sum()/final_train_df.isnull().count()*100).sort_values(ascending = False)\npd.concat([total, percent], axis=1, keys=['Total', 'Percent']).transpose()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is no missing data in the entire dataset, so we have nothing to worry about substituting for any missing values.\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"final_train_df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are 21,000 distinct credit card clients.\n\nThe average value for the amount of credit card limit is 167,214. The standard deviation is unusually large (at 128,965), and the max value is 800,000.\n\nWe observe -ve values in BILL_AMT* and PAY_AMT*\n\nLet us analyze all of this in detail.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#                     Univariate Analysis\n\n\n\n## Target Variable\n\n\nWe will first look at the target variable, i.e., default_payment_next_month. As it is a categorical variable, let us look at its frequency table, percentage distribution and bar plot.\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"final_train_df['default_payment_next_month'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(final_train_df['default_payment_next_month'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us print proportions instead of number.\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"final_train_df['default_payment_next_month'].value_counts(normalize=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_train_df['default_payment_next_month'].value_counts(normalize=True).plot.bar()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that 22% of the customers shall be defaulting on their payments next month.\n\nThe global benchmark for payment default is less than 2%.\n\nThis is an extremely high number of defaulting customers.\n\nAlso, while this can be called an \"Imbalanced dataset\", this should have been far more imbalanced if we consider that only 2-3% customers should have been classified as defaulting.\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Now lets visualize each variable separately. \n\nLet’s visualize the categorical and ordinal features first.\n\n\n##              Independent Variable (Categorical)\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"final_train_df['SEX'].value_counts(normalize=True).plot.bar(figsize=(10,5), title= 'SEX') \nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It can be inferred from the above bar plot that about 60% of the customers are females.\n\nPlease note that our initial hypothesis was that our male - female ratio is same. Clearly the given data violates our initial hypothesis.\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Now let’s visualize the ordinal variables.\n\n\n##                        Independent Variables (Ordinal)\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(1) \nplt.subplot(221) \nfinal_train_df['MARRIAGE'].value_counts(normalize=True).plot.bar(figsize=(20,10), title= 'MARRIAGE') \nplt.subplot(222) \nfinal_train_df['EDUCATION'].value_counts(normalize=True).plot.bar(title= 'EDUCATION') \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(1)\nplt.subplot(221) \nfinal_train_df['AGE_BIN'].value_counts(normalize=True).plot.bar(figsize=(20,10), title= 'AGE') \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Following inferences can be made from the above bar plots:\n\n1. About 55% of the customers are 'Single' status\n\n2. About 44% of the customers are 'Married' status\n\n3. About 0.5% of the customers have 'Other' \n\n4. About 0.5% of the customers have 'Invalid' status (with a Value = 0)\n\n5. About 50% of the customers have 'University' level education\n\n6. About 35% of the customers have 'Graduate School' level education\n\n7. About 14% of the customers have 'High School' level education\n\n8. About 0.4% of the customers have 'Other' level education\n\n9. About 0.6% of the customers have 'Invalid' level education (with Values = 0, 5, 6)\n\n10. Majority of the customers are in 20 - 50 years Age bracket\n\n11. We also see lot customers in 51-75 years Age bracket\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"##                   Independent Variable (Numerical)\n\n\nNow lets visualize the numerical variables. Lets look at the distribution of 'LIMIT_BAL' first.\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(1) \nplt.subplot(121) \nsns.distplot(final_train_df['LIMIT_BAL']); \nplt.subplot(122) \nfinal_train_df['LIMIT_BAL'].plot.box(figsize=(16,5)) \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us try to place the LIMIT_BAL in 20 bins and see the distribution.\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"bins = 20\nplt.hist(final_train_df.LIMIT_BAL, bins = bins, label = 'Total', alpha=0.5)\n\n# plt.hist(data.LIMIT_BAL[data['default.payment.next.month'] == 1], bins = bins, color='b',label = 'Default')\n\nplt.xlabel('Credit Limit (NT dollar)');plt.ylabel('Number of Accounts')\nplt.title('Fig.1 : Credit Limit ',fontweight=\"bold\", size=12)\nplt.legend();\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can clearly see that very few customers have credit limit exceeding 6,00,000.\n\nLet us count how many customers belong to this categoty.\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"len(final_train_df[final_train_df['LIMIT_BAL']>= 600000])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that we have 57 customers out of 21000 which appear as outliers, which is about 0.03%","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"len(final_train_df[(final_train_df['LIMIT_BAL']>= 600000) & (final_train_df['default_payment_next_month']== 1)])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We also see that out of this 57 outlier customers, only 7 are candidates for 'default_payment_next_month'. \n\nSo....we certainly reserve the option to remove these outliers if we find that this can improve our prediction ability.\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"It can be inferred that most of the data in the distribution of 'LIMIT_BAL' is towards left (right-skewed), which means it is not normally distributed. \n\nThe boxplot confirms the presence of a lot of outliers/extreme values. \n\nPlease note that from the Box Plot, the inference about the outliers is drawn based on the mathematical hypothesis that anything outside of \"Inter-Quartile-Range (IQR)\" is considered Outlier. \n\nFrom the Business perspective this may not always be true. There could be genuin transactions with large values in the dataset. Removing them may not be the correct way in such cases.\n\nAlways the Business needs to be consulted about the correctness of these values.\n\nIn case they seem to be data entry errors, they can safely be either removed (OR) substituted with mean / mode / median values.\n\n**(OR)**\n\nIf we find that there are relatively very few (say less that 3-5%) significant outliers whose presence is likely to bias the model, we can take a decision to remove these outliers.\n\nWe will study these outliers carefully.\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Similar to LIMIT_BAL, let us see if we also have outliers for BILL_AMT* and PAY_AMT* variables.\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(1) \nplt.subplot(121) \nsns.distplot(final_train_df['BILL_AMT1']); \nplt.subplot(122) \nfinal_train_df['BILL_AMT1'].plot.box(figsize=(16,5)) \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bins = 20\nplt.hist(final_train_df.BILL_AMT1, bins = bins, label = 'Total', alpha=0.8)\n\nplt.xlabel('Bill Amount Sep-2005 (NT dollar)');plt.ylabel('Number of Accounts')\nplt.title('Bill Amount Sep-2005 ',fontweight=\"bold\", size=12)\nplt.legend();\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(1) \nplt.subplot(121) \nsns.distplot(final_train_df['BILL_AMT2']); \nplt.subplot(122) \nfinal_train_df['BILL_AMT2'].plot.box(figsize=(16,5)) \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bins = 20\nplt.hist(final_train_df.BILL_AMT2, bins = bins, label = 'Total', alpha=0.8)\n\nplt.xlabel('Bill Amount Aug-2005 (NT dollar)');plt.ylabel('Number of Accounts')\nplt.title('Bill Amount Aug-2005 ',fontweight=\"bold\", size=12)\nplt.legend();\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(1) \nplt.subplot(121) \nsns.distplot(final_train_df['BILL_AMT3']); \nplt.subplot(122) \nfinal_train_df['BILL_AMT3'].plot.box(figsize=(16,5)) \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bins = 20\nplt.hist(final_train_df.BILL_AMT3, bins = bins, label = 'Total', alpha=0.8)\n\nplt.xlabel('Bill Amount July-2005 (NT dollar)');plt.ylabel('Number of Accounts')\nplt.title('Bill Amount July-2005 ',fontweight=\"bold\", size=12)\nplt.legend();\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(1) \nplt.subplot(121) \nsns.distplot(final_train_df['BILL_AMT4']); \nplt.subplot(122) \nfinal_train_df['BILL_AMT4'].plot.box(figsize=(16,5)) \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bins = 20\nplt.hist(final_train_df.BILL_AMT4, bins = bins, label = 'Total', alpha=0.8)\n\nplt.xlabel('Bill Amount June-2005 (NT dollar)');plt.ylabel('Number of Accounts')\nplt.title('Bill Amount June-2005 ',fontweight=\"bold\", size=12)\nplt.legend();\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(1) \nplt.subplot(121) \nsns.distplot(final_train_df['BILL_AMT5']); \nplt.subplot(122) \nfinal_train_df['BILL_AMT5'].plot.box(figsize=(16,5)) \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bins = 20\nplt.hist(final_train_df.BILL_AMT5, bins = bins, label = 'Total', alpha=0.8)\n\nplt.xlabel('Bill Amount May-2005 (NT dollar)');plt.ylabel('Number of Accounts')\nplt.title('Bill Amount May-2005 ',fontweight=\"bold\", size=12)\nplt.legend();\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(1) \nplt.subplot(121) \nsns.distplot(final_train_df['BILL_AMT6']); \nplt.subplot(122) \nfinal_train_df['BILL_AMT6'].plot.box(figsize=(16,5)) \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bins = 20\nplt.hist(final_train_df.BILL_AMT6, bins = bins, label = 'Total', alpha=0.8)\n\nplt.xlabel('Bill Amount April-2005 (NT dollar)');plt.ylabel('Number of Accounts')\nplt.title('Bill Amount April-2005 ',fontweight=\"bold\", size=12)\nplt.legend();\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The plots for all the BILL_AMT* show very skewed distributions.\n\nWe should closely analyze the customers with BILL_AMT* >= 200000, as well as with BILL_AMT* < 0.\n\nLet us loot at the -ve values first.\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"final_train_df[final_train_df['BILL_AMT1'] <0][['BILL_AMT1','BILL_AMT2','BILL_AMT3','BILL_AMT4','BILL_AMT5','BILL_AMT6']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_train_df[final_train_df['BILL_AMT2'] <0][['BILL_AMT1','BILL_AMT2','BILL_AMT3','BILL_AMT4','BILL_AMT5','BILL_AMT6']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We clearly see -ve bill values indicating perhaps some amount of excess payment in previous months.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for billamt in ['BILL_AMT1','BILL_AMT2','BILL_AMT3','BILL_AMT4','BILL_AMT5','BILL_AMT6']:\n    print(\"\\nNo. of Customers with \", billamt, \" >= 200000 : \", len(final_train_df[final_train_df[billamt] >=200000]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for billamt in ['BILL_AMT1','BILL_AMT2','BILL_AMT3','BILL_AMT4','BILL_AMT5','BILL_AMT6']:\n    print(\"\\nNo. of Customers with \", billamt, \" >= 400000 : \", len(final_train_df[final_train_df[billamt] >=400000]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for billamt in ['BILL_AMT1','BILL_AMT2','BILL_AMT3','BILL_AMT4','BILL_AMT5','BILL_AMT6']:\n    print(\"\\nNo. of Customers with \", billamt, \" >= 600000 : \", len(final_train_df[final_train_df[billamt] >=600000]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We observe from Sept-2005 billing that 6 customers have billing in excess of 600,000, 119 customers have billing in excess of 400,000 and 1070 customers have billing in excess of 200,000. \n\nWith good understanding of the BILL_AMT* variables, we will now look at the PAY_AMT* variables.\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(1) \nplt.subplot(121) \nsns.distplot(final_train_df['PAY_AMT1']); \nplt.subplot(122) \nfinal_train_df['PAY_AMT1'].plot.box(figsize=(16,5)) \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bins = 20\nplt.hist(final_train_df.PAY_AMT1, bins = bins, label = 'Total', alpha=0.8)\n\nplt.xlabel('Payment Amount Sept-2005 (NT dollar)');plt.ylabel('Number of Accounts')\nplt.title('Payment Amount Sept-2005 ',fontweight=\"bold\", size=12)\nplt.legend();\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(1) \nplt.subplot(121) \nsns.distplot(final_train_df['PAY_AMT2']); \nplt.subplot(122) \nfinal_train_df['PAY_AMT2'].plot.box(figsize=(16,5)) \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bins = 20\nplt.hist(final_train_df.PAY_AMT2, bins = bins, label = 'Total', alpha=0.8)\n\nplt.xlabel('Payment Amount Aug-2005 (NT dollar)');plt.ylabel('Number of Accounts')\nplt.title('Payment Amount Aug-2005 ',fontweight=\"bold\", size=12)\nplt.legend();\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(1) \nplt.subplot(121) \nsns.distplot(final_train_df['PAY_AMT3']); \nplt.subplot(122) \nfinal_train_df['PAY_AMT3'].plot.box(figsize=(16,5)) \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bins = 20\nplt.hist(final_train_df.PAY_AMT3, bins = bins, label = 'Total', alpha=0.8)\n\nplt.xlabel('Payment Amount July-2005 (NT dollar)');plt.ylabel('Number of Accounts')\nplt.title('Payment Amount July-2005 ',fontweight=\"bold\", size=12)\nplt.legend();\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(1) \nplt.subplot(121) \nsns.distplot(final_train_df['PAY_AMT4']); \nplt.subplot(122) \nfinal_train_df['PAY_AMT4'].plot.box(figsize=(16,5)) \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bins = 20\nplt.hist(final_train_df.PAY_AMT4, bins = bins, label = 'Total', alpha=0.8)\n\nplt.xlabel('Payment Amount June-2005 ');plt.ylabel('Number of Accounts')\nplt.title('Payment Amount June-2005 ',fontweight=\"bold\", size=12)\nplt.legend();\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(1) \nplt.subplot(121) \nsns.distplot(final_train_df['PAY_AMT5']); \nplt.subplot(122) \nfinal_train_df['PAY_AMT5'].plot.box(figsize=(16,5)) \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bins = 20\nplt.hist(final_train_df.PAY_AMT5, bins = bins, label = 'Total', alpha=0.8)\n\nplt.xlabel('Payment Amount May-2005 (NT dollar)');plt.ylabel('Number of Accounts')\nplt.title('Payment Amount May-2005 ',fontweight=\"bold\", size=12)\nplt.legend();\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(1) \nplt.subplot(121) \nsns.distplot(final_train_df['PAY_AMT6']); \nplt.subplot(122) \nfinal_train_df['PAY_AMT6'].plot.box(figsize=(16,5)) \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bins = 20\nplt.hist(final_train_df.PAY_AMT6, bins = bins, label = 'Total', alpha=0.8)\n\nplt.xlabel('Payment Amount April-2005 (NT dollar)');plt.ylabel('Number of Accounts')\nplt.title('Payment Amount April-2005 ',fontweight=\"bold\", size=12)\nplt.legend();\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the PAY_AMT* plots, we can easily derive following insights about the customer's payment behavior :\n\n\n1. Most of the payments seem to be for amounts < 100000.\n\n\n2. Very few payments are seen for amounts >= 100000.\n\n\nTo get a clearer picture, we may need to divide this data in more bins.\n\nBut before we do that, let us compute what % of customers make paymenmts >= 100000.\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for pmnt in ['PAY_AMT1','PAY_AMT2','PAY_AMT3','PAY_AMT4','PAY_AMT5','PAY_AMT6',]:\n    print(\"\\nNo. of Customers with \", pmnt, \" >= 100000 : \", len(final_train_df[final_train_df[pmnt]>=100000]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"What we see is that out of 21000 customers, about 150+ customers seem to be paying more than 100000 per month for the last 6 months.\n\nEarlier, we saw that the customers with the BILL_AMT* >= 200000 were resulting is the skew, and now we see that the customers with PAY_AMT* >= 100000 are also resulting in the skewed distribution.\n\nWhat this means is that the resultant model may get biased with these very few customers, but these are not outliers. These are valid records, and must be retained for the analysis and modeling.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Apart from the original variables in the dataset, we had created few additional variables. \n\nLet us analyze this data based on these derived variables as follows:\n\nLet us look at the distribution of 'DELINQ_1', 'NO_PMNT1', 'CREDIT_UTIL_RATIO', and 'AVG_6MTH_BAL'.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's start by visualizing the distribution of 'DELINQ_1' in the dataset.  \n\nfig, ax = plt.subplots()\n\nx = final_train_df.DELINQ_1.unique()\n\n# Counting total delinquencies in the dataset\n\ny = final_train_df.DELINQ_1.value_counts()\n\n# Plotting the bar graph\n\nax.bar(x, y)\nax.set_xlabel('Total Number of Delinquencies in last 6 months.')\nax.set_ylabel('No. of Customers')\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's start by visualizing the distribution of 'NO_PMNT1' in the dataset.  \n\nfig, ax = plt.subplots()\n\nx = final_train_df.NO_PMNT1.unique()\n\n# Counting total delinquencies in the dataset\n\ny = final_train_df.NO_PMNT1.value_counts()\n\n# Plotting the bar graph\n\nax.bar(x, y)\nax.set_xlabel('Total Number of No Payments in last 6 months.')\nax.set_ylabel('No. of Customers')\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As the objective of this case is to find who shall default next month, the above bar-graphs need to be analyzed in detail.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(1) \nplt.subplot(221) \nfinal_train_df['DELINQ_1'].value_counts(normalize=True).plot.bar(figsize=(20,10), title= 'Total Number of Delinquencies in last 6 months.') \nplt.subplot(222) \nfinal_test_df['NO_PMNT1'].value_counts(normalize=True).plot.bar(title= 'Total Number of No Payments in last 6 months.') \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The \"Number of Delinquencies\" bar plot indicates that most of the months, most of the customers are paying less than the \"Minimum Payment\" required to keep the card active.\n\nSo....even if the customers pay, most of the customers are paying less than the Minimum payment required.\n\n\n**So...ideally the focus of this company should have been to predict customers who shall pay less then the Minimum Payment, rather than customers who make No Payment.**\n\n\n**That change in strategy and focus would deliver far greater financial and bottom line benefiits to the company in question, rather than focusing on Non Payment.**\n\nTo understand this in money terms, let us do few additional computations.\n\nWe have created derived variables 'PENDING_AMT*' - which represent amounts that are pending from the customer's side (ie. Bills - Payments) for the last 1 - 6 months.\n\nLet us study the distributions of these derived variables :\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(1) \nplt.subplot(121) \nsns.distplot(final_train_df['PENDING_AMT5']); \nplt.subplot(122) \nfinal_train_df['PENDING_AMT5'].plot.box(figsize=(16,5)) \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bins = 20\nplt.hist(final_train_df.PENDING_AMT5, bins = bins, label = 'Total', alpha=0.8)\n\nplt.xlabel('Pending Amount till May-2005 (NT dollar)');plt.ylabel('Number of Accounts')\nplt.title('Pending Amount till May-2005 ',fontweight=\"bold\", size=12)\nplt.legend();\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(1) \nplt.subplot(121) \nsns.distplot(final_train_df['PENDING_AMT4']); \nplt.subplot(122) \nfinal_train_df['PENDING_AMT4'].plot.box(figsize=(16,5)) \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bins = 20\nplt.hist(final_train_df.PENDING_AMT4, bins = bins, label = 'Total', alpha=0.8)\n\nplt.xlabel('Pending Amount till June-2005 (NT dollar)');plt.ylabel('Number of Accounts')\nplt.title('Pending Amount till June-2005 ',fontweight=\"bold\", size=12)\nplt.legend();\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(1) \nplt.subplot(121) \nsns.distplot(final_train_df['PENDING_AMT3']); \nplt.subplot(122) \nfinal_train_df['PENDING_AMT3'].plot.box(figsize=(16,5)) \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bins = 20\nplt.hist(final_train_df.PENDING_AMT3, bins = bins, label = 'Total', alpha=0.8)\n\nplt.xlabel('Pending Amount till July-2005 (NT dollar)');plt.ylabel('Number of Accounts')\nplt.title('Pending Amount till July-2005 ',fontweight=\"bold\", size=12)\nplt.legend();\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(1) \nplt.subplot(121) \nsns.distplot(final_train_df['PENDING_AMT2']); \nplt.subplot(122) \nfinal_train_df['PENDING_AMT2'].plot.box(figsize=(16,5)) \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bins = 20\nplt.hist(final_train_df.PENDING_AMT2, bins = bins, label = 'Total', alpha=0.8)\n\nplt.xlabel('Pending Amount till August-2005 (NT dollar)');plt.ylabel('Number of Accounts')\nplt.title('Pending Amount till August-2005 ',fontweight=\"bold\", size=12)\nplt.legend();\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(1) \nplt.subplot(121) \nsns.distplot(final_train_df['PENDING_AMT1']); \nplt.subplot(122) \nfinal_train_df['PENDING_AMT1'].plot.box(figsize=(16,5)) \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bins = 20\nplt.hist(final_train_df.PENDING_AMT1, bins = bins, label = 'Total', alpha=0.8)\n\nplt.xlabel('Pending Amount till Sept-2005 (NT dollar)');plt.ylabel('Number of Accounts')\nplt.title('Pending Amount till Sept-2005 ',fontweight=\"bold\", size=12)\nplt.legend();\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(1) \nplt.subplot(121) \nsns.distplot(final_train_df['AVG_6MTH_BAL']); \nplt.subplot(122) \nfinal_train_df['AVG_6MTH_BAL'].plot.box(figsize=(16,5)) \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bins = 20\nplt.hist(final_train_df.AVG_6MTH_BAL, bins = bins, label = 'Total', alpha=0.8)\n\nplt.xlabel('Avg. 6 Month Balance in Sept-2005 (NT dollar)');plt.ylabel('Number of Accounts')\nplt.title('Avg. 6 Month Balance in Sept-2005 ',fontweight=\"bold\", size=12)\nplt.legend();\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we shall focus on analysing the impact of no payments and delayed payments on the financial health of this company.\n\nLet us start with understanding the Total Credit given to the customers vs. Total Pending Amount from the customers as on Sept-2005.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"\\nTotal Credit given to all the customers (as on Sept-2005): \", final_train_df['LIMIT_BAL'].sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"\\nTotal Credit given to all Defaulting customers (as on Sept-2005): \", final_train_df.groupby('default_payment_next_month')['LIMIT_BAL'].sum()[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"\\nTotal Pending Amount from all the customers (as on Sept-2005): \", final_train_df['PENDING_AMT1'].sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"\\nTotal Pending Amount from all Defaulting customers (as on Sept-2005): \", final_train_df.groupby('default_payment_next_month')['PENDING_AMT1'].sum()[1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see, the company gave a credit of 3.5 billion NT dollars, whereas the total amount pending is 5 billion NT dollars.\n\nOut of this, the company gave a credit of 600 million NT dollars to defaulting customers, whereas total amount pending from defaulting customers is 1.1 billion NT dollars. \n\n\n**We can clearly establish that certain basic \"Credit Controls\" have not been observed by the company. They have obviously allowed customers to continue to use cards beyond the authorized credit.**\n\n\n**The reason is very simple - the entire focus has been on customers who shall not pay next month, rather than on customers who are paying less than the \"Minimum Amounts\" and falling into \"Delinquent\" status.**\n\n\nLet us analyze pending amounts according to the \"DELINQ_1\" counts.\n\nLet us understand the distribution of Total Credit and Total Pending Amount based on No of delinquencies over 6-month period.\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"delinquencies = list(final_train_df['DELINQ_1'].unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for num_delinq in delinquencies:\n    \n    print(\"\\nTotal Credit Amount for : \", num_delinq, \" delinquencies is : \", sum(final_train_df[final_train_df['DELINQ_1']==num_delinq]['LIMIT_BAL']))\n    \n    print(\"\\nTotal Pending Amount for : \", num_delinq, \" delinquencies is : \", sum(final_train_df[final_train_df['DELINQ_1']==num_delinq]['PENDING_AMT1']))\n              ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above, we get the following insights :\n\n1. About 1.5 billion dollars credit has been allocated to customers who have never been delinquent.\n\n2. Only about 111 million dollars is pending from such non-delinquent customers.\n\n3. About 303 million dollars credit has been allocated to customers who have been delinquent for 1 months\n\n4. About 146 million dollars is pending from such customers who have been delinquent for 1 month.\n\n5. About 209 million dollars credit has been allocated to customers who have been delinquent for 2 months\n\n6. About 255 million dollars is pending from such customers who have been delinquent for 2 months.\n\n7. About 236 million dollars credit has been allocated to customers who have been delinquent for 3 months\n\n8. About 502 million dollars is pending from such customers who have been delinquent for 3 months.\n\n9. About 411 million dollars credit has been allocated to customers who have been delinquent for 4 months\n\n10. About 1.15 billion dollars is pending from such customers who have been delinquent for 4 months.\n\n11. About 856 million dollars credit has been allocated to customers who have been delinquent for 5 months\n\n12. About 2.9 billion dollars is pending from such customers who have been delinquent for 5 months.\n\n**We can clearly see that pending amount keeps increasing by an order of magnitude as the customers keep getting delinquent month after month.**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Now let us understand the Total Pending amount that is stuck by \"No Payment\" customers.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"nopayments = list(final_train_df['NO_PMNT1'].unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for num_pmnt in nopayments:\n    \n    print(\"\\nTotal Credit Amount for : \", num_pmnt, \" no payments is : \", sum(final_train_df[final_train_df['NO_PMNT1']==num_pmnt]['LIMIT_BAL']))\n    \n    print(\"\\nTotal Pending Amount for : \", num_pmnt, \" no payments is : \", sum(final_train_df[final_train_df['NO_PMNT1']==num_pmnt]['PENDING_AMT1']))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above, we get the following insights :\n\n1. About 2.1 billion dollars credit has been allocated to customers who have never defaulted on payments - even if they may have paid token amounts.\n\n2. About 3.8 billion dollars is pending from such non-defaulting customers. This is the most surprising finding from the data. This is a proof that the company is focussed on the wrong metric.\n\n3. About 577 million dollars credit has been allocated to customers who have defaulted on payments just once.\n\n4. About 817 million dollars is pending from customers who have defaulted just once. \n\n5. About 299 million dollars credit has been allocated to customers who have defaulted on payments 2 times.\n\n6. About 266 million dollars is pending from customers who have defaulted two times. \n\n7. About 186 million dollars credit has been allocated to customers who have defaulted on payments 3 times.\n\n8. About 79 million dollars is pending from customers who have defaulted 3 times. \n\n9. About 132 million dollars credit has been allocated to customers who have defaulted on payments 4 times.\n\n10. About 21 million dollars is pending from customers who have defaulted 4 times. \n\n11. About 205 million dollars credit has been allocated to customers who have defaulted on payments 5 times.\n\n12. About 18 million dollars is pending from customers who have defaulted 5 times. \n\n**We can clearly see that pending amount keeps decreasing by an order of magnitude as the customers keep defaulting month after month. This is also because there is a regular decrease in the number of customers who are defaulting multiple times.**\n\n**This behavior seems to be exactly opposite of the customers who are delinquent month after month. This is because there is a regular increase in the number of customers who are becoming delinquent.**\n\n**It is obvious that the focus of this company needs to change towards customers becoming delinquent.**\n\nFinally, let us understand the Total Pending amount with respect to Credit Utilization Ratios.\n\nWe know that Credit utilization ratio of <= 0.3 is considered good in the industry.\n\nLet us understand how many of the customers fall in this safe category.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"bins = 20\nplt.hist(final_train_df.CREDIT_UTIL_RATIO, bins = bins, label = 'Total', alpha=0.8)\n\nplt.xlabel('Credit Utilization Ratios in Sept-2005');plt.ylabel('Number of Accounts')\nplt.title('Credit Utilization Ratios in Sept-2005 ',fontweight=\"bold\", size=12)\nplt.legend();\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(final_train_df[final_train_df['CREDIT_UTIL_RATIO'] <= 0.3])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(final_train_df[(final_train_df['CREDIT_UTIL_RATIO'] > 0.3) & (final_train_df['CREDIT_UTIL_RATIO'] <= 0.7)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(final_train_df[(final_train_df['CREDIT_UTIL_RATIO'] > 0.7) & (final_train_df['CREDIT_UTIL_RATIO'] <= 1)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(final_train_df[final_train_df['CREDIT_UTIL_RATIO'] > 1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that almost 50% of the customers have a credit utilization ratio of less than 30% which is good.\n\nHowever, more than 20% customers have credit utilization ratios of more than 70%.\n\nThis again points to the fact that company should be focusing on the delinquent customers more than the once who miss the payments altogether.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Correlation Coefficients:\n\nNow we shall identify the correlation coefficients of the variables to shortlist the variables which have positive correlation with the target variable 'default_payment_next_month\".\n\nIt is important to remove the variables which have no impact (OR) negative impact on the target variable, and hence improve out ML model performance metrices.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"corr = final_train_df.corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr['default_payment_next_month']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that only a handful of features are showing +ve correlation with our target variable.\n\nWe will only select these features for predictive modeling.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"corr['default_payment_next_month']>= 0.1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Making correlation coefficients pair plot of the selected features\n\nselected_columns = ['CREDIT_UTIL_RATIO','PAY_1','PAY_2','PAY_3','PAY_4','PAY_5','PAY_6','NO_PMNT1','NO_PMNT2','NO_PMNT3','NO_PMNT4','NO_PMNT5','default_payment_next_month']\nplt.figure(figsize=(20,20))\nax = plt.axes()\ncorr_selected = final_train_df[selected_columns].corr()\nsns.heatmap(corr_selected, vmax=1,vmin=-1, square=True, annot=True, cmap='Spectral',linecolor=\"white\", linewidths=0.01, ax=ax)\nax.set_title('Correlation Coefficient Pair Plot',fontweight=\"bold\", size=20)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Let us create training set X and target set y.**\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_columns = ['PAY_1','PAY_2','PAY_3','PAY_4','PAY_5','PAY_6']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target_column = ['default_payment_next_month']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = final_train_df[train_columns]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = final_train_df['default_payment_next_month']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will use the train_test_split function from sklearn to divide our train dataset. \n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_val, y_train, y_val = train_test_split(X, y, train_size=0.8, random_state=2020)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Building and Evaluating Predictive Models\n\n\nAs this is a classification problem, we can use the following algorithms:\n\n\n* Logistic regression\n* Decision tree\n* Random forest\n* Support Vector Classifications\n* Stocastic Gradient Descend\n* Adaboost\n* XGBoost\n* Neural Network Models\n\n\nConsidering a well known fact that out of all the above, Random Forest, Stocastic Gradient Descend, AdaBoost and XGBoost are the best suited algorithms for the problem at hand, we shall only focus on these algorithms.\n\n\nWe shall be using stratified k-folds cross validation, and \"class_weight balancing\" capability of the algorithms for estimating model parameters.\n\n\nWe shart with the Random Forest predictive model.\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#                         Random Forest\n\n\nLet’s import stratified KFold from sklearn and fit the model.\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\nimport xgboost as xgb\n\nfrom sklearn import metrics \nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve, auc, f1_score\n\nfrom statistics import mean \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"i=1 \nkf = StratifiedKFold(n_splits=5,random_state=2020,shuffle=True) \n\nscore_rf = []\n\nX_train = np.array(X_train)\n                     \nfor train_index,test_index in kf.split(X_train,y_train):\n    print('\\n{} of kfold {}'.format(i,kf.n_splits))\n    xtr,xvl = X_train[train_index],X_train[test_index]\n    ytr,yvl = y_train.iloc[train_index],y_train.iloc[test_index]\n    model_rf = RandomForestClassifier(class_weight='balanced',random_state=2020)\n    model_rf.fit(xtr, ytr)\n    pred_test_rf = model_rf.predict(xvl)\n    score_rf.append(accuracy_score(yvl,pred_test_rf))\n    print('\\nAccuracy_score : ',score_rf[i-1])\n    i+=1 \n    \nprint(\"\\nThe mean validation accuracy of Random Forest model is : \", mean(score_rf))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Output confusion matrix\n\npred_rf = model_rf.predict(X_val)\n\nprint(\"Confusion Matrix:\")\nprint(confusion_matrix(y_val, pred_rf))\nprint()\nprint(\"Classification Report\")\nprint(classification_report(y_val, pred_rf))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualize the ROC curve\n\npred_rf_prob=model_rf.predict_proba(xvl)[:,1]\n\nfpr, tpr, _ = metrics.roc_curve(yvl,  pred_rf_prob)\nauc = metrics.roc_auc_score(yvl, pred_rf_prob)\nplt.figure(figsize=(12,8))\nplt.plot(fpr,tpr,label=\"Validation, auc=\"+str(auc))\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.legend(loc=4) \nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"importances=pd.Series(model_rf.feature_importances_, index=X.columns) \nimportances.plot(kind='barh', figsize=(10,6))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that PAY_1 seems to be the most important feature.\n\nThen it is followed by PAY_2, PAY_3 etc..\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"##                     Gradient Boosting\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"i=1 \n\nscore_gb = []\n\nfor train_index,test_index in kf.split(X_train,y_train):\n    print('\\n{} of kfold {}'.format(i,kf.n_splits))\n    xtr,xvl = X_train[train_index],X_train[test_index]\n    ytr,yvl = y_train.iloc[train_index],y_train.iloc[test_index]\n    \n    model_gb = GradientBoostingClassifier(random_state=2020)\n    \n    model_gb.fit(xtr, ytr)\n    pred_test_gb = model_gb.predict(xvl)\n    score_gb.append(accuracy_score(yvl,pred_test_gb))\n    print('\\nAccuracy_score : ',score_gb[i-1])\n    i+=1 \n    \nprint(\"\\nThe mean validation accuracy of the Gradient Boosting model is : \", mean(score_gb))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Output confusion matrix and classification report of Gradient Boosting algorithm on validation set\n\npred_gb = model_gb.predict(X_val)\n\nprint(\"Confusion Matrix:\")\nprint(confusion_matrix(y_val, pred_gb))\nprint()\nprint(\"Classification Report\")\nprint(classification_report(y_val, pred_gb))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualize the ROC curve\n\npred_gb_prob=model_gb.predict_proba(xvl)[:,1]\n\nfpr, tpr, _ = metrics.roc_curve(yvl,  pred_gb_prob)\nauc = metrics.roc_auc_score(yvl, pred_gb_prob)\nplt.figure(figsize=(12,8))\nplt.plot(fpr,tpr,label=\"Validation, auc=\"+str(auc))\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.legend(loc=4) \nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"importances=pd.Series(model_gb.feature_importances_, index=X.columns) \nimportances.plot(kind='barh', figsize=(12,8))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##                      AdaBoost\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"i=1 \n\nscore_adb = []\n\nfor train_index,test_index in kf.split(X_train,y_train):\n    print('\\n{} of kfold {}'.format(i,kf.n_splits))\n    xtr,xvl = X_train[train_index],X_train[test_index]\n    ytr,yvl = y_train.iloc[train_index],y_train.iloc[test_index]\n    model_adb = AdaBoostClassifier(random_state=2020)\n    model_adb.fit(xtr, ytr)\n    pred_test_adb = model_adb.predict(xvl)\n    score_adb.append(accuracy_score(yvl,pred_test_adb))\n    print('\\nAccuracy_score : ',score_adb[i-1])\n    i+=1 \n    \nprint(\"\\nThe mean validation accuracy of the Ada Boosting model is : \", mean(score_adb))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Output confusion matrix and classification report of Ada Boosting algorithm on validation set\n\npred_adb = model_adb.predict(X_val)\n\nprint(\"Confusion Matrix:\")\nprint(confusion_matrix(y_val, pred_adb))\nprint()\nprint(\"Classification Report\")\nprint(classification_report(y_val, pred_adb))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualize the ROC curve\n\npred_adb_prob=model_adb.predict_proba(xvl)[:,1]\n\nfpr, tpr, _ = metrics.roc_curve(yvl,  pred_adb_prob)\nauc = metrics.roc_auc_score(yvl, pred_adb_prob)\nplt.figure(figsize=(12,8))\nplt.plot(fpr,tpr,label=\"Validation, auc=\"+str(auc))\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.legend(loc=4) \nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"importances=pd.Series(model_adb.feature_importances_, index=X.columns) \nimportances.plot(kind='barh', figsize=(12,8))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#                                            XGBOOST\n\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"i=1 \n\nscore_xgb = []\n\nfor train_index,test_index in kf.split(X_train,y_train):\n    print('\\n{} of kfold {}'.format(i,kf.n_splits))\n    xtr,xvl = X_train[train_index],X_train[test_index]\n    ytr,yvl = y_train.iloc[train_index],y_train.iloc[test_index]\n    \n    model_xgb = xgb.sklearn.XGBClassifier(objective=\"binary:logistic\", random_state=2020)\n    model_xgb.fit(xtr, ytr)\n    pred_test_xgb = model_xgb.predict(xvl)\n    score_xgb.append(accuracy_score(yvl,pred_test_xgb))\n    print('\\nAccuracy_score : ',score_xgb[i-1])\n    i+=1 \n    \nprint(\"\\nThe mean validation accuracy of the XGBoost model is : \", mean(score_xgb))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualize the ROC curve\n\npred_xgb_prob=model_xgb.predict_proba(xvl)[:,1]\n\nfpr, tpr, _ = metrics.roc_curve(yvl,  pred_xgb_prob)\nauc = metrics.roc_auc_score(yvl, pred_xgb_prob)\nplt.figure(figsize=(12,8))\nplt.plot(fpr,tpr,label=\"Validation, auc=\"+str(auc))\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.legend(loc=4) \nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"importances=pd.Series(model_xgb.feature_importances_, index=X.columns) \nimportances.plot(kind='barh', figsize=(12,8))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Out of the 4 algorithms, we find that GradientBoosting and XGBoosting algorithms have performed the BEST.\n\nAs our performance metric for submission is AUC, we find that Gradient Boosting has given us the best AUC.\n\nWe can now explaore if we can improve this AUC by parameter tuning.\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Provide range for max_depth from 2 to 20 with an interval of 2 \n# and from 40 to 200 with an interval of 20 for n_estimators \n\nparamgrid_gb = {'learning_rate':[0.05, 0.1, 0.15], 'max_depth': list(range(3, 21, 3)), 'n_estimators': list(range(60, 160, 20))}\n\ngrid_search_gb=GridSearchCV(GradientBoostingClassifier(max_features='auto',random_state=2020),paramgrid_gb)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fit the grid search model \n\ngrid_search_gb.fit(X_train,y_train)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Estimating the optimized value \n\n\ngrid_search_gb.best_estimator_\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we will use these best parameters to run the model again and see what is the best result that we can get from this.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"i=1 \n# kf = KFold(n_splits=5,random_state=2020,shuffle=True) \n\nscore_gb = []\n\nfor train_index,test_index in kf.split(X_train,y_train):\n    print('\\n{} of kfold {}'.format(i,kf.n_splits))\n    xtr,xvl = X_train[train_index],X_train[test_index]\n    ytr,yvl = y_train.iloc[train_index],y_train.iloc[test_index]\n    # model_gb = GradientBoostingClassifier(learning_rate=0.1, max_features='sqrt', max_depth=18, n_estimators=120, random_state=2020)\n    \n    model_gb = GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,\n                                          learning_rate=0.15, loss='deviance', max_depth=3,\n                                          max_features='auto', max_leaf_nodes=None,\n                                          min_impurity_decrease=0.0, min_impurity_split=None,\n                                          min_samples_leaf=1, min_samples_split=2,\n                                          min_weight_fraction_leaf=0.0, n_estimators=80,\n                                          n_iter_no_change=None, presort='deprecated',\n                                          random_state=2020, subsample=1.0, tol=0.0001,\n                                          validation_fraction=0.1, verbose=0,warm_start=False)\n    \n    model_gb.fit(xtr, ytr)\n    pred_test_gb = model_gb.predict(xvl)\n    score_gb.append(accuracy_score(yvl,pred_test_gb))\n    print('\\nAccuracy_score : ',score_gb[i-1])\n    i+=1 \n    \nprint(\"\\nThe mean validation accuracy of the Gradient Boosting model is : \", mean(score_gb))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Output confusion matrix and classification report of Gradient Boosting algorithm on validation set\n\npred_gb = model_gb.predict(X_val)\n\nprint(\"Confusion Matrix:\")\nprint(confusion_matrix(y_val, pred_gb))\nprint()\nprint(\"Classification Report\")\nprint(classification_report(y_val, pred_gb))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualize the ROC curve\n\npred_gb_prob=model_gb.predict_proba(xvl)[:,1]\n\nfpr, tpr, _ = metrics.roc_curve(yvl,  pred_gb_prob)\nauc = metrics.roc_auc_score(yvl, pred_gb_prob)\nplt.figure(figsize=(12,8))\nplt.plot(fpr,tpr,label=\"Validation, auc=\"+str(auc))\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.legend(loc=4) \nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After tuning of the GradientBoosting model, we see an improvement in AUC from 0.7652 to 0.7687, and accuracy from 0.8216 to 0.8219.\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We will now prepare the data for submission to the AnalyticsVidhya and Kaggle websites.\n\nWe will predict on the provided test dataset using the GradientBoosting model, and use these predictions to populate the submission.csv file. \n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"final_test_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_train_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_test_df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_train_df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_test_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target_column","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test = final_test_df[train_columns]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we shall predict using the GB model on this test data, that has not been seen by the model so far.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_gb_test = model_gb.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_gb_prob_test=model_gb.predict_proba(X_test)[:,1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_gb_prob_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_gb_prob_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission=pd.read_csv(\"../input/av-janata-hack-payment-default-prediction/sample_submission_gm6gE0l.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission['default_payment_next_month']=pred_gb_prob_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"original_test_df['ID']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission['ID']=original_test_df['ID']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv(\"CC_Payment_Default_Janata_Hack_31_May.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}