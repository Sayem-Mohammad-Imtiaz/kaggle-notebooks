{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Real or Not? NLP with Disaster Tweets\nPredict which Tweets are about real disasters and which ones are not\n    \nในปัจจุบันนี้ไม่ว่าบนโลกจะเกิดเหตุการณ์อะไรขึ้น ผู้คนมักใช้ Tweeter ในการติดตามข่าวสารที่เกิดขึ้นเป็นจำนวนมากซึ่ง\n    เป็นแหล่งข่าวที่มีการอัพเดตอยู่ตลอดและว่องไวและผู้คนมักจะ tweet ข้อความข่าวสารเหล่านั้น โดยเฉพาะข่าวสารที่เกี่ยวกับภัยพิบัติ \n    tweeter กลายเป็นแอพพลิเคชั่นสำคัญที่เอาไว้ใช้เฝ้าระวังเหตุฉุกเฉินแบบเรียลไทม์ แต่คำบางคำที่อาจจะเกี่ยวข้องกับภัยพิบัตินั้น\n     ก็นำมาใช้เขียนในเชิงเปรียบเทียบที่คนเราสามารถอ่านแล้วเข้าใจได้ แต่ machine นั้นก็ไม่สามารถเข้าใจได้อย่างชัดเจน \n     และเมื่อมีปัญหาที่เกิดขึ้น ทางกลุ่มเราจึงได้เข้าร่วมการแข่งขันการสร้างโมเดลที่คาดการณ์ว่าทวีตใดเกี่ยวกับภัยพิบัติจริงและทวีตใดที่ไม่ใช่ \n    \n"},{"metadata":{},"cell_type":"markdown","source":"# Data Set\nประกอบด้วย\n1. sample_submission.csv ที่เก็บข้อมูลของ id และ target ที่บ่งบอกว่า id นั้นได้พูดถึงภัยพิบัติจริงหรือไม่ ให้ 1 พูดถึงพูดถึงภัยพิบัติจริงและ 0 ไม่ได้พูดถึงภัยพิบัติ\n\n\n2. test.csv ชุดการทดสอด จำนวน 7,613 ข้อมูล ซึ่งประกอบด้วย\n  \n    1. id : ตัวระบุที่ไม่ซ้ำกันของแต่ละทวีตแทนเป็นตัวเลข\n     \n    2. Keyword : คำสำคัญสำหรับการทำนาย\n    \n    3. location : ตำแหน่งที่อยู่ของผู้ใช้งานทวีต\n      \n    4. text : ข้อความที่ผู้ใช้โพสต์\n      \n\n3. train.csv ชุดการฝึก จำนวน 3,263 ข้อมูล ซึ่งประกอบด้วย\n\n    1. id : ตัวระบุที่ไม่ซ้ำกันของแต่ละทวีตแทนเป็นตัวเลข\n    \n    2. Keyword : คำสำคัญสำหรับการทำนาย\n    \n    3. location : ตำแหน่งที่อยู่ของผู้ใช้งานทวีต\n    \n    4. text : ข้อความที่ผู้ใช้โพสต์\n    \n    5. target : สิ่งนี้บ่งชี้ว่าทวีตนั้นเกี่ยวกับภัยพิบัติจริงให้เป็น (1) หรือไม่จริงให้เป็น (0)\n    "},{"metadata":{},"cell_type":"markdown","source":"# Algorithm\n* อันดับแรกเราได้ทำการ import module ต่างๆเพื่อใช้งานในโปรแกรม อันดับแรกคือ numpy สำหรับการคำนวณทางคณิตศาสตร์ ต่อมาคือ pandas ที่ทำให้เราจัดการกับข้อมูลต่างๆได้ง่ายขึ้นและสามารถแสดงข้อมูลออกมาเป็นตารางได้ nltk เป็นเครื่องมือการประมวลผลภาษาธรรมชาติ  เพื่อให้คอมพิวเตอร์สามารถตีความและเข้าใจภาษามนุษย์ได้ re ที่ทำให้สามารถค้นหากลุ่มตัวหนังสือที่มีรูปแบบตามที่ต้องการจากข้อความหรือกลุ่มตัวอักษรได้ string เป็นลำดับของตัวอักษรหลายตัวเรียงต่อกัน ซึ่งในภาษา Python นั้นการที่จะประกาศ String ค่าของมันจะอยู่ในเครื่องหมาย Double quote หรือ Single quote เท่านั้น sklearn เป็น module นึงของภาษา Python ที่เราจะใช้จัดการเกี่ยวกับ supervised machine learning และได้ import math ที่เป็นฟังก์ชันที่ใช้คำนวณทางคณิตศาสตร์ เพื่อหาค่าทางคณิตศาสตร์ และที่สำคัญเราได้ import os เพื่อเช็ค dataset ว่ามีไฟล์อะไรบ้าง  "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport nltk\nimport re\nimport string\nimport pandas as pd\nimport sklearn as sk\nimport math  \n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{},"cell_type":"markdown","source":"เราได้ตั้งค่าพารามิเตอร์ train และ test ในการอ่านไฟล์และโหลดข้อมูล csv ไปยังดาต้าเฟรมและได้แสดงข้อมูลของ train เป็นจำนวน 10 แถว"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/nlpgettingstarted/train.csv')\ntest = pd.read_csv('../input/nlpgettingstarted/test.csv')\ntrain.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"ทำการ drop คอลัมน์ที่ 1 ถึงชื่อว่า location ใน train"},{"metadata":{"trusted":true},"cell_type":"code","source":"train=train.drop(['location'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{},"cell_type":"markdown","source":"# clean data "},{"metadata":{},"cell_type":"markdown","source":"ต่อมาเราทำการ clean ข้อมูล\n* ขั้นแรกเราต้องการลบอิโมจิออกโดยใช้ regrex_pattern ในการตรวจสอบข้อมูลแพตเทิร์น(Pattern) คือ อิโมจิ, สัญลักษร์และภาษาภาพ, สัญลักษณ์ของการขนส่งและแผนที่ และอิโมจิรูปธงชาติใน ios และใช้ re.compile() รวบรวมรูปแบบ (Pattern) เป็นวัตถุเก็บในรูปแบบunicode แล้วทำการลบออกโดยใช้ฟังก์ชัน remove_emo\n* ขั้นตอนต่อมาคือการลบข้อความ เช่น ลบทุกตัวที่ไม่ใช่ตัวเลขและอักษรภาษาอังกฤษรวมถึงขีดล่างเทียบเท่า [A-Za-z0–9_],ขีดและอักขระ,url, แปลงอักษรตัวพิมพ์ใหญ่ทั้งหมดในสตริงข้อความให้เป็นตัวพิมพ์เล็ก,สเปซบาร์หรือแท็บหรือขึ้นบรรทัดใหม่และเครื่องหมาย \" \" ให้ถูกแทนที่ด้วยช่องว่าง และสุดท้ายคือ ตัวเลขที่มีเลข 0 ถึง 9\n* และขั้นตอนสุดท้ายคือการลบเครื่องหมายวรรคตอนออก โดยการเขียนโค้ดดังกล่าว\nและแสดงข้อมูลเป็นจำนวน 10 แถว"},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_emo(text) :\n    regrex_pattern = re.compile(pattern = \"[\"\n        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           \"]+\", flags = re.UNICODE)\n    return regrex_pattern.sub(r'',text)\ntrain['text'] = train['text'].apply(lambda x : remove_emo(x))\ntest['text'] = test['text'].apply(lambda x : remove_emo(x))\n\ndef removetext(text):\n    text = re.sub(r'[^\\w]', ' ', text)#Remove all \n    text = re.sub(\"\\'\\w+\", '', text) #Remove ticks and the next character\n    text = re.sub(\"https*\\S+\", \" \", text)#Remove URL\n    text = text.lower()\n    text = re.sub('\\s{2,}', \" \", text)#Replace the over spaces\n    text=re.sub(r'[0-9]', ' ', text)\n    return text\ntrain['text'] = train['text'].apply(lambda x : removetext(x))\ntest['text'] = test['text'].apply(lambda x : removetext(x))\n\ndef punctuation(text):\n    all_list = [char for char in text if char not in string.punctuation]\n    clean_str = ''.join(all_list)\n    return clean_str\ntrain['text'] = train['text'].apply(punctuation)\ntest['text'] = test['text'].apply(punctuation)\ntrain.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# remove stop word"},{"metadata":{},"cell_type":"markdown","source":"ต่อไปเราต้องการที่จะลบคำที่พบบ่อยๆ เมื่อลบแล้วแต่ยังคงความหมายเดิม ซึ่งเป็นคำที่ไม่จำเป็น เช่น of, is, the, and เป็นต้น เพื่อกรองข้อมูลที่ไร้ประโยชน์ออก โดยการใช้คำสั่งstop ในการเอา stopword จากโมเดล nltk ที่เก็บคลังภาษาอังกฤษที่พบบ่อยๆแต่ไม่จำเป็นออกจากประโยค "},{"metadata":{"trusted":true},"cell_type":"code","source":"stop = nltk.corpus.stopwords.words('english')\ntrain['text'] = train['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\ntest['text'] = test['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\ntrain.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Token"},{"metadata":{},"cell_type":"markdown","source":"จากนั้น import sent_tokenize ที่ใช้ในการตัดคำจากช่องว่าง มหัพภาค บรรทัดใหม่ และ word_tokenize เป็นวิธีการที่ดึง token ออกจาก string และได้แสดงผลออกในตาราง text_tokens"},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.tokenize import sent_tokenize, word_tokenize\ntrain['text_tokens'] = train['text'].apply(lambda x: word_tokenize(x))\ntrain.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Stemmer "},{"metadata":{},"cell_type":"markdown","source":"เราได้ import PorterStemmer เพื่อทำ stemming ที่เป็นเทคนิคการตัดคำจำพวก affixes ที่ประกอบด้วย prefix และ suffix ออกไป และแสดงผลในตาราง text_clean_tokens "},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.stem import PorterStemmer \nfrom nltk.tokenize import word_tokenize\ndef word_stemmer(text):\n    stem_text = [PorterStemmer().stem(i) for i in text]\n    return stem_text\ntrain['text_clean_tokens'] = train['text_tokens'].apply(lambda x: word_stemmer(x))\ntrain.head(10)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Lemmatizer"},{"metadata":{},"cell_type":"markdown","source":"ต่อมาได้ import WordNetLemmatizer ใน nltk เพื่อทำ Lemmatization เป็นกระบวนการของคำเพื่อให้ได้คำรูปแบบพื้นฐาน (รูปแบบพจนานุกรม) เพื่อให้ได้ความแม่นยำมากขึ้น และได้แสดงผลในตาราง text_clean_tokens"},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.stem import WordNetLemmatizer\ndef word_lemmatizer(text):\n    lem_text = [WordNetLemmatizer().lemmatize(i) for i in text]\n    return lem_text\ntrain['text_clean_tokens'] = train['text_tokens'].apply(lambda x: word_lemmatizer(x))\ntrain.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Analyzing"},{"metadata":{},"cell_type":"markdown","source":"# TFIDF"},{"metadata":{},"cell_type":"markdown","source":"เราต้องการที่จะคัดแยกคำตามความสำคัญเพื่อที่จะทำนายคำที่เกี่ยวกับภัยพิบัติจริงๆนั้น ทางกลุ่มจึงได้เลือกวิธีการ TFIDF เพื่อหาค่าการทำนาย โดยขั้นแรกได้ import TfidfTransformer สำหรับใช้ในการคำนวณจำนวนคำ (TF) และ counterVectorizer เพื่อคำนวณ inverse document frequency (IDF) จาก sklearn.feature_extraction และทำการหา TFIDF โดยการใช้ Tfidfvectorizer ที่แปลงข้อความเป็นเวกเตอร์ และตั้งค่า\nmin_df = 1 หมายถึง ลบคำที่เกิดขึ้นแค่ 1 ครั้ง หรือคำที่ปรากฏในเอกสารน้อยกว่า 1 ฉบับ \nmax_df=0.5 หมายถึง ลบคำที่ปรากฎขึ้นมากกว่า 50 % ในdocument \nngram_range=(1, 2) คือ การนำโทเคน 2 คำมาเพิ่มด้วยกันกลายเป็นโทเคน 1 คำ"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import CountVectorizer\ntfidf = TfidfVectorizer(min_df=1, max_df=0.5, ngram_range=(1, 2))\ntrain_tfidf = tfidf.fit_transform(train[\"text\"])\ntest_tfidf = tfidf.transform(test[\"text\"])\nprint(train_tfidf)\nprint(test_tfidf)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"จากนั้น import LogisticRegression และ model_selection  "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn import model_selection\n\nclf = LogisticRegression(C=1.0)\nscores = model_selection.cross_val_score(clf, train_tfidf, train[\"target\"], cv=5, scoring=\"f1\")\nprint(scores)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf.fit(train_tfidf, train[\"target\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv('../input/nlpgettingstarted/sample_submission.csv')\nsubmission[\"target\"] = clf.predict(test_tfidf)\nsubmission.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.head(20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Reference\narupjyoti_dutta 2020 ,Removing stop words with NLTK in Python , view 21 Nov 2020 ,<https://www.geeksforgeeks.org/removing-stop-words-nltk-python/>\n\nKatharine Jarmul 2017,Detecting Fake News with Scikit-Learn , view 21 Nov 2020 , <https://www.datacamp.com/community/tutorials/scikit-learn-fake-news>\n\nkoPytok 2018,Corpora/stopwords not found when import nltk library,view 21 Nov 2020,<https://stackoverflow.com/questions/41610543/corpora-stopwords-not-found-when-import-nltk-library>\n\nNaim Mhedhbi 2020 , Text Classification Step by Step , view 21 Nov 2020 , <https://www.kaggle.com/naim99/text-classification-step-by-step/output>\n\nZhi Li 2019,A Beginner’s Guide to Word Embedding with Gensim Word2Vec Model ,view 21 Nov 2020 ,<https://towardsdatascience.com/a-beginners-guide-to-word-embedding-with-gensim-word2vec-model-5970fa56cc92>\n\ndef love(x) 2017 ,CountVectorizer, TfidfVectorizer, Predict Comments, view 22 Nov 2020 ,<https://www.kaggle.com/adamschroeder/countvectorizer-tfidfvectorizer-predict-comments#CountVectorizer----Brief-Tutorial>\n\nJason Brownlee 2017, Deep Learning for Natural Language Processing ,view 22 Nov 2020 ,\n<https://machinelearningmastery.com/prepare-text-data-machine-learning-scikit-learn/>\n\nKavita Ganesan 2020 , AI Implementation, Hands-On NLP\nScikit-learn’s Tfidft,view 21 Nov 2020<https://kavita-ganesan.com/tfidftransformer-tfidfvectorizer-usage-differences/#.X7o9lWj7SM8>"},{"metadata":{},"cell_type":"markdown","source":"**น.ส.นันท์นภัส บุญเชิด 6209656021**\n\n**น.ส.สุภาวินี แจ๊ะซ้าย 6209656039**\n\n**น.ส.ณัฐนิช เพิ่มหรรษา 6209656328**"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}