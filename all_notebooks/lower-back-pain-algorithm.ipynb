{"nbformat_minor":1,"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","name":"python3","language":"python"},"language_info":{"codemirror_mode":{"version":3,"name":"ipython"},"name":"python","file_extension":".py","mimetype":"text/x-python","version":"3.6.1","pygments_lexer":"ipython3","nbconvert_exporter":"python"}},"cells":[{"metadata":{"_uuid":"5f689f6908c2f1162be1758da42eb3e019a82892","_cell_guid":"858145c6-5614-e2a6-5c1c-e542c44cf7fe"},"cell_type":"markdown","source":"<h1>Lower Back Pain Classification Algorithm </h1>\n\n<p>This dataset contains the anthropometric measurements of the curvature of the spine to support the model towards a more accurate classification.\n<br />\nLower back pain affects around 80% of individuals at some point in their life. If this model becomes robust enough, then these measurements may soon become predictive and treatable measures. \n<br /> \n<a href=\"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.471.4845&rep=rep1&type=pdf\">This study</a> asserts the validity of the manual goniometer measurements as a valid clinical tool. </p>"},{"outputs":[],"metadata":{"collapsed":true,"_uuid":"c9216d6c22612eca89a726cc57a7863c58f37198","_cell_guid":"404d4f99-505b-c6ba-0700-d42db0d9ef36","_execution_state":"idle"},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\n\n# read data into dataset variable\ndata = pd.read_csv(\"../input/Dataset_spine.csv\")\n\n# Drop the unnamed column in place (not a copy of the original)#\ndata.drop('Unnamed: 13', axis=1, inplace=True)\n\n# Concatenate the original df with the dummy variables\ndata = pd.concat([data, pd.get_dummies(data['Class_att'])], axis=1)\n\n# Drop unnecessary label column in place. \ndata.drop(['Class_att','Normal'], axis=1, inplace=True)","execution_count":null},{"outputs":[],"metadata":{"collapsed":true,"_uuid":"aca89c1650278ac4335835590f3231b432a989fe","_cell_guid":"21ed290e-b96d-fe40-a174-e572a7391358","_execution_state":"idle"},"cell_type":"code","source":"data.info()","execution_count":null},{"outputs":[],"metadata":{"collapsed":true,"_uuid":"38bafb8481eed9ddce74a618e40fdffd267b0f01","_cell_guid":"2771308d-d37a-fe21-5ba7-e2de782e9ee7","_execution_state":"idle"},"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","execution_count":null},{"metadata":{"_uuid":"b80ae877f82a9e578ac4852a1674a858d5c2edf4","_cell_guid":"779a9779-dd74-18ae-2ca8-e1d135bcfda8"},"cell_type":"markdown","source":"<h1>Exploratory Data Analysis </h1>"},{"outputs":[],"metadata":{"collapsed":true,"_uuid":"ae6c227c7230978a1900c8ef8edbbd34c2d64f3e","_cell_guid":"6f8224da-58f4-5d4e-6723-21cf33cbf660","_execution_state":"idle"},"cell_type":"code","source":"data.columns = ['Pelvic Incidence','Pelvic Tilt','Lumbar Lordosis Angle','Sacral Slope','Pelvic Radius', 'Spondylolisthesis Degree', 'Pelvic Slope', 'Direct Tilt', 'Thoracic Slope', 'Cervical Tilt','Sacrum Angle', 'Scoliosis Slope','Outcome']\n\ncorr = data.corr()\n\n# Set up the matplot figure\nf, ax = plt.subplots(figsize=(12,9))\n\n#Draw the heatmap using seaborn\nsns.heatmap(corr, cmap='inferno', annot=True)","execution_count":null},{"outputs":[],"metadata":{"_uuid":"f6fa6852b23eb3915f8ed3b9f3b0d27c823bc312","_cell_guid":"20548bf9-41e2-470d-b774-d9a25ecaee2c","_execution_state":"idle"},"cell_type":"code","source":"data.describe()","execution_count":null},{"outputs":[],"metadata":{"_uuid":"55c4d84b54de8dc4ca5e79d67f0474ed2c501b3b","_cell_guid":"8598e4cc-c1d0-4443-be64-293d9cb6172d","_execution_state":"idle"},"cell_type":"code","source":"from pylab import *\nimport copy\noutlier = data[[\"Spondylolisthesis Degree\", \"Outcome\"]]\n#print(outlier[outlier >200])\nabspond = outlier[outlier[\"Spondylolisthesis Degree\"]>15]\nprint(\"1= Abnormal, 0=Normal\\n\",abspond[\"Outcome\"].value_counts())\n\n\n#   Dropping Outlier\ndata = data.drop(115,0)\ncolr = copy.copy(data[\"Outcome\"])\nco = colr.map({1:0.44, 0:0.83})\n\n#   Plot scatter\nplt.scatter(data[\"Cervical Tilt\"], data[\"Spondylolisthesis Degree\"], c=co, cmap=plt.cm.RdYlGn)\nplt.xlabel(\"Cervical Tilt\")\nplt.ylabel(\"Spondylolisthesis Degree\")\n\ncolors=[ 'c', 'y', 'm',]\nab =data[\"Outcome\"].where(data[\"Outcome\"]==1)\nno = data[\"Outcome\"].where(data[\"Outcome\"]==0)\nplt.show()\n# UNFINISHED ----- OBJECTIVE: Color visual by Outcome - 0 for green, 1 for Red (example)","execution_count":null},{"outputs":[],"metadata":{"collapsed":true,"_uuid":"986451a57b060a176c65d63d6f07055b4276b24e","_cell_guid":"fcccdf79-d649-8ccb-94c2-4254fcff905c","_execution_state":"idle"},"cell_type":"code","source":"#   Create the training dataset\ntraining = data.drop('Outcome', axis=1)\ntesting = data['Outcome']","execution_count":null},{"outputs":[],"metadata":{"_uuid":"ff09f370383c6916971a586c2e604b9321619191","_cell_guid":"f2bf6558-1261-438c-b6ce-d53e6de822b7","_execution_state":"idle"},"cell_type":"code","source":"#   Import necessary ML packages\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import classification_report\n\n#   Split into training/testing datasets using Train_test_split\nX_train, X_test, y_train, y_test = train_test_split(training, testing, test_size=0.33, random_state=22, stratify=testing)","execution_count":null},{"metadata":{"_uuid":"dd30e2f500e183126c29f2cd8c2c43f9943dce41","_cell_guid":"373761ba-c10a-46ba-b139-fb7740547306","_execution_state":"idle"},"cell_type":"markdown","source":"<h1> Convert DataFrame Object to a numpy array due to faster computation in modelling</h1>"},{"outputs":[],"metadata":{"collapsed":true,"_uuid":"3a2c5944d93783afd20e6354dcaab69ed176b715","_cell_guid":"b747abd2-ec17-de05-149d-37a9ed8e510e","_execution_state":"idle"},"cell_type":"code","source":"import numpy as np\n\n# convert to numpy.ndarray and dtype=float64 for optimal\narray_train = np.asarray(training)\narray_test = np.asarray(testing)\n\n#   Convert each pandas DataFrame object into a numpy array object. \narray_XTrain, array_XTest, array_ytrain, array_ytest = np.asarray(X_train), np.asarray(X_test), np.asarray(y_train), np.asarray(y_test)","execution_count":null},{"metadata":{"_uuid":"c2409f6ee8da93a43a9cc10269a49e75ae4b01bb","_cell_guid":"575740d0-e70b-4617-93ab-c77e7600e034","_execution_state":"idle"},"cell_type":"markdown","source":"<h1> Employing Support Vector Machine as a Classifier - 85% </h1>"},{"outputs":[],"metadata":{"collapsed":true,"_uuid":"2b2726b4581aa64bd23ac269571c712c5dfdc0f5","_cell_guid":"16dedd0c-65c5-1ab3-0bec-5a9a60a3ca6f","_execution_state":"idle"},"cell_type":"code","source":"#    Import Necessary Packages\nfrom sklearn import svm\nfrom sklearn.metrics import accuracy_score\n\n#   Instantiate the classifier\nclf = svm.SVC(kernel='linear')\n\n#   Fit the model to the training data\nclf.fit(array_XTrain, array_ytrain)\n\n#   Generate a prediction and store it in 'pred'\npred = clf.predict(array_XTest)\n\n#   Print the accuracy score/percent correct\nsvmscore = accuracy_score(array_ytest, pred)\nprint(\"Support Vector Machines are \", svmscore*100, \"accurate\")\n","execution_count":null},{"metadata":{"_uuid":"cd4db2235186edf22fb8b01a763dfaa0d86fd40f","_cell_guid":"5e0cf111-d686-436d-b16a-9ba967514caf","_execution_state":"idle"},"cell_type":"markdown","source":"<h1> Employing Linear Regression as a Classifier - 82% </h1>"},{"outputs":[],"metadata":{"collapsed":true,"_uuid":"a8acc75255ac1d48f8097cb8e796443a822cd6d0","_cell_guid":"2c3e9583-bebe-437b-c7b1-7f6a7d9b783f","_execution_state":"idle"},"cell_type":"code","source":"estimators = [('clf', LogisticRegression())]\n\npl = Pipeline(estimators)\n\npl.fit(X_train, y_train)\n\naccuracy = pl.score(X_test, y_test)\nprint(\"\\nAccuracy on sample data\",accuracy)","execution_count":null},{"outputs":[],"metadata":{"collapsed":true,"_uuid":"7ba25fbcc87afdb0803893f28cceba90b476a8c5","_cell_guid":"522d3325-68d5-91cf-eed7-745e8a0d4681","_execution_state":"idle"},"cell_type":"code","source":"ypred = pl.predict(X_test)","execution_count":null},{"outputs":[],"metadata":{"collapsed":true,"_uuid":"daf009bb16dc29e8325f3e9199bda4b397e05135","_cell_guid":"93a909bd-6169-a40c-ce1c-7424d49e1192","_execution_state":"idle"},"cell_type":"code","source":"report = classification_report(y_test, ypred)\nprint(report)","execution_count":null},{"metadata":{"_uuid":"1697fc38841d9885d8cfe209cae04b8be371acf4","_cell_guid":"33d3be64-17bc-79fd-b886-f04bbf85325d"},"cell_type":"markdown","source":"<h1> That's it! </h1>\n<p>~85% prediction accuracy with Support Vector Machines!  To increase the accuracy of the model, feature engineering is a suitable solution - as well as creating new variables based on domain knowledge.</p>"},{"outputs":[],"metadata":{"collapsed":true,"_uuid":"e91816a2a4c54c13f9b081ffaa47deb83ee65c1c","_cell_guid":"9c07b6ad-40d2-0f84-1f1e-89dac46a7cea","_execution_state":"idle"},"cell_type":"code","source":"\n","execution_count":null}],"nbformat":4}