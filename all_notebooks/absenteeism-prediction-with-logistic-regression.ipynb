{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Content:\n\n- <a href='#1.'> 1. Importing Libraries</a>\n- <a href='#2.'> 2. Loading and Checking Data</a>\n- <a href='#3.'> 3. Variable Description</a>\n- <a href='#4.'> 4. Data Analysis</a>\n- <a href='#5.'> 5. Preprocessing</a>\n    - <a href='#5.1.'> 5.1. Outlier Detection and Missing Values</a>\n    - <a href='#5.2.'> 5.2. Dropping Useless Variables</a>\n    - <a href='#5.3.'> 5.3. Feature Engineering</a>\n        - <a href='#5.3.1.'> 5.3.1. Checking Reason for Absence Feature</a>\n        - <a href='#5.3.2.'> 5.3.2. Checking Date Feature</a>\n        - <a href='#5.3.3.'> 5.3.3. Checking Other Features</a>\n        - <a href='#5.3.4.'> 5.3.4. Creating Target Variable</a>\n    - <a href='#5.4.'> 5.4. Scaling and Splitting</a>\n- <a href='#6.'> 6. Single Logistic Regression and Evaluation</a>\n- <a href='#7.'> 7. References</a>"},{"metadata":{},"cell_type":"markdown","source":"## <a id='1.'> 1. Importing Libraries</a>"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <a id='2.'> 2. Loading and Checking Data</a>"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Loading and checking data\nraw_data = pd.read_csv(\"../input/employee-absenteeism-prediction/Absenteeism-data.csv\")\nraw_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating checkpoint\ndf = raw_data.copy()\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the shape of dataset\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking info\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the statistical summary of dataset\ndf.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <a id='3.'> 3. Variable Description</a>"},{"metadata":{},"cell_type":"markdown","source":"* ID: Identification number of the employees (categorical)  \n* Reason for Absence: Reason for the absenteeism of employees (categorical)  \n* Date: Date of the absenteeism time of the employees (categorical) \n* Transportation Expense: The amount of transportaion expense of the employees  (numerical)\n* Distance to Work: Distance to work for every employee (numerical)  \n* Age: Age of the employees (numerical)  \n* Daily Work Load Average:  Daily work load average of the employees (numerical)\n* Body Mass Index: Body index of the employees (numerical)  \n* Education: Education levels (categorical)   \n* Children: Children number (numerical)  \n* Pets: Pet number (numerical)                      \n* Absenteeism Time in Hours:  Daily absenteeism time of the employees \n \ndtypes: float64(1), int64(10), object(1)"},{"metadata":{},"cell_type":"markdown","source":"## <a id='4.'> 4. Data Analysis</a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"#We can make a simple EDA with \"pandasprofiling\".\nfrom pandas_profiling import ProfileReport\ndf_profile = ProfileReport(df, title='Pandas Profiling Report', html={'style':{'full_width':True}})\ndf_profile","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <a id='5.'> 5. Preprocessing</a>"},{"metadata":{},"cell_type":"markdown","source":"## <a id='5.1.'> 5.1. Outlier Detection and Missing Values</a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Outlier detection\ndef detect_outlier(df, features):\n    outlier_indices = []\n    \n    for c in features:\n        # 1st quartile\n        Q1 = np.percentile(df[c],25)\n        # 3rd quartile\n        Q3 = np.percentile(df[c],75)\n        # IQR\n        IQR = Q3 - Q1\n        # outlier step\n        outlier_step = IQR * 1.5\n        # detect outlier and their indices\n        outlier_list_col = df[(df[c] < Q1 - outlier_step) | (df[c] > Q3 + outlier_step)].index\n        # store indices\n        outlier_indices.extend(outlier_list_col)\n    \n    outlier_indices = Counter(outlier_indices)\n    multiple_outliers = list(i for i, v in outlier_indices.items() if v > 2)\n    \n    return multiple_outliers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import Counter\ndf.loc[detect_outlier(df,['Transportation Expense','Distance to Work', 'Age', 'Daily Work Load Average',\n                              'Body Mass Index', 'Children', 'Pets'])]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is no outliers."},{"metadata":{"trusted":true},"cell_type":"code","source":"# To check the missing values\ndf.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are no missing values in the dataset."},{"metadata":{},"cell_type":"markdown","source":"## <a id='5.2.'> 5.2. Dropping Useless Variables</a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# ID variable is a number that is there to distinguish the individuals from one another, not to carry any numeric information.\n# We should drop it. \ndf = df.drop([\"ID\"], axis = 1) # axis=1 for columns, axis=0 for rows.\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <a id='5.3.'> 5.3. Feature Engineering</a>"},{"metadata":{},"cell_type":"markdown","source":"## <a id='5.3.1.'> 5.3.1. Checking Reason for Absence Feature</a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking Reason for Absence feature.\nprint(df[\"Reason for Absence\"].max())\nprint(df[\"Reason for Absence\"].min())\nprint(df[\"Reason for Absence\"].unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(df[\"Reason for Absence\"].unique())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"One number is missing."},{"metadata":{"trusted":true},"cell_type":"code","source":"sorted(df[\"Reason for Absence\"].unique())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- 20 is missing in \"Reason for Absence\".\n- \"Reason for Absence\" feature is categorical. We can get dummies for this feature but first we must be sure that every empoyee has only one \"Reason for Absence\". "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Convert categorical variable into dummy variables.\nreason_columns = pd.get_dummies(df[\"Reason for Absence\"])\nreason_columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reason_columns[\"Check\"] = reason_columns.sum(axis=1)\nreason_columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- 0 : missing value, \n- 1: single value, \n- 2,3,4...: Not possible because there can only be one reason for absence."},{"metadata":{"trusted":true},"cell_type":"code","source":"reason_columns[\"Check\"].sum(axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reason_columns[\"Check\"].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reason_columns = reason_columns.drop([\"Check\"], axis=1)\nreason_columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# To avoid potencial multicollinearity issues\nreason_columns = pd.get_dummies(df[\"Reason for Absence\"], drop_first = True)\nreason_columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Group the Reasons for Absence"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reason_columns.columns.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.drop([\"Reason for Absence\"], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Classifying Reason for Absence\n- 1-14: Desease\n- 15-17: Pregnancy\n- 18-21: Emergency issues\n- 22-28: Light reasons "},{"metadata":{"trusted":true},"cell_type":"code","source":"reason_type_1 = reason_columns.loc[:,1:14].max(axis = 1)\nreason_type_2 = reason_columns.loc[:,15:17].max(axis = 1)\nreason_type_3 = reason_columns.loc[:,18:21].max(axis = 1)\nreason_type_4 = reason_columns.loc[:,22:28].max(axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Concatenate column values\ndf = pd.concat([df, reason_type_1, reason_type_2, reason_type_3, reason_type_4], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Rename columns\ndf.columns.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"column_names = ['Date', 'Transportation Expense', 'Distance to Work', 'Age',\n       'Daily Work Load Average', 'Body Mass Index', 'Education',\n       'Children', 'Pets', 'Absenteeism Time in Hours', 'Reason_1',\n       'Reason_2', 'Reason_3', 'Reason_4']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns = column_names","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reorder columns\ncolumn_names_reordered = ['Reason_1', 'Reason_2', 'Reason_3', 'Reason_4',\n                          'Date', 'Transportation Expense', 'Distance to Work', 'Age',\n                          'Daily Work Load Average', 'Body Mass Index', 'Education',\n                          'Children', 'Pets', 'Absenteeism Time in Hours']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df[column_names_reordered]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a checkpoint\ndf_reason_mod = df.copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <a id='5.3.2.'> 5.3.2. Checking Date Feature</a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Analysis of Date feature\ndf_reason_mod[\"Date\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking type of Date variable\ntype(df_reason_mod[\"Date\"][0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Converting datatype to datetime\ndf_reason_mod[\"Date\"] = pd.to_datetime(df_reason_mod[\"Date\"], format = \"%d/%m/%Y\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"type(df_reason_mod[\"Date\"][0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_reason_mod.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Extracting month value\ndf_reason_mod[\"Date\"][0].month","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating month variable\nlist_months = []\n\nfor i in range(df_reason_mod.shape[0]):\n    list_months.append(df_reason_mod[\"Date\"][i].month)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(list_months)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_reason_mod[\"Month Value\"] = list_months","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_reason_mod.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Extract the day of the week\ndf_reason_mod[\"Date\"][699].weekday()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def date_to_weekday(date_value):\n    return date_value.weekday()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating Day of the Week variable\ndf_reason_mod[\"Day of the Week\"] = df_reason_mod[\"Date\"].apply(date_to_weekday)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_reason_mod.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_reason_mod = df_reason_mod.drop([\"Date\"], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_reason_mod.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <a id='5.3.3.'> 5.3.3. Checking Other Features</a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Analysis other columns\ntype(df_reason_mod[\"Transportation Expense\"][0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"type(df_reason_mod[\"Distance to Work\"][0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"type(df_reason_mod[\"Age\"][0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"type(df_reason_mod[\"Daily Work Load Average\"][0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"type(df_reason_mod[\"Body Mass Index\"][0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_reason_mod[\"Education\"].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_reason_mod[\"Education\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Converting Education Feature\ndf_reason_mod[\"Education\"] = df_reason_mod[\"Education\"].map({1:0, 2:1, 3:1, 4:1})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_reason_mod[\"Education\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Final checkpoint\ndf_preprocessed = df_reason_mod.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating the targets\ndf_preprocessed[\"Absenteeism Time in Hours\"].median()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Absenteeism Time in Hours =<3, Moderately absent\n- Absenteeism Time in Hours >3, Excessively absent"},{"metadata":{},"cell_type":"markdown","source":"## <a id='5.3.4.'> 5.3.4. Creating Target Variable</a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"targets = np.where(df_preprocessed[\"Absenteeism Time in Hours\"] > 3,1,0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"targets","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_preprocessed[\"Excessive Absenteeism\"] = targets","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_preprocessed.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking imbalance\ntargets.sum()/targets.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create a checkpoint by dropping the unnecessary variables\n# also drop the variables we 'eliminated' after exploring the weights.\n# after implementing the ml model we also add 3 more columns to remove list because they have very less affect on target feature.\ndf_with_targets = df_preprocessed.drop(['Absenteeism Time in Hours','Day of the Week',\n                                        'Daily Work Load Average','Distance to Work'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_with_targets is df_preprocessed","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_with_targets.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Selecting input for the regression\ndf_with_targets.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <a id='5.4.'> 5.4. Scaling and Splitting</a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"unscaled_inputs = df_with_targets.iloc[:,:-1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import the libraries needed to create the Custom Scaler\n# note that all of them are a part of the sklearn package\n# moreover, one of them is actually the StandardScaler module, \n# so you can imagine that the Custom Scaler is build on it\n\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.preprocessing import StandardScaler\n\n# create the Custom Scaler class\n\nclass CustomScaler(BaseEstimator,TransformerMixin): \n    \n    # init or what information we need to declare a CustomScaler object\n    # and what is calculated/declared as we do\n    \n    def __init__(self,columns,copy=True,with_mean=True,with_std=True):\n        \n        # scaler is nothing but a Standard Scaler object\n        self.scaler = StandardScaler(copy,with_mean,with_std)\n        # with some columns 'twist'\n        self.columns = columns\n        self.mean_ = None\n        self.var_ = None\n        \n    \n    # the fit method, which, again based on StandardScale\n    \n    def fit(self, X, y=None):\n        self.scaler.fit(X[self.columns], y)\n        self.mean_ = np.mean(X[self.columns])\n        self.var_ = np.var(X[self.columns])\n        return self\n    \n    # the transform method which does the actual scaling\n\n    def transform(self, X, y=None, copy=None):\n        \n        # record the initial order of the columns\n        init_col_order = X.columns\n        \n        # scale all features that you chose when creating the instance of the class\n        X_scaled = pd.DataFrame(self.scaler.transform(X[self.columns]), columns=self.columns)\n        \n        # declare a variable containing all information that was not scaled\n        X_not_scaled = X.loc[:,~X.columns.isin(self.columns)]\n        \n        # return a data frame which contains all scaled features and all 'not scaled' features\n        # use the original order (that you recorded in the beginning)\n        return pd.concat([X_not_scaled, X_scaled], axis=1)[init_col_order]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check what are all columns that we've got\nunscaled_inputs.columns.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create the columns to scale, based on the columns to omit\n# use list comprehension to iterate over the list\ncolumns_to_omit = ['Reason_1', 'Reason_2', 'Reason_3', 'Reason_4','Education']\ncolumns_to_scale = [x for x in unscaled_inputs.columns.values if x not in columns_to_omit]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# declare a scaler object, specifying the columns you want to scale\nabsenteeism_scaler = CustomScaler(columns_to_scale)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fit the data (calculate mean and standard deviation); they are automatically stored inside the object \nabsenteeism_scaler.fit(unscaled_inputs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# standardizes the data, using the transform method \n# in the last line, we fitted the data - in other words\n# we found the internal parameters of a model that will be used to transform data. \n# transforming applies these parameters to our data\n# note that when you get new data, you can just call 'scaler' again and transform it in the same way as now\nscaled_inputs = absenteeism_scaler.transform(unscaled_inputs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# the scaled_inputs are now an ndarray, because sklearn works with ndarrays\nscaled_inputs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaled_inputs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check the shape of the inputs\nscaled_inputs.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split the data\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(scaled_inputs, targets, train_size = 0.8, random_state = 42)\nprint(x_train.shape)\nprint(x_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <a id='6.'> 6. Single Logistic Regression and Evaluation</a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Logistic Regression\nfrom sklearn.linear_model import LogisticRegression","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reg = LogisticRegression()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reg.fit(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reg.score(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Manually check the accuracy\nmodel_outputs = reg.predict(x_train)\nnp.sum(model_outputs == y_train)/(model_outputs.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Finding the intercept and coefficients\nreg.intercept_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reg.coef_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"unscaled_inputs.columns.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_name = unscaled_inputs.columns.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"summary_table = pd.DataFrame(columns=[\"feature_name\"], data=feature_name)\nsummary_table[\"Coefficient\"] = np.transpose(reg.coef_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"summary_table","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"summary_table.index = summary_table.index +1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"summary_table","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"summary_table.loc[0] = [\"Intercept\", reg.intercept_[0]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"summary_table = summary_table.sort_index()\nsummary_table","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"summary_table[\"Odds_ratio\"] = np.exp(summary_table.Coefficient)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"summary_table","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"summary_table.sort_values(\"Odds_ratio\", ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Testing the model\nreg.score(x_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted_proba = reg.predict_proba(x_test)\npredicted_proba","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted_proba[:,1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <a id='7.'> 7. References</a>\n\n* https://www.udemy.com/course/the-data-science-course-complete-data-science-bootcamp/\n* https://365datascience.com/"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}