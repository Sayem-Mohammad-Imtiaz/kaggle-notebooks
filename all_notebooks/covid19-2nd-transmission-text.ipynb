{"cells":[{"metadata":{},"cell_type":"markdown","source":"#Code by Latong https://www.kaggle.com/latong/food-review-text-summarization"},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\nfrom bs4 import BeautifulSoup\nimport string\nfrom nltk.tokenize import RegexpTokenizer\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"nRowsRead = 1000 # specify 'None' if want to read whole file\ndf = pd.read_csv('../input/cusersmarildownloadssecondcsv/second.csv', delimiter=';', encoding = \"utf8\", nrows = nRowsRead)\ndf.dataframeName = 'second.csv'\nnRow, nCol = df.shape\nprint(f'There are {nRow} rows and {nCol} columns')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#We need only three columns Score, Summary & Text. Let 's remove all the other columns.\n\n#df=df[['Study Type','Factors', 'Excerpt', 'Sample Text', 'Matches']]\n#df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I wanted to choose another feature though its length must result INTEGER. "},{"metadata":{"trusted":true},"cell_type":"code","source":"#calculating length of lists in pandas dataframe column \n#ref. https://stackoverflow.com/questions/41340341/pythonic-way-for-calculating-length-of-lists-in-pandas-dataframe-column\ndate=df['Date'].str.len()\nprint(date)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's check the length of summaries, the average length is 20 characters.\ndf['date length'] = df['Date'].apply(len)\ndf['date length'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nsns.boxplot(x='Study Type', y=df['date length'], data=df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's do the same to the text, the avearage length is 302 characters.\ndf['study length'] = df['Study'].apply(len)\ndf['study length'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nsns.boxplot(x='Study Type', y=df['study length'], data=df)\nplt.ylim(0, 900)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let 's keep the max length of Date to 30 characters and Study to 300 characters. \ndf= df[df['date length'] <=30]\ndf= df[df['study length'] <=300]\n\nlen(df)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df=df[['Date', 'Study Type','Study']]\ndf.head()\n#len(df) the length of updated dataset is 239868","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.reset_index(drop=True)# we need to reindex the dataset after removed some rows\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import requests\nfrom bs4 import BeautifulSoup \n\ndef preprocess_text(text):\n    \"\"\" Apply any preprocessing methods\"\"\"\n    text = BeautifulSoup(text).get_text()\n    text = text.lower()\n    text = text.replace('[^\\w\\s]','')\n    return text\n\ndf[\"Study\"] = df.Study.apply(preprocess_text)\ndf[\"Date\"] = df.Date.apply(preprocess_text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"field_length = df.Study.astype(str).map(len)\nprint()\nprint(\"###This is the longest text###\")\nprint (df.loc[field_length.idxmax(), 'Study'])\nprint()\nprint(\"###This is the shortest text###\")\nprint()\nprint (df.loc[field_length.idxmin(), 'Study'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk import word_tokenize\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\n#not is in the stopword list. W/O adding whitelist, the summary would change from \"do not recommend\" to \"recommend\". This solution was borrowed from bertcarremans https://github.com/bertcarremans/TwitterUSAirlineSentiment\nwhitelist = [\"n't\", \"not\", \"no\"]\n\ndf['Study_after_removed_stopwords'] = df['Study'].apply(lambda x: ' '.join([word for word in x.split() if word in whitelist or word not in (stop)]))\nprint()\nprint('###Study after removed stopwords###'+'\\n'+df['Study_after_removed_stopwords'][1])\nprint()\nprint('###Study before removed stopwords###'+'\\n'+ df['Study'][1])\nprint()\ndf['Date_after_removed_stopwords'] = df['Date'].apply(lambda x: ' '.join([word for word in x.split() if word in whitelist or word not in (stop)]))\nprint('###Date after removed stopwords###'+ '\\n'+df['Date_after_removed_stopwords'][1])\nprint()\nprint('###Date before removed stopwords###'+'\\n'+df['Date'][1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# A list of contractions from http://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python\ncontractions = { \n\"ain't\": \"am not\",\n\"aren't\": \"are not\",\n\"can't\": \"cannot\",\n\"can't've\": \"cannot have\",\n\"'cause\": \"because\",\n\"could've\": \"could have\",\n\"couldn't\": \"could not\",\n\"couldn't've\": \"could not have\",\n\"didn't\": \"did not\",\n\"doesn't\": \"does not\",\n\"don't\": \"do not\",\n\"hadn't\": \"had not\",\n\"hadn't've\": \"had not have\",\n\"hasn't\": \"has not\",\n\"haven't\": \"have not\",\n\"he'd\": \"he would\",\n\"he'd've\": \"he would have\",\n\"he'll\": \"he will\",\n\"he's\": \"he is\",\n\"how'd\": \"how did\",\n\"how'll\": \"how will\",\n\"how's\": \"how is\",\n\"i'd\": \"i would\",\n\"i'll\": \"i will\",\n\"i'm\": \"i am\",\n\"i've\": \"i have\",\n\"isn't\": \"is not\",\n\"it'd\": \"it would\",\n\"it'll\": \"it will\",\n\"it's\": \"it is\",\n\"let's\": \"let us\",\n\"ma'am\": \"madam\",\n\"mayn't\": \"may not\",\n\"might've\": \"might have\",\n\"mightn't\": \"might not\",\n\"must've\": \"must have\",\n\"mustn't\": \"must not\",\n\"needn't\": \"need not\",\n\"oughtn't\": \"ought not\",\n\"shan't\": \"shall not\",\n\"sha'n't\": \"shall not\",\n\"she'd\": \"she would\",\n\"she'll\": \"she will\",\n\"she's\": \"she is\",\n\"should've\": \"should have\",\n\"shouldn't\": \"should not\",\n\"that'd\": \"that would\",\n\"that's\": \"that is\",\n\"there'd\": \"there had\",\n\"there's\": \"there is\",\n\"they'd\": \"they would\",\n\"they'll\": \"they will\",\n\"they're\": \"they are\",\n\"they've\": \"they have\",\n\"wasn't\": \"was not\",\n\"we'd\": \"we would\",\n\"we'll\": \"we will\",\n\"we're\": \"we are\",\n\"we've\": \"we have\",\n\"weren't\": \"were not\",\n\"what'll\": \"what will\",\n\"what're\": \"what are\",\n\"what's\": \"what is\",\n\"what've\": \"what have\",\n\"where'd\": \"where did\",\n\"where's\": \"where is\",\n\"who'll\": \"who will\",\n\"who's\": \"who is\",\n\"won't\": \"will not\",\n\"wouldn't\": \"would not\",\n\"you'd\": \"you would\",\n\"you'll\": \"you will\",\n\"you're\": \"you are\"\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#This two blocks of code was refered from https://www.kaggle.com/currie32/summarizing-text-with-amazon-reviews\ndef clean_text(text):\n\n    # Replace contractions with longer forms in the above list\n    if True:\n        text = text.split()\n        new_text = []\n        for word in text:\n            if word in contractions:\n                new_text.append(contractions[word])\n            else:\n                new_text.append(word)\n        text = \" \".join(new_text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nclean_studies = []\nfor study in df.Study:\n    clean_studies.append(clean_text(study))\nprint(\"Studies are complete.\")\n\nprint(len(clean_studies))\n\n\nclean_texts = []\nfor text in df.Date:\n    clean_texts.append(clean_text(text))\nprint(\"Texts are complete.\")\nprint(len(clean_texts))\nclean_text","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###According to this link https://stackoverflow.com/questions/54891464/is-it-better-to-keras-fit-to-text-on-the-entire-x-data-or-just-the-train-data\nand other papers, I will do the tokenization to only train dataset. As I don't have enough memory, I will first export the cleaned dataset into csv format; \nSecond subsample the dataset and split the toy dataset into train and test sets; Third, tokenize the train set. \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"r'/amazon_clean","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.to_csv(r'/cusersmarildownloadssecondcsv_clean.csv',index=False)\nlen(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The value of frac was frac=0.001  That resulted in 0 and df.tail was empty. So I changed to 0.100"},{"metadata":{"trusted":true},"cell_type":"code","source":"df1=df.sample(frac=0.100, replace=True, random_state=1)\nlen(df1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1 = df1.reset_index(drop=True)# we need to reindex the dataset after removed some rows","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2=df1[['Study','Date']]\ndf2.head()\ndocs=df2.apply(lambda x: \" \".join(x), axis=1)\ndocs.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\n#create a vocabulary of words, \n#ignore words that appear in 85% of documents, \n#eliminate stop words\ndocs=df2.apply(lambda x: \" \".join(x), axis=1).tolist()\n\ncv=CountVectorizer(max_df=0.85,max_features=10000)\nword_count_vector=cv.fit_transform(docs)\n \nlist(cv.vocabulary_.keys())[:10]\n ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfTransformer\n \ntfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\ntfidf_transformer.fit(word_count_vector)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_count_vector.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nhttps://kavita-ganesan.com/tfidftransformer-tfidfvectorizer-usage-differences/#.XYtStuczbOQ\nneed the term frequency (term count) vectors for different tasks, use Tfidftransformer.\nneed to compute tf-idf scores on documents within“training” dataset, use Tfidfvectorizer\nneed to compute tf-idf scores on documents outside “training” dataset, use either one, both will work.\n\n\"\"\"\n\n# print idf values\ndf_idf = pd.DataFrame(tfidf_transformer.idf_, index=cv.get_feature_names(),columns=[\"idf_weights\"])\n \n# sort ascending\ndf_idf.sort_values(by=['idf_weights'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df3=df[['Study','Date']]\n\ndf3.tail()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In February, we were  \"simulating the infected population and spread of Covid-19\". Curves, curves, and  more curves from the whole World."},{"metadata":{"trusted":true},"cell_type":"code","source":"#https://github.com/kavgan/nlp-in-practice/blob/master/tf-idf/Keyword%20Extraction%20with%20TF-IDF%20and%20SKlearn.ipynb\ndef sort_coo(coo_matrix):\n    tuples = zip(coo_matrix.col, coo_matrix.data)\n    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def extract_topn_from_vector(feature_names, sorted_items, topn=10):\n    \"\"\"get the feature names and tf-idf score of top n items\"\"\"\n    \n    #use only topn items from vector\n    sorted_items = sorted_items[:topn]\n\n    score_vals = []\n    feature_vals = []\n\n    for idx, score in sorted_items:\n        fname = feature_names[idx]\n        \n        #keep track of feature name and its corresponding score\n        score_vals.append(round(score, 3))\n        feature_vals.append(feature_names[idx])\n\n    #create a tuples of feature,score\n    #results = zip(feature_vals,score_vals)\n    results= {}\n    for idx in range(len(feature_vals)):\n        results[feature_vals[idx]]=score_vals[idx]\n    \n    return results","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# you only needs to do this once, this is a mapping of index to \nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfeature_names=cv.get_feature_names()\n \n# get the document that we want to extract keywords from\ndoc=df['Date'][8]\n \n#generate tf-idf for the given document\ntf_idf_vector=tfidf_transformer.transform(cv.transform([doc]))\n \n#sort the tf-idf vectors by descending order of scores\nsorted_items=sort_coo(tf_idf_vector.tocoo())\n \n#extract only the top n; n here is 10\nkeywords=extract_topn_from_vector(feature_names,sorted_items,10)\n \n# now print the results\nprint(\"\\n=====Doc=====\")\nprint(doc)\nprint(\"\\n===Keywords===\")\nfor k in keywords:\n    print(k,keywords[k])\n    \nprint(\"\\n===original study===\")\nprint(df['Study'][8])\n \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#I have no clue if I made anything wrong. Probably, yes. Though the Programm didn't return any error.\n\n#Let me know if you read what I should correct. OK?"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"#Code by Olga Belitskaya https://www.kaggle.com/olgabelitskaya/sequential-data/comments\nfrom IPython.display import display,HTML\nc1,c2,f1,f2,fs1,fs2=\\\n'#eb3434','#eb3446','Akronim','Smokum',30,15\ndef dhtml(string,fontcolor=c1,font=f1,fontsize=fs1):\n    display(HTML(\"\"\"<style>\n    @import 'https://fonts.googleapis.com/css?family=\"\"\"\\\n    +font+\"\"\"&effect=3d-float';</style>\n    <h1 class='font-effect-3d-float' style='font-family:\"\"\"+\\\n    font+\"\"\"; color:\"\"\"+fontcolor+\"\"\"; font-size:\"\"\"+\\\n    str(fontsize)+\"\"\"px;'>%s</h1>\"\"\"%string))\n    \n    \ndhtml('Be patient. Marília Prata, @mpwolke was Here' )","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}