{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as  np\nfrom  sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom sklearn import model_selection \nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom pandas.plotting import scatter_matrix\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import ShuffleSplit\n\ndata=pd.read_csv(\"../input/housingdataset/HousingData.csv\")\n\ndata.head()\n\ndata[\"CRIM\"].value_counts#describe each district of category\n\ndata.info()#used to get quick informatioon of data\n\ndata.describe()#show sumaary of attribute\n\n#plot graphs\ndata.hist(bins=20,figsize=(20,15))\nplt.show\n\n#create test dataset\ntrain_set,test_set=model_selection .train_test_split(data,test_size=0.2  ,random_state=42)\n\n#pd.cut() this function is used to create an income category attribute with 5 categories\ndata['tax_data']=pd.cut(data['TAX'],bins=[0,1.5,3.,4., np.inf],labels=[1,2,3,4])\n\ndata['tax_data'].hist()\n\n#\nsplit= StratifiedShuffleSplit(n_splits=1,test_size=0.2 ,random_state=42)\nfor train_index,test_index in split.split(data,data['tax_data']):\n    strat_train_set=data.iloc[train_index]\n    strat_test_set=data.iloc[test_index]\n\nstrat_test_set[\"tax_data\"].value_counts() / len(strat_test_set)\n\n#now we should remove the data back into its original position\nfor set_ in (strat_train_set,strat_test_set):\n    set_.drop(\"tax_data\",axis =1,inplace =True)\n\n#create a copy\ndata=strat_train_set.copy()\n\n#correlations\ncorr_matrix=data.corr()\n\ncorr_matrix['AGE'].sort_values(ascending=False)\n\nattributes = [\"CRIM\", \"ZN\", \"INDUS\",\n \"CHAS\"]\nscatter_matrix(data[attributes], figsize=(12, 8))\n\n# Get rid of missing values\nimputer = SimpleImputer(strategy=\"median\")\ndata_num=data.drop('CHAS',axis=1)\nimputer.fit(data_num)\n\nimputer.statistics_\n\ndata_num.median().values\n\n\nx=imputer.transform(data_num)\n\n#if you put back into pandas Dtaframe\ndata_=pd.DataFrame(x, columns=data_num.columns)\n\ndata_cat = data[[\"CHAS\"]]\ndata_cat.head(10)\n\n\ndata.isnull().sum()\n\ndata.dropna(inplace=True)\ndata.isnull().sum()\n\ndata['CRIM'].mean()\n\ndata['CRIM'].replace(np.NAN,data['CRIM'].mean()).head(10)\n\ndata['ZN'].mean()\n\ndata['ZN'].replace(np.NAN,data['ZN'].mean()).head(10)\n\ndata['INDUS'].mean()\n\ndata['INDUS'].replace(np.NAN,data['INDUS'].mean()).head(10)\n\ndata['CHAS'].mean()\ndata['CHAS'].replace(np.NAN,data['CHAS'].mean()).head(10)\n\n\ndata['RM'].mean()\ndata['RM'].replace(np.NAN,data['RM'].mean()).head(10)\n\ndata['NOX'].mean()\ndata['NOX'].replace(np.NAN,data['NOX'].mean()).head(10)\n\ndata['AGE'].mean()\ndata['AGE'].replace(np.NAN,data['AGE'].mean()).head(10)\n\ndata['DIS'].mean()\ndata['DIS'].replace(np.NAN,data['DIS'].mean()).head(10)\n\ndata['RAD'].mean()\ndata['RAD'].replace(np.NAN,data['RAD'].mean()).head(10)\n\ndata['TAX'].mean()\ndata['TAX'].replace(np.NAN,data['TAX'].mean()).head(10)\n\ndata['PTRATIO'].mean()\ndata['PTRATIO'].replace(np.NAN,data['PTRATIO'].mean()).head(10)\n\ndata['B'].mean()\ndata['B'].replace(np.NAN,data['B'].mean()).head(10)\n\ndata['LSTAT'].mean()\ndata['LSTAT'].replace(np.NAN,data['LSTAT'].mean()).head(10)\n\ndata['MEDV'].mean()\ndata['MEDV'].replace(np.NAN,data['MEDV'].mean()).head(10)\n\n\nnum_pipeline = Pipeline([\n ('imputer', SimpleImputer(strategy=\"median\")),\n\n ('std_scaler', StandardScaler()),\n ])\ndata_num = num_pipeline.fit_transform(data_num)\n\n\n\ndata_num\n\n\n# Import 'train_test_split'\n# my target is MEDV for training and testing data\nprices = data['MEDV']\nfeatures = data.drop('MEDV', axis = 1)\nfrom sklearn.model_selection import train_test_split\n\n# Shuffle and split the data into training and testing subsets\nX_train, X_test, y_train, y_test = train_test_split(features,prices ,test_size=0.2, random_state = 42)\n\n# Success\nprint(\"Training and testing split was successful.\")\n\n\n\n\n\n\n\ndef fit_model(x,y):\n    cv_sets=ShuffleSplit(n_splits=10,test_size=0.2,random_state=0)\n    # Create a decision tree regressor object\n    regressor=DecisionTreeRegressor()\n    # Create a dictionary for the parameter 'max_depth' with a range from 1 to 10\n    params={'max_depth':[1,2,3,4,5,6,7,8,9,10]}\n    # Create the grid search cv object --> GridSearchCV()\n    grid=GridSearchCV(estimator=regressor,param_grid=params,cv=cv_sets)\n    \n# Fit the grid search object to the data to compute the optimal model\n    grid.fit(x,y)\n# Return the optimal model after fitting the data\n    return grid.best_estimator_\n\nreg = fit_model(X_train, y_train)\n# produce the value of 'max_depth'\nprint(\"parameter 'max_depth'is {} for optimal model.\".format(reg.get_params()['max_depth']))\n\n\n\n\n\n\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-09-04T15:25:24.320236Z","iopub.execute_input":"2021-09-04T15:25:24.320888Z","iopub.status.idle":"2021-09-04T15:25:30.838887Z","shell.execute_reply.started":"2021-09-04T15:25:24.32076Z","shell.execute_reply":"2021-09-04T15:25:30.837878Z"},"trusted":true},"execution_count":null,"outputs":[]}]}