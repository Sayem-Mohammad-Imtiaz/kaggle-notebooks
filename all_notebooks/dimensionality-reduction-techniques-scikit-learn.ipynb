{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"Created by: Sangwook Cheon\n\nDate: Dec 31, 2018\n\nThis is step-by-step guide to Dimensionality Reduction, which I created for reference. I added some useful notes along the way to clarify things. This notebook's content is from A-Z Datascience course, and I hope this will be useful to those who want to review materials covered, or anyone who wants to learn the basics of Dimensionality Reduction.\n\n## Content:\n### 1. Principal Component Analysis (PCA)\n### 2. Linear Discriminant Analysis (LDA)\n### 3. Kernel PCA\n--------\nThese are are all part of Feature Extraction. Dimensionality regression is used to reduce the number of independent variables. Two variables will be left that most explains the variance.\n\n### Note:\nPCA and LDA are used for **linear** problems, and Kernel PCA is used for **non-linear** problems.\n\n\n# 1. Principal Component Analysis (PCA)\nReduce the dimensions of a d-dimensional dataset by projecting it onto a (k)-dimensional subspace (where k<d). Extracted features (independent variables) are called principal components."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndataset = pd.read_csv('../input/winedata1/Wine.csv')\ndataset.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a83362e4643300e2e50ba16470ac89ae41c6fac6"},"cell_type":"markdown","source":"### Dataset overview\nBased on independent variables about the wine, the algorithm should be able to predict which customer segment a new wine should belong to so that it can be recommended to this specific segment. We need to choose 2 most important variables (principal components) among more than 10 variables present right now. To do this, we need to use PCA. "},{"metadata":{"trusted":true,"_uuid":"2452813e22fe856714e385246b70c956328d5862"},"cell_type":"code","source":"#data preparation\nx = dataset.iloc[:, 0:13].values\ny = dataset.iloc[:, 13].values\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size = 0.2, random_state = 0)\n\n#feature scaling\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"79471799d79940a58be8e94a65854069ce9381a2"},"cell_type":"markdown","source":"Right after data preprocessing, we need to apply PCA. So, let's apply PCA below."},{"metadata":{"trusted":true,"_uuid":"2d4dc83446aa1ea3099ccefc6d88af609734b4ed"},"cell_type":"code","source":"#Applying PCA\nfrom sklearn.decomposition import PCA\npca = PCA(n_components = None)\n#n_componentes: number of extracted features (independent variables) to get. \n                #None is used as we do not know what is the right amount of features\nX_train = pca.fit_transform(X_train)\nX_test = pca.transform(X_test)\nexplained_variance = pca.explained_variance_ratio_\n#percentage of variance explained by each of the principal components that we extracted\n\nexplained_variance","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b252a1d081a7f789c994eb2e0661e550abc760da"},"cell_type":"markdown","source":"As we had 13 independent variables, the algorithm ranked these variables from the first principal component that explains the most variance to the one with the least variance. Therefore, if we include one component, we will take 37 % of  the variance. If two, 37 % + 19 % = 56 %. In this case, taking two components would be sufficient.\n\nNow that the number of n_components is set, we can redo the PCA process."},{"metadata":{"trusted":true,"_uuid":"af175941554013dfb907ab6e763079ed8eac85b3"},"cell_type":"code","source":"# --------------------------------------------------------------------------------------------------\nx = dataset.iloc[:, 0:13].values\ny = dataset.iloc[:, 13].values\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size = 0.2, random_state = 0)\n\n#feature scaling\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)\n#----------------------------------------------------------------------------------------------------\n#Doing this part again just because X_train and X_test are already transformed from the previous step\n\npca = PCA(n_components = 2)\nX_train = pca.fit_transform(X_train)\nX_test = pca.transform(X_test)\n\nX_train[:5, :]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"98b855bfe131f9d5df1d2d0a07d8dbc75e081c09"},"cell_type":"markdown","source":"Now, we only have two independent variables! Let's simply apply Logistic Regression model as we prepared our dataset using PCA. If you want to learn about Logistic regression and other classification techniques (Logistic regression is also part of classification), [please refer to this kernel.](https://www.kaggle.com/sangwookchn/classification-techniques-using-scikit-learn)"},{"metadata":{"trusted":true,"_uuid":"93490b6f82342bbb9f0b42da3c33f340439217e1"},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nclassifier = LogisticRegression()\nclassifier.fit(X_train, Y_train)\ny_pred = classifier.predict(X_test)\n\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(Y_test, y_pred)\n\ncm","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7f99be48b90652b5ced5b2a2efe0b97ccc6c90ba"},"cell_type":"markdown","source":"As numbers on the diagonal line shows the correct predictions, we can say the model is really good. Let's move onto the next one.\n\n# 2. Linear Discriminant Analysis (LDA)\nFrom the n independent variables of the dataset, LDA extracts p â‰¤ n new independent variables that separate the most the classes of the dependent variable. While PCA is unsupervised learning, LDA is supervised as it chooses variables in relation to the dependent variable."},{"metadata":{"trusted":true,"_uuid":"49ed290e92abc6546c105c9634fce6587960e43f"},"cell_type":"code","source":"x = dataset.iloc[:, 0:13].values\ny = dataset.iloc[:, 13].values\n\nX_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size = 0.2, random_state = 0)\n\n#feature scaling\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)\n\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\nlda = LDA(n_components = 2)\nX_train = lda.fit_transform(X_train, Y_train) #y_train is required as this is supervised learning\nX_test = lda.transform(X_test)\n\nclassifier = LogisticRegression()\nclassifier.fit(X_train, Y_train)\ny_pred = classifier.predict(X_test)\n\ncm = confusion_matrix(Y_test, y_pred)\ncm","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0d155582b368a6815d2137d6c51a3b6524f75938"},"cell_type":"markdown","source":"As you can notice, LDA worked better than PCA. This is because the algorithm found the two variables that separate between classes the most. Finally, let's see how Kernel PCA works\n\n# 3. Kernel PCA\nIn fact, previous models are linear models. For non-linear problems, Kernel PCA works very well. Kernel PCA is not much different from PCA because we are only using an extra kernel trick to map the dataset to a higher dimension, and then make the dataset linearly separable. Let's use a dataset that has non-linear patterns: Social Network Ads dataset.\n\n### Dataset Overview"},{"metadata":{"trusted":true,"_uuid":"8104edf15dc32f0d9ac2ce13494d3a62bea8e03c"},"cell_type":"code","source":"dataset2 = pd.read_csv('../input/social-network-ads/Social_Network_Ads.csv')\ndataset2.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d18176a9dfba45a193d9773be617fcba8c14efbf"},"cell_type":"markdown","source":"In this dataset, the company is trying to see whether or not a customer will buy a product according to gender, age and estimated salary. "},{"metadata":{"trusted":true,"_uuid":"f20b15af0857e91638ffbe0ac3fb19e220df7f3e"},"cell_type":"code","source":"x = dataset2.iloc[:, [2,3]].values\ny = dataset2.iloc[:, 4].values\n\nX_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size = 0.2, random_state = 0)\n\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)\n\n# ------------ Please pay attention to this following part ---------------\nfrom sklearn.decomposition import KernelPCA\nkpca = KernelPCA(n_components = 2, kernel = 'rbf') \n#rbf is \"gaussian\" method that maps values to a higher dimension\n\nX_train = kpca.fit_transform(X_train)\nX_test = kpca.transform(X_test)\n# ------------------------------------------------------------------------\n\nclassifier = LogisticRegression()\nclassifier.fit(X_train, Y_train)\ny_pred = classifier.predict(X_test)\n\ncm = confusion_matrix(Y_test, y_pred)\ncm","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5f680aa060fbddc48d7f963d09bc1d9b3e78dabc"},"cell_type":"markdown","source":" The algorithm produced very good results! \n \n -------------\n Thank you for reading this kernel. If you found this helpful, I would appreciate if you upvote the kernel or put a short comment.  "},{"metadata":{"trusted":true,"_uuid":"3ff5b60095ba5fefedeca3e66709f88d1ab6abbe"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}