{"cells":[{"metadata":{},"cell_type":"markdown","source":"ATTENTION : this second step takes the database cleant and created at the end of this step [step1](https://www.kaggle.com/leticehs/nlp-analysis-part1-dataset-cut)"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"First of all, I cut the dataset linked to this challenge in this notebook, https://www.kaggle.com/leticehs/nlp-analysis-part1-dataset-cut\nCheck it out first, as we will now use the dataset with cleant dates and only english tweets, below."},{"metadata":{},"cell_type":"markdown","source":"applying deep learning on twitterâ€™s sentiment analysis as below\n\n*   Prepare Data - download and prepare 21 millions tweets dat - check part 1 as linked above\n*   Perform Pre-processing - use NLP techniques to pre-process the tweets => this is the notebook\n\n\nLater on NLP3 to 5, I will describe the steps below, until topic clustering and prediction\n*   Discover Important Words - use tf-idf and counting to find important words\n*   Convert Word Representation - download word2vec and apply to the important word\n\n*   Train Model - use keras to build and train a deep neural network model\n\n*   Evaluate Model - measure the accuracy of the predictive model, and suggest further improvements\n"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"!pip install contractions\nfrom time import time\nimport pandas as pd\nimport numpy as np\nimport re\nimport sys\nimport csv\nimport matplotlib.pyplot as plt\nfrom matplotlib import rcParams\nimport seaborn as sns\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.tokenize import casual\n\nimport itertools\nimport datetime\n\nimport pprint\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier,AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reading the dataset with no columns titles and with latin encoding \ndf = pd.read_csv('/kaggle/input/bitcoin-tweets-20160101-to-20190329/tweetsENdates.csv')\ndf.sample(3)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking if there is any missing value and datatype \ndf.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# checking for null values, if any\ndf.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#ditching all row when text is null, as need text for analysis\ndf.dropna(how='any', inplace=True)\ndf.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#setting column width wider to read text \npd.set_option('display.max_colwidth',700)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Ommiting every column except for the text and the date, as we won't need any of the other information\ndf = df[['date','text']]\ndf.sample(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1 - Is dataset unbalanced ? do we need to split according to a criteria and take sample of those? splitting, skimming and concatening?"},{"metadata":{},"cell_type":"markdown","source":"1a/ CLEANING DATE"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create split by dates \ndf.date.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndf.date = pd.to_datetime(df['date'], format = '%Y-%m-%d')\ndf.sample(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1b/ Checking if tweets DO PROMOTE with subscribe"},{"metadata":{"trusted":true},"cell_type":"code","source":"#can we get counts of tweets that contain Follow us or subscribe?\n(df['text'].str.contains('subscribe|follow')).sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#checking the few tweets that do NOT contain follow or subscribe\n(~df['text'].str.contains('subscribe|follow')).sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1c/ CHecking if tweets can be CLASSIFIED by TAGS\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#checking balance of tweets having # or not\ndf['text'].str.contains('#').sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"SOME VISUALIZATION"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.sort_values(by='date')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['year'] = df['date'].dt.year\ndf['month'] = df['date'].dt.month\ndf['day'] = df['date'].dt.day","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.groupby('year').sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.year.value_counts().loc[lambda x : x>10000] ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualization of news article count per year\n\nplt.figure(figsize=(10,5))\nplt.xlabel(\"Year\")\nplt.ylabel(\"Counts\")\n\ndf.year.value_counts().plot(kind='bar')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualization of news publications names and article count in the dataset\n\nimport matplotlib.pyplot as plt\n\ncolor_list = list('rgbkymc')  #red, green, blue, black, etc.\nplt.figure(figsize=(10,7))\n\nplt.xlabel(\"date\")\nplt.ylabel(\"counts\")\n\ndf.month.value_counts().plot(kind='bar', color=color_list)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"2 - skimming tokenize, lemmatize"},{"metadata":{},"cell_type":"markdown","source":"2a / PART OF SPEECH CLEANING"},{"metadata":{"trusted":true},"cell_type":"code","source":"#expanding contractions\ntry:\n  df['text1'] = df['text'].apply(lambda x: [contractions.fix(word) for word in x.split()])\nexcept:\n  df['text1'] = df['text']\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#extracting the URL to clean what can be analysed\ndef get_url(x):\n  urls = re.findall('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', x)\n  \n  if len(urls)==0:\n    return ' '\n  else:\n    return urls","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['url'] = df['text1'].apply(lambda x: get_url(x) )\ndf['url'] = [' '.join(map(str, l)) for l in df['url']]\ndf.sample(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#extracting the Tags to be able to clearer\ndef tag(x):\n  hashtag = set(part[1:] for part in x.split() if part.startswith('#'))\n  \n  if len(hashtag)==0:\n    return ' '\n  else:\n    return hashtag\n\ndf['tags'] = df['text1'].apply(lambda x: tag(x) )\ndf['tags'] = [' '.join(map(str, l)) for l in df['tags']]\ndf.sample(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#extracting the follow or subscribe info on a specific column\ndef prom(x):\n  promoting = re.findall('subscribe | follow | promo', x)\n  \n  if len(promoting)==0:\n    return ' '\n  else:\n    return promoting\n\ndf['promote'] = df['text1'].apply(lambda x: prom(x) )\ndf['promote'] = [' '.join(map(str, l)) for l in df['promote']]\ndf.sample(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"3 - CLEAN TEXT"},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\nimport string\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords \nimport regex as re\nimport contractions\n# Install spaCy (run in terminal/prompt)\nimport sys\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#!!!!! CLEANING PROCESS, TAKES 38-40 minutes...\n#this command needs to be pushed before tokenization\n# >>>> COULD ADD LINE FOR AUTOCORRECT ET REMPLACER LES EMOJI PAR CE QU ILS MEAN\n\nimport nltk\nnltk.download('stopwords')\n\ndef process(text):\n\n    #Convert to lower case\n    text = text.lower()\n    #Convert www.* or https?://* to URL\n    text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))',' ',text)\n    #Remove @username \n    text = re.sub('@[^\\s]+',' ',text)\n    #Remove additional white spaces\n    text = re.sub('[\\s]+', ' ', text)\n    #Replace #word with word\n    text = re.sub(r'#([^\\s]+)', r'\\1', text)\n    #trim\n    text = text.strip('\\'\"')\n\n    #remove punctuation\n    for punctuation in string.punctuation: \n        text = text.replace(punctuation, ' ') \n\n    #words only\n    text = ''.join([i for i in text if not i.isdigit()])\n\n    #tokenize\n    words = nltk.tokenize.casual_tokenize(text)\n\n    #Remove stopwords\n    stops = set(stopwords.words('english'))\n    clean = [w for w in words if not w in stops]\n\n    return ' '.join(clean)\n\ndf['clean'] = df['text1'].apply(lambda text: process(text))\n\ndf.sample(3, random_state = 3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.reset_index()\ndf = df[['date', 'year','clean','url','tags','promote']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#export to new CSV as it crashes otherwise\ndf.to_csv('/content/drive/MyDrive/Colab Notebooks/NLP/tweetsClean.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"no need to stem or lemmatize, the words are so small, that it ends up just letters"},{"metadata":{},"cell_type":"markdown","source":"STOP VISU - TESTING WORD CLOUD WITH WHAT WE HAVE"},{"metadata":{"trusted":true},"cell_type":"code","source":"pip install wordcloud","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud, STOPWORDS\nimport matplotlib.pyplot as plt\n# Light pre-processing\nstopwords = set(STOPWORDS)\n# Define Word Cloud generation function\ndef show_wordcloud(data, title = None):\n    wordcloud = WordCloud(\n        background_color='white',\n        stopwords=stopwords,\n        max_words=1000,\n        max_font_size=40, \n        scale=3,\n        random_state=1 # chosen at random by flipping a coin; it was heads\n    ).generate(str(data))\n    fig = plt.figure(1, figsize=(12, 12))\n    plt.axis('off')\n    if title: \n        fig.suptitle(title, fontsize=20)\n        fig.subplots_adjust(top=2.3)\n    plt.imshow(wordcloud)\n    plt.show()\nwordworld_text = df['clean']\nshow_wordcloud (wordworld_text)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Visualization n-grams of most important words"},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.util import ngrams\nfrom sklearn.feature_extraction.text import CountVectorizer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Mono-grams - takes a while - 10 minutes\n\ndef plot_top_ngrams_barchart(text, n=1):\n\n    new= text.str.split()\n    new=new.values.tolist()\n    corpus=[word for i in new for word in i]\n\n    def _get_top_ngram(corpus, n=None):\n        vec = CountVectorizer(ngram_range=(n, n)).fit(corpus)\n        bag_of_words = vec.transform(corpus)\n        sum_words = bag_of_words.sum(axis=0) \n        words_freq = [(word, sum_words[0, idx]) \n                      for word, idx in vec.vocabulary_.items()]\n        words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n        return words_freq[:20]\n\n    top_n_bigrams=_get_top_ngram(text,n)[:20]\n    x,y=map(list,zip(*top_n_bigrams))\n    plt.figure(figsize=(10,10))\n    plt.xlabel(\"Monogram Frequency\")\n    plt.ylabel(\"Top 20 KKW in EN tweets\")\n    sns.barplot(x=y,y=x)\n\n\nplot_top_ngrams_barchart(df['clean'],1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Bi-grams - takes a while - 5 minutes\n\ndef plot_top_ngrams_barchart(text, n=2):\n\n    new= text.str.split()\n    new=new.values.tolist()\n    corpus=[word for i in new for word in i]\n\n    def _get_top_ngram(corpus, n=None):\n        vec = CountVectorizer(ngram_range=(n, n)).fit(corpus)\n        bag_of_words = vec.transform(corpus)\n        sum_words = bag_of_words.sum(axis=0) \n        words_freq = [(word, sum_words[0, idx]) \n                      for word, idx in vec.vocabulary_.items()]\n        words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n        return words_freq[:20]\n\n    top_n_bigrams=_get_top_ngram(text,n)[:20]\n    x,y=map(list,zip(*top_n_bigrams))\n    plt.figure(figsize=(10,10))\n    plt.xlabel(\"Bi-gram Frequency\")\n    plt.ylabel(\"Top 20 KW in EN tweets\")\n    sns.barplot(x=y,y=x)\n\n\nplot_top_ngrams_barchart(df['clean'],2)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"STOP NOW AND IT WILL BE FOLLOWED BY NOTEBOOK NLP 3"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}