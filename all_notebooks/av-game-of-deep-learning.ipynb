{"cells":[{"metadata":{},"cell_type":"markdown","source":"In this competition, i will be using Kaggle platform for modelling (GPU and internet enabled). The libraries i will be using are fastai datablock api which is built on Pytorch. please visit [here](https://www.fast.ai//) for all the course materials.\nThanks to `Jeremy Howard` and `Rachel Thomas`.\n"},{"metadata":{},"cell_type":"markdown","source":"### Load libraries and read the data"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n# from sklearn.metrics import f1_score\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nimport cv2\nimport glob\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- lets import the fastai libraries "},{"metadata":{"trusted":true},"cell_type":"code","source":"from pathlib import Path\nfrom fastai import *\nfrom fastai.vision import *\nimport torch\nfrom fastai.callbacks.hooks import *","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## set the data folder\ndata_folder = Path(\"../input\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_path = \"../input/train/images/\"\npath = os.path.join(data_path , \"*jpg\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"files = glob.glob(path)\ndata=[]\nfor file in files:\n    image = cv2.imread(file)\n    data.append(image)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"## read the csv data files\ntrain_df = pd.read_csv('../input/train/train.csv')\ntest_df = pd.read_csv('../input/test_ApKoW4T.csv')\nsubmit = pd.read_csv('../input/sample_submission_ns2btKE.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.shape, test_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# submit.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.groupby('category').count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x='category' , data=train_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- we have 5 categories as mentioned in the problem"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_images = data[:6252]\ntest_images= data[6252:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## mapping the ship categories  \ncategory = {'Cargo': 1, \n'Military': 2, \n'Carrier': 3, \n'Cruise': 4, \n'Tankers': 5}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- we will plot pictures from all the classes to look at those cool ships"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_class(cat):\n    \n    fetch = train_df.loc[train_df['category']== category[cat]][:3]\n    fig = plt.figure(figsize=(20,15))\n    \n    for i , index in enumerate(fetch.index ,1):\n        plt.subplot(1,3 ,i)\n        plt.imshow(train_images[index])\n        plt.xlabel(cat + \" (Index:\" +str(index)+\")\" )\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_class('Cargo')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_class('Military')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_class('Carrier')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_class('Tankers')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_class('Cruise')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"These pictures seems to be taken from side and not from top (like picture taken from a satellite) except may be few of them which look like taken from above (although not from high above). We also noticed that all the pictures are of different sizes. we have to make sure they are of same sizes before modelling.\n\nFrom deep learning context, we do not have very large number of images per category. So we will heavily depend on data augmentation, otherwise, it will easily cause overfitting."},{"metadata":{},"cell_type":"markdown","source":"### Modelling Approach"},{"metadata":{},"cell_type":"markdown","source":"- i am using fastai datablock api to create our databunch and train model using cnn"},{"metadata":{"trusted":true},"cell_type":"code","source":"# doc(src.transform)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's define the transformations to be done to the images.\n- random flipping of the images `do_flip=True`. Tried with `False` as well.\n- switch off vertical flipping (it's default behaviour). This option is useful when pictures are taken from high above.\n- let's rotate the pictures a bit randomly `max_rotate=10` (it's already default ).\n- zoom in a higher bit `max_zoom` (as we are dealing with ships picture that are small compared to overall image).\n- `max_warp` is set to zero as it seems to perform better in this case.\n- apply lighting and probability of affine function .\nFor details on transformation visit [here][1]\n\n[1]: https://docs.fast.ai/vision.transform.html"},{"metadata":{"trusted":true},"cell_type":"code","source":"##transformations to be done to images\ntfms = get_transforms(do_flip=False,flip_vert=False ,max_rotate=10.0, max_zoom=1.22, max_lighting=0.22, max_warp=0.0, p_affine=0.75,\n                      p_lighting=0.75)\n#, xtra_tfms=zoom_crop(scale=(0.9,1.8), do_rand=True, p=0.8))\n\n## create databunch of test set to be passed\ntest_img = ImageList.from_df(test_df, path=data_folder/'train', folder='images')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.random.seed(145)\n## create source of train image databunch\nsrc = (ImageList.from_df(train_df, path=data_folder/'train', folder='images')\n       .split_by_rand_pct(0.2)\n       #.split_none()\n       .label_from_df()\n       .add_test(test_img))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's create our databunch. I will be using `size = 299` for modelling purpose, however let's try even higher size picture to improve accuracy further. But beware that we have to adjust batchsize accordingly to run out of memory. in case of 299 size `bs=32` is used while for 484 or even 599, a smaller batchsize should be used. \n\nThe reflection padding mode seems to work better in this case (`padding_mode='reflection'`).      \nwe will use Squishing resize method.      \nFinally, we normalize the parameters using imagenet_stats "},{"metadata":{"trusted":true},"cell_type":"code","source":"data = (src.transform(tfms, size=299,padding_mode='reflection',resize_method=ResizeMethod.SQUISH)\n        .databunch(path='.', bs=32, device= torch.device('cuda:0')).normalize(imagenet_stats))\n\n# data = (src.transform(tfms, size=484,padding_mode='reflection',resize_method=ResizeMethod.SQUISH)\n#         .databunch(path='.', bs=16, device= torch.device('cuda:0')).normalize(imagenet_stats))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## lets see the few images from our databunch\ndata.show_batch(rows=3, figsize=(12,12))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(data.classes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# doc(cnn_learner)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we will create cnn learner. "},{"metadata":{"trusted":true},"cell_type":"code","source":"#lets create learner. tried with resnet152, densenet201, resnet101\nlearn = cnn_learner(data=data, base_arch=models.resnet101, metrics=[FBeta(beta=1, average='macro'), accuracy],\n                    callback_fns=ShowGraph)\n\n# learn = cnn_learner(data=data, base_arch=models.densenet161, metrics=[FBeta(beta=1, average='macro'), accuracy],\n#                     callback_fns=ShowGraph).mixup()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"`mixup()` on the above learner refer to a pretty aberrant feature which seems very unfamailiar to us (human) however this technique works better for computers. Details of the paper can be found [here](https://arxiv.org/abs/1710.09412).  The theory behind is like this:\n- we do not train dirctely on the raw data images instead the model is trained on mixes of images i.e. we add 2 or more images to combine a single picture by this: `new_image = t * image1 + (1-t) * image2` (not necessary to take only 2 images). By using this same technique, targets are changed as well `new_target = t * target1 + (1-t) * target2` ; where t is a float between 0 and 1.\n- Let's take an example: we are training on cat/dog dataset and we mix 2 images (one of each kind) and we finally get a single image of like this. It not clear to us what image is this; may be 70% dog and 30% cat.\n![image](https://docs.fast.ai/imgs/mixup.png)\n- One  thing to note that, the mixup model may perform better than the regular one but when you compare the traing/validation losses, the mixup model has loss far greater than the regular one (although accuracy seems better in mixup model). This is because the mixup model predictions are less confident about the target. i.e. when we do prediction through our normal model, model seems pretty confident about the target (one target probability prediction will be close to 1 and others will be close to 0). When we predict through mixup model, the probabilities values of the targets will more likely to be close to one another).\nFor details visit [here](https://docs.fast.ai/callbacks.mixup.html)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#learn.opt_func = optim.Adam\n#learn.crit = FocalLoss()\n# learn_gen = None\n# gc.collect()\n# torch.cuda.empty_cache()\nlearn.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Learning rate finder plots lr vs loss relationship for a Learner. The idea is to reduce the amount of guesswork on picking a good starting learning rate.\nIf you pass `suggestion=True` in `learn.recorder.plot`, you will see the point where the gardient is the steepest with a\nred dot on the graph. We can use that point as a first guess for an LR. Details can be found [here](https://docs.fast.ai/basic_train.html#lr_find)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#lets find the correct learning rate to be used from lr finder\nlearn.lr_find()\nlearn.recorder.plot(suggestion=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#lets start with steepset slope point. adding wd (weight decay) not to overfit as we are running 15 epochs \nlr = 3e-03\n#learn.fit_one_cycle(10, slice(lr))\nlearn.fit_one_cycle(15, slice(lr), wd=0.2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now Unfreeze entire model.This Sets every layer group to trainable (i.e. `requires_grad=True`)."},{"metadata":{"trusted":true},"cell_type":"code","source":"#lets plot the lr finder record\nlearn.unfreeze()\nlearn.lr_find()\nlearn.recorder.plot(suggestion=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train for  more cycles after unfreezing\nlearn.fit_one_cycle(10,slice(1e-05,lr/8),wd=0.15)\n#learn.fit_one_cycle(10, slice(5e-06, lr/8))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"lets freeze the all layers except last 3  as these are initial layers for finding recurring pattern/ shapes/corners etc. (not exactly helpful in finding ships). so its better not to change stats of those layers"},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.freeze_to(-3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## finding the LR\nlearn.lr_find()\nlearn.recorder.plot(suggestion=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Train for few  more cycles (we will be setting two LRs in below trainings: first one to train the initial layers and second to\ntrain last layers ).    \n- As initial layers' stats are imagenet stats which are helpful in finding patterns (discussed above) not \nthe exact ships, so we will be training those layers with very low learning rates (to not to greatly change those initial layer\nparameters)"},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.fit_one_cycle(6, slice(1e-06, lr/10),wd=0.1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## freezing initial all layers except last 2 layers\nlearn.freeze_to(-2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## training for few cylcles more\nlearn.fit_one_cycle(6, slice(5e-07, lr/20),wd=0.1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.freeze_to(-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## training even more\nlearn.fit_one_cycle(5, slice(1e-07, lr/30),wd=0.05)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.fit_one_cycle(6, slice(1e-07, lr/100))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Interpretation"},{"metadata":{"trusted":true},"cell_type":"code","source":"#lets see the most mis-classified images (on validation set)\ninterp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_top_losses(9, figsize=(7,6))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.recorder.plot_losses()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"interp.plot_confusion_matrix(figsize=(6,6), dpi=60) ## on validation set","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seems like our model is finding difficult to distinguish between ship 1 (cargo) and 5 (tanker)"},{"metadata":{"trusted":true},"cell_type":"code","source":"interp.most_confused(min_val=4) ## on validation set","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"idx=1\nx,y = data.valid_ds[idx]\nx.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"k = tensor([\n    [0.  ,-5/3,1],\n    [-5/3,-5/3,1],\n    [1.  ,1   ,1],\n]).expand(1,3,3,3)/6","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t = data.valid_ds[1][0].data; t.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"edge = F.conv2d(t[None], k)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"show_image(edge[0], figsize=(5,5));","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"m = learn.model.eval();\nxb,_ = data.one_item(x)\nxb_im = Image(data.denorm(xb)[0])\nxb = xb.cuda()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def hooked_backward(cat=y):\n    with hook_output(m[0]) as hook_a: \n        with hook_output(m[0], grad=True) as hook_g:\n            preds = m(xb)\n            preds[0,int(cat)].backward()\n    return hook_a,hook_g","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hook_a,hook_g = hooked_backward()\nacts  = hook_a.stored[0].cpu()\nacts.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"avg_acts = acts.mean(0)\navg_acts.shape\ntorch.Size([11, 11])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def show_heatmap(hm):\n    _,ax = plt.subplots()\n    xb_im.show(ax)\n    ax.imshow(hm, alpha=0.6, extent=(0,352,352,0),\n              interpolation='bilinear', cmap='magma');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"show_heatmap(avg_acts)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Predicted using TTA as it could improve accuracy further (Test time augmentation).\nApplies Test Time Augmentation to our learner on the dataset.    \nHere We take the average of our regular predictions (with a weight beta) with the average of predictions obtained through augmented \nversions of the training set (with a weight 1-beta). Details can be found [here](https://docs.fast.ai/basic_train.html#Test-time-augmentation)"},{"metadata":{"trusted":true},"cell_type":"code","source":"##learn.TTA improves score further. lets see for the validation set\npred_val,y = learn.TTA(ds_type=DatasetType.Valid)\nfrom sklearn.metrics import f1_score, accuracy_score\nvalid_preds = [np.argmax(pred_val[i])+1 for i in range(len(pred_val))]\nvalid_preds = np.array(valid_preds)\ny = np.array(y+1)\naccuracy_score(valid_preds,y),f1_score(valid_preds,y, average='micro')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we have achieved more than 98% accuracy."},{"metadata":{},"cell_type":"markdown","source":"### Prediction"},{"metadata":{},"cell_type":"markdown","source":"- Before predicting on the test set, i generally (sometimes) remove the validation set and try all these steps above on whole train set for modelling.  This is to produce  model not on 80% data but 100% data and predict the testset\n- This can done by changing in one line in data creation i.e. change `.split_by_rand_pct(0.2)` to `.split_none()`."},{"metadata":{"trusted":true},"cell_type":"code","source":"preds,_ = learn.TTA(ds_type=DatasetType.Test)\n#preds,_ = learn.get_preds(ds_type = DatasetType.Test)\nlabelled_preds = [np.argmax(preds[i])+1 for i in range(len(preds))]\n\nlabelled_preds = np.array(labelled_preds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#create submission file\ndf = pd.DataFrame({'image':test_df['image'], 'category':labelled_preds}, columns=['image', 'category'])\ndf.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## function to create download link\nfrom IPython.display import HTML\ndef create_download_link(title = \"Download CSV file\", filename = \"data.csv\"):  \n    html = '<a href={filename}>{title}</a>'\n    html = html.format(title=title,filename=filename)\n    return HTML(html)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"create_download_link(filename = 'submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.category.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Final submission"},{"metadata":{},"cell_type":"markdown","source":"- At this point, i have submission files of around 22 which are created with different augmentation techniques, with/without mixup, different pretrained model (used resnet101, resnet152, densenet161, densenet169), different image sizes (tried with size 224, 299, 484, 599). All the prediction are based on TTA. \n- The final submission was created based on voting technique by all the submission predicted categories.\n- I could have tried with predicting the probabilities of the image category of all these submissions and then the final submission could have been based on average of those probabilities values. But I am guessing there might have been one challenge: as mentioned above in the notebook, the mixup model predctions are less confident about the target and hence due to this the average probabilities could have been very different leading to different results (may be). \n\nFor e.g. lets take cat/dog image: without mixup, the model predicts cat (prob: 0.94) and dog (0.06) while the mixup model predicts cat (prob:0.56) and dog (0.44) and the average of these 2 models is cat( 0.75) and dog(0.25). Well this might seems fine, however let's imagine we have more target categories to predict, and in this scenario the mixup model probabilities could alter the average probabilities. "},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}