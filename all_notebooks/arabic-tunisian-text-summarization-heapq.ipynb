{"cells":[{"metadata":{"_uuid":"0485530c-1a21-4fc0-97e0-a13885a68759","_cell_guid":"acb3dcd5-358b-4b67-9443-35df40635adf","trusted":true},"cell_type":"code","source":"import nltk \nfrom nltk.corpus import stopwords\nfrom nltk.cluster.util import cosine_distance\nimport numpy as np\nimport networkx as nx\nimport pandas as pd \n\n    \nimport collections\n\nimport random\nimport re\nfrom collections import Counter\nfrom itertools import islice\nimport nltk\nfrom nltk.corpus import stopwords\nimport numpy as np \nimport pandas as pd \npd.set_option('display.max_colwidth', -1)\nfrom time import time\nimport re\nimport string\nimport os\nimport emoji\nfrom pprint import pprint\nimport collections\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style=\"darkgrid\")\nsns.set(font_scale=1.3)\nfrom sklearn.metrics import f1_score\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.metrics import classification_report\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression\nimport joblib\nimport gensim\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.tokenize import word_tokenize\nimport warnings\nwarnings.filterwarnings('ignore')\nnp.random.seed(37)  \nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder                                              ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"arabic_diacritics = re.compile(\"\"\" ّ    | # Tashdid\n                             َ    | # Fatha\n                             ً    | # Tanwin Fath\n                             ُ    | # Damma\n                             ٌ    | # Tanwin Damm\n                             ِ    | # Kasra\n                             ٍ    | # Tanwin Kasr\n                             ْ    | # Sukun\n                             ـ     # Tatwil/Kashida\n                         \"\"\", re.VERBOSE)\n\n\ndef remove_diacritics(text):\n    text = re.sub(arabic_diacritics, '', str(text))\n    return text\n\n\n\ndef remove_repeating_char(text):\n    # return re.sub(r'(.)\\1+', r'\\1', text)     # keep only 1 repeat\n    return re.sub(r'(.)\\1+', r'\\1\\1', text)  # keep 2 repeat\n\ndef remove_short_words(text): \n    return str(text).replace(r'\\b(\\w{1,3})\\b', '')\n\ndef process_text(text, grams=False):\n    clean_text = remove_diacritics(text)\n    clean_text = remove_repeating_char(clean_text)\n    clean_text = remove_short_words(clean_text)\n    if grams is False:\n        return clean_text.split()\n    else:\n        tokens = clean_text.split()\n        grams = list(window(tokens))\n        grams = [' '.join(g) for g in grams]\n        grams = grams + tokens\n        return grams\n\n\ndef window(words_seq, n=2):\n    \"Returns a sliding window (of width n) over data from the iterable\"\n    \"   s -> (s0,s1,...s[n-1]), (s1,s2,...,sn), ...                   \"\n    it = iter(words_seq)\n    result = tuple(islice(it, n))\n    if len(result) == n:\n        yield result\n    for elem in it:\n        result = result[1:] + (elem,)\n        yield result\ndef document_features(document, corpus_features):\n    document_words = set(document)\n    features = {}\n    for word in corpus_features:\n        features['contains({})'.format(word)] = (word in document_words)\n    return features\n\ndef process_text1(text, grams=False):\n    clean_text = remove_diacritics(text)\n    clean_text = remove_repeating_char(clean_text)\n    \n    if grams is False:\n        return nltk.tokenize.sent_tokenize(clean_text)\n    else:\n        tokens = nltk.tokenize.sent_tokenize(clean_text)\n        grams = list(window(tokens))\n        grams = [' '.join(g) for g in grams]\n        grams = grams + tokens\n        return grams\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nsentence_list = list() \nall_features = list()\ntexts = list()\ndata_labels = list()\n#import pyarabic.arabrepr\n#arepr = pyarabic.arabrepr.ArabicRepr()\n#repr = arepr.repr\n#from tashaphyne.stemming import ArabicLightStemmer\n#ArListem = ArabicLightStemmer()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nnegative_file = open(\"../input/ts-naim-mhedhbi/negative_tweets.txt\", encoding =\"utf8\")\npositive_file = open(\"../input/ts-naim-mhedhbi/positive_tweets.txt\", encoding =\"utf8\")\nnegative_file1 = open(\"../input/ts-naim-mhedhbi/negative_tweets.txt\", encoding =\"utf8\")\npositive_file1 = open(\"../input/ts-naim-mhedhbi/positive_tweets.txt\", encoding =\"utf8\")\n\n\n\nn_grams_flag = False\nmin_freq = 13\n\nprint('read data ...')\nprint('read data ...')\n# read positive data\n ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for line in positive_file:\n    \n    text_features = process_text(line, grams=n_grams_flag)\n    stop_words = set(stopwords.words('arabic'))\n    text_features = [w for w in text_features if not w in stop_words]\n    all_features += text_features\n    texts.append(text_features)\n    data_labels.append('pos')   \n\n\nfor line in negative_file:\n    \n    text_features = process_text(line, grams=n_grams_flag)\n    stop_words = set(stopwords.words('arabic'))\n    text_features = [w for w in text_features if not w in stop_words]\n    all_features += text_features\n    texts.append(text_features)\n    data_labels.append('neg')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for line in positive_file1:\n    \n    text_features = process_text1(line, grams=n_grams_flag)\n    stop_words = set(stopwords.words('arabic'))\n    text_features = [w for w in text_features if not w in stop_words]\n    sentence_list += text_features\n    texts.append(text_features)\n    data_labels.append('pos')   \n\n\nfor line in negative_file1:\n    \n    text_features = process_text1(line, grams=n_grams_flag)\n    stop_words = set(stopwords.words('arabic'))\n    text_features = [w for w in text_features if not w in stop_words]\n    sentence_list += text_features\n    texts.append(text_features)\n    data_labels.append('neg')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('data size', len(data_labels))\nprint('# of positive', data_labels.count('pos'))\nprint('# of negative', data_labels.count('neg'))  \n  \ntweets = [(t, l) for t, l in zip(texts, data_labels)]\n\nrandom.shuffle(tweets)\nprint('sample tweets')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for t in tweets[:10]: print(t)  # see the first 10 instances\nprint('all words sample')\nprint(all_features[:20])\nprint('len(all_words):', len(all_features))\nall_features_freq = nltk.FreqDist(w for w in all_features)\nprint(all_features_freq)\nprint('sample frequencies')\nprint(all_features_freq.most_common(20))\nprint('freq of في', all_features_freq.freq('في'))\nprint('features frequencies are computed')\nthr = min_freq / len(all_features)\nprint('selecting features')\n\n#Finally, to find the weighted frequency, we can simply divide the number of occurances of all the words by the frequency of the most occurring word, as shown below:\nmaximum_frequncy = max(all_features_freq.values())\nfor word in all_features_freq.keys():\n    all_features_freq[word] = (all_features_freq[word]/maximum_frequncy) \n\n    \nsentence_scores = {}\nfor sent in sentence_list:\n    for word in nltk.word_tokenize(sent.lower()):\n        if word in all_features_freq.keys():\n            if len(sent.split(' ')) < 30:\n                if sent not in sentence_scores.keys():\n                    sentence_scores[sent] = all_features_freq[word]\n                else:\n                    sentence_scores[sent] += all_features_freq[word]\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import heapq\nsummary_sentences = heapq.nlargest(7, sentence_scores, key=sentence_scores.get)\n\nsummary = ' '.join(summary_sentences)\nprint(summary)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}