{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Práctica 2: Aprendizaje y selección de modelos de clasificación*\n\n### Minería de Datos: Curso académico 2020-2021\n\n### Profesorado:\n\n* Juan Carlos Alfaro Jiménez\n* José Antonio Gámez Martín\n\n### Alumnos:\n\n* Pablo Moreira Garcia\n* Ruben Martinez Sotoca\n\nAdaptado de las prácticas de Jacinto Arias Martínez y Enrique González Rodrigo","metadata":{"papermill":{"duration":0.06623,"end_time":"2020-11-23T16:24:22.347851","exception":false,"start_time":"2020-11-23T16:24:22.281621","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"En esta práctica estudiaremos los modelos más utilizados en `scikit-learn` para conocer los distintos hiperparámetros que los configuran y estudiar los clasificadores resultantes. Además, veremos métodos de selección de modelos orientados a obtener una configuración óptima de hiperparámetros.","metadata":{"papermill":{"duration":0.062556,"end_time":"2020-11-23T16:24:22.475065","exception":false,"start_time":"2020-11-23T16:24:22.412509","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# 1. Preliminares","metadata":{"papermill":{"duration":0.062397,"end_time":"2020-11-23T16:24:22.600511","exception":false,"start_time":"2020-11-23T16:24:22.538114","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"Antes de comenzar cargamos las librerías para que estén disponibles posteriormente:","metadata":{"papermill":{"duration":0.065153,"end_time":"2020-11-23T16:24:22.72861","exception":false,"start_time":"2020-11-23T16:24:22.663457","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Third party\nfrom sklearn.base import clone\nfrom sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import HistGradientBoostingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import KBinsDiscretizer\nfrom sklearn.metrics import recall_score, roc_auc_score\nfrom sklearn.metrics import make_scorer\n\n# Local application\nimport utilidades_practica_2_ordinaria as utils","metadata":{"papermill":{"duration":1.542704,"end_time":"2020-11-23T16:24:24.335163","exception":false,"start_time":"2020-11-23T16:24:22.792459","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-10T13:36:59.288332Z","iopub.execute_input":"2021-06-10T13:36:59.288863Z","iopub.status.idle":"2021-06-10T13:37:00.680682Z","shell.execute_reply.started":"2021-06-10T13:36:59.288805Z","shell.execute_reply":"2021-06-10T13:37:00.679603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Además, fijamos una semilla para que los experimentos sean reproducibles:","metadata":{"papermill":{"duration":0.063159,"end_time":"2020-11-23T16:24:24.462541","exception":false,"start_time":"2020-11-23T16:24:24.399382","status":"completed"},"tags":[]}},{"cell_type":"code","source":"random_state = 27912","metadata":{"papermill":{"duration":0.071393,"end_time":"2020-11-23T16:24:24.597448","exception":false,"start_time":"2020-11-23T16:24:24.526055","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-10T13:37:00.683452Z","iopub.execute_input":"2021-06-10T13:37:00.68378Z","iopub.status.idle":"2021-06-10T13:37:00.687991Z","shell.execute_reply.started":"2021-06-10T13:37:00.68375Z","shell.execute_reply":"2021-06-10T13:37:00.686937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Carga de datos","metadata":{"papermill":{"duration":0.06276,"end_time":"2020-11-23T16:24:24.724012","exception":false,"start_time":"2020-11-23T16:24:24.661252","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"Vamos a utilizar, como en la practica 1, los conjuntos de datos `Breast Cancer Wisconsin` y `Pima Indians Diabetes`","metadata":{}},{"cell_type":"code","source":"filepath = \"../input/breast-cancer-wisconsin-data/data.csv\"\n\nindex_col = \"id\"\ntarget = \"diagnosis\"\n\ndata_Cancer = utils.load_data(filepath, index_col, target)","metadata":{"papermill":{"duration":0.351212,"end_time":"2020-11-23T16:24:25.29299","exception":false,"start_time":"2020-11-23T16:24:24.941778","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-10T13:37:00.689226Z","iopub.execute_input":"2021-06-10T13:37:00.689532Z","iopub.status.idle":"2021-06-10T13:37:00.737002Z","shell.execute_reply.started":"2021-06-10T13:37:00.689503Z","shell.execute_reply":"2021-06-10T13:37:00.735933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"filepath = \"../input/pima-indians-diabetes-database/diabetes.csv\"\n\nindex_col = None\ntarget = \"Outcome\"\n\ndata_Diabetes = utils.load_data(filepath, index_col, target)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T13:37:00.738558Z","iopub.execute_input":"2021-06-10T13:37:00.73887Z","iopub.status.idle":"2021-06-10T13:37:00.754803Z","shell.execute_reply.started":"2021-06-10T13:37:00.738841Z","shell.execute_reply":"2021-06-10T13:37:00.753939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Comprobando que se ha cargado correctamente:","metadata":{"papermill":{"duration":0.063289,"end_time":"2020-11-23T16:24:25.419999","exception":false,"start_time":"2020-11-23T16:24:25.35671","status":"completed"},"tags":[]}},{"cell_type":"code","source":"data_Cancer.sample(5, random_state=random_state)","metadata":{"papermill":{"duration":0.092546,"end_time":"2020-11-23T16:24:25.576003","exception":false,"start_time":"2020-11-23T16:24:25.483457","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-10T13:37:00.756973Z","iopub.execute_input":"2021-06-10T13:37:00.757448Z","iopub.status.idle":"2021-06-10T13:37:00.796235Z","shell.execute_reply.started":"2021-06-10T13:37:00.757404Z","shell.execute_reply":"2021-06-10T13:37:00.795228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_Diabetes.sample(5, random_state=random_state)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T13:37:00.797403Z","iopub.execute_input":"2021-06-10T13:37:00.797692Z","iopub.status.idle":"2021-06-10T13:37:00.815898Z","shell.execute_reply.started":"2021-06-10T13:37:00.797658Z","shell.execute_reply":"2021-06-10T13:37:00.81457Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A su vez, lo dividimos en variables predictoras (`X`) y variable clase (`y`):","metadata":{"papermill":{"duration":0.074323,"end_time":"2020-11-23T16:24:25.71586","exception":false,"start_time":"2020-11-23T16:24:25.641537","status":"completed"},"tags":[]}},{"cell_type":"code","source":"target = \"diagnosis\"\n\n(X_Cancer, y_Cancer) = utils.divide_dataset(data_Cancer, target)","metadata":{"papermill":{"duration":0.097026,"end_time":"2020-11-23T16:24:25.879785","exception":false,"start_time":"2020-11-23T16:24:25.782759","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-10T13:37:00.822728Z","iopub.execute_input":"2021-06-10T13:37:00.823166Z","iopub.status.idle":"2021-06-10T13:37:00.830271Z","shell.execute_reply.started":"2021-06-10T13:37:00.82313Z","shell.execute_reply":"2021-06-10T13:37:00.829381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target = \"Outcome\"\n\n(X_Diabetes, y_Diabetes) = utils.divide_dataset(data_Diabetes, target)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T13:37:00.835162Z","iopub.execute_input":"2021-06-10T13:37:00.835538Z","iopub.status.idle":"2021-06-10T13:37:00.844653Z","shell.execute_reply.started":"2021-06-10T13:37:00.835505Z","shell.execute_reply":"2021-06-10T13:37:00.843542Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Vamos a comprobar que se ha separado correctamente. Comenzamos con las variables predictoras:","metadata":{"papermill":{"duration":0.066928,"end_time":"2020-11-23T16:24:26.016047","exception":false,"start_time":"2020-11-23T16:24:25.949119","status":"completed"},"tags":[]}},{"cell_type":"code","source":"X_Cancer.sample(5, random_state=random_state)","metadata":{"papermill":{"duration":0.087896,"end_time":"2020-11-23T16:24:26.173434","exception":false,"start_time":"2020-11-23T16:24:26.085538","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-10T13:37:00.846104Z","iopub.execute_input":"2021-06-10T13:37:00.846413Z","iopub.status.idle":"2021-06-10T13:37:00.882763Z","shell.execute_reply.started":"2021-06-10T13:37:00.846384Z","shell.execute_reply":"2021-06-10T13:37:00.881546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_Diabetes.sample(5, random_state=random_state)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T13:37:00.884464Z","iopub.execute_input":"2021-06-10T13:37:00.884897Z","iopub.status.idle":"2021-06-10T13:37:00.90278Z","shell.execute_reply.started":"2021-06-10T13:37:00.88485Z","shell.execute_reply":"2021-06-10T13:37:00.901665Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Y continuamos con la variable clase:","metadata":{"papermill":{"duration":0.066391,"end_time":"2020-11-23T16:24:26.3061","exception":false,"start_time":"2020-11-23T16:24:26.239709","status":"completed"},"tags":[]}},{"cell_type":"code","source":"y_Cancer.sample(5, random_state=random_state)","metadata":{"papermill":{"duration":0.082366,"end_time":"2020-11-23T16:24:26.458255","exception":false,"start_time":"2020-11-23T16:24:26.375889","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-10T13:37:00.904048Z","iopub.execute_input":"2021-06-10T13:37:00.904475Z","iopub.status.idle":"2021-06-10T13:37:00.920173Z","shell.execute_reply.started":"2021-06-10T13:37:00.904432Z","shell.execute_reply":"2021-06-10T13:37:00.918914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_Diabetes.sample(5, random_state=random_state)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T13:37:00.921573Z","iopub.execute_input":"2021-06-10T13:37:00.921898Z","iopub.status.idle":"2021-06-10T13:37:00.931379Z","shell.execute_reply.started":"2021-06-10T13:37:00.921865Z","shell.execute_reply":"2021-06-10T13:37:00.93033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Por último, dividimos el conjunto de datos en entrenamiento y prueba mediante un *holdout* estratificado:","metadata":{"papermill":{"duration":0.065436,"end_time":"2020-11-23T16:24:26.590379","exception":false,"start_time":"2020-11-23T16:24:26.524943","status":"completed"},"tags":[]}},{"cell_type":"code","source":"stratify = y_Cancer\ntrain_size = 0.7\n\n(X_train_Cancer, X_test_Cancer, y_train_Cancer, y_test_Cancer) = train_test_split(X_Cancer, y_Cancer,\n                                                      stratify=stratify,\n                                                      train_size=train_size,\n                                                      random_state=random_state)","metadata":{"papermill":{"duration":0.081251,"end_time":"2020-11-23T16:24:26.738222","exception":false,"start_time":"2020-11-23T16:24:26.656971","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-10T13:37:00.932876Z","iopub.execute_input":"2021-06-10T13:37:00.933502Z","iopub.status.idle":"2021-06-10T13:37:00.948102Z","shell.execute_reply.started":"2021-06-10T13:37:00.933452Z","shell.execute_reply":"2021-06-10T13:37:00.947312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stratify = y_Diabetes\ntrain_size = 0.7\n\n(X_train_Diabetes, X_test_Diabetes, y_train_Diabetes, y_test_Diabetes) = train_test_split(X_Diabetes, y_Diabetes,\n                                                      stratify=stratify,\n                                                      train_size=train_size,\n                                                      random_state=random_state)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T13:37:00.949445Z","iopub.execute_input":"2021-06-10T13:37:00.95004Z","iopub.status.idle":"2021-06-10T13:37:00.963172Z","shell.execute_reply.started":"2021-06-10T13:37:00.949994Z","shell.execute_reply":"2021-06-10T13:37:00.962354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Y nos aseguramos que se ha realizado adecuadamente. Comenzamos con el conjunto de datos de entrenamiento:","metadata":{"papermill":{"duration":0.069879,"end_time":"2020-11-23T16:24:26.874445","exception":false,"start_time":"2020-11-23T16:24:26.804566","status":"completed"},"tags":[]}},{"cell_type":"code","source":"X_train_Cancer.sample(5, random_state=random_state)","metadata":{"papermill":{"duration":0.092462,"end_time":"2020-11-23T16:24:27.0344","exception":false,"start_time":"2020-11-23T16:24:26.941938","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-10T13:37:00.964446Z","iopub.execute_input":"2021-06-10T13:37:00.964941Z","iopub.status.idle":"2021-06-10T13:37:00.996007Z","shell.execute_reply.started":"2021-06-10T13:37:00.96488Z","shell.execute_reply":"2021-06-10T13:37:00.994976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train_Cancer.sample(5, random_state=random_state)","metadata":{"papermill":{"duration":0.08069,"end_time":"2020-11-23T16:24:27.183952","exception":false,"start_time":"2020-11-23T16:24:27.103262","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-10T13:37:00.997478Z","iopub.execute_input":"2021-06-10T13:37:00.99809Z","iopub.status.idle":"2021-06-10T13:37:01.012803Z","shell.execute_reply.started":"2021-06-10T13:37:00.998028Z","shell.execute_reply":"2021-06-10T13:37:01.011828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_Diabetes.sample(5, random_state=random_state)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T13:37:01.014314Z","iopub.execute_input":"2021-06-10T13:37:01.014724Z","iopub.status.idle":"2021-06-10T13:37:01.032392Z","shell.execute_reply.started":"2021-06-10T13:37:01.014681Z","shell.execute_reply":"2021-06-10T13:37:01.031413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train_Diabetes.sample(5, random_state=random_state)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T13:37:01.033779Z","iopub.execute_input":"2021-06-10T13:37:01.034113Z","iopub.status.idle":"2021-06-10T13:37:01.047305Z","shell.execute_reply.started":"2021-06-10T13:37:01.034052Z","shell.execute_reply":"2021-06-10T13:37:01.046543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Y finalizamos con el conjunto de datos de prueba:","metadata":{"papermill":{"duration":0.068603,"end_time":"2020-11-23T16:24:27.321973","exception":false,"start_time":"2020-11-23T16:24:27.25337","status":"completed"},"tags":[]}},{"cell_type":"code","source":"X_test_Cancer.sample(5, random_state=random_state)","metadata":{"papermill":{"duration":0.088782,"end_time":"2020-11-23T16:24:27.482332","exception":false,"start_time":"2020-11-23T16:24:27.39355","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-10T13:37:01.048304Z","iopub.execute_input":"2021-06-10T13:37:01.048682Z","iopub.status.idle":"2021-06-10T13:37:01.077813Z","shell.execute_reply.started":"2021-06-10T13:37:01.048653Z","shell.execute_reply":"2021-06-10T13:37:01.076675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test_Cancer.sample(5, random_state=random_state)","metadata":{"papermill":{"duration":0.084058,"end_time":"2020-11-23T16:24:27.636235","exception":false,"start_time":"2020-11-23T16:24:27.552177","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-10T13:37:01.078959Z","iopub.execute_input":"2021-06-10T13:37:01.079262Z","iopub.status.idle":"2021-06-10T13:37:01.08876Z","shell.execute_reply.started":"2021-06-10T13:37:01.079234Z","shell.execute_reply":"2021-06-10T13:37:01.087845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test_Diabetes.sample(5, random_state=random_state)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T13:37:01.089984Z","iopub.execute_input":"2021-06-10T13:37:01.090333Z","iopub.status.idle":"2021-06-10T13:37:01.110598Z","shell.execute_reply.started":"2021-06-10T13:37:01.090301Z","shell.execute_reply":"2021-06-10T13:37:01.109892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test_Diabetes.sample(5, random_state=random_state)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T13:37:01.111804Z","iopub.execute_input":"2021-06-10T13:37:01.112334Z","iopub.status.idle":"2021-06-10T13:37:01.124845Z","shell.execute_reply.started":"2021-06-10T13:37:01.1123Z","shell.execute_reply":"2021-06-10T13:37:01.123865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Evaluación de modelos","metadata":{"papermill":{"duration":0.074955,"end_time":"2020-11-23T16:24:34.577739","exception":false,"start_time":"2020-11-23T16:24:34.502784","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"Para la evaluación de los modelos vamos a utilizar validación cruzada, lo cual nos garantiza que los resultados no van a estar sesgados. Con esto lo que hacemos es dividir el conjunto de datos en `n_splits` particiones, de las cuales se elige una como test y las demas como train. Este proceso se repite `n_repeats` cada vezz con conjuntos de entrenamiento y de test distintos.","metadata":{}},{"cell_type":"code","source":"n_splits = 10\nn_repeats = 5\n\ncv = RepeatedStratifiedKFold(n_splits=n_splits,\n                             n_repeats=n_repeats,\n                             random_state=random_state)","metadata":{"papermill":{"duration":0.094853,"end_time":"2020-11-23T16:24:34.899978","exception":false,"start_time":"2020-11-23T16:24:34.805125","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-10T13:37:01.126336Z","iopub.execute_input":"2021-06-10T13:37:01.126767Z","iopub.status.idle":"2021-06-10T13:37:01.138124Z","shell.execute_reply.started":"2021-06-10T13:37:01.126733Z","shell.execute_reply":"2021-06-10T13:37:01.13729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Antes de comenzar a utilizar los modelos, es necesario que almacenemos en el conjunto de entrenamiento de nuestras bases de datos en la variable que vamos a utilizar. De lo contrario, usaríamos todo el conjunto de datos y las pruebas sobre el conjunto test perderían su eficacia.","metadata":{}},{"cell_type":"code","source":"X_Cancer = X_train_Cancer\ny_Cancer = y_train_Cancer","metadata":{"execution":{"iopub.status.busy":"2021-06-10T13:37:01.139782Z","iopub.execute_input":"2021-06-10T13:37:01.140313Z","iopub.status.idle":"2021-06-10T13:37:01.149464Z","shell.execute_reply.started":"2021-06-10T13:37:01.140267Z","shell.execute_reply":"2021-06-10T13:37:01.148705Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Gracias a lo estudiado en la práctica anterior, vamos a generar los transformadores con los que crearemos los pipelines con los que trabajaremos más adelante.","metadata":{}},{"cell_type":"code","source":"del_columns_wisconsin = utils.QuitarColumnasTransformer(['perimeter_mean', 'area_mean', 'compactness_mean', 'concave points_mean','radius_se','texture_se',\n                                    'perimeter_se', 'area_se', 'smoothness_se', 'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se',\n                                    'fractal_dimension_se', 'radius_worst', 'smoothness_worst', 'symmetry_worst', 'texture_worst', 'perimeter_worst', \n                                    'area_worst', 'compactness_worst', 'concavity_worst', 'concave points_worst', 'fractal_dimension_worst', 'Unnamed: 32'])\n\nnan_anomalos_wisconsin=utils.AnomalosANanTransformer({'radius_mean':[6.981, 22.27], 'texture_mean':[10.38, 29.97],'smoothness_mean':[0.06251, 0.1326],\n                                     'symmetry_mean':[0.1203, 0.2459], 'fractal_dimension_mean':[0.04996, 0.0795],'concavity_mean':[0, 0.2871] })\n\nimp = SimpleImputer(missing_values=float('nan'), strategy='mean')\n\ndiscretizer_wisconsin = KBinsDiscretizer(n_bins=5, strategy=\"uniform\")","metadata":{"execution":{"iopub.status.busy":"2021-06-10T13:37:01.150487Z","iopub.execute_input":"2021-06-10T13:37:01.150885Z","iopub.status.idle":"2021-06-10T13:37:01.161836Z","shell.execute_reply.started":"2021-06-10T13:37:01.150855Z","shell.execute_reply":"2021-06-10T13:37:01.160612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Basicamente como explicamos en la practica anterior, borramos las variables que no nos son necesarias como todas las `x_se` o `x_worst`, las que tienen alta correlacion como `perimeter_mean`, `area_mean`... y las que tienen un alto numero de valores anomalos o ruidosos como puede ser la variable `Unnamed: 32`\n\nLos valores anomalos que tengan las distintas variables, que si que utilizamos, los cambiamos a nan gracias al segundo transformador. Y luego creamos un imputador que cambia los valores Nan por la media de esa variable.\n\nPor ultimo utilizamos un discretizador de 5 conjuntos ya que como explicamos en la practica anterior al representar graficamente estas variables en una nube de putos y diferenciar por colores vamos a ver en algunas variables que hay segmentos bien diferenciados con solo un tipo de clase.","metadata":{}},{"cell_type":"markdown","source":"Y lo mismo para la base de datos de diabetes.","metadata":{}},{"cell_type":"code","source":"X_Diabetes = X_train_Diabetes\ny_Diabetes = y_train_Diabetes","metadata":{"execution":{"iopub.status.busy":"2021-06-10T13:37:01.163397Z","iopub.execute_input":"2021-06-10T13:37:01.163713Z","iopub.status.idle":"2021-06-10T13:37:01.179909Z","shell.execute_reply.started":"2021-06-10T13:37:01.163682Z","shell.execute_reply":"2021-06-10T13:37:01.178905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del_columns_pima = utils.QuitarColumnasTransformer(['SkinThickness', 'Insulin'])\n\nnan_anomalos_pima=utils.AnomalosANanTransformer({'Glucose':[56,199], 'BloodPressure':[38,102], 'Pregnancies':[0,13],\n                                      'BMI':[18.2,50], 'DiabetesPedigreeFunction':[0.078,1.182], 'Age':[21,64]})\n\nimp = SimpleImputer(missing_values=float('nan'), strategy='mean')\n\ndiscretizer_pima = KBinsDiscretizer(n_bins=5, strategy=\"kmeans\")","metadata":{"execution":{"iopub.status.busy":"2021-06-10T13:37:01.181455Z","iopub.execute_input":"2021-06-10T13:37:01.181769Z","iopub.status.idle":"2021-06-10T13:37:01.191262Z","shell.execute_reply.started":"2021-06-10T13:37:01.181738Z","shell.execute_reply":"2021-06-10T13:37:01.19034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Funcion utilizada para normalizar nuestra base de datos para el algoritmo de los vecinos más cercanos.","metadata":{}},{"cell_type":"code","source":"def normalize(dataset):\n    dataNorm=((dataset)/(dataset.max()))\n    return dataNorm","metadata":{"execution":{"iopub.status.busy":"2021-06-10T13:37:01.192703Z","iopub.execute_input":"2021-06-10T13:37:01.193008Z","iopub.status.idle":"2021-06-10T13:37:01.201113Z","shell.execute_reply.started":"2021-06-10T13:37:01.192977Z","shell.execute_reply":"2021-06-10T13:37:01.200219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Selección de modelos","metadata":{"papermill":{"duration":0.077878,"end_time":"2020-11-23T16:24:39.21174","exception":false,"start_time":"2020-11-23T16:24:39.133862","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"En este apartado vamos a poder elegir los mejores hiperparametros gracias a un algoritmo de seleccion de modelos por fuerza bruta `GridSearch` al cual le pasamos un conjunto de hiperparametros (que nosotros seleccionamos) y este hace una evaluacion mediante validacion cruzada de todas las posibles combinaciones entre los hiperparametros para poder seleccionar los mejores vaalores.\n\nUna vez obtenidos los mejores valores, evaluaremos los clasificadores para comprobar si la estimación es buena.","metadata":{}},{"cell_type":"markdown","source":"## Selección para Wisconsin","metadata":{}},{"cell_type":"markdown","source":"Con scoring estamos eligiendo las metricas que utilizaremos, en este caso vamos a utilizar `recall_score` y `roc_auc`.","metadata":{}},{"cell_type":"code","source":"scoring = {'AUC':'roc_auc', 'Recall':make_scorer(recall_score, pos_label=\"M\")}","metadata":{"execution":{"iopub.status.busy":"2021-06-10T13:37:01.202477Z","iopub.execute_input":"2021-06-10T13:37:01.20286Z","iopub.status.idle":"2021-06-10T13:37:01.212676Z","shell.execute_reply.started":"2021-06-10T13:37:01.202827Z","shell.execute_reply":"2021-06-10T13:37:01.211724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Vamos a usar la función `utils.optimize_params` para encontrar los mejores hiperparámetros y asi optimizar los distintos algoritmos y al mismo tiempo mostrar los resultados de la validación cruzada para poder ver los distintos resultados que vamos a ir obteniendo.","metadata":{}},{"cell_type":"markdown","source":"### KNeighborsClassifier","metadata":{}},{"cell_type":"markdown","source":"Es un método que simplemente busca en las observaciones más cercanas a la que se está tratando de predecir y clasifica el punto de interés basado en la mayoría de datos que le rodean.","metadata":{}},{"cell_type":"code","source":"estimator = KNeighborsClassifier()\npipeline = Pipeline(steps=[('del_columns_wisconsin',del_columns_wisconsin),('nan_anomalos_wisconsin', nan_anomalos_wisconsin),('imp', imp),\n                           ('discretizer_wisconsin', discretizer_wisconsin),('knn', estimator)])\n\nX_Cancer_Norm = normalize(X_Cancer)\n\nweights = [\"uniform\", \"distance\"]\n\nn_neighbors = [3,5,7,9,11,13,15,17,19]\n\n#algorithm = [\"auto\", \"ball_tree\", \"kd_tree\", \"brute\"]\n\nmetric = [\"euclidean\", \"manhattan\"]\n#NO UTILIZAMOS MINKOWSKI PORQUE LA MEDIDA MINKOWSKI ES UNA GENERALIZACION DE LA DISTANCIA EUCLIDEA Y MANHATTAN AÑADIENDO UN PARAMETRO P\n\n#p =[1, 2, 3, 4, 5, 6, 7]\n\n#leaf_size = [30, 40, 50, 60, 70, 80, 90, 100]\n\n#n_jobs = default -1\n\nk_neighbors_clf = utils.optimize_params(pipeline,X_Cancer_Norm, y_Cancer, cv,\n                                        scoring=scoring,\n                                        knn__n_neighbors=n_neighbors,\n                                        knn__weights=weights,\n                                        knn__metric=metric)","metadata":{"papermill":{"duration":4.727678,"end_time":"2020-11-23T16:24:44.17359","exception":false,"start_time":"2020-11-23T16:24:39.445912","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-10T13:37:01.214091Z","iopub.execute_input":"2021-06-10T13:37:01.214746Z","iopub.status.idle":"2021-06-10T13:39:59.778775Z","shell.execute_reply.started":"2021-06-10T13:37:01.214712Z","shell.execute_reply":"2021-06-10T13:39:59.77791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Algunos de los parametros que podriamos utilizar son:\n\n* `weights`: La forma de calcular los pesos para las predicciones, existen 2 posibilidades.:\n   - uniform : Peso uniforme, donde todos los puntos de cada grupo pesan lo mismo.\n\n   - distance :El peso equivale a la inversa de la distancia entre los puntos, va a valer mas si los puntos están muy cerca que si están alejados.\n\n   - [callable] : Tambien esta la posibilidad donde acepta un array de distancias.\n* `n_neighbors`: Numero de grupos (neighbors) a utilizar, si no se especifica sería 5.\n* `algorithm`: Algoritmos para procesar los vecinos más cercanos:\n\n   - ‘ball_tree’ utilizará BallTree\n    \n   - ‘kd_tree’ utilizará KDTree\n\n   - ‘brute’ utilizará busqueda con fuerza bruta.\n\n   - ‘auto’ elegira de forma automatica el algoritmo más adecuado segun los valores que se le pasen al fit.\n   \n* `metric`: La medida de distancia que se va a utilizar, si no se especifica, se utiliza `Minkowski` donde utilizamos la variable `p`, si la `p` es 2 sería equivalente a utilizar `Euclidean`, y si fuera 1, sería equivalente a utilizar `Manhattan`.\n\n* `leaf_size`: Tamaño de las hojas utilizado en los algooritmos `ball_tree` o `kd_tree`.\n\n* `n_jobs`: El número de busquedas paralelas que va a realizar el ordenador, cuanto más alto mejor va a ser el resultado pero más costoso.","metadata":{}},{"cell_type":"markdown","source":"En este caso hemos utilizado unicamente `metric`, `n_neighbors` y `weighs`, donde los mejores hiperparametros han sido `euclidean`, `3` y `uniform` respectivamente, donde vemos que los resultados en cuanto a score no cambian, cuando se prueba con `euclidean` o `manhattan`, o con `uniform` o `distance`, pero lo que si que vemos que cambia son los tiempos.","metadata":{}},{"cell_type":"markdown","source":"### DecisionTreeClassifier","metadata":{}},{"cell_type":"markdown","source":"Los `árboles de decisión` son una técnica de aprendizaje automático supervisado muy utilizada. Como su nombre indica, esta técnica toma una serie de decisiones en forma de árbol. Los nodos finales (las hojas) nos dan la predicción que vamos buscando.","metadata":{"papermill":{"duration":0.081944,"end_time":"2020-11-23T16:24:44.49744","exception":false,"start_time":"2020-11-23T16:24:44.415496","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Should not modify the original model\nestimator = DecisionTreeClassifier(random_state=random_state)\npipeline = Pipeline(steps=[('del_columns_wisconsin',del_columns_wisconsin),('nan_anomalos_wisconsin', nan_anomalos_wisconsin),('imp', imp),\n                           ('discretizer_wisconsin', discretizer_wisconsin),('DecisionTree', estimator)])\n\ncriterion = [\"gini\", \"entropy\"]\nmax_depth = [4, 5, 6, 7, 8]\n\nccp_alpha = [0.0, 0.1, 0.2, 0.3]\n#class_weight = [\"balanced\", None]\n#max_features = [int, float, \"auto\", \"sqrt\", \"log2\", None]\n#max_leaf_nodes = [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, None]\n#min_impurity_decrease = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5]\n#min_samples_split = [10, 15, 20, 30, 40]\n#min_samples_leaf = [5,6,7,8,9,10,11,12,13,14,15]\n#min_weight_fraction_leaf = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5]\n\nsplitter = [\"best\",\"random\"]\n\n\n\ndecision_tree_clf = utils.optimize_params(pipeline,X_Cancer, y_Cancer, cv,\n                                          scoring=scoring,\n                                          DecisionTree__criterion=criterion,\n                                          DecisionTree__max_depth=max_depth,\n                                          DecisionTree__ccp_alpha=ccp_alpha,\n                                          DecisionTree__splitter=splitter)","metadata":{"papermill":{"duration":14.390014,"end_time":"2020-11-23T16:24:58.968284","exception":false,"start_time":"2020-11-23T16:24:44.57827","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-10T13:39:59.780548Z","iopub.execute_input":"2021-06-10T13:39:59.781136Z","iopub.status.idle":"2021-06-10T13:48:34.074Z","shell.execute_reply.started":"2021-06-10T13:39:59.781098Z","shell.execute_reply":"2021-06-10T13:48:34.073005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Algunos de los parametros que podriamos utilizar son:\n\n* `criterion`: tipo de cadena, opcional (el valor predeterminado es \"gini\") Mide la calidad de la clasificación. Los estándares admitidos son \"gini\" para la impureza y \"entropy\" para la ganancia de información.\n\n* `splitter`: {“best”, “random”}, default=”best”. Una estrategia utilizada para seleccionar categorías en nodos. Las estrategias admitidas son \"best\", seleccione la mejor categoría y \"random\" seleccione la mejor categoría aleatoria.\n\n* `max_depth`: int, default=None. Profundidad máxima que puede alcanzar el árbol. Si no se especifica el arbol se expanderá hasta que todas las hojas sean de una única clase o mientras cada hoja contenga menos ejemplos que el parametro `min_samples_leaf` \n\n* `min_samples_split`: int or float, default=2. Número mínimo de observaciones que debe de tener un nodo para que pueda dividirse. Si es un valor decimal se interpreta como fracción del total de observaciones de entrenamiento.\n\n* `min_samples_leaf`: int or float, default=1. número mínimo de muestras que debe haber en un nodo final (hoja). También se puede expresar en porcentaje.\n\n* `min_weight_fraction_leaf`: float, opcional (el valor predeterminado es 0) La puntuación mínima ponderada requerida por la muestra de entrada de un nodo hoja.La puntuación mínima ponderada requerida por la muestra de entrada de un nodo hoja\n\n* `max_features`: int, float, string o None opcional (el valor predeterminado es None) La cantidad de características que deben tenerse en cuenta al clasificar.\n    1. Si es un int, las características de max_features deben ser consideradas en cada clasificación.\n    \n    2. Si es un flotante, entonces max_features es un porcentaje y la cantidad de características que se deben considerar durante la clasificación es int (max_features * n_features, donde n_features es la cantidad de características enviadas cuando se completa el entrenamiento).\n    \n    3. Si es automático, max_features = sqrt (n_features)\n    \n    4. Si es sqrt, max_features = sqrt (n_features)\n    \n    5. Si es log2, max_features = log2 (n_features)\n    \n    6. Si es None, max_features = n_features\n\n    Nota: Cuando se encuentre al menos un punto de muestra clasificado, la búsqueda y clasificación se detendrá.\n\n* `random_stateint`: Semilla para que los resultados sean reproducibles. Tiene que ser un valor entero.\n\n* `max_leaf_nodes`: int, default=None Número máximo de nodos terminales.\n\n* `min_impurity_decrease`: float, default=0.0 Un nodo se dividirá si esta division produce un decremento de la impureza igual o mayor a este valor.\n\n* `min_impurity_split`:float, default=0 Limite para parar el crecimiento del arbol. Si un nodo esta por encima del limite se divide, si no, sería una hoja.\n\n* `ccp_alphanon-negative`: float, default=0.0 Determina el grado de penalización por complejidad. Cuanto mayor es este valor, más agresivo el podado y menor el tamaño del árbol resultante.","metadata":{}},{"cell_type":"markdown","source":"En este caso hemos decidido utilizar `cpp_alpha`, `criterion` , `max_depth` y `splitter` y donde los hiperparametros utilizados son `0.0`, `entropy`, `8` y `random` respectivamente. Con cpp_alpha = 0.0 podemos ver que el podado va a ser minimo permitiendo que el arbol se expanda, con una profundidad maxima de 8. Y vemos que la categoria de los nodos se elige aleatoriamente.","metadata":{}},{"cell_type":"markdown","source":"### AdaBoostClassifier","metadata":{}},{"cell_type":"markdown","source":"A diferencia de muchos modelos ML que se enfocan en un solo modelo para completar predicciones de alta calidad, el algoritmo `Boosting` intenta mejorar las capacidades de predicción entrenando una serie de modelos débiles, cada uno de los cuales puede compensar las debilidades de sus predecesores.\n\n`AdaBoost` es un algoritmo de `Boosting` específico (también conocido como `AdaBoost discreto`) desarrollado para problemas de clasificación. La debilidad está determinada por la tasa de error del estimador débil.","metadata":{}},{"cell_type":"code","source":"estimator = AdaBoostClassifier(random_state=random_state)\npipeline = Pipeline(steps=[('del_columns_wisconsin',del_columns_wisconsin),('nan_anomalos_wisconsin', nan_anomalos_wisconsin),\n                           ('imp', imp),('discretizer_wisconsin', discretizer_wisconsin),('AdaBoost', estimator)])\n\n#algorithm = [\"SAMME\", \"SAMME.R\"]\nbase_estimator = [DecisionTreeClassifier(random_state=random_state)]\nlearning_rate = [0.1, 0.2,0.95, 1.0]\nn_estimators = [100, 120, 150, 200]\n\ncriterion = [\"gini\", \"entropy\"]\nmax_depth = [1, 2, 3]\n\n\n\nadaboost_clf = utils.optimize_params(pipeline, X_Cancer, y_Cancer, cv,\n                                     scoring=scoring,\n                                     AdaBoost__base_estimator=base_estimator,\n                                     AdaBoost__learning_rate=learning_rate,\n                                     AdaBoost__n_estimators=n_estimators,\n                                     AdaBoost__base_estimator__criterion=criterion,\n                                     AdaBoost__base_estimator__max_depth=max_depth)","metadata":{"papermill":{"duration":68.706754,"end_time":"2020-11-23T16:26:08.155059","exception":false,"start_time":"2020-11-23T16:24:59.448305","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-10T13:48:34.075747Z","iopub.execute_input":"2021-06-10T13:48:34.076108Z","iopub.status.idle":"2021-06-10T14:16:20.771558Z","shell.execute_reply.started":"2021-06-10T13:48:34.076056Z","shell.execute_reply":"2021-06-10T14:16:20.770546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Algunos de los parametros que podriamos utilizar son:\n\n* `base_estimator`: object, default=None El estimador base a partir del cual se construye el conjunto. Se requiere soporte para la ponderación de la muestra, así como atributos adecuados de classes_ y n_classes_ . Si es None, entonces el estimador base es DecisionTreeClassifier inicializado con max_depth = 1.\n\n* `n_estimators`: int, default=50 El número máximo de estimadores en los que finaliza el boosting. En caso de un ajuste perfecto, el procedimiento de aprendizaje se detiene antes de tiempo.\n\n* `learning_rate`: float, default=1. Peso aplicado a cada clasificador en cada iteración del boosting. Una tasa de aprendizaje más alta aumenta la contribución de cada clasificador. Existe una compensación entre los parámetros learning_rate y n_estimators.\n\n* `algorithm`: {‘SAMME’, ‘SAMME.R’}, default=’SAMME.R’ Si es 'SAMME.R', utiliza el algoritmo de real boosting de SAMME.R. base_estimator debe soportar el cálculo de probabilidades de clase. Si es 'SAMME', utiliza el algoritmo de discrete boosting de SAMME. El algoritmo SAMME.R generalmente converge más rápido que SAMME, logrando un error de prueba menor con menos iteraciones de impulso.\n\n* `random_state`: int, RandomState instance o None, default=None Controla la semilla aleatoria dada en cada base_estimator en cada iteración de boosting. Por lo tanto, solo se usa cuando base_estimator expone un random_state.\n\nA parte de estos tambien existirian los parametros del estimador base.","metadata":{}},{"cell_type":"markdown","source":"Nosotros unicamente vamos a utilizar los hiperparametros `n_estimators`, `learning_rate` y `base_estimator` como `base_estimator` utilizamos arboles de decisión con `max_depth = 1` y `criterion = entropy`","metadata":{}},{"cell_type":"markdown","source":"### BaggingClassifier","metadata":{}},{"cell_type":"markdown","source":"`BagginClassifier` es un metaestimador de conjunto que ajusta los clasificadores base cada uno en subconjuntos aleatorios del conjunto de datos original y luego agrega sus predicciones individuales (ya sea por votación o promediando) para formar una predicción final. \nUn metaestimador de este tipo se puede utilizar típicamente como una forma de reducir la varianza de un estimador de caja negra (por ejemplo, un árbol de decisión), al introducir la aleatorización en su procedimiento de construcción y luego hacer un conjunto a partir de él.","metadata":{}},{"cell_type":"code","source":"estimator = BaggingClassifier(random_state=random_state)\npipeline = Pipeline(steps=[('del_columns_wisconsin',del_columns_wisconsin),('nan_anomalos_wisconsin', nan_anomalos_wisconsin),\n                           ('imp', imp),('discretizer_wisconsin', discretizer_wisconsin),('Bagging', estimator)])\n\nbase_estimator = [DecisionTreeClassifier(random_state=random_state)]\nn_estimators = [ 10, 50, 75, 100 ]\nmax_samples = [ 0.2, 1.0]\nbootstrap = [ True, False ]\n\n\ncriterion = [\"gini\", \"entropy\"]\nmax_depth = [1, 2, 3]\n\n\nbagging_clf = utils.optimize_params(pipeline,X_Cancer, y_Cancer, cv,\n                                    scoring=scoring,\n                                    Bagging__n_estimators=n_estimators,\n                                    Bagging__max_samples=max_samples,\n                                    Bagging__bootstrap=bootstrap,\n                                    Bagging__base_estimator=base_estimator,\n                                    Bagging__base_estimator__criterion=criterion,\n                                    Bagging__base_estimator__max_depth=max_depth)","metadata":{"papermill":{"duration":1.934253,"end_time":"2020-11-23T16:26:10.571821","exception":false,"start_time":"2020-11-23T16:26:08.637568","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-10T14:16:20.773026Z","iopub.execute_input":"2021-06-10T14:16:20.773376Z","iopub.status.idle":"2021-06-10T14:34:45.72321Z","shell.execute_reply.started":"2021-06-10T14:16:20.773345Z","shell.execute_reply":"2021-06-10T14:34:45.722193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Algunos de los parametros que podriamos utilizar son:\n\n* `base_estimator`: object, default=None El estimador base para ajustarse a subconjuntos aleatorios del conjunto de datos. Si es None, entonces el estimador base es un DecisionTreeClassifier.\n\n* `n_estimators`: int, default=10 El número de estimadores de base en el conjunto.\n\n* `max_samples`: int or float, default=1.0 El número de muestras a extraer de X para entrenar a cada estimador base.\n\n    Si es int entonces extrae `max_samples` muestras.\n\n    Si es float entonces extrae `max_samples` * X.shape[0] muestras.\n\n* `max_features`: int or float, default=1.0 El número de características a extraer de X para entrenar cada estimador base.\n\n    Si es int entonces extrae `max_features` caracteristicas.\n\n    Si es float entonces extrae`max_features` * X.shape[1] caracteristicas.\n\n* `bootstrap`: bool, default=True. Las muestras se extraen con reemplazo. Si es Falso, se realiza un muestreo sin reemplazo.\n\n* `bootstrap_features`: bool, default=False Si los rasgos se dibujan con el reemplazo.\n\n* `oob_score`: bool, default=False Si utilizar muestras out-of-bag para estimar el error de generalización. Solo disponible si bootstrap=True.\n\n* `warm_start`: bool, default=False Cuando es True, reutiliza la solución de la llamada anterior para ajustar y agregar más estimadores al conjunto, de lo contrario, simplemente ajusta un conjunto completamente nuevo.\n\n* `n_jobs`: int, default=None El número de busquedas que se ejecutarán en paralelo tanto para ajustar como para predecir.\n\n* `random_state`: int, RandomState instance o None, default=None Controla el remuestreo aleatorio del conjunto de datos original (en cuanto a muestras y características). Si el estimador base acepta un atributo de `random_state`, se genera una semilla diferente para cada instancia en el conjunto. Pasar un int para una salida reproducible a través de múltiples llamadas a funciones.\n\n* `verbose`: int, default=0 Controla la verbosidad al ajustar y predecir.\n\nA parte de estos tambien se podrían ajustar los parametros del estimador base.","metadata":{}},{"cell_type":"markdown","source":"Nosotros utilizamos los hiperparametros `bootstrap` para extraer las muestras con reemplazo, `max_samples` y `n_estimators`, a parte de algunos hiperparametros para nuesto estimador base que en este caso es un arbol de decisión.","metadata":{}},{"cell_type":"markdown","source":"### RandomForestClassifier","metadata":{}},{"cell_type":"markdown","source":"Es un conjunto (ensemble) de `árboles de decisión` combinados con `bagging`. Al usar `bagging`, lo que en realidad está pasando, es que distintos árboles ven distintas porciones de los datos. Ningún árbol ve todos los datos de entrenamiento. Esto hace que cada árbol se entrene con distintas muestras de datos para un mismo problema. De esta forma, al combinar sus resultados, unos errores se compensan con otros y tenemos una predicción que generaliza mejor.","metadata":{}},{"cell_type":"code","source":"estimator = RandomForestClassifier(random_state=random_state)\npipeline = Pipeline(steps=[('del_columns_wisconsin',del_columns_wisconsin),('nan_anomalos_wisconsin', nan_anomalos_wisconsin),\n                           ('imp', imp),('discretizer_wisconsin', discretizer_wisconsin),('RandForest', estimator)])\n\ncriterion = [\"gini\", \"entropy\"]\nmax_features = [\"sqrt\", \"log2\"]\n#max_depth = [5,6,7,8,9]\n#class_weight = [\"balanced\", None]\nn_estimators = [50, 100, 150, 200]\nrandom_forest_clf = utils.optimize_params(pipeline,X_Cancer, y_Cancer, cv,\n                                          scoring=scoring,\n                                          RandForest__criterion=criterion,\n                                          RandForest__n_estimators=n_estimators,\n                                          RandForest__max_features=max_features)","metadata":{"papermill":{"duration":20.58856,"end_time":"2020-11-23T16:26:31.65819","exception":false,"start_time":"2020-11-23T16:26:11.06963","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-10T14:34:45.724915Z","iopub.execute_input":"2021-06-10T14:34:45.725388Z","iopub.status.idle":"2021-06-10T14:38:12.946308Z","shell.execute_reply.started":"2021-06-10T14:34:45.725341Z","shell.execute_reply":"2021-06-10T14:38:12.945298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Algunos de los parametros que podriamos utilizar son:\n\n* `n_estimators`: int, default=100 número de árboles que va a tener el bosque aleatorio. Normalmente cuantos más mejor, pero a partir de cierto punto deja de mejorar y sólo hace que vaya más lento. Un buen valor por defecto puede ser el uso de 100 árboles.\n\n* `criterion`: {“gini”, “entropy”}, default=”gini” Mide la calidad de la clasificación. Los estándares admitidos son \"gini\" para la impureza y \"entropy\" para la ganancia de información.\n\n* `max_depth`: int, default=None La profundidad máxima del árbol.\n\n* `min_samples_split`: int or float, default=2 número mínimo de muestras necesarias antes de dividir este nodo.\n\n* `min_samples_leaf`: int or float, default=1 número mínimo de muestras que debe haber en un nodo final (hoja). También se puede expresar en porcentaje.\n\n* `min_weight_fraction_leaf`: float, default=0.0 La puntuación mínima ponderada requerida por la muestra de entrada de un nodo hoja\n\n* `max_features`:{“auto”, “sqrt”, “log2”}, int or float, default=”auto” La cantidad de características que deben tenerse en cuenta al clasificar.\n    1. Si es un int, las características de max_features deben ser consideradas en cada clasificación.\n    \n    2. Si es un flotante, entonces max_features es un porcentaje y la cantidad de características que se deben considerar durante la clasificación es int (max_features * n_features, donde n_features es la cantidad de características enviadas cuando se completa el entrenamiento).\n    \n    3. Si es automático, max_features = sqrt (n_features)\n    \n    4. Si es sqrt, max_features = sqrt (n_features)\n    \n    5. Si es log2, max_features = log2 (n_features)\n    \n    6. Si es None, max_features = n_features\n\n    Nota: Cuando se encuentre al menos un punto de muestra clasificado, la búsqueda y clasificación se detendrá.\n\n* `max_leaf_nodes`: int, default=None número máximo de nodos finales\n\n* `min_impurity_decrease`: float, default=0.0 Un nodo se dividirá si esta division produce un decremento de la impureza igual o mayor a este valor.\n\n* `min_impurity_split`: float, default=None Limite para parar el crecimiento del arbol. Si un nodo esta por encima del limite se divide, si no, sería una hoja.\n\n* `bootstrap`: bool, default=True Si se utilizan muestras de bootstrap al construir árboles. Si es False, se usa todo el conjunto de datos para construir cada árbol.\n\n* `oob_score`: bool, default=False Si utilizar muestras out-of-bag para estimar el error de generalización. Solo disponible si bootstrap=True.\n\n* `n_jobs`: int, default=None Número de cores que se pueden usar para entrenar los árboles. Cada árbol es independiente del resto, así que entrenar un bosque aleatorio es una tarea muy paralelizable. Por defecto sólo utiliza 1 core de la CPU. Para mejorar el rendimiento puedes usar tantos cores como estimes necesario. Si usas n_jobs = -1, estás indicando que quieres usar tantos cores como tenga tu máquina.\n\n* `random_state`: int, RandomState instance or None, default=None Controla tanto la aleatoriedad del bootstrapping de las muestras utilizadas al construir árboles (si bootstrap = True) como el muestreo de las características a considerar cuando se busca la mejor división en cada nodo (si max_features <n_features).\n\n* `verbose`: int, default=0 Controla la verbosidad al ajustar y predecir.\n\n* `warm_start`: bool, default=False Cuando se establece en True, reutilice la solución de la llamada anterior para ajustar y agregar más estimadores al conjunto; de lo contrario, simplemente ajuste un bosque completamente nuevo.\n\n* `class_weight`: {“balanced”, “balanced_subsample”}, diccionario o lista de diccionarios, default=None Pesos asociados con clases en el formato {class_label: weight}. Si no se da, se supone que todas las clases tienen un peso uno. Para problemas de múltiples salidas, se puede proporcionar una lista de dictados en el mismo orden que las columnas de y.\n\n* `ccp_alpha`: non-negative float, default=0.0 Parámetro de complejidad utilizado para la poda Minimal Cost-Complexity. Se elegirá el subárbol con la mayor complejidad de costo siendo más pequeño que ccp_alpha. De forma predeterminada, no se realiza ninguna poda.\n    \n* `max_samples`: int or float, default=None Si bootstrap es True, El número de muestras a extraer de X para entrenar a cada estimador base.\n\n    Si es None entonces extrae X.shape[0] muestras.\n\n    Si es int entonces extrae `max_samples` muestras.\n\n    Si es float entonces extrae `max_samples` * X.shape[0] muestras. Por lo tanto, max_samples debe estar en el intervalo (0, 1).","metadata":{}},{"cell_type":"markdown","source":"Para `random_forest` utilizamos los hiperparametros `criterion`, `max_features` y `n_estimators` y `gini`, `sqrt` y `150` respectivamente son los mejores hiperparametros seleccionados.","metadata":{}},{"cell_type":"markdown","source":"### GradientBoostingClassifier","metadata":{}},{"cell_type":"markdown","source":"Algoritmo basado en la combinación de modelos predictivos débiles (weak learners) normalmente árboles de decisión para crear un modelo predictivo fuerte. La generación de los árboles de decisión débiles se realiza de forma secuencial, creándose cada árbol de forma que corrija los errores del árbol anterior. Los aprendices suelen ser árboles \"poco profundos\", de apenas uno, dos o tres niveles de profundidad","metadata":{}},{"cell_type":"code","source":"estimator = GradientBoostingClassifier(random_state=random_state)\npipeline = Pipeline(steps=[('del_columns_wisconsin',del_columns_wisconsin),('nan_anomalos_wisconsin', nan_anomalos_wisconsin),\n                           ('imp', imp),('discretizer_wisconsin', discretizer_wisconsin),('GradientBoosting', estimator)])\n\nlearning_rate = [0.01, 0.05, 0.1]\ncriterion = [\"friedman_mse\", \"mse\"]\nmax_depth = [1, 2, 3]\nsubsample = [0.2, 0.8, 1.0]\n#ccp_alpha = [0.0, 0.1]\nn_estimators = [50, 100, 200]\n\ngradient_boosting_clf = utils.optimize_params(pipeline,X_Cancer, y_Cancer, cv,\n                                              scoring=scoring,\n                                              GradientBoosting__learning_rate=learning_rate,\n                                              GradientBoosting__criterion=criterion,\n                                              GradientBoosting__max_depth=max_depth,\n                                              GradientBoosting__subsample=subsample,\n                                              GradientBoosting__n_estimators = n_estimators)","metadata":{"papermill":{"duration":170.181387,"end_time":"2020-11-23T16:29:22.336358","exception":false,"start_time":"2020-11-23T16:26:32.154971","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-10T14:38:12.948209Z","iopub.execute_input":"2021-06-10T14:38:12.94867Z","iopub.status.idle":"2021-06-10T15:01:11.871174Z","shell.execute_reply.started":"2021-06-10T14:38:12.948612Z","shell.execute_reply":"2021-06-10T15:01:11.870259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Algunos de los parametros que podriamos utillizar son:\n\n* `loss`: {‘deviance’, ‘exponential’}, default=’deviance’ La función `loss` a optimizar. \"deviance\" se refiere a la desviación (regresión logística) para la clasificación con resultados probabilísticos. \"exponencial\" gradientBoosting recupera el algoritmo AdaBoost.\n\n* `learning_rate`: float, default=0.1 Reduce la contribución de cada árbol multiplicando su influencia original por este valor.\n\n* `n_estimators`: int, default=100 El número de etapas de boosting a realizar. Gradient Boosting es bastante robusto ante el sobreajuste, por lo que un gran número generalmente da como resultado un mejor rendimiento.\n\n* `subsample`: float, default=1.0 La proporción de submuestras utilizadas en el entrenamiento de cada árbol de decisión con respecto al total de muestras, y la selección de submuestras es aleatoria. El uso de un valor ligeramente inferior a 1 puede hacer que el modelo sea más robusto porque reduce la varianza.\n\n* `criterion`: {‘friedman_mse’, ‘mse’, ‘mae’}, default=’friedman_mse’ La función para medir la calidad de una división. Los criterios admitidos son \"friedman_mse\" para el error cuadrático medio con puntuación de mejora de Friedman, \"mse\" para el error cuadrático medio y \"mae\" para el error absoluto medio.\n\n* `min_samples_split`: int or float, default=2 Número mínimo de observaciones que debe de tener un nodo para que pueda dividirse. Si es un valor decimal se interpreta como fracción del total de observaciones de entrenamiento.\n\n* `min_samples_leaf`: int or float, default=1 número mínimo de muestras que debe haber en un nodo final (hoja). También se puede expresar en porcentaje.\n\n* `min_weight_fraction_leaf`: float, default=0.0 La fracción ponderada mínima de la suma total de pesos (de todas las muestras de entrada) que se requiere para estar en un nodo hoja. Las muestras tienen el mismo peso cuando no se proporciona sample_weight.\n\n* `max_depth`: int, default=3 La profundidad máxima de los estimadores de regresión individuales.\n\n* `min_impurity_decrease`: float, default=0.0 Un nodo se dividirá si esta división induce una disminución de la impureza mayor o igual a este valor.\n\n* `min_impurity_split`: float, default=None Limite para parar el crecimiento del arbol. Si un nodo esta por encima del limite se divide, si no, sería una hoja.\n\n* `random_state`: int, RandomState instance or None, default=None Controla la semilla aleatoria dada a cada Tree Estimator en cada iteración de boosting. Además, controla la permutación aleatoria de las características en cada división. \n\n* `max_features`: {‘auto’, ‘sqrt’, ‘log2’}, int or float, default=None La cantidad de características a considerar al buscar la mejor división:\n\n    Si es int, entonces considere las características de max_features en cada división.\n\n    Si es flotante, max_features es una fracción y las características int (max_features * n_features) se consideran en cada división.\n\n    Si es \"auto\", entonces max_features = sqrt (n_features).\n\n    Si es \"sqrt\", entonces max_features = sqrt (n_features).\n\n    Si es \"log2\", entonces max_features = log2 (n_features).\n\n    Si es None, entonces max_features = n_features.\n\n    La elección de max_features <n_features conduce a una reducción de la varianza y un aumento del sesgo.\n\n    Nota: la búsqueda de una división no se detiene hasta que se encuentra al menos una partición válida de las muestras de nodo, incluso si requiere inspeccionar de manera efectiva más de las características de max_features.\n\n* `verbose`: int, default=0 Habilita la salida detallada. Si es 1, muestra el progreso y el rendimiento de vez en cuando (cuantos más árboles, menor es la frecuencia). Si es mayor que 1, imprime el progreso y el rendimiento de cada árbol.\n\n* `max_leaf_nodes`: int, default=None Los árboles crecen con max_leaf_nodes numero de hojas de la mejor manera. Los mejores nodos se definen como una reducción relativa de la impureza. Si es None, entonces un número ilimitado de nodos hoja.\n\n* `warm_start`: bool, default=False Cuando se establece en Verdadero, reutiliza la solución de la llamada anterior para ajustar y agregar más estimadores al conjunto; de lo contrario, simplemente borra la solución anterior. \n\n* `validation_fraction`: float, default=0.1 La proporción de datos de entrenamiento que se deben reservar como conjunto de validación para la detención anticipada. Debe estar entre 0 y 1.Solo se usa si n_iter_no_change se establece en un número entero.\n\n* `n_iter_no_change`: int, default=None n_iter_no_change se utiliza para decidir si la parada anticipada se utilizará para finalizar el entrenamiento cuando la puntuación de validación no mejora. De forma predeterminada, está configurado en Ninguno para deshabilitar la parada anticipada. Si se establece en un número, dejará de lado el tamaño de validation_fraction de los datos de entrenamiento como validación y finalizará el entrenamiento cuando la puntuación de validación no esté mejorando en todos los n_iter_no_change anteriores números de iteraciones.\n\n* `tol`: float, default=1e-4 Tolerancia a la parada anticipada. Cuando la pérdida no mejora en al menos `tol` para `n_iter_no_change` iteraciones (si se establece en un número), el entrenamiento se detiene.\n\n* `ccp_alpha`: non-negative float, default=0.0 Parámetro de complejidad utilizado para la poda Minimal Cost-Complexity. Se elegirá el subárbol con la mayor complejidad de costo siendo más pequeño que ccp_alpha. De forma predeterminada, no se realiza ninguna poda.","metadata":{}},{"cell_type":"markdown","source":"Para `GradientBoosting` teniamos una grán cantidad de hiperparametros para seleccionar, aunque nosotros unicamente vamos a utilizar `criterion`, `learning_rate`, `max_depth`, `n_estimators` y `subsample`.","metadata":{}},{"cell_type":"markdown","source":"### HistGradientBoostingClassifier","metadata":{}},{"cell_type":"markdown","source":"Este estimador tiene soporte nativo para valores perdidos (NaN). Durante el entrenamiento, cuando el arbol crece, aprende en cada punto de división si las muestras con valores perdidos deben ir al hijo izquierdo o derecho, según la ganancia potencial. Al predecir, las muestras con valores perdidos se asignan al hijo izquierdo o derecho en consecuencia. Si no se encontraron valores perdidos para una característica determinada durante el entrenamiento, las muestras con valores perdidos se asignan al hijo que tenga más muestras.\n\nComo el algoritmo Histogram Gradient Boosting realiza un discretizado interno, no permite el uso de discretizadores en pasos previos del pipeline, por lo que no utilizamos nuestro discretizador.","metadata":{}},{"cell_type":"code","source":"estimator = HistGradientBoostingClassifier(random_state=random_state)\npipeline = Pipeline(steps=[('del_columns_wisconsin',del_columns_wisconsin),('nan_anomalos_wisconsin', nan_anomalos_wisconsin),\n                           ('imp', imp),('HistGradientBoosting', estimator)])\n\nlearning_rate = [0.01, 0.05, 0.1]\n#max_leaf_nodes = [15, 31, 65, 127]\n#max_depth = [2, 3, 4, 5, 6]\nmin_samples_leaf = [10, 20, 30]\nloss = [\"binary_crossentropy\", \"auto\"]\nmax_iter = [50, 100, 150]\nmax_bins = [200, 255, 300, 350]\n\nhist_gradient_boosting_clf = utils.optimize_params(pipeline,X_Cancer, y_Cancer, cv,\n                                                   scoring=scoring,\n                                                   HistGradientBoosting__learning_rate=learning_rate,\n                                                   HistGradientBoosting__min_samples_leaf=min_samples_leaf,\n                                                   HistGradientBoosting__loss=loss,\n                                                   HistGradientBoosting__max_iter=max_iter)","metadata":{"papermill":{"duration":98.455839,"end_time":"2020-11-23T16:31:01.312872","exception":false,"start_time":"2020-11-23T16:29:22.857033","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-10T15:01:11.872962Z","iopub.execute_input":"2021-06-10T15:01:11.873454Z","iopub.status.idle":"2021-06-10T15:10:43.476754Z","shell.execute_reply.started":"2021-06-10T15:01:11.873405Z","shell.execute_reply":"2021-06-10T15:10:43.475928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* `loss`: {‘auto’, ‘binary_crossentropy’, ‘categorical_crossentropy’}, default=’auto’ La función de pérdida para usar en el proceso de refuerzo. \"Binary_crossentropy\" (también conocido como pérdida logística) se utiliza para la clasificación binaria y se generaliza a \"categorical_crossentropy\" para la clasificación multiclase. \"Auto\" elegirá automáticamente cualquiera de las pérdidas dependiendo de la naturaleza del problema.\n\n* `learning_rate`: float, default=0.1 La tasa de aprendizaje, también conocida como contracción. Esto se usa como factor multiplicativo para los valores de las hojas. Utilice 1 para que no se encoja.\n\n* `max_iter`: int, default=100 El número máximo de iteraciones del proceso de refuerzo, es decir, el número máximo de árboles para la clasificación binaria. Para la clasificación multiclase, se construyen n árboles de clases por iteración.\n\n* `max_leaf_nodes`: int or None, default=31 El número máximo de hojas de cada árbol. Debe ser mayor que 1. Si es None, no hay límite máximo.\n\n* `max_depth`: int or None, default=None La profundidad máxima de cada árbol.\n\n* `min_samples_leaf`: int, default=20 El número mínimo de muestras por hoja. Para conjuntos de datos pequeños con menos de unos pocos cientos de muestras, se recomienda reducir este valor, ya que solo se construirían árboles muy poco profundos.\n\n* `l2_regularization`: float, default=0 El parámetro de regularización L2. Utiliza 0 para no regularización.\n\n* `max_bins`: int, default=255 El número máximo de contenedores que se utilizarán para los valores que no faltan. Antes del entrenamiento, cada característica de la matriz de entrada X se agrupa en bins con valores enteros, lo que permite una etapa de entrenamiento mucho más rápida. Las funciones con una pequeña cantidad de valores únicos pueden usar menos de max_bins bins. Además de los contenedores max_bins, siempre se reserva un contenedor más para los valores perdidos. No debe ser mayor de 255.\n\n* `categorical_features`: array-like of {bool, int} of shape (n_features) or shape (n_categorical_features,), default=None. Indica las características categóricas.\n\n  Ninguno: ninguna característica se considerará categórica.\n\n  tipo matriz booleana: máscara booleana que indica características categóricas.\n\n  tipo matriz de enteros: índices enteros que indican características categóricas.\n\n    Para cada característica categórica, debe haber como máximo max_bins categorías únicas, y cada valor categórico debe estar en [0, max_bins -1].\n\n* `monotonic_cst`: array-like of int of shape (n_features), default=None Indica la restricción monótona que se debe aplicar a cada función. -1, 1 y 0 corresponden respectivamente a una restricción negativa, una restricción positiva y ninguna restricción.\n\n* `warm_start`: bool, default=False Cuando se establece en Verdadero, reutiliza la solución de la llamada anterior para ajustar y agregar más estimadores al conjunto. Para que los resultados sean válidos, el estimador debe volver a entrenarse solo con los mismos datos.\n\n* `early_stopping`: ‘auto’ or bool, default=’auto’ Si es \"auto\", la parada anticipada está habilitada si el tamaño de la muestra es mayor que 10000. Si es Verdadero, la parada anticipada está habilitada; de lo contrario, la parada anticipada está deshabilitada.\n\n* `scoring`: str or callable or None, default=’loss’ Parámetro de puntuación que se utilizará para la parada anticipada. Puede ser una sola cadena o un invocable. Si es None, se usa el puntaje predeterminado del estimador. Si la puntuación = 'loss', se verifica la detención anticipada con el valor de la pérdida. Solo se utiliza si se realiza una parada anticipada.\n\n* `validation_fraction`: int or float or None, default=0.1 Proporción (o tamaño absoluto) de los datos de entrenamiento que se deben reservar como datos de validación para la detención anticipada. Si es None, la detención anticipada se realiza en los datos de entrenamiento. Solo se utiliza si se realiza una parada anticipada.\n\n* `n_iter_no_change`: int, default=10 Utilizado para saber cuando hacer una parada anticipada. El proceso de ajuste se detiene cuando ninguna de las últimas puntuaciones n_iter_no_change es mejor que la n_iter_no_change - 1 -th-to-last one, hasta cierta tolerancia. Solo se utiliza si se realiza una parada anticipada.\n\n* `tol`: float or None, default=1e-7 La tolerancia absoluta a utilizar al comparar puntuaciones. Cuanto mayor sea la tolerancia, más probabilidades hay de que nos detengamos antes de tiempo: una mayor tolerancia significa que será más difícil que las iteraciones posteriores se consideren una mejora en la puntuación de referencia.\n\n* `verbose`: int, default=0 El nivel de verbosidad. Si no es cero, imprima alguna información sobre el proceso de ajuste.\n\n* `random_state`: int, RandomState instance or None, default=None Generador de números pseudoaleatorios para controlar el submuestreo en el proceso de agrupamiento y la división de datos de validación/entrenamiento si la parada anticipada está habilitada. Pase un int para una salida reproducible a través de múltiples llamadas a funciones.","metadata":{}},{"cell_type":"markdown","source":"Respecto a `HistGradientBoosting` tambien tenemos una gran cantidad de hiperparametros que podriamos utilizar aunque nosotros unicamente hemos utilizado `learning_rate`, `loss`, `max_iter` ,`min_samples_leaf` y `max_bins`","metadata":{}},{"cell_type":"markdown","source":"## Selección para Pima","metadata":{}},{"cell_type":"code","source":"scoring = {'AUC':'roc_auc', 'Recall':make_scorer(recall_score, pos_label=1)}","metadata":{"execution":{"iopub.status.busy":"2021-06-10T15:10:43.478113Z","iopub.execute_input":"2021-06-10T15:10:43.478585Z","iopub.status.idle":"2021-06-10T15:10:43.486386Z","shell.execute_reply.started":"2021-06-10T15:10:43.47855Z","shell.execute_reply":"2021-06-10T15:10:43.485179Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Tras haber explicado detalladamente los hiperparametros de los distintos algoritmos anteriormente, en la seleccion para Wisconsin, aqui no vamos a explicar mucho, luego comentaremos los resultados en la parte final.","metadata":{}},{"cell_type":"markdown","source":"### KNeighborsClassifier","metadata":{}},{"cell_type":"code","source":"estimator = KNeighborsClassifier()\npipeline = Pipeline(steps=[('del_columns_pima',del_columns_pima),('nan_anomalos_pima', nan_anomalos_pima),('imp', imp),\n                           ('discretizer_pima', discretizer_pima),('knn', estimator)])\n\n\nX_Diabetes_Norm = normalize(X_Diabetes)\n\n\nweights = [\"uniform\", \"distance\"]\nn_neighbors = [3,5,7,9,11,13,15,17,19]\n#algorithm = [\"auto\", \"ball_tree\", \"kd_tree\", \"brute\"]\n#p =[2, 3, 4, 5, 6, 7]\n#leaf_size = [30, 40, 50, 60, 70, 80, 90, 100]\nmetric = [\"euclidean\", \"manhattan\"]\n#n_jobs = default -1\n\nk_neighbors_clf_pima = utils.optimize_params(pipeline,X_Diabetes_Norm, y_Diabetes, cv,\n                                             scoring=scoring,\n                                             knn__weights=weights,\n                                             knn__n_neighbors=n_neighbors)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T15:10:43.489255Z","iopub.execute_input":"2021-06-10T15:10:43.490476Z","iopub.status.idle":"2021-06-10T15:12:40.582252Z","shell.execute_reply.started":"2021-06-10T15:10:43.49043Z","shell.execute_reply":"2021-06-10T15:12:40.581175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### DecisionTreeClassifier","metadata":{}},{"cell_type":"code","source":"estimator = DecisionTreeClassifier(random_state=random_state)\npipeline = Pipeline(steps=[('del_columns_pima',del_columns_pima),('nan_anomalos_pima', nan_anomalos_pima),('imp', imp),\n                           ('discretizer_pima', discretizer_pima),('DecisionTree', estimator)])\n\ncriterion = [\"gini\", \"entropy\"]\nmax_depth = [4, 5, 6, 7, 8, 9, 10, None]\nccp_alpha = [0.0, 0.1, 0.2, 0.3]\n\n#class_weight = [\"balanced\", None]\n#max_features = [int, float, \"auto\", \"sqrt\", \"log2\", None]\n#max_leaf_nodes = [8, 9, 10, 11, 12]\n#min_impurity_decrease = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5]\n#min_samples_split = [2, 3, 4, 5, 6, 7, 8, 9, 10]\n#min_samples_leaf = [5,6,7,8,9,10,11,12,13,14,15]\n#min_weight_fraction_leaf = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5]\n#splitter = [\"best\",\"random\"]\n\n\n\n\n\ndecision_tree_clf_pima = utils.optimize_params(pipeline,X_Diabetes, y_Diabetes, cv,\n                                               scoring=scoring,\n                                               DecisionTree__criterion=criterion,\n                                               DecisionTree__max_depth=max_depth,\n                                               DecisionTree__ccp_alpha=ccp_alpha)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T15:12:40.583876Z","iopub.execute_input":"2021-06-10T15:12:40.584516Z","iopub.status.idle":"2021-06-10T15:22:07.552584Z","shell.execute_reply.started":"2021-06-10T15:12:40.584465Z","shell.execute_reply":"2021-06-10T15:22:07.55149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### AdaBoostClassifier","metadata":{}},{"cell_type":"code","source":"estimator = AdaBoostClassifier(random_state=random_state)\npipeline = Pipeline(steps=[('del_columns_pima',del_columns_pima),('nan_anomalos_pima', nan_anomalos_pima),('imp', imp),\n                           ('discretizer_pima', discretizer_pima),('AdaBoost', estimator)])\n\n# Should not modify the base original model\nbase_estimator = DecisionTreeClassifier(random_state=random_state)\n\nbase_estimator = [base_estimator]\nlearning_rate = [0.95, 1.0]\n#n_estimators = [100, 150, 200]\n\ncriterion = [\"gini\", \"entropy\"]\nmax_depth = [1, 2, 3]\nccp_alpha = [0.0, 0.1]\n\n\nadaboost_clf_pima = utils.optimize_params(pipeline,X_Diabetes, y_Diabetes, cv,\n                                          scoring=scoring,\n                                          AdaBoost__base_estimator=base_estimator,\n                                          AdaBoost__learning_rate=learning_rate,\n                                          AdaBoost__base_estimator__criterion=criterion,\n                                          AdaBoost__base_estimator__max_depth=max_depth,\n                                          AdaBoost__base_estimator__ccp_alpha=ccp_alpha)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T15:22:07.554033Z","iopub.execute_input":"2021-06-10T15:22:07.554394Z","iopub.status.idle":"2021-06-10T15:27:20.358048Z","shell.execute_reply.started":"2021-06-10T15:22:07.554358Z","shell.execute_reply":"2021-06-10T15:27:20.357159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### BaggingClassifier","metadata":{}},{"cell_type":"code","source":"estimator = BaggingClassifier(random_state=random_state)\npipeline = Pipeline(steps=[('del_columns_pima',del_columns_pima),('nan_anomalos_pima', nan_anomalos_pima),('imp', imp),\n                           ('discretizer_pima', discretizer_pima),('Bagging', estimator)])\n\n# Should not modify the base original model\nbase_estimator = DecisionTreeClassifier(random_state=random_state)\n\nbase_estimator = [base_estimator]\ncriterion = [\"gini\", \"entropy\"]\nmax_depth = [4, 5, 6, 7, 8]\nmin_samples_leaf = [5,6,7,8,9,10,11,12,13,14,15]\n\nbagging_clf_pima = utils.optimize_params(pipeline,X_Diabetes, y_Diabetes, cv,\n                                         scoring=scoring,\n                                         Bagging__base_estimator=base_estimator,\n                                         Bagging__base_estimator__criterion=criterion,\n                                         Bagging__base_estimator__max_depth=max_depth,\n                                         Bagging__base_estimator__min_samples_leaf=min_samples_leaf)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T15:27:20.359762Z","iopub.execute_input":"2021-06-10T15:27:20.360115Z","iopub.status.idle":"2021-06-10T15:45:22.755055Z","shell.execute_reply.started":"2021-06-10T15:27:20.360057Z","shell.execute_reply":"2021-06-10T15:45:22.754112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### RanfomForestClassifier","metadata":{}},{"cell_type":"code","source":"estimator = RandomForestClassifier(random_state=random_state)\npipeline = Pipeline(steps=[('del_columns_pima',del_columns_pima),('nan_anomalos_pima', nan_anomalos_pima),('imp', imp),\n                           ('discretizer_pima', discretizer_pima),('RandForest', estimator)])\n\ncriterion = [\"gini\", \"entropy\"]\nmax_features = [\"sqrt\", \"log2\"]\nmax_depth = [5,6,7,8,9]\n\nrandom_forest_clf_pima = utils.optimize_params(pipeline,X_Diabetes, y_Diabetes, cv,\n                                               scoring=scoring,\n                                               RandForest__criterion=criterion,\n                                               RandForest__max_features=max_features,\n                                               RandForest__max_depth=max_depth)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T15:45:22.756521Z","iopub.execute_input":"2021-06-10T15:45:22.75714Z","iopub.status.idle":"2021-06-10T15:50:05.140931Z","shell.execute_reply.started":"2021-06-10T15:45:22.757094Z","shell.execute_reply":"2021-06-10T15:50:05.139846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### GradientBoostingClassifier","metadata":{}},{"cell_type":"code","source":"estimator = GradientBoostingClassifier(random_state=random_state)\npipeline = Pipeline(steps=[('del_columns_pima',del_columns_pima),('nan_anomalos_pima', nan_anomalos_pima),('imp', imp),\n                           ('discretizer_pima', discretizer_pima),('GradBoosting', estimator)])\n\nlearning_rate = [0.01, 0.05, 0.1]\ncriterion = [\"friedman_mse\", \"mse\"]\nmax_depth = [1, 2, 3]\n#ccp_alpha = [0.0, 0.1]\nn_estimators = [50, 100, 200]\nsubsample = [0.2, 0.8, 1.0]\n\ngradient_boosting_clf_pima = utils.optimize_params(pipeline,X_Diabetes, y_Diabetes, cv,\n                                                   scoring=scoring,\n                                                   GradBoosting__learning_rate=learning_rate,\n                                                   GradBoosting__criterion=criterion,\n                                                   GradBoosting__max_depth=max_depth,\n                                                   GradBoosting__subsample=subsample,\n                                                   GradBoosting__n_estimators = n_estimators)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T15:50:05.142628Z","iopub.execute_input":"2021-06-10T15:50:05.143287Z","iopub.status.idle":"2021-06-10T16:19:43.125751Z","shell.execute_reply.started":"2021-06-10T15:50:05.143238Z","shell.execute_reply":"2021-06-10T16:19:43.124652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### HistGradientBoostingClassifier","metadata":{}},{"cell_type":"code","source":"estimator = HistGradientBoostingClassifier(random_state=random_state)\npipeline = Pipeline(steps=[('del_columns_pima',del_columns_pima),('nan_anomalos_pima', nan_anomalos_pima),('imp', imp),('HistGradBoosting', estimator)])\n\nlearning_rate = [0.01, 0.05, 0.1]\n#max_leaf_nodes = [15, 31, 65, 127]\n#max_depth = [2, 3, 4, 5, 6]\nmin_samples_leaf = [10, 20, 30]\nloss = [\"binary_crossentropy\", \"auto\"]\nmax_iter = [50, 100, 150]\nmax_bins = [200, 255, 300, 350]\n\nhist_gradient_boosting_clf_pima = utils.optimize_params(pipeline,X_Diabetes, y_Diabetes, cv,\n                                                        scoring=scoring,\n                                                        HistGradBoosting__learning_rate=learning_rate,\n                                                        HistGradBoosting__min_samples_leaf=min_samples_leaf,\n                                                        HistGradBoosting__loss=loss,\n                                                        HistGradBoosting__max_iter=max_iter)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T16:19:43.127416Z","iopub.execute_input":"2021-06-10T16:19:43.12803Z","iopub.status.idle":"2021-06-10T16:31:55.294622Z","shell.execute_reply.started":"2021-06-10T16:19:43.127971Z","shell.execute_reply":"2021-06-10T16:31:55.293776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. Construcción y validación del modelo final","metadata":{"papermill":{"duration":0.114841,"end_time":"2020-11-23T16:31:01.780198","exception":false,"start_time":"2020-11-23T16:31:01.665357","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"Ya tenemos todos los clasificadores optimizados, por lo que ahora tendremos que elegir el mejor de todos. \nComo es complicado ver todos los resultados en el apartado anterior, gracias a las funciones de evaluate_estimators2 vamos a poder ver de manera clara los resultados obtenidos por los distintos clasificadores en unas tablas.","metadata":{}},{"cell_type":"code","source":"estimators = {\n    \"Nearest neighbors\": k_neighbors_clf,\n    \"Decision tree\": decision_tree_clf,\n    \"AdaBoost\": adaboost_clf,\n    \"Bagging\": bagging_clf,\n    \"Random Forests\": random_forest_clf,\n    \"Gradient Boosting\": gradient_boosting_clf,\n    \"Histogram Gradient Boosting\": hist_gradient_boosting_clf \n}","metadata":{"papermill":{"duration":0.119679,"end_time":"2020-11-23T16:31:02.228822","exception":false,"start_time":"2020-11-23T16:31:02.109143","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-10T16:31:55.296125Z","iopub.execute_input":"2021-06-10T16:31:55.296684Z","iopub.status.idle":"2021-06-10T16:31:55.303451Z","shell.execute_reply.started":"2021-06-10T16:31:55.296645Z","shell.execute_reply":"2021-06-10T16:31:55.302557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_Cancer = X_test_Cancer\ny_Cancer = y_test_Cancer\n\nutils.evaluate_estimators2(estimators, X_Cancer, y_Cancer, 'M')","metadata":{"papermill":{"duration":0.200521,"end_time":"2020-11-23T16:31:02.53922","exception":false,"start_time":"2020-11-23T16:31:02.338699","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-10T16:31:55.304906Z","iopub.execute_input":"2021-06-10T16:31:55.305262Z","iopub.status.idle":"2021-06-10T16:31:55.832333Z","shell.execute_reply.started":"2021-06-10T16:31:55.305228Z","shell.execute_reply":"2021-06-10T16:31:55.831324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"estimators = {\n    \"Nearest neighbors\": k_neighbors_clf_pima,\n    \"Decision tree\": decision_tree_clf_pima,\n    \"AdaBoost\": adaboost_clf_pima,\n    \"Bagging\": bagging_clf_pima,\n    \"Random Forests\": random_forest_clf_pima,\n    \"Gradient Boosting\": gradient_boosting_clf_pima,\n    \"Histogram Gradient Boosting\": hist_gradient_boosting_clf_pima\n}","metadata":{"execution":{"iopub.status.busy":"2021-06-10T16:31:55.833707Z","iopub.execute_input":"2021-06-10T16:31:55.834049Z","iopub.status.idle":"2021-06-10T16:31:55.841857Z","shell.execute_reply.started":"2021-06-10T16:31:55.834007Z","shell.execute_reply":"2021-06-10T16:31:55.839568Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_Diabetes = X_test_Diabetes\ny_Diabetes = y_test_Diabetes\n\nutils.evaluate_estimators2(estimators, X_Diabetes, y_Diabetes,1)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T16:31:55.843703Z","iopub.execute_input":"2021-06-10T16:31:55.844095Z","iopub.status.idle":"2021-06-10T16:31:56.409548Z","shell.execute_reply.started":"2021-06-10T16:31:55.844044Z","shell.execute_reply":"2021-06-10T16:31:56.408512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Viendo estos datos, el modelo que peor resultados brinda es el de `Nearest neighbors`. Pensamos que esto es así porque tenemos bastantes variables y KNN se comporta de forma que todas las variables se consideran de igual importancia, lo que hace que una variable que aporte poco al problema le quite importancia a una variable que sea significativa.\n\nPodemos observar que `Histogram Gradient Boosting` obtiene los mejores resultados en Wisconsin pero en Pima quien obtiene los mejores resultados es `Decision Tree Classifier`, esto puede deberse por la gran diferencia que hay entre las dos bases de datos en cuanto a tamaño y cantidad de información.\n\nEn cuanto a los ensembles vemos que generalmente obtienen buenos resultados en general.\n\n\n\n","metadata":{}},{"cell_type":"markdown","source":"# 6. Titanic","metadata":{}},{"cell_type":"markdown","source":"Cargamos los datos y obtenemos los conjuntos de variables predictoras X y la variable clase y.","metadata":{}},{"cell_type":"code","source":"filepath = \"../input/titanic/train.csv\"\n\nindex = \"PassengerId\"\ntarget = \"Survived\"\n\nTitanic_data = utils.load_data(filepath, index, target)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T16:31:56.41698Z","iopub.execute_input":"2021-06-10T16:31:56.417401Z","iopub.status.idle":"2021-06-10T16:31:56.443809Z","shell.execute_reply.started":"2021-06-10T16:31:56.417365Z","shell.execute_reply":"2021-06-10T16:31:56.4427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Titanic_data.sample(5, random_state=random_state)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T16:31:56.445511Z","iopub.execute_input":"2021-06-10T16:31:56.446113Z","iopub.status.idle":"2021-06-10T16:31:56.468128Z","shell.execute_reply.started":"2021-06-10T16:31:56.446048Z","shell.execute_reply":"2021-06-10T16:31:56.467211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Titanic_data[\"Survived\"]=Titanic_data[\"Survived\"].astype(\"category\")\nTitanic_data.info(memory_usage=False)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T16:31:56.469542Z","iopub.execute_input":"2021-06-10T16:31:56.470104Z","iopub.status.idle":"2021-06-10T16:31:56.488166Z","shell.execute_reply.started":"2021-06-10T16:31:56.47005Z","shell.execute_reply":"2021-06-10T16:31:56.487229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"(X_Titanic_data, y_Titanic_data)=utils.divide_dataset(Titanic_data,target=\"Survived\")","metadata":{"execution":{"iopub.status.busy":"2021-06-10T16:31:56.48958Z","iopub.execute_input":"2021-06-10T16:31:56.490206Z","iopub.status.idle":"2021-06-10T16:31:56.497291Z","shell.execute_reply.started":"2021-06-10T16:31:56.490161Z","shell.execute_reply":"2021-06-10T16:31:56.496308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_Titanic_data.sample(5, random_state=random_state)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T16:31:56.498863Z","iopub.execute_input":"2021-06-10T16:31:56.49949Z","iopub.status.idle":"2021-06-10T16:31:56.527162Z","shell.execute_reply.started":"2021-06-10T16:31:56.49945Z","shell.execute_reply":"2021-06-10T16:31:56.526289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_Titanic_data.sample(5, random_state=random_state)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T16:31:56.528651Z","iopub.execute_input":"2021-06-10T16:31:56.529285Z","iopub.status.idle":"2021-06-10T16:31:56.537977Z","shell.execute_reply.started":"2021-06-10T16:31:56.529244Z","shell.execute_reply":"2021-06-10T16:31:56.537156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Vemos que la particion de la base de datos en variables predictoras y por otro lado la variable clase se ha realizado correctamente, por lo que ahora dividimos en train/test.","metadata":{}},{"cell_type":"code","source":"(X_Titanic_data_train,X_Titanic_data_test,y_Titanic_data_train,y_Titanic_data_test)=train_test_split(X_Titanic_data,\n                                                                                                     y_Titanic_data,\n                                                                                                     stratify=y_Titanic_data,\n                                                                                                     random_state=random_state,\n                                                                                                     train_size=0.7)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T16:31:56.539432Z","iopub.execute_input":"2021-06-10T16:31:56.540208Z","iopub.status.idle":"2021-06-10T16:31:56.551967Z","shell.execute_reply.started":"2021-06-10T16:31:56.540162Z","shell.execute_reply":"2021-06-10T16:31:56.551035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_Titanic = X_Titanic_data_train\ny_Titanic = y_Titanic_data_train","metadata":{"execution":{"iopub.status.busy":"2021-06-10T16:31:56.553492Z","iopub.execute_input":"2021-06-10T16:31:56.554136Z","iopub.status.idle":"2021-06-10T16:31:56.561121Z","shell.execute_reply.started":"2021-06-10T16:31:56.554091Z","shell.execute_reply":"2021-06-10T16:31:56.560097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_Titanic.sample(5, random_state=random_state)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T16:31:56.562524Z","iopub.execute_input":"2021-06-10T16:31:56.562888Z","iopub.status.idle":"2021-06-10T16:31:56.590185Z","shell.execute_reply.started":"2021-06-10T16:31:56.562854Z","shell.execute_reply":"2021-06-10T16:31:56.589396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Creamos el Pipeline para titanic obtenido de la practica anterior:","metadata":{}},{"cell_type":"code","source":"del_columns = utils.QuitarColumnasTransformer(['Cabin', 'Name', 'Ticket', 'Fare'])\n\nstring_int1 = utils.StringAIntTransformer('Sex',['female','male'],[0,1])\nstring_int2 = utils.StringAIntTransformer('Embarked',['Q','C','S'],[1,2,3])\n\nnan_anomalos_titanic = utils.AnomalosANanTransformer({'Pclass':[1,3], 'Sex':[0,1], 'SibSp':[0, 8],\n                                         'Parch':[0, 6], 'Embarked':[1, 3] })\n\nimp = SimpleImputer(missing_values=float('nan'), strategy='most_frequent')\n\ndiscretizer_titanic = KBinsDiscretizer(n_bins=2, strategy=\"uniform\")","metadata":{"execution":{"iopub.status.busy":"2021-06-10T16:31:56.591404Z","iopub.execute_input":"2021-06-10T16:31:56.591872Z","iopub.status.idle":"2021-06-10T16:31:56.60086Z","shell.execute_reply.started":"2021-06-10T16:31:56.591839Z","shell.execute_reply":"2021-06-10T16:31:56.600152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Creamos la funcion para normalizar la base de datos de titanic.","metadata":{}},{"cell_type":"code","source":"def normalizeTitanic(dataset):\n    dataNorm=((dataset[\"Pclass\"])/(dataset[\"Pclass\"].max()))\n    dataNorm=((dataset[\"SibSp\"])/(dataset[\"SibSp\"].max()))\n    dataNorm=((dataset[\"Parch\"])/(dataset[\"Parch\"].max()))\n    \n    dataNorm[\"Sex\"]=dataset[\"Sex\"]\n    dataNorm[\"Embarked\"]=dataset[\"Embarked\"]\n    \n    dataNorm[\"Age\"]=dataset[\"Age\"]\n    dataNorm[\"Cabin\"]=dataset[\"Cabin\"]\n    dataNorm[\"Name\"]=dataset[\"Name\"]\n    dataNorm[\"Ticket\"]=dataset[\"Ticket\"]\n    dataNorm[\"Fare\"]=dataset[\"Fare\"]\n    return dataNorm","metadata":{"execution":{"iopub.status.busy":"2021-06-10T16:31:56.602212Z","iopub.execute_input":"2021-06-10T16:31:56.602738Z","iopub.status.idle":"2021-06-10T16:31:56.613665Z","shell.execute_reply.started":"2021-06-10T16:31:56.602671Z","shell.execute_reply":"2021-06-10T16:31:56.612856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Elegimos los scorers que vamos a utilizar.","metadata":{}},{"cell_type":"code","source":"scoring = {'AUC':'roc_auc', 'Recall':make_scorer(recall_score, pos_label=1)}","metadata":{"execution":{"iopub.status.busy":"2021-06-10T16:31:56.614883Z","iopub.execute_input":"2021-06-10T16:31:56.61522Z","iopub.status.idle":"2021-06-10T16:31:56.631183Z","shell.execute_reply.started":"2021-06-10T16:31:56.615189Z","shell.execute_reply":"2021-06-10T16:31:56.63016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Empezamos a optimizar los hiperparametros de los modelos y comentaremos los resultados al final de todo.","metadata":{}},{"cell_type":"code","source":"estimator = KNeighborsClassifier()\npipeline = Pipeline(steps=[('del_columns',del_columns),('string_int1', string_int1),('string_int2',string_int2),\n                           ('nan_anomalos_titanic',nan_anomalos_titanic),('imp', imp),\n                           ('discretizer_titanic', discretizer_titanic),('knn', estimator)])\n\n\nX_Titanic_Norm = normalizeTitanic(X_Titanic)\n\n\nweights = [\"uniform\", \"distance\"]\nn_neighbors = [3,5,7,9,11,13,15,17,19]\n#algorithm = [\"auto\", \"ball_tree\", \"kd_tree\", \"brute\"]\n#p =[2, 3, 4, 5, 6, 7]\n#leaf_size = [30, 40, 50, 60, 70, 80, 90, 100]\nmetric = [\"euclidean\", \"manhattan\"]\n#n_jobs = default -1\n\nk_neighbors_clf_titanic = utils.optimize_params(pipeline,X_Titanic, y_Titanic, cv,\n                                             scoring=scoring,\n                                             knn__weights=weights,\n                                             knn__n_neighbors=n_neighbors,\n                                             knn__metric=metric)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T16:31:56.632388Z","iopub.execute_input":"2021-06-10T16:31:56.632893Z","iopub.status.idle":"2021-06-10T16:37:15.07402Z","shell.execute_reply.started":"2021-06-10T16:31:56.632855Z","shell.execute_reply":"2021-06-10T16:37:15.072621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"estimator = DecisionTreeClassifier(random_state=random_state)\npipeline = Pipeline(steps=[('del_columns',del_columns),('string_int1', string_int1),('string_int2',string_int1),\n                           ('nan_anomalos_titanic',nan_anomalos_titanic),('imp', imp),\n                           ('discretizer_titanic', discretizer_titanic),('DecisionTree', estimator)])\n\n\n\ncriterion = [\"gini\", \"entropy\"]\nmax_depth = [4, 5, 6, 7, 8, 9, 10, None]\nccp_alpha = [0.0, 0.1, 0.2, 0.3]\n\n#class_weight = [\"balanced\", None]\n#max_features = [int, float, \"auto\", \"sqrt\", \"log2\", None]\n#max_leaf_nodes = [8, 9, 10, 11, 12]\n#min_impurity_decrease = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5]\n#min_samples_split = [2, 3, 4, 5, 6, 7, 8, 9, 10]\n#min_samples_leaf = [5,6,7,8,9,10,11,12,13,14,15]\n#min_weight_fraction_leaf = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5]\n#splitter = [\"best\",\"random\"]\n\ndecision_tree_clf_titanic = utils.optimize_params(pipeline,X_Titanic, y_Titanic, cv,\n                                             scoring=scoring,\n                                             DecisionTree__criterion=criterion,\n                                             DecisionTree__max_depth=max_depth,\n                                             DecisionTree__ccp_alpha=ccp_alpha)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T16:37:15.075537Z","iopub.execute_input":"2021-06-10T16:37:15.075832Z","iopub.status.idle":"2021-06-10T16:44:52.044006Z","shell.execute_reply.started":"2021-06-10T16:37:15.075803Z","shell.execute_reply":"2021-06-10T16:44:52.043046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"estimator = AdaBoostClassifier(random_state=random_state)\npipeline = Pipeline(steps=[('del_columns',del_columns),('string_int1', string_int1),('string_int2',string_int1),\n                           ('nan_anomalos_titanic',nan_anomalos_titanic),('imp', imp),\n                           ('discretizer_titanic', discretizer_titanic),('adaBoost', estimator)])\n\n\n# Should not modify the base original model\nbase_estimator = DecisionTreeClassifier(random_state=random_state)\n\nbase_estimator = [base_estimator]\nlearning_rate = [0.95, 1.0]\n#n_estimators = [100, 150, 200]\n\ncriterion = [\"gini\", \"entropy\"]\nmax_depth = [1, 2, 3]\nccp_alpha = [0.0, 0.1]\n\nadaboost_clf_titanic = utils.optimize_params(pipeline,X_Titanic, y_Titanic, cv,\n                                             scoring=scoring,\n                                             adaBoost__learning_rate=learning_rate,\n                                             adaBoost__base_estimator=base_estimator,\n                                             adaBoost__base_estimator__criterion=criterion,\n                                             adaBoost__base_estimator__max_depth=max_depth,\n                                             adaBoost__base_estimator__ccp_alpha=ccp_alpha)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T16:44:52.045399Z","iopub.execute_input":"2021-06-10T16:44:52.045694Z","iopub.status.idle":"2021-06-10T16:49:26.760377Z","shell.execute_reply.started":"2021-06-10T16:44:52.045666Z","shell.execute_reply":"2021-06-10T16:49:26.759329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"estimator = BaggingClassifier(random_state=random_state)\npipeline = Pipeline(steps=[('del_columns',del_columns),('string_int1', string_int1),('string_int2',string_int1),\n                           ('nan_anomalos_titanic',nan_anomalos_titanic),('imp', imp),\n                           ('discretizer_titanic', discretizer_titanic),('bagging', estimator)])\n\n\nbase_estimator = DecisionTreeClassifier(random_state=random_state)\n\nbase_estimator = [base_estimator]\ncriterion = [\"gini\", \"entropy\"]\nmax_depth = [4, 5, 6, 7, 8]\nmin_samples_leaf = [5,6,7,8,9,10,11,12,13,14,15]\n\n\nbagging_clf_titanic = utils.optimize_params(pipeline,X_Titanic, y_Titanic, cv,\n                                             scoring=scoring,\n                                             bagging__base_estimator=base_estimator,\n                                             bagging__base_estimator__criterion=criterion,\n                                             bagging__base_estimator__max_depth=max_depth,\n                                             bagging__base_estimator__min_samples_leaf=min_samples_leaf)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T16:49:26.76178Z","iopub.execute_input":"2021-06-10T16:49:26.7621Z","iopub.status.idle":"2021-06-10T17:04:12.645512Z","shell.execute_reply.started":"2021-06-10T16:49:26.762056Z","shell.execute_reply":"2021-06-10T17:04:12.644361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"estimator = RandomForestClassifier(random_state=random_state)\npipeline = Pipeline(steps=[('del_columns',del_columns),('string_int1', string_int1),('string_int2',string_int1),\n                           ('nan_anomalos_titanic',nan_anomalos_titanic),('imp', imp),\n                           ('discretizer_titanic', discretizer_titanic),('RFC', estimator)])\n\ncriterion = [\"gini\", \"entropy\"]\nmax_features = [\"sqrt\", \"log2\"]\n#max_depth = [5,6,7,8,9]\n\nrandom_forest_clf_titanic = utils.optimize_params(pipeline,X_Titanic, y_Titanic, cv,\n                                             scoring=scoring,\n                                             RFC__criterion=criterion,\n                                             RFC__max_features=max_features)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T17:04:12.647384Z","iopub.execute_input":"2021-06-10T17:04:12.647822Z","iopub.status.idle":"2021-06-10T17:05:02.014364Z","shell.execute_reply.started":"2021-06-10T17:04:12.647776Z","shell.execute_reply":"2021-06-10T17:05:02.013025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"estimator = GradientBoostingClassifier(random_state=random_state)\npipeline = Pipeline(steps=[('del_columns',del_columns),('string_int1', string_int1),('string_int2',string_int1),\n                           ('nan_anomalos_titanic',nan_anomalos_titanic),('imp', imp),\n                           ('discretizer_titanic', discretizer_titanic),('gradBoosting', estimator)])\n\nlearning_rate = [0.01, 0.05, 0.1]\ncriterion = [\"friedman_mse\", \"mse\"]\nmax_depth = [1, 2, 3]\n#ccp_alpha = [0.0, 0.1]\nn_estimators = [50, 100, 200]\nsubsample = [0.2, 0.8, 1.0]\n\ngradient_boosting_clf_titanic = utils.optimize_params(pipeline,X_Titanic, y_Titanic, cv,\n                                             scoring=scoring,\n                                             gradBoosting__learning_rate=learning_rate,\n                                             gradBoosting__criterion=criterion,\n                                             gradBoosting__max_depth=max_depth,\n                                             gradBoosting__n_estimators=n_estimators,\n                                             gradBoosting__subsample=subsample)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T17:05:02.016184Z","iopub.execute_input":"2021-06-10T17:05:02.016649Z","iopub.status.idle":"2021-06-10T17:29:59.296114Z","shell.execute_reply.started":"2021-06-10T17:05:02.016602Z","shell.execute_reply":"2021-06-10T17:29:59.295377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"estimator = HistGradientBoostingClassifier(random_state=random_state)\npipeline = Pipeline(steps=[('del_columns',del_columns),('string_int1', string_int1),('string_int2',string_int1),\n                           ('nan_anomalos_titanic',nan_anomalos_titanic),('imp', imp),('histGradBoosting', estimator)])\n\n\n\n\n\nlearning_rate = [0.01, 0.05, 0.1]\n#max_leaf_nodes = [15, 31, 65, 127]\n#max_depth = [2, 3, 4, 5, 6]\nmin_samples_leaf = [10, 20, 30]\nloss = [\"binary_crossentropy\", \"auto\"]\nmax_iter = [50, 100, 150]\n\nhist_gradient_boosting_clf_titanic = utils.optimize_params(pipeline,X_Titanic, y_Titanic, cv,\n                                             scoring=scoring,\n                                             histGradBoosting__learning_rate=learning_rate,\n                                             histGradBoosting__min_samples_leaf=min_samples_leaf,\n                                             histGradBoosting__loss=loss,\n                                             histGradBoosting__max_iter=max_iter)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T17:29:59.297322Z","iopub.execute_input":"2021-06-10T17:29:59.297765Z","iopub.status.idle":"2021-06-10T17:41:01.386552Z","shell.execute_reply.started":"2021-06-10T17:29:59.297734Z","shell.execute_reply":"2021-06-10T17:41:01.385706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finalmente construimos y validamos el modelo final.","metadata":{}},{"cell_type":"code","source":"estimators = {\n    \"Nearest neighbors\": k_neighbors_clf_titanic,\n    \"Decision tree\": decision_tree_clf_titanic,\n    \"AdaBoost\": adaboost_clf_titanic,\n    \"Bagging\": bagging_clf_titanic,\n    \"Random Forests\": random_forest_clf_titanic,\n    \"Gradient Boosting\": gradient_boosting_clf_titanic,\n    \"Histogram Gradient Boosting\": hist_gradient_boosting_clf_titanic \n}","metadata":{"execution":{"iopub.status.busy":"2021-06-10T17:41:01.38791Z","iopub.execute_input":"2021-06-10T17:41:01.388436Z","iopub.status.idle":"2021-06-10T17:41:01.39553Z","shell.execute_reply.started":"2021-06-10T17:41:01.388396Z","shell.execute_reply":"2021-06-10T17:41:01.394547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_Titanic = X_Titanic_data_test\ny_Titanic = y_Titanic_data_test\n\nutils.evaluate_estimators2(estimators, X_Titanic, y_Titanic,1)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T17:41:01.396844Z","iopub.execute_input":"2021-06-10T17:41:01.397427Z","iopub.status.idle":"2021-06-10T17:41:01.89111Z","shell.execute_reply.started":"2021-06-10T17:41:01.397388Z","shell.execute_reply":"2021-06-10T17:41:01.890237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Con los hiperparametros optimizados podemos observar que el mejor modelo es el `Histogram Gradient Boosting` en cuanto a ROC_AUC_Score, en cuanto a Recall_score el mejor modelo es el de `KNN` ","metadata":{}},{"cell_type":"markdown","source":"# Enlace al kernel donde hemos realizado el estudio:\n \n https://www.kaggle.com/pablomoreiragarcia/estudio-kernel","metadata":{}}]}