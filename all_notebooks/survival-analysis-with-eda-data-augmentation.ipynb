{"cells":[{"metadata":{},"cell_type":"markdown","source":"<a id=\"part0\"></a>\n# 0.Intro:\n\n![](https://upload.wikimedia.org/wikipedia/commons/1/1a/MS_Estonia_model.jpg)\n\nWhat i did at this notebook let me summarize it for you. Dataset has 9 columns and 989 rows. 3 column(\"PassengerId\",\"Firstname\",\"Lastname\") needless for training so i dropped these columns at the begining.After dropping i check nan values. Lucky for me dataset has 0 nan values :). I drew 8 plot to understand column releationships.It looks like 0-20 age range ,crew and males has more chance to survive from disaster.To analyze data i have to transform all categorical data to numerical data.To do that i applied OneHotEncode and Label Encode to 3 column. At disaster %80 of passengers died. That means we have unbalanced dataset here. I used smote for balancing.I didn't tune any hyperparameter but i compared 4 different classification method success rates.\n\n**Table of Contents **\n* [0.Intro:](#part0)\n* [1.Deleting unnecessary columns:](#part1)\n* [2.Exploratory Data Analysis:](#part2)\n* [3.Encoding:](#part3)\n* [4.Traning:](#part4)\n* [5.Data Augmentation:](#part5)\n* [6.Training Again:](#part6)\n* [7.Evaluation:](#part7)","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder , OneHotEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/input/passenger-list-for-the-estonia-ferry-disaster/estonia-passenger-list.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head(),data.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have 989 rows and 8 columns.But we are going to drop passengerId , firstname and lastname columns because we dont need these for training.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.Survived.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets check null values","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"No missing values. Perfect!","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id=\"part1\"></a>\n# 1.Deleting unnecessary columns:","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We don't need PassengerId , Firstname and Lastname columns for training so we can drop this columns.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.drop([\"PassengerId\",\"Firstname\",\"Lastname\"],axis = 1,inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"part2\"></a>\n## 2.Exploratory Data Analysis: ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,10))\nsns.barplot(x = data.Country.unique(),y = data.Country.value_counts()) , plt.title(\"Passenger count by country\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Most of our passengers from Sweden and Estonia.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,10))\nsns.barplot(x = data.Sex.unique(),y = data[data.Survived == 1][\"Sex\"].value_counts() * 100 / data.Sex.value_counts()) , plt.title(\"survival percentage by gender\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Seems like males survival rate 4 times bigger than females","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,10))\nsns.barplot(x = list(np.sort(data[data.Survived == 1][\"Country\"].unique())),y = data[data[\"Survived\"] == 1][\"Country\"].value_counts().sort_index())\nplt.title(\"How many passengers survived by country\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Estonian and Swedish passengers survived most. Because most of our passengers from Sweden and Estonia.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.cut(data[data[\"Survived\"] == 1][\"Age\"],bins= 8, labels = [\"0-10\",\"11-21\",\"22-32\",\"33-43\",\"44-54\",\"55-65\",\"66-76\",\"76-90\"]).value_counts().sort_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig , axs = plt.subplots(1,2,figsize = (15,8))\nsns.distplot(data.Age , bins = 8, ax = axs[0]).set_title('Passengers age distribution')\nsns.barplot(x = [\"0-10\",\"11-21\",\"22-32\",\"33-43\",\"44-54\",\"55-65\",\"66-76\",\"76-90\"], y= pd.cut(data[data[\"Survived\"] == 1][\"Age\"],bins= 8, labels = [\"0-10\",\"11-21\",\"22-32\",\"33-43\",\"44-54\",\"55-65\",\"66-76\",\"76-90\"]).value_counts().sort_index()\n            * 100 / pd.cut(data.Age,bins= 8, labels = [\"0-10\",\"11-21\",\"22-32\",\"33-43\",\"44-54\",\"55-65\",\"66-76\",\"76-90\"]).value_counts().sort_index(), ax = axs[1]).set_title('Passengers age distribution')\nplt.title(\"Survive percentage by age.\" )\ndata.Age.mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Mean value of passengers age is 44. We have less passengers at 0-20 and +70 age range also survive rate is higher at 0-20 and 76-90 age range.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig , axss = plt.subplots(1,2,figsize = (15,8))\nsns.barplot(data.Category.unique(),data[data[\"Survived\"] == 1][\"Category\"].value_counts() *100 / data.Category.value_counts(),ax = axss[0]).set_title(\"Survive percentage by category.(C=Crew, P=Passenger)\")\nsns.barplot(data.Category.unique(),data.Category.value_counts(),ax = axss[1]).set_title(\"Passenger and Crew count\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Crew members have more chance to survive than passengers but still its around 20%","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id=\"part3\"></a>\n# 3.Encoding:","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"I'am going to label encode \"Sex\" and \"Category\" columns because these columns has 2 unique values.But \"Country\" column has unique values more than 2 and Country value is nominal data.We are going to apply OneHotEncoding to Country column.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"le = LabelEncoder()\nohe = OneHotEncoder(sparse = False)\ndata.Sex = le.fit_transform(data.Sex) # Label Encoding\ndata.Category = le.fit_transform(data.Category)\ndata = pd.concat((data ,pd.DataFrame(ohe.fit_transform(data.Country.to_frame()),columns = ohe.get_feature_names())),axis = 1) #I concatenate the data and one hot encoded column here.\ndata.drop([\"Country\"],inplace = True , axis = 1) # After concatenation i dropped country column.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets check the dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,10))\nsns.heatmap(data.corr() , annot = True) # Correlation matrix","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"part4\"></a>\n# 4.Traning:","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Before training lets split the dataset first.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X = data.drop(columns = [\"Survived\"])\ny = data.Survived\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.2, random_state=10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"![](https://miro.medium.com/max/712/1*Z54JgbS4DUwWSknhDCvNTQ.png)\n* Sensitivity - True Positive Rate = TP / (TP + FN)\n* Specificity - True Negative Rate = TN / (TN + FP)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = LogisticRegression()\nlr.fit(X_train, y_train)\ny_predlr = lr.predict(X_test)\ncmlr = confusion_matrix(y_test , y_predlr)\nprint(f\"Accuracy: {accuracy_score(y_test, y_predlr)} Sensitivity: {cmlr[0,0] / (cmlr[0,0]+cmlr[1,0])} Specificity: {cmlr[1,1] / (cmlr[1,1]+cmlr[0,1])}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Accually model predict very bad. Specificity 0 means we predicted nothing at survived = 1. This is happening because of imbalanced dataset.lets balance data.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id=\"part5\"></a>\n# 5.Data Augmentation:","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We are going to use Smote for over-sampling.First we have to download imblearn library","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pip install imblearn","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from imblearn.over_sampling import SMOTE\nsm = SMOTE()\nX_sm, y_sm = sm.fit_resample(X, y)\ny.value_counts() , y_sm.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Before we had 137 survivors. Now we have 852. Perfect!Lets train again and evaluate results.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id=\"part6\"></a>\n# 6.Training Again:\n\nBefore training we have to split data again","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_sm, X_test_sm, y_train_sm, y_test_sm = train_test_split(X_sm,y_sm,test_size = 0.2, random_state=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = LogisticRegression()\nlr.fit(X_train_sm, y_train_sm)\ny_predlr2 = lr.predict(X_test_sm)\ncmlr2 = confusion_matrix(y_test_sm , y_predlr2)\nprint(f\"Accuracy: {accuracy_score(y_test_sm, y_predlr2)} Sensitivity: {cmlr2[0,0] / (cmlr2[0,0]+cmlr2[1,0])} Specificity: {cmlr2[1,1] / (cmlr2[1,1]+cmlr2[0,1])}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our accuracy decreased %15 but as you can see now we started to predict 1 values from y variable.The variable y was filled with zeros before smote.Now we can try different classifiers.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"svc = SVC() #Assignment\nrf = RandomForestClassifier()\nknn = KNeighborsClassifier()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svc.fit(X_train_sm, y_train_sm) # Traning\nrf.fit(X_train_sm, y_train_sm)\nknn.fit(X_train_sm, y_train_sm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_predsvc = svc.predict(X_test_sm) #Prediction\ny_predrf = rf.predict(X_test_sm)\ny_predknn = knn.predict(X_test_sm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cmsvc = confusion_matrix(y_test_sm , y_predsvc) #Confusion Matrix\ncmrf = confusion_matrix(y_test_sm , y_predrf)\ncmknn = confusion_matrix(y_test_sm , y_predknn)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"part7\"></a>\n# 7.Evaluation:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Accuracy of Logistic Regression: {accuracy_score(y_test_sm, y_predlr2)} Sensitivity: {cmlr2[0,0] / (cmlr2[0,0]+cmlr2[1,0])} Specificity: {cmlr2[1,1] / (cmlr2[1,1]+cmlr2[0,1])}\")\nprint(f\"Accuracy of Support Vector Machine Classifier: {accuracy_score(y_test_sm, y_predsvc)} Sensitivity: {cmsvc[0,0] / (cmsvc[0,0]+cmsvc[1,0])} Specificity: {cmsvc[1,1] / (cmsvc[1,1]+cmsvc[0,1])}\")\nprint(f\"Accuracy of Random Forest Classifier: {accuracy_score(y_test_sm, y_predrf)} Sensitivity: {cmrf[0,0] / (cmrf[0,0]+cmrf[1,0])} Specificity: {cmrf[1,1] / (cmrf[1,1]+cmrf[0,1])}\")\nprint(f\"Accuracy of KNeighbor Classifier: {accuracy_score(y_test_sm, y_predknn)} Sensitivity: {cmknn[0,0] / (cmknn[0,0]+cmknn[1,0])} Specificity: {cmknn[1,1] / (cmknn[1,1]+cmknn[0,1])}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Without any hyperparameter tuning random forest classifier gave us best results. Thanks for your time. What you think about dataset and my analyze? If you have any questions or suggestions let me know. Have a nice day.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}