{"cells":[{"metadata":{},"cell_type":"markdown","source":"# 1. Goodbooks-10k \n- Link : https://www.kaggle.com/zygmunt/goodbooks-10k"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport plotnine \nfrom plotnine import *\nimport os, sys, gc\nfrom tqdm.notebook import tqdm\nimport warnings \nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"path = '../input/t-academy-recommendation2/books/'\nprint(os.listdir(path))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"books = pd.read_csv(path + \"books.csv\")\nbook_tags = pd.read_csv(path + \"book_tags.csv\")\ntrain = pd.read_csv(path + \"train.csv\")\ntest = pd.read_csv(path + \"test.csv\")\ntags = pd.read_csv(path + \"tags.csv\")\nto_read = pd.read_csv(path + \"to_read.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['book_id'] = train['book_id'].astype(str)\ntest['book_id'] = test['book_id'].astype(str)\nbooks['book_id'] = books['book_id'].astype(str)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"popular_rec_model = books.sort_values(by='books_count', ascending=False)['book_id'].values[0:500]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sol = test.groupby(['user_id'])['book_id'].agg({'unique'}).reset_index()\ngt = {}\nfor user in tqdm(sol['user_id'].unique()): \n    gt[user] = list(sol[sol['user_id'] == user]['unique'].values[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rec_df = pd.DataFrame()\nrec_df['user_id'] = train['user_id'].unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## TF-IDF를 이용한 Contents Based Model "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\ntfidf = TfidfVectorizer(stop_words='english')\ntfidf_matrix = tfidf.fit_transform(books['title'])\nprint(tfidf_matrix.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics.pairwise import cosine_similarity\ncosine_matrix = cosine_similarity(tfidf_matrix, tfidf_matrix)\ncosine_matrix.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# book title와 id를 매핑할 dictionary를 생성해줍니다. \nbook2id = {}\nfor i, c in enumerate(books['title']): book2id[i] = c\n\n# id와 book title를 매핑할 dictionary를 생성해줍니다. \nid2book = {}\nfor i, c in book2id.items(): id2book[c] = i\n    \n# book_id와 title를 매핑할 dictionary를 생성해줍니다.\nbookid2book = {}\nfor i, j in zip(books['title'].values, books['book_id'].values):\n    bookid2book[i] = j","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"books['title'].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"idx = id2book['Twilight (Twilight, #1)']  \nsim_scores = [(book2id[i], c) for i, c in enumerate(cosine_matrix[idx]) if i != idx] \nsim_scores = sorted(sim_scores, key = lambda x: x[1], reverse=True)\nsim_scores[0:10] ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"0. 학습셋에서 제목이 있는 경우에 대해서만 진행\n1. 각 유저별로 읽은 책의 목록을 수집 \n2. 읽은 책과 유사한 책 추출 \n3. 모든 책에 대해서 유사도를 더한 값을 계산 \n4. 3에서 유사도가 가장 높은 순서대로 추출 "},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.merge(train, books[['book_id', 'title']], how='left', on='book_id')\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 0. 학습셋에서 제목이 있는 경우에 대해서만 진행\ntf_train = train[train['title'].notnull()].reset_index(drop=True)\ntf_train['idx2title'] = tf_train['title'].apply(lambda x: id2book[x])\ntf_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"idx2title2book = {}\nfor i, j in zip(tf_train['idx2title'].values, tf_train['book_id'].values):\n    idx2title2book[i] = j","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 1. 각 유저별로 읽은 책의 목록을 수집 \nuser = 7\nread_list = tf_train.groupby(['user_id'])['idx2title'].agg({'unique'}).reset_index()\nseen = read_list[read_list['user_id'] == user]['unique'].values[0]\nseen","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 2. 읽은 책과 유사한 책 추출 \n## 343번째 책과 다른 책들간의 유사도 \ncosine_matrix[343]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 2. 읽은 책과 유사한 책 추출 \ntotal_cosine_sim = np.zeros(len(book2id))\nfor book_ in seen: \n    # 3. 모든 책에 대해서 유사도를 더한 값을 계산 \n    # 343번째 책과 248의 유사도가 모두 결합된 유사도\n    total_cosine_sim += cosine_matrix[book_]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 4. 3에서 유사도가 가장 높은 순서대로 추출\nsim_scores = [(i, c) for i, c in enumerate(total_cosine_sim) if i not in seen] # 자기 자신을 제외한 영화들의 유사도 및 인덱스를 추출 \nsim_scores = sorted(sim_scores, key = lambda x: x[1], reverse=True) # 유사도가 높은 순서대로 정렬 \nsim_scores[0:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"book2id[4809]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bookid2book[book2id[4809]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tf_train['user_id'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tf_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## 전체 영화에 대해서 진행 \ntotal_rec_list = {}\n\nread_list1 = train.groupby(['user_id'])['book_id'].agg({'unique'}).reset_index()\nread_list2 = tf_train.groupby(['user_id'])['idx2title'].agg({'unique'}).reset_index()\n\nfor user in tqdm(train['user_id'].unique()):\n    rec_list = []\n        \n    # 만약 TF-IDF 소속의 추천대상이라면 Contents 기반의 추천 \n    if user in tf_train['user_id'].unique():\n        # 1. 각 유저별로 읽은 책의 목록을 수집 \n        seen = read_list2[read_list2['user_id'] == user]['unique'].values[0]\n        # 2. 읽은 책과 유사한 책 추출 \n        total_cosine_sim = np.zeros(len(book2id))\n        for book_ in seen: \n            # 3. 모든 책에 대해서 유사도를 더한 값을 계산 \n            # 343번째 책과 248의 유사도가 모두 결합된 유사도\n            total_cosine_sim += cosine_matrix[book_]\n            \n        # 4. 3에서 유사도가 가장 높은 순서대로 추출\n        sim_scores = [(bookid2book[book2id[i]], c) for i, c in enumerate(total_cosine_sim) if i not in seen] # 자기 자신을 제외한 영화들의 유사도 및 인덱스를 추출 \n        recs = sorted(sim_scores, key = lambda x: x[1], reverse=True)[0:300] # 유사도가 높은 순서대로 정렬 \n        for rec in recs: \n            if rec not in seen:\n                rec_list.append(rec)   \n        \n    # 그렇지 않으면 인기도 기반의 추천 \n    else: \n        seen = read_list1[read_list1['user_id'] == user]['unique'].values[0]\n        for rec in popular_rec_model[0:400]:\n            if rec not in seen:\n                rec_list.append(rec)\n                \n    total_rec_list[user] = rec_list[0:200]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import six\nimport math\n\n# https://github.com/kakao-arena/brunch-article-recommendation/blob/master/evaluate.py\n\nclass evaluate():\n    def __init__(self, recs, gt, topn=100):\n        self.recs = recs\n        self.gt = gt \n        self.topn = topn \n        \n    def _ndcg(self):\n        Q, S = 0.0, 0.0\n        for u, seen in six.iteritems(self.gt):\n            seen = list(set(seen))\n            rec = self.recs.get(u, [])\n            if not rec or len(seen) == 0:\n                continue\n\n            dcg = 0.0\n            idcg = sum([1.0 / math.log(i + 2, 2) for i in range(min(len(seen), len(rec)))])\n            for i, r in enumerate(rec):\n                if r not in seen:\n                    continue\n                rank = i + 1\n                dcg += 1.0 / math.log(rank + 1, 2)\n            ndcg = dcg / idcg\n            S += ndcg\n            Q += 1\n        return S / Q\n\n\n    def _map(self):\n        n, ap = 0.0, 0.0\n        for u, seen in six.iteritems(self.gt):\n            seen = list(set(seen))\n            rec = self.recs.get(u, [])\n            if not rec or len(seen) == 0:\n                continue\n\n            _ap, correct = 0.0, 0.0\n            for i, r in enumerate(rec):\n                if r in seen:\n                    correct += 1\n                    _ap += (correct / (i + 1.0))\n            _ap /= min(len(seen), len(rec))\n            ap += _ap\n            n += 1.0\n        return ap / n\n\n\n    def _entropy_diversity(self):\n        sz = float(len(self.recs)) * self.topn\n        freq = {}\n        for u, rec in six.iteritems(self.recs):\n            for r in rec:\n                freq[r] = freq.get(r, 0) + 1\n        ent = -sum([v / sz * math.log(v / sz) for v in six.itervalues(freq)])\n        return ent\n    \n    def _evaluate(self):\n        print('MAP@%s: %s' % (self.topn, self._map()))\n        print('NDCG@%s: %s' % (self.topn, self._ndcg()))\n        print('EntDiv@%s: %s' % (self.topn, self._entropy_diversity()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"evaluate_func = evaluate(recs=total_rec_list, gt = gt, topn=200)\nevaluate_func._evaluate()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" ## Word2vec을 이용한 추천시스템 \n - Tag간의 유사도 \n - 제목간의 유사도 \n - 책의 읽은 순서를 통한 유사도 "},{"metadata":{"trusted":true},"cell_type":"code","source":"agg = train.groupby(['user_id'])['book_id'].agg({'unique'})\nagg.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# int형식은 Word2vec에서 학습이 안되어서 String으로 변경해줍니다. \nsentence = []\nfor user_sentence in agg['unique'].values:\n    sentence.append(list(map(str, user_sentence)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Word2vec의 학습을 진행해줍니다. \nfrom gensim.models import Word2Vec\nembedding_model = Word2Vec(sentence, size=20, window = 5, \n                           min_count=1, workers=4, iter=200, sg=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_model.wv.most_similar(positive=['4893'], topn=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## 전체 영화에 대해서 진행 \ntotal_rec_list = {}\n\nread_list = train.groupby(['user_id'])['book_id'].agg({'unique'}).reset_index()\nfor user in tqdm(train['user_id'].unique()):\n    rec_list = []     \n    seen = read_list1[read_list1['user_id'] == user]['unique'].values[0]\n    word2vec_dict = {}\n    for book in seen: \n        for i in embedding_model.wv.most_similar(positive=[book], topn=300):\n            if i[0] not in seen: \n                if i[0] not in word2vec_dict.keys(): \n                    word2vec_dict[i[0]] = i[1]\n                else:\n                    word2vec_dict[i[0]] += i[1]\n                \n    rec_list = list(dict(sorted(word2vec_dict.items(), key = lambda x: x[1], reverse=True)).keys())\n    total_rec_list[user] = rec_list[0:200]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"evaluate_func = evaluate(recs=total_rec_list, gt = gt, topn=200)\nevaluate_func._evaluate()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 태그를 통한 유사도 계산 "},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"book_tags.columns = ['book_id', 'tag_id', 'count']\nbook_tags['book_id'] = book_tags['book_id'].astype(str)\nbook_tags['tag_id'] = book_tags['tag_id'].astype(str)\n\ntags['tag_id'] = tags['tag_id'].astype(str)\n\nbook_tags = pd.merge(book_tags, tags, how='left', on='tag_id')\nbook_tags.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"agg = book_tags.groupby(['book_id'])['tag_name'].agg({'unique'}).reset_index()\nagg.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 태그간의 유사도 계산 \n# int형식은 Word2vec에서 학습이 안되어서 String으로 변경해줍니다. \nsentence = []\nfor user_sentence in agg['unique'].values:\n    sentence.append(list(map(str, user_sentence)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from gensim.models import doc2vec\ndoc_vectorizer = doc2vec.Doc2Vec(\n    dm=0,            # PV-DBOW / default 1\n    dbow_words=1,    # w2v simultaneous with DBOW d2v / default 0\n    window=10,        # distance between the predicted word and context words\n    size=100,        # vector size\n    alpha=0.025,     # learning-rate\n    seed=1234,\n    min_count=5,    # ignore with freq lower\n    min_alpha=0.025, # min learning-rate\n    workers=4,   # multi cpu\n    hs = 1,          # hierar chical softmax / default 0\n    negative = 10   # negative sampling / default 5\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import namedtuple\n\nTaggedDocument = namedtuple('TaggedDocument', 'words tags')\ntagged_train_docs = [TaggedDocument(c, [d]) for c, d in agg[['unique', 'book_id']].values]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"doc_vectorizer.build_vocab(tagged_train_docs)\nprint(str(doc_vectorizer))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 벡터 문서 학습\nfrom time import time\n\nstart = time()\n\nfor epoch in tqdm(range(5)):\n    doc_vectorizer.train(tagged_train_docs, total_examples=doc_vectorizer.corpus_count, epochs=doc_vectorizer.iter)\n    doc_vectorizer.alpha -= 0.002 # decrease the learning rate\n    doc_vectorizer.min_alpha = doc_vectorizer.alpha # fix the learning rate, no decay\n\n#doc_vectorizer.train(tagged_train_docs, total_examples=doc_vectorizer.corpus_count, epochs=doc_vectorizer.iter)\nend = time()\nprint(\"During Time: {}\".format(end-start))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"doc_vectorizer.docvecs.most_similar('1', topn=20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# tag 정보가 있는 책이 있고 아닌 책이 있어서 해당 책만 추출 \nagg['type'] = '1'\ntrain = pd.merge(train, agg, how='left', on='book_id')","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"## 전체 영화에 대해서 진행 \ntotal_rec_list = {}\n\nread_list1 = train.groupby(['user_id'])['book_id'].agg({'unique'}).reset_index()\nread_list2 = train[train['type'] == '1'].groupby(['user_id'])['book_id'].agg({'unique'}).reset_index()\nfor user in tqdm(train['user_id'].unique()):\n    rec_list = []\n    if user in read_list2['user_id'].unique():\n        seen = read_list2[read_list2['user_id'] == user]['unique'].values[0]\n        doc2vec_dict = {}\n        for book in seen: \n            for i in doc_vectorizer.docvecs.most_similar(positive=[book], topn=300): \n                if i[0] not in doc2vec_dict.keys(): \n                    doc2vec_dict[i[0]] = i[1]\n                else:\n                    doc2vec_dict[i[0]] += i[1]\n\n        rec_list = list(dict(sorted(doc2vec_dict.items(), key = lambda x: x[1], reverse=True)).keys())\n    else:\n        \n        seen = read_list1[read_list1['user_id'] == user]['unique'].values[0]\n        for rec in popular_rec_model[0:300]:\n            if rec not in seen:\n                rec_list.append(rec)\n    total_rec_list[user] = rec_list[0:200]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"evaluate_func = evaluate(recs=total_rec_list, gt = gt, topn=200)\nevaluate_func._evaluate()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}