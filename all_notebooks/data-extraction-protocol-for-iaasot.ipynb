{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install geocoder\n!pip install bs4","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Extraction Protocol for Iron Age Archaeological Sites of Turkey Dataset","metadata":{}},{"cell_type":"code","source":"# packages\nfrom bs4 import BeautifulSoup\nimport json\nimport pandas as pd\nimport geocoder","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def read_html(p: str):\n    \"\"\n    with open(p, \"r\", encoding=\"utf-8\") as f:\n        return BeautifulSoup(f, \"html\")\n    \ndef save_json(p: str, o):\n    with open(p, \"w\", encoding=\"utf-8\") as f:\n        jf = json.dumps(o, ensure_ascii=False, indent=2)\n        f.write(jf)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"page = read_html(\"taydata.html\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"retrieval_date = \"2021-03-21\"\nupdate_date = \"2021-01-01\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_section(p, section_id: str):\n    \"\"\n    return p.find(\"section\", id=section_id)\n\ndef get_date_range(el):\n    \"get date range from element\"\n    metadata = el.find(class_=\"metadata\")\n    date_start = metadata.find(\"span\", class_=\"date-start\").get_text(strip=True)\n    date_end = metadata.find(\"span\", class_=\"date-end\").get_text(strip=True)\n    return {\n        \"date-start\": int(date_start) if date_start else None,\n        \"date-end\": int(date_end) if date_end else None,\n        \"period-name\": el[\"id\"].replace(\"-data\", \"\")\n           }\n\ndef get_section_date_range(page, sid):\n    section = get_section(page, sid)\n    return get_date_range(section)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_data_section(el):\n    return el.find(\"section\", class_=\"data\")\n\ndef extract_id_no(el) -> str:\n    link = el.find(\"a\", href=lambda x: \"CagNo\" in x)\n    href = link[\"href\"]\n    idno = href.find(\"CagNo\")\n    nhref = href[idno:]\n    ampersand = nhref.find(\"&\")\n    nhref = nhref[:ampersand]\n    nhref = nhref.replace(\"CagNo=\", \"\")\n    return nhref\n\ndef extract_info(el) -> dict:\n    \"extract region city and administrative division from element\"\n    font_txt = el.find(\"font\", size=1).get_text(strip=True)\n    infos_lst = [f.strip() for f in font_txt.split(\"-\") if f]\n    return {\n        \"region\": infos_lst.pop(),\n        \"city\": infos_lst.pop(),\n        \"administrative-division\": infos_lst.pop(),\n        \"research-status\": infos_lst.pop(),\n        \"site-type\": infos_lst.pop()\n    }\n\ndef info_tpl(info):\n    \"\"\n    return (\n        (\"region\", info[\"region\"]), \n        (\"city\", info[\"city\"]),\n        (\"administrative-division\", info[\"administrative-division\"]),\n        (\"research-status\", info[\"research-status\"]),\n        (\"site-type\", info[\"site-type\"])\n    )\n\ndef date_range_tpl(date_range):\n    return ((\"date-start\", date_range[\"date-start\"]), \n            (\"date-end\", date_range[\"date-end\"]),\n           (\"period-name\", date_range[\"period-name\"]))\n\ndef take_subrows(trows: list, counter: int) -> list:\n    return trows[counter:counter+3]\n\ndef rearrange_subrow(subrow: list) -> tuple:\n    \"rearrange subrows\"\n    idn = extract_id_no(subrow[0])\n    info = extract_info(subrow[1])\n    return idn, info\n\ndef get_table_rows(data_section):\n    \"find table and take a list of its rows\"\n    table_rows = data_section.find_all(\"tr\")\n    data = []\n    counter = 0\n    while counter < len(table_rows):\n        subrows = take_subrows(table_rows, counter)\n        counter+=3\n        data.append(subrows)\n    return data\n\ndef arrange_data(page, sid: str, data):\n    \"arrange data for given section\"\n    section = get_section(page, sid)\n    date_range = get_date_range(section)\n    data_section = get_data_section(section)\n    rows = get_table_rows(data_section)\n    for subrow in rows:\n        data_id, info = rearrange_subrow(subrow)\n        if data_id not in data:\n            data[data_id] = {\"active-dates\": [date_range_tpl(date_range)], \n                             \"infos\": [info_tpl(info)]}\n        else:\n            adates = set(data[data_id][\"active-dates\"])\n            infos = set(data[data_id][\"infos\"])\n            infotpl = info_tpl(info)\n            date_tpl = date_range_tpl(date_range)\n            adates.add(date_tpl)\n            infos.add(infotpl)\n            data[data_id][\"active-dates\"] = list(adates)\n            data[data_id][\"infos\"] = list(infos)     \n    return\n\ndef extract_data(page):\n    \"\"\n    section_ids = [f[\"id\"] for f in page.find_all(\"section\", id=True)]\n    data = {}\n    for sid in section_ids:\n        arrange_data(page, sid, data)\n    for d, vs in data.items():\n        vs[\"infos\"] = vs[\"infos\"].pop()\n    d = {}\n    d[\"data\"] = data\n    d[\"metadata\"] = {\n        \"date-range-assumptions\": {\n            sid.replace(\"-data\",\"\"): get_section_date_range(page,\n                                                            sid) for sid in section_ids\n        }\n    }\n    \n    return d","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = extract_data(page)\ndata[\"metadata\"][\"acquisition-date\"] = retrieval_date\ndata[\"metadata\"][\"tay-update-date\"] = update_date","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"save_json(\"tayraw.json\",data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_info_key(key, data):\n    vals = set()\n    for k, vd in data.items():\n        if \"infos\" not in vd:\n            print(k)\n            print(vd)\n            raise ValueError(\"fffffff\")\n        infos = dict(vd[\"infos\"])\n        vals.add(infos[key])\n    return list(vals)\n\ndef group_by_info_key(key, data):\n    vals = get_info_key(key, data)\n    vdata = {v: [] for v in vals}\n    for k, vd in data.items():\n        info_tpl = vd[\"infos\"]\n        infos = dict(info_tpl)\n        kval = infos[key]\n        vdata[kval].append(k)\n    return vdata\n\ndef get_city(data) -> list:\n    \"\"\n    return group_by_info_key(\"city\", data)\n\ndef get_region(data) -> list:\n    return group_by_info_key(\"region\", data)\n\ndef get_research_status(data) -> list:\n    return group_by_info_key(\"research-status\", data)\n\ndef get_site_type(data) -> list:\n    return group_by_info_key(\"site-type\", data)\n\ndef get_administrative_division(data) -> list:\n    return group_by_info_key(\"administrative-division\", data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"save_json(\"tay_by_city.json\", get_city(data[\"data\"]))\nsave_json(\"tay_by_region.json\", get_region(data[\"data\"]))\nsave_json(\"tay_by_research_status.json\", get_research_status(data[\"data\"]))\nsave_json(\"tay_by_site_type.json\", get_site_type(data[\"data\"]))\nsave_json(\"tay_by_administrative_division.json\", get_administrative_division(data[\"data\"]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# geonames related\ndef addr_from_info(info: dict):\n    addr = \",\".join([info[\"administrative-division\"], \n                     info[\"city\"],\n                    \"TÃ¼rkiye\"])\n    return addr\n\ndef get_geoinfo(info: dict, ads: dict):\n    addr = addr_from_info(info)\n    if addr in ads:\n        return False, addr\n    \n    g = geocoder.osm(addr, maxRows=10)\n    gjson = g.geojson\n    ads[addr] = gjson\n    return True, gjson\n\ndef get_geodata(data):\n    geodata = {}\n    addresses = {}\n    for k, vd in data.items():\n        info = dict(vd[\"infos\"])\n        opcode, k_geodata = get_geoinfo(info, addresses)\n        if opcode is False:\n            geodata[k] = addresses[k_geodata]\n        else:\n            geodata[k] = k_geodata\n    return geodata","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"geodata = get_geodata(data[\"data\"])  # takes a lot of time\nsave_json(\"tay_by_geodata.json\", geodata)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def add_info_to_dfdict(dfdict, info, k):\n    \"\"\n    for ki, kv in info.items():\n        dfdict[ki].append(kv)\n    dfdict[\"id\"].append(k)\n    return dfdict\n    \n\ndef info_dataframe(data):\n    dfdict = {\n        \"region\": [],\n        \"city\": [],\n        \"administrative-division\": [],\n        \"research-status\": [],\n        \"site-type\": [],\n        \"id\": [],\n        \"date-start\": [],\n        \"date-end\": [],\n        \"period-name\": []\n    }\n    for k, vd in data.items():\n        info = dict(vd[\"infos\"])\n        active_dates = vd[\"active-dates\"]\n        for adate in active_dates:\n            ad = dict(adate)\n            add_info_to_dfdict(dfdict, info, k)\n            for a, d in ad.items():\n                dfdict[a].append(d)\n    return pd.DataFrame(dfdict)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfi = info_dataframe(data[\"data\"])\ndfi.to_json(\"tay_by_info.json\", force_ascii=False, indent=2)\ndfi.to_csv(\"tay_by_info.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_group_serie_df(df: pd.DataFrame, gnames: list, condition=None):\n    if condition is None:\n        return df.groupby(gnames)[\"id\"].count()\n    return df[condition].groupby(gnames)[\"id\"].count()\n\ndef get_group_dict(df: pd.DataFrame, gnames=[\"city\", \n                                             \"region\", \n                                             \"research-status\",\n                                            \"site-type\",\n                                            \"administrative-division\",\n                                            \"date-start\",\n                                            \"date-end\",\n                                            \"period-name\"],\n                  condition=None) -> dict:\n    return {gname:get_group_serie_df(df,gname, condition) for gname in gnames}\n\ndef mk_condition(df, s, e, sk=\"date-start\", ek=\"date-end\"):\n    \"get date interval\"\n    return (df[sk] >= s) & (df[ek] <= e)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# all tay places\nseries = get_group_dict(dfi)\n\n# interval -1200 to -700\nseries_1200_700 = get_group_dict(dfi, condition=mk_condition(dfi, s=-1200, e=-700))\n\n# interval -800 to -600\nseries_800_600 = get_group_dict(dfi, condition=mk_condition(dfi, s=-800, e=-600))\n\n# interval -600 to -300\nseries_600_300 = get_group_dict(dfi, condition=mk_condition(dfi, s=-600, e=-300))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def serie_stats(s: pd.Series):\n    serie_stat = s.describe()\n    smax = s.max()    \n    smin = s.min()\n    condmax = (s <= smax) & (s > s.mean())\n    condmin = (s >= smin) & (s < s.mean())\n    smax = s.where(condmax)\n    smin = s.where(condmin)\n    df = pd.DataFrame({\"counts\": s})\n    df[\"rank\"] = None\n    for index, value in s.items():\n        sminval = smin[index]\n        smaxval = smax[index]\n        if pd.notnull(sminval):\n            df.at[index, \"rank\"] = \"below-mean\"\n        elif pd.notnull(smaxval):\n            df.at[index, \"rank\"] = \"above-mean\"\n    d = {}\n    d[\"descriptive-stats\"] = serie_stat.to_dict()\n    d[\"below-mean\"] = s.loc[condmin].to_dict()\n    d[\"above-mean\"] = s.loc[condmax].to_dict()\n    return d, df, serie_stat\n\ndef save_serie_stats(ss: dict, prefix=\"tay_stats_by_\", suffix=\"\", ext=\"json\"):\n    \"save series\"\n    for sk, serie in ss.items():\n        save_name = prefix\n        save_name += sk.replace(\"-\",\"_\")\n        save_name += suffix\n        statd, df, sstat = serie_stats(serie)\n        if ext == \"json\":\n            save_name += \".\" + ext\n            save_json(save_name, statd)\n        else:\n            save_name1 = save_name + \".\" + ext\n            save_name2 = save_name + \"_descriptive\" + \".\" + ext\n            df.to_csv(save_name1)\n            sstat.to_csv(save_name2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"save_serie_stats(series, suffix=\"-forall-periods\", ext=\"csv\")\nsave_serie_stats(series_1200_700, suffix=\"-between-1200-700-bce\", ext=\"csv\")\nsave_serie_stats(series_800_600, suffix=\"-between-800-600-bce\", ext=\"csv\")\nsave_serie_stats(series_600_300, suffix=\"-between-600-300-bce\", ext=\"csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def combine_data(data, geodata):\n    for k, v in geodata.items():\n        data[k][\"geodata\"] = geodata.get(k, None)\n    return data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_data = combine_data(data[\"data\"], geodata)\nsave_json(\"tay_data.json\", all_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# now let's make some historical maps\ndef groupby_active_dates(data,metadata):\n    date_ranges = {}\n    date_name = metadata[\"date-range-assumptions\"]\n    range_to_name = {date_range_tpl(v):k for k,v in date_name.items()}\n    \n    for k, vd in data.items():\n        adates = vd[\"active-dates\"]\n        for adate in adates:\n            if adate not in date_ranges:\n                date_ranges[adate] = [k]\n            else:\n                date_ranges[adate].append(k)\n    return date_ranges\n\n\ndef get_date_geodata(data):\n    date_ranges = groupby_active_dates(data[\"data\"],data[\"metadata\"])\n    date_range_geodata = {}\n    for date_range, keys in date_ranges.items():\n        name = date_range[-1][1]\n        if name not in date_range_geodata:\n            date_range_geodata[name] = {}\n        date_range_geodata[name][\"place-ids\"] = keys\n        date_range_geodata[name][\"date-range\"] = dict(date_range)\n        gdata = {\"type\": \"FeatureCollection\", \"features\": []}\n        for key in keys:\n            key_geodata = data[\"data\"][key][\"geodata\"]\n            key_features = key_geodata[\"features\"]\n            # obtain highest accuracy feature\n            keyf = sorted(key_features, key=lambda x: x[\"properties\"][\"accuracy\"])\n            if keyf:\n                gdata[\"features\"].append(keyf.pop())\n        date_range_geodata[name][\"geodata\"] = gdata\n    return date_range_geodata","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"date_geodata = get_date_geodata(data)\nsave_json(\"tay_by_date_geodata.json\", date_geodata)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def mk_geodata_with_info(data):\n    geo = {\"type\": \"FeatureCollection\", \"features\": []}\n    for key, vdata in data.items():\n        geodata = vdata[\"geodata\"]\n        key_features = geodata[\"features\"]\n        # obtain highest accuracy feature\n        keyf = sorted(key_features, key=lambda x: x[\"properties\"][\"accuracy\"])\n        if keyf:\n            feature = keyf.pop()\n            feature[\"properties\"][\"tay-infos\"] = dict(vdata[\"infos\"])\n            feature[\"properties\"][\"tay-active-dates\"] = [dict(a) for a in vdata[\"active-dates\"]]\n            feature[\"properties\"][\"tay-place-id\"] = key\n            geo[\"features\"].append(feature)\n    return geo","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"geo_data_with_info = mk_geodata_with_info(data[\"data\"])\nsave_json(\"tay_as_geojson.json\", geo_data_with_info)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}