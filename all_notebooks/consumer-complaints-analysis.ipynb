{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport pandas_profiling\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n\n#load packages\nimport sys #access to system parameters https://docs.python.org/3/library/sys.html\nprint(\"Python version: {}\". format(sys.version))\n\nimport pandas as pd #collection of functions for data processing and analysis modeled after R dataframes with SQL like features\nprint(\"pandas version: {}\". format(pd.__version__))\n\nimport matplotlib #collection of functions for scientific and publication-ready visualization\nprint(\"matplotlib version: {}\". format(matplotlib.__version__))\n\nimport numpy as np #foundational package for scientific computing\nprint(\"NumPy version: {}\". format(np.__version__))\n\nimport scipy as sp #collection of functions for scientific computing and advance mathematics\nprint(\"SciPy version: {}\". format(sp.__version__)) \n\nimport IPython\nfrom IPython import display #pretty printing of dataframes in Jupyter notebook\nprint(\"IPython version: {}\". format(IPython.__version__)) \n\nimport sklearn #collection of machine learning algorithms\nprint(\"scikit-learn version: {}\". format(sklearn.__version__))\n\n#misc libraries\nimport random\nimport time\n\n\n#ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')\nprint('-'*25)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Common Model Algorithms\nfrom sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\nfrom xgboost import XGBClassifier\n\n#Common Model Helpers\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn import feature_selection\nfrom sklearn import model_selection\nfrom sklearn import metrics\n\n#Visualization\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport matplotlib.pylab as pylab\nimport seaborn as sns\n\n\n#Configure Visualization Defaults\n#%matplotlib inline = show plots in Jupyter Notebook browser\n%matplotlib inline\nmpl.style.use('ggplot')\nsns.set_style('white')\npylab.rcParams['figure.figsize'] = 12,8","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_data = pd.read_csv(\"/kaggle/input/us-consumer-finance-complaints/consumer_complaints.csv\")\nraw_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_data.sample(5,random_state=89)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##find missing values\nraw_data.isnull().mean().round(4)*100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## We only want to analyze the data that has actual text complaints included, so i'm going to remove all data without that information in the column\n\nraw_data.dropna(subset = [\"consumer_complaint_narrative\"], inplace=True)\n\n##find missing values Now we can see we have no nulls in that specific column\nraw_data.isnull().mean().round(4)*100\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make the index Product, so that we can use it as the dependent variable\n\nraw_data2 = raw_data[['product', 'consumer_complaint_narrative']].copy()\n\n\nraw_data3 = raw_data2.set_index('product')\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_data3\n\nraw_data3.consumer_complaint_narrative['Mortgage']\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# Apply a first round of text cleaning techniques\nimport re\nimport string\n\ndef clean_text_round1(text):\n    '''Make text lowercase, remove text in square brackets, remove punctuation and remove words containing numbers.'''\n    text = text.lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text\n\nround1 = lambda x: clean_text_round1(x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's take a look at the updated text\nraw_data4 = pd.DataFrame(raw_data3.consumer_complaint_narrative.apply(round1))\nraw_data4","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# now everything is lowercase, and there is less punctuation\nraw_data4.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# Apply a second round of cleaning\ndef clean_text_round2(text):\n    '''Get rid of some additional punctuation and non-sensical text that was missed the first time around.'''\n    text = re.sub('[‘’“”…]', '', text)\n    text = re.sub('\\n', '', text)\n    return text\n\nround2 = lambda x: clean_text_round2(x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nraw_data5 = pd.DataFrame(raw_data4.consumer_complaint_narrative.apply(round2))\n\nraw_data5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pickle\n\n# Let's pickle it for later use\nraw_data5.to_pickle(\"corpus.pkl\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##Since there are so many rows in this data, I want to only show the unique products, therefore I will join and concatenate all the consumer complaints into one cell essentially of data\n\nraw_data6 = raw_data5.groupby(['product']).agg(lambda col: ','.join(col))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# We are going to create a document-term matrix using CountVectorizer, and exclude common English stop words\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ncv = CountVectorizer(stop_words='english')\ndata_cv = cv.fit_transform(raw_data6.consumer_complaint_narrative)\ndata_dtm = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names())\ndata_dtm.index = raw_data6.index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_dtm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_dtm.to_pickle(\"dtm1.pkl\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's also pickle the cleaned data (before we put it in document-term matrix format) and the CountVectorizer object\nraw_data5.to_pickle('data_clean.pkl')\npickle.dump(cv, open(\"cv.pkl\", \"wb\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Read in the document-term matrix\nimport pandas as pd \nimport pickle\n\ndata = pd.read_pickle('dtm1.pkl')\ndata = data.transpose()\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Find the top 30 words said by each product complaint\ntop_dict = {}\nfor c in data.columns:\n    top = data[c].sort_values(ascending=False).head(30)\n    top_dict[c]= list(zip(top.index, top.values))\n\ntop_dict\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# Print the top 15 words said by each product complaint\nfor product, top_words in top_dict.items():\n    print(product)\n    print(', '.join([word for word, count in top_words[0:14]]))\n    print('---')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# by looking at these top words, you can see that some of them have very little meaning and could be added to a stop words list, so let's do just that.\n# Look at the most common top words --> add them to the stop word list\n\nfrom collections import Counter\n\n# Let's first pull out the top 30 words for each comedian\nwords = []\nfor product in data.columns:\n    top = [word for (word, count) in top_dict[product]]\n    for t in top:\n        words.append(t)\n        \nwords","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's aggregate this list and identify the most common words along with how many routines they occur in\nCounter(words).most_common()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# If more than 6 of the products have it as a top word, exclude it from the list\nadd_stop_words = [word for word, count in Counter(words).most_common() if count > 6]\nadd_stop_words","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# Let's make some word clouds!\n\nfrom wordcloud import WordCloud\n\nwc = WordCloud(background_color=\"white\", colormap=\"Dark2\",\n               max_font_size=150, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reset the output dimensions\nimport matplotlib.pyplot as plt\n\nplt.rcParams['figure.figsize'] = [16, 6]\n\nfull_names = raw_data6.index\n\n\n# Create subplots for each comedian\nfor index, product in enumerate(data.columns):\n    wc.generate(raw_data6.consumer_complaint_narrative[product])\n    \n    plt.subplot(3, 4, index+1)\n    plt.imshow(wc, interpolation=\"bilinear\")\n    plt.axis(\"off\")\n    plt.title(full_names[index])\n    \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Find the number of unique words that each product\n\n# Identify the non-zero items in the document-term matrix, meaning that the word occurs at least once\nunique_list = []\nfor product in data.columns:\n    uniques = data[product].nonzero()[0].size\n    unique_list.append(uniques)\n\n# Create a new dataframe that contains this unique word count\ndata_words = pd.DataFrame(list(zip(full_names, unique_list)), columns=['product', 'unique_words'])\ndata_unique_sort = data_words.sort_values(by='unique_words')\ndata_unique_sort","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"total_list = []\nfor product in data.columns:\n    totals = sum(data[product])\n    total_list.append(totals)\n    \nprint(total_list)\n\n\ndata_words['total_words'] = total_list","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_words","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Unique words per product\nsns.catplot(x=\"product\",y=\"unique_words\",kind='bar',data=data_words, height = 10, aspect = 2.25)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Total words per product\nsns.catplot(x=\"product\",y=\"total_words\",kind='bar',data=data_words, height = 10, aspect = 2.25, legend = True, legend_out = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# We'll start by reading in the corpus, which preserves word order\nimport pandas as pd\n\ndata = pd.read_pickle('corpus.pkl')\ndata","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_data6.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# Create quick lambda functions to find the polarity and subjectivity of each routine\n# Terminal / Anaconda Navigator: conda install -c conda-forge textblob\nfrom textblob import TextBlob\n\npol = lambda x: TextBlob(x).sentiment.polarity\nsub = lambda x: TextBlob(x).sentiment.subjectivity\n\nraw_data6['polarity'] = raw_data6['consumer_complaint_narrative'].apply(pol)\nraw_data6['subjectivity'] = raw_data6['consumer_complaint_narrative'].apply(sub)\nraw_data6","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"full_names","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.scatterplot(x='polarity', y='subjectivity', hue = full_names, data=data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# placeholder for sentiment analysis over time","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Topic Modeling\n\ndata_dtm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Import the necessary modules for LDA with gensim\n# Terminal / Anaconda Navigator: conda install -c conda-forge gensim\nfrom gensim import matutils, models\nimport scipy.sparse\n\n# import logging\n# logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# One of the required inputs is a term-document matrix\ntdm = data_dtm.transpose()\ntdm.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#We're going to put the term-document matrix into a new gensim format, from df --> sparse matrix --> gensim corpus\nsparse_counts = scipy.sparse.csr_matrix(tdm)\ncorpus = matutils.Sparse2Corpus(sparse_counts)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Gensim also requires dictionary of the all terms and their respective location in the term-document matrix\ncv = pickle.load(open(\"cv.pkl\", \"rb\"))\nid2word = dict((v, k) for k, v in cv.vocabulary_.items())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now that we have the corpus (term-document matrix) and id2word (dictionary of location: term),\n# we need to specify two other parameters as well - the number of topics and the number of passes\nlda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=2, passes=10)\nlda.print_topics()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# LDA for num_topics = 3\nlda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=3, passes=10)\nlda.print_topics()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# LDA for num_topics = 4\nlda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=4, passes=10)\nlda.print_topics()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's create a function to pull out nouns from a string of text\nfrom nltk import word_tokenize, pos_tag\n\ndef nouns(text):\n    '''Given a string of text, tokenize the text and pull out only the nouns.'''\n    is_noun = lambda pos: pos[:2] == 'NN'\n    tokenized = word_tokenize(text)\n    all_nouns = [word for (word, pos) in pos_tag(tokenized) if is_noun(pos)] \n    return ' '.join(all_nouns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_data6","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's create a function to pull out nouns from a string of text\ndef nouns_adj(text):\n    '''Given a string of text, tokenize the text and pull out only the nouns and adjectives.'''\n    is_noun_adj = lambda pos: pos[:2] == 'NN' or pos[:2] == 'JJ'\n    tokenized = word_tokenize(text)\n    nouns_adj = [word for (word, pos) in pos_tag(tokenized) if is_noun_adj(pos)] \n    return ' '.join(nouns_adj)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a new document-term matrix using only nouns and adjectives, also remove common words with max_df\ncvna = CountVectorizer(stop_words=stop_words, max_df=.8)\ndata_cvna = cvna.fit_transform(data_nouns_adj.transcript)\ndata_dtmna = pd.DataFrame(data_cvna.toarray(), columns=cvna.get_feature_names())\ndata_dtmna.index = data_nouns_adj.index\ndata_dtmna","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_dtm","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}