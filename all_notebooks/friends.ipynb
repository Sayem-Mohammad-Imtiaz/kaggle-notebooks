{"cells":[{"metadata":{"_kg_hide-input":true,"trusted":true,"_kg_hide-output":false},"cell_type":"code","source":"Image(\"../input/giffyy/giphy.gif\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**When you watch friends, you just don't watch it, you live it. Every character has its own way to be funny. \n\nThis note book is my first experiement in NLP.**"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"from IPython.display import Image\nimport os\n!ls ../input/","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom pandas import DataFrame","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"train = pd.read_csv('../input/friends-transcript/friends_quotes.csv')","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"One of the most basic features we can extract is the number of words in each quote. The basic intuition behind this is that generally, the negative sentiments contain a lesser amount of words than the positive ones."},{"metadata":{"trusted":true},"cell_type":"code","source":"train['word_count'] = train['quote'].apply(lambda x: len(str(x).split(\" \")))\ntrain[['quote','word_count']].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Number of characters\nThis feature is also based on the previous feature intuition. Here, we calculate the number of characters in each tweet. This is done by calculating the length of the quote."},{"metadata":{"trusted":true},"cell_type":"code","source":"train['char_count'] = train['quote'].str.len() ## this also includes spaces\ntrain[['quote','char_count']].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Average Word Length\nWe will also extract another feature which will calculate the average word length of each quote. This can also potentially help us in improving our model.\n\nHere, we simply take the sum of the length of all the words and divide it by the total length of the quote:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def avg_word(sentence):\n  words = sentence.split()\n  return (sum(len(word) for word in words)/len(words))\n\ntrain['avg_word'] = train['episode_title'].apply(lambda x: avg_word(x))\ntrain[['quote','avg_word']].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Number of stopwords\nGenerally, while solving an NLP problem, the first thing we do is to remove the stopwords. But sometimes calculating the number of stopwords can also give us some extra information which we might have been losing before.\n\nHere, we have imported stopwords from NLTK, which is a basic NLP library in python."},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.corpus import stopwords\nstop = stopwords.words('english')\n\ntrain['stopwords'] = train['quote'].apply(lambda x: len([x for x in x.split() if x in stop]))\ntrain[['quote','stopwords']].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Number of special characters\nOne more interesting feature which we can extract from a tweet is calculating the number of hashtags or mentions present in it. This also helps in extracting extra information from our text data.\n\nHere, we make use of the ‘starts with’ function because hashtags (or mentions) always appear at the beginning of a word."},{"metadata":{"trusted":true},"cell_type":"code","source":"train['hastags'] = train['quote'].apply(lambda x: len([x for x in x.split() if x.startswith('#')]))\ntrain[['quote','hastags']].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Number of numerics\nJust like we calculated the number of words, we can also calculate the number of numerics which are present in the tweets. It does not have a lot of use in our example, but this is still a useful feature that should be run while doing similar exercises. For example, "},{"metadata":{"trusted":true},"cell_type":"code","source":"train['numerics'] = train['quote'].apply(lambda x: len([x for x in x.split() if x.isdigit()]))\ntrain[['quote','numerics']].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Number of Uppercase words\nAnger or rage is quite often expressed by writing in UPPERCASE words which makes this a necessary operation to identify those words."},{"metadata":{"trusted":true},"cell_type":"code","source":"train['upper'] = train['quote'].apply(lambda x: len([x for x in x.split() if x.isupper()]))\ntrain[['quote','upper']].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lower case\nThe first pre-processing step which we will do is transform our tweets into lower case. This avoids having multiple copies of the same words. For example, while calculating the word count, ‘Analytics’ and ‘analytics’ will be taken as different words."},{"metadata":{"trusted":true},"cell_type":"code","source":"train['quote'] = train['quote'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\ntrain['quote'].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Removing Punctuation\nThe next step is to remove punctuation, as it doesn’t add any extra information while treating text data. Therefore removing all instances of it will help us reduce the size of the training data."},{"metadata":{"trusted":true},"cell_type":"code","source":"train['quote'] = train['quote'].str.replace('[^\\w\\s]','')\ntrain['quote'].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Removal of Stop Words\nAs we discussed earlier, stop words (or commonly occurring words) should be removed from the text data. For this purpose, we can either create a list of stopwords ourselves or we can use predefined libraries."},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.corpus import stopwords\nstop = stopwords.words('english')\ntrain['quote'] = train['quote'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\ntrain['quote'].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Common word removal\nPreviously, we just removed commonly occurring words in a general sense. We can also remove commonly occurring words from our text data First, let’s check the 10 most frequently occurring words in our text data then take call to remove or retain."},{"metadata":{"trusted":true},"cell_type":"code","source":"freq = pd.Series(' '.join(train['quote']).split()).value_counts()[:10]\nfreq","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, let’s remove these words as their presence will not of any use in classification of our text data."},{"metadata":{"trusted":true},"cell_type":"code","source":"freq = list(freq.index)\ntrain['quote'] = train['quote'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\ntrain['quote'].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" Rare words removal\nSimilarly, just as we removed the most common words, this time let’s remove rarely occurring words from the text. Because they’re so rare, the association between them and other words is dominated by noise. You can replace rare words with a more general form and then this will have higher counts"},{"metadata":{"trusted":true},"cell_type":"code","source":"freq = pd.Series(' '.join(train['quote']).split()).value_counts()[-10:]\nfreq","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"freq = list(freq.index)\ntrain['quote'] = train['quote'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\ntrain['quote'].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Spelling check"},{"metadata":{"trusted":true},"cell_type":"code","source":"from textblob import TextBlob\ntrain['quote'][:5].apply(lambda x: str(TextBlob(x).correct()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TextBlob(train['quote'][1]).words","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Steaming"},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.stem import PorterStemmer\nst = PorterStemmer()\ntrain['quote'][:5].apply(lambda x: \" \".join([st.stem(word) for word in x.split()]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lemmatization"},{"metadata":{"trusted":true},"cell_type":"code","source":"from textblob import Word\ntrain['quote'] = train['quote'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\ntrain['quote'].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"-ngrams\nN-grams are the combination of multiple words used together. Ngrams with N=1 are called unigrams. Similarly, bigrams (N=2), trigrams (N=3) and so on can also be used."},{"metadata":{"trusted":true},"cell_type":"code","source":"TextBlob(train['quote'][0]).ngrams(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"term frequency\nTerm frequency is simply the ratio of the count of a word present in a sentence, to the length of the sentence.\n\nTherefore, we can generalize term frequency as:\n\nTF = (Number of times term T appears in the particular row) / (number of terms in that row)"},{"metadata":{"trusted":true},"cell_type":"code","source":"tf1 = (train['quote'][1:2]).apply(lambda x: pd.value_counts(x.split(\" \"))).sum(axis = 0).reset_index()\ntf1.columns = ['words','tf']\ntf1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Inverse Document Frequency\nThe intuition behind inverse document frequency (IDF) is that a word is not of much use to us if it’s appearing in all the documents."},{"metadata":{"trusted":true},"cell_type":"code","source":"for i,word in enumerate(tf1['words']):\n  tf1.loc[i, 'idf'] = np.log(train.shape[0]/(len(train[train['quote'].str.contains(word)])))\n\ntf1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Term Frequency – Inverse Document Frequency (TF-IDF)\nTF-IDF is the multiplication of the TF and IDF which we calculated above."},{"metadata":{"trusted":true},"cell_type":"code","source":"tf1['tfidf'] = tf1['tf'] * tf1['idf']\ntf1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that the TF-IDF has penalized words like ‘don’t’, ‘can’t’, and ‘use’ because they are commonly occurring words. However, it has given a high weight to “disappointed” since that will be very useful in determining the sentiment of the tweet.\n\nWe don’t have to calculate TF and IDF every time beforehand and then multiply it to obtain TF-IDF. Instead, sklearn has a separate function to directly obtain it:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\ntfidf = TfidfVectorizer(max_features=1000, lowercase=True, analyzer='word',\n stop_words= 'english',ngram_range=(1,1))\ntrain_vect = tfidf.fit_transform(train['quote'])\n\ntrain_vect","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Bag of Words\nBag of Words (BoW) refers to the representation of text which describes the presence of words within the text data. The intuition behind this is that two similar text fields will contain similar kind of words, and will therefore have a similar bag of words. Further, that from the text alone we can learn something about the meaning of the document."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nbow = CountVectorizer(max_features=1000, lowercase=True, ngram_range=(1,1),analyzer = \"word\")\ntrain_bow = bow.fit_transform(train['quote'])\ntrain_bow","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Sentiment Analysis\nIf you recall, our problem was to detect the sentiment of the tweet. So, before applying any ML/DL models (which can have a separate feature detecting the sentiment using the textblob library), let’s check the sentiment of the first few tweets."},{"metadata":{"trusted":true},"cell_type":"code","source":"train['quote'][:5].apply(lambda x: TextBlob(x).sentiment)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Above, you can see that it returns a tuple representing polarity and subjectivity of each tweet. Here, we only extract polarity as it indicates the sentiment as value nearer to 1 means a positive sentiment and values nearer to -1 means a negative sentiment. This can also work as a feature for building a machine learning model."},{"metadata":{"trusted":true},"cell_type":"code","source":"train['sentiment'] = train['quote'].apply(lambda x: TextBlob(x).sentiment[0] )\ntrain[['quote','sentiment']].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nSentiment_count=train.groupby('sentiment').count()\nplt.bar(Sentiment_count.index.values, Sentiment_count['quote'])\nplt.xlabel('Review Sentiments')\nplt.ylabel('Number of Review')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#WordCloud\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud, STOPWORDS\nplt.rcParams['font.size']= 15              \nplt.rcParams['savefig.dpi']= 100         \nplt.rcParams['figure.subplot.bottom']= .1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plt.figure(figsize=(15,15))\nstopwords = set(STOPWORDS)\n\nwordcloud = WordCloud(background_color='black', stopwords=stopwords, max_words=2000, max_font_size=80,\n                      random_state=420).generate(str(train['quote']))\nprint(wordcloud)\nfig = plt.figure(1)\nplt.imshow(wordcloud)\nplt.title(\"Friends [WordCloud]\")\nplt.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plt.figure(figsize=(15,15))\nstopwords = set(STOPWORDS)\n\nwordcloud = WordCloud(background_color='black', stopwords=stopwords, max_words=2000, max_font_size=100,\n                      random_state=420).generate(str(train['quote']))\nprint(wordcloud)\nfig = plt.figure(1)\nplt.imshow(wordcloud)\nplt.title(\"Friends <3 [WordCloud]\")\nplt.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}