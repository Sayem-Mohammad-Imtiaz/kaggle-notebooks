{"cells":[{"metadata":{},"cell_type":"markdown","source":"<h3>About the dataset</h3>\nA company called Adright is assigned the task to identify the profile of the typical customer for each treadmill product offered by CardioGood Fitness. The market research team decides to investigate whether there are differences across the product lines with respect to customer characteristics. The team decides to collect data on individuals who purchased a treadmill at a CardioGoodFitness retail store during the prior three months. The data are stored in the CardioGoodFitness.csv file. The team identifies the following customer variables to study: product purchased, TM195, TM498, or TM798; gender; age, in years;education, in years; relationship status, single or partnered; annual household income ($); average number of times the customer plans to use the treadmill each week; average number of miles the customer expects to walk/run each week; and self-rated fitness on an 1-to-5 scale, where 1 is poor shape and 5 is excellent shape.\n\n<br><h3>What we need to predict</h3>\nWe need to predict which product the customer is likely to buy based on several parameters.\n\n<h3>We will do following things in order to come to our conclusion</h3>\n<ol>\n<li>Importing Libraries</li>\n<li>Loading the dataset</li>\n<li>Do some basic EDA</li>\n<li>Model the data</li>\n</ol>\n\n\n\n\n\n"},{"metadata":{},"cell_type":"markdown","source":"<h3>Importing the libraries</h3>"},{"metadata":{"trusted":true},"cell_type":"code","source":"#!pip install category_encoders\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nsns.set(\"notebook\")\nsns.set_style(\"darkgrid\")\nfrom scipy.special import boxcox\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom xgboost import XGBClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.metrics import confusion_matrix,accuracy_score,classification_report\nfrom keras.layers import Dense,Dropout\nfrom keras.models import Sequential\nfrom keras.utils import to_categorical","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>Loading the dataset</h3>"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Reading Data\ndf=pd.read_csv('../input/cardiogoodfitness/CardioGoodFitness.csv')\nprint(\"\\n\")\nprint(\"Few Rows\")\nprint(df.head())\nprint(\"\\n\")\nprint(\"Data Dictionary\")\nprint(df.info())\nprint(\"\\n\")\nprint(\"Descriptive Statistics\")\nprint(df.describe().T)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.crosstab(df['Product'],df['MaritalStatus'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.crosstab(df['Product'],df['Gender'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>Lets do some basic EDA</h3>\nWe will check if the Age is normally distributed."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df['Age'].median())\nsns.distplot(df['Age'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looks like the Age is normally distributed"},{"metadata":{},"cell_type":"markdown","source":"Lets check the distribution of target variable"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(df['Product'])\ndf['Product'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,8))\ncorr=df.corr()\nsns.heatmap(corr,square=True,annot=True,cmap='RdYlGn')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[['Product','Usage']].groupby(['Product'],as_index=False).median().sort_values(by='Product',ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[['Product','Fitness']].groupby(['Product'],as_index=False).median().sort_values(by='Product',ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df['Age'].min())\nprint(df['Age'].max())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>Splitting the age into different groups</h3>"},{"metadata":{},"cell_type":"markdown","source":"<h3>Feature Engineering</h3>\nLets do a bit of Feature Engineering.<br>In this simple scenario we will transform Age into different groups. "},{"metadata":{"trusted":true},"cell_type":"code","source":"category=pd.cut(df['Age'],bins=[17,26,42,50],labels=['Young','Middle','Senior'])\ndf.insert(3,'Age Group',category)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.crosstab(df['Product'],df['Age Group'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>Modelling the data</h3>"},{"metadata":{"trusted":true},"cell_type":"code","source":"X=df.drop(['Product'],axis=1)\ny=df['Product']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X=X.drop(['Age','Education'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>Encoding categorical variables</h3>\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import category_encoders as ce\nencoder=ce.OrdinalEncoder(cols=['Gender','Age Group','MaritalStatus'],return_df=True,verbose=None)\nX=encoder.fit_transform(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Changing the category of categorical variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_cols=['Gender','Age Group','MaritalStatus','Usage','Fitness']\nfor col in X[categorical_cols]:\n    X[col]=X[col].astype('category')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train,X_val,y_train,y_val=train_test_split(X,y,test_size=0.1,random_state=42,shuffle=y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_train.shape)\nprint(X_val.shape)\nprint(y_train.shape)\nprint(y_val.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>Scaling the data</h3>"},{"metadata":{"trusted":true},"cell_type":"code","source":"sc=StandardScaler()\nct=ColumnTransformer([('scaler',sc,[5,6])],remainder='passthrough')\nX_train=ct.fit_transform(X_train)\nX_val=ct.transform(X_val)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have used column transformer here to scale the columns Income and Miles."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train=pd.DataFrame(X_train,columns=['Income','Miles','Gender','Age Group','MaritalStatus','Usage','Fitness'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_val=pd.DataFrame(X_val,columns=['Income','Miles','Gender','Age Group','MaritalStatus','Usage','Fitness'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>We have selected 3 models for our classification task</h3>\n<ol>\n<li>Logistic Regression</li>\n<li>XGBoost</li>\n<li>K Nearest Neighbours</li>\n</ol>    "},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_log=LogisticRegression(C=0.1)\nclf_log.fit(X_train,y_train)\nlog_y_preds=clf_log.predict(X_val)\nprint('Accuracy Score %0.2f'%(100*accuracy_score(y_val,log_y_preds)))\nprint(classification_report(y_val,log_y_preds))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We got a slightly decent 72%"},{"metadata":{"trusted":true},"cell_type":"code","source":"xg_clf=XGBClassifier(n_estimators=120,learning_rate=0.1)\nxg_clf.fit(X_train,y_train)\nxg_y_preds=xg_clf.predict(X_val)\nprint('Accuracy Score %0.2f'%(100*accuracy_score(y_val,xg_y_preds)))\nprint(classification_report(y_val,log_y_preds))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Wow! 6% bump up from logistic regression score"},{"metadata":{"trusted":true},"cell_type":"code","source":"knn_clf=KNeighborsClassifier(n_neighbors=3)\nknn_clf.fit(X_train,y_train)\nknn_y_preds=knn_clf.predict(X_val)\nprint('Accuracy Score %0.2f'%(100*accuracy_score(y_val,knn_y_preds)))\nprint(classification_report(y_val,knn_y_preds))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"XGBoost and KNN got a tie with both models getting the same accuracy score.<br>\nWe can further bump up the score by tuning hyperparameters.<br>\nBut lets not get into this"},{"metadata":{},"cell_type":"markdown","source":"<h3>Lets try something new and model our data with ANN(Artificial Neural Network)</h3>"},{"metadata":{"trusted":true},"cell_type":"code","source":"model=Sequential()\nmodel.add(Dense(64,input_dim=(7),activation='relu'))\nmodel.add(Dense(32,activation='relu'))\nmodel.add(Dense(16,activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(3,activation='softmax'))\nmodel.compile(loss='categorical_crossentropy',optimizer='rmsprop',metrics=['accuracy'])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train.value_counts()\nproduct_code={'TM195':1,'TM498':2,'TM798':3}\ny_train=y_train.map(product_code)\ny_val=y_val.map(product_code)\ny_train=pd.get_dummies(y_train)\ny_val=pd.get_dummies(y_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(y_train.shape)\nprint(y_val.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(X_train,y_train,epochs=100,verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"_,accuracy=model.evaluate(X_val,y_val)\nprint('Accuracy is {:0.2f}%'.format(100*accuracy))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We got almost same accuracy as XGBoost and KNN.I feel this may be because of two reasons\n* Not many features to find the pattern\n* Not many samples to train"},{"metadata":{},"cell_type":"markdown","source":"Please upvote if you find this kernel helpful."},{"metadata":{},"cell_type":"markdown","source":""}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}