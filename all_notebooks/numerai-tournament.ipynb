{"cells":[{"metadata":{},"cell_type":"markdown","source":"![](https://numer.ai/img/Numerai-Logo-Side-Black.8393ed16.png)\n\n（履歴）\n- version 3: 公開\n- version 4: タイポなど修正\n- version 5: target nomiへ変更。加筆。\n\n「Numeraiはじめてみたいけど、英語ドキュメントが多くてちょっと...」という方も多いと思いますので、日本語で「データのロード→モデリング→評価→提出」の一連の流れをまとめてみました。\n\nただ、最近Numerai関連の日本語ドキュメントが充実してきまして、公式でまとまってるので一度ご覧になると良いかも知れないです。\n\n[Numerai日本語公式ドキュメント](https://jp.docs.numer.ai/numerai-tournament/new-users)\n\nそれではやっていきましょう。\n\nまず、NumeraiにはAPIが用意されていますので、利用するためのモジュールをインストールします。"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install numerapi\nimport numerapi","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"他の必要なライブラリをインストールします。"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os, sys\nimport gc\nimport pathlib\nfrom typing import List, NoReturn, Union, Tuple, Optional, Text, Generic, Callable, Dict\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder, QuantileTransformer\nfrom sklearn.model_selection import KFold, StratifiedKFold, TimeSeriesSplit\nfrom sklearn.metrics import accuracy_score, roc_auc_score, log_loss, mean_squared_error, mean_absolute_error, f1_score\nfrom scipy.stats import spearmanr\nimport joblib\n\n# model\nimport lightgbm as lgb\nimport xgboost as xgb\nimport operator\n\n# visualize\nimport matplotlib.pyplot as plt\nimport matplotlib.style as style\nimport seaborn as sns\nfrom matplotlib import pyplot\nfrom matplotlib.ticker import ScalarFormatter\nsns.set_context(\"talk\")\nstyle.use('seaborn-colorblind')\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Tournamentデータのダウンロード\nAWSに公開されていますので、誰でも以下のようにダウンロード可能です。データの特徴量は離散値になっているので、メモリ節約のため整数値にキャストしています。Round期間を表す'Era'も整数にします。"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"def get_int(x):\n    try:\n        return int(x[3:])\n    except:\n        return 1000\n    \ndef read_data(data='train'):\n    # get data \n    if data == 'train':\n        df = pd.read_csv('https://numerai-public-datasets.s3-us-west-2.amazonaws.com/latest_numerai_training_data.csv.xz')\n    elif data == 'test':\n        df = pd.read_csv('https://numerai-public-datasets.s3-us-west-2.amazonaws.com/latest_numerai_tournament_data.csv.xz')\n    \n    # features\n    feature_cols = df.columns[df.columns.str.startswith('feature')]\n    \n    # map to int, to reduce the memory demand\n    mapping = {0.0 : 0, 0.25 : 1, 0.5 : 2, 0.75 : 3, 1.0 : 4}\n    for c in feature_cols:\n        df[c] = df[c].map(mapping).astype(np.uint8)\n        \n    # also cast era to int\n    df[\"era\"] = df[\"era\"].apply(get_int)\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\n# load train　(半年間固定)\ntrain = read_data('train')\nprint(train.shape)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\n# load test (毎週Roundごとに更新)\ntest = read_data('test')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Tournamentデータには、ラベルが与えられているValidationデータと、ラベルがあたえられていないTestデータが入っているので、分離します。また、Validationデータは主に2期間に分かれている（前の期間は予測が簡単で、後の方が予測が難しい）ため、ここを区別したラベルを付与します。"},{"metadata":{"trusted":true},"cell_type":"code","source":"valid = test[test[\"data_type\"] == \"validation\"].reset_index(drop = True)\n\n# validation split\nvalid.loc[valid[\"era\"] > 180, \"valid2\"] = True # むずいやつ\nvalid.loc[valid[\"era\"] <= 180, \"valid2\"] = False # 簡単なやつ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# remove data_type to save memory\ntrain.drop(columns=[\"data_type\"], inplace=True)\nvalid.drop(columns=[\"data_type\"], inplace=True)\ntest.drop(columns=[\"data_type\"], inplace=True)\n\nprint('The number of records: train {:,}, valid {:,}, test {:,}'.format(train.shape[0], valid.shape[0], test.shape[0]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EDA (Exploratory Data Analysis)の実行\n簡単にですが、どんなデータなのか見てみましょう。"},{"metadata":{},"cell_type":"markdown","source":"## 特徴量"},{"metadata":{"trusted":true},"cell_type":"code","source":"# features (特徴量)\nfeatures = [f for f in train.columns.values.tolist() if 'feature' in f]\nprint('There are {} features.'.format(len(features)))\nprint(features)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"310個の匿名特徴量がありますね。\n\n- intelligence (1 ~ 12)\n- charisma (1 ~ 86)\n- strength (1 ~ 38)\n- dexterity (1 ~ 14)\n- constitution (1 ~ 114)\n- wisdom (1 ~ 46)\n\nと6種類に大別できそうですが、それぞれどういった特徴量なのかはわかりません。"},{"metadata":{},"cell_type":"markdown","source":"## Target\ntargetはどうでしょうか。"},{"metadata":{"trusted":true},"cell_type":"code","source":"# target\ntarget = train.columns[train.columns.str.startswith('target')].values.tolist()[0]\nprint(f'Taget name = {target}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[target].hist()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"正規分布っぽい5つの離散値であることがわかります。"},{"metadata":{},"cell_type":"markdown","source":"# Modelingの実行\nデータの整理ができたので、[公式Example](https://github.com/numerai/example-scripts/blob/master/example_model.py)にならいモデリングをしていきます。\n\n[公式ではXGBoostを使用](https://jp.docs.numer.ai/numerai-tournament/tournament-overview)していますが、時短のためここではLightGBMを使用します。"},{"metadata":{},"cell_type":"markdown","source":"XGBoostもLightGBMもGBDT (Gradient Boosting Decision Tree)と呼ばれる類のモデルで、テーブル形式のデータに大して非常に強力です。パラメータが複数あってどうしたらいいかわからない方は、こちらのブログに非常に簡潔にわかりやすくまとまっていますのでオススメです。\n\n[勾配ブースティングで大事なパラメータの気持ち](https://nykergoto.hatenablog.jp/entry/2019/03/29/%E5%8B%BE%E9%85%8D%E3%83%96%E3%83%BC%E3%82%B9%E3%83%86%E3%82%A3%E3%83%B3%E3%82%B0%E3%81%A7%E5%A4%A7%E4%BA%8B%E3%81%AA%E3%83%91%E3%83%A9%E3%83%A1%E3%83%BC%E3%82%BF%E3%81%AE%E6%B0%97%E6%8C%81%E3%81%A1)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# # create a model and fit (公式example)\n# model = xgb.XGBRegressor(max_depth=5, learning_rate=0.01, n_estimators=2000, n_jobs=-1, colsample_bytree=0.1)\n# model.fit(train[features], train[target])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\n# create a model and fit（LGBのハイパラは↑の公式XGBに寄せてみました）\nparams = {\n            'n_estimators': 2000,\n            'objective': 'regression',\n            'boosting_type': 'gbdt',\n            'max_depth': 5,\n            'learning_rate': 0.01, \n            'feature_fraction': 0.1,\n            'seed': 42\n            }    \nmodel = lgb.LGBMRegressor(**params)\nmodel.fit(train[features], train[target])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# save model（あとでロードして予測できるように保存します）\njoblib.dump(model, 'my_lightgbm.joblib') \nprint('model saved!')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature importance\nせっかくGBDTを使っているので、どの特徴量が大事か見てみましょう。"},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(model.feature_importances_, index=features, columns=['importance']).sort_values(by='importance', ascending=False).style.background_gradient(cmap='viridis')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"特徴量は全て匿名なので、それぞれ具体的に何を意味しているかはわかりませんが、少なくとも特別強い、あるいは特別弱い特徴量というのはなさそうです。"},{"metadata":{},"cell_type":"markdown","source":"# Validation Score\nValidationデータが与えられているので、訓練したモデルがどの程度のものか、スコアを計算してみましょう。金融モデルなので、ただ精度 (targetとのrank correlation)だけでなく、運用期間で安定したパフォーマンスを出せているかチェックします。多くの関数はNumeraiの[公式Github](https://github.com/numerai/example-scripts/blob/master/example_model.py)にあるので、拾って改変し使っていきます。\n\nMMC（meta model correlation）は、運営のメタモデルが手に入らないため計算できませんが、精度 (rank correlation)と、精度と安定性のバランスを評価する**correlation sharpe**などは、自分で計算できますので計算しましょう。以下の指標を、Validation期間を全て、前半（簡単なやつ）、後半（難しいやつ）に分けて計算しています。\n\n- rank correlation (NumeraiでCORRと呼ばれているもの。高いほど良い)\n- sharpe ratio（期間ベースでのCORR平均を標準偏差で割ったもの。高いほど良い）\n- max drawdown (ある1ラウンドでの最大の損失CORR。0に近いほど良い)\n- feature exposure (モデル予測値が一部の特徴量に依存している度合。低いほど良い)\n\n実は提出すると、Numerai側で全部計算して自分のページで確認することができるのですが、全validation期間を使用したスコアのみが返ってくるため、「予測が難しい時期でもいいパフォーマンスが出ているか？」という肝心の疑問には答えてくれません。なので、自分で計算しましょう...^^"},{"metadata":{"trusted":true},"cell_type":"code","source":"# naming conventions\nPREDICTION_NAME = 'prediction'\nTARGET_NAME = target\n# EXAMPLE_PRED = 'example_prediction'\n\n# ---------------------------\n# Functions\n# ---------------------------\ndef valid4score(valid : pd.DataFrame, pred : np.ndarray, load_example: bool=True, save : bool=False) -> pd.DataFrame:\n    \"\"\"\n    Generate new valid pandas dataframe for computing scores\n    \n    :INPUT:\n    - valid : pd.DataFrame extracted from tournament data (data_type='validation')\n    \n    \"\"\"\n    valid_df = valid.copy()\n    valid_df['prediction'] = pd.Series(pred).rank(pct=True, method=\"first\")\n    valid_df.rename(columns={target: 'target'}, inplace=True)\n    \n    if load_example:\n        valid_df[EXAMPLE_PRED] = pd.read_csv(EXP_DIR + 'valid_df.csv')['prediction'].values\n    \n    if save==True:\n        valid_df.to_csv(OUTPUT_DIR + 'valid_df.csv', index=False)\n        print('Validation dataframe saved!')\n    \n    return valid_df\n\ndef compute_corr(valid_df : pd.DataFrame):\n    \"\"\"\n    Compute rank correlation\n    \n    :INPUT:\n    - valid_df : pd.DataFrame where at least 2 columns ('prediction' & 'target') exist\n    \n    \"\"\"\n    \n    return np.corrcoef(valid_df[\"target\"], valid_df['prediction'])[0, 1]\n\ndef compute_max_drawdown(validation_correlations : pd.Series):\n    \"\"\"\n    Compute max drawdown\n    \n    :INPUT:\n    - validation_correaltions : pd.Series\n    \"\"\"\n    \n    rolling_max = (validation_correlations + 1).cumprod().rolling(window=100, min_periods=1).max()\n    daily_value = (validation_correlations + 1).cumprod()\n    max_drawdown = -(rolling_max - daily_value).max()\n    \n    return max_drawdown\n\ndef compute_val_corr(valid_df : pd.DataFrame):\n    \"\"\"\n    Compute rank correlation for valid periods\n    \n    :INPUT:\n    - valid_df : pd.DataFrame where at least 2 columns ('prediction' & 'target') exist\n    \"\"\"\n    \n    # all validation\n    correlation = compute_corr(valid_df)\n    print(\"rank corr = {:.4f}\".format(correlation))\n    return correlation\n    \ndef compute_val_sharpe(valid_df : pd.DataFrame):\n    \"\"\"\n    Compute sharpe ratio for valid periods\n    \n    :INPUT:\n    - valid_df : pd.DataFrame where at least 2 columns ('prediction' & 'target') exist\n    \"\"\"\n    # all validation\n    d = valid_df.groupby('era')[['target', 'prediction']].corr().iloc[0::2,-1].reset_index()\n    me = d['prediction'].mean()\n    sd = d['prediction'].std()\n    max_drawdown = compute_max_drawdown(d['prediction'])\n    print('sharpe ratio = {:.4f}, corr mean = {:.4f}, corr std = {:.4f}, max drawdown = {:.4f}'.format(me / sd, me, sd, max_drawdown))\n    \n    return me / sd, me, sd, max_drawdown\n    \ndef feature_exposures(valid_df : pd.DataFrame):\n    \"\"\"\n    Compute feature exposure\n    \n    :INPUT:\n    - valid_df : pd.DataFrame where at least 2 columns ('prediction' & 'target') exist\n    \"\"\"\n    feature_names = [f for f in valid_df.columns\n                     if f.startswith(\"feature\")]\n    exposures = []\n    for f in feature_names:\n        fe = spearmanr(valid_df['prediction'], valid_df[f])[0]\n        exposures.append(fe)\n    return np.array(exposures)\n\ndef max_feature_exposure(fe : np.ndarray):\n    return np.max(np.abs(fe))\n\ndef feature_exposure(fe : np.ndarray):\n    return np.sqrt(np.mean(np.square(fe)))\n\ndef compute_val_feature_exposure(valid_df : pd.DataFrame):\n    \"\"\"\n    Compute feature exposure for valid periods\n    \n    :INPUT:\n    - valid_df : pd.DataFrame where at least 2 columns ('prediction' & 'target') exist\n    \"\"\"\n    # all validation\n    fe = feature_exposures(valid_df)\n    fe1, fe2 = feature_exposure(fe), max_feature_exposure(fe)\n    print('feature exposure = {:.4f}, max feature exposure = {:.4f}'.format(fe1, fe2))\n     \n    return fe1, fe2\n\n# to neutralize a column in a df by many other columns\ndef neutralize(df, columns, by, proportion=1.0):\n    scores = df.loc[:, columns]\n    exposures = df[by].values\n\n    # constant column to make sure the series is completely neutral to exposures\n    exposures = np.hstack(\n        (exposures,\n         np.asarray(np.mean(scores)) * np.ones(len(exposures)).reshape(-1, 1)))\n\n    scores = scores - proportion * exposures.dot(\n        np.linalg.pinv(exposures).dot(scores))\n    return scores / scores.std()\n\n\n# to neutralize any series by any other series\ndef neutralize_series(series, by, proportion=1.0):\n    scores = series.values.reshape(-1, 1)\n    exposures = by.values.reshape(-1, 1)\n\n    # this line makes series neutral to a constant column so that it's centered and for sure gets corr 0 with exposures\n    exposures = np.hstack(\n        (exposures,\n         np.array([np.mean(series)] * len(exposures)).reshape(-1, 1)))\n\n    correction = proportion * (exposures.dot(\n        np.linalg.lstsq(exposures, scores, rcond=None)[0]))\n    corrected_scores = scores - correction\n    neutralized = pd.Series(corrected_scores.ravel(), index=series.index)\n    return neutralized\n\n\ndef unif(df):\n    x = (df.rank(method=\"first\") - 0.5) / len(df)\n    return pd.Series(x, index=df.index)\n\ndef get_feature_neutral_mean(df):\n    feature_cols = [c for c in df.columns if c.startswith(\"feature\")]\n    df.loc[:, \"neutral_sub\"] = neutralize(df, [PREDICTION_NAME],\n                                          feature_cols)[PREDICTION_NAME]\n    scores = df.groupby(\"era\").apply(\n        lambda x: np.corrcoef(x[\"neutral_sub\"].rank(pct=True, method=\"first\"), x[TARGET_NAME])).mean()\n    return np.mean(scores)\n\ndef compute_val_mmc(valid_df : pd.DataFrame):    \n    # MMC over validation\n    mmc_scores = []\n    corr_scores = []\n    for _, x in valid_df.groupby(\"era\"):\n        series = neutralize_series(pd.Series(unif(x[PREDICTION_NAME])),\n                                   pd.Series(unif(x[EXAMPLE_PRED])))\n        mmc_scores.append(np.cov(series, x[TARGET_NAME])[0, 1] / (0.29 ** 2))\n        corr_scores.append(np.corrcoef(unif(x[PREDICTION_NAME]).rank(pct=True, method=\"first\"), x[TARGET_NAME]))\n\n    val_mmc_mean = np.mean(mmc_scores)\n    val_mmc_std = np.std(mmc_scores)\n    val_mmc_sharpe = val_mmc_mean / val_mmc_std\n    corr_plus_mmcs = [c + m for c, m in zip(corr_scores, mmc_scores)]\n    corr_plus_mmc_sharpe = np.mean(corr_plus_mmcs) / np.std(corr_plus_mmcs)\n    corr_plus_mmc_mean = np.mean(corr_plus_mmcs)\n\n    print(\"MMC Mean = {:.6f}, MMC Std = {:.6f}, CORR+MMC Sharpe = {:.4f}\".format(val_mmc_mean, val_mmc_std, corr_plus_mmc_sharpe))\n\n    # Check correlation with example predictions\n    corr_with_example_preds = np.corrcoef(valid_df[EXAMPLE_PRED].rank(pct=True, method=\"first\"),\n                                          valid_df[PREDICTION_NAME].rank(pct=True, method=\"first\"))[0, 1]\n    print(\"Corr with example preds: {:.4f}\".format(corr_with_example_preds))\n    \n    return val_mmc_mean, val_mmc_std, corr_plus_mmc_sharpe, corr_with_example_preds\n    \ndef score_summary(valid_df : pd.DataFrame):\n    score_df = {}\n    \n    try:\n        score_df['correlation'] = compute_val_corr(valid_df)\n    except:\n        print('ERR: computing correlation')\n    try:\n        score_df['corr_sharpe'], score_df['corr_mean'], score_df['corr_std'], score_df['max_drawdown'] = compute_val_sharpe(valid_df)\n    except:\n        print('ERR: computing sharpe')\n    try:\n        score_df['feature_exposure'], score_df['max_feature_exposure'] = compute_val_feature_exposure(valid_df)\n    except:\n        print('ERR: computing feature exposure')\n    try:\n        score_df['mmc_mean'], score_df['mmc_std'], score_df['corr_mmc_sharpe'], score_df['corr_with_example_xgb'] = compute_val_mmc(valid_df)\n    except:\n        print('ERR: computing MMC')\n    \n    return pd.DataFrame.from_dict(score_df, orient='index')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# prediction for valid periods   \npred = model.predict(valid[features])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# scores\nvalid_df = valid4score(valid, pred, load_example=False, save=False)\n\nscore_df = pd.DataFrame()\nprint('------------------')\nprint('ALL:')\nprint('------------------')\nall_ = score_summary(valid_df).rename(columns={0: 'all'})\n\nprint('------------------')\nprint('VALID 1:')\nprint('------------------')\nval1_ = score_summary(valid_df.query('era < 150')).rename(columns={0: 'val1'})\n\nprint('------------------')\nprint('VALID 2:')\nprint('------------------')\nval2_ = score_summary(valid_df.query('era > 150')).rename(columns={0: 'val2'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# scores\nscore_df = pd.concat([all_, val1_, val2_], axis=1)\nscore_df.style.background_gradient(cmap='viridis', axis=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"明らかにVALID2（予測が難しいValidation期間）の方が、CORRが小さかったりと難しいですね。\n\nただ、valid 2の予測が難しい期間でもCORR = 0.015あたりのスコアなので、**RoundでCORRのみにBetすれば（週次）1.5%のリターンが期待できそう**だということになります。CORR+MMCにBetすれば、（MMCがプラスなら）更なるリターンが見込めるモデルになっています。\n\nもちろん相場の動きは気まぐれです。これで「絶対に儲かる...!」というものではないので、各自データサイエンティストとして腕の見せ所です。"},{"metadata":{},"cell_type":"markdown","source":"# Submission\n以下提出の形式です。実際に提出するには、[Numerai tournament](https://numer.ai/tournament)でユーザ登録を行い、APIキーとモデルIDを取得してください。Rank correlationで評価されるため、こちらでrank化して提出します。"},{"metadata":{"trusted":true},"cell_type":"code","source":"public_id = \"NYANNYAN\" # replace with yours\nsecret_key = \"WANWAN\" # replace with yours\nmodel_id = \"KOKEKOKKOOOO\" # replace with yours\nPREDICTION_NAME = \"prediction_kazutsugi\" # 現在はこれ（いずれprediction_nomiになるらしい）\nOUTPUT_DIR = '' # prediction dataframeを保存するpath\n\ndef submit(tournament : pd.DataFrame, pred : np.ndarray, model_id='abcde'):\n    predictions_df = tournament[\"id\"].to_frame()\n    predictions_df[PREDICTION_NAME] = pred\n    \n    # to rank\n    predictions_df[PREDICTION_NAME] = predictions_df[PREDICTION_NAME].rank(pct=True, method=\"first\")\n    \n    # save\n    predictions_df.to_csv(pathlib.Path(OUTPUT_DIR + f\"predictions_{model_id}.csv\"), index=False)\n    \n    # Upload your predictions using API\n    napi = numerapi.NumerAPI(public_id=public_id, secret_key=secret_key)\n    submission_id = napi.upload_predictions(pathlib.Path(OUTPUT_DIR + f\"predictions_{model_id}.csv\"), model_id=model_id)\n    print('submitted to {model_id}', model_id=model_id)\n    \n    return predictions_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# prediction\npred = model.predict(test[features])\nplt.hist(pred);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# submit!（本当に提出する人はコメントアウトしてください）\n# predictions_df = submit(tournament, pred, model_id=model_id)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 終わりに\nNumeraiは英語の情報が多く、とっつきにくかった人も多いとは思いますが、このNotebookがみなさんのNumerai lifeの参考になれば幸いです。"},{"metadata":{},"cell_type":"markdown","source":"# 参考\n\n- [KagglerへのNumeraiのススメ](https://zenn.dev/katsu1110/articles/bb2b5cba9b04c9e30bfe)\n- [Numerai公式Github](https://github.com/numerai)"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}