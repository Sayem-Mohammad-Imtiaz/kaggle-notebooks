{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"from google.cloud import bigquery","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"client = bigquery.Client()\n\ndataset_ref = client.dataset(\"openaq\", project=\"bigquery-public-data\")\ndataset = client.get_dataset(dataset_ref)\n\ntables = list(client.list_tables(dataset))\nfor table in tables:\n    print (table.table_id)\n\ntable_ref = dataset_ref.table(\"global_air_quality\")\ntable = client.get_table(table_ref)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"table.schema","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = client.list_rows(table, max_results=5).to_dataframe()\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Query to select all the items from the \"city\" column where the \"country\" column is 'US'\nQuery = \"\"\" \n        SELECT city\n        FROM `bigquery-public-data.openaq.global_air_quality`\n        WHERE country = \"US\"\n        \"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#setup the query\nquery_job = client.query(Query)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# API request - run the query, and return a pandas DataFrame\nus_cities = query_job.to_dataframe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"us_cities.city.value_counts().head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Working with big datasets\n\nyou can estimate the size of any query before running it. Here is an example using the (very large!) Hacker News dataset. To see how much data a query will scan, we create a QueryJobConfig object and set the dry_run parameter to True."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Query to get the score column from every row where the type column has value \"job\"\nQuery = \"\"\" \n        SELECT score, title\n        FROM `bigquery-public-data.hacker_news.full`\n        WHERE type=\"job\"\n        \"\"\"\n\n# Create a QueryJobConfig object to estimate size of query without running it\ndry_run_config = bigquery.QueryJobConfig(dry_run=True)\n\n# API request - dry run query to estimate costs\ndry_run_query_job = client.query(Query, job_config=dry_run_config )\n\nprint(\"This Query will process {} bytes.\".format(dry_run_query_job.total_bytes_processed))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"You can also specify a parameter when running the query to limit how much data you are willing to scan. Here's an example with a low limit."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Only run the query if it's less than 100 MB\nONE_HUNDRED_MB = 100*1000*1000\nsafe_config = bigquery.QueryJobConfig(maximum_bytes_billed=ONE_HUNDRED_MB)\n\n# Set up the query (will only run if it's less than 100 MB)\nsafe_run_job = client.query(Query, job_config = safe_config)\n\n# API request - try to run the query, and return a pandas DataFrame\nsafe_run_job.to_dataframe()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Only run the query if it's less than 1 GB\nONE_GB = 1000*1000*1000\nsafe_config = bigquery.QueryJobConfig(maximum_bytes_billed=ONE_GB)\n\n# Set up the query (will only run if it's less than 100 MB)\nsafe_run_job = client.query(Query, job_config = safe_config)\n\n# API request - try to run the query, and return a pandas DataFrame\njob_post_scores = safe_run_job.to_dataframe()\n\n# Print average score for job posts\njob_post_scores.score.mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Group By and Having"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Query to select comments that received more than 10 replies\nquery_popular = \"\"\"\n                SELECT parent, COUNT(id)\n                FROM `bigquery-public-data.hacker_news.comments`\n                GROUP BY parent\n                HAVING COUNT(id) > 10\n                \"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set up the query (cancel the query if it would use too much of \n# your quota, with the limit set to 10 GB)\nsafe_config = bigquery.QueryJobConfig(maximum_bytes_billed=10**10)\nquery_job = client.query(query_popular, job_config=safe_config)\n\n# API request - run the query, and convert the results to a pandas DataFrame\npopular_comments = query_job.to_dataframe()\n\npopular_comments.head()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Improved version of earlier query, now with aliasing & improved readability\nquery_improved = \"\"\"\n                SELECT parent, COUNT(id) AS NumPosts\n                FROM `bigquery-public-data.hacker_news.comments`\n                GROUP BY parent\n                HAVING COUNT(id) > 10\n                \"\"\"\n\n# Set up the query (cancel the query if it would use too much of \n# your quota, with the limit set to 10 GB)\nsafe_config = bigquery.QueryJobConfig(maximum_bytes_billed=10**10)\nquery_job = client.query(query_improved, job_config=safe_config)\n\n# API request - run the query, and convert the results to a pandas DataFrame\npopular_comments = query_job.to_dataframe()\n\npopular_comments.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## working with Dates\nclient = bigquery.Client()\n# Construct a reference to the \"nhtsa_traffic_fatalities\" dataset\ndataset_ref = client.dataset(\"nhtsa_traffic_fatalities\", project=\"bigquery-public-data\")\n#API request\ndataset = client.get_dataset(dataset_ref)\n\ntables = client.list_tables(dataset)\n\nfor t in tables:\n    print (t.table_id)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"table_ref = dataset_ref.table(\"accident_2015\")\ntable = client.get_table(table_ref)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"table.schema","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"client.list_rows(table, max_results=5).to_dataframe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Query to find out the number of accidents for each day of the week\nQuery = \"\"\"\n        SELECT COUNT(consecutive_number) AS num_accidents,\n               EXTRACT(DAYOFWEEK FROM timestamp_of_crash) AS day_of_week\n        FROM `bigquery-public-data.nhtsa_traffic_fatalities.accident_2015`\n        GROUP BY day_of_week\n        ORDER BY num_accidents desc\n       \"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set up the query (cancel the query if it would use too much of \n# your quota, with the limit set to 1 GB)\nsafe_config = bigquery.QueryJobConfig(maximum_bytes_billed=10**9)\nquery_job = client.query(Query, job_config=safe_config)\n\n# API request - run the query, and convert the results to a pandas DataFrame\naccidents_by_day = query_job.to_dataframe()\naccidents_by_day","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"client = bigquery.Client()\n\ndataset_ref = client.dataset(\"crypto_bitcoin\", project=\"bigquery-public-data\")\ndataset = client.get_dataset(dataset_ref)\n\ntables = list(client.list_tables(dataset))\nfor table in tables:\n    print (table.table_id)\n\ntable_ref = dataset_ref.table(\"transactions\")\ntable = client.get_table(table_ref)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"client.list_rows(table, max_results=5).to_dataframe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Query to select the number of transactions per date, sorted by date\nquery_with_CTE = \"\"\" \n                 WITH time AS\n                 (\n                     SELECT DATE(block_timestamp) as trans_date\n                     FROM `bigquery-public-data.crypto_bitcoin.transactions`\n                 )\n                 SELECT count(1) AS transactions, trans_date\n                 FROM time\n                 GROUP BY trans_date\n                 ORDER BY trans_date\n                 \n                \"\"\"\n\n# Set up the query (cancel the query if it would use too much of \n# your quota, with the limit set to 10 GB)\nsafe_config = bigquery.QueryJobConfig(maximum_bytes_billed=10**10)\nquery_job = client.query(query_with_CTE, job_config=safe_config)\n\n# API request - run the query, and convert the results to a pandas DataFrame\ntransactions_by_date = query_job.to_dataframe()\n\n# Print the first five rows\ntransactions_by_date.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"transactions_by_date.set_index('trans_date').plot()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Joins"},{"metadata":{"trusted":true},"cell_type":"code","source":"client = bigquery.Client()\ndataset_ref = client.dataset(\"github_repos\", project=\"bigquery-public-data\")\ndataset = client.get_dataset(dataset_ref)\n\n#tables = list(client.list_tables(dataset))\n#for t in tables:\n#    print(t.table_id)\n\ntable_ref = dataset_ref.table('licenses')\ntable = client.get_table(table_ref)\n\nclient.list_rows(table, max_results=5).to_dataframe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"table_ref2 = dataset_ref.table('sample_files')\ntable2 = client.get_table(table_ref2)\nclient.list_rows(table2, max_results=5).to_dataframe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Query to determine the number of files per license, sorted by number of files\nQuery = \"\"\"\n        SELECT L.license, count(1) as num_of_files\n        FROM `bigquery-public-data.github_repos.sample_files` as SF\n        INNER JOIN `bigquery-public-data.github_repos.licenses` as L\n               ON L.repo_name = SF.repo_name\n        GROUP BY L.license\n        ORDER BY num_of_files\n        \"\"\"\n\n# Set up the query (cancel the query if it would use too much of \n# your quota, with the limit set to 10 GB)\nsafe_config = bigquery.QueryJobConfig(maximum_bytes_billed=10**10)\nquery_job = client.query(Query, job_config=safe_config)\n\n# API request - run the query, and convert the results to a pandas DataFrame\nfile_count_by_license = query_job.to_dataframe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"file_count_by_license","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}