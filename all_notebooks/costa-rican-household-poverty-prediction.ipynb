{"cells":[{"metadata":{"id":"5C5y_FPyh1fR"},"cell_type":"markdown","source":"# DSC 540 Final Project\n\n[1. Abstract](#abstract)\n\n[2. Introduction](#introduction)\n[3. Exploratory Analysis Part 1 (Preprocessing and Data Cleaning)](#packages)\n\n* [3 a. Packages](#packages)\n* [3 b. Explaining the Data ](#explaining_data)\n* [3 c. Record Counts ](#counts)\n* [3 d. Missing Values ](#missing_value)\n* [3 e. Data Dictionary ](#data_dictionary)\n* [3 f. Dropped and Created Features ](#dropped_created)\n* [3 g. Statistical Summary ](#statistical_summary)\n* [3 h. KNN Imputation for Monthly Rent Payment Variable ](#knn_imputation)\n* [3 i. Distribution of Target Variable](#pie_chart)\n* [3 j. Correlation Matrix](#correlation_matrix)\n* [3 k. Education](#education)\n* [3 l. Exploring Regional Difference](#regions)\n* [3 m. Urban vs. Rural](#urban_vs_rural)\n* [3 n. Gender Differences](#gender)\n* [3 o. Overcrowding](#overcrowding)\n* [3 p. House Quality (and some feature engineering)](#house_quality)\n* [3 q. Dependency](#dependency)\n* [3 r. Age](#age)\n* [3 s. Distribution of Monthly Rent Payments](#rent)\n* [3 t. Distribution of Missing Rent Values](#missing_rent)\n\n[4. Data Reduction Techniques (PCA, KPCA, Feature Selection)](#pca)\n* [4 a. PCA](#pca)\n* [4 b. Kernel PCA](#kpca)\n* [4 c. Feature Selection Using Feature Importance From a Logistic Regression Model](#feature_selection)\n\n[5. Research Questions](#research_questions)\n* [5 a. Research Question 1: Can we use PCA to identify the main components that determine poverty level?](#research_question_1)\n* [5 b. Research Question 2: Can we develop a model better than our baseline model?](#research_question_2)\n* [5 c. Research Question 3: How important are regional differences?](#research_question_3)\n\n[6. Machine Learning](#models)\n* [6 a. Train-Test Split](#models)\n* [6 b. Logistic Regression](#logistic_regression)\n* [6 c. AdaBoost](#adaboost)\n* [6 d. Gaussian Naive Bayes](#gnb)\n* [6 e. Decision Tree](#decision_tree)\n* [6 f. Random Forest](#random_forest)\n* [6 g. KNN](#knn)\n* [6 h. Neural Network](#neural)\n* [6 i. Stochastic Gradient Descent](#sgd)\n* [6 j. Support Vector Classifier](#svc)\n* [6 k. Rebalancing Techniques](#rebalance)\n\n\n\n\n\n[7 a. Experiment Results](#experiment_results)\n\n[7 b. Experiment Results for Rebalancing Techniques](#experiment_results_b)\n\n[8. Conclusion](#conclusion)\n\n[9. Contributions](#contributions)\n* [9 a. Max Rodrigues](#max_rodrigues)\n* [9 b. Andrew Schiek](#andrew_schiek)\n* [9 c. Xintong Li](#Xintong_Li)\n* [9 c. Mashall Jahangir](#Mashall_Jahangir)\n\n[10. References](#references)\n\n\n\n"},{"metadata":{"id":"Liyju4ep4km0"},"cell_type":"markdown","source":"<a id='abstract'></a>\n\n# 1. Abstract\nSocial programs have a difficulty determining which households are in most desperate need of welfare assistance. Many of the poorest households do not have the records (salary slips, tax returns, etc) to indicate that they are deserving of welfare assistance. Currently, machine learning models that utilize data that describe observable characteristics of homes help to ameliorate this issue. These models are called Proxy Means Tests (PMTs). Performance using these limited features is often inconsistent, especially as populations grow.\n\nThe data used for this project is from a Kaggle competition, and the purpose of the competition was to determine if the use of non-traditional econometric features (such as the level of education the head of the household has) can help in determine the households that most need welfare assistance. After preprocessing the data (feature engineering, scaling, PCA, etc), we applied the full data to multiple models and also applied rebalancing techniques to some of these models. We also took a subset of the data to mimic a typical dataset used for PMTs and compared the results. We found that the most important features are actually observable ones, but the inclusion of non-traditional features improves model performance. Data were highly imbalanced, so balancing techniques typically helped with performance. KNN also had strong performance due to its resilience to handle imbalanced data. Our findings indicate that the improvement in metrics with the inclusion of non-traditional features may not be worth the many long man-hours involved in collecting this difficult to obtain data if these techniques are to be applied in other countries.\n"},{"metadata":{"id":"jXf_11hhh1fR"},"cell_type":"markdown","source":"<a id='introduction'></a>\n\n# 2. Introduction\nMany social programs around the world face an issue of assessing social welfare needs (many of the poorest people often do not have the records to indicate they qualify). A workaround to this issue is through the use of machine learning: rather than using income verification records or tax returns, models use a Proxy Means Test (PMT) to assess income qualification. These models use observable features of households (e.g wall or ceiling quality) as an approximation of income level. However, these models are often inaccurate and become unstable as populations change over time.\n\nFor this project, we are using data from a Kaggle competition (/www.kaggle.com/c/costa-rican-household-poverty-prediction/data?select=train.csv) to predict how impoverished a household is. Data collectors from the Inter-American Development Bank classified a large number of households (9,557) in Costa Rica into four groups: \n\n1 = extreme poverty\n\n2 = moderate poverty\n\n3 = vulnerable households\n\n4 = non vulnerable household\n\nIn addition to features typically included in a PMT, data regarding information about individuals in the household (e.g. level of education, number of people living in the home, etc) was included with the hopes of improving predictive performance without the need of actual financial records. For this problem, we plan to use a variety of multiclass classification techniques to determine a model that has a high overall performance (F-1 macro average) and identifies extreme & moderate poverty households well (precision, recall and F-1 scores will be used as evaluative measures for these classes). Given the imbalanced nature of the dataset, we will also try rebalancing techniques (under/over-sampling).\n\nIn addition to finding a best model, we also want to determine the effectiveness of including non-traditional variables in a PMT model. We used both the entire dataset for prediction as well as a subset of the data that are the features typically included in PMT models. We are also interested in identifying the most important features for predicting poverty to see if non-traditional PMT features are better or worse predictors than than traditional PMT features. The conclusions developed from this analysis can help determine best approaches for assessing social welfare needs for other countries by finding a best model and determining the types of features that should be collected. We will also apply our best model to the Kaggle holdout test set to see how our model performs relative to other Kagglers.\n\n"},{"metadata":{"id":"Lb0N_oI3h1fR"},"cell_type":"markdown","source":"<a id='packages'></a>\n## 3. Exploratory Data Analysis Part 1 (Data Cleaning and Preprocessing)\n### 3 a. Packages\n\n"},{"metadata":{"id":"PCbx-S_Lh1fR","outputId":"0b95712a-4b5d-4763-baf2-739ae9b90c33","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.metrics import balanced_accuracy_score\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.decomposition import PCA\nfrom sklearn import preprocessing\nimport warnings\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom collections import OrderedDict\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\nfrom sklearn import tree\nimport graphviz\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.decomposition import KernelPCA\nfrom sklearn.inspection import permutation_importance\nfrom sklearn.impute import KNNImputer\nfrom sklearn.feature_selection import RFE\nfrom sklearn.svm import SVC\n\n\n\n#from google.colab import files \n#uploaded = files.upload()\n\n#import io \n#data = pd.read_csv(io.StringIO(uploaded[\"train.csv\"].decode('utf-8')),sep=',')\ndata = pd.read_csv(\"../input/rs-costa-rican-household-poverty-level-prediction/train.csv\")\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"PmI9HWo81t3V"},"cell_type":"markdown","source":"<a id='explaining_data[link text](https://)'></a>\n\n## 3 b. Explain your data (Explanatory & Target Variable)\n\n\nThe data are a mixture of categorical, numeric and one-hot encoded variables with a Target variable coded by the Inter-American Developmental Bank indicating the level of poverty a household is facing (distribution of the target variable is below and summaries of the explanatory variables are included in the EDA).\n\n\n"},{"metadata":{"id":"Vm1QkTUnn5Ye","outputId":"3eb19e89-3368-4d55-ffe4-525aa176ad18","trusted":true},"cell_type":"code","source":"data.Target.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"id":"HC0Yx9Ls2kEM"},"cell_type":"markdown","source":"<a id='counts[link text](https://)'></a>\n\n## 3 c. Record Counts"},{"metadata":{"id":"1snz7NdPh1fT","outputId":"37c0a5c1-7ea2-415d-b4e4-65a442272ee8","trusted":true},"cell_type":"code","source":"print(\"The data have\", data.shape[0], \"rows and\",data.shape[1],\"columns\")","execution_count":null,"outputs":[]},{"metadata":{"id":"qfAku7xi83wW"},"cell_type":"markdown","source":"<a id='missing_values[link text](https://)'></a>\n\n## 3 d. Missing Values\nBelow we identified that several columns have large amounts of missing values. There is likely a non-random nature to some of these missing values (espcially monthly rent payment since financial records are typically harder to obtain for more impoverished households). We created a binary variable (\"rent_missing\") indicating if the rent payment value was missing for an observation. We used KNN imputation to fill in the missing values. This allows us to retain the information of non-missing values of these rows and gain potential information of the non-random nature of the distribution of these missing values.\n\n"},{"metadata":{"id":"sG8IM5gsh1fT","outputId":"53f3f294-9ee5-4fd2-aa25-b2565b6f369f","trusted":true},"cell_type":"code","source":"total_missing_values = 0\nfor i in data.columns:\n    if sum(data[i].isnull()) > 0:\n        print(\"Missing values for\", i,\":\",sum(data[i].isnull()))\n    total_missing_values += sum(data[i].isnull())\nprint(\"Total missing values:\", total_missing_values)","execution_count":null,"outputs":[]},{"metadata":{"id":"qn3oSnSEAiSs"},"cell_type":"markdown","source":"\"v18q1\" indicates the number of tablets the household has and \"rez_esc\" indicates the number of years behind in school the homeowner is. While these are both potentially useful variables, we have a number of education variables that are not missing available for use. As for the number of tablets, this feature more closely aligns with a categorical feature than a numeric and imputation techniques will likely be biased (given approximately 80% of values are missing for this feature). We therefore dropped \"v18q1\" and \"rez_esc\".\n"},{"metadata":{"id":"1RH3zNDvh1fT","trusted":true},"cell_type":"code","source":"data[\"rent_missing\"] = np.where(data[\"v2a1\"].isnull(),1,0)\n#reverse one-hot encode to group by region to get monthly rent averages to fill missing values\nregions_df = data[['lugar1','lugar2','lugar3','lugar4','lugar5','lugar6']]\nwarnings.filterwarnings(\"ignore\")\nregions_df = data[['lugar1','lugar2','lugar3','lugar4','lugar5','lugar6']]\nregions_df['regions']='' # to create an empty column\nregions_df2 = pd.get_dummies(regions_df).idxmax(1)\n\ndata[\"regions\"] = regions_df2\ndata[\"meaneduc\"] = data.groupby(\"regions\").transform(lambda x: x.fillna(x.mean()))['meaneduc']\ndata[\"SQBmeaned\"] = data.groupby(\"regions\").transform(lambda x: x.fillna(x.mean()))['SQBmeaned']\ndata = data.drop(columns=['regions'])\ndata = data.drop(columns=['rez_esc'])\ndata = data.drop(columns=['v18q1'])\nwarnings.resetwarnings()\n","execution_count":null,"outputs":[]},{"metadata":{"id":"RmmGbA9SZwzV"},"cell_type":"markdown","source":"<a id='data_dictionary[link text](https://)'></a>\n\n## 3 e. Data Dictionary"},{"metadata":{"id":"ELsMPzra6ayJ"},"cell_type":"markdown","source":"##Traditional PMT Features:\n\n**Variables Related to House Size**: tamhog, hhsize\n\n**Variables Describing Materials of Wall, Floor, or Roof** : paredblolad, paredzocalo, paredpreb, pareddes , paredmad, paredzinc, paredfibras, paredother, pisomoscer, pisocemento, pisoother, pisonatur, pisonotiene, pisomadera, techozinc, techoentrepiso, techocane, techootro\", cielorazo\n\n**Variables Describing Quality of Wall, Floor, or Roof (Good,Avg,Poor)**:\nepared1, epared2, epared3, etecho1, etecho2, etecho3,\neviv1, eviv2, eviv3\n- as a note, these were converted into variables indicating the quality of the house as a whole (house_poor_quality, house_average_quality, house_good_quality)\n\n\n**Features Describing Region Household is Located**: lugar1, lugar2, lugar3, lugar4, lugar5, lugar6, urban\n\n\n##Non-Traditional PMT Features (Features that typically require verification from households to obtain):\n\n**Monthly rent payment**: v2a1\n\n**Features Decribing Rent/Mortgage Payment**:  \ntipovivi1 (fully paid), tipovivi2 (paying in installments), tipovivi3 (rented), tipovivi4 (precarious), tipovivi5 (borrowed)\n\n**Features Indicating Overcrowding**: hacdor, hacapo,overcrowding\n\n**Features Indicating Types of Technology Owned**: v18q (tablet),computer, television, mobilephone ,qmobilephone (# of phones)\n\n**Features Describing Status of Head of Household**: status, estadocivil1, estadocivil2, estadocivil3, estadocivil4, estadocivil5, estadocivil6, estadocivil7, parentesco1, parentesco2,parentesco3, parentesco4, parentesco5, parentesco6, parentesco7, parentesco8, parentesco9, parentesco10, parentesco11, parentesco12\n\n**Features Describing Number of Individuals Living in Household (Within Different Age Groups)**: hogar_nin (# age 0-19), hogar_adul(# adults), hogar_mayor (#adults 65+), hogar_total (total #), dependency (# under 19 + # 64+/# between 19 and 64)]\n\n**Age of Head of Household**: age\n\n**Features Describing Education Level of Head of Household & Members of the Household**: edjefe, edjefa, meaneduc, instlevel1, instlevel2, instlevel3, instlevel4, instlevel5, instlevel6, instlevel7, instlevel8, instlevel9, escolari\n\n**Features Indicating Type of Water Provision**: abastaguadentro (inside), abastaguafuera (outside), abastaguano(none)\n\n**Features Indicating Electricity Source**: electricity (public), planpri (private), noelec (none), coopele (from coopeartive)\n\n**Features Describing Plumbing**: sanitario1 (no toilet), \nsanitario2 (toilet connected to sewer), sanitario3, (septic tank),sanitario5 (toiled connected to black hole),\nsanitario6 (other), v14a (has a bathroom)\n\n**Features Describing How Trash is Disposed**: elimbasu1 (truck), elimbasu2 (buried), elimbasu3 (burning), elimbasu4 (throwing in unoccupied space), elimbasu5 (other)\n\n**Features Describing Number of Rooms the Household Has**: rooms (total #), bedrooms (# of bedrooms)\n\n**Features Describing Number of People Living in the Household**: r4h1 (# males under 12), r4h2 (# males over 12), r4h3 (total # males), r4m1 (# females under 12), r4m2 (# females over 12), r4t1 (total # females), r4t1 (total # under 12), r4t2 ( total # over 12), r4t3 (total # in household), tamviv (total # in household)\n\n**Does the house have a refrigerator?**: refrig\n\n##Dropped Features\n**Years behind in school**: rez_esc \n\n**Total Number of Tablets in the Household**: v18q1\n"},{"metadata":{"id":"JtAX9h9cOycg"},"cell_type":"markdown","source":"<a id='dropped_created[link text](https://)'></a>\n\n## 3 f. Dropped and Created Features\n\n**Created (Feature Engineered)**\n* Redundant variables converted to single variable (male/female -> gender), area1/area2 -> urban\n* \"yes/no\" variables made numeric for analysis\n* binary variable indicating if the rent variable is missing\n* house quality features made instead of aspects of the house (e.g. if the house has a poor quality roof, wall or floor then the house is considered poor quality. Same for medium or good quality roof, wall or floor). The features used for feature engineering are dropped later prior to building models but are included for EDA.\n\n**Dropped**\n* ID variables\n* elimbasu5 (rubbish disposed of by discarding in river) all values were 0\n* dropped variables that are other variables squared\n"},{"metadata":{"id":"tqCz0G6Wh1fT","trusted":true},"cell_type":"code","source":"data = data.drop(columns=['ID_num']) #dont need id for modeling\ndata = data.drop(columns=['idhogar']) #household Id\ndata = data.drop(columns=['elimbasu5']) #all values are zero\n#male and female are redundant. Making one gender variable\ndata['gender'] = data['male']\ndata = data.drop(columns=['male'])\ndata = data.drop(columns=['female'])\n\n#rural and urban variables are redundant. making one variable\ndata['urban'] = data['area1']\ndata = data.drop(columns=['area1'])\ndata = data.drop(columns=['area2'])\n\n#dropping feature engineered variables... these are other features squared\ndata = data.drop(columns=['SQBescolari'])\ndata = data.drop(columns=['SQBage'])\ndata = data.drop(columns=['SQBhogar_total'])\ndata = data.drop(columns=['SQBedjefe'])\ndata = data.drop(columns=['SQBhogar_nin'])\ndata = data.drop(columns=['SQBovercrowding'])\ndata = data.drop(columns=['SQBdependency'])\ndata = data.drop(columns=['SQBmeaned'])\ndata = data.drop(columns=['agesq'])\n\n#converting variables to be numeric\ndata['edjefe'] = data.edjefe.replace('yes', 1) \ndata['edjefe'] = data.edjefe.replace('no', 0)\n\ndata['dependency'] = data.edjefe.replace('yes', 1)\ndata['dependency'] = data.edjefe.replace('no', 0)\n\ndata['edjefa'] = data.edjefe.replace('yes', 1)\ndata['edjefa'] = data.edjefe.replace('no', 0)\n\ndata[\"dependency\"] = pd.to_numeric(data[\"dependency\"])\ndata[\"edjefe\"] = pd.to_numeric(data[\"edjefe\"])\ndata[\"edjefa\"] = pd.to_numeric(data[\"edjefa\"])\n\nhouse_poor_quality = data[\"epared1\"] + data[\"etecho1\"] + data[\"eviv1\"]\ndata[\"house_poor_quality\"] = np.where(house_poor_quality>0,1,0)\n\nhouse_medium_quality = data[\"epared2\"] + data[\"etecho2\"] + data[\"eviv2\"]\ndata[\"house_medium_quality\"] = np.where(house_medium_quality>0,1,0)\n\nhouse_good_quality = data[\"epared3\"] + data[\"etecho3\"] + data[\"eviv3\"]\ndata[\"house_good_quality\"] = np.where(house_good_quality>0,1,0)\n\nX = data.drop('Target',axis=1)\ny = data[\"Target\"]","execution_count":null,"outputs":[]},{"metadata":{"id":"xfVJsWSHaIZX"},"cell_type":"markdown","source":"<a id='statistical_summary[link text](https://)'></a>\n\n## 3 g. Statistical Summary"},{"metadata":{"id":"zGX7WMVQaUdY","outputId":"75885dad-2495-40c2-b389-4e0e1df6d6cd","trusted":true},"cell_type":"code","source":"pd.set_option('display.max_columns', 150)\nX.describe()","execution_count":null,"outputs":[]},{"metadata":{"id":"PcQP1mhWeF31"},"cell_type":"markdown","source":"We can inspect differences in mean between poor quality houses and average to good quality houses. We can see differences in most attributes. Most notably, rent payments (\"v2a1\") average 114,228 cólones whereas good/average home rent payments average 172,366 cólones."},{"metadata":{"id":"Tju2ztfrc_hE","outputId":"86e24d82-4329-493c-f3a0-c6335ad3ae9f","trusted":true},"cell_type":"code","source":"X.groupby(\"house_poor_quality\").mean()","execution_count":null,"outputs":[]},{"metadata":{"id":"sTGSdQcQh1fT"},"cell_type":"markdown","source":"<a id='knn_imputation[link text](https://)'></a>\n\n## 3 h. KNN Imputation for Monthly Rent Payment Variable\n\nKNN imputation uses a KNN algorithm to fill missing values. We used n = 3."},{"metadata":{"id":"iKrL2Yz6h1fT","trusted":true},"cell_type":"code","source":"imputer = KNNImputer(n_neighbors=3)\ndf_filled = imputer.fit_transform(X)\nX[\"v2a1\"] = df_filled.T[0]","execution_count":null,"outputs":[]},{"metadata":{"id":"N2uhYihph1fT"},"cell_type":"markdown","source":"Now there are no missing values in the data."},{"metadata":{"id":"RV02XNUzh1fT","outputId":"acb88ec9-6832-4f12-9b5b-78da8d5862c7","trusted":true},"cell_type":"code","source":"total_missing_values = 0\nfor i in X.columns:\n    if sum(X[i].isnull()) > 0:\n        print(\"Missing values for\", i,\":\",sum(X[i].isnull()))\n    total_missing_values += sum(X[i].isnull())\nprint(\"Total missing values:\", total_missing_values)","execution_count":null,"outputs":[]},{"metadata":{"id":"4FytqVYvh1fU"},"cell_type":"markdown","source":"<a id='pie_chart'></a>\n\n## 3 i. Distribution of Target Variable\n\n\nThe pie chart shows that the majority of people are non vulnerable households i.e 62.7%.\nOnly 7.9% people fall into extreme poverty and 16.7% fall into moderate poverty when looked at their income distribution.\nThis indicates we need to consider imbalanced classes in model construction\n"},{"metadata":{"id":"guJ84vxbh1fU","outputId":"8d692697-9dea-4ff2-c0a0-15fc34825fb6","trusted":true},"cell_type":"code","source":"# Creating plots \ngrouped = pd.DataFrame(data.groupby(['Target'])['r4t3'].count())\ngrouped.rename(columns={'r4t3':'Target_count'}, inplace=True)\nfig = plt.figure(figsize=(12,12))\nax = fig.add_subplot(122)\n    \n# Create colour palette\nlabels = ['extreme poverty', 'moderate poverty', 'vulnerable households', 'non vulnerable households']\ntheme = plt.get_cmap('Blues')\nax.set_prop_cycle(\"color\", [theme(1. * i / len(labels))\n                                 for i in range(len(labels))])\nsns.set(font_scale=1.25)\n\n# Create pie chart\npie = ax.pie(grouped['Target_count'],\n                 autopct='%1.1f%%',\n                 shadow=True,\n                 startangle=20,\n                 pctdistance=1.115,\n                 explode=(0.1, 0.1, 0.1, 0.1))\n   \n# Turn pie chart into a donut chart\ncentre_circle = plt.Circle((0,0),0.70,fc='white')\nfig = plt.gcf()\nfig.gca().add_artist(centre_circle)\n\n# Display donut plot with legend\nplt.legend(pie[0], labels, loc=\"lower left\")\nax.set_title('Income distribution among People surveyed \\n', fontsize=15)\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"cmUc1p2Oh1fU"},"cell_type":"markdown","source":"# <a id='correlation_matrix'></a>\n## 3 j. Correlation Matrix\n\nFor the correlation matrix, I'm taking all variables that are not binary (or one-hot encoded) and putting these variables into a new dataframe to find all correlations."},{"metadata":{"id":"dXI2S0N9h1fU","trusted":true},"cell_type":"code","source":"#for the correlation heatmap\nnon_binary_df = pd.DataFrame()\nfor i in X.columns:\n    if max(X[i]) != 1:\n        non_binary_df[i] = X[i]","execution_count":null,"outputs":[]},{"metadata":{"id":"SvUYyn73h1fU","outputId":"75439005-bd65-4878-b868-411240db0395","trusted":true},"cell_type":"code","source":"train_corr = non_binary_df.iloc[:,:-1]\ncorrelations = train_corr.corr(method=\"pearson\")\ncorrelations\nmask = np.zeros_like(correlations)\nmask[np.triu_indices_from(mask)] = True\nwith sns.axes_style(\"white\"):\n    f, ax = plt.subplots(figsize=(30, 15))\n    ax = sns.heatmap(correlations, mask=mask, vmax=1, square=True, cmap=\"YlGnBu\", annot = True)","execution_count":null,"outputs":[]},{"metadata":{"id":"mjSW_17Eh1fU"},"cell_type":"markdown","source":"<a id='education'></a>\n\n## 3 k. Education\n\nThe highest correlation with v2a1 (monthly rent payment) is meadeduc (average years of education for adults). Looking at the scatterplot, there seems to be a slight increase in rent as education goes up up to about 25 years of education"},{"metadata":{"id":"gB2bAxKhh1fU","outputId":"ee1468cc-63da-4d4d-d407-3246b2754c79","trusted":true},"cell_type":"code","source":"correlation = X[\"v2a1\"].corr(X[\"meaneduc\"]) \nplt.scatter(X[\"meaneduc\"], X[\"v2a1\"], alpha=0.5)\nplt.xlabel(\"Average Years of Education for Adults in the Household\")\nplt.ylabel(\"Monthly Rent Payment\")\nplt.title(\"Correlation of \" + str(round(correlation,4)))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"jhHP_oeEh1fU"},"cell_type":"markdown","source":"The plot below is the same as the previous plot but only goes to a meaneducation of 25. This more clearly demonstrates a positive correlation between monthly rent payments and education level."},{"metadata":{"id":"0e1GHpyNh1fU","outputId":"fb6ceec7-e9bf-4f25-dc30-3595392da32e","trusted":true},"cell_type":"code","source":"under_20_years_educ = X[X[\"meaneduc\"] < 25]\n\ncorrelation = under_20_years_educ[\"v2a1\"].corr(under_20_years_educ[\"meaneduc\"]) \nplt.scatter(under_20_years_educ[\"meaneduc\"], under_20_years_educ[\"v2a1\"], alpha=0.5)\nplt.xlabel(\"Average Years of Education for Adults in the Household\")\nplt.ylabel(\"Monthly Rent Payment\")\nplt.title(\"Correlation of \" + str(round(correlation,4)))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"4qM3iAoXh1fU"},"cell_type":"markdown","source":"From the boxplots below we can see, Mean education of males and females in household is low in extreme poverty and high in non-vulnerable. The total population of male and female is also moderately high given their education in non-vulnerable income levels. "},{"metadata":{"id":"a8QXUMC9h1fU","outputId":"b2e68b22-8c25-4db2-a24b-cc48a4e2bbfd","trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(8,8))\nax = sns.boxplot(x=\"Target\", y=\"meaneduc\", hue='r4h3', data=data, palette=\"Set3\").set_title('Mean Education by Income levels and Total males in the household \\n')","execution_count":null,"outputs":[]},{"metadata":{"id":"8TpVrhbbh1fU","outputId":"d8719d1e-4cf3-42c2-fbb0-d34aff0ec823","trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(8,8))\nax = sns.boxplot(x=\"Target\", y=\"meaneduc\", hue='r4m3', data=data, palette=\"Set3\").set_title('Mean Education by Income levels and Total females in the household \\n')","execution_count":null,"outputs":[]},{"metadata":{"id":"GNfve1dbh1fU"},"cell_type":"markdown","source":"<a id='regions'></a>\n\n## 3 l. Regional Differences\nWe can look at monthly rent payment averages for the different regions. Below we see lugar1 is the most expensive while lugar6 is the least expensive. This implies lugar6 will have more impoverished people."},{"metadata":{"id":"DxrwB1zAh1fU","outputId":"c2c63229-30a1-41c4-94d1-4f4ef5f6c775","trusted":true},"cell_type":"code","source":"for i in range(7):\n    for j in X.columns:\n        if j == \"lugar\" + str(i):\n            print(\"Average monthly rent payment for region lugar\" +str(i),\":\",X.groupby('lugar'+str(i), as_index=False)['v2a1'].mean().iloc[1,1])","execution_count":null,"outputs":[]},{"metadata":{"id":"8yrRJ-lzh1fU","trusted":true},"cell_type":"code","source":"warnings.filterwarnings(\"ignore\")\n\nregions_df = data[['lugar1','lugar2','lugar3','lugar4','lugar5','lugar6']]\nregions_df['regions']='' # to create an empty column\nregions_df2 = pd.get_dummies(regions_df).idxmax(1)\n\nX[\"regions\"] = regions_df2\n \nX[\"regions\"] = regions_df[\"regions\"]\nwarnings.resetwarnings()\n","execution_count":null,"outputs":[]},{"metadata":{"id":"bpps_BWvh1fU","outputId":"63c521f1-e454-40d7-d963-4940a2471e72","trusted":true},"cell_type":"code","source":"sns.set(rc={'figure.figsize':(11.7,8.27)})\nsns.lmplot(x=\"meaneduc\", y=\"v2a1\",row = \"regions\",col = \"urban\", data=X)","execution_count":null,"outputs":[]},{"metadata":{"id":"J_S9Or2Dh1fU"},"cell_type":"markdown","source":"<a id='urban_vs_rural'></a>\n\n## 3 m. Urban vs Rural"},{"metadata":{"id":"LjuBvKcsh1fU"},"cell_type":"markdown","source":"The urban variable may be useful in identifying target=4 (\"non-vulnerable households\"). There is not a significant difference in the proportion of urban vs rural for the other targets (see below for actual proportions)."},{"metadata":{"id":"5fvl0cx9h1fU","outputId":"60c0200a-0296-4168-c728-8f17b32c05de","trusted":true},"cell_type":"code","source":"ax = sns.countplot(x=y,hue=X[\"urban\"])\nax.set_xticklabels(ax.get_xticklabels(),rotation=45)","execution_count":null,"outputs":[]},{"metadata":{"id":"EKlyK63th1fV","outputId":"570554d2-7c34-45d0-e387-d06a140d4a9a","trusted":true},"cell_type":"code","source":"cross_tab_urban_target = pd.crosstab(y, X[\"urban\"], dropna=False)\nprops = cross_tab_urban_target[1]/(cross_tab_urban_target[0]+cross_tab_urban_target[1])\ncounter = 1\nfor i in props:\n    print(\"Proportion of Urban dwellers in target\",counter,\":\", i)\n    counter+=1\n    ","execution_count":null,"outputs":[]},{"metadata":{"id":"Uf6Xd5qUh1fV"},"cell_type":"markdown","source":"<a id='gender'></a>\n\n## 3 n. Gender Differences"},{"metadata":{"id":"ehSILgCKh1fV"},"cell_type":"markdown","source":"For gender, 0 indicates female and 1 indicates male. Women seem to be more likely to be impoverished than men since the differences in bar sizes are greater for classes 1 and 2."},{"metadata":{"id":"Gz-ZHXzEh1fV","outputId":"98513c69-2e89-4c8c-cbe3-c37915566564","trusted":true},"cell_type":"code","source":"ax = sns.countplot(x=y,hue=X[\"gender\"])\nax.set_xticklabels(ax.get_xticklabels(),rotation=45)\nax.set_title(\"Gender Distribution for Each Income Level\")","execution_count":null,"outputs":[]},{"metadata":{"id":"ybZR2IaNh1fV"},"cell_type":"markdown","source":"<a id='overcrowding'></a>\n\n## 3 o. Overcrowding"},{"metadata":{"id":"jTbm3XB0h1fV"},"cell_type":"markdown","source":"It is difficult to distinguish differences in overcrowding with the countplot below. However, when we calculate the proportion of homes with overcrowding for each target, we can see that lower income households have higher levels of overcrowding (7.5% for the most impoverished class compared to <1% for the least impoverished class)"},{"metadata":{"id":"vvonbxqah1fV","outputId":"5964a4f1-e9e2-48f6-ce3f-1d84b00061b8","trusted":true},"cell_type":"code","source":"ax = sns.countplot(x=y,hue=X[\"hacapo\"])\nax.set_xticklabels(ax.get_xticklabels(),rotation=45)\nax.set_title(\"Overcrowding of Rooms for Each Income Level\")","execution_count":null,"outputs":[]},{"metadata":{"id":"zXbpD_Jth1fV","outputId":"060fb346-a8ad-46dd-f788-f370fc9d3d70","trusted":true},"cell_type":"code","source":"cross_tab_overcrowding_target = pd.crosstab(y, X[\"hacapo\"], dropna=False)\nprops = cross_tab_overcrowding_target[1]/(cross_tab_overcrowding_target[0]+cross_tab_overcrowding_target[1])\ncounter = 1\nfor i in props:\n    print(\"Proportion of overcrowding in target\",counter,\":\", i)\n    counter+=1\n    ","execution_count":null,"outputs":[]},{"metadata":{"id":"rhKcUQZNh1fV"},"cell_type":"markdown","source":"<a id='house_quality'></a>\n\n## 3 p. Distribution of Feature Engineered House Quality Variables\n\nThe bar charts below demonstrate that the quality of the house is a strong predictor of the level of poverty."},{"metadata":{"id":"35z1tj8Fh1fV","outputId":"75dbd20d-e385-4cc9-915b-794d2c57e121","trusted":true},"cell_type":"code","source":"ax = sns.countplot(x=y,hue=X.house_poor_quality)\nax.set_xticklabels(ax.get_xticklabels(),rotation=45)\nax.set_title(\"Count of Homes with Either a Poor Quality Wall, Roof or Floor for Each Income Level\")","execution_count":null,"outputs":[]},{"metadata":{"id":"-cEB5SqIh1fV","outputId":"c9146413-e953-4a94-8677-a2cbde2447fc","trusted":true},"cell_type":"code","source":"cross_tab_house_quality_target = pd.crosstab(y, X.house_poor_quality, dropna=False)\nprops = cross_tab_house_quality_target[1]/(cross_tab_house_quality_target[0]+cross_tab_house_quality_target[1])\ncounter = 1\nfor i in props:\n    print(\"Proportion poor house quality in each target\",counter,\":\", i)\n    counter+=1","execution_count":null,"outputs":[]},{"metadata":{"id":"_NN2LEV6h1fV","outputId":"b2c88350-4300-4d80-f26b-0f8794bd19ba","trusted":true},"cell_type":"code","source":"ax = sns.countplot(x=y,hue=X.house_medium_quality)\nax.set_xticklabels(ax.get_xticklabels(),rotation=45)\nax.set_title(\"Count of Homes with Either a Poor Quality Wall, Roof or Floor for Each Income Level\")","execution_count":null,"outputs":[]},{"metadata":{"id":"Zn6R3lnmh1fV","outputId":"66ea797f-4d40-485e-d680-4e8b146c08fb","trusted":true},"cell_type":"code","source":"cross_tab_house_medium_quality_target = pd.crosstab(y, X.house_medium_quality, dropna=False)\nprops = cross_tab_house_medium_quality_target[1]/(cross_tab_house_medium_quality_target[0]+cross_tab_house_medium_quality_target[1])\ncounter = 1\nfor i in props:\n    print(\"Proportion medium house quality in each target\",counter,\":\", i)\n    counter+=1","execution_count":null,"outputs":[]},{"metadata":{"id":"9hyzxc2Ph1fW","outputId":"b36e726f-7e04-4681-ee4e-c79a6755b030","trusted":true},"cell_type":"code","source":"ax = sns.countplot(x=y,hue=X.house_good_quality)\nax.set_xticklabels(ax.get_xticklabels(),rotation=45)\nax.set_title(\"Count of Homes with Either a Good Quality Wall, Roof or Floor for Each Income Level\")","execution_count":null,"outputs":[]},{"metadata":{"id":"CeDpN4Bph1fW","outputId":"6fc1fc52-0439-4fc6-8ca8-1d3dc7a5e11e","trusted":true},"cell_type":"code","source":"cross_tab_house_good_quality_target = pd.crosstab(y, X.house_good_quality, dropna=False)\nprops = cross_tab_house_good_quality_target[1]/(cross_tab_house_good_quality_target[0]+cross_tab_house_good_quality_target[1])\ncounter = 1\nfor i in props:\n    print(\"Proportion good house quality in each target\",counter,\":\", i)\n    counter+=1","execution_count":null,"outputs":[]},{"metadata":{"id":"_wGdscM6h1fW"},"cell_type":"markdown","source":"<a id='dependency'></a>\n\n## 3 q. Dependency\n\nThe graph shows the distribution of dependency colored by the value of the Target. We can see that there isn’t a significant difference in the variable distribution depending on the household poverty level.\nThe rate is higher for non-vulnerable and vulnerable.\n"},{"metadata":{"id":"MNGjIO45h1fW","outputId":"97cc2185-ac8f-4753-af1b-6779c7506771","trusted":true},"cell_type":"code","source":"plt.figure(figsize = (15, 8))\nplt.style.use('fivethirtyeight')\n\n# Color mapping\ncolors = OrderedDict({1: 'red', 2: 'orange', 3: 'blue', 4: 'green'})\npoverty_mapping = OrderedDict({1: 'extreme', 2: 'moderate', 3: 'vulnerable', 4: 'non vulnerable'})\n\n# For Dependency column plot against the density\nfor i, col in enumerate(['dependency']):\n    ax = plt.subplot(4, 2, i + 1)\n    # Iterate through the poverty levels\n    for poverty_level, color in colors.items():\n        # Plot each poverty level as a separate line\n        sns.kdeplot(data.loc[data['Target'] == poverty_level, col].dropna(), \n                    ax = ax, color = color, label = poverty_mapping[poverty_level])\n        \n    plt.title(f'Dependency Distribution \\n'); plt.xlabel(f'dependency'); plt.ylabel('Density')\nplt.subplots_adjust(top = 2)","execution_count":null,"outputs":[]},{"metadata":{"id":"CLSPBskQh1fW"},"cell_type":"markdown","source":"<a id='age'></a>\n\n## 3 r. Age\n\n\nMost of the population lies within the age group of 20 years to 60 years.\nFor every age group, non vulnerable class is exceeding in numbers\nModerate poverty came in second place\n"},{"metadata":{"id":"6PJ4owfLh1fW","outputId":"7ff2e49b-3fe3-4fef-cdfd-2b57f33b2bd5","trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,6))\nplt.hist(X[\"age\"].loc[y == 1], bins = 30, alpha = 0.5, label = \"Extreme Poverty\")\nplt.hist(X[\"age\"].loc[y == 2], bins = 30, alpha = 0.5, label = \"Moderate Poverty\")\nplt.hist(X[\"age\"].loc[y == 3], bins = 30, alpha = 0.5, label = \"Vulnerable\")\nplt.hist(X[\"age\"].loc[y == 4], bins = 30, alpha = 0.5, label = \"Not Vulnerable\")\nplt.title(\"Distribution of Age by Poverty Level\", fontsize = 22)\nplt.xlabel('age', fontsize=15)\nplt.legend(loc='upper right', fontsize=15)\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"zGNcgp5rh1fW"},"cell_type":"markdown","source":"<a id='rent'></a>\n\n## 3 s. Monthly Rent Payment Distribution"},{"metadata":{"id":"a4yFFf4Qh1fW","outputId":"3d2a1b9f-6f44-4634-dd3b-cc81efd73bd4","trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,6))\nplt.hist(X[\"v2a1\"].loc[y == 1], bins = 15, alpha = 0.5, label = \"Extreme Poverty\")\nplt.hist(X[\"v2a1\"].loc[y == 2], bins = 15, alpha = 0.5, label = \"Moderate Poverty\")\nplt.hist(X[\"v2a1\"].loc[y == 3], bins = 15, alpha = 0.5, label = \"Vulnerable\")\nplt.hist(X[\"v2a1\"].loc[y == 4], bins = 15, alpha = 0.5, label = \"Not Vulnerable\")\nplt.title(\"Distribution of Monthly Rent Payments by Poverty Level\", fontsize = 22)\nplt.xlabel('v2a1', fontsize=15)\nplt.legend(loc='upper right', fontsize=15)\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"XyzvYXX5LQ3Q"},"cell_type":"markdown","source":"<a id='missing_rent'></a>\n\n## 3 t. Missing Rent Value Distribution"},{"metadata":{"id":"4WYZiDOMLPVO","outputId":"5a3ad318-f9ee-4bf1-a366-6724ffb6606f","trusted":true},"cell_type":"code","source":"ax = sns.countplot(x=y,hue=X[\"rent_missing\"])\nax.set_xticklabels(ax.get_xticklabels(),rotation=45)\nax.set_title(\"Missing Rent Distribution for Each Income Level\")","execution_count":null,"outputs":[]},{"metadata":{"id":"ORvHPdcSLVXq","trusted":true},"cell_type":"code","source":"X = X.drop(columns=['epared1'])\nX = X.drop(columns=['etecho1'])\nX = X.drop(columns=['eviv1'])\nX = X.drop(columns=['epared2'])\nX = X.drop(columns=['etecho2'])\nX = X.drop(columns=['eviv2'])\nX = X.drop(columns=['epared3'])\nX = X.drop(columns=['etecho3'])\nX = X.drop(columns=['eviv3'])","execution_count":null,"outputs":[]},{"metadata":{"id":"mCqJXtuUh1fW"},"cell_type":"markdown","source":"<a id='pca'></a>\n# 4. Data Reduction Techniques (PCA, KPCA & Feature Selection)\nWe scaled the data and performed PCA (including the number of components that captured 95% of the explainable variance).\n\n## 4 a. PCA\n"},{"metadata":{"id":"xRxvELM_h1fW","trusted":true},"cell_type":"code","source":"X = X.drop(columns=['regions'])\nstandard_scaler = preprocessing.StandardScaler().fit_transform(X.values)","execution_count":null,"outputs":[]},{"metadata":{"id":"h87-v0Tlh1fW","trusted":true},"cell_type":"code","source":"pca = PCA()\npca_components = pca.fit_transform(standard_scaler)","execution_count":null,"outputs":[]},{"metadata":{"id":"UBuvBwMLh1fW","outputId":"e35df250-3c43-412d-b7ad-da268177a378","trusted":true},"cell_type":"code","source":"#determine how many components to keep\ncounter = 0\nratio = 0\nfor i in range(len(pca.explained_variance_ratio_)):\n    if ratio < 0.95:\n        ratio += pca.explained_variance_ratio_[i]\n        counter +=1\n    else:\n        break\nprint(counter)","execution_count":null,"outputs":[]},{"metadata":{"id":"Eauf_OW6h1fW","outputId":"3e5245b8-f91e-4849-fed1-e7d8794a7c3c","trusted":true},"cell_type":"code","source":"print(f'Explained Variance: {round(sum(PCA(n_components=counter).fit(pca_components).explained_variance_ratio_), 2)}%')","execution_count":null,"outputs":[]},{"metadata":{"id":"c7axG0IVh1fW","outputId":"6ef33cdd-bf47-4f77-b992-ef9e5edf8251","trusted":true},"cell_type":"code","source":"#PCA Visualization\nnum_vars = len(pca.explained_variance_ratio_)\nfig = plt.figure()\nax = plt.axes()\nx = np.linspace(1, 121, 121)\ny_pca = np.cumsum(pca.explained_variance_ratio_)\nax.plot(x, y_pca)\nplt.ylabel('Percentage of Variance Explained')\nplt.xlabel('Number of Principal Components')\nplt.axvline(x=counter,color='k', linestyle='--')\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{"id":"AuFnFIjPh1fW","trusted":true},"cell_type":"code","source":"pca = PCA(n_components=counter)\nfeatures_norm_pca = pca.fit_transform(standard_scaler)","execution_count":null,"outputs":[]},{"metadata":{"id":"a0z33S68VRUV"},"cell_type":"markdown","source":"<a id='kpca'></a>\n## 4 b. Kernel PCA"},{"metadata":{"id":"_8XL1ywhVQi9","trusted":true},"cell_type":"code","source":"kpca = KernelPCA()\nkpca_components = kpca.fit_transform(standard_scaler)","execution_count":null,"outputs":[]},{"metadata":{"id":"IngJj6AtT2C_","trusted":true},"cell_type":"code","source":"explained_variance = np.var(kpca_components, axis=0)\nexplained_variance_ratio = explained_variance / np.sum(explained_variance)","execution_count":null,"outputs":[]},{"metadata":{"id":"ZhnOg4dQT5Mz","outputId":"6eab7ebb-e6e6-45b6-a7f5-d2fb7d3e11f9","trusted":true},"cell_type":"code","source":"#determine how many components to keep\ncounter = 0\nratio = 0\nfor i in range(len(explained_variance_ratio)):\n    if ratio < 0.95:\n        ratio += explained_variance_ratio[i]\n        counter +=1\n    else:\n        break\nprint(\"95% of the explainable variance is captured in KPCA when\", counter,\"components are included\")","execution_count":null,"outputs":[]},{"metadata":{"id":"wPcW-XWDT9MI","trusted":true},"cell_type":"code","source":"kpca = KernelPCA(n_components=counter, kernel='rbf', n_jobs=-1, remove_zero_eig=True)\nkpca_components = kpca.fit_transform(standard_scaler)","execution_count":null,"outputs":[]},{"metadata":{"id":"1M6pArP1YGPv"},"cell_type":"markdown","source":"We can test a model using KPCA components to determine the effectiveness of this data reduction strategy for this data. We will use the "},{"metadata":{"id":"d9axeJ0_X1Pi","outputId":"041965c9-b366-47c5-852f-bc1cb81b88a3","trusted":true},"cell_type":"code","source":"X_train_kpca, X_val_kpca, y_train_kpca, y_val_kpca = train_test_split(kpca_components, y, test_size=0.30, random_state=2020)\nlr_kpca = LogisticRegression(max_iter = 10000, solver='lbfgs', penalty='l2', C=0.1,multi_class=\"multinomial\")\nlr_kpca.fit(X_train_kpca,y_train_kpca)\nlog_reg_pred_train_kpca = lr_kpca.predict(X_train_kpca)\n\nlog_reg_pred_test_kpca = lr_kpca.predict(X_val_kpca)\nprint(\"Logistic regression with KPCA training set accuracy:\",round(metrics.accuracy_score(y_true = y_train_kpca, y_pred = log_reg_pred_train_kpca),4))\nprint(\"Logistic regression with KPCA testing set accuracy:\",round(metrics.accuracy_score(y_true = y_val_kpca, y_pred = log_reg_pred_test_kpca),4))","execution_count":null,"outputs":[]},{"metadata":{"id":"82b7hDNlZEW-","outputId":"e48c11e9-de89-4011-e0a5-c26aa8097aec","trusted":true},"cell_type":"code","source":"print(classification_report(y_train_kpca, log_reg_pred_train_kpca, target_names=[\"Extreme Poverty\",\"Moderate Poverty\",\"Vulnerable\",\"Not Vulnerable\"]))\nprint(\"Validation set classification report:\")\nprint(classification_report(y_val_kpca, log_reg_pred_test_kpca, target_names=[\"Extreme Poverty\",\"Moderate Poverty\",\"Vulnerable\",\"Not Vulnerable\"]))","execution_count":null,"outputs":[]},{"metadata":{"id":"DrlqsQsSZZyd"},"cell_type":"markdown","source":"As we will see later, KPCA has very similar results compared to PCA. Given the similarities, rather than doing both PCA and KPCA for each model we will focus our analysis on PCA.\n\n\n\n"},{"metadata":{"id":"BMi8tr8Zh2zF"},"cell_type":"markdown","source":"<a id='feature_selection'></a>\n## 4 c. Feature Selection Using Feature Importance From a Logistic Regression Model\n\nUsing logistic regression, we can take *all* standardized features, split these features into train and test sets, then rank those features based on the values of the calculated coefficients. (As a note we tried multiple numbers of features to include and 75-80 seems to be ideal. This is also approximately the number of components PCA calculates to capture 95% of variance). This further indicates that for optimal performance, we need to include around 75-80 features regardless of data reduction strategy."},{"metadata":{"id":"zg7EIeb1iKaC","outputId":"9f7b0936-eb2a-4259-d39c-caf95e67ebca","trusted":true},"cell_type":"code","source":"X_train_fs, X_val_fs, y_train_fs, y_val_fs = train_test_split(standard_scaler, y, test_size=0.30, random_state=2020)\n\nlr_fs = LogisticRegression(max_iter = 10000, solver='lbfgs', penalty='l2', C=0.1,multi_class=\"multinomial\")\nlr_fs.fit(X_train_fs,y_train_fs)\nlog_reg_pred_train_fs = lr_fs.predict(X_train_fs)\n\nlog_reg_pred_test_fs = lr_fs.predict(X_val_fs)\nprint(\"Logistic regression training set accuracy:\",round(metrics.accuracy_score(y_true = y_train_fs, y_pred = log_reg_pred_train_fs),4))\nprint(\"Logistic regression testing set accuracy:\",round(metrics.accuracy_score(y_true = y_val_fs, y_pred = log_reg_pred_test_fs),4))","execution_count":null,"outputs":[]},{"metadata":{"id":"ruSJM41Iil98","trusted":true},"cell_type":"code","source":"selector = RFE(lr_fs, 10)\nselector = selector.fit(X_train_fs, y_train_fs)","execution_count":null,"outputs":[]},{"metadata":{"id":"G6M-27GHiukD","outputId":"2e8afe41-d413-429a-d8c4-d6a27376a8d4","trusted":true},"cell_type":"code","source":"order = selector.ranking_\nfeature_ranks = []\nfor i in range(len(order)):\n  if order[i] < 70:\n    feature_ranks.append(X.columns[i])\nprint(\"Number of features included:\", len(feature_ranks))","execution_count":null,"outputs":[]},{"metadata":{"id":"ThKMEtnxWy1X"},"cell_type":"markdown","source":"<a id='research_questions'></a>\n# 5. Research Questions\n\n<a id='research_question_1'></a>\n#Research Question 1: Can we use PCA to identify the main components that determine poverty level?\n\nAs discussed earlier, we used PCA to identify the components that make up 95% of the explained variance. To determine variables that were important in the construction of the principal components, we can look at loading scores which are coefficients of the linear combination of the scaled attributes. Loading scores were moderately low, so we can say scores with absolute values of 0.15 indicate the attribute was important for that components. Below are the features with high loading scores for the first 3 components and their associated group as designated by the data dictionary built earlier\n\n**PC1**\n\n* **r4h3**: Features Describing Number of People Living in the Household\n* **r4m2**: Features Describing Number of People Living in the Household\n* **r4m3**: Features Describing Number of People Living in the Household\n* **r4t1**: Features Describing Number of People Living in the Household\n* **r4t2**: Features Describing Number of People Living in the Household\n* **r4t3**: Features Describing Number of People Living in the Household\n* **tamviv**: Features Describing Number of People Living in the Household\n* **tamhog**: Variables Related to House Size\n* **hhsize**: Variables Related to House Size\n* **hogar_adul**: Features Describing Number of Individuals Living in Household (Within Different Age Groups)\n* **hogar_total**: Features Describing Number of Individuals Living in Household (Within Different Age Groups)\n\n**PC2**\n\n* **v2a1**: Monthly Rent Payment\n* **paredblolad**: Variables Describing Materials of Wall, Floor, or Roof\n* **pisomoscer**: Variables Describing Materials of Wall, Floor, or Roof\n* **pisocemento**: Variables Describing Materials of Wall, Floor, or Roof\n* **cielorazo**: Variables Describing Materials of Wall, Floor, or Roof\n\n* **escolari**: Features Describing Education Level of Head of Household & Members of the Household\n* **edjefe** Features Describing Education Level of Head of Household & Members of the Household\n* **edjefa** Features Describing Education Level of Head of Household & Members of the Household\n* **meaneduc**: Features Describing Education Level of Head of Household & Members of the Household\n* **elimbasu1**: Features Describing How Trash is Disposed\n* **dependency**: Features Describing Number of Individuals Living in Household (Within Different Age Groups)\n\n* **qmobilephone**: Features Indicating Types of Technology Owned\n* **lugar1**: Features Describing Region Household is Located\n* **house_good_quality**: Variables Describing Quality of House (Good,Avg,Poor)\n* **bedrooms**: Features Describing Number of Rooms the Household Has\n* **rooms**: Features Describing Number of Rooms the Household Has\n\n**PC3**\n* **r4h1**: Features Describing Number of People Living in the Household\n* **r4h2**: Features Describing Number of People Living in the Household\n* **r4m1**: Features Describing Number of People Living in the Household\n* **r4t1**: Features Describing Number of People Living in the Household\n* **r4t2**: Features Describing Number of People Living in the Household\n* **estadocivil1**: Features Describing Status of Head of Household\n* **hogar_nin**: Features Describing Number of Individuals Living in Household (Within Different Age Groups)\n* **hogar_adul**: Features Describing Number of Individuals Living in Household (Within Different Age Groups)\n* **hogar_mayor**: Features Describing Number of Individuals Living in Household (Within Different Age Groups)\n* **instlevel1**: Features Describing Education Level of Head of Household & Members of the Household\n* **bedrooms**: Features Describing Number of Rooms the Household Has\n* **tipovivi1**: Features Decribing Rent/Mortgage Payment\n* **tipovivi3**: Features Decribing Rent/Mortgage Payment\n* **rent_missing**: Features Decribing Rent/Mortgage Payment\n\n**Analysis of loadings**:\n\nBased on these results, Principal Component 1 is built almost entirely from features describing the size of the household (# of people and physical size). The expectation is that these would be good variables for distinguishing poverty level: wealthier people will generally have larger homes. We may also expect poorer people to fit more people into a single home than a wealthier house would.\n\nPrincipal Component 2 has mostly variables describing the house materials, education level of people in the house, as well as the region and if the overall house quality is good. Lugar1 is the region with the highest average rent payments, and the variable indicating if the house quality is good would be best at identifying more afflued households. Therefore, *the features that build PC2 are better at identifying less vulnerable households.* \n\nPrincipal Component 3 Is similar to principal component 1 but had a strong loading from the \"rent_missing\" attribute. The presumption is that poorer households have more difficulty in providing records regarding finances (e.g. rent), so missing values for rent may be an indicator for more impoverished household. There, *the features that build PC3 are better at identifying more vulnerable households.*\n\n"},{"metadata":{"id":"MRX6mh5rXqo3","outputId":"e60d3481-25d2-4627-b7ee-2361131fc76a","trusted":true},"cell_type":"code","source":"pca_find_vars = PCA(n_components=3)\npca_find_vars.fit_transform(standard_scaler)\nprint(pd.DataFrame(pca_find_vars.components_,columns=X.columns,index = ['PC-1','PC-2', 'PC-3']))","execution_count":null,"outputs":[]},{"metadata":{"id":"xFaPmbtSaaY-"},"cell_type":"markdown","source":"**PCA & Feature Importance**\n\nAdditionally, we can supplement this principal component analysis with the feature importance rankings determined by the logistic regression used earlier (top 10 below). All but two were strong contributors to the first 3 principal components, indicating that these features are strong predictors of poverty\n\n**v2a1** --> PC2\n\n**v18q** --> Not in first 3 PCs\n\n**r4t2** --> PC1 & PC2\n\n**tamviv** --> PC1\n\n**coopele** --> Not in first 3 PCs\n\n**hogar_nin** --> PC3\n\n**hogar_adul** --> PC3\n\n**edjefa** --> PC2\n\n**meaneduc** --> PC2\n\n**qmobilephone** --> PC2"},{"metadata":{"id":"-M9l57ALa-TB","outputId":"3755ed6a-5568-498c-c416-b7e7b6521734","trusted":true},"cell_type":"code","source":"for i in range(len(order)):\n  if order[i] == 1:\n    print(X.columns[i])","execution_count":null,"outputs":[]},{"metadata":{"id":"V8K3t-U0nU4P"},"cell_type":"markdown","source":"<a id='research_question_2'></a>\n\n#Research Question 2: Can we develop a model better than our baseline model?\n\nWe decided to use a K-Nearest Neighbors algorithm to act as a baseline model. Given the imbalanced nature of the data, a KNN algorithm should fair well even without any hyperparameter tuning. The baseline model uses no data transformation (scaling, data reduction, resampling, etc)."},{"metadata":{"id":"E9z4SwXeoi9C","trusted":true},"cell_type":"code","source":"X_train_base, X_val_base, y_train_base, y_val_base = train_test_split(X, y, test_size=0.30, random_state=2020)","execution_count":null,"outputs":[]},{"metadata":{"id":"FsFNUP1ZolGG","outputId":"8526d876-7a58-472c-a327-8e3eae242d30","trusted":true},"cell_type":"code","source":"clf_knn_base = KNeighborsClassifier()\nclf_knn_base.fit(X_train_base, y_train_base)\nKNN_base_fit_train = clf_knn_base.predict(X_train_base)\nKNN_base_fit_val = clf_knn_base.predict(X_val_base)\nprint(\"Training set classification report:\")\nprint(classification_report(y_train_base, KNN_base_fit_train, target_names=[\"Extreme Poverty\",\"Moderate Poverty\",\"Vulnerable\",\"Not Vulnerable\"]))\nprint(\"Validation set classification report:\")\nprint(classification_report(y_val_base, KNN_base_fit_val, target_names=[\"Extreme Poverty\",\"Moderate Poverty\",\"Vulnerable\",\"Not Vulnerable\"]))","execution_count":null,"outputs":[]},{"metadata":{"id":"VNp9T-NVpIkQ"},"cell_type":"markdown","source":"There is a pretty striking difference in results between the training set and test set for the KNN model (overfitting). The baseline model indicates that the data to provide some predictive power, and we will investigate many algorithms to determine which is best at solving our problem."},{"metadata":{"id":"zfj26OY9qB7H"},"cell_type":"markdown","source":"<a id='research_question_3'></a>\n\n#Research Question 3: How important are regional differences?\n\nTo understand the generalizability of the models we develop, we should understand regional differences in our data and see if predictive modeling is similar regardless of region.\n\nFirst we compared identified general differences between regions based on the rent variable. We can see that Lugar1 pays the most rent while Lugar6 pays the least (on average)."},{"metadata":{"id":"aM-Su_eTrFe9","outputId":"2d94e956-8c69-47a5-dfb0-ccde0220ccf3","trusted":true},"cell_type":"code","source":"for i in range(7):\n    for j in X.columns:\n        if j == \"lugar\" + str(i):\n            print(\"Average monthly rent payment for region lugar\" +str(i),\":\",X.groupby('lugar'+str(i), as_index=False)['v2a1'].mean().iloc[1,1])","execution_count":null,"outputs":[]},{"metadata":{"id":"GKwt1L1NrYJt"},"cell_type":"markdown","source":"More than half of all observations come from lugar1. We will build a logistic regression model with default parameters for two subsets of the data: 1 for observations in lugar1 and another for observations not in lugar1. We will also scale the attributes."},{"metadata":{"id":"YuXOJGiDrfIA","outputId":"08feaa72-c678-4c17-c20f-d8f6e574b9b8","trusted":true},"cell_type":"code","source":"X_lugar_data = X\nX_lugar_data[\"target\"] = y\n\nX_lugar1 = X_lugar_data[X_lugar_data[\"lugar1\"]==1]\nX_lugar_other = X_lugar_data[(X_lugar_data[\"lugar6\"]==1) | (X_lugar_data[\"lugar5\"]==1) | (X_lugar_data[\"lugar4\"]==1) | (X_lugar_data[\"lugar3\"]==1)]\nprint(\"Number of households in lugar1:\",X_lugar1.shape[0])\nprint(\"Number of households not it lugar 1:\", X_lugar_other.shape[0])","execution_count":null,"outputs":[]},{"metadata":{"id":"CanuVfVNu8zE","trusted":true},"cell_type":"code","source":"y_lugar1 = X_lugar1[\"target\"]\ny_lugar_other = X_lugar_other[\"target\"]\nX_lugar1 = X_lugar1.drop(columns=['target'])\nX_lugar_other = X_lugar_other.drop(columns=['target'])\n\n","execution_count":null,"outputs":[]},{"metadata":{"id":"-bAAOwJwuCRD","trusted":true},"cell_type":"code","source":"standard_scaler_lugar1 = preprocessing.StandardScaler().fit_transform(X_lugar1.values)\nstandard_scaler_lugar_other = preprocessing.StandardScaler().fit_transform(X_lugar_other.values)\nX_train_lugar1, X_val_lugar1, y_train_lugar1, y_val_lugar1 = train_test_split(standard_scaler_lugar1, y_lugar1, test_size=0.30, random_state=2020)\nX_train_lugar_other, X_val_lugar_other, y_train_lugar_other, y_val_lugar_other = train_test_split(standard_scaler_lugar_other, y_lugar_other, test_size=0.30, random_state=2020)\n","execution_count":null,"outputs":[]},{"metadata":{"id":"Nap48iwevtsk","outputId":"769f8e44-bf56-4830-fd82-c4c183554de0","trusted":true},"cell_type":"code","source":"lr_lugar1 = LogisticRegression(max_iter = 10000,multi_class=\"multinomial\")\nlr_lugar1.fit(X_train_lugar1,y_train_lugar1)\nlog_reg_pred_train_lugar1 = lr_lugar1.predict(X_train_lugar1)\n\nlog_reg_pred_test_lugar1 = lr_lugar1.predict(X_val_lugar1)\nprint(\"Logistic regression training set accuracy for Lugar 1:\",round(metrics.accuracy_score(y_true = y_train_lugar1, y_pred = log_reg_pred_train_lugar1),4))\nprint(\"Logistic regression testing set accuracy for Lugar 1:\",round(metrics.accuracy_score(y_true = y_val_lugar1, y_pred = log_reg_pred_test_lugar1),4))","execution_count":null,"outputs":[]},{"metadata":{"id":"OtxtAOizwYiU","outputId":"61ba04c9-901b-403f-af78-25f4b843fc6c","trusted":true},"cell_type":"code","source":"\nprint(classification_report(y_train_lugar1, log_reg_pred_train_lugar1, target_names=[\"Extreme Poverty\",\"Moderate Poverty\",\"Vulnerable\",\"Not Vulnerable\"]))\nprint(\"Validation set classification report:\")\nprint(classification_report(y_val_lugar1, log_reg_pred_test_lugar1, target_names=[\"Extreme Poverty\",\"Moderate Poverty\",\"Vulnerable\",\"Not Vulnerable\"]))\n","execution_count":null,"outputs":[]},{"metadata":{"id":"KLwh5e-Lw1zu","outputId":"a6bd26a4-ea3f-4719-9a4f-dcb07b69e12c","trusted":true},"cell_type":"code","source":"lr_lugar_other = LogisticRegression(max_iter = 10000,multi_class=\"multinomial\")\nlr_lugar_other.fit(X_train_lugar_other,y_train_lugar_other)\nlog_reg_pred_train_lugar_other = lr_lugar_other.predict(X_train_lugar_other)\n\nlog_reg_pred_test_lugar_other = lr_lugar_other.predict(X_val_lugar_other)\nprint(\"Logistic regression training set accuracy for regions not in Lugar 1:\",round(metrics.accuracy_score(y_true = y_train_lugar_other, y_pred = log_reg_pred_train_lugar_other),4))\nprint(\"Logistic regression testing set accuracy for regions not in Lugar 1:\",round(metrics.accuracy_score(y_true = y_val_lugar_other, y_pred = log_reg_pred_test_lugar_other),4))","execution_count":null,"outputs":[]},{"metadata":{"id":"6c2KIW3GxUYw","outputId":"d59d58e4-9a1b-4c46-f344-cfbb5618a268","trusted":true},"cell_type":"code","source":"print(classification_report(y_train_lugar_other, log_reg_pred_train_lugar_other, target_names=[\"Extreme Poverty\",\"Moderate Poverty\",\"Vulnerable\",\"Not Vulnerable\"]))\nprint(\"Validation set classification report:\")\nprint(classification_report(y_val_lugar_other, log_reg_pred_test_lugar_other, target_names=[\"Extreme Poverty\",\"Moderate Poverty\",\"Vulnerable\",\"Not Vulnerable\"]))","execution_count":null,"outputs":[]},{"metadata":{"id":"i33rYuiAxwxt"},"cell_type":"markdown","source":"Interestingly, the logistic regression models perform better on both subsets of the data compared to when the entire data is used (as we will see later). This indicates that models may be better applied on more homogeneous groups than taking many groups and putting them together in a single dataset.\n\nWe are also interested in seeing differences in the region in a more general sense: rural vs urban. We can treat rural vs urban as a target variable and treat all other other variables as explanatory features. The plot below using the average education of members of the household shows that there will clearly be differences between rural and urban (urban areas have a much larger proportion of people with a mean education greater than 15)\n"},{"metadata":{"id":"62M0bqBCnTFp","outputId":"85c88652-21b5-496b-c329-9dfedf30e465","trusted":true},"cell_type":"code","source":"sns.set(rc={'figure.figsize':(11.7,8.27)})\nsns.lmplot(x=\"meaneduc\", y=\"v2a1\",col = \"urban\", data=X)","execution_count":null,"outputs":[]},{"metadata":{"id":"7EcCCqDSnvJK"},"cell_type":"markdown","source":"We ran our SVC and KNN classifiers trying to predict region to see if either method could identify regional differences."},{"metadata":{"id":"9T38LwMzn5at","trusted":true},"cell_type":"code","source":"y_reg = X[\"urban\"]\nX_train_reg, X_val_reg, y_train_reg, y_val_reg = train_test_split(standard_scaler, y_reg, test_size=0.30, random_state=2020)","execution_count":null,"outputs":[]},{"metadata":{"id":"QFosvnmMn8rD","outputId":"c90418ba-2fd9-4754-f070-110ae6cfe314","trusted":true},"cell_type":"code","source":"knn_urban_rural = KNeighborsClassifier()\nknn_urban_rural.fit(X_train_reg, y_train_reg)\nknn_urban_rural_val_pred = knn_urban_rural.predict(X_val_reg)\nprint(classification_report(y_val_reg, knn_urban_rural_val_pred,target_names=[\"Rural\",\"Urban\"]))","execution_count":null,"outputs":[]},{"metadata":{"id":"2v2AzJhQox2B","outputId":"739c1b9d-0f4f-47ac-eb9f-fa075c06aef1","trusted":true},"cell_type":"code","source":"disp = plot_confusion_matrix(knn_urban_rural, X_val_reg, y_val_reg,\n                                 display_labels=[\"urban\", \"rural\"],\n                                cmap=plt.cm.Blues,values_format = '')\ndisp.ax_.set_title(\"Confusion matrix\")\ndisp.im_.colorbar.remove()\nprint(disp)","execution_count":null,"outputs":[]},{"metadata":{"id":"p-yFcK1JrS7Y"},"cell_type":"markdown","source":"The confusion matrix above demonstrates that urban vs rural is much easier to predict in the data than poverty (however recall is slightly weak with many false positives, possibly due to class imbalance)\n\n## 4. Can we build a useful model using only data on the house (traditional PMT attributes)?\n\nGiven that one of the reasons this competition was created was to see how non-traditional PMT features would help in the prediction of social welfare need, we wanted to create our own traditional PMT model as a means of comparison to models that utilize traditional and non-traditional PMT features. First we need to subset the data based on these features (as determined by our data dictionary)"},{"metadata":{"id":"nJX_BDFDfuYm","trusted":true},"cell_type":"code","source":"houseFactors_subset = X.loc[:,[\"tamhog\", \"hhsize\",\"paredblolad\", \"paredzocalo\", \"paredpreb\", \"pareddes\" , \"paredmad\", \"paredzinc\", \"paredfibras\", \"paredother\", \"pisomoscer\", \"pisocemento\", \"pisoother\", \"pisonatur\", \"pisonotiene\", \"pisomadera\", \"techozinc\", \"techoentrepiso\", \"techocane\", \"techootro\", \"cielorazo\",\"house_poor_quality\",\"house_medium_quality\",\"house_good_quality\",\"lugar1\", \"lugar2\", \"lugar3\", \"lugar4\", \"lugar5\", \"lugar6\",\"urban\"]]\nmin_max_scaler = preprocessing.MinMaxScaler().fit(houseFactors_subset.values)\nfeatures_norms = min_max_scaler.transform(houseFactors_subset.values)\nX_train_PMT, X_val_PMT, y_train_PMT, y_val_PMT = train_test_split(features_norms, y, test_size=0.30, random_state=2020)","execution_count":null,"outputs":[]},{"metadata":{"id":"XUk0lWkDhEO-"},"cell_type":"markdown","source":"Using a train-test split of normalized PMT features, we will try two models: KNN and SVC to determine the predictive performance of these features.\n\n**KNN with Traditional PMT Features**"},{"metadata":{"id":"pL5uv_L-hD-K","outputId":"e236b115-bcf1-4ad1-8102-9764696e60e7","trusted":true},"cell_type":"code","source":"clf = KNeighborsClassifier()\nclf.fit(X_train_PMT, y_train_PMT)","execution_count":null,"outputs":[]},{"metadata":{"id":"iFG-cs-BhYjE","outputId":"c7831f54-48f5-4074-cb4a-daa9489c7c1a","trusted":true},"cell_type":"code","source":"KNN_Fit_PMT = clf.predict(X_val_PMT)\nprint('KNN PMT Classification Report: ')\nprint(classification_report(y_val_PMT, KNN_Fit_PMT, target_names=[\"Extreme Poverty\",\"Moderate Poverty\",\"Vulnerable\",\"Not Vulnerable\"]))","execution_count":null,"outputs":[]},{"metadata":{"id":"j-x-OKAZiIFS"},"cell_type":"markdown","source":"**SVC with Traditional PMT Features**"},{"metadata":{"id":"yFlOujOJiCE-","outputId":"728064ed-d216-44a1-f544-bee3b2868114","trusted":true},"cell_type":"code","source":"svc = SVC(kernel ='rbf', C = 20, gamma = 'scale')\nsvc.fit(X_train_PMT, y_train_PMT)\nSVC_fit_PMT = svc.predict(X_val_PMT)\nprint('SVC PMT Classification Report: ')\nprint(classification_report(y_val_PMT, SVC_fit_PMT, target_names=[\"Extreme Poverty\",\"Moderate Poverty\",\"Vulnerable\",\"Not Vulnerable\"]))","execution_count":null,"outputs":[]},{"metadata":{"id":"A_jTWY8qiXG2"},"cell_type":"markdown","source":"From the above Classification Reports, performance is actually quite strong (macro F-1 Average scores of 0.4 or higher are quite good for this data). Both of these models actually do fairly well at identifying classes with fewer samples (compared to other models analyzed later with recall and f-1 scores below 0.2). We will see later that the difference in KNN is minimal while SVC has a moderate drop in performance when only using traditional PMT variables. This is likely due to the fact that the parameters that we removed were critical to the boundary and thus the preformance of the classifier.\n"},{"metadata":{"id":"COgOhuuAh1fX"},"cell_type":"markdown","source":"<a id='models'></a>\n# 6. Models\n\nThis section includes code for running machine learning models and hyperparameter tuning. See section 6 for experiment results.\n\n## 6 a. Train-Test Split\n\nModels will use the PCA components calculate earlier, which are split into train and test sets. Traditional and Non-Traditional PMT features were included for this section."},{"metadata":{"id":"uwfeOL1Uh1fX","trusted":true},"cell_type":"code","source":"X_train, X_val, y_train, y_val = train_test_split(features_norm_pca, y, test_size=0.30, random_state=2020)","execution_count":null,"outputs":[]},{"metadata":{"id":"vT7tNISNh1fX"},"cell_type":"markdown","source":"## 6 b. Logistic Regression\n<a id='logistic_regression'></a>\n\nWe will first build a logistic regression model without gridsearch as a baseline to compare against hyperparameter tuning."},{"metadata":{"id":"iM4Grfych1fX","outputId":"4b98ec3f-1b94-42c5-e794-6242f4b872a0","trusted":true},"cell_type":"code","source":"lr1 = LogisticRegression(max_iter = 10000,multi_class=\"multinomial\")\nlr1.fit(X_train,y_train)\nlog_reg_pred_train = lr1.predict(X_train)\n\nlog_reg_pred_test = lr1.predict(X_val)\nprint(\"Logistic regression training set accuracy:\",round(metrics.accuracy_score(y_true = y_train, y_pred = log_reg_pred_train),4))\nprint(\"Logistic regression testing set accuracy:\",round(metrics.accuracy_score(y_true = y_val, y_pred = log_reg_pred_test),4))","execution_count":null,"outputs":[]},{"metadata":{"id":"zgnZXGZOh1fX","trusted":true},"cell_type":"code","source":"def get_cv_accuracies(model_fit, n_folds = 5):\n    results = model_fit.cv_results_\n    accuracies_lst = []\n    accuracy = 0\n    for i in range(len(results[\"mean_fit_time\"])):\n        for j in range(n_folds):\n            accuracy += results[\"split\"+str(j)+\"_test_score\"][i]\n        accuracies_lst.append(accuracy/n_folds)\n        accuracy = 0\n    indx = accuracies_lst.index(max(accuracies_lst))\n    best_param = results['params'][indx]\n    \n    return best_param","execution_count":null,"outputs":[]},{"metadata":{"id":"BrBqvqN3h1fX"},"cell_type":"markdown","source":"Logistic regression with gridsearch cv."},{"metadata":{"id":"9CLRBeZPh1fX","outputId":"f84b9caf-60e8-44be-8f46-0cb8f275fec0","trusted":true},"cell_type":"code","source":"grid={\"C\":np.logspace(-3,3,7)}\nlogreg=LogisticRegression(max_iter=10000,multi_class=\"multinomial\",penalty= 'l2')\nlogreg_cv=GridSearchCV(logreg,grid,cv=10)\nlogreg_cv.fit(X_train,y_train)\n\n\nprint(\"Best Parameters:\",get_cv_accuracies(logreg_cv))","execution_count":null,"outputs":[]},{"metadata":{"id":"8bOXRXreh1fX","outputId":"6f65efc8-1b3b-453f-a794-1c664ca0b1f0","trusted":true},"cell_type":"code","source":"lr1 = LogisticRegression(max_iter = 10000, solver='lbfgs', penalty='l2', C=10,multi_class=\"multinomial\")\nlr1.fit(X_train,y_train)\nlog_reg_pred_train = lr1.predict(X_train)\nlog_reg_pred_test = lr1.predict(X_val)\nprint(\"Logistic regression training set accuracy:\",round(metrics.accuracy_score(y_true = y_train, y_pred = log_reg_pred_train),4))\nprint(\"Logistic regression testing set accuracy:\",round(metrics.accuracy_score(y_true = y_val, y_pred = log_reg_pred_test),4))","execution_count":null,"outputs":[]},{"metadata":{"id":"b1dRhhGuh1fX"},"cell_type":"markdown","source":"# <a id='adaboost'></a>\n## 6 c. AdaBoost Model"},{"metadata":{"id":"pcnN6i5Uh1fX","trusted":true},"cell_type":"code","source":"ac = AdaBoostClassifier()\nac.fit(X_train,y_train.ravel())\n\npred9 = ac.predict(X_train)\npred10 = ac.predict(X_val)","execution_count":null,"outputs":[]},{"metadata":{"id":"KsiJpyLjh1fX","outputId":"e6d9f5e4-ed9f-485e-fbfd-19bb9cb02849","trusted":true},"cell_type":"code","source":"pred_ada_train = ac.predict(X_train) # make prediction on the training set \npred_ada_test = ac.predict(X_val) # make prediction on the testing set \n\nprint(\"The training accuacy is: \",metrics.accuracy_score(y_true = y_train, y_pred = pred_ada_train))\nprint(\"The testing accuacy is: \",metrics.accuracy_score(y_true = y_val, y_pred = pred_ada_test))","execution_count":null,"outputs":[]},{"metadata":{"id":"4zKm-Nw2h1fY"},"cell_type":"markdown","source":"# <a id='gnb'></a>\n## 6 d. Gaussian Naive Bayes Classifier "},{"metadata":{"id":"h_IuC_q2h1fY","outputId":"3690b147-2e3d-4c23-dd94-33b6b4df5bc7","trusted":true},"cell_type":"code","source":"clf_nb = GaussianNB()\nclf_nb.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"id":"rLnGFzK_h1fY","outputId":"6ffd206c-0fce-42ed-c5bf-724b0941b049","trusted":true},"cell_type":"code","source":"pred_train = clf_nb.predict(X_train)\nmetrics.accuracy_score(y_true = y_train, y_pred = pred_train)\nprint(\"Gaussian Naive Bayes classifier training set accuracy:\",round(metrics.accuracy_score(y_true = y_train, y_pred = pred_train),4))","execution_count":null,"outputs":[]},{"metadata":{"id":"9y9M29-mh1fY","outputId":"67a05440-a5c4-4cae-b73a-549add90b835","trusted":true},"cell_type":"code","source":"pred_nb = clf_nb.predict(X_val)\nmetrics.accuracy_score(y_true = y_val, y_pred = pred_nb)\nprint(\"Gaussian Naive Bayes classifier testing set accuracy:\",round(metrics.accuracy_score(y_true = y_val, y_pred = pred_nb),4))","execution_count":null,"outputs":[]},{"metadata":{"id":"oXWwtQQqh1fY","outputId":"ce233adf-b4af-4a90-ad92-63647773822c","trusted":true},"cell_type":"code","source":"param_grid = {\n    'var_smoothing': [1e-11, 1e-10, 1e-09, 1e-08, 1e-07, 1e-5],\n    }\n\ngrid_search = GridSearchCV(estimator = clf_nb, param_grid = param_grid, cv = 5)\ngrid_search.fit(features_norm_pca, y)\n\nprint(grid_search.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"id":"ZpEZ-G_Eh1fY","trusted":true},"cell_type":"code","source":"clf_nb2 = GaussianNB(var_smoothing = 1e-11)\nclf_nb2.fit(X_train, np.ravel(y_train))\nNB_fit_train = clf_nb2.predict(X_train)\nNB_fit_CV5 = clf_nb2.predict(X_val)","execution_count":null,"outputs":[]},{"metadata":{"id":"YsNCmcdmh1fY"},"cell_type":"markdown","source":"<a id='decision_tree'></a>\n\n## 6 e. Decision Tree"},{"metadata":{"id":"9n9vF1rJh1fY","trusted":true},"cell_type":"code","source":"def print_results(results):\n    print('BEST PARAMS: {}\\n'.format(results.best_params_))\n\n    means = results.cv_results_['mean_test_score']\n    stds = results.cv_results_['std_test_score']\n    for mean, std, params in zip(means, stds, results.cv_results_['params']):\n        print('{} (+/-{}) for {}'.format(round(mean, 3), round(std * 2, 3), params))","execution_count":null,"outputs":[]},{"metadata":{"id":"xJvogGnOh1fY","outputId":"dcaeb28c-31ba-4a8d-8dd4-8270b32e66c5","trusted":true},"cell_type":"code","source":"tree_clf = DecisionTreeClassifier()\n\nparameters = {\"min_samples_split\":[50,100,200],\n             \"max_depth\":[1,5,10,50,100]}\n\ntree_clf.fit(X_train,y_train.ravel()) # fit the model","execution_count":null,"outputs":[]},{"metadata":{"id":"IK3dY4DRh1fY","outputId":"c10bfd77-67df-44fd-c7a9-7eb8bfa42d42","trusted":true},"cell_type":"code","source":"# find the best model use scoring = balanced_accuracy\ngrid_cv1 = GridSearchCV(estimator=tree_clf, param_grid=parameters, cv=5,scoring=\"balanced_accuracy\") \ngrid_cv1.fit(X_train,y_train.ravel()) \n\nprint_results(grid_cv1)","execution_count":null,"outputs":[]},{"metadata":{"id":"XdboAHwhh1fZ","outputId":"0863a87d-0943-4be7-c31d-8b69e708a50f","trusted":true},"cell_type":"code","source":"# use f1_macro scoring \ngrid_cv2 = GridSearchCV(estimator=tree_clf, param_grid=parameters, cv=5,scoring=\"f1_macro\") \ngrid_cv2.fit(X_train,y_train.ravel()) \n\nprint_results(grid_cv2)","execution_count":null,"outputs":[]},{"metadata":{"id":"alfMp668h1fZ","outputId":"87d562ed-5d1d-477c-8d36-edd51e63267f","trusted":true},"cell_type":"code","source":"dot_data = tree.export_graphviz(grid_cv2.best_estimator_, out_file=None, filled=True)\n# Draw graph\ngraph = graphviz.Source(dot_data, format=\"png\") \ngraph","execution_count":null,"outputs":[]},{"metadata":{"id":"5UWSsv15h1fa","outputId":"a8e7ee79-d4d5-4b4f-97ad-78be89de148e","trusted":true},"cell_type":"code","source":"# scoring = balanced_accuracy\nclf1 = DecisionTreeClassifier(max_depth=50,min_samples_split=50)\nclf1= clf1.fit(X_train,y_train)\ny_pred1 = clf1.predict(X_val)\nprint(\"Decision Tree accuracy on testing set is:\",metrics.accuracy_score(y_val, y_pred1))","execution_count":null,"outputs":[]},{"metadata":{"id":"wTxD31dhh1fa","outputId":"74b9d514-ecb4-4aad-c101-39076ed0442a","trusted":true},"cell_type":"code","source":"# scoring = f1_macro\nclf2 = DecisionTreeClassifier(max_depth=100,min_samples_split=50)\nclf2.fit(X_train,y_train)\ny_pred_dt_train = clf2.predict(X_train)\ny_pred2 = clf2.predict(X_val)\nprint(\"Decision Tree accuracy on testing set is:\",metrics.accuracy_score(y_train, y_pred_dt_train))\nprint(\"Decision Tree accuracy on testing set is:\",metrics.accuracy_score(y_val, y_pred2))","execution_count":null,"outputs":[]},{"metadata":{"id":"pv9kOk1Zh1fa"},"cell_type":"markdown","source":"<a id='random_forest'></a>\n\n## 6 f. Random Forest"},{"metadata":{"id":"tUq1pXVxh1fa","outputId":"985889f9-24e3-4443-dcba-4ea5303f67b5","trusted":true},"cell_type":"code","source":"rf = RandomForestClassifier()\nrf.fit(X_train,y_train.ravel())","execution_count":null,"outputs":[]},{"metadata":{"id":"jLNu71HCh1fa","outputId":"7a9465e7-2864-4f3a-bace-79913f3c981e","trusted":true},"cell_type":"code","source":"#takes a long time to run... BEST PARAMS: {'max_depth': 50, 'min_samples_split': 50, 'n_estimators': 5}\n\"\"\"\nrf_parameters = {\"min_samples_split\":[50,100,200],\n             \"max_depth\":[1,5,10,50,100],\n             \"n_estimators\":[5, 50, 250, 500]}\n\nrf_grid_cv = GridSearchCV(estimator=rf, param_grid=rf_parameters, cv=5,scoring=\"f1_macro\") \nrf_grid_cv.fit(X_train,y_train.ravel()) # fit the model\n\nprint_results(rf_grid_cv)\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"id":"ezTWVJO4h1fa","outputId":"01ebd0e5-5ed5-4b49-96e6-b2f4af8e5f8a","trusted":true},"cell_type":"code","source":"clf_RF = RandomForestClassifier(max_depth=50,min_samples_split=50,n_estimators=5)\nclf_RF = clf_RF.fit(X_train,y_train)\ny_pred_rf_train = clf_RF.predict(X_train)\ny_pred_rf = clf_RF.predict(X_val)\nprint(\"Random Forest accuracy on training set is:\",metrics.accuracy_score(y_train, y_pred_rf_train))\nprint(\"Random Forest accuracy on testing set is:\",metrics.accuracy_score(y_val, y_pred_rf))","execution_count":null,"outputs":[]},{"metadata":{"id":"8flJ1DgOh1fa"},"cell_type":"markdown","source":"<a id='knn'></a>\n\n## 6 g. KNN"},{"metadata":{"id":"HGn5Foozh1fa","outputId":"f724e926-269f-4ed3-a593-2fc80dc7a56f","trusted":true},"cell_type":"code","source":"clf = KNeighborsClassifier()\nclf.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"id":"R9jmVmcOh1fa","trusted":true},"cell_type":"code","source":"KNN_fit = clf.predict(X_val)","execution_count":null,"outputs":[]},{"metadata":{"id":"hh1qkQVZh1fa","outputId":"0c8d8338-074f-43cb-ab04-e100b23fe506","trusted":true},"cell_type":"code","source":"clf.get_params()","execution_count":null,"outputs":[]},{"metadata":{"id":"mQZzwl2Rh1fa","trusted":true},"cell_type":"code","source":"#commented out because this code takes a very long time to run.\n#best params: {'algorithm': 'auto', 'n_neighbors': 50, 'p': 1, 'weights': 'distance'}\n#param_grid = {\n#    'algorithm': ['auto', 'brute', 'ball_tree', 'kd_tree'],\n#    'n_neighbors': [2, 5, 10, 20, 50],\n#    'p': [ 1, 2, 3], #1 is the same as specifying euclidean distance and 2 is the same as Manhattan\n#    'weights': ['uniform', 'distance']\n#    }\n\n#grid_search = GridSearchCV(estimator = clf, param_grid = param_grid, cv = 5)\n#grid_search.fit(features_norms, Y)\n\n#print(grid_search.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"id":"2pUC1bSHh1fa","trusted":true},"cell_type":"code","source":"clf = KNeighborsClassifier(algorithm = 'auto', n_neighbors = 50, p = 1, weights = 'distance')","execution_count":null,"outputs":[]},{"metadata":{"id":"kocJwP2Rh1fa","outputId":"596df67c-5bd6-4ad9-c8a0-ca944789dfac","trusted":true},"cell_type":"code","source":"clf.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"id":"_YJUIs8kh1fa","outputId":"7291ffa8-7db2-498d-91b4-d605ba570367","trusted":true},"cell_type":"code","source":"KNN_train_pred = clf.predict(X_train)\nKNN_fit = clf.predict(X_val)\nprint(\"KNN accuracy on training set is:\",metrics.accuracy_score(y_train, KNN_train_pred))\nprint(\"KNN accuracy on testing set is:\",metrics.accuracy_score(y_val, KNN_fit))","execution_count":null,"outputs":[]},{"metadata":{"id":"Va2-GId9h1fa"},"cell_type":"markdown","source":"<a id='neural'></a>\n\n## 6 h. Neural Network"},{"metadata":{"id":"hqvByWRBh1fa","trusted":true},"cell_type":"code","source":"warnings.filterwarnings(\"ignore\")\n\nhidden_layer_range = range(10,25)\nscores_mlp = {}\nscores_list_mlp = []\nfor h in hidden_layer_range:\n    mlp = MLPClassifier(solver='lbfgs', alpha=1e-5,hidden_layer_sizes=(h,), random_state=2020,max_iter=150)\n    mlp.fit(X_train, y_train) \n    y_pred=mlp.predict(X_val)\n    scores_mlp[h] = accuracy_score(y_val,y_pred)\n    scores_list_mlp.append(accuracy_score(y_val,y_pred))","execution_count":null,"outputs":[]},{"metadata":{"id":"MBfpat5Bh1fb","outputId":"f256e7fe-f5db-46e4-d85e-2dc4631d601f","trusted":true},"cell_type":"code","source":"hidden_layers = scores_list_mlp.index(max(scores_list_mlp))+1\nmlp = MLPClassifier(solver='lbfgs', alpha=1e-5,hidden_layer_sizes=hidden_layers,max_iter=150, warm_start=True, random_state=1)\nmlp.fit(X_train, y_train)\ny_pred_nn_train=mlp.predict(X_train)\ny_pred_nn=mlp.predict(X_val)\nprint(\"Training accuracy for Neural Network: \",accuracy_score(y_train,y_pred_nn_train)*100)\n\nprint(\"Testing accuracy for Neural Network: \",accuracy_score(y_val,y_pred_nn)*100)\nwarnings.resetwarnings()\n","execution_count":null,"outputs":[]},{"metadata":{"id":"9Q4ZX4r5XbaJ"},"cell_type":"markdown","source":"<a id='sgd'></a>\n\n## 6 i. Stochastic Gradient Descent"},{"metadata":{"id":"bq-4tEctXl4d","outputId":"fcbe2552-dd81-45b5-dce8-0d9bcf1c86a2","trusted":true},"cell_type":"code","source":"clf_SGD = SGDClassifier()\nclf_SGD.fit(X_train, y_train)\n\nSGD_parameters = {'alpha': [1e-4, 1e-3, 1e-2, 1e-1, 1e0, 1e1, 1e2, 1e3], # learning rate\n                #'n_iter_': [1000], # number of epochs\n                'penalty': ['l2'],\n                'n_jobs': [-1]}\n\nSGD_grid_cv = GridSearchCV(estimator=clf_SGD, param_grid=SGD_parameters, cv=5,scoring=\"f1_macro\") \nSGD_grid_cv.fit(X_train,y_train.ravel()) # fit the model\n\nprint_results(SGD_grid_cv)","execution_count":null,"outputs":[]},{"metadata":{"id":"ec6SgU0AXsoI","outputId":"94b50f90-3e8f-4e91-ba21-6ff117003e51","trusted":true},"cell_type":"code","source":"clf_SGD = SGDClassifier(alpha = 0.0001,n_jobs = -1, penalty = 'l2')\nclf_SGD.fit(X_train, y_train)\npred_SGD_train = clf_SGD.predict(X_train)\npred_SGD = clf_SGD.predict(X_val)\nprint(\"SGD testing set accuracy:\",round(metrics.accuracy_score(y_true = y_train, y_pred = pred_SGD_train),4))\nprint(\"SGD testing set accuracy:\",round(metrics.accuracy_score(y_true = y_val, y_pred = pred_SGD),4))","execution_count":null,"outputs":[]},{"metadata":{"id":"O0IEHlN5XvTM","outputId":"de1cfaaf-799b-4848-9a54-65b1b8674375","trusted":true},"cell_type":"code","source":"clf_SGD.fit(X_train_kpca,y_train)\nsgd_kpca_pred = clf_SGD.predict(X_val_kpca)\nprint(\"SGD testing set accuracy after applying KPCA:\",round(metrics.accuracy_score(y_true = y_val, y_pred = sgd_kpca_pred),4))","execution_count":null,"outputs":[]},{"metadata":{"id":"sQSc-jfv5d9i"},"cell_type":"markdown","source":"<a id='svc'></a>\n## 6 j. Support Vector Classifier"},{"metadata":{"id":"ioe3ihJR5Z0l","trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\nsvc = SVC(kernel ='rbf')\nsvc.fit(X_train, y_train)\npredicted_train = svc.predict(X_train)\npredicted_val = svc.predict(X_val)","execution_count":null,"outputs":[]},{"metadata":{"id":"Y6qgXFu95rVL","trusted":true},"cell_type":"code","source":"#param_grid = {\n#    'C': [0.1, 0.5, 1, 2, 5, 10, 20],\n#    'gamma': ['scale', 'auto', 1, 2, 3]\n#    }\n\n#grid_search = GridSearchCV(estimator = svc, param_grid = param_grid, scoring = 'f1_macro')\n#predicted = grid_search.fit(X_train, y_train)\n\n#print(grid_search.best_params_)\n\n#Takes a long time to run, output: {'C': 20, 'gamma': 'scale'}\n","execution_count":null,"outputs":[]},{"metadata":{"id":"pOSAPi0v5s45","trusted":true},"cell_type":"code","source":"svc = SVC(kernel ='rbf', C = 20, gamma = 'scale')\nsvc.fit(X_train, y_train)\npredicted_train = svc.predict(X_train)\nSVC_fit = svc.predict(X_val)","execution_count":null,"outputs":[]},{"metadata":{"id":"Gs-PFtvvh1fb"},"cell_type":"markdown","source":"<a id='imbalanced'></a>\n\n## 5 k. Imbalanced Class Techniques\n\nTo combat the imbalanced distribution of the target variable, we tried rebalancing techniques on a few models (logistic regression and a neural network)\n\n## Oversampling\n\nBelow you will see that some of the accuracies are actually higher on the test set for oversampling. This has to do with the oversampling process: only the training set is oversampled to avoid data leakage. However the neural network model has high variance on the training model leading to a high discrepancy in training/testing errors."},{"metadata":{"id":"UcXmxRCXh1fb","trusted":true},"cell_type":"code","source":"X_train_oversample = pd.DataFrame(X_train)\ny_train_oversample = np.array(y_train)\nX_train_oversample[\"y\"] = y_train_oversample\n\ncount_4, count_3, count_2, count_1 = X_train_oversample[\"y\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"id":"NDtMH-cRh1fb","trusted":true},"cell_type":"code","source":"features_class_1 = X_train_oversample[y_train_oversample==1]\nfeatures_class_2 = X_train_oversample[y_train_oversample==2]\nfeatures_class_3 = X_train_oversample[y_train_oversample==3]\nfeatures_class_4 = X_train_oversample[y_train_oversample==4]","execution_count":null,"outputs":[]},{"metadata":{"id":"6CMWWhMhh1fb","trusted":true},"cell_type":"code","source":"features_class_1_over = features_class_1.sample(count_4,replace=True)\nfeatures_class_2_over = features_class_2.sample(count_4,replace=True)\nfeatures_class_3_over = features_class_3.sample(count_4,replace=True)\nfeatures_over_sampled = pd.concat([features_class_1_over,features_class_2_over,features_class_3_over,features_class_4],axis=0)\nX_train_over = features_over_sampled.drop('y',axis=1)\ny_train_over = features_over_sampled['y']","execution_count":null,"outputs":[]},{"metadata":{"id":"-HctC9eoh1fb","outputId":"9cc5c0fa-cf0e-47ca-b762-61e8897b9d49","trusted":true},"cell_type":"code","source":"clf_RF_over = RandomForestClassifier(max_depth=50,min_samples_split=50,n_estimators=5)\nclf_RF_over.fit(X_train_over,y_train_over)\ny_pred_rf_train_over = clf_RF.predict(X_train_over)\ny_pred_rf_over = clf_RF.predict(X_val)\nprint(\"Random Forest accuracy on train set with over sampling is:\",metrics.accuracy_score(y_train_over, y_pred_rf_train_over))\nprint(\"Random Forest accuracy on test set with over sampling is:\",metrics.accuracy_score(y_val, y_pred_rf_over))","execution_count":null,"outputs":[]},{"metadata":{"id":"h91pUHXhh1fb","outputId":"25b4083c-d269-45a8-cc17-2800d5c884c0","trusted":true},"cell_type":"code","source":"lr1_over = LogisticRegression(max_iter = 10000, solver='lbfgs', penalty='l2', C=1,multi_class=\"multinomial\")\nlr1_over.fit(X_train_over,y_train_over)\nlog_reg_pred_train_over = lr1_over.predict(X_train_over)\nlog_reg_pred_test_over = lr1_over.predict(X_val)\nprint(\"Logistic regression with over-sampling training set accuracy:\",round(metrics.accuracy_score(y_true = y_train_over, y_pred = log_reg_pred_train_over),4))\nprint(\"Logistic regression with over-sampling testing set accuracy:\",round(metrics.accuracy_score(y_true = y_val, y_pred = log_reg_pred_test_over),4))\n","execution_count":null,"outputs":[]},{"metadata":{"id":"kj8vjZx_h1fb","trusted":true},"cell_type":"code","source":"warnings.filterwarnings(\"ignore\")\n\nhidden_layer_range = range(10,25)\nscores_mlp = {}\nscores_list_mlp = []\nfor h in hidden_layer_range:\n    mlp = MLPClassifier(solver='lbfgs', alpha=1e-5,hidden_layer_sizes=(h,), random_state=2020,max_iter=150)\n    mlp.fit(X_train_over, y_train_over) \n    y_pred_over=mlp.predict(X_val)\n    scores_mlp[h] = accuracy_score(y_val,y_pred_over)\n    scores_list_mlp.append(accuracy_score(y_val,y_pred_over))","execution_count":null,"outputs":[]},{"metadata":{"id":"Nrd6_bkYh1fb","outputId":"68c6f23b-1799-4f5d-b15d-1e20a2f6e23a","trusted":true},"cell_type":"code","source":"hidden_layers = scores_list_mlp.index(max(scores_list_mlp))+1\nmlp = MLPClassifier(solver='lbfgs', alpha=1e-5,hidden_layer_sizes=hidden_layers,max_iter=150, warm_start=True, random_state=1)\nmlp.fit(X_train_over, y_train_over)\ny_pred_over_nn_train=mlp.predict(X_train_over)\ny_pred_over_nn=mlp.predict(X_val)\n\nprint(\"Accuracy for Neural Network training set with over-sampling:\",accuracy_score(y_train_over, y_pred_over_nn_train)*100)\nprint(\"Accuracy for Neural Network testing set with over-sampling: \",accuracy_score(y_val,y_pred_over_nn)*100)\nwarnings.resetwarnings()\n","execution_count":null,"outputs":[]},{"metadata":{"id":"bwb8Dmjzh1fb"},"cell_type":"markdown","source":"## Undersampling"},{"metadata":{"id":"Yco9XVo2h1fb","trusted":true},"cell_type":"code","source":"features_class_2_under = features_class_2.sample(count_1,replace=True)\nfeatures_class_3_under = features_class_3.sample(count_1,replace=True)\nfeatures_class_4_under = features_class_4.sample(count_1,replace=True)\n\nfeatures_under_sampled = pd.concat([features_class_1,features_class_2_under,features_class_3_under,features_class_4_under],axis=0)\nX_train_under = features_under_sampled.drop('y',axis=1)\ny_train_under = features_under_sampled['y']\n","execution_count":null,"outputs":[]},{"metadata":{"id":"2bArqeKsh1fb"},"cell_type":"markdown","source":"**Logistic Regression**"},{"metadata":{"id":"wyarnT9Ih1fb","outputId":"ce13028b-3cb6-4963-cb21-77304c4a4e1c","trusted":true},"cell_type":"code","source":"lr1_under = LogisticRegression(max_iter = 10000, solver='lbfgs', penalty='l2', C=1,multi_class=\"multinomial\")\nlr1_under.fit(X_train_under,y_train_under)\nlog_reg_pred_train_under = lr1_under.predict(X_train_under)\nlog_reg_pred_test_under = lr1_under.predict(X_val)\nprint(\"Logistic regression training set accuracy with under-sampling:\",round(metrics.accuracy_score(y_true = y_train_under, y_pred = log_reg_pred_train_under),4))\nprint(\"Logistic regression testing set accuracy with under-sampling:\",round(metrics.accuracy_score(y_true = y_val, y_pred = log_reg_pred_test_under),4))\n","execution_count":null,"outputs":[]},{"metadata":{"id":"adFnZ5Vdh1fc"},"cell_type":"markdown","source":"**Neural Network**"},{"metadata":{"id":"Idh0Lc2Wh1fc","trusted":true},"cell_type":"code","source":"warnings.filterwarnings(\"ignore\")\n\nhidden_layer_range = range(10,25)\nscores_mlp = {}\nscores_list_mlp = []\nfor h in hidden_layer_range:\n    mlp = MLPClassifier(solver='lbfgs', alpha=1e-5,hidden_layer_sizes=(h,), random_state=2020,max_iter=150)\n    mlp.fit(X_train_under, y_train_under) \n    y_pred_under=mlp.predict(X_val)\n    scores_mlp[h] = accuracy_score(y_val,y_pred_under)\n    scores_list_mlp.append(accuracy_score(y_val,y_pred_under))","execution_count":null,"outputs":[]},{"metadata":{"id":"fLX71Vzch1fd","outputId":"07ac22b5-2a0f-45cd-ef33-a4efbb560609","trusted":true},"cell_type":"code","source":"hidden_layers = scores_list_mlp.index(max(scores_list_mlp))+1\nmlp = MLPClassifier(solver='lbfgs', alpha=1e-5,hidden_layer_sizes=hidden_layers,max_iter=150, warm_start=True, random_state=1)\nmlp.fit(X_train_under, y_train_under)\ny_pred_under_nn_train=mlp.predict(X_train_under)\ny_pred_under_nn=mlp.predict(X_val)\n\nprint(\"Accuracy for Neural Network training set with under-sampling: \",accuracy_score(y_train_under,y_pred_under_nn_train)*100)\nprint(\"Accuracy for Neural Network testing set with under-sampling: \",accuracy_score(y_val,y_pred_under_nn)*100)\nwarnings.resetwarnings()\n","execution_count":null,"outputs":[]},{"metadata":{"id":"l4Rj9usHh1fd"},"cell_type":"markdown","source":"<a id='experiment_results'></a>\n\n# 7 a. Experiment Results\n"},{"metadata":{"id":"p2rUyri0h1fd"},"cell_type":"markdown","source":"Classification reports are included for each of the models for the test set. The main metric of interest is macro avg F-1 score (the measure used to evaluate performance in the Kaggle competition). We also include training and testing set confusion matrices.\n\n**Logistic Regression :** \n\nGrid Search CV found the best parameters as max_iter = 10000, solver='lbfgs', penalty='l2', C=10,multi_class=\"multinomial\". Macro average F-1 was 0.4 but the \"Vulnerable\" class had weak measures indicating there may be opportunity to improve.\n"},{"metadata":{"id":"t9KH4TXbh1fd","outputId":"c8e4ec15-8870-4054-d03f-806f340dba1a","trusted":true},"cell_type":"code","source":"print(classification_report(y_val, log_reg_pred_test, target_names=[\"Extreme Poverty\",\"Moderate Poverty\",\"Vulnerable\",\"Not Vulnerable\"]))","execution_count":null,"outputs":[]},{"metadata":{"id":"OXHm_UCCrrrq","outputId":"52183ad9-1c60-4138-870c-29e5180cec19","trusted":true},"cell_type":"code","source":"print(\"Logistic Regression with Over-Sampling confusion matix for training set:\")\nprint(confusion_matrix(y_train, log_reg_pred_train))\n\nprint(\"Logistic Regression with Over-Sampling confusion matix for testing set:\")\nprint(confusion_matrix(y_val, log_reg_pred_test))","execution_count":null,"outputs":[]},{"metadata":{"id":"iiCYb2a2h1fd"},"cell_type":"markdown","source":"**Adaboost:**\n\nAccuracy on the testing set is 0.64. Macro average F-1 is lower than most of the other models. The vulnerable class had poor performance (similar to logistic regression)."},{"metadata":{"id":"7uuDgyDAh1fd","outputId":"0bec0faf-52fc-4829-def0-8ff253312634","trusted":true},"cell_type":"code","source":"print(classification_report(y_val, pred10, target_names=[\"Extreme Poverty\",\"Moderate Poverty\",\"Vulnerable\",\"Not Vulnerable\"]))","execution_count":null,"outputs":[]},{"metadata":{"id":"q3p1a4Znh1fd","outputId":"d246f34b-3ce9-48a5-f645-baf707f43ed4","trusted":true},"cell_type":"code","source":"print(\"Adaboost confusion matix for training set:\")\nprint(confusion_matrix(y_train, pred9))\n\nprint(\"Adaboost confusion matix for testing set:\")\nprint(confusion_matrix(y_val, pred10))","execution_count":null,"outputs":[]},{"metadata":{"id":"-Iv42Q2zh1fd"},"cell_type":"markdown","source":"**Gaussian Naive Bayes:**\n\nThe accuracy we got on the testing set is 0.57 after running GridSearch trying to find the best parameter for var_smoothing. The macro average is 0.34. While GNB runs rather quickly, model performance is not as good as other models we used.\n"},{"metadata":{"id":"zYowwEzSh1fd","outputId":"31d990ad-a4ef-4cf8-ee10-16ba8e469c7e","trusted":true},"cell_type":"code","source":"print(classification_report(y_val, NB_fit_CV5, target_names=[\"Extreme Poverty\",\"Moderate Poverty\",\"Vulnerable\",\"Not Vulnerable\"]))","execution_count":null,"outputs":[]},{"metadata":{"id":"A7LGc_mgvM9d","outputId":"6a700507-e293-405a-9794-21acfdd04112","trusted":true},"cell_type":"code","source":"print(\"GNB confusion matix for training set:\")\nprint(confusion_matrix(y_train, NB_fit_train))\n\nprint(\"GNB confusion matix for testing set:\")\nprint(confusion_matrix(y_val, NB_fit_CV5))","execution_count":null,"outputs":[]},{"metadata":{"id":"SSmVQlWWh1fd"},"cell_type":"markdown","source":"**Decision Tree:**\n\nUsing GridSearch for hyperparameter tuning, the best model with the current data used max_depth = 100 and min_samples_split = 50. While accuracy was slightly lower than logistic regression, decision tree was actually more sensitive to classes with lower samples. F-1 macro average was 0.43, an improvement from logistic regression. The \"Vulnerable\" class was much better than logistic regression."},{"metadata":{"id":"WyHbe4hch1fd","outputId":"e868f9b8-079f-4916-f277-f93d1e35709c","trusted":true},"cell_type":"code","source":"print(classification_report(y_val, y_pred2,target_names=[\"Extreme Poverty\",\"Moderate Poverty\",\"Vulnerable\",\"Not Vulnerable\"]))","execution_count":null,"outputs":[]},{"metadata":{"id":"gYYfG8MWvnVA","outputId":"2420194a-a575-48f8-a2c2-27332891e677","trusted":true},"cell_type":"code","source":"print(\"Decision Tree confusion matix for training set:\")\nprint(confusion_matrix(y_train, y_pred_dt_train))\n\nprint(\"Decision Tree confusion matix for testing set:\")\nprint(confusion_matrix(y_val, y_pred2))","execution_count":null,"outputs":[]},{"metadata":{"id":"ZK8rbGRkh1fd"},"cell_type":"markdown","source":"**Random Forest:** \n\nThe best model used a max_depth = 50, min_samples_split = 50, and n-estimators = 5. The accuracy on the testing set was higher but other metrics were worse than the decision tree. Given the extremely slow process of hyperparameter tuning and worse performance on random forest (possibly due to overfitting), random forests may not be a useful model going forward."},{"metadata":{"id":"T8XFIKdbh1fd","outputId":"8160249b-2496-498e-e937-23eca7f7904d","trusted":true},"cell_type":"code","source":"print(classification_report(y_val, y_pred_rf,target_names=[\"Extreme Poverty\",\"Moderate Poverty\",\"Vulnerable\",\"Not Vulnerable\"]))","execution_count":null,"outputs":[]},{"metadata":{"id":"4OEKcbhlwC2b","outputId":"72b8b635-57b3-47a2-f3c9-063975c2264e","trusted":true},"cell_type":"code","source":"print(\"Random Forest confusion matix for training set:\")\nprint(confusion_matrix(y_train, y_pred_rf_train))\n\nprint(\"Random Forest confusion matix for testing set:\")\nprint(confusion_matrix(y_val, y_pred_rf))","execution_count":null,"outputs":[]},{"metadata":{"id":"K-tdUp83h1fd"},"cell_type":"markdown","source":"**K-NN:**\n\nKNN seems to have the best performance with this data. Without Gridsearch CV, all classes have very good performance (relative to other models) with macro average F-1 of 0.6 and accuracy of 0.74"},{"metadata":{"id":"e9_nh-J8h1fd","outputId":"1c49f79a-0359-48be-cfd7-cba6787cac57","trusted":true},"cell_type":"code","source":"print(classification_report(y_val, KNN_fit, target_names=[\"Extreme Poverty\",\"Moderate Poverty\",\"Vulnerable\",\"Not Vulnerable\"]))","execution_count":null,"outputs":[]},{"metadata":{"id":"za25mElawnSo","outputId":"1eed74bc-9b07-4890-86fd-e647a30e965b","trusted":true},"cell_type":"code","source":"print(\"KNN confusion matix for training set:\")\nprint(confusion_matrix(y_train, KNN_train_pred))\n\nprint(\"KNN confusion matix for testing set:\")\nprint(confusion_matrix(y_val, KNN_fit))","execution_count":null,"outputs":[]},{"metadata":{"id":"4ng3vrtFh1fd"},"cell_type":"markdown","source":"**Neural Network**\n"},{"metadata":{"id":"x2A5myCxh1fd","outputId":"9c1a713b-92c9-4b6f-fb3e-3f64023f9d17","trusted":true},"cell_type":"code","source":"print(classification_report(y_val, y_pred_nn, target_names=[\"Extreme Poverty\",\"Moderate Poverty\",\"Vulnerable\",\"Not Vulnerable\"]))","execution_count":null,"outputs":[]},{"metadata":{"id":"5Ksvk2eGxcoH","outputId":"eacc9c3d-08b8-41ae-bc00-7e25c777f0d0","trusted":true},"cell_type":"code","source":"print(\"Neural Network confusion matix for training set:\")\nprint(confusion_matrix(y_train, y_pred_nn_train))\n\nprint(\"Neural Network confusion matix for testing set:\")\nprint(confusion_matrix(y_val, y_pred_nn))","execution_count":null,"outputs":[]},{"metadata":{"id":"uHAZwJFKXzPO"},"cell_type":"markdown","source":"**Stochastic Gradient Descent**"},{"metadata":{"id":"7gGS79EBX9_m","outputId":"7e423e05-f594-47d9-e292-cd1fb98ba25f","trusted":true},"cell_type":"code","source":"print(classification_report(y_val, pred_SGD.round(), target_names=[\"Extreme Poverty\",\"Moderate Poverty\",\"Vulnerable\",\"Not Vulnerable\"]))","execution_count":null,"outputs":[]},{"metadata":{"id":"g_mEJ1eCxrkK","outputId":"2010174d-c0b7-4674-e988-c45ef66ae877","trusted":true},"cell_type":"code","source":"print(\"SGD confusion matix for training set:\")\nprint(confusion_matrix(y_train, pred_SGD_train))\n\nprint(\"SGD confusion matix for testing set:\")\nprint(confusion_matrix(y_val, pred_SGD))","execution_count":null,"outputs":[]},{"metadata":{"id":"9Xb74oaN9qFf"},"cell_type":"markdown","source":"**Support Vector Classifier**"},{"metadata":{"id":"avJODiwt91aJ","outputId":"39066154-9397-4315-aa93-4e7808e2705a","trusted":true},"cell_type":"code","source":"print(classification_report(y_val, SVC_fit.round(), target_names=[\"Extreme Poverty\",\"Moderate Poverty\",\"Vulnerable\",\"Not Vulnerable\"]))","execution_count":null,"outputs":[]},{"metadata":{"id":"ajzFvhrj90eo","outputId":"a75ec7e8-602e-4da2-b89a-cd6b5fd76a31","trusted":true},"cell_type":"code","source":"confusion_matrix_svc =  pd.crosstab(index=np.ravel(y_train), columns=predicted_train.ravel(), rownames=['Expected'], colnames=['Predicted'])\nsns.heatmap(confusion_matrix_svc, annot=True, square=False, fmt='', cbar=False)\nplt.title(\"Confusion Matrix (Training Set)\", fontsize = 15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"7IsMOrAM5wqr","outputId":"a5cbdc84-7854-436f-b624-4378759f4282","trusted":true},"cell_type":"code","source":"confusion_matrix_svc =  pd.crosstab(index=np.ravel(y_val), columns=SVC_fit.ravel(), rownames=['Expected'], colnames=['Predicted'])\nsns.heatmap(confusion_matrix_svc, annot=True, square=False, fmt='', cbar=False)\nplt.title(\"Confusion Matrix (Validation Set)\", fontsize = 15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"hBiXdIGHh1fe"},"cell_type":"markdown","source":"<a id='experiment_results_b'></a>\n\n# 7 b. Experiment Results for Models Using Class Rebalancing Techniques (select models)\n\nOversampling generally performed better than oversampling. Oversampling helped logistic regression slightly while oversampling hindered neural network performance."},{"metadata":{"id":"qpL8K_RSh1fe"},"cell_type":"markdown","source":"**Logistic Regression (oversampling)**"},{"metadata":{"id":"x8XJTl1bh1fe","outputId":"0bad7cbc-204b-4acc-b2b6-0820ba7f74de","trusted":true},"cell_type":"code","source":"print(classification_report(y_val, log_reg_pred_test_over, target_names=[\"Extreme Poverty\",\"Moderate Poverty\",\"Vulnerable\",\"Not Vulnerable\"]))","execution_count":null,"outputs":[]},{"metadata":{"id":"W6mNWJsXh1ff"},"cell_type":"markdown","source":"**Neural Network (oversampling)**"},{"metadata":{"id":"87KzAL1xh1ff","outputId":"192c6c9f-f382-4882-8269-7735b49b6833","trusted":true},"cell_type":"code","source":"print(classification_report(y_val, y_pred_over_nn, target_names=[\"Extreme Poverty\",\"Moderate Poverty\",\"Vulnerable\",\"Not Vulnerable\"]))","execution_count":null,"outputs":[]},{"metadata":{"id":"QYU2Pl1Zh1ff"},"cell_type":"markdown","source":"**Logistic Regression (undersampling)**"},{"metadata":{"id":"nvo2-E7ih1ff","outputId":"91a06a62-238c-424f-e09d-87e1bda7d89b","trusted":true},"cell_type":"code","source":"print(classification_report(y_val, log_reg_pred_test_under, target_names=[\"Extreme Poverty\",\"Moderate Poverty\",\"Vulnerable\",\"Not Vulnerable\"]))","execution_count":null,"outputs":[]},{"metadata":{"id":"wY99zBHKh1ff"},"cell_type":"markdown","source":"**Neural Network (undersampling)**"},{"metadata":{"id":"CdP4ANJsh1ff","outputId":"ae7bb500-973e-4146-fe28-e4b50c3a164d","trusted":true},"cell_type":"code","source":"print(classification_report(y_val, y_pred_under_nn, target_names=[\"Extreme Poverty\",\"Moderate Poverty\",\"Vulnerable\",\"Not Vulnerable\"]))","execution_count":null,"outputs":[]},{"metadata":{"id":"hxm-N5L8h1fj"},"cell_type":"markdown","source":"<a id='conclusion'></a>\n\n# 8. Conclusion\n\nTo summarize this analysis and refer back to our original research questions, we were able to identify features that made up the first few principal components in our PCA. Components tended to have high loading values from features that were in similar groups as specified by our data dictionary. We also determined that certain principal components are likely more useful in identifying affluent households while other principal components are more useful in identifying impoverished households.\n\nWe developed multiple models that perform better than a baseline model without hyperparameter tuning. Data reduction, rebalancing techniques, and hyperparameter tuning generally help model performance. Certain algorithms also perform better than others for a multitude of reasons, which we examined extensively in our experiment results section.\n\nFor regional differences (our third research question) we identified that models build on subsets of the data split on the region helped with model performance compared to using all of the data together. This is an important discovery: this indicates that to maximize generalizability of this model, the model should be applied to homogeneous groups when possible rather than treating all observations as equal. We also demonstrated that the data can predict if a household is in a rural or urban environment, indicating stark differences in households between urban and rural areas.\n\nFor our fourth research question (model differences between traditional PMT features vs all features), KNN actually had better performance when only using traditional PMT features while SVC had much better performance when including all features. Based on our results from regional differences, this seems to be a result of differences in similarity between different groups of observations (i.e. observations from different regions). For example, perhaps in a more affluent region, people who are well off tend to have more kids and therefore more people living in the house since finances are not a worry for them. In poorer regions where access to birth control may not be available, more children would be indicative of poverty. It seems the conflation of non-homogenous groups hurts the predictive power of most models, but SVC manages to do well even when including all features. This might be because the conflation of non-homogeneuous groups does not significantly impact the data points where the decision boundary is located. Therefore, **SVC would be considered our best model**: it has the highest overall metrics (F-1 Macro average, recall and precision), does much better with additional features, and appears to be the most generalizable if social welfare needs are to be predicted in other countries.\n\n**What more could you do on this data if you had more time?**\n\nWith more time, we could have tried more models on the Kaggle holdout set. We tried some with moderate success, but as we gained more insights based on findings from our research questions, it became more clear on approaches we could have taken to have a successful model on the Kaggle holdout test set. Given the Kaggle competition is a Kernel only competition, making drastic changes required essentially making an entire other notebook which takes a lot of time.\n\nIt would also be interesting to apply the knowledge we gained to other PMT related datasets to see if findings are similar.\n\n\n\n\n\n\n\n\n\n\n    \n"},{"metadata":{"id":"1yW19JkUh1fj"},"cell_type":"markdown","source":"<a id='contributions'></a>\n\n# 9. Contributions\n\n<a id='max_rodrigues'></a>\n\n## 9 a. Max Rodrigues\n* Wrote code to handle redundant variables and missing values (handled missing values by both KNN imputation and by taking averages of aggregations of data). \n* Did exploratory analysis (correlation plot, scatterplots, countplots and lmplots)\n* Wrote code for PCA/Feature Selection\n* logistic regression with & without gridsearch CV neural network. \n* Applied rebalancing techniques.\n* Made updates to data dictionary to be more concise.\n* Wrote/helped with abstract, introduction, conclusion.\n* Analyzed PCA loadings and how they related to feature importance (from logistic regression)\n* Helped with research questions (built models that demonstrate some models perform worse with the conflation of dissimilar regions)\n* Cleaned and organized code in the notebook\n* found reference that described PMT in detail, and reference describing why KNN handles imbalanced data well\n\n"},{"metadata":{"id":"GJH-6-5bh1fj"},"cell_type":"markdown","source":"<a id='andrew_schiek'></a>\n\n## 9 b. Andrew Schiek\nAndrew Schiek: I have contributed to the exploratory analysis, machine learning applications, and the research questions. For the exploratory analysis I examined histograms of the float variables. For machine learning applications I developed the models for the Guassian Naive Bayes, K-Nearest Neighbors, and SVC algorithms. I created the baseline and grid search for each as well as doing the analysis. I answered each of the research questions and developed the models associated, which included but was not limited to the non-observable parameter models. Additionally I contributed to the conclusion for the write-up\n"},{"metadata":{"id":"S6WW0u3Ih1fj"},"cell_type":"markdown","source":"<a id='Xintong_Li'></a>\n\n## 9 c. Xintong Li\n* Preparing Data: Data Cleaning\n* Exploratory Analysis: PCA (not included in the report) and KPCA and Histograms\n* Machine Learning Models: Decison Tree, Random Forest, SGD\n* Feature Importance\n* Expriment Results: Summarizing ML models and results"},{"metadata":{"id":"8QEPjqJZh1fj"},"cell_type":"markdown","source":"<a id='Mashall_Jahangir'></a>\n\n## 9 d. Mashall Jahangir\n- Exploratory Analysis :\n    Pie Chart, Boxplots and Variable Distribution\n- Machine Learning Model :\n    Gaussian Naive Bayes Classifier, Random Forest and Adaboost Model\n- Overall : Helped writing the notebook sections data dictionary, eda, references, abstract."},{"metadata":{"id":"mmqXgTQGM8s9"},"cell_type":"markdown","source":"<a id='references'></a>\n## 10. References\n\n\n1.   Zhang, J.P. and Mani, I. (2003) KNN Approach to Unbalanced Data Distributions: A Case Study Involving Information Extraction. Proceeding of International Conference on Machine Learning (ICML 2003), Workshop on Learning from Imbalanced Data Sets, Washington DC, 21 August 2003.\n\n2.   Social Protection & Labor team (2010). Measuring income and poverty using Proxy Means Tests. Retrieved from https://olc.worldbank.org/sites/default/files/1.pdf\n\n3. Adrian G. Rodriguez, Stephen M. Smith, A comparison of determinants of urban, rural and farm poverty in Costa Rica, World Development, Volume 22, Issue 3,1994,Pages 381-397,ISSN 0305-750X,https://doi.org/10.1016/0305-750X(94)90129-5.\n\n4. J. H. Mohamud and O. N. Gerek, \"Poverty Level Characterization via Feature Selection and Machine Learning,\" 2019 27th Signal Processing and Communications Applications Conference (SIU), Sivas, Turkey, 2019, pp. 1-4, doi: 10.1109/SIU.2019.8806548."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}