{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.\n\n\nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Given were the 5 datasets,we would like to work on the EDA and Feature Engineering of data separately and then merge them together for bringing out the insights from the data and for predictive analysis"},{"metadata":{},"cell_type":"markdown","source":"**Importing the dataset Admission Details** "},{"metadata":{"trusted":true},"cell_type":"code","source":"admission = pd.read_excel('../input/admission_details.xlsx')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"admission.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"admission.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Checking for any NULL values"},{"metadata":{"trusted":true},"cell_type":"code","source":"admission.isnull().values.any()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"admission.payer_code.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"admission.medical_specialty.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### We can observe that, Payer code and medical speciality have more than 50% of the missing data, and prefer to drop those features."},{"metadata":{"trusted":true},"cell_type":"code","source":"admission = admission.drop(['medical_specialty','payer_code'],axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"admission.time_in_hospital.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"admission.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"admission.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"admission.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Loading the Diabetes data of the patients"},{"metadata":{"trusted":true},"cell_type":"code","source":"diabetic = pd.read_csv('../input/diabetic_data.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"diabetic.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"diabetic.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"diabetic.readmitted.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"diabetic.diabetesMed.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"diabetic.change.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Checking for NULL values"},{"metadata":{"trusted":true},"cell_type":"code","source":"diabetic.isnull().values.any()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" #### Custom encoding for the 23 Drug Features\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"Diabetic_transform=diabetic.replace(['No','Steady','Up','Down'],[0,1,1,1])\nDiabetic_transform.set_index('encounter_id',inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Diabetic_transform.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Diabetic_transform.sum(axis=1).value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Patients are Given at max a combination of 6 drugs for treating diabetes"},{"metadata":{"trusted":true},"cell_type":"code","source":"#diabetic.set_index('encounter_id',inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature Engineering - Creating a new feature \"Treatments\""},{"metadata":{},"cell_type":"markdown","source":"**1. When the value of Insuin is '1' , creating the classes \"insulin\" & \"io\" (insulin + others )********"},{"metadata":{"trusted":true},"cell_type":"code","source":"i1 = Diabetic_transform[Diabetic_transform['insulin']==1].sum(axis = 1).replace([1,2,3,4,5,6],['insulin','io','io','io','io','io'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"i1.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**2. When the value of Insuin is '0' , creating the classes \"others\" & \"no med\"**"},{"metadata":{"trusted":true},"cell_type":"code","source":"i0=Diabetic_transform[Diabetic_transform['insulin']==0].sum(axis=1).replace([0,1,2,3,4,5,6],['no med','other','other','other','other','other','other'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"i0.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"treatments=pd.concat([i1,i0])\ntreatments = pd.DataFrame({'treatments':treatments})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"treatments.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Adding the new feature to the Diabetic Dataframe**"},{"metadata":{"trusted":true},"cell_type":"code","source":"diabetic=diabetic.join(treatments,on='encounter_id') #setting index as encounter_id","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"diabetic.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"diabetic.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"diabetic.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Importing Lab Sessions Dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"lab_sessions = pd.read_excel('../input/Lab-session.xlsx')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lab_sessions.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lab_sessions.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lab_sessions.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Checking for null values"},{"metadata":{"trusted":true},"cell_type":"code","source":"lab_sessions.isnull().values.any()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lab_sessions.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Importing the Patient Details Dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"patient_details = pd.read_excel('../input/Paitent_details.xlsx')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"patient_details.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"patient_details.isnull().values.any()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Since weight has more than 50% missing values, we tend to drop that feature**"},{"metadata":{"trusted":true},"cell_type":"code","source":"patient_details = patient_details.drop(['weight'],axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"patient_details.race.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**We can observe that the \"Race\" Feature has some missing values**"},{"metadata":{},"cell_type":"markdown","source":"**Missing value Imputation using MODE for Race Feature as most of the people in the Dataset are Caucasian**"},{"metadata":{},"cell_type":"markdown","source":"##### 1. Replacing the ? with NaN's"},{"metadata":{"trusted":true},"cell_type":"code","source":"patient_details['race']=patient_details.race.replace('?',np.nan)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### 2. Filling the NaN's with the mode"},{"metadata":{"trusted":true},"cell_type":"code","source":"patient_details['race'].fillna(patient_details['race'].mode()[0], inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"patient_details.race.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"patient_details.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Concatinatin the Dataframes \"Admission\" , \"Diabetic\" ,\"Patient_details\" & \"Lab_sessions\""},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Admission\" , admission.shape)\nprint(\"Diabetic\" ,diabetic.shape)\nprint(\"Lab Sessions\",lab_sessions.shape)\nprint(\"Patient_details\",patient_details.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.concat([patient_details,admission,lab_sessions,diabetic],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.shape","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"#data = pd.read_csv('Final_Diabetes_withallrows.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#data[['admission_source_id','time_in_hospital']] = admission[['admission_source_id','time_in_hospital']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#data[['num_lab_procedures','num_medication','number_outpatient','number_emergency','number_inpatient','num_procedures']] = lab_sessions[['num_lab_procedures','num_medications',\n                                                                                                                        #'number_outpatient','number_emergency','number_inpatient','num_procedures']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Storing Encounter_id and Patient_nbr columns in a seperate DataFrame"},{"metadata":{"trusted":true},"cell_type":"code","source":"df1=data.iloc[:,:2]\n\ndf1.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Dropping all Duplicate columns from DataFrame"},{"metadata":{"trusted":true},"cell_type":"code","source":"df2=data.drop(['encounter_id','patient_nbr'],axis=1)\n\ndf2.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Concatenation of df1 and df2 Dataframes"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_final=pd.concat([df1,df2],axis=1)\n\ndata_final.shape\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_final.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Filtering the records of patients having Diabetes**"},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndata_diamed_yes=data_final[data_final.diabetesMed=='Yes']\ndata_diamed_yes.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Considering records of Diabetic Patients who didn't Readmit**"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_readmit_no=data_diamed_yes[data_diamed_yes.readmitted=='NO']\ndata_readmit_no.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Excluding the patients who are Dead and are in Hospice**"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_new=data_readmit_no[~data_readmit_no.discharge_disposition_id.isin([11,13,14,19,20])]\ndata_new.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"****Choosing the records with treatments Insulin and Insulin + other ( w.r.t Problem Statement)**"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_model=data_new[data_new.treatments!='other']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_model.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_model.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_model.head().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#data_cat = data_model.select_dtypes(include=['object']).copy()\ndata_model.treatments.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Since Treatments column is the combination of the 23 drug features,we will be dropping them**"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_model = data_model.drop(['metformin',\n       'repaglinide', 'nateglinide', 'chlorpropamide', 'glimepiride',\n       'acetohexamide', 'glipizide', 'glyburide', 'tolbutamide',\n       'pioglitazone', 'rosiglitazone', 'acarbose', 'miglitol', 'troglitazone',\n       'tolazamide', 'examide', 'citoglipton', 'insulin',\n       'glyburide-metformin', 'glipizide-metformin',\n       'glimepiride-pioglitazone', 'metformin-rosiglitazone',\n       'metformin-pioglitazone'],axis = 1)\ndata_model.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_model.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_model.num_procedures.plot(kind='hist')\nplt.xlabel(\"No.of Lab Procedures\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nsns.barplot(data_model.discharge_disposition_id)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_model.num_medications.plot(kind='hist')\nplt.xlabel(\"No.of Medications\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Here the features which contains numeric values are of type Discrete Quantitative and has a finite set of values. Discrete data can be both Quantitative and Qualitative. So treating outliers in this dataset is not possible"},{"metadata":{},"cell_type":"markdown","source":"**One hot encoding the nominal categorical values**"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_onehot = pd.get_dummies(data_model, columns=['race', 'gender','max_glu_serum', 'A1Cresult', 'change',\n       'diabetesMed', 'readmitted'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Label Encoding the AGE(ordinal) categorical column**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_onehot['age']=data_onehot.apply(le.fit_transform)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_onehot[['discharge_disposition_id','admission_type_id', 'admission_source_id','num_lab_procedures',\n       'num_procedures','time_in_hospital',\n       'num_medications', 'number_outpatient', 'number_emergency',\n       'number_inpatient']]=  data_model[['discharge_disposition_id','admission_type_id','admission_source_id','num_lab_procedures',\n       'num_procedures','time_in_hospital',\n       'num_medications', 'number_outpatient', 'number_emergency',\n       'number_inpatient']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_onehot.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Feature selection**"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_onehot.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_onehot.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#data_onehot = data_onehot.drop(['Unnamed: 0'],axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_onehot.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_onehot.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Chi-Square Test of Independence"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport scipy.stats as stats\nfrom scipy.stats import chi2_contingency\n\nclass ChiSquare:\n    def __init__(self, dataframe):\n        self.df = dataframe\n        self.p = None #P-Value\n        self.chi2 = None #Chi Test Statistic\n        self.dof = None\n        \n        self.dfObserved = None\n        self.dfExpected = None\n        \n    def _print_chisquare_result(self, colX, alpha):\n        result = \"\"\n        if self.p<alpha:\n            result=\"{0} is IMPORTANT for Prediction\".format(colX)\n        else:\n            result=\"{0} is NOT an important predictor. (Discard {0} from model)\".format(colX)\n\n        print(result)\n        \n    def TestIndependence(self,colX,colY, alpha=0.05):\n        X = self.df[colX].astype(str)\n        Y = self.df[colY].astype(str)\n        \n        self.dfObserved = pd.crosstab(Y,X) \n        chi2, p, dof, expected = stats.chi2_contingency(self.dfObserved.values)\n        self.p = p\n        self.chi2 = chi2\n        self.dof = dof \n        \n        self.dfExpected = pd.DataFrame(expected, columns=self.dfObserved.columns, index = self.dfObserved.index)\n        \n        self._print_chisquare_result(colX,alpha)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_onehot['dummyCat'] = np.random.choice([0, 1], size=(len(data_onehot),), p=[0.5, 0.5])\n\ndata_onehot.dummyCat.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Initialize ChiSquare Class\ncT = ChiSquare(data_onehot)\n\n#Feature Selection\ntestColumns = ['encounter_id', 'patient_nbr', 'age', 'admission_type_id',\n       'discharge_disposition_id', 'admission_source_id', 'time_in_hospital',\n       'num_lab_procedures', 'num_procedures', 'num_medications',\n       'number_outpatient', 'number_emergency', 'number_inpatient',\n        'race_AfricanAmerican', 'race_Asian', 'race_Caucasian',\n       'race_Hispanic', 'race_Other', 'gender_Female', 'gender_Male',\n       'max_glu_serum_>200', 'max_glu_serum_>300', 'max_glu_serum_None',\n       'max_glu_serum_Norm', 'A1Cresult_>7', 'A1Cresult_>8', 'A1Cresult_None',\n       'A1Cresult_Norm', 'change_Ch', 'change_No', 'diabetesMed_Yes',\n       'readmitted_NO']\nfor var in testColumns:\n    cT.TestIndependence(colX=var,colY=\"treatments\" ) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.stats import chisquare,chi2_contingency\n\ncat_col = []\nchi_pvalue = []\nchi_name = []\n\ndef chi_sq(i):\n    ct = pd.crosstab(data_onehot['treatments'],data_onehot[i])\n    chi_pvalue.append(chi2_contingency(ct)[1])\n    chi_name.append(i)\n\nfor i in testColumns:\n    chi_sq(i)\n\nchi_data = pd.DataFrame()\nchi_data['Pvalue'] = chi_pvalue\nchi_data.index = chi_name\n\nplt.figure(figsize=(11,8))\nplt.title('P-Values of Chisquare with ''Treatments'' as Target Categorical Attribute',fontsize=16)\nx = chi_data.Pvalue.sort_values().plot(kind='barh')\nx.set_xlabel('P-Values',fontsize=15)\nx.set_ylabel('Independent Categorical Attributes',fontsize=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Feature Selection using Random Forest Algorithm**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import `RandomForestClassifier`\nfrom sklearn.ensemble import RandomForestClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Isolate Data, class labels and column values\nX = data_onehot.drop(['treatments'],axis=1)\nY = data_onehot['treatments']\nY=Y.replace(['insulin','io'],[0,1])\nnames = data_onehot.columns.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Build the model\nrfc = RandomForestClassifier()\n\n# Fit the model\nrfc.fit(X, Y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Finding the feature importance using Random Forest\nfeature_imp=pd.DataFrame({'Features':X.columns,'Importance':rfc.feature_importances_})\nfeature_imp.sort_values(by = 'Importance',ascending=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_onehot.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Out of Two techniques for Feature Selection, Chi-Square Test of Independence seems to be more efficient when compared to Random Forest. So dropping the Features which are labelled as not important predictor by Chi-Square test\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = data_onehot.drop(['encounter_id','patient_nbr','age','num_lab_procedures','number_outpatient','number_emergency',\n                      'race_Asian','race_Other','diabetesMed_Yes','max_glu_serum_>200','A1Cresult_>8','A1Cresult_Norm',\n                      'readmitted_NO','dummyCat','treatments'],axis=1)\nX.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import MultinomialNB,GaussianNB,BernoulliNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score,confusion_matrix,classification_report","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=2)\nX_train.shape,X_test.shape,y_train.shape,y_test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Baseline model"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_p=[]\nfor i in range(y_test.shape[0]):\n    y_p.append(y_test.mode()[0])#Highest class is assigned to a list which is compared with ytest\nlen(y_p) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred=pd.Series(y_p)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy_score(y_test,y_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Baseline Accuracy is around 54.4%"},{"metadata":{},"cell_type":"markdown","source":"### Below Built every model is a base model so there is furthur possibilty that a model can perform better by tuning it."},{"metadata":{},"cell_type":"markdown","source":"# Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Logistic Regression\nm1=LogisticRegression()\nm1.fit(X_train,y_train)\ny_pred_lr=m1.predict(X_test)\nTrain_Score_lr = m1.score(X_train,y_train)\nTest_Score_lr = accuracy_score(y_test,y_pred_lr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Training Accuracy is:',Train_Score_lr)\nprint('Testing Accuracy is:',Test_Score_lr)\nprint(classification_report(y_test,y_pred_lr))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### The training accuracy and testing accuracy of model is almost similar which helps us to understand that model is neither overfitting nor underfitting"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_curve, auc\nfpr,tpr, _ = roc_curve(y_test,y_pred_lr)\nroc_auc_lr = auc(fpr, tpr)\n\nprint('Auc for Logistic Regression is:',roc_auc_lr)\nplt.figure()\nplt.plot(fpr, tpr)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## KNN Classifier\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"m2 = KNeighborsClassifier()\nm2.fit(X_train,y_train)\ny_pred_knn = m2.predict(X_test)\nTrain_Score_knn = m2.score(X_train,y_train)\nTest_Score_knn = accuracy_score(y_test,y_pred_knn)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Training Accuracy is :',Train_Score_knn)\nprint('Testing Accuracy is:',Test_Score_knn)\nprint(classification_report(y_test,y_pred_knn))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### The training accuracy when compared to testing accuracy of model is more which helps us to understand that model is overfitting"},{"metadata":{"trusted":true},"cell_type":"code","source":"fpr,tpr, _ = roc_curve(y_test,y_pred_knn)\nroc_auc_knn = auc(fpr, tpr)\n\nprint('Auc for KNN is:',roc_auc_knn)\nplt.figure()\nplt.plot(fpr, tpr)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Bernoulli Naives Bayes"},{"metadata":{"trusted":true},"cell_type":"code","source":"m3=BernoulliNB()\nm3.fit(X_train,y_train)\ny_pred_bnb=m3.predict(X_test)\nTrain_Score_bnb = m3.score(X_train,y_train)\nTest_Score_bnb = accuracy_score(y_test,y_pred_bnb)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Training Accuracy :',Train_Score_bnb)\nprint('Testing Accuracy  :',Test_Score_bnb)\nprint(classification_report(y_test,y_pred_bnb))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### The training accuracy and testing accuracy of model is almost similar which helps us to understand that model is neither overfitting nor underfitting"},{"metadata":{"trusted":true},"cell_type":"code","source":"fpr,tpr, _ = roc_curve(y_test,y_pred_bnb)\nroc_auc_bnb = auc(fpr, tpr)\n\nprint('Auc for Bernoulli Naive Bayes is:',roc_auc_bnb)\nplt.figure()\nplt.plot(fpr, tpr)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Decision Tree"},{"metadata":{"trusted":true},"cell_type":"code","source":"m4 = DecisionTreeClassifier()\nm4.fit(X_train,y_train)\ny_pred_dt=m4.predict(X_test)\nTrain_Score_dt = m4.score(X_train,y_train)\nTest_Score_dt = accuracy_score(y_test,y_pred_dt)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Training Accuracy :',Train_Score_dt)\nprint('Testing Accuracy :',Test_Score_dt)\nprint(classification_report(y_test,y_pred_dt))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### The training accuracy when compared to testing accuracy of model is more which helps us to understand that model is overfitting"},{"metadata":{"trusted":true},"cell_type":"code","source":"fpr,tpr, _ = roc_curve(y_test,y_pred_dt)\nroc_auc_dt = auc(fpr, tpr)\n\nprint('Auc for Decision Tree is:',roc_auc_dt)\nplt.figure()\nplt.plot(fpr, tpr)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"m5 = RandomForestClassifier()\nm5.fit(X_train,y_train)\ny_pred_rf=m5.predict(X_test)\nTrain_Score_rf = m5.score(X_train,y_train)\nTest_Score_rf = accuracy_score(y_test,y_pred_rf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Training Accuracy :',Train_Score_rf)\nprint('Testing Accuracy :',Test_Score_rf)\nprint(classification_report(y_test,y_pred_rf))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### The training accuracy when compared to testing accuracy of model is more which helps us to understand that model is overfitting"},{"metadata":{"trusted":true},"cell_type":"code","source":"fpr,tpr, _ = roc_curve(y_test,y_pred_rf)\nroc_auc_rf = auc(fpr, tpr)\n\nprint('Auc for Random Forest is:',roc_auc_rf)\nplt.figure()\nplt.plot(fpr, tpr)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Let us tune the hyper parameters of Non-parametric models ( Decision Tree and KNN ) using GridSearch"},{"metadata":{"trusted":true},"cell_type":"code","source":"# GridSearchCV to find optimal max_depth\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\n\n\n# specify number of folds for k-fold CV\nn_folds = 3\n\n# parameters to build the model on\nparameters = {'max_depth': range(5, 15, 5),\n    'min_samples_leaf': range(50, 150, 50),\n    'min_samples_split': range(50, 150, 50),\n    'criterion': [\"entropy\", \"gini\"]}\n\n# instantiate the model\ndtree = DecisionTreeClassifier(random_state = 100)\n\n# fit tree on training data\ntree = GridSearchCV(dtree, parameters, \n                    cv=n_folds, \n                   scoring=\"accuracy\")\ntree.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tree.best_params_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Building a decision tree model with tuned parameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"m6 = DecisionTreeClassifier(criterion='entropy',max_depth=5,min_samples_leaf=100,min_samples_split=50)\nm6.fit(X_train,y_train)\ny_pred_tdt=m6.predict(X_test)\nTrain_Score_tdt = m6.score(X_train,y_train)\nTest_Score_tdt = accuracy_score(y_test,y_pred_tdt)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Training Accuracy :',Train_Score_tdt)\nprint('Testing Accuracy  :',Test_Score_tdt)\nprint(classification_report(y_test,y_pred_tdt))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### The training accuracy and testing accuracy of model is almost similar which helps us to understand that model is neither overfitting nor underfitting"},{"metadata":{"trusted":true},"cell_type":"code","source":"fpr,tpr, _ = roc_curve(y_test,y_pred_tdt)\nroc_auc_tdt = auc(fpr, tpr)\n\nprint('Auc for Tuned Decision Tree is:',roc_auc_tdt)\nplt.figure()\nplt.plot(fpr, tpr)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Gridsearch CV to find Optimal K value for KNN model\ngrid = {'n_neighbors':np.arange(1,50)}\nknn=KNeighborsClassifier()\nknn_cv=GridSearchCV(knn,grid,cv=3)\nknn_cv.fit(X_train,y_train)\n\n\nprint(\"Tuned Hyperparameter k: {}\".format(knn_cv.best_params_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"m7 = KNeighborsClassifier(n_neighbors=45)\nm7.fit(X_train,y_train)\ny_pred_tknn=m7.predict(X_test)\nTrain_Score_tknn = m7.score(X_train,y_train)\nTest_Score_tknn = accuracy_score(y_test,y_pred_tknn)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Training Accuracy :',Train_Score_tknn)\nprint('Testing Accuracy  :',Test_Score_tknn)\nprint(classification_report(y_test,y_pred_tknn))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### The training accuracy when compared to testing accuracy of model is more which helps us to understand that model is overfitting"},{"metadata":{"trusted":true},"cell_type":"code","source":"fpr,tpr, _ = roc_curve(y_test,y_pred_tknn)\nroc_auc_tknn = auc(fpr, tpr)\n\nprint('Auc for Tuned KNN is:',roc_auc_tknn)\nplt.figure()\nplt.plot(fpr, tpr)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Hyper Tuning the Random Forest Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"parameter={'n_estimators':np.arange(1,101)}\ngs = GridSearchCV(m5,parameter,cv=3)\ngs.fit(X_train,y_train)\ngs.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"m8 = RandomForestClassifier(n_estimators=71)\nm8.fit(X_train,y_train) \ny_pred_trf=m8.predict(X_test)\nTrain_Score_trf = m8.score(X_train,y_train)\nTest_Score_trf = accuracy_score(y_test,y_pred_trf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Training Accuracy :',Train_Score_trf)\nprint('Testing Accuracy  :',Test_Score_trf)\nprint(classification_report(y_test,y_pred_trf))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### The training accuracy when compared to testing accuracy of model is more which helps us to understand that model is overfitting"},{"metadata":{"trusted":true},"cell_type":"code","source":"fpr,tpr, _ = roc_curve(y_test,y_pred_trf)\nroc_auc_trf = auc(fpr, tpr)\n\nprint('Auc for Tuned Random Forest is:',roc_auc_trf)\nplt.figure()\nplt.plot(fpr, tpr)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# CatBoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_model.treatments.replace(['insulin','io'],[0,1],inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_model.head().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a = data_model.drop(['age','diabetesMed','readmitted','treatments'],axis=1)\nb = data_model.treatments","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cate_features_index = np.where(a.dtypes != int)[0]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xtrain,xtest,ytrain,ytest = train_test_split(a,b,train_size=.70,random_state=2)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from catboost import CatBoostClassifier, Pool,cv\n#let us make the catboost model, use_best_model params will make the model prevent overfitting\nmodel = CatBoostClassifier(eval_metric='Accuracy',use_best_model=True,random_seed=42)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"model.fit(xtrain,ytrain,cat_features=cate_features_index,eval_set=(xtest,ytest))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#show the model test acc, but you have to note that the acc is not the cv acc,\n#so recommend to use the cv acc to evaluate your model!\nprint('the test accuracy is :{:.6f}'.format(accuracy_score(ytest,model.predict(xtest))))\ntest_score_catboost = accuracy_score(ytest,model.predict(xtest))\nprint(\"the train accuracy is :\",model.score(xtrain,ytrain))\ntrain_score_catboost = model.score(xtrain,ytrain)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fpr,tpr, _ = roc_curve(ytest,model.predict(xtest))\nroc_auc_cb = auc(fpr, tpr)\n\nprint('Auc for Cat Boost is:',roc_auc_cb)\nplt.figure()\nplt.plot(fpr, tpr)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.predict(xtest)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Model_Scores=pd.DataFrame({'Models':['Logistic Regression','KNN','Bernauli Naives Bayes','Decision Tree','Random Forest','Tuned Decison Tree','Tuned KNN','Tuned Random Forest','Cat Boost'],\n             'Training Accuracy':[Train_Score_lr,Train_Score_knn,Train_Score_bnb,Train_Score_dt,Train_Score_rf,Train_Score_tdt,Train_Score_tknn,Train_Score_trf,train_score_catboost],\n             'Testing Accuracy':[Test_Score_lr,Test_Score_knn,Test_Score_bnb,Test_Score_dt,Test_Score_rf,Test_Score_tdt,Test_Score_tknn,Test_Score_trf,test_score_catboost],\n                'AUC':[roc_auc_lr,roc_auc_knn,roc_auc_bnb,roc_auc_dt,roc_auc_rf,roc_auc_tdt,roc_auc_tknn,roc_auc_trf,roc_auc_cb]})\n\nModel_Scores.sort_values(by=('Testing Accuracy'),ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Logistic Regression gives us the Best Test Accuracy so we choose Logistic Regression as our BenchMark model"},{"metadata":{},"cell_type":"markdown","source":"# Applying Boosting Technique on Benchmark Model (Logistic Regression)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import AdaBoostClassifier,BaggingClassifier\nbslr=AdaBoostClassifier(base_estimator=LogisticRegression())\nbslr.fit(X_train,y_train)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_blr=bslr.predict(X_test)\nTrain_Score_bslr = bslr.score(X_train,y_train)\nTest_Score_bslr = accuracy_score(y_test,y_pred_blr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Training Accuracy :',Train_Score_bslr)\nprint('Testing Accuracy  :',Test_Score_bslr)\nprint(classification_report(y_test,y_pred_blr))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fpr,tpr, _ = roc_curve(y_test,y_pred_blr)\nroc_auc_bslr = auc(fpr, tpr)\n\nprint('Auc for Boosted Logistic Regression is:',roc_auc_bslr)\nplt.figure()\nplt.plot(fpr, tpr)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bglr=BaggingClassifier(base_estimator=LogisticRegression())\nbglr.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_bglr=bglr.predict(X_test)\nTrain_Score_bglr = bglr.score(X_train,y_train)\nTest_Score_bglr = accuracy_score(y_test,y_pred_blr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Training Accuracy :',Train_Score_bglr)\nprint('Testing Accuracy  :',Test_Score_bglr)\nprint(classification_report(y_test,y_pred_bglr))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fpr,tpr, _ = roc_curve(y_test,y_pred_bglr)\nroc_auc_bglr = auc(fpr, tpr)\n\nprint('Auc for Bagged Logistic Regression is:',roc_auc_bglr)\nplt.figure()\nplt.plot(fpr, tpr)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Comparing Benchmark model with Boosted Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"Model_Scores=pd.DataFrame({'Models':['Logistic Regression','Boosted Logistic Regression','Cat Boost'],\n             'Training Accuracy':[Train_Score_lr,Train_Score_bslr,train_score_catboost],\n             'Testing Accuracy':[Test_Score_lr,Test_Score_bslr,test_score_catboost],\n                    'AUC':[roc_auc_lr,roc_auc_bslr,roc_auc_cb]})\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Model_Scores.sort_values(by='Testing Accuracy',ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# After applying boosted technique on Logistic Regression still the best accuracy is given by the Benchmark Model"},{"metadata":{},"cell_type":"markdown","source":"# Performing Stacking technique on the models"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import VotingClassifier\nfrom sklearn.model_selection import cross_val_score\nstacked = VotingClassifier(estimators=[('Logistic Regression',m1),('KNN',m2),('Naive Bayes',m3),('Decision Tree',m4),\n                                      ('RandomForest',m5),('Tuned Decision Tree',m6),('Tuned KNN',m7),\n                                       ('Tuned Random Forest',m8),('Boosted Logistic Regression',bslr)],voting='hard')\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for model, name in zip([m1,m2,m3,m4,m5,m6,m7,m8,bslr,stacked],['Logistic Regression','KNN','Naive Bayes','Decision Tree','RandomForest',\n                                                               'Tuned Decision Tree','Tuned KNN','Tuned Random Forest',\n                                                               'Boosted Logistic Regression','stacked']):\n    scores=cross_val_score(model,X,Y,cv=5,scoring='accuracy')\n    print('Accuarcy: %0.02f (+/- %0.4f)(%s)'%(scores.mean(),scores.var(),name))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Gradient Boosting Algorithm to check the accuracy"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier\ngbdt=GradientBoostingClassifier(n_estimators=150,random_state=2)\ngbdt.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_gbdt=gbdt.predict(X_test)\nTrain_Score_gbdt = gbdt.score(X_train,y_train)\nTest_Score_gbdt = accuracy_score(y_test,y_pred_gbdt)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Training Accuracy :',Train_Score_gbdt)\nprint('Testing Accuracy  :',Test_Score_gbdt)\nprint(classification_report(y_test,y_pred_gbdt))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# CV method to check the bias and variance error"},{"metadata":{"trusted":true},"cell_type":"code","source":"models=[]\nmodels.append(('Logistic_Regression',m1))\nmodels.append(('KNN',m2))\nmodels.append(('Bernoulli_NB',m3))\nmodels.append(('Decison Tree',m4))\nmodels.append(('Random Forest',m5))\nmodels.append(('Tuned Decision Tree',m6))\nmodels.append(('Tuned KNN',m7))\nmodels.append(('Tuned Random Forest',m8))\nmodels.append(('Bagged Logistic Regression',bglr))\nmodels.append(('Boosted Logistic regression',bslr))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nresults=[]\nnames=[]\nscoring='accuracy'\nfor name,model in models:\n    kfold=KFold(n_splits=5,random_state=2)\n    cv_results=cross_val_score(model,X,Y,cv=kfold,scoring=scoring)\n    results.append(cv_results)\n    names.append(name)\n    msg=\"%s: %f (%f)\"%(name,np.mean(cv_results),cv_results.var())\n    print(msg)\n#boxplot alogorithm comparision\nfig=plt.figure(figsize=(16,8))\nfig.suptitle('Algorithm Comparision')\nax=fig.add_subplot(111)\nplt.boxplot(results)\nax.set_xticklabels(names)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import log_loss\n\nlog_loss(y_test, y_pred_lr, eps=1e-15)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusion"},{"metadata":{},"cell_type":"markdown","source":"**As per occum's razor rule and also by considering bias-variance trade off, base logistic regression stands out to be the best model with 76 percent accuracy**\n"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":1}