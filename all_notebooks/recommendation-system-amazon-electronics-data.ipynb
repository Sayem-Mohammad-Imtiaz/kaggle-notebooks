{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set(color_codes=True)\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data_set=pd.read_csv(\"/kaggle/input/amazon-product-reviews/ratings_Electronics (1).csv\",names=['userid', 'productid','rating','timestamp'])\ndata_set.info()\ndata_set.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **EDA**"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_set_buffer = data_set.drop('timestamp',axis=1)\n#consider only 10% of the data\ndata_set_buffer = data_set_buffer.sample(frac=0.1)\ndel data_set","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_set_buffer.head().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#print missing value\nprint(data_set_buffer.isna().sum())\nprint(data_set_buffer.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_set_buffer.describe().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_set_buffer.rating.describe().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_set_buffer.groupby('userid')['rating'].count().sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(style=\"white\", palette=\"tab10\", color_codes=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = sns.countplot(data=data_set_buffer,x='rating');\nax.set_ylim(0, len(data_set_buffer))\nax.set_xlim(0, 5)\nfor p in ax.patches:\n    height = p.get_height()\n    ax.text(p.get_x()+p.get_width()/2.,\n            height + 3,\n            '{:%}'.format(height/float(len(data_set_buffer))),\n            ha=\"center\") \nplt.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Observation : 56% of users have rated 5. So we have highest number of 5 ratings.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_set_buffer_with_threshold50=data_set_buffer.groupby(\"productid\").filter(lambda x:x['rating'].count() >=50)\ndel data_set_buffer","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**keep the users only who has given 50 or more number of ratings**"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_set_buffer_with_threshold50.groupby('productid')['rating'].count().sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Build Popularity Recommender model**\nOur definition of popularity : A product with highest average rating meets the basic criteria of atleast reviewed by 50 unique users."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ntrain_data, test_data = train_test_split(data_set_buffer_with_threshold50, test_size = 0.3, random_state=0)\ntrain_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Count no of user_id for each unique product as recommendation score \ntrain_data_grouped = train_data.groupby('productid').agg({'userid': 'count'}).reset_index()\ntrain_data_grouped.rename(columns = {'userid': 'noofusers'},inplace=True)\ntrain_data_grouped.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Count no of user_id for each unique product as recommendation score \ntrain_data_grouped_rating= train_data.groupby(['productid'])['rating'].sum().reset_index()\ntrain_data_grouped_rating.rename(columns = {'rating': 'ratingsum'},inplace=True)\ntrain_data_grouped_rating.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#top five prouducts as per their avg rating\n#Count of user_id for each unique product as recommendation score \ntrain_data_grouped_users = train_data.groupby('productid').agg({'userid': 'count'}).reset_index()\ntrain_data_grouped_users.rename(columns = {'userid': 'noofuser'},inplace=True)\ntrain_data_grouped_users.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data_merged_grouped = pd.merge(train_data_grouped_rating, train_data_grouped_users, on='productid')\ntrain_data_merged_grouped.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data_merged_grouped['averagerating']= train_data_merged_grouped['ratingsum']/train_data_merged_grouped['noofuser']\ntrain_data_merged_grouped.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data_merged_grouped.sort_values('averagerating',ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# TOP 5 popular products"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Find top 5 popular products\ntrain_data_merged_grouped.sort_values('averagerating',ascending=False).head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del train_data_grouped\ndel train_data_grouped_rating\ndel train_data_merged_grouped","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Collaborative Filtering model."},{"metadata":{"trusted":true},"cell_type":"code","source":"from surprise import KNNWithMeans\nfrom surprise import Dataset\nfrom surprise import accuracy\nfrom surprise import Reader\nfrom surprise.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Load the dataframe to surprise. Observation : Got memory error so considering almost 1% only of the original dataset!!\ndata_set_buffer_with_threshold50 = data_set_buffer_with_threshold50.sample(frac=0.1)\ndata = Dataset.load_from_df(data_set_buffer_with_threshold50,Reader(rating_scale=(1, 5)))\ntrainset, testset = train_test_split(data, test_size=.30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use user_based true/false to switch between user-based or item-based collaborative filtering\nuserusercollaborativefiltering = KNNWithMeans(k=5, sim_options={'name': 'pearson_baseline', 'user_based': True})\nuserusercollaborativefiltering.fit(trainset)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainset.n_users","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_pred = userusercollaborativefiltering.test(testset)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#RMSE\nprint(\"User-based Model : Test Set RMSE score\")\naccuracy.rmse(test_pred, verbose=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# item-based collaborative filtering\nitembasedcollaborativefiltering = KNNWithMeans(k=5, sim_options={'name': 'pearson_baseline', 'user_based': False})\nitembasedcollaborativefiltering.fit(trainset)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_pred_I = itembasedcollaborativefiltering.test(testset)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#RMSE\nprint(\"Irem-based Model : Test Set RMSE score\")\naccuracy.rmse(test_pred_I, verbose=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del test_pred_I\ndel trainset\ndel data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Recommend top 5 products to every user**"},{"metadata":{},"cell_type":"markdown","source":"# **SVD or Matrix Factorization**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import defaultdict\nfrom surprise import SVD","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# First train an SVD algorithm with dataset.data_set_buffer = data_set_buffer.sample(frac=0.1)\ndataset_svd = data_set_buffer_with_threshold50.sample(frac=0.01)\ndataset_svd = Dataset.load_from_df(dataset_svd,Reader(rating_scale=(1, 5)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainset = dataset_svd.build_full_trainset()\nsvd_algo = SVD()\nsvd_algo.fit(trainset)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Predict ratings for all pairs (u, i) that are NOT in the training set.\ntestset = trainset.build_anti_testset()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = svd_algo.test(testset)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_top_n_recommendations(reccomemndations, n=5):\n    # First map the reccommendations to each user.\n    top_n = defaultdict(list)\n    for uid, iid, true_r, est, _ in reccomemndations:\n        top_n[uid].append((iid, est))\n\n    #sort predictions for each user and retrieve the k highest ones.\n    for uid, user_ratings in top_n.items():\n        user_ratings.sort(key=lambda x: x[1], reverse=True)\n        top_n[uid] = user_ratings[:n]\n\n    return top_n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"top_5 = get_top_n_recommendations(predictions, n=5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Top 5 products for each user:"},{"metadata":{"trusted":true},"cell_type":"code","source":"for uid, user_ratings in top_5.items():\n    print(uid, [iid for (iid, _) in user_ratings])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Observation :**\n1. We have 1st explored the basic EDA where we got to know the large no of users rated 5 and we have considered the products minimum of 50 users reviewed.\n2. We used a  popularity based recommender model where we recommended top 5 products where we had cold start problem.(No user info/user insight availble)\n3. We have developed a Collaborative Filtering model where we recommended similar users with similar products and similar product items to different users. We call it user-user and item-item collaborative filtering model. For similarity we have used pearson correlation but we can use cosine similarity also to find out distance between our feature vectors.\n4. We have also used Matrix factorization or SVD (Singular Vector Decomposition) to develop a model where we can recommend set of 5 products to each individual users.\n\nNote : I am not sure if we can use apriori or market basket analysis to recommend products. Please do let me know if we can use market basket analysis here too. My idea is we don't have support or lift data required or a kind of basket available in the form of input data. Please vote up if you found helpful and happy learning!\n    "},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}