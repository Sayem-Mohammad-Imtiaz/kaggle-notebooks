{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Imports"},{"metadata":{"trusted":true},"cell_type":"code","source":"from mlxtend.frequent_patterns import apriori\nfrom mlxtend.frequent_patterns import association_rules","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Load and Examine Data"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"retail = pd.read_csv('../input/onlineretail/OnlineRetail.csv', encoding = 'unicode_escape')\nretail.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Preprocessing\n\nThere is a little cleanup, we need to do. First, some of the descriptions have spaces that need to be removed. We’ll also drop the rows that don’t have invoice numbers and remove the credit transactions (those with invoice numbers containing C)."},{"metadata":{"trusted":true},"cell_type":"code","source":"retail['Description'] = retail['Description'].str.strip()\nretail.dropna(axis=0, subset=['InvoiceNo'], inplace=True)\nretail['InvoiceNo'] = retail['InvoiceNo'].astype('str')\nretail = retail[~retail['InvoiceNo'].str.contains('C')]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Consolidate Items into 1 Transaction Per Row\n\nAfter the cleanup, we need to consolidate the items into 1 transaction per row with each product 1 hot encoded. For the sake of keeping the data set small, I’m only looking at sales for France. However, in additional code below, I will compare these results to sales from Germany. Further country comparisons would be interesting to investigate."},{"metadata":{"trusted":true},"cell_type":"code","source":"basket = (retail[retail['Country'] ==\"France\"]\n          .groupby(['InvoiceNo', 'Description'])['Quantity']\n          .sum().unstack().reset_index().fillna(0)\n          .set_index('InvoiceNo'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"basket.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Postprocessing"},{"metadata":{},"cell_type":"markdown","source":"There are a lot of zeros in the data but we also need to make sure any positive values are converted to a 1 and anything less the 0 is set to 0. This step will complete the one hot encoding of the data and remove the postage column (since that charge is not one we wish to explore):"},{"metadata":{"trusted":true},"cell_type":"code","source":"def encode_units(x):\n    if x <= 0:\n        return 0\n    if x >= 1:\n        return 1\n\nbasket_sets = basket.applymap(encode_units)\nbasket_sets.drop('POSTAGE', inplace=True, axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Generate Frequent Itemsets\n\nNow that the data is structured properly, we can generate frequent item sets that have a support of at least 7% (this number was chosen so that I could get enough useful examples):"},{"metadata":{"trusted":true},"cell_type":"code","source":"frequent_itemsets = apriori(basket_sets, min_support=0.07, use_colnames=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Building Association Rules Using Frequent Itemset\n\nThe final step is to generate the rules with their corresponding support, confidence and lift:"},{"metadata":{"trusted":true},"cell_type":"code","source":"rules = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=1)\nrules.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Visualizing Relationship between Support, Confidence and Support"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import seaborn under its standard alias\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\n# Generate scatterplot using support and confidence\nsns.scatterplot(x = \"support\", y = \"confidence\", \n                size = \"lift\", data = rules)\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Filtering Rules Dataframe\n\nNow, the tricky part is figuring out what this tells us. For instance, we can see that there are quite a few rules with a high lift value which means that it occurs more frequently than would be expected given the number of transaction and product combinations. We can also see several where the confidence is high as well. This part of the analysis is where the domain knowledge will come in handy.\n\nWe can filter the dataframe using standard pandas code. In this case, look for a large lift (6) and high confidence (.8):"},{"metadata":{"trusted":true},"cell_type":"code","source":"rules[ (rules['lift'] >= 6) &\n       (rules['confidence'] >= 0.8) ]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Further Analysis\n \nYou may want to look at how much opportunity there is to use the popularity of one product to drive sales of another. For instance, we can see that we sell 340 Green Alarm clocks but only 316 Red Alarm Clocks so maybe we can drive more Red Alarm Clock sales through recommendations?"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"ALARM CLOCK BAKELIKE GREEN : \",basket['ALARM CLOCK BAKELIKE GREEN'].sum())\nprint(\"ALARM CLOCK BAKELIKE RED : \",basket['ALARM CLOCK BAKELIKE RED'].sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Popular Product Combinations by Country"},{"metadata":{"trusted":true},"cell_type":"code","source":"basket2 = (retail[retail['Country'] ==\"Germany\"]\n          .groupby(['InvoiceNo', 'Description'])['Quantity']\n          .sum().unstack().reset_index().fillna(0)\n          .set_index('InvoiceNo'))\n\n\nbasket2.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"basket_sets2 = basket2.applymap(encode_units)\nbasket_sets2.drop('POSTAGE', inplace=True, axis=1)\nfrequent_itemsets2 = apriori(basket_sets2, min_support=0.05, use_colnames=True)\nrules2 = association_rules(frequent_itemsets2, metric=\"lift\", min_threshold=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import seaborn under its standard alias\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\n# Generate scatterplot using support and confidence\nsns.scatterplot(x = \"support\", y = \"confidence\", \n                size = \"lift\", data = rules2)\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rules2[ (rules2['lift'] >= 4) &\n        (rules2['confidence'] >= 0.5)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import seaborn under its standard alias\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Transform the DataFrame of rules into a matrix using the lift metric\n\npivot = rules2.pivot(index = 'consequents', columns = 'antecedents', values= 'lift')\n\n# Generate a heatmap with annotations on and the colorbar off\n\nsns.heatmap(pivot, annot = True, cbar=False)\nplt.yticks(rotation=0)\nplt.xticks(rotation=90)\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## References\n\nhttps://pbpython.com/market-basket-analysis.html"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}