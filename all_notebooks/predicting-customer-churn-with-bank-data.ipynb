{"cells":[{"metadata":{"ExecuteTime":{"end_time":"2020-11-14T23:17:09.018066Z","start_time":"2020-11-14T23:17:09.012084Z"}},"cell_type":"markdown","source":"# Table of Contents\n\n* [Bank Customer Churn Modelling](#modelling)\n  * [Understanding the dataset](#understanding)\n  * [Visualization](#visualization)\n  * [Correlation](#correlation)\n* [Transformation](#transformation)\n  * [Log Balance](#log-balance)\n  * [One-Hot Encoding](#one-hot)\n* [Principal Component Analysis (optional)](#pca)\n* [Partitioning](#partitioning)\n* [Prediction](#prediction)\n  * [Logistic Regression](#logistic)\n    * [scikit](#scikit)\n    * [keras](#keras)\n    * [log](#log)\n  * [k-NN](#knn)\n  * [Decision Tree Classifier](#decision)\n  * [Neural Network (optional)](#nn)\n    * [FCNN](#fcnn)\n    * [FCNN with SMOTE](#smote)\n  * [Ensemble Methods](#ensemble)\n    * [Soft Vote](#soft-vote)\n    * [Stacking](#stacking)\n    * [AdaBoost](#adaboost)\n* [Model Selection](#selection)"},{"metadata":{},"cell_type":"markdown","source":"# Bank Customer Churn Modelling <a class=\"anchor\" id=\"modelling\"></a>\n\nPredicting customer churn using this [kaggle dataset](https://www.kaggle.com/adammaus/predicting-churn-for-bank-customers)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Need to do until kaggle supports seaborn 0.11.0 by default (I use histplot)\n!pip install seaborn --upgrade","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-11-30T09:25:41.569334Z","start_time":"2020-11-30T09:25:30.930152Z"},"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow as tf\n\nfrom tensorflow import keras\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n\nfrom sklearn import tree\nfrom sklearn.decomposition import PCA\nfrom sklearn.ensemble import VotingClassifier, AdaBoostClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, confusion_matrix, f1_score, accuracy_score, roc_curve, auc\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import MinMaxScaler, QuantileTransformer, StandardScaler\nfrom sklearn.tree import DecisionTreeClassifier\n\nfrom imblearn.over_sampling import SMOTE\nimport scikitplot as skplt\nfrom mlxtend.classifier import EnsembleVoteClassifier\n\nfrom numpy.random import seed\nseed(12345)\ntf.random.set_seed(12345)\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Understanding the dataset <a class=\"anchor\" id=\"understanding\"></a>"},{"metadata":{"ExecuteTime":{"end_time":"2020-11-30T09:25:41.63373Z","start_time":"2020-11-30T09:25:41.579038Z"},"trusted":true},"cell_type":"code","source":"bank_data = pd.read_csv(\"../input/predicting-churn-for-bank-customers/Churn_Modelling.csv\");\ndisplay(bank_data.shape) # rows & columns","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-11-30T09:25:41.654566Z","start_time":"2020-11-30T09:25:41.637157Z"},"trusted":true},"cell_type":"code","source":"bank_data.head()","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-11-30T09:25:41.664385Z","start_time":"2020-11-30T09:25:41.657682Z"},"trusted":true},"cell_type":"code","source":"bank_data.dtypes","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-11-13T01:52:22.7035Z","start_time":"2020-11-13T01:52:22.679628Z"}},"cell_type":"markdown","source":"There are 10000 records. Both RowNumber and CustomerId are unique throughout the entire set, therefore we will remove them. Surname also should not have any information (unless we are profiling by name, which we will not)."},{"metadata":{"ExecuteTime":{"end_time":"2020-11-30T09:25:41.67873Z","start_time":"2020-11-30T09:25:41.667079Z"},"trusted":true},"cell_type":"code","source":"display(bank_data.isnull().sum()) # display missing values","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-11-30T09:25:41.696984Z","start_time":"2020-11-30T09:25:41.681036Z"},"trusted":true},"cell_type":"code","source":"display(bank_data.nunique()) # display unique values","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-11-13T02:07:15.907223Z","start_time":"2020-11-13T02:07:15.892056Z"}},"cell_type":"markdown","source":"There are 10000 rows and no missing values. CustomerID and RowNumber are both unique and can be removed. Surname will also be removed."},{"metadata":{"ExecuteTime":{"end_time":"2020-11-30T09:25:41.703914Z","start_time":"2020-11-30T09:25:41.69893Z"},"trusted":true},"cell_type":"code","source":"bank_data.drop(columns=['RowNumber','CustomerId','Surname'], inplace=True);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Visualization <a class=\"anchor\" id=\"visualization\"></a>"},{"metadata":{"ExecuteTime":{"end_time":"2020-11-30T09:25:43.143791Z","start_time":"2020-11-30T09:25:41.712768Z"},"trusted":true},"cell_type":"code","source":"# Display all numeric values\nfig, axes = plt.subplots(2, 3, figsize=(16, 8));\nsns.histplot(ax=axes[0, 0], data=bank_data[\"CreditScore\"]);\nsns.histplot(ax=axes[0, 1], data=bank_data[\"Age\"]);\nsns.histplot(ax=axes[0, 2], data=bank_data[\"Tenure\"]);\nsns.histplot(ax=axes[1, 0], data=bank_data[\"Balance\"]);\nsns.histplot(ax=axes[1, 1], data=bank_data[\"NumOfProducts\"]);\nsns.histplot(ax=axes[1, 2], data=bank_data[\"EstimatedSalary\"]);","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-11-30T09:26:01.94088Z","start_time":"2020-11-30T09:25:43.148283Z"},"scrolled":false,"trusted":true},"cell_type":"code","source":"# Plot all numerical variables versus each other\nbd_numeric = bank_data[[\"CreditScore\", \"Age\", \"Tenure\", \"Balance\", \"NumOfProducts\", \"EstimatedSalary\", \"Exited\"]];\nsns.pairplot(bd_numeric, hue=\"Exited\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Observations:\n\n* Balance is bimodal, might need to try a log transformation.\n* People with more products tend to exit\n* Older people tend to exit"},{"metadata":{"ExecuteTime":{"end_time":"2020-11-30T09:26:02.633574Z","start_time":"2020-11-30T09:26:01.943057Z"},"trusted":true},"cell_type":"code","source":"# Plot the categorical variables\nfig, axes = plt.subplots(2, 3, figsize=(16, 8));\nsns.countplot(ax=axes[0, 0], data=bank_data, x=\"Geography\", hue=\"Exited\");\nsns.countplot(ax=axes[0, 1], data=bank_data, x=\"Gender\", hue=\"Exited\");\nsns.countplot(ax=axes[0, 2], data=bank_data, x=\"HasCrCard\", hue=\"Exited\");\nsns.countplot(ax=axes[1, 0], data=bank_data, x=\"IsActiveMember\", hue=\"Exited\");\nsns.countplot(ax=axes[1, 1], data=bank_data, x=\"Exited\");\nfig.delaxes(axes[1][2]);","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-11-30T09:26:03.410165Z","start_time":"2020-11-30T09:26:02.636785Z"},"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(3, 2, figsize=(14, 16));\nsns.boxplot(ax=axes[0, 0], data=bank_data, x=\"Exited\", y=\"CreditScore\");\nsns.boxplot(ax=axes[0, 1], data=bank_data, x=\"Exited\", y=\"Age\");\nsns.boxplot(ax=axes[1, 0], data=bank_data, x=\"Exited\", y=\"Tenure\");\nsns.boxplot(ax=axes[1, 1], data=bank_data, x=\"Exited\", y=\"Balance\");\nsns.boxplot(ax=axes[2, 0], data=bank_data, x=\"Exited\", y=\"NumOfProducts\");\nsns.boxplot(ax=axes[2, 1], data=bank_data, x=\"Exited\", y=\"EstimatedSalary\");","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-11-30T09:26:03.50083Z","start_time":"2020-11-30T09:26:03.414963Z"},"trusted":true},"cell_type":"code","source":"display(pd.crosstab(bank_data[\"Exited\"], bank_data[\"Geography\"], margins=True, normalize=False))\ndisplay(pd.crosstab(bank_data[\"Exited\"], bank_data[\"Geography\"], margins=True, normalize=True))","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-11-30T09:26:03.626258Z","start_time":"2020-11-30T09:26:03.503001Z"},"trusted":true},"cell_type":"code","source":"display(pd.crosstab(bank_data[\"Exited\"], bank_data[\"Gender\"], margins=True, normalize=False))\ndisplay(pd.crosstab(bank_data[\"Exited\"], bank_data[\"Gender\"], margins=True, normalize=True))","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-11-30T09:26:03.708174Z","start_time":"2020-11-30T09:26:03.628381Z"},"trusted":true},"cell_type":"code","source":"display(pd.crosstab(bank_data[\"Exited\"], bank_data[\"HasCrCard\"], margins=True, normalize=False))\ndisplay(pd.crosstab(bank_data[\"Exited\"], bank_data[\"HasCrCard\"], margins=True, normalize=True))","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-11-30T09:26:03.784135Z","start_time":"2020-11-30T09:26:03.709884Z"},"trusted":true},"cell_type":"code","source":"display(pd.crosstab(bank_data[\"Exited\"], bank_data[\"IsActiveMember\"], margins=True, normalize=False))\ndisplay(pd.crosstab(bank_data[\"Exited\"], bank_data[\"IsActiveMember\"], margins=True, normalize=True))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Correlation <a class=\"anchor\" id=\"correlation\"></a>"},{"metadata":{"ExecuteTime":{"end_time":"2020-11-30T09:26:04.770558Z","start_time":"2020-11-30T09:26:03.786492Z"},"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,4));\nsns.heatmap(bank_data.corr(), annot=True, fmt=\".2f\", vmin=-1.0, vmax=1, cmap=\"Spectral\");","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-11-13T02:06:06.726829Z","start_time":"2020-11-13T02:06:06.724514Z"}},"cell_type":"markdown","source":"# Transformation <a class=\"anchor\" id=\"transformation\"></a>"},{"metadata":{"ExecuteTime":{"end_time":"2020-11-30T09:26:04.777639Z","start_time":"2020-11-30T09:26:04.774431Z"},"trusted":true},"cell_type":"code","source":"bank_data_transformed = bank_data;","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Log_Balance <a class=\"anchor\" id=\"log-balance\"></a>\nNeed to change all 0 balances to 1. This will still be valid for all of the data."},{"metadata":{"ExecuteTime":{"end_time":"2020-11-30T09:26:04.789328Z","start_time":"2020-11-30T09:26:04.780536Z"},"trusted":true},"cell_type":"code","source":"print(\"         Balance < 0: \", bank_data_transformed[bank_data_transformed[\"Balance\"].lt(0)].shape[0])\nprint(\"         Balance = 0: \", bank_data_transformed[bank_data_transformed[\"Balance\"].eq(0)].shape[0])\nprint(\"1 <= Balance <= 1000: \", bank_data_transformed[bank_data_transformed[\"Balance\"].between(1, 1000)].shape[0])","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-11-30T09:26:04.800183Z","start_time":"2020-11-30T09:26:04.791547Z"},"trusted":true},"cell_type":"code","source":"bank_data_transformed[\"Balance_1\"] = bank_data_transformed[\"Balance\"].replace(0, 1);\nprint(\"         Balance_1 = 0: \", bank_data_transformed[bank_data_transformed[\"Balance_1\"].eq(0)].shape[0])\nprint(\"         Balance_1 = 1: \", bank_data_transformed[bank_data_transformed[\"Balance_1\"].eq(1)].shape[0])","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-11-30T09:26:04.807753Z","start_time":"2020-11-30T09:26:04.80336Z"},"trusted":true},"cell_type":"code","source":"bank_data_transformed[\"Log10_Balance_1\"] = np.log10(bank_data_transformed[\"Balance_1\"]);","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-11-30T09:26:05.021388Z","start_time":"2020-11-30T09:26:04.810252Z"},"trusted":true},"cell_type":"code","source":"sns.histplot(data=bank_data_transformed[\"Log10_Balance_1\"]);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## One-Hot Encoding <a class=\"anchor\" id=\"one-hot\"></a>"},{"metadata":{"ExecuteTime":{"end_time":"2020-11-30T09:26:05.067648Z","start_time":"2020-11-30T09:26:05.041026Z"},"trusted":true},"cell_type":"code","source":"bank_data_transformed = pd.concat([bank_data_transformed,pd.get_dummies(bank_data_transformed['Geography'], prefix='Geography')],axis=1)\nbank_data_transformed = pd.concat([bank_data_transformed,pd.get_dummies(bank_data_transformed['Gender'], prefix='Gender')],axis=1)\nbank_data_transformed.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Principal Component Analysis (Optional) <a class=\"anchor\" id=\"pca\"></a>"},{"metadata":{"ExecuteTime":{"end_time":"2020-11-30T09:26:05.657553Z","start_time":"2020-11-30T09:26:05.073901Z"},"trusted":true},"cell_type":"code","source":"standard_scaler = StandardScaler();\nscaled_columns = [\"CreditScore\", \"Age\", \"Tenure\", \"Balance\", \"NumOfProducts\", \"HasCrCard\", \"IsActiveMember\", \"EstimatedSalary\", \"Geography_France\", \"Geography_Germany\", \"Geography_Spain\", \"Gender_Male\", \"Gender_Female\"]\nscaled_x = pd.DataFrame(standard_scaler.fit_transform(bank_data_transformed[scaled_columns]), columns=scaled_columns)\nscaled_y = bank_data_transformed[\"Exited\"]\n\npca = PCA()\npca.fit(scaled_x)\n\nfig, axes = plt.subplots(1, 2, figsize=(16, 6));\n\nskplt.decomposition.plot_pca_component_variance(clf=pca, ax=axes[0]);\nskplt.decomposition.plot_pca_2d_projection(pca, scaled_x, scaled_y, ax=axes[1]);","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-11-13T09:08:58.773847Z","start_time":"2020-11-13T09:08:58.77027Z"}},"cell_type":"markdown","source":"# Partitioning (60:20:20) <a class=\"anchor\" id=\"partitioning\"></a>"},{"metadata":{"ExecuteTime":{"end_time":"2020-11-30T09:26:05.716227Z","start_time":"2020-11-30T09:26:05.665164Z"},"trusted":true},"cell_type":"code","source":"random_seed=12345\nbdt_train = bank_data_transformed.sample(frac=0.6, random_state=random_seed) # Train 60%\nbdt_test = bank_data_transformed.drop(bdt_train.index)\n\nbdt_validate = bdt_test.sample(frac=0.5,random_state=random_seed)   # Validate = 50% of remaining 40% ==> 20%\nbdt_test = bdt_test.drop(bdt_validate.index)                        # Test = 20%\n\nprint(\"training set: \", len(bdt_train))\nprint(\"validation set: \", len(bdt_validate))\nprint(\"test set: \", len(bdt_test))\n\ndisplay(bdt_train.head());\ndisplay(bdt_validate.head());\ndisplay(bdt_test.head());","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-11-13T09:06:12.538822Z","start_time":"2020-11-13T09:06:12.535278Z"}},"cell_type":"markdown","source":"# Prediction <a class=\"anchor\" id=\"prediction\"></a>"},{"metadata":{"ExecuteTime":{"end_time":"2020-11-30T09:26:05.733125Z","start_time":"2020-11-30T09:26:05.718931Z"},"trusted":true},"cell_type":"code","source":"# Predict output, confusion matrix, and ROC curve\n#def create_summary(name, data, prediction, metrics, summaries):\ndef create_summary(name, y, y_scores, train_accuracy, val_accuracy, summaries):\n    fpr, tpr, threshold = roc_curve(y.to_numpy(), y_scores[:,1])    \n    optimal_idx = np.argmax(tpr - fpr) # Youden's J-statistic\n    optimal_threshold = threshold[optimal_idx]\n\n    print(\"[\" + name + \"] Optimal threshold: {:.3}\".format(optimal_threshold))\n    y_hat = y_scores[:,1] > optimal_threshold\n    \n    name = name + \" (c={:.3})\".format(optimal_threshold);\n\n    cnf_matrix = confusion_matrix(y, y_hat)\n    display(cnf_matrix)\n    print(classification_report(y, y_hat))\n    \n    y_hat_0_5 = y_scores[:,1] > 0.5\n    auc_val = auc(fpr, tpr)\n\n    summary = pd.DataFrame([[name,\n                             train_accuracy,\n                             val_accuracy,\n                             accuracy_score(y, y_hat_0_5),\n                             f1_score(y, y_hat_0_5),\n                             auc_val,\n                             tpr,\n                             fpr,\n                             cnf_matrix,\n                             y_scores[:,1]\n                            ]], \n                           columns=summary_column_names);\n    summaries = summaries.append(summary, ignore_index=True);\n\n    fig = sns.heatmap(cnf_matrix, annot=True, fmt=\"d\", linewidths=.5, cmap=\"Blues\");\n    fig.set_title(\"Confusion Matrix (Exited=1)\");\n    fig.set_ylabel(\"Actual\");\n    fig.set_xlabel(\"Predicted\");\n\n    title = name + \": \" + str(round(auc_val, 3))\n    \n    fig, axes = plt.subplots(2, 2, figsize=(16, 16));\n    fig.suptitle(title)\n    skplt.metrics.plot_roc(ax=axes[0][0], y_true=y, y_probas=y_scores)\n    skplt.metrics.plot_ks_statistic(ax=axes[0][1], y_true=np.ravel(y), y_probas=y_scores)\n    skplt.metrics.plot_cumulative_gain(ax=axes[1][0], y_true=y, y_probas=y_scores)\n    skplt.metrics.plot_lift_curve(ax=axes[1][1], y_true=y, y_probas=y_scores)\n\n    return summaries;\n\nsummary_column_names = [\"method\", \"training_accuracy\", \"validation_accuracy\", \"test_accuracy\", \"f1_score\", \"auc\", \"tpr\", \"fpr\", \"cnf_matrix\", \"y_scores\"]\nsummaries = pd.DataFrame(columns = summary_column_names)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-11-30T09:26:05.740257Z","start_time":"2020-11-30T09:26:05.73543Z"},"trusted":true},"cell_type":"code","source":"# summarize results\ndef summarize_grid_results(grid_result):\n    print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n    means = grid_result.cv_results_['mean_test_score']\n    stds = grid_result.cv_results_['std_test_score']\n    params = grid_result.cv_results_['params']\n    for mean, stdev, param in zip(means, stds, params):\n        print(\"%f (%f) with: %r\" % (mean, stdev, param))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Logistic Regression <a class=\"anchor\" id=\"logistic\"></a>"},{"metadata":{"ExecuteTime":{"end_time":"2020-11-30T09:26:05.771335Z","start_time":"2020-11-30T09:26:05.742758Z"},"trusted":true},"cell_type":"code","source":"# First I am going to scale all inputs from 0 to 1 using the min-max method\n# minmax = (x_i - x_min) / (x_max - x_min)\nminmax_scaler = MinMaxScaler()\n\nlr_columns_x = [\"CreditScore\", \"Age\", \"Tenure\", \"Balance\", \"NumOfProducts\", \"HasCrCard\", \"IsActiveMember\", \"EstimatedSalary\", \"Geography_France\", \"Geography_Germany\", \"Gender_Male\"]\nlr_columns_y = \"Exited\"\n\nlr_train_x = pd.DataFrame(minmax_scaler.fit_transform(bdt_train[lr_columns_x]), columns=lr_columns_x)\nlr_train_y = bdt_train[lr_columns_y]\n\nlr_val_x = pd.DataFrame(minmax_scaler.fit_transform(bdt_validate[lr_columns_x]), columns=lr_columns_x)\nlr_val_y = bdt_validate[lr_columns_y]\n\nlr_test_x = pd.DataFrame(minmax_scaler.fit_transform(bdt_test[lr_columns_x]), columns=lr_columns_x)\nlr_test_y = bdt_test[lr_columns_y]","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-11-30T09:26:05.813578Z","start_time":"2020-11-30T09:26:05.777992Z"},"trusted":true},"cell_type":"code","source":"log_columns_x = [\"CreditScore\", \"Age\", \"Tenure\", \"Log10_Balance_1\", \"NumOfProducts\", \"HasCrCard\", \"IsActiveMember\", \"EstimatedSalary\", \"Geography_France\", \"Geography_Germany\", \"Gender_Male\"]\nlog_columns_y = \"Exited\"\n\nlog_train_x = pd.DataFrame(minmax_scaler.fit_transform(bdt_train[log_columns_x]), columns=log_columns_x)\nlog_train_y = bdt_train[log_columns_y]\n\nlog_val_x = pd.DataFrame(minmax_scaler.fit_transform(bdt_validate[log_columns_x]), columns=log_columns_x)\nlog_val_y = bdt_validate[log_columns_y]\n\nlog_test_x = pd.DataFrame(minmax_scaler.fit_transform(bdt_test[log_columns_x]), columns=log_columns_x)\nlog_test_y = bdt_test[log_columns_y]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Simple Logistic Regression (scikit-learn) <a class=\"anchor\" id=\"scikit\"></a>"},{"metadata":{"ExecuteTime":{"end_time":"2020-11-30T09:26:05.864557Z","start_time":"2020-11-30T09:26:05.819005Z"},"trusted":true},"cell_type":"code","source":"lr_model = LogisticRegression(solver='liblinear', C=10.0, random_state=0)\nlr_model.fit(lr_train_x, lr_train_y)\nlr_y_scores = lr_model.predict_proba(lr_test_x);","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-11-30T09:26:06.907613Z","start_time":"2020-11-30T09:26:05.866497Z"},"trusted":true},"cell_type":"code","source":"training_accuracy = lr_model.score(lr_train_x, lr_train_y)\nvalidation_accuracy = lr_model.score(lr_val_x, lr_val_y)\n\nsummaries = create_summary(\"slr-scikit\",\n                           lr_test_y, \n                           lr_y_scores, \n                           training_accuracy,\n                           validation_accuracy,\n                           summaries);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Simple Logistic Regression (Keras) <a class=\"anchor\" id=\"keras\"></a>"},{"metadata":{"ExecuteTime":{"end_time":"2020-11-30T09:26:31.256697Z","start_time":"2020-11-30T09:26:06.910396Z"},"scrolled":false,"trusted":true},"cell_type":"code","source":"# 2-class logistic regression in Keras\nlrk_callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\nlrk_model = Sequential()\nlrk_model.add(Dense(units=1, kernel_initializer='glorot_uniform', activation='sigmoid', input_dim=lr_train_x.shape[1]))\nlrk_model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['binary_accuracy'])\nlrk_history = lrk_model.fit(x=lr_train_x, y=lr_train_y, \n                            batch_size=64,\n                            epochs=500, validation_data=(lr_val_x, lr_val_y), \n                            callbacks=[lrk_callback],\n                            verbose=0);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Note: This model takes around 30s to run**"},{"metadata":{"ExecuteTime":{"end_time":"2020-11-30T09:26:31.476563Z","start_time":"2020-11-30T09:26:31.258603Z"},"trusted":true},"cell_type":"code","source":"fig = sns.lineplot(data=lrk_history.history['loss']);\nfig.set(ylabel=\"loss\", xlabel = \"epoch\");","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-11-30T09:26:32.511883Z","start_time":"2020-11-30T09:26:31.478463Z"},"scrolled":false,"trusted":true},"cell_type":"code","source":"# Predict output, confusion matrix, and ROC curve\nlrk_y_scores = lrk_model.predict(lr_test_x);\nlrk_y_scores = np.append(1-lrk_y_scores, lrk_y_scores, axis=1)\n\ntraining_accuracy = lrk_history.history[\"binary_accuracy\"][-1]\nvalidation_accuracy = lrk_history.history[\"val_binary_accuracy\"][-1]\n\nsummaries = create_summary(\"slr-keras\",\n                           lr_test_y, \n                           lrk_y_scores, \n                           training_accuracy,\n                           validation_accuracy,\n                           summaries);","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-11-13T09:13:27.733275Z","start_time":"2020-11-13T09:13:27.729855Z"}},"cell_type":"markdown","source":"### Logistic Regression with Log_Balance and Regularization (Optional) <a class=\"anchor\" id=\"log\"></a>"},{"metadata":{"ExecuteTime":{"end_time":"2020-11-30T09:26:32.519949Z","start_time":"2020-11-30T09:26:32.514116Z"},"trusted":true},"cell_type":"code","source":"log_callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n\nlog_parameters = {\n    'epochs': 500,\n    'verbose': 0\n}\n\ndef create_log_model(kernel_initializer='zeros',\n                     activation='sigmoid', \n                     l1_lambda=0.001,\n                     l2_lambda=0.0, \n                     optimizer='sgd',\n                     loss='binary_crossentropy', \n                     metrics=['accuracy']):\n    model = Sequential()\n    model.add(Dense(units=1,\n                    kernel_initializer=kernel_initializer,\n                    activation=activation,\n                    input_dim=log_train_x.shape[1], \n                    kernel_regularizer=tf.keras.regularizers.L1L2(l1=l1_lambda, l2=l2_lambda),\n                    bias_initializer='zeros'))\n    model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n\n    return model;","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-11-30T09:26:32.527336Z","start_time":"2020-11-30T09:26:32.523419Z"},"scrolled":true,"trusted":true},"cell_type":"code","source":"# Tuning the model\n\n# best: sgd\n#log_optimizer=['rmsprop', 'sgd', 'adam']\n#log_param_grid = dict(optimizer=log_optimizer)\n\n# best: zeros\n#log_kernel_initializer=['glorot_uniform', 'glorot_normal', 'random_normal', 'zeros']\n#log_param_grid = dict(kernel_initializer=log_kernel_initializer)\n\n# best: 0.001\n#log_l1_lambda=[0.0, 0.0001, 0.0003, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3]\n#log_param_grid = dict(l1_lambda=log_l1_lambda)\n\n# best: 0.0\n# log_l2_lambda=[0.0, 0.0001, 0.0003, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3]\n# log_param_grid = dict(l2_lambda=log_l2_lambda)\n\n# perform a Grid Search\n#log_model = KerasClassifier(build_fn=create_log_model, \n#                            epochs=log_parameters['epochs'], \n#                            verbose=log_parameters['verbose'])\n\n#log_grid = GridSearchCV(estimator=log_model, param_grid=log_param_grid, n_jobs=-1)\n#log_grid_result = log_grid.fit(X=log_train_x, y=log_train_y, callbacks=[log_callback])\n\n#summarize_grid_results(log_grid_result)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-11-30T09:28:05.719997Z","start_time":"2020-11-30T09:26:32.529539Z"},"trusted":true},"cell_type":"code","source":"# uses the parameters tuned above\nlog_model = create_log_model()\nlog_history = log_model.fit(x=log_train_x, y=log_train_y,\n                            epochs=log_parameters['epochs'],\n                            callbacks=[log_callback],\n                            validation_data=(log_val_x, log_val_y),\n                            verbose=log_parameters['verbose']);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Note: This model takes ~30s to run**"},{"metadata":{"ExecuteTime":{"end_time":"2020-11-30T09:28:05.95247Z","start_time":"2020-11-30T09:28:05.721996Z"},"trusted":true},"cell_type":"code","source":"fig = sns.lineplot(data=log_history.history['loss']);\nfig.set(ylabel=\"loss\", xlabel = \"epoch\");","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-11-30T09:28:06.967063Z","start_time":"2020-11-30T09:28:05.95807Z"},"trusted":true},"cell_type":"code","source":"# Predict output, confusion matrix, and ROC curve\nlog_y_scores = log_model.predict(log_test_x);\nlog_y_scores = np.append(1-log_y_scores, log_y_scores, axis=1)\n\ntraining_accuracy = log_history.history[\"accuracy\"][-1]\nvalidation_accuracy = log_history.history[\"val_accuracy\"][-1]\n\nsummaries = create_summary(\"slr-log-reg\",\n                           log_test_y, \n                           log_y_scores, \n                           training_accuracy,\n                           validation_accuracy,\n                           summaries);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## k-NN <a class=\"anchor\" id=\"knn\"></a>"},{"metadata":{"ExecuteTime":{"end_time":"2020-11-30T09:28:07.006262Z","start_time":"2020-11-30T09:28:06.968604Z"},"trusted":true},"cell_type":"code","source":"standard_scaler = StandardScaler();\nknn_x_columns = [\"CreditScore\", \"Age\", \"Tenure\", \"Balance\", \"NumOfProducts\", \"HasCrCard\", \"IsActiveMember\", \"EstimatedSalary\", \"Geography_France\", \"Geography_Germany\", \"Geography_Spain\", \"Gender_Male\", \"Gender_Female\"]\nknn_y_columns = [\"Exited\"]\n\nknn_train_x = pd.DataFrame(standard_scaler.fit_transform(bdt_train[knn_x_columns]), columns=knn_x_columns)\nknn_train_y = bdt_train[knn_y_columns]\n\nknn_val_x = pd.DataFrame(standard_scaler.fit_transform(bdt_validate[knn_x_columns]), columns=knn_x_columns)\nknn_val_y = bdt_validate[knn_y_columns]\n\nknn_test_x = pd.DataFrame(standard_scaler.fit_transform(bdt_test[knn_x_columns]), columns=knn_x_columns)\nknn_test_y = bdt_test[knn_y_columns]","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-11-30T09:28:24.052055Z","start_time":"2020-11-30T09:28:07.008346Z"},"trusted":true},"cell_type":"code","source":"neighbor_column_names = [\"n\", \"validation_accuracy\", \"f1_score\"]\nneighbors = pd.DataFrame(columns = neighbor_column_names)\n\nfor n in range(1, 30):\n    knn_model = KNeighborsClassifier(n_neighbors=n)\n    knn_model.fit(knn_train_x, np.ravel(knn_train_y))\n    y_scores = knn_model.predict_proba(knn_test_x);\n    y_pred = knn_model.predict(knn_test_x);\n    y_scores=y_scores[:,1]\n    y_pred = y_scores > 0.5    \n    neighbor = pd.DataFrame([[n,\n                             knn_model.score(knn_val_x, knn_val_y),\n                             f1_score(knn_test_y, y_pred)\n              ]], \n             columns=neighbor_column_names);\n    neighbors = neighbors.append(neighbor, ignore_index=True);","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-11-30T09:28:24.340876Z","start_time":"2020-11-30T09:28:24.054112Z"},"trusted":true},"cell_type":"code","source":"reshaped_neighbors = pd.melt(neighbors, id_vars=\"n\", var_name=\"metric\", value_name=\"values\")\nplt.figure(figsize=(16,4))\nsns.set_style(\"whitegrid\")\nfig = sns.lineplot(data=reshaped_neighbors, x=\"n\", y=\"values\", hue=\"metric\");\nfig.set_title(\"kNN(n = x) comparison\");\nfig.legend(loc='best');\nfig.set_ylim([0, 1.05]);","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-11-30T09:28:24.659678Z","start_time":"2020-11-30T09:28:24.343811Z"},"trusted":true},"cell_type":"code","source":"max_k_neighbor = neighbors.at[neighbors[\"validation_accuracy\"].idxmax(), \"n\"]\nknn_model = KNeighborsClassifier(n_neighbors=max_k_neighbor)\nknn_model.fit(knn_train_x, np.ravel(knn_train_y))\nknn_y_scores = knn_model.predict_proba(knn_test_x);","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-11-30T09:28:26.487478Z","start_time":"2020-11-30T09:28:24.662673Z"},"trusted":true},"cell_type":"code","source":"title = \"kNN [n=\" + str(max_k_neighbor) + \"]\"\nprint(\"The best kNN fit was \" + title)\n\ntraining_accuracy = knn_model.score(knn_train_x, knn_train_y)\nvalidation_accuracy = knn_model.score(knn_val_x, knn_val_y)\n\nsummaries = create_summary(title, \n                           knn_test_y, \n                           knn_y_scores, \n                           training_accuracy,\n                           validation_accuracy,\n                           summaries);\n","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-11-15T06:35:03.144462Z","start_time":"2020-11-15T06:35:03.133488Z"}},"cell_type":"markdown","source":"## Decision Tree Classifier <a class=\"anchor\" id=\"decision\"></a>"},{"metadata":{"ExecuteTime":{"end_time":"2020-11-30T09:28:26.514606Z","start_time":"2020-11-30T09:28:26.489904Z"},"trusted":true},"cell_type":"code","source":"dtc_columns_x = [\"CreditScore\", \"Age\", \"Tenure\", \"Balance\", \"NumOfProducts\", \"HasCrCard\", \"IsActiveMember\", \"EstimatedSalary\", \"Geography_France\", \"Geography_Germany\", \"Geography_Spain\", \"Gender_Male\", \"Gender_Female\"]\ndtc_columns_y = \"Exited\"\n\ndtc_train_x = pd.DataFrame(minmax_scaler.fit_transform(bdt_train[dtc_columns_x]), columns=dtc_columns_x)\ndtc_train_y = bdt_train[dtc_columns_y]\n\ndtc_val_x = pd.DataFrame(minmax_scaler.fit_transform(bdt_validate[dtc_columns_x]), columns=dtc_columns_x)\ndtc_val_y = bdt_validate[dtc_columns_y]\n\ndtc_test_x = pd.DataFrame(minmax_scaler.fit_transform(bdt_test[dtc_columns_x]), columns=dtc_columns_x)\ndtc_test_y = bdt_test[dtc_columns_y]","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-11-30T09:28:39.115527Z","start_time":"2020-11-30T09:28:26.516629Z"},"trusted":true},"cell_type":"code","source":"# Tuning a classification tree from scikit\n# https://scikit-learn.org/stable/auto_examples/tree/plot_cost_complexity_pruning.html\nclf = DecisionTreeClassifier()\npath = clf.cost_complexity_pruning_path(dtc_train_x, dtc_train_y)\nccp_alphas, impurities = path.ccp_alphas, path.impurities\n\nfig, axes = plt.subplots(2, 2, figsize=(16, 16));\nfig = sns.lineplot(ax=axes[0][0], x=ccp_alphas[:-1], y=impurities[:-1], marker='o', drawstyle=\"steps-post\")\nfig.set_xlabel(\"effective alpha\")\nfig.set_ylabel(\"total impurity of leaves\")\nfig.set_title(\"Total Impurity vs effective alpha for training set\")\n\nclfs = []\nfor ccp_alpha in ccp_alphas:\n    clf = DecisionTreeClassifier(random_state=0, ccp_alpha=ccp_alpha)\n    clf.fit(dtc_train_x, dtc_train_y)\n    clfs.append(clf)\n\nprint(\"Number of nodes in the last tree is: {} with ccp_alpha: {}\".format(\n      clfs[-1].tree_.node_count, ccp_alphas[-1]))\n\nclfs = clfs[:-1]\nccp_alphas = ccp_alphas[:-1]\n\nnode_counts = [clf.tree_.node_count for clf in clfs]\ndepth = [clf.tree_.max_depth for clf in clfs]\n\nfig=sns.lineplot(ax=axes[0][1], x=ccp_alphas, y=node_counts, marker='o', drawstyle=\"steps-post\")\nfig.set_xlabel(\"alpha\")\nfig.set_ylabel(\"number of nodes\")\nfig.set_title(\"Number of nodes vs alpha\")\n\nfig=sns.lineplot(ax=axes[1][0], x=ccp_alphas, y=depth, marker='o', drawstyle=\"steps-post\")\nfig.set_xlabel(\"alpha\")\nfig.set_ylabel(\"depth of tree\")\nfig.set_title(\"Depth vs alpha\")\n\ntrain_scores = [clf.score(dtc_train_x, dtc_train_y) for clf in clfs]\nval_scores = [clf.score(dtc_val_x, dtc_val_y) for clf in clfs]\n\nfig=sns.lineplot(ax=axes[1][1], x=ccp_alphas, y=train_scores, marker='o', label=\"train\",\n        drawstyle=\"steps-post\")\nfig.plot(ccp_alphas, val_scores, marker='o', label=\"validation\",\n        drawstyle=\"steps-post\")\nfig.set_xlabel(\"alpha\")\nfig.set_ylabel(\"accuracy\")\nfig.set_title(\"Accuracy vs alpha for training and validation sets\")\nfig.legend();\n","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-11-30T09:28:39.122925Z","start_time":"2020-11-30T09:28:39.118188Z"},"trusted":true},"cell_type":"code","source":"# find the highest accuracy on validation set\nmax_idx = np.argmax(val_scores)\nmax_ccp_alpha=ccp_alphas[max_idx]\nmax_val_accuracy=val_scores[max_idx]\n#max_ccp_alpha = neighbors.at[neighbors[\"validation_accuracy\"].idxmax(), \"n\"]\ndtc_title = \"CART [α={:.4}]\".format(max_ccp_alpha)\nprint(\"The best CART ccp_alpha was \" + dtc_title)\nprint(\"Best validation accuracy: {:.4} with α={:.4}\".format(\n      max_val_accuracy, max_ccp_alpha)\n     )","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-11-30T09:28:39.161142Z","start_time":"2020-11-30T09:28:39.125968Z"},"trusted":true},"cell_type":"code","source":"# Using the tree selected above\ndtc_model = DecisionTreeClassifier(random_state=random_seed, ccp_alpha=max_ccp_alpha)\ndtc_tree = dtc_model.fit(dtc_train_x, dtc_train_y)\ndtc_y_scores = dtc_model.predict_proba(dtc_test_x);","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-11-30T09:28:40.423102Z","start_time":"2020-11-30T09:28:39.162946Z"},"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(16,16))\ntree.plot_tree(dtc_tree,\n               feature_names=dtc_columns_x,\n               class_names=[\"0\", \"1\"],\n               filled=True);\nplt.title(dtc_title);","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-11-30T09:28:41.398107Z","start_time":"2020-11-30T09:28:40.426278Z"},"trusted":true},"cell_type":"code","source":"training_accuracy = dtc_model.score(dtc_train_x, dtc_train_y)\nvalidation_accuracy = dtc_model.score(dtc_val_x, dtc_val_y)\n\nsummaries = create_summary(dtc_title, \n                           dtc_test_y, \n                           dtc_y_scores, \n                           training_accuracy,\n                           validation_accuracy,\n                           summaries);\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Neural Network (Optional) <a class=\"anchor\" id=\"nn\"></a>"},{"metadata":{"ExecuteTime":{"end_time":"2020-11-30T09:28:41.406015Z","start_time":"2020-11-30T09:28:41.400838Z"},"trusted":true},"cell_type":"code","source":"## Inputs\n## 16 hidden layers (relu)\n## Dropout (0.2)\n## 32 hidden layers (relu)\n## Dropout (0.2)\n## Output - sigmoid\n\nnn_parameters = {\n    'kernel_initializer': 'glorot_uniform',\n    'bias_initializer': 'zeros',\n    'l2_regularizer': 0.003,\n    'first_layer': 32,\n    'second_layer': 64,\n    'third_layer': 0,\n    'dropout': 0.3,\n    'activation': 'relu',\n    'learning_rate': 0.001,\n    'optimizer': 'adam',\n    'batch_size': 32,\n    'epochs': 200,\n    'verbose': 0\n}","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-11-30T09:28:41.417672Z","start_time":"2020-11-30T09:28:41.409432Z"},"trusted":true},"cell_type":"code","source":"def create_nn_model(parameters):\n    model = Sequential()\n    model.add(Dense(parameters['first_layer'],\n                input_dim=nn_train_x.shape[1],\n                kernel_regularizer=tf.keras.regularizers.l2(parameters['l2_regularizer']),  \n                activation=parameters['activation'],\n                kernel_initializer=parameters['kernel_initializer'],\n                bias_initializer=parameters['bias_initializer']))\n    model.add(Dropout(rate=parameters['dropout'], seed=random_seed))\n    model.add(Dense(parameters['second_layer'],\n                kernel_regularizer=tf.keras.regularizers.l2(parameters['l2_regularizer']), \n                activation=parameters['activation'],\n                kernel_initializer=parameters['kernel_initializer'],\n                bias_initializer=parameters['bias_initializer']))\n    model.add(Dropout(rate=parameters['dropout'], seed=random_seed))\n\n    if (parameters['third_layer'] != 0):\n        model.add(Dense(parameters['third_layer'],\n                    kernel_regularizer=tf.keras.regularizers.l2(parameters['l2_regularizer']), \n                    activation=parameters['activation'],\n                    kernel_initializer=parameters['kernel_initializer'],\n                    bias_initializer=parameters['bias_initializer']))\n        model.add(Dropout(rate=parameters['dropout'], seed=random_seed))\n\n    model.add(Dense(1, activation='sigmoid',\n                kernel_initializer=parameters['kernel_initializer'],\n                bias_initializer=parameters['bias_initializer']))\n\n    if parameters['optimizer'] == 'adam':\n        parameters['optimizer'] = keras.optimizers.Adam(learning_rate=parameters['learning_rate'])\n        \n    model.compile(loss = 'binary_crossentropy', \n                  optimizer=parameters['optimizer'], \n                  metrics=['accuracy'])\n    return model;","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-11-30T09:28:41.426344Z","start_time":"2020-11-30T09:28:41.420244Z"},"trusted":true},"cell_type":"code","source":"def tune_nn_model(kernel_initializer = nn_parameters['kernel_initializer'],\n                  bias_initializer = nn_parameters['bias_initializer'],\n                  l2_regularizer = nn_parameters['l2_regularizer'],\n                  first_layer = nn_parameters['first_layer'],\n                  second_layer = nn_parameters['second_layer'],\n                  third_layer=nn_parameters['third_layer'],\n                  dropout = nn_parameters['dropout'],\n                  activation = nn_parameters['activation'],\n                  optimizer = nn_parameters['optimizer'],\n                  learning_rate = nn_parameters['learning_rate'],\n                  batch_size = nn_parameters['batch_size'],\n                  epochs = nn_parameters['epochs'],\n                  verbose = nn_parameters['verbose']):\n    parameters = {\n        'kernel_initializer': kernel_initializer,\n        'bias_initializer': bias_initializer,\n        'l2_regularizer': l2_regularizer,\n        'first_layer': first_layer,\n        'second_layer': second_layer,\n        'third_layer': third_layer,\n        'dropout': dropout,\n        'activation': activation,\n        'optimizer': optimizer,\n        'learning_rate': learning_rate,\n        'batch_size': batch_size,\n        'epochs': epochs,\n        'verbose': verbose\n    }\n    return create_nn_model(parameters);\n    ","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-11-18T07:39:13.314947Z","start_time":"2020-11-18T07:39:13.31104Z"}},"cell_type":"markdown","source":"### FCNN <a class=\"anchor\" id=\"fcnn\"></a>"},{"metadata":{"ExecuteTime":{"end_time":"2020-11-30T09:28:41.617288Z","start_time":"2020-11-30T09:28:41.428446Z"},"trusted":true},"cell_type":"code","source":"quantile_transformer = QuantileTransformer(output_distribution='normal')\nnn_columns_x = [\"CreditScore\", \"Age\", \"Tenure\", \"Balance\", \"NumOfProducts\", \"HasCrCard\", \"IsActiveMember\", \"EstimatedSalary\", \"Geography_France\", \"Geography_Germany\", \"Geography_Spain\", \"Gender_Male\", \"Gender_Female\"]\nnn_columns_y = [\"Exited\"]\n\nnn_train_x = pd.DataFrame(quantile_transformer.fit_transform(bdt_train[nn_columns_x]), columns=nn_columns_x)\nnn_train_y = bdt_train[nn_columns_y]\n\nnn_val_x = pd.DataFrame(quantile_transformer.fit_transform(bdt_validate[nn_columns_x]), columns=nn_columns_x)\nnn_val_y = bdt_validate[nn_columns_y]\n\nnn_test_x = pd.DataFrame(quantile_transformer.fit_transform(bdt_test[nn_columns_x]), columns=nn_columns_x)\nnn_test_y = bdt_test[nn_columns_y]","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-11-30T09:29:40.24513Z","start_time":"2020-11-30T09:28:41.619773Z"},"scrolled":false,"trusted":true},"cell_type":"code","source":"## Neural network architecture\n## https://medium.com/finc-engineering/user-churn-prediction-using-neural-network-with-keras-c48f23ef4e8b\n## http://drunkendatascience.com/predicting-customer-churn-with-neural-networks-in-keras/\n## - Dropouts to decrease over fitting (if necessary_)\nnn_model = KerasClassifier(create_nn_model, \n                           parameters=nn_parameters,\n                           batch_size=nn_parameters['batch_size'],\n                           epochs=nn_parameters['epochs'], \n                           verbose=nn_parameters['verbose']);\n\nnn_history = nn_model.fit(nn_train_x, nn_train_y, \n                          validation_data=(nn_val_x, nn_val_y));\n","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-11-30T09:29:40.252361Z","start_time":"2020-11-30T09:29:40.247137Z"},"trusted":true},"cell_type":"code","source":"tune_nn = False\nif tune_nn:\n    #Best: 0.842326 using {'learning_rate': 0.001}\n    #learning_rates = [0.00001, 0.00003, 0.0001, 0.0003, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 3]\n\n    #Best: 0.845853 using {'batch_size': 128}\n    #batch_sizes = [32, 64, 128, 256, 512, 1024, 2048, 4096]\n    batch_sizes = [32, 128]\n\n    # Best fit: adam\n    #optimizers=['sgd', 'rmsprop', 'adam']\n\n    #Best: 0.853359 using {'l2_regularizer': 0.003}\n    #l2_regularizers=[0, 0.0001, 0.0003, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 3]\n\n    #Best: 0.846593 using {'first_layer': 32, 'second_layer': 64, 'third_layer': 0}\n    #first_layers = [16, 32, 64]\n    #second_layers = [32, 64, 128]\n    #third_layers = [0, 32, 64, 128, 256]\n\n    #Best: 0.859833 using {'dropout': 0.3}\n    #dropouts = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5]\n\n    nn_param_grid = dict(\n        #learning_rate=learning_rates,\n        #l2_regularizer=l2_regularizers,\n        #optimizer=optimizers,\n        batch_size=batch_sizes,\n        #first_layer=first_layers,\n        #second_layer=second_layers,\n        #third_layer=third_layers,\n        #dropout=dropouts\n    )\n    \n    nn_model_tune = KerasClassifier(tune_nn_model,\n                                    batch_size=nn_parameters['batch_size'],\n                                    epochs=nn_parameters['epochs'], \n                                    verbose=nn_parameters['verbose']);\n\n    nn_grid = GridSearchCV(estimator=nn_model_tune, \n                           param_grid=nn_param_grid,\n                           scoring='roc_auc',\n                           n_jobs=2)\n    \n    nn_grid_result = nn_grid.fit(X=nn_train_x, y=nn_train_y,\n                                 validation_data=[nn_val_x, nn_val_y])\n    summarize_grid_results(nn_grid_result)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Note: This model takes ~60s to run**"},{"metadata":{"ExecuteTime":{"end_time":"2020-11-30T09:29:40.515444Z","start_time":"2020-11-30T09:29:40.255801Z"},"trusted":true},"cell_type":"code","source":"fig = sns.lineplot(data=nn_history.history['loss']);\nfig.set(ylabel=\"loss\", xlabel = \"epoch\");","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-11-30T09:29:41.989656Z","start_time":"2020-11-30T09:29:40.518453Z"},"trusted":true},"cell_type":"code","source":"nn_y_scores = nn_model.predict_proba(nn_test_x);\n\ntraining_accuracy = nn_model.score(nn_train_x, nn_train_y)\nvalidation_accuracy = nn_model.score(nn_val_x, nn_val_y)\n\nsummaries = create_summary(\"neural-network\", \n                           nn_test_y, \n                           nn_y_scores, \n                           training_accuracy,\n                           validation_accuracy,\n                           summaries);","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-11-18T07:40:21.353598Z","start_time":"2020-11-18T07:40:21.348622Z"}},"cell_type":"markdown","source":"### FCNN w/ SMOTE <a class=\"anchor\" id=\"smote\"></a>\n\nThe NN seems to good a good job of classifying class 1, but we still do not see much gain in class 0. This may be because 80% of the data is class 0, so the default position of the algorithm and dataset is just to predict 0. I am going to use the Synthetic Minority Oversampling Technique (SMOTE) to augment the minority class and see if we realize improvements."},{"metadata":{"ExecuteTime":{"end_time":"2020-11-30T09:29:41.996762Z","start_time":"2020-11-30T09:29:41.99261Z"},"trusted":true},"cell_type":"code","source":"# Create the SMOTE dataset\nsmote_train_x = nn_train_x;\nsmote_train_y = nn_train_y;\n\nsmote_val_x = nn_val_x;\nsmote_val_y = nn_val_y;\n\nsmote_test_x = nn_test_x;\nsmote_test_y = nn_test_y;","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-11-30T09:29:42.049584Z","start_time":"2020-11-30T09:29:41.999637Z"},"trusted":true},"cell_type":"code","source":"print(\"Before SMOTE, count (y = 1): \", np.sum(smote_train_y==1))\nprint(\"Before SMOTE, count (y = 0): \", np.sum(smote_train_y==0))\n\nsm = SMOTE(random_state=2, k_neighbors=max_k_neighbor)\nsmote_train_x, smote_train_y = sm.fit_sample(smote_train_x, smote_train_y)\n\nprint(\"After SMOTE, count (y = 1): \", np.sum(smote_train_y==1))\nprint(\"After SMOTE, count (y = 0): \", np.sum(smote_train_y==0))","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-11-30T09:33:02.323892Z","start_time":"2020-11-30T09:29:42.051847Z"},"scrolled":false,"trusted":true},"cell_type":"code","source":"# Increase epochs\nsmote_parameters = nn_parameters;\nsmote_parameters['epochs'] = 500;\n\n# We use the same NN above and train it w/ the smote dataset.\nsmote_model = KerasClassifier(create_nn_model, \n                              parameters=smote_parameters,\n                              batch_size=smote_parameters['batch_size'],\n                              epochs=smote_parameters['epochs'], \n                              verbose=smote_parameters['verbose']);\n\nsmote_history = smote_model.fit(smote_train_x, smote_train_y, \n                             validation_data=(smote_val_x, smote_val_y));\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**The above model takes ~5m to train**"},{"metadata":{"ExecuteTime":{"end_time":"2020-11-30T09:33:02.591466Z","start_time":"2020-11-30T09:33:02.325689Z"},"trusted":true},"cell_type":"code","source":"# Plot the loss function\nfig = sns.lineplot(data=smote_history.history['loss']);\nfig.set(xlabel=\"loss\", ylabel = \"epoch\");","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-11-30T09:33:04.04244Z","start_time":"2020-11-30T09:33:02.594427Z"},"trusted":true},"cell_type":"code","source":"# Print the summary information\nsmote_y_scores = smote_model.predict_proba(smote_test_x);\n\ntraining_accuracy = nn_model.score(smote_train_x, smote_train_y)\nvalidation_accuracy = nn_model.score(smote_val_x, smote_val_y)\n\nsummaries = create_summary(\"nn-smote\", \n                           smote_test_y, \n                           smote_y_scores, \n                           training_accuracy,\n                           validation_accuracy,\n                           summaries);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Ensemble <a class=\"anchor\" id=\"ensemble\"></a>"},{"metadata":{"ExecuteTime":{"end_time":"2020-11-30T09:33:04.055933Z","start_time":"2020-11-30T09:33:04.044072Z"},"trusted":true},"cell_type":"code","source":"ensemble_columns_x = [\"CreditScore\", \"Age\", \"Tenure\", \"Balance\", \"NumOfProducts\", \"HasCrCard\", \"IsActiveMember\", \"EstimatedSalary\", \"Geography_France\", \"Geography_Germany\", \"Geography_Spain\", \"Gender_Male\", \"Gender_Female\"]\nensemble_columns_y = [\"Exited\"]\n\nensemble_train_x = pd.DataFrame(bdt_train[ensemble_columns_x], columns=ensemble_columns_x)\nensemble_train_y = bdt_train[ensemble_columns_y]\n\nensemble_val_x = pd.DataFrame(bdt_validate[ensemble_columns_x], columns=ensemble_columns_x)\nensemble_val_y = bdt_validate[ensemble_columns_y]\n\nensemble_test_x = pd.DataFrame(bdt_test[ensemble_columns_x], columns=ensemble_columns_x)\nensemble_test_y = bdt_test[ensemble_columns_y]","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-11-28T02:27:34.642226Z","start_time":"2020-11-28T02:27:34.638292Z"}},"cell_type":"markdown","source":"### Soft Vote <a class=\"anchor\" id=\"soft-vote\"></a>\n\nThe soft voting consensus method takes the average of the predictions."},{"metadata":{"ExecuteTime":{"end_time":"2020-11-30T09:33:04.065297Z","start_time":"2020-11-30T09:33:04.059028Z"},"trusted":true},"cell_type":"code","source":"all_y_scores = []\nmethods = ['slr-log-reg', 'kNN', 'CART', 'neural-network']\n\nfor m in methods:\n    s = summaries.loc[summaries['method'].str.startswith(m), 'y_scores'].item()\n    all_y_scores.append(s)    ","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-11-30T09:33:05.169802Z","start_time":"2020-11-30T09:33:04.067946Z"},"trusted":true},"cell_type":"code","source":"soft_vote_y_scores = np.mean(all_y_scores, axis=0)\nsoft_vote_y_scores = soft_vote_y_scores[..., None]\nsoft_vote_y_scores = np.append(1-soft_vote_y_scores, soft_vote_y_scores, axis=1)\n\ntraining_accuracy = 0.0\nvalidation_accuracy = 0.0\n\nsummaries = create_summary(\"soft_vote\", \n                           ensemble_test_y,\n                           soft_vote_y_scores, \n                           training_accuracy,\n                           validation_accuracy,\n                           summaries);","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-11-28T02:27:34.642226Z","start_time":"2020-11-28T02:27:34.638292Z"}},"cell_type":"markdown","source":"### Stacking Classifier <a class=\"anchor\" id=\"stacking\"></a>\n\nThe stacking classification develops a logistic regression method based on the outputs from other classifiers."},{"metadata":{"ExecuteTime":{"end_time":"2020-11-30T09:33:06.571632Z","start_time":"2020-11-30T09:33:05.172007Z"},"trusted":true},"cell_type":"code","source":"# the x values are the prior probabilities\nstacking_y_column = \"Exited\"\n\nstacking_train_x = np.column_stack((log_model.predict(log_train_x),\n                                   knn_model.predict_proba(knn_train_x)[:,1],\n                                   dtc_model.predict_proba(dtc_train_x)[:,1],\n                                   nn_model.predict_proba(nn_train_x)[:,1]))\nstacking_train_y = bdt_train[stacking_y_column]\n\nstacking_val_x = np.column_stack((log_model.predict(log_val_x),\n                                   knn_model.predict_proba(knn_val_x)[:,1],\n                                   dtc_model.predict_proba(dtc_val_x)[:,1],\n                                   nn_model.predict_proba(nn_val_x)[:,1]))\nstacking_val_y = bdt_validate[stacking_y_column]\n\nstacking_test_x = np.column_stack((log_model.predict(log_test_x),\n                                   knn_model.predict_proba(knn_test_x)[:,1],\n                                   dtc_model.predict_proba(dtc_test_x)[:,1],\n                                   nn_model.predict_proba(nn_test_x)[:,1]))\nstacking_test_y = bdt_test[stacking_y_column]","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-11-30T09:33:06.586734Z","start_time":"2020-11-30T09:33:06.573777Z"},"trusted":true},"cell_type":"code","source":"stacking_model = LogisticRegression(solver='liblinear', C=10.0, random_state=0)\nstacking_model.fit(stacking_train_x, stacking_train_y)\nstacking_y_scores = stacking_model.predict_proba(stacking_test_x);","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-11-30T09:33:08.269703Z","start_time":"2020-11-30T09:33:06.59852Z"},"trusted":true},"cell_type":"code","source":"training_accuracy = stacking_model.score(stacking_train_x, stacking_train_y)\nvalidation_accuracy = stacking_model.score(stacking_val_x, stacking_val_y)\n\nsummaries = create_summary(\"stacking\",\n                           stacking_test_y, \n                           stacking_y_scores, \n                           training_accuracy,\n                           validation_accuracy,\n                           summaries);","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-11-28T02:27:34.642226Z","start_time":"2020-11-28T02:27:34.638292Z"}},"cell_type":"markdown","source":"### AdaBoost <a class=\"anchor\" id=\"adaboost\"></a>\n\nBoosting algorithm using decision trees."},{"metadata":{"ExecuteTime":{"end_time":"2020-11-30T09:34:47.706747Z","start_time":"2020-11-30T09:33:08.274472Z"},"trusted":true},"cell_type":"code","source":"# define the model with default hyperparameters\nada_model = AdaBoostClassifier()\n# define the grid of values to search\nada_parameters = dict()\nada_parameters['n_estimators'] = [100, 500, 1000]\nada_parameters['learning_rate'] = [0.01, 0.03, 0.1, 0.3, 1, 3]\n\n# define the grid search procedure\nada_grid = GridSearchCV(estimator=ada_model, param_grid=ada_parameters,\n                        scoring='roc_auc', n_jobs=2)\n    \nada_grid_result = ada_grid.fit(X=dtc_train_x, y=dtc_train_y)\nsummarize_grid_results(ada_grid_result)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-11-30T09:34:51.997535Z","start_time":"2020-11-30T09:34:47.710769Z"},"trusted":true},"cell_type":"code","source":"# Using the tree selected above\nada_best_model = AdaBoostClassifier(learning_rate=ada_grid_result.best_params_['learning_rate'],\n                                    n_estimators=ada_grid_result.best_params_['n_estimators'])\nada_best_model.fit(X=dtc_train_x, y=dtc_train_y)\nada_y_scores = ada_best_model.predict_proba(dtc_test_x);","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-11-30T09:34:53.894052Z","start_time":"2020-11-30T09:34:51.999513Z"},"trusted":true},"cell_type":"code","source":"training_accuracy = ada_best_model.score(dtc_train_x, dtc_train_y)\nvalidation_accuracy = ada_best_model.score(dtc_val_x, dtc_val_y)\n\nsummaries = create_summary(\"ada-boost\", \n                           dtc_test_y, \n                           ada_y_scores, \n                           training_accuracy,\n                           validation_accuracy,\n                           summaries);\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Selection <a class=\"anchor\" id=\"selection\"></a>"},{"metadata":{"ExecuteTime":{"end_time":"2020-11-30T09:34:54.386435Z","start_time":"2020-11-30T09:34:53.896024Z"},"scrolled":true,"trusted":true},"cell_type":"code","source":"metrics = summaries[[\"method\", \"training_accuracy\", \"validation_accuracy\", \"test_accuracy\", \"f1_score\", \"auc\"]]\nreshaped_barplot = pd.melt(metrics, id_vars=\"method\", var_name=\"metric\", value_name=\"values\")\n\nplt.figure(figsize=(16,4))\nsns.set_style(\"whitegrid\")\nfig = sns.barplot(data=reshaped_barplot, x=\"metric\", y=\"values\", hue=\"method\");\nfig.set_title(\"Method comparison\");\nfig.legend(loc='center left', bbox_to_anchor=(1, 0.5))\nfig.set_ylim([0, 1.05]);","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-11-30T09:35:30.718752Z","start_time":"2020-11-30T09:34:54.389205Z"},"scrolled":false,"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(1, 1, figsize=(16, 8));\n \nfor index, row in summaries.iterrows():\n    sns.lineplot(ax=fig.axes[0], x=row[\"fpr\"], y=row[\"tpr\"], label = row[\"method\"] + \": \" + str(round(row[\"auc\"], 3)))\n\nplt.plot([0,1], [0,1], 'r--', label = 'Random: 0.5')\nplt.plot([0,0,1], [0,1,1], 'g-', label = 'Optimal: 1.0')\n\nplt.xlabel('False positive rate')\nplt.ylabel('True positive rate')\nplt.title('ROC Curve')\nplt.legend(loc='best')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-11-30T09:35:34.927938Z","start_time":"2020-11-30T09:35:30.721886Z"},"trusted":true},"cell_type":"code","source":"cnf_grid_cols = 3;\ncnf_n_items = summaries.shape[0]\ncnf_grid_rows = np.ceil(cnf_n_items/cnf_grid_cols).astype('int')\n\nfig, axes = plt.subplots(cnf_grid_rows, cnf_grid_cols, figsize=(16, 4*cnf_grid_rows));\n\nfor index, row in summaries.iterrows():\n    r = np.trunc(index/3).astype('int')\n    c = index%cnf_grid_cols;\n    sub_fig = sns.heatmap(ax=axes[r,c], data=row[\"cnf_matrix\"], annot=True, fmt=\"d\", linewidths=.5, cmap=\"Blues\", vmin=0, vmax=1600);\n    title = row[\"method\"]\n    sub_fig.set_title(title);\n    sub_fig.set_ylabel(\"Actual\");\n    sub_fig.set_xlabel(\"Predicted\");\n\n# delete the unused columns\ncols_to_delete = (cnf_grid_cols-cnf_n_items%cnf_grid_cols)%cnf_grid_cols\nfor i in range(cols_to_delete):\n    fig.delaxes(axes[r][cnf_grid_cols-1-i])\n    \nfig.suptitle(\"Confusion Matrix Summary (Exited=1)\");\nfig.subplots_adjust(hspace=0.4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us calculate the value of the above charts:\n\nWe want to penalize a lazy learner (since ~80% of the customers do not churn). A lost customer costs us \\\\$100 in revenue. A kept customer has no net change in expected revenue. We spend $5 to reach out to customers who might churn. If contacted, 20% of the customers who would churn will not. (e.g. if 10 people will churn, and we contact all 10 of them, only 8 will leave). Based on this analysis, here is how much money we keep by using each of the algorithms.\n\nThe benefit of this equation is that correctly detecting customers who will churn (TP) is rewarded and missing customers who will churn is heavily penalized (FN). Misclassifying customers who will stay are penalized based on the cost of reaching out to customers.\n\nIn real life, a manager could then choose to optimize further by lowering the cost to reach out, or else improving the retention rate."},{"metadata":{"ExecuteTime":{"end_time":"2020-11-30T09:35:35.198932Z","start_time":"2020-11-30T09:35:34.931268Z"},"trusted":true},"cell_type":"code","source":"revenue_gain_customer = 0\nrevenue_lost_customer = 100\nretention_rate = 0.20\ncost_per_contact = 5\n\nrevenues = np.empty(summaries.shape[0])\n\nfor index, row in summaries.iterrows():\n    cfm = row[\"cnf_matrix\"]\n    tp = cfm[0][0]  # Customer we keep\n    fp = cfm[0][1]  # Customer will keep but reach out to anyways\n    fn = cfm[1][0]  # Customer we will lose but we do not predict it so we do not reach out\n    tn = cfm[1][1]  # Customers we predict will leave and we reach out\n    \n    customer_revenue = tp*revenue_gain_customer - fp*cost_per_contact - fn*revenue_lost_customer - tn*cost_per_contact - tn*(1-retention_rate)*revenue_lost_customer + tn*retention_rate*revenue_gain_customer\n    revenues[index] = customer_revenue;\n    \nsummaries[\"revenue\"] = revenues;\n\nplt.figure(figsize=(16,4));\nchart = sns.lineplot(data=summaries, x=\"method\", y=\"revenue\", marker=\"o\")\nplt.xticks(\n    rotation=30, \n    horizontalalignment='right',\n)\n\nmax_index = summaries[\"revenue\"].idxmax()\nmax_method = summaries.loc[max_index, \"method\"]\nmax_revenue = summaries.loc[max_index, \"revenue\"]\nprint(max_method + \" has the highest revenue at ${:,.0f}\".format(max_revenue))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}