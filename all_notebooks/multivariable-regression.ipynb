{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np # linear algebra","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"adver = pd.read_csv(\"../input/advertising-dataset/advertising.csv\")\n#admissions = admissions.drop('Serial No.',axis = 1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"adver.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"adver.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = adver.drop('Sales',axis = 1)\ny = adver['Sales']\n\n#X_train,X_val,y_train,y_val = train_test_split(X,y,test_size = .25,random_state = 99)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.shape, X.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y.shape, y.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Convert to arrays"},{"metadata":{"trusted":true},"cell_type":"code","source":"features=np.array(X)\n\ntargets=np.array(y).reshape(-1,1)\nfeatures [:4,], len(features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"targets[:4], len(targets)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# normalize feature\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"def normalize(features):\n    \"\"\"\n    features     -   (200, 3)\n    features.T   -   (3, 200)\n\n    We transpose the input matrix, swapping\n    cols and rows to make vector math easier\n    \"\"\"\n    \n\n    for feature in features.T:\n        fmean = np.mean(feature)\n        frange = np.amax(feature) - np.amin(feature)\n\n        #Vector Subtraction\n        feature -= fmean\n\n        #Vector Division\n        feature /= frange\n\n    return features","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# make predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict(features, weights):\n  \"\"\"\n  features - (200, 3)\n  weights - (3, 1)\n  predictions - (200,1)\n  \"\"\"\n  predictions = np.dot(features, weights)\n  return predictions","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# compute the cost\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"def cost_function(features, targets, weights):\n    \"\"\"**\n    features:(200,3)\n    targets: (200,1)\n    weights:(3,1)\n    returns average squared error among predictions\n    **\"\"\"\n    N = len(targets)\n\n    predictions = predict(features, weights)\n\n    # Matrix math lets use do this without looping\n    sq_error = (predictions - targets)**2\n    #print(sq_error.sum())\n    # Return average squared error among predictions\n    \n\n    return 1.0/(2*N) * sq_error.sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# gradient descent"},{"metadata":{"trusted":true},"cell_type":"code","source":"def update_weights(features, targets, weights, learning_rate):\n    '''\n    Features:(200, 3)\n    Targets: (200, 1)\n    Weights:(3, 1)\n    '''\n    predictions = predict(features, weights)\n\n    #Extract our features\n    x1 = features[:,0]\n    x2 = features[:,1]\n    x3 = features[:,2]\n\n    # Use matrix cross product (*) to simultaneously\n    # calculate the derivative for each weight\n    d_w1 = -x1*(targets - predictions)\n    d_w2 = -x2*(targets - predictions)\n    d_w3 = -x3*(targets - predictions)\n\n    # Multiply the mean derivative by the learning rate\n    # and subtract from our weights (remember gradient points in direction of steepest ASCENT)\n    weights[0][0] -= (learning_rate * np.mean(d_w1))\n    weights[1][0] -= (learning_rate * np.mean(d_w2))\n    weights[2][0] -= (learning_rate * np.mean(d_w3))\n\n    return weights","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Vectorization"},{"metadata":{"trusted":true},"cell_type":"code","source":"def update_weights_vectorized(features, targets, weights, learning_rate):\n    \"\"\"\n    gradient = X.T * (predictions - targets) / N\n    X: (200, 3)\n    Targets: (200, 1)\n    Weights: (3, 1)\n    \"\"\"\n    companies = len(features)\n\n    #1 - Get Predictions\n    predictions = predict(features, weights)\n\n    #2 - Calculate error/loss\n    error = targets - predictions\n\n    #3 Transpose features from (200, 3) to (3, 200)\n    # So we can multiply w the (200,1)  error matrix.\n    # Returns a (3,1) matrix holding 3 partial derivatives --\n    # one for each feature -- representing the aggregate\n    # slope of the cost function across all observations\n    gradient = np.dot(-features.T,  error)\n\n    #4 Take the average error derivative for each feature\n    gradient /= companies\n\n    #5 - Multiply the gradient by our learning rate\n    gradient *= learning_rate\n\n    #6 - Subtract from our weights to minimize cost\n    weights -= gradient\n\n    return weights\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# train your model"},{"metadata":{"trusted":true},"cell_type":"code","source":"def train(features, targets, weight,  learning_rate, iters):\n    cost_history = []\n   \n    for i in range(iters):\n        weight = update_weights_vectorized(features, targets, weight, learning_rate)\n    \n    #Calculate cost for auditing purposes\n        cost = cost_function(features, targets, weight)\n        cost_history.append(cost)\n      \n        # Log Progress\n        #if i ==(iters-1) :\n       \n        if i% 10 == 0:\n            #print(\"iter={:d}  weight={:.2f}    cost={:.2}\".format(i, weight, bias, cost))\n            print(\"iter=\" + str(i) + \" weight: \" + str(weight)  + \" cost \" +str(cost))#\" {:d}    weight={:.2f}    bias={:.4f}    cost={:.2}\".format(i, weight, bias, cost))\n \n    \n    plt.figure()\n\n    plt.plot(np.arange(iters), cost_history,'r')  \n\n    plt.xlabel(\"Training iterations\")\n    plt.ylabel(\"Mean Squared Error\")\n    plt.title(\"Error Rate\")\n\n    plt.show()\n\n    return  weight, cost_history","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Start implementation of MVLR"},{"metadata":{"trusted":true},"cell_type":"code","source":"W1 = 0.0\nW2 = 0.0\nW3 = 0.0\nweights = np.array([\n    [W1],\n    [W2],\n    [W3]\n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"normalized_features=normalize(features)\nnormalized_features[:4,]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = predict(normalized_features, weights)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cost_function(normalized_features,targets,weights)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"update_weights(normalized_features,targets,weights,0.0005)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"update_weights_vectorized(normalized_features, targets, weights, 0.0005)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bias = np.ones(shape=(len(features),1))\nbfeatures = np.append(bias, normalized_features, axis=1)\n\nW4 = 0.0\nweights_bias = np.array([\n     [W1],\n     [W2],\n     [W3],\n     [W4]\n ])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"weights_bias,cost_history=train(bfeatures,targets,weights_bias,0.005,iters=10000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"weights_bias,cost_history=train(bfeatures,targets,weights_bias,0.05,iters=10000)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}