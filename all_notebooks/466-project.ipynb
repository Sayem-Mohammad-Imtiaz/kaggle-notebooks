{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.feature_extraction.text import CountVectorizer,HashingVectorizer\nimport time\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix\nimport os\nimport plotly.express as px\nimport plotly\nimport seaborn as sns\nimport csv\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n'''\nI used a slightly modified version of NaiveBayes classifier from AS1 here\n\n'''\n\nclass MyBayesClassifier():\n    def __init__(self, smooth=1):\n        self._smooth = smooth # This is for additive smoothing\n        \n\n    def train(self, X, y):\n        alpha_smooth = self._smooth\n        cls = np.unique(y)\n        Ncls, Nfeat = len(cls), X.shape[1] #Ncls: number of classes, Nfeat: number of features.\n\n        self._cls = cls\n        self._prior = np.zeros((1,Ncls))\n        #initialize a matrix stand for all p(x|y),the likelyhood for every attribute\n        self._likehood = np.zeros((Ncls,Nfeat))\n        #for each class,find rows that satisfies the condition,and compute the likehood of each feature\n        for i in range(Ncls):\n            cla = cls[i]\n            x_cla = X[y==cla]                                              #the rows that belong to current class\n            self._prior[0,i] =(x_cla.shape[0]+alpha_smooth)/(X.shape[0]+alpha_smooth*Ncls)                    #compute prior probability of current class\n            #self._likehood[i] = np.sum(x_cla,axis=0)/x_cla.shape[0]       #verticlly summation along each column to get frequency of each feature and then divide by # of rows\n\n            self._likehood[i,:] = (np.sum(x_cla, axis=0)+alpha_smooth) / (x_cla.shape[0]+ alpha_smooth * 2)                  #apply smooth to frequency divide # of rows\n            #print(self._likehood)\n            #print(self._prior)\n\n    def train_JM_smooth(self, X, y,X_dataset):\n        alpha_smooth = self._smooth\n        cls = np.unique(y)\n        Ncls, Nfeat = len(cls), X.shape[1] #Ncls: number of classes, Nfeat: number of features.\n\n        size_dataset = X_dataset.shape[0]             #number of data in dataset\n        feature_dataset = np.sum(X_dataset,axis=0)    #count the appearnce of each word in dataset by verticlly sum along each column from the dataset\n\n\n        self._cls = cls\n        self._prior = np.zeros((1,Ncls))\n        #initialize a matrix stand for all p(x|y),the likelyhood for every attribute\n        self._likehood = np.zeros((Ncls,Nfeat))\n        #for each class,find rows that satisfies the condition,and compute the likehood of each feature\n        for i in range(Ncls):\n            cla = cls[i]\n            x_cla = X[y==cla]                                              #the rows that belong to current class\n            self._prior[0,i] =x_cla.shape[0]/X.shape[0]                    #compute prior probability of current class\n            #self._likehood[i] = np.sum(x_cla,axis=0)/x_cla.shape[0]       #verticlly summation along each column to get frequency of each feature and then divide by # of rows\n            numerator1 = np.sum(x_cla, axis=0)\n            denominator1 = np.sum(x_cla)\n\n            self._likehood[i,:] = (1- alpha_smooth)*numerator1/denominator1 + alpha_smooth * feature_dataset / size_dataset                  #apply JM smooth to frequency divide # of rows\n\n        #after the for loop,self._prior stores the prior probability of all catergories and\n        #selef._likehood stores the probabality of all feature  such that  P(feature_i | catergory)\n            ###confusion  : smoothing????????\n        #self._notlikehood = 1-self._likehood              #P(not feature i | catergory )\n    def predict(self, X):\n\n        Ncls,Ntest,Nfeat = len(self._cls),X.shape[0],X.shape[1]                         #number of test sample\n        pred = np.zeros(Ntest)\n        loglikehood = np.log(self._likehood)       \n        X_not = 1-X                   #since in original data,1 for appearence of feature i and 0 for not appear,the logic not means 1 for not appear and 0 for appear\n\n        #X is size Ntest x Nfeat,selef._likehood.T is the shape Nfeat x Ncls\n        log_appear = X.dot(np.log(self._likehood.T))\n        \n        #print(\"appear\",log_appear)\n        \n        log_absence = X_not.dot (np.log(1-self._likehood.T))\n        #print(\"absence\",log_absence)\n\n        log_post = log_appear+log_absence\n        #log_post = X.dot(np.log(self._likehood.T)) + X_not.dot (np.log(1-self._likehood.T))    #consider both appearence and absence\n        #log_post = X.dot(np.log(self._likehood.T))                                             #only consider appearence\n\n        log_post = log_post + np.log(self._prior.reshape(1,Ncls))\n        #print(log_post)\n        pred = self._cls[np.argmax(log_post,axis=1)]\n\n        return pred\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/mbti-type/mbti_1.csv',header=0)\ndf.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.gcf()\nfig.set_size_inches(50, 20)\nsns.catplot(x=\"type\", kind=\"count\", data=df,height=8.27, aspect=11.7/8.27)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#\n# Just striping the string incase of any whitespace before or after the string\ndf[\"type\"] = df[\"type\"].str.strip()\n# Seperate the label into four different parts\ntarget_multi_label = df[\"type\"].str.split(\"\" , expand=True)\ntarget_multi_label = target_multi_label.iloc[: , 1:-1]\ntarget_multi_label.columns = [\"Personality-1\",\"Personality-2\",\"Personality-3\",\"Personality-4\"]\n\ndf = pd.concat([df,target_multi_label] , axis=1)\n'''\npersonality_map = {\n    \"I\":\"Introvert\",\n    \"E\":\"Extrovert\",\n    \"N\":\"Intuitive\",\n    \"S\":\"Sensitive\",\n    \"F\":\"Emotional\",\n    \"T\":\"Thinker\",\n    \"J\":\"Judgemental\",\n    \"P\":\"Perceiving\"\n}\nfor col in df.loc[: , \"Personality-1\":\"Personality-4\"].columns:\n    df[col] = df[col].map(personality_map)\n'''\ndf.head()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.gcf()\nfig.set_size_inches(50, 20)\nsns.catplot(x=\"Personality-1\", kind=\"count\", data=df,height=5, aspect=4/5)\nsns.catplot(x=\"Personality-2\", kind=\"count\", data=df,height=5, aspect=4/5)\nsns.catplot(x=\"Personality-3\", kind=\"count\", data=df,height=5, aspect=4/5)\nsns.catplot(x=\"Personality-4\", kind=\"count\", data=df,height=5, aspect=4/5)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#version1 of text pre-processing\n\n#source:https://towardsdatascience.com/nlp-text-preprocessing-a-practical-guide-and-template-d80874676e79\n!pip install Unidecode\n!pip install contractions\n!pip install BeautifulSoup4\nimport nltk\nnltk.download('wordnet')\n\nfrom bs4 import BeautifulSoup\nimport spacy\nimport unidecode \n#from word2number import w2n\nimport contractions\nfrom nltk.stem import WordNetLemmatizer \nimport re\n\ndef preprocessing_v1(text):\n    #remove html information\n    soup = BeautifulSoup(text, \"html.parser\")\n    processed = soup.get_text(separator=\" \")\n    \n    #remove http// \n    processed = re.sub(r\"http\\S+\", \"\", processed)\n\n    #remove ||| seperate\n    processed = re.sub(r'\\|\\|\\|', r' ', processed)\n\n    #lower case\n    processed = processed.lower()\n\n    #expand shortened words, e.g. don't to do not\n    processed = contractions.fix(processed)\n\n    #remove accented char\n    processed = unidecode.unidecode(processed)\n\n    #remove white space\n    #processed = processed.strip()\n    #processed = \" \".join(processed.split())\n\n    # Lemmatizing \n    lemmatizer = WordNetLemmatizer() \n    processed=lemmatizer.lemmatize(processed)\n\n\n    return processed","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['posts'] = df['posts'].apply(preprocessing_v1)\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#split traning and test data and vectorize them\n\n\n\nnumber_training = 6000\ndata_size = df['type'].shape[0]\n\n\n\nall_data = df['posts'].astype('U').values\ndata_train = df['posts'][:number_training].astype('U').values\ndata_test = df['posts'][number_training:].astype('U').values\n\ny_train = df['type'][:number_training].astype('U').values\ny_test = df['type'][number_training:].astype('U').values\n\n\n#Note here, increase max_features may result in increasing ram usage and cause crush of colab\n#By defaut,it will geneate over 140000 features without any text preprocessing,it would decrease to near 100000 but still not acceptable\n#therefore I added a upper bound for max_features\nvectorizer = CountVectorizer(\n        lowercase=True, stop_words='english',\n        max_df=1.0, min_df=1, max_features=2000,  binary=True\n      )\nprocessed_data = vectorizer.fit_transform(all_data).toarray()\n\nX_train = processed_data[0:number_training, :]\nX_test = processed_data[number_training:, :]\n\nprint(\"X_train.shape = {}\".format(X_train.shape))\nprint(\"X_test.shape = {}\".format(X_test.shape))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"####   tempoary test cell , only used to debug some non-sense\nprint(X_train)\nprint(sum(X_train[0]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#perform naive bayes,predict 1 among 16 personality types at once\nclf = MyBayesClassifier(1.0)\nclf.train(X_train, y_train);\ny_pred = clf.predict(X_test)\nprint(X_test)\nprint(y_pred)\nprint(\"Absolute accuracy = {}\".format(np.mean(y_test==y_pred)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#for each sub-personality type,train the model and make prediction\n#first test I/E,then N/S.......Cancadinate the result together to form final result\ny_pred = a2 = np.array(['' for i in range(data_size-number_training)])\nclf = MyBayesClassifier(1.0)\n\nfor col in df.loc[: , \"Personality-1\":\"Personality-4\"].columns:\n    y_train_sub = df[col][:6000].astype('U').values\n    clf.train(X_train, y_train_sub);\n    y_pred_sub = clf.predict(X_test)\n    y_test_sub = df[col][6000:].astype('U').values\n    print(col, \"accuracy = {}\".format(np.mean(y_test_sub==y_pred_sub)))\n\n\n    y_pred=np.core.defchararray.add(y_pred, y_pred_sub)\n\nprint(y_pred)\nprint(y_test)\nprint(\"Absolute accuracy = {}\".format(np.mean(y_test==y_pred)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#test the special case when max_feature is None\n#print(k_Fold_CV(10, None, 1))\n\n#Conclusion:\n#the project will forced to restart due to excessive memory requirement","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nThis will load the csv\n'''\nclass CsvToDf:\n    '''\n    This class will simply turn the given data to a dataframe\n    '''\n    def __init__(self,filename,batchSize=None,cols=None):\n        #batchSize is the size of data to be read incrementally. This is for data that is to big to fit\n        #into memory\n        self._cols = cols\n        self._header = None\n        self._filename = filename\n        self._curIndex = 0     #this will be the current index that we are in the csv\n        self._isRead = False\n        self._df = None\n        self._storeHeader()\n        self._batchSize = batchSize\n    def _storeHeader(self):\n        with open(self._filename) as csvFile:\n            f = csv.reader(csvFile)\n            self._header = next(f)\n    def getWholeCsv(self):\n        if not(self._isRead):\n            if self._cols != None:\n                self._df = pd.read_csv(self._filename,usecols=self._cols)\n            else:\n                self._df = pd.read_csv(self._filename)\n            self._isRead = True\n        return self._df\n    def getHeader(self):\n        return self._header\n    def _checkIfRead(self):\n        if not(self._isRead):\n            if self._cols != None:\n                self._df = pd.read_csv(self._filename,iterator=True,chunksize=self._batchSize,usecols=self._cols)\n            else:\n                self._df = pd.read_csv(self._filename,iterator=True,chunksize=self._batchSize)\n            self._isRead = True\n            return False\n        return True\n    def getNextBatchCsv(self):\n        self._checkIfRead()\n        return next(self._df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\ntest for small Dset\n\ntest = CsvToDf(\"../input/mbti-type/mbti_1.csv\",batchSize=200)\nprint(test.getNextBatchCsv())\nres = test.getWholeCsv()\nprint(type(res))\nprint(test.getWholeCsv())\nprint(test.getHeader())\n'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\ntest big dset\n\ntest2 = CsvToDf(\"../input/mbti-full-pull-samplecsv/mbti_full_pull_sample.csv\",batchSize=100,cols=['title','type'])\n#test2.eliminateCols(['created_utc', 'subreddit', 'author', 'domain', 'url', 'num_comments', 'score', 'ups', 'downs', 'selftext', 'saved', 'id', 'from_kind', 'gilded', 'from', 'stickied', 'retrieved_on', 'over_18', 'thumbnail', 'subreddit_id', 'hide_score', 'link_flair_css_class', 'author_flair_css_class', 'archived', 'is_self', 'from_id', 'permalink', 'name', 'author_flair_text', 'quarantine', 'link_flair_text', 'distinguished'])\nprint(type(test2.getNextBatchCsv()))\nprint(test2.getNextBatchCsv())\nprint(test2.getNextBatchCsv())\nprint(test2.getHeader())\n'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nTurn data into matrix\n'''\ndef formatData(data,label,trainSize):\n    vectorizer = CountVectorizer(\n        lowercase=True, stop_words='english',\n        max_df=1.0, min_df=1, max_features=2000,  binary=True\n    )\n    out_data = vectorizer.fit_transform(data.astype('U').values).toarray()\n    out_label = label.astype('U').values\n    return (out_data[:trainSize],out_data[trainSize:],out_label[:trainSize],out_label[trainSize:])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nformat data test\n'''\ntest = CsvToDf(\"../input/reddit-data-3/mbti9k_comments.csv\",cols=['comment','type'],batchSize=200)\ndata = test.getNextBatchCsv()\nxTrain,xTest,yTrain,yTest = formatData(data['comment'],data['type'],100)\nprint(xTrain)\nprint(yTrain)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"####Zepeng Xiao Version\n###In this version,we only need train data once to capture needed probability\n#could change smooth factor by cls._smooth = smoothfactor or update_smooth and call predict method\n####\nclass BayesClassifier_smooth():\n    def __init__(self, smooth=1):\n        self._smooth = smooth # This is for additive smoothing\n    \n    def update_smooth(i):\n        self._smooth = i\n    \n    #the train method would only count the probability now\n    def train(self, X, y):\n        alpha_smooth = self._smooth\n        cls = np.unique(y)\n        Ncls, Nfeat = len(cls), X.shape[1] #Ncls: number of classes, Nfeat: number of features.\n\n        self._train_size = X.shape[0]                           #store the number of training data\n        self._feature_count = np.sum(X,axis=0)                  #count the total appear time of all features(words) by vertically summation every column of training set\n        self._cls = cls                                         #store classes for predict use\n        self._prior = np.zeros((Ncls,1))                        #initialize a (Ncl * 1) shape matrix to store the count of each label,used to calculate prior probability later\n        self._word_count = np.zeros((Ncls,1))                   #initialize the (Ncl *1) matrix to store the total count of appearence of each word given class \n\n        self._likehood = np.zeros((Ncls,Nfeat))                 #initialize to store for all the count of each word given class\n                                                                #the number of rows equal to number of classes,column number equal to number of features(words)\n                                                                #therefore it will be used to calculate vectorlized likelyhood p(x|y) later\n\n        #for each class,find rows that satisfies the condition,and capture the count from training set so that it can be used to calculate probability\n        for i in range(Ncls):\n            cla = cls[i]                                                   #cla <---- current class\n            x_cla = X[y==cla]                                              #a subset of the rows that belong to current class\n            self._prior[i,0] =x_cla.shape[0]                               #count frequency of current class \n\n            #verticlly summation along each column to get frequency of each feature appear given current class\n            self._likehood[i,:] = np.sum(x_cla, axis=0)\n\n            #sum through the subset training data of current class,count the total number of appearence of all words,used for JMM smoothing later\n            self._word_count[i,:] = np.sum(x_cla)\n\n\n    #this method used for report the predict when using JM smooth approach for pard(d)\n    #parameter X is the training data,X_dataset is also used here to calculate the feature appreance count in whole dataset to avoid zero probability\n    def predict_JM(self,X,X_dataset):                       \n        alpha_smooth = self._smooth                               #the smooth parameter alpha\n        Ncls,Ntest,Nfeat = len(self._cls),X.shape[0],X.shape[1]   #Ncls:number of class,Ntest:Number of test data,Nfeat:number of features\n        pred = np.zeros(Ntest)                                    #initialized the numpy array of predicted result,its size equals to number of test data\n        X_not = np.logical_not(X)                                 #for original data,1 for appearence of feature i and 0 for not appear,its logic not means 1 for not appear and 0 for appear\n\n                                                      #prior and likelyhood(probability) after additive smoothing\n        size_dataset = X_dataset.shape[0]             #number of data in dataset\n        feature_dataset = np.sum(X_dataset,axis=0)    #count the appearnce of each word in dataset by verticlly sum along each column from the dataset\n\n        #calculte prior probability by divide the count of class in training data by the training size\n        prior = self._prior / self._train_size\n\n        #compute the likelyhood probabity such that p(word i exist | class y) using JM smooth\n        likelyhood = (1-alpha_smooth)*(self._likehood/self._word_count) + (alpha_smooth) * (feature_dataset / size_dataset)\n        \n        #apply log transformation of likelyhood P(xi exists | y) and P(xi not exists | y)\n        not_likelyhood = np.log(1-likelyhood)\n        likelyhood = np.log(likelyhood)\n\n        #apply dot product to obtain the summation of log(P(xi exists | y)) and P(xi not exists | y)\n        log_appear = np.dot(X,likelyhood.T)\n        log_absence = np.dot (X_not,not_likelyhood.T)\n\n        #calculate log tranformed posterior probability\n        log_post = log_appear+log_absence + np.log(prior.reshape(1,Ncls))\n\n        #choose the y such that maximum the postior from the and return it\n        pred = np.argmax(log_post,axis=1)\n\n\n        return pred\n\n    def predict(self, X):\n        '''\n        This function has to return a numpy array of shape X.shape[0] (i.e. of shape \"number of testing examples\")\n        '''\n\n        alpha_smooth = self._smooth                                              #the smooth parameter alpha\n        Ncls,Ntest,Nfeat = len(self._cls),X.shape[0],X.shape[1]                  #Ncls:number of class,Ntest:Number of test data,Nfeat:number of features\n        pred = np.zeros(Ntest)                                                   #initialized the numpy array of predicted result,its size equals to number of test data\n        X_not = np.logical_not(X)                   #since in original data,1 for appearence of feature i and 0 for not appear,the logic not will give us the 1 for word not appear,and 0 for appear \n                                                    #used for vectorlizd multiplication(dot product) to get posterior\n        #calculte prior probability by divide the count of class in training data by the training size\n        prior = self._prior / self._train_size\n\n\n        #compute the likelyhood probabity such that p(word i exist | class y) using additive smooth\n        likelyhood = (self._likehood+alpha_smooth)/(self._prior + alpha_smooth*2)      #add alpha to numeriter and 2*alpha(cases of appear or not) to denominator\n\n\n        #X is in size Ntest x Nfeat, log(likehood.T) is the shape Nfeat x Ncls, each feature has 2 column in this case,each record its likelyhood of given y\n        #their product in shape Ntest x Ncls,for each test data,it has the summation of log(P(Xi exist | yi))\n      \n        log_appear = X.dot(np.log(likelyhood.T))\n\n        #1-likelyhood.T would generate all P(word i does not exist|class y),same size as likelyhood.T\n        #X_not dot product the log tranformation likelyhood for not appearence is the summation of log(P(xi does not exist | yi)) \n        log_absence = X_not.dot (np.log(1-likelyhood.T))\n\n        #add them up,it would equal the log transformed likelyhood used for naive bayes\n        log_post = log_appear+log_absence\n        \n        #add the log transformed prior probability,it become log transformed posterior probability\n        log_post = log_post + np.log(prior.reshape(1,Ncls))\n\n        #choose the y such that maximum the postior from the and return it\n        pred = self._cls[np.argmax(log_post,axis=1)]\n\n\n        return pred","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nNB classifier that can train on batches\n'''\nclass BayesClassifier_smooth_I():\n    def __init__(self,smooth=1):\n        self._smooth = smooth # This is for additive smoothing\n        self._cls = ['estj', 'estp', 'esfj', 'esfp', 'entj', 'entp', 'enfj', 'enfp', 'istj', 'istp', 'isfj', 'isfp', 'intj', 'intp', 'infj', 'infp']\n        self._prior = np.zeros((16,1))                        #initialize a (Ncl * 1) shape matrix to store the count of each label,used to calculate prior probability later\n        self._likehood = None\n        self._notInit = True\n        self._prob_matrix = None\n        #given test dataset we need to reshape likelihood to size of test data\n    def update_smooth(i):\n        self._smooth = i\n        \n    #the train method would only count the probability now\n    def train(self, X, y):\n        cls = np.unique(y)\n        Nfeat = X.shape[1] #Nfeat: number of features.\n        if self._notInit:\n            self._likehood = np.zeros((16,Nfeat))\n            self._notInit = False\n        for i in range(16):\n            cla = self._cls[i]                                                #cla <---- current class\n            x_cla = X[y==cla]                                              #a subset of the rows that belong to current class\n            self._prior[i,0] += x_cla.shape[0]                               #count frequency of current class \n            #verticlly summation along each column to get frequency of each feature appear given current class\n            self._likehood[i,:] += np.sum(x_cla, axis=0)\n\n\n    #this method used for report the predict when using JM smooth approach for pard(d)\n    #parameter X is the training data,X_dataset is also used here to calculate the feature appreance count in whole dataset to avoid zero probability\n    def _compute_prediction(self,data_point):\n        '''\n        precondition: datapoint must be a list of integers that are either 0 or 1. And its length must be the same as the number of features.\n        postcondition: no side effects\n        this will return an array of 16 floats [0,1] each corresponding to one of the possible personality types\n        '''\n        data_point_matrix = np.array([data_point,]*self._prob_matrix.shape[0])\n        true_matrix = data_point_matrix * self._prob_matrix\n        false_matrix = ((data_point_matrix+1)%2) * (1-self._prob_matrix)\n        label_array = np.squeeze(np.sum(np.log(true_matrix+false_matrix),axis=1))\n        prior_array = np.squeeze(np.log((self._prior+self._smooth)/(16*self._smooth+sum(self._prior))))\n        return np.squeeze(label_array+prior_array)\n    def predict(self, X):\n        '''\n        This function has to return a numpy array of shape X.shape[0] (i.e. of shape \"number of testing examples\")\n        '''\n        self._prob_matrix = (self._likehood+self._smooth)/(self._prior + self._smooth*2)\n        #For each row in X determine the likelihood of it being one of the personalities\n        result = np.apply_along_axis(self._compute_prediction,axis=1,arr=X)\n        #result will be a matrix that has the shape of (# rows in X,# of possible personalities).\n        #each entry will represent the probability of a data point being a specific personality\n        pred = np.argmax(result,axis=1)\n\n        return pred\n    \n    def formatData(data,label,trainSize):\n        vectorizer = HashingVectorizer(\n        lowercase=True, stop_words='english',binary=True)\n        out_data = vectorizer.fit_transform(data.astype('U').values).toarray()\n        out_label = label.str.lower().astype('U').values\n        return (out_data[:trainSize],out_data[trainSize:],out_label[:trainSize],out_label[trainSize:])\n    \n    def strLabelToInt(inpLabel):\n        out = np.zeros((inpLabel.shape))\n        labels = ['estj', 'estp', 'esfj', 'esfp', 'entj', 'entp', 'enfj', 'enfp', 'istj', 'istp', 'isfj', 'isfp', 'intj', 'intp', 'infj', 'infp']\n        for idx,i in enumerate(inpLabel):\n            if i.lower() in labels:\n                out[idx] = labels.index(i)\n            else:\n                out[idx] = -100\n        return out","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nNaive Bayes Test 1\n'''\n'''classifier = BayesClassifier_smooth()\ntest = CsvToDf(\"../input/mbti-type/mbti_1.csv\",batchSize=200,cols=['posts', 'type'])\nprint(test.getHeader())\ndata = test.getNextBatchCsv()\nxTrain,xTest,yTrain,yTest = formatData(data['posts'],data['type'],100)\nyTest = strLabelToInt(yTest)\nclassifier.train(xTrain,yTrain)\npred = classifier.predict(xTest)\nprint(\"accuracy = {}\".format(np.mean((yTest-pred)==0))) '''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"RNN implementation","metadata":{}},{"cell_type":"code","source":"full_type_labels = df['type']\n# Enumerating the personality types so that our model can work with numbers\npersonality_dict = {\"ENTJ\" : 0, \"INTJ\" : 1, \"ENTP\" : 2, \"INTP\" : 3, \"INFJ\" : 4, \"INFP\" : 5, \"ENFJ\" : 6 , \n                    \"ENFP\" : 7, \"ESTP\" : 8, \"ESTJ\" : 9, \"ISTP\" : 10, \"ISTJ\" : 11, \"ISFJ\" : 12, \"ISFP\" : 13, \n                    \"ESFJ\" : 14, \"ESFP\" : 15}\n\ntype_labels = []\n\n# Go through the array and turn the personality type into its corresponding number\nfor idx, personality in enumerate(full_type_labels):\n    type_labels.append(personality_dict[personality])\n\ntype_labels = np.array(type_labels)\n\nprint(type_labels)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\n# Only considering the top 10000 most common words\nvocab_size = 10000\nmax_length = 2016\n# We have 8675 rows\ntraining_size = 8675//2\ntraining_posts = posts[0:training_size]\ntesting_posts = posts[training_size:]\n# Right now is only predicting introversion or extroversion\ntraining_labels = type_labels[0:training_size]\ntesting_labels = type_labels[training_size:]\n\n# We only want to fit the tokenizer on the training, not the testing\ntokenizer = Tokenizer(num_words = vocab_size, oov_token = \"<OOV>\")\ntokenizer.fit_on_texts(training_posts)\n\nword_index = tokenizer.word_index\n\n# Puts the padding (which are 0) at the end of the vectorized sentence.\n# The longest post in our dataset is 2016, but we should truncate='post' earlier than 2016 words\ntraining_sequences = tokenizer.texts_to_sequences(training_posts)\ntraining_padded = pad_sequences(training_sequences, padding = 'post', maxlen = max_length)\n# training_sequences = np.array(training_sequences)\ntraining_padded = np.array(training_padded)\n\ntesting_sequences = tokenizer.texts_to_sequences(testing_posts)\ntesting_padded = pad_sequences(testing_sequences, padding = 'post', maxlen=max_length)\n# testing_sequences = np.array(testing_sequences)\ntraining_padded = np.array(training_padded)\n\n\nprint(word_index)\nprint(training_padded[0])\nprint(training_padded.shape)\n'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Commenting out for similar reasons\n\n# #Second parameter is the output dimension. Therefore, when we are changing this to predict 4 dimensions of personality we should change it to 4\n# embedding_dim = 1\n\n# model = tf.keras.Sequential([ \n#                              tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n#                              tf.keras.layers.GlobalAveragePooling1D(),\n#                              tf.keras.layers.Dense(24, activation='relu'),\n#                              tf.keras.layers.Dense(1, activation='sigmoid')\n# ])\n\n# model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''Second parameter is the output dimension. Therefore, when we are changing this to predict 4 dimensions of personality we should change it to 4\n# ^^ actually i dont know if that is true\nembedding_dim = 256\n\n#Embedding layer will always have vocab_size*embedding_dim parameters. Since vocab_size is 10,000 the number of parameters on this layer will always be large\n\nmodel = tf.keras.Sequential([ \n                             tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n                            #  tf.keras.layers.LSTM(100),\n                             tf.keras.layers.GlobalAveragePooling1D(),\n                            #tf.keras.layers.Dense(1000, activation='relu'),\n                            # tf.keras.layers.Dense(400, activation='relu'),\n                             tf.keras.layers.Dense(128, activation='relu'),\n                             tf.keras.layers.Dense(48, activation='relu'),\n                             tf.keras.layers.Dense(16, activation='softmax')\n])\n\nmodel.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''num_epochs = 10\n\nhistory = model.fit(training_padded, training_labels, epochs = num_epochs, validation_data=(testing_padded, testing_labels), verbose = 1)'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"K_Fold_CV","metadata":{}},{"cell_type":"code","source":"#this one is the final version of the k_Fold_CV method\ndef k_Fold_CV(k, maxFeature):\n    all_data = df['posts'].astype('U').values\n    all_type = df['type'].astype('U').values\n    #use int() to eliminate decimals\n    data_fragment_size = int(len(all_data) / k)\n    #we vectorize the data first, but due to the ram overload, it is still an issue to be resolved\n    #for now, let's keep the max_feature limit\n    #the all_data should have the size of 2000 for now\n    vectorizer = CountVectorizer(\n        lowercase=True, stop_words=None,\n        max_df=1.0, min_df=1, max_features=maxFeature,  binary=True\n    )\n    processed_data = vectorizer.fit_transform(all_data).toarray()\n    accuracyList = []\n\n    for i in range(0, k):\n        lower_bound = i * data_fragment_size\n        upper_bound = lower_bound + data_fragment_size\n        #split the data into training and testing based on k\n        #this part is just the modified version of the normal test part written by Zepeng Xiao\n        validationSize = int((len(all_data) - data_fragment_size) / 4)\n        \n        if i == 0:\n            y_validate = df['type'][upper_bound:upper_bound+validationSize].astype('U').values\n            x_validate = processed_data[upper_bound:upper_bound+validationSize, :]\n            y_train = df['type'][upper_bound+validationSize:].astype('U').values\n            x_train = processed_data[upper_bound+validationSize:, :]\n            \n        else:\n            y_validate = all_type[np.r_[:validationSize]]\n            x_validate = processed_data[np.r_[:validationSize]]\n            y_train = all_type[np.r_[validationSize:lower_bound, upper_bound:]]\n            x_train = processed_data[np.r_[validationSize:lower_bound, upper_bound:]]\n\n        y_test = df['type'][lower_bound:upper_bound].astype('U').values\n        x_test = processed_data[lower_bound:upper_bound]\n        \n        maxAccuracy = 0\n        bestsf = 0\n        #validation\n        for i in range(1, 21, 1):\n            sf = i/10\n            NBS = BayesClassifier_smooth(sf)\n            NBS.train(x_train, y_train)\n            y_predict = NBS.predict(x_validate)\n            accuracy = np.mean(y_validate==y_predict)\n            if accuracy > maxAccuracy: #find the smoothing Factor with the best performance\n                bestsf = sf\n\n        #feed the data to the model with the best smooth factor obtained from above and get the results\n        #first test the normal NB\n        #then test the smoothed NB\n\n        NBS = BayesClassifier_smooth(bestsf)\n        NBS.train(x_train, y_train)\n        y_predict = NBS.predict(x_test)\n        #print(\"Absolute accuracy = {}\".format(np.mean(y_test==y_predict)))\n        accuracy = np.mean(y_test==y_predict)\n        accuracyList.append(accuracy)\n        \n    return accuracyList","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"k_Fold_CV(2, maxFeature)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#this one is the smoothFactor specifiable version of k_Fold_CV method\ndef k_Fold_CV_sf(k, maxFeature, smoothFactor):\n    all_data = df['posts'].astype('U').values\n    all_type = df['type'].astype('U').values\n    #use int() to eliminate decimals\n    data_fragment_size = int(len(all_data) / k)\n    #we vectorize the data first, but due to the ram overload, it is still an issue to be resolved\n    #for now, let's keep the max_feature limit\n    #the all_data should have the size of 2000 for now\n    vectorizer = CountVectorizer(\n        lowercase=True, stop_words=None,\n        max_df=1.0, min_df=1, max_features=maxFeature,  binary=True\n    )\n    processed_data = vectorizer.fit_transform(all_data).toarray()\n    accuracyList = []\n\n    for i in range(0, k):\n        lower_bound = i * data_fragment_size\n        upper_bound = lower_bound + data_fragment_size\n        #split the data into training and testing based on k\n        #this part is just the modified version of the normal test part written by Zepeng Xiao\n        \n        if i == 0:\n            y_train = df['type'][upper_bound:].astype('U').values\n            x_train = processed_data[upper_bound:, :]\n            \n        else:\n            y_train = all_type[np.r_[:lower_bound, upper_bound:]]\n            x_train = processed_data[np.r_[:lower_bound, upper_bound:]]\n\n        y_test = df['type'][lower_bound:upper_bound].astype('U').values\n        x_test = processed_data[lower_bound:upper_bound]\n\n        #feed the data to the model and get the results\n        #first test the normal NB\n        #then test the smoothed NB\n        \n        NBS = BayesClassifier_smooth(smoothFactor)\n        NBS.train(x_train, y_train)\n        y_predict = NBS.predict(x_test)\n        #print(\"Absolute accuracy = {}\".format(np.mean(y_test==y_predict)))\n        accuracy = np.mean(y_test==y_predict)\n        accuracyList.append(accuracy)\n        \n    return accuracyList\n\n#test for different smooth factors\navgList = []\nmaxList = []\nsmoothFactorList = []\nfor smoothFactor in range(0, 51, 1):\n    smoothFactor = smoothFactor / 10\n    print(\"smooth factor:\", smoothFactor)\n    accuracyList = k_Fold_CV_sf(10, 2000, smoothFactor)\n    avgAccuracy = np.average(accuracyList)\n    maxAccuracy = np.max(accuracyList)\n    avgList.append(avgAccuracy)\n    maxList.append(maxAccuracy)\n    smoothFactorList.append(smoothFactor)\n    print(accuracyList, maxAccuracy)\n    \nplt.title(\"Average Accuracy vs Smooth Factor\")\nplt.xlabel(\"smooth factor\")\nplt.ylabel(\"average accuracy\")\nplt.plot(smoothFactorList, avgList)\nplt.show()\n\nplt.title(\"Maximum Accuracy vs Smooth Factor\")\nplt.xlabel(\"smooth factor\")\nplt.ylabel(\"maximum accuracy\")\nplt.plot(smoothFactorList, maxList)\nplt.show()\n\n#test for different max features\navgList = []\nmaxList = []\nmaxFeatureList = []\nfor maxFeature in range(1500, 10001, 500):\n    accuracyList = k_Fold_CV_sf(3, maxFeature)\n    avgAccuracy = np.average(accuracyList)\n    maxAccuracy = np.max(accuracyList)\n    avgList.append(avgAccuracy)\n    maxList.append(maxAccuracy)\n    maxFeatureList.append(maxFeature)\n    \nplt.title(\"Average Accuracy vs Maximum Feature\")\nplt.xlabel(\"maximum feature\")\nplt.ylabel(\"average accuracy\")\nplt.plot(maxFeatureList, avgList)\nplt.show()\n\nplt.title(\"Maximum Accuracy vs Maximum Feature\")\nplt.xlabel(\"maximum feature\")\nplt.ylabel(\"maximum accuracy\")\nplt.plot(maxFeatureList, maxList)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"k = 2: \", k_Fold_CV(2, 2000))\nprint(\"k = 3: \", k_Fold_CV(3, 2000))\nprint(\"k = 5: \", k_Fold_CV(5, 2000))\nprint(\"k = 10: \", k_Fold_CV(10, 2000))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#modifying the maxFeatures\navgList = []\nmaxList = []\nmaxFeatureList = []\nfor maxFeature in range(2000, 10001, 1000):\n    accuracyList = k_Fold_CV(3, maxFeature)\n    avgAccuracy = np.average(accuracyList)\n    maxAccuracy = np.max(accuracyList)\n    avgList.append(avgAccuracy)\n    maxList.append(maxAccuracy)\n    maxFeatureList.append(maxFeature)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.title(\"Average Accuracy vs Maximum Feature\")\nplt.xlabel(\"maximum feature\")\nplt.ylabel(\"average accuracy\")\nplt.plot(maxFeatureList, avgList)\nplt.show()\n\nplt.title(\"Maximum Accuracy vs Maximum Feature\")\nplt.xlabel(\"maximum feature\")\nplt.ylabel(\"maximum accuracy\")\nplt.plot(maxFeatureList, maxList)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}