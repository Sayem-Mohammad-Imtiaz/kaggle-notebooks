{"cells":[{"metadata":{},"cell_type":"markdown","source":"# About this kernel\n\n*Tell me, Mr. AI... how much is my question worth?*\n\nHave you ever thought about an incredible piece of trivia, and wondered how much that would have been worth if it was asked in a game of Jeopardy? This is what we will try to predict today by using this list of 200k+ questions from the popular game show. To do that, we will use:\n\n1. Load the data, and create a train and test split.\n2. A **Simple Linear Model** with a simple bag-of-words encoding.\n3. A **Bidirectional LSTM** model, with GlobalMaxPooling, fully-connected layers, and softmax output."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom sklearn.preprocessing import LabelEncoder\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, GlobalMaxPooling1D, LSTM, Bidirectional, Embedding, Dropout\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1. Preprocessing\n\nThe first step is to load the data (in CSV format), and split the data."},{"metadata":{"trusted":true},"cell_type":"code","source":"data_df = pd.read_csv('/kaggle/input/200000-jeopardy-questions/JEOPARDY_CSV.csv')\ndata_df = data_df[data_df[' Value'] != 'None']\n\nprint(data_df.shape)\ndata_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Creating bins\n\nSince the values could easily vary, this means we would have way too many classes to classify! Instead, we will bin it in this way: if the value is smaller than 1000, then we round to the nearest hundred. Otherwise, if it's between 1000 and 10k, we round it to nearest thousand. If it's greater than 10k, then we round it to the nearest 10-thousand."},{"metadata":{"trusted":true},"cell_type":"code","source":"data_df['ValueNum'] = data_df[' Value'].apply(\n    lambda value: int(value.replace(',', '').replace('$', ''))\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def binning(value):\n    if value < 1000:\n        return np.round(value, -2)\n    elif value < 10000:\n        return np.round(value, -3)\n    else:\n        return np.round(value, -4)\n\ndata_df['ValueBins'] = data_df['ValueNum'].apply(binning)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"print(\"Total number of categories:\", data_df[' Value'].unique().shape[0])\nprint(\"Number of categories after binning:\", data_df['ValueBins'].unique().shape[0])\nprint(\"\\nBinned Categories:\", data_df['ValueBins'].unique())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Then, we will split our data by randomly selected 20% of the shows, and use the questions from that show as what we will try to predict."},{"metadata":{"trusted":true},"cell_type":"code","source":"show_numbers = data_df['Show Number'].unique()\ntrain_shows, test_shows = train_test_split(show_numbers, test_size=0.2, random_state=2019)\n\ntrain_mask = data_df['Show Number'].isin(train_shows)\ntest_mask = data_df['Show Number'].isin(test_shows)\n\ntrain_labels = data_df.loc[train_mask, 'ValueBins']\ntrain_questions = data_df.loc[train_mask, ' Question']\ntest_labels = data_df.loc[test_mask, 'ValueBins']\ntest_questions = data_df.loc[test_mask, ' Question']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"# 2. Simple Linear Model"},{"metadata":{},"cell_type":"markdown","source":"## Transform questions to bag-of-words\n\nBag of words is a very simple, but very convenient way of representing any type of freeform text using vectors. [This article on medium](https://machinelearningmastery.com/gentle-introduction-bag-words-model/) goes in depth about the subject.\n\nIn our model, we will limit ourselves to only using the top 2000 most frequent words as features, in order for the logistic regression model to not overfit on too many features. Further, we are removing **stop words**, which are very common words in English that we wish to remove in order to only keep relevant information. Feel free to try different values of `max_features` and `stop_words`!"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nbow = CountVectorizer(stop_words='english', max_features=2000)\nbow.fit(data_df[' Question'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = bow.transform(train_questions)\nX_test = bow.transform(test_questions)\n\ny_train = train_labels\ny_test = test_labels\n\nprint(\"Shape of X_train:\", X_train.shape)\nprint(\"Shape of X_test:\", X_test.shape)\nprint(\"Shape of y_train:\", y_train.shape)\nprint(\"Shape of y_test:\", y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train the Logistic Regression model\n\nLogistic Regression is perhaps the simplest regression model out there."},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nlr = LogisticRegression(solver='saga', multi_class='multinomial', max_iter=200)\nlr.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Evaluate the results"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = lr.predict(X_test)\n\nprint(classification_report(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. LSTM"},{"metadata":{},"cell_type":"markdown","source":"## Tokenize & Pad\n\nWe are doing 3 things here:\n\n1. Train a tokenizer in all the text. This tokenizer will create an dictionary mapping words to an index, aka `tokenizer.word_index`.\n2. Convert the questions (which are strings of text) into a list of list of integers, each representing the index of a word in the `word_index`.\n3. Pad each \"list of list\" into a single numpy array. To do this, we use the `pad_sequences` function, and set a maximum length (50 is reasonable since most questions will be at most 20 words), after which any word is cutoff.\n\nNote:\n* Tokenizer will take at most 50k words. Here, we are using more words than Logistic Regression since the input dimension does not account for **all** words, but only the words that are actually given in the sequence."},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = Tokenizer(num_words=50000)\ntokenizer.fit_on_texts(data_df[' Question'])\n\ntrain_sequence = tokenizer.texts_to_sequences(train_questions)\ntest_sequence = tokenizer.texts_to_sequences(test_questions)\n\nprint(\"Original text:\", train_questions[0])\nprint(\"Converted sequence:\", train_sequence[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = pad_sequences(train_sequence, maxlen=50)\nX_test = pad_sequences(test_sequence, maxlen=50)\n\nprint(X_train.shape)\nprint(X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Encode labels as counts\n\nUnlike Sklearn, Keras requires your labels to be either one-hot-encoded, or encoded using label encoders. For the former, you will need to use a `categorical_crossentropy` loss when you compile the model, and for the latter you need to use `sparse_categorical_crossentropy`. We will use the latter for simplicity, but if you want to learn more about one-hot-encoding you can check out [this user guide](https://scikit-learn.org/stable/modules/preprocessing.html#preprocessing-categorical-features)."},{"metadata":{"trusted":true},"cell_type":"code","source":"le = LabelEncoder()\nle.fit(data_df['ValueBins'])\n\ny_train = le.transform(train_labels)\ny_test = le.transform(test_labels)\n\nprint(y_train.shape)\nprint(y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Building and running the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"num_words = tokenizer.num_words\noutput_size = len(le.classes_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential([\n    Embedding(input_dim=num_words, \n              output_dim=200, \n              mask_zero=True, \n              input_length=50),\n    Bidirectional(LSTM(150, return_sequences=True)),\n    GlobalMaxPooling1D(),\n    Dense(300, activation='relu'),\n    Dropout(0.5),\n    Dense(output_size, activation='softmax')\n    \n])\n\nmodel.compile('adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(X_train, y_train, epochs=10, batch_size=1024, validation_split=0.1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Evaluate the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = model.predict(X_test, batch_size=1024).argmax(axis=1)\nprint(classification_report(y_test, y_pred))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}