{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set(style=\"whitegrid\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv(\"../input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv\", nrows=2000)\nprint(data.shape)\ndata.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(data['sentiment'].value_counts())\n\nfig = plt.figure(figsize = (10,6))\nsns.countplot(data=data, x='sentiment', palette = [\"green\",\"red\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['sentiment'] = data['sentiment'].apply(lambda x: 1 if x=='positive' else 0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Cleaning","metadata":{}},{"cell_type":"code","source":"# data[data['review'].str.contains(r'<.*?', regex=True)]\n\n# data[data['review'].str.contains('http')]\n# a=data[data['review'].str.contains(r'\\\\W', regex=True)].reset_index()\n# print(a['review'][0])\n\n# print(\"<: \", data['review_clean'].str.contains('<').sum())\n# print(\">: \", data['review_clean'].str.contains('>').sum())\n# print(\"/: \", data['review_clean'].str.contains('/').sum())\n# print(\"http: \", data['review_clean'].str.contains('http').sum())\n# print(\"<br />: \", data['review_clean'].str.contains('<br />').sum())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install contractions","metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import contractions\nimport re\nimport string\nfrom nltk.corpus import wordnet\nfrom nltk.tag import pos_tag\nfrom nltk.tokenize import word_tokenize\n\nfrom nltk.corpus import stopwords\nstop_words=set(stopwords.words(\"english\"))\n\nfrom nltk.stem.wordnet import WordNetLemmatizer\nwnl = WordNetLemmatizer()\n\ndef get_wordnet_pos(word):\n    tag = pos_tag([word])[0][1][0].upper()\n    tag_dict = {\"J\": wordnet.ADJ,\n                \"N\": wordnet.NOUN,\n                \"V\": wordnet.VERB,\n                \"R\": wordnet.ADV}\n\n    return tag_dict.get(tag, wordnet.NOUN)\n\ndef remove_punct(text):\n    message=[]\n    \n    for word in text:\n        message_not_punc = []\n        \n        if word not in stop_words:\n            for char in word:\n                if char not in string.punctuation:\n                    message_not_punc.append(char)\n\n            text_nopunct = \"\".join(message_not_punc)\n            \n            if text_nopunct!=\"\":\n                message.append(text_nopunct)\n                \n    return message\n\ndef preprocessing(text):\n    text = text.lower().strip()\n    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n    text = text.replace(\"http: //video.google.com/ videoplay?docid=-3001837218936089620&q =innerviews+ jamie+foxx&hl=en \",\"\")\n    text = re.sub('\\n', '', text)\n    text = re.sub('<.*?>', '', text)\n    text = re.sub('\\[.*?\\]', '', text)\n    text = contractions.fix(text)\n    text = re.sub(\"\\\\W\",\" \",text) # remove special chars\n    text=word_tokenize(text)\n    \n    message = []\n    \n    for word in text:\n        message.append(wnl.lemmatize(word, get_wordnet_pos(word)))\n    \n    message = remove_punct(message)\n    message = \" \".join(message)\n    \n    return message\n\n\ndata['review_clean'] = data['review'].apply(lambda x: preprocessing(x))\n\ndata.head(20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create features","metadata":{}},{"cell_type":"markdown","source":"## Length","metadata":{}},{"cell_type":"code","source":"data['length'] = data['review'].apply(lambda x: len(x) - x.count(\" \"))\n\nsns.displot(data=data, x=\"length\", hue='sentiment', col=\"sentiment\", bins=20, multiple=\"dodge\", aspect=1.5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Count of punctuations marks","metadata":{}},{"cell_type":"code","source":"def count_punct(text):\n    \n    count=0\n    for char in text:\n        if char in string.punctuation:\n            count+=1\n    \n    return count\n\ndata['count_punct'] = data['review'].apply(lambda x: count_punct(x))\n\nsns.displot(data=data, x=\"count_punct\", hue='sentiment', col=\"sentiment\", bins=20, multiple=\"dodge\", aspect=1.5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Count uppercase letters","metadata":{}},{"cell_type":"code","source":"def count_uppercase(text):  \n    count=0\n    for char in text:\n        if char.isupper():\n            count+=1\n    \n    return count\n\ndata['count_uppercase'] = data['review'].apply(lambda x: count_uppercase(x))\n\nsns.displot(data=data, x=\"count_uppercase\", hue='sentiment', col=\"sentiment\", bins=20, multiple=\"dodge\", aspect=1.5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Count exclamation marks","metadata":{}},{"cell_type":"code","source":"data['exclamation_marks'] = data['review'].apply(lambda x: x.count(\"!\"))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Count words","metadata":{}},{"cell_type":"code","source":"data['count_words'] = data['review'].apply(lambda x: len(x.split()))\n\nsns.displot(data=data, x=\"count_words\", hue='sentiment', col=\"sentiment\", bins=20, multiple=\"dodge\", aspect=1.5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Average word length","metadata":{}},{"cell_type":"code","source":"def avg_word_len(text):  \n    text=text.split()\n    for i, val in enumerate(text):\n        text[i]=len(val)\n    \n    return round(np.mean(text),3)\n\ndata['avg_word_len'] = data['review'].apply(lambda x: avg_word_len(x))\n\nsns.displot(data=data, x=\"avg_word_len\", hue='sentiment', col=\"sentiment\", bins=20, multiple=\"dodge\", aspect=1.5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## VADER","metadata":{}},{"cell_type":"code","source":"import nltk\nnltk.download('vader_lexicon')\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\n\nsid = SentimentIntensityAnalyzer()\n\ndata['scores'] = data['review'].apply(lambda review: sid.polarity_scores(review))\ndata['VADER_negative']  = data['scores'].apply(lambda score_dict: score_dict['neg'])\ndata['VADER_neutral']  = data['scores'].apply(lambda score_dict: score_dict['neu'])\ndata['VADER_positive']  = data['scores'].apply(lambda score_dict: score_dict['pos'])\ndata['VADER_compound']  = data['scores'].apply(lambda score_dict: score_dict['compound'])","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.displot(data=data, x=\"VADER_negative\", hue='sentiment', col=\"sentiment\", bins=20, multiple=\"dodge\", aspect=1.5)\nsns.displot(data=data, x=\"VADER_neutral\", hue='sentiment', col=\"sentiment\", bins=20, multiple=\"dodge\", aspect=1.5)\nsns.displot(data=data, x=\"VADER_positive\", hue='sentiment', col=\"sentiment\", bins=20, multiple=\"dodge\", aspect=1.5)\nsns.displot(data=data, x=\"VADER_compound\", hue='sentiment', col=\"sentiment\", bins=20, multiple=\"dodge\", aspect=1.5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_corpus(text):\n    words = []\n    \n    for i in text:\n        for j in i.split():\n            words.append(j.strip())\n            \n    return words\n\ncorpus = get_corpus(data[\"review_clean\"])\n\n\nfrom collections import Counter\ncounter = Counter(corpus)\nmost_common = counter.most_common(30)\nmost_common = dict(most_common)\n\ndf = pd.DataFrame.from_dict(most_common, orient='index').reset_index()\ndf = df.rename(columns={'index':'Word', 0:'Count'})\n\nfig = plt.figure(figsize = (10,6))\nsns.barplot(data=df, x=\"Count\", y=\"Word\", palette=\"Blues_r_d\", orient='h')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\n\ndef get_top_text_ngrams(corpus, n, g):\n    vec = CountVectorizer(ngram_range=(g, g)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq = sorted(words_freq, key = lambda x: x[1], reverse=True)\n    \n    return words_freq[:n]\n\nmost_common_bi = get_top_text_ngrams(data[\"review_clean\"], 20, 2)\nmost_common_bi = dict(most_common_bi)\n\ndf = pd.DataFrame.from_dict(most_common_bi, orient='index').reset_index()\ndf = df.rename(columns={'index':'Word', 0:'Count'})\n\nfig = plt.figure(figsize = (10,6))\nsns.barplot(data=df, x=\"Count\", y=\"Word\", palette=\"Blues_r_d\", orient='h')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\n\ncount_vect = CountVectorizer()\nX_counts = count_vect.fit_transform(data[\"review_clean\"])\nprint(X_counts.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_counts_df = pd.DataFrame(X_counts.toarray())\nX_counts_df.columns = count_vect.get_feature_names()\nX_counts_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_features = pd.concat([data['VADER_negative'], data['VADER_neutral'], data['VADER_positive'], data['VADER_compound'], data['count_words'], data['avg_word_len'], data['length'], data['count_punct'], data['count_uppercase'], data['exclamation_marks'], pd.DataFrame(X_counts.toarray())], axis=1)\nX_features.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import KFold, cross_val_score\n\nrf = RandomForestClassifier(n_jobs=-1)\nk_fold = KFold(n_splits=5)\ncross_val_score(rf, X_features, data['sentiment'], cv=k_fold, scoring='accuracy', n_jobs=-1).mean()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import precision_recall_fscore_support as score\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X_features, data['sentiment'], test_size=0.2)\n\nrf = RandomForestClassifier(n_jobs=-1)\nrf_model = rf.fit(X_train, y_train)\n\ny_pred = rf_model.predict(X_test)\nprecision, recall, fscore, support = score(y_test, y_pred, average='binary')\nprint(f'Precision: {round(precision, 3)} \\nRecall: {round(recall, 3)} \\nAccuracy: {round((y_pred==y_test).sum() / len(y_pred),3)}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_importance = rf_model.feature_importances_[:15]\nsorted_idx = np.argsort(feature_importance)\npos = np.arange(sorted_idx.shape[0]) + .5\n\nfig = plt.figure(figsize=(17, 6))\nplt.barh(pos, feature_importance[sorted_idx], align='center')\nplt.yticks(pos, np.array(X_train.columns)[sorted_idx])\nplt.title('Feature Importance')","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}