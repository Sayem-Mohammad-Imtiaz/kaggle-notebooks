{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Since we have to accurately predict the probability of loanee/borrower defaulting on a vehicle loan in the first EMI on the due date, along with AUC-ROC score, have taken F1-score(1s) and binary log loss as the performance metrics.**","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/lt-vehicle-loan-default-prediction/train.csv')\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.columns = [i.lower() for i in df.columns]\ndf.columns = [i.replace('.','_') for i in df.columns]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('The number of duplicates:',df.duplicated().sum())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Dropping the unnecessary features\n\ndf.drop(['uniqueid','branch_id','supplier_id','mobileno_avl_flag','current_pincode_id','employee_code_id','manufacturer_id','state_id'],axis=1,inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Dropping the rows with Employment Type Null as it constitutes just 3% of data\ndf = df.dropna()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe(include='all')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dependent variable distribution","metadata":{}},{"cell_type":"code","source":"pct_loan_default = df['loan_default'].value_counts(normalize=True)*100\npct_loan_default","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import plotly.express as px \nfig = px.pie(values=pct_loan_default.values, names=['Not defaulted','Defaulted']) \nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**78.3% observations in the dataset have not defaulted vehicle loan while 21.7% have defaulted loan. It is slighly imbalanced dataset.**","metadata":{}},{"cell_type":"markdown","source":"# Categorical Features Analysis","metadata":{}},{"cell_type":"markdown","source":"## Employment_Type","metadata":{}},{"cell_type":"code","source":"df1 = pd.crosstab(df['employment_type'],df['loan_default'])\nfig = px.bar(df1, barmode = 'group',width=600,height=400)\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['employment_type'].value_counts(normalize=True)*100","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Percentage of salaried people who have defaulted:',\n     np.round(df[(df['employment_type']=='Salaried') & (df['loan_default']==1)].shape[0]/(df[df['employment_type']=='Salaried'].shape[0])*100,3))\nprint('Percentage of self employed people who have defaulted:',\n     np.round(df[(df['employment_type']=='Self employed') & (df['loan_default']==1)].shape[0]/(df[df['employment_type']=='Self employed'].shape[0])*100,3))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Aadhar","metadata":{}},{"cell_type":"code","source":"df1 = pd.crosstab(df['aadhar_flag'],df['loan_default'])\nfig = px.bar(df1, barmode = 'group',width=600,height=400)\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['aadhar_flag'].value_counts(normalize=True)*100","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Percentage of people who have given their Aadhar details and defaulted:',\n     np.round(df[(df['aadhar_flag']==1) & (df['loan_default']==1)].shape[0]/(df[df['aadhar_flag']==1].shape[0])*100,3))\nprint('Percentage of people who have not given their Aadhar details and defaulted:',\n     np.round(df[(df['aadhar_flag']==0) & (df['loan_default']==1)].shape[0]/(df[df['aadhar_flag']==0].shape[0])*100,3))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## PAN","metadata":{}},{"cell_type":"code","source":"df1 = pd.crosstab(df['pan_flag'],df['loan_default'])\nfig = px.bar(df1, barmode = 'group',width=600,height=400)\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['pan_flag'].value_counts(normalize=True)*100","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Percentage of people who have given their PAN details and defaulted:',\n     np.round(df[(df['pan_flag']==1) & (df['loan_default']==1)].shape[0]/(df[df['pan_flag']==1].shape[0])*100,3))\nprint('Percentage of people who have not given their PAN details and defaulted:',\n     np.round(df[(df['pan_flag']==0) & (df['loan_default']==1)].shape[0]/(df[df['pan_flag']==0].shape[0])*100,3))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Voter ID","metadata":{}},{"cell_type":"code","source":"df1 = pd.crosstab(df['voterid_flag'],df['loan_default'])\nfig = px.bar(df1, barmode = 'group',width=600,height=400)\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['voterid_flag'].value_counts(normalize=True)*100","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Percentage of people who have given their voter_id details and defaulted:',\n     np.round(df[(df['voterid_flag']==1) & (df['loan_default']==1)].shape[0]/(df[df['voterid_flag']==1].shape[0])*100,3))\nprint('Percentage of people who have not given their voter_id details and defaulted:',\n     np.round(df[(df['voterid_flag']==0) & (df['loan_default']==1)].shape[0]/(df[df['voterid_flag']==0].shape[0])*100,3))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## DL","metadata":{}},{"cell_type":"code","source":"df1 = pd.crosstab(df['driving_flag'],df['loan_default'])\nfig = px.bar(df1, barmode = 'group',width=600,height=400)\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['driving_flag'].value_counts(normalize=True)*100","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Percentage of people who have given their DL details and defaulted:',\n     np.round(df[(df['driving_flag']==1) & (df['loan_default']==1)].shape[0]/(df[df['driving_flag']==1].shape[0])*100,3))\nprint('Percentage of people who have not given their DL details and defaulted:',\n     np.round(df[(df['driving_flag']==0) & (df['loan_default']==1)].shape[0]/(df[df['driving_flag']==0].shape[0])*100,3))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Passport","metadata":{}},{"cell_type":"code","source":"df1 = pd.crosstab(df['passport_flag'],df['loan_default'])\nfig = px.bar(df1, barmode = 'group',width=600,height=400)\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['passport_flag'].value_counts(normalize=True)*100","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Percentage of people who have given their Passport details and defaulted:',\n     np.round(df[(df['passport_flag']==1) & (df['loan_default']==1)].shape[0]/(df[df['passport_flag']==1].shape[0])*100,3))\nprint('Percentage of people who have not given their Passport details and defaulted:',\n     np.round(df[(df['passport_flag']==0) & (df['loan_default']==1)].shape[0]/(df[df['passport_flag']==0].shape[0])*100,3))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Encoding Employment Type\ndf['self_employed'] = pd.get_dummies(df['employment_type'],drop_first=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.drop('employment_type',axis=1,inplace=True)\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Feature importances of different categorical features**","metadata":{}},{"cell_type":"code","source":"df1 = df[['self_employed','aadhar_flag','pan_flag','voterid_flag','driving_flag',\n         'passport_flag']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Feature importances using SelectKBest algorithm using chi2\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.feature_selection import SelectKBest,chi2\nn = SelectKBest(score_func=chi2, k='all')\ncatcols=n.fit(df1,df['loan_default'])\nplt.figure(figsize=(7,5))\nsns.barplot(x=catcols.scores_,y=df1.columns)\nplt.title('Best Categorical Features')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Feature importances using Extra Trees Classifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nimport matplotlib.pyplot as plt\nmodel=ExtraTreesClassifier()\nmodel.fit(df1,df['loan_default'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(model.feature_importances_,index=df1.columns,columns=['Feature_Importance']).sort_values(by='Feature_Importance',ascending=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ranked_features=pd.Series(model.feature_importances_,index=df1.columns)\nranked_features.nlargest(6).plot(kind='barh')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Dropping unncessary features based on the above analysis\n\ndf.drop(['pan_flag','driving_flag','passport_flag'],axis=1,inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Numerical Features Analysis","metadata":{}},{"cell_type":"markdown","source":"## Disbursed Amount","metadata":{}},{"cell_type":"code","source":"df['disbursed_amount'].describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"disbursed_amount_non_default = df[df['loan_default']==0]['disbursed_amount']\ndisbursed_amount_default = df[df['loan_default']==1]['disbursed_amount']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame([disbursed_amount_non_default.describe(), disbursed_amount_default.describe()], index=['non_defaulters','defaulters'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The maximum disbursed amount is way higher for non-defaulters.","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\nplt.figure(figsize=(15,8))\n\nplt.subplot(1,2,1)\nsns.distplot(df['disbursed_amount'])\n\nplt.subplot(1,2,2)\nsns.boxplot(df['disbursed_amount'])\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The distribution is highly right skewed and there are extreme values","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(15,6))\nsns.violinplot(x ='loan_default',y='disbursed_amount',data=df)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"More outliers/extreme values are present for non defaulters","metadata":{}},{"cell_type":"markdown","source":"## Asset Cost","metadata":{}},{"cell_type":"code","source":"df['asset_cost'].describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"asset_cost_non_default = df[df['loan_default']==0]['asset_cost']\nasset_cost_default = df[df['loan_default']==1]['asset_cost']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame([asset_cost_non_default.describe(), asset_cost_default.describe()], index=['non_defaulters','defaulters'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The maximum asset cost of non defaulters is way higher than that of defaulters","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(15,8))\n\nplt.subplot(1,2,1)\nsns.distplot(df['asset_cost'])\n\nplt.subplot(1,2,2)\nsns.boxplot(df['asset_cost'])\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The distribution is highly right skewed and there are extreme values.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(15,6))\nsns.violinplot(x ='loan_default',y='asset_cost',data=df)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Extreme/outlier values of asset_cost are present among non defaulters","metadata":{}},{"cell_type":"markdown","source":"## Ltv","metadata":{}},{"cell_type":"code","source":"df['ltv'].describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ltv_non_default = df[df['loan_default']==0]['ltv']\nltv_default = df[df['loan_default']==1]['ltv']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame([ltv_non_default.describe(), ltv_default.describe()], index=['non_defaulters','defaulters'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The ltv is almost same for both defaulters and non defaulters","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(15,8))\n\nplt.subplot(1,2,1)\nsns.distplot(df['ltv'])\n\nplt.subplot(1,2,2)\nsns.boxplot(df['ltv'])\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(15,6))\nsns.violinplot(x ='loan_default',y='ltv',data=df)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Age","metadata":{}},{"cell_type":"code","source":"# We have DOB of the customer and the date of disbursal, from which we need to calculate the age of the customer at the\n# time of loan disbursal\n\ndef age(dob):\n    yr = int(dob.split('-')[2])\n    if yr >=0 and yr < 21:\n        return yr + 2000\n    else:\n         return yr + 1900\n        \ndf['date_of_birth'] = df['date_of_birth'].apply(age)\ndf['disbursaldate'] = df['disbursaldate'].apply(age)\n# Age of the customer at the time of disbursement of fund\ndf['age'] = df['disbursaldate'] - df['date_of_birth']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Dropping the DOB and Disbursal Date\ndf.drop(['date_of_birth','disbursaldate'],axis=1,inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['age'].describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"age_non_defaulters = df[df['loan_default'] == 0]['age']\nage_defaulters = df[df['loan_default'] == 1]['age']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame([age_non_defaulters.describe(), age_defaulters.describe()], index=['non_defaulters','defaulters'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"All the stats of age are almost same for defaulters and non defaulters","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(15,8))\n\nplt.subplot(1,2,1)\nsns.distplot(df['age'])\n\nplt.subplot(1,2,2)\nsns.boxplot(df['age'])\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(15,6))\nsns.boxplot(x ='loan_default',y='age',data=df)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The Age at the time of loan disbursement is almost similar among defaulters and non defaulters","metadata":{}},{"cell_type":"markdown","source":"## Perform CNS score / CIBIL score","metadata":{}},{"cell_type":"code","source":"df['perform_cns_score'].describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cibil_non_default = df[df['loan_default']==0]['perform_cns_score']\ncibil_default = df[df['loan_default']==1]['perform_cns_score']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame([cibil_non_default.describe(), cibil_default.describe()], index=['non_defaulters','defaulters'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we can observe a difference in the mean and median cibil scores among the defaulters and non defaulters. The mean and median cibil scores are higher for non defaulters.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(15,6))\n\nplt.subplot(1,2,1)\nsns.distplot(df['perform_cns_score'])\n\nplt.subplot(1,2,2)\nsns.boxplot(df['perform_cns_score'])\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(15,6))\n\nsns.distplot(cibil_non_default, color='blue', label = 'Non Defaulter')\nsns.distplot(cibil_default, color='red', label = 'Defaulter')\n\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"CIBIL score distribution is looking almost similar for defaulters and non defaulters","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(15,6))\nsns.boxplot(x ='loan_default',y='perform_cns_score',data=df)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that the mean and median cibil scores of non defaulters is sligtly higher than that of defaulters. Also the 75th percentile value of cibil score is higher for defaulters. The max score is nearly same for defaulters and non defaulters","metadata":{}},{"cell_type":"code","source":"sns.scatterplot(df['age'], df['perform_cns_score'])\n\nplt.show()\n\n# Here we can see that irrespective of age CIBIL score variation is same ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Perform cns score description","metadata":{}},{"cell_type":"code","source":"df['perform_cns_score_description'].describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['perform_cns_score_description'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cnsd = pd.crosstab(index=df['perform_cns_score_description'], columns=df['loan_default'])\ncnsd['Percent of Defaulters'] = (cnsd[1] / (cnsd[0] + cnsd[1]))*100\ncnsd","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.groupby(by='perform_cns_score_description')['perform_cns_score'].agg([min,max]).sort_values(by='min')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that as the risk increases, the percent of default also increases","metadata":{}},{"cell_type":"code","source":"# grouping all same risk into one and creating a new feature with only six classes\n# e.g A-Very Low Risk to Very Low Risk\n#     B-Very Low Risk to Very Low Risk\n\nrisk = []\nfor i in df['perform_cns_score_description']:\n    if('Very Low' in i):\n        risk.append('Very Low Risk')\n    elif('Low' in i):\n        risk.append('Low Risk')\n    elif('Medium' in i):\n        risk.append('Medium Risk')\n    elif('Very High' in i):\n        risk.append('Very High Risk')\n    elif('High' in i):\n        risk.append('High Risk')\n    else:\n        risk.append('Not Scored')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['risk'] = risk","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"risk_counts = pd.Series(risk).value_counts().sort_values()\nrisk_counts","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.barh(y = risk_counts.index,width=risk_counts.values)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"risk_counts = pd.crosstab(index=df['risk'], columns=df['loan_default'])\n\nrisk_counts['Percent_of_default'] = round((risk_counts[1]/risk_counts.sum(axis=1))*100,2)\n\nrisk_counts.sort_values(by='Percent_of_default',ascending=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.groupby(by='risk')['perform_cns_score'].agg([min,max]).sort_values(by='min')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"risk_map = {'Not Scored':-1, \n            'Very Low Risk':4,\n            'Low Risk':3,\n            'Medium Risk':2, \n            'High Risk':1,\n            'Very High Risk':0}\n\ndf['risk'] = df['risk'].map(risk_map)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"risk_counts = pd.crosstab(index=df['risk'], columns=df['loan_default'])\nrisk_counts['Percent of Defaluters'] = (risk_counts[1] / (risk_counts[0] + risk_counts[1]))*100\nrisk_counts.sort_values(by='Percent of Defaluters', ascending=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that the percent of defaulters are less for low risk and very low risk categories.","metadata":{}},{"cell_type":"code","source":"pd.crosstab(index=df['risk'], columns=df['loan_default']).plot(kind='bar')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.drop('perform_cns_score_description',axis=1,inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Average Account Age and Credit History Length","metadata":{}},{"cell_type":"code","source":"# We have 2 Columns named \"AVERAGE_ACCT_AGE\" & \"CREDIT_HISTORY_LENGTH\".\n# They have AplhaNumeric Values,changing them to Months\n\ndef duration(dur):\n    yrs = int(dur.split(' ')[0].replace('yrs',''))\n    mon = int(dur.split(' ')[1].replace('mon',''))\n    return yrs*12+mon","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['credit_history_length'] = df['credit_history_length'].apply(duration)\ndf['average_acct_age'] = df['average_acct_age'].apply(duration)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['average_acct_age'].describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"acct_age_non_defaulters = df[df['loan_default'] == 0]['average_acct_age']\nacct_age_defaulters = df[df['loan_default'] == 1]['average_acct_age']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame([acct_age_non_defaulters.describe(), acct_age_defaulters.describe()], index=['non_defaulters','defaulters'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The maximum average account age is higher for non defaulters","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(15,6))\n\nplt.subplot(1,2,1)\nsns.distplot(df['average_acct_age'])\n\nplt.subplot(1,2,2)\nsns.boxplot(df['average_acct_age'])\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(15,6))\nsns.boxplot(x ='loan_default',y='average_acct_age',data=df)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It is highly right skewed","metadata":{}},{"cell_type":"markdown","source":"## Credit History Length","metadata":{}},{"cell_type":"code","source":"df['credit_history_length'].describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"credit_non_default = df[df['loan_default'] == 0]['credit_history_length']\ncredit_default = df[df['loan_default'] == 1]['credit_history_length']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame([credit_non_default.describe(), credit_default.describe()], index=['non_defaulters','defaulters'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The mean and std is slightly higher for non defaulters","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(15,6))\n\nplt.subplot(1,2,1)\nsns.distplot(df['credit_history_length'])\n\nplt.subplot(1,2,2)\nsns.boxplot(df['credit_history_length'])\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Highly right skewed","metadata":{}},{"cell_type":"markdown","source":"## New accounts in last six months","metadata":{}},{"cell_type":"code","source":"counts = df['new_accts_in_last_six_months'].value_counts()\npercent = df['new_accts_in_last_six_months'].value_counts(normalize=True)*100\n\npd.DataFrame({'counts':counts,'percent_of_data':percent})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Most of them have not opened any new account in the last 6 months","metadata":{}},{"cell_type":"markdown","source":"## Delinquent Accounts in last six months","metadata":{}},{"cell_type":"code","source":"counts = df['delinquent_accts_in_last_six_months'].value_counts()\npercent = df['delinquent_accts_in_last_six_months'].value_counts(normalize=True)*100\n\npd.DataFrame({'counts':counts,'percent_of_data':percent})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that 92% of customers have not defaulted loans in last six months. 8% of customers have deafulted loans for >= 1 time","metadata":{}},{"cell_type":"markdown","source":"## No of Inquiries","metadata":{}},{"cell_type":"code","source":"counts = df['no_of_inquiries'].value_counts()\npercent = df['no_of_inquiries'].value_counts(normalize=True)*100\n\npd.DataFrame({'counts':counts,'percent_of_data':percent})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Most of the customers have not made any enquiries regarding loans","metadata":{}},{"cell_type":"code","source":"no_inquiries = pd.crosstab(index=df['no_of_inquiries'], columns=df['loan_default'])\nno_inquiries['pct_default'] = (no_inquiries[1]/no_inquiries.sum(axis=1))*100\nno_inquiries","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here, except for few cases, as the number of enquires increase, there is an increase in the pct of default.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(15,8))\nplt.bar(no_inquiries.index,no_inquiries['pct_default'])\nplt.xticks(no_inquiries.index)\nplt.xlabel('No of Enquires')\nplt.ylabel('Percent of default')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Feature importances of different numerical features**","metadata":{}},{"cell_type":"code","source":"df2 = df[['disbursed_amount','asset_cost', 'ltv','perform_cns_score', 'pri_no_of_accts', 'pri_active_accts',\n       'pri_overdue_accts', 'pri_current_balance', 'pri_sanctioned_amount',\n       'pri_disbursed_amount', 'sec_no_of_accts', 'sec_active_accts',\n       'sec_overdue_accts', 'sec_current_balance', 'sec_sanctioned_amount',\n       'sec_disbursed_amount', 'primary_instal_amt', 'sec_instal_amt',\n       'new_accts_in_last_six_months', 'delinquent_accts_in_last_six_months',\n       'average_acct_age', 'credit_history_length', 'no_of_inquiries',\n       'age', 'risk']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import ExtraTreesClassifier\nimport matplotlib.pyplot as plt\nmodel=ExtraTreesClassifier()\nmodel.fit(df2,df['loan_default'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(model.feature_importances_,index=df2.columns,columns=['Feature_Importances']).sort_values(by='Feature_Importances',ascending=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(8,8))\nranked_features=pd.Series(model.feature_importances_,index=df2.columns)\nranked_features.nlargest(25).plot(kind='barh')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We cannot drop the secondary account details as they are asked by the institutions before granting loan.","metadata":{}},{"cell_type":"code","source":"# Checking the correlation between primary and secondary accounts\nplt.figure(figsize=(12,8))\nsns.heatmap(df[['pri_no_of_accts','pri_active_accts','pri_overdue_accts','pri_current_balance','pri_sanctioned_amount',\n               'pri_disbursed_amount','primary_instal_amt','sec_no_of_accts','sec_active_accts','sec_overdue_accts',\n               'sec_current_balance','sec_sanctioned_amount','sec_disbursed_amount','sec_instal_amt']].corr(),annot=True)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is no correlation between primary and secondary accounts","metadata":{}},{"cell_type":"markdown","source":"## Primary and Secondary Accounts","metadata":{}},{"cell_type":"code","source":"# Combining the Primary and Secondary Accounts\n\ndf['no_of_accts'] = df['pri_no_of_accts'] + df['sec_no_of_accts']\ndf['active_accts'] = df['pri_active_accts'] + df['sec_active_accts']\ndf['overdue_accts'] = df['pri_overdue_accts'] + df['sec_overdue_accts']\ndf['outstanding_amount'] = df['pri_current_balance'] + df['sec_current_balance']\ndf['sanctioned_amount'] = df['pri_sanctioned_amount'] + df['sec_sanctioned_amount']\ndf['psdisbursed_amount'] = df['pri_disbursed_amount'] + df['sec_disbursed_amount']\ndf['install_amt'] = df['primary_instal_amt'] + df['sec_instal_amt']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.drop(['pri_no_of_accts','sec_no_of_accts','pri_active_accts','sec_active_accts',\n        'pri_overdue_accts','sec_overdue_accts','pri_current_balance','sec_current_balance',\n        'pri_sanctioned_amount','sec_sanctioned_amount','pri_disbursed_amount','sec_disbursed_amount',\n        'primary_instal_amt','sec_instal_amt'],axis=1,inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Account and amount description\n\ndf[['no_of_accts','active_accts','overdue_accts','outstanding_amount','sanctioned_amount','psdisbursed_amount','install_amt']].describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Total number of accounts","metadata":{}},{"cell_type":"code","source":"df['no_of_accts'].describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"na_non_default = df[df['loan_default']==0]['no_of_accts']\nna_default = df[df['loan_default']==1]['no_of_accts']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame([na_non_default.describe(), na_default.describe()], index=['non_defaulters','defaulters'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Active Accounts","metadata":{}},{"cell_type":"code","source":"counts = df['active_accts'].value_counts()\npercent = df['active_accts'].value_counts(normalize=True)*100\n\npd.DataFrame({'counts':counts,'percent_of_data':percent})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are over 50 percent inactive accounts. Around 18 percent have 1 active account present","metadata":{}},{"cell_type":"markdown","source":"## Overdue Accounts","metadata":{}},{"cell_type":"code","source":"counts = df['overdue_accts'].value_counts()\npercent = df['overdue_accts'].value_counts(normalize=True)*100\n\npd.DataFrame({'counts':counts,'percent_of_data':percent})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Most of the accounts are not overdue. Around 9 percent of data contain 1 overdue account, and around 2 percent of data contain 2 overdue accounts","metadata":{}},{"cell_type":"code","source":"no_inquiries = pd.crosstab(index=df['overdue_accts'], columns=df['loan_default'])\nno_inquiries['pct_default'] = (no_inquiries[1]/no_inquiries.sum(axis=1))*100\nno_inquiries","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Upto 5 overdue accounts, we can see that as the number of overdue accounts increase, the percentage of default also increase. However we do not observe the same pattern/any pattern beyond 5 overdue accounts","metadata":{}},{"cell_type":"markdown","source":"## Outstanding Amount","metadata":{}},{"cell_type":"code","source":"counts = df['outstanding_amount'].value_counts()\npercent = df['outstanding_amount'].value_counts(normalize=True)*100\n\npd.DataFrame({'counts':counts,'percent_of_data':percent})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Around 60 percent of data do not have any outstanding amount","metadata":{}},{"cell_type":"markdown","source":"## Sanctioned Amount","metadata":{}},{"cell_type":"code","source":"counts = df['sanctioned_amount'].value_counts()\npercent = df['sanctioned_amount'].value_counts(normalize=True)*100\n\npd.DataFrame({'counts':counts,'percent_of_data':percent})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For around 58 percent of the accounts, no amount was sanctioned for all the loans at the time of disbursement","metadata":{}},{"cell_type":"markdown","source":"## Psdisbursed Amount","metadata":{}},{"cell_type":"code","source":"counts = df['psdisbursed_amount'].value_counts()\npercent = df['psdisbursed_amount'].value_counts(normalize=True)*100\n\npd.DataFrame({'counts':counts,'percent_of_data':percent})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For 58 percent of all accounts, no amount was disbursed for all the loans at the time of disbursement","metadata":{}},{"cell_type":"markdown","source":"## Instalment Amount","metadata":{}},{"cell_type":"code","source":"counts = df['install_amt'].value_counts()\npercent = df['install_amt'].value_counts(normalize=True)*100\n\npd.DataFrame({'counts':counts,'percent_of_data':percent})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Around 68 percent of data do not have any installment amount to pay","metadata":{}},{"cell_type":"code","source":"df3 = df[['disbursed_amount', 'asset_cost', 'ltv','perform_cns_score', 'new_accts_in_last_six_months',\n       'delinquent_accts_in_last_six_months', 'average_acct_age',\n       'credit_history_length', 'no_of_inquiries',\n       'age', 'risk', 'no_of_accts', 'active_accts',\n       'overdue_accts', 'outstanding_amount', 'sanctioned_amount',\n       'psdisbursed_amount', 'install_amt']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(20,15))\nsns.heatmap(df3.corr(),annot=True,cmap='Blues')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the above correlation heatmap, we can see that some of the features are highly correlated(>0.75) with each other.\n* --- disbursed amount and asset cost - 0.75\n* --- perform_cns.score and risk - 0.98\n* --- average_acct_age and credit_history_length - 0.83\n* --- no_of_accts and active_accts - 0.76\n* --- sanctioned_amount and psdisbursed_amount - 1","metadata":{}},{"cell_type":"markdown","source":"**Feature importances of above numerical features**","metadata":{}},{"cell_type":"code","source":"# Feature importance using Extra Trees classifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nimport matplotlib.pyplot as plt\nmodel=ExtraTreesClassifier()\nmodel.fit(df3,df['loan_default'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(model.feature_importances_,index=df3.columns,columns=['Feature_Importances']).sort_values(by='Feature_Importances',ascending=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(8,8))\nranked_features=pd.Series(model.feature_importances_,index=df3.columns)\nranked_features.nlargest(18).plot(kind='barh')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.drop(['asset_cost','perform_cns_score','average_acct_age','no_of_accts','psdisbursed_amount','delinquent_accts_in_last_six_months'],axis=1,inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Checking and Handling outliers","metadata":{}},{"cell_type":"code","source":"df12 = df[['disbursed_amount', 'ltv',\n       'new_accts_in_last_six_months',\n       'credit_history_length', 'no_of_inquiries',\n       'age', 'active_accts', 'overdue_accts',\n       'outstanding_amount', 'sanctioned_amount', 'install_amt']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(20,20))\nx = 1\nfor column in df12.columns:\n    if x<=11:\n        plt.subplot(5,3,x)\n        sns.boxplot(df[column])\n    x+=1\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.boxplot(df['disbursed_amount'])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#calculating 0-100th percentile to find a the correct percentile value for removal of outliers\nfor i in range(0,100,10):\n    var = df['disbursed_amount'].values\n    var = np.sort(var,axis = None)\n    print(\"{} percentile value is {}\".format(i,var[int(len(var)*(float(i)/100))]))\nprint (\"100 percentile value is \",var[-1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#looking further from the 99th percecntile\nfor i in range(90,100):\n    var = df['disbursed_amount'].values\n    var = np.sort(var,axis = None)\n    print(\"{} percentile value is {}\".format(i,var[int(len(var)*(float(i)/100))]))\nprint (\"100 percentile value is \",var[-1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df[df['disbursed_amount']<max(df['disbursed_amount'])]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df[df['new_accts_in_last_six_months']<25]\ndf = df[df['credit_history_length']<400]\ndf = df[df['no_of_inquiries']<25]\ndf = df[df['active_accts']<50]\ndf = df[df['sanctioned_amount']<max(df['sanctioned_amount'])]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.boxplot(df['outstanding_amount'])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#calculating 0-100th percentile to find a the correct percentile value for removal of outliers\nfor i in range(0,100,10):\n    var = df['outstanding_amount'].values\n    var = np.sort(var,axis = None)\n    print(\"{} percentile value is {}\".format(i,var[int(len(var)*(float(i)/100))]))\nprint (\"100 percentile value is \",var[-1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#looking further from the 99th percecntile\nfor i in range(90,100):\n    var = df['outstanding_amount'].values\n    var = np.sort(var,axis = None)\n    print(\"{} percentile value is {}\".format(i,var[int(len(var)*(float(i)/100))]))\nprint (\"100 percentile value is \",var[-1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df[(df['outstanding_amount']>-6678296) & (df['outstanding_amount']<75603400)]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.boxplot(df['install_amt'])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#calculating 0-100th percentile to find a the correct percentile value for removal of outliers\nfor i in range(0,100,10):\n    var = df['install_amt'].values\n    var = np.sort(var,axis = None)\n    print(\"{} percentile value is {}\".format(i,var[int(len(var)*(float(i)/100))]))\nprint (\"100 percentile value is \",var[-1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#looking further from the 99th percecntile\nfor i in range(90,100):\n    var = df['install_amt'].values\n    var = np.sort(var,axis = None)\n    print(\"{} percentile value is {}\".format(i,var[int(len(var)*(float(i)/100))]))\nprint (\"100 percentile value is \",var[-1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df[df['install_amt']<10000000]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking the distributions again\n\nplt.figure(figsize=(20,20))\nx = 1\nfor column in df12.columns:\n    if x<=13:\n        plt.subplot(5,3,x)\n        sns.boxplot(df[column])\n    x+=1\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Removing few more outliers/extreme values\n\ndf = df[df['disbursed_amount']<250000]\ndf = df[df['outstanding_amount']<40000000]\ndf = df[df['sanctioned_amount']<0.800000e+08]\ndf = df[df['install_amt']<=5.000000e+06]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(20,20))\nx = 1\nfor column in df12.columns:\n    if x<=11:\n        plt.subplot(5,3,x)\n        sns.boxplot(df[column])\n    x+=1\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Transforming the features outstanding amount, sanctioned amount and install_amt\ndf['log_outstanding_amount'] = np.log(df['outstanding_amount']+1-min(df['outstanding_amount']))\nsns.boxplot(df['log_outstanding_amount'])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['log_outstanding_amount'] = df[df['log_outstanding_amount']>12]\nsns.boxplot(df['log_outstanding_amount'])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['log_outstanding_amount'].describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['log_outstanding_amount'] = df.fillna(df['log_outstanding_amount'].median())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['log_sanctioned_amount'] = np.log(df['sanctioned_amount']+1)\nsns.boxplot(df['log_sanctioned_amount'])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['log_install_amt'] = np.log(df['install_amt']+1)\nsns.boxplot(df['log_install_amt'])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.drop(['outstanding_amount','sanctioned_amount','install_amt'],axis=1,inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Pairplot**","metadata":{}},{"cell_type":"code","source":"n = df.shape[0]\nsns.pairplot(df[['disbursed_amount', 'ltv','new_accts_in_last_six_months', \n                 'loan_default']][0:n], hue='loan_default', \n             vars=['disbursed_amount', 'ltv','new_accts_in_last_six_months'])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n = df.shape[0]\nsns.pairplot(df[['credit_history_length', 'no_of_inquiries',\n       'self_employed', 'age', 'risk','loan_default']][0:n], hue='loan_default', \n             vars=['credit_history_length', 'no_of_inquiries',\n       'self_employed', 'age', 'risk'])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n = df.shape[0]\nsns.pairplot(df[['active_accts', 'overdue_accts',\n       'log_outstanding_amount', 'log_sanctioned_amount', 'log_install_amt','loan_default']][0:n], hue='loan_default', \n             vars=['active_accts', 'overdue_accts',\n       'log_outstanding_amount', 'log_sanctioned_amount', 'log_install_amt'])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12,8))\nsns.heatmap(df[['disbursed_amount', 'ltv','new_accts_in_last_six_months','credit_history_length','no_of_inquiries',\n               'age','risk','active_accts', 'overdue_accts', 'log_outstanding_amount','log_sanctioned_amount', \n                'log_install_amt']].corr(),annot=True)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is a correlation of 1 between disbursed amount and log_outstanding_amount. Hence removing the log_outstanding_amount feature after comparing the feature importances","metadata":{}},{"cell_type":"code","source":"df.drop('log_outstanding_amount',axis=1,inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modelling","metadata":{}},{"cell_type":"markdown","source":"## Logistic Regression","metadata":{}},{"cell_type":"code","source":"y = df['loan_default']\nX = df.drop('loan_default',axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = list(y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Scaling\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nXscaled = sc.fit_transform(X)\nXscaled = pd.DataFrame(Xscaled,columns=X.columns)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import statsmodels.api as sm\nXc = sm.add_constant(Xscaled)\nmodel = sm.Logit(y, Xc).fit()\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix,roc_auc_score,log_loss,roc_curve,accuracy_score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = model.predict(Xc)\nprob = pd.DataFrame(y_pred, columns=['probability'])\nprob['loan_default'] = y\nprob['y_est'] = prob['probability'].apply(lambda x: 0 if x<0.5 else 1)\nprob.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Confusion matrix\nconfusion_matrix(prob['loan_default'], prob['y_est'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# AUC score\nroc_auc_score(prob['loan_default'],prob['probability'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking for multicollinearity\n\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor as vif\nvf = [vif(Xscaled.values,i) for i in range(X.shape[1])]\npd.DataFrame(vf,index=X.columns,columns=['vif'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Building sklearn Linear Regression model\ny = df['loan_default']\nX = df.drop('loan_default',axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Scaling\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_scaled = sc.fit_transform(X)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X_scaled,y,test_size=0.3,random_state=120)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression(solver='liblinear',random_state=42)\nlr.fit(X_train,y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train_pred = lr.predict(X_train)\ny_test_pred = lr.predict(X_test)\ny_train_prob = lr.predict_proba(X_train)\ny_test_prob = lr.predict_proba(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('The train AUC score is:',roc_auc_score(y_train,y_train_prob[:,1]))\nprint('The test AUC score is:',roc_auc_score(y_test,y_test_prob[:,1]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fpr, tpr, thresholds = roc_curve(y_test, y_test_prob[:,1])\nplt.plot(fpr,fpr)\nplt.plot(fpr,tpr)\nplt.grid()\nplt.title('Test ROC curve')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"confusion_matrix(y_test,y_test_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.heatmap(confusion_matrix(y_test,y_test_pred),annot=True)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# FNs are too high, TPs are too low. Maybe Applying SMOTE and balancing the data might help.\nfrom sklearn.metrics import classification_report\nprint('Test Classification Report\\n')\nprint(classification_report(y_test,y_test_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# For 1s the f1 score is really low\n\n# Before computing the binary log loss, we need to perform caliberation\n\n# https://machinelearningmastery.com/calibrated-classification-model-in-scikit-learn/\n# https://scikit-learn.org/stable/modules/generated/sklearn.calibration.CalibratedClassifierCV.html\n\nfrom sklearn.calibration import CalibratedClassifierCV\nmodel_isotonic = CalibratedClassifierCV(lr, cv=3, method='isotonic')\nmodel_isotonic.fit(X_train,y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"isotonic_predict_prob_test = model_isotonic.predict_proba(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"log_loss(y_test,isotonic_predict_prob_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Using SMOTE to handle imbalance**","metadata":{}},{"cell_type":"code","source":"from imblearn.over_sampling import SMOTE\nsmote = SMOTE()\nX_train_sm,y_train_sm = smote.fit_resample(X_train,y_train)\nX_train_sm.shape, y_train_sm.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr1 = LogisticRegression(solver='liblinear',random_state=42)\nlr1.fit(X_train_sm,y_train_sm)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train_pred = lr1.predict(X_train_sm)\ny_test_pred = lr1.predict(X_test)\ny_train_prob = lr1.predict_proba(X_train_sm)\ny_test_prob = lr1.predict_proba(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('The train AUC score is:',roc_auc_score(y_train_sm,y_train_prob[:,1]))\nprint('The test AUC score is:',roc_auc_score(y_test,y_test_prob[:,1]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fpr, tpr, thresholds = roc_curve(y_test, y_test_prob[:,1])\nplt.plot(fpr,fpr)\nplt.plot(fpr,tpr)\nplt.grid()\nplt.title('Test ROC curve')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"confusion_matrix(y_test,y_test_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.heatmap(confusion_matrix(y_test,y_test_pred),annot=True)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report\nprint('Test Classification Report\\n')\nprint(classification_report(y_test,y_test_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.calibration import CalibratedClassifierCV\nmodel_isotonic = CalibratedClassifierCV(lr1, cv=3, method='isotonic')\nmodel_isotonic.fit(X_train_sm,y_train_sm)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"isotonic_predict_prob_test = model_isotonic.predict_proba(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"log_loss(y_test,isotonic_predict_prob_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Random Forest Classifier","metadata":{}},{"cell_type":"markdown","source":"### Modelling without SMOTE","metadata":{}},{"cell_type":"markdown","source":"Hyperparameter tuning is done using Random Search CV and best parameters are obtained and used for the modelling","metadata":{}},{"cell_type":"code","source":"rsearch1_best_params = {'max_depth': 13,\n 'min_samples_leaf': 10,\n 'min_samples_split': 11,\n 'n_estimators': 374}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nrfc1 = RandomForestClassifier(**rsearch1_best_params, random_state=300)\nrfc1.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train_pred = rfc1.predict(X_train)\ny_test_pred = rfc1.predict(X_test)\ny_train_prob = rfc1.predict_proba(X_train)\ny_test_prob = rfc1.predict_proba(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('The train AUC score is:',roc_auc_score(y_train,y_train_prob[:,1]))\nprint('The test AUC score is:',roc_auc_score(y_test,y_test_prob[:,1]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fpr, tpr, thresholds = roc_curve(y_test, y_test_prob[:,1])\nplt.plot(fpr,fpr)\nplt.plot(fpr,tpr)\nplt.grid()\nplt.title('Test ROC curve')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"confusion_matrix(y_test,y_test_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.heatmap(confusion_matrix(y_test,y_test_pred),annot=True)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report\nprint('Test Classification Report\\n')\nprint(classification_report(y_test,y_test_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.calibration import CalibratedClassifierCV\nmodel_isotonic = CalibratedClassifierCV(rfc1, cv=3, method='isotonic')\nmodel_isotonic.fit(X_train,y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"isotonic_predict_prob_test = model_isotonic.predict_proba(X_test)\nlog_loss(y_test,isotonic_predict_prob_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Modelling with SMOTE","metadata":{}},{"cell_type":"code","source":"rsearch_best_params = {'max_depth': 17,\n 'min_samples_leaf': 2,\n 'min_samples_split': 4,\n 'n_estimators': 317}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rfc = RandomForestClassifier(**rsearch_best_params, random_state=300)\nrfc.fit(X_train_sm, y_train_sm)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train_pred = rfc.predict(X_train_sm)\ny_test_pred = rfc.predict(X_test)\ny_train_prob = rfc.predict_proba(X_train_sm)\ny_test_prob = rfc.predict_proba(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('The train AUC score is:',roc_auc_score(y_train_sm,y_train_prob[:,1]))\nprint('The test AUC score is:',roc_auc_score(y_test,y_test_prob[:,1]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fpr, tpr, thresholds = roc_curve(y_test, y_test_prob[:,1])\nplt.plot(fpr,fpr)\nplt.plot(fpr,tpr)\nplt.grid()\nplt.title('Test ROC curve')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"confusion_matrix(y_test,y_test_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.heatmap(confusion_matrix(y_test,y_test_pred),annot=True)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report\nprint('Test Classification Report\\n')\nprint(classification_report(y_test,y_test_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.calibration import CalibratedClassifierCV\nmodel_isotonic = CalibratedClassifierCV(rfc, cv=3, method='isotonic')\nmodel_isotonic.fit(X_train_sm,y_train_sm)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"isotonic_predict_prob_test = model_isotonic.predict_proba(X_test)\nlog_loss(y_test,isotonic_predict_prob_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## LightGBM","metadata":{}},{"cell_type":"markdown","source":"### Modelling without SMOTE","metadata":{}},{"cell_type":"code","source":"import lightgbm as lgb","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rsearch1_best_params = {'learning_rate': 0.10308835171850986,\n 'max_depth': 3,\n 'n_estimators': 275,\n 'num_leaves': 18}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lgbmc1 = lgb.LGBMClassifier(**rsearch1_best_params, importance_type='gain',random_state=300)\nlgbmc1.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train_pred = lgbmc1.predict(X_train)\ny_test_pred = lgbmc1.predict(X_test)\ny_train_prob = lgbmc1.predict_proba(X_train)\ny_test_prob = lgbmc1.predict_proba(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('The train AUC score is:',roc_auc_score(y_train,y_train_prob[:,1]))\nprint('The test AUC score is:',roc_auc_score(y_test,y_test_prob[:,1]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fpr, tpr, thresholds = roc_curve(y_test, y_test_prob[:,1])\nplt.plot(fpr,fpr)\nplt.plot(fpr,tpr)\nplt.grid()\nplt.title('Test ROC curve')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"confusion_matrix(y_test,y_test_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.heatmap(confusion_matrix(y_test,y_test_pred),annot=True)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report\nprint('Test Classification Report\\n')\nprint(classification_report(y_test,y_test_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.calibration import CalibratedClassifierCV\nmodel_isotonic = CalibratedClassifierCV(lgbmc1, cv=3, method='isotonic')\nmodel_isotonic.fit(X_train,y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"isotonic_predict_prob_test = model_isotonic.predict_proba(X_test)\nlog_loss(y_test,isotonic_predict_prob_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Modelling with SMOTE","metadata":{}},{"cell_type":"code","source":"rsearch_best_params = {'learning_rate': 0.32585614358745185,\n 'max_depth': 12,\n 'n_estimators': 540,\n 'num_leaves': 31}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lgbmc = lgb.LGBMClassifier(**rsearch_best_params, importance_type='gain',random_state=300)\nlgbmc.fit(X_train_sm, y_train_sm)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train_pred = lgbmc.predict(X_train_sm)\ny_test_pred = lgbmc.predict(X_test)\ny_train_prob = lgbmc.predict_proba(X_train_sm)\ny_test_prob = lgbmc.predict_proba(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('The train AUC score is:',roc_auc_score(y_train_sm,y_train_prob[:,1]))\nprint('The test AUC score is:',roc_auc_score(y_test,y_test_prob[:,1]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fpr, tpr, thresholds = roc_curve(y_test, y_test_prob[:,1])\nplt.plot(fpr,fpr)\nplt.plot(fpr,tpr)\nplt.grid()\nplt.title('Test ROC curve')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"confusion_matrix(y_test,y_test_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.heatmap(confusion_matrix(y_test,y_test_pred),annot=True)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report\nprint('Test Classification Report\\n')\nprint(classification_report(y_test,y_test_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.calibration import CalibratedClassifierCV\nmodel_isotonic = CalibratedClassifierCV(lgbmc, cv=3, method='isotonic')\nmodel_isotonic.fit(X_train_sm,y_train_sm)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"isotonic_predict_prob_test = model_isotonic.predict_proba(X_test)\nlog_loss(y_test,isotonic_predict_prob_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## XGBoost","metadata":{}},{"cell_type":"markdown","source":"### Modelling without SMOTE","metadata":{}},{"cell_type":"code","source":"import xgboost\nfrom xgboost import XGBClassifier\nrsearch1_best_params = {'eval_metric': 'auc',\n 'gamma': 0.3,\n 'learning_rate': 0.1,\n 'max_depth': 3,\n 'n_estimators': 270,\n 'reg_alpha': 0.01}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgbc1 = XGBClassifier(**rsearch1_best_params, random_state=300)\nxgbc1.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train_pred = xgbc1.predict(X_train)\ny_test_pred = xgbc1.predict(X_test)\ny_train_prob = xgbc1.predict_proba(X_train)\ny_test_prob = xgbc1.predict_proba(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('The train AUC score is:',roc_auc_score(y_train,y_train_prob[:,1]))\nprint('The test AUC score is:',roc_auc_score(y_test,y_test_prob[:,1]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fpr, tpr, thresholds = roc_curve(y_test, y_test_prob[:,1])\nplt.plot(fpr,fpr)\nplt.plot(fpr,tpr)\nplt.grid()\nplt.title('Test ROC curve')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"confusion_matrix(y_test,y_test_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.heatmap(confusion_matrix(y_test,y_test_pred),annot=True)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report\nprint('Test Classification Report\\n')\nprint(classification_report(y_test,y_test_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.calibration import CalibratedClassifierCV\nmodel_isotonic = CalibratedClassifierCV(xgbc1, cv=3, method='isotonic')\nmodel_isotonic.fit(X_train,y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"isotonic_predict_prob_test = model_isotonic.predict_proba(X_test)\nlog_loss(y_test,isotonic_predict_prob_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Modelling with SMOTE","metadata":{}},{"cell_type":"code","source":"rsearch_best_params = {'eval_metric': 'auc',\n 'gamma': 0.2,\n 'learning_rate': 0.2,\n 'max_depth': 9,\n 'n_estimators': 192,\n 'reg_alpha': 0.1}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgbc = XGBClassifier(**rsearch_best_params, random_state=300)\nxgbc.fit(X_train_sm, y_train_sm)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train_pred = xgbc.predict(X_train_sm)\ny_test_pred = xgbc.predict(X_test)\ny_train_prob = xgbc.predict_proba(X_train_sm)\ny_test_prob = xgbc.predict_proba(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('The train AUC score is:',roc_auc_score(y_train_sm,y_train_prob[:,1]))\nprint('The test AUC score is:',roc_auc_score(y_test,y_test_prob[:,1]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fpr, tpr, thresholds = roc_curve(y_test, y_test_prob[:,1])\nplt.plot(fpr,fpr)\nplt.plot(fpr,tpr)\nplt.grid()\nplt.title('Test ROC curve')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"confusion_matrix(y_test,y_test_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.heatmap(confusion_matrix(y_test,y_test_pred),annot=True)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report\nprint('Test Classification Report\\n')\nprint(classification_report(y_test,y_test_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.calibration import CalibratedClassifierCV\nmodel_isotonic = CalibratedClassifierCV(xgbc, cv=3, method='isotonic')\nmodel_isotonic.fit(X_train_sm,y_train_sm)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"isotonic_predict_prob_test = model_isotonic.predict_proba(X_test)\nlog_loss(y_test,isotonic_predict_prob_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Stacking","metadata":{}},{"cell_type":"markdown","source":"### Modelling without SMOTE","metadata":{}},{"cell_type":"markdown","source":"All the hyperparameters are tuned and are same as used before for individual modelling","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import StackingClassifier\nestimators = [\n('rfc',RandomForestClassifier(max_depth = 13,\n min_samples_leaf = 10,\n min_samples_split = 11,\n n_estimators = 374)),\n \n('lgbmc',lgb.LGBMClassifier(learning_rate = 0.10308835171850986,\n max_depth = 3,\n n_estimators = 275,\n num_leaves = 18)),\n \n('xgbc', XGBClassifier(eval_metric = 'auc',\n gamma = 0.3,\n learning_rate = 0.1,\n max_depth = 3,\n n_estimators = 270,\n reg_alpha = 0.01))\n]\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clf1 = StackingClassifier(estimators=estimators,final_estimator=LogisticRegression(solver='liblinear'),\n                        cv = 5, n_jobs=-1)\nclf1.fit(X_train,y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train_pred = clf1.predict(X_train)\ny_test_pred = clf1.predict(X_test)\ny_train_prob = clf1.predict_proba(X_train)\ny_test_prob = clf1.predict_proba(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('The train AUC score is:',roc_auc_score(y_train,y_train_prob[:,1]))\nprint('The test AUC score is:',roc_auc_score(y_test,y_test_prob[:,1]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fpr, tpr, thresholds = roc_curve(y_test, y_test_prob[:,1])\nplt.plot(fpr,fpr)\nplt.plot(fpr,tpr)\nplt.grid()\nplt.title('Test ROC curve')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"confusion_matrix(y_test,y_test_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.heatmap(confusion_matrix(y_test,y_test_pred),annot=True)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report\nprint('Test Classification Report\\n')\nprint(classification_report(y_test,y_test_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.calibration import CalibratedClassifierCV\nmodel_isotonic = CalibratedClassifierCV(clf1, cv=3, method='isotonic')\nmodel_isotonic.fit(X_train,y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"isotonic_predict_prob_test = model_isotonic.predict_proba(X_test)\nlog_loss(y_test,isotonic_predict_prob_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Modelling with SMOTE","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import StackingClassifier\nestimators = [\n('rfc',RandomForestClassifier(max_depth = 17,\n min_samples_leaf = 2,\n min_samples_split = 4,\n n_estimators = 317)),\n \n('lgbmc',lgb.LGBMClassifier(learning_rate = 0.32585614358745185,\n max_depth = 12,\n n_estimators = 540,\n num_leaves = 31)),\n \n('xgbc', XGBClassifier(eval_metric = 'auc',\n gamma = 0.2,\n learning_rate = 0.2,\n max_depth = 9,\n n_estimators = 192,\n reg_alpha = 0.1))\n]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clf = StackingClassifier(estimators=estimators,final_estimator=LogisticRegression(solver='liblinear'),\n                        cv = 5, n_jobs=-1)\nclf.fit(X_train_sm,y_train_sm)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train_pred = clf.predict(X_train_sm)\ny_test_pred = clf.predict(X_test)\ny_train_prob = clf.predict_proba(X_train_sm)\ny_test_prob = clf.predict_proba(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('The train AUC score is:',roc_auc_score(y_train_sm,y_train_prob[:,1]))\nprint('The test AUC score is:',roc_auc_score(y_test,y_test_prob[:,1]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fpr, tpr, thresholds = roc_curve(y_test, y_test_prob[:,1])\nplt.plot(fpr,fpr)\nplt.plot(fpr,tpr)\nplt.grid()\nplt.title('Test ROC curve')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"confusion_matrix(y_test,y_test_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.heatmap(confusion_matrix(y_test,y_test_pred),annot=True)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report\nprint('Test Classification Report\\n')\nprint(classification_report(y_test,y_test_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.calibration import CalibratedClassifierCV\nmodel_isotonic = CalibratedClassifierCV(clf, cv=3, method='isotonic')\nmodel_isotonic.fit(X_train_sm,y_train_sm)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"isotonic_predict_prob_test = model_isotonic.predict_proba(X_test)\nlog_loss(y_test,isotonic_predict_prob_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Summary stats of all Models","metadata":{}},{"cell_type":"code","source":"# http://zetcode.com/python/prettytable/\nfrom prettytable import PrettyTable\n\nx = PrettyTable()\nx.field_names = [\"Model\",\"Train-AUC\",\"Test-AUC\",\"Test-Binary_Log_Loss\",\"F1-Score(1)\",\"SMOTE-applied\"]\n\nx.add_row([\"Logistic Regression\", 0.623, 0.623, 0.509, 0.01,'No'])\nx.add_row([\"Logistic Regression\", 0.624, 0.623, 0.670, 0.39,'Yes'])\nx.add_row([\"Random Forest Classifier\", 0.727, 0.637, 0.503, 0.00,'No'])\nx.add_row([\"Random Forest Classifier\", 0.855, 0.625, 0.614, 0.38,'Yes'])\nx.add_row([\"LightGBM Classifier\", 0.652, 0.639, 0.503, 0.01,'No'])\nx.add_row([\"LightGBM Classifier\", 0.928, 0.601, 0.527, 0.22,'Yes'])\nx.add_row([\"XGBoost Classifier\", 0.652, 0.638, 0.503, 0.01,'No'])\nx.add_row([\"XGBoost Classifier\", 0.936, 0.610, 0.523, 0.20,'Yes'])\nx.add_row([\"Stacked Classifier\", 0.690, 0.639, 0.503, 0.03,'No'])\nx.add_row([\"Stacked Classifier\", 0.936, 0.618, 0.523, 0.29,'Yes'])\n\n\nprint(x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Inference:\nThe given problem statement requires us to determine the probability of loanee/borrower defaulting on a vehicle loan in the first EMI (Equated Monthly Instalments) on the due date. Hence along with the prediction of whether a person is a defautee/not a defaultee, we also need to predict the probability that a person might default the loan.\n\nHence to measure the performance of models, we have taken AUC-score, F1-score of 1's and Binary Log Loss as the performance metrics.\n\nWithout application of SMOTE, all the models are giving way less F1-score(1s). By the application of SMOTE this issue is cleared(Though the f1-scores can be controlled by selecting the appropriate threshold from the ROC curve).\n\nBy looking at the above table containing the performance metrics of various models, we can clearly say that Logistic Regression with SMOTE is performing really well, as compared to other models. It is giving a good AUC scores(not overfitting), and best F1-Score(1). Though the Binary log loss is a bit higher when compared to other models.\n\nThe next best model is Random Forest Classifier with SMOTE. Compared to Logistic Regression, it is overfitting a seen from AUC scores. However, it also shows good F1-score(1), slightly lower than Logistic Regression. It has a better(lower) binary log loss, when compared to Logistic Regression.\n\nApart from these, we have used LightGBM Classifier, XGBoost Classifier and a Stacked Classifier and their performance metrics are displayed in the pretty table above.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}