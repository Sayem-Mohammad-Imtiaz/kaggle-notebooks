{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport random\nfrom sklearn.utils import shuffle\nimport pandas as pd\nimport pickle\nfrom matplotlib.pyplot import MultipleLocator","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 来源：中科院硕士论文-闻博\n# 在自己的数据集上复现其模型","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')\ntf.config.experimental_connect_to_cluster(resolver)\n# This is the TPU initialization code that has to be at the beginning.\ntf.tpu.experimental.initialize_tpu_system(resolver)\nprint(\"All devices: \", tf.config.list_logical_devices('TPU'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"strategy = tf.distribute.TPUStrategy(resolver)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = pd.read_csv(r'../input/blood-pressure-datasets/Train_Merge_Data.csv')\nvalidation_data = pd.read_csv(r'../input/blood-pressure-datasets/Validation_Merge_Data.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = shuffle(train_data)\nvalidation_data = shuffle(validation_data)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_label_Sbp = train_data.iloc[:,610]\ntrain_label_Sbp = train_label_Sbp.values\n\ntrain_label_Dbp = train_data.iloc[:,611]\ntrain_label_Dbp = train_label_Dbp.values\n\ntrain_data = train_data.iloc[:,:600]\ntrain_data = train_data.values","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"validation_label_Sbp = validation_data.iloc[:,610]\nvalidation_label_Sbp = validation_label_Sbp.values\n\nvalidation_label_Dbp = validation_data.iloc[:,611]\nvalidation_label_Dbp = validation_label_Dbp.values\n\nvalidation_data = validation_data.iloc[:,:600]\nvalidation_data = validation_data.values","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"train_data_information:\")\nprint(train_data.shape)\nprint(train_label_Sbp.shape)\nprint(train_label_Dbp.shape)\nprint(\"validation_data_information:\")\nprint(validation_data.shape)\nprint(validation_label_Sbp.shape)\nprint(validation_label_Dbp.shape)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x=32\ndef create_model():\n    inputs = keras.Input(shape=(600,1))\n    #1\n    conv1 = layers.Conv1D(x*2,3,padding='same')(inputs)\n    conv1 = layers.BatchNormalization()(conv1)\n    conv1 = layers.Activation(tf.nn.relu)(conv1)\n    \n    conv1 = layers.Conv1D(x*2,3,padding='same')(conv1)\n    conv1 = layers.BatchNormalization()(conv1)\n    conv1 = layers.Activation(tf.nn.relu)(conv1)\n    \n    pool1 = layers.MaxPooling1D(strides=3)(conv1)\n    #2\n    conv2 = layers.Conv1D(x*4,3,padding='same')(pool1)\n    conv2 = layers.BatchNormalization()(conv2)\n    conv2 = layers.Activation(tf.nn.relu)(conv2)\n\n    conv2 = layers.Conv1D(x*4,3,padding='same')(conv2)\n    conv2 = layers.BatchNormalization()(conv2)\n    conv2 = layers.Activation(tf.nn.relu)(conv2)\n\n    pool2 = layers.MaxPooling1D(strides=3)(conv2)\n    \n    #3\n    conv3 = layers.Conv1D(x*8,3,padding='same')(pool2)\n    conv3 = layers.BatchNormalization()(conv3)\n    conv3 = layers.Activation(tf.nn.relu)(conv3)\n\n    conv3 = layers.Conv1D(x*8,3,padding='same')(conv3)\n    conv3 = layers.BatchNormalization()(conv3)\n    conv3 = layers.Activation(tf.nn.relu)(conv3)\n    \n    conv3 = layers.Conv1D(x*8,3,padding='same')(conv3)\n    conv3 = layers.BatchNormalization()(conv3)\n    conv3 = layers.Activation(tf.nn.relu)(conv3)\n    \n    pool3 = layers.MaxPooling1D(strides=3)(conv3)\n    \n    #\n    conv4 = layers.Conv1D(x*16,3,padding='same')(pool3)\n    conv4 = layers.BatchNormalization()(conv4)\n    conv4 = layers.Activation(tf.nn.relu)(conv4)\n\n    conv4 = layers.Conv1D(x*16,3,padding='same')(conv4)\n    conv4 = layers.BatchNormalization()(conv4)\n    conv4 = layers.Activation(tf.nn.relu)(conv4)\n    \n    conv4 = layers.Conv1D(x*16,3,padding='same')(conv4)\n    conv4 = layers.BatchNormalization()(conv4)\n    conv4 = layers.Activation(tf.nn.relu)(conv4)\n    \n    pool4 = layers.MaxPooling1D(strides=3)(conv4)\n    \n    gru_bi = layers.Bidirectional(layers.LSTM(64,return_sequences=True))(pool4)\n    \n    \n\n\n    com_layer = layers.GlobalAveragePooling1D()(conv9)\n    \n    \n    com_layer = layers.Dense(32,activation='relu')(com_layer)\n    \n    outputs_sbp = layers.Dense(1,name='Sbp')(com_layer)\n    outputs_dbp = layers.Dense(1,name='Dbp')(com_layer)\n\n    model = keras.Model(inputs=inputs,outputs=[outputs_sbp,outputs_dbp])\n    \n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x=32\ninputs = keras.Input(shape=(600,1))\n#1\nconv1 = layers.Conv1D(x*2,3,padding='same')(inputs)\nconv1 = layers.BatchNormalization()(conv1)\nconv1 = layers.Activation(tf.nn.relu)(conv1)\n\nconv1 = layers.Conv1D(x*2,3,padding='same')(conv1)\nconv1 = layers.BatchNormalization()(conv1)\nconv1 = layers.Activation(tf.nn.relu)(conv1)\n\npool1 = layers.MaxPooling1D(strides=3)(conv1)\n#2\nconv2 = layers.Conv1D(x*4,3,padding='same')(pool1)\nconv2 = layers.BatchNormalization()(conv2)\nconv2 = layers.Activation(tf.nn.relu)(conv2)\n\nconv2 = layers.Conv1D(x*4,3,padding='same')(conv2)\nconv2 = layers.BatchNormalization()(conv2)\nconv2 = layers.Activation(tf.nn.relu)(conv2)\n\npool2 = layers.MaxPooling1D(strides=3)(conv2)\n\n#3\nconv3 = layers.Conv1D(x*8,3,padding='same')(pool2)\nconv3 = layers.BatchNormalization()(conv3)\nconv3 = layers.Activation(tf.nn.relu)(conv3)\n\nconv3 = layers.Conv1D(x*8,3,padding='same')(conv3)\nconv3 = layers.BatchNormalization()(conv3)\nconv3 = layers.Activation(tf.nn.relu)(conv3)\n\nconv3 = layers.Conv1D(x*8,3,padding='same')(conv3)\nconv3 = layers.BatchNormalization()(conv3)\nconv3 = layers.Activation(tf.nn.relu)(conv3)\n\npool3 = layers.MaxPooling1D(strides=3)(conv3)\n\n#\nconv4 = layers.Conv1D(x*16,3,padding='same')(pool3)\nconv4 = layers.BatchNormalization()(conv4)\nconv4 = layers.Activation(tf.nn.relu)(conv4)\n\nconv4 = layers.Conv1D(x*16,3,padding='same')(conv4)\nconv4 = layers.BatchNormalization()(conv4)\nconv4 = layers.Activation(tf.nn.relu)(conv4)\n\nconv4 = layers.Conv1D(x*16,3,padding='same')(conv4)\nconv4 = layers.BatchNormalization()(conv4)\nconv4 = layers.Activation(tf.nn.relu)(conv4)\n\npool4 = layers.MaxPooling1D(strides=3)(conv4)\n\ngru_bi = layers.Bidirectional(layers.GRU(64,return_sequences=True))(pool4)\n\n#attention\n\n# batch_size, time_steps, lstm_units -> batch_size, lstm_units, time_steps\nattention_layer = layers.Permute((2, 1))(gru_bi)\n# batch_size, lstm_units, time_steps -> batch_size, lstm_units, time_steps\nattention_layer = layers.Dense(7, activation='softmax')(attention_layer)\n# batch_size, lstm_units, time_steps -> batch_size, time_steps, lstm_units\nattention_layer_probs = layers.Permute((2, 1), name='attention_vec')(attention_layer)\n\n# 相当于获得每一个step中，每个特征的权重\noutput_attention_mul = layers.multiply([gru_bi, a_probs], name='attention_mul', mode='mul')\n\noutput_attention_mul = layers.Flatten()(output_attention_mul)\nprint(output_attention_mul)\n\noutputs_sbp = layers.Dense(1,name='Sbp')(output_attention_mul)\noutputs_dbp = layers.Dense(1,name='Dbp')(output_attention_mul)\n\nmodel = keras.Model(inputs=inputs,outputs=[outputs_sbp,outputs_dbp])\n\nreturn model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}