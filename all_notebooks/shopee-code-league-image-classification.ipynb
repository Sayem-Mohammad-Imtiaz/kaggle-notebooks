{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport os\nfrom tqdm import tqdm\n\nimport cv2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploratory Data Analysis"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"../input/shopee-code-league-2020-product-detection/train.csv\")\nprint(train.shape)\ntrain.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Observations\n\n1. The labels are not shuffled\n2. The feature column contains the filename of the images"},{"metadata":{},"cell_type":"markdown","source":"# Take a look at the distribution of categories"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,10))\nplt.title(\"Distribution of labels for training data\")\nsns.countplot(train['category'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Observations\n1. There are a total of 42 categories\n2. Category 33 is significantly lesser than the rest (approx 570), resulting in the model learning disproportionately for that category\n\n## Solution\n* We can resample the training data to contain ~570 images from each category.\n* Alternatively, we can augment images to increase the amount of data\n"},{"metadata":{},"cell_type":"markdown","source":"# Modifying our training dataset\n\n* In order to reduce computational storage and time taken, we will only be using the first 10 categories of which contains 1500 image each\n* Shuffle the training data so that the examples fed into the model will create an 'independent' change"},{"metadata":{"trusted":true},"cell_type":"code","source":"new_train=pd.DataFrame()\n\nCATEGORIES=[n for n in range(10)]\n\nfor cat in CATEGORIES:\n    new_train=new_train.append(train[train['category']==cat][:1600])\n\ndel train\n\ntrain=new_train.sample(frac=1)\ntrain","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Visualize the modified training dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,10))\nsns.countplot(train['category'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Reading the training images\n\n* Convert category labels from int to strings defined from 00 to 41.\n* Resize image resolution (trail and error)"},{"metadata":{"trusted":true},"cell_type":"code","source":"resized_img_dim=150\n\ndef read_img(train,resized_img_dim):\n    \n    DATADIR='../input/shopee-code-league-2020-product-detection/resized/train'\n    X=[]\n\n    for fname,cat in tqdm(train.values):\n        if(cat<10):\n            cat='0'+str(cat)\n        else:\n            cat=str(cat)\n\n        path=os.path.join(DATADIR,cat,fname)\n\n        try:\n            img=cv2.imread(path).astype('float32')\n            img=cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n            img=cv2.resize(img, (resized_img_dim,resized_img_dim))\n        except:\n            pass\n        X.append(img)\n    return X\n\nX=read_img(train,resized_img_dim)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Setting up training labels\n* One Hot Encode labels"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\n\ny=train['category']\n\nohe=OneHotEncoder()\ny=ohe.fit_transform(y.values.reshape(-1,1)).astype('float32')\ny=y.todense()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Splitting training data"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX=np.array(X).reshape(-1,resized_img_dim,resized_img_dim,3)\n\nxtrain,xtest,ytrain,ytest=train_test_split(X,y,test_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del X\ndel y\ndel train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Augmenting image\n\n* Provides a wider range of image for the model to learn from "},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.image import ImageDataGenerator\n\ntrain_datagen = ImageDataGenerator(rescale=1./255, zoom_range=0.3, rotation_range=30,\n                                   width_shift_range=0.1, height_shift_range=0.1, shear_range=0.1, \n                                   horizontal_flip=True, fill_mode='constant')\n\nval_datagen = ImageDataGenerator(rescale=1./255)\n\ntrain_generator = train_datagen.flow(xtrain, ytrain, batch_size=30)\nval_generator = val_datagen.flow(xtest, ytest, batch_size=20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Examples of augmented images"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,10))\nfor xbatch,ybatch in train_generator:\n    for i in range(1,10):\n        plt.subplot(3,3,i)\n        plt.axis('off')\n        plt.imshow(((xbatch[i]*255).astype('uint8')))\n    break","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training and Evaluation\n\n1. Using a simple self defined CNN\n2. Using a pretrained model (VGG)\n\n\n* Evaluation by visualizing training accuracy/loss vs validation accuracy/loss"},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nimport keras\nfrom keras.models import Sequential, Model\nfrom keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Input, BatchNormalization\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.applications.vgg19 import VGG19\n\ninput_shape_=(resized_img_dim,resized_img_dim,3)\n\nvgg=VGG19(include_top=False, input_shape=input_shape_)\n\noutput = vgg.layers[-1].output\noutput = Flatten()(output)\n\nvgg_model=Model(vgg.input,output)\n\n\nprint(vgg_model.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pretrained_model = Sequential()\n\npretrained_model.add(vgg_model)\n\npretrained_model.add(Dense(256,activation='relu', input_dim=input_shape_))\npretrained_model.add(Dropout(0.4))\n\npretrained_model.add(Dense(10, activation='softmax'))\n\npretrained_model.compile(loss='categorical_crossentropy',\n              optimizer=keras.optimizers.RMSprop(lr=2e-5),\n              metrics=['accuracy'])\n\nmodel_callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, verbose=0)]\n\nhistory = pretrained_model.fit_generator(train_generator, steps_per_epoch=100, epochs=100,\n                              validation_data=val_generator, validation_steps=50, \n                              verbose=1, callbacks=model_callbacks)  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Accuracy\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title(\"Training accuracy vs Validation accuracy\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Accuracy\")\nplt.show()\n\n#Loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title(\"Training loss vs Validation loss\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score=pretrained_model.evaluate(xtest,ytest)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}