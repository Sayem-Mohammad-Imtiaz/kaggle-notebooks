{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction\n\nA dataset can be considered imbalanced, if one or more of the classes have a larger proportion of\nexamples comparatively with the others. These classes are called majority classes and the others\nare called minority classes.\n\n## Main causes:\n1. When the examples were collected and sampled from the problem domain, error during data collection (bias sampling, measurement errors).\n2. The imbalance might be a property of the problem domain.\n\n## Why care about imbalance?\n* Most ML algorithms for classification were designed around the assumption of an equal number of examples for each class; therefore imbalanced model will prone to majority class, which is bad for generalization.\n* In real world, we're mostly interested in minority class so it's useless if a model shows poor performance on minor population.","metadata":{}},{"cell_type":"markdown","source":"# Survey on multi-class imbalance techniques\n\n* Haixiang et al.: Two basic strategies are **preprocessing** and **cost-sensitive learning**, which are further integrated into classification models and divided into **ensemble-based classifiers** and **algorithm modified classifiers**.\n* Yasir Arafat et al.: Three strategies -> sampling techniques (over/under-sampling), cost-sensitive learning methods and ensemble-based methods (combine different classifiers to form a strong classifier).\n* R.Cruz et al. and M.Galar et al.: Four strategies -> algorithm-level approaches (takes into account imbalance between different classes), data-level approaches (sampling preprocessing), cost-sensitive learning frameworks (combines algorithm-level & data-level), ensemble-based approaches.\n* Multi-class: One-Against-All (OAA) and One-Against-One (OAO)\n\nShuo Wang et al.: It is then concluded that oversampling does not help in either multi minority and multi majority\ncases because it causes overfitting of the minority-class. Undersampling techniques on the other hand, in the multi-minority case, can be sensitive to the class number while in the multi majority case, there is a high risk of sacrificing too much majority-class performance. If we think about\nthe problem of multi class imbalance itself and not just the techniques applied, the multi majority situation seems to be more difficult than multi minority.\n\n## 1. Data-level approaches\n1. Resampling: select or generate a specific amount of examples from the majority or minority classes in order to rebalance them and diminish the impact of imbalance (undersampling, oversampling, hybrid).\n2. Distance-based algorithm:\n    * SMOTE: This can cause a problem of overgeneralization because the majority class is not taken into account. Since the generated synthetic samples disregard the majority class, it can lead to overlapping between classes especially in multiclass cases.\n    * MDO: oversampling technique\n\n## 2. Algorithm-level approaches\n1. Modified multi-class HDDT (Hellinger Distance Decision Trees)\n\n## 3. Cost-sensitive learning:\n1. Example-dependent costs\n2. Class-dependent costs\n\n## 4. Ensemble-based approaches:\n1. Bagging\n    * SMOTEBagging: take class distribution into consideration among all minority classes after sampling.\n    * Multi-class Roughly Balanced Bagging: Multinomial distribution is considered by using the classesâ€™ prior probabilities.\n    * SOUP-Bagging (Similarity Oversampling & Undersampling Preprocessing): utilizes hybrid sampling in order to obtain a more balanced distribution of classes in datasets.\n2. Boosting\n    * BPSO-AdaBoost-KNN: BPSO - feature selection algorithm and then the conjunction classifier Adaboost-KNN, measured by a new evaluation metric AUCarea.\n    * SMOTEBoost (data-level): SMOTE sampling then boosting. Drawbacks of the SMOTEBoost is generating a large training dataset, therefore, slow in training phase of boosting.\n    * RUSBoost (improve to reduce the computational requirement of SMOTEBoost): Use RUS (Random Under Sampling) from the majority class until all the classes reach the same size then AdaBoost is applied on the sampled data, the same for each iteration. Drawbacks of the RUSBoost is data loss in the under-sampling.\n    * CatBoost (novel algorithm): Gradient boosting on decision trees, including 2 phases, building trees then setting value of leaves for the fixed tree.\n3. Hybrid\n    * Hybrid Ensemble for Classification of Multiclass Imbalanced (HECMI): for datasets with high imbalance ratio, give better recall rates for minority classes.\n    \n# Evaluation Metrics\nMost notable metrics: F-measure, Geometric Mean (G-Mean) and AUC_ROC.\n\n    * F-measure: the weighted average of the precision and recall metrics.\n    * G-Mean: less sensitive to value skewness and especially outlier presence.\n    * AUC_ROC curve: performance measurements used for classification problems that attempt to describe a model's ability to distinguish between classes. ROC is probability curve and AUC is the degree or measure of separability.\n","metadata":{}}]}