{"cells":[{"source":"This is my first attempt at a Machine Learning project after completing Andrew Ng's[ Machine Learning](https://www.coursera.org/learn/machine-learning/home/welcome) course on Coursera.<br> It is also my first time using Jupyter Notebook.<br><br>\nAs this was my first project I set myself some simple aims:\n\n1. Put what I had learned on the course into practice\n2. Develop a workflow through a machine learning project\n3. Learn from how other people have tackled this challenge\n4. Come in the top half of the competition.<br>\n\nI am using Python for this kernel.<br>\nThe workflow that I am going to follow is:<br>\n\n1. Load the data<br>\n2. Summarise and explore the data<br>\n3. Complete missing data<br>\n4. Create new features<br>\n5. Evaluate a range of models <br>\n6. Choose the best model and create a submission file.\n\nI have read and re-used the content from a number of other people's notebooks and in particular would like to say thank you to [Sina Khorami](https://www.kaggle.com/sinakhorami) for his clearly explained [Titanic best working Classifier](https://www.kaggle.com/sinakhorami/titanic-best-working-classifier) and [Poonam Ligade](https://www.kaggle.com/poonaml) for her comprehensive [Titanic Survival Prediction End to End ML Pipeline](https://www.kaggle.com/poonaml/titanic-survival-prediction-end-to-end-ml-pipeline). I also referred to [Jason Brownlee's blog](https://machinelearningmastery.com/compare-machine-learning-algorithms-python-scikit-learn/), especially for the model evaluation part of the work.\n\n","metadata":{"_cell_guid":"cf1ee234-d63f-4db9-b538-8b6d615a2c08","_uuid":"d6275fc0f6cd7392b4b9c70fe5cbce99eb54bd58"},"cell_type":"markdown"},{"source":"# 1. Load the data<br>\nFirst off, we need to import the libraries that we are going to be using in this kernel.<br> ","metadata":{"_cell_guid":"4216405f-95de-4c2c-8e70-7cdccfe2471f","_uuid":"ac89512847330cb363b0ab78d93177147807b354"},"cell_type":"markdown"},{"source":"# data analysis and wrangling\nimport pandas as pd\nimport numpy as np\nimport random as rnd\nimport math\n\n# visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# machine learning\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn import model_selection\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn import preprocessing\nimport xgboost as xgb\nfrom xgboost.sklearn import XGBClassifier # <3\n\n","metadata":{"collapsed":true,"_cell_guid":"6c38f5f9-74d7-4d84-8013-d8c4fdee74c6","_uuid":"a2cfd80b7862b6593b9fe13db64737f3720da025"},"outputs":[],"execution_count":null,"cell_type":"code"},{"source":"Now, load in the training and test datasets. These have been provided as comma-separated variable(csv) files. We load each csv file as a Pandas DataFrame object. By doing this we can use the [public pandas objects, functions and methods](https://pandas.pydata.org/pandas-docs/stable/api.html)","metadata":{"_cell_guid":"3a0c2fe2-b9b8-4ed2-847f-be3864eb1c63","_uuid":"c20626491efcdcb03775119847eeb3d36ab33c4e"},"cell_type":"markdown"},{"source":"train = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')","metadata":{"collapsed":true,"_cell_guid":"5a25cbea-eece-4c2c-adba-9657c1a60143","_uuid":"ff1c7dbcc80a0ec5619bbf8b77cbcc26af6d0edf"},"outputs":[],"execution_count":null,"cell_type":"code"},{"source":"# 2. Summarise and explore the data<br>\nNow we make our first use of pandas through its *info()* function. This lists the data columns in the* train* DataFrame, the number of non-null values and the data type for each of them.","metadata":{"_cell_guid":"71709bb6-d21c-48ae-824e-6813bc898644","_uuid":"3e03d5a2dff8a2f38168ae17838521a3405bbe48"},"cell_type":"markdown"},{"source":"print(train.info())","metadata":{"collapsed":true,"_cell_guid":"93f59908-c316-4672-91c3-ef53e50e66dc","_uuid":"f35f85dd6993a6e9a66d7fed78ed933117eb4e8c"},"outputs":[],"execution_count":null,"cell_type":"code"},{"source":"Now we do the same for the *test* DataFrame.","metadata":{"_cell_guid":"a3114c51-4376-4b69-89b9-70b93cd508cc","_uuid":"cbed4eb78ca7138be43819f873655ff8c357a5e1"},"cell_type":"markdown"},{"source":"print(test.info())","metadata":{"collapsed":true,"_cell_guid":"c2a8e3a0-da65-4033-9bd3-eb3102f3385d","_uuid":"e843aefccfd89ad583688b0ad2afcc7677cf242f"},"outputs":[],"execution_count":null,"cell_type":"code"},{"source":"Train has 12 columns and test has 11. Test is lacking the \"Survived\" column, which is what we are going to calculate as our answer to the exercise. Let's hope that train has 891 rows and test 418, as a lot of the columns have that number of non-null values.<br><br>\nBefore we can start running our models across our data we need all of our columns to be complete and holding numerical values.","metadata":{"_cell_guid":"78e2c6ac-6d76-4350-91bb-529f4a0fcea1","_uuid":"fb538fd54f4d922c31e927a55dcde4b3097cf6ed"},"cell_type":"markdown"},{"source":"Next, let's take a look at some of the data itself. First, the training set","metadata":{"_cell_guid":"6b966a10-2a6c-4b43-a565-a5f1c806c0a1","_uuid":"cc055fceec485e0fcb3c3d79090b390975f929e9"},"cell_type":"markdown"},{"source":"train.head()","metadata":{"collapsed":true,"_cell_guid":"74542f35-48d0-40fc-9e28-558dfaeebfb3","_uuid":"656fe7d5f3e010e5f34e4d008e2ceb8f13a80327"},"outputs":[],"execution_count":null,"cell_type":"code"},{"source":"I'll just observe here that Survived is a Boolean field with a 0 standing for the passenger not surviving and a 1 meaning that they did survive.","metadata":{"_cell_guid":"7da54112-abe6-4037-8980-c53a963b2efc","_uuid":"c8d38ce140e17b2fe2334cf0d95ddf4c398979cb"},"cell_type":"markdown"},{"source":"(test.head())","metadata":{"_kg_hide-input":false,"collapsed":true,"_cell_guid":"3f09609d-9af1-4e43-9af5-2f53f84ac907","_kg_hide-output":false,"_uuid":"14da97ea986b4ac4d132beec1edb2bba8c8f9293"},"outputs":[],"execution_count":null,"cell_type":"code"},{"source":"Taking a look at the data in the train and test DataFrames, and referring back to the [overview of the data](https://www.kaggle.com/c/titanic/data) we can see the following:<br>\n**PassengerId** is an incremental field that is most likely used as a unique identifier for a passenger record or row.<br>\n**Pclass** is the class of the ticket and is a proxy for social class. <br>\n**Name** is the passenger name. It includes the title/honorific of the passenger. There is a cumbersome method for naming women who are married.<br>\n**Sex** is either male or female.<br>\n**Age** is a float value. Babies who are less than one have a fractional age value. An age in the form 'x.0' is a known age whereas an age in the form 'y.5' is an estimated age. Not all missing ages have been estimated.<br>\n**SibSp** is an integer and refers to the number of their siblings plus spouse on board with the passenger.<br>\n**Parch** is an integer and refers to the number of parents and children on board with the passenger.<br>\n**Ticket number** is an alphanumeric field. Where it has been prefixed with characters there is one (row three in the training set) that looks like it indicates the port of embarkation (STON could indicate Southampton, which is the port of embarkation for that passenger). The first digit in the ticket number looks like it indicates the ticket class.<br>\n**Fare** is a float and is the price paid for the ticket.<br>\n**Cabin** is the cabin that the passenger was in. There is a lot of missing data her. Looking at the rows we have printed out the missing data may be weighted by Pclass.<br>\n**Embarked** is the port of embarkation, with C = Cherbourg, Q = Queenstown, S = Southampton","metadata":{"_cell_guid":"2ca883c3-7e2b-423c-b1e3-15e45cb91523","_uuid":"ab3da980f3263dda38ef4c9df405123071dfcb5f"},"cell_type":"markdown"},{"source":"One hypothesis is that, if age data was added to after the sinking, then ages have been estimated more when people didn't survive. The following lines manipulate the Age values so that the value to the left of the decimal point is stripped and value to the right of the decimal point is multiplied by 10. We then print out the rows in the training set that equal five, ie those values for age in the format 'y.5'","metadata":{"_cell_guid":"32ce9d5a-730b-4a44-9078-97b520aa53a5","_uuid":"637b47cd7e5b5acd31cee25cb2f019f21b687f2b"},"cell_type":"markdown"},{"source":"x = ((train.Age - train.Age.round())*10)\nprint(train.loc[x == 5])","metadata":{"collapsed":true,"_cell_guid":"3165cac5-f1df-4179-a996-30831acefb46","_uuid":"0acda9e63f7259e844586c7d0e503b4e6c873fc6"},"outputs":[],"execution_count":null,"cell_type":"code"},{"source":"x = ((test.Age - test.Age.round())*10)\nprint(test.loc[x > 1])","metadata":{"collapsed":true,"_cell_guid":"528b6fbd-f5fe-46d0-98a4-e1a775c08e4c","_uuid":"37275ccc33f445b067144c18ae2f4d99279a3ce0"},"outputs":[],"execution_count":null,"cell_type":"code"},{"source":"So, that hypothesis turns out to be invalid as an estimated age correlates with the passenger being in 3rd class..","metadata":{"_cell_guid":"1bed7023-f4af-4ce0-8837-2971e6b630f6","_uuid":"60a9b93b2faeb65874bb76b56e46d8317d44bdcf"},"cell_type":"markdown"},{"source":"Next, we describe the datasets. For each column there is a count of the non-null values and their mean and standard deviation. Miniumum and maximum values are also given as well as 25%, 50% and 75% percentiles.\nFirst, we describe the training set","metadata":{"_cell_guid":"9b410309-8805-4303-85e1-05e31eb003b5","_uuid":"5bea9be4c6012e56ad68bd04f7b062095ebe4211"},"cell_type":"markdown"},{"source":"print(train.describe())","metadata":{"collapsed":true,"_cell_guid":"a3b37126-335c-4ba0-8f3a-983f8768aeca","_uuid":"4759178efda413335d95430376503842bcb3aca9"},"outputs":[],"execution_count":null,"cell_type":"code"},{"source":"Next, we describe the test set.","metadata":{"_cell_guid":"36b140db-3093-45c0-9b06-1678f9d9f3d5","_uuid":"94066ee113b35f09b101112af77d4749af8902bf"},"cell_type":"markdown"},{"source":"print(test.describe())","metadata":{"collapsed":true,"_cell_guid":"81abb894-fe91-440f-9eac-d30b0c44c6a2","_uuid":"9c22864847eda71286b3616aa6cebc032930d11c"},"outputs":[],"execution_count":null,"cell_type":"code"},{"source":"Roughly 38% of passengers in the training set survived. It looks as though there were more people in the lower classes than first class, which seems reasonable. The average age is about 30 with a standard deviation of 14/15 in both sets. Family groupings exist with at least one outlier family of considerable size. There is a large standard deviation in Fare, that is probably attributable to first class passengers paying a lot more than those in third class. ","metadata":{"_cell_guid":"6f8c7ca7-49e9-4825-8df5-bdec5b8efd96","_uuid":"0da1c6cd76205166452d20176ba3285bf30218df"},"cell_type":"markdown"},{"source":"We are very interested in correlation between features, not least between individual features and \"Survived\". Here we plot a heatmap of the correlation between features.","metadata":{"_cell_guid":"eddf5763-74f9-4f38-b93f-68d49ae127ed","_uuid":"4fe5489dd74f2543aee5d906d3dab965ad0d0508"},"cell_type":"markdown"},{"source":"corr=train.corr()#[\"Survived\"]\nplt.figure(figsize=(10, 10))\nsns.heatmap(corr, vmax=.8, linewidths=0.01,\n            square=True,annot=True,cmap='YlGnBu',linecolor=\"white\")\nplt.title('Correlation between features');","metadata":{"collapsed":true,"_cell_guid":"7567f10c-c021-4e45-80a7-a3e965eb8d37","_uuid":"8542cd1a49e2f782abea39823648a0e6a2945eb2"},"outputs":[],"execution_count":null,"cell_type":"code"},{"source":"The largest positive correlation with Survived is with Fare, while the largest negative correlation is with Pclass. There is a strong negative correlation between Fare and Pclass as well.<br><br>\nI'm reading this as there being a higher fare paid for Class = 1 (First Class) than lower classes and that you were more likely to survive if you were in a \"better\" class and paid more for your ticket (which are themselves, heavily correlated).<br><br>\nSibSp and Parch are strongly correlated, indicating a significant number of family groups.<br><br>\nSex is not included in the correlation heatmap as it is not a numerical field. But, the [code of conduct, \"women and children first\"](https://en.wikipedia.org/wiki/Women_and_children_first) mandated that *\"the lives of women and children were to be saved first in a life-threatening situation, typically abandoning ship, when survival resources such as lifeboats were limited\"*<br><br>\nTo test this, first we can look at the number of Men and Women who survived by passenger class.","metadata":{"_cell_guid":"c9f61f3c-ed77-49d7-9796-6ffb5608b303","_uuid":"4ab4239227039385b17cae3ca27b9ee14715199b"},"cell_type":"markdown"},{"source":"sns.set(font_scale=1)\ng = sns.factorplot(x=\"Sex\", y=\"Survived\", col=\"Pclass\",\n                    data=train, saturation=.5,\n                    kind=\"bar\", ci=None, aspect=.6)\n(g.set_axis_labels(\"\", \"Survival Rate\")\n    .set_xticklabels([\"Men\", \"Women\"])\n    .set_titles(\"{col_name} {col_var}\")\n    .set(ylim=(0, 1))\n    .despine(left=True))  \nplt.subplots_adjust(top=0.8)\ng.fig.suptitle('How many Men and Women Survived by Passenger Class');","metadata":{"collapsed":true,"_cell_guid":"8cce4bba-33fc-4074-ba3c-922b03e3b4d5","_uuid":"97d5c04c9d39fe9a2f15f5bf02ebdca5a5e9d985"},"outputs":[],"execution_count":null,"cell_type":"code"},{"source":"That's pretty stark. Being a woman in first class gave you a strong chance of survival whereas a man in third class had less than a 1 in 5 chance of surviving the wreck. This confirms that both Pclass and Sex influence a person's chance of survival.<br>\nNext, we plot a barchart to see the proportion of men and women who survived based on where they embarked.","metadata":{"_cell_guid":"dc46a951-e948-41d5-aeab-f8b4f2c289a1","_uuid":"74b5fd80e4d2e85416ff78c5aa3d73df1569afd6"},"cell_type":"markdown"},{"source":"sns.set(font_scale=1)\ng = sns.factorplot(x=\"Sex\", y=\"Survived\", col=\"Embarked\",\n                    data=train, saturation=.5,\n                    kind=\"bar\", ci=None, aspect=.6)\n(g.set_axis_labels(\"\", \"Survival Rate\")\n    .set_xticklabels([\"Men\", \"Women\"])\n    .set_titles(\"{col_name} {col_var}\")\n    .set(ylim=(0, 1))\n    .despine(left=True))  \nplt.subplots_adjust(top=0.8)\ng.fig.suptitle('How many Men and Women Survived by Port of Embarkation');","metadata":{"collapsed":true,"_cell_guid":"c9e74889-ce50-4e6a-9e7d-849b7643f7a5","_uuid":"62dc027d5ad20b4e66dbf75edb4a6c64e174144e"},"outputs":[],"execution_count":null,"cell_type":"code"},{"source":"This isn't as clear. There are differences in survival rate based on where a person embarked. However, a man had the worst chance if he embarked at Queensbury, whereas for women it was Southampton.<br><br>\nThere are possible reasons for this, such as low sample sizes and/or people embarking at certain ports tending to travel in a particular class. I'm going to press on without investigating this, although I will include Embarked in the model.<br><br>\nNext, we look at survival rates based on age.\n","metadata":{"_cell_guid":"b8d948dc-cc9a-4ee3-a972-004eff7e7172","_uuid":"78eef52abc0d9c47706ebda29c0035887402afc0"},"cell_type":"markdown"},{"source":"figure = plt.figure(figsize=(15,8))\nwithAge = (train[train['Age'].notnull()])\nplt.hist([withAge[withAge['Survived']==1]['Age'], withAge[withAge['Survived']==0]['Age']], stacked=True, color = ['g','r'],bins = 30,label = ['Survived','Dead'])\nplt.xlabel('Age')\nplt.ylabel('Number of passengers')\nplt.legend()","metadata":{"collapsed":true,"_cell_guid":"14a0a29b-9916-4ef0-af55-e2ec2ef76b02","_uuid":"6485a43eee7cdb3c149a30c2afaa553f6de75804"},"outputs":[],"execution_count":null,"cell_type":"code"},{"source":"Here it looks as though children under the age of ten had a better survival rate than other passengers. This supports the theory that the \"Women and children first\" code of conduct was upheld during the sinking.<br><br>\nNow, we look at the null values. For both train and test there are a significant number of null values for Age and Cabin","metadata":{"_cell_guid":"8b9dd303-5418-48df-936e-36adb711c900","_uuid":"4f46467d29ba4450ddbbbc8117952a621930b158"},"cell_type":"markdown"},{"source":"train.isnull().sum()","metadata":{"collapsed":true,"_cell_guid":"1a423e77-9b62-4a55-8b0b-c3f8ee10ab0d","_uuid":"6f9a1882e8888923e15513dbfb80322e617914aa"},"outputs":[],"execution_count":null,"cell_type":"code"},{"source":"test.isnull().sum()","metadata":{"collapsed":true,"_cell_guid":"7b1f49fd-def1-4d78-9bec-bf0b790a3dda","_uuid":"18d21b04fdf57e72933ef3ac4b55e7beeb5b8beb"},"outputs":[],"execution_count":null,"cell_type":"code"},{"source":"There are a significant number of Age and Cabin values missing in each set. One hypothesis is that age and cabin values are null more when people didn't survive. The following lines separate out rows where an age exists and those where they don't as DataFrames and then compares their survival rate by sex.","metadata":{"_cell_guid":"48fe3243-146c-4849-964e-e0664510c7e6","_uuid":"f4106795b5a40096159984e660ac56d6de607fc3"},"cell_type":"markdown"},{"source":"ageless = (train[train['Age'].isnull()])\nwithAge = (train[train['Age'].notnull()])\nprint (\"Full training set\")\nprint (train[[\"Sex\", \"Survived\", \"Pclass\"]].groupby(['Sex', \"Pclass\"], as_index=False).mean())\nprint (\"Training set with an age\")\nprint (withAge[[\"Sex\", \"Survived\", \"Pclass\"]].groupby(['Sex', \"Pclass\"], as_index=False).mean())\nprint (\"Training set without an age\")\nprint (ageless[[\"Sex\", \"Survived\", \"Pclass\"]].groupby(['Sex', \"Pclass\"], as_index=False).mean())","metadata":{"collapsed":true,"_cell_guid":"e7be23b9-c3e1-41b8-b459-c99736876d3a","_uuid":"63ba95c7285ced8c83570aa2f019d7c655e5d800"},"outputs":[],"execution_count":null,"cell_type":"code"},{"source":"This looks promising. There is a distinct difference between survival rates depending on  gender and whether an age value is present or not. Our original hypothesis that no age indicates a lower survival rate is true for men, but not for women. <br><br>\nLater, we can create a new column, EstimatedAge, and set this as 1 where an age value exists and 0 where it is NaN.","metadata":{"_cell_guid":"c06d654e-7173-4b50-99bf-8bba9b30a3fa","_uuid":"511c5b844b1c1de5c7032a74907eb9c0112582d3"},"cell_type":"markdown"},{"source":"cabinless = (train[train['Cabin'].isnull()])\nwithCabin = (train[train['Cabin'].notnull()])\nprint (\"Full training set\")\nprint (train[[\"Sex\", \"Survived\", \"Pclass\"]].groupby(['Sex', \"Pclass\"], as_index=False).mean())\nprint (\"Training set with a cabin\")\nprint (withCabin[[\"Sex\", \"Survived\", \"Pclass\"]].groupby(['Sex', \"Pclass\"], as_index=False).mean())\nprint (\"Training set without a cabin\")\nprint (cabinless[[\"Sex\", \"Survived\", \"Pclass\"]].groupby(['Sex', \"Pclass\"], as_index=False).mean())","metadata":{"collapsed":true,"_cell_guid":"926c0d38-9ade-479d-af9d-250193288615","_uuid":"15b4f330cc7f1c747c69d779c52001cbf54b32a1"},"outputs":[],"execution_count":null,"cell_type":"code"},{"source":"This also looks promising. There is a distinct difference between survival rates depending on gender and whether a cabin value is present or not across all men and women in third class. Later, we can create a new column, EstimatedCabin, and set this as 1 where an age value exists and 0 where it is NaN.","metadata":{"_cell_guid":"ae807f5f-8f25-4bf4-a81f-7a98794c0400","_uuid":"89457e0fb97220f6d8a104b65ad1f3be7a25487e"},"cell_type":"markdown"},{"source":"That is all the work we are going to do on the training and test datasets separately. Now we combine them together as one DataFrame, full_data, that we will complete the data and perform feature engineering on.<br><br> First, separate out and then drop the 'Survived' column from the train Dataframe","metadata":{"_cell_guid":"a37f501f-da5f-44d7-af18-266b938bb1ff","_uuid":"0cba0b9ae1b96d496292b678d4b4cd8a45dc538f"},"cell_type":"markdown"},{"source":"targets = train.Survived\ntrain.drop('Survived', 1, inplace=True)","metadata":{"collapsed":true,"_cell_guid":"1bcc09af-9edc-4b0d-8570-6d38df3ce863","_kg_hide-output":false,"_uuid":"65ecade841541402e8af015bc72db946a788fb39"},"outputs":[],"execution_count":null,"cell_type":"code"},{"source":"Merge the training and test sets so we can perform feature engineering on them together.<br>\nfull_data = [train,test] was initially tried, but didn't create the desired DataFrame.<br>\n","metadata":{"_cell_guid":"bae44eda-6a15-426f-b60b-bafccb912e4b","_uuid":"24a9f6d5bba84ac5b068b54315cf3d9afbbded55"},"cell_type":"markdown"},{"source":"full_data = train.append(test)\nfull_data.reset_index(inplace=True)\nfull_data.drop('index', inplace=True, axis=1)","metadata":{"collapsed":true,"_cell_guid":"9d5a6bc9-9265-44c9-a45a-a5f5587a4ea6","_uuid":"144bfd4072294b2e30c1bc5dc529360ce45fad2c"},"outputs":[],"execution_count":null,"cell_type":"code"},{"source":"Check that target and full_data look ok.","metadata":{"_cell_guid":"44735740-4fdf-4204-a1bd-a8670e19c324","_uuid":"977453b4604ad6a935b42dda63ab08b3e2a3cfbd"},"cell_type":"markdown"},{"source":"print(full_data.shape)\nprint(targets.shape)","metadata":{"collapsed":true,"_cell_guid":"316d08b4-4139-49a5-9182-bd4cc580ecb5","_uuid":"8aa95171a9c35f56667fb34d4ac09420840a1428"},"outputs":[],"execution_count":null,"cell_type":"code"},{"source":"# 3. Complete missing data","metadata":{"_cell_guid":"aa984745-d93d-40f9-a449-aa9d7e142e8a","_uuid":"6e37073e83abe4b73dda7ee63a7bab476cd30e92"},"cell_type":"markdown"},{"source":"Now that we have combined our training and test sets we can create the EstimatedAge and EstimatedCabin columns. They are populated with 0s and 1s depending on whether there are nulls in the Age and Cabin values for each row.","metadata":{"_cell_guid":"c109c81f-f970-4685-9b0f-e3ae4a688546","_uuid":"4a391abde819797977e96a951123650aa3e1bb5c"},"cell_type":"markdown"},{"source":"full_data['EstimatedAge']=full_data.Age.isnull().astype(int)\nfull_data['EstimatedCabin']=full_data.Cabin.isnull().astype(int)","metadata":{"collapsed":true,"_cell_guid":"7592d985-05fe-413a-9441-93767629a375","_uuid":"07e6453d5ba8a0d3f1c37115dd8e11f97ab1e4e9"},"outputs":[],"execution_count":null,"cell_type":"code"},{"source":"Now, take a look at the full_data DataFrame for missing values.","metadata":{"_cell_guid":"4480d2b2-2f6f-4bdb-bae7-65701567328b","_uuid":"3364b6b814ee27b75174bad66a9bd1c35c2bb640"},"cell_type":"markdown"},{"source":"full_data.isnull().sum()","metadata":{"collapsed":true,"_cell_guid":"75ac8296-27cd-4f65-b113-069b39bf060d","_uuid":"a319a5ab7baa97ae0b4549a0486c8761d068de62"},"outputs":[],"execution_count":null,"cell_type":"code"},{"source":"So, we are going to need to either estimate the missing values from the Age, Cabin, Embarked and Fare columns or drop the column altogether. Let's look at Fare and Embarked first. They only have a small number of missing values and so we won't spend much time on them.","metadata":{"_cell_guid":"a6a703cc-4956-4efb-8333-8f9aa05bc525","_uuid":"6ef46c4de7a8cff08954b57579b9c12aa9401ad7"},"cell_type":"markdown"},{"source":"print(full_data[full_data['Fare'].isnull()])","metadata":{"collapsed":true,"_cell_guid":"ae9ab3d4-2417-4417-b085-8d5cd59453f2","_uuid":"eaefc9ddf533c474624c19df204c9a317fa3418f"},"outputs":[],"execution_count":null,"cell_type":"code"},{"source":"There is one missing Fare value. It has come from the test DataFrame and it is for a man in third class. We are going to calculate the median value of the fare paid by a male passenger for a 3rd class ticket in the test DataFrame and use that to populate this Fare value.","metadata":{"_cell_guid":"56be8f36-74b0-48b8-9b04-ef01b7045d49","_uuid":"e9531876b3d08efd8f3d71bd006f1e5b8498e090"},"cell_type":"markdown"},{"source":"grouped_test = full_data.iloc[891:].groupby(['Sex','Pclass'])\ngrouped_median_test = grouped_test.median()\n\nprint(grouped_median_test)","metadata":{"collapsed":true,"_cell_guid":"d5ff0fd7-988c-4d7a-86f4-aee503b9aef9","_uuid":"64be7531ca1a6ea520ac0f23f8b07bdc801a1c0f"},"outputs":[],"execution_count":null,"cell_type":"code"},{"source":"The average paid for a male in 3rd class in the test datset was 7.895 so we use that for the one missing value.","metadata":{"_cell_guid":"a50f2ef6-cb22-427f-b15f-2a972e2a445c","_uuid":"e044c2e491eeaebae91cb5ede8f027684f74594a"},"cell_type":"markdown"},{"source":"full_data.Fare.fillna(7.895, inplace=True)\nfull_data.Fare = full_data.Fare.astype(int)","metadata":{"collapsed":true,"_cell_guid":"f84112f6-14ac-4e0d-937c-918a0e5044a6","_uuid":"1b1a7aa70f01ef04f194ec98d8e35010afd4cf50"},"outputs":[],"execution_count":null,"cell_type":"code"},{"source":"## Embarked<br>\nThere are only two rows where embarked is a null value. First we take a look at those rows.","metadata":{"_cell_guid":"1ecd7fb9-ab00-445f-acce-354a021b2c42","_uuid":"d5ee3150ad6c1090497a31cb8fe119dc63602e14"},"cell_type":"markdown"},{"source":"print(train[train['Embarked'].isnull()])","metadata":{"collapsed":true,"_cell_guid":"d6c7093a-71ae-4c51-b70e-981ee05413d6","_uuid":"2cccf0ac2fb9f8306f18b92469a58eef2d3eb1b0"},"outputs":[],"execution_count":null,"cell_type":"code"},{"source":"It looks as though they were travelling together, as they share a ticket number and a first class cabin. Both of these passengers were female and they survived. It doesn't matter much where we estimate their port of embarkation because being female in first class is so strongly correlated with survival. We'll estimate that they got on at Cherbourg, 'C' as women who embarked there had the strongest chance of survival.","metadata":{"_cell_guid":"f7a58abd-d23f-4bf1-a9df-a3a69342aebb","_uuid":"c5dcf9a8484e8fc4a16d3df80e36498cdfc8d313"},"cell_type":"markdown"},{"source":"full_data[\"Embarked\"] = full_data[\"Embarked\"].fillna('C')","metadata":{"collapsed":true,"_cell_guid":"2973f658-fc10-4880-874d-fd1984ebbcbc","_uuid":"117503cfab818059c3823deac2cff0682f486a1e"},"outputs":[],"execution_count":null,"cell_type":"code"},{"source":"Now, we are going to separate out the Embarked column, which has possible values of 'C', 'Q' or 'S', into three columns Embarked_C, Embarked_Q and Embarked_S which have boolean values encoded by 0 and 1. Once that is done we drop the Embarked column.","metadata":{"_cell_guid":"0b54ce84-1be6-40d9-bfdd-ac4b0333bd2f","_uuid":"86c24feef632be5261ffda54a13891f5e0d4c657"},"cell_type":"markdown"},{"source":"embarked_dummies = pd.get_dummies(full_data['Embarked'],prefix='Embarked')\nfull_data = pd.concat([full_data,embarked_dummies],axis=1)\nfull_data = full_data.drop([\"Embarked\"], axis=1)","metadata":{"collapsed":true,"_cell_guid":"c2caf532-1405-4823-b95e-5e1ad935f08e","_uuid":"d35078d397b8266b8a9defcb9db916bb9fe7187d"},"outputs":[],"execution_count":null,"cell_type":"code"},{"source":"See below how the new columns have been created.","metadata":{"_cell_guid":"3d046dd6-347a-4739-b911-6a412e3526db","_uuid":"5bbb53a5511a673d877eb6d617d21c901af5a35e"},"cell_type":"markdown"},{"source":"full_data.info()","metadata":{"collapsed":true,"_cell_guid":"3dacb4cd-bc17-4d32-9462-43a0a63e046b","_uuid":"59b5dd822d1a493b9846a71bd8bad1dd42dbe0c9"},"outputs":[],"execution_count":null,"cell_type":"code"},{"source":"### Ages\nI have seen a number of methods for estimating the missing age values<br><br>\n1. Generate random values based on the mean and standard deviation across the whole of the training and test datasets<br>\n2. Generate random values based on the mean and standard deviation across segments of the training and test datasets, slicing for Pclass and sex, for instance.<br>\n3. Use a machine learning model to estimate the missing age values.<br><br>\n\nI have chosen to use a machine learning model, [based on this kernel](https://www.kaggle.com/poonaml/titanic-survival-prediction-end-to-end-ml-pipeline) that I looked at from [Poonam Ligade](https://www.kaggle.com/poonaml/titanic-survival-prediction-end-to-end-ml-pipeline).","metadata":{"_cell_guid":"3eb33e68-8053-413a-a259-09bf95ea01d1","_uuid":"1822bce4b26e3bcd1a16794d72eabccf91f4af70"},"cell_type":"markdown"},{"source":"Before we start we need to categorise Sex using integer values. This is because we will be using a Random Forest categoriser which requires numerical values.","metadata":{"_cell_guid":"6431fee7-5a94-44af-b2e5-77691243e7c9","_uuid":"517ebb6c3fb25c0e999ca91e43aa8eeab62fbaa1"},"cell_type":"markdown"},{"source":"full_data['Sex'] = full_data['Sex'].map( {'female': 1, 'male': 0} ).astype(int)","metadata":{"collapsed":true,"_cell_guid":"43e28567-213d-49e1-8300-5aab41ead5dc","_uuid":"d2a8257a608989d0f656baca7e6f000aaffebedd"},"outputs":[],"execution_count":null,"cell_type":"code"},{"source":"We also need to do some work on the Name column. We will extract the title from each name into a new Title column and then use the get_dummies function to separate this out into indicator variables.","metadata":{"_cell_guid":"e335fd28-b722-4a2f-993b-25db76397c78","_uuid":"5a18ea4fa7a6034eba83b7175a897b0d46b39a9f"},"cell_type":"markdown"},{"source":"full_data['Title'] = full_data['Name'].map(lambda name:name.split(',')[1].split('.')[0].strip())","metadata":{"collapsed":true,"_cell_guid":"e216abaa-f765-4ef9-a9bb-fe5f8b5f6132","_uuid":"8e08d3a0868b59af0824e3d1f3f8e18c2881747b"},"outputs":[],"execution_count":null,"cell_type":"code"},{"source":"Next we categorise the titles. 'Dr' and 'Rev' are a bit of a fudge here.","metadata":{"_cell_guid":"87f64637-c9e4-4f42-aa2d-3a753913fae2","_uuid":"49c8c5abca684354f8c5285bfb60e1d5e602d548"},"cell_type":"markdown"},{"source":"Title_Dictionary = {\n                    \"Capt\":       \"Officer\",\n                    \"Col\":        \"Officer\",\n                    \"Major\":      \"Officer\",\n                    \"Jonkheer\":   \"Nobility\",\n                    \"Don\":        \"Nobility\",\n                    \"Sir\" :       \"Nobility\",\n                    \"Dr\":         \"Officer\",\n                    \"Rev\":        \"Officer\",\n                    \"Countess\":   \"Nobility\",\n                    \"Dona\":       \"Nobility\",\n                    \"Mme\":        \"Mrs\",\n                    \"Mlle\":       \"Miss\",\n                    \"Ms\":         \"Mrs\",\n                    \"Mr\" :        \"Mr\",\n                    \"Mrs\" :       \"Mrs\",\n                    \"Miss\" :      \"Miss\",\n                    \"Master\" :    \"Master\",\n                    \"Lady\" :      \"Nobility\"\n                    }\nfull_data['Title'] = full_data.Title.map(Title_Dictionary)","metadata":{"collapsed":true,"_cell_guid":"81d64793-23f8-41a3-94db-493d41b38227","_uuid":"1f3542f31d01d9337f7d6ed43f213c3a8e9c2418"},"outputs":[],"execution_count":null,"cell_type":"code"},{"source":"Now we can drop the Name variable. All we are going to be using is the title. ","metadata":{"_cell_guid":"8d7b84cf-4f4d-462d-8de1-e8c320bbca50","_uuid":"a14db38d531ff762a9661625df2384a1d0be276f"},"cell_type":"markdown"},{"source":"full_data.drop('Name',axis=1,inplace=True)","metadata":{"collapsed":true,"_cell_guid":"7152f7ec-eff3-40e7-ae50-47c7dc7849ce","_uuid":"423e7dd00fb0faafec431bb300fec4ff34212875"},"outputs":[],"execution_count":null,"cell_type":"code"},{"source":"Finally we use get_dummies to separate out the Title column into indicator variables","metadata":{"_cell_guid":"475fbd47-5b53-4411-bb15-3fcb588bbfe8","_uuid":"d06348080f663873c673358f608d06d552939f02"},"cell_type":"markdown"},{"source":"titles_dummies = pd.get_dummies(full_data['Title'],prefix='Title')\nfull_data = pd.concat([full_data,titles_dummies],axis=1)","metadata":{"collapsed":true,"_cell_guid":"97a766cd-c3ea-4027-8fff-0201803910a5","_uuid":"244db7aea225a2e188103269b6a4e19ec372c9ad"},"outputs":[],"execution_count":null,"cell_type":"code"},{"source":"Now we take a look at the first five rows.","metadata":{"_cell_guid":"675ab1d9-3a65-4258-bce7-4d7fe84fee16","_uuid":"04d6c7bbb00ea00f4d12020f80ea11e1367938b2"},"cell_type":"markdown"},{"source":"print(full_data.head())","metadata":{"collapsed":true,"_cell_guid":"2a61cb84-61c7-47d1-9a4a-3ae240cdeb6e","_uuid":"28a2731aabaaf4ca98786cd3df7a1cb2755cdd9e"},"outputs":[],"execution_count":null,"cell_type":"code"},{"source":"Now we have columns for age, sex, title and Pclass all in a number format. This means that we can now estimate the missing age values using the Random Forest Classifier.<br><br>\nFirst we create an age_train DataFrame consisting of the relevant columns from the training set.\n","metadata":{"_cell_guid":"330f2822-5ce8-45cb-9e16-c13bc8c54734","_uuid":"c918c3d494eb69ffd5a8626ff3f82afd37a338f0"},"cell_type":"markdown"},{"source":"age_train = full_data.head(891)[['Age','Sex', 'Title_Master', 'Title_Miss', 'Title_Mr', 'Title_Mrs', 'Title_Nobility', 'Title_Officer', 'Pclass']]\nprint(age_train.info())","metadata":{"collapsed":true,"_cell_guid":"a8d89332-b202-43a2-ba7a-bd21c853d2a1","_uuid":"f96efcd696f8faf0e27d5589938624215bceacb0"},"outputs":[],"execution_count":null,"cell_type":"code"},{"source":"Split this into known and unknown sets according to whether the Age value in the row is null or not.","metadata":{"_cell_guid":"495c331a-ea91-44d3-9c49-d4115b70500f","_uuid":"9fa6f6fde0c713daa1c8bbc9d9ce1939dcfa05a9"},"cell_type":"markdown"},{"source":"known_train  = age_train.loc[ (age_train.Age.notnull()) ]# known Age values for training set\nunknown_train = age_train.loc[ (age_train.Age.isnull()) ]# unknown values for training set","metadata":{"collapsed":true,"_cell_guid":"84ff1a36-9ffa-47aa-9700-9174675d5ae2","_uuid":"523dc0dd4a3adb40afa06bc0421db4aa3e5a4226"},"outputs":[],"execution_count":null,"cell_type":"code"},{"source":"All age values are stored in y_train. All the other values are stored in the X_train feature array","metadata":{"_cell_guid":"13a480b7-1128-49f8-a2a8-f96d05ae962f","_uuid":"802c36c1f75efbf2cf00d86b855939f30f14a7d2"},"cell_type":"markdown"},{"source":"y_train = known_train.values[:, 0]\nX_train = known_train.values[:, 1::]","metadata":{"collapsed":true,"_cell_guid":"bf475f46-1f53-4ccb-adda-666b0b39675a","_uuid":"66f05d22f679a55542ddf1bcf7be064b573b3b49"},"outputs":[],"execution_count":null,"cell_type":"code"},{"source":"Create and fit a model. Use the fitted model to predict the missing values","metadata":{"_cell_guid":"24fcba50-5ef2-4224-a262-b5ad6c9904c1","_uuid":"5220de5c0f0802d3d7189634214dd2d5d1315adc"},"cell_type":"markdown"},{"source":"rtr = RandomForestRegressor(n_estimators=2000, n_jobs=-1)\nrtr.fit(X_train, y_train)\npredictedAges = rtr.predict(unknown_train.values[:, 1::])","metadata":{"collapsed":true,"_cell_guid":"3f62b3ef-eb60-4982-8390-b93672384532","_uuid":"81f2e963f77f8564d174e6aec407378f616215c9"},"outputs":[],"execution_count":null,"cell_type":"code"},{"source":"Assign those predictions to the full data set and check that all of the Age values in the training set are now not null.","metadata":{"_cell_guid":"dd259a4b-ca5c-4c8d-8bd2-dc529f1d1774","_uuid":"599e314f7165a33d63d14471eadeb440eacd35f3"},"cell_type":"markdown"},{"source":"dup_train = full_data.head(891)\ndup_train.loc[ (dup_train.Age.isnull()), 'Age' ] = predictedAges \nprint(dup_train[dup_train['Age'].isnull()])","metadata":{"collapsed":true,"_cell_guid":"3fc407de-27bc-49b6-b9ca-15e8227576ce","_uuid":"81de7f15c0d5e6c89411b86f1feaa2771d077335"},"outputs":[],"execution_count":null,"cell_type":"code"},{"source":"Repeat the process for the test datset","metadata":{"_cell_guid":"d8811705-a4a0-417b-9674-84186690646a","_uuid":"d75c120b1240a90dd40fd2b637072fde4a3dc058"},"cell_type":"markdown"},{"source":"age_test = full_data.iloc[891:][['Age','Sex', 'Title_Master', 'Title_Miss', 'Title_Mr', 'Title_Mrs', 'Title_Nobility', 'Title_Officer', 'Pclass']]\n\n# Split sets into known and unknown test\nknown_test  = age_test.loc[ (age_test.Age.notnull()) ]# known Age values for testing set\nunknown_test = age_test.loc[ (age_test.Age.isnull()) ]# unknown values for testing set\n   \n# All age values are stored in target arrays\ny_test = known_test.values[:, 0]\n    \n# All the other values are stored in the feature array\nX_test = known_test.values[:, 1::]\n    \n# Create and fit a model\nrtr = RandomForestRegressor(n_estimators=2000, n_jobs=-1)\nrtr.fit(X_test, y_test)\n    \n# Use the fitted model to predict the missing values\npredictedAgesTest = rtr.predict(unknown_test.values[:, 1::])\n\n# Assign those predictions to the full data set\ndup_test = full_data.iloc[891:]\ndup_test.loc[ (dup_test.Age.isnull()), 'Age' ] = predictedAgesTest \n","metadata":{"collapsed":true,"_cell_guid":"3af4a7ba-c5e4-4794-bede-1b01d4e2be02","_uuid":"5419b1cb355fb7018842c58da3a96543f3e3e431","scrolled":true},"outputs":[],"execution_count":null,"cell_type":"code"},{"source":"Merge the enhanced training and test sets back into our full_data DataFrame.","metadata":{"_cell_guid":"ba00b1bf-5a20-4aa1-a66d-42d28b13d2db","_uuid":"e5ea7d515dab46c16df852cffbfdf933b621b6bf"},"cell_type":"markdown"},{"source":"full_data = dup_train.append(dup_test)\nfull_data.reset_index(inplace=True)\nfull_data.drop('index', inplace=True, axis=1)\nprint(full_data.shape)","metadata":{"collapsed":true,"_cell_guid":"ab752c2b-bbb6-4cf5-959e-c2a3d7b6ca08","_uuid":"acfa8a361be0351bdb507e3ffca6f902b3aed5f3"},"outputs":[],"execution_count":null,"cell_type":"code"},{"source":"Now we can drop the title variable as we won't be needing it any more.","metadata":{"_cell_guid":"3ab6579e-8890-4568-9e2f-d4f50a4bd843","_uuid":"374f282aadc963de85e4659720b9e2b275480b10"},"cell_type":"markdown"},{"source":"full_data.drop('Title',axis=1,inplace=True)","metadata":{"collapsed":true,"_cell_guid":"3dc9c5a6-371f-479f-987c-b67aadd36fc5","_uuid":"195cc2f1fe963d8afd978a62799d8323279a37f1"},"outputs":[],"execution_count":null,"cell_type":"code"},{"source":"## Cabin\nCabin has a lot of missing values: 1014 in total. This makes it difficult to estimate values for it. It is also hard to see how much additional information it can give us. Let's drop it.","metadata":{"_cell_guid":"1b200b9a-84b9-463e-8fec-a9a3d9b280da","_uuid":"8cb31bdc62a773a074af4c7541b86b391d0f08f7"},"cell_type":"markdown"},{"source":"full_data.drop('Cabin',axis=1,inplace=True)","metadata":{"collapsed":true,"_cell_guid":"f4f3e9a9-122e-46fe-8bd8-d47186e94455","_uuid":"fbb6873197396f7ecfc99517b12b3f590680fcc8"},"outputs":[],"execution_count":null,"cell_type":"code"},{"source":"Now, we shouldn't have any null values in our full_data DataFrame. We check that and continue onto the next stage, enhancing the data.","metadata":{"_cell_guid":"b9c3c1b3-07f4-4d1c-bba7-b9d3f52e694b","_uuid":"34b3f93ea38e7b15ec19f82f16b15dfc29785580"},"cell_type":"markdown"},{"source":"full_data.isnull().sum()","metadata":{"collapsed":true,"_cell_guid":"74900002-7a76-4736-9598-46926d1dc9ad","_uuid":"5ac972b71b57bd35b4b41bfc75b0b5a04604fca2"},"outputs":[],"execution_count":null,"cell_type":"code"},{"source":"## 4. Create new features<br>\nWe have already seen that Parch and SibSp are strongly correlated. So, we are goinf to create a new feature for total family size and then another one to indicate when a person appears to be travelling alone.<br><br>\nCreate a new column for family size. Form this by adding the values from the Parch and SibSp columns together","metadata":{"collapsed":true,"_cell_guid":"cd1bcf32-1a69-4a9e-8cd4-10369b36a189","_uuid":"3cf8cf9e43ac7bb0efeca89a560eda270721e71f"},"cell_type":"markdown"},{"source":"full_data['FamilySize'] = full_data['Parch'] + full_data['SibSp'] + 1","metadata":{"collapsed":true,"_cell_guid":"efe5658c-0102-4cde-bd4a-beb141f24317","_uuid":"290ec7fcbf88cc7edd1c1376405e7bce64267281"},"outputs":[],"execution_count":null,"cell_type":"code"},{"source":"Create a new column to identify people travelling alone","metadata":{"_cell_guid":"eb4d3c17-edac-4aae-96c3-809b2feea4f6","_uuid":"ddfd50b4a0a98efb358828b05b79bd8ddad2edc4"},"cell_type":"markdown"},{"source":"full_data['IsAlone'] = full_data['FamilySize'].map(lambda s: 1 if s == 1 else 0)","metadata":{"collapsed":true,"_cell_guid":"14fcfce2-41be-48c4-ac8a-643da3b7d1fd","_uuid":"76f27b0d371effe56c266e0adacfe0386e235970"},"outputs":[],"execution_count":null,"cell_type":"code"},{"source":"Having created FamilySize and IsAlone we no longer require the SibSp and Parch columns. We are also not going to be making use of the Ticket or PassengerId columns and so we drop all four of these columns now.","metadata":{"_cell_guid":"775eda91-1a4c-449c-a6e5-d6237f528361","_uuid":"7d5ef159f850db4fa3f1882b2c7b0f426ff46964"},"cell_type":"markdown"},{"source":"full_data = full_data.drop([\"SibSp\"], axis=1)\nfull_data = full_data.drop([\"Parch\"], axis=1)\nfull_data = full_data.drop([\"Ticket\"], axis=1)","metadata":{"collapsed":true,"_cell_guid":"0f711d24-a40f-4f8e-a3f1-91598b94d3ba","_uuid":"70bd11261a21af05b596e7eb96ae303a3ba57d21"},"outputs":[],"execution_count":null,"cell_type":"code"},{"source":"## 5. Evaluate a range of models<br><br>\nThe following has been taken/adapted from [Jason Brownlee's blog](https://machinelearningmastery.com/compare-machine-learning-algorithms-python-scikit-learn/), which I have found very useful.\n\nWe start with our full_dataset and separate it into the X (train) and test dataframes. Targets is the Survived column from the original training set","metadata":{"_cell_guid":"f7ce6d5e-78d0-4fa1-866a-30aa00fae147","_uuid":"c052b87103406cb7bfa81ca34addeff0eb1fb032"},"cell_type":"markdown"},{"source":"X = full_data.head(891)\ntest = full_data.iloc[891:]\nY = targets","metadata":{"collapsed":true,"_cell_guid":"7dbcf7ec-e50a-4267-84d5-6ec8ae9ad668","_uuid":"6ebae8eb15f105a3862b816d4e1b0d8b2d0e5797"},"outputs":[],"execution_count":null,"cell_type":"code"},{"source":"Now we create a validation set, which is to be 20% of our X training dataset and set its test options and evaluation metric.\n","metadata":{"_cell_guid":"c90c35ec-834c-476e-a9f5-9eb38f4343ef","_uuid":"d786d3f9be5cc0a10e4155505914d5a087bce615"},"cell_type":"markdown"},{"source":"validation_size = 0.20\nseed = 7\nX_train, X_validation, Y_train, Y_validation = model_selection.train_test_split(X, Y, test_size=validation_size, random_state=seed)\nseed = 7\nscoring = 'accuracy'","metadata":{"collapsed":true,"_cell_guid":"ba0dce07-c98a-4452-955b-35f982b0e0fc","_uuid":"4b801e9f1c204049a728b273a5eb06a8ded71323"},"outputs":[],"execution_count":null,"cell_type":"code"},{"source":"Now we are going to run our data through a number of models, evaluate their accuracy and standard deviation and output those. The loop goes through each of the classifiers. For each classifier it splits up the available data into ten subsets, referred to as folds, fits the model on nine of those folds and then evaluates it against the remaining tenth fold. It then repeats the process for the other folds in turn. The resulting cross validation score is an average of the ten results.","metadata":{"_cell_guid":"2c982f18-ade8-416d-912c-f3509617d3a3","_uuid":"cce2246751e1e3887881be436f84189c35895a4a"},"cell_type":"markdown"},{"source":"# Spot Check Algorithms\nmodels = []\nmodels.append(('LogisticRegression', LogisticRegression()))\nmodels.append(('KNeighborsClassifier', KNeighborsClassifier()))\nmodels.append(('DecisionTreeClassifier', DecisionTreeClassifier()))\nmodels.append(('GaussianNB', GaussianNB()))\nmodels.append(('SVM', SVC()))\nmodels.append(('RandomForestClassifier', RandomForestClassifier()))\nmodels.append(('XGBClassifier',  XGBClassifier()))\n# evaluate each model in turn\nresults = []\nmnames = []\nfor mname, model in models:\n\tkfold = model_selection.KFold(n_splits=10, random_state=seed)\n\tcv_results = model_selection.cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)\n\tresults.append(cv_results)\n\tmnames.append(mname)\n\tmsg = \"%s: %f (%f)\" % (mname, cv_results.mean(), cv_results.std())\n\tprint(msg)","metadata":{"collapsed":true,"_cell_guid":"cd280019-c1af-4193-abaf-2b5ab27f97a5","_uuid":"ff52a27141463fe07a09813990c54bd0d6cd31b1"},"outputs":[],"execution_count":null,"cell_type":"code"},{"source":"Of the models that we have tested, Logistic Regression looks the most promising. We can also evaluate our models using a confusion matrix and its associated classification report.","metadata":{"_cell_guid":"46009e89-a723-487f-b725-570d0937c89b","_uuid":"bb565174f29ed1dad6365de1452f2809c4da87df"},"cell_type":"markdown"},{"source":"# Make predictions on validation dataset\nlr = LogisticRegression()\nlr.fit(X_train, Y_train)\npredictions = lr.predict(X_validation)\nprint(\"Logistic Regression\")\nprint(accuracy_score(Y_validation, predictions))\nprint(confusion_matrix(Y_validation, predictions))\nprint(classification_report(Y_validation, predictions))\n\nknn = KNeighborsClassifier()\nknn.fit(X_train, Y_train)\npredictions = knn.predict(X_validation)\nprint(\"K Nearest Neighbours\")\nprint(accuracy_score(Y_validation, predictions))\nprint(confusion_matrix(Y_validation, predictions))\nprint(classification_report(Y_validation, predictions))\n\ndt = DecisionTreeClassifier()\ndt.fit(X_train, Y_train)\npredictions = dt.predict(X_validation)\nprint(\"Decision Tree Classifier\")\nprint(accuracy_score(Y_validation, predictions))\nprint(confusion_matrix(Y_validation, predictions))\nprint(classification_report(Y_validation, predictions))\n\nnb = GaussianNB()\nnb.fit(X_train, Y_train)\npredictions = nb.predict(X_validation)\nprint(\"Gaussain NB\")\nprint(accuracy_score(Y_validation, predictions))\nprint(confusion_matrix(Y_validation, predictions))\nprint(classification_report(Y_validation, predictions))\n\nsvm = SVC()\nsvm.fit(X_train, Y_train)\npredictions = svm.predict(X_validation)\nprint(\"Support Vector Machine\")\nprint(accuracy_score(Y_validation, predictions))\nprint(confusion_matrix(Y_validation, predictions))\nprint(classification_report(Y_validation, predictions))\n\nrf = RandomForestClassifier()\nrf.fit(X_train, Y_train)\npredictions = rf.predict(X_validation)\nprint(\"Random Forest\")\nprint(accuracy_score(Y_validation, predictions))\nprint(confusion_matrix(Y_validation, predictions))\nprint(classification_report(Y_validation, predictions))\n\nXGB = XGBClassifier()\nXGB.fit(X_train, Y_train)\npredictions = XGB.predict(X_validation)\nprint(\"XGBoost\")\nprint(accuracy_score(Y_validation, predictions))\nprint(confusion_matrix(Y_validation, predictions))\nprint(classification_report(Y_validation, predictions))\n\n","metadata":{"collapsed":true,"_cell_guid":"53643383-7e90-4520-a660-28a733f0bbc9","_uuid":"f7df9844c98d526d92a3a76289ffadaf9ad0f6ae"},"outputs":[],"execution_count":null,"cell_type":"code"},{"source":"Logistic Regression hasn't always given the best results here. However, it has given me the best scores when I have made submissions.<br><br>\nOn this somewhat less-than-scientific basis I have chosen to submit a file based on the Logistic Regression classifier, as shown below. First, we prepare the training and test sets, before running the Logistic Regression classifier to create Y_pred, a single column file containing our predictions.","metadata":{"_cell_guid":"f39027dd-4371-4583-8dbd-dc3d67515da5","_uuid":"14bc200d3778636a520dbb41811d58c781e4153b"},"cell_type":"markdown"},{"source":"X_train = X.drop([\"PassengerId\"], axis=1)\nY_train = Y\nX_test  = test.drop([\"PassengerId\"], axis=1)\nprint(X_train.shape, Y_train.shape, X_test.shape)\n\n# LR = LogisticRegression()\n# LR.fit(X_train, Y_train)\n# Y_pred = LR.predict(X_test)\n# LR.score(X_train, Y_train)\n# acc_LR = round(LR.score(X_train, Y_train) * 100, 2)\n# print(acc_LR)\n\nfrom xgboost import XGBRegressor\n\nmy_model = XGBClassifier(n_estimators=1000, learning_rate=0.05, early_stopping_rounds=5)\nmy_model.fit(X_train, Y_train)\n\nY_pred = my_model.predict(X_test)\n# We will look at the predicted survival to ensure we have something sensible.\nprint(Y_pred)\n","metadata":{"collapsed":true,"_cell_guid":"7c451b0c-6618-47e9-87f9-0f4fec7fd80d","_uuid":"6508aa5e60aed05a51aed8701cabd85f7a5e68c0"},"outputs":[],"execution_count":null,"cell_type":"code"},{"source":"Finally, we create our submission file by merging Y_pred with the PassengerId column from our test DataFrame and creating a csv file called submission.csv with it.","metadata":{"_cell_guid":"820139ab-4249-482c-9cde-df7245a04356","_uuid":"f5bddec8388cfc5a4548e54471d4f38dbfac8245"},"cell_type":"markdown"},{"source":"submission = pd.DataFrame({\n        \"PassengerId\": test[\"PassengerId\"],\n        \"Survived\": Y_pred\n    })\n# print(submission)\nsubmission.to_csv('submission.csv', index=False)","metadata":{"collapsed":true,"_cell_guid":"6d21f9fd-c588-40be-b2ca-217cc1bd9ee5","_uuid":"2f0bf0f62e8b589664e3ccfbb3f05a60d83d6e4e"},"outputs":[],"execution_count":null,"cell_type":"code"},{"source":"While there are many faults that I can see in the above, I have done the things that I set out to at the start of entering this competition, which were:\n\n1. Put what I had learned on Andrew Ng's[ Machine Learning](https://www.coursera.org/learn/machine-learning/home/welcome) course on Coursera into practice\n2. Develop a workflow through a machine learning project\n3. Learn from how other people have tackled this challenge\n4. Come in the top half of the competition.<br>\n\nSo, overall, I am happy with my progress to date. My best submission was 0.78947 which put me about two thirds of the way up the leaderboard at the time I submitted.\n\nI have a lot more to learn and welcome comments about where I have gone wrong or improvements that I could have made to my submission. I hope that the way this is presented is clear, especially for people who are starting out, and if you think I can help, please feel free to ask questions.","metadata":{"collapsed":true,"_cell_guid":"d69a5c8a-3cb1-488f-9a67-9917d6291d53","_uuid":"257f809be2217ddbfe0951226397589e94f2bdc2"},"cell_type":"markdown"}],"nbformat_minor":1,"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"version":3,"name":"ipython"},"file_extension":".py","version":"3.6.4","mimetype":"text/x-python","pygments_lexer":"ipython3","nbconvert_exporter":"python","name":"python"}},"nbformat":4}