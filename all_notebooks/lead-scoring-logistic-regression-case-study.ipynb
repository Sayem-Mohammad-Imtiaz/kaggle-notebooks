{"cells":[{"metadata":{},"cell_type":"markdown","source":"# <span style=\"color:navy\"> Lead Scoring for X Education \n#### <span style=\"color:navy\"> A case study in Logistic Regression"},{"metadata":{},"cell_type":"markdown","source":"### <span style=\"color:navy\"> Problem Statement\n\nAn education company named X Education sells online courses to industry professionals. The professionals who are interested in the courses land on their website and browse for courses. \n\nThe company markets its courses on several websites and search engines like Google. Once these people land on the website, they might browse the courses or fill up a form for the course or watch some videos. When these people fill up a form providing their email address or phone number, they are classified to be a lead. The company also gets leads through past referrals. Once these leads are acquired, employees from the sales team start making calls, writing emails, etc. Through this process, some of the leads get converted while most do not. The typical lead conversion rate at X education is around 30%. \n\nNow, although X Education gets a lot of leads, its lead conversion rate is very poor. To make this process more efficient, the company wishes to identify the most potential leads, also known as ‘Hot Leads’. If they successfully identify this set of leads, the lead conversion rate should go up as the sales team will now be focusing more on communicating with the potential leads rather than making calls to everyone. \n\n### <span style=\"color:navy\"> Objective: \n\nThe objective is to help X Education select the most promising leads by building a model and assigning a lead score to each of the leads such that the customers with higher lead score have a higher conversion chance and the customers with lower lead score have a lower conversion chance. The CEO, in particular, has given a ballpark of the target lead conversion rate to be around 80%."},{"metadata":{},"cell_type":"markdown","source":"### <span style=\"color:navy\"> 1. Import Libraries and Initial Data Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n%matplotlib inline\n\npd.set_option('display.max_rows', 100)\npd.set_option('display.max_columns', 100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Read the dataset Leads.csv\n\ndf = pd.read_csv(\"/kaggle/input/lead-scoring-dataset/Lead Scoring.csv\")\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Take a copy of the original dataset to assign the Lead score to the original rows. \n\ndf_orig = df.copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <span style=\"color:navy\">  1.1 Summary Data Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <span style=\"color:navy\"> 1.2 Imbalance Analysis\n\n**To check the balance and data with respect to the target variable - 'Converted'**\n    \nThe data is not too much imbalanced. As such, we can proceed with the data for analysis and model building"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dividing the dataset into two dataset with Converted = 0 and Converted = 1\n\ndf_0=df.loc[df[\"Converted\"]==0]\ndf_1=df.loc[df[\"Converted\"]==1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculating Imbalance percentage \n# Since the majority is target0 and minority is target1\nprint (f'Count of Converted = 0: {len(df_0)} \\nCount of Converted = 1: {len(df_1)}')\nprint (f'Imbalance Ratio is : {round(len(df_0)/len(df_1),2)}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting the imbalance Analysis:\nsns.set_style('whitegrid')\nfig, ax = plt.subplots(figsize = (6,4))\nplt.title('Imbalance Analysis',  fontsize=20)\nchart = sns.countplot(data=df, x='Converted', palette='muted')\nplt.xlabel('Converted', fontsize=18)\nplt.ylabel('count', fontsize=18)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <span style=\"color:navy\"> 2. Data Cleaning\n1. Replace the 'Select' value in the categorical values to NaN. \n2. Check Percentage of Missing values for all columns\n3. Drop columns with a high percentage of missing values\n4. Drop categorical columns that are highly skewed\n5. Impute columns with less percentage of missing values\n6. We can also drop the columns that were completed by the Sales team after progressing with the leads. "},{"metadata":{},"cell_type":"markdown","source":"### <span style=\"color:navy\"> 2.1 Convert 'Select' values to NaN\n\nReplace the 'Select' value in the categorical values to NaN. These values are mostly from dropdown menus where nothing is selected"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Converting 'Select' values to NaN.\ndf = df.replace('Select', np.nan)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Drop the duplicate rows if any"},{"metadata":{"trusted":true},"cell_type":"code","source":"row1 , column1 = df.shape[0], df.shape[1]\n\n# delete duplicates\ndf = df.drop_duplicates() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Calculate the percentage of the retained rows**"},{"metadata":{"trusted":true},"cell_type":"code","source":"row2 , column2 = df.shape[0], df.shape[1]\n\npercentRows = round ((row2/row1 * 100), 2)\nprint (f'Rows retained after Duplicate Deletion: {row2} or {percentRows} percent')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <span style=\"color:navy\">   2.2 Find missing values and delete columns with a lot of missing values\n\nFor our analysis, we have to find the columns with missing values and handle them by either deleting or imputing. "},{"metadata":{},"cell_type":"markdown","source":"**Define a function to get the missing values and missing percentage for the dataframes.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# To find percent of Nan values\n# We can define a function to get the missing values and missing percentage for the dataframes.\ndef missing_data(data):\n    count_missing = data.isnull().sum().sort_values(ascending=False)\n    percent_missing = (data.isnull().sum() * 100 / len(data)).sort_values(ascending=False)\n    missing_value_df = pd.DataFrame({'count_missing': count_missing,\n                                 'percent_missing': percent_missing})\n    return missing_value_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#To find percent of Nan values \nmissing_data(df).head(20).transpose()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <span style=\"color:navy\">  2.3. Drop the unwanted variables \n\nSince we do not need all the columns provided in the dataset for our analysis, we can drop some of the columns based on our analysis.  "},{"metadata":{},"cell_type":"markdown","source":"#### Drop Prospect ID and Lead Number as they are unique identifiers and need not be used in prediction\n\nClearly Prospect ID and Lead Number are two variables that represent the unique identfier of the Contacted People and as such will not add value to the model. These columns can be dropped. There are no duplicates int he Prospect ID and the Lead Number columns."},{"metadata":{"trusted":true},"cell_type":"code","source":"# To check if there are any duplicate values in Prospect ID and Lead Number columns\n\nprint (f'Duplicates in Prospect ID - {any(df[\"Prospect ID\"].duplicated())}')\nprint (f'Duplicates in Lead Number - {any(df[\"Lead Number\"].duplicated())}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dropping the columns as mentioned in the above comment. \ndropFeatures = ['Prospect ID', 'Lead Number']\ndf.drop(df[dropFeatures], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Create a funtion to drop the columns with a certain percentage of NaN values\n\nWe can drop certain columns with more than certain percentage of missing values. As they have high value of missing percentage, they will not be indicative of the correct weight of the columns in prediction."},{"metadata":{"trusted":true},"cell_type":"code","source":"# we will drop the columns having more than 70% NA values.\ndef drop_columns(data, miss_per):\n    cols_to_drop = list(round(100*(data.isnull().sum()/len(data.index)), 2) >= miss_per )\n    dropcols = data.loc[:,cols_to_drop].columns\n    print (f'Features dropping now: {dropcols}')\n    data = data.drop(dropcols, axis=1)\n    return data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Drop the columns with more than 70% NaN values"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = drop_columns(df, 70.0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#missing_data(df).head(20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Analysis of Score columns assigned by the Sales Team**\n\nThe following are the score columns assigned by the sales team to the dataset after progressing with the leads. \nThese columns can be dropped as they will not add to the model building. \n\nAnalyse the following features before dropping them. \n\n* Lead Quality \n* Asymmetrique Activity Index\n* Asymmetrique Profile Index\n* Asymmetrique Activity Score\n* Asymmetrique Profile Score"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Analyse the score columns assigned by the sales team to the dataset before dropping them\n\nscoreFeatures = ['Lead Quality', 'Asymmetrique Activity Index', 'Asymmetrique Profile Index' ]\n\n# Count plot for the categorical variables\nsns.set(style='ticks',color_codes=True)\ncolors =['Accent', 'PiYG' , 'RdPu']\n\nplt.figure(figsize = (15,5))\nfor i in enumerate(scoreFeatures):\n    plt.subplot(1, 3, i[0]+1)\n    chart = sns.countplot(x = i[1], hue = 'Converted', data = df, palette = colors[i[0]])\n    chart.set_xticklabels(chart.get_xticklabels(), rotation=45, ha='right',)\n    plt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Analyse the score columns assigned by the sales team to the dataset\n\nfig, axis = plt.subplots(1, 2, figsize = (12,4))\nplt1 = sns.distplot(df_0['Asymmetrique Activity Score'], hist=False, kde=True , color='b' , ax = axis[0])\nplt1 = sns.distplot(df_1['Asymmetrique Activity Score'], hist=False, kde=True , color='r' , ax = axis[0])\nplt2 = sns.distplot(df_0['Asymmetrique Profile Score'], hist=False, kde=True , color='b' , ax = axis[1])\nplt2 = sns.distplot(df_1['Asymmetrique Profile Score'], hist=False, kde=True , color='r' , ax = axis[1])\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Drop the columns with more than 45% NaN values\n\nAs all the score features have more than 45% Nan values, these can be dropped without affecting our analysis. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop the score columns assigned by the sales team to the dataset\n\ndf = drop_columns(df, 45.0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* **Drop the columns 'Tag' and 'Last Activity' as the columns are added by Sales team while working on the leads and does not directly contribute to identifying the hot leads**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop the unwanted features\ndropFeatures = ['Tags', 'Last Notable Activity']\n\ndf.drop(dropFeatures, axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---------------"},{"metadata":{},"cell_type":"markdown","source":"## <span style=\"color:navy\"> 3. EDA and Data Visualizations for futher analysis\n\nThe next step is to visualise the data using matplotlib and seaborn.\n\nThis is one of the most important step - understanding the data. This step will help us understand the properties of data.\n\n* Helps to identify any outliers.\n* If there is some obvious multicollinearity going on, this can be identified here.\n* Identify the data types of the features and make any conversions if needed.\n    \n### <span style=\"color:navy\"> 3.1 Check the data types of all the columns and make changes if needed\n\n* The Constant features can be removed. Constant features are those features that have only one value.\n* The Categorical features should be identified to create the Dummy variables for them later.\n* The Boolean features ('Yes' or 'No' features) can be mapped to 0 and 1 to prepare them for modeling. "},{"metadata":{},"cell_type":"markdown","source":"**Delete the constant features**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# A function to find the constant features. Constant features are those features which have only one distinct value.\n\ndef find_constant_features(df):\n    constFeatures = []\n    for column in list(df.columns):\n        if df[column].unique().size < 2:\n            constFeatures.append(column)\n    return constFeatures\n\nconstFeatures = find_constant_features(df)\nprint(constFeatures)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop the constant features as they will not add value to the analysis\n\ndf = df.drop(constFeatures, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Identify the number of unique features in a column"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Look at the number of unique categories in a column\ndef unique_count(data):\n    data_type = data.dtypes\n    unique_count = data.nunique()\n    \n    unique_count_df = pd.DataFrame({'data_type': data_type,\n                                 'unique_count': unique_count})\n    return unique_count_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"unique_count(df).transpose() # Used transpose so as to avoid using more space. `","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Identify all the Categorical, boolean and numeric features"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Identify and separate all the Categorical, boolean and numeric features for analysis\ntargetFeature = []\ncatFeatures = []\nboolFeatures = []\nnumFeatures = []\n\nfor each in df.columns:\n    if each in ('Converted'):\n        targetFeature.append(each)\n    elif df[each].nunique() == 2:  #Features with only 2 unique values as boolean\n        boolFeatures.append(each)\n    elif df[each].dtype == 'object':\n        catFeatures.append(each)\n    elif df[each].dtype in ('int64','float64'):\n        numFeatures.append(each)\n    else:\n        numFeatures.append(each)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print (f'The Target Feature is : \\n {targetFeature} \\n')\nprint (f'The Boolean Features are : \\n {boolFeatures} \\n')\nprint (f'The Categorical Features are : \\n {catFeatures} \\n')\nprint (f'The Numeric Features are :\\n {numFeatures} \\n')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <span style=\"color:navy\"> 3.2 Univariate Analysis of Boolean Features\n\n* Convert the values 'Yes' and 'No' to 1 and 0 in the Binary Features. \n* Check if the columns are skewed and drop them if they are skewed."},{"metadata":{"trusted":true},"cell_type":"code","source":"boolFeatures","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Convert the values 'Yes' and 'No' to 1 and 0 in the Binary Features. \n# value_counts is checked each time to ensure the mapping is done only once \n# If mapped multiple times, the values are converted to NaNs\n\nfor each in boolFeatures:\n    if df[each].value_counts().values.sum() > 0:  # To check if the step was already completed\n        df[each] = df[each].map(dict(Yes=1, No=0))\n        print (f'Binary mapping is completed for {each}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Convert the boolean features to type boolean\ndf[boolFeatures] = df[boolFeatures].astype('int64')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"boolFeatures","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Count plot for the Boolean variables\n# colors = ['Accent', 'PiYG' , 'RdPu', 'icefire' , 'ocean' , 'gist_earth', 'magma', 'plasma', 'rocket']\ncolors = ['Accent', 'ocean', 'rocket'] * 3\nsns.set(style='ticks',color_codes=True)\nplt.figure(figsize = (10,10))\nfor i, x_var in enumerate(boolFeatures):\n    plt.subplot(3, 3, i+1)\n    chart = sns.countplot(x = x_var, data = df, hue='Converted', palette=colors[i])\n    chart.set_xticklabels(chart.get_xticklabels())\n    plt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Identify the value counts of the boolean features to confirm if they have only one value\n\nfor each in boolFeatures:\n    print (df[each].value_counts(dropna=False))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Observations:\n\n* Only two fields, 'A free copy of Mastering The Interview' and 'Do Not Email' have values for 1 and 0\n* All the other binary features have a very high percent of values as No.\n* We can drop these columns as they will not contribute to the analysis."},{"metadata":{"trusted":true},"cell_type":"code","source":"# we can drop the boolean Features with most values as 0 as they all have the value True and do not help in the analysis\n\ndropFeatures = [ 'Do Not Call',\n                 'Search',\n                 'Newspaper Article',\n                 'X Education Forums',\n                 'Newspaper',\n                 'Digital Advertisement',\n                 'Through Recommendations']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop the unwanted features\n\ndf.drop(dropFeatures, axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#To find percent of Nan values \nmissing_data(df).head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <span style=\"color:navy\"> 3.3 EDA and missing values handling for the Numeric Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"numFeatures","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Analyze the numeric features\n\nsns.set(style='ticks',color_codes=True)\nfig = plt.figure(figsize = (15, 15))\ng = sns.pairplot(data=df, hue='Converted', vars=numFeatures + targetFeature);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Frequency Ditribution for Numeric Features\nsns.set(style='ticks',color_codes=True)\nplt.figure(figsize = (12, 12))\nfor i, x_var in enumerate(numFeatures):\n    plt.subplot(3, 2, i+1)\n    sns.distplot(df_0[x_var], hist=False, kde=True , color='b')\n    sns.distplot(df_1[x_var], hist=False, kde=True , color='r')\n    plt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.Converted.dtype","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"numFeatures","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Outlier Handling for the Numeric Features\n\nThe features 'TotalVisits', 'Page Views Per Visit' have outliers and they can be capped at 0.01 and 0.99 th quantiles"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Box plot to identify the outliers\n# Frequency Ditribution for Numeric Features\nsns.set(style='ticks',color_codes=True)\ncolors = ['Accent', 'ocean' , 'RdPu']\nplt.figure(figsize = (12, 12))\nfor i, var in enumerate(numFeatures):\n    plt.subplot(3,3,i+1)\n    sns.boxplot(x='Converted', y = var, data = df, palette =colors[i])\n    plt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cap_outliers = ['TotalVisits', 'Page Views Per Visit']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Cap the outliers for the Numeric features at 0.01 and 0.99\n\nfor i, var in enumerate(cap_outliers):\n    q1 = df[var].quantile(0.01)\n    q4 = df[var].quantile(0.99)\n    df[var][df[var]<=q1] = q1\n    df[var][df[var]>=q4] = q4","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Box plot to visualise numeric features after outlier capping\nsns.set(style='ticks',color_codes=True)\ncolors = ['Accent', 'ocean' , 'RdPu'] # 'icefire' , 'ocean' , 'gist_earth', 'magma', 'prism', 'rocket', 'seismic']\nplt.figure(figsize = (12, 12))\nfor i, var in enumerate(numFeatures):\n    plt.subplot(3,3,i+1)\n    sns.boxplot(x = 'Converted', y = var, data = df, palette=colors[i])\n    plt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Impute the missing values with mean for 'TotalVisits' and 'Page Views Per Visit' \n\n* After the outlier handling, the mean of the columns for the columns 'TotalVisits' and 'Page Views Per Visit' are same for \n  converted and non converted leads. \n* We can impute the missing values with mean for the columns."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Impute the missing values for the columns with Mean\n\ndf['TotalVisits'].fillna((df['TotalVisits'].mean()), inplace=True)\ndf['Page Views Per Visit'].fillna((df['Page Views Per Visit'].mean()), inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Correlation Heat map for the numeric features\n\ncorrFeatures = numFeatures + targetFeature\n\nsns.set(style='ticks',color_codes=True)\nplt.figure(figsize = (6,6))\n\nsns.heatmap(df[corrFeatures].corr(), cmap=\"YlGnBu\", annot=True, square=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"numFeatures","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <span style=\"color:navy\"> 3.4 EDA and Data analysis for Categorical Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"#To find percent of Nan values \n#missing_data(df).head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Identify the Unique Counts for the categorical Features\n\nunique_count(df[catFeatures]).transpose() # Used transpose so as to avoid using more space. `","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"catFeatures","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"unique_count(df[catFeatures]).sort_values(by = 'unique_count', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"catFeatures[:4]\ncatFeatures[4:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Count plot for the categorical variables\nsns.set(style='ticks',color_codes=True)\n# colors =['Accent', 'PiYG' , 'RdPu', 'icefire' , 'ocean' , 'gist_earth', 'magma', 'prism', 'rocket', 'seismic']\ncolors =['gist_earth', 'magma', 'ocean', 'rocket'] * 2\nplt.figure(figsize = (15,12))\nfor i, x_var in enumerate(catFeatures[:4]):\n    plt.subplot(2, 2, i+1)\n    chart = sns.countplot(x = x_var, hue = 'Converted', data = df, palette = colors[i])\n    chart.set_xticklabels(chart.get_xticklabels(), fontsize=14, rotation=45, ha='right',)\n    plt.xlabel(x_var, fontsize=14)\n    plt.ylabel('count', fontsize=14)\n    plt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Count plot for the categorical variables\nsns.set(style='ticks',color_codes=True)\n# colors =['Accent', 'PiYG' , 'RdPu', 'icefire' , 'ocean' , 'gist_earth', 'magma', 'prism', 'rocket', 'seismic']\ncolors =['gist_earth', 'magma', 'ocean', 'rocket'] * 2\nplt.figure(figsize = (15,12))\nfor i, x_var in enumerate(catFeatures[4:]):\n    plt.subplot(2, 2, i+1)\n    chart = sns.countplot(x = x_var, hue = 'Converted', data = df, palette = colors[i])\n    chart.set_xticklabels(chart.get_xticklabels(), fontsize=14, rotation=45, ha='right',)\n    plt.xlabel(x_var, fontsize=14)\n    plt.ylabel('count', fontsize=14)\n    plt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Drop the unwanted columns: \n* **Drop the columns 'Country' and 'What matters most to you in choosing a course' as these are highly skewed**\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dropFeatures = ['Country', 'What matters most to you in choosing a course']\n\ndf.drop(dropFeatures, axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"catFeatures = []\n\nfor each in df.columns:\n    if df[each].dtype == 'object':\n        catFeatures.append(each)\n\ncatFeatures","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Replace the values with spelling corrections in the categories for categorical columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Lead Source'] = df['Lead Source'].replace(['google'], 'Google')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Replace the missing values for 'City' column with the mode"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Replace all the NaN values for categorical variables\ndf['City'] = df['City'].replace(np.nan, 'Mumbai')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for each in catFeatures:\n    print (f'Value Counts for {each}: \\n {df[each].value_counts(dropna=False)} \\n')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Bucketing the categories with lesser count for the categorical features"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Since there are so many categories in the categorical features with less than 2% counts each, we can \n# combine all those categories into one category called 'Others'\n\nfor each in catFeatures:\n    replaceFeatures = []\n    categories = df[each].value_counts()\n    list1 = df[each].value_counts().keys().tolist()\n    for i, v in enumerate (categories):\n        if v <= 200:  ## Anything less than 200\n            replaceFeatures.append(list1[i])\n    df[each] = df[each].replace(replaceFeatures, 'Others')\n    print (f'Categories replaced for column {each} are: \\n {replaceFeatures} \\n')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Replace the missing values with 'Missing' category for categorical columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"#To find percent of Nan values \n# missing_data(df).head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Replace all the NaN values with 'Missing' for the remaining Categorical variables with NaN in them\nnanFeatures = ['Specialization', 'What is your current occupation', 'Lead Source', 'Last Activity']\n\nfor each in nanFeatures:\n    df[each].replace(np.nan,'Missing', inplace=True)\n    print (f'NaNs are converted to \"Missing\" category for column {each}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Visualize the Categorical variables after handling missing values and bucketing"},{"metadata":{"trusted":true},"cell_type":"code","source":"catFeatures","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# Count plot for the categorical variables\nsns.set(style='ticks',color_codes=True)\nplt.figure(figsize = (25, 18))\ncolors = [ 'RdBu', 'rocket' , 'gist_earth'] * 2\nfor i, x_var in enumerate(catFeatures):\n    plt.subplot(2, 3, i+1)\n    chart = sns.countplot(x = x_var, hue = 'Converted', data = df, palette = colors[i])\n    chart.set_xticklabels(chart.get_xticklabels(), fontsize=16, rotation=45, ha='right')\n    plt.xlabel(x_var, fontsize=16)\n    plt.ylabel('count', fontsize=16)\n    plt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#To find percent of Nan values \nmissing_data(df).head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**There are no missing values and we can proceed with the model building**"},{"metadata":{},"cell_type":"markdown","source":"-------------"},{"metadata":{},"cell_type":"markdown","source":"## <span style=\"color:navy\"> 4. Model Building\n    \nNow that the data analysis is completed, data is cleaned and outliers handled, we can proceed to building the model. "},{"metadata":{},"cell_type":"markdown","source":"### <span style=\"color:navy\"> 4.1 Get Dummy Variables:\n    \n* For all the categorical features, dummy variables need to be created.\n* Instead of dropping the first dummy varibale for each categorical variable (using drop_first = True), we can select a specified dummy variable and drop it, so that we can have explainable features. "},{"metadata":{"trusted":true},"cell_type":"code","source":"catFeatures","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Getting dummy variables and adding the results to the master dataframe\n\nfor each in catFeatures:\n    dummy = pd.get_dummies(df[each], drop_first=False, prefix=each)\n    df = pd.concat([df,dummy],1)\n    print (f'dummy columns are added for the feature {each}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop the sepcific dummy columns created after the dummy variables are added for these categorical columns\n\ndummydropFeatures = ['Lead Origin_Others', \n                     'City_Others',\n                     'Lead Source_Missing',\n                     'Specialization_Missing',\n                     'What is your current occupation_Missing',\n                     'Last Activity_Missing']\n\ndf.drop(dummydropFeatures, axis=1, inplace=True )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"catFeatures","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop the original categorical columns since the dummy variables are added for these categorical columns\n\ndf.drop(catFeatures, axis=1, inplace=True )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <span style=\"color:navy\"> 4.2 Train-Test Split and Logistic Regression Model Building:\n\nThe following steps are followed in building a model: \n    \n* Import the necessary packages for model preprocessing and model building\n* Split the train data and test data at 70% and 30%\n* Scale the Numeric features using MinMaxScaler\n* Build the model using a combination of automatic and manual processing\n* Start the model with RFE features (automatic) and use feature reduction by dropping one feature at a time. \n* Build the model and fit the training data."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nimport statsmodels.api as sm\nfrom sklearn.linear_model import LogisticRegression\n#from sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The target variable in y\ny = df['Converted']\ny.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The feature variables in X\n\nX=df.drop('Converted', axis=1)\nX.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Splitting the data into train and test**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Splitting the data into train and test\nX_train, X_test, y_train, y_test = train_test_split(X,y, train_size=0.7, test_size=0.3, random_state=101)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <span style=\"color:navy\"> 4.3 Scaling the Numerical features \n    \n* The Numeric features need to be scaled before building the model. \n* 'TotalVisits', 'Total Time Spent on Website', 'Page Views Per Visit' are the numeric features to be scaled. "},{"metadata":{"trusted":true},"cell_type":"code","source":"numFeatures","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#### Scaling the numerical columns\nscaler = MinMaxScaler()\n\nX_train[numFeatures] = scaler.fit_transform(X_train[numFeatures])\n\nX_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <span style=\"color:navy\"> 4.4 Build the Logistic Regression model with RFE features"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Build the Logistic Regression Model\nlogmodel = LogisticRegression()\n\nfrom sklearn.feature_selection import RFE\nrfe = RFE(logmodel, 20)             # running RFE with 20 variables as output\nrfe = rfe.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print (rfe.support_)\n# list(zip(X_train.columns, rfe.support_, rfe.ranking_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#list of RFE supported columns\ncols = X_train.columns[rfe.support_]\ncols","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# Defining a function to generate the model by passing the model name and the columns used for the model \n\ndef gen_model(model_no, cols):\n    X_train_sm = sm.add_constant(X_train[cols])\n    model_no = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\n    res = model_no.fit()\n    print (res.summary())\n    return res","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.stats.outliers_influence import variance_inflation_factor\n\n# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\ndef calcVIF(col):\n    vif = pd.DataFrame()\n    vif['Features'] = X_train[col].columns\n    vif['VIF'] = [variance_inflation_factor(X_train[col].values, i) for i in range(X_train[col].shape[1])]\n    vif['VIF'] = round(vif['VIF'], 2)\n    vif = vif.sort_values(by = \"VIF\", ascending = False)\n    return vif","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model - Iteration 1"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Generate the first model using the RFE features\n\nlogm1 = LogisticRegression()\n\n#Pass the columns to generate the model and print summary\nres = gen_model(logm1, cols)\n\n# Check the VIF for the features\ncalcVIF(cols).head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"res","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---------------"},{"metadata":{},"cell_type":"markdown","source":"### <span style=\"color:navy\"> 4.5 Building Iterations of the model after reducing the features\n    \nThe next step is to build iterations of the model after dropping one feature at a time using P values and VIFs"},{"metadata":{},"cell_type":"markdown","source":"### Model - Iteration 2"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dropping the next unwanted variable to pass to the model.\ncols = cols.drop('Specialization_Supply Chain Management',1)\nlogm2 = LogisticRegression()\n\n#Pass the columns to generate the model and print summary\nres = gen_model(logm2, cols)\n\n# Check the VIF for the features\ncalcVIF(cols).head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"-------------------"},{"metadata":{},"cell_type":"markdown","source":"### Model - Iteration 3"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dropping the next unwanted variable to pass to the model.\ncols = cols.drop('Specialization_Banking, Investment And Insurance',1)\nlogm3 = LogisticRegression()\n\n#Pass the columns to generate the model and print summary\nres = gen_model(logm3, cols)\n\n# Check the VIF for the features\ncalcVIF(cols).head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"------"},{"metadata":{},"cell_type":"markdown","source":"### Model - Iteration 4"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dropping the next unwanted variable to pass to the model.\ncols = cols.drop('Specialization_Finance Management',1)\nlogm4 = LogisticRegression()\n\n#Pass the columns to generate the model and print summary\nres = gen_model(logm4, cols)\n\n# Check the VIF for the features\ncalcVIF(cols).head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"-----"},{"metadata":{},"cell_type":"markdown","source":"### Model - Iteration 5"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dropping the next unwanted variable to pass to the model.\ncols = cols.drop('Specialization_Marketing Management',1)\nlogm5 = LogisticRegression()\n\n#Pass the columns to generate the model and print summary\nres = gen_model(logm5, cols)\n\n# Check the VIF for the features\ncalcVIF(cols).head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"-------------"},{"metadata":{},"cell_type":"markdown","source":"### Model - Iteration 6"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dropping the next unwanted variable to pass to the model.\ncols = cols.drop('Lead Source_Reference',1)\nlogm6 = LogisticRegression()\n\n#Pass the columns to generate the model and print summary\nres = gen_model(logm6, cols)\n\n# Check the VIF for the features\ncalcVIF(cols).head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"-------------"},{"metadata":{},"cell_type":"markdown","source":"### Model - Iteration 7"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dropping the next unwanted variable to pass to the model.\ncols = cols.drop('Page Views Per Visit',1)\nlogm7 = LogisticRegression()\n\n#Pass the columns to generate the model and print summary\nres = gen_model(logm7, cols)\n\n# Check the VIF for the features\ncalcVIF(cols).head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"-------------"},{"metadata":{},"cell_type":"markdown","source":"### Model - Iteration 8"},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Dropping the next unwanted variable to pass to the model.\n# cols = cols.drop('',1)\n# logm8 = LogisticRegression()\n\n# #Pass the columns to generate the model and print summary\n# res = gen_model(logm8, cols)\n\n# # Check the VIF for the features\n# calcVIF(cols).head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"-----------------"},{"metadata":{},"cell_type":"markdown","source":"### <span style=\"color:navy\"> 4.6 Getting the predicted values on the train set\n    \nThe following steps are done after building the model\n    \n* Get the predictions on the training dataset with the final model \n* Use the cut-off with 0.5 for the initial predictions\n* Derive the Classification report and Classification metrics with the initial cutoff and predictions\n* Derive the Area under the ROC curve for the initial cut-off and predictions\n* Calculate the predicted values for the different cut-offs to arrive at the optimal cutoff\n* Plot the Sensitivity / Specificity curve for the different cut-offs and identify the optimal cut-off\n* Get the final_Predictions and the metrics for the Predictions with the optimal cut-off\n* Assign a Lead Score to the Training dataset based on the Conversion probability of the final_Predictions\n* Measuring the Precision Recall Trade-off"},{"metadata":{},"cell_type":"markdown","source":"**Get the predictions on the training dataset with the final model.**\n* Use the cut-off with 0.5 for the initial predictions."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Getting the predicted values on the train set\n\nX_train_sm = sm.add_constant(X_train[cols])\ny_train_pred = res.predict(X_train_sm)\ny_train_pred[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_pred_final = pd.DataFrame({'Converted':y_train.values, 'Converted_prob':y_train_pred})\ny_train_pred_final['Prospect ID'] = y_train.index\ny_train_pred_final.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_pred_final['Predicted'] = y_train_pred_final.Converted_prob.map(lambda x: 1 if x > 0.5 else 0)\n\n# Let's see the head\ny_train_pred_final.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <span style=\"color:navy\"> 4.7 Evaluation Metrics for the Train dataset\n    \n**Derive the Classification report and Classification metrics with the initial cutoff and predictions**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print (classification_report(y_train_pred_final['Converted'], y_train_pred_final['Predicted']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\nfrom sklearn.metrics import confusion_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_metrics(actual, predicted):\n    confusion = confusion_matrix(actual, predicted)\n\n    # Let's check the overall accuracy.\n    Accuracy = metrics.accuracy_score(actual, predicted)\n\n    TN = confusion[0,0] # true negatives\n    FP = confusion[0,1] # false positives\n    FN = confusion[1,0] # false negatives\n    TP = confusion[1,1] # true positive \n\n    # Calculate the different Metrics\n    Sensitivity = TP / float(TP+FN) # calculate Sensitivity\n    Specificity = TN / float(TN+FP) # calculate specificity\n    Precision   = TP / float(TP+FP) # calculate Precision\n    Recall      = TN / float(TN+FP) # calculate Recall\n    FPR = (FP/ float(TN+FP))        # Calculate False Postive Rate - predicting conversion when customer does not convert\n    PPV = (TP / float(TP+FP))       # positive predictive value \n    NPV = (TN / float(TN+ FN))      # Negative predictive value\n    \n    F1 = 2*(Precision*Recall)/(Precision+Recall)\n\n    # Print the Metrics\n    print (f'The Confusion Matrix is \\n {confusion}')\n    print (f'The Accuracy is    : {round (Accuracy,2)} ({Accuracy})')\n    print (f'The Sensitivity is : {round (Sensitivity,2)} ({Sensitivity})')\n    print (f'The Specificity is : {round (Specificity,2)} ({Specificity})')\n    print (f'The Precision is   : {round (Precision, 2)} ({Precision})')\n    print (f'The Recall is      : {round (Recall, 2)} ({Recall})')\n    print (f'The f1 score is    : {round (F1, 2)} ({F1})')\n    print (f'The False Positive Rate is       : {round (FPR, 2)} ({FPR})')\n    print (f'The Positive Predictive Value is : {round (PPV, 2)} ({PPV})')\n    print (f'The Negative Predictive Value is : {round (NPV, 2)} ({NPV})')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_confusion_metrics(actual, predicted):\n    sns.set_style('white')\n    cm = confusion_matrix(actual, predicted)\n    plt.clf()\n    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Wistia)\n    classNames = ['Negative','Positive']\n    plt.title('True Converted and Predicted Converted Confusion Matrix', fontsize=14)\n    plt.ylabel('True Converted', fontsize=14)\n    plt.xlabel('Predicted Converted', fontsize=14)\n    tick_marks = np.arange(len(classNames))\n    plt.xticks(tick_marks, classNames, fontsize=14)\n    plt.yticks(tick_marks, classNames, fontsize=14)\n    s = [['TN','FP'], ['FN', 'TP']]\n    for i in range(2):\n        for j in range(2):\n            plt.text(j,i, str(s[i][j])+\" = \"+str(cm[i][j]), fontsize=14, ha='center')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_metrics(y_train_pred_final.Converted, y_train_pred_final.Predicted)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_confusion_metrics(y_train_pred_final.Converted, y_train_pred_final.Predicted)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Derive the Area under the ROC curve for the initial cut-off and predictions**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def draw_roc( actual, probs ):\n    fpr, tpr, thresholds = metrics.roc_curve( actual, probs,\n                                              drop_intermediate = False )\n    auc_score = metrics.roc_auc_score( actual, probs )\n    plt.figure(figsize=(5, 5))\n    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic')\n    plt.legend(loc=\"lower right\")\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fpr, tpr, thresholds = metrics.roc_curve( y_train_pred_final.Converted, \n                                          y_train_pred_final.Converted_prob, drop_intermediate = False )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"draw_roc(y_train_pred_final.Converted, y_train_pred_final.Converted_prob)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###  <span style=\"color:navy\"> 4.8 Getting the Optimal cutoff and final evaluation Metrics for Train Dataset\n    \n**Calculate the predicted values for the different cut-offs to arrive at the optimal cutoff**"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# Let's create columns with different probability cutoffs \nnumbers = [float(x)/10 for x in range(10)]\nfor i in numbers:\n    y_train_pred_final[i]= y_train_pred_final.Converted_prob.map(lambda x: 1 if x > i else 0)\ny_train_pred_final.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"numbers","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Plot the Sensitivity / Specificity curve for the different cut-offs and identify the optimal cut-off**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now let's calculate accuracy sensitivity and specificity for various probability cutoffs.\ncutoff_df = pd.DataFrame( columns = ['probability','accuracy','sensitivity','specificity'])\nfrom sklearn.metrics import confusion_matrix\n\n#     TN = confusion[0,0] # true negatives\n#     FP = confusion[0,1] # false positives\n#     FN = confusion[1,0] # false negatives\n#     TP = confusion[1,1] # true positive \n    \nfor i in numbers:\n    cm1 = metrics.confusion_matrix(y_train_pred_final['Converted'], y_train_pred_final[i] )\n    total1=sum(sum(cm1))\n    accuracy = (cm1[0,0]+cm1[1,1])/total1\n    \n    specificity = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n    sensitivity = cm1[1,1]/(cm1[1,0]+cm1[1,1])\n    cutoff_df.loc[i] =[ i , accuracy, sensitivity, specificity]\nprint(cutoff_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's plot accuracy sensitivity and specificity for various probabilities.\n\nsns.set_style(\"whitegrid\") # white/whitegrid/dark/ticks\nsns.set_context(\"paper\") # talk/poster\ncutoff_df.plot.line(x='probability', y=['accuracy','sensitivity','specificity'], figsize=(10,6))\n\nplt.xticks(np.arange(0, 1, step=0.05), size = 12)\nplt.yticks(size = 12)\nplt.title('Accuracy, Sensitivity and Specificity for various probabilities', fontsize=14)\nplt.xlabel('Probability', fontsize=14)\nplt.ylabel('Metrics', fontsize=14)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### <span style=\"color:blue\"> From the curve above, 0.36 can be taken as the optimum point to take it as a cutoff probability"},{"metadata":{},"cell_type":"markdown","source":"**Get the final_Predictions and the metrics for the Predictions with the optimal cut-off**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#### From the curve above, 0.36 is the optimum point to take it as a cutoff probability.\n\ny_train_pred_final['final_Predicted'] = y_train_pred_final.Converted_prob.map( lambda x: 1 if x > 0.36 else 0)\ny_train_pred_final.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get all the necessary Metrics for the Training dataset for cut-off 0.36\nprint (f'The Final Evaluation Metrics for the train Dataset: ')\nprint (f'----------------------------------------------------')\n\nget_metrics(y_train_pred_final['Converted'], y_train_pred_final['final_Predicted'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot Confusion metrics for final predicted for train data\n\nplot_confusion_metrics(y_train_pred_final.Converted, y_train_pred_final.final_Predicted)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Classification report for the training dataset\nprint (classification_report(y_train_pred_final['Converted'], y_train_pred_final['final_Predicted']))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Assign a Lead Score to the Training dataset based on the Conversion probability of the final_Predictions**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Assign a Lead score based on the predictions\n\ny_train_pred_final['Lead_Score'] = y_train_pred_final.Converted_prob.map( lambda x: round(x*100))\n\ny_train_pred_final[['Converted','Converted_prob','Prospect ID','final_Predicted','Lead_Score']].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_pred_final.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Measuring the Precision Recall Trade-off"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import precision_recall_curve\n\np, r, thresholds = precision_recall_curve(y_train_pred_final['Converted'], y_train_pred_final['Converted_prob'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot the Precision / Recall tradeoff chart\nsns.set_style(\"whitegrid\") # white/whitegrid/dark/ticks\nsns.set_context(\"paper\") # talk/poster\n\nplt.figure(figsize=(8, 4), dpi=100, facecolor='w', edgecolor='k', frameon='True')\nplt.plot(thresholds, p[:-1], \"g-\")\nplt.plot(thresholds, r[:-1], \"r-\")\nplt.xticks(np.arange(0, 1, step=0.05))\nplt.title('Precision and Recall for various probabilities', fontsize=14)\nplt.xlabel('Probability', fontsize=14)\nplt.ylabel('Metrics', fontsize=14)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"--------------"},{"metadata":{},"cell_type":"markdown","source":"## <span style=\"color:navy\"> 5. Model Validation \n    \nThe next step is to validate the model with the test dataset. \n\nThe following are the steps invoved:\n* Fit the Numeric features of the Test dataset with the Scaler method\n* Making Predictions on the X_test dataset\n* Create a Dataset with the Prospect ID and the conversion probability for the test dataset\n* Generate the Lead Score for the test dataset based on the predicted probability from the model\n* Get the final Predicted values using the optimal threshold value\n* Get the Final evaluation Metrics for the test dataset with the actual converted values and final predicted values\n\n    \n### <span style=\"color:navy\"> 5.1 Making Predictions for the Test Dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Fit the Numeric features of the Test dataset with the Scaler method**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fit the Numeric features of the Test dataset with the Scaler method\nX_test[numFeatures] = scaler.transform(X_test[numFeatures])\nX_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Making Predictions on the X_test dataset using the final model**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Making Predictions on the X_test dataset\n\nX_test = X_test[cols]\nX_test_sm = sm.add_constant(X_test)\nX_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test_pred = res.predict(X_test_sm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test_pred[:5]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Create a Dataset with the Prospect ID and the conversion probability for the test dataset**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Converting y_pred to a dataframe from an array\ny_test_pred_df = pd.DataFrame(y_test_pred)\n\n# Converting y_test to dataframe\ny_test_df = pd.DataFrame(y_test)\n\n# Putting CustID to index\ny_test_pred_df['Prospect ID'] = y_test_df.index\n\n# Removing index for both dataframes to append them side by side \ny_test_pred_df.reset_index(drop=True, inplace=True)\ny_test_df.reset_index(drop=True, inplace=True)\n\n# Appending y_test_df and y_testest_pred_1\ny_test_pred_final = pd.concat([y_test_df, y_test_pred_df],axis=1)\n\n# Renaming the column \ny_test_pred_final= y_test_pred_final.rename(columns={ 0 : 'Converted_prob'})\ny_test_pred_final.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Generate the Lead Score for the test dataset based on the predicted probability from the model**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Rearranging the columns\ny_test_pred_final = y_test_pred_final[['Prospect ID','Converted','Converted_prob']]\ny_test_pred_final['Lead_Score'] = y_test_pred_final.Converted_prob.map( lambda x: round(x*100))\ny_test_pred_final.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Get the final Predicted values using the optimal threshold value**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predict the final y values based on the threshold of 0.3\ny_test_pred_final['final_Predicted'] = y_test_pred_final['Converted_prob'].map(lambda x: 1 if x > 0.36 else 0)\n\ny_test_pred_final.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <span style=\"color:navy\"> 5.2 Final Evaluation Metrics for the Test Dataset"},{"metadata":{},"cell_type":"markdown","source":"**Get the Final evaluation Metrics for the test dataset with the actual converted values and final predicted values**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get all the necessary Metrics for the Test dataset \n\nprint (f'The Final Evaluation Metrics for the test Dataset: ')\nprint (f'---------------------------------------------------')\nget_metrics(y_test_pred_final['Converted'], y_test_pred_final['final_Predicted'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot Confusion metrics for final predicted for test data\n\nplot_confusion_metrics(y_test_pred_final.Converted, y_test_pred_final.final_Predicted)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Print the classification report for the Test Dataset\nprint (classification_report(y_test_pred_final['Converted'], y_test_pred_final['final_Predicted']))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"----------"},{"metadata":{},"cell_type":"markdown","source":"## <span style=\"color:navy\"> 6. Assigning the Lead score for each Prospect ID from the original data\n\nThe final step is to merge the datasets from Train and Test datasets with the predicted Lead Score and attach the Lead score to the original dataset. "},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_pred_final.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create Dataset with y_train Prospect ID and Lead score\ny_train_score = y_train_pred_final[['Prospect ID','Lead_Score']]\n\n# Create Dataset with y_test Prospect ID and Lead score\ny_test_score = y_test_pred_final[['Prospect ID','Lead_Score']]\n\n# Concatenate the y_train scores and the y_test scores\ndf_score = pd.concat([y_train_score, y_test_score], ignore_index=True)\n\n# Set the index of the final score dataset as the Prospect ID to concatenate the score dataset to the original data\ndf_score.set_index('Prospect ID', inplace=True)\n\n# Inner Join the Original Leads dataset with the scores dataset. This will add a new column 'Lead_Score' to the \n# Original dataset. \ndf_orig = df_orig.join(df_score['Lead_Score'])\n\ndf_orig.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"------------------"},{"metadata":{},"cell_type":"markdown","source":"## <span style=\"color:navy\"> 7. Determining Feature Importance"},{"metadata":{},"cell_type":"markdown","source":"#### Selecting the coefficients of the selected features from our final model excluding the intercept"},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.options.display.float_format = '{:.2f}'.format\nmodel_params = res.params[1:]\nmodel_params","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Getting a relative coeffient value for all the features wrt the feature with the highest coefficient"},{"metadata":{"trusted":true},"cell_type":"code","source":"#feature_importance = abs(new_params)\n\nfeature_importance = model_params\nfeature_importance = 100.0 * (feature_importance / feature_importance.max())\nfeature_importance","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Sorting the feature variables based on their relative coefficient values"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# Sort the feature variables based on their relative coefficient values\n\nsorted_idx = np.argsort(feature_importance,kind='quicksort',order='list of str')\nsorted_idx","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Plot showing the feature variables based on their relative coefficient values"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# Plot to show the realtive Importance of each feature in the model \npos = np.arange(sorted_idx.shape[0]) + .5\n\nfig = plt.figure(figsize=(10,6))\nax = fig.add_subplot(1, 1, 1)\nax.barh(pos, feature_importance[sorted_idx], align='center', color = 'tab:blue',alpha=0.8)\nax.set_yticks(pos)\nax.set_yticklabels(np.array(X_train[cols].columns)[sorted_idx], fontsize=12)\nax.set_xlabel('Relative Feature Importance', fontsize=14)\n\nplt.tight_layout()   \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---------------"},{"metadata":{},"cell_type":"markdown","source":"## <span style=\"color:navy\"> 8. Final Observations and Recommendations"},{"metadata":{},"cell_type":"markdown","source":"#### <span style=\"color:navy\"> The Final Evaluation Metrics for the train Dataset: \n\n* The Accuracy is    : 0.80\n* The Sensitivity is : 0.80\n* The Specificity is : 0.81\n* The Precision is   : 0.73\n* The Recall is      : 0.81\n* The f1 score is    : 0.76\n    \n#### <span style=\"color:navy\"> The Final Evaluation Metrics for the test Dataset: \n\n* The Accuracy is    : 0.81\n* The Sensitivity is : 0.81\n* The Specificity is : 0.81\n* The Precision is   : 0.72\n* The Recall is      : 0.81\n* The f1 score is    : 0.76\n    \n#### <span style=\"color:navy\"> X-Education has a better chance of converting a potential lead when:\n* **The total time spent on the Website is high:**\nLeads who have spent more time on the website have converted\n* **Current Occupation is specified:**\nLeads who are working professionals have high chances of getting converted. People who were looking for better prospects like Unemployed, students, Housewives and Business professionals were also good prospects to focus on. \n* **When the Lead origin was Lead Add form**\nLeads who have responded/ or engaged through Lead Add Forms have had a higher chances of getting converted\n* **Number of Total Visits were high** \nLeads who have made a greater number of visits have higher chances of getting converted. \n* **When the last activity was SMS sent or Email opened**\nMembers who have sent an SMS for enquiry or who have opened the email have a higher chance of getting converted."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}