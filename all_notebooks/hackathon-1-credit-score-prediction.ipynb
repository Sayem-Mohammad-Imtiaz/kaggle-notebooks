{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Problem statement:\n\n\nUse the Root Mean Square Error (RMSE).\nIf the absolute error of a prediction is greater than 4.0, I regard the prediction as \"wrong\". Otherwise, it is \"correct\".\nAny other evaluation measure that you believe is appropriate.\n"},{"metadata":{},"cell_type":"markdown","source":"We will use pandas and scikit-learn to load and explore the dataset. The dataset can easily be loaded from scikit-learn’s datasets module using read_csv function.\n\nAlso we will try all Linear Regression models and Benchmark the best and predict using the same."},{"metadata":{},"cell_type":"markdown","source":"## Descriptive Analysis\n\nLet's check the file: ../input/innercity.csv\n\nIt is an important first step for conducting statistical analysis. It gives an idea of the distribution of the data, helps us to detect outliers and typos, and identify associations among variables, thus preparing you for conducting further statistical analyses"},{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt # plotting\nimport numpy as np # linear algebra\nimport os # accessing directory structure\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is 1 csv file in the current version of the dataset:"},{"metadata":{"trusted":true},"cell_type":"code","source":"for dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The next hidden code cells define functions for plotting data. Click on the \"Code\" button in the published kernel to reveal the hidden code."},{"metadata":{"trusted":true},"cell_type":"code","source":"nRowsRead = 1000 # specify 'None' if want to read whole file\n# innercity.csv has 21613 rows in reality, but we are only loading/previewing the first 1000 rows\ndf1 = pd.read_csv('/kaggle/input/credit-score-prediction/CreditScore_train.csv', delimiter=',')\ndf1.dataframeName = 'CreditScore_train.csv'\nnRow, nCol = df1.shape\nprint(f'TRAIN DATA : There are {nRow} rows and {nCol} columns')\n\ndf2 = pd.read_csv('/kaggle/input/credit-score-prediction/CreditScore_test.csv', delimiter=',')\ndf2.dataframeName = 'CreditScore_test.csv'\nnRow, nCol = df2.shape\nprint(f'TEST DATA : There are {nRow} rows and {nCol} columns')\n\ndf1[\"source\"] = \"train\"\ndf2[\"source\"] = \"test\"\n\nmerged_df = pd.concat([df1,df2])\nmerged_df.dataframeName = 'Merged_DF'\n\nnRow, nCol = merged_df.shape\nprint(f'MERGED DATA : There are {nRow} rows and {nCol} columns')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's take a quick look at what the data looks like:"},{"metadata":{},"cell_type":"markdown","source":"We can easily convert the dataset into a pandas dataframe to perform exploratory data analysis. Simply pass in the dataset.data as an argument to pd.DataFrame(). We can view the first 5 rows in the dataset using head() function."},{"metadata":{"trusted":true},"cell_type":"code","source":"merged_df.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Find the dimension of given data\n"},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"merged_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"merged_df.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can check the datatype of each column using dtypes to make sure every column has numeric datatype. If a column has different datatype such as string or character, we need to map that column to a numeric datatype such as integer or float. For this dataset, luckily there is no such column."},{"metadata":{"trusted":true},"cell_type":"code","source":"merged_df.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"merged_df.isnull().any()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"merged_df.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"merged_df.duplicated().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, we will understand the statistical summary of the dataset using the describe() function. Using this function, we can understand the count, min, max, mean and standard deviation for each attribute (column) in the dataset. \n\nEach of these can also be displayed individually using df.count(), df.min(), df.max(), df.median() and df.quantile(q)."},{"metadata":{"trusted":true},"cell_type":"code","source":"merged_df.describe(include='all').T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Missing data\nImportant questions when thinking about missing data:\n\n* How prevalent is the missing data?\n* Is missing data random or does it have a pattern?\n\nThe answer to these questions is important for practical reasons because missing data can imply a reduction of the sample size. This can prevent us from proceeding with the analysis. Moreover, from a substantive perspective, we need to ensure that the missing data process is not biased and hidding an inconvenient truth.\n\nSometimes, in a dataset we will have missing values such as NaN or empty string in a cell. We need to take care of these missing values so that our machine learning model doesn’t break. To handle missing values, there are three approaches followed.\n\n    Replace the missing value with a large negative number (e.g. -999).\n    Replace the missing value with mean of the column.\n    Replace the missing value with median of the column.\n\nTo find if a column in our dataset has missing values, you can use pd.isnull(df).any() which returns a boolean for each column in the dataset that tells if the column contains any missing value. In this dataset, there are no missing values!"},{"metadata":{"trusted":true},"cell_type":"code","source":"##missing data\ntotal = merged_df.count()\nsumcol=merged_df.isnull().sum()\ncountcol=merged_df.isnull().count()\n\npercent = (merged_df.isnull().sum()/countcol*100).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent,sumcol,countcol], axis=1, keys=['Total', 'Percent','Sumcol','countcol'])\nmissing_data.sort_values(['Percent'], axis=0, ascending=False)\n#missing_data.head(20)\n\nmiss_perc=missing_data.sort_values(['Percent'], axis=0, ascending=False)\nmiss_perc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\n#missing data\ntotal = merged_df.count()\nsumcol=merged_df.isnull().sum()\ncountcol=merged_df.isnull().count()\n\npercent = (merged_df.isnull().sum()/countcol*100).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent,sumcol,countcol], axis=1, keys=['Total', 'Percent','Sumcol','countcol'])\n#missing_data.head(20)\nmiss_perc=missing_data.sort_values(['Percent'], axis=0, ascending=False)\nm_per = miss_perc[miss_perc.Percent > 60]\nprint(m_per)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"drop_cols=m_per.index\nprint(drop_cols)\n#[cols.append(i) for i in drop_cols if df[i].isnull().sum()/row*100 > 60 ]\n#count=0\nfiltered_df=merged_df.drop(columns=drop_cols,axis=1)\n\n#for i in drop_cols:\n #   print(i)\n#    count=count+1\n#filt_concat_df=df_concat.drop(columns=[i],axis=1)\nprint(filtered_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"filtered_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"filtered_df['y']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Exploratory Analysis\nTo begin this exploratory analysis, first import libraries and define functions for plotting the data using `matplotlib`. Depending on the data, not all plots will be made."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# Distribution graphs (histogram/bar graph) of column data\ndef plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):\n    nunique = df.nunique()\n    df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values\n    nRow, nCol = df.shape\n    columnNames = list(df)\n    nGraphRow = (nCol + nGraphPerRow - 1) / nGraphPerRow\n    plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')\n    for i in range(min(nCol, nGraphShown)):\n        plt.subplot(nGraphRow, nGraphPerRow, i + 1)\n        columnDf = df.iloc[:, i]\n        if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):\n            valueCounts = columnDf.value_counts()\n            valueCounts.plot.bar()\n        else:\n            columnDf.hist()\n        plt.ylabel('counts')\n        plt.xticks(rotation = 90)\n        plt.title(f'{columnNames[i]} (column {i})')\n    plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)\n    plt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Correlation\n\nFinding correlation between attributes is a highly useful way to check for patterns in the dataset. Pandas offers three different ways to find correlation between attributes (columns). The output of each of these correlation functions fall within the range [-1, 1].\n\n    1 - Positively correlated\n    -1 - Negatively correlated.\n    0 - Not correlated.\n    \nWe will use df.corr() function to compute the correlation between attributes\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"filtered_df.corr()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have 3 methods\n\n* PEARSON CORRELATION\n* SPEARMAN CORRELATION\n* KENDALL CORRELATION\n\nLet's do only PEARSON CORRELATION\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#PEARSON CORRELATION\n\nplt.figure(figsize = (15,10))\nsns.heatmap(filtered_df.corr(method=\"pearson\"))\nplt.title('PEARSON CORRELATION', fontsize=15)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# Correlation matrix\ndef plotCorrelationMatrix(df, graphWidth):\n    filename = df.dataframeName\n    df = df.dropna('columns') # drop columns with NaN\n    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n    if df.shape[1] < 2:\n        print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')\n        return\n    corr = df.corr()\n    plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')\n    corrMat = plt.matshow(corr, fignum = 1)\n    plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)\n    plt.yticks(range(len(corr.columns)), corr.columns)\n    plt.gca().xaxis.tick_bottom()\n    plt.colorbar(corrMat)\n    plt.title(f'Correlation Matrix for {filename}', fontsize=15)\n    plt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# Scatter and density plots\ndef plotScatterMatrix(df, plotSize, textSize):\n    df = df.select_dtypes(include =[np.number]) # keep only numerical columns\n    # Remove rows and columns that would lead to df being singular\n    df = df.dropna('columns')\n    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n    columnNames = list(df)\n    if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots\n        columnNames = columnNames[:10]\n    df = df[columnNames]\n    ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')\n    corrs = df.corr().values\n    for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):\n        ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)\n    plt.suptitle('Scatter and Density Plot')\n    plt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Visualize the dataset\n\nNow you're ready to read in the data and use the plotting functions to visualize the data.\n\nWe will use two types of visualization strategy namely univariate plots and bivariate plots. As the name suggests, univariate plot is used to visualize a single column or an attribute whereas bivariate plot is used to visualize two columns or two attributes.\n\n## Box plot\n\nA box-whisker plot is a univariate plot used to visualize a data distribution.\n\n* * The ends of whiskers are the maximum and minimum range of data distribution.\n* The central line in the box is the median of the entire data distribution.\n* The right and left edges in the box are the medians of data distribution to the right and left from the central median, respectively.\n"},{"metadata":{},"cell_type":"markdown","source":"View the above Box plot clearly by adding figsize"},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline\nplt.figure(figsize = (28,8))\nsns.boxplot(data=filtered_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Distribution graphs (histogram/bar graph) of sampled columns:"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(pd.isnull(filtered_df).any())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Extraction\n\nNot applicable"},{"metadata":{"trusted":true},"cell_type":"code","source":"filtered_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##filtered_df = filtered_df.drop(columns=['source'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"filtered_df.isnull().any()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"filtered_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Correlation with output variable\ncor_target = abs(filtered_df.corr()[\"y\"])\n#Selecting highly correlated features\nrelevant_features = cor_target[cor_target<0.3]\nrelevant_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"relevant_features.item","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"filtered_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lst_key=[]\nnull_key=[]\nfor i,j in relevant_features.items():\n    lst_key.append(i)\n#print(lst_key.count())\n\nfinal_df=filtered_df.drop(columns=lst_key,axis=1)\nprint(final_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a=final_df.isnull().any()==True","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i,j in a.items():\n    if j==True:\n        null_key.append(i)\nprint(null_key)\nfinal_df=filtered_df.drop(columns=lst_key,axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"type(a)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i,j in a.items():\n    if j==True:\n        null_key.append(i)\nprint(null_key)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in null_key:\n    final_df[i].fillna(final_df[i].mean(),inplace=True)\nfinal_df.shape ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_final = final_df[final_df.source==\"train\"]\ntest_final = final_df[final_df.source==\"test\"]\n\nprint(train_final.shape)\nprint(test_final.shape)\n\ntrain_final.drop(columns=\"source\",inplace=True)\ntest_final.drop(columns=\"source\",inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train_final.drop(\"y\", axis=1)\nY = train_final[\"y\"]\nprint(X.shape)\nprint(Y.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we see different data distributions, we will standardize the dataset using StandardScaler function in scikit-learn. This is a useful technique where the attributes are transformed to a standard gaussian distribution with a mean of 0 and a standard deviation of 1."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler, MinMaxScaler\nscaler = MinMaxScaler().fit(X)\nscaled_X = scaler.transform(X)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, we will split the data into train and test set. We can easily do this using scikit-learn’s train_test_split() function using a test_size parameter."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nseed      = 42\ntest_size = 0.20\n\nX_train, X_test, Y_train, Y_test = train_test_split(scaled_X, Y, test_size = test_size, random_state = seed)\n\nprint(X_train.shape)\nprint(X_test.shape)\nprint(Y_train.shape)\nprint(Y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let’s dive into Regression. We will use different Regression models offered by scikit-learn to produce a baseline accuracy for this problem. \n\nWe will use the MAE (Mean Absolute Error) as the performance metric for the regression models."},{"metadata":{},"cell_type":"markdown","source":"## Training Regression Model\n\nBy looking at the dataset, we simply can’t suggest the best Regression Model for this problem. So, we will try out different Regression models available in scikit-learn with a k-fold cross validation method.\n\nlet's assume k = 5 (k-fold cross validation)\n\nIt means we split the training data into train and test data using a test_size parameter for 10-folds. Each fold will have different samples that are not present in other folds. By this way, we can throughly train our model on different samples in the dataset.\n\nBefore doing anything, we will split our dataframe df into features X and target Y."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\n\nimport time\nimport datetime\n\nstart = 0\nend = 0\nstart = time.time()\n\n# user variables to tune\nfolds   = 10\nmetric  = \"neg_mean_absolute_error\"\n\n# hold different regression models in a single dictionary\nmodels = {}\nmodels[\"Linear\"]        = LinearRegression()\nmodels[\"Lasso\"]         = Lasso()\nmodels[\"Ridge\"]         = Ridge()\nmodels[\"ElasticNet\"]    = ElasticNet()\nmodels[\"DecisionTree\"]  = DecisionTreeRegressor()\nmodels[\"KNN\"]           = KNeighborsRegressor()\nmodels[\"RandomForest\"]  = RandomForestRegressor()\nmodels[\"AdaBoost\"]      = AdaBoostRegressor()\nmodels[\"GradientBoost\"] = GradientBoostingRegressor()\nmodels[\"XGBoost\"] = XGBRegressor()\n\n# 10-fold cross validation for each model\nmodel_results = []\nmodel_names   = []\nfor model_name in models:\n\tmodel   = models[model_name]\n\tk_fold  = KFold(n_splits=folds, random_state=seed)\n\tresults = cross_val_score(model, X_train, Y_train, cv=k_fold, scoring=metric)\n\t\n\tmodel_results.append(results)\n\tmodel_names.append(model_name)\n\tprint(\"{}: {}, {}\".format(model_name, round(results.mean(), 3), round(results.std(), 3)))\n\tend = time.time()\n\tlist_lapse = end - start\n\tprint(\"Time taken for processing {}: {}\".format(model_name, str(datetime.timedelta(seconds=list_lapse))))\n\n# box-whisker plot to compare regression models\nfigure = plt.figure(figsize = (20,8))\n\nfigure.suptitle('Regression models comparison')\naxis = figure.add_subplot(111)\nplt.boxplot(model_results)\naxis.set_xticklabels(model_names, rotation = 45, ha=\"right\")\naxis.set_ylabel(\"Mean Absolute Error (MAE)\")\nplt.margins(0.05, 0.1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Choosing the best model\n\nBased on the above comparison, we can see that Gradient Boosting Regression model outperforms all the other regression models. So, we will choose it as the best Regression Model for this problem."},{"metadata":{"trusted":true},"cell_type":"code","source":"model = XGBRegressor(objective ='reg:squarederror')\nmodel.fit(X_train,Y_train)\n\n#Predicting TEST & TRAIN DATA\ntrain_predict = model.predict(X_train)\ntest_predict = model.predict(X_test)\n\nerror_percent = np.mean(np.abs((Y_train - train_predict) / Y_train)) * 100\nprint(\"MAPE - Mean Absolute Percentage Error (TRAIN DATA): \",error_percent )\nY_train, train_predict = np.array(Y_train), np.array(train_predict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = XGBRegressor(objective ='reg:squarederror')\nmodel.fit(X_test,Y_test)\n\n#Predicting TEST & TRAIN DATA\ntrain_predict = model.predict(X_train)\ntest_predict = model.predict(X_test)\n\nerror_percent = np.mean(np.abs((Y_train - train_predict) / Y_train)) * 100\nprint(\"MAPE - Mean Absolute Percentage Error (TEST DATA): \",error_percent )\nY_train, train_predict = np.array(Y_train), np.array(train_predict)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can visualize the predictions made by our best model and the original targets Y_test using the below code."},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot between predictions and Y_test\nx_axis = np.array(range(0, test_predict.shape[0]))\nplt.figure(figsize=(20,10))\nplt.plot(x_axis, test_predict, linestyle=\"--\", marker=\"o\", alpha=0.7, color='r', label=\"predictions\")\nplt.plot(x_axis, Y_test, linestyle=\"--\", marker=\"o\", alpha=0.7, color='g', label=\"Y_test\")\nplt.xlabel('Row number')\nplt.ylabel('PRICE')\nplt.title('Predictions vs Y_test')\nplt.legend(loc='lower right')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We could still tune different regression models used in this example using scikit-learn’s GridSearchCV() function. By tuning, we mean trying out different hyper-parameters for each model."},{"metadata":{},"cell_type":"markdown","source":"## Feature Importance\n\nOnce we have a trained model, we can understand feature importance (or variable importance) of the dataset which tells us how important each feature is, to predict the target. Below chart shows relative importance of different feature in the dataset made by our best model Gradient Boosting Regressor (GBR)."},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_importance = model.feature_importances_\nfeature_importance = 100.0 * (feature_importance / feature_importance.max())\n\nsorted_idx = np.argsort(feature_importance)\npos        = np.arange(sorted_idx.shape[0]) + .5\n\nplt.figure(figsize = (15,18))\n\n#Make a horizontal bar plot.\nplt.barh(pos, feature_importance[sorted_idx], align='center')\nplt.yticks(pos, df1.columns[sorted_idx])\nplt.xlabel('Relative Importance')\nplt.title('Variable Importance')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Steps for Further Improvement\n\nSome additional steps that may be taken to improve score could be:\n\nDo additional pre-processing on the given data\nNot sure regularized Linear Regression approach is fine? Or prefer Ensemble methods? Or maybe something else?\nIntroduce a greater variety of base models for learning. The more uncorrelated the results, the better the final score.\n\n## It's up to you to find out.😉\n"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"nbformat":4,"nbformat_minor":1}