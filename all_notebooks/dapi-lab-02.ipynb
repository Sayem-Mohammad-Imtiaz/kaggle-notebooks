{"cells":[{"metadata":{},"cell_type":"markdown","source":"Primero se cargan las librerias que se usarán:"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neural_network import MLPRegressor as mlp_r\nfrom sklearn.neural_network import MLPClassifier as mlp_c\nfrom sklearn.metrics import confusion_matrix\n\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nfrom sklearn import datasets\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Regresión\n\nEl ejercicio de regresión se realizará con la base de datos  [\"Wine Quality\"](https://www.kaggle.com/uciml/red-wine-quality-cortez-et-al-2009). Es fundamental antes de comenzar cualquier ejercicio de Machine Learning o Inteligencia Artificial conocer los datos, por lo cual se les recomienda realizar un Análisis Exploratorio de los Datos previamente ([EDA](https://towardsdatascience.com/a-gentle-introduction-to-exploratory-data-analysis-f11d843b8184)). Primero cargamos la base de datos de vinos:"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"wine_data = pd.read_csv(\"../input/winequality-red.csv\")\nwine_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Se construyen nuestras variables de trabajo, por lo notaciòn \"X\" corresponde a las variables independientes y \"y\" a la variable dependiente."},{"metadata":{"trusted":true},"cell_type":"code","source":"X = (wine_data.iloc[:,:-1]).as_matrix()\ny = (wine_data.iloc[:,-1]).as_matrix()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Llevamos nuestras variables al hypercubo (0,1). **NOTA**: La variable dependiente se divide por 10 dado que se conoce la naturaleza de los datos."},{"metadata":{"trusted":true},"cell_type":"code","source":"X = (X - X.min(axis=0))/(X.max(axis=0) - X.min(axis=0))\ny = y.reshape((-1,1))/10\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Se divide nuestro dataset en dos subconjuntos: Train y Test"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.33, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Inicializamos nuestro MLP y entrenamos:"},{"metadata":{"trusted":true},"cell_type":"code","source":"regressor = mlp_r(\n    hidden_layer_sizes=(100,),  activation='tanh', solver='adam', alpha=0.001, batch_size='auto',\n    learning_rate='constant', learning_rate_init=0.01, power_t=0.5, max_iter=1000, shuffle=True,\n    random_state=0, tol=0.0001, verbose=False, warm_start=False, momentum=0.9, nesterovs_momentum=True,\n    early_stopping=False, validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"regressor.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_hats = regressor.predict(X_test)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Contrastamos el resultado de nuestro estimador con el estimador ideal:"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(y_test, y_hats, c='k')\n\nplt.plot([0.2, 0.9], [0.2, 0.9], 'r')\nplt.xlabel('Real')\nplt.ylabel('Estimada')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Realizamos el  mismo procedimiento reduciendo las dimensiones de los datos:"},{"metadata":{"trusted":true},"cell_type":"code","source":"pca = PCA(n_components=3)\n\nX_pca = pca.fit_transform(X)\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X_pca, np.ravel(y), test_size=0.33, random_state=42)\n\nregressor2 = mlp_r(\n    hidden_layer_sizes=(100,),  activation='tanh', solver='adam', alpha=0.001, batch_size='auto',\n    learning_rate='constant', learning_rate_init=0.01, power_t=0.5, max_iter=1000, shuffle=True,\n    random_state=0, tol=0.0001, verbose=False, warm_start=False, momentum=0.9, nesterovs_momentum=True,\n    early_stopping=False, validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n\nregressor2.fit(X_train, y_train)\n\ny_hat2 = regressor2.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(y_test, y_hat2, c='k')\n\nplt.plot([0.2, 0.8], [0.2, 0.8], 'r')\nplt.xlabel('Real')\nplt.ylabel('Estimada')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**TO DO**\n1. Como sabemos que el modelo esta bien entrenado y como sabemos si es un buen modelo?  [Aplicar métricas de evaluación](https://scikit-learn.org/stable/model_selection.html#model-selection)\n2. Seleccionar 1 métrica de evaluación (mse, mape, mae, r2, F-score, AIC, etc.), y realizar un grafico de este bi-dimensional del promedio de dicha métrica y otro de la desviación estandár de dicha métrica, en el cual se varie la cantidad de neuronas en la primer capa, y en la segunda capa.\n3. Repetir el experimento anterior seleccionando el mejor modelo, pero variando el momentum (por lo tanto es un grafico unidimensional)."},{"metadata":{},"cell_type":"markdown","source":"**TO DO 1**\n\nSe selecciona la métrica [mse](https://en.wikipedia.org/wiki/Mean_squared_error) (clasica para evaluar ejercicios de regresión), y se realiza el gráfico bi-dimensional:"},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\n\ncapa_1 = [5, 7, 9, 11, 13, 17,  19, 23, 29, 31]\ncapa_2 = [1, 5, 7, 9, 11, 13, 17,  19, 23, 29]\n\nmse_m = np.zeros((len(capa_1),len(capa_1)))\nmse_std = np.zeros((len(capa_1),len(capa_1)))\n\nfor j, n_1 in enumerate(capa_1):\n    for k, n_2 in enumerate(capa_2):\n        mse_temp = []\n    \n        for i in range(10):\n            X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.33)\n            \n            regresor_temp = mlp_r(hidden_layer_sizes=(n_1, n_2, ),  activation='tanh')\n            regresor_temp.fit(X_train, y_train)\n            y_pred = regressor2.predict(X_test)\n            mse_temp.append(mean_squared_error(y_test, y_pred))\n        mse_m[j, k] = np.mean(mse_temp)\n        mse_std[j, k] = np.std(mse_temp)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.imshow(mse_m)\nplt.colorbar()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.imshow(mse_std)\nplt.colorbar()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Clasificación"},{"metadata":{},"cell_type":"markdown","source":"El ejercicio de clasificación se hará empleando la famosa base de datos de Iris. Primero cargamos los datos:"},{"metadata":{"trusted":true},"cell_type":"code","source":"iris = datasets.load_iris()\nX = iris.data \ny = iris.target","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Generamos los conjuntos de entrenamiento y prueba:"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.33, random_state=42)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Inicializamos nuestro MLP y entrenamos:"},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = mlp_c(solver='lbfgs', alpha=1e-5,\n                    hidden_layer_sizes=(100, 2), random_state=1)\n\nclf.fit(X_train, y_train)     ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_hat = clf.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Se calcula la matriz de confusión para el clasificador:"},{"metadata":{"trusted":true},"cell_type":"code","source":"cm1 = confusion_matrix(y_test, y_hat)\ncm2 = confusion_matrix(y_test, y_hat)\ncm2 = cm2.astype('float') / cm2.sum(axis=1)[:, np.newaxis]\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Sin normalizar"},{"metadata":{"trusted":true},"cell_type":"code","source":"cm1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Normalizada"},{"metadata":{"trusted":true},"cell_type":"code","source":"cm2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**TO DO**\n1. Como sabemos que el modelo esta bien entrenado y como sabemos si es un buen modelo?  [Aplicar métricas de evaluación](https://scikit-learn.org/stable/model_selection.html#model-selection)\n2. Seleccionar un dataset nuevo, implementar validación cruzada con k-folds, y encontrar los parametros con los cuales se puede lograr clasificar el dataset escogido.\n"},{"metadata":{},"cell_type":"markdown","source":"# Tarea:\n\n1. Implementar un algoritmo de clustering de redes neuronales, tomar de referencia: https://hagan.okstate.edu/NNDesign.pdf \n2. Aplicar este algoritmo a una base de datos de Kaggle, y aplicar métricas de evaluación como:\n    * Adjusted Rand index\n    * Mutual Information\n    * Silhouette Coefficient\n    * Calinski-Harabasz Index\n    * Entre otros.\n2. TO DO de Regresión\n3. Conclusiones sobre los resultados obtenidos\n4. TO DO de Clasificación\n5. Conclusiones sobre los resultados obtenidos"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}