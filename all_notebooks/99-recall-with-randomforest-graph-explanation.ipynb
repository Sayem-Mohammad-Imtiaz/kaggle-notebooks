{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/credit-card-customers/BankChurners.csv')\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data.drop(['Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_1', \n                  'Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_2'], axis = 1)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"so there is no NaN's and empty cells"},{"metadata":{"trusted":true},"cell_type":"code","source":"for column in data:\n    print(column)\n    print(data[column].unique())\n    print()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1 Exploring data\n### 1.0 CLIENTNUM\n\nit is unique id of a person. before we delete it, let's check if there is any duplicates"},{"metadata":{"trusted":true},"cell_type":"code","source":"len(data['CLIENTNUM'].unique()) == len(data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sorted(data['CLIENTNUM'].unique()) == sorted(data['CLIENTNUM'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"so there isn't. now we can drop it"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data.drop(['CLIENTNUM'], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1.1 Attrition_Flag\n\nInternal event (customer activity) variable - if the account is closed then 1 else 0\n\nbasicly is what we need to predict"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.groupby(['Attrition_Flag'])['Attrition_Flag'].count().plot.bar()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(data[data['Attrition_Flag'] == 'Attrited Customer']) / len(data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"16% of customers are attrited so in next steps (when we will be separating our data on train and test parts) we need to remember that. Now I'll just change values on 0's and 1's\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.loc[data['Attrition_Flag'] == 'Attrited Customer', 'Attrition_Flag'] = 0\ndata.loc[data['Attrition_Flag'] == 'Existing Customer', 'Attrition_Flag'] = 1\n\ndata.groupby(['Attrition_Flag'])['Attrition_Flag'].count().plot.bar()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1.2 Customer_Age"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (10, 5))\ndata_to_plot = data.groupby(['Customer_Age'])['Customer_Age'].count()\n\ndata_to_plot.plot.bar(width = 0.75, color = 'C4')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"it seems like it has normal distribution. I wouldn't change anything (for now) here\n\n### 1.3 Gender"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.groupby(['Gender'])['Gender'].count().plot.bar(color = 'C2')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(data[data['Gender'] == 'F']) / len(data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1.4 Dependent_count\n\nDemographic variable - Number of dependents\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (8, 4))\ndata_to_plot = data.groupby(['Dependent_count'])['Dependent_count'].count()\n\ndata_to_plot.plot.bar(width = 0.75, color = 'C8')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"it seems like it has normal distribution. I wouldn't change anything (for now) here\n\n### 1.5 Education_Level, Marital_Status, Income_Category"},{"metadata":{"trusted":true},"cell_type":"code","source":"for column in ['Education_Level', 'Marital_Status', 'Income_Category']:\n\n    plt.figure(figsize = (8, 4))\n    data_to_plot = data.groupby([column])[column].count()\n\n    data_to_plot.plot.bar(width = 0.75, color = 'C7')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"there is some columns with variables 'unknown'. I think we shoudn't do anything with it\n\n### 1.6 Card_Category\nProduct Variable - Type of Card (Blue, Silver, Gold, Platinum)"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (7, 4))\ndata_to_plot = data.groupby(['Card_Category'])['Card_Category'].count()\n\ndata_to_plot.plot.bar(width = 0.75)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Blue cards are the the most popular (which is pretty obvious, they are the cheapest). But other cards are rare:\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"len(data[data['Card_Category'] == 'Platinum'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"only 20 cards out of 10K -- is very small amount"},{"metadata":{"trusted":true},"cell_type":"code","source":"len(data[data['Card_Category'] != 'Blue'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I think we should connect not Blue cards in one category bc there is too little of them"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.loc[data['Card_Category'] != 'Blue', 'Card_Category'] = 'not Blue'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (7, 4))\ndata_to_plot = data.groupby(['Card_Category'])['Card_Category'].count()\n\ndata_to_plot.plot.bar(width = 0.75)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1.7 Months_on_book\n\nPeriod of relationship with bank"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (10, 5))\ndata_to_plot = data.groupby(['Months_on_book'])['Months_on_book'].count()\n\ndata_to_plot.plot.bar(width = 0.75, color = 'C5')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"there is much more values with 36 values. maybe there was an discount or raffle prizes\n\n### 1.8 Total_Relationship_Count, Months_Inactive_12_mon, Contacts_Count_12_mon\n\n* Total_Relationship_Count -- Total no. of products held by the customer\n* Months_Inactive_12_mon -- No. of months inactive in the last 12 months\n* Contacts_Count_12_mon -- No. of Contacts in the last 12 months"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_to_plot = data.groupby(['Total_Relationship_Count'])['Total_Relationship_Count'].count()\ndata_to_plot.plot.bar(width = 0.75, color = 'C5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_to_plot = data.groupby(['Months_Inactive_12_mon'])['Months_Inactive_12_mon'].count()\ndata_to_plot.plot.bar(width = 0.75, color = 'C8')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(data[data['Months_Inactive_12_mon'] == 0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(data[data['Months_Inactive_12_mon'] > 4])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"there is to little examples with 0 and 5&6. but because it is numerical variable (not categorical) we won't do anything"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_to_plot = data.groupby(['Contacts_Count_12_mon'])['Contacts_Count_12_mon'].count()\ndata_to_plot.plot.bar(width = 0.75, color = 'C9')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1.9 Credit_Limit, Total_Revolving_Bal, Avg_Open_To_Buy, Total_Amt_Chng_Q4_Q1, Total_Trans_Amt, Total_Trans_Ct, Total_Ct_Chng_Q4_Q1, Avg_Utilization_Ratio"},{"metadata":{"trusted":true},"cell_type":"code","source":"for column in ['Credit_Limit', 'Total_Revolving_Bal', 'Avg_Open_To_Buy', 'Total_Amt_Chng_Q4_Q1', \n               'Total_Trans_Amt', 'Total_Trans_Ct', 'Total_Ct_Chng_Q4_Q1', 'Avg_Utilization_Ratio']:\n    plt.figure(figsize = (10, 4))\n    plt.hist(data[column], bins = 50)\n    plt.title(column)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2 Data Preparation\n\nWe should transform object values to numeric values."},{"metadata":{"trusted":true},"cell_type":"code","source":"#we are giving a point if person are Male(I`m not sexist I swear)\ndata.loc[data['Gender'] == 'F', 'Gender'] = 0\ndata.loc[data['Gender'] == 'M', 'Gender'] = 1\n\n#and if person has not a Blue card (because it`s more prestigious) \ndata.loc[data['Card_Category'] == 'Blue', 'Card_Category'] = 0\ndata.loc[data['Card_Category'] == 'not Blue', 'Card_Category'] = 1\n\ndata[['Gender', 'Card_Category', 'Attrition_Flag']] = data[['Gender', 'Card_Category', 'Attrition_Flag']].astype('int32')\n\n#We can change Education_Level and Income_Category on numeric variable:\n#the better education/the higher income -- the higher number\neducation_dict = {'Unknown': 0, 'Uneducated': 1, 'High School': 2, 'College': 3, 'Graduate': 4, 'Post-Graduate': 5, 'Doctorate': 6}\nincome_category_dict = {'Unknown' : 0, 'Less than $40K' : 1, '$40K - $60K' : 2, '$60K - $80K' : 3, '$80K - $120K' : 4, '$120K +' : 5}\n\ndata['Education_Level'] = data['Education_Level'].replace(education_dict)\ndata['Income_Category'] = data['Income_Category'].replace(income_category_dict)\n\n#But we can`t do the same with Marital_Status so we'll do a column for each status\ndata = pd.concat([data, pd.get_dummies(data['Marital_Status'], prefix='Marital_Status')], axis=1)\ndata = data.drop(['Marital_Status', 'Marital_Status_Unknown'], axis = 1)\n\ndata.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (12, 10))\nsns.heatmap(data.corr(), annot=True, fmt=\".2f\", cmap=\"YlGnBu\", cbar=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data[['Credit_Limit', 'Avg_Open_To_Buy']].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(data[data['Credit_Limit'] == data['Avg_Open_To_Buy']]) / len(data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(data[data['Credit_Limit'] >= data['Avg_Open_To_Buy']]) / len(data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Avg_Open_To_Buy describes Open to Buy Credit Line (Average of last 12 months). That's why it more or equals then Credit_Limit and corr between them =1."},{"metadata":{"trusted":true},"cell_type":"code","source":"len(data[data['Credit_Limit'] - data['Avg_Open_To_Buy'] == data['Total_Revolving_Bal']]) / len(data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"so 'Credit_Limit' = 'Avg_Open_To_Buy' + 'Total_Revolving_Bal'"},{"metadata":{},"cell_type":"markdown","source":"## 3 Predicting\n\nFirst of all I want to find the most important features (to drop some of them)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(data.drop(['Attrition_Flag'], axis=1), data['Attrition_Flag'], \n                                                    test_size=0.2, stratify = data['Attrition_Flag'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n\nclf = RandomForestClassifier()\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)\n\nfrom sklearn.metrics import confusion_matrix\nconfusion_matrix(y_test, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list(zip(X_train.columns, clf.feature_importances_))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I think we can try to drop Gender and Card_Category. That makes sence: Gender is almost 50/50, in Card_Category mostly people have 'Blue' value.\n\nHere we see that Marital_Status also doen't make big effort. Let's try to drop it"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nprint('accuracy_score:', accuracy_score(y_test, y_pred))\n\ncol_to_drop = ['Gender', 'Card_Category', 'Marital_Status_Divorced', 'Marital_Status_Married', 'Marital_Status_Single']\n\nX_train = X_train.drop(col_to_drop, axis=1)\nX_test = X_test.drop(col_to_drop, axis=1)\n\nclf = RandomForestClassifier()\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)\n\nprint('accuracy_score after dropping Gender, Marital_Status and Card_Category:', accuracy_score(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After redoing train_test_split a few times, result of dropping columns almost always gave better results"},{"metadata":{"trusted":true},"cell_type":"code","source":"list(zip(X_train.columns, clf.feature_importances_))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"sometimes Desission Trees gets retraining. The way to predict it: set max depth to the trees. Let's check if it does this time:"},{"metadata":{"trusted":true},"cell_type":"code","source":"x, test_res, train_res = [], [], []\n\nfor i in range(3,25):\n    clf = RandomForestClassifier(max_depth = i)\n    clf.fit(X_train, y_train)\n    \n    train_res += [accuracy_score(y_train, clf.predict(X_train))]\n    test_res += [accuracy_score(y_test, clf.predict(X_test))]\n    x += [i]\n    \n    \nplt.figure(figsize = (12, 4))\nplt.plot(x, train_res, label = 'accuracy on train data')\nplt.plot(x, test_res, label = 'accuracy on test data')\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"so it does. in next steps i suggest to set max_depth around 10-11. It won't make results much worse, but the model will be working faster\n\nnow we can check other parameters. one of them if class_weight (\"how important\" each class is for us):"},{"metadata":{"trusted":true},"cell_type":"code","source":"x, test_res, train_res = [], [], []\n\nfor i in range(1, 10):\n    clf = RandomForestClassifier(max_depth = 11, class_weight = {0 : i, 1: 10-i})\n    clf.fit(X_train, y_train)\n    \n    train_res += [accuracy_score(y_train, clf.predict(X_train))]\n    test_res += [accuracy_score(y_test, clf.predict(X_test))]\n    x += [i]\n    \n    \nplt.figure(figsize = (12, 4))\nplt.plot(x, train_res, label = 'accuracy on train data')\nplt.plot(x, test_res, label = 'accuracy on test data')\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"X axis means our value to class '0' (and 10 - x will be value to class '1'). Standart values are eqnal (5:5 on our graph)\n\nBefore that making any decisions let's make one more graph, but for another metric: recall (which is important for us)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import recall_score\n\nx, test_res, train_res = [], [], []\n\nfor i in range(1, 10):\n    clf = RandomForestClassifier(max_depth = 11, class_weight = {0 : i, 1: 10-i})\n    clf.fit(X_train, y_train)\n    \n    train_res += [recall_score(y_train, clf.predict(X_train))]\n    test_res += [recall_score(y_test, clf.predict(X_test))]\n    x += [i]\n    \n    \nplt.figure(figsize = (12, 4))\nplt.plot(x, train_res, label = 'recall_score on train data')\nplt.plot(x, test_res, label = 'recall_score on test data')\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our top priority in this business problem is to identify customers who are getting churned. Even if we predict non-churning customers as churned (FP), it won't harm our business. But predicting churning customers as Non-churning will do. So recall (TP/TP + FN) need to be higher.\n\nSo basicly, we sholdn't care about accuracy, but we should care about recall. That's why changing class_weight will be the right decision.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = RandomForestClassifier(max_depth = 11, class_weight = {0 : 1, 1: 9})\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)\n\nprint('accuracy_score:', accuracy_score(y_test, y_pred))\nprint('recall_score:', recall_score(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"confusion_matrix(y_test, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}