{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Ah, Reddit\n\nThe front page of the internet as exclaimed by itself and rightly so. You can find posts about anything and everything over there. Every community in reddit is known as a \"subreddit\" and users there are called \"redditors\"\n\n## About r/india\n\nThis is the official subreddit of everything about India. \n\n## Flairs\n\nThese are something which can be best defined as subtopics in a subreddit that are set by the moderators of the subreddit. I will slowly show what they are in this notebook.\n"},{"metadata":{},"cell_type":"markdown","source":"# The toolkit"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#The baseline modules\nimport numpy as np\nimport pandas as pd\n\n#For text cleaning\nimport spacy\n\n#For plotting\nimport missingno as msno\nimport matplotlib.pyplot as plt\nimport plotly.graph_objects as go\nimport plotly.express as px\n\n#Model packages\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.svm import SVC\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression\n\n#Pipeline, Vectorizers and accuracy metrics\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/reddit-india-flair-detection/datafinal.csv', index_col='Unnamed: 0')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Null values"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum().any()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"msno.matrix(df)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Cleaning Data"},{"metadata":{},"cell_type":"markdown","source":"## Removing uneccessary columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Things that can be removed:\n\n* score - Karma(basically the number of upvotes a post gets) doesn't contribute in figuring out a flair\n* url - We can't do much off the url of the post\n* comms_num - Number of comments isn't that important\n* timestamp - Timestamp cannot factor into predicting the flair \n* author - We can't base the flair based on who writes it"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop(['score','url','comms_num','author','timestamp'], axis=1, inplace=True)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['title'][0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['body'][0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['comments'][0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['combined_features'][0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Well will you look at that. The combined features column is just the combo of title, body and comments. So, that's safe to drop as well"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop(['combined_features'], axis=1, inplace=True)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Time to explore the flairs cause that's our target to predict**"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"How many flairs are present?"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['flair'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.groupby('flair')['title'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fla_df = pd.DataFrame({\"Flair\":df['flair'].unique(), \"Number\":df.groupby('flair')['title'].describe()['freq']})\n\nfig = px.bar(fla_df, x='Flair', y='Number', title='Flair Counts by Title in r/india')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Same thing can be followed for body and comments"},{"metadata":{"trusted":true},"cell_type":"code","source":"fla_df_1 = pd.DataFrame({\"Flair\":df['flair'].unique(), \"Number\":df.groupby('flair')['body'].describe()['freq']})\n\nfig = px.bar(fla_df_1, x='Flair', y='Number', title='Flair Counts by body in r/india')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fla_df_2 = pd.DataFrame({\"Flair\":df['flair'].unique(), \"Number\":df.groupby('flair')['comments'].describe()['freq']})\n\nfig = px.bar(fla_df_2, x='Flair', y='Number', title='Flair Counts by comments in r/india')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[df['flair'] == np.nan].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.dropna(subset=['flair'], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can combine title, body and comments into a single column called text "},{"metadata":{"trusted":true},"cell_type":"code","source":"df['text'] = df['title'].astype(str) + df['body'].astype(str) + df['comments'].astype(str)\ndf.drop(['title', 'body', 'comments'], axis=1, inplace=True)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now to clean the text"},{"metadata":{},"cell_type":"markdown","source":"# Normalisation"},{"metadata":{"trusted":true},"cell_type":"code","source":"nlp = spacy.load('en')\n\ndef normalize(msg):\n    \n    doc = nlp(msg)\n    res=[]\n    \n    for token in doc:\n        if(token.is_stop or token.is_punct or not(token.is_oov)): #Removing stopwords punctuations and words out of vocab\n            pass\n        else:\n            res.append(token.lemma_.lower())\n    \n    return \" \".join(res)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['text'] = df['text'].apply(normalize)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Training and Prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"c = TfidfVectorizer() # Convert our strings to numerical values\nmat=pd.DataFrame(c.fit_transform(df[\"text\"]).toarray(),columns=c.get_feature_names(),index=None)\nmat","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = mat\ny = df[\"flair\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Gradient Boosting Classifier takes the crown with a 81.14% accuracy but since it takes too long we'll go with Decision Tree Classifier"},{"metadata":{},"cell_type":"markdown","source":"# Final Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"pipeline = Pipeline([\n    ('classifier',DecisionTreeClassifier()),\n    ])\n\npipeline.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = pipeline.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(confusion_matrix(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Accuracy: {:.2f} %\".format(accuracy_score(y_test, y_pred)*100))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Final Output"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(df['text'], df['flair'], test_size = 0.2, random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ids = [df.iloc[int(i)]['id'] for i in X_test.index]\nfinal_df = pd.DataFrame({\"ID\":ids, \"Text\":X_test, \"Flair\":y_pred}).reset_index()\n\nfinal_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_df.to_csv('./test.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Finding Best Classifier\n\nThese cells are used to find which classifier is the best. Takes a VERY long time"},{"metadata":{"trusted":true},"cell_type":"code","source":"'''classifiers = {\n    'mnb': MultinomialNB(),\n    'gnb': GaussianNB(),\n    'svm1': SVC(kernel='linear'),\n    'svm2': SVC(kernel='rbf'),\n    'svm3': SVC(kernel='sigmoid'),\n    'mlp1': MLPClassifier(),\n    'mlp2': MLPClassifier(hidden_layer_sizes=[100,100]),\n    'ada': AdaBoostClassifier(),\n    'dtc': DecisionTreeClassifier(),\n    'rfc': RandomForestClassifier(),\n    'gbc': GradientBoostingClassifier(),\n    'lr': LogisticRegression()\n}'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''acc_scores = dict()\nfor classifier in classifiers:\n    pipeline = Pipeline([\n    ('classifier',classifiers[classifier]),\n    ])\n    pipeline.fit(X_train, y_train)\n    y_pred = pipeline.predict(X_test)\n    acc_scores[classifier] = accuracy_score(y_test, y_pred)\n    print(classifier, acc_scores[classifier])'''","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}