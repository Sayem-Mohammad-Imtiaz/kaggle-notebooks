{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"\n# E-commerce Business Data Cleaning","metadata":{}},{"cell_type":"markdown","source":"This notebook provides the data cleaning process of an online retail business' sales data. The aim of this notebook is to prepare the dataset for further analysis. I have also published a notebook with the sales analysis and visualizations, which you can find [<ins>here</ins>](https://www.kaggle.com/atanaskanev/sales-analysis-and-visualization).\n\nThe business questions answered in the analysis notebook include:\n* What is the overall sales trend?\n* Which is the best selling product in each country?\n* How many new customers are there each month?\n* When do customers make the most purchases?\n\nThe data contains 541,909 sales records and 8 columns, including a product description, quanitity of items sold, unit price, date of sale and country. In short, the cleaning process includes:\n* cleaning erroneous and missing data\n* removing duplicated descriptions for the same stockcodes\n* handling outliers\n\n","metadata":{}},{"cell_type":"markdown","source":"## Table of Contents\nClick on any heading to jump straight to the content\n\n[<font size=\"5\">Importing Libraries and Data. Initial Data Overview</font>](#section-one)\n\n[<font size=\"5\">Data Cleaning</font>](#section-two)\n* [Negative and 0 Unit Price](#section-three)\n    - [Unit Price = 0](#section-four)\n    - [Unit Price < 0](#section-five)\n* [Clean Erroneous and Non-Sales Related Descriptions](#section-six)\n* [Assign Unique Descriptions to Each StockCode](#section-seven)\n* [Outliers](#section-eight)","metadata":{}},{"cell_type":"markdown","source":"<a id=\"section-one\"></a>\n# Importing Libraries and Data. Initial Data Overview","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy import stats # used to calculate z-scores to investigate outliers\n\nimport warnings        \nwarnings.filterwarnings(\"ignore\") # ignores warnings\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.options.display.float_format = \"{:.2f}\".format # formats floats to two decimal places","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv(\"../input/ecommerce-data/data.csv\", encoding = \"ISO-8859-1\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head(10)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Something to note is that product descriptions are capitalised.","metadata":{}},{"cell_type":"code","source":"data.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# cast InvoiceDate as a date type\ndata[\"InvoiceDate\"] = pd.to_datetime(data[\"InvoiceDate\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.isna().sum()","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"section-two\"></a>\n# Data Cleaning","metadata":{}},{"cell_type":"markdown","source":"We check whether the data contains any missing values which are strings and not NaNs by checking against a simple list, which usually finds some missing data:","metadata":{}},{"cell_type":"code","source":"data[data.isin([\"NA\",\"NaN\",\"Na\",\"na\",\"N/A\",\n                \"n/a\",\"missing\",\"MISSING\",\n                \"no data\",\"nodata\",\"\",\"?\",\n                \"??\",\"???\",\"????\",\"?????\"]).any(axis=1)].shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We did infact find some missing data. Let's have a look at some of those records:","metadata":{}},{"cell_type":"code","source":"data[data.isin([\"NA\",\"NaN\",\"Na\",\"na\",\"N/A\",\n                \"n/a\",\"missing\",\"MISSING\",\n                \"no data\",\"nodata\",\"\",\"?\",\n                \"??\",\"???\",\"????\",\"?????\"]).any(axis=1)].head(20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We see that when there are missing values for Description: UnitPrice is 0, CustomerID is missing, and we have negative values for quantity, but not always.\n\nLet's start by investigating negative and 0 unit prices:","metadata":{}},{"cell_type":"markdown","source":"<a id=\"section-three\"></a>\n## Negative and 0 Unit Prices","metadata":{}},{"cell_type":"code","source":"data[\"UnitPrice\"].describe()","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"section-four\"></a>\n### UnitPrice = 0\nFirst let's investigate records with a UnitPrice of 0.","metadata":{}},{"cell_type":"code","source":"data[data[\"UnitPrice\"] == 0].shape","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"2515 records have a UnitPrice = 0","metadata":{}},{"cell_type":"code","source":"# number of unique Descriptions when UnitPrice is 0\ndata[data[\"UnitPrice\"] == 0][\"Description\"].nunique(dropna = False) # also counts NaN","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# look at these descriptions\ndata[data[\"UnitPrice\"] == 0][\"Description\"].unique()[0:10] # only the first 10 shown here, but I have looked at them","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Some are actual product Descriptions e.g. 'ROUND CAKE TIN VINTAGE GREEN', but others are ambiguous: '?', '?display?' etc. These were probably entered as errors and a UnitPrice of 0 was assigned, so that they do not affect the sales figures.\n\nA lot of these descriptions are adjustments for missing or damaged items e.g. 'missing?', 'Breakages' etc.\nSuch records have negative quantities in order to adjust for the missing/damaged inventory, but have UnitPrices = 0 so that they do no affect sales negatively. \n\nIn other words, records with a UnitPrice of 0 are likely related to inventory and not sales. It could be that the system used to enter this data does not have separate accounts for sales and inventory, and they were entered in the same place.\n\nWe can therefore drop records with a UnitPrice of 0.","metadata":{}},{"cell_type":"code","source":"data = data[data[\"UnitPrice\"] != 0]","metadata":{"cell_style":"center","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.isna().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This has cleaned the NaN values for Description.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"section-five\"></a>\n### UnitPrice < 0\nLet's check records with a negative UnitPrice.","metadata":{}},{"cell_type":"code","source":"data[data[\"UnitPrice\"] < 0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data[data[\"Description\"].str.lower().str.contains(\"adjust\")]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Bad debts are expenses and not sales, therefore we drop them.","metadata":{}},{"cell_type":"code","source":"data = data[data[\"Description\"].str.lower().str.contains(\"adjust\") == False]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We now have records with positive UnitPrices only. ","metadata":{}},{"cell_type":"markdown","source":"<a id=\"section-six\"></a>\n## Clean Erroneous and Non-Sales Related Descriptions\nLet's start with cleaning some of the Descriptions, since we saw issues with them above.","metadata":{}},{"cell_type":"code","source":"# number of unique descriptions\ndata[\"Description\"].nunique(dropna = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# first strip any leading and trailing whitespace\ndata[\"Description\"] = data[\"Description\"].str.strip()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# number of unique descriptions\ndata[\"Description\"].nunique(dropna = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A simple striping of whitespaces has reduced the number of unique Descriptions.\n\nAbove we saw InvoiceNos starting with C, which probably refers to credit. To illustrate what credit entries are, the below records show that an error was likely made: Quantity = 80995, and then it is cancelled out with a credit entry. ","metadata":{}},{"cell_type":"code","source":"data[data[\"Description\"].str.contains(\"PAPER CRAFT\")]","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" Let's see the Descriptions of records with credit InvoiceNos and clean some of them:","metadata":{}},{"cell_type":"code","source":"data[data[\"InvoiceNo\"].str.startswith(\"C\")].head(10)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#  list of credit Descriptions\ncredit_descr = pd.Series(data[data[\"InvoiceNo\"].str.startswith(\"C\")][\"Description\"].unique())\ncredit_descr","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's look into the not capitalised Descriptions since capitalised Descriptions are usually for products.","metadata":{}},{"cell_type":"code","source":"credit_descr[credit_descr.str.isupper() == False]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Manual, Discounts and Next Day Carriage are related to sales, but the other Descriptions are expenses, so we drop them.","metadata":{}},{"cell_type":"code","source":"data = data[data[\"Description\"].isin([\"Bank Charges\", \"CRUK Commission\"]) == False]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# chech short credit Descriptions\ncredit_descr[credit_descr.str.len() < 15]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Drop \"AMAZON FEE\", \"SAMPLES\", \"POSTAGE\" and \"PACKING CHARGE\" since they are not related to sales: ","metadata":{}},{"cell_type":"code","source":"data = data[data[\"Description\"].isin([\"AMAZON FEE\", \"SAMPLES\", \"POSTAGE\", \"PACKING CHARGE\"]) == False]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The other credit Descriptions have actual product names, so they are likely related to returned items, and therefore affect sales. \"Manual\" and \"Discount\" also affect sales, so we leave them in the data as well.\n\nThere are also items sold on DOTCOM:","metadata":{}},{"cell_type":"code","source":"list_dotcom = data[data[\"Description\"].replace({np.nan:\"\"}).str.lower().str.contains(\"dotcom\", regex = True)] \\\n[\"Description\"].unique().tolist()\n\nlist_dotcom","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's have all items related to DOTCOM share the same Description \"DOTCOM\":","metadata":{}},{"cell_type":"code","source":"data[\"Description\"] = data[\"Description\"].replace(list_dotcom, \"DOTCOM\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"section-seven\"></a>\n## Assign Unique Descriptions to Each StockCode","metadata":{}},{"cell_type":"markdown","source":"The next step is to remove very similar Descriptions which relate to the same item and have the same StockCode. To do this, we will match each StockCode with a unique Description.","metadata":{}},{"cell_type":"code","source":"# check number of unique Descriptions\ndata[\"Description\"].nunique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check number of unique StockCodes\ndata[\"StockCode\"].nunique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We see there are more unique Descriptions than unique StockCodes, so some StockCodes likely have more than 1 Description. Let's see which these StockCodes are:","metadata":{}},{"cell_type":"code","source":"num_descriptions = data.groupby(\"StockCode\")[\"Description\"].nunique().sort_values(ascending = False)\nnum_descriptions","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We see StockCodes with mulptiple Descriptions. Let's see the distribution:","metadata":{}},{"cell_type":"code","source":"num_descriptions.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We see that 200 StockCodes have 2 Descriptions each; 15 StockCodes have 3, and 2 StockCodes have 4.\n\nWe can group by StockCode and create a list of Descriptions for each StockCode:","metadata":{}},{"cell_type":"code","source":"groups = data.groupby(\"StockCode\")[\"Description\"].unique()\ngroups","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check some of the groups with multiple Descriptions\ngroups[groups.str.len() > 1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see, some StockCodes have multiple very similar Descriptions. In order to remove duplicated Descriptions, we can take the first Description in each group and assign this Desctiption to all records with the same StockCode. \nFor this purpose we create a dictionary matching each StockCode with the first Description from its corresponding group:","metadata":{}},{"cell_type":"code","source":"dictionary = {}\nfor index, group in groups.items():\n    dictionary[index] =  group[0]","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# what the dictionary looks like\nlist(dictionary.items())[0:10]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Create a DataFrame from the dictionary:","metadata":{}},{"cell_type":"code","source":"descriptions = pd.DataFrame()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"descriptions[\"StockCode\"] = list(dictionary.keys())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"descriptions[\"Unique_Description\"] = list(dictionary.values())","metadata":{"cell_style":"center","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"descriptions.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We now have a reference table matching every StockCode with a unique Description.\n\nWe can now assign these unique Descriptions to their StockCode in the original data:","metadata":{}},{"cell_type":"code","source":"data = data.merge(descriptions, on = \"StockCode\", how = \"inner\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now every record has an assigned unique Description.","metadata":{}},{"cell_type":"code","source":"data[\"Description\"].nunique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data[\"Unique_Description\"].nunique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we see, all similar Descriptions have been converted to a single unique Description. We can now drop the original Description column and replace it with the cleaned Unique_Description column:","metadata":{}},{"cell_type":"code","source":"data[\"Description\"] = data[\"Unique_Description\"]\ndata = data.drop(\"Unique_Description\", axis = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data[\"StockCode\"].nunique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We see that the unique StockCodes are more than the unique Descriptions, which means that some Descriptions are repeated for different StockCodes. Let's see which these are:","metadata":{}},{"cell_type":"code","source":"descr_counts = pd.Series(descriptions[\"Unique_Description\"].value_counts())\ndescr_counts[descr_counts > 1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Check a few of those:","metadata":{}},{"cell_type":"code","source":"descriptions[descriptions[\"Unique_Description\"].str.contains(\"METAL SIGN,CUPCAKE\")]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"descriptions[descriptions[\"Unique_Description\"].str.contains(\"COLUMBIAN CANDLE ROUND\")]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We see that these products are similar, so they can be grouped together in analyses.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"section-eight\"></a>\n## Outliers","metadata":{}},{"cell_type":"code","source":"# add an ItemTotal column\ndata[\"ItemTotal\"] = data[\"Quantity\"] * data[\"UnitPrice\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.style.use(\"default\")\nfig, (ax1,ax2) = plt.subplots(1,2, figsize = (8,3.7))\n\nax1.boxplot(data[\"UnitPrice\"])\nax1.set_title(\"Unit Price\")\nax2.boxplot(data[\"Quantity\"])\nax2.set_title(\"Quantity\")\nfig.suptitle(\"Unit Price and Quantity Outlier Analysis\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We see some big outliers.\n\nSince there are a lot of values outside the interquartile range, we expect that there would also be a lot of values beyond 3 standard deviations (z-score > 3), which means that using z-score as a means to eliminate outliers would be problematic.\n\nLet's see how many values are outside 3 standard deviations:\n","metadata":{}},{"cell_type":"code","source":"z = np.abs(stats.zscore(data[\"Quantity\"])) # calculate z-scores for Quantity\nlen(np.where(z>3)[0]) # how many values are outside 3 std.dev","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"z = np.abs(stats.zscore(data[\"UnitPrice\"])) # calculate z-scores for UnitPrice\nlen(np.where(z>3)[0]) # how many values are outside 3 std.dev","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we see, there are a lot of values beyond 3 std. deviations. Moreover, we expect high values for Quantity to be related to very cheap items. \n\nLet's investigate by starting from the extreme outliers for Quantity:","metadata":{}},{"cell_type":"code","source":"data[np.abs(data[\"Quantity\"]) > 10000]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We see that these records were likely entered by mistake, so the amounts were cancelled out with credit entries. We drop these records for data clarity, although they have a net impact of 0 on ItemTotal.","metadata":{}},{"cell_type":"code","source":"# remove extreme outliers\ndata = data[np.abs(data[\"Quantity\"]) < 10000]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# look at some more records \ndata[np.abs(data[\"Quantity\"]) > 2000]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As expected, these records relate to high Quantity sales of cheap items - perhaps purchases by other retailers. Some other records are error reversals by credit entries, as seen above. We leave those records as they are.\n\nLet's now look at high UnitPrice records:","metadata":{}},{"cell_type":"code","source":"z = np.abs(stats.zscore(data[\"UnitPrice\"])) # calculate z-scores for UnitPrice\ndata[z > 3].sort_values(by = \"UnitPrice\", ascending = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data[z > 3][\"Description\"].unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We see Manual entries, discounts and DOTCOM sales. We see Manual entries that cancel each other out.\n\nIn reality, we would contact the sales department to inquire about what Manual entries refer to. For the present analysis, we assume that these are correct entries, possibly about returned items or crediting wrongly entered sales. \n\nDiscounts affect sales, so we leave them in the data as well.\n\nWe see vintage and antique items, which are expected to have high prices, so we keep such records as well.\n\nSomething interesting we see, however, is the \"PICNIC BASKET WICKER SMALL\" item.\nLet's investigate:","metadata":{}},{"cell_type":"code","source":"data[data[\"Description\"].str.contains(\"PICNIC BASKET\")].sort_values(by = \"UnitPrice\", ascending = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We see that the first two records are likely mistakes, since these UnitPrices are way above the normal prices for this product, and since we see a Manual credit entry of the exactly same amount above. We therefore drop these records, and drop the corresponding Manual credit adjustment.","metadata":{}},{"cell_type":"code","source":"# drop erroneous high UnitPrice records\ndata = data.drop(data.index[[88771,88772,297271]])","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The data is now cleaned and ready for analyses and visualizations.","metadata":{}},{"cell_type":"code","source":"# data.to_csv(\"online_retail.csv\", index = False) # saves the file","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" I have continued the analysis of this dataset including visualizations in another notebook, which you can find [<ins>here</ins>](https://www.kaggle.com/atanaskanev/sales-analysis-and-visualization).\n \n If you wish to download the cleaned dataset, you can do so [<ins>here</ins>](https://www.kaggle.com/atanaskanev/online-retail-business-cleaned-dataset).","metadata":{}},{"cell_type":"markdown","source":"<font size=\"5\">Thank you for reading my notebook!</font>\n\nAny comments and suggestions are highly appreciated!","metadata":{}}]}