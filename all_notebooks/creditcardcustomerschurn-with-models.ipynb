{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_credit = pd.read_csv(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# General information retrival"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_credit.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### As already told in the main page that to remove bith the Naive_Bayes columns, lets do that and then we will retirive info"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_credit = df_credit.drop(columns=['Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_1', 'Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_2'], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def basic_infos(data):\n    print(\"Dataset shape is: \", data.shape,\"\\n\")\n    print(\"Dataset columns are: \",data.columns,\"\\n\")\n    print(\"Dataset dimensions are:\",data.ndim,\"\\n\")\n    print(\"Dataset information is:\\n\",data.info(),\"\\n\")\n    categorical, numerical = [], []\n    for i in data.columns:\n        if data[i].dtype==object:\n            categorical.append(i)\n        else:\n            numerical.append(i)\n    print(\"Categorical datatype columns are: \", [i for i in categorical],\"\\n\")\n    print(\"Numercial datatype columns are: \", [i for i in numerical],\"\\n\")\n    return categorical, numerical","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat, num = basic_infos(df_credit)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df_credit.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# General Exploration"},{"metadata":{},"cell_type":"markdown","source":"## Attrition_Flag"},{"metadata":{},"cell_type":"markdown","source":"Attrition_Flag tells us whether the customer is existing or attrited"},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = df_credit['Attrition_Flag'].value_counts().index.to_list()\nsizes = df_credit['Attrition_Flag'].value_counts()\nexplode=(0,0.2)\nfig, ax=plt.subplots()\nax.pie(sizes, labels = labels, explode = explode, autopct=\"%1.1f%%\", shadow=True, startangle=90)\nax.axis('equal')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We got the info that 16.1% customers are attrited customers"},{"metadata":{},"cell_type":"markdown","source":"## Customer_Age"},{"metadata":{},"cell_type":"markdown","source":"Lets see what is the age composition of all credit card users"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(25,7))\nplt.hist(df_credit['Customer_Age'], edgecolor = 'red')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can roughly estimate that most of the credit card holders are in the range of age 40 to 55"},{"metadata":{},"cell_type":"markdown","source":"## Gender\n"},{"metadata":{},"cell_type":"markdown","source":"Lets get how many people of different genders are the credit card holders"},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = df_credit['Gender'].value_counts().index.to_list()\nsizes = df_credit['Gender'].value_counts()\nexplode=(0,0.1)\nfig, ax=plt.subplots()\np, tx, autotexts = ax.pie(sizes, labels = labels, explode = explode, autopct=\"\", shadow=True, startangle=90)\nfor i, a in enumerate(autotexts):\n    a.set_text(\"{}\".format(sizes[i]))\nax.axis('equal')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see the number of females and males acquiring credit cards"},{"metadata":{},"cell_type":"markdown","source":"## Dependent_count"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_credit['Dependent_count'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize = (20,8))\ngraph = sns.countplot(df_credit['Dependent_count'])\nfor p in graph.patches:\n    for p in graph.patches:\n        height = p.get_height()\n        graph.text(p.get_x()+p.get_width()/2., height + 0.1,height ,ha=\"center\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Education_Level"},{"metadata":{},"cell_type":"markdown","source":"Lets see our holders highest education level"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize = (20,8))\ngraph = sns.countplot(df_credit['Education_Level'])\nfor p in graph.patches:\n    for p in graph.patches:\n        height = p.get_height()\n        graph.text(p.get_x()+p.get_width()/2., height + 0.1,height ,ha=\"center\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Marital_Status"},{"metadata":{},"cell_type":"markdown","source":"Lets know the marital status of the individuals"},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = df_credit['Marital_Status'].value_counts().index.to_list()\nsizes = df_credit['Marital_Status'].value_counts()\nfig, ax=plt.subplots()\nax.pie(sizes, labels = labels, autopct=\"%1.1f%%\", shadow=True, startangle=90)\nax.axis('equal')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Income_Category"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize = (20,8))\ngraph = sns.countplot(df_credit['Income_Category'])\nfor p in graph.patches:\n    for p in graph.patches:\n        height = p.get_height()\n        graph.text(p.get_x()+p.get_width()/2., height + 0.1,height ,ha=\"center\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Card_Category\nLets know what are the types of cards, and whats their composition\n\nTo get to know about card category, we are gonna use treemap"},{"metadata":{"trusted":true},"cell_type":"code","source":"import squarify #for making treemap, we need squarify\nplt.figure(figsize=(20,8))\nlabels=[i for i in zip(df_credit['Card_Category'].value_counts().index.to_list(), list(df_credit['Card_Category'].value_counts()))]\ncolors = [plt.cm.Spectral(i/float(len(labels))) for i in range(len(labels))]\nsquarify.plot(sizes=df_credit['Card_Category'].value_counts(),color=colors, label=labels, alpha=.8)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Months_on_book"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,8))\nplt.hist(df_credit['Months_on_book'], edgecolor = 'pink')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Total_Relationship_Count"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize = (20,8))\ngraph = sns.countplot(df_credit['Total_Relationship_Count'])\nfor p in graph.patches:\n    for p in graph.patches:\n        height = p.get_height()\n        graph.text(p.get_x()+p.get_width()/2., height + 0.1,height ,ha=\"center\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Months_Inactive_12_mon"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize = (20,8))\ngraph = sns.countplot(df_credit['Months_Inactive_12_mon'])\nfor p in graph.patches:\n    for p in graph.patches:\n        height = p.get_height()\n        graph.text(p.get_x()+p.get_width()/2., height + 0.1,height ,ha=\"center\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Contacts_Count_12_mon"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize = (20,8))\ngraph = sns.countplot(df_credit['Contacts_Count_12_mon'])\nfor p in graph.patches:\n    for p in graph.patches:\n        height = p.get_height()\n        graph.text(p.get_x()+p.get_width()/2., height + 0.1,height ,ha=\"center\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Engineering"},{"metadata":{"trusted":true},"cell_type":"code","source":" cat","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### These are the categorical variables, we will now either do encoding or make dummy so as to make them all numerical so that we can plot out heatmap and proceed further"},{"metadata":{},"cell_type":"markdown","source":"#### Attrition_Flag"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_credit['Attrition_Flag'] = df_credit['Attrition_Flag'].astype('category')\ndf_credit['Attrition_Flag'] = df_credit['Attrition_Flag'].cat.codes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_credit.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1 - Existing Customer 0 - Attrited Customer"},{"metadata":{},"cell_type":"markdown","source":"#### Converting rest cat into dummies"},{"metadata":{"trusted":true},"cell_type":"code","source":"def making_new_df(data, columnlist):\n    for i in columnlist:\n        dummy = pd.get_dummies(data[i])\n        #print(dummy)\n        del dummy[dummy.columns[-1]]\n        data = pd.concat([data, dummy], axis = 1)\n    return data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_credit_new = making_new_df(df_credit, cat[1:])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_credit_new","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### lets make a heatmap so as to get, whats the correlation b/w every other columns. But as we have categorical columns as well, so we will use integer encoding so make them numerical and then make heatmap"},{"metadata":{"trusted":true},"cell_type":"code","source":"corr = df_credit_new.corr(method='pearson')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(35,35))\nax= sns.heatmap(corr, vmin = -1, vmax = 1, square = True, annot=True)\nax.set_xticklabels(ax.get_xticklabels(), rotation = 45, horizontalalignment = 'right', fontsize=10)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Even though being informative, its clumsy, so now we will try different method so as to get those upmost independent variables upon which Attrition_Flag depends upon"},{"metadata":{},"cell_type":"markdown","source":"Removing all those categorical columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_credit_new = df_credit_new.drop(columns= cat[1:], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_credit_new","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Selection"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (20,6))\nsns.set_style('darkgrid')\n\nx = df_credit_new.loc[:, df_credit_new.columns !='Attrition_Flag']\ny = df_credit_new.loc[:, df_credit_new.columns == 'Attrition_Flag']\n\nfrom sklearn.ensemble import ExtraTreesClassifier\nmodel = ExtraTreesClassifier()\nmodel.fit(x,y)\nprint(model.feature_importances_)\nfeat_importances = pd.Series(model.feature_importances_, index = x.columns)\nimg = feat_importances.nlargest(10).plot(kind = 'barh')\nlabels1 = feat_importances.nlargest(10).plot(kind = 'barh').get_yticklabels()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"required_labels = list()\nfor i in labels1:\n    j = str(i)\n    required_labels.append(j[12:len(j)-2])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Making"},{"metadata":{},"cell_type":"markdown","source":"### Lets split our model into training and testing set"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df_credit_new.loc[:, required_labels]\nY = df_credit_new.iloc[:, 1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_x, test_x, train_y, test_y = train_test_split(X, Y, test_size = 0.25, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nLR = LogisticRegression()\nmodel_lr = LR.fit(train_x, train_y)\ny_lr_predict = model_lr.predict(test_x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"LR_df = pd.DataFrame(data = {\"Actual\": test_y.to_numpy(), \"Predicted\": y_lr_predict})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"LR_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_lr.score(test_x, test_y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Random Forest Classififer"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier()\nmodel_rfr = rfc.fit(train_x, train_y)\ny_rfr_predict = model_rfr.predict(test_x)\nRFR_df = pd.DataFrame(data = {\"Actual\": test_y, \"Predicted\": y_rfr_predict})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"RFR_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_rfr.score(test_x, test_y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Naive Bayes"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\ngnb = GaussianNB()\nmodel_gnb = gnb.fit(train_x, train_y)\ny_gnb_predict = model_gnb.predict(test_x)\nGNB_df = pd.DataFrame(data = {\"Actual\":test_y, \"Predicted\": y_gnb_predict})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"GNB_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_gnb.score(test_x, test_y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Support Vector Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\nsvc = SVC()\nmodel_svc = svc.fit(train_x, train_y)\ny_svc_predict = model_svc.predict(test_x)\nsvc_df = pd.DataFrame(data = {\"Actual\":test_y, \"Predicted\": y_svc_predict})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svc_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_svc.score(test_x, test_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_axis = ['LR', 'RFC', 'NB', 'SVC']\ny_axis = [model_lr.score(test_x, test_y), model_rfr.score(test_x, test_y), model_gnb.score(test_x, test_y), model_svc.score(test_x, test_y)]\nplt.figure(figsize = (10,7))\nplt.plot(x_axis, y_axis, \"*\")\nplt.xticks(rotation = -45)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## We got the highest accuracy by Random Forest Classifier algorithm."},{"metadata":{},"cell_type":"markdown","source":"So with this we come to an end of this notebook, do upvote if you think my work is good. If you find anything wrong, or have any doubt, do ask. I will try to answer them with the knowledge I have."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}