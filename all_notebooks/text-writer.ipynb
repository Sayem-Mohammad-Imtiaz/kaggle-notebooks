{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport random\nimport string\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\ndf = pd.read_csv('/kaggle/input/wikipedia-movie-plots/wiki_movie_plots_deduped.csv')\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"chunk_size = 2\nnum_chunks = 4\nhashed_buckets = 1024\nvocab = list(string.ascii_lowercase) + list(' ().,!?\"\\'-:;')\n\ndef split_text(text):\n    split = [(text[i:i+chunk_size]) for i in range(0, len(text), chunk_size)] \n    return split\n\ndef encode_text(text):\n    x = layers.experimental.preprocessing.StringLookup(vocabulary=vocab)(list(text.lower()))\n    x = layers.experimental.preprocessing.CategoryEncoding(max_tokens=len(vocab)+1)(x)\n    return x\n\ndef decode_text(tensor):\n    x = tf.argmax(tensor, axis=1)\n    x = layers.experimental.preprocessing.StringLookup(vocabulary=['[?]']+vocab, invert=True)(x)\n    x = ''.join([tf.compat.as_str_any(tensor.numpy()) for tensor in x])\n    return x\n\ninp = x = 'This is a test! Let\\'s see how it goes.'\nx = encode_text(x)\nx = decode_text(x)\n\nprint(x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def gen():\n    while True:\n        batch_inputs = []\n        batch_outputs = []\n        while len(batch_inputs) < 64:\n            text = df['Plot'].sample().values[0].lower()\n            if (len(text) <= num_chunks * chunk_size + 1):\n                continue\n            segment_start = random.randint(0, len(text)-num_chunks*chunk_size-1)\n            text_segment = text[segment_start:segment_start+num_chunks*chunk_size]\n            answer = text[segment_start+num_chunks*chunk_size:segment_start+num_chunks*chunk_size+1]\n            batch_inputs.append(split_text(text_segment))\n            batch_outputs.append(encode_text(answer)[0])\n            \n        yield np.array(batch_inputs), np.array(batch_outputs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def calc_freq():\n    letters = { '[?]': 0 }\n    total = 0\n    for letter in vocab:\n        letters[letter] = 0\n    for i in range(1000):\n        text = df['Plot'].sample().values[0].lower()\n        for letter in text:\n            total += 1\n            try:\n                letters[letter] += 1\n            except:\n                letters['[?]'] += 1\n    for letter in letters:\n        letters[letter] /= total\n    return letters\n    \nfrequencies = calc_freq()\nprint(frequencies)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"inputs = x = keras.Input(shape=(num_chunks, ), dtype='string')\nx = layers.experimental.preprocessing.Hashing(num_bins=hashed_buckets)(x)\nx = layers.Lambda(lambda x:keras.backend.one_hot(keras.backend.cast(x,'int64'),hashed_buckets))(x)\nx = layers.Flatten()(x)\nx = layers.Dense(1024, activation=\"relu\")(x)\nx = layers.Dense(512, activation=\"relu\")(x)\nx = layers.Dense(128, activation=\"relu\")(x)\noutputs = x = layers.Dense(len(vocab)+1, activation=\"softmax\")(x)\n\nmodel = keras.Model(inputs=inputs, outputs=outputs, name=\"text_predict_model\")\nmodel.summary()\n\nmodel.compile(\n    loss=keras.losses.CategoricalCrossentropy(),\n    optimizer=keras.optimizers.Adam(0.01),\n    metrics=[\"categorical_crossentropy\", \"categorical_accuracy\"],\n)\n\nmodel.fit(gen(), epochs=50, steps_per_epoch=100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict(text):\n    prediction_raw = model.predict(np.array([split_text(text[-num_chunks * chunk_size:])]))\n    print(tf.argmax(prediction_raw, axis=1))\n#     print(layers.experimental.preprocessing.StringLookup(vocabulary=['[?]']+vocab, invert=True)(prediction_raw))\n    print(decode_text(prediction_raw))\n    prediction = np.array(list(prediction_raw[0][1:]) + [0])\n    possibilities = ['[?]'] + vocab\n    data = [(possibilities[i], prediction[i] ) for i in range(len(prediction))]\n    data.sort(reverse=True, key=lambda letter: letter[1])\n    print(data)\n    print(len(prediction), len(possibilities))\n    print(random.choices(possibilities, prediction))\n\ndef generate_next(text, temperature):\n    prediction_raw = model.predict(np.array([split_text(text[-num_chunks * chunk_size:])]))\n    prediction = np.array(list(prediction_raw[0][1:]) + [0])\n    possibilities = ['[?]'] + vocab\n    return random.choices(possibilities, prediction ** (1/temperature))[0]\n\ntext = 'This is a story about '\nfor i in range(100):\n    text += generate_next(text, 0.35)\n\nprint(text)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}