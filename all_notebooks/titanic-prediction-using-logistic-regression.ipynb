{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"4a3d81a3-5987-fd30-d97a-9cae0d957b0a"},"source":"The purpose of this code, is to demonstrate , the power of Feature Engineering on traditional dataset \nThe weights trained in this code , can be used to predict the Titanic Data set tutorial competition available on   Kaggle.Even with Logistic model being a linear classifier , has some limitations , with feature engineering, the resulting trained weights produce an astonishing, 0.79 classification rate on the actual competition.\nThus proving , that the cleaned dataset, works well on the competition and also on the given test data set."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9e476b91-07a1-d25e-f09f-edc105222ef8","collapsed":true},"outputs":[],"source":"import numpy as np \nimport matplotlib.pyplot as plt \nimport pandas\nimport os "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a50052ee-0034-1547-bd46-45bdf315d1a9"},"outputs":[],"source":"os.listdir()\ndf = pandas.read_csv(\"../input/train_data.csv\")\ndf.head()# Analysing the data set and its columns "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"916e83fd-34b2-5ce9-c468-25fb24c52eb3"},"outputs":[],"source":"def get_data(string):\n    if string==\"train\":\n        df = pandas.read_csv(\"../input/train_data.csv\")\n       \n    else:\n        df=pandas.read_csv(\"../input/test_data.csv\")\n        \n        \n    T = np.asarray(df.ix[:,\"Survived\"]).reshape(df[\"Survived\"].shape[0],1)\n    X=np.asarray(df.ix[:,\"Sex\":])# splitting our input & target variables \n    #adding bias\n    bias = np.ones((df[\"Survived\"].shape[0],1))\n    X = np .hstack([bias,X])\n    return X,T\ndef sigmoid(Z):\n    Z =np.exp(-Z)\n    return 1/(1+Z)\ndef forward(X,W):\n    out = sigmoid(X.dot(W))\n    return out\ndef cross_entropy(T,Y):\n    return -(T*np.log(Y)+(1-T)*np.log(1-Y)).sum()\ndef classification_rate(T,Y):\n    return np.mean(T==Y)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"48fc3023-af23-421f-e8d3-487a660983b7"},"outputs":[],"source":"X,T = get_data(\"train\")\nX_test , Y_test = get_data(\"test\")\nN,D = X.shape\nW = np.random.randn(D,1) # setting up the weight matrix \nlearning_rate = 10e-3\nreg =10e-1\ncost=[]\n#running gradient descent now \nfor i in range(0,2000):\n    Y_train=forward(X,W)\n    W = W - learning_rate*(X.T.dot(Y_train-T)+reg*np.sign(W))#We used l1 regularization here \n    if i%20 ==0:\n        Y_pred = forward(X_test,W)\n        c = cross_entropy(Y_test,Y_pred)\n        cost.append(c)\n        r=classification_rate(Y_test,np.round(Y_pred))\n        R =classification_rate(T,np.round(Y_train))\n\nplt.plot(cost)\nplt.xlabel(\"Iterations\")\nplt.ylabel(\"Cost\")\nplt.show()\nprint(\"Classification rate on training set\",R)\nprint(\"Classification rate on test set \",r)"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}