{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\n\nimport numpy as np \nimport pandas as pd \n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Over-ride Pyhton build-in str class to support Turkish lower-casing\nclass UnicodeTr(str):\n    CHAR_MAP = {\n        \"to_upper\": {\n            u\"ı\": u\"I\",\n            u\"i\": u\"İ\",\n        },\n        \"to_lower\": {\n            u\"I\": u\"ı\",\n            u\"İ\": u\"i\",\n        }\n    }\n\n    def lower(self):\n        for key, value in self.CHAR_MAP['to_lower'].items():\n            self = self.replace(key, value)\n        return self.lower()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\n\n# regular expression for Turkish word tokenizer\ndef compile_word_tokenizer_regex():\n    suffixes = r\"[a-zğçşöüı]{3,}' ?[a-zğçşöüı]+\"\n    numbers = r\"%\\d{2,}[.,:/\\d-]+\"\n    any_word = r\"[a-zğçşöüı_+%\\.()@&`’/\\\\\\d-]+\"\n    punctuations = r\"[a-zğçşöüı]*[,!?;:]\"\n\n    return re.compile(\n        \"|\".join(\n            [suffixes,\n             numbers,\n             any_word,\n             punctuations\n             ]\n        ), re.I\n    )\n\n# hold compiled version only once for performance issues\nword_tokenizer_pre_compiled_regex = compile_word_tokenizer_regex()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from typing import Tuple\n\n# Main word tokenizer function dedicated to token given sentence using compiled regular expression. \n# Output is Tuple for performance issues.\ndef word_tokenize(sentence: str, word_regex) -> Tuple:\n    try:\n        words: Union[List] = word_regex.findall(sentence)\n    except (re.error, TypeError):\n        return ()\n    else:\n        # If last word ends with dot, it should be another word\n        words: Union[Tuple] = tuple(words)\n        if words:\n            end_dots = re.search(r'\\b(\\.+)$', words[-1])\n            if end_dots:\n                dots: str = end_dots.group(1)\n                words = words[:-1] + (words[-1][:-len(dots)],) + (dots,)\n        return words","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Read data\ndata_path = '/kaggle/input/ttc4900/7allV03.csv'\ndata = pd.read_csv(data_path, encoding='utf-8', sep=',')\n\n# white space removal at category names\ndata['category'] = data['category'].str.strip()\n\n# see samples\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Data distribution over classes\n# As it is seen data is balanced\n\nunique_categories = data.category.unique()\nfor category in unique_categories:\n    print(f\"{category}: {len(data.loc[data['category'] == category])}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Split data\ntrain, test = train_test_split(data, test_size=0.2, random_state=42)\ntrain.reset_index(drop=True, inplace=True)\ntest.reset_index(drop=True, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Pre-processing"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load Turkish stop words\nstop_words = set()\nwith open('/kaggle/input/turkish-stop-words/tr_stop_words', 'r',\n          encoding='utf-8') as fp:\n    for line in fp:\n        stop_words.add(re.sub(r'\\n', '', line))\n\npunctuation = '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^`{|}~'\ndef pre_process(line: str):\n    # normalization (lower case, punctuation, numbers, white space)\n    line = line.translate(str.maketrans('', '', punctuation))\n    line = re.sub(r'( +)|([\\d\\n])', ' ', UnicodeTr(line).lower().strip())\n    \n    # stop words removal\n    line_words = word_tokenize(line, word_tokenizer_pre_compiled_regex)\n    line_words = [word for word in line_words if word not in stop_words]\n    return ' '.join(line_words)\n\n\n# Reformat data\ntrain[\"label_format\"] = 0\nfor index, i in enumerate(range(len(train))):\n    line = f'__label__{str(train.category[i])} {str(train.text[i])}'\n    train.label_format[i] = pre_process(line)\ntrain.label_format.to_csv('ttc4900.train', index=False, header=None, sep=',')\n\ntest[\"label_format\"] = 0\nfor index, i in enumerate(range(len(test))):\n    line = f\"__label__{str(test.category[i])} {str(test.text[i])}\"\n    test.label_format[i] = pre_process(line)\ntest.label_format.to_csv('ttc4900.test', index=False, header=None, sep=',')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Fasttext Train"},{"metadata":{"trusted":true},"cell_type":"code","source":"from fasttext import train_supervised\n\nmodel = train_supervised('ttc4900.train',\n                         epoch=50,\n                         lr=1,\n                         label_prefix='__label__',\n                         wordNgrams=2,\n                         dim=100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Traning accuracy\nmodel.test('ttc4900.train')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Test accuracy\nmodel.test('ttc4900.test')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Quantize\nmodel.quantize(input='ttc4900.train',\n               qnorm=True,\n               retrain=True,\n               lr=1,\n               epoch=50,\n               verbose=True,\n               cutoff=100_000)\n\nmodel.is_quantized()  # True\nmodel.save_model('ttc4900.model.quantised')  # just 6.4 MB","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls -l","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Single Test\nimport time\n\ntext = \"\"\"\nÖzdağ'ın açıklamalarından satır başları:\n\nEvet, bekliyordum. Çünkü sizin programınıza katıldıktan sonra Genel Başkan başta olmak üzere Genel Merkez yetkililerinden doğrusu çok ağır, hakaret içeren bir söylemle karşılaştım. Buna 'üzülmedim' desem yalan olur. Neticede aynı siyasal hareket içerisinde, daha sonra da aynı siyasi partide, partinin kuruşundan itibaren birlikte çalıştığımız arkadaşlar... İhraç edilmeyi bekliyordum. Davet edebilirlerdi, konuşabilirdik. Bunun yerine bana çok ağır ifadelerle saldırmaya başladılar.\n\"\"\"\n\nstart = time.time()\nprediction = model.predict(\n    pre_process(text),\n    k=-1, threshold=0.4)\n\nprint(f'{prediction} in {(time.time() - start) * 1_000} miliseconds')  # 0.9534 milisecond\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **Conclusion**\n\nWith very few data, feature engineering together with Logistic Regressionm stacked with hierarchical soft-max layer enables to create an almost fit model.\nFuture work: this data should be augmented, and precision should be pulled up over 95. \n\nPlease do not hesitate to contact with me, if you have a bright idea\n\ne-mail: : apdullah.yayik@mobildev.com"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}