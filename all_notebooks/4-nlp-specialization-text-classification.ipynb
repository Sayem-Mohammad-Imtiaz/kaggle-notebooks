{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom tqdm import tqdm\n\npd.options.display.max_colwidth = 200\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames[:5]:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"execution":{"iopub.status.busy":"2021-06-26T16:21:12.417957Z","iopub.execute_input":"2021-06-26T16:21:12.418487Z","iopub.status.idle":"2021-06-26T16:21:12.712611Z","shell.execute_reply.started":"2021-06-26T16:21:12.418383Z","shell.execute_reply":"2021-06-26T16:21:12.711834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Text classification using Machine Learning\n\nIn this notebook we learn how to classify texts using machine learning.\n\n### Table of Contents\n\n* Classification using few ML techniques\n    * Logistic Regression\n    * Naive Bayes\n    * Random Forest\n* Cross Validation, Model evaluation\n* Model interpretation using ELI5\n* Hyperparameter tuning\n* Ensemble","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/nlp-specialization-data/Cleaned_POS_Medical_Notes.csv') #for excel file use read_excel\ndf","metadata":{"execution":{"iopub.status.busy":"2021-06-26T16:21:16.45717Z","iopub.execute_input":"2021-06-26T16:21:16.457688Z","iopub.status.idle":"2021-06-26T16:21:16.607261Z","shell.execute_reply.started":"2021-06-26T16:21:16.457651Z","shell.execute_reply":"2021-06-26T16:21:16.606519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let us first check the distribution of the outputs.","metadata":{}},{"cell_type":"code","source":"df['label'].value_counts(normalize=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-26T16:21:32.285881Z","iopub.execute_input":"2021-06-26T16:21:32.286507Z","iopub.status.idle":"2021-06-26T16:21:32.315221Z","shell.execute_reply.started":"2021-06-26T16:21:32.286458Z","shell.execute_reply":"2021-06-26T16:21:32.314445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As discussed in the previous session, we need numeric values to use in the models. We use Tfidf representation of texts.","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\ntfidf_vector = TfidfVectorizer(lowercase=True, #this will convert all the tokens into lower case\n                         stop_words='english', #remove english stopwords from vocabulary. if we need the stopwords this value should be None\n                         analyzer='word', #tokens should be words. we can also use char for character tokens\n                         max_features=50000, #maximum vocabulary size to restrict too many features\n                         min_df = 5,\n                         max_df = .6\n                        )\n\ntfidf_vectorized_corpus = tfidf_vector.fit_transform(df.clean_text)","metadata":{"execution":{"iopub.status.busy":"2021-06-26T16:22:02.369665Z","iopub.execute_input":"2021-06-26T16:22:02.37016Z","iopub.status.idle":"2021-06-26T16:22:03.66555Z","shell.execute_reply.started":"2021-06-26T16:22:02.370126Z","shell.execute_reply":"2021-06-26T16:22:03.664319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tfidf_vectorized_corpus","metadata":{"execution":{"iopub.status.busy":"2021-06-26T16:23:51.369282Z","iopub.execute_input":"2021-06-26T16:23:51.369677Z","iopub.status.idle":"2021-06-26T16:23:51.376528Z","shell.execute_reply.started":"2021-06-26T16:23:51.369644Z","shell.execute_reply":"2021-06-26T16:23:51.375243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print (tfidf_vectorized_corpus.shape)","metadata":{"execution":{"iopub.status.busy":"2021-06-26T16:22:04.62097Z","iopub.execute_input":"2021-06-26T16:22:04.621382Z","iopub.status.idle":"2021-06-26T16:22:04.627818Z","shell.execute_reply.started":"2021-06-26T16:22:04.621347Z","shell.execute_reply":"2021-06-26T16:22:04.626462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have total 818 texts (data points) and 3842 features (words) for the model. We will use Simple Logistic Regression, Naive Bayes and random forest classifier for our modelling. \n\n#### Logistic Regression\n\nLogistic regression assumes a linear relationship among the features and predicts log-odd $\\log{\\frac{p}{(1-p)}}$ of $Y=1$.\n\n#### Naive Bayes\n\nIn machine learning, naïve Bayes classifiers are a family of simple \"probabilistic classifiers\" based on applying Bayes' theorem with strong (naïve) independence assumptions between the features.\n\n<img src=https://uc-r.github.io/public/images/analytics/naive_bayes/naive_bayes_icon.png>\n\n#### Random Forest\n\nRandom forests are a collection of simple decision trees. Decision tree is a modelling technique that uses logical cummulation of decision rules to predict target from a set of features.\n\n<img src=https://upload.wikimedia.org/wikipedia/commons/f/f3/CART_tree_titanic_survivors.png>\n\nRandom forests are based on ensemble methods, which uses averaging of multiple such decision trees. Each decision tree learns different types of decision rules. Individual decision trees are prone to overfitting. To reduce the variance, we use averaging of decision trees which lead to more robust model.","metadata":{}},{"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score, StratifiedKFold","metadata":{"execution":{"iopub.status.busy":"2021-06-26T16:24:27.429671Z","iopub.execute_input":"2021-06-26T16:24:27.430127Z","iopub.status.idle":"2021-06-26T16:24:27.768971Z","shell.execute_reply.started":"2021-06-26T16:24:27.43009Z","shell.execute_reply":"2021-06-26T16:24:27.767723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For evaluation, we use cross validation. i.e. train on a part of data and testing on the remaining. We will use 5-fold cross validation. \n\n<img src=https://miro.medium.com/max/1710/1*rgba1BIOUys7wQcXcL4U5A.png width=\"500\">","metadata":{}},{"cell_type":"code","source":"lg = LogisticRegression(multi_class='auto',solver='lbfgs')\ncv_scores = cross_val_score(X=tfidf_vectorized_corpus,y=df.label,cv=5,estimator=lg)\nprint (cv_scores, np.mean(cv_scores),np.std(cv_scores))","metadata":{"execution":{"iopub.status.busy":"2021-06-26T16:29:28.432181Z","iopub.execute_input":"2021-06-26T16:29:28.432538Z","iopub.status.idle":"2021-06-26T16:29:31.359318Z","shell.execute_reply.started":"2021-06-26T16:29:28.432508Z","shell.execute_reply":"2021-06-26T16:29:31.358081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cv_scores = cross_val_score(X=tfidf_vectorized_corpus,y=df.label,cv=StratifiedKFold(5,random_state=42,shuffle=True),estimator=lg)\nprint (cv_scores, np.mean(cv_scores),np.std(cv_scores))","metadata":{"execution":{"iopub.status.busy":"2021-06-26T16:34:51.35529Z","iopub.execute_input":"2021-06-26T16:34:51.355667Z","iopub.status.idle":"2021-06-26T16:34:55.184361Z","shell.execute_reply.started":"2021-06-26T16:34:51.355635Z","shell.execute_reply":"2021-06-26T16:34:55.182926Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nb = MultinomialNB()\ncv_scores = cross_val_score(X=tfidf_vectorized_corpus,y=df.label,cv=5,estimator=nb)\nprint (cv_scores, np.mean(cv_scores),np.std(cv_scores))","metadata":{"execution":{"iopub.status.busy":"2021-06-26T16:30:15.587932Z","iopub.execute_input":"2021-06-26T16:30:15.588393Z","iopub.status.idle":"2021-06-26T16:30:15.642165Z","shell.execute_reply.started":"2021-06-26T16:30:15.588353Z","shell.execute_reply":"2021-06-26T16:30:15.640881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"NB and Logistic regression both achieve ~75% accuracy on cross validation dataset. As our dataset is not balanced, stratified sampling is better than random kfold. Similarly, f1 metric is better evaluation metric.","metadata":{}},{"cell_type":"code","source":"cv_scores = cross_val_score(X=tfidf_vectorized_corpus,y=df.label,cv=StratifiedKFold(5,random_state=42,shuffle=True),estimator=nb)\nprint (cv_scores, np.mean(cv_scores),np.std(cv_scores))","metadata":{"execution":{"iopub.status.busy":"2021-06-26T16:30:43.103338Z","iopub.execute_input":"2021-06-26T16:30:43.10377Z","iopub.status.idle":"2021-06-26T16:30:43.158745Z","shell.execute_reply.started":"2021-06-26T16:30:43.103732Z","shell.execute_reply":"2021-06-26T16:30:43.157527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Naive Bayes performs same on stratified KFold, which shows the robustness of the model. Logistic regression performs pretty much similar to Naive Bayes.","metadata":{}},{"cell_type":"markdown","source":"Now let us use a simple RF classifier and see how does it perform on 5 fold cross validation.","metadata":{}},{"cell_type":"code","source":"model = RandomForestClassifier(n_estimators=101, random_state=42) #n_estimator is the parameter to control number of decision tress\ncv_scores = cross_val_score(X=tfidf_vectorized_corpus,y=df.label,cv=5,estimator=model)\nprint (cv_scores, np.mean(cv_scores),np.std(cv_scores))","metadata":{"execution":{"iopub.status.busy":"2021-06-26T16:35:22.074977Z","iopub.execute_input":"2021-06-26T16:35:22.075389Z","iopub.status.idle":"2021-06-26T16:35:33.930329Z","shell.execute_reply.started":"2021-06-26T16:35:22.075345Z","shell.execute_reply":"2021-06-26T16:35:33.929236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cv_scores = cross_val_score(X=tfidf_vectorized_corpus,y=df.label,cv=StratifiedKFold(5,random_state=42,shuffle=True),estimator=model)\nprint (cv_scores, np.mean(cv_scores),np.std(cv_scores))","metadata":{"execution":{"iopub.status.busy":"2021-06-26T16:35:33.931916Z","iopub.execute_input":"2021-06-26T16:35:33.932237Z","iopub.status.idle":"2021-06-26T16:35:45.76176Z","shell.execute_reply.started":"2021-06-26T16:35:33.932207Z","shell.execute_reply":"2021-06-26T16:35:45.760505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"RF achieves 69% macro F1 score on cross validation, much worse than Naive Bayes and Logistic Regression. Now let us use Logistic regrssion on a particular fold and interpret the results.","metadata":{}},{"cell_type":"code","source":"for train_idx, val_idx in StratifiedKFold(n_splits=5,random_state=42,shuffle=True).split(tfidf_vectorized_corpus,df.label.values):\n    break","metadata":{"execution":{"iopub.status.busy":"2021-06-26T16:36:25.907208Z","iopub.execute_input":"2021-06-26T16:36:25.907603Z","iopub.status.idle":"2021-06-26T16:36:25.916415Z","shell.execute_reply.started":"2021-06-26T16:36:25.907571Z","shell.execute_reply":"2021-06-26T16:36:25.915177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainX = tfidf_vectorized_corpus[train_idx]\nvalX = tfidf_vectorized_corpus[val_idx]\ntrainy = df.label.values[train_idx]\nvaly = df.label.values[val_idx]\n\nprint (trainX.shape, valX.shape)","metadata":{"execution":{"iopub.status.busy":"2021-06-26T16:36:37.794992Z","iopub.execute_input":"2021-06-26T16:36:37.795969Z","iopub.status.idle":"2021-06-26T16:36:37.808059Z","shell.execute_reply.started":"2021-06-26T16:36:37.79591Z","shell.execute_reply":"2021-06-26T16:36:37.806941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report","metadata":{"execution":{"iopub.status.busy":"2021-06-26T16:36:50.686901Z","iopub.execute_input":"2021-06-26T16:36:50.687433Z","iopub.status.idle":"2021-06-26T16:36:50.691145Z","shell.execute_reply.started":"2021-06-26T16:36:50.6874Z","shell.execute_reply":"2021-06-26T16:36:50.690216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lg = LogisticRegression(multi_class='auto',solver='lbfgs')\nlg.fit(trainX,trainy)\n\nval_train= lg.predict(trainX)\nval_pred = lg.predict(valX)","metadata":{"execution":{"iopub.status.busy":"2021-06-26T16:37:52.171316Z","iopub.execute_input":"2021-06-26T16:37:52.171674Z","iopub.status.idle":"2021-06-26T16:37:52.644138Z","shell.execute_reply.started":"2021-06-26T16:37:52.171644Z","shell.execute_reply":"2021-06-26T16:37:52.642222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print (\"Accuracy score: {}\".format(accuracy_score(trainy,val_train)))\nprint (\"F1 score: {}\".format(f1_score(trainy,val_train,average='macro')))","metadata":{"execution":{"iopub.status.busy":"2021-06-26T16:38:11.33495Z","iopub.execute_input":"2021-06-26T16:38:11.335347Z","iopub.status.idle":"2021-06-26T16:38:11.351362Z","shell.execute_reply.started":"2021-06-26T16:38:11.335315Z","shell.execute_reply":"2021-06-26T16:38:11.350448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print (\"Accuracy score: {}\".format(accuracy_score(valy,val_pred)))\nprint (\"F1 score: {}\".format(f1_score(valy,val_pred,average='macro')))","metadata":{"execution":{"iopub.status.busy":"2021-06-26T16:38:16.538624Z","iopub.execute_input":"2021-06-26T16:38:16.539174Z","iopub.status.idle":"2021-06-26T16:38:16.549634Z","shell.execute_reply.started":"2021-06-26T16:38:16.539116Z","shell.execute_reply":"2021-06-26T16:38:16.548087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_cm(y_true, y_pred, labels, title):\n    figsize=(14,10)\n    cm = confusion_matrix(y_true, y_pred, labels=np.unique(labels))\n    cm_sum = np.sum(cm, axis=1, keepdims=True)\n    cm_perc = cm / cm_sum.astype(float) * 100\n    annot = np.empty_like(cm).astype(str)\n    nrows, ncols = cm.shape\n    for i in range(nrows):\n        for j in range(ncols):\n            c = cm[i, j]\n            p = cm_perc[i, j]\n            if i == j:\n                s = cm_sum[i]\n                annot[i, j] = '%.1f%%\\n%d/%d' % (p, c, s)\n            elif c == 0:\n                annot[i, j] = ''\n            else:\n                annot[i, j] = '%.1f%%\\n%d' % (p, c)\n    cm = pd.DataFrame(cm, index=np.unique(y_true), columns=np.unique(y_true))\n    cm.index.name = 'Actual'\n    cm.columns.name = 'Predicted'\n    fig, ax = plt.subplots(figsize=figsize)\n    plt.title(title)\n    sns.heatmap(cm, cmap='viridis', annot=annot, fmt='', ax=ax)","metadata":{"execution":{"iopub.status.busy":"2021-06-26T16:38:53.199241Z","iopub.execute_input":"2021-06-26T16:38:53.199625Z","iopub.status.idle":"2021-06-26T16:38:53.211516Z","shell.execute_reply.started":"2021-06-26T16:38:53.199593Z","shell.execute_reply":"2021-06-26T16:38:53.209816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"execution":{"iopub.status.busy":"2021-06-26T16:39:28.35481Z","iopub.execute_input":"2021-06-26T16:39:28.355191Z","iopub.status.idle":"2021-06-26T16:39:28.658172Z","shell.execute_reply.started":"2021-06-26T16:39:28.35516Z","shell.execute_reply":"2021-06-26T16:39:28.657014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#print (confusion_matrix(valy, val_pred,labels=model.classes_))\nlabels = lg.classes_\nplot_cm(valy,val_pred,labels,'Confusion matrix: F1 {}'.format(f1_score(valy,val_pred,average='macro')))","metadata":{"execution":{"iopub.status.busy":"2021-06-26T16:39:30.629906Z","iopub.execute_input":"2021-06-26T16:39:30.630296Z","iopub.status.idle":"2021-06-26T16:39:31.012483Z","shell.execute_reply.started":"2021-06-26T16:39:30.630264Z","shell.execute_reply":"2021-06-26T16:39:31.011697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the above confusion matrix we can clearly see where our model performed good and where it requires improvment.","metadata":{}},{"cell_type":"markdown","source":"## Model interpretation\n\nFor interpretability, we must need to understand how our model has learned the task. We will use Eli5. It uses LIME (Local Interpretable Model-agnostic Explanation) technique. Similarly, another popular technique to explain ML models is SHAP (Shapley Additive Explanations).","metadata":{}},{"cell_type":"code","source":"import eli5","metadata":{"execution":{"iopub.status.busy":"2021-06-26T16:43:50.355396Z","iopub.execute_input":"2021-06-26T16:43:50.355803Z","iopub.status.idle":"2021-06-26T16:43:58.982916Z","shell.execute_reply.started":"2021-06-26T16:43:50.355765Z","shell.execute_reply":"2021-06-26T16:43:58.981678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"First let us see the top words corresponding to each speciality and whether they make any sense","metadata":{}},{"cell_type":"code","source":"eli5.show_weights(lg, vec=tfidf_vector, top=25)","metadata":{"execution":{"iopub.status.busy":"2021-06-26T16:44:00.866325Z","iopub.execute_input":"2021-06-26T16:44:00.866749Z","iopub.status.idle":"2021-06-26T16:44:00.95341Z","shell.execute_reply.started":"2021-06-26T16:44:00.866715Z","shell.execute_reply":"2021-06-26T16:44:00.952323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the above visualization we see that different words have different importance for different specialities. Now let us explain few test predictions. Eli5 highlights top positive words in yellow and top negative words in red.","metadata":{}},{"cell_type":"code","source":"df.iloc[val_idx[:3]]['label']","metadata":{"execution":{"iopub.status.busy":"2021-06-26T16:47:10.851186Z","iopub.execute_input":"2021-06-26T16:47:10.851629Z","iopub.status.idle":"2021-06-26T16:47:10.861321Z","shell.execute_reply.started":"2021-06-26T16:47:10.851589Z","shell.execute_reply":"2021-06-26T16:47:10.860259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.clean_text.values[val_idx[0]]","metadata":{"execution":{"iopub.status.busy":"2021-06-26T16:47:41.740011Z","iopub.execute_input":"2021-06-26T16:47:41.740482Z","iopub.status.idle":"2021-06-26T16:47:41.748746Z","shell.execute_reply.started":"2021-06-26T16:47:41.740441Z","shell.execute_reply":"2021-06-26T16:47:41.747348Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eli5.show_prediction(lg, doc=df.clean_text.values[val_idx[0]], vec=tfidf_vector, top=10)","metadata":{"execution":{"iopub.status.busy":"2021-06-26T16:47:16.878526Z","iopub.execute_input":"2021-06-26T16:47:16.879158Z","iopub.status.idle":"2021-06-26T16:47:16.947817Z","shell.execute_reply.started":"2021-06-26T16:47:16.879102Z","shell.execute_reply":"2021-06-26T16:47:16.94662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eli5.show_prediction(lg, doc=df.clean_text.values[val_idx[1]], vec=tfidf_vector, top=10)","metadata":{"execution":{"iopub.status.busy":"2021-06-26T16:49:15.255012Z","iopub.execute_input":"2021-06-26T16:49:15.255449Z","iopub.status.idle":"2021-06-26T16:49:15.319351Z","shell.execute_reply.started":"2021-06-26T16:49:15.255414Z","shell.execute_reply":"2021-06-26T16:49:15.318301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eli5.show_prediction(lg, doc=df.clean_text.values[val_idx[2]], vec=tfidf_vector, top=10)","metadata":{"execution":{"iopub.status.busy":"2021-06-26T16:49:44.905117Z","iopub.execute_input":"2021-06-26T16:49:44.905682Z","iopub.status.idle":"2021-06-26T16:49:44.978545Z","shell.execute_reply.started":"2021-06-26T16:49:44.905645Z","shell.execute_reply":"2021-06-26T16:49:44.977803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This clearly shows the inside of our model. As we observed that our model performed poorly particularly for \"neurology\" and \"radiology\", we need to make our model robust on those classes. There are lots of different ways to increase model's performance. Here we discuss briefly about hyperparameter tuning and ensemble methods and how they can lead to better result.","metadata":{}},{"cell_type":"markdown","source":"### Hyperparameter tuning\n\nEvery model has a set of hyper parameters. By tuning different hyperparameters, we can increase model's performance. In this notebook, we tune different hyper parameters of random forest classifier.\n\n* max_depth - maximum depth of each tree\n* n_estimators - number of trees\n\nGridsearch or, randomsearch are used to tune hyperparameters and check oof (out of fold) score. In this notebook, we use hyperopt, a technique that use Bayesian Optimization to search for better hyperparameter.","metadata":{}},{"cell_type":"code","source":"from hyperopt import hp\nfrom hyperopt import fmin, tpe, space_eval, Trials","metadata":{"execution":{"iopub.status.busy":"2021-06-26T12:09:07.725391Z","iopub.status.idle":"2021-06-26T12:09:07.725843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def rf_cv(params, random_state=42, cv=5, X=tfidf_vectorized_corpus, y=df.label.values):\n    # the function gets a set of variable parameters in \"param\"\n    params = {'n_estimators': int(params['n_estimators']), \n              'max_depth': int(params['max_depth'])}\n    \n    # we use this params to create a new LGBM Regressor\n    model = RandomForestClassifier(random_state=random_state, **params)\n    \n    # and then conduct the cross validation with the same folds as before\n    score = -cross_val_score(model, X, y, cv=cv, scoring=\"f1_macro\", n_jobs=-1).mean()\n\n    return score\n\n\nspace = {'n_estimators': hp.quniform('n_estimators', 100, 1000, 50),\n       'max_depth' : hp.quniform('max_depth', 2, 20, 1)\n      }\n\n# trials will contain logging information\ntrials = Trials()\n\nbest = fmin(fn=rf_cv, # function to optimize\n          space=space, \n          algo=tpe.suggest, # optimization algorithm, hyperotp will select its parameters automatically\n          max_evals=10, # maximum number of iterations\n          trials=trials, # logging\n          rstate=np.random.RandomState(42) # fixing random state for the reproducibility\n         )","metadata":{"execution":{"iopub.status.busy":"2021-06-26T12:09:07.726759Z","iopub.status.idle":"2021-06-26T12:09:07.727193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Best F1 {:.3f} params {}\".format(-rf_cv(best), best))","metadata":{"execution":{"iopub.status.busy":"2021-06-26T12:09:07.728107Z","iopub.status.idle":"2021-06-26T12:09:07.728598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Ensemble\n\nEnsemble is very useful technique to increase model's performance and reduce overfitting. By combining multiple models, we can make our prediction more robust, reduce overfitting and increase overall performance. We use max voting of multiple classifiers.","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import VotingClassifier","metadata":{"execution":{"iopub.status.busy":"2021-06-26T12:09:07.729646Z","iopub.status.idle":"2021-06-26T12:09:07.730122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"voting_classifier = VotingClassifier(estimators=[('rf',RandomForestClassifier(n_estimators=500,max_depth=19,random_state=42)),\n                                     ('nb', MultinomialNB()),\n                                     ('lg', LogisticRegression(multi_class='auto',solver='lbfgs'))], voting='hard')\n\ncv_scores = cross_val_score(X=tfidf_vectorized_corpus,y=df.label,cv=StratifiedKFold(5,random_state=42),estimator=voting_classifier,scoring='f1_macro')\nprint (cv_scores, np.mean(cv_scores))","metadata":{"execution":{"iopub.status.busy":"2021-06-26T12:09:07.73105Z","iopub.status.idle":"2021-06-26T12:09:07.731555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Practical Tips\n\n* Always use cross validation to check model performance. Understand the data to use - KFold or, StratifiedKFold, GroupKFold\n* Start with simple model and gradually use more complex model\n* For production, use scikit-learn's pipeline for E2E feature learning and modelling.\n* Use joblib to persist model after training so that it can be called directly during inference\n* Fix random seeds to avoid any randomization","metadata":{}},{"cell_type":"markdown","source":"### References\n\n1. https://towardsdatascience.com/interpreting-your-deep-learning-model-by-shap-e69be2b47893\n\n2. https://towardsdatascience.com/understanding-model-predictions-with-lime-a582fdff3a3b","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}