{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Configure the Environment "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\n%matplotlib inline\nimport os\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nimport warnings; warnings.filterwarnings(action='once')\nimport seaborn as sns\n# from pywaffle import Waffle\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn import metrics\nfrom keras.models import Sequential\nfrom keras import layers\nfrom keras.optimizers import RMSprop\nfrom keras.utils import to_categorical\nimport keras\n\n\n# Keras version.\nprint('Using Keras version', keras.__version__)\n\nsns.set(style=\"darkgrid\")\n\nRANDOM_SEED = 42\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Read / Explore the Data"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Read data.\ndata_path = '/kaggle/input/mushroom-classification/mushrooms.csv'\ndf = pd.read_csv(data_path)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Print unique values for columns.\ncolumns = df.columns\nfor col in columns:\n    print('{feat_name}: {feat_values}'.format(feat_name=col, feat_values=df[col].unique()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Plots are minimally helpful since the data are all categorical.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x='class', data=df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**The way these examples have been recorded, however, make them ideal to be formed as sequences. We encode the categorical data, then convert them to sequences.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# First, create the list of labels as our y values.\nle = preprocessing.LabelEncoder()\ny = le.fit_transform(df['class'])\nprint(y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop the labels from the dataframe, encode all features.\nX = df.drop('class', axis=1)\ncolumns = X.columns\nfor i in range(len(X.columns)):\n    le = preprocessing.LabelEncoder()\n    X[columns[i]] = le.fit_transform(X[columns[i]])\n    \nX.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Inspect unique values again.\nfor col in columns:\n    print('{}: {}'.format(col, X[col].unique()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We need to know the maximum number of possible values for the embedding layer.\n# If we were using text, this would be the size of the vocabulary.\n# Find number of uniabsque values for each feature.\nfor col in columns:\n    print('{}: {}'.format(col, X[col].nunique()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Convert features to sequences.\nsequences = []\n# seq = '{}' * 22\ncolumns = X.columns\nfor idx, row in X.iterrows():\n    sequence = []\n    for i in range(len(columns)):\n        sequence.append(row[columns[i]])\n    sequences.append(sequence)\n    \n# Print first example and label, length of example sequence.\nprint('{sequence}: {label}'.format(sequence=sequences[0], label=y[0]))\nprint('len of sequences:', len(sequences[0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Build train/test sets.\nx_train, x_test, y_train, y_test = train_test_split(sequences, y,\n                                                    test_size=0.1,\n                                                    random_state=RANDOM_SEED)\n# Convert to numpy arrays.\nx_train = np.array(x_train)\nx_test = np.array(x_test)\ny_train = np.array(y_train)\ny_test = np.array(y_test)\n\nprint(x_train)\nprint(y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Build and Train the Model\n\n**Define a function to build the CNN model.[](http://)**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_model():\n    embeddings_dims = 300\n    max_seq_length = len(sequences[0])\n    max_features = 12\n    filters = 250\n    kernel_size = 3\n    hidden_dims = 250\n    \n    using_pretrained_emb = False #@param {type:\"boolean\"}\n\n    # CNN via Keras.\n    model = Sequential()\n\n    if using_pretrained_emb:\n      model.add(layers.Embedding(max_features,\n                                 embeddings_dims,\n                                 embeddings_initializer=Constant(vocab),\n                                 input_length=max_seq_length,\n                                 trainable=False))\n    else:\n      model.add(layers.Embedding(max_features,\n                                 embeddings_dims,\n                                 input_length=max_seq_length))\n    model.add(layers.Dropout(0.5))\n    model.add(layers.Conv1D(filters,\n                            kernel_size,\n                            padding='valid',\n                            activation='relu',\n                            strides=1))\n    model.add(layers.MaxPooling1D())\n    model.add(layers.Conv1D(filters,\n                            kernel_size,\n                            padding='valid',\n                            activation='relu',\n                            strides=1))\n    model.add(layers.MaxPooling1D())\n    model.add(layers.Conv1D(filters,\n                            kernel_size,\n                            padding='valid',\n                            activation='relu',\n                            strides=1))\n    model.add(layers.GlobalMaxPooling1D())\n    model.add(layers.Dense(hidden_dims))\n    model.add(layers.Dropout(0.5))\n    model.add(layers.Activation('relu'))\n    model.add(layers.Dense(1))\n    model.add(layers.Activation('sigmoid'))\n\n    model.compile(loss='binary_crossentropy',\n                  optimizer='adadelta',\n                  metrics=['accuracy'])\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Build the model, check out the summary.\nmodel = build_model()\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train.\nhistory = model.fit(x_train, y_train,\n                    epochs=10,\n                    verbose=True,\n                    validation_data=(x_test, y_test),\n                    batch_size=16)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Metrics\n\n**The metrics class of scikit-learn provides an easy way to produce a variety of metrics for the performance of the model.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get predictions for test set.\npreds_test = model.predict_classes(x_test)\n\ncnn_metrics = {'acc': metrics.accuracy_score(y_test, preds_test)}\ncnn_metrics['prec'] = metrics.precision_score(y_test, preds_test)\ncnn_metrics['rec'] = metrics.recall_score(y_test, preds_test)\ncnn_metrics['f1'] = metrics.f1_score(y_test, preds_test)\ncnn_metrics['f1_macro'] = metrics.f1_score(y_test, preds_test,\n                                           average='macro')\ncnn_metrics['auc'] = metrics.roc_auc_score(y_test, preds_test)\n\nfor metric in cnn_metrics:\n  print('{metric_name}: {metric_value}'.format(metric_name=metric, metric_value=cnn_metrics[metric]))\n\n# Get training and test loss histories\ntraining_loss = history.history['loss']\ntest_loss = history.history['val_loss']\n\n# Get training and test accuracy history.\ntraining_acc = history.history['acc']\ntest_acc = history.history['val_acc']\n\n# Create count of the number of epochs\nepoch_count = range(1, len(training_loss) + 1)\n\n# Visualize loss history\nplt.plot(epoch_count, training_loss, 'r--')\nplt.plot(epoch_count, test_loss, 'b-')\nplt.legend(['Training Loss', 'Test Loss'])\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.show();\n\n# Visualize acc history\nplt.plot(epoch_count, training_acc, 'r--')\nplt.plot(epoch_count, test_acc, 'b-')\nplt.legend(['Training Acc', 'Test Acc'])\nplt.xlabel('Epoch')\nplt.ylabel('Acc')\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Visualize Activations\n**Convolutional networks often produce interesting patterns in the activations of their layers, so let's take a took at them.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Using the Model class from keras, rebuild the model using the layer outputs and model inputs.\nfrom keras.models import Model\nlayer_outputs = [layer.output for layer in model.layers]\nactivation_model = Model(inputs=model.input, outputs=layer_outputs)\n\n# Run prediction for a single example to get activations.\nactivations = activation_model.predict(x_train[1].reshape(1, 22))\n\n# Plot them (switching to a heatmap once the shape of the data requires it).\nfor layer_num, act in enumerate(activations):\n    if len(act.shape) > 2:\n        plt.rcParams[\"axes.grid\"] = False\n        plt.matshow(act[0, :, :], cmap='viridis')\n    else:\n        plt.figure(figsize = (16,1))\n        sns.heatmap(act, cbar=False, cmap='viridis')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}