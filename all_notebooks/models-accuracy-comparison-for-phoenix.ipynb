{"cells":[{"metadata":{},"cell_type":"markdown","source":"# **Accuracy comparison of energy predictions for different building types in Phoenix with three machine learning models**\n\n<font size=4>Dou Zhenyu A0213310E</font>"},{"metadata":{},"cell_type":"markdown","source":"**<font size=5>1. Introduction</font>**"},{"metadata":{},"cell_type":"markdown","source":"This notebook would like to use energy consumption data of buildings in Phoenix in the dataset 'Building Genome Project 1' and then get the average energy consumption per square meter of different building types.  \n  \nAfter getting that values, this notebook would like to use three kinds of machine learning models to do the energy prediction for them, which are LightGBM, Random forest and XGBoost. After the prediction, this notebook would compare the accuracy of different prediction models and get some conclusions."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom pandas_profiling import ProfileReport\n\nimport matplotlib.pyplot as plt\nfrom matplotlib import dates as md\nimport seaborn as sns\nimport plotly.graph_objs as go\nimport plotly\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\nimport cufflinks as cf\ncf.set_config_file(offline=True)\nimport os\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.ensemble import RandomForestRegressor\n\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nimport statsmodels.tsa.api as smt\n\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" **<font size=5>2. Data filtering and cleaning</font>**"},{"metadata":{},"cell_type":"markdown","source":"The first step is to filter buildings in Phoenix, based on the preliminary understanding of the data below, there will be several weather files for each city. This may be because the buildings are located in different areas of the city. To simplify the analysis, this notebook will select the weather with the largest proportion in Phoenix to analyze the buildings contained in it."},{"metadata":{},"cell_type":"markdown","source":"# **<font size=4>2.1 Weather data and Schedule data filtering</font>**"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df_meta = pd.read_csv('/kaggle/input/building-data-genome-project-v1/meta_open.csv')\ndf_meta_Phoenix = df_meta[df_meta['timezone']=='America/Phoenix']\ndf_meta_Phoenix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_meta_Phoenix.pivot_table(index='timezone',columns='newweatherfilename', values='uid', aggfunc='count').plot.bar(stacked=True, figsize=(8,5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_meta_Phoenix.pivot_table(index='timezone',columns='annualschedule', values='uid', aggfunc='count').plot.bar(stacked=True, figsize=(8,5))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**<font size=4>2.2 Buildings filtering and getting avarage value of different building types</font>**"},{"metadata":{},"cell_type":"markdown","source":"From the diagram above, we can notice that there are only weather0 and Schedule2 in Phoenix. Hence, this notebook will next filter the energy consumption data of those buildings in weather0. In order to reduce the error caused by different buildings, this article hopes to use the energy consumption per square meter and obtain the average value of the same type of buildings."},{"metadata":{"trusted":true},"cell_type":"code","source":"list_Phoenix = df_meta_Phoenix['uid'].to_list()\nlist_Phoenix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_powermeter = pd.read_csv('/kaggle/input/building-data-genome-project-v1/temp_open_utc_complete.csv', index_col='timestamp', parse_dates=True)\ndf_powermeter.index = df_powermeter.index.tz_localize(None)\ndf_powermeter = df_powermeter/df_meta.set_index('uid').loc[df_powermeter.columns, 'sqm']\ndf_powermeter_Phoenix =  df_powermeter[list_Phoenix].dropna(how='all')\ndf_powermeter_Phoenix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_Office = df_powermeter_Phoenix.iloc[:, 0:22]\ndf_Office['Office'] = df_Office.apply(lambda x: x.mean(), axis=1)\ndf_Office = df_Office.iloc[:, 22:23]\ndf_PrimClass = df_powermeter_Phoenix.iloc[:, 22:24]\ndf_PrimClass['PrimClass'] = df_PrimClass.apply(lambda x: x.mean(), axis=1)\ndf_PrimClass = df_PrimClass.iloc[:, 2:3]\ndf_UnivClass = df_powermeter_Phoenix.iloc[:, 24:54]\ndf_UnivClass['UnivClass'] = df_UnivClass.apply(lambda x: x.mean(), axis=1)\ndf_UnivClass = df_UnivClass.iloc[:, 30:31]\ndf_UnivDorm = df_powermeter_Phoenix.iloc[:, 54:67]\ndf_UnivDorm['UnivDorm'] = df_UnivDorm.apply(lambda x: x.mean(), axis=1)\ndf_UnivDorm = df_UnivDorm.iloc[:, 13:14]\ndf_UnivLab = df_powermeter_Phoenix.iloc[:, 67:96]\ndf_UnivLab['UnivLab'] = df_UnivLab.apply(lambda x: x.mean(), axis=1)\ndf_UnivLab = df_UnivLab.iloc[:, 29:30]\ndf_Phoenix_avarage = pd.concat([df_Office,df_PrimClass,df_UnivClass,df_UnivDorm,df_UnivLab],axis=1)\ndf_Phoenix_avarage","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" **<font size=5>3. Prediction modeling</font>**"},{"metadata":{},"cell_type":"markdown","source":"As for the modelling part, this notebook will use LightGBM model, Random Forest and XGBoost model as prediction models and use time, temperature and schedule as trainnig features. Three months' data (Apr, Aug, Dec) would be the test data and the remaining 9 months' data would be training data. After prediction, this notebook would use R-SQUARED and MAPE to check the model accuracy and make some comparisons."},{"metadata":{},"cell_type":"markdown","source":" **<font size=4>3.1 Pre-processing features</font>**"},{"metadata":{},"cell_type":"markdown","source":"According to part2, the weather file of Phoenix is weather0 and the schedule file is scedule2. Next step is to do some pre-processing work for the prediction model."},{"metadata":{},"cell_type":"markdown","source":"3.1.1 Weather data processing"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_weather = pd.read_csv('/kaggle/input/building-data-genome-project-v1/weather0.csv', index_col='timestamp', parse_dates=True)\ndf_weather = df_weather.select_dtypes(['int', 'float'])\n\nfor col in df_weather.columns:\n    df_weather.loc[df_weather[col]<-100, col] = np.nan\n    \ndf_weather.fillna(method='ffill')\n\ndf_weather = df_weather.reset_index().drop_duplicates(subset=['timestamp'])\n\ndf_weather = df_weather.set_index('timestamp').resample('1H').mean()\n\ndf_weather.loc[:, df_weather.columns.str.contains('TemperatureC')].iplot()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"3.1.2 Schedule data processing"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_schedule = pd.read_csv('/kaggle/input/building-data-genome-project-v1/schedule2.csv', header=None)\ndf_schedule = df_schedule.rename(columns={0:'date',1:'date_type'})\ndf_schedule['date'] = pd.to_datetime(df_schedule['date'])\ndf_schedule_encode = df_schedule.copy()\ndf_schedule_encode['date_type'] = LabelEncoder().fit_transform(df_schedule_encode['date_type'])\ndf_schedule_encode.set_index('date').iplot()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" **<font size=4>3.2 LightGBM model</font>**"},{"metadata":{},"cell_type":"markdown","source":"[LightGBM](https://lightgbm.readthedocs.io/en/latest/) uses histogram-based algorithms, which bucket continuous feature (attribute) values into discrete bins. This speeds up training and reduces memory usage. (Ref: https://lightgbm.readthedocs.io/en/latest/)"},{"metadata":{},"cell_type":"markdown","source":"The following code cells were greatly helped by the Kaggle notebook from\nFu Chun: https://www.kaggle.com/patrick0302/load-prediction-for-bdg1-0"},{"metadata":{"trusted":true},"cell_type":"code","source":"R_light=[]\nM_light=[]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for name in ['Office','PrimClass','UnivClass','UnivDorm','UnivLab']:\n    \n    print(name)\n\n    # Prepare data    \n    \n    df_temp = df_Phoenix_avarage[[name]].copy()\n    df_temp = df_temp.dropna()\n\n    # Add time features\n    df_temp['weekday'] = df_temp.index.weekday\n    df_temp['hour'] = df_temp.index.hour\n    df_temp['date'] = pd.to_datetime(df_temp.index.date)\n\n    # Add temperature features\n    df_temp = df_temp.rename(columns={name: 'load_meas'})\n    df_temp = df_temp.merge(df_weather.loc[:, df_weather.columns.str.contains('TemperatureC')], left_index=True, right_index=True)\n\n    # Add schedule features\n    index = df_temp.index.copy()\n    df_temp = df_temp.merge(df_schedule_encode, on='date')\n    df_temp.index = index\n\n    # Split data to tainning data and testing data\n    traindata = df_temp.loc[df_temp.index.month.isin([1,2,3,5,6,7,9,10,11])].dropna().copy()\n    testdata = df_temp.loc[df_temp.index.month.isin([4,8,12])].copy()\n\n    train_labels = traindata['load_meas']\n    test_labels = testdata['load_meas']\n\n    train_features = traindata.drop(['load_meas', 'date'], axis=1)\n    test_features = testdata.drop(['load_meas', 'date'], axis=1)\n    \n    # Instantiate model \n    LGB_model = lgb.LGBMRegressor()\n    \n    # Train the model on training data\n    LGB_model.fit(train_features, train_labels)\n\n    testdata['load_pred'] = LGB_model.predict(test_features)\n    df_temp.loc[testdata.index, 'load_pred'] = testdata['load_pred']\n    \n    # Calculate the absolute errors\n    errors = abs(testdata['load_pred'] - test_labels)\n\n    RSQUARED = r2_score(testdata.dropna()['load_meas'], testdata.dropna()['load_pred'])\n    MAPE = errors/test_labels\n    MAPE = MAPE.loc[MAPE!=np.inf]\n    MAPE = MAPE.loc[MAPE!=-np.inf]\n    MAPE = MAPE.dropna().mean()*100\n    \n    # Visualization\n    print(\"R SQUARED: \"+str(round(RSQUARED,3)))\n    print(\"MAPE: \"+str(round(MAPE,1))+'%')\n    testdata[['load_meas', 'load_pred']].reset_index(drop=True).iplot()\n    \n    #Summary\n    R_light.append(RSQUARED)\n    M_light.append(MAPE)    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" **<font size=4>3.3 Random forest model</font>**"},{"metadata":{},"cell_type":"markdown","source":"[Random forests](https://en.wikipedia.org/wiki/Random_forest) or random decision forests are an ensemble learning method for classification, regression and other tasks that operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean/average prediction (regression) of the individual trees. (Ref: https://en.wikipedia.org/wiki/Random_forest)"},{"metadata":{"trusted":true},"cell_type":"code","source":"R_random=[]\nM_random=[]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for name in ['Office','PrimClass','UnivClass','UnivDorm','UnivLab']:\n    \n    print(name)\n\n    # Prepare data    \n    \n    df_temp = df_Phoenix_avarage[[name]].copy()\n    df_temp = df_temp.dropna()\n\n    # Add time features\n    df_temp['weekday'] = df_temp.index.weekday\n    df_temp['hour'] = df_temp.index.hour\n    df_temp['date'] = pd.to_datetime(df_temp.index.date)\n\n    # Add temperature features\n    df_temp = df_temp.rename(columns={name: 'load_meas'})\n    df_temp = df_temp.merge(df_weather.loc[:, df_weather.columns.str.contains('TemperatureC')], left_index=True, right_index=True)\n\n    # Add schedule features\n    index = df_temp.index\n    df_temp = df_temp.merge(df_schedule_encode, on='date')\n    df_temp.index=index\n\n    # Split data to tainning data and testing data\n    traindata = df_temp.loc[df_temp.index.month.isin([1,2,3,5,6,7,9,10,11])].dropna().copy()\n    testdata = df_temp.loc[df_temp.index.month.isin([4,8,12])].copy()\n\n    train_labels = traindata['load_meas'].fillna(0)\n    test_labels = testdata['load_meas'].fillna(0)\n\n    train_features = traindata.drop(['load_meas', 'date'], axis=1).fillna(0)\n    test_features = testdata.drop(['load_meas', 'date'], axis=1).fillna(0)\n\n    # Instantiate model \n    rf = RandomForestRegressor()\n    \n    # Train the model on training data\n    rf.fit(train_features, train_labels);\n    \n    testdata['load_pred'] = rf.predict(test_features)\n    df_temp.loc[testdata.index, 'load_pred'] = testdata['load_pred']\n    \n    # Calculate the absolute errors\n    errors = abs(testdata['load_pred'] - test_labels)\n\n    RSQUARED = r2_score(testdata.dropna()['load_meas'], testdata.dropna()['load_pred'])\n    MAPE = errors/test_labels\n    MAPE = MAPE.loc[MAPE!=np.inf]\n    MAPE = MAPE.loc[MAPE!=-np.inf]\n    MAPE = MAPE.dropna().mean()*100\n    \n    #Visualization\n    print(\"R SQUARED: \"+str(round(RSQUARED,3)))\n    print(\"MAPE: \"+str(round(MAPE,1))+'%')\n    testdata[['load_meas', 'load_pred']].reset_index(drop=True).iplot()\n    \n    #Summary\n    R_random.append(RSQUARED)\n    M_random.append(MAPE)  \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" **<font size=4>3.4 XGBoost model</font>**"},{"metadata":{},"cell_type":"markdown","source":"[XGBoost](https://xgboost.readthedocs.io/en/latest/) is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. It implements machine learning algorithms under the Gradient Boosting framework. XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way. The same code runs on major distributed environment (Hadoop, SGE, MPI) and can solve problems beyond billions of examples. (Ref: https://xgboost.readthedocs.io/en/latest/)"},{"metadata":{"trusted":true},"cell_type":"code","source":"R_xg=[] \nM_xg=[] ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for name in ['Office','PrimClass','UnivClass','UnivDorm','UnivLab']:\n    \n    print(name)\n\n    # Prepare data  \n    \n    df_temp = df_Phoenix_avarage[[name]].copy()\n    df_temp = df_temp.dropna()\n\n    # Add time features\n    df_temp['weekday'] = df_temp.index.weekday\n    df_temp['hour'] = df_temp.index.hour\n    df_temp['date'] = pd.to_datetime(df_temp.index.date)\n\n    # Add temperature features\n    df_temp = df_temp.rename(columns={name: 'load_meas'})\n    df_temp = df_temp.merge(df_weather.loc[:, df_weather.columns.str.contains('TemperatureC')], left_index=True, right_index=True)\n\n    # Add schedule features\n    index = df_temp.index\n    df_temp = df_temp.merge(df_schedule_encode, on='date')\n    df_temp.index=index\n\n    # Split data to tainning data and testing data\n    traindata = df_temp.loc[df_temp.index.month.isin([1,2,3,5,6,7,9,10,11])].dropna().copy()\n    testdata = df_temp.loc[df_temp.index.month.isin([4,8,12])].copy()\n\n    train_labels = traindata['load_meas'].fillna(0)\n    test_labels = testdata['load_meas'].fillna(0)\n\n    train_features = traindata.drop(['load_meas', 'date'], axis=1).fillna(0)\n    test_features = testdata.drop(['load_meas', 'date'], axis=1).fillna(0)\n\n    # Instantiate model \n    xg_reg = xgb.XGBRegressor()\n    \n    # Train the model on training data\n    xg_reg.fit(train_features, train_labels);\n    \n    testdata['load_pred'] = xg_reg.predict(test_features)\n    df_temp.loc[testdata.index, 'load_pred'] = testdata['load_pred']\n    \n    # Calculate the absolute errors\n    errors = abs(testdata['load_pred'] - test_labels)\n\n    RSQUARED = r2_score(testdata.dropna()['load_meas'], testdata.dropna()['load_pred'])\n    MAPE = errors/test_labels\n    MAPE = MAPE.loc[MAPE!=np.inf]\n    MAPE = MAPE.loc[MAPE!=-np.inf]\n    MAPE = MAPE.dropna().mean()*100\n    \n    #Visualization\n    print(\"R SQUARED: \"+str(round(RSQUARED,3)))\n    print(\"MAPE: \"+str(round(MAPE,1))+'%')\n    testdata[['load_meas', 'load_pred']].reset_index(drop=True).iplot()\n    \n    #Summary\n    R_xg.append(RSQUARED)\n    M_xg.append(MAPE)  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" **<font size=5>4. Accuracy comparison</font>**"},{"metadata":{},"cell_type":"markdown","source":"From the diagrams above, we can summarize the RSQUARED and MAPE of different models."},{"metadata":{"trusted":true},"cell_type":"code","source":"# For RSQUARED\nR = {'Type':['Office','PrimClass','UnivClass','UnivDorm','UnivLab'],'LightGBM':R_light,'Randomforest':R_random,'XGBoost':R_xg}\ndf_R=pd.DataFrame(R)\ndf_R=df_R.set_index('Type')\ndf_R.iplot(kind='bar',title='R-SQUARED',yaxis_title='R-SQUARED',xaxis_title='Building Type')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# For MAPE\nM = {'Type':['Office','PrimClass','UnivClass','UnivDorm','UnivLab'],'LightGBM':M_light,'Randomforest':M_random,'XGBoost':M_xg}\ndf_M=pd.DataFrame(M)\ndf_M=df_M.set_index('Type')\ndf_M.iplot(kind='bar',title='MAPE',yaxis_title='MAPE(%)',xaxis_title='Building Type')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" **<font size=5>5. Conclusion</font>**"},{"metadata":{},"cell_type":"markdown","source":"From the analysis above, this notebook concluded that:  \n1. Compared with Random Forest, LightGBM and XGBoost have better accuracy, along with better running speed.  \n2. Office and Lab are easiest to predict comparing with Class and Dormitory, this may due to the more regular schedule of Offices and Labs. \n3. The error of Primary Class is most unacceptable, this may because there are only two Primary Class buildings in this dataset."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}