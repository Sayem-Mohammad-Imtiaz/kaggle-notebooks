{"nbformat_minor":1,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**はじめに**\n\n従業員を幸せにし満足させるという問題は多年生であり、古くからの課題です。従業員が \"緑の牧草地\"のために多くの時間とお金を投資した場合、これはあなたが他の人を雇うのにさらに時間とお金を費やす必要があることを意味します。 Kaggleの精神を踏まえて、IBMの予測モデリング機能に目を向けると、このIBMデータセットの従業員の減少を予測できるかどうかを確認してください。\n\nこのノートブックは次のように構成されています。\n\nExploratory Data Analysis：このセクションでは、フィーチャの分布、相関のあるフィーチャが他のフィーチャとどのように関係しているかを見て、SeabornとPlotlyの視覚化を作成してデータセットを調べます\nフィーチャ・エンジニアリングとカテゴリ・エンコーディング：いくつかのフィーチャ・エンジニアリングを実行し、すべてのカテゴリ・フィーチャをダミー変数にエンコードします\nマシンラーニングモデルの実装：ランダムフォレストとグラディエントブーストモデルを実装し、それぞれのモデルからフィーチャーインヴァージョンを見ます\n\n# Introduction \n\nThe issue of keeping one's employees happy and satisfied is a perennial and age-old challenge. If an employee you have invested so much time and money leaves for \"greener pastures\",  then this would mean that you would have to spend even more time and money to hire somebody else. In the spirit of Kaggle, let us therefore turn to our predictive modelling capabilities and see if we can predict employee attrition on this IBM dataset. \n\nThis notebook is structured as follows:\n\n 1. **Exploratory Data Analysis** : In this section, we explore the dataset by taking a look at the feature distributions, how correlated one feature is to the other and create some Seaborn and Plotly visualisations\n 2. **Feature Engineering and Categorical Encoding** : Conduct some feature engineering as well as encode all our categorical features into dummy variables\n 3. **Implementing Machine Learning models** : We implement a Random Forest and a Gradient Boosted Model after which we look at feature importances from these respective models\n\nLet's Go.","metadata":{"_uuid":"fadf38039b50ca40f8139726d2f8eb904b222196","_cell_guid":"6ec85556-c52a-d279-8c95-6787a8520ca8"}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra 線形代数\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv) データ処理\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Import statements required for Plotly  ステートメントのインポート\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\n\n\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, log_loss\nfrom imblearn.over_sampling import SMOTE\nimport xgboost\n\n# Import and suppress warnings\nimport warnings\n\n\nwarnings.filterwarnings('ignore')","metadata":{"_uuid":"7348f0fe7e1d71585b7cf4c562515a08e6e06636","_cell_guid":"adfbe30e-7ebb-0f88-d917-d9a8f97c638e"}},{"cell_type":"markdown","source":"\n\n**＃1.探索的データ分析**\n\n信頼できるPandasパッケージを介してデータセットにロードして、** attrition **と呼ばれるデータフレームオブジェクトにロードして、最初の数行を素早く見てみましょう\n\n# 1. Exploratory Data Analysis\n\nLet us load in the dataset via the trusty Pandas package into a dataframe object which we call **attrition** and have a quick look at the first few rows","metadata":{"_uuid":"500550fd8e19133081d68d2b91619b0aa2df27a8","_cell_guid":"5af03c82-cb84-d943-f82c-fc0a15d46b48"}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"attrition = pd.read_csv('../input/WA_Fn-UseC_-HR-Employee-Attrition.csv')\nattrition.head()","metadata":{"_uuid":"5f3ab5389bf7ba167a5e7250d007a0e9f8c22683","_cell_guid":"e035b071-50f8-43ca-9611-fc47272bb05e"}},{"cell_type":"markdown","source":"\n\nデータセットから明らかなように、私たちのモデルを訓練することを指し示すことができる私たちの目標列は、「Attrition」列になります。\n\nさらに、数値データ型とカテゴリ型データ型が混在していることがわかります。 カテゴリの列については、後の章で数字のエンコーディングを扱います。 このセクションでは、データの探索に専念し、最初のステップとして、単純なデータ完全性チェックを迅速に実行して、データにNULLまたは無限の値があるかどうかを確認します\n\n**データ品質チェック**\n\nnull値を探すために、次のように** isnull **呼び出しを呼び出すことができます\n\nAs evinced from the dataset, our target column with which we can point our model to train on would be the \"Attrition\" column. \n\nFurthermore, we see that we have a mix of numerical and categorical data types. For the categorical columns, we shall handle their numerical encoding in the latter chapter. This section will be devoted to data exploration and as a first step, let us quickly carry our some simple data completeness checks to see if there are nulls or infinite values in the data\n\n**Data quality checks**\n\nTo look for any null values, we can just invoke the **isnull** call as follows","metadata":{"_uuid":"c1cf95d67b9797532aabf4388ca97027f0da6642","_cell_guid":"8beb6c02-af60-5327-dc85-9ba988def85b"}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"# Looking for NaN\nattrition.isnull().any()","metadata":{"_uuid":"2303742612421e59ffce57111a6a8796fb6e0f71","_cell_guid":"57e2bf45-5920-af03-50c1-b5bba334eb11"}},{"cell_type":"markdown","source":"**###データセットの配布**\n\n一般に、データを探索する最初のいくつかのステップの1つは、フィーチャが互いにどのように分散されているかを大まかに知ることです。 そうするために、私は、Seabornプロッティングライブラリーからおなじみの** kdeplot **関数を呼び出すでしょう。これは、次のように二変量プロットを生成します：\n\n### Distribution of the dataset\n\nGenerally one of the first few steps in exploring the data would be to have a rough idea of how the features are distributed with one another. To do so, I shall invoke the familiar **kdeplot** function from the Seaborn plotting library and this generates bivariate plots as follows:","metadata":{"_uuid":"9abedc29a4f8d8c68fe5ca7d174914544ddaa134","_cell_guid":"d33af20c-a8a1-2341-d0c1-fb91afa51274"}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"# Plotting the KDEplots プロット\nf, axes = plt.subplots(3, 3, figsize=(10, 10), sharex=False, sharey=False)\n\n# Defining our colormap scheme カラーマップスケーマ\ns = np.linspace(0, 3, 10)\ncmap = sns.cubehelix_palette(start=0.0, light=1, as_cmap=True)\n\n# Generate and plot プロット生成\nx = attrition['Age'].values\ny = attrition['TotalWorkingYears'].values\nsns.kdeplot(x, y, cmap=cmap, shade=True, cut=5, ax=axes[0,0])\naxes[0,0].set( title = 'Age against Total working years')\n\ncmap = sns.cubehelix_palette(start=0.333333333333, light=1, as_cmap=True)\n# Generate and plot プロット生成\nx = attrition['Age'].values\ny = attrition['DailyRate'].values\nsns.kdeplot(x, y, cmap=cmap, shade=True, ax=axes[0,1])\naxes[0,1].set( title = 'Age against Daily Rate')\n\ncmap = sns.cubehelix_palette(start=0.666666666667, light=1, as_cmap=True)\n# Generate and plot プロット生成\nx = attrition['YearsInCurrentRole'].values\ny = attrition['Age'].values\nsns.kdeplot(x, y, cmap=cmap, shade=True, ax=axes[0,2])\naxes[0,2].set( title = 'Years in role against Age')\n\ncmap = sns.cubehelix_palette(start=1.0, light=1, as_cmap=True)\n# Generate and plot プロット生成\nx = attrition['DailyRate'].values\ny = attrition['DistanceFromHome'].values\nsns.kdeplot(x, y, cmap=cmap, shade=True,  ax=axes[1,0])\naxes[1,0].set( title = 'Daily Rate against DistancefromHome')\n\ncmap = sns.cubehelix_palette(start=1.333333333333, light=1, as_cmap=True)\n# Generate and plot プロット生成\nx = attrition['DailyRate'].values\ny = attrition['JobSatisfaction'].values\nsns.kdeplot(x, y, cmap=cmap, shade=True,  ax=axes[1,1])\naxes[1,1].set( title = 'Daily Rate against Job satisfaction')\n\ncmap = sns.cubehelix_palette(start=1.666666666667, light=1, as_cmap=True)\n# Generate and plot プロット生成\nx = attrition['YearsAtCompany'].values\ny = attrition['JobSatisfaction'].values\nsns.kdeplot(x, y, cmap=cmap, shade=True,  ax=axes[1,2])\naxes[1,2].set( title = 'Daily Rate against distance')\n\ncmap = sns.cubehelix_palette(start=2.0, light=1, as_cmap=True)\n# Generate and plot\nx = attrition['YearsAtCompany'].values\ny = attrition['DailyRate'].values\nsns.kdeplot(x, y, cmap=cmap, shade=True,  ax=axes[2,0])\naxes[2,0].set( title = 'Years at company against Daily Rate')\n\ncmap = sns.cubehelix_palette(start=2.333333333333, light=1, as_cmap=True)\n# Generate and plot\nx = attrition['RelationshipSatisfaction'].values\ny = attrition['YearsWithCurrManager'].values\nsns.kdeplot(x, y, cmap=cmap, shade=True,  ax=axes[2,1])\naxes[2,1].set( title = 'Relationship Satisfaction vs years with manager')\n\ncmap = sns.cubehelix_palette(start=2.666666666667, light=1, as_cmap=True)\n# Generate and plot\nx = attrition['WorkLifeBalance'].values\ny = attrition['JobSatisfaction'].values\nsns.kdeplot(x, y, cmap=cmap, shade=True,  ax=axes[2,2])\naxes[2,2].set( title = 'WorklifeBalance against Satisfaction')\n\nf.tight_layout()","metadata":{"_uuid":"02ccc970753f3f32e5bcfbec7fda58aacdc0e7b2","_cell_guid":"4bbd2965-fb1f-6a9d-6661-cb02988c2299"}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"# Define a dictionary for the target mapping ターゲットマップでの辞書定義\ntarget_map = {'Yes':1, 'No':0}\n# Use the pandas apply method to numerically encode our attrition target variable pandasを適用して離職率をエンコード\nattrition[\"Attrition_numerical\"] = attrition[\"Attrition\"].apply(lambda x: target_map[x])","metadata":{"collapsed":true,"_uuid":"3cb3c933d19ee9124dfe92167d3afac9478f579e","_cell_guid":"5f44820b-847c-5465-3251-e6e69ac4c3fe"}},{"cell_type":"markdown","source":"###フィーチャの相関\n\nデータエクスプローラの次のツールは、相関行列のツールです。 相関行列をプロットすることで、フィーチャが互いにどのように関連しているかを非常に素早く概観できます。 Pandasのデータフレームでは、**。corr **という呼出しを便利に使うことができます。デフォルトでは、そのデータフレーム内の列のPearson Correlation値がペアで提供されます。\n\nこの相関プロットでは、Plotlyライブラリを使用して、次のようにヒートマップ関数を介して対話型ピアソン相関行列を生成します。\n\n\n### Correlation of Features\n\nThe next tool in a data explorer's arsenal is that of a correlation matrix. By plotting a correlation matrix, we have a very nice overview of how the features are related to one another. For a Pandas dataframe, we can conveniently use the call **.corr** which by default provides the Pearson Correlation values of the columns pairwise in that dataframe.\n\nIn this correlation plot, I will use the the Plotly library to produce a interactive Pearson correlation matrix via the Heatmap function as follows:","metadata":{"_uuid":"3156ef9242b43567520b5de8b7e51f50d8580794","_cell_guid":"5c5dc2ed-7608-4d84-c4f6-c591a3be7570"}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"# creating a list of only numerical values\nnumerical = [u'Age', u'DailyRate', u'DistanceFromHome', u'Education', u'EmployeeNumber', u'EnvironmentSatisfaction',\n       u'HourlyRate', u'JobInvolvement', u'JobLevel', u'JobSatisfaction',\n       u'MonthlyIncome', u'MonthlyRate', u'NumCompaniesWorked',\n       u'PercentSalaryHike', u'PerformanceRating', u'RelationshipSatisfaction',\n       u'StockOptionLevel', u'TotalWorkingYears',\n       u'TrainingTimesLastYear', u'WorkLifeBalance', u'YearsAtCompany',\n       u'YearsInCurrentRole', u'YearsSinceLastPromotion',\n       u'YearsWithCurrManager']\ndata = [\n    go.Heatmap(\n        z= attrition[numerical].astype(float).corr().values, # Generating the Pearson correlation\n        x=attrition[numerical].columns.values,\n        y=attrition[numerical].columns.values,\n        colorscale='Viridis',\n        reversescale = False,\n        text = True ,\n        opacity = 1.0\n        \n    )\n]\n\n\nlayout = go.Layout(\n    title='Pearson Correlation of numerical features',\n    xaxis = dict(ticks='', nticks=36),\n    yaxis = dict(ticks='' ),\n    width = 900, height = 700,\n    \n)\n\n\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename='labelled-heatmap')","metadata":{"_uuid":"c2a700061d8fbee81adfbd28010c87df9847c9a3","_cell_guid":"a2266ca6-3589-f065-4ca3-f3a8fe558000"}},{"cell_type":"markdown","source":"\n\n\n**プロットからのテイクアウト**\n\n相関プロットから、非常に多くの列が相互にあまり相関していないように見えることがわかります。 一般的に、予測モデルを作成する場合、冗長なフィーチャを処理する必要がないように、相互にあまり関連していないフィーチャを使用してモデルを訓練することが望ましいでしょう。 非常に多くの相関した特徴がある場合には、主成分分析（PCA）のような手法を適用して特徴空間を縮小することができる。\n\n**Takeaway from the plots**\n\nFrom the correlation plots, we can see that quite a lot of our columns seem to be poorly correlated with one another. Generally when making a predictive model, it would be preferable to train a model with features that are not too correlated with one another so that we do not need to deal with redundant features. In the case that we have quite a lot of correlated features one could perhaps apply a technique such as Principal Component Analysis (PCA) to reduce the feature space.","metadata":{"_uuid":"bb0b6fac681f7c1f7fc8b3598817086e89c61fb2","_cell_guid":"88e730c3-f7a9-50fa-05a9-2ac9f507f882"}},{"cell_type":"markdown","source":"### Pairplot Visualisations\n\n今度は、いくつかのSeabornペアプロットを作成し、Attrition列である目標変数に対して設定して、さまざまな機能が従業員の喪失に対してどのように分散されているかを感じてみましょう\n\n### Pairplot Visualisations\n\nNow let us create some Seaborn pairplots and set it against the target variable which is our Attrition column to get a feel for how the various features are distributed vis-a-vis employee attrition","metadata":{"_uuid":"ea923651ea98bd9c32cad054c0b49480522fec69","_cell_guid":"a1437185-68f3-5927-1ad9-041e9c5551d9"}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"# Refining our list of numerical variables\nnumerical = [u'Age', u'DailyRate',  u'JobSatisfaction',\n       u'MonthlyIncome', u'PerformanceRating',\n        u'WorkLifeBalance', u'YearsAtCompany', u'Attrition_numerical']\n\n#g = sns.pairplot(attrition[numerical], hue='Attrition_numerical', palette='seismic', diag_kind = 'kde',diag_kws=dict(shade=True))\n#g.set(xticklabels=[])","metadata":{"collapsed":true,"_uuid":"82d13f3e6bdd98dc6f116503fa1ceefc50988560","_cell_guid":"aaecd545-abb3-0513-5ae5-fc178f7e5de3"}},{"cell_type":"markdown","source":"\n\n＃2。特徴エンジニアリングとカテゴリエンコーディング\n\nデータセットを簡単に探索した後、フィーチャエンジニアリングのタスクに進み、データセットのカテゴリ値を数値的にエンコードします。 フィーチャー・エンジニアリングとは、現在の機能から新しい機能や関係を作成することです。 フィーチャエンジニアリングはかなり\n\n最初に、次のようにdtypeメソッドを使用して、カテゴリの列から数値列を分離します。\n\n\n\n\n# 2. Feature Engineering & Categorical Encoding\n\nHaving carried out a brief exploration into the dataset, let us now proceed onto the task of Feature engineering and numerically encoding the categorical values in our dataset. Feature engineering in a nutshell involves creating new features and relationships from the current features that we have. Feature engineering has been quite \n\nTo start off, we shall segregate numerical columns from categorical columns via the use of the dtype method as follows:","metadata":{"_uuid":"31f3cfc523c1051c3e908b8e0e2a8e0de87418cf","_cell_guid":"112cef65-78b8-7790-e705-b173beea6986"}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"# Drop the Attrition_numerical column from attrition dataset first - Don't want to include that\nattrition = attrition.drop(['Attrition_numerical'], axis=1)\n\n# Empty list to store columns with categorical data\ncategorical = []\nfor col, value in attrition.iteritems():\n    if value.dtype == 'object':\n        categorical.append(col)\n\n# Store the numerical columns in a list numerical\nnumerical = attrition.columns.difference(categorical)","metadata":{"collapsed":true,"_uuid":"421505a599cb7230373208291a1b0ce05cb1ee7b","_cell_guid":"937385c7-7b7f-f6d0-d974-0527a7118e98"}},{"cell_type":"markdown","source":"\nどの機能にカテゴリデータが含まれているかを確認した後、データを数値でエンコードすることができます。 これを行うには、カテゴリ変数から符号化されたダミー変数を作成するPandasの** get_dummies **メソッドを使用します。\n\n\n\n\nHaving identified which of our features contain categorical data, we can set about numerically encoding the data. To do this, I shall use the **get_dummies** method from Pandas which creates encoded dummy variables from the categorical variables.\n\n\n","metadata":{"_uuid":"7999f6b9428c358d63aa03a74cc109d137cd7e70","_cell_guid":"29b16ff2-e3bb-bb81-348c-995553409ad0"}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"# Store the categorical data in a dataframe called attrition_cat\nattrition_cat = attrition[categorical]\nattrition_cat = attrition_cat.drop(['Attrition'], axis=1) # Dropping the target column","metadata":{"collapsed":true,"_uuid":"68bade525a132e01aaaf90b728810082087db46a","_cell_guid":"5ec5cd49-f8b3-e36b-75dd-ac95fe0373ac"}},{"cell_type":"markdown","source":"Applying the **get_dummies** method, we see that we have encoded our categorical values conveniently by just applying one line of Python code. ","metadata":{"_uuid":"550a5467c4edb1d946e53d50607c050c8e693da1","_cell_guid":"7c3c0c95-3725-80dd-0a73-5c840451a438"}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"attrition_cat = pd.get_dummies(attrition_cat)\nattrition_cat.head(3)","metadata":{"_uuid":"89acf0739ab1df0d5655b3f0854c3d9bc500667f","_cell_guid":"7ea5b0d8-1f13-e56b-72cf-bcbe7dd6fad2"}},{"cell_type":"markdown","source":"**Creating new features from Numerical data**","metadata":{"_uuid":"df9ba32a34e869a607cdcfded41d9f5b84331f2b","_cell_guid":"ea6f0750-644f-c81f-e03a-4699387bc1b1"}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"# Store the numerical features to a dataframe attrition_num\nattrition_num = attrition[numerical]","metadata":{"collapsed":true,"_uuid":"8b757df493f77381024347fec3196b98f03e964e","_cell_guid":"de8b3a57-6aba-eae7-2be3-dbe0ae761d6a"}},{"cell_type":"markdown","source":"\nカテゴリの列とエンジニアリングをエンコードし、数値データからいくつかの新機能を作成した後、両方のデータフレームを最終セットに統合して、モデルのトレーニングとテストを行います。\n\nHaving encoded our categorical columns as well as engineering and created some new features from the numerical data, we can now proceed to merging both dataframes into a final set with which we will train and test our models on. ","metadata":{"_uuid":"d44d38f32bd7a4089ec9f839be8502c7c8dfabdb","_cell_guid":"9de23a93-10b6-33b8-eea8-0cf44c6e5e08"}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"# Concat the two dataframes together columnwise\nattrition_final = pd.concat([attrition_num, attrition_cat], axis=1)","metadata":{"collapsed":true,"_uuid":"1873921768ece3c112a5a613f3932d2f51f18b7a","_cell_guid":"b90b69ba-f19d-0707-7c2c-183b8d01130f"}},{"cell_type":"markdown","source":"目標変数\n\n覚えておく必要がある最後のステップの1つは、目標変数を生成することです。 この場合のターゲットは、カテゴリ変数を含む喪失列によって与えられるため、数値エンコーディングが必要です。 数値的には、1：Yesと0：Noのマッピングを持つ辞書を作成します。","metadata":{"_uuid":"75237ab3f8e7c9e6d1b97f0795a1fd0dc42fd67f","_cell_guid":"1a295568-fab4-b79a-bc0d-be32ad032b3e"}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"# Define a dictionary for the target mapping\ntarget_map = {'Yes':1, 'No':0}\n# Use the pandas apply method to numerically encode our attrition target variable\ntarget = attrition[\"Attrition\"].apply(lambda x: target_map[x])\ntarget.head(3)","metadata":{"_uuid":"69e6181ef1028a88e6b05ef1b531eda61b7b9411","_cell_guid":"bfa5e82f-2dd3-1bee-5b2b-367468be7040"}},{"cell_type":"markdown","source":"However just by a quick inspection of the counts of the number of 'Yes' and 'No' in the target variable tells us that there is quite a large skew in target as shown","metadata":{"_uuid":"89b8fe9593a5c24905e035eb8b31e50a495a62e2","_cell_guid":"1f56b814-92a6-3042-461e-44f9e6352e86"}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"data = [go.Bar(\n            x=attrition[\"Attrition\"].value_counts().index.values,\n            y= attrition[\"Attrition\"].value_counts().values\n    )]\n\npy.iplot(data, filename='basic-bar')","metadata":{"collapsed":true,"_uuid":"1bad2d8eee33f19d6e040d326199d0ddc7ac41bb","_cell_guid":"f47479ed-e975-413d-37f6-e6ad58342b6d"}},{"cell_type":"markdown","source":"Therefore we have to keep in mind that there is quite a big imbalance in our target variable. Many statistical techniques have been put forth to treat imbalances in data (oversampling or undersampling). In this notebook, I will use an oversampling technique known as SMOTE to treat this imbalance.","metadata":{"_uuid":"8ba627a950fbc2e095a7f13a44c4fd65822f1db3","_cell_guid":"891264f0-63f0-fb53-c8c2-f9ceb6b7f00d"}},{"cell_type":"markdown","source":"# 3. Implementing Machine Learning Models\n\nHaving performed some exploratory data analysis and simple feature engineering as well as having ensured that all categorical values are encoded, we are now ready to proceed onto building our models.\n\nAs alluded to in the introduction of this notebook, we will aim to evaluate and contrast the performances of a handful of different learning models. \n\n**Splitting Data into Train and Test sets**\n\nBut before we even start training a model, we will have to partition our dataset into a training set and a test set (unlike Kaggle competitions where the train and test data are already segregated for you). To split our data we will utilise sklearn's ","metadata":{"_uuid":"95d5824b85075d1ae5a8dc94dc4203fd51c24d58","_cell_guid":"5564e6e1-83ed-75de-2540-0d037e31291b"}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"# Import the train_test_split method\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.cross_validation import StratifiedShuffleSplit\n\n# Split data into train and test sets as well as for validation and testing\ntrain, test, target_train, target_val = train_test_split(attrition_final, target, train_size= 0.75,random_state=0);\n#train, test, target_train, target_val = StratifiedShuffleSplit(attrition_final, target, random_state=0);","metadata":{"collapsed":true,"_uuid":"27bf0f3ee0670b33c177ef4edc0201e5dba47032","_cell_guid":"c197f8ee-76b0-7137-f001-83f969637521"}},{"cell_type":"markdown","source":"**SMOTE to oversample due to the skewness in target**\n\nSince we have already noted the severe imbalance in the values within the target variable, let us implement the SMOTE method in the dealing with this skewed value via the imblearn Python package.","metadata":{"_uuid":"4d47a1d6ce9ab80449993c462629818c9a9729ab","_cell_guid":"868ff39d-a894-1f8a-2375-12b2b49fb0f2"}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"oversampler=SMOTE(random_state=0)\nsmote_train, smote_target = oversampler.fit_sample(train,target_train)","metadata":{"collapsed":true,"_uuid":"37c32088993412e11ddc37b527b30f3b040a8634","_cell_guid":"a865e8bc-21c0-b8a6-85f2-60804b027103"}},{"cell_type":"markdown","source":"## A. Random Forest Classifier \n\nThe Random Forest method, first introduced by Breiman in 2001 can be grouped under the category of ensemble models. Why ensemble? The building block of a Random Forest is the ubiquitous Decision Tree. The decision tree as a standalone model is often considered a \"weak learner\" as its predictive performance is relatively poor. However a Random Forest gathers a group (or ensemble) of decision trees and uses their combined predictive capabilities to obtain relatively strong predictive performance - \"strong learner\". \n\nThis principle of using a collection of \"weak learners\" to come together to create a \"strong learner\" underpins the basis of ensemble methods which one regularly comes across in Machine learning. For a really good read that drives home the basics of the Random Forest, refer to this [CitizenNet blog][1]\n\n\n  [1]: http://blog.citizennet.com/blog/2012/11/10/random-forests-ensembles-and-performance-metrics","metadata":{"_uuid":"e0af3a4bbed4c4e859cf36acfeff4b5831e1312b","_cell_guid":"fc5471cf-b4a3-6a3b-c558-fd6dda2e239a"}},{"cell_type":"markdown","source":"**Initialising Random Forest parameters**\n\nWe will utilise the Scikit-learn library to construct a Random Forest model. To do so, we have to first define our set of parameters that we will feed into our Random Forest classifier as follows","metadata":{"_uuid":"ae6ba2e41ca329c1e6a0aee813159fff359226e7","_cell_guid":"fdb4257b-8307-e96e-bf5b-09a724e609b0"}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"seed = 0   # We set our random seed to zero for reproducibility\n# Random Forest parameters\nrf_params = {\n    'n_jobs': -1,\n    'n_estimators': 800,\n    'warm_start': True, \n    'max_features': 0.3,\n    'max_depth': 9,\n    'min_samples_leaf': 2,\n    'max_features' : 'sqrt',\n    'random_state' : seed,\n    'verbose': 0\n}","metadata":{"collapsed":true,"_uuid":"fe506c091170fbba24108dcf5c7d520aadbd4584","_cell_guid":"89521bf0-95f5-de1d-a4f9-a981c564aa55"}},{"cell_type":"markdown","source":"Having defined our parameters, we can initialise a Random Forest object by using scikit-learn's **RandomForestClassifier** and unpacking the parameters by adding the double asterisks symbols as follows","metadata":{"_uuid":"153f214af78cd202280a6b4e701477557214d14c","_cell_guid":"c4af80e2-032d-db67-b1ec-6691eb57563a"}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"rf = RandomForestClassifier(**rf_params)","metadata":{"collapsed":true,"_uuid":"f087dbc645f25e559c66ef7e7f07e467707131e9","_cell_guid":"04516a2f-0cd3-b521-160a-614c6421cab0"}},{"cell_type":"markdown","source":"The next step after prepping our Random Forest model would be to start building a forest of trees using our training set and fitting it to our attrition target variable. We do so by simply using the **fit** call as follows","metadata":{"_uuid":"164ac84124e53861a402c6857cb9cdc0e521451f","_cell_guid":"7445e9fc-9434-3e99-3f18-0e72dd1963be"}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"rf.fit(smote_train, smote_target)\nprint(\"Fitting of Random Forest as finished\")","metadata":{"collapsed":true,"_uuid":"206c828d1bb23f21b707157556cb14e89c0204f1","_cell_guid":"5c1a0e3e-ccbb-8b33-2c79-23800a893523"}},{"cell_type":"markdown","source":"Having fitted our forest of trees with our parameters to the training set against our target variable, we now have a learning model **rf** which we can make predictions out of. To use our Random Forest in predicting against our test data, we can use sklearn's **.predict** method as follows","metadata":{"_uuid":"b7b52a9998f1331a91b0e0d9edeade75674250cf","_cell_guid":"c20ee19d-075b-0b58-6eb3-25c9bd93ceeb"}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"rf_predictions = rf.predict(test)\nprint(\"Predictions finished\")","metadata":{"collapsed":true,"_uuid":"9d00b151a93ada206339cb720845e26dbee807d7","_cell_guid":"b2f76111-da81-dc47-5075-76de75ee7636"}},{"cell_type":"markdown","source":"And a nice touch with sklearn (and the use of train_test_split method) is that you can conveniently set aside a part of the ","metadata":{"_uuid":"7b8749aaa9dd3b2e75fe66915a4f2c9f6c20d111","_cell_guid":"14046a92-63a3-b7ef-f26f-c782816c1c4c"}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"accuracy_score(target_val, rf_predictions)","metadata":{"collapsed":true,"_uuid":"b925c51b9e57a093af8c05ede89c37768850a8c5","_cell_guid":"5f212f92-952b-e4a8-ac8f-c3349ca91dde"}},{"cell_type":"markdown","source":"**Accuracy of the model**\n\nAs observed, our Random Forest returns an accuracy of 88% for its predictions. On first glance this might seem to be a very good performing model. However when we think about how skewed our target variable where the distribution of yes and no's are 84% and 26%, therefore our model is only predicting slightly better than random guessing.","metadata":{"_uuid":"f75d6eb89f766ea43d14e293e9bbe7d034f677fe","_cell_guid":"5c6074c8-45cb-bfdd-1394-09bbd259b4b2"}},{"cell_type":"markdown","source":"### Feature Ranking via the Random Forest \n\nThe Random Forest classifier in Sklearn also contains a very convenient and most useful attribute **feature_importances_** which tells us which features within our dataset has been given most importance through the Random Forest algorithm. Shown below is an Interactive Plotly diagram of the various feature importances.","metadata":{"_uuid":"ed8f72bf6f3c6378e589240bfa8b181fcfece4df","_cell_guid":"56c04982-9ec1-5602-4940-98ff888cbed6"}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"# Scatter plot \ntrace = go.Scatter(\n    y = rf.feature_importances_,\n    x = attrition_final.columns.values,\n    mode='markers',\n    marker=dict(\n        sizemode = 'diameter',\n        sizeref = 1,\n        size = 13,\n        #size= rf.feature_importances_,\n        #color = np.random.randn(500), #set color equal to a variable\n        color = rf.feature_importances_,\n        colorscale='Portland',\n        showscale=True\n    ),\n    text = attrition_final.columns.values\n)\ndata = [trace]\n\nlayout= go.Layout(\n    autosize= True,\n    title= 'Random Forest Feature Importance',\n    hovermode= 'closest',\n     xaxis= dict(\n         ticklen= 5,\n         showgrid=False,\n        zeroline=False,\n        showline=False\n     ),\n    yaxis=dict(\n        title= 'Feature Importance',\n        showgrid=False,\n        zeroline=False,\n        ticklen= 5,\n        gridwidth= 2\n    ),\n    showlegend= False\n)\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig,filename='scatter2010')","metadata":{"collapsed":true,"_uuid":"f01b66f22c20044ed24be81f4e329fc803167c3b","_cell_guid":"1a44123f-4630-4125-cc60-745b60ac0073"}},{"cell_type":"markdown","source":"#### **Most RF important features** : Overtime, Marital Status\n\nAs observed in the plot of feature importances,  it seems that our Random Forest Classifier has decided to rank the features of OverTime highest, which is followed by marital status. \n\nI don't know about you, but working overtime to me does indeed affect my satisfaction derived from any job (and I have worked many an overtime). Maybe then it should come as no surprise that our classifier has caught on to this and thus ranked overtime the highest. ","metadata":{"_uuid":"4b7c9293da975ea5fc436de2daa76821e3027bd4","_cell_guid":"81becb9d-c912-13e8-c36d-404d0f08ffb0"}},{"cell_type":"markdown","source":"### Visualising Tree Diagram with Graphviz\n\nLet us now visualise how a single decision tree traverses the features in our data as the DecisionTreeClassifier object of sklearn comes with a very convenient **export_graphviz** method that exports the tree diagram into a .png format which you can view from the output of this kernel.","metadata":{"_uuid":"91aa448a562ba4a08346d15919e7e04d78bd7a6d","_cell_guid":"1a05c475-cde3-9bd9-d24f-3143f906bbc1"}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"from sklearn import tree\nfrom IPython.display import Image as PImage\nfrom subprocess import check_call\nfrom PIL import Image, ImageDraw, ImageFont\nimport re\n\ndecision_tree = tree.DecisionTreeClassifier(max_depth = 4)\ndecision_tree.fit(train, target_train)\n\n# Predicting results for test dataset\ny_pred = decision_tree.predict(test)\n\n# Export our trained model as a .dot file\nwith open(\"tree1.dot\", 'w') as f:\n     f = tree.export_graphviz(decision_tree,\n                              out_file=f,\n                              max_depth = 4,\n                              impurity = False,\n                              feature_names = attrition_final.columns.values,\n                              class_names = ['No', 'Yes'],\n                              rounded = True,\n                              filled= True )\n        \n#Convert .dot to .png to allow display in web notebook\ncheck_call(['dot','-Tpng','tree1.dot','-o','tree1.png'])\n\n# Annotating chart with PIL\nimg = Image.open(\"tree1.png\")\ndraw = ImageDraw.Draw(img)\nimg.save('sample-out.png')\nPImage(\"sample-out.png\")","metadata":{"collapsed":true,"_uuid":"76bff225f9336cbc885dc3e6696daa70a8fc7fab","_cell_guid":"ebaae88e-3aea-ec26-4ae0-65227a857eaf"}},{"cell_type":"markdown","source":"## B. Gradient Boosted Classifier\n\nGradient Boosting is also an ensemble technique much like the Random Forest where a combination of weak Tree learners are brought together to form a relatively stronger learner. The technique involves defining some sort of function (loss function) that you want minimised and an method/algorithm to minimise this. Therefore as the name suggests, the algorithm used to minimise the loss function is that of a gradient descent method which adds decision trees which \"point\" in the direction that reduces our loss function (downward gradient).\n\nTo set up a Gradient Boosting classifier is easy enough in Sklearn and it involves only a handful of lines of code. Again we first set up our classifier's parameters\n\n**Initialising Gradient Boosting Parameters** \n\nIn general there are a handful of key parameter when setting up tree-based or gradient boosted models. These are always going to be the number of estimators, the maximum depth with which you want your model to be trained to, and the minimum samples per leaf","metadata":{"_uuid":"4272eaf135516f99ab7dabf41b6c67e68ccd2794","_cell_guid":"610cfa87-0b9d-4671-cd51-c99ef9c9151d"}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"# Gradient Boosting Parameters\ngb_params ={\n    'n_estimators': 500,\n    'max_features': 0.9,\n    'learning_rate' : 0.2,\n    'max_depth': 11,\n    'min_samples_leaf': 2,\n    'subsample': 1,\n    'max_features' : 'sqrt',\n    'random_state' : seed,\n    'verbose': 0\n}","metadata":{"collapsed":true,"_uuid":"47b1c9c08ce0cfb6ce8e0380545e5e31d3e4da3f","_cell_guid":"d2f3de1c-8b21-7f2a-244b-429e4ae22786"}},{"cell_type":"markdown","source":"Having defined our parameters, we can now apply the usual fit and predict methods on our train and test sets respectively ","metadata":{"_uuid":"c53df3d50184f82e1d603b4be422d4b07e4f1148","_cell_guid":"30d46aa6-98b4-178f-e337-4ecf20291777"}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"gb = GradientBoostingClassifier(**gb_params)\n# Fit the model to our SMOTEd train and target\ngb.fit(smote_train, smote_target)\n# Get our predictions\ngb_predictions = gb.predict(test)\nprint(\"Predictions have finished\")","metadata":{"collapsed":true,"_uuid":"1413d517e813628aae7acff23167b83ed1e5cebd","_cell_guid":"ed6a837e-2864-291c-be8d-3c8e9ed900b7"}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"accuracy_score(target_val, gb_predictions)","metadata":{"collapsed":true,"_uuid":"175fb956b2c4defd08e08a3def4033df8adc31c3","_cell_guid":"40c37011-76df-fcc7-9cd0-e689374a8d1a"}},{"cell_type":"markdown","source":"### Feature Ranking via the Gradient Boosting Model\n\nMuch like the Random Forest, we can invoke the feature_importances_ attribute of the gradient boosting model and dump it in an interactive Plotly chart","metadata":{"_uuid":"3e52363b3284f8bd22aa496ec55ebaa0efdc9556","_cell_guid":"21cc0476-b03e-731f-97b4-89d81977c3a7"}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"# Scatter plot \ntrace = go.Scatter(\n    y = gb.feature_importances_,\n    x = attrition_final.columns.values,\n    mode='markers',\n    marker=dict(\n        sizemode = 'diameter',\n        sizeref = 1,\n        size = 13,\n        #size= rf.feature_importances_,\n        #color = np.random.randn(500), #set color equal to a variable\n        color = gb.feature_importances_,\n        colorscale='Portland',\n        showscale=True\n    ),\n    text = attrition_final.columns.values\n)\ndata = [trace]\n\nlayout= go.Layout(\n    autosize= True,\n    title= 'Gradient Boosting Model Feature Importance',\n    hovermode= 'closest',\n     xaxis= dict(\n         ticklen= 5,\n         showgrid=False,\n        zeroline=False,\n        showline=False\n     ),\n    yaxis=dict(\n        title= 'Feature Importance',\n        showgrid=False,\n        zeroline=False,\n        ticklen= 5,\n        gridwidth= 2\n    ),\n    showlegend= False\n)\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig,filename='scatter')","metadata":{"collapsed":true,"_uuid":"66f370599c7c1efbd64423cb20b6b66de2417fe0","_cell_guid":"082ca641-ffd2-fc3b-a7b6-9418b08767d9"}},{"cell_type":"markdown","source":"**Takeaway from the Plot**\n\n**GBM most important features**  : Monthly Income, Overtime, Daily and Monthly Rate\n\nInterestingly, the features that the Gradient Boosting model has placed its importance on differs quite greatly from that of the Random Forest. It has decided to rank features with a lot of economic significance highest such as the monthly income and the daily and monthly rate on top of placing importance on overtime. However what is puzzling is the inclusion of the Employee number as an important feature. \n\nThis may be an edge case in the training or perhaps there is","metadata":{"_uuid":"cc20f5e10a576c56e2780aa35a3b3fa89542d29d","_cell_guid":"7b4ad49b-6f98-0d16-7a55-933a002cf878"}},{"cell_type":"markdown","source":"### CONCLUSION\n\nWe have constructed a very simple pipeline of predicting employee attrition, from some basic Exploratory Data Analysis to feature engineering as well as implementing two learning models in the form of a Random Forest and a Gradient Boosting classifier. This whole notebook takes less than a minute to run and it even returns a 89% accuracy in its predictions.\n\nThat being said, there is quite a lot of room for improvement. For one, more features could be engineered from the data.  Furthermore one could squeeze performance out of this pipeline by perhaps using some form of blending or stacking of models. I myself am quite keen to implement a classifier voting where a handful of classifiers votes on the outcome of the predictions and we take the majority vote. ","metadata":{"_uuid":"2689057a66cb06d2c397e3e605795a10ed255f26","_cell_guid":"17c50e00-fc85-5a96-20e2-de168e6fb060"}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"","metadata":{"collapsed":true,"_uuid":"4983633eb6b70205fc9a19bfd6f85f0b5c69bc24","_cell_guid":"92715557-b0c3-da9c-b950-ed9a8ee72fef"}}],"metadata":{"language_info":{"pygments_lexer":"ipython3","mimetype":"text/x-python","file_extension":".py","version":"3.6.1","codemirror_mode":{"version":3,"name":"ipython"},"name":"python","nbconvert_exporter":"python"},"_change_revision":0,"kernelspec":{"language":"python","name":"python3","display_name":"Python 3"},"_is_fork":false}}