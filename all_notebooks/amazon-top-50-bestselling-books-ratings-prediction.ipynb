{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Amazon Top 50 Bestselling Books 2009 - 2019 | Rating prediction using several methods\n\nFirst we will take a general look at the data, then we will generate more useful variables based on the original ones and then we will explore several methods to see which one predicts user ratings better."},{"metadata":{},"cell_type":"markdown","source":"## 0. Setting up the analysis. Importing the libraries and the datafile."},{"metadata":{"trusted":true},"cell_type":"code","source":"# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\n\nimport matplotlib.pyplot as plt\n\nimport seaborn as sns\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import mean_squared_error\n\nfrom sklearn.linear_model import SGDRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.neural_network import MLPRegressor\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\ndatapath=\"/kaggle/input/amazon-top-50-bestselling-books-2009-2019/bestsellers with categories.csv\"\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1. First look at the data"},{"metadata":{},"cell_type":"markdown","source":"### 1.1 Visualizing the original data"},{"metadata":{"trusted":true},"cell_type":"code","source":"rawdata=pd.read_csv(datapath)\nrawdata","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The data shows the user rating, the total number of reviews and the price at 2019 and the genre (fic/non fic) each book, which occupy a row for each year as a bestseller. Now let's see the pairplot of the numeric variables."},{"metadata":{},"cell_type":"markdown","source":"### 1.2 Looking for direct correlations."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(rawdata)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"At first glance we don't see a direct correlation between the user ratings and the other numeric variables with the pairplot. Let's try to visualize the user rating distribution for fiction/not fiction books and the distribution of the bestsellers of each year."},{"metadata":{"trusted":true},"cell_type":"code","source":"genredata=rawdata.groupby(\"Name\").mean()\ngenredata[\"Genre\"]=genredata.index.map(lambda name: rawdata[rawdata[\"Name\"]==name][\"Genre\"].values[0])\n\nbins_=np.arange(3.15,5.05,0.1)\n\nsns.displot(genredata, x=\"User Rating\",hue=\"Genre\", element=\"step\", bins=bins_)\nsns.displot(rawdata, x=\"User Rating\",hue=\"Year\", element=\"poly\", bins=bins_,fill=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The plots show that fiction books are slightly better rated than the not fictional ones and the books that have been bestsellers in the last few years have better ratings than the ones they have not. In both cases the correlation is not very strong."},{"metadata":{},"cell_type":"markdown","source":"## 2. Exploring the data"},{"metadata":{},"cell_type":"markdown","source":"### 2.1 Creating new variables from the original ones"},{"metadata":{},"cell_type":"markdown","source":"We will compute the number of years as a bestseller (YearsBs) and the number of years elapsed since the first time a book was a bestseller (YearsEl)."},{"metadata":{"trusted":true},"cell_type":"code","source":"cleandata=rawdata\nyearsBSdata=rawdata.groupby(\"Name\").count().Year\nyearsELdata=rawdata.groupby(\"Name\").min().Year.apply(lambda x: 2020-x)\ncleandata[\"YearsBs\"]=cleandata[\"Name\"].apply(lambda name: yearsBSdata[name])\ncleandata[\"YearsEl\"]=cleandata[\"Name\"].apply(lambda name: yearsELdata[name])\ncleandata=cleandata.rename(columns={\"User Rating\":\"Rating\"})\ncleandata","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.2 Looking for new correlations"},{"metadata":{"trusted":true},"cell_type":"code","source":"yearsdata=cleandata.groupby(\"Name\").mean()\nsns.scatterplot(x=yearsdata.YearsBs,y=yearsdata.Rating)\nsns.displot(yearsdata, x=\"Rating\",hue=\"YearsBs\", element=\"poly\", stat=\"probability\",common_norm=False,bins=bins_,fill=False)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"sns.scatterplot(x=yearsdata.YearsEl,y=yearsdata.Rating)\nsns.displot(yearsdata, x=\"Rating\",hue=\"YearsEl\", element=\"poly\", stat=\"probability\",common_norm=False,bins=bins_,fill=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that the more years as a bestseller a book has been, the more likely is to have a good rating. Also the newer bestsellershave slightly better ratings that the older ones."},{"metadata":{},"cell_type":"markdown","source":"## 3. Preparing the data to be machine learnt"},{"metadata":{},"cell_type":"markdown","source":"### 3.1 Data cleaning"},{"metadata":{},"cell_type":"markdown","source":"There is some data that wasn't scrapped as intended. Some books have different versions.\n\nThere are books with slightly altered names for each version:"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"def is_name(string_searched,string_evaluated):\n    match=re.search(string_searched,string_evaluated)\n    if match!=None:\n       return True\n    else:\n       return False\n    \ndirtydata1=cleandata[cleandata.Name.apply(lambda name: is_name(r\"The 5 Love Languages: The Secret to Love [Tt]hat Lasts\", name))]\ndirtydata1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"However there are some books with 2 versions with the same name variable."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"dirtydata2=cleandata[(cleandata.Rating.round(2)!=cleandata.Name.apply(lambda name: yearsdata.Rating[name].round(2))) |\n                   (cleandata.Price!=cleandata.Name.apply(lambda name: yearsdata.Price[name]))]\ndirtydata2[\"MeanRating\"]=dirtydata2.Name.apply(lambda name: yearsdata.Rating[name].round(2))\ndirtydata2[\"MeanPrice\"]=dirtydata2.Name.apply(lambda name: yearsdata.Price[name].round(2))\ndirtydata2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let's evaluate if we can remove the anomalous data or if it's worth it to keep it and try to correct it."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"anomalous_ratio=(dirtydata1.shape[0]+dirtydata2.shape[0])/cleandata.shape[0]\nprint(\"The anomalous data ratio is {:.2f} %\".format(anomalous_ratio*100.))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The anomalous data is less than 10% of total data. We can remove it."},{"metadata":{"trusted":true},"cell_type":"code","source":"cleanerdata=cleandata\ncleanerdata=cleanerdata.drop(index=dirtydata1.index)\ncleanerdata=cleanerdata.drop(index=dirtydata2.index)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3.2 Outlier cleaning"},{"metadata":{},"cell_type":"markdown","source":"Now we will clean the outliers, using the quantile criteria.\n\n\\begin{equation}\nQ_1- 1.5 \\ Q_{13}<{(X,y)} \\leq Q_3+1.5 \\ Q_{13}\n\\end{equation}\n\nwhere\n\n\\begin{equation}\nQ_{13}=Q_3-Q_1\n\\end{equation}"},{"metadata":{"trusted":true},"cell_type":"code","source":"def outlier_remover(columnseries):\n    Q1=columnseries.describe()[\"25%\"]\n    Q3=columnseries.describe()[\"75%\"]\n    Q13=Q3-Q1\n    lowerbound=Q1-1.5*Q13\n    upperbound=Q3+1.5*Q13\n    newcolumnseries=columnseries[columnseries.between(lowerbound,upperbound)]\n    return newcolumnseries\n\nfor columnname in cleanerdata.columns:\n    if columnname not in [\"Name\",\"Author\",\"Genre\"]: cleanerdata[columnname]=outlier_remover(cleanerdata[columnname])\ncleanerdata=cleanerdata.dropna()\ncleanerdata.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Before continuing we may repeat the rating distribution figures to see how much the anomalous data affected the results."},{"metadata":{"trusted":true},"cell_type":"code","source":"newgenredata=cleanerdata.groupby(\"Name\").mean()\nnewgenredata[\"Genre\"]=newgenredata.index.map(lambda name: cleanerdata[cleanerdata[\"Name\"]==name][\"Genre\"].values[0])\n\nbins_=np.arange(3.15,5.05,0.1)\n\nsns.displot(newgenredata, x=\"Rating\",hue=\"Genre\", element=\"step\", bins=bins_)\nsns.displot(cleanerdata, x=\"Rating\",hue=\"Year\", element=\"poly\", bins=bins_,fill=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"groupeddata=cleanerdata.groupby(\"Name\").mean()\nsns.scatterplot(x=groupeddata.YearsBs,y=groupeddata.Rating)\nsns.displot(groupeddata, x=\"Rating\",hue=\"YearsBs\", element=\"poly\", stat=\"probability\",common_norm=False,bins=bins_,fill=False)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"sns.scatterplot(x=groupeddata.YearsEl,y=groupeddata.Rating)\nsns.displot(groupeddata, x=\"Rating\",hue=\"YearsEl\", element=\"poly\", stat=\"probability\",common_norm=False,bins=bins_,fill=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Fortunately our initial assumptions haven't changed so much."},{"metadata":{},"cell_type":"markdown","source":"### 3.3 Dummy variables generation"},{"metadata":{},"cell_type":"markdown","source":"Now we generate the dummy variables of \"author\", which indicate the author of the book, and \"year\", which indicate in which years the book was a bestseller. First we generate them and assign them to \"Name\", but in multiple year bestsellers each row will only activate the dummy variable corresponding to that year, so we have to group by \"Name\" and take the maximum value of the dummy variables so each year as a bestseller will be activated. Then we assign the new dummy variables to the cleaned dataset. In the end, multirow books will have each row with the author, genre and corresponding year dummy variables activated and they will have the same other variables except \"Year\"."},{"metadata":{"trusted":true},"cell_type":"code","source":"author_dummies=pd.get_dummies(cleanerdata[\"Author\"], prefix=\"author\")\nyear_dummies=pd.get_dummies(cleanerdata[\"Year\"], prefix=\"year\")\ngenre_dummies=pd.get_dummies(cleanerdata[\"Genre\"], prefix=\"genre\")\ndummydata=pd.concat([cleanerdata[\"Name\"],author_dummies,year_dummies,genre_dummies] ,axis=1)\ndummydata_summed=dummydata.groupby(\"Name\").max()\nfor column in dummydata_summed.columns:\n    cleanerdata[column]=cleanerdata[\"Name\"].apply(lambda name: dummydata_summed.loc[name][column]) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally we group the clean data by \"Name\" with the mean values to drop the non numeric columns (\"Author\", \"Genre\"). Then we drop \"Year\" so now each book occupies a single row with all the correct numeric and dummy variables."},{"metadata":{"trusted":true},"cell_type":"code","source":"learningdata=cleanerdata.groupby(\"Name\").mean().drop(columns=\"Year\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3.4 Splitting and scaling the data"},{"metadata":{},"cell_type":"markdown","source":"Now that we have the data properly structured, we can prepare it to be machine learned. First we will split it into training and testing data and then we will rescale it so it will be easier to learn. We will use the StandardScaler tool, which will make our data have mean=0 and variance=1."},{"metadata":{},"cell_type":"markdown","source":"#### 3.4.1 Splitting"},{"metadata":{"trusted":true},"cell_type":"code","source":"learningdata_=learningdata\nlearningdata_y=learningdata_.pop(\"Rating\").values\nlearningdata_X=learningdata_.values\nX_train, X_test, y_train, y_test = train_test_split(learningdata_X,learningdata_y,test_size=0.15, random_state=29)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 3.4.2 Scaling"},{"metadata":{"trusted":true},"cell_type":"code","source":"Xscaler=StandardScaler().fit(X_train)\nyscaler=StandardScaler().fit(y_train.reshape(-1,1))\nX_train=Xscaler.transform(X_train)\nX_test=Xscaler.transform(X_test)\ny_train=np.ravel(yscaler.transform(y_train.reshape(-1,1)))\ny_test=np.ravel(yscaler.transform(y_test.reshape(-1,1)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4. Machine learning the data"},{"metadata":{},"cell_type":"markdown","source":"### 4.1 Model evaluation"},{"metadata":{},"cell_type":"markdown","source":"To evaluate each algorythm we use cross-validation and parameter searching to avoid overfitting issues and to select a decent parameter combination."},{"metadata":{"trusted":true},"cell_type":"code","source":"predratings_train=pd.DataFrame()\npredratings_test=pd.DataFrame()\n\npredratings_train[\"Reality\"]=np.ravel(yscaler.inverse_transform(y_train.reshape(-1,1)))\npredratings_test[\"Reality\"]=np.ravel(yscaler.inverse_transform(y_test.reshape(-1,1)))\n\nmodelRMSE_train={}\nmodelRMSE_test={}\n\ndef tuned_model(model, param_grid, results=False):\n    tuner=GridSearchCV(model,param_grid)\n    tuner.fit(X_train,y_train)\n    best_model=tuner.best_estimator_\n    best_params=tuner.best_params_\n    print(f\"Best model: {best_model}\")\n    if results==True:\n\n        print(pd.DataFrame(tuner.cv_results_).sort_values(by=['rank_test_score']).dropna())\n    return best_model\n\ndef execute_model(model,modelname):\n    model.fit(X_train,y_train)\n    y_train_pred=model.predict(X_train)\n    y_test_pred=model.predict(X_test)\n    predratings_train[modelname]=np.ravel(yscaler.inverse_transform(y_train_pred.reshape(-1,1)))\n    predratings_test[modelname]=np.ravel(yscaler.inverse_transform(y_test_pred.reshape(-1,1)))\n\n\n    modelRMSE_train[modelname]=mean_squared_error(y_train,y_train_pred,squared=False)\n    modelRMSE_test[modelname]=mean_squared_error(y_test,y_test_pred,squared=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 4.1.1 Stochastic Gradient Descent Regressor"},{"metadata":{"trusted":true},"cell_type":"code","source":"SGDR_param_grid={\"alpha\":[1,0.5,0.1,0.05,0.01],\"loss\":[\"squared_loss\", \"huber\", \"epsilon_insensitive\", \"squared_epsilon_insensitive\"],\"penalty\":[\"l2\", \"l1\", \"elasticnet\"]}\nSGDR=tuned_model(SGDRegressor(),SGDR_param_grid,results=True)\nexecute_model(SGDR,\"Stochastic Gradient Descent\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 4.1.2 Support Vector Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"SVectoR_param_grid={\"C\":[0.1,0.5,1,5,10],\"kernel\":[\"linear\", \"poly\", \"rbf\", \"sigmoid\"]}\nSVectoR=tuned_model(SVR(),SVectoR_param_grid,results=True)\nexecute_model(SVectoR,\"Support Vector\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 4.1.3 Decision Tree Regressor"},{"metadata":{"trusted":true},"cell_type":"code","source":"DTreeR_param_grid={\"ccp_alpha\":[0.8,0.5,0.01,0.005,0.0001],\"max_depth\":[None, 10, 20, 40],\"criterion\":[\"mse\", \"friedman_mse\", \"mae\", \"poisson\"]}\nDTreeR=tuned_model(DecisionTreeRegressor(),DTreeR_param_grid,results=True)\nexecute_model(DTreeR,\"Decision Tree\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 4.1.4 Multi-Layer Perceptron Regressor"},{"metadata":{"trusted":true},"cell_type":"code","source":"MLPR_param_grid={\"alpha\":[0.1,0.01,0.001,0.0001],\n                 \"max_iter\":[10,20,40,80], \n                 \"hidden_layer_sizes\":[10,20,40,60],\n                 \"activation\":[\"identity\",\"tanh\",\"logistic\",\"relu\"]}\nMLPR=tuned_model(MLPRegressor(),MLPR_param_grid,results=True)\nexecute_model(MLPR,\"Multi-Layer Perceptron\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.2 Algorythm comparation"},{"metadata":{},"cell_type":"markdown","source":"#### 4.2.1 Graphical comparation."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,ax=plt.subplots(figsize=(20,10),ncols=4, nrows=2)\n\nfig.suptitle(\"Model predictions vs actual ratings (Top: Training data, Bottom: Testing data)\",fontsize=16)\nfigrows=ax.shape[0]\nfigcols=ax.shape[1]\nfor i in range(0,figrows):\n    for j in range(0,figcols):\n        ax[i][j].set_xlim(4,5)\n        ax[i][j].set_ylim(4,5)\n\nsns.scatterplot(x=predratings_train[\"Reality\"],y=predratings_train[\"Stochastic Gradient Descent\"],ax=ax[0][0])\nsns.scatterplot(x=predratings_train[\"Reality\"],y=predratings_train[\"Support Vector\"],ax=ax[0][1])\nsns.scatterplot(x=predratings_train[\"Reality\"],y=predratings_train[\"Decision Tree\"],ax=ax[0][2])\nsns.scatterplot(x=predratings_train[\"Reality\"],y=predratings_train[\"Multi-Layer Perceptron\"],ax=ax[0][3])\n\nsns.scatterplot(x=predratings_test[\"Reality\"],y=predratings_test[\"Stochastic Gradient Descent\"],ax=ax[1][0])\nsns.scatterplot(x=predratings_test[\"Reality\"],y=predratings_test[\"Support Vector\"],ax=ax[1][1])\nsns.scatterplot(x=predratings_test[\"Reality\"],y=predratings_test[\"Decision Tree\"],ax=ax[1][2])\nsns.scatterplot(x=predratings_test[\"Reality\"],y=predratings_test[\"Multi-Layer Perceptron\"],ax=ax[1][3])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All 4 algorythms struggle to learn the data as shown in the width of the training plot \"lines\", that limits the quality of the testing data rating predictions."},{"metadata":{},"cell_type":"markdown","source":"#### 4.2.2 RMSE comparation"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"print(\"Root Mean Square Error (Training data)\")\nfor key in modelRMSE_train:\n    print(\"{}: {}\".format(key,modelRMSE_train[key]))\nprint(\"\\n\")    \nprint(\"Root Mean Square Error (Test data)\")\nfor key in modelRMSE_test:\n    print(\"{}: {}\".format(key,modelRMSE_test[key]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 4.2.3 Data comparation"},{"metadata":{},"cell_type":"markdown","source":"TRAINING DATASET"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"predratings_train.round(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"TESTING DATASET"},{"metadata":{"trusted":true},"cell_type":"code","source":"predratings_test.round(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5. Conclusions"},{"metadata":{},"cell_type":"markdown","source":"1. The first data analysis shows that:\n   - The newer bestsellers tend to have slightly better ratings than the older ones.\n   - The more years a book has been a bestseller, the more likely is to have a good rating.\n   - Fiction books are better rated than non fiction ones.\n2. About the model selection and the machine learning algorythms used:\n   - The lack of samples after the data cleaning have made the data difficult to learn properly.\n   - The Decision Tree algorythm is notably the less effective algorythm used.\n   - The Stochastic Gradient Descent is the one which gives better results, closely followed by the Support Vector\n     and the Multi-Layer Perceptron.\n   "},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}