{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Using Logistic Regression to Predict Red Wine Quality"},{"metadata":{},"cell_type":"markdown","source":"In this notebook, we will try to predict quality of red wines using logistic regression. Logistic regression is used because goal of this classification is to predict wines as \"Good\" or \"Bad\" based on their components."},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nWe need these imports\n'''\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn import preprocessing\nimport matplotlib.pyplot as plt \nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nimport seaborn as sns\n\nplt.rc(\"font\", size=14)\nsns.set(style=\"white\")\nsns.set(style=\"whitegrid\", color_codes=True)\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nNecessary function to turn quality values to binary.\nScores above and equal to 7 will be considered good.\nScores below 7 are considered bad.\n(Because quality in this dataset ranges from 3 to 8)\n'''\n\ndef good_to_one(col):\n    quality = col[0]\n    \n    if quality >= 7:\n        return 1\n    else:\n        return 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"df = pd.read_csv(\"../input/red-wine-quality-cortez-et-al-2009/winequality-red.csv\")\n\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Preparation:\n\nThankfully, there are no missing values in this dataset. We will create a dummy variable to replace the \"quality\" variable. Dummy variables have 2 values, 0 and 1. Fitting for logistic regression and the classification we are hoping to achieve. We will name our dummy variable \"quality_1\"."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"#We don't want to change the original set, as we might need it later.\ndf_good = df.copy()\n\ndf_good['quality'] = df_good[['quality']].apply(good_to_one, axis=1)\n\nquality = pd.get_dummies(df_good['quality'],drop_first=True, prefix=\"quality\")\ndf_good.drop(['quality'],axis=1,inplace=True)\n\ndf_good = pd.concat([df_good, quality], axis=1)\n\ndf_good.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Exploration:"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"df_good['quality_1'].value_counts()\nsns.countplot(x='quality_1', data=df_good, palette='hls')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are 217 good quality and 1382 bad quality wines. Our classes are not balanced, majority is \"bad\" quality wines. If we train a model now, it will predict every instance as having \"bad\" quality. We either only take 217 samples from \"bad\" group or do over-sampling to increase the size of \"good\" group. In this notebook, we will use over-sampling because reducing the size of one group randomly creates inconsistent models. Before balancing the groups, we can make more plots for further exploration."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.groupby('quality').mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_good.groupby('quality_1').mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Bad quality wines have on average higher volatile acidity (acetic acid) but lower citric acid.\n- Good quality wines have less chlorides.\n- Good quality wines have lower free and total sulfur dioxide but higher sulphates.\n- Bad quality wines tend to be less alcoholic\n\nWe will keep those in mind while creating our model."},{"metadata":{},"cell_type":"markdown","source":"## Over-sampling"},{"metadata":{},"cell_type":"markdown","source":"We will use SMOT (Synthetic Minority Oversampling Technique) for oversampling our data.\nSMOT:\n\n1. Works by creating synthetic samples from the minor class (good quality) instead of creating copies.\n2. Randomly choosing one of the k-nearest-neighbors and using it to create a similar, but randomly tweaked, new observations.\n\nSource used for Over-sampling, RFE (next section) and ROC (last section) with logistic regression:\nhttps://towardsdatascience.com/building-a-logistic-regression-in-python-step-by-step-becd4d56c9c8\n\n(I suggest you give it a read as we didn't have to use some techniques in this notebook thanks to our complete dataset)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from imblearn.over_sampling import SMOTE\n\nx = df_good.loc[:, df_good.columns != 'quality_1']\ny = df_good.loc[:, df_good.columns == 'quality_1']\n\nover_samp = SMOTE(random_state=0)\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=0)\ncolumns = x_train.columns\n\novers_data_x, overs_data_y = over_samp.fit_resample(x_train, y_train)\novers_data_x = pd.DataFrame(data=overs_data_x,columns=columns )\novers_data_y= pd.DataFrame(data=overs_data_y,columns=['quality_1'])\n\n# we can Check the numbers of our data\nprint(\"Length of oversampled data is \",len(overs_data_x))\nprint(\"Number of good quality in oversampled data \",len(overs_data_y[overs_data_y['quality_1']==0]))\nprint(\"Number of bad quality \",len(overs_data_y[overs_data_y['quality_1']==1]))\nprint(\"Proportion of good quality data in oversampled data is \",len(overs_data_y[overs_data_y['quality_1']==0])/len(overs_data_x))\nprint(\"Proportion of bad quality data in oversampled data is \",len(overs_data_y[overs_data_y['quality_1']==1])/len(overs_data_x))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Recursive Feature Elimination\n\nRFE lets us eliminate worst performing features. Removing these features will make our model more accurate as our model won't be distracted by non-significant features. Think of it like removing noise from a voice recording or an image."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import RFE","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_good_vars = df_good.columns.values.tolist()\ny = ['quality_1']\nx = [d for d in df_good_vars if d not in y]\n\nlogreg = LogisticRegression(solver='liblinear')\nrfe = RFE(logreg, n_features_to_select=20)\nrfe = rfe.fit(x_train, y_train.values.ravel())\n\nprint(rfe.support_)\nprint(rfe.ranking_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will select all features as RFE couldn't eliminate any of them. But don't worry, in the next section we will eliminate them manually."},{"metadata":{"trusted":true},"cell_type":"code","source":"cols=['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', \n      'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', \n      'pH', 'sulphates', 'alcohol']\nx = overs_data_x[cols]\ny = overs_data_y['quality_1']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"import statsmodels.api as sm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logit_model = sm.Logit(y, x)\nresult = logit_model.fit()\nprint(result.summary2())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"(Don't mind the warnings, they are about top half of the results which we can calculate in the cell below)\n\nA-ha! Take a look at the column named \"P > |z|\". We can remove variables with p-values higher than 0.05. They are not significant enough to bother our model. Goodbye fixed acidity, citric acid, free sulfur dioxide and pH."},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"glm_model = sm.GLM(y, x, family=sm.families.Binomial())\nresult = glm_model.fit()\nprint(result.summary2())","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"cols = ['volatile acidity', 'residual sugar', 'chlorides', 'total sulfur dioxide', \n        'density', 'sulphates', 'alcohol']\nx = overs_data_x[cols]\ny = overs_data_y['quality_1']\n\nlogit_model = sm.Logit(y, x)\nresult = logit_model.fit()\nprint(result.summary2())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Notice how p-values of;\n\n- residual sugar decreased from 0.0044 to 0.0013,\n- chlorides decreased from 0.0006 to 0.0002,\n- density decreased from 0.0002 to <0.0001\n\nAs you can see, removing features which perform badly gets rid of the noise and now our model performs better with other features."},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"cols = ['volatile acidity', 'residual sugar', 'chlorides', 'total sulfur dioxide', \n        'density', 'sulphates', 'alcohol']\nx = overs_data_x[cols]\ny = overs_data_y['quality_1']\n\nglm_model = sm.GLM(y, x, family=sm.families.Binomial())\nresult = glm_model.fit()\nprint(result.summary2())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train, x_test, y_train, y_test = train_test_split(x,\n                                                    y,\n                                                    test_size=0.30,\n                                                    random_state=0)\nlogreg = LogisticRegression(solver='liblinear')\nlogreg.fit(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = logreg.predict(x_test)\nprint('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg.score(x_test, y_test)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Confusion Matrix\n\nConfusion matrix is important. A medical test used for diagnosis with high accuracy might be rendered totally useless by having recall. Quick summary about values in the confusion matrix:\n\n- Top left: True negative (tn)\n- Top right: False positive (fp)\n- Bottom left: False negative (fn)\n- Bottom right True positive (tp)\n\nEssentially you want top-left and bottom-right numbers to be as high as possible while top-right and bottom-left stays as low as possible."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\n\nconfusion_matrix = confusion_matrix(y_test, y_pred)\nprint(confusion_matrix)\nprint(classification_report(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"You probably know accuracy, but what are those precision and recall values?\n\n- precision: tp / (fp + tp)\n- recall: tp / (fn + tp)\n- f1-score: 2(precision*recall) / (precision+recall)\n\nRemember the medical test example? If your precision is low, you will falsely diagnose healthy people with cancer. If your recall is low, you will falsely diagnose people with cancer as healthy. Our model looks fine."},{"metadata":{},"cell_type":"markdown","source":"## ROC Curve"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\n\nlogit_roc_auc = roc_auc_score(y_test, logreg.predict(x_test))\nfpr, tpr, thresholds = roc_curve(y_test, logreg.predict_proba(x_test)[:,1])\nplt.figure()\nplt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.legend(loc=\"lower right\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Cross-validation\n\nWe are done with our model. One last thing left to do, which is cross validation."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_validate, cross_val_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cv_dict = cross_validate(logreg, x, y, cv=5, return_train_score=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cv_dict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scores = cross_val_score(logreg, x, y, cv=5)\nprint(\"Accuracy with cross-validation: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Aaand we are done! Now we have a model which can predict red wines' quality based on their components with 0.82 accuracy. Hopefully you had fun or learned something new."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}