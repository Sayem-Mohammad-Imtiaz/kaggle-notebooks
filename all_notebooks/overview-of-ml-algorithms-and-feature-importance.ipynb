{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"0e7eb2c3-e624-23c6-790a-27f89454c85a"},"source":"# Comparing machine learning algorithms on predicting edible/poisonous mushrooms\n\n## Let's start with our imports!"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2f6f923e-6216-f053-3030-96635dc3bc22"},"outputs":[],"source":"# Imports\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n%matplotlib inline"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a452e740-1437-c657-d7e1-4df7742802bf"},"outputs":[],"source":"# Let's import the data and start exploring it\ndata = pd.read_csv('../input/mushrooms.csv')\ndata.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ce042608-63f7-643b-03ca-0d6f1f51bfc8"},"outputs":[],"source":"data.info()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c0966bc9-2244-d8be-a3da-a772ba8ac9d7"},"outputs":[],"source":"data.describe()"},{"cell_type":"markdown","metadata":{"_cell_guid":"22e61fbf-b376-2e8d-54c0-7e23d762d2b9"},"source":"It becomes clear that we are dealing with all categorical variables here.\nWe can use sci-kit learn's Label Encoder to deal with these categorical variables."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b9061cc5-dbd6-bd75-01bd-7727a999b974"},"outputs":[],"source":"labelEncoder = preprocessing.LabelEncoder()\nfor col in data.columns:\n    data[col] = labelEncoder.fit_transform(data[col])\n    \n# Train Test Split\nX = data.drop('class', axis=1)\ny = data['class']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)"},{"cell_type":"markdown","metadata":{"_cell_guid":"b6f9446d-9da7-921d-9ac0-12e31f6ff8a2"},"source":"Time to compare some Machine Learning models\nWe use a for loop to loop throuh the different models. The empty lists are made to create \nthe overview table at the end."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"bff1e945-9583-c4f5-c324-b546f5282cb9"},"outputs":[],"source":"keys = []\nscores = []\nmodels = {'Logistic Regression': LogisticRegression(), 'Decision Tree': DecisionTreeClassifier(),\n          'Random Forest': RandomForestClassifier(n_estimators=30), \n          'K-Nearest Neighbors':KNeighborsClassifier(n_neighbors=1),\n            'Linear SVM':SVC(kernel='rbf', gamma=.10, C=1.0)}\n\nfor k,v in models.items():\n    mod = v\n    mod.fit(X_train, y_train)\n    pred = mod.predict(X_test)\n    print('Results for: ' + str(k) + '\\n')\n    print(confusion_matrix(y_test, pred))\n    print(classification_report(y_test, pred))\n    acc = accuracy_score(y_test, pred)\n    print(acc)\n    print('\\n' + '\\n')\n    keys.append(k)\n    scores.append(acc)\n    table = pd.DataFrame({'model':keys, 'accuracy score':scores})\n\nprint(table)"},{"cell_type":"markdown","metadata":{"_cell_guid":"286ef9c8-31f9-cded-fead-472ba89dad0c"},"source":"Logistic Regression clearly performs the poorest of our algorithms. The k-NN classifier comes extremely close to 100% accuracy. The tree-based methods and the linear SVM all achieve 100% accuracy. It looks like these machine learning algorithms have little trouble with this dataset. Let's explore the important features in predicting poisonous mushrooms next."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7394532e-adb2-6e32-be93-5ec027fa290b"},"outputs":[],"source":"# Re-training the Random Forest\nrfc = RandomForestClassifier(n_estimators = 30)\nrfc.fit(X_train, y_train)\npred_rfc = rfc.predict(X_test)\n\nimportances = rfc.feature_importances_\nplot = sns.barplot(x=X.columns, y=importances)\n\nfor item in plot.get_xticklabels():\n    item.set_rotation(90)"},{"cell_type":"markdown","metadata":{"_cell_guid":"c9b2409c-2ab8-0964-df70-9488b4ada7d6"},"source":"Odor has the highest feature importance in the Random Forest. We can explore the effect of odor on the predicted class a bit further with this next plot."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e05b1150-1a4d-10f7-4800-c3f43068f616"},"outputs":[],"source":"sns.countplot(x = 'odor', data = data, hue='class', palette='coolwarm')\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"374e72de-5649-a46a-14b9-c7641aafc14c"},"source":"From this plot we can see how important odor is in predicting the right classes. Most odor categories are only linked to one outcome class. And for odor #5 almost all mushrooms belong to class 0.\nThis was a very clear dataset where most ML algorithms will not have a problem with."}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}