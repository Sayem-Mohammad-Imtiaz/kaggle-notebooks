{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\n\n# Bokeh\nfrom bokeh.io import output_notebook\nfrom bokeh.plotting import figure, show\nfrom bokeh.models import HoverTool, CustomJS, ColumnDataSource, Slider, Range1d\nfrom bokeh.layouts import column\nfrom bokeh.palettes import all_palettes\noutput_notebook()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"1f1d92b9-af21-47ba-a756-e70bf7ca984c","_uuid":"4f55c83e1638be7971359264469ba7185d1e5656"},"cell_type":"markdown","source":"## 1. Loading data\nWe load docs from [NIPS Papers](https://www.kaggle.com/benhamner/nips-papers) dataset."},{"metadata":{"_cell_guid":"eda986a3-0134-4d61-881a-3573182b7843","_uuid":"eec14a4ce07160a4e1de9f61a542662e24652f5d","trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"../input/nips-papers/papers.csv\")\nprint(df.paper_text[0][:500] + ' ...')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Lemmatization\n\nApply lemmatization `spaCy` [framework](https://spacy.io/). **Lemmatization** is the redusing a word to its \"dictionary form\" (word's *lemma*). "},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nimport spacy\n\nnlp = spacy.load('en', disable=['parser', 'ner'])\ndf['paper_text_lemma'] = df.paper_text.map(lambda x: [token.lemma_ for token in nlp(x) if token.lemma_ != '-PRON-' and token.pos_ in {'NOUN', 'VERB', 'ADJ', 'ADV'}])\n\n# Final cleaning\ndf['paper_text_lemma'] = df.paper_text_lemma.map(lambda x: [t for t in x if len(t) > 1])\n\n# Example\nprint(df['paper_text_lemma'][0][:25], end='\\n\\n')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c7daa2f1-cc2c-42d5-9a2e-84d66ab16a08","_uuid":"748485c63116b471a7334c32e6623c55ca89bc1d","collapsed":true},"cell_type":"markdown","source":"## 3. TFIDF and UMAP\n\nConstructing [TFIDF-matrix](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)."},{"metadata":{"_cell_guid":"43974244-e1c6-4bd1-adfd-20f24adfc141","_uuid":"6a7693efaf4cd6ffd355c0dae5a61827641a4c36","trusted":true},"cell_type":"code","source":"%%time\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nnp.random.seed(42)\nn_features=2000\ntfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, max_features=n_features, ngram_range=(1,2), stop_words='english')\ntfidf = tfidf_vectorizer.fit_transform(df.paper_text_lemma.map(lambda x: ' '.join(x)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, we'll build an embedding of our `n_feature`-dimesional space into 2D using `UMAP` ([Uniform Manifold Approximation and Projection for Dimension Reduction](https://umap-learn.readthedocs.io/en/latest/)) packege for visualization."},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nimport umap\n\numap_embr = umap.UMAP(n_neighbors=10, metric='cosine', min_dist=0.1, random_state=42)\nembedding = umap_embr.fit_transform(tfidf.todense())\nembedding = pd.DataFrame(embedding, columns=['x','y'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So, let's see what we have..."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"source = ColumnDataSource(\n        data=dict(\n            x = embedding.x,\n            y = embedding.y,\n            title = df.title,\n            year = df.year,\n        )\n    )\nhover_emb = HoverTool(names=[\"df\"], tooltips=\"\"\"\n    <div style=\"margin: 10\">\n        <div style=\"margin: 0 auto; width:300px;\">\n            <span style=\"font-size: 12px; font-weight: bold;\">Title:</span>\n            <span style=\"font-size: 12px\">@title</span>\n            <span style=\"font-size: 12px; font-weight: bold;\">Year:</span>\n            <span style=\"font-size: 12px\">@year</span>\n        </div>\n    </div>\n    \"\"\")\ntools_emb = [hover_emb, 'pan', 'wheel_zoom', 'reset']\nplot_emb = figure(plot_width=600, plot_height=600, tools=tools_emb, title='Papers')\nplot_emb.circle('x', 'y', size=5, fill_color='green',\n                alpha=0.7, line_alpha=0, line_width=0.01, source=source, name=\"df\")\n\nplot_emb.x_range = Range1d(-8, 6)\nplot_emb.y_range = Range1d(-8, 7)\n\nlayout = column(plot_emb)\nshow(layout)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seems we have some structure there, let's investigate further."},{"metadata":{},"cell_type":"markdown","source":"## 4. Gensim NMF model and Coherence\n\nLet's organize the text into a datastructure sutable for `gensim` [non-negative matrix factorization](https://en.wikipedia.org/wiki/Non-negative_matrix_factorization) model."},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfrom gensim import corpora, models\nnp.random.seed(42)\n\n# Create a corpus from a list of texts\ntexts = df['paper_text_lemma'].values\ndictionary = corpora.Dictionary(texts, prune_at=2000)\ncorpus = [dictionary.doc2bow(text) for text in texts]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Training the `NMF` models. Here, we'll train approximately $50$ models (for the numbers of topics (`n_topics`) between $3$ and $50$). For each model we calculate the *coherence score* (coherence score is cculated for each topic within a particular module). All those scores will be saved into `coh_list` (a list of coherence scores for every model). For example, the first element of the list is a list consisting of $3$ scores (since the first model will have only $3$ topics. The second element is the list of length $4$, and so on.\n\nWe are using the coherence metric called `UMass` (aka *intrinsic measure*)."},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfrom gensim.models.nmf import Nmf\nfrom gensim.models.coherencemodel import CoherenceModel\n\ncoh_list = []\nfor n_topics in range(3,50+1):\n    # Train the model on the corpus\n    nmf = Nmf(corpus, num_topics=n_topics, id2word=dictionary, random_state=42)\n    # Estimate coherence\n    cm = CoherenceModel(model=nmf, texts=texts, dictionary=dictionary, coherence='u_mass')\n    coherence = cm.get_coherence_per_topic() # get coherence value\n    coh_list.append(coherence)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let plot the coherence scores and guess the number of topics. First, we calculate mean score and the standard deviation for each model. The blue line shows the means and the green region represents the standard deviations."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Coherence scores:\ncoh_means = np.array([np.mean(l) for l in coh_list])\ncoh_stds = np.array([np.std(l) for l in coh_list])\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.xticks(np.arange(3, 50+1, 3.0));\nplt.plot(range(3,50+1), coh_means);\nplt.fill_between(range(3,50+1), coh_means-coh_stds, coh_means+coh_stds, color='g', alpha=0.05);\nplt.vlines([6, 12, 23], -1.1, 0, color='red', linestyles='dashed',  linewidth=1);\nplt.hlines([-0.645], 3, 50, color='black', linestyles='dotted',  linewidth=0.5);\nplt.ylim(-1.1,0);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As \"good candidates\" for the number of topics we'll chose a few local minima of the graph. Those are `n_topic=6` and `n_topic=12`. Also, the mean coherence plot seems to have a starting plato around `n_topic=23`. We'll investigate those values below."},{"metadata":{},"cell_type":"markdown","source":"## 5. NMF models in details"},{"metadata":{},"cell_type":"markdown","source":"### 5.1 NMF-6"},{"metadata":{},"cell_type":"markdown","source":"#### 5.1.1. Topics\n\nFor further investigation we'll use NMF algorithm from another packege (`NMF` from `sklearn.decomposition`)."},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfrom sklearn.decomposition import NMF\n\nn_topics=6\nn_top_words = 15\nnmf = NMF(n_components=n_topics, random_state=42, alpha=.1, l1_ratio=.5).fit(tfidf)\nnmf_embedding = nmf.transform(tfidf)\nfeature_names = tfidf_vectorizer.get_feature_names()\nprint(\"Topics found via NMF:\")\nfor topic_idx, topic in enumerate(nmf.components_):\n    print(\"\\nTopic {}:\".format(topic_idx))\n    print(\" \".join(['[{}]'.format(feature_names[i]) for i in topic.argsort()[:-n_top_words - 1:-1]]))\nprint()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"topics = ['Optimization Algorithms',\n          'Artificial Neurons',\n          'Game Theory/Reinf. Learn.',\n          'Neural Networks',\n          'Bayesian Methods',\n          'Kernel Methods'          \n         ]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 5.1.2. Bokeh interactive plot"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"centroids = umap_embr.transform(nmf.components_)\nembedding['hue'] = nmf_embedding.argmax(axis=1)\nmy_colors = [all_palettes['Category20'][20][i] for i in embedding.hue]\nsource = ColumnDataSource(\n        data=dict(\n            x = embedding.x,\n            y = embedding.y,\n            colors = my_colors,\n            topic = [topics[i] for i in embedding.hue],\n            title = df.title,\n            year = df.year,\n            alpha = [0.7] * embedding.shape[0],\n            size = [7] * embedding.shape[0]\n        )\n    )\nhover_emb = HoverTool(names=[\"df\"], tooltips=\"\"\"\n    <div style=\"margin: 10\">\n        <div style=\"margin: 0 auto; width:300px;\">\n            <span style=\"font-size: 12px; font-weight: bold;\">Topic:</span>\n            <span style=\"font-size: 12px\">@topic</span>\n            <span style=\"font-size: 12px; font-weight: bold;\">Title:</span>\n            <span style=\"font-size: 12px\">@title</span>\n            <span style=\"font-size: 12px; font-weight: bold;\">Year:</span>\n            <span style=\"font-size: 12px\">@year</span>\n        </div>\n    </div>\n    \"\"\")\ntools_emb = [hover_emb, 'pan', 'wheel_zoom', 'reset']\nplot_emb = figure(plot_width=700, plot_height=700, tools=tools_emb, title='Papers')\nplot_emb.circle('x', 'y', size='size', fill_color='colors', \n                 alpha='alpha', line_alpha=0, line_width=0.01, source=source, name=\"df\", legend='topic')\n\nfor i in range(n_topics):\n    plot_emb.cross(x=centroids[i,0], y=centroids[i,1], size=15, color='black', line_width=2, angle=0.79)\nplot_emb.legend.location = \"bottom_left\"\nplot_emb.legend.label_text_font_size= \"8pt\"\nplot_emb.legend.spacing = -5\nplot_emb.x_range = Range1d(-9, 7)\nplot_emb.y_range = Range1d(-9, 7)\n\ncallback = CustomJS(args=dict(source=source), code=\n    \"\"\"\n    var data = source.data;\n    var f = cb_obj.value\n    x = data['x']\n    y = data['y']\n    colors = data['colors']\n    alpha = data['alpha']\n    title = data['title']\n    year = data['year']\n    size = data['size']\n    for (i = 0; i < x.length; i++) {\n        if (year[i] <= f) {\n            alpha[i] = 0.9\n            size[i] = 7\n        } else {\n            alpha[i] = 0.05\n            size[i] = 4\n        }\n    }\n    source.change.emit();\n    \"\"\")\n\nslider = Slider(start=df.year.min()-1, end=df.year.max(), value=2016, step=1, title=\"Before year\")\nslider.js_on_change('value', callback)\n\nlayout = column(slider, plot_emb)\nshow(layout)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 5.1.3. Static Picture"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\n%matplotlib inline\n\nlegend_list = []\nfor color in all_palettes['Category20'][20][:n_topics]:   \n    legend_list.append(mpatches.Ellipse((0, 0), 1, 1, fc=color))\n    \nfig,ax = plt.subplots(figsize=(12,13))\nax.scatter(embedding.x, embedding.y, c=my_colors, alpha=0.7)\nax.scatter(centroids[:,0], centroids[:,1], c='black', s=100, alpha=0.7, marker='x')\nax.set_title('6 topics found via NMF');\nfig.legend(legend_list, topics, loc=(0.18,0.87), ncol=3)\nplt.subplots_adjust(top=0.82)\nplt.suptitle(\"NIPS clustered by topic\", **{'fontsize':'14','weight':'bold'});\nplt.figtext(.51,0.95, 'topic modeling with NMF + 2D-embedding with UMAP', \n            **{'fontsize':'12','weight':'light'}, ha='center');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5.2. NMF-12"},{"metadata":{},"cell_type":"markdown","source":"#### 5.2.1. Topics"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfrom sklearn.decomposition import NMF\nn_topics=12\nn_top_words = 15\nnmf = NMF(n_components=n_topics, random_state=42, alpha=.1, l1_ratio=.5).fit(tfidf)\nnmf_embedding = nmf.transform(tfidf)\nfeature_names = tfidf_vectorizer.get_feature_names()\nprint(\"Topics found via NMF:\")\nfor topic_idx, topic in enumerate(nmf.components_):\n    print(\"\\nTopic {}:\".format(topic_idx))\n    print(\" \".join(['[{}]'.format(feature_names[i]) for i in topic.argsort()[:-n_top_words - 1:-1]]))\nprint()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"topics = ['Optimization Algorithms',\n          'Neural Networks',\n          'Reinforcement Learning',\n          'Image Recognition',\n          'Bayesian Methods',\n          'Visual Neurons',\n          'Graph/Tree Methods',\n          'Classification Problems',\n          'Kernel Methods',\n          'Clastering Methods',\n          'Game Theory',\n          'Artificial Neurons'\n         ]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 5.2.2. Bokeh interactive plot"},{"metadata":{"_cell_guid":"97fa8b2e-7cda-4146-9c1a-8207c71f18f2","_uuid":"fec8f21fc2db8037e5724c71c5c23b66b49b02c8","_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"centroids = umap_embr.transform(nmf.components_)\nembedding['hue'] = nmf_embedding.argmax(axis=1)\nmy_colors = [all_palettes['Category20'][20][i] for i in embedding.hue]\nsource = ColumnDataSource(\n        data=dict(\n            x = embedding.x,\n            y = embedding.y,\n            colors = my_colors,\n            topic = [topics[i] for i in embedding.hue],\n            title = df.title,\n            year = df.year,\n            alpha = [0.7] * embedding.shape[0],\n            size = [7] * embedding.shape[0]\n        )\n    )\nhover_emb = HoverTool(names=[\"df\"], tooltips=\"\"\"\n    <div style=\"margin: 10\">\n        <div style=\"margin: 0 auto; width:300px;\">\n            <span style=\"font-size: 12px; font-weight: bold;\">Topic:</span>\n            <span style=\"font-size: 12px\">@topic</span>\n            <span style=\"font-size: 12px; font-weight: bold;\">Title:</span>\n            <span style=\"font-size: 12px\">@title</span>\n            <span style=\"font-size: 12px; font-weight: bold;\">Year:</span>\n            <span style=\"font-size: 12px\">@year</span>\n        </div>\n    </div>\n    \"\"\")\ntools_emb = [hover_emb, 'pan', 'wheel_zoom', 'reset']\nplot_emb = figure(plot_width=700, plot_height=700, tools=tools_emb, title='Papers')\nplot_emb.circle('x', 'y', size='size', fill_color='colors', \n                 alpha='alpha', line_alpha=0, line_width=0.01, source=source, name=\"df\", legend='topic')\n\nfor i in range(n_topics):\n    plot_emb.cross(x=centroids[i,0], y=centroids[i,1], size=15, color='black', line_width=2, angle=0.79)\nplot_emb.legend.location = \"bottom_left\"\nplot_emb.legend.label_text_font_size= \"8pt\"\nplot_emb.legend.spacing = -5\nplot_emb.x_range = Range1d(-9, 7)\nplot_emb.y_range = Range1d(-9, 7)\n\ncallback = CustomJS(args=dict(source=source), code=\n    \"\"\"\n    var data = source.data;\n    var f = cb_obj.value\n    x = data['x']\n    y = data['y']\n    colors = data['colors']\n    alpha = data['alpha']\n    title = data['title']\n    year = data['year']\n    size = data['size']\n    for (i = 0; i < x.length; i++) {\n        if (year[i] <= f) {\n            alpha[i] = 0.9\n            size[i] = 7\n        } else {\n            alpha[i] = 0.05\n            size[i] = 4\n        }\n    }\n    source.change.emit();\n    \"\"\")\n\nslider = Slider(start=df.year.min()-1, end=df.year.max(), value=2016, step=1, title=\"Before year\")\nslider.js_on_change('value', callback)\n\nlayout = column(slider, plot_emb)\nshow(layout)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 5.2.3. Static Picture"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\n%matplotlib inline\n\nlegend_list = []\nfor color in all_palettes['Category20'][20][:n_topics]:   \n    legend_list.append(mpatches.Ellipse((0, 0), 1, 1, fc=color))\n    \nfig,ax = plt.subplots(figsize=(12,13))\nax.scatter(embedding.x, embedding.y, c=my_colors, alpha=0.7)\nax.scatter(centroids[:,0], centroids[:,1], c='black', s=100, alpha=0.7, marker='x')\nax.set_title('11 topics found via NMF');\nfig.legend(legend_list, topics, loc=(0.09,0.87), ncol=4)\nplt.subplots_adjust(top=0.82)\nplt.suptitle(\"NIPS clustered by topic\", **{'fontsize':'14','weight':'bold'});\nplt.figtext(.51,0.95, 'topic modeling with NMF + 2D-embedding with UMAP', \n            **{'fontsize':'12','weight':'light'}, ha='center');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5.3. NMF-23"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfrom sklearn.decomposition import NMF\nn_topics=23\nn_top_words = 15\nnmf = NMF(n_components=n_topics, random_state=42, alpha=.1, l1_ratio=.5).fit(tfidf)\nnmf_embedding = nmf.transform(tfidf)\nfeature_names = tfidf_vectorizer.get_feature_names()\nprint(\"Topics found via NMF:\")\nfor topic_idx, topic in enumerate(nmf.components_):\n    print(\"\\nTopic {}:\".format(topic_idx))\n    print(\" \".join(['[{}]'.format(feature_names[i]) for i in topic.argsort()[:-n_top_words - 1:-1]]))\nprint()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"topics = ['Optimization Algorithms',\n          'Neural Networks',\n          'Reinforcement Learning',\n          'Image Recognition', \n          'Probabilistic Methods',\n          'Visual Neurons',\n          'Graph/Networks',\n          'Classification Problems',          \n          'Kernel Methods',\n          'Bayesian Methods',\n          'Multiiarm Bandits',\n          'General Neurons',          \n          'Clastering Methods',\n          'Matrix Decompositions',\n          'Control Theory',\n          'Topic Modeling',          \n          'Tree Methods',\n          'Greedy Algorithms',\n          'Speech Recognition',\n          'Dimensionality Reduction',          \n          'Chips/Circuit',\n          'Game Theory',\n          'Feature Engineering'\n         ]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 5.3.2. Bokeh interactive plot"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"centroids = umap_embr.transform(nmf.components_)\nembedding['hue'] = nmf_embedding.argmax(axis=1)\nmy_colors = [(all_palettes['Category20'][20] + all_palettes['Category20'][20])[i] for i in embedding.hue]\nsource = ColumnDataSource(\n        data=dict(\n            x = embedding.x,\n            y = embedding.y,\n            colors = my_colors,\n            topic = [topics[i] for i in embedding.hue],\n            title = df.title,\n            year = df.year,\n            alpha = [0.7] * embedding.shape[0],\n            size = [7] * embedding.shape[0]\n        )\n    )\nhover_emb = HoverTool(names=[\"df\"], tooltips=\"\"\"\n    <div style=\"margin: 10\">\n        <div style=\"margin: 0 auto; width:300px;\">\n            <span style=\"font-size: 12px; font-weight: bold;\">Topic:</span>\n            <span style=\"font-size: 12px\">@topic</span>\n            <span style=\"font-size: 12px; font-weight: bold;\">Title:</span>\n            <span style=\"font-size: 12px\">@title</span>\n            <span style=\"font-size: 12px; font-weight: bold;\">Year:</span>\n            <span style=\"font-size: 12px\">@year</span>\n        </div>\n    </div>\n    \"\"\")\ntools_emb = [hover_emb, 'pan', 'wheel_zoom', 'reset']\nplot_emb = figure(plot_width=700, plot_height=700, tools=tools_emb, title='Papers')\nplot_emb.circle('x', 'y', size='size', fill_color='colors', \n                 alpha='alpha', line_alpha=0, line_width=0.01, source=source, name=\"df\", legend='topic')\n\nfor i in range(n_topics):\n    plot_emb.cross(x=centroids[i,0], y=centroids[i,1], size=15, color='black', line_width=2, angle=0.79)\nplot_emb.legend.location = \"bottom_left\"\nplot_emb.legend.label_text_font_size= \"8pt\"\nplot_emb.legend.spacing = -5\nplot_emb.x_range = Range1d(-9, 7)\nplot_emb.y_range = Range1d(-9, 7)\n\ncallback = CustomJS(args=dict(source=source), code=\n    \"\"\"\n    var data = source.data;\n    var f = cb_obj.value\n    x = data['x']\n    y = data['y']\n    colors = data['colors']\n    alpha = data['alpha']\n    title = data['title']\n    year = data['year']\n    size = data['size']\n    for (i = 0; i < x.length; i++) {\n        if (year[i] <= f) {\n            alpha[i] = 0.9\n            size[i] = 7\n        } else {\n            alpha[i] = 0.05\n            size[i] = 4\n        }\n    }\n    source.change.emit();\n    \"\"\")\n\nslider = Slider(start=df.year.min()-1, end=df.year.max(), value=2016, step=1, title=\"Before year\")\nslider.js_on_change('value', callback)\n\nlayout = column(slider, plot_emb)\nshow(layout)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 5.3.3. Static Picture"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\n%matplotlib inline\n\nlegend_list = []\nfor color in (all_palettes['Category20'][20] + all_palettes['Category20'][20])[:n_topics]:   \n    legend_list.append(mpatches.Ellipse((0, 0), 1, 1, fc=color))\n    \nfig,ax = plt.subplots(figsize=(12,13))\nax.scatter(embedding.x, embedding.y, c=my_colors, alpha=0.7)\nax.scatter(centroids[:,0], centroids[:,1], c='black', s=100, alpha=0.7, marker='x')\nax.set_title('23 topics found via NMF');\nfig.legend(legend_list, topics, loc=(0.075,0.835), ncol=4)\nplt.subplots_adjust(top=0.82)\nplt.suptitle(\"NIPS clustered by topic\", **{'fontsize':'14','weight':'bold'});\nplt.figtext(.51,0.95, 'topic modeling with NMF + 2D-embedding with UMAP', \n            **{'fontsize':'12','weight':'light'}, ha='center');","execution_count":null,"outputs":[]}],"metadata":{"language_info":{"nbconvert_exporter":"python","codemirror_mode":{"version":3,"name":"ipython"},"version":"3.6.1","pygments_lexer":"ipython3","mimetype":"text/x-python","file_extension":".py","name":"python"},"kernelspec":{"display_name":"Python 3","name":"python3","language":"python"}},"nbformat":4,"nbformat_minor":1}