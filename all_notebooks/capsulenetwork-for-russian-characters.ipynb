{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","collapsed":true,"trusted":false},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\n#credit to Hinton for concepts and Geron for example implementation\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport tensorflow as tf #for creating of neural networks\nimport imageio #processing of input images\nimport glob #processing of input images\nimport matplotlib # plotting\nimport matplotlib.pyplot as plt #plotting\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"trusted":false},"cell_type":"code","source":"#defining norm function that can avoids zero value problems, credit to Geron\ndef safe_norm(s, axis=-1, epsilon=1e-7, keepdims=False, name=None):\n    with tf.name_scope(name, default_name=\"safe_norm\"):\n        squared_norm = tf.reduce_sum(tf.square(s), axis=axis,\n                                     keepdims=keepdims)\n        return tf.sqrt(squared_norm + epsilon)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"a2d87aff-9209-491e-a5f1-f3b85083239a","_uuid":"367923761403f43d58760a2c92e56ced414e3285","collapsed":true,"trusted":false},"cell_type":"code","source":"#define normalize function for ensuring that input values of image matrixes are normalized from 0 to 1\ndef normalize(x):\n\treturn (x - np.min(x)) / (np.max(x) - np.min(x))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"2453b742-b4e6-4fc6-a4d2-d288fff00fa5","_uuid":"2b9f8aa188e6201494d2d38d3f0a171f4d34fdf2","collapsed":true,"trusted":false},"cell_type":"code","source":"#define safe squashing function, credit to Geron\ndef squash(s, axis=-1, epsilon=1e-7, name=None):\n    with tf.name_scope(name, default_name=\"squash\"):\n        squared_norm = tf.reduce_sum(tf.square(s), axis=axis,\n                                     keepdims=True)\n        safe_norm = tf.sqrt(squared_norm + epsilon)\n        squash_factor = squared_norm / (1. + squared_norm)\n        unit_vector = s / safe_norm\n        return squash_factor * unit_vector","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"ad02dff322b62bc85efb9555719e14035f3b7b08"},"cell_type":"code","source":"#load images for processing, using letters2 for training set and letters for validation set, note the normalization of the images to ensure the 0 to 255 values are converted to 0 to 1 \n# Note: I had to manually resize some images in the set to correct dimensions; some were 31x32 ect..\nfilelist_validation = glob.glob(r'..\\classification-of-handwritten-letters\\letters\\*.png')\nimages_validation= [imageio.imread(fname) for fname in filelist_validation]\nimagearray_validation = normalize(np.stack(images_validation,axis=0))\nimagearray_validation = imagearray_validation.astype('float32')\n\nfilelist = glob.glob(r'..\\classification-of-handwritten-letters\\letters2\\*.png')\nimages= [imageio.imread(fname) for fname in filelist]\nimagearray = normalize(np.stack(images,axis=0))\nimagearray = imagearray.astype('float32')\n\ndata_validation = pd.read_csv(r\"..\\classification-of-handwritten-letters\\letters.csv\")\ndata = pd.read_csv(r\"..\\classification-of-handwritten-letters\\letters2.csv\")\n\n\n\nlabels = data['label'].as_matrix()-1\nlabels_validation = data_validation['label'].as_matrix()-1\n\n#randomize training set so as not to bias initial learning\n\nidx = np.random.permutation(len(imagearray))\nimagearray,labels = imagearray[idx], labels[idx]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"2974dab462d4617d93d78862343d91cce4f97c88"},"cell_type":"code","source":"#image dimensions\nim_x = 32\nim_y = 32\n\n#window dimensions for convolution\nwinx = 9\nwiny = 9\n\n#number of feature filters\nnum_filters = 256\n\n# number of channels in image\nnum_channels = 4","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"cb191f41d57544d1b3df9d67fe02d93449d0cf7d"},"cell_type":"code","source":"#tensorflow placeholder for input images\nX = tf.placeholder(shape=[None, im_x, im_y, num_channels], dtype=tf.float32, name=\"X\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"9e6470939e49a4e3a687092ad258ef9111097bb5"},"cell_type":"code","source":"#number of capsule channels and other capsule network parameters\nCapsule1_Channels = 32\nCapsules_per_Channel = 8*8\nTotal_Cap = Capsules_per_Channel * Capsule1_Channels\nCapsule1_Dimensions = 8","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"99eabf9cb39aaacc744a012e872e856ad7bd93df"},"cell_type":"code","source":"#defining convolution parameters for two convolutional layers according to CAPSNET paper \nconv1_params = {\n    \"filters\": 256,\n    \"kernel_size\": 9,\n    \"strides\": 1,\n    \"padding\": \"valid\",\n    \"activation\": tf.nn.relu,\n}\n\nconv2_params = {\n    \"filters\": Capsule1_Channels * Capsule1_Dimensions, # 256 convolutional filters\n    \"kernel_size\": 9,\n    \"strides\": 2,\n    \"padding\": \"valid\",\n    \"activation\": tf.nn.relu\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"0afe70aed1ced61ae086279c25b8779f9261300e"},"cell_type":"code","source":"#initialize the convolutional layers of the capsule network\nconv1 = tf.layers.conv2d(X, name=\"conv1\", **conv1_params)\nconv2 = tf.layers.conv2d(conv1, name=\"conv2\", **conv2_params)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"4c9c813ff754a7101cc6012171f96d336d27d3b8"},"cell_type":"code","source":"# raw capsule network 1 input\ncaps1_raw = tf.reshape(conv2, [-1, Total_Cap, Capsule1_Dimensions],\n                       name=\"caps1_raw\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"419bee050abc26d5eaf3a4aa0142090a4e210299"},"cell_type":"code","source":"# Capsule network 2 parameters, adjusted from 10 to 33 to adjust for 33 Russian characters vs 10 MNIST digits\nCapsule2_NumCharacters = 33\nCapsule2_Dimensions = 16","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"a64cf53e339463d8a4b225c19b8d7289d842e1ad"},"cell_type":"code","source":"#initialize capsule network weights according to CAPSNET paper\ninit_sigma = 0.1\n\nW_init = tf.random_normal(\n    shape=(1, Total_Cap, Capsule2_NumCharacters, Capsule2_Dimensions, Capsule1_Dimensions),\n    stddev=init_sigma, dtype=tf.float32, name=\"W_init\")\nW = tf.Variable(W_init, name=\"W\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"4bd55c1f9c001419db01caf8dd0af9a030deb436"},"cell_type":"code","source":"#calculate the capsule network output\ncaps1_output = squash(caps1_raw, name=\"caps1_output\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"80ac419e88ec340d410bb14d838a78ea0077383b"},"cell_type":"code","source":"#implement the initial round of routing by agreement to get agreement matrix\n\nbatch_size = tf.shape(X)[0]\nW_tiled = tf.tile(W, [batch_size, 1, 1, 1, 1], name=\"W_tiled\")\n\n\ncaps1_output_expanded = tf.expand_dims(caps1_output, -1,\n                                       name=\"caps1_output_expanded\")\ncaps1_output_tile = tf.expand_dims(caps1_output_expanded, 2,\n                                   name=\"caps1_output_tile\")\ncaps1_output_tiled = tf.tile(caps1_output_tile, [1, 1, Capsule2_NumCharacters, 1, 1],\n                             name=\"caps1_output_tiled\")\n\n\ncaps2_predicted = tf.matmul(W_tiled, caps1_output_tiled,\n                            name=\"caps2_predicted\")\n\n\nraw_weights = tf.zeros([batch_size, Total_Cap, Capsule2_NumCharacters, 1, 1],\n                       dtype=np.float32, name=\"raw_weights\")\n\n\n\nrouting_weights = tf.nn.softmax(raw_weights, axis=2, name=\"routing_weights\")\n\n\nweighted_predictions = tf.multiply(routing_weights, caps2_predicted,\n                                   name=\"weighted_predictions\")\nweighted_sum = tf.reduce_sum(weighted_predictions, axis=1, keepdims=True,\n                             name=\"weighted_sum\")\n\n\ncaps2_output_round_1 = squash(weighted_sum, axis=-2,\n                              name=\"caps2_output_round_1\")\n\n\n\ncaps2_output_round_1_tiled = tf.tile(\n    caps2_output_round_1, [1, Total_Cap, 1, 1, 1],\n    name=\"caps2_output_round_1_tiled\")\n\n\nagreement = tf.matmul(caps2_predicted, caps2_output_round_1_tiled,\n                      transpose_a=True, name=\"agreement\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"3d84fd56054cb998aa1f803a8d3e11b8e89d2685"},"cell_type":"code","source":"#define tensorflow routing by agreement loop to implement subsequent rounds of routing by agreement and make number of rounds adjustable by changing value in tf.less function of the condition loop # currently runs 3 additional rounds for 4 ( 1+3 ) total\ndef condition(raw_weights_loop,agreement_loop,caps2_output_loop, counter):\n    return tf.less(counter, 4)\n\ndef loop_body(raw_weights_loop,agreement_loop,caps2_output_loop, counter):\n    raw_weights_loop = tf.add(raw_weights_loop, agreement_loop,\n                             name=\"raw_weights_loop\")\n    routing_weights_loop = tf.nn.softmax(raw_weights_loop,\n                                        axis=2,\n                                        name=\"routing_weights_loop\")\n    weighted_predictions_loop = tf.multiply(routing_weights_loop,\n                                           caps2_predicted,\n                                           name=\"weighted_predictions_loop\")\n    weighted_sum_loop = tf.reduce_sum(weighted_predictions_loop,\n                                     axis=1, keepdims=True,\n                                     name=\"weighted_sum_loop\")\n    caps2_output_loop = squash(weighted_sum_loop,\n                              axis=-2,\n                              name=\"caps2_output_loop\")\n    caps2_output_loop_tiled = tf.tile(\n    caps2_output_loop, [1, Total_Cap, 1, 1, 1],\n    name=\"caps2_output_loop_tiled\")\n    agreement_loop = tf.matmul(caps2_predicted, caps2_output_loop_tiled,\n                      transpose_a=True, name=\"agreement_loop\")\n    return  raw_weights_loop,agreement_loop,caps2_output_loop, tf.add(counter,1)\nwith tf.name_scope(\"weights_loop\"):\n    counter = tf.constant(1)\n    raw_weights_loop = raw_weights\n    agreement_loop = agreement\n    caps2_output_loop=caps2_output_round_1\n    result = tf.while_loop(condition, loop_body, [raw_weights_loop,agreement_loop,caps2_output_loop,counter],swap_memory=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"b93d22836efb447ac0d2995d7049b652e92b49fc"},"cell_type":"code","source":"#use output of  capsule2 network to calculate label probabilities and select the most likely label candidate\ny_proba = safe_norm(caps2_output_loop, axis=-2, name=\"y_proba\")\n\ny_proba_argmax = tf.argmax(y_proba, axis=2, name=\"y_proba\")\n\ny_proba_argmax\n\ny_pred = tf.squeeze(y_proba_argmax, axis=[1,2], name=\"y_pred\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"86b5069b9f64f5da225d152926f6a3742f4f72dc"},"cell_type":"code","source":"#placeholder for training labels when training the network\ny = tf.placeholder(shape=[None], dtype=tf.int64, name=\"y\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"4d92d541a5318c7deef82049b458afe2e7d45cd7"},"cell_type":"code","source":"#define margin loss parameters given in CAPSNET paper\nm_plus = 0.9\nm_minus = 0.1\nlambda_ = 0.5\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"3418e5ad52004ac82a13b8414c57f82be92c7bb6"},"cell_type":"code","source":"#create matrix using labels y for use in calculating margin loss\nT = tf.one_hot(y, depth=Capsule2_NumCharacters, name=\"T\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"aa96745816a46b180efe40e675b99715fb5bac7b"},"cell_type":"code","source":"#calculate norm of capsule network 2 output\ncaps2_output_norm = safe_norm(caps2_output_loop, axis=-2, keepdims=True,\n                              name=\"caps2_output_norm\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"ceaf8d2122455e93f4c1658a7229d5b5bd216f04"},"cell_type":"code","source":"# calculate present and absence error for incorrect guesses of presence or absence of a character in images\npresent_error_raw = tf.square(tf.maximum(0., m_plus - caps2_output_norm),\n                              name=\"present_error_raw\")\npresent_error = tf.reshape(present_error_raw, shape=(-1, Capsule2_NumCharacters),\n                           name=\"present_error\")\nabsent_error_raw = tf.square(tf.maximum(0., caps2_output_norm - m_minus),\n                             name=\"absent_error_raw\")\nabsent_error = tf.reshape(absent_error_raw, shape=(-1, Capsule2_NumCharacters),\n                          name=\"absent_error\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"c5ba6c760ae3062ca2a912ff6a7adfdba977c096"},"cell_type":"code","source":"# loss matrix for input batch\nL = tf.add(T * present_error, lambda_ * (1.0 - T) * absent_error,\n           name=\"L\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"16f0a5cd2f4e3ba386d070a8dbe9213f3c5993af"},"cell_type":"code","source":"#calculate margin loss according to paper\nmargin_loss = tf.reduce_mean(tf.reduce_sum(L, axis=1), name=\"margin_loss\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"f70a9dea12154ee45b4eafe6085e5594bcad8a33"},"cell_type":"code","source":"# masking for reconstruction loss \nmask_with_labels = tf.placeholder_with_default(False, shape=(),\n                                               name=\"mask_with_labels\")\n\nreconstruction_targets = tf.cond(mask_with_labels, # condition\n                                 lambda: y,        # if True\n                                 lambda: y_pred,   # if False\n                                 name=\"reconstruction_targets\")\n\nreconstruction_mask = tf.one_hot(reconstruction_targets,\n                                 depth=Capsule2_NumCharacters,\n                                 name=\"reconstruction_mask\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"17e0b636b6e01831d4fc1b7cfb82b89507add0ac"},"cell_type":"code","source":"#mask the capsule network output to define decoder input\nreconstruction_mask_reshaped = tf.reshape(\n    reconstruction_mask, [-1, 1, Capsule2_NumCharacters, 1, 1],\n    name=\"reconstruction_mask_reshaped\")\n\n\ncaps2_output_masked = tf.multiply(\n    caps2_output_loop, reconstruction_mask_reshaped,\n    name=\"caps2_output_masked\")\n\n\ndecoder_input = tf.reshape(caps2_output_masked,\n                           [-1, Capsule2_NumCharacters * Capsule2_Dimensions],\n                           name=\"decoder_input\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"48efd31bc71da651cfa689d89a1b211037b074a2"},"cell_type":"code","source":"#define decoder network parameters\nn_hidden1 = 512\nn_hidden2 = 1024\nn_output = im_x * im_y * num_channels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"9ac5cfa435fff5dbab36826e5893f0d81a403b77"},"cell_type":"code","source":"#implement decoder with tensorflow\nwith tf.name_scope(\"decoder\"):\n    hidden1 = tf.layers.dense(decoder_input, n_hidden1,\n                              activation=tf.nn.relu,\n                              name=\"hidden1\")\n    hidden2 = tf.layers.dense(hidden1, n_hidden2,\n                              activation=tf.nn.relu,\n                              name=\"hidden2\")\n    decoder_output = tf.layers.dense(hidden2, n_output,\n                                     activation=tf.nn.sigmoid,\n                                     name=\"decoder_output\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"54bd3a026a3f18e043b4c9bc375818e28b3dc681"},"cell_type":"code","source":"#compare original image input to decoder output to calculate reconstruction loss\nX_flat = tf.reshape(X, [-1, n_output], name=\"X_flat\")\nsquared_difference = tf.square(X_flat - decoder_output,\n                               name=\"squared_difference\")\nreconstruction_loss = tf.reduce_mean(squared_difference,\n                                    name=\"reconstruction_loss\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"ecd9c4c52c694e583f6d5eac318daa285da52f7f"},"cell_type":"code","source":"#define alpha parameter to tune inclusion of reconstruction relative to margin loss\nalpha = 0.0005\n#calculate hybrid loss\nloss = tf.add(margin_loss, alpha * reconstruction_loss, name=\"loss\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"88422dc6eebaf3015c159f18645da34fda8ba132"},"cell_type":"code","source":"#define correctness and accuracy\ncorrect = tf.equal(y, y_pred, name=\"correct\")\naccuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"b4934a02c76f8539d344c58012e6c3b7da883e9e"},"cell_type":"code","source":"#define optimizer as ADAM optimizer as used in paper, tell tensorflow to minimize loss using the optimizer\noptimizer = tf.train.AdamOptimizer()\ntraining_op = optimizer.minimize(loss, name=\"training_op\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"a7ba33a44c509608f4336fd9fa121127320c0800"},"cell_type":"code","source":"#initialize and run training\ninit = tf.global_variables_initializer()\nsaver = tf.train.Saver()\n\n\nn_epochs = 30\nbatch_size = 10\nrestore_checkpoint = True\n\nn_iterations_per_epoch = 594#letters2 / batch_size\nn_iterations_validation = 165 #letters / batch_size\nbest_loss_val = np.infty\ncheckpoint_path = os.getenv(\"TMP\") + \"/color/my_capsule_network\"\n\nwith tf.Session() as sess:\n    if restore_checkpoint and tf.train.checkpoint_exists(checkpoint_path):\n        saver.restore(sess, checkpoint_path)\n    else:\n        init.run()\n\n    for epoch in range(n_epochs):\n        for iteration in range(1, n_iterations_per_epoch + 1):\n            X_batch = imagearray[(iteration-1)*batch_size:iteration*batch_size]\n            y_batch  = labels[(iteration-1)*batch_size:iteration*batch_size]\n            # Run the training operation and measure the loss:\n            _, loss_train = sess.run(\n                [training_op, loss],\n                feed_dict={X: X_batch.reshape([-1, im_x, im_y, num_channels]),\n                           y: y_batch,\n                           mask_with_labels: True})\n            print(\"\\rIteration: {}/{} ({:.1f}%)  Loss: {:.5f}\".format(\n                      iteration, n_iterations_per_epoch,\n                      iteration * 100 / n_iterations_per_epoch,\n                      loss_train),\n                  end=\"\")\n        # At the end of each epoch,\n        # measure the validation loss and accuracy:\n        loss_vals = []\n        acc_vals = []\n        for iteration in range(1, n_iterations_validation + 1):\n            X_batch = imagearray_validation[(iteration-1)*batch_size:iteration*batch_size]\n            y_batch  = labels_validation[(iteration-1)*batch_size:iteration*batch_size]\n            loss_val, acc_val, prediction, real = sess.run(\n                    [loss, accuracy,y_pred,y],\n                    feed_dict={X: X_batch.reshape([-1, im_x, im_y, num_channels]),\n                               y: y_batch})\n            loss_vals.append(loss_val)\n            acc_vals.append(acc_val)\n            print(prediction, real)\n            print(\"\\rEvaluating the model: {}/{} ({:.1f}%)\".format(\n                      iteration, n_iterations_validation,\n                      iteration * 100 / n_iterations_validation),\n                  end=\" \" * 10)\n        loss_val = np.mean(loss_vals)\n        acc_val = np.mean(acc_vals)\n        print(\"\\rEpoch: {}  Val accuracy: {:.4f}%  Loss: {:.6f}{}\".format(\n            epoch + 1, acc_val * 100, loss_val,\n            \" (improved)\" if loss_val < best_loss_val else \"\"))\n\n        # And save the model if it improved:\n        if loss_val < best_loss_val:\n            save_path = saver.save(sess, checkpoint_path)\n            best_loss_val = loss_val","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"26b39569bec6dd0053482943fe4c76fb0756bfe4"},"cell_type":"markdown","source":"After 50 or so epochs of training on the color images, the accuracy on the validation set (1650 letters images) is hovering around 20%, with a loss around .58. The loss on the training set(5940 letters2 images) has decreased much faster  to around .2 to .3 depending on the image. While the accuracy is much better than chance (chance being roughly 3% accuracy) it falls short of something that could be considered consistently correct. The difference between training and validation set loss indicates the network is learning alot of the irrelevant or noisy features of the training set. Increasing the size of the training set or compressing training set images to reduce noise using autoencoders could assist in improving the network."}],"metadata":{"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}