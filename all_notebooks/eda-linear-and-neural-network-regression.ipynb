{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# EDA, Linear and Neural Network\n\nIn this notebook we will explore the data, run a linear regression and use a neural network to do a more robust regression.","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import rcParams","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-25T15:16:34.65988Z","iopub.execute_input":"2021-08-25T15:16:34.66027Z","iopub.status.idle":"2021-08-25T15:16:35.594445Z","shell.execute_reply.started":"2021-08-25T15:16:34.660185Z","shell.execute_reply":"2021-08-25T15:16:35.593572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.rcParams['figure.figsize'] = (15,16)\nsns.set_theme(style=\"darkgrid\")","metadata":{"execution":{"iopub.status.busy":"2021-08-25T15:16:36.758201Z","iopub.execute_input":"2021-08-25T15:16:36.758549Z","iopub.status.idle":"2021-08-25T15:16:36.764003Z","shell.execute_reply.started":"2021-08-25T15:16:36.75852Z","shell.execute_reply":"2021-08-25T15:16:36.762564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('../input/house-price-dataset-with-other-information/kc_house_data.csv')\ndf","metadata":{"execution":{"iopub.status.busy":"2021-08-25T15:16:37.015409Z","iopub.execute_input":"2021-08-25T15:16:37.015897Z","iopub.status.idle":"2021-08-25T15:16:37.178072Z","shell.execute_reply.started":"2021-08-25T15:16:37.015866Z","shell.execute_reply":"2021-08-25T15:16:37.177133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Checking if there is any null value, and dropping the date column, because we will not use it.","metadata":{}},{"cell_type":"code","source":"df.isnull().values.any()","metadata":{"execution":{"iopub.status.busy":"2021-08-25T15:16:37.619096Z","iopub.execute_input":"2021-08-25T15:16:37.619441Z","iopub.status.idle":"2021-08-25T15:16:37.630882Z","shell.execute_reply.started":"2021-08-25T15:16:37.619412Z","shell.execute_reply":"2021-08-25T15:16:37.629661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df  = df.drop('date', 1)\ndf","metadata":{"execution":{"iopub.status.busy":"2021-08-25T15:16:38.302834Z","iopub.execute_input":"2021-08-25T15:16:38.303327Z","iopub.status.idle":"2021-08-25T15:16:38.340715Z","shell.execute_reply.started":"2021-08-25T15:16:38.303296Z","shell.execute_reply":"2021-08-25T15:16:38.339941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's check how correlated the features are:","metadata":{}},{"cell_type":"code","source":"sns.heatmap(df.corr(), annot=True, fmt=\".2f\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-25T15:16:39.821471Z","iopub.execute_input":"2021-08-25T15:16:39.821869Z","iopub.status.idle":"2021-08-25T15:16:42.172507Z","shell.execute_reply.started":"2021-08-25T15:16:39.821837Z","shell.execute_reply":"2021-08-25T15:16:42.171728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is definitely some correlation, especially with the 'squarefeet' features.\n\n# Linear Regression\n\nOur first try to predict the house prices will be a simple Linear Regression. For this, we'll use sklearn.","metadata":{}},{"cell_type":"code","source":"from sklearn import  linear_model\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.model_selection import train_test_split\nimport xgboost","metadata":{"execution":{"iopub.status.busy":"2021-08-25T15:16:42.173679Z","iopub.execute_input":"2021-08-25T15:16:42.174069Z","iopub.status.idle":"2021-08-25T15:16:42.520369Z","shell.execute_reply.started":"2021-08-25T15:16:42.174039Z","shell.execute_reply":"2021-08-25T15:16:42.519288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = df.loc[:, df.columns != 'price']\ny = df[['price']]","metadata":{"execution":{"iopub.status.busy":"2021-08-25T15:16:42.522441Z","iopub.execute_input":"2021-08-25T15:16:42.522865Z","iopub.status.idle":"2021-08-25T15:16:42.533894Z","shell.execute_reply.started":"2021-08-25T15:16:42.522813Z","shell.execute_reply":"2021-08-25T15:16:42.532775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Split the data, with 30% test set.","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30)","metadata":{"execution":{"iopub.status.busy":"2021-08-25T15:17:25.039115Z","iopub.execute_input":"2021-08-25T15:17:25.039462Z","iopub.status.idle":"2021-08-25T15:17:25.050323Z","shell.execute_reply.started":"2021-08-25T15:17:25.039431Z","shell.execute_reply":"2021-08-25T15:17:25.049248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we will normalize the data","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaler.fit(X_train)\nX_train = scaler.transform(X_train)","metadata":{"execution":{"iopub.status.busy":"2021-08-25T15:17:26.456081Z","iopub.execute_input":"2021-08-25T15:17:26.456431Z","iopub.status.idle":"2021-08-25T15:17:26.471475Z","shell.execute_reply.started":"2021-08-25T15:17:26.456402Z","shell.execute_reply":"2021-08-25T15:17:26.470496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scaler.fit(X_test)\nX_test = scaler.transform(X_test)","metadata":{"execution":{"iopub.status.busy":"2021-08-25T15:17:27.476406Z","iopub.execute_input":"2021-08-25T15:17:27.476799Z","iopub.status.idle":"2021-08-25T15:17:27.48844Z","shell.execute_reply.started":"2021-08-25T15:17:27.476764Z","shell.execute_reply":"2021-08-25T15:17:27.487717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we fit the data to the Linear model.","metadata":{}},{"cell_type":"code","source":"regression = linear_model.LinearRegression()\nregression.fit(X_train, y_train)\n","metadata":{"execution":{"iopub.status.busy":"2021-08-25T15:17:29.413386Z","iopub.execute_input":"2021-08-25T15:17:29.413904Z","iopub.status.idle":"2021-08-25T15:17:29.427331Z","shell.execute_reply.started":"2021-08-25T15:17:29.413866Z","shell.execute_reply":"2021-08-25T15:17:29.426549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To measure the accuracy, we'll use r2 score. For more info about how it is calculated, check https://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html","metadata":{}},{"cell_type":"code","source":"y_pred = regression.predict(X_test)\nprint('Coefficient of determination: %.2f' % r2_score(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2021-08-25T15:17:31.293605Z","iopub.execute_input":"2021-08-25T15:17:31.294179Z","iopub.status.idle":"2021-08-25T15:17:31.310778Z","shell.execute_reply.started":"2021-08-25T15:17:31.294145Z","shell.execute_reply":"2021-08-25T15:17:31.309407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can try something a little more robust.\n# Neural Network\n\nNow we'll use pytorch to create a neural network to predict the housing prices.","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\nimport torch.utils.data as Data","metadata":{"execution":{"iopub.status.busy":"2021-08-25T15:17:33.176422Z","iopub.execute_input":"2021-08-25T15:17:33.176794Z","iopub.status.idle":"2021-08-25T15:17:34.078582Z","shell.execute_reply.started":"2021-08-25T15:17:33.176761Z","shell.execute_reply":"2021-08-25T15:17:34.077681Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here, we define our neural net. It has 4 layers, with 32 neurouns in each hidden layer. The activation used was Relu.","metadata":{}},{"cell_type":"code","source":"class Net(torch.nn.Module):\n    def __init__(self, n_feature, n_hidden, n_output):\n        super(Net, self).__init__()\n        self.fc1 = torch.nn.Linear(n_feature, n_hidden)\n        self.fc2 = torch.nn.Linear(n_hidden, n_hidden)\n        self.fc3 = torch.nn.Linear(n_hidden, n_hidden)\n        self.fc4 = torch.nn.Linear(n_hidden, n_output)   \n\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = F.relu(self.fc3(x))\n        x = self.fc4(x)     \n        return x","metadata":{"execution":{"iopub.status.busy":"2021-08-25T15:17:34.079808Z","iopub.execute_input":"2021-08-25T15:17:34.080081Z","iopub.status.idle":"2021-08-25T15:17:34.089689Z","shell.execute_reply.started":"2021-08-25T15:17:34.080055Z","shell.execute_reply":"2021-08-25T15:17:34.086319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We'll use MSELoss to calculate the loss and Adam as the optimizer","metadata":{}},{"cell_type":"code","source":"net = Net(n_feature=len(df.columns)-1, n_hidden=32, n_output=1)    \nprint(net)  # net architecture\noptimizer = torch.optim.Adam(net.parameters(), lr=0.01)\nloss_func = torch.nn.MSELoss()  ","metadata":{"execution":{"iopub.status.busy":"2021-08-25T15:17:34.387281Z","iopub.execute_input":"2021-08-25T15:17:34.387781Z","iopub.status.idle":"2021-08-25T15:17:34.415454Z","shell.execute_reply.started":"2021-08-25T15:17:34.387748Z","shell.execute_reply":"2021-08-25T15:17:34.414755Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Converting the data to tensors, so pytorch can use it.","metadata":{}},{"cell_type":"code","source":"X_train, X_test = torch.tensor(X_train).float(), torch.tensor(X_test).float()\ny_train, y_test = torch.tensor(y_train.values).float(), torch.tensor(y_test.values).float()","metadata":{"execution":{"iopub.status.busy":"2021-08-25T15:17:34.946582Z","iopub.execute_input":"2021-08-25T15:17:34.947098Z","iopub.status.idle":"2021-08-25T15:17:34.960969Z","shell.execute_reply.started":"2021-08-25T15:17:34.947066Z","shell.execute_reply":"2021-08-25T15:17:34.960193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now the training. We'll have 10 epochs. The code below has some comments to indicate what it is doing.","metadata":{}},{"cell_type":"code","source":"running_loss = 0\n\nfor epoch in range(10):\n    for i in range(len(X_train)):\n        prediction = net(X_train[i])     # input x and predict based on x\n        loss = loss_func(prediction, y_train[i])     # must be (1. nn output, 2. target)\n\n        optimizer.zero_grad()   # clear gradients for next train\n        loss.backward()         # backpropagation, compute gradients\n        optimizer.step()        # apply gradients\n\n        running_loss += loss.item()\n        if i % 15000 == 14999:    # print every 15000 items\n            print('[%d, %5d] loss: %.3f' %\n                  (epoch + 1, i + 1, running_loss / 15000))\n            running_loss = 0","metadata":{"execution":{"iopub.status.busy":"2021-08-25T15:17:35.660111Z","iopub.execute_input":"2021-08-25T15:17:35.660469Z","iopub.status.idle":"2021-08-25T15:21:36.208914Z","shell.execute_reply.started":"2021-08-25T15:17:35.660438Z","shell.execute_reply":"2021-08-25T15:21:36.207975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The results are better, but there is definitely some room for improvements","metadata":{}},{"cell_type":"code","source":"print('Coefficient of determination: %.2f' % r2_score(y_test, net(X_test).data.numpy()))","metadata":{"execution":{"iopub.status.busy":"2021-08-25T15:21:36.210381Z","iopub.execute_input":"2021-08-25T15:21:36.210702Z","iopub.status.idle":"2021-08-25T15:21:36.231414Z","shell.execute_reply.started":"2021-08-25T15:21:36.210671Z","shell.execute_reply":"2021-08-25T15:21:36.230666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}