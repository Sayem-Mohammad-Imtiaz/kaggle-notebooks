{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/income/train.csv')\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in list(df.columns):\n    print(col + ' --> ' + str(df[col].nunique()) + ' , data type '  + str(df[col].dtype))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_columns = [ col for col in list(df.columns) if df[col].dtype =='object']\ncat_columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nplt.figure(figsize=(15,10))\nsns.countplot(data = df, x = cat_columns[0])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,10))\nsns.countplot(data = df, x = cat_columns[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,10))\nsns.countplot(data = df, x = cat_columns[2])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,10))\nsns.countplot(data = df, x = cat_columns[3])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,10))\nsns.countplot(data = df, x = cat_columns[4])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,10))\nsns.countplot(data = df, x = cat_columns[5])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,8))\nsns.countplot(data = df, x = cat_columns[6])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,10))\n# selecting top 10 countries \ntop_10 = list(df[cat_columns[7]].value_counts().head(10).index)\nsns.countplot(data = df.loc[df[cat_columns[7]].isin(top_10)], x = cat_columns[7])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets drop all Country Except US\n\ndf = df[df['native-country']=='United-States']\n\n# drop 'native-country' column from cat_columns list \ndf= df.drop('native-country', axis = 1)\ncat_columns.remove('native-country')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#list of categorical \ncat_columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_cols = [ col for col in list(df.columns) if df[col].dtype !='object'  and  col != 'income_>50K']\nnum_cols\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_dummy = pd.get_dummies(df[cat_columns])\ndf_dummy.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(list(df_dummy.columns))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#final_dataset\n\nfinal_df = pd.DataFrame()\nfinal_df = pd.concat([df_dummy , df[num_cols] , df['income_>50K'] ] , axis = 1)\nfinal_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list(final_df.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_df.isna().sum().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split \nX = final_df.drop('income_>50K', axis =1)\ny = final_df['income_>50K']\nX_train, X_test, y_train, y_test = train_test_split( X,y , test_size = 0.3, random_state = 0) \nprint(X_train.shape)\nprint(X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training using Decision Tree Classifier \n\nfrom sklearn.tree import DecisionTreeClassifier  \nclassifier1 = DecisionTreeClassifier(criterion='gini')  \nclassifier1.fit(X_train, y_train) \n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ny_pred_1 = classifier1.predict(X_test)  \nprint(y_pred_1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score #importing accuracy_score function from sklearn.metrics package\nacc_1 = accuracy_score(y_test,y_pred_1)\nprint(\"Accuracy for Gini model {} %\".format(acc_1*100))\n\nfrom sklearn.metrics import classification_report, confusion_matrix  \nprint(confusion_matrix(y_test, y_pred_1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## [[8760 1306]\n##  [1141 1981]]\n\n\n# 8760 -->  people having less $50 k  - rightly predicted  # True Negative  ( target ==0 )\n# 1981 -->  people having more $50 k  - rightly predicted  # True Positive  ( target ==1 )\n\n# 1306 -->  people having less $50 k  - model say - they have more than $50K # False Positive \n# 1141 -->  people having more $50 k  - model say - they have less than $50K # False Negative\n\naccuracy  = (8760 + 1981)/(8760+1306+1141+1981)\nprint(accuracy)\n\n# precision  = True Positive / ( True Positive + False Positive)\nprecision = 1981/(1981 +1306)\nprint(precision)\n\n# If I predict 10 people have more than $50K salary - then 60% of that \n#i.e. 6 people will have more than $50K salary\n\n# Recall  = True Positive / (True Positive + False Negative)\nrecall = 1981/(1981+1141)\nprint(recall)\n\n# That model will catch 63% of people who have salary more than $50K \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\n\n\nclassifier2 = KNeighborsClassifier(n_neighbors= 3)  \nclassifier2.fit(X_train, y_train) \n\ny_pred_2 = classifier2.predict(X_test)  \n\nacc_2 = accuracy_score(y_test,y_pred_2)\nprint(\"Accuracy for KNN model {} %\".format(acc_2*100))\nprint(confusion_matrix(y_test, y_pred_2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nclassifier3 = LogisticRegression(random_state=0)\nclassifier3.fit(X_train, y_train) \n\ny_pred_3 = classifier3.predict(X_test)  \n\nacc_3 = accuracy_score(y_test,y_pred_3)\nprint(\"Accuracy for LR model {} %\".format(acc_3*100))\nprint(confusion_matrix(y_test, y_pred_3))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier  \n\nclassifier4 = GradientBoostingClassifier()  \nclassifier4.fit(X_train, y_train) \n\ny_pred_4 = classifier4.predict(X_test)  \n\nacc_4 = accuracy_score(y_test,y_pred_4)\nprint(\"Accuracy for Gradient Boost model {} %\".format(acc_4*100))\nprint(confusion_matrix(y_test, y_pred_4))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier  \n\nclassifier5 = RandomForestClassifier()  \nclassifier5.fit(X_train, y_train) \n\ny_pred_5 = classifier5.predict(X_test)  \n\nacc_5 = accuracy_score(y_test,y_pred_5)\nprint(\"Accuracy for Random Forest model {} %\".format(acc_5*100))\nprint(confusion_matrix(y_test, y_pred_5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets create a manual ensemble \nfrom sklearn.metrics import f1_score\n\nensemble_df = pd.DataFrame()\nensemble_df['Pred1'] = y_pred_1\nensemble_df['Pred2'] = y_pred_2\nensemble_df['Pred3'] = y_pred_3\nensemble_df['Pred4'] = y_pred_4\nensemble_df['Pred5'] = y_pred_5\nensemble_df['Sum'] = ensemble_df.sum(axis = 1)\nensemble_df['Final'] = ensemble_df['Sum'] > 2 \nensemble_df['Final'] = ensemble_df['Final'].astype(int)\n\nprint(ensemble_df.head())\n\nacc = accuracy_score(y_test,ensemble_df['Final'])\nprint(\"Accuracy for Emsemble model {} %\".format(acc*100))\nprint(confusion_matrix(y_test,ensemble_df['Final']))\nprint('f1 Score -->' ,f1_score(y_test,ensemble_df['Final']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets create a manual ensemble with weighted average\n\nensemble_df = pd.DataFrame()\nensemble_df['Pred1'] = y_pred_1\nensemble_df['Pred2'] = y_pred_2\nensemble_df['Pred3'] = y_pred_3\nensemble_df['Pred4'] = y_pred_4\nensemble_df['Pred5'] = y_pred_5\n# DT 10% , KNN 5%  LR 5%  GB 40% RF 40 % \n\nensemble_df['Sum'] = 0.1*ensemble_df['Pred1'] + 0.05*ensemble_df['Pred2'] + \\\n                     0.05*ensemble_df['Pred3'] + 0.4*ensemble_df['Pred4'] + \\\n                     0.4*ensemble_df['Pred5']\nensemble_df['Final'] = ensemble_df['Sum'] >= 0.4\nensemble_df['Final'] = ensemble_df['Final'].astype(int)\n\nprint(ensemble_df.head())\n\nacc = accuracy_score(y_test,ensemble_df['Final'])\nprint(\"Accuracy for Emsemble model {} %\".format(acc*100))\nprint(confusion_matrix(y_test,ensemble_df['Final']))\nprint('f1 Score -->' ,f1_score(y_test,ensemble_df['Final']))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Hyper Parameter Tuning of Decision Tree"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## base model & parameter grid"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Base model \ng1 = DecisionTreeClassifier()\n\nparam_grid = { \n    'criterion': ['gini', 'entropy'],\n    'max_depth': [5,10,15,20,25]\n}\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Initiate Grid Search "},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time \ngs1 = GridSearchCV(estimator=g1, param_grid=param_grid, cv= 5, verbose = 3)\ngs1.fit(X_train, y_train) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Best Parameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"gs1.best_params_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Best Model "},{"metadata":{"trusted":true},"cell_type":"code","source":"best_model_1 = gs1.best_estimator_\n\ny1 = best_model_1.predict(X_test)  \n\nacc = accuracy_score(y_test,y1)\nprint(\"Accuracy for Grid Search DT  model {} %\".format(acc*100))\n\n\nprint(confusion_matrix(y_test, y1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Hyper Parameter Tuning RandomForest "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Base model \ng2 = RandomForestClassifier()\n\nparam_grid = { \n    'criterion': ['gini', 'entropy'],\n    'n_estimators': [50,100,200],\n    'max_depth': [5,10,15],\n    'max_features': ['auto', 'sqrt', ]\n}\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time \ngs2 = GridSearchCV(estimator=g2, param_grid=param_grid, cv= 5, verbose = 3)\ngs2.fit(X_train, y_train) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gs2.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_model_2 = gs2.best_estimator_\n\ny2 = best_model_2.predict(X_test)  \n\nacc = accuracy_score(y_test,y2)\nprint(\"Accuracy for Grid Search RF  model {} %\".format(acc*100))\n\n\nprint(confusion_matrix(y_test, y2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Hyper Parameter Tuning of Gradient Boosting"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Base model \ng3 = GradientBoostingClassifier()\n\nparam_grid = { \n    'n_estimators': [50,100,200],\n    'max_depth': [5,10,15],\n}\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time \ngs3 = GridSearchCV(estimator=g3, param_grid=param_grid, cv= 5, verbose = 3)\ngs3.fit(X_train, y_train) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gs3.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_model_3 = gs3.best_estimator_\n\ny3 = best_model_3.predict(X_test)  \n\nacc = accuracy_score(y_test,y3)\nprint(\"Accuracy for Grid Search Gradient Boosting  model {} %\".format(acc*100))\n\n\nprint(confusion_matrix(y_test, y3))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Implementation of SVM \n\n# from sklearn.svm import SVC\n# svc_clf = SVC(C= 1.0 , kernel='poly')\n\n# svc_clf.fit(X_train, y_train)\n# svc_pred = svc_clf.predict(X_test)  \n\n# acc_svc = accuracy_score(y_test,svc_pred)\n# print(\"Accuracy for Support Vector Model {} %\".format(acc_svc*100))\n# print(confusion_matrix(y_test, svc_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Scaling  \n\nfrom sklearn.preprocessing import StandardScaler\nsc =StandardScaler()\n\nsc.fit(X_train)\nXS_train = sc.transform(X_train)\nXS_test = sc.transform(X_test)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"XS_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Implementation of SVM \n\nfrom sklearn.svm import SVC\nsvc_clf = SVC(C= 1.0 , kernel='poly')\n\nsvc_clf.fit(XS_train, y_train)\nsvc_pred = svc_clf.predict(XS_test)  \n\nacc_svc = accuracy_score(y_test,svc_pred)\nprint(\"Accuracy for Support Vector Model {} %\".format(acc_svc*100))\nprint(confusion_matrix(y_test, svc_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\nsvc_clf = SVC(C= 1.0 , kernel='rbf')\n\nsvc_clf.fit(XS_train, y_train)\nsvc_pred = svc_clf.predict(XS_test)  \n\nacc_svc = accuracy_score(y_test,svc_pred)\nprint(\"Accuracy for Support Vector Model {} %\".format(acc_svc*100))\nprint(confusion_matrix(y_test, svc_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Data set Info  - check missing values \n# Visulaization  - \n# Transformation  - Dummies \n# Test - Train Split \n# Modeling\n\n# R2 and adj R2  ","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}