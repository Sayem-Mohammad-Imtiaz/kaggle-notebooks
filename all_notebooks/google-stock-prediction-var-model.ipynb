{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom statsmodels.tsa.vector_ar.var_model import VAR\nimport seaborn as sns\nimport sklearn\nfrom sklearn.metrics import r2_score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3 style=\"text-align: center;\"> Time series analysis and prediction </h3>\n<img src=\"https://www.seebiz.eu/img/394f9846c06e08fbb7e116315e63f909.jpg\" width=\"400\" height= \"400\"> ","metadata":{}},{"cell_type":"markdown","source":"<h3>Data investigation</h3>  ","metadata":{}},{"cell_type":"code","source":"data= pd.read_csv('../input/google-stock-prediction/GOOG.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['data']= pd.to_datetime(data['date'])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dates=[]\nfor i in range(len(data)):\n    dates.append(data['date'].iloc[i][0:10]) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.drop(['symbol'], axis= 1, inplace= True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['date']= pd.to_datetime(dates)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## checking the duplicated stime stamps:","metadata":{}},{"cell_type":"code","source":"data['date'].duplicated().sum()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Finding missting time stamps and filling them using interpolation:","metadata":{}},{"cell_type":"code","source":"r = pd.date_range(start=data.date.min(), end=data.date.max())\ndata= data.set_index('date').reindex(r).fillna(np.nan).rename_axis('date').reset_index()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"data.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.drop(['date', 'data'], axis=1, inplace= True)\ncl_names= list(data.columns)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cl_names","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Interpolating the NAN values:","metadata":{}},{"cell_type":"code","source":"for i in cl_names:\n    data[i]= data[i].interpolate()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Correlation:","metadata":{}},{"cell_type":"code","source":"corr = data.corr()\nmask = np.triu(np.ones_like(corr, dtype=bool))\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Stationary check:\nstationay check is done using Dicky fuller test.","metadata":{}},{"cell_type":"code","source":"from statsmodels.tsa.stattools import adfuller\nseries1= data['open']\nresult = adfuller(series1)\nprint('ADF Statistic: %f' % result[0])\nprint('p-value: %f' % result[1])\nprint('Critical Values:')\nfor key, value in result[4].items():\n    print('\\t%s: %.3f' % (key, value))\n \nprint('**********************')\nseries2= data['close']\nresult = adfuller(series2)\nprint('ADF Statistic: %f' % result[0])\nprint('p-value: %f' % result[1])\nprint('Critical Values:')\nfor key, value in result[4].items():\n    print('\\t%s: %.3f' % (key, value))\n\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The results of test shows that data is non-stationary.","metadata":{}},{"cell_type":"markdown","source":"<h3> Data Visualization: <\\3h>","metadata":{}},{"cell_type":"code","source":"# I am visualizing the target values \nplt.figure(figsize=(15, 5))\nplt.subplot(1,2,1)\nplt.plot(data['close']) \nplt.title('close values')\nplt.xlabel('close')\nplt.subplot(1,2,2)\nplt.plot(data['open'])\nplt.title('open values')\nplt.xlabel('open')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### train- test split:","metadata":{}},{"cell_type":"code","source":"train = data.iloc[0:int(0.8*len(data)), :]\ntest= data.iloc[int(0.8*len(data)):, :]\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Droping the constant values as they will be trouble some for Var modeling \ntrain.drop(['divCash', 'splitFactor'], axis=1, inplace= True)\ntest.drop(['divCash', 'splitFactor'], axis=1, inplace= True)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model definition and Training:","metadata":{}},{"cell_type":"code","source":"def VarForecasting(Actual):\n        model = VAR(Actual)\n        model_fit = model.fit()\n        prediction = model_fit.forecast(model_fit.y, steps=10) # predicting all the next 10 values at each step\n        return np.array(prediction)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.drop(['divCash', 'splitFactor'], axis=1, inplace= True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"close_predictions=[]\nopen_predictions=[]\nfor timepoint in range(0, len(test)):\n    Actual_train = data.iloc[timepoint:timepoint + len(train)]\n    Prediction = VarForecasting(Actual_train)\n    close_predictions.append(Prediction[:,0])\n    open_predictions.append(Prediction[:,3])\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"close_predictions[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.array(close_predictions).shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Prediction visualization and evaluation ","metadata":{}},{"cell_type":"markdown","source":"One chunck of actual data and corresponding model prediction is plotted to visualize the model performance.","metadata":{}},{"cell_type":"code","source":"# at each step, the te\np_close= [close_predictions[i][0] for i in range(len(close_predictions))]\np_open= [open_predictions[i][0] for i in range(len(open_predictions))]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(15, 5))\nplt.subplot(1,2,1)\nplt.plot(np.array(test['close'])[0:100])\nplt.plot(p_close[0:100],'r')\nplt.title('close values')\nplt.xlabel('close')\nplt.subplot(1,2,2)\nplt.plot(np.array(test['open'])[0:100])\nplt.plot(p_open[0:100], 'r' )\nplt.title('open values')\nplt.xlabel('open')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\nimport math \nprint(math.sqrt(mean_squared_error(np.array(test['open']),p_open )))\nprint(math.sqrt(mean_squared_error(np.array(test['close']),p_close )))\n\n## R2 score:\nprint('R2 score of the Close series',r2_score(np.array(test['close']),p_close ))\nprint('R2 score of the open series',r2_score(np.array(test['open']),p_close ))","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}