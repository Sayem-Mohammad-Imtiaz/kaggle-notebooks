{"cells":[{"metadata":{},"cell_type":"markdown","source":"* Dataset contains 4 million of the reddit comments, 2 million of which are the lowest scored (highly downvoted), and 2 million of which are the highest scored (highly upvoted).\n* We can approach this as regression, scaled regression (control for surrounding variables such as parent post, and author), and/or classification (e.g. top/bottom rank).\n* NLP","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split\nfrom catboost import Pool, cv, CatBoostRegressor,CatBoostClassifier\nfrom sklearn.metrics import classification_report,mean_squared_error\n\nimport shap\nshap.initjs()\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"columns = ['parent_id', 'subreddit_id', 'text', 'score',\n#            'ups', 'parent_ups',  ## these appear to be redundnat with score, especially in this dataset (normally would allow info about upvotes * downvotes)\n       'author', 'controversiality',  'parent_text',\n       'parent_score', 'parent_author',\n       'parent_controversiality']\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# we'll use a sample of the data to speed things up\ndf_pos = pd.read_csv(\"../input/reddit-comment-score-prediction/comments_positive.csv\",nrows=2e5, usecols=columns)\ndf_pos[\"binary_label\"] = 1\ndf_neg = pd.read_csv(\"../input/reddit-comment-score-prediction/comments_negative.csv\",nrows=2e5, usecols=columns)\ndf_neg[\"binary_label\"] = 0\n\ndf = pd.concat([df_pos,df_neg], ignore_index=True).drop_duplicates(['text', 'parent_text','binary_label']).sample(frac=1) # concat and drop duplicates. We'll still have duplicate texts, but this should help clean  bot autoposts at least\n\ndf = df.loc[(~df.text.isna()) & (df['parent_text'].notnull())] # drop empty comments\ndel df_pos, df_neg\n\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## feature to mark parent or post deleted (rather than getting it via proxy, since [\"deleted\"] is very common):\ndf[\"deleted\"] = (df[\"parent_author\"].str.contains(\"deleted\",case=False) | df[\"author\"].str.contains(\"deleted\",case=False)).astype(int)\n\ndf[\"deleted\"].sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.nunique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that most authors (and parent posts) appear just once in this dataset. Thus, encoding `author` as a categorical variable won't help much as a feature, but count encoding may still be helpful.\n\n* We'll replace these high cardinality features with counts. (I note that this is not the same as the author's \"real\" amount of posting, since the sampliong in this dataset is very biased and incomplete).\n    \nOn the other hand, there are far fewer subreddits! It's quite possible that the ranking can be determined purely by those without the endogenous/text features. \nWe'll test this later with a simple model without text","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"author\"].value_counts().nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(df[\"author\"].value_counts()<2).sum() # majority of authors appear just once in the data. ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### we could replace only authors with less than 3 posts with their count, but there are so few that I won't bother. \n### https://stackoverflow.com/questions/37239627/change-values-in-pandas-dataframe-according-to-value-counts\n## # df.where(df.apply(lambda x: x.map(x.value_counts()))>=2, \"other\")\n\n## replace values with their counts/occurences #\ndf[\"author\"] = df[\"author\"].map(df[\"author\"].value_counts())\ndf[\"parent_id\"] = df[\"parent_id\"].map(df[\"parent_id\"].value_counts())\ndf[\"parent_author\"] = df[\"parent_author\"].map(df[\"parent_author\"].value_counts())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Drop (or replace) very rare subreddits (easier for subsequent analyses)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# df[\"subreddit_id\"].where(df[\"subreddit_id\"].map(df[\"subreddit_id\"].value_counts())<=5,df[\"subreddit_id\"].map(df[\"subreddit_id\"].value_counts()),df[\"subreddit_id\"]) ## todo - fix for replacing rare subreddits (if any)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Subreddit level features\n* Note that score based features will be a big target leak if few posts per subreddit (and it is still technically a bit leaky)\n* The models would learn this by itself, but it's nice to have it explicitly (more interpretable)\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"subreddit_counts\"] = df[\"subreddit_id\"].map(df[\"subreddit_id\"].value_counts())\nprint(\"min subreddit occurence before filtering\",df[\"subreddit_counts\"].min())\n\ndf = df.loc[df[\"subreddit_counts\"]>40 ] # drop posts from very rare subreddits (a minor fraction)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Ideally we should combine these, but that means dropping duplicate parent posts then merging. This is good enough for now. \ndf[\"subreddit_mean_post_score\"] = df.groupby(\"subreddit_id\")[\"score\"].transform(\"mean\")\ndf[\"subreddit_mean_post_parent_score\"] = df.groupby(\"subreddit_id\")[\"parent_score\"].transform(\"mean\")\n\n# df[\"subreddit_mean_binary_label\"] = 100*df.groupby(\"subreddit_id\")[\"binary_label\"].transform(\"mean\")\n\n## we could also normalize/z-score by sub-reddit as a target transforamtion!! \n\n# sum controversiality from post and parent post. silly but good enough for now\ndf[\"subreddit_mean_controversiality\"] = df.groupby(\"subreddit_id\")[\"controversiality\"].transform(\"mean\")\ndf[\"subreddit_mean_controversiality\"] = 100*(df[\"subreddit_mean_controversiality\"] + df.groupby(\"subreddit_id\")[\"parent_controversiality\"].transform(\"mean\"))/2\n\ndf[\"subreddit_mean_deleted\"] = 100*df.groupby(\"subreddit_id\")[\"deleted\"].transform(\"mean\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Naive model - no text features\n* We'll drop leak columns though\n* We could use this model to \"scale\" the score, and predict the residual score using text features! ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_cols = [\"subreddit_id\"]\ntext_cols = [ 'text', 'parent_text',]\n\n## columns to drop \nleak_cols = ['score', 'binary_label'] # I'd be suspicious of 'parent_score', but it can be left in, arguably\n\ntarget_col = ['binary_label'] ###  'binary_label' if we want binary classification by dataset's authgor's prior ranking","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.drop(text_cols+leak_cols,axis=1)\ny = df[target_col]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_pool = Pool(X_train,y_train,\n                 cat_features=categorical_cols,\n                 )\n\n\ntest_pool = Pool(X_test,y_test,\n                 cat_features=categorical_cols,\n                 )\n\nmodel = CatBoostClassifier(iterations=150,\n#                             task_type=\"GPU\",\n                           custom_metric=['Logloss',\"Precision\",\n                                          'AUC'])\n\n## we can accellerate training greatly if we run with GPU. \n### using early stopping with the test set and also evaluating on it is \"cheating\", but we don't care about exact numbers as much here. \nmodel.fit(train_pool,\n          eval_set=test_pool,\n          use_best_model=True,\n          verbose=False,plot=True,\n         ) # use less iterations to speed things up , especially when not running on GPU\n\nprint(model.get_best_score())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that the model effectively has near perfect seperation of high vs low ranking posts, without looking at actual post content! \n\n### Naive model's SHAP  feature importance\n* We'll look at the SHAP values for feature importance","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"explainer = shap.TreeExplainer(model)\nshap_values = explainer.shap_values(test_pool) # get explanations on test data. Could do so on the train data\n\n# feature importance plot\nshap.summary_plot(shap_values, X_test, plot_type=\"bar\")\n\n# summarize the effects of all the features\nshap.summary_plot(shap_values, X_test)\n\n### grey is missing values, although we are not missing any subreddit_ids. It's a bug in catboost/shap. #df.subreddit_id.isna().sum() ## =0","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Naive model conclusions:\n* There we have it, the score of the parent post is massively influential on the score on the comment itself (and it's a direct correlation). \n* To a degree, this reflects a leak in time as well, since a popular post will garner more views -> likes. We're peeking at the end. \n* The proper way around this would be to build a model that looked at a specific point in time, and not post-hoc. We lack that. \n\nWe may want to simply drop the `parent_score`, even from target rescaling purposes, due to how \"leaky\" and correlated it is.\n\n - We can also look at it's effects when predicting the exact score (rather than a binary target), but there's no reason to assume that'll change much. It'll just mean we'll have more surrounding noise.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* Let's build 1 more naive model, without the direct parent score","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_pool = Pool(X_train.drop(['parent_id','parent_score'],axis=1),y_train,\n                 cat_features=categorical_cols)\n\n\ntest_pool = Pool(X_test.drop(['parent_id','parent_score'],axis=1),y_test,\n                 cat_features=categorical_cols)\n\nmodel2 = CatBoostClassifier(iterations=100,\n                           custom_metric=['Logloss',\"Precision\",\n                                          'AUC'])\nmodel2.fit(train_pool,\n          eval_set=test_pool,\n          use_best_model=True,\n          verbose=False,plot=True,\n         ) \n\nprint(model2.get_best_score())\n\nexplainer = shap.TreeExplainer(model2)\nshap_values = explainer.shap_values(test_pool) # get explanations on test data. Could do so on the train data\n\n# feature importance plot\nshap.summary_plot(shap_values, X_test.drop(['parent_id','parent_score'],axis=1), plot_type=\"bar\")\n\n# summarize the effects of all the features\nshap.summary_plot(shap_values, X_test.drop(['parent_id','parent_score'],axis=1))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"we still have a strong \"prior\" predictive model without the parent (/`parent_score`) features, but it's  a lot \"weaker\" now","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Text features engineering\n* We can choose to add features from the parent post +- \n* Catboost does word count bag of words +- some target encoding \"out of the box\"\n    * We may want to add character level n-grams, since our data has lots of misspellings (and there are important features such as hashtags, punctuations, counts of `/r` etc', that would be caught in a generalizable way by these feats). \n    \n* I'll also add some high level text features for interpretation\n* I'll also be dropping the parent score feature - it's too strong/leaky in my opinion :)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_cols = [\"subreddit_id\"]\ntext_cols = [ 'text']#[ 'text', 'parent_text',] ## we won't featurize the parent text for now. overkill (and can let leaks thorough, in the form of popular/upvoted parent topics)\n\n## columns to drop \ndrop_cols = ['score', 'binary_label','parent_score', 'parent_text'] # I'd be suspicious of , but it can be left in, arguably\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nimport string\nfrom string import punctuation\nfrom nltk import pos_tag\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import TweetTokenizer\nstop_words = set(stopwords.words('english')) \n\n# !pip install TextBlob\nfrom textblob import TextBlob\n\n# functions to get polatiy and subjectivity of text using the module textblob\ndef get_polarity(text):\n    try:\n        textblob = TextBlob(unicode(text, 'utf-8'))\n        pol = textblob.sentiment.polarity\n    except:\n        pol = 0.0\n    return pol\n\ndef get_subjectivity(text):\n    try:\n        textblob = TextBlob(unicode(text, 'utf-8'))\n        subj = textblob.sentiment.subjectivity\n    except:\n        subj = 0.0\n    return subj\n\n\ndef tag_part_of_speech(text):\n    text_splited = text.split(' ')\n    text_splited = [''.join(c for c in s if c not in string.punctuation) for s in text_splited]\n    text_splited = [s for s in text_splited if s]\n    pos_list = pos_tag(text_splited)\n    noun_count = len([w for w in pos_list if w[1] in ('NN','NNP','NNPS','NNS')])\n    adjective_count = len([w for w in pos_list if w[1] in ('JJ','JJR','JJS')])\n    verb_count = len([w for w in pos_list if w[1] in ('VB','VBD','VBG','VBN','VBP','VBZ')])\n    return[noun_count, adjective_count, verb_count]\n\ndef text_features(df:pd.DataFrame,text:str=\"text\", get_pos_feats=False,get_textblob_sentiment=True) -> pd.DataFrame:\n    \"\"\"\n    Extract and add in place many text/NLP features on a pandas dataframe for a given column.\n    Functions are generic, but some were basedon  :  https://www.kaggle.com/shivamb/extensive-text-data-feature-engineering , \n    https://www.kaggle.com/shaz13/feature-engineering-for-nlp-classification\n    I modified to use vectorized functions - many, many times faster, Can be optimized further easily.\n    \"\"\"\n\n\n    # https://www.kaggle.com/shaz13/feature-engineering-for-nlp-classification\n    df[f'{text}_char_count'] = df[text].str.len()\n    df[f'{text}_num_words'] = df[text].str.split().str.len()\n\n    df['capitals'] = df[text].apply(lambda comment: sum(1 for c in comment if c.isupper()))\n    df['caps_vs_length'] = df.apply(lambda row: float(row['capitals'])/float(row[f'{text}_char_count']),axis=1)\n    df['num_exclamation_marks'] = df[text].str.count('!')\n    df['num_question_marks'] = df[text].str.count('\\?')\n    df['num_punctuation'] = df[text].apply(lambda comment: sum(comment.count(w) for w in '.,;:'))\n    df['num_symbols'] = df[text].apply(lambda comment: sum(comment.count(w) for w in r'*&$%/:;'))\n\n    df['num_unique_words'] = df[text].apply(lambda comment: len(set(w for w in comment.split())))\n    df['words_vs_unique'] = df['num_unique_words'] / df[f'{text}_num_words']\n    df['num_smilies'] = df[text].apply(lambda comment: sum(comment.count(w) for w in (':-)', ':)', ';-)', ';)')))\n    df['num_sad'] = df[text].apply(lambda comment: sum(comment.count(w) for w in (':-<', ':()', ';-()', ';(')))\n\n#     df['char_count'] = df['text'].apply(len)\n#     df['word_count'] = df['text'].apply(lambda x: len(x.split()))\n    df['word_density'] = df[f'{text}_char_count'] / (df[f'{text}_num_words']+1)\n    df['punctuation_count'] = df['text'].apply(lambda x: len(\"\".join(_ for _ in x if _ in punctuation))) \n\n    df['upper_case_word_count'] = df[text].apply(lambda x: len([wrd for wrd in x.split() if wrd.isupper()]))\n    df['stopword_count'] = df[text].apply(lambda x: len([wrd for wrd in x.split() if wrd.lower() in stop_words]))\n#     df['title_word_count'] = df['text'].apply(lambda x: len([wrd for wrd in x.split() if wrd.istitle()]))\n    df[\"count_words_title\"] = df[text].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n    df[\"mean_word_len\"] = df[text].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n    df['punct_percent']= df['num_punctuation']*100/df[f'{text}_num_words']\n    \n    if get_textblob_sentiment:\n        df['polarity'] = df[text].apply(get_polarity)\n        df['subjectivity'] = df[text].apply(get_subjectivity)\n    \n    if get_pos_feats:\n        df['nouns'], df['adjectives'], df['verbs'] = zip(*df[text].apply(\n            lambda comment: tag_part_of_speech(comment)))\n        df['nouns_vs_length'] = df['nouns'] / df[f'{text}_char_count']\n        df['adjectives_vs_length'] = df['adjectives'] / df[f'{text}_char_count']\n        df['verbs_vs_length'] = df['verbs'] /df[f'{text}_char_count']\n        df['nouns_vs_words'] = df['nouns'] / df[f'{text}_num_words']\n        df['adjectives_vs_words'] = df['adjectives'] / df[f'{text}_num_words']\n        df['verbs_vs_words'] = df['verbs'] / df[f'{text}_num_words']\n\n        df.drop(['nouns','adjectives','verbs'],axis=1,inplace=True) # drop the count of POS, keep only the percentages. Can change to keep them..\n\n        \n    df[\"ends_on_alphanumeric\"] = df[text].str.strip().str[-1].str.isalpha() # does word end on alphanumeric, vs \".\". Interesting for comments. Note the strip. \n        \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ndf = text_features(df, get_pos_feats=False)\n\ndf.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.drop(drop_cols,axis=1)\ny = df[target_col]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_pool = Pool(X_train,y_train,\n                 cat_features=categorical_cols,\n                  text_features=text_cols\n                 )\n\ntest_pool = Pool(X_test,y_test,\n                 cat_features=categorical_cols,\n                 text_features=text_cols\n                 )\n\n## catboost text featurizer params (e.g. tokenizer, n-grams, BoW..) - https://catboost.ai/docs/features/text-features.html\n\nmodel = CatBoostClassifier(iterations=350,custom_metric=['Logloss',\"Precision\", 'AUC'])\n\nmodel.fit(train_pool,\n          eval_set=test_pool,\n          use_best_model=True,\n          verbose=False,plot=True,\n         )\n\nprint(model.get_best_score())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"explainer = shap.TreeExplainer(model)\nshap_values = explainer.shap_values(test_pool) # get explanations on test data\n\n# feature importance plot\nshap.summary_plot(shap_values, X_test, plot_type=\"bar\")\n\n# summarize the effects of all the features\nshap.summary_plot(shap_values, X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## \"Scaled\" Regression\n* Let's consider the actual score as a target. We'll scale by the subreddits, to help account for the bias of popular subreddits (albeit, popular subreddits may have more better writing, so it's hard to control for this confounder!)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"display(df.score.describe())\ndf.score.hist();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display(df[\"subreddit_mean_post_score\"].describe())\ndf[\"subreddit_mean_post_score\"].hist();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## what are subreddits with a mean negative score?\n\ndf.loc[df[\"subreddit_mean_post_score\"]<-20].drop_duplicates([\"parent_id\",\"subreddit_id\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}