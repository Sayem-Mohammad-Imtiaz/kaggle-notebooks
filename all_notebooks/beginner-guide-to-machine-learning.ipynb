{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# LSVT Voice Rehabilitation\n**Abstract: 126 samples from 14 participants, 309 features. Aim: assess whether voice rehabilitation treatment lead to phonations considered 'acceptable' or 'unacceptable' (binary class classification problem).**<br>\n**Original dataset :- https://archive.ics.uci.edu/ml/datasets/LSVT+Voice+Rehabilitation**<br>","metadata":{}},{"cell_type":"markdown","source":"### Lets Start with importing our neccessary libraries\n**That includes:**:-<br>\n    *1.) Tensorflow*<br>\n    *2.) Pandas*<br>\n    *3.) Numpy*<br>\n    *4.) Matplotlib*<br>\n    *5.) Seaborn*<br>\n    *6.) Svm classifier*<br>\n    *7.) Decision tree cassifier*<br> \n    *8.) Random Forest classifier.*<br>\n    *9.) Pandas profiling*<br>\n    etc","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport pandas_profiling \nimport datetime as dt\nimport pandas as pd\nimport numpy as np\nimport graphviz\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score\nfrom tensorflow import keras\nfrom sklearn import tree\nfrom sklearn import svm\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\n%matplotlib inline","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Lets import our dataset data.csv\n**Link:-** <a>https://www.openml.org/d/1484 </a><br>\n**I have used dataset from openml because is has a classification column so that we can train and test our model, The testing on this dataset will also work for the original raw dataset from UCI Machine Learning LSVT dataset.**<br>","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('../input/lsvt-voice-rehabilation-dataset/data.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Lets looks at our Target feature","metadata":{}},{"cell_type":"code","source":"df['Class']","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Create a Correlation matrix with our target feature to see on which feature our target feature is dependent","metadata":{}},{"cell_type":"code","source":"#plt.figure(figsize=(7,5))\ncor = df.corr()\ncor\n#heat_map = sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)\n#plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cor_target = abs(cor[\"Class\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### The Relevant Features of our Target colums are:-\n**V80**<br>\n**V82**<br>\n**V84**<br>\n**V85**<br>\n**V86**<br>\n**V153**<br>\n***Note :- The feature Class is itself only thats why correlation is 1.***<br>","metadata":{}},{"cell_type":"code","source":"relevant_features = cor_target[cor_target>0.48]\nrelevant_features","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Create a new dataframe from the selected features","metadata":{}},{"cell_type":"code","source":"data = [df['V80'],df['V82'],df['V84'],df['V85'],df['V86'],df['V153'],df['Class']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"head = ['V80','V82','V84','V85','V86','V153','Class']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_df = pd.concat(data,axis=1,keys=head)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_df.describe","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**There are no missing value in our dataset**","metadata":{}},{"cell_type":"code","source":"new_df.isna().sum() #no missing values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Lets plot a box plot to see how the values in our dataset is distributed<br>\n**As we can see the values are not evenly distributed some have very large number of negative values like v153 where as some has values between 0 and 1. So we need to scale them up so that we can train our model efficiently**","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(18,9))\nnew_df[['V80','V82','V84','V85','V86','V153','Class']].boxplot()\nplt.title(\"Numerical variables in Our Dataset\", fontsize=20)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**I am here performing min-max Normalization**<br>\n**For more info:- <a>https://www.geeksforgeeks.org/ml-feature-scaling-part-2/</a>**","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_df['V86'] = (new_df['V86']- new_df['V86'].min())/(new_df['V86'].max() - new_df['V86'].min())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_df['V85'] = (new_df['V85']- new_df['V85'].min())/(new_df['V85'].max() - new_df['V85'].min())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_df['V84'] = (new_df['V84']- new_df['V84'].min())/(new_df['V84'].max() - new_df['V84'].min())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_df['V82'] = (new_df['V82']- new_df['V82'].min())/(new_df['V82'].max() - new_df['V82'].min())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_df['V80'] = (new_df['V80']- new_df['V80'].min())/(new_df['V80'].max() - new_df['V80'].min())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_df['V153'] = (new_df['V153']- new_df['V153'].min())/(new_df['V153'].max() - new_df['V153'].min())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**For our Target class we need to make it in binary form ie in term of 0 and 1. 0 for false and 1 for true.**","metadata":{}},{"cell_type":"code","source":"new_df['Class'] = new_df['Class'].factorize()[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Lets see the box plot after Normalization :-\n**Now the Values are Betweeen 0 and 1.**","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(18,9))\nnew_df[['V80','V82','V84','V85','V86','V153','Class']].boxplot()\nplt.title(\"Numerical variables in Our Dataset\", fontsize=20)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Using pandas profiling :-\n**Pandas profiling can be used for expolration data analysis, It plot correlation matrix, gives you the valuable information about the feature and all**<br>\n**For more info read this blog :- https://towardsdatascience.com/exploratory-data-analysis-with-pandas-profiling-de3aae2ddff3**","metadata":{}},{"cell_type":"code","source":"start_time = dt.datetime.now()\nprint(\"Started at \", start_time)\nreport = pandas_profiling.ProfileReport(new_df)\nreport","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Now lets extract our feature which are :- 'V80', 'V82', 'V84', 'V85', 'V86', 'V153'.**","metadata":{}},{"cell_type":"code","source":"X= new_df.drop(['Class'],axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Our label is our target class ie 'Class'.**","metadata":{}},{"cell_type":"code","source":"y = new_df['Class']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Hyperparameter Tuning (hpt is done to get the optimal parameters so that our model can work efficiently as in sklearn you have so many parameter so it become difficult to get the optimal parameter below code can also be used as a template for hpt):-","metadata":{}},{"cell_type":"code","source":"model_params = {\n    'svm': {\n        'model': svm.SVC(gamma='auto'),\n        'params' : {\n            'C': [1,5,20],\n            'kernel': ['rbf','linear'],\n        }  \n    },\n    'random_forest': {\n        'model': RandomForestClassifier(),\n        'params' : {\n            'max_depth':[1,5,9],\n            'n_estimators': [1,5,20,100],\n        }\n    },\n    'logistic_regression' : {\n        'model': LogisticRegression(solver='liblinear',multi_class='auto'),\n        'params': {\n            'C': [1,2,5,10,15]\n        }\n    }\n}\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## The Optimal Parameters are(I have used gridsearcv as datset has only 126 instances if dataset is large one can use randomizedsearchcv):-","metadata":{}},{"cell_type":"code","source":"scores = []\n\nfor model_name, mp in model_params.items():\n    clf =  GridSearchCV(mp['model'], mp['params'], cv=5, return_train_score=False)\n    clf.fit(X, y)\n    scores.append({\n        'model': model_name,\n        'best_score': clf.best_score_,\n        'best_params': clf.best_params_,\n    })\n    \ndf = pd.DataFrame(scores,columns=['model','best_score','best_params'])\ndf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Training(as it is a classification problem) :- <br>\n### 1.) Using Random forest classifier <br>\n### 2.) Using Logistic Regression<br>\n### 3.) Using SVM <br>","metadata":{}},{"cell_type":"markdown","source":"### Random Forest model performance using cross_val_score :-","metadata":{}},{"cell_type":"code","source":"scores = cross_val_score(RandomForestClassifier(max_depth=9,n_estimators=100),X, y,cv=5)\nprint(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Logistic Regression model performance using cross_val_score :-","metadata":{}},{"cell_type":"code","source":"scores = cross_val_score(LogisticRegression(solver='liblinear',multi_class='auto',C=1),X, y,cv=5)\nprint(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### SVM performance using cross_val_score","metadata":{}},{"cell_type":"code","source":"scores = cross_val_score(svm.SVC(gamma='auto',kernel='linear',C=20),X, y,cv=5)\nprint(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**As it is clearly visivle that Random Forest is best classifier for our prediction**","metadata":{}},{"cell_type":"code","source":"x_train,x_test,y_train,y_test=train_test_split( X, y, test_size=0.3, random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = RandomForestClassifier(max_depth=9,n_estimators=100)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit(x_train,y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = model.predict(x_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracy_score(y_test, y_pred)*100","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cm = tf.math.confusion_matrix(labels=y_test,predictions=y_pred)\nimport seaborn as sns\nplt.figure(figsize =(5,4))\nsns.heatmap(cm,annot=True,fmt = 'd')\nplt.xlabel('Predicted')\nplt.ylabel('Truth')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conclusion:-\n**Random Forest Classifier is the best fitted classifier for our prediction.It has cross_val_score greater than svm and logistic regression.<br>On training and testing on our dataset, Our model gave a accuracy of greater than 90%.**<br>\n#### Key learning :-\n**Learnt about confusion matrix,classifiers,feature extraction techniques,how to process raw data,feature scaling,how to make model with better accuracy using data preprocessing and pandas profiling.**","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}