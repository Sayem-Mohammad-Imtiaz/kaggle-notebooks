{"cells":[{"metadata":{},"cell_type":"markdown","source":"<center><img src='https://thecitymagazineelp.com/wp-content/uploads/2019/03/AdobeStock_103569152-5BConverted5D-1000.jpg' height=350></center>\n<p>\n<h1><center> Normal VS Cataract VS Glaucoma </center></h1>"},{"metadata":{},"cell_type":"markdown","source":"## 1.Objectifs\n\n* L’objectif ici est de développer un réseau CNN en se basant sur des architecture différentes (vgg16,vgg19,EfficientNetB7,DenseNet201) pour la classification de deux maladie (Normal VS Cataract VS Glaucoma)"},{"metadata":{},"cell_type":"markdown","source":"## 2. Préparation de la base de données\n\n### 2.1 importer les bibliothèques nécessaires"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport random, re, math\nimport tensorflow as tf, tensorflow.keras.backend as K\nimport tensorflow_addons as tfa\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras import optimizers\nfrom kaggle_datasets import KaggleDatasets\nfrom tensorflow.keras.models import Sequential\nimport tensorflow.keras.layers as L\nfrom tensorflow.keras.applications import ResNet152V2, InceptionResNetV2, InceptionV3, Xception, VGG19\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D,GlobalMaxPooling2D\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau , EarlyStopping , ModelCheckpoint , LearningRateScheduler\nfrom keras import regularizers\n\nimport matplotlib.pyplot as plt\n\n!pip install efficientnet\nimport efficientnet.tfkeras as efn","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.2 Configuration de tpu"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"AUTO = tf.data.experimental.AUTOTUNE\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy()\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"GCS_DS_PATH = KaggleDatasets().get_gcs_path('ocular-disease-recognition-odir5k')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/new-df-csv-oc/new_df_oc.csv')\ntrain_paths = train.filename.apply(lambda x: GCS_DS_PATH+ '/ODIR-5K/ODIR-5K/Training Images/' + x).values\ntrain_labels = train.target.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train=train.drop(columns=['D','M','H','A','O'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.3 Normal Vs Cataract"},{"metadata":{"trusted":true},"cell_type":"code","source":"train=train[((train['N']== 1) | (train['C'] == 1)| (train['G'] == 1))]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.4 Diviser notre dataset en 80% l'entraînement et 20% pour le test"},{"metadata":{"trusted":true},"cell_type":"code","source":"train,valid = train_test_split(train,test_size = 0.2,random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3 Hyperparamètre"},{"metadata":{"trusted":true},"cell_type":"code","source":"BATCH_SIZE = 8* strategy.num_replicas_in_sync\nimg_size = 512\nEPOCHS = 20\nSEED = 42","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4. Prétraitement des données"},{"metadata":{"trusted":true},"cell_type":"code","source":"def decode_image(filename, label=None, image_size=(img_size,img_size)):\n    bits = tf.io.read_file(filename)\n    image = tf.image.decode_jpeg(bits, channels=3) \n    image = tf.image.resize(image, image_size)\n    image = tf.cast(image, tf.float32)\n    image = tf.image.per_image_standardization(image)\n    if label is None:\n        return image\n    else:\n        return image, label\n    \ndef preprocess(df,test=False):\n    paths = df.filename.apply(lambda x: GCS_DS_PATH + '/ODIR-5K/ODIR-5K/Training Images/' + x).values\n    labels = df.loc[:, ['N', 'C', 'G']].values\n    if test==False:\n        return paths,labels\n    else:\n        return paths\n    \ndef data_augment(image, label=None, seed=SEED):\n    image = tf.image.random_flip_left_right(image, seed=seed)\n    image = tf.image.random_flip_up_down(image, seed=seed)\n           \n    if label is None:\n        return image\n    else:\n        return image, label","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5. Augmentation"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_mat(rotation, shear, height_zoom, width_zoom, height_shift, width_shift):\n    rotation = math.pi * rotation / 180.\n    shear = math.pi * shear / 180.\n\n    c1 = tf.math.cos(rotation)\n    s1 = tf.math.sin(rotation)\n    one = tf.constant([1],dtype='float32')\n    zero = tf.constant([0],dtype='float32')\n    rotation_matrix = tf.reshape( tf.concat([c1,s1,zero, -s1,c1,zero, zero,zero,one],axis=0),[3,3] )\n\n    c2 = tf.math.cos(shear)\n    s2 = tf.math.sin(shear)\n    \n    shear_matrix = tf.reshape( tf.concat([one,s2,zero, zero,c2,zero, zero,zero,one],axis=0),[3,3] )    \n    zoom_matrix = tf.reshape( tf.concat([one/height_zoom,zero,zero, zero,one/width_zoom,zero, zero,zero,one],axis=0),[3,3] )\n    shift_matrix = tf.reshape( tf.concat([one,zero,height_shift, zero,one,width_shift, zero,zero,one],axis=0),[3,3] )\n    \n    return K.dot(K.dot(rotation_matrix, shear_matrix), K.dot(zoom_matrix, shift_matrix))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def transform(image,label=None):\n    DIM = img_size\n    XDIM = DIM%2 \n    \n    rot = 15. * tf.random.normal([1],dtype='float32')\n    shr = 5. * tf.random.normal([1],dtype='float32') \n    h_zoom = 1.0 + tf.random.normal([1],dtype='float32')/10.\n    w_zoom = 1.0 + tf.random.normal([1],dtype='float32')/10.\n    h_shift = 8. * tf.random.normal([1],dtype='float32') \n    w_shift = 8. * tf.random.normal([1],dtype='float32') \n  \n    m = get_mat(rot,shr,h_zoom,w_zoom,h_shift,w_shift) \n\n    x = tf.repeat( tf.range(DIM//2,-DIM//2,-1), DIM )\n    y = tf.tile( tf.range(-DIM//2,DIM//2),[DIM] )\n    z = tf.ones([DIM*DIM],dtype='int32')\n    idx = tf.stack( [x,y,z] )\n    \n    idx2 = K.dot(m,tf.cast(idx,dtype='float32'))\n    idx2 = K.cast(idx2,dtype='int32')\n    idx2 = K.clip(idx2,-DIM//2+XDIM+1,DIM//2)\n              \n    idx3 = tf.stack( [DIM//2-idx2[0,], DIM//2-1+idx2[1,]] )\n    d = tf.gather_nd(image,tf.transpose(idx3))\n    \n    if label is None:\n        return tf.reshape(d,[DIM,DIM,3])\n    else:\n        return tf.reshape(d,[DIM,DIM,3]),label","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 6.Création d'un générateur pour l'ensemble de données d'entraînement "},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset = (tf.data.Dataset\n    .from_tensor_slices(preprocess(train))\n    .map(decode_image, num_parallel_calls=AUTO)\n    .map(data_augment, num_parallel_calls=AUTO)\n    .map(transform,num_parallel_calls=AUTO)\n    .shuffle(SEED)\n    .batch(BATCH_SIZE)\n    .repeat()\n    .prefetch(AUTO))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 7. Création d'un générateur pour l'ensemble de données de test"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_dataset= (tf.data.Dataset\n    .from_tensor_slices(preprocess(valid))\n    .map(decode_image, num_parallel_calls=AUTO)\n    .batch(BATCH_SIZE)\n    .cache()\n    .prefetch(AUTO))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 8. Fonction du taux d'apprentissage"},{"metadata":{"trusted":true},"cell_type":"code","source":"LR_START = 0.00001\nLR_MAX = 0.00005 * strategy.num_replicas_in_sync\nLR_MIN = 0.00001\nLR_RAMPUP_EPOCHS = 5\nLR_SUSTAIN_EPOCHS = 0\nLR_EXP_DECAY = .8\n\ndef lrfn(epoch):\n    if epoch < LR_RAMPUP_EPOCHS:\n        lr = (LR_MAX - LR_START) / LR_RAMPUP_EPOCHS * epoch + LR_START\n    elif epoch < LR_RAMPUP_EPOCHS + LR_SUSTAIN_EPOCHS:\n        lr = LR_MAX\n    else:\n        lr = (LR_MAX - LR_MIN) * LR_EXP_DECAY**(epoch - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS) + LR_MIN\n    return lr\n    \nlr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=True)\n\nrng = [i for i in range(EPOCHS)]\ny = [lrfn(x) for x in rng]\nplt.plot(rng, y)\nprint(\"Learning rate schedule: {:.3g} to {:.3g} to {:.3g}\".format(y[0], max(y), y[-1]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 9. Fonction de perte"},{"metadata":{"trusted":true},"cell_type":"code","source":"def categorical_focal_loss(gamma=2., alpha=.25):\n    def categorical_focal_loss_fixed(y_true, y_pred):\n        y_pred /= K.sum(y_pred, axis=-1, keepdims=True)\n        epsilon = K.epsilon()\n        y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n        cross_entropy = -y_true * K.log(y_pred)\n        loss = alpha * K.pow(1 - y_pred, gamma) * cross_entropy\n        return K.sum(loss, axis=1)\n    return categorical_focal_loss_fixed","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p>\n<h1><center> EfficientNetB7 </center></h1>\n<center><img src='https://1.bp.blogspot.com/-DjZT_TLYZok/XO3BYqpxCJI/AAAAAAAAEKM/BvV53klXaTUuQHCkOXZZGywRMdU9v9T_wCLcBGAs/s1600/image2.png' height=350></center>\n<p>"},{"metadata":{"trusted":true},"cell_type":"code","source":"with strategy.scope():\n    enet = efn.EfficientNetB7(input_shape=(img_size, img_size, 3),weights='noisy-student',include_top=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with strategy.scope():\n    for layer in  enet.layers:\n        layer.trainable = False\n\n    for i in range(-3,0):\n         enet.layers[i].trainable = True","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with strategy.scope():\n    ef7 =tf.keras.Sequential()\n    ef7.add(enet)\n    ef7.add(tf.keras.layers.MaxPooling2D())\n    ef7.add(tf.keras.layers.Conv2D(4096,3,padding='same'))\n    ef7.add(tf.keras.layers.BatchNormalization())\n    ef7.add(tf.keras.layers.ReLU())\n    ef7.add(tf.keras.layers.GlobalAveragePooling2D())\n    ef7.add(tf.keras.layers.Dropout(0.35))\n    ef7.add(tf.keras.layers.Flatten())\n\n    ef7.add(tf.keras.layers.Dense(2048,activation='relu'))\n    ef7.add(tf.keras.layers.BatchNormalization())\n    ef7.add(tf.keras.layers.LeakyReLU())\n    ef7.add(tf.keras.layers.Dropout(0.35))\n\n    ef7.add(tf.keras.layers.Dense(1024,activation='relu'))\n    ef7.add(tf.keras.layers.BatchNormalization())\n    ef7.add(tf.keras.layers.LeakyReLU())\n    ef7.add(tf.keras.layers.Dropout(0.25))\n    ef7.add(tf.keras.layers.Dense(3,activation='softmax'))\n    ef7.compile(\n                optimizer=tf.optimizers.Adam(lr=0.0001),\n                loss=categorical_focal_loss(gamma=2., alpha=.25),\n                metrics=['categorical_accuracy',\n                        tf.keras.metrics.Recall(),\n                        tf.keras.metrics.Precision(),   \n                        tf.keras.metrics.AUC(),\n                        tfa.metrics.F1Score(num_classes=3, average=\"macro\")\n                       ])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 10 Entraînement"},{"metadata":{"trusted":true},"cell_type":"code","source":"h7=ef7.fit(\n    train_dataset,\n    steps_per_epoch=train_labels.shape[0] // BATCH_SIZE,\n    callbacks=[lr_callback],\n    epochs=EPOCHS)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 11. Affichage des courbes (acc,loss)"},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nsns.set()\nfig = plt.figure(0, (12, 4))\n\nax = plt.subplot(1, 2, 1)\nsns.lineplot(h7.epoch,h7.history['categorical_accuracy'], label = 'train')\nplt.title('Accuracy')\nplt.tight_layout()\n\nax = plt.subplot(1, 2, 2)\nsns.lineplot(h7.epoch,h7.history['loss'], label = 'train')\nplt.title('Loss')\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 11. Test et évaluation"},{"metadata":{"trusted":true},"cell_type":"code","source":"ef7.evaluate(test_dataset)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nclasses=['N','C','G']\nY_pred = ef7.predict(test_dataset)\ntrue_classes = valid.loc[:, ['N','C','G']].values\nprint('Confusion Matrix')\ncm=confusion_matrix(true_classes.argmax(axis=1),Y_pred.argmax(axis=1))\ncm","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 12. Matrice de confusion"},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nsns.set_style(\"darkgrid\")\nimport itertools\ndef plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Oranges):\n    plt.figure(figsize=(6,6))\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        cm = np.around(cm, decimals=2)\n        cm[np.isnan(cm)] = 0.0\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_confusion_matrix(cm,classes)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 13. specificity et sensitivity"},{"metadata":{"trusted":true},"cell_type":"code","source":"def calculate_sensitivity_specificity(y_test, y_pred_test):\n    actual_pos = y_test == 1\n    actual_neg = y_test == 0\n    \n    true_pos = (y_pred_test == 1) & (actual_pos)\n    false_pos = (y_pred_test == 1) & (actual_neg)\n    true_neg = (y_pred_test == 0) & (actual_neg)\n    false_neg = (y_pred_test == 0) & (actual_pos)\n    \n    # Calculate sensitivity and specificity\n    sensitivity = np.sum(true_pos) / np.sum(actual_pos)\n    specificity = np.sum(true_neg) / np.sum(actual_neg)\n    \n    return sensitivity, specificity","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sensitivity, specificity= calculate_sensitivity_specificity(true_classes.argmax(axis=1),Y_pred.argmax(axis=1))\nprint ('Sensitivity:', sensitivity)\nprint ('Specificity:', specificity)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p>\n<h1><center> DenseNet201 </center></h1>\n<center><img src='https://oi.readthedocs.io/en/latest/_images/cnn_vs_resnet_vs_densenet.png' height=20></center>\n<p>\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"with strategy.scope():\n    DenseNet201 = tf.keras.applications.DenseNet201(input_shape=(512, 512, 3), weights='imagenet', include_top=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with strategy.scope():\n    for layer in  DenseNet201.layers:\n        layer.trainable = False\n\n    for i in range(-3,0):\n         DenseNet201.layers[i].trainable = True","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with strategy.scope():\n    model_D201=tf.keras.Sequential()\n    model_D201.add(DenseNet201)\n    model_D201.add(tf.keras.layers.MaxPooling2D())\n    model_D201.add(tf.keras.layers.Conv2D(4096,3,padding='same'))\n    model_D201.add(tf.keras.layers.BatchNormalization())\n    model_D201.add(tf.keras.layers.ReLU())\n    model_D201.add(tf.keras.layers.GlobalAveragePooling2D())\n    model_D201.add(tf.keras.layers.Dropout(0.35))\n    model_D201.add(tf.keras.layers.Flatten())\n\n    model_D201.add(tf.keras.layers.Dense(2048,activation='relu'))\n    model_D201.add(tf.keras.layers.BatchNormalization())\n    model_D201.add(tf.keras.layers.LeakyReLU())\n    model_D201.add(tf.keras.layers.Dropout(0.35))\n    \n    model_D201.add(tf.keras.layers.Dense(1024,activation='relu'))\n    model_D201.add(tf.keras.layers.BatchNormalization())\n    model_D201.add(tf.keras.layers.LeakyReLU())\n    model_D201.add(tf.keras.layers.Dropout(0.25))\n    model_D201.add(tf.keras.layers.Dense(3,activation='softmax'))\n    model_D201.compile(\n                optimizer=tf.optimizers.Adam(lr=0.0001),\n                loss=categorical_focal_loss(gamma=2., alpha=.25),\n                metrics=['categorical_accuracy',\n                        tf.keras.metrics.Recall(),\n                        tf.keras.metrics.Precision(),   \n                        tf.keras.metrics.AUC(),\n                        tfa.metrics.F1Score(num_classes=3, average=\"macro\")\n                       ])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1. Entraînement"},{"metadata":{"trusted":true},"cell_type":"code","source":"D_201=model_D201.fit(\n    train_dataset, \n    steps_per_epoch=train_labels.shape[0] // BATCH_SIZE,\n    callbacks=[lr_callback],\n    epochs=EPOCHS)\n    #validation_data=test_dataset)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.Affichage des courbes (acc,loss)"},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nsns.set()\nfig = plt.figure(0, (12, 4))\n\nax = plt.subplot(1, 2, 1)\nsns.lineplot(D_201.epoch,D_201.history['categorical_accuracy'], label = 'train')\nplt.title('Accuracy')\nplt.tight_layout()\n\nax = plt.subplot(1, 2, 2)\nsns.lineplot(D_201.epoch,D_201.history['loss'], label = 'train')\nplt.title('Loss')\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. Test et évaluation"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_D201.evaluate(test_dataset)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix,classification_report\nY_pred = model_D201.predict(test_dataset)\ntrue_classes = valid.loc[:, ['N','C','G']].values\nprint('Confusion Matrix')\ncm=confusion_matrix(true_classes.argmax(axis=1),Y_pred.argmax(axis=1))\ncm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_confusion_matrix(cm,classes)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4. Matrice de confusion"},{"metadata":{"trusted":true},"cell_type":"code","source":"sensitivity, specificity= calculate_sensitivity_specificity(true_classes.argmax(axis=1),Y_pred.argmax(axis=1))\nprint ('Sensitivity:', sensitivity)\nprint ('Specificity:', specificity)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential, load_model\nfrom keras.layers import (Activation, Dropout, Flatten, Dense, GlobalMaxPooling2D,\n                          BatchNormalization, Input, Conv2D, GlobalAveragePooling2D,concatenate,Concatenate,multiply, LocallyConnected2D, Lambda)\nfrom keras.callbacks import ModelCheckpoint\nfrom keras import metrics\nfrom keras.optimizers import Adam \nimport keras\nfrom keras.models import Model\nfrom keras.activations import hard_sigmoid\nCFG = dict(\n    inp_size          = 512,\n    read_size         = 512, \n    crop_size         = 512,\n    net_size          = 512)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with strategy.scope():\n    in_lay = Input(shape=(CFG['inp_size'], CFG['inp_size'],3))\n    base_model = efn.EfficientNetB7(weights='noisy-student',\n        input_shape=(CFG['inp_size'], CFG['inp_size'],3),\n        include_top=False\n                       )\n    #base_model.load_weights(\"../input/efficientnet-keras-weights-b0b5/efficientnet-b5_imagenet_1000_notop.h5\")\n    pt_depth = base_model.get_output_shape_at(0)[-1]\n    pt_features = base_model(in_lay)\n    bn_features = BatchNormalization()(pt_features)\n    \n    # ici nous faisons un mécanisme d'attention pour activer et désactiver les pixels dans le GAP\n    # lidee est baser sur cette explication \n    #1-http://akosiorek.github.io/ml/2017/10/14/visual-attention.html\n    #2-https://machinelearningmastery.com/how-does-attention-work-in-encoder-decoder-recurrent-neural-networks/\n    \n    \n    attn_layer = Conv2D(64, kernel_size = (1,1), padding = 'same', activation = 'relu')(Dropout(0.5)(bn_features))\n    attn_layer = Conv2D(16, kernel_size = (1,1), padding = 'same', activation = 'relu')(attn_layer)\n    attn_layer = Conv2D(8, kernel_size = (1,1), padding = 'same', activation = 'relu')(attn_layer)\n    attn_layer = Conv2D(1, \n                        kernel_size = (1,1), \n                        padding = 'valid', \n                        activation = 'sigmoid')(attn_layer)\n    # diffusez sur toutes les chaînes\n    # kernel_size  détermine les dimensions du noyau. Les dimensions courantes comprennent 1×1, 3×3, 5×5 et 7×7, qui peuvent être passées en (1, 1), (3, 3), (5, 5) ou (7, 7) tuples.\n    # Il s'agit d'un nombre entier ou d'un tuple/liste de 2 nombres entiers, spécifiant la hauteur et la largeur de la fenêtre de convolution 2D.\n    #  Ce paramètre doit être un nombre entier impair\n    # pour plus de details sur cette partie (mask et use_bias ... ) il ya  une bonne explication sur geekforgeeks\n    #https://www.geeksforgeeks.org/keras-conv2d-class/\n    \n    up_c2_w = np.ones((1, 1, 1, pt_depth))\n    up_c2 = Conv2D(pt_depth, kernel_size = (1,1), padding = 'same', \n                   activation = 'linear', use_bias = False, weights = [up_c2_w])\n    up_c2.trainable = False\n    attn_layer = up_c2(attn_layer)\n\n    mask_features = multiply([attn_layer, bn_features])\n    gap_features = GlobalAveragePooling2D()(mask_features)\n    gap_mask = GlobalAveragePooling2D()(attn_layer)\n    \n    # pour tenir compte des valeurs manquantes du modèle d'attention\n    # pour bien comprendre resaclegap il ya un bon exemple ici qui explique tellemnt bien cette partie \n    # https://codefellows.github.io/sea-python-401d5/lectures/rescaling_data.html\n    \n    gap = Lambda(lambda x: x[0]/x[1], name = 'RescaleGAP')([gap_features, gap_mask])\n    gap_dr = Dropout(0.25)(gap)\n    dr_steps = Dropout(0.25)(Dense(128, activation = 'relu')(gap_dr))\n    out_layer = Dense(3, activation = 'softmax')(dr_steps)\n    model = Model(inputs = [in_lay], outputs = [out_layer])  \n    model.compile(\n                optimizer=tf.optimizers.Adam(lr=0.0001),\n                loss=categorical_focal_loss(gamma=2., alpha=.25),\n                metrics=['categorical_accuracy',\n                        tf.keras.metrics.Recall(),\n                        tf.keras.metrics.Precision(),   \n                        tf.keras.metrics.AUC(),\n                        tfa.metrics.F1Score(num_classes=3, average=\"macro\")\n                       ])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ATT_EF7= model.fit(\n    train_dataset, \n    steps_per_epoch=train_labels.shape[0] // BATCH_SIZE,\n    callbacks=[lr_callback],\n    epochs=EPOCHS)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nsns.set()\nfig = plt.figure(0, (12, 4))\n\nax = plt.subplot(1, 2, 1)\nsns.lineplot(ATT_EF7.epoch,ATT_EF7.history['categorical_accuracy'], label = 'train')\nplt.title('Accuracy')\nplt.tight_layout()\n\nax = plt.subplot(1, 2, 2)\nsns.lineplot(ATT_EF7.epoch,ATT_EF7.history['loss'], label = 'train')\nplt.title('Loss')\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.evaluate(test_dataset)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix,classification_report\nY_pred = model.predict(test_dataset)\ntrue_classes = valid.loc[:, ['N','C','G']].values\nprint('Confusion Matrix')\ncm=confusion_matrix(true_classes.argmax(axis=1),Y_pred.argmax(axis=1))\ncm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_confusion_matrix(cm,classes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sensitivity, specificity= calculate_sensitivity_specificity(true_classes.argmax(axis=1),Y_pred.argmax(axis=1))\nprint ('Sensitivity:', sensitivity)\nprint ('Specificity:', specificity)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with strategy.scope():\n    in_lay = Input(shape=(CFG['inp_size'], CFG['inp_size'],3))\n    base_model = tf.keras.applications.DenseNet201(weights='imagenet',\n        input_shape=(CFG['inp_size'], CFG['inp_size'],3),\n        include_top=False\n                       )\n    #base_model.load_weights(\"../input/efficientnet-keras-weights-b0b5/efficientnet-b5_imagenet_1000_notop.h5\")\n    pt_depth = base_model.get_output_shape_at(0)[-1]\n    pt_features = base_model(in_lay)\n    bn_features = BatchNormalization()(pt_features)\n    \n    # ici nous faisons un mécanisme d'attention pour activer et désactiver les pixels dans le GAP\n    # lidee est baser sur cette explication \n    #1-http://akosiorek.github.io/ml/2017/10/14/visual-attention.html\n    #2-https://machinelearningmastery.com/how-does-attention-work-in-encoder-decoder-recurrent-neural-networks/\n    \n    \n    attn_layer = Conv2D(64, kernel_size = (1,1), padding = 'same', activation = 'relu')(Dropout(0.5)(bn_features))\n    attn_layer = Conv2D(16, kernel_size = (1,1), padding = 'same', activation = 'relu')(attn_layer)\n    attn_layer = Conv2D(8, kernel_size = (1,1), padding = 'same', activation = 'relu')(attn_layer)\n    attn_layer = Conv2D(1, \n                        kernel_size = (1,1), \n                        padding = 'valid', \n                        activation = 'sigmoid')(attn_layer)\n    # diffusez sur toutes les chaînes\n    # kernel_size  détermine les dimensions du noyau. Les dimensions courantes comprennent 1×1, 3×3, 5×5 et 7×7, qui peuvent être passées en (1, 1), (3, 3), (5, 5) ou (7, 7) tuples.\n    # Il s'agit d'un nombre entier ou d'un tuple/liste de 2 nombres entiers, spécifiant la hauteur et la largeur de la fenêtre de convolution 2D.\n    #  Ce paramètre doit être un nombre entier impair\n    # pour plus de details sur cette partie (mask et use_bias ... ) il ya  une bonne explication sur geekforgeeks\n    #https://www.geeksforgeeks.org/keras-conv2d-class/\n    \n    up_c2_w = np.ones((1, 1, 1, pt_depth))\n    up_c2 = Conv2D(pt_depth, kernel_size = (1,1), padding = 'same', \n                   activation = 'linear', use_bias = False, weights = [up_c2_w])\n    up_c2.trainable = False\n    attn_layer = up_c2(attn_layer)\n\n    mask_features = multiply([attn_layer, bn_features])\n    gap_features = GlobalAveragePooling2D()(mask_features)\n    gap_mask = GlobalAveragePooling2D()(attn_layer)\n    \n    # pour tenir compte des valeurs manquantes du modèle d'attention\n    # pour bien comprendre resaclegap il ya un bon exemple ici qui explique tellemnt bien cette partie \n    # https://codefellows.github.io/sea-python-401d5/lectures/rescaling_data.html\n    \n    gap = Lambda(lambda x: x[0]/x[1], name = 'RescaleGAP')([gap_features, gap_mask])\n    gap_dr = Dropout(0.25)(gap)\n    dr_steps = Dropout(0.25)(Dense(128, activation = 'relu')(gap_dr))\n    out_layer = Dense(3, activation = 'softmax')(dr_steps)\n    model_d201 = Model(inputs = [in_lay], outputs = [out_layer])  \n    model_d201.compile(\n                optimizer=tf.optimizers.Adam(lr=0.0001),\n                loss=categorical_focal_loss(gamma=2., alpha=.25),\n                metrics=['categorical_accuracy',\n                        tf.keras.metrics.Recall(),\n                        tf.keras.metrics.Precision(),   \n                        tf.keras.metrics.AUC(),\n                        tfa.metrics.F1Score(num_classes=3, average=\"macro\")\n                       ])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d201= model_d201.fit(\n    train_dataset, \n    steps_per_epoch=train_labels.shape[0] // BATCH_SIZE,\n    callbacks=[lr_callback],\n    epochs=EPOCHS)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nsns.set()\nfig = plt.figure(0, (12, 4))\n\nax = plt.subplot(1, 2, 1)\nsns.lineplot(d201.epoch,d201.history['categorical_accuracy'], label = 'train')\nplt.title('Accuracy')\nplt.tight_layout()\n\nax = plt.subplot(1, 2, 2)\nsns.lineplot(d201.epoch,d201.history['loss'], label = 'train')\nplt.title('Loss')\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_d201.evaluate(test_dataset)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix,classification_report\nY_pred = model_d201.predict(test_dataset)\ntrue_classes = valid.loc[:, ['N','C','G']].values\nprint('Confusion Matrix')\ncm=confusion_matrix(true_classes.argmax(axis=1),Y_pred.argmax(axis=1))\ncm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_confusion_matrix(cm,classes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sensitivity, specificity= calculate_sensitivity_specificity(true_classes.argmax(axis=1),Y_pred.argmax(axis=1))\nprint ('Sensitivity:', sensitivity)\nprint ('Specificity:', specificity)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5. Étude comparative"},{"metadata":{},"cell_type":"markdown","source":"**l'évaluation des 4 modèles (vgg16,vgg19,DenseNet-201,EfficientNetB7) est basé sur Accuracy,Précision,Recall,F1_Score,AUC,Specificity,Sensitivity**\n\n**Positifs vrais (TP)** - Ce sont les valeurs positives correctement prédites, ce qui signifie que la valeur de la classe réelle est oui et que la valeur de la classe prédite est également oui.\n\n**Vrais négatifs (TN)** - Il s'agit des valeurs négatives correctement prédites, ce qui signifie que la valeur de la classe réelle est non et que la valeur de la classe prédite est également non.\n\n**Faux positifs (FP) et faux négatifs (FN)** , ces valeurs se produisent lorsque votre classe réelle est en contradiction avec la classe prédite.\n\n**Accuracy** - La précision est la mesure de performance la plus intuitive et il s'agit simplement d'un rapport entre l'observation correctement prédite et le total des observations.\n\n**Accuracy  = TP+TN/TP+FP+FN+TN**\n\n**Précision** - La précision est le rapport entre les observations positives correctement prédites et le total des observations positives prédites. \n\n**Précision = TP/TP+FP**\n\n**Recall (sensibilité)**- Le Recall est le rapport entre les observations positives correctement prédites et toutes les observations de la classe réelle \n\n**Recall = TP/TP+FN**\n\n**F1_Score** - Le score F1 est la moyenne pondérée de la précision et du rappel. Par conséquent, ce score tient compte à la fois des faux positifs et des faux négatifs. \n\n**F1_Score = 2*(Rappel * Précision) / (Rappel + Précision)**\n\n**Sensitivity mesure la proportion de vrais positifs qui sont correctement identifiés**\n**Specificity mesure la proportion de vrais négatifs qui sont correctement identifiés comme non**\n"},{"metadata":{},"cell_type":"markdown","source":"| Methodes | Loss | Accuracy  | Precision | Recall  |F1_score  |specificity |sensitivity  |\n| --- | --- | --- | --- | --- | --- | --- | --- |\n| VGG16 |  0.9876 | 0.9884  | 0.9840|0.9874 |0.9991 |0.9991 |0.9991 |\n| VGG19 | 0.9662  | 0.9807| 0.9484 |0.9657 |1.0000 |0.9991 |0.9991 |\n|DenseNet201|0.9662  | 0.9807| 0.9484 |0.9657 |1.0000 |0.9991 |0.9991 |\n|EfficientNetB7 |0.9662  | 0.9807| 0.9484 |0.9657 |1.0000 |0.9991 |0.9991 |"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}