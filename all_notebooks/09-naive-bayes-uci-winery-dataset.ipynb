{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"## Naive Bayes (generative) model\n\nNaive Bayes model predicts the likelihood of an event based on the evidence present in the test dataset. \n\n**Conditional probability and Bayes' rule:**  \n\n$$ P(A|B) = \\frac{P(A \\cap B)}{P(B)} = \\frac{P(A) P(B|A)}{P(B)}  $$ \n\nThree types of Naive Bayes model:\n1. Multinomial (categorical or continuos, discrete frequency)\n2. Bernoulli (binary features)\n3. Gaussian (continuous, normally distributed)\n\n**Learning with practical dataset:**\nHere we fit each class (independently) with a model. Say we have two classes with one dimensional probability distributions $P_1(x)$ and $P_2(x)$. \n\nLet's say our training set has $\\pi_1$ fraction of class one and $\\pi_2$ fraction of class two ($\\pi_1 + \\pi_2 = 1$). \n\nNow for a test point (x), we predict its class for which $\\pi_iP_i(x)$ is maximum. Note that $\\pi_i$ is determined based on our training dataset. "},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# load python packages\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import GaussianNB\n\n%matplotlib inline\nplt.rcParams[\"figure.figsize\"] = (8, 6)\nplt.rcParams[\"font.size\"] = 16","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import UC Irvine wine classification dataset\nheaders = ['Category','Alcohol', 'Malic acid', 'Ash', 'Alcalinity of ash','Magnesium', \\\n           'Total phenols', 'Flavanoids', 'Nonflavanoid phenols', 'Proanthocyanins', \\\n           'Color intensity', 'Hue', 'OD280/OD315 of diluted wines', 'Proline']\ndata = pd.read_csv(\"../input/wineuci/Wine.csv\", names=headers)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's consider one feature (Alcohol) and plot it for the first category\ncat1 = data.loc[data['Category'] == 1]\nplt.hist(cat1[\"Alcohol\"], density=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's fit it with Gaussian distribution\nmu = np.mean(cat1[\"Alcohol\"])                # mean\nvar = np.var(cat1[\"Alcohol\"])                # variance\nstd = np.sqrt(var)                           # standard deviation\n\nx_axis = np.linspace(mu - 3*std, mu + 3*std, 1000)\nplt.plot(x_axis, norm.pdf(x_axis,mu,std), 'r', lw=2)\nplt.hist(cat1[\"Alcohol\"], density=True)\nplt.xlabel('Alcohol content')\nplt.ylabel('Probability density')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We fit probabilty distribution ($P_i$) for each category. The probability of each category is simply the $\\pi_i$ (frequency of that category)/(total sample size) in the training dataset. Now for a given new data, we simply calculate $\\pi_i P_i$, and choose the label for which it is maximum. \n\nNow we will do the same using scikit learn modules. Where we will use all the predictor variable in the data.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"# first do a train test split of our data\nX, X_test, y, y_test = train_test_split(data.drop(['Category'],axis=1),\\\n                       data['Category'], test_size=0.3, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check that we have sufficient data of each wine category \ny.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"GaussNB = GaussianNB()\nGaussNB.fit(X, y)\nGaussNB.predict([X_test.iloc[4]])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"GaussNB.score(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}