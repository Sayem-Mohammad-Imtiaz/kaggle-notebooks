{"cells":[{"metadata":{"id":"hbDFWx07ulOR","colab_type":"text"},"cell_type":"markdown","source":"# Importing Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"#!pip install --upgrade imutils","execution_count":null,"outputs":[]},{"metadata":{"id":"CBMdYH_LulOU","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport cv2\nimport os\nfrom scipy import ndimage\nfrom matplotlib.image import imread\nimport gc\nimport re\n#import imutils","execution_count":null,"outputs":[]},{"metadata":{"id":"hu9vb9CiuqZ5","colab_type":"code","outputId":"8d0e278d-cb71-4756-a452-92b1a6bd234a","colab":{"base_uri":"https://localhost:8080/","height":34},"trusted":true},"cell_type":"code","source":"#from google.colab import drive\n#drive.mount('/content/drive')","execution_count":null,"outputs":[]},{"metadata":{"id":"VkBa3QYn0jIA","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"#!unzip -uq \"/content/drive/My Drive/leaf_dataset\" -d \"/content/drive/My Drive/leaf/images\"","execution_count":null,"outputs":[]},{"metadata":{"id":"06ix3sJeulOX","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"from matplotlib.image import imread\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"id":"Pc6KKTqUulOa","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"images_dir = \"../input/leaf-dataset\"","execution_count":null,"outputs":[]},{"metadata":{"id":"tXniV2L2ulOc","colab_type":"text"},"cell_type":"markdown","source":"# Splitting and DataGenerator"},{"metadata":{"trusted":true},"cell_type":"code","source":"def images_to_XandY(base_dir):\n    \"\"\"\n    Input: base_dir : The string of base_dir containing the dataset\n    Output: X, Y, Y_map\n        X : A (m,nH,nW,nC) vector where:\n            m : No of samples\n            nH: Height of images\n            nW: Width of images\n            nC: No of channels = 3\n        Y : A (m,C) vector, where:\n            m: No of. samples\n            C: No. of classifications of images\n        Y_map: A dict describing the label of 'Y' for each class of leaf starting from 0\n    \"\"\"\n    X_imgs_locs = []\n    Y = []\n    X = []\n    Y_map = {}\n    \n    \n    for i,leaf_class in enumerate(os.listdir(base_dir)):\n        new_data = [os.path.join(base_dir,leaf_class,'{}'.format(i)) for i in os.listdir(os.path.join(base_dir,leaf_class))] #list of images\n        X_imgs_locs.extend(new_data)\n        Y.extend(i*np.ones(len(new_data)))\n        leaf_name = re.sub('[^a-zA-Z]+','',leaf_class)\n        Y_map[leaf_name] = i\n        \n    #Now the X_imgs_locs conatains a list of locations of all images\n    #We need to convert these image locations to numpy arrays\n    for img_loc in X_imgs_locs:\n        img = imread(img_loc)\n        if (img.shape[0]<img.shape[1]):\n            img = img.transpose([1,0,2])\n        \n        #Resizing image\n        img = cv2.resize(img, (224, 224))\n        \n        #Rescale\n        np.true_divide(img, 255.0)\n        X.append(img)\n    \n    #Deleting useless items\n    del X_imgs_locs\n    del new_data\n    del leaf_name\n    \n    #Converting X and Y to numpy arrays\n    X = np.stack(X)\n    Y = np.array(Y)\n    \n    return X,Y,Y_map","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X, y, y_map = images_to_XandY(images_dir)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_map","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.utils import to_categorical","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_new = to_categorical(y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_new.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Splitting"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_val, y_train, y_val = train_test_split(X, y_new, test_size = 0.3, random_state=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del X, y  #Freeing ram","execution_count":null,"outputs":[]},{"metadata":{"id":"Ln5VIMmuulOd","colab_type":"code","outputId":"3d526db2-c100-41fa-fde1-21c0a0a13064","colab":{"base_uri":"https://localhost:8080/","height":34},"trusted":true},"cell_type":"markdown","source":"from keras.preprocessing.image import ImageDataGenerator"},{"metadata":{"id":"Bc2LpB1BulOg","colab_type":"code","colab":{},"trusted":true},"cell_type":"markdown","source":"#15% of total images will be kept as validation\nimage_generator = ImageDataGenerator(rescale=1./255,\n                                     rotation_range = 40,\n                                     zoom_range = 0.15,\n                                   data_format='channels_last',\n                                   validation_split = 0.15)"},{"metadata":{"id":"Bc2LpB1BulOg","colab_type":"code","colab":{},"trusted":true},"cell_type":"markdown","source":"#15% of total images will be kept as validation\nimage_generator = ImageDataGenerator(rescale=1./255,\n                                     rotation_range = 40,\n                                     zoom_range = 0.15,\n                                   data_format='channels_last',\n                                   validation_split = 0.15)"},{"metadata":{"id":"TyEkh8_MulOj","colab_type":"code","outputId":"3f916b58-ea10-4126-86cb-b233d9857032","colab":{"base_uri":"https://localhost:8080/","height":52},"trusted":true},"cell_type":"markdown","source":"train_gen = image_generator.flow_from_directory(images_dir,\n                                        target_size=(224,224),\n                                        color_mode='rgb',\n                                        class_mode='categorical',\n                                        batch_size = 32,\n                                        seed = 2,\n                                        shuffle = True,\n                                        subset='training')\n\nval_gen = image_generator.flow_from_directory(images_dir,\n                                        target_size=(224,224),\n                                        color_mode='rgb',\n                                        class_mode='categorical',\n                                        batch_size = 32,\n                                        seed = 2,\n                                        shuffle = True,\n                                        subset='validation')"},{"metadata":{"id":"p-WTnstQulOn","colab_type":"text"},"cell_type":"markdown","source":"# Modelling"},{"metadata":{"id":"aZffDrBVulOo","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"from keras.preprocessing.image import ImageDataGenerator, array_to_img\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D, MaxPooling2D\nfrom keras.layers import Activation, Dropout, Flatten, Dense\nfrom keras import backend as K\nfrom keras import optimizers","execution_count":null,"outputs":[]},{"metadata":{"id":"bEkQsOXrulOq","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"from keras.layers import (\n    Input, Dense, Conv2D, MaxPooling2D, AveragePooling2D, ZeroPadding2D,\n    Flatten, Activation, GlobalAveragePooling2D, GlobalMaxPooling2D, add)\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.models import Model\nfrom keras import initializers\nfrom keras.engine import Layer, InputSpec\nfrom keras.engine.topology import get_source_inputs\nfrom keras import backend as K\n#from keras.applications.imagenet_utils import _obtain_input_shape\nfrom keras.utils.data_utils import get_file\n\nimport warnings\nimport sys\n\nclass Scale(Layer):\n    '''Learns a set of weights and biases used for scaling the input data.\n    the output consists simply in an element-wise multiplication of the input\n    and a sum of a set of constants:\n        out = in * gamma + beta,\n    where 'gamma' and 'beta' are the weights and biases larned.\n    # Arguments\n        axis: integer, axis along which to normalize in mode 0. For instance,\n            if your input tensor has shape (samples, channels, rows, cols),\n            set axis to 1 to normalize per feature map (channels axis).\n        momentum: momentum in the computation of the\n            exponential average of the mean and standard deviation\n            of the data, for feature-wise normalization.\n        weights: Initialization weights.\n            List of 2 Numpy arrays, with shapes:\n            `[(input_shape,), (input_shape,)]`\n        beta_init: name of initialization function for shift parameter\n            (see [initializers](../initializers.md)), or alternatively,\n            Theano/TensorFlow function to use for weights initialization.\n            This parameter is only relevant if you don't pass a `weights`\n            argument.\n        gamma_init: name of initialization function for scale parameter (see\n            [initializers](../initializers.md)), or alternatively,\n            Theano/TensorFlow function to use for weights initialization.\n            This parameter is only relevant if you don't pass a `weights`\n            argument.\n        gamma_init: name of initialization function for scale parameter (see\n            [initializers](../initializers.md)), or alternatively,\n            Theano/TensorFlow function to use for weights initialization.\n            This parameter is only relevant if you don't pass a `weights`\n            argument.\n    '''\n    def __init__(self,\n                 weights=None,\n                 axis=-1,\n                 momentum=0.9,\n                 beta_init='zero',\n                 gamma_init='one',\n                 **kwargs):\n        self.momentum = momentum\n        self.axis = axis\n        self.beta_init = initializers.get(beta_init)\n        self.gamma_init = initializers.get(gamma_init)\n        self.initial_weights = weights\n        super(Scale, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        self.input_spec = [InputSpec(shape=input_shape)]\n        shape = (int(input_shape[self.axis]),)\n\n        self.gamma = K.variable(\n            self.gamma_init(shape),\n            name='{}_gamma'.format(self.name))\n        self.beta = K.variable(\n            self.beta_init(shape),\n            name='{}_beta'.format(self.name))\n        self.trainable_weights = [self.gamma, self.beta]\n\n        if self.initial_weights is not None:\n            self.set_weights(self.initial_weights)\n            del self.initial_weights\n\n    def call(self, x, mask=None):\n        input_shape = self.input_spec[0].shape\n        broadcast_shape = [1] * len(input_shape)\n        broadcast_shape[self.axis] = input_shape[self.axis]\n\n        out = K.reshape(\n            self.gamma,\n            broadcast_shape) * x + K.reshape(self.beta, broadcast_shape)\n        return out\n\n    def get_config(self):\n        config = {\"momentum\": self.momentum, \"axis\": self.axis}\n        base_config = super(Scale, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\ndef identity_block(input_tensor, kernel_size, filters, stage, block):\n    '''The identity_block is the block that has no conv layer at shortcut\n    # Arguments\n        input_tensor: input tensor\n        kernel_size: defualt 3, the kernel size of middle conv layer at main\n            path\n        filters: list of integers, the nb_filters of 3 conv layer at main path\n        stage: integer, current stage label, used for generating layer names\n        block: 'a','b'..., current block label, used for generating layer names\n    '''\n    eps = 1.1e-5\n    if K.image_data_format() == 'channels_last':\n        bn_axis = 3\n    else:\n        bn_axis = 1\n    nb_filter1, nb_filter2, nb_filter3 = filters\n    conv_name_base = 'res' + str(stage) + block + '_branch'\n    bn_name_base = 'bn' + str(stage) + block + '_branch'\n    scale_name_base = 'scale' + str(stage) + block + '_branch'\n\n    x = Conv2D(nb_filter1, (1, 1), name=conv_name_base + '2a',\n               use_bias=False)(input_tensor)\n    x = BatchNormalization(epsilon=eps, axis=bn_axis,\n                           name=bn_name_base + '2a')(x)\n    x = Scale(axis=bn_axis, name=scale_name_base + '2a')(x)\n    x = Activation('relu', name=conv_name_base + '2a_relu')(x)\n\n    x = ZeroPadding2D((1, 1), name=conv_name_base + '2b_zeropadding')(x)\n    x = Conv2D(nb_filter2, (kernel_size, kernel_size),\n               name=conv_name_base + '2b', use_bias=False)(x)\n    x = BatchNormalization(epsilon=eps, axis=bn_axis,\n                           name=bn_name_base + '2b')(x)\n    x = Scale(axis=bn_axis, name=scale_name_base + '2b')(x)\n    x = Activation('relu', name=conv_name_base + '2b_relu')(x)\n\n    x = Conv2D(nb_filter3, (1, 1), name=conv_name_base + '2c',\n               use_bias=False)(x)\n    x = BatchNormalization(epsilon=eps, axis=bn_axis,\n                           name=bn_name_base + '2c')(x)\n    x = Scale(axis=bn_axis, name=scale_name_base + '2c')(x)\n\n    x = add([x, input_tensor], name='res' + str(stage) + block)\n    x = Activation('relu', name='res' + str(stage) + block + '_relu')(x)\n    return x\n\n\ndef conv_block(input_tensor,\n               kernel_size,\n               filters,\n               stage,\n               block,\n               strides=(2, 2)):\n    '''conv_block is the block that has a conv layer at shortcut\n    # Arguments\n        input_tensor: input tensor\n        kernel_size: defualt 3, the kernel size of middle conv layer at main\n            path\n        filters: list of integers, the nb_filters of 3 conv layer at main path\n        stage: integer, current stage label, used for generating layer names\n        block: 'a','b'..., current block label, used for generating layer names\n    Note that from stage 3, the first conv layer at main path is with\n    strides=(2,2). And the shortcut should have strides=(2,2) as well\n    '''\n    eps = 1.1e-5\n    if K.image_data_format() == 'channels_last':\n        bn_axis = 3\n    else:\n        bn_axis = 1\n    nb_filter1, nb_filter2, nb_filter3 = filters\n    conv_name_base = 'res' + str(stage) + block + '_branch'\n    bn_name_base = 'bn' + str(stage) + block + '_branch'\n    scale_name_base = 'scale' + str(stage) + block + '_branch'\n\n    x = Conv2D(nb_filter1, (1, 1), strides=strides,\n               name=conv_name_base + '2a', use_bias=False)(input_tensor)\n    x = BatchNormalization(epsilon=eps, axis=bn_axis,\n                           name=bn_name_base + '2a')(x)\n    x = Scale(axis=bn_axis, name=scale_name_base + '2a')(x)\n    x = Activation('relu', name=conv_name_base + '2a_relu')(x)\n\n    x = ZeroPadding2D((1, 1), name=conv_name_base + '2b_zeropadding')(x)\n    x = Conv2D(nb_filter2, (kernel_size, kernel_size),\n               name=conv_name_base + '2b', use_bias=False)(x)\n    x = BatchNormalization(epsilon=eps, axis=bn_axis,\n                           name=bn_name_base + '2b')(x)\n    x = Scale(axis=bn_axis, name=scale_name_base + '2b')(x)\n    x = Activation('relu', name=conv_name_base + '2b_relu')(x)\n\n    x = Conv2D(nb_filter3, (1, 1),\n               name=conv_name_base + '2c', use_bias=False)(x)\n    x = BatchNormalization(epsilon=eps, axis=bn_axis,\n                           name=bn_name_base + '2c')(x)\n    x = Scale(axis=bn_axis, name=scale_name_base + '2c')(x)\n\n    shortcut = Conv2D(nb_filter3, (1, 1), strides=strides,\n                      name=conv_name_base + '1', use_bias=False)(input_tensor)\n    shortcut = BatchNormalization(epsilon=eps, axis=bn_axis,\n                                  name=bn_name_base + '1')(shortcut)\n    shortcut = Scale(axis=bn_axis, name=scale_name_base + '1')(shortcut)\n\n    x = add([x, shortcut], name='res' + str(stage) + block)\n    x = Activation('relu', name='res' + str(stage) + block + '_relu')(x)\n    return x\n\n\ndef ResNet101(include_top=True,\n              input_tensor=None,\n              input_shape=None,\n              pooling=None,\n              classes=1000):\n    \"\"\"Instantiates the ResNet-101 architecture.\n.\n    Parameters\n    ----------\n        input_tensor: optional Keras tensor (i.e. output of `layers.Input()`)\n            to use as image input for the model.\n        input_shape: optional shape tuple, only to be specified if\n            `include_top` is False (otherwise the input shape has to be\n            `(224, 224, 3)` (with `channels_last` data format) or\n            `(3, 224, 224)` (with `channels_first` data format). It should have\n            exactly 3 inputs channels, and width and height should be no\n            smaller than 197.\n            E.g. `(200, 200, 3)` would be one valid value.\n        pooling: Optional pooling mode for feature extraction when\n            `include_top` is `False`.\n            - `None` means that the output of the model will be the 4D tensor\n                output of the last convolutional layer.\n            - `avg` means that global average pooling will be applied to the\n                output of the last convolutional layer, and thus the output of\n                the model will be a 2D tensor.\n            - `max` means that global max pooling will be applied.\n        classes: optional number of classes to classify images into, only to be\n            specified if `include_top` is True, and if no `weights` argument is\n            specified.\n    Returns\n    -------\n        A Keras model instance.\n    Raises\n    ------\n        ValueError: in case of invalid argument for `weights`, or invalid input\n        shape.\n    \"\"\"\n\n\n\n    if input_tensor is None:\n        img_input = Input(shape=input_shape, name='data')\n    else:\n        if not K.is_keras_tensor(input_tensor):\n            img_input = Input(\n                tensor=input_tensor, shape=input_shape, name='data')\n        else:\n            img_input = input_tensor\n    if K.image_data_format() == 'channels_last':\n        bn_axis = 3\n    else:\n        bn_axis = 1\n    eps = 1.1e-5\n\n    x = ZeroPadding2D((3, 3), name='conv1_zeropadding')(img_input)\n    x = Conv2D(64, (7, 7), strides=(2, 2), name='conv1', use_bias=False)(x)\n    x = BatchNormalization(epsilon=eps, axis=bn_axis, name='bn_conv1')(x)\n    x = Scale(axis=bn_axis, name='scale_conv1')(x)\n    x = Activation('relu', name='conv1_relu')(x)\n    x = MaxPooling2D((3, 3), strides=(2, 2), name='pool1')(x)\n\n    x = conv_block(x, 3, [64, 64, 256], stage=2, block='a', strides=(1, 1))\n    x = identity_block(x, 3, [64, 64, 256], stage=2, block='b')\n    x = identity_block(x, 3, [64, 64, 256], stage=2, block='c')\n\n    x = conv_block(x, 3, [128, 128, 512], stage=3, block='a')\n    for i in range(1, 3):\n        x = identity_block(x, 3, [128, 128, 512], stage=3, block='b' + str(i))\n\n    x = conv_block(x, 3, [256, 256, 1024], stage=4, block='a')\n    for i in range(1, 23):\n        x = identity_block(x, 3, [256, 256, 1024], stage=4, block='b' + str(i))\n\n    x = conv_block(x, 3, [512, 512, 2048], stage=5, block='a')\n    x = identity_block(x, 3, [512, 512, 2048], stage=5, block='b')\n    x = identity_block(x, 3, [512, 512, 2048], stage=5, block='c')\n\n    x = AveragePooling2D((7, 7), name='avg_pool')(x)\n\n    if include_top:\n        x = Flatten()(x)\n        x = Dense(classes, activation='softmax', name='fc1000')(x)\n    else:\n        if pooling == 'avg':\n            x = GlobalAveragePooling2D()(x)\n        elif pooling == 'max':\n            x = GlobalMaxPooling2D()(x)\n\n    # Ensure that the model takes into account\n    # any potential predecessors of `input_tensor`.\n    if input_tensor is not None:\n        inputs = get_source_inputs(input_tensor)\n    else:\n        inputs = img_input\n    # Create model.\n    model = Model(inputs, x, name='resnet101')\n    \n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"id":"OuhFsoBPy7Xn","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"id":"PHpda1qdulOt","colab_type":"code","outputId":"547d923e-5010-4232-d42b-363027b9ea86","colab":{"base_uri":"https://localhost:8080/","height":494},"trusted":true},"cell_type":"code","source":"resnet = ResNet101(input_shape=(224, 224, 3), classes=185)","execution_count":null,"outputs":[]},{"metadata":{"id":"fOlTlGsDulOw","colab_type":"code","outputId":"ed431248-3041-4afa-f713-101e896fcdeb","colab":{"base_uri":"https://localhost:8080/","height":1000},"trusted":true},"cell_type":"code","source":"resnet.summary()","execution_count":null,"outputs":[]},{"metadata":{"id":"XQa9FrDtulOz","colab_type":"code","outputId":"5ff00f2b-2af0-4e62-a189-6b74d4650d2f","colab":{"base_uri":"https://localhost:8080/","height":87},"trusted":true},"cell_type":"code","source":"sgd = optimizers.SGD(lr=0.01, decay=1e-5, momentum=0.9, nesterov=False)\nresnet.compile(optimizer=sgd,loss='categorical_crossentropy',metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"id":"X0ZVGc65k1fs","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"from keras.callbacks import ModelCheckpoint\ncheckpoint = ModelCheckpoint(\"resweight.h5\", monitor='loss', verbose=1,\n    save_best_only=False, mode='auto', period=5)","execution_count":null,"outputs":[]},{"metadata":{"id":"K4lFBWN5ulO1","colab_type":"code","outputId":"1df8c85d-50e5-4239-cc0b-2b501c2cddda","colab":{"base_uri":"https://localhost:8080/"},"trusted":true},"cell_type":"code","source":"# 10% oversampling\nhistory = resnet.fit(x=X_train, y=y_train, batch_size=32, epochs=20, verbose=1, callbacks=[checkpoint], validation_split=0.0, validation_data=(X_val, y_val), shuffle=False)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\n#os.chdir(r'kaggle/working')\nresnet.save('model.h5')\nresnet.save_weights('rewsweights.h5')","execution_count":null,"outputs":[]},{"metadata":{"id":"rxVPbi_CulO4","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.save(r'X_train', X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.save(r'X_val', X_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.save(r'y_train', y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history.history","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 10% oversampling\nhistory2 = resnet.fit(x=X_train, y=y_train, batch_size=32, epochs=20, verbose=1, callbacks=[checkpoint], validation_split=0.0, validation_data=(X_val, y_val), shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"resnet.save('model2.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"resnet.save_weights('resweights2.h5')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Overfitting"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_combined = np.concatenate((X_train, X_val), axis = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_combined.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del X_train, X_val","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_combined = np.concatenate((y_train, y_val), axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.display import FileLink\nlink = FileLink('model.h5')\nlink","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.display import HTML\nimport base64\n\ndef create_download_link(title = \"Download model file\", filename = \"model.h5\"):  \n    csv = df.to_csv()\n    b64 = base64.b64encode(encode())\n    payload = b64.decode()\n    html = f'<a target=\"_blank\">{title}</a>'\n    return HTML(html)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.0"},"colab":{"name":"Kriti 2019.ipynb","provenance":[],"machine_shape":"hm"}},"nbformat":4,"nbformat_minor":1}