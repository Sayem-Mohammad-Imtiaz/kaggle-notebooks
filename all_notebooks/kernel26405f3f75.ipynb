{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"%cd /kaggle/input","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"!ls","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''Some helper functions for PyTorch, including:\n    - get_mean_and_std: calculate the mean and std value of dataset.\n    - msr_init: net parameter initialization.\n    - progress_bar: progress bar mimic xlua.progress.\n'''\nimport sys\nimport time\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.init as init\n\n\ndef get_mean_and_std(dataset):\n    '''Compute the mean and std value of dataset.'''\n    dataloader = torch.utils.data.DataLoader(\n        dataset, batch_size=1, shuffle=True, num_workers=2)\n    mean = torch.zeros(3)\n    std = torch.zeros(3)\n    print('==> Computing mean and std..')\n    for inputs, _ in dataloader:\n        for i in range(3):\n            mean[i] += inputs[:, i, :, :].mean()\n            std[i] += inputs[:, i, :, :].std()\n    mean.div_(len(dataset))\n    std.div_(len(dataset))\n    return mean, std\n\n\ndef init_params(net):\n    '''Init layer parameters.'''\n    for m in net.modules():\n        if isinstance(m, nn.Conv2d):\n            init.kaiming_normal(m.weight, mode='fan_out')\n            if m.bias:\n                init.constant(m.bias, 0)\n        elif isinstance(m, nn.BatchNorm2d):\n            init.constant(m.weight, 1)\n            init.constant(m.bias, 0)\n        elif isinstance(m, nn.Linear):\n            init.normal(m.weight, std=1e-3)\n            if m.bias:\n                init.constant(m.bias, 0)\n\n\nTERM_WIDTH = 100\nTOTAL_BAR_LENGTH = 20.\nlast_time = time.time()\nbegin_time = last_time\n\n\ndef progress_bar(current, total, msg=None):\n    global last_time, begin_time\n    if current == 0:\n        begin_time = time.time()  # Reset for new bar.\n\n    cur_len = int(TOTAL_BAR_LENGTH*current/total)\n    rest_len = int(TOTAL_BAR_LENGTH - cur_len) - 1\n\n    sys.stdout.write(' [')\n    for i in range(cur_len):\n        sys.stdout.write('=')\n    sys.stdout.write('>')\n    for i in range(rest_len):\n        sys.stdout.write('.')\n    sys.stdout.write(']')\n\n    cur_time = time.time()\n    step_time = cur_time - last_time\n    last_time = cur_time\n    tot_time = cur_time - begin_time\n\n    L = []\n    L.append('  Step: %s' % format_time(step_time))\n    L.append(' | Tot: %s' % format_time(tot_time))\n    if msg:\n        L.append(' | ' + msg)\n\n    msg = ''.join(L)\n    sys.stdout.write(msg)\n    for i in range(TERM_WIDTH-int(TOTAL_BAR_LENGTH)-len(msg)-3):\n        sys.stdout.write(' ')\n\n    # Go back to the center of the bar.\n    for i in range(TERM_WIDTH-int(TOTAL_BAR_LENGTH/2)+2):\n        sys.stdout.write('\\b')\n    sys.stdout.write(' %d/%d ' % (current+1, total))\n\n    if current < total-1:\n        sys.stdout.write('\\r')\n    else:\n        sys.stdout.write('\\n')\n    sys.stdout.flush()\n\n\ndef format_time(seconds):\n    days = int(seconds / 3600/24)\n    seconds = seconds - days*3600*24\n    hours = int(seconds / 3600)\n    seconds = seconds - hours*3600\n    minutes = int(seconds / 60)\n    seconds = seconds - minutes*60\n    secondsf = int(seconds)\n    seconds = seconds - secondsf\n    millis = int(seconds*1000)\n\n    f = ''\n    i = 1\n    if days > 0:\n        f += str(days) + 'D'\n        i += 1\n    if hours > 0 and i <= 2:\n        f += str(hours) + 'h'\n        i += 1\n    if minutes > 0 and i <= 2:\n        f += str(minutes) + 'm'\n        i += 1\n    if secondsf > 0 and i <= 2:\n        f += str(secondsf) + 's'\n        i += 1\n    if millis > 0 and i <= 2:\n        f += str(millis) + 'ms'\n        i += 1\n    if f == '':\n        f = '0ms'\n    return f","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch.utils.data\nimport random\n\nused_indices = []\nclass FilteredDataset(torch.utils.data.dataset.Dataset):\n    def __init__(self, dataset, wanted_labels):\n        self.parent = dataset\n        self.indices = []\n        for index, (img, lab) in enumerate(dataset):\n            if lab in wanted_labels:\n                if random.random() > 0.8:\n                    self.indices.append(index)\n                    used_indices.append(index)\n            else:\n                if random.random() <= 0.8:\n                    self.indices.append(index)\n                    used_indices.append(index)\n    \n    def __getitem__(self, index):\n        return self.parent[self.indices[index]]\n    \n    def __len__(self):\n        return len(self.indices)\n    \nclass RestOfDataset(torch.utils.data.dataset.Dataset):\n    def __init__(self, dataset, used_inds):\n        self.parent = dataset\n        self.indices = []\n        for index, (img, lab) in enumerate(dataset):\n           if not (index in used_inds):\n                self.indices.append(index)\n    \n    def __getitem__(self, index):\n        return self.parent[self.indices[index]]\n    \n    def __len__(self):\n        return len(self.indices)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from __future__ import print_function\n\nimport argparse\nimport os\nfrom datetime import datetime\n\nimport torch\nimport torch.backends.cudnn as cudnn\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nimport torchvision.datasets as datasets\nimport torchvision.transforms as transforms","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dataset 1 ==> airplane automobile dog ship truck\n# Dataset 2 ==> the rest\n!ls cinic10/train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"traindir = os.path.join('cinic10/', 'train')\nvalidatedir = os.path.join('cinic10', 'valid')\ntestdir = os.path.join('cinic10', 'test')\ncinic_mean = [0.47889522, 0.47227842, 0.43047404]\ncinic_std = [0.24205776, 0.23828046, 0.25874835]\nnormalize = transforms.Normalize(mean=cinic_mean, std=cinic_std)\n\ntrain_transform = transforms.Compose([\n    transforms.RandomCrop(32, padding=4),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=cinic_mean, std=cinic_std)\n])\n\ntrainset_primary = datasets.ImageFolder(root=traindir, transform=train_transform)\ntrainset_1 = FilteredDataset(trainset_primary, [0, 1, 5, 8, 9])\ntrainset_2 = RestOfDataset(trainset_primary, used_indices)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"[+] len of trainset_1: \", len(trainset_1))\nprint(\"[+] len of trainset_2: \", len(trainset_2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nshould_buffer = True\n\ncfg = {\n    'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n}\n\n# Both workers and master run a VGG16 but the master's VGG is modified to have only one output value\n\n\nclass VGG(nn.Module):\n    def __init__(self, vgg_name):\n        super(VGG, self).__init__()\n        self.features = self._make_layers(cfg[vgg_name])\n        self.classifier = nn.Linear(512, 10)\n        self.buffer = []\n\n    def forward(self, x):\n        out = self.features(x)\n        out = out.view(out.size(0), -1)\n        out = self.classifier(out)\n        if should_buffer:\n            self.buffer.append(out)\n        return out\n\n    def _make_layers(self, cfg):\n        layers = []\n        in_channels = 3\n        for x in cfg:\n            if x == 'M':\n                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n            else:\n                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1),\n                           nn.BatchNorm2d(x),\n                           nn.ReLU(inplace=True)]\n                in_channels = x\n        layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\n        return nn.Sequential(*layers)\n    \n    def return_buffer(self):\n        return self.buffer\n\nclass VGG_MASTER(nn.Module):\n    def __init__(self, vgg_name):\n        super(VGG, self).__init__()\n        self.features = self._make_layers(cfg[vgg_name])\n        self.classifier = nn.Linear(512, 1) \n        self.buffer = []\n\n    def forward(self, x):\n        out = self.features(x)\n        out = out.view(out.size(0), -1)\n        out = self.classifier(out)\n        if should_buffer:\n            self.buffer.append(out)\n        return out\n\n    def _make_layers(self, cfg):\n        layers = []\n        in_channels = 3\n        for x in cfg:\n            if x == 'M':\n                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n            else:\n                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1),\n                           nn.BatchNorm2d(x),\n                           nn.ReLU(inplace=True)]\n                in_channels = x\n        layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\n        return nn.Sequential(*layers)\n    \n    def return_buffer(self):\n        return self.buffer\n\ndef vgg16():\n    return VGG('VGG16')\n\ndef vgg16_master():\n    return VGG_MASTER('VGG16')\n\ndef test():\n    net = vgg16()\n    x = torch.randn(2, 3, 32, 32)\n    y = net(x)\n    print(y.size())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"parser = argparse.ArgumentParser(description='PyTorch CINIC10 Training')\nparser.add_argument('--data', metavar='DIR', default='cinic10',\n                    help='path to dataset (default: cinic10)')\nparser.add_argument('-j', '--workers', default=2, type=int, metavar='N',\n                    help='number of data loading workers (default: 2)')\nparser.add_argument('--epochs', default=15, type=int, metavar='N',\n                    help='number of total epochs to run')\nparser.add_argument('-b', '--batch-size', default=64, type=int,\n                    metavar='N',\n                    help='mini-batch size (default: 64), this is the total '\n                         'batch size of all GPUs on the current node when '\n                         'using Data Parallel or Distributed Data Parallel')\nparser.add_argument('--lr', '--learning-rate', default=0.1, type=float,\n                    metavar='LR', help='initial learning rate', dest='lr')\nparser.add_argument('--momentum', default=0.9, type=float, metavar='M',\n                    help='momentum')\nparser.add_argument('--wd', '--weight-decay', default=1e-4, type=float,\n                    metavar='W', help='weight decay (default: 1e-4)',\n                    dest='weight_decay')\nparser.add_argument('--no-cuda', action='store_true', default=False,\n                    help='disables CUDA training')\n\nargs = parser.parse_args(['--data', 'cinic10'])\nargs.cuda = not args.no_cuda and torch.cuda.is_available()\n\nmodel1 = vgg16()\nmodel2 = vgg16()\n\nif args.cuda:\n    model1.features = torch.nn.DataParallel(model.features)\n    model2.features = torch.nn.DataParallel(model.features)\n    model1.cuda()\n    model2.cuda()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define loss function (criterion), optimizer and learning rate scheduler\ncriterion = torch.nn.CrossEntropyLoss()\noptimizer1 = torch.optim.SGD(model1.parameters(),\n                            lr=args.lr,\n                            momentum=args.momentum,\n                            weight_decay=args.weight_decay)\noptimizer2 = torch.optim.SGD(model2.parameters(),\n                            lr=args.lr,\n                            momentum=args.momentum,\n                            weight_decay=args.weight_decay)\nscheduler1 = CosineAnnealingLR(optimizer=optimizer1, T_max=args.epochs, eta_min=0)\nscheduler2 = CosineAnnealingLR(optimizer=optimizer2, T_max=args.epochs, eta_min=0)\n\n\ndef train1(epoch):\n    should_buffer = False\n    print('\\nEpoch: %d' % epoch)\n    cudnn.benchmark = True\n    model1.train()\n    train_loss = 0\n    correct = 0\n    total = 0\n    for batch_idx, (inputs, targets) in enumerate(trainset_1):\n        print(inputs.shape)\n        if args.cuda:\n            inputs, targets = inputs.cuda(), targets.cuda()\n        optimizer1.zero_grad()\n        outputs = model1(inputs)\n        loss = criterion(outputs, targets)\n        loss.backward()\n        optimizer1.step()\n\n        train_loss += loss.item()\n        _, predicted = outputs.max(1)\n        total += targets.size(0)\n        correct += predicted.eq(targets).sum().item()\n\n        progress_bar(batch_idx, len(trainloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n                     % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n\n\ndef train2(epoch):\n    should_buffer = False\n    print('\\nEpoch: %d' % epoch)\n    cudnn.benchmark = True\n    model2.train()\n    train_loss = 0\n    correct = 0\n    total = 0\n    for batch_idx, (inputs, targets) in enumerate(trainset_2):\n        if args.cuda:\n            inputs, targets = inputs.cuda(), targets.cuda()\n        optimizer2.zero_grad()\n        outputs = model2(inputs)\n        loss = criterion(outputs, targets)\n        loss.backward()\n        optimizer2.step()\n\n        train_loss += loss.item()\n        _, predicted = outputs.max(1)\n        total += targets.size(0)\n        correct += predicted.eq(targets).sum().item()\n\n        progress_bar(batch_idx, len(trainloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n                     % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for epoch in range(0, args.epochs):\n    scheduler1.step()\n    train1(epoch)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}