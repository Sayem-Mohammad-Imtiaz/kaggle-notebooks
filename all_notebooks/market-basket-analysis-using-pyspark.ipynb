{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Market Basket Analysis using PySpark's Implementation of FPGrowth\n\nFPGrowth is an algorithm that performs market basket analysis, similar to the Apriori algorithm. I first used it when I ran into resource issues with Apriori and I was impressed with the speed. So I am giving it a try on this dataset using pyspark. The [documentation for FPGrowth](https://spark.apache.org/docs/latest/ml-frequent-pattern-mining.html) is pretty straightforward and describes the hyperparameters and the results.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"execution":{"iopub.status.busy":"2021-07-04T09:37:00.641532Z","iopub.execute_input":"2021-07-04T09:37:00.641973Z","iopub.status.idle":"2021-07-04T09:37:00.666505Z","shell.execute_reply.started":"2021-07-04T09:37:00.641867Z","shell.execute_reply":"2021-07-04T09:37:00.665345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install pyspark\n!pip install pyspark_dist_explore # Used for a histogram","metadata":{"execution":{"iopub.status.busy":"2021-07-04T09:37:54.606339Z","iopub.execute_input":"2021-07-04T09:37:54.606715Z","iopub.status.idle":"2021-07-04T09:38:08.65672Z","shell.execute_reply.started":"2021-07-04T09:37:54.606676Z","shell.execute_reply":"2021-07-04T09:38:08.655278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pyspark import SparkContext\n# Rather than generally using the functions, I should explicitly import the ones I want.\nfrom pyspark.sql import functions as f, SparkSession, Column\nfrom pyspark_dist_explore import hist\nimport matplotlib.pyplot as plt\nfrom pyspark.ml.fpm import FPGrowth\n\n","metadata":{"execution":{"iopub.status.busy":"2021-07-04T09:38:42.367318Z","iopub.execute_input":"2021-07-04T09:38:42.367709Z","iopub.status.idle":"2021-07-04T09:38:42.785463Z","shell.execute_reply.started":"2021-07-04T09:38:42.367666Z","shell.execute_reply":"2021-07-04T09:38:42.784419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a spark session. All sorts of settings can be specified here. \nspark = SparkSession.builder \\\n    .appName(\"arlUsingPyspark\") \\\n    .getOrCreate()","metadata":{"execution":{"iopub.status.busy":"2021-07-04T09:39:29.611325Z","iopub.execute_input":"2021-07-04T09:39:29.611734Z","iopub.status.idle":"2021-07-04T09:39:29.617392Z","shell.execute_reply.started":"2021-07-04T09:39:29.611698Z","shell.execute_reply":"2021-07-04T09:39:29.616317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Read in the data \n\nI didn't end up using the ID number of the customer, but one thing that is important to know about pyspark dataframes is that they do not preserve order once they are sliced and diced. This dataset relies on the order of the two dataframes from the csv files having their order preserved, because the basket does not contain the customer ID number. Since I didn't need the customer ID number, I assigned a monotonically increasing ID number to each row as the file is read in. \n\nThis monotonically increasing ID number is not sequential, so it cannot be used directly to match the rows of the two dataframes. If I had needed the customer ID number to be associated with the basket, I would have had to use a window function over the ID number to create an index, and then match on the index. ","metadata":{}},{"cell_type":"code","source":"df = spark.read.csv(\"/kaggle/input/groceries-dataset-for-market-basket-analysismba/basket.csv\", header=True).withColumn(\"id\", f.monotonically_increasing_id())\ndf_all = spark.read.csv(\"/kaggle/input/groceries-dataset-for-market-basket-analysismba/Groceries data.csv\", header=True).withColumn(\"id\", f.monotonically_increasing_id())","metadata":{"execution":{"iopub.status.busy":"2021-07-04T09:46:46.416895Z","iopub.execute_input":"2021-07-04T09:46:46.417439Z","iopub.status.idle":"2021-07-04T09:46:51.587809Z","shell.execute_reply.started":"2021-07-04T09:46:46.4174Z","shell.execute_reply":"2021-07-04T09:46:51.586429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Show is pyspark's version of head(), although it can be slow so I do try to skip this. \ndf.show(5)\ndf_all.show(5)","metadata":{"execution":{"iopub.status.busy":"2021-07-04T09:48:03.838359Z","iopub.execute_input":"2021-07-04T09:48:03.838791Z","iopub.status.idle":"2021-07-04T09:48:04.314251Z","shell.execute_reply.started":"2021-07-04T09:48:03.838757Z","shell.execute_reply":"2021-07-04T09:48:04.313204Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# printSchema() shows the structure of the dataframe. This is important for debugging.\ndf.printSchema()\ndf_all.printSchema()","metadata":{"execution":{"iopub.status.busy":"2021-07-04T09:49:04.538402Z","iopub.execute_input":"2021-07-04T09:49:04.538795Z","iopub.status.idle":"2021-07-04T09:49:04.556372Z","shell.execute_reply.started":"2021-07-04T09:49:04.538763Z","shell.execute_reply":"2021-07-04T09:49:04.55534Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# How many baskets are there per customer? \n\nI wanted to look at the number of baskets each customer had in the dataset. ","metadata":{}},{"cell_type":"code","source":"num_baskets = df_all.groupBy(\"Member_number\").count()\nnum_baskets.show(5)","metadata":{"execution":{"iopub.status.busy":"2021-07-04T09:51:10.229339Z","iopub.execute_input":"2021-07-04T09:51:10.229706Z","iopub.status.idle":"2021-07-04T09:51:10.763396Z","shell.execute_reply.started":"2021-07-04T09:51:10.229674Z","shell.execute_reply":"2021-07-04T09:51:10.762314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# The distribution of the number of baskets\n\nCreate a histogram of the number of baskets using pyspark_dist_explore. This library can create some fast visualizations on a pyspark dataframe, similar to matplotlib. ","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots()\n\nhist(ax, num_baskets.select('count'), bins = 30, color=['blue'])","metadata":{"execution":{"iopub.status.busy":"2021-07-04T09:52:50.195446Z","iopub.execute_input":"2021-07-04T09:52:50.195815Z","iopub.status.idle":"2021-07-04T09:52:57.665162Z","shell.execute_reply.started":"2021-07-04T09:52:50.195785Z","shell.execute_reply":"2021-07-04T09:52:57.66415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Run PySpark's implementation of FPGrowth\n\nFirst step is to collect the baskets into sets. FPGrowth requires each basket to be an array that looks like:\n\n* ['item1','item2', 'imem3']\n\nThe basket dataframe uses wide rather than long format, with Null if the basket contains fewer than 10 items. ","metadata":{}},{"cell_type":"code","source":"df_basket = df.select(\"id\", f.array([df[c] for c in df.columns[:11]]).alias(\"basket\"))\ndf_basket.printSchema()\ndf_basket.show(3, False) # False tells show to not truncate the columns when printing.","metadata":{"execution":{"iopub.status.busy":"2021-07-04T09:59:56.020396Z","iopub.execute_input":"2021-07-04T09:59:56.02086Z","iopub.status.idle":"2021-07-04T09:59:56.247351Z","shell.execute_reply.started":"2021-07-04T09:59:56.02082Z","shell.execute_reply":"2021-07-04T09:59:56.245969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### There should not be any nulls in the array. Remove using array_except()\n\nThis will be the final dataframe used for FPGrowth. ","metadata":{}},{"cell_type":"code","source":"df_aggregated = df_basket.select(\"id\", f.array_except(\"basket\", f.array(f.lit(None))).alias(\"basket\"))\ndf_aggregated.show(3, False)","metadata":{"execution":{"iopub.status.busy":"2021-07-04T10:01:32.843431Z","iopub.execute_input":"2021-07-04T10:01:32.843827Z","iopub.status.idle":"2021-07-04T10:01:33.058533Z","shell.execute_reply.started":"2021-07-04T10:01:32.843795Z","shell.execute_reply":"2021-07-04T10:01:33.057357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Hyperparameters\n\nThe hyperparameters used in FPGrowth are minimum support, minimum confidence, and number of partitions. \n\n* minSupport - The minimum support of an item to be considered in a frequent itemset. \n* minConfidence - The minimum confidence for generating an association rule from an itemset. \n* numPartitions - The number of partitions used to distribute the work. This is Spark-specific. \n\nThe default number of partitions is the number of partitions for the input dataset. ","metadata":{}},{"cell_type":"code","source":"# Run FPGrowth and fit the model.\nfp = FPGrowth(minSupport=0.001, minConfidence=0.001, itemsCol='basket', predictionCol='prediction')\nmodel = fp.fit(df_aggregated)","metadata":{"execution":{"iopub.status.busy":"2021-07-04T10:08:20.161187Z","iopub.execute_input":"2021-07-04T10:08:20.161629Z","iopub.status.idle":"2021-07-04T10:08:21.325713Z","shell.execute_reply.started":"2021-07-04T10:08:20.161594Z","shell.execute_reply":"2021-07-04T10:08:21.324594Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# View a subset of the frequent itemset. \nmodel.freqItemsets.show(10, False)","metadata":{"execution":{"iopub.status.busy":"2021-07-04T10:08:35.581304Z","iopub.execute_input":"2021-07-04T10:08:35.581691Z","iopub.status.idle":"2021-07-04T10:08:36.60265Z","shell.execute_reply.started":"2021-07-04T10:08:35.581658Z","shell.execute_reply":"2021-07-04T10:08:36.601537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Use filter to view just the association rules with the highest confidence.\nmodel.associationRules.filter(model.associationRules.confidence>0.15).show(20, False)","metadata":{"execution":{"iopub.status.busy":"2021-07-04T10:10:28.995859Z","iopub.execute_input":"2021-07-04T10:10:28.996238Z","iopub.status.idle":"2021-07-04T10:10:29.194028Z","shell.execute_reply.started":"2021-07-04T10:10:28.996183Z","shell.execute_reply":"2021-07-04T10:10:29.193049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Let's create a prediction based on the generated association rules\n\nThis is pretty similar to creating a prediction using other methods. The data column needs to have the same column name as the column specified in the model fit.","metadata":{}},{"cell_type":"code","source":"# Create a PySpark dataframe\ncolumns = ['basket']\nnew_data = [(['ham', 'yogurt', 'light bulbs'],), (['jam', 'cocoa drinks', 'pet care'],)]\nrdd = spark.sparkContext.parallelize(new_data)\nnew_df = rdd.toDF(columns)\nnew_df.printSchema()\nnew_df.show(2,False)","metadata":{"execution":{"iopub.status.busy":"2021-07-04T10:36:12.610872Z","iopub.execute_input":"2021-07-04T10:36:12.611373Z","iopub.status.idle":"2021-07-04T10:36:12.991287Z","shell.execute_reply.started":"2021-07-04T10:36:12.611335Z","shell.execute_reply":"2021-07-04T10:36:12.990147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Predict!\n\nNow that we have a new PySpark dataframe with data, predict. The first basket generates numerous predictions based on the association rules, however the second basket does not generate any. ","metadata":{}},{"cell_type":"code","source":"model.transform(new_df).show(5, False)","metadata":{"execution":{"iopub.status.busy":"2021-07-04T10:36:17.449539Z","iopub.execute_input":"2021-07-04T10:36:17.449952Z","iopub.status.idle":"2021-07-04T10:36:17.81537Z","shell.execute_reply.started":"2021-07-04T10:36:17.449916Z","shell.execute_reply":"2021-07-04T10:36:17.814112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}