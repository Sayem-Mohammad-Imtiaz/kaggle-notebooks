{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Machine learning is a problem of trade-offs. The classic issue is over-fitting versus under-fitting. Over-fitting happens when a model fits on training data so well and it fails to generalize well.ie, it also learns noises on top of the signal. Under-fitting is an opposite event: the model is too simple to find the patterns in the data. \n\n[Decision trees](https://medium.com/datadriveninvestor/decision-tree-algorithm-with-hands-on-example-e6c2afb40d38) are extremly popular and useful model in machine learning. But it can easily get overfit. Pruning is one of the mainly used technique to avoid/overcome overfitting. In this kernal we will discuss about 2 commonly used pruning types.\n\n<br>\n\n***1. Prepruning*** <br>\n***2. Postpruning***\n\n<br>","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom sklearn import tree\nfrom sklearn.metrics import accuracy_score,confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = '/kaggle/input/heart-disease-uci/heart.csv'\ndf = pd.read_csv(data)\ndf.head()","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We are not getting into the nitty-gritty details of this dataset. The main aim of this kernel is to show you how to pre prune and post prune the decision tree.s","metadata":{}},{"cell_type":"code","source":"X = df.drop(columns=['target'])\ny = df['target']\nprint(X.shape)\nprint(y.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Splitting dataset to train and test","metadata":{}},{"cell_type":"code","source":"x_train,x_test,y_train,y_test = train_test_split(X,y,stratify=y)\nprint(x_train.shape)\nprint(x_test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"First we will fit a normal decision tree without any fine tuning and check the results ","metadata":{}},{"cell_type":"code","source":"clf = tree.DecisionTreeClassifier(random_state=0)\nclf.fit(x_train,y_train)\ny_train_pred = clf.predict(x_train)\ny_test_pred = clf.predict(x_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Visualizing decision tree","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(20,20))\nfeatures = df.columns\nclasses = ['Not heart disease','heart disease']\ntree.plot_tree(clf,feature_names=features,class_names=classes,filled=True)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# helper function\ndef plot_confusionmatrix(y_train_pred,y_train,dom):\n    print(f'{dom} Confusion matrix')\n    cf = confusion_matrix(y_train_pred,y_train)\n    sns.heatmap(cf,annot=True,yticklabels=classes\n               ,xticklabels=classes,cmap='Blues', fmt='g')\n    plt.tight_layout()\n    plt.show()\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'Train score {accuracy_score(y_train_pred,y_train)}')\nprint(f'Test score {accuracy_score(y_test_pred,y_test)}')\nplot_confusionmatrix(y_train_pred,y_train,dom='Train')\nplot_confusionmatrix(y_test_pred,y_test,dom='Test')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that in our train data we have 100% accuracy (100 % precison and recall). But in \ntest data model is not well generalizing. We have just 75% accuracy.\nOver model is clearly overfitting. We will avoid overfitting through pruning. We will do cost complexity prunning","metadata":{}},{"cell_type":"markdown","source":"## 1. Pre pruning techniques\n\nPre pruning is nothing but stoping the growth of decision tree on an early stage. For that we can limit the growth of trees by setting constrains. We can limit parameters like *max_depth* , *min_samples* etc.\n\nAn effective way to do is that we can grid search those parameters and choose the optimum values that gives better performace on test data. ","metadata":{}},{"cell_type":"markdown","source":"As of now we will control these parameters\n* max_depth: maximum depth of decision tree\n* min_sample_split: The minimum number of samples required to split an internal node:\n* min_samples_leaf: The minimum number of samples required to be at a leaf node.","metadata":{}},{"cell_type":"code","source":"params = {'max_depth': [2,4,6,8,10,12],\n         'min_samples_split': [2,3,4],\n         'min_samples_leaf': [1,2]}\n\nclf = tree.DecisionTreeClassifier()\ngcv = GridSearchCV(estimator=clf,param_grid=params)\ngcv.fit(x_train,y_train)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = gcv.best_estimator_\nmodel.fit(x_train,y_train)\ny_train_pred = model.predict(x_train)\ny_test_pred = model.predict(x_test)\n\nprint(f'Train score {accuracy_score(y_train_pred,y_train)}')\nprint(f'Test score {accuracy_score(y_test_pred,y_test)}')\nplot_confusionmatrix(y_train_pred,y_train,dom='Train')\nplot_confusionmatrix(y_test_pred,y_test,dom='Test')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(20,20))\nfeatures = df.columns\nclasses = ['Not heart disease','heart disease']\ntree.plot_tree(model,feature_names=features,class_names=classes,filled=True)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that tree is pruned and there is improvement in test accuracy.But still there is still scope of improvement.","metadata":{}},{"cell_type":"markdown","source":"## 2. Post pruning techniques\n\nThere are several post pruning techniques. Cost complexity pruning is one of the important among them.\n\n## Cost Complexity Pruning\n\n\nDecision trees can easily overfit. One way to avoid it is to limit the growth of trees by setting constrains. We can limit parameters like *max_depth* , *min_samples* etc. But a most effective way is to use post pruning methods like cost complexity pruning. This helps to improve test accuracy and get a better model.\n\nCost complexity pruning is all about finding the right parameter for **alpha**.We will get the alpha values for this tree and will check the accuracy with the pruned trees.\n\n\nTo know more about cost complexity pruning watch [this vedio from Josh Starmer.](https://www.youtube.com/watch?v=D0efHEJsfHo&list=PLblh5JKOoLUICTaGLRoHQDuF_7q2GfuJF&index=41)\n\n\n","metadata":{}},{"cell_type":"code","source":"path = clf.cost_complexity_pruning_path(x_train, y_train)\nccp_alphas, impurities = path.ccp_alphas, path.impurities\nprint(ccp_alphas)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# For each alpha we will append our model to a list\nclfs = []\nfor ccp_alpha in ccp_alphas:\n    clf = tree.DecisionTreeClassifier(random_state=0, ccp_alpha=ccp_alpha)\n    clf.fit(x_train, y_train)\n    clfs.append(clf)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> \n\nWe will remove the last element in clfs and ccp_alphas, because it is the trivial tree with only one node.\n","metadata":{}},{"cell_type":"code","source":"clfs = clfs[:-1]\nccp_alphas = ccp_alphas[:-1]\nnode_counts = [clf.tree_.node_count for clf in clfs]\ndepth = [clf.tree_.max_depth for clf in clfs]\nplt.scatter(ccp_alphas,node_counts)\nplt.scatter(ccp_alphas,depth)\nplt.plot(ccp_alphas,node_counts,label='no of nodes',drawstyle=\"steps-post\")\nplt.plot(ccp_alphas,depth,label='depth',drawstyle=\"steps-post\")\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Observation: As alpha increases no of nodes and depth decreases","metadata":{}},{"cell_type":"code","source":"train_acc = []\ntest_acc = []\nfor c in clfs:\n    y_train_pred = c.predict(x_train)\n    y_test_pred = c.predict(x_test)\n    train_acc.append(accuracy_score(y_train_pred,y_train))\n    test_acc.append(accuracy_score(y_test_pred,y_test))\n\nplt.scatter(ccp_alphas,train_acc)\nplt.scatter(ccp_alphas,test_acc)\nplt.plot(ccp_alphas,train_acc,label='train_accuracy',drawstyle=\"steps-post\")\nplt.plot(ccp_alphas,test_acc,label='test_accuracy',drawstyle=\"steps-post\")\nplt.legend()\nplt.title('Accuracy vs alpha')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can choose alpha = 0.020","metadata":{}},{"cell_type":"code","source":"clf_ = tree.DecisionTreeClassifier(random_state=0,ccp_alpha=0.020)\nclf_.fit(x_train,y_train)\ny_train_pred = clf_.predict(x_train)\ny_test_pred = clf_.predict(x_test)\n\nprint(f'Train score {accuracy_score(y_train_pred,y_train)}')\nprint(f'Test score {accuracy_score(y_test_pred,y_test)}')\nplot_confusionmatrix(y_train_pred,y_train,dom='Train')\nplot_confusionmatrix(y_test_pred,y_test,dom='Test')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that now our model is not overfiting and performance on test data have improved","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(20,20))\nfeatures = df.columns\nclasses = ['Not heart disease','heart disease']\ntree.plot_tree(clf_,feature_names=features,class_names=classes,filled=True)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that the size of decision tree significantly got reduced. Also postpruning is much efficient than prepruning.\n\n\nNote: In this kernel we have used accuracy as metric.But the target label is imbalenced. So it is better to use metrics like auc, f1 score etc\n\n**If you find my kernel useful do Upvote!**","metadata":{}}]}