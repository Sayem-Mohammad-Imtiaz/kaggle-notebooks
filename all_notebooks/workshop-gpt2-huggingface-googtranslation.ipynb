{"cells":[{"metadata":{},"cell_type":"markdown","source":"# เกริ่นนำ\n\n\n> นักวิทยาศาสตร์ได้ค้นพบแบคทีเรียชนิดใหม่ที่สามารถอยู่ได้โดยปราศจากออกซิเจน แบคทีเรียเหล่านี้มีชื่อว่า Bacteroides lactis พบได้ในทะเลสาบแม่น้ำและหนองน้ำทั่วโลก นักวิจัยเชื่อว่า B. lactis อาจมีอยู่ในโลกเป็นเวลา 2 ล้านปี พวกเขาเชื่อว่านี่เป็นรูปแบบสิ่งมีชีวิตที่เก่าแก่ที่สุดที่รู้จักบนโลก แม้ว่าสิ่งมีชีวิตใหม่ไม่ได้ผลิตซ้ำหรือวิวัฒนาการในมหาสมุทรเหมือนกับจุลินทรีย์อื่น ๆ แต่มันก็เป็นบรรพบุรุษโดยตรงของจุลินทรีย์อื่น ๆ ทั้งหมดที่อาศัยอยู่ในโลก\n\n> สวัสดีชื่อของฉันคือเจมส์และฉันจะทำหน้าที่เป็นหมอดูแลคุณ เพื่อสุขภาพและความปลอดภัยคุณต้องถอดแจ็คเก็ตและหมวกออก มีปืนอยู่ข้างแขนซ้ายของคุณ ดูอย่างใกล้ชิดและคุณจะเห็นว่ามันถูกโหลด มันเป็นเรมิงตัน 641 นั่นคือทั้งหมดที่ ไม่มีใครได้รับอนุญาตให้มีปืนหรือพกอาวุธเลย ถ้าคุณทำคุณจะถูกยิงทันที อย่าแตะต้องปืน\n> \n\n\nสวัสดีครับ วันนี้เราเปิดบทความกันด้วยนิยายซึ่งอาจจะดูแปลกประหลาดไปสักหน่อย แต่นิยายเหล่านี้ถูกแต่งแบบสุ่มๆ โดยโมเดล Deep Learning ที่ชื่อว่า GPT-2 !!  \n\nวันนี้ ขอต้อนรับเข้าสู่ ThaiKeras Workshop -- GPT-2 Playground ที่ที่เราจะปลดปล่อยจินตนาการ และแต่งเนื้อเรื่องนิยายหรือข่าวต่างๆ ในภาษาไทยด้วยพลังของ GPT-2 และ Google Translate ครับ :D :D \n\nเพื่อนๆ สามารถดู Workshop อื่นๆ ทั้งหมดของเราไม่ว่าจะเป็นงานด้าน Computer Vision, NLP, Fashion, Medical Images พื้นฐาน Numpy และอื่นๆ อีกมากมายบน Kaggle ได้ที่นีี่ครับ https://thaikeras.com/category/workshop/ สำหรับเพื่อนๆ ที่ไม่เคยมี Kaggle account สามารถ set up ได้ด้วยตัวเองตามนี้ครับ https://thaikeras.com/2018/setup-kaggle-workshop/\n\n## ว่าด้วย GPT2\n\nในช่วงปีที่ผ่านมา 1 ในข่าวบันลือโลกของวงการ AI และ DeepLearning ก็คือ โมเดลที่ชื่อ GPT-2 ของทีม OpenAI ที่สามารถแต่งประโยคได้ราวกับมนุษย์ (ในแง่ของไวยากรณ์) บนเรื่อง \"อะไรก็ได้\" ไม่ว่าจะเป็นแนวนิยายวิทยาศาสตร์ นิยายโรแมนติก นิยายสยองขวัญ สารานุกรมแบบ Wikipedia ข่าวบนสื่อต่างๆ ฯลฯ เนื่องจาก GPT-2 นั้นถูกฝึกสอนบน texts ระดับมหาศาลบน internet ชนิดที่มนุษย์ใช้เวลาอ่านทั้งชีวิตก็ไม่จบไม่สิ้น แฟนพันธุ์แท้ของวงการ AI นี้ไม่มีใครไม่รู้จัก GPT-2 แน่นอน \n\nอัน GPT-2 นั้นเป็นหนึ่งในโมเดลของตระกูล Transformers ที่เก่งเรื่อง \"สร้างเนื้อเรื่อง\" ส่วนโมเดลในตระกูล Transformer อื่นๆ ที่ดังระดับโลก อาทิเช่น BERT, ROBERTA, BART, T5 นั้นต่างก็เก่งกาจด้านภาษาอื่นๆ แตกต่างกันออกไป เช่น งานตีความหมายของประโยค งานย่อความ งานแปลภาษา เป็นต้น อย่างไรก็ดีใน Workshop นี้เราจะมาเล่น GPT-2 กันครับ\n\nการทำงานของ GPT-2 ในหลักการนั้นง่ายมากครับ เราเพียงใส่ประโยคเริ่มต้นอะไรไปก็ได้ แล้วเจ้า GPT-2 มันก็จะไปแต่งประโยคต่อขึ้นมาเองอย่างน่ามหัศจรรย์!!! ตัวอย่างเช่น ในตัวอย่างแบคทีเรียข้างต้น เราเพียงใส่ประโยคเริ่มต้นว่า *\"นักวิทยาศาสตร์ได้ค้นพบแบคทีเรียชนิดใหม่ที่สามารถอยู่ได้โดยปราศจากออกซิเจน\"*  ส่วนที่เหลือเจ้า GPT-2 ก็จะเป็นผู้จัดการจินตนการต่อเองครับ และที่เจ๋งสุดๆ ก็คือเราสามารถให้มันแต่งเนื้อเรื่องเป็นร้อยแบบที่แตกต่างกันจากประโยคเริ่มต้นประโยคเดียวของเราก็ยังได้ แบบง่ายๆ !! เช่น\n\n> นักวิทยาศาสตร์ได้ค้นพบแบคทีเรียชนิดใหม่ที่สามารถอยู่ได้โดยปราศจากออกซิเจน จุลินทรีย์ที่ไม่ได้อยู่ในมหาสมุทรเหล่านี้มีชื่อเสียงในด้านความสามารถในการอยู่รอดในสภาวะที่รุนแรงเช่นช่องระบายอากาศใต้ทะเลลึกและอุณหภูมิสูง\n\n> ทีมนักวิจัยนำโดยดร. Danyel Wiercinski แห่งมหาวิทยาลัยฮาร์วาร์ดได้ค้นพบแบคทีเรียเหล่านี้ในตัวอย่างจากพื้นมหาสมุทรนอกชายฝั่งของออสเตรเลีย ในการวิจัยที่ตีพิมพ์ใน PNAS เมื่อสัปดาห์ที่แล้วทีมได้ค้นพบสายพันธุ์ชื่อ Oligophaga melan\n\nสังเกตความเจ๋งที่ว่ามันสามารถตั้งชื่อสายพันธุ์แบคทีเรีย ชื่อนักวิทยาศาสตร์ รวมทั้งชื่อวารสารวิชาการได้อย่างแนบเนียนสุดๆ อีกด้วย  GPT-2 ยังสามารถแต่งเนื้อเรื่องอื่นๆ อีกมากมายที่เราสามารถอ่านกันได้เพลิดเพลินใจไปเลย!! \n\n## HugggingFace ฮีโร่ผู้ทลายกำแพงด้านภาษาของ Deep NLP\n\nถึงแม้ตอนแรกจะมีข้อถกเถียงกันมากมายว่าการปลดปล่อย GPT-2 นั้นจะเป็นอันตรายหรือไม่เพราะคนสามารถนำไปช่วยสร้าง Fake News ต่างๆ ได้ อย่างไรก็ดีหลังจากทีม OpenAI ได้ศึกษาเรื่องข้อดีข้อเสียร่วมกับทีม AI อื่นๆ อีกหลายทีม ว่าข้อดีนั้นมีมากกว่าข้อเสีย ทีม OpenAI ก็เลยตัดสินใจเผยแพร่โมเดล GPT-2 ให้คนทั่วไปใช้ในที่สุด \n\nโดย GPT-2 นั้นได้ถูกเผยแพร่มาสักพักแล้ว โดยมีหลายขนาดตั้งแต่ GPT-2 จิ๋ว กลาง ใหญ่ จนถึง GPT-2 \"ยักษ์\" ที่เก่งกาจขึ้นเรื่อยๆ แต่ก็ทำงานช้าลงเรื่อยๆ เช่นกัน\n\nอย่างไรก็ดี โค้ดของทีม OpenAI นั้นค่อนข้างใช้งานยาก และทีมงาน ThaiKeras เราเองก็หาโค้ด version Keras ไม่ได้สักที เราเลยไม่ได้มีโอกาสลองเล่นอะไรมากนัก จนกระทั่งมีทีม AI ฮีโร่ของโลกที่ชื่อว่า ***HuggingFace*** ออกมาทำ API Opensource สำหรับ transformer ทั้งหมด (ไม่ใช่แค่ GPT-2) ทั้งในเวอร์ชั่น Tensorflow+Keras และ Pytorch ซึ่งใช้งานได้สะดวกและสุดยอดมากๆๆๆๆ\n\nด้วยพลังของ HuggingFace นั้น เราสามารถเรียกใช้ Transformer ต่างๆ กันแบบง่ายเหลือเชื่อ เขียนโปรแกรมสิบกว่าบรรทัดเท่านั้น :D และยังสามารถเซพโหลดข้อมูลจาก Pytorch สลับไปมากับ Keras ได้อีกด้วยทำให้ทำลายกำแพงความแตกต่างทางภาษาของวงการ DeepNLP ไปโดยสิ้นเชิง\n\nนอกจากนี้การ Generate texts ให้เป็นธรรมชาตินั้นยังยากเทคนิกมาตรฐานเช่น Greedy หรือ Beam Search นั้นพบว่าไม่เป็นธรรมชาติและมักให้ประโยคซ้ำๆ มีงานวิจัยใหม่ๆ ออกมามากมายในช่วงปี 2019 และ Team HuggingFace นี้ยัง implement 2 วิธีสร้าง texts ที่ได้รับการยอมรับที่สุดในปัจจุบัน (Top-K & Top-P sampling) ให้เราใช้เพียงกระดิกนิ้ว ซึ่งเพื่อนๆ จะได้เรียนรู้วิธีใน Workshop นี้ด้วยครับ\n\nจึงต้องขอกราบขอบพระคุณทีม HuggingFace https://huggingface.co/ งามๆ มา ณ ที่นี้ด้วยครับ \n\n## Google Translation Python Library\nท้ายที่สุดขอพูดถึง library จิ๋วแต่แจ๋ว https://github.com/ssut/py-googletrans ซึ่งสามารถทำให้เราใช้ google translate ได้ง่ายๆ เพียงหนึ่งบรรทัดเท่านั้น\n\nใน Workshop นี้เราจะไม่ได้ใช้ GPT-2 ที่ถูกสอนบน \"ภาษาไทย\" โดยตรง (เพราะต้องใช้ทรัพยากรสอนที่มหาศาลมาก) แต่เราจะ \"ลักไก่\" ด้วยการ \"แปลไป แปลกลับ\" แทน นั้นคือ เราจะแปล \"ประโยคเริ่มต้นภาษาไทย\" เป็นภาษาอังกฤษแล้วให้ GPT-2 ไปสร้างเนื้อเรื่องต่อ เมื่อแต่งเรื่องเสร็จแล้ว เราก็จะ \"แปลกลับ\" เป็นภาษาไทยอีกรอบกัน :D \n\nถ้าพร้อมแล้วก็เริ่มกันเลยครับ"},{"metadata":{},"cell_type":"markdown","source":"# เริ่มกันเลย\n\nก่อนอื่นเรา install หรือติดตั้ง library สำหรับ Google Translate กันก่อนเลยครับ"},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"!pip install git+https://github.com/ssut/py-googletrans.git","execution_count":null,"outputs":[]},{"metadata":{"cellView":"form","id":"VRLVEKiTEn04","trusted":true},"cell_type":"code","source":"\nimport tensorflow as tf\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# เรา import HuggingFace GPT2 ที่นี่ครับ , TF ข้างหน้า GPT2 หมายถึง Tensorflow-Keras\nfrom transformers import GPT2Tokenizer, TFGPT2LMHeadModel","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ตั้งแต่ version 2.0 เป็นต้นมา Tensorflow ได้ผนวกกับ Keras โดยสมบูรณ์\n# โมเดลหลักๆ ทั้งหมดของ Tensorflow จะอยู่ในรูป Keras โมเดลโดยอัตโนมัติ\ntf.__version__","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# โหลดโมเดลและตัวตัดคำของ GPT-2\n\nการสร้างโมเดล GPT-2 โดยใช้ HuggingFace library มีแค่สองขั้นตอนเท่านั้นคือโหลดตัวตัดคำ และโหลดโมเดลครับ จบ ใช้งานได้เลย\n\nก่อนอื่นโหลดตัวตัดคำของ GPT-2 ซึ่งจะมีคำศัพท์ทั้งหมด 50257 ซึ่งคำศัพท์ที่ GPT2 ใช้เป็น sub-words หรือบางส่วนของคำได้ ไม่จำเป็นต้องเต็มคำ (ในทางเทคนิกเรียกว่า Byte Pair Encoding)"},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = GPT2Tokenizer.from_pretrained('gpt2-large')\ntokenizer.vocab_size","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"ลองตัดคำภาษาอังกฤษดูครับ จะได้ output เป็น index ของศัพท์ที่ตัดคำเรียบร้อยแล้ว"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Encode a text inputs\ntext = \"Who was Jim Henson ? Jim Henson was a\"\nindexed_tokens = tokenizer.encode(text, add_special_tokens=True)\nprint(indexed_tokens, len(indexed_tokens), len(text.split()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"เซลล์ข้างล่างนี้เราลอง tokenizer ชนิดใหม่ที่ทาง HuggingFace เพิ่งเผยแพร่สดๆ ร้อนๆ ครับ ซึ่งเราเรียกว่าว่า fast_tokenizer ซึ่งมีความเร็วมากกว่า Tokenizer ที่เราโหลดมาข้างต้นมากๆ และเราสามารถดูศัพท์ของคำที่ตัดไปแล้วได้ด้วย ว่า index ที่ได้ในเซลล์ข้างบนนั้นแทนคำว่าอะไรบ้าง สังเกตคำว่า jim ถูกตัดเป็น \"j\" กับ \"im\" , \"henson\" ถูกตัดเป็น \"hen\" กับ \"son\" เป็นต้นครับ\n\nใน fast_tokenizer มีรายละเอียดของคำที่ตัดแล้วมากมาย เช่น attention_mask, offset, special_tokens_mask ซึ่งเรายังไม่ได้ใช้ใน notebook นี้\n\nดังนั้นนอกจากเซลล์ข้างล่างที่เราโชว์ไส้ข้างใน tokenzier แล้ว เราจะกลับไปใช้ tokenizer ตัวเก่า (ซึ่งช้ากว่า แต่ไม่มีผลมากกับ playground นี้) เพื่อนๆ จะลองเปลี่ยนโค้ดเล่นไปใช้ตัว fast_tokenizer ก็ย่อมได้ครับ"},{"metadata":{"trusted":true},"cell_type":"code","source":"## นี่คือ fast_tokenizer\nfrom tokenizers import ByteLevelBPETokenizer\n\ntokenizer.save_pretrained('.')\n# Reload it with the huggingface tokenizers library\n# using lowercase=True making fast_tokenizer different from the original one\nfast_tokenizer = ByteLevelBPETokenizer(\"./vocab.json\", \"./merges.txt\", lowercase=True) \n\noutput = fast_tokenizer.encode(text, \n#                                add_special_tokens=True # this option does not exist\n                              )\nprint(output)\nprint(output.ids)\nprint(output.tokens)\nprint(output.attention_mask)\nprint(output.special_tokens_mask)\nprint(output.original_str)\nprint(output.normalized_str)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"โหลด model GPT-2 ขนาด large มาใช้ด้วยคำสั่งเพียงหนึ่งบรรทัด!!\nหมายเหตุ GPT-2 ตัว large นี้ไม่ใช่ ตัวที่แต่งเก่งที่สุด แต่เพื่อประหยัดเวลาในการโหลดและทำนาย"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = TFGPT2LMHeadModel.from_pretrained('gpt2-large')\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ทดลองแต่งประโยคภาษาอังกฤษเล็กน้อย\n\nGPT-2 แท้จริงแล้วเป็นโมเดลที่ใช้แต่งประโยค เรื่องราวเป็นภาษาอังกฤษ โดยเราจะลองใส่ประโยคเริ่มต้นอย่างไรก็ได้ ในที่นี้ผมลองใส่ประโยคเริ่มต้นว่า \"My name is James and I am your doctor\" และลองดูว่า GPT-2 จะแต่งอย่างไรต่อ\n\nโดย HuggingFace นั้นได้ทำฟังก์ชัน generate text ไว้สุดยอด โดยขั้นพื้นฐานที่สุดคือการ generate แบบ greedy นั่นคือการสร้างคำที่น่าจะเป็นคำต่อไป \"มากที่สุด\" (ในทางสถิติที่โมเดลเก็บไว้)\n\nอย่างไรก็ดี ในทางปฏิบัตินั้นการสร้างคำที่น่าจะเป็นมากที่สุดแบบ greedy นั้น มักจะได้คำที่ไม่เป็นธรรมชาติ  และเมื่อประโยคเริ่มยาว จะพูดซ้ำไปซ้ำมา (เนื่องจากสถิติที่โมเดลเก็บไว้นั้นไม่สมบูรณ์แบบในประโยคยาวๆ)"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nids = tokenizer.encode(\"My name is James and I am your doctor\", \n                       return_tensors='tf') # ระบุให้ tokenizer ส่งค่าเป็น tensor ของ tensorflow\ngreedy_output = model.generate(ids, max_length=60)\n\nprint(\"Output:\\n\" + 100 * '-')\nprint(tokenizer.decode(greedy_output[0], skip_special_tokens=False))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"ในช่วงก่อนปี 2018 เราพยายามแก้ประโยคซ้ำๆ ด้วยเทคนิกที่เรียกว่า Beam Search แต่นักวิจัยก็พบว่ามันยังทำงานไม่เป็นธรรมชาติมากนัก\n\nในงานวิจัยใหม่ๆ ในปี 2018-2019 ได้เสนอวิธีสุ่มคำด้วยความน่าจะเป็นมามากมายซึ่ง 2 เทคนิกที่ได้รับการยอมรับคือ 2 เทคนิกที่เรียกว่า top-p กับ top-k sampling ครับ โดยอธิบายง่ายๆ ได้ดังนี้\n\n### Top-K Sampling\nคือการสุ่มคำจากคำที่น่าจะเป็นมากที่สุด K คำแรก (เช่น K = 50)\n\n### Top-P Sampling\nคือการสุ่มคำจาก N คำแรก โดยคำที่ 1 ถึง N นั้นรวมแล้วมีความน่าจะเป็นที่จะเป็น \"คำต่อไป\" อย่างน้อย P (เช่น P = 95%)\n\nHuggingFace อนุญาตให้เราใช้ทั้ง Top-P และ Top-K สองเงื่อนไขพร้อมกันได้ง่ายๆ อีกด้วย (ขอกราบอีกครั้ง) โดยคำที่สุ่มมาต้องเป็นทั้ง top-p และ top-k ครับ\n\nเพื่อนๆ สามารถอ่านรายละเอียดเรื่องการสุ่มคำได้อย่างละเอียดที่นี่ครับ https://huggingface.co/blog/how-to-generate\n\noption สุดพิเศษอีกอันที่ HuggingFace เตรียมให้เราก็คือ `num_return_sequences` ซึ่งระบุให้โมเดลแต่งเนื้อเรื่องมากี่เรื่องก็ได้เพียงระบุง่ายๆ ด้วย option นี้ครับ (ในที่นี้เราให้แต่งมา 5 เรื่อง)\n\nเวลาแต่งแต่ละครั้งเราจะได้เนื้อเรื่องแตกต่างกันออกไป เราลองแต่งซ้ำๆ จนกว่าจะเจอเนื้อเรื่องที่ถูกใจครับ และลองปรับค่า top-p top-k เล่นดูก็ได้ครับ ;D"},{"metadata":{},"cell_type":"markdown","source":"ดูคร่าวๆ จะเห็นว่าทั้ง 5 เรื่องมีความหลากหลายพอสมควร และถ้าเราไม่พอใจ เราสามารถสั่งรันใหม่เพื่อให้สร้างเรื่องใหม่ๆ ได้เรื่อยๆ อย่างไรก็ดีเรารีบไปต่อกันที่ภาษาไทยเลยดีกว่า"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nsample_outputs = model.generate(\n    ids,\n    do_sample=True, \n    max_length=100, # สร้าง texts จำนวน 100 คำ\n    top_k=55, \n    top_p=0.925, \n    num_return_sequences=5 # ให้สร้าง 5 เรื่อง\n)\n\nprint(\"Output:\\n\" + 100 * '-')\nfor i, sample_output in enumerate(sample_outputs):\n  print(\"\\n\"+\"===\"*10)\n  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=False)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# GPT-2 ภาษาไทย!!!\nเราจะมาลักไก่แต่งเรื่องภาษาไทยด้วยการ \"แปลไป\" (แปลประโยคเริ่มต้นเป็น eng) \"แปลกลับ\" (แปล texts จาก GPT-2 เป็นไทย) เพื่อความสนุกกันครับ"},{"metadata":{"trusted":true},"cell_type":"code","source":"from googletrans import Translator\n\ntranslator = Translator()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## ลอง Greedy กันก่อน ซึ่งจะได้คำซ้ำๆ เหมือนเดิม\norig_input=\"สวัสดีครับ ผมชื่อเจมส์ และผมจะทำหน้าที่เป็นหมอดูแลคุณ\"\nnew_input = translator.translate(orig_input, dest='en').text\n\nids = tokenizer.encode(new_input, return_tensors='tf')\ngreedy_output = model.generate(ids, max_length=60)\n\nprint(\"Output:\\n\" + 100 * '-')\norig_output = tokenizer.decode(greedy_output[0], skip_special_tokens=True)\nnew_output = translator.translate(orig_output, dest='th').text\nprint(new_output)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nsample_outputs = model.generate(\n    ids,\n    do_sample=True, \n    max_length=100, \n    top_k=50, \n    top_p=0.925, \n    num_return_sequences=5\n)\n\nprint(\"Output:\\n\" + 100 * '-')\nfor i, sample_output in enumerate(sample_outputs):\n    out = tokenizer.decode(sample_output, skip_special_tokens=True)\n    new_out = translator.translate(out, dest='th').text\n    print(\"\\n\"+\"===\"*10)\n    print(\"ตัวอย่างที่{}: {}\\n\".format(i, new_out))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\norig_input=\"นักวิทยาศาสตร์ได้ค้นพบแบคทีเรียสายพันธุ์ใหม่ที่สามารถอยู่ได้โดยไม่ต้องมีออกซิเจน\"\nnew_input = translator.translate(orig_input, dest='en').text\n\nids = tokenizer.encode(new_input, return_tensors='tf')\n\nsample_outputs = model.generate(\n    ids,\n    do_sample=True, \n    max_length=100, \n    top_k=55, \n    top_p=0.95, \n    num_return_sequences=5\n)\nprint(\"Input: %s\\n\\n\" % orig_input)\nprint(\"Output:\\n\" + 100 * '-')\nfor i, sample_output in enumerate(sample_outputs):\n    out = tokenizer.decode(sample_output, skip_special_tokens=True)\n    new_out = translator.translate(out, dest='th').text\n    print(\"\\n\"+\"===\"*10)\n    print(\"ตัวอย่างที่{}: {}\\n\".format(i, new_out))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\norig_input=\"ทีมนักวิทยาศาสตร์ชาวเยอรมันและญี่ปุ่นได้ค้นพบยาที่จะมาจัดการ Corona Virus\"\nnew_input = translator.translate(orig_input, dest='en').text\n\nids = tokenizer.encode(new_input, return_tensors='tf')\n\nsample_outputs = model.generate(\n    ids,\n    do_sample=True, \n    max_length=100, \n    top_k=55, \n    top_p=0.925, \n    num_return_sequences=5\n)\nprint(\"Input: %s\\n\\n\" % orig_input)\nprint(\"Output:\\n\" + 100 * '-')\nfor i, sample_output in enumerate(sample_outputs):\n    out = tokenizer.decode(sample_output, skip_special_tokens=True)\n    new_out = translator.translate(out, dest='th').text\n    print(\"\\n\"+\"===\"*10)\n    print(\"ตัวอย่างที่{}: {}\\n\".format(i, new_out))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\norig_input=\"เหตุการณ์ราคาน้ำมันตกต่ำหนักเป็นประวัติการณ์ทำให้หุ้นกลุ่มพลังงาน\"\nnew_input = translator.translate(orig_input, dest='en').text\n\nids = tokenizer.encode(new_input, return_tensors='tf')\n\nsample_outputs = model.generate(\n    ids,\n    do_sample=True, \n    max_length=100, \n    top_k=55, \n    top_p=0.95, \n    num_return_sequences=5\n)\nprint(\"Input: %s\\n\\n\" % orig_input)\nprint(\"Output:\\n\" + 100 * '-')\nfor i, sample_output in enumerate(sample_outputs):\n    out = tokenizer.decode(sample_output, skip_special_tokens=True)\n    new_out = translator.translate(out, dest='th').text\n    print(\"\\n\"+\"===\"*10)\n    print(\"ตัวอย่างที่{}: {}\\n\".format(i, new_out))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# เมื่อ translation แปลไม่ถูก\nแทนที่เราจะได้เรื่องโรแมนติก ก็ได้เรื่องฮาๆ หรือสยองมาแทน :D :p ;D"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\norig_input=\"เมื่อเธอเดินเข้ามาที่ห้อง หัวใจของผมก็เต้นไม่เป็นจังหวะ\"\nnew_input = translator.translate(orig_input, dest='en').text\n\nids = tokenizer.encode(new_input, return_tensors='tf')\n\nsample_outputs = model.generate(\n    ids,\n    do_sample=True, \n    max_length=100, \n    top_k=55, \n    top_p=0.95, \n    num_return_sequences=5\n)\nprint(\"Input: %s\\n\\n\" % orig_input)\nprint(\"Output:\\n\" + 100 * '-')\nfor i, sample_output in enumerate(sample_outputs):\n    out = tokenizer.decode(sample_output, skip_special_tokens=True)\n    new_out = translator.translate(out, dest='th').text\n    print(\"\\n\"+\"===\"*10)\n    print(\"ตัวอย่างที่{}: {}\\n\".format(i, new_out))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## ลองให้แต่งเรื่องที่ใช้ความรู้เฉพาะทาง\nเช่น ให้แต่งเนื้อเรื่องนารุโตะหรือกัปตันอเมริกา พบว่า GPT-2 รู้จักตัวละครพอสมควร แต่ไม่สามารถเชื่อมโยงความสัมพันธ์และเนื้อเรื่องได้ ทำให้ส่วนใหญ่เนื้อเริ่องออกมาค่อนข้างมั่ว อาจมองเป็นจุดเริ่มต้นของไอเดียสร้างสรรค์และอาจจะต้องลอง generate หลายรอบหรือใช้คนมาช่วยปรับเนื้อเรื่องอีกแรง"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\norig_input=\"ที่หมู่บ้านแห่งไฟ เมื่อนารุโตะ เห็นซาสุเกะถือมีดเปื้อนคราบเลือดในมือ และเห็นซากุระนอนกองอยู่กับพื้น ก็ตะโกนออกไปว่า\"\nnew_input = translator.translate(orig_input, dest='en').text\n\nids = tokenizer.encode(new_input, return_tensors='tf')\n\nsample_outputs = model.generate(\n    ids,\n    do_sample=True, \n    max_length=100, \n    top_k=55, \n    top_p=0.925, \n    num_return_sequences=5\n)\nprint(\"Input: %s\\n\\n\" % orig_input)\nprint(\"Output:\\n\" + 100 * '-')\nfor i, sample_output in enumerate(sample_outputs):\n    out = tokenizer.decode(sample_output, skip_special_tokens=True)\n    new_out = translator.translate(out, dest='th').text\n    print(\"\\n\"+\"===\"*10)\n    print(\"ตัวอย่างที่{}: {}\\n\".format(i, new_out))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\norig_input=\"เมื่อนักวิทยาศาสตร์ได้ค้นพบแร่ธาตุใหม่ vibranium ที่มีความทนทานสูงกว่าเพชร ความฝันที่จะสร้างยอดมนุษย์ ก็ไม่ใช่ความฝันอีกต่อไป \"\nnew_input = translator.translate(orig_input, dest='en').text\n\nids = tokenizer.encode(new_input, return_tensors='tf')\n\nsample_outputs = model.generate(\n    ids,\n    do_sample=True, \n    max_length=100, \n    top_k=60, \n    top_p=0.925, \n    num_return_sequences=5\n)\nprint(\"Input: %s\\n\\n\" % orig_input)\nprint(\"Output:\\n\" + 100 * '-')\nfor i, sample_output in enumerate(sample_outputs):\n    out = tokenizer.decode(sample_output, skip_special_tokens=True)\n    new_out = translator.translate(out, dest='th').text\n    print(\"\\n\"+\"===\"*10)\n    print(\"ตัวอย่างที่{}: {}\\n\".format(i, new_out))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### ลองเรื่อง Fantasy หรือเหตุการณ์ปัจจุบันในบ้านเราเล่นๆ ครับ "},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\norig_input=\"นักวิทยาศาสตร์ได้ค้นพบยูนิคอร์นและมังกรเป็นครั้งแรกของโลกในป่าลึกอเมซอน!!\"\nnew_input = translator.translate(orig_input, dest='en').text\n\nids = tokenizer.encode(new_input, return_tensors='tf')\n\nsample_outputs = model.generate(\n    ids,\n    do_sample=True, \n    max_length=100, \n    top_k=55, \n    top_p=0.95, \n    num_return_sequences=5\n)\nprint(\"Input: %s\\n\\n\" % orig_input)\nprint(\"Output:\\n\" + 100 * '-')\nfor i, sample_output in enumerate(sample_outputs):\n    out = tokenizer.decode(sample_output, skip_special_tokens=True)\n    new_out = translator.translate(out, dest='th').text\n    print(\"\\n\"+\"===\"*10)\n    print(\"ตัวอย่างที่{}: {}\\n\".format(i, new_out))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\norig_input=\"ตั้งแต่ต้นปีเป็นต้นมา เราเจอเหตุการณ์วิกฤตหลายอย่าง นับจากไฟไหม้ครั้งใหญ่ที่สุดในออสเตรเลีย เหตุการณ์ควันพิษในประเทศไทย รวมทั้งล่าสุดไวรัสโคโรน่าทำให้\"\nnew_input = translator.translate(orig_input, dest='en').text\n\nids = tokenizer.encode(new_input, return_tensors='tf')\n\nsample_outputs = model.generate(\n    ids,\n    do_sample=True, \n    max_length=100, \n    top_k=55, \n    top_p=0.9, \n    num_return_sequences=5\n)\nprint(\"Input: %s\\n\\n\" % orig_input)\nprint(\"Output:\\n\" + 100 * '-')\nfor i, sample_output in enumerate(sample_outputs):\n    out = tokenizer.decode(sample_output, skip_special_tokens=True)\n    new_out = translator.translate(out, dest='th').text\n    print(\"\\n\"+\"===\"*10)\n    print(\"ตัวอย่างที่{}: {}\\n\".format(i, new_out))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\norig_input=\"นพ.ทวีศิลป์ วิษณุโยธิน โฆษกศูนย์บริหารสถานการณ์การแพร่ระบาดของโรคติดเชื้อ virus corona กล่าวถึงแนวโน้มการผ่อนคลายมาตรการหลังตัวเลขผู้ติดเชื้อโควิด-19 ลดลง\"\nnew_input = translator.translate(orig_input, dest='en').text\n\nids = tokenizer.encode(new_input, return_tensors='tf')\n\nsample_outputs = model.generate(\n    ids,\n    do_sample=True, \n    max_length=100, \n    top_k=55, \n    top_p=0.95, \n    num_return_sequences=5\n)\nprint(\"Input: %s\\n\\n\" % orig_input)\nprint(\"Output:\\n\" + 100 * '-')\nfor i, sample_output in enumerate(sample_outputs):\n    out = tokenizer.decode(sample_output, skip_special_tokens=True)\n    new_out = translator.translate(out, dest='th').text\n    print(\"\\n\"+\"===\"*10)\n    print(\"ตัวอย่างที่{}: {}\\n\".format(i, new_out))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\norig_input=\"คุณกำลังอยู่ในโลกที่เอเลี่ยนได้บินเข้ามา ทว่าแท้จริงแล้วเอเลี่ยนเหล่านั้นมาขอให้คุณช่วยเหลือดาวของพวกเขาที่กำลังจะถูกทำลาย!\"\nnew_input = translator.translate(orig_input, dest='en').text\n\nids = tokenizer.encode(new_input, return_tensors='tf')\n\nsample_outputs = model.generate(\n    ids,\n    do_sample=True, \n    max_length=100, \n    top_k=55, \n    top_p=0.925, \n    num_return_sequences=5\n)\nprint(\"Input: %s\\n\\n\" % orig_input)\nprint(\"Output:\\n\" + 100 * '-')\nfor i, sample_output in enumerate(sample_outputs):\n    out = tokenizer.decode(sample_output, skip_special_tokens=True)\n    new_out = translator.translate(out, dest='th').text\n    print(\"\\n\"+\"===\"*10)\n    print(\"ตัวอย่างที่{}: {}\\n\".format(i, new_out))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\norig_input=\"==ความเป็นมา==\\n ทานุกิเป็นหมีประเภทหนึ่งที่มีความลับซ่อนไว้มากมาย ความจริงก็คือพวกมันแปลงร่างเป็นสัตว์ประเภทอื่นได้  \" \nnew_input = translator.translate(orig_input, dest='en').text\n\nids = tokenizer.encode(new_input, return_tensors='tf')\n\nsample_outputs = model.generate(\n    ids,\n    do_sample=True, \n    max_length=100, \n    top_k=50, \n    top_p=0.90, \n    num_return_sequences=5\n)\nprint(\"Input: %s\\n\\n\" % orig_input)\nprint(\"Output:\\n\" + 100 * '-')\nfor i, sample_output in enumerate(sample_outputs):\n    out = tokenizer.decode(sample_output, skip_special_tokens=True)\n    new_out = translator.translate(out, dest='th').text\n    print(\"\\n\"+\"===\"*10)\n    print(\"ตัวอย่างที่{}: {}\\n\".format(i, new_out))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"image_captioning.ipynb","private_outputs":true,"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"}},"nbformat":4,"nbformat_minor":4}