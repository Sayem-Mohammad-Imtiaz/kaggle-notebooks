{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport sklearn\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport keras\nimport matplotlib.pyplot as plt\ndf = pd.read_csv(\"../input/all_data.csv\")\nn_samples = 10000\nrecent_tags = df['tags'].tail(n_samples)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f48a40fdfe72b6587e574860274a22b1026c449e","collapsed":true},"cell_type":"code","source":"count_vectorizer = CountVectorizer(min_df=0.01,\n                                   analyzer=lambda x: x.split(' ')\n                                  )\ncount_vectorizer.fit(recent_tags)\n\nN_CLASSES = len(count_vectorizer.vocabulary_)\n\ndef fit_generator(all_tags):\n    while True:\n        for tags in all_tags:\n            words = count_vectorizer.transform(tags.split(' ')).toarray()\n            contexts = count_vectorizer.transform([tags]).toarray().repeat(len(words), axis=0)\n            yield words, contexts\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"collapsed":true},"cell_type":"code","source":"model = keras.models.Sequential()\nmodel.add(keras.layers.InputLayer((N_CLASSES,)))\nmodel.add(keras.layers.Dense(2))\nmodel.add(keras.layers.Activation('linear',name='embedding'))\nmodel.add(keras.layers.Dense(N_CLASSES))\nmodel.add(keras.layers.Activation('sigmoid'))\nmodel.summary()\n\nembedding = keras.models.Model(inputs=model.input, outputs=model.get_layer('embedding').output)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6ceef8214bd2ed674802459bcd4e137d9caf1dd0","collapsed":true},"cell_type":"code","source":"model.compile(loss='binary_crossentropy',\n             optimizer=keras.optimizers.Adam(0.001),\n             metrics=[])\nmodel.fit_generator(fit_generator(recent_tags), \n                   epochs=10,\n                   steps_per_epoch=n_samples,\n                   callbacks=[keras.callbacks.EarlyStopping(monitor='loss', min_delta=0, patience=2, verbose=0, mode='auto')],\n                   verbose=2)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e52674ca99fb3c94fdbd7f9eb42b47df3316c155"},"cell_type":"markdown","source":"One can expect that related tags like `tohou` and `alice_margatroid` should cluster together."},{"metadata":{"trusted":true,"_uuid":"a151114ef815f20450c6238ef37ebf81d7f1c528","collapsed":true},"cell_type":"code","source":"keys = count_vectorizer.vocabulary_.keys()\ninput_vecs = count_vectorizer.transform(keys).toarray()\nlatents = embedding.predict_on_batch(input_vecs)\nplt.figure(figsize=(30,30))\ncolors = ['b','g','r','c','m','y','k']\nfor k, l in zip(keys, latents):\n    plt.scatter(latents.T[0], latents.T[1])\nfor k, l in zip(keys, latents):\n    plt.text(l[0], l[1], k,\n             alpha=0.5, \n             color=colors[np.random.randint(0, len(colors))],\n             rotation=np.random.randint(-30, 30))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}