{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"**Unsupervised Learning:** We have data that has hidden labels and our aim is to find these labels of the data"},{"metadata":{"_uuid":"1010dc193607892aaa3aa113c2b9208d43254242"},"cell_type":"markdown","source":"**1. K Means Algorithm**\n\n1.  We choose a k value \n2. Then it is randomly created k centroids\n3. Every data point is clusterd according to the nearest centroid\n4. By taking average of all data points that belog to a centroid, it is created new centroids.\n5. Using these new centroids repeat 3 and 4\n6. Finally, when centroids remain stationary, the algorith stops there.\n7. As a result, according to these centroids, data is clustered"},{"metadata":{"_uuid":"d0121aded8e77e5b80a1d043c6780c88b51427c9"},"cell_type":"markdown","source":"**How k value is selected**\n\n1. For k=1, run KMeans algorithm\n2. For each cluster (k cluster we have), it is calculated WCSS (within cluster sum of squares) value\n3. repeat 1 and 2 for 1<k<15\n4. obtain k vs WCSS plot\n5. Using elbow rule,  choose the optimum k value to be used in K Means Algorithm"},{"metadata":{"trusted":true,"_uuid":"e8d88762d3f8d00971c9719b1b443bdb3fe0e922"},"cell_type":"code","source":"data = pd.read_csv('../input/column_2C_weka.csv')\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"23301932a03d89b08a84ab67e1c4d29001638d34"},"cell_type":"code","source":"# As you can see there is no labels in data\nx = data['pelvic_radius']\ny = data['degree_spondylolisthesis']\nplt.figure(figsize=(13,5))\nplt.scatter(x,y)\nplt.xlabel('pelvic_radius')\nplt.ylabel('degree_spondylolisthesis')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1cf7a0992ffe27f42734eb0941221e8324f7b919"},"cell_type":"markdown","source":"**Lets find optimum k value**"},{"metadata":{"trusted":true,"_uuid":"fae1cdb40456228d16b2b141d9e7972a7646d17e"},"cell_type":"code","source":"df = data.loc[:, ['degree_spondylolisthesis', 'pelvic_radius']]\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d2c61eea5217f24085883ad711e54dcd69552d23"},"cell_type":"code","source":"# which k value to choose\nfrom sklearn.cluster import KMeans\nwcss = []\nfor k in range(1,15):\n    kmeans = KMeans(n_clusters=k)\n    kmeans.fit(df)\n    wcss.append(kmeans.inertia_) # kmeans.inertia : calculate wcss\n    \nplt.plot(range(1,15), wcss, '-o')\nplt.xlabel('number of k (cluster) value')\nplt.ylabel('wcss')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"058f50ec20f87aec1987dd3f55cb6e5d9e76d56b"},"cell_type":"markdown","source":"using elbow rule we can select k=2, 3 or 4 (the elbow point is not quite obvious here)"},{"metadata":{"_uuid":"7d08bd31f6873aac4885adabd1f9ab0846a04cd8"},"cell_type":"markdown","source":"**for k=2**"},{"metadata":{"trusted":true,"_uuid":"9f60cfc61dff08f273307e61029b152b5f10d656"},"cell_type":"code","source":"# for k=2, lets write KMeans\nfrom sklearn.cluster import KMeans\nkmeans2 = KMeans(n_clusters = 2)\nclusters =kmeans2.fit_predict(df) # fit first and then predict\n\n# add labels for df\ndf['label'] = clusters","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"380c54c18a058682616c8688c1d6a12a45bf1ebd"},"cell_type":"code","source":"# plot\nplt.scatter(data['pelvic_radius'],data['degree_spondylolisthesis'],c = clusters)\nplt.xlabel('pelvic_radius')\nplt.xlabel('degree_spondylolisthesis')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6b52c40a7e07be856e661b62b197ea3d1efb4f2e"},"cell_type":"markdown","source":"**for k=3**"},{"metadata":{"trusted":true,"_uuid":"c75b5f7abd1017ce15f0f5e8a5e2a186fec891f1"},"cell_type":"code","source":"# if we choose k=3\nfrom sklearn.cluster import KMeans\nkmeans3 = KMeans(n_clusters = 3)\nclusters =kmeans3.fit_predict(df) # fit first and then predict\n\n# add labels for df\ndf['label'] = clusters\n\n# plot\nplt.scatter(data['pelvic_radius'],data['degree_spondylolisthesis'],c = clusters)\nplt.xlabel('pelvic_radius')\nplt.xlabel('degree_spondylolisthesis')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bcfad346d1f9d68364df10ae16fe669ea98f6761"},"cell_type":"markdown","source":"**for k=4**"},{"metadata":{"trusted":true,"_uuid":"85abe11ced4bbae6b11deab3e92a6d4cbfd8e267"},"cell_type":"code","source":"# if we choose k=4\nfrom sklearn.cluster import KMeans\nkmeans4 = KMeans(n_clusters = 4)\nclusters =kmeans4.fit_predict(df) # fit first and then predict\n\n# add labels for df\ndf['label'] = clusters\n\n# plot\nplt.scatter(data['pelvic_radius'],data['degree_spondylolisthesis'],c = clusters)\nplt.xlabel('pelvic_radius')\nplt.xlabel('degree_spondylolisthesis')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"96e1f56429692109d42c529561ebb06b859ff522"},"cell_type":"markdown","source":"**Original data is as follow**"},{"metadata":{"trusted":true,"_uuid":"4fac9abbaff4d153893c1f8496db6d656ca56df4"},"cell_type":"code","source":"# plot\ncolors = [0 if i=='Abnormal' else 1 for i in data['class']] # to create colors\nplt.scatter(data['pelvic_radius'],data['degree_spondylolisthesis'],c = colors)\nplt.xlabel('pelvic_radius')\nplt.ylabel('degree_spondylolisthesis')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"84cceea07a7f96c4defb24d6e06d45684f61c14a"},"cell_type":"markdown","source":"**2. Hierarcical Clustering**\n\n1. Assign each data point as a cluster\n2. Create a new cluster by choosing the closest two clusters  \n3. repeat 2 until it remains only one cluster"},{"metadata":{"trusted":true,"_uuid":"fbc3d4964e386f5642e12b7457bcace995372232"},"cell_type":"code","source":"# DENDOGRAM \n# here we will try to predict how many clusters we have \nfrom scipy.cluster.hierarchy import linkage, dendrogram # linkage: create dendrogram\ndf1 = data.loc[:, ['pelvic_radius', 'degree_spondylolisthesis']]\nmerg = linkage(df1, method='ward') # ward: cluster icindeki yayilimlari minimize et (wcss gibi bisey)\ndendrogram(merg, leaf_rotation=90)\nplt.xlabel('data points')\nplt.ylabel('euclidian distance')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"89e760f322e6d1f8cb9a6c17bee8853a067bbf79"},"cell_type":"markdown","source":"* vertical lines are clusters\n* height on dendogram: distance between merging cluster\n* method= 'single' : closest points of clusters\n* we are going to choose the highest distance between merging clusters, which are not cut by horizontal line\n* this suggest that choose 3 clusters (draw a horizontal line approx from euc. distance=400, it cuts at 3 points)"},{"metadata":{"trusted":true,"_uuid":"1d256b4d8c5323ff6691fda04e0e7637d2924840"},"cell_type":"code","source":"from sklearn.cluster import AgglomerativeClustering\n\nhierarcical_cluster = AgglomerativeClustering(n_clusters=3, affinity='euclidean', linkage='ward')\ncluster = hierarcical_cluster.fit_predict(df1)\n\n# add label for df1\ndf1['label'] = cluster\n\n#plot\nplt.scatter(df1['pelvic_radius'],df1['degree_spondylolisthesis'],c = cluster)\nplt.xlabel('pelvic_radius')\nplt.ylabel('degree_spondylolisthesis')\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d1ed0ee638969a7097efff7dbec61ee4d0465138"},"cell_type":"markdown","source":"**CONCLUSION**\n\n* We wee that the predictions that we made using Kmeans algorihm does not suit well for this problem\n* But for Hierarcical Clustering, we have predicted the original data better as compared to the Kmeans. "}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}