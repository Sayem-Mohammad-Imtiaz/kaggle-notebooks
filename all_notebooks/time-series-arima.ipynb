{"cells":[{"metadata":{},"cell_type":"markdown","source":"Time series analysis is a statistical method to analyse the past data within a given duration of time to forecast the future. It comprises of ordered sequence of data at equally spaced interval.To understand the time series data & the analysis let us consider an example. Consider an example of Airline Passenger data. It has the count of passenger over a period of time."},{"metadata":{},"cell_type":"markdown","source":"Loading the basic libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np \nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom statsmodels.tsa.stattools import acf, pacf\nfrom statsmodels.tsa.arima_model import ARIMA","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a1=pd.read_csv('../input/airpassengers/AirPassengers.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a1.tail()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's rename \"#Passengers\", seems really annoying the column name."},{"metadata":{"trusted":true},"cell_type":"code","source":"a1.rename(columns={'#Passengers':'Passengers'},inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a1.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a1.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are 144 records in 2 datasets and 2 columns. There are no null records present. But, look at the Month column. We need to convert them in to datetime datatype."},{"metadata":{"trusted":true},"cell_type":"code","source":"from datetime import datetime\na1['Month']=pd.to_datetime(a1['Month'],infer_datetime_format=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a1.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, we will need to index Month column."},{"metadata":{"trusted":true},"cell_type":"code","source":"airpass = a1.set_index('Month',inplace=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"airpass.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's plot the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.xlabel('Date')\nplt.ylabel('Number Of Air Passengers')\nplt.plot(airpass)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above below, we can see that there is a Trend compoenent in the series. Hence, we now check for stationarity of the data.\n\n"},{"metadata":{},"cell_type":"markdown","source":"Let's make one function consisting of stationary data checking and ADCF test working. Because we will need to repeat the steps many times, therefore, making function will become very handy"},{"metadata":{"trusted":true},"cell_type":"code","source":"def test_stationarity(timeseries):\n    \n    #Determine rolling statistics\n    movingAverage = timeseries.rolling(window=12).mean()\n    movingSTD = timeseries.rolling(window=12).std()\n    \n    #Plot rolling statistics\n    plt.plot(timeseries, color='blue', label='Original')\n    plt.plot(movingAverage, color='red', label='Rolling Mean')\n    plt.plot(movingSTD, color='black', label='Rolling Std')\n    plt.legend(loc='best')\n    plt.title('Rolling Mean & Standard Deviation')\n    plt.show(block=False)\n    \n    #Perform Dickeyâ€“Fuller test:\n    print('Results of Dickey Fuller Test:')\n    airpass_test = adfuller(timeseries['Passengers'], autolag='AIC')\n    dfoutput = pd.Series(airpass_test[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n    for key,value in airpass_test[4].items():\n        dfoutput['Critical Value (%s)'%key] = value\n    print(dfoutput)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's determine & plot rolling statistics."},{"metadata":{"trusted":true},"cell_type":"code","source":"test_stationarity(airpass)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From above plot, we can see that Rolling Mean itself has a trend component even though Rolling Standard Deviation is fairly constant with time.\n\nFor time series to be stationary, we need to ensure that both Rolling Mean and Rolling Standard Deviation remain fairly constant WRT time.\n\nBoth the curves needs to be parallel to X-Axis, in our case it is not so.\n\nWe've also conducted the ADCF ie Augmented Dickey Fuller Test. Having the Null Hypothesis to be Time Series is Non Stationary."},{"metadata":{},"cell_type":"markdown","source":"Data Transformation To Achieve Stationarity\nNow, we will have to perform some data transformation to achieve Stationarity. We can perform any of the transformations like taking log scale, square, square root, cube, cube root, time shift, exponential decay, etc.\n\nLet's perform Log Transformation.\n\nBasically we need to remove the trend component."},{"metadata":{"trusted":true},"cell_type":"code","source":"airpass_log = np.log(airpass)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(airpass_log)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Working on Rolling stats seperately (not using function) because we would need Rolling stats separately for computing"},{"metadata":{"trusted":true},"cell_type":"code","source":"rollmean_log = airpass_log.rolling(window=12).mean()\nrollstd_log = airpass_log.rolling(window=12).std()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(airpass_log, color='blue', label='Original')\nplt.plot(rollmean_log, color='red', label='Rolling Mean')\nplt.plot(rollstd_log, color='black', label='Rolling Std')\nplt.legend(loc='best')\nplt.title('Rolling Mean & Standard Deviation (Logarithmic Scale)')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From above graph we can say that, we slightly bettered our previous results. Now, we are heading into the right direction.\n\nFrom the above graph, Time series with log scale as well as Rolling Mean(moving avg) both have the trend component. Thus subtracting one from the other should remove the trend component.\n\nR (result) = Time Series Loca Scale - Rolling Mean Log Scale -> this can be our final non trend curve"},{"metadata":{"trusted":true},"cell_type":"code","source":"airpass_new = airpass_log - rollmean_log","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"airpass_new.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"airpass_new.dropna(inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"airpass_new.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_stationarity(airpass_new)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above plot, we came to know that \"indeed subtracting two related series having similar trend components actually removed trend and made the dataset stationary\""},{"metadata":{},"cell_type":"markdown","source":"Also, after concluding the results from ADFC test:\n\np-value has reduced from 0.99 to 0.022\nCritical values at 1%,5%,10% confidence intervals are pretty close to the Test Statistic\nSo we can now say that given series is now STATIONARY"},{"metadata":{},"cell_type":"markdown","source":"Time Shift Transformation"},{"metadata":{"trusted":true},"cell_type":"code","source":"airpass_log_diff = airpass_log - airpass_log.shift()\nplt.plot(airpass_log_diff)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"airpass_log_diff.dropna(inplace=True)\nplt.plot(airpass_log_diff)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_stationarity(airpass_log_diff)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From above plot, we can see that, visually this is the very best result as our series along with rolling stats values of moving avg(mean) & moving standard deviation is very much flat & stationary.\n\nBut, the ADCF test shows us that:\n\np-value of 0.07 is not as good as 0.02 of previous instance.\nTest Statistic value not as close to the critical values as that of previous instance."},{"metadata":{},"cell_type":"markdown","source":"Let us now break down the 3 components of the log scale series using a system libary function. Once, we separate our the components, we can simply ignore trend & seasonality and check on the nature of the residual part."},{"metadata":{"trusted":true},"cell_type":"code","source":"decomposition = seasonal_decompose(airpass_log)\n\ntrend = decomposition.trend\nseasonal = decomposition.seasonal\nresidual = decomposition.resid\n\nplt.subplot(411)\nplt.plot(airpass_log, label='Original')\nplt.legend(loc='best')\n\nplt.subplot(412)\nplt.plot(trend, label='Trend')\nplt.legend(loc='best')\n\nplt.subplot(413)\nplt.plot(seasonal,label='Seasonality')\nplt.legend(loc='best')\n\nplt.subplot(414)\nplt.plot(residual, label='Residuals')\nplt.legend(loc='best')\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There can be cases where an observation simply consist of trend & seasonality. In that case, there won't be any residual component & that would be a null or NaN. Hence, we also remove such cases."},{"metadata":{"trusted":true},"cell_type":"code","source":"airpass_decompose = residual\nairpass_decompose.dropna(inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rollmean_decompose = airpass_decompose.rolling(window=12).mean()\nrollstd_decompose = airpass_decompose.rolling(window=12).std()\n\nplt.plot(airpass_decompose, color='blue', label='Original')\nplt.plot(rollmean_decompose, color='red', label='Rolling Mean')\nplt.plot(rollstd_decompose, color='black', label='Rolling Std')\nplt.legend(loc='best')\nplt.title('Rolling Mean & Standard Deviation')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plotting ACF & PACF"},{"metadata":{"trusted":true},"cell_type":"code","source":"lag_acf = acf(airpass_log_diff, nlags=20)\nlag_pacf = pacf(airpass_log_diff, nlags=20, method='ols')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plot ACF:\nplt.subplot(121)\nplt.plot(lag_acf)\nplt.axhline(y=0, linestyle='--', color='gray')\nplt.axhline(y=-1.96/np.sqrt(len(airpass_log_diff)), linestyle='--', color='gray')\nplt.axhline(y=1.96/np.sqrt(len(airpass_log_diff)), linestyle='--', color='gray')\nplt.title('Autocorrelation Function')            \n\n#Plot PACF\nplt.subplot(122)\nplt.plot(lag_pacf)\nplt.axhline(y=0, linestyle='--', color='gray')\nplt.axhline(y=-1.96/np.sqrt(len(airpass_log_diff)), linestyle='--', color='gray')\nplt.axhline(y=1.96/np.sqrt(len(airpass_log_diff)), linestyle='--', color='gray')\nplt.title('Partial Autocorrelation Function')\n            \nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the ACF graph, we can see that curve touches y=0.0 line at x=2. Thus, from theory, Q = 2 From the PACF graph, we see that curve touches y=0.0 line at x=2. Thus, from theory, P = 2\n\nARIMA is AR + I + MA. Before, we see an ARIMA model, let us check the results of the individual AR & MA model. Note that, these models will give a value of RSS. Lower the RSS values indicates a better model."},{"metadata":{},"cell_type":"markdown","source":"AR Model\nMaking order = (2,1,0)"},{"metadata":{"trusted":true},"cell_type":"code","source":"model1 = ARIMA(airpass_log, order=(2,1,0))\nresults_AR = model1.fit(disp=-1)\nplt.plot(airpass_log_diff)\nplt.plot(results_AR.fittedvalues, color='red')\nplt.title('RSS: %.4f'%sum((results_AR.fittedvalues - airpass_log_diff['Passengers'])**2))\nprint('Plotting AR model')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"MA Model\nMaking order = (0,1,2)"},{"metadata":{"trusted":true},"cell_type":"code","source":"model2 = ARIMA(airpass_log, order=(0,1,2))\nresults_MA = model2.fit(disp=-1)\nplt.plot(airpass_log_diff)\nplt.plot(results_MA.fittedvalues, color='red')\nplt.title('RSS: %.4f'%sum((results_MA.fittedvalues - airpass_log_diff['Passengers'])**2))\nprint('Plotting MA model')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"AR+I+MA = ARIMA Model\nMaking order = (2,1,2)"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = ARIMA(airpass_log, order=(2,1,2))\nresults_ARIMA = model.fit(disp=-1)\nplt.plot(airpass_log_diff)\nplt.plot(results_ARIMA.fittedvalues, color='red')\nplt.title('RSS: %.4f'%sum((results_ARIMA.fittedvalues - airpass_log_diff['Passengers'])**2))\nprint('Plotting ARIMA model')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"RSS value for: AR Model - 1.5023 MA Model - 1.4721\n\nARIMA Model - 1.0292\n\nBy combining AR & MA into ARIMA, we see that RSS value has decreased from either case to 1.0292, indicating ARIMA to be better than its individual component models.\n\nWith the ARIMA model built, we will now generate predictions. But, before we do any plots for predictions ,we need to reconvert the predictions back to original form. This is because, our model was built on log transformed data.\n\n"},{"metadata":{},"cell_type":"markdown","source":"Prediction & Reverse Transformation"},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions_ARIMA_diff = pd.Series(results_ARIMA.fittedvalues, copy=True)\npredictions_ARIMA_diff.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions_ARIMA_diff_cumsum = predictions_ARIMA_diff.cumsum()\npredictions_ARIMA_diff_cumsum.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions_ARIMA_log = pd.Series(airpass_log['Passengers'].iloc[0], index=airpass_log.index)\npredictions_ARIMA_log = predictions_ARIMA_log.add(predictions_ARIMA_diff_cumsum, fill_value=0)\npredictions_ARIMA_log.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Inverse of log is exp"},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions_ARIMA = np.exp(predictions_ARIMA_log)\nplt.plot(airpass)\nplt.plot(predictions_ARIMA)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From above plot, we can see that our predicted forecasts are very close to the real time series values. It also indicates a fairly accurate model."},{"metadata":{"trusted":true},"cell_type":"code","source":"airpass_log.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have 144 (existing data of 12 yrs in months) data points. Now, we want to forecast for additional 10 yrs (10x12 months=120 data points).\n\n144+120 = 264 records/data points"},{"metadata":{"trusted":true},"cell_type":"code","source":"results_ARIMA.plot_predict(1,264)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}