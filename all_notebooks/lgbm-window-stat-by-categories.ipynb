{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/real-time-advertisers-auction/Dataset.csv')\ndf['date'] = pd.to_datetime(df['date'])\ndf.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['CPM'] = (df['total_revenue'] / df['measurable_impressions']) * 100 * 1000\ndf.loc[df['measurable_impressions']==0, 'CPM'] = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[['revenue_share_percent', 'integration_type_id']].nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop(['revenue_share_percent', 'integration_type_id'], axis='columns', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"split_mask = df['date'] < pd.to_datetime('2019-06-22')\ntrain_df, test_df = df[split_mask], df[~split_mask]\ntrain_df = train_df[train_df['CPM'] >= 0]\ntest_df = test_df[test_df['CPM'] >= 0]\n\ntrain_df = train_df[train_df['CPM'] <= np.percentile(train_df['CPM'], 95)]\ntest_df = test_df[test_df['CPM'] <= np.percentile(test_df['CPM'], 95)]\n\ntrain_df['sample'] = 'train'\ntest_df['sample'] = 'test'\n\ndf = pd.concat([train_df, test_df])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_cols = [\n    'site_id', 'ad_type_id', 'device_category_id', 'line_item_type_id', 'os_id',\n    'monetization_channel_id'\n]\nid_cols = [\n    'geo_id', 'advertiser_id', 'order_id', 'ad_unit_id'\n]\nall_discrete_cols = cat_cols + id_cols\n\nother_cols = [\n    'total_impressions', 'viewable_impressions', 'measurable_impressions'\n]\nother_cols_wo_targets = [\n    'total_impressions', 'viewable_impressions'\n]\n\ntarget_col = 'CPM'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[all_discrete_cols].nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_squared_error(test_df[target_col], np.zeros(test_df.shape[0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_squared_error(\n    test_df[target_col], \n    np.ones(test_df.shape[0]) * np.mean(train_df[target_col])\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Baseline on OHE + LinReg"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import TimeSeriesSplit, cross_validate\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.compose import ColumnTransformer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = train_df.sort_values('date')\ncv = TimeSeriesSplit(n_splits=5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"baseline_model = Pipeline([\n    ('ohe', OneHotEncoder(handle_unknown='ignore')),\n    ('linreg', LinearRegression())\n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# baseline_cv = cross_validate(\n#     baseline_model, \n#     train_df[all_discrete_cols], train_df[target_col],\n#     cv=cv,\n#     scoring='neg_mean_squared_error',\n#     n_jobs=-1,\n# )\n\n# baseline_cv","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"baseline_model.fit(train_df[all_discrete_cols], train_df[target_col])\nmean_squared_error(baseline_model.predict(test_df[all_discrete_cols]), test_df[target_col])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### MSE = 4569 :good-enough: :pepe-happy:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from matplotlib import pyplot as plt\nfig, [ax1, ax2] = plt.subplots(nrows=1, ncols=2, figsize=(20, 10))\n\ntrain_df['CPM'].hist(bins=40, ax=ax1)\nax1.set_title('train')\n\ntest_df['CPM'].hist(bins=40, ax=ax2)\nax2.set_title('test');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## try to classify CPM == 0"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cpm_divider = Pipeline([\n    ('ohe', OneHotEncoder(handle_unknown='ignore')),\n    ('logreg', LogisticRegression(solver='sag', max_iter=500))\n])\n# cv_logreg = cross_validate(\n#     cpm_divider, \n#     train_df[all_discrete_cols], (train_df[target_col]>0), \n#     scoring=['roc_auc', 'precision', 'recall'],\n# )\n# cv_logreg","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.base import BaseEstimator\nclass CustomPredictor(BaseEstimator):\n    def __init__(self, divider, predictor):\n        self.divider = divider\n        self.predictor = predictor\n        self.threshold = 0.5\n        \n    def fit(self, X, y):\n        cpm_over_zero = y > 0\n        self.divider.fit(X, cpm_over_zero)\n        self.predictor.fit(X[cpm_over_zero], y[cpm_over_zero])\n    \n    def predict(self, X):\n        divider_score = self.divider.predict_proba(X)[:, 1]\n        predicted_cpm = self.predictor.predict(X)\n        predicted_cpm[divider_score < self.threshold] = 0\n        return predicted_cpm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cpm_divider = Pipeline([\n    ('ohe', OneHotEncoder(handle_unknown='ignore')),\n    ('logreg', LogisticRegression(solver='sag', max_iter=500))\n])\ncpm_predictor = Pipeline([\n    ('ohe', OneHotEncoder(handle_unknown='ignore')),\n    ('linreg', LinearRegression())\n])\n\ncustom = CustomPredictor(cpm_divider, cpm_predictor)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# custom_cv = cross_validate(\n#     custom, \n#     train_df[all_discrete_cols], train_df[target_col], \n#     scoring='neg_mean_squared_error',\n# )\n# custom_cv","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"custom.fit(train_df[all_discrete_cols], train_df[target_col])\nmean_squared_error(test_df[target_col], custom.predict(test_df[all_discrete_cols]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### MSE = 4234 without tuning threshold for logreg"},{"metadata":{},"cell_type":"markdown","source":"## Feature engineering and boosting"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_first = df.copy()\ndf_first.sort_values('date', inplace=True)\ndf_first['view_to_total_impressions'] = df_first['viewable_impressions'] / df_first['total_impressions']\ndf_first.loc[df_first['total_impressions']==0, 'view_to_total_impressions'] = 0\n\ndf_first = df_first.drop('total_revenue',axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Simple boosting"},{"metadata":{"trusted":true},"cell_type":"code","source":"from lightgbm import LGBMRegressor\nfrom sklearn.model_selection import train_test_split\n\n# clip irrelevant predictions\ndef clip(x):\n    return np.clip(x, 0, None)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {\n    'alpha': 0.8,\n    'colsample_bytree': 0.6000000000000001,\n    'learning_rate': 0.01,\n    'n_estimators': 1950,\n    'num_leaves': 511,\n    'subsample': 0.6,\n    'objective': 'mse'\n}\nmodel_base = LGBMRegressor(random_state=17, **params)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"is_train = df_first['sample'] == 'train'\nis_test = df_first['sample'] == 'test'\n\nnew_train, new_test = df_first[is_train], df_first[is_test]\ntrain_df, eval_df = train_test_split(new_train, test_size=0.2, random_state=17)\n\nmodel_base.fit(\n    train_df.drop(['CPM','date','sample'],axis=1), train_df['CPM'],\n    eval_set=(eval_df.drop(['CPM','date','sample'],axis=1), eval_df['CPM']),\n    early_stopping_rounds=20,\n    verbose=80,\n)\nmean_squared_error(new_test['CPM'], clip(model_base.predict(new_test.drop(['CPM','date','sample'],axis=1))))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### MSE 2667"},{"metadata":{},"cell_type":"markdown","source":"## Add historical features without CPM\n### Compute rolling stats"},{"metadata":{"trusted":true},"cell_type":"code","source":"num_columns = [\n    'viewable_impressions',\n    'measurable_impressions',\n    'total_impressions', \n    'view_to_total_impressions'\n]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"aggs = ['mean', 'sum', 'median']\nagg_df = []\nfor agg in aggs:\n    agg_df.append(df_first.rolling(1)[num_columns].agg(agg).add_suffix(f'_{agg}'))\nagg_df = pd.concat(agg_df, axis='columns')\nagg_df.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm.auto import tqdm\nfrom collections import defaultdict\nfrom itertools import product\n\ndef compute_agg_by_cat_cols(df, cols_to_agg):\n    dates = df['date'].unique()\n    dates.sort()\n\n    aggs = ['sum', 'mean', 'median']\n\n    group_stats_df = defaultdict(lambda: defaultdict(list))\n    for agg, date, window_days in tqdm(list(product(aggs, dates, range(1, 8)))):\n        idx = np.searchsorted(dates, date)\n        window_dates = dates[idx - window_days:idx]\n\n        window_df = df[df['date'].isin(window_dates)]\n        if window_df.shape[0] == 0:\n            continue\n        stat_df = (\n            window_df\n            .groupby(id_cols)[cols_to_agg]\n            .agg(agg)\n            .add_suffix(f'_win_{window_days}_{agg}')\n        )\n        for id_col in id_cols:\n            grouped_by_col = (\n                window_df\n                .groupby(id_col)[cols_to_agg]\n                .agg(agg)\n                .add_suffix(f'_win_{window_days}_{agg}_{id_col}')\n            )\n            stat_df = stat_df.join(grouped_by_col)\n        stat_df['date'] = date\n        group_stats_df[agg][window_days].append(stat_df)\n    return group_stats_df\n\ndef compute_agg_by_id_cols(df, cols_to_agg):\n    dates = df['date'].unique()\n    dates.sort()\n    \n    aggs = ['sum', 'mean', 'median']\n    \n    group_stats_df_cat = defaultdict(lambda: defaultdict(list))\n    for agg, date, window_days in tqdm(list(product(aggs, dates, range(1, 8)))):\n        idx = np.searchsorted(dates, date)\n        window_dates = dates[idx - window_days:idx]\n\n        window_df = df[df['date'].isin(window_dates)][cat_cols + cols_to_agg]\n        stat_df = window_df[cat_cols].copy().drop_duplicates()\n        for id_col in cat_cols:\n            grouped_by_col = (\n                window_df\n                .groupby(id_col)[cols_to_agg]\n                .agg(agg)\n                .add_suffix(f'_win_{window_days}_{agg}_{id_col}')\n            )\n            stat_df = stat_df.join(grouped_by_col, on=id_col)\n        stat_df['date'] = date\n        group_stats_df_cat[agg][window_days].append(stat_df)\n    return group_stats_df_cat","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols_to_agg = num_columns\n\ngroup_stats_df = compute_agg_by_cat_cols(df_first, cols_to_agg)\ngroup_stats_df_cat = compute_agg_by_id_cols(df_first, cols_to_agg)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from itertools import chain\nfull_concated = df_first\n\njoin_cols = id_cols + ['date']\naggs, win_days = ['mean'], [7]\nfor agg, win_day in product(aggs, win_days):\n    stat_df = pd.concat(group_stats_df[agg][win_day]).reset_index().set_index(join_cols)\n    full_concated = full_concated.join(stat_df, on=join_cols)\n\njoin_cols = cat_cols + ['date']\naggs, win_days = ['mean'], [7]\nfor agg, win_day in product(aggs, win_days):\n    stat_df = pd.concat(group_stats_df_cat[agg][win_day]).set_index(join_cols)\n    \n    full_concated = full_concated.join(stat_df, on=join_cols, how='left')\nfull_concated = full_concated.join(agg_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"is_train = full_concated['sample'] == 'train'\nis_test = full_concated['sample'] == 'test'\n\nnew_train, new_test = full_concated[is_train], full_concated[is_test]\ntrain_df, eval_df = train_test_split(new_train, test_size=0.2, random_state=17)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_hist = LGBMRegressor(random_state=17, **params)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_hist.fit(\n    train_df.drop(['CPM','date','sample'],axis=1), train_df['CPM'],\n    eval_set=(eval_df.drop(['CPM','date','sample'],axis=1), eval_df['CPM']),\n    early_stopping_rounds=20,\n    verbose=80,\n)\nmean_squared_error(new_test['CPM'], clip(model_hist.predict(new_test.drop(['CPM','date','sample'],axis=1))))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### MSE 2720 > 2667 for model without historical features. May be we should add some smoothing or drop stat for smth categories"},{"metadata":{},"cell_type":"markdown","source":"## Add historical CPM from previous dates with different window size \nIt's ok in production case, because on date[i] we already know CPM for date[j] when j < i"},{"metadata":{"trusted":true},"cell_type":"code","source":"cols_to_agg = num_columns + ['CPM']\n\ngroup_stats_cpm_df = compute_agg_by_cat_cols(df_first, cols_to_agg)\ngroup_stats_cpm_cat_df = compute_agg_by_id_cols(df_first, cols_to_agg)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from itertools import chain\nfull_concated_cpm = df_first\n\njoin_cols = id_cols + ['date']\naggs, win_days = ['mean'], [1, 2, 7]\nfor agg, win_day in product(aggs, win_days):\n    stat_df = pd.concat(group_stats_cpm_df[agg][win_day]).reset_index().set_index(join_cols)\n    full_concated_cpm = full_concated_cpm.join(stat_df, on=join_cols)\n\njoin_cols = cat_cols + ['date']\naggs, win_days = ['mean'], [1, 2, 7]\nfor agg, win_day in product(aggs, win_days):\n    stat_df = pd.concat(group_stats_cpm_cat_df[agg][win_day]).set_index(join_cols)\n    \n    full_concated_cpm = full_concated_cpm.join(stat_df, on=join_cols, how='left')\nfull_concated_cpm = full_concated_cpm.join(agg_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"is_train = full_concated_cpm['sample'] == 'train'\nis_test = full_concated_cpm['sample'] == 'test'\n\nnew_train_cpm, new_test_cpm = full_concated_cpm[is_train], full_concated_cpm[is_test]\ntrain_df, eval_df = train_test_split(new_train_cpm, test_size=0.2, random_state=17)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_hist_cpm = LGBMRegressor(random_state=17, **params)\nmodel_hist_cpm.fit(\n    train_df.drop(['CPM','date','sample'],axis=1), train_df['CPM'],\n    eval_set=(eval_df.drop(['CPM','date','sample'],axis=1), eval_df['CPM']),\n    early_stopping_rounds=20,\n    verbose=80,\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_squared_error(new_test_cpm['CPM'], clip(model_hist_cpm.predict(new_test_cpm.drop(['CPM','date','sample'],axis=1))))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### MSE 2485"},{"metadata":{},"cell_type":"markdown","source":"## Add historical features based only on CPM"},{"metadata":{"trusted":true},"cell_type":"code","source":"cols_to_agg = ['CPM']\n\ngroup_stats_only_cpm_df = compute_agg_by_cat_cols(df_first, cols_to_agg)\ngroup_stats_only_cpm_df_cat = compute_agg_by_id_cols(df_first, cols_to_agg)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from itertools import chain\nfull_concated = df_first\n\njoin_cols = id_cols + ['date']\naggs, win_days = ['mean'], [1, 5, 7]\nfor agg, win_day in product(aggs, win_days):\n    stat_df = pd.concat(group_stats_only_cpm_df[agg][win_day]).reset_index().set_index(join_cols)\n    full_concated = full_concated.join(stat_df, on=join_cols)\n\njoin_cols = cat_cols + ['date']\naggs, win_days = ['mean'], [1, 5, 7]\nfor agg, win_day in product(aggs, win_days):\n    stat_df = pd.concat(group_stats_only_cpm_df_cat[agg][win_day]).set_index(join_cols)\n    \n    full_concated = full_concated.join(stat_df, on=join_cols, how='left')\nfull_concated = full_concated.join(agg_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"is_train = full_concated['sample'] == 'train'\nis_test = full_concated['sample'] == 'test'\n\nnew_train, new_test = full_concated[is_train], full_concated[is_test]\ntrain_df, eval_df = train_test_split(new_train, test_size=0.2, random_state=17)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = LGBMRegressor(random_state=17, **params)\nmodel.fit(\n    train_df.drop(['CPM','date','sample'],axis=1), train_df['CPM'],\n    eval_set=(eval_df.drop(['CPM','date','sample'],axis=1), eval_df['CPM']),\n    early_stopping_rounds=20,\n    verbose=80,\n)\nmean_squared_error(new_test['CPM'], clip(model.predict(new_test.drop(['CPM','date','sample'],axis=1))))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## MSE 2457"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}