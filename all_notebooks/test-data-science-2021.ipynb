{"cells":[{"metadata":{},"cell_type":"markdown","source":"# מבחן בית בדאטא סיינס לתעשייה 2020"},{"metadata":{"trusted":true},"cell_type":"code","source":"#install \n!pip install imblearn","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#imports\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.model_selection import train_test_split\nimport sklearn.linear_model as sk\nfrom sklearn.metrics import classification_report\nimport sklearn.metrics as metrics\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# --- Data Understanding ---\n* **Load Dataset**\n "},{"metadata":{"trusted":true},"cell_type":"code","source":"depression_data = pd.read_csv('../input/depression-1/depression_45901.csv')\ndepression_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Check columns: \ndepression_data.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"depression_data.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"depression_data.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"depression_data['depressed'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**As we can see the class feature is imbalnced**"},{"metadata":{},"cell_type":"markdown","source":"**info of each fetaure include histogram(MEAN/MAX/MIN value):**"},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in depression_data.columns:\n    plt.figure()\n    # Generate data on commute times.\n    data = depression_data[i]\n    data.plot.hist(grid=True, bins=30, rwidth=0.9,\n                       color='#607c8e')\n    plt.title(i)\n    plt.xlabel('Actual Value')\n    plt.ylabel('Counts')\n    ##### ---- mean\n    plt.axvline(data.mean(), color='k', linestyle='dashed', linewidth=2)\n\n    #text added\n    min_ylim, max_ylim = plt.ylim()\n    plt.text(data.mean(), max_ylim*0.5, 'Mean: {:.2f}'.format(data.mean()))\n    plt.text(data.max(), max_ylim*0.9, 'Max: {:.2f}'.format(data.max()))\n    plt.text(data.min(), max_ylim*0.9, 'Min: {:.2f}'.format(data.min()))\n    ####\n    plt.grid(axis='y', alpha=0.75)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in depression_data.columns:\n    sns.violinplot(x=\"depressed\", y=col, data=depression_data)\n    plt.show()\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# --- Data preparation ---"},{"metadata":{},"cell_type":"markdown","source":"nan values:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Clean results\ndepression_data = depression_data.dropna()\ndepression_data.groupby('depressed').describe()\n#resultQ3_fail","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Count nan values in depression_data\ndepression_data.isna().sum().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**now we have 1409 records instead of 1429(after we removed the nan values)**"},{"metadata":{},"cell_type":"markdown","source":"# Features Selection:\n\nI choose to stay with the 10 features that have the strongest connection with the depression class"},{"metadata":{"trusted":true},"cell_type":"code","source":"#1 ------ correlation matrix --------- \nplt.subplots(figsize=(20,15)) \ncor=depression_data.corr() \nsns.set(font_scale=0.8)\nsns.heatmap(cor, annot = True, cmap=plt.cm.Reds)\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#print 10 best from the corelation matrix using threshold of 0.2\ncor_target = abs(cor[\"depressed\"])\n\n#Selecting best correlated features according to the threshold\nrelevant_features_cor = cor_target[cor_target>0.02]\nrelevant_features_cor\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#2 ---------- Statistical tests (chi-squared for choosing the best k  features that have the strongest relationship with the output variable.)\ndata = depression_data\nX = data.iloc[:,0:21]  #independent columns\ny = data.iloc[:,-1]    #target column i.e price range\n#apply SelectKBest class to extract top 10 best features\nbestfeatures = SelectKBest( k=10) #using Anova f-value\nfit = bestfeatures.fit(X,y)\ndfscores = pd.DataFrame(fit.scores_)\ndfcolumns = pd.DataFrame(X.columns)\n#concat two dataframes for better visualization \nfeatureScores = pd.concat([dfcolumns,dfscores],axis=1)\nfeatureScores.columns = ['Specs','Score']  #naming the dataframe columns\nprint(featureScores.nlargest(10,'Score'))  #print 10 best feature","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features_df_new_cor = depression_data[['Age', 'Married','education_level', 'total_members',\n                'durable_asset','living_expenses','incoming_business', 'incoming_no_business',\n                'no_lasting_investmen','Ville_id', 'incoming_agricultural']]\nfeatures_df_new_cor","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# --- Modeling & Evaluation ---"},{"metadata":{},"cell_type":"markdown","source":"# Train & Test:"},{"metadata":{"trusted":true},"cell_type":"code","source":"depression_data_depressed = depression_data['depressed']\nX = features_df_new_cor\ny = depression_data_depressed.astype('bool')\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=19)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#print size of train and test:\nprint(\"X - Train data shape: \" , X_train.shape)\nprint(\"X - Test data shape: \" , X_test.shape)\nprint(\"y - Train data shape: \" , y_train.shape)\nprint(\"y - Test data shape: \" , y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1. Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"# train the model on train set \nmodel = sk.LogisticRegression().fit(X_train, y_train.ravel()) \n  \ny_pred = model.predict(X_test) \n  \n# print classification report \nlog_acc = metrics.accuracy_score(y_test, y_pred)\nlog_recall = metrics.recall_score(y_test, y_pred)\nprint(classification_report(y_test, y_pred)) \nprint(\"Accuracy:\",log_acc)\nprint(\"recall:\",log_recall)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\nprint(\"True Negatives: \",tn)\nprint(\"False Positives: \",fp)\nprint(\"False Negatives: \",fn)\nprint(\"True Positives: \",tp)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. KNN"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=19)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"knn_model = KNeighborsClassifier().fit(X_train, y_train)\n\ny_pred = knn_model.predict(X_test)\n\n# print classification report \nknn_acc = metrics.accuracy_score(y_test, y_pred)\nknn_recall = metrics.recall_score(y_test, y_pred)\nprint(classification_report(y_test, y_pred)) \nprint(\"Accuracy:\",knn_acc)\nprint(\"recall:\",knn_recall)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=19)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_model = RandomForestClassifier().fit(X_train, y_train)\n\ny_pred = rf_model.predict(X_test)\n\n# print classification report\nrandom_acc = metrics.accuracy_score(y_test, y_pred)\nrandom_recall = metrics.recall_score(y_test, y_pred)\nprint(classification_report(y_test, y_pred)) \nprint(\"Accuracy:\",random_acc)\nprint(\"recall:\",random_recall)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**As we can see we got a great accuracy BUT we got a very low recall!!**"},{"metadata":{},"cell_type":"markdown","source":"# Comparison Accuracy of Classification Algorithms"},{"metadata":{"trusted":true},"cell_type":"code","source":"# taken from: https://www.kaggle.com/osmanozen/comparison-of-classification-algorithms\nresult = []\nresults_acc = pd.DataFrame(columns= [\"Algorithms\",\"Accuracy\"])\n\nresult = pd.DataFrame([[\"LogisticRegression\", log_acc*100]], columns= [\"Algorithms\",\"Accuracy\"])\nresults_acc = results_acc.append(result)\n\nresult = pd.DataFrame([[\"KNeighborsClassifier\", knn_acc*100]], columns= [\"Algorithms\",\"Accuracy\"])\nresults_acc = results_acc.append(result)\n\nresult = pd.DataFrame([[\"RandomForest\", random_acc*100]], columns= [\"Algorithms\",\"Accuracy\"])\nresults_acc = results_acc.append(result)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(x= 'Accuracy', y = 'Algorithms', data=results_acc, color=\"b\")\nplt.xlabel('Accuracy %')\nplt.title(' Comparison of Classification Algorithms');\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Comparison Recall of Classification Algorithms"},{"metadata":{"trusted":true},"cell_type":"code","source":"# taken from: https://www.kaggle.com/osmanozen/comparison-of-classification-algorithms\nresult = []\nresults_recall = pd.DataFrame(columns= [\"Algorithms\",\"Recall\"])\n\nresult = pd.DataFrame([[\"LogisticRegression\", log_recall*100]], columns= [\"Algorithms\",\"Recall\"])\nresults_recall = results_recall.append(result)\n\nresult = pd.DataFrame([[\"KNeighborsClassifier\", knn_recall*100]], columns= [\"Algorithms\",\"Recall\"])\nresults_recall = results_recall.append(result)\n\nresult = pd.DataFrame([[\"RandomForest\", random_recall*100]], columns= [\"Algorithms\",\"Recall\"])\nresults_recall = results_recall.append(result)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(x= 'Recall', y = 'Algorithms', data=results_recall, color=\"b\")\nplt.xlabel('Recall %')\nplt.title(' Comparison of Classification Algorithms');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Improve results:"},{"metadata":{},"cell_type":"markdown","source":"*Ass you can see we have got a bad recall.. though we got a great accuracy.. \nso we need to deal with the imbalandced data.. i am gonna use SMOTE to use these problem*"},{"metadata":{},"cell_type":"markdown","source":"# Imbalanced Dataset:\n\ni had help from these website: https://medium.com/@saeedAR/smote-and-near-miss-in-python-machine-learning-in-imbalanced-datasets-b7976d9a7a79"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train.value_counts()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# well will use SMOTE only on the train sets! \nsmt = SMOTE()\nX_train, y_train = smt.fit_sample(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"now we can see that the x-train and y-train are balanced!!"},{"metadata":{},"cell_type":"markdown","source":"# 3.1. Logistic Regression\n\nmodling after dealing with imbalnced class"},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = LogisticRegression()\nlr.fit(X_train, y_train)\ny_pred = lr.predict(X_test)\n\n\nlog_score_s = metrics.accuracy_score(y_test, y_pred)\nlog_recall_s = metrics.recall_score(y_test, y_pred)\nprint(classification_report(y_test, y_pred)) \nprint(\"Accuracy:\",log_score_s)\nprint(\"Recall:\",log_recall_s)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\nprint(\"True Negatives: \",tn)\nprint(\"False Positives: \",fp)\nprint(\"False Negatives: \",fn)\nprint(\"True Positives: \",tp)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3.2.KNN"},{"metadata":{"trusted":true},"cell_type":"code","source":"knn_model = KNeighborsClassifier().fit(X_train, y_train)\n\ny_pred = knn_model.predict(X_test)\n\n# print classification report \nknn_score_s = metrics.accuracy_score(y_test, y_pred)\nknn_recall_s = metrics.recall_score(y_test, y_pred)\nprint(classification_report(y_test, y_pred)) \nprint(\"Accuracy:\",knn_score_s)\nprint(\"recall:\",knn_recall_s)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3.3 Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_model = RandomForestClassifier().fit(X_train, y_train)\n\ny_pred = rf_model.predict(X_test)\n\n# print classification report \nrandom_score_s = metrics.accuracy_score(y_test, y_pred)\nrandom_recall_s = metrics.recall_score(y_test, y_pred)\nprint(classification_report(y_test, y_pred)) \nprint(\"Accuracy:\",random_score_s)\nprint(\"recall:\",random_recall_s)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Comparison Accuracy of Classification Algorithms After SOMTE"},{"metadata":{"trusted":true},"cell_type":"code","source":"# taken from: https://www.kaggle.com/osmanozen/comparison-of-classification-algorithms\nresult = []\nresults_acc_s = pd.DataFrame(columns= [\"Algorithms\",\"Accuracy\"])\n\nresult = pd.DataFrame([[\"LogisticRegression\", log_score_s*100]], columns= [\"Algorithms\",\"Accuracy\"])\nresults_acc_s = results_acc_s.append(result)\n\nresult = pd.DataFrame([[\"KNeighborsClassifier\", knn_score_s*100]], columns= [\"Algorithms\",\"Accuracy\"])\nresults_acc_s = results_acc_s.append(result)\n\nresult = pd.DataFrame([[\"RandomForest\", random_score_s*100]], columns= [\"Algorithms\",\"Accuracy\"])\nresults_acc_s = results_acc_s.append(result)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(x= 'Accuracy', y = 'Algorithms', data=results_acc_s, color=\"b\")\nplt.xlabel('Accuracy %')\nplt.title(' Comparison of Classification Algorithms');\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Comparison Recall of Classification Algorithms After SOMTE"},{"metadata":{"trusted":true},"cell_type":"code","source":"# taken from: https://www.kaggle.com/osmanozen/comparison-of-classification-algorithms\nresult = []\nresults_recall_s = pd.DataFrame(columns= [\"Algorithms\",\"Recall\"])\n\nresult = pd.DataFrame([[\"LogisticRegression\", log_recall_s*100]], columns= [\"Algorithms\",\"Recall\"])\nresults_recall_s = results_recall_s.append(result)\n\nresult = pd.DataFrame([[\"KNeighborsClassifier\", knn_recall_s*100]], columns= [\"Algorithms\",\"Recall\"])\nresults_recall_s = results_recall_s.append(result)\n\nresult = pd.DataFrame([[\"RandomForest\", random_recall_s*100]], columns= [\"Algorithms\",\"Recall\"])\nresults_recall_s = results_recall_s.append(result)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(x= 'Recall', y = 'Algorithms', data=results_recall_s, color=\"b\")\nplt.xlabel('Recall %')\nplt.title(' Comparison of Classification Algorithms');","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}