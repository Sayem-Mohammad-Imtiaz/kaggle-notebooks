{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Attribute Information:\n\nThere are total 95 features. This Dataset has a lot of features. The Dataset description is given on the Data's page itself.\nBefore blindly performing EDA it's important to have information about the data.\n\nhttps://www.kaggle.com/fedesoriano/company-bankruptcy-prediction\n\nFirst we will train the model on raw data, and we will use <b> Feature Selection </b> technique to highlight some of the features and train on selected features. Hence, we will compare the models and accuracy.\n\n\n## Our Plan\n\n\n\n- <b> 1. Observe Dataset </b>\n\n\n- <b> 2. Exploratory Data Analysis </b>\n\n    - 2.1 Datset Cleaning\n    - 2.2 Check for data imbalance\n    \n\n\n- <b> 3. Data Preprocessing </b>\n\n    - 3.2 Split Training and testing\n    - 3.2 Feature Selection with RandomForest\n    - 3.3 PCA\n    \n    \n- <b> 4. Models, Hyperparameter Tuning, Cross Validation and Model Evaluation </b>\n\n    - 4.1 Logistic Regression\n    - 4.2 Naive Bayes\n    - 4.3 K-Nearest Neighbor\n    - 4.4 Decision Tree\n    - 4.5 Random Forest\n    - 4.6 XGBoost\n    \n","metadata":{}},{"cell_type":"markdown","source":"# 1. Observe Dataset","metadata":{}},{"cell_type":"code","source":"import pandas as pd\npd.set_option('max_columns', None)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path = \"../input/company-bankruptcy-prediction/\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(path + 'data.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr = df.corr()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize = (15,15))\nsns.heatmap(corr, ax = ax, cmap = 'viridis', linewidth = 0.1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Observation\n\n- All the features are numerical (int64 or float64)\n- All the values are scaled between -1 to 1.","metadata":{}},{"cell_type":"markdown","source":"# Exploratory Data Analysis\n\n- 2.1 Checking for data imbalance\n- 2.2 Outliers\n- 2.3 Filling null values\n","metadata":{}},{"cell_type":"markdown","source":"### 2.1 Checking for Data imbalance","metadata":{}},{"cell_type":"code","source":"df['Bankrupt?'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Financially stable:', round(df['Bankrupt?'].value_counts()[0] / len(df) * 100,2) ,'%')\nprint('Financially unstable:', round(df['Bankrupt?'].value_counts()[1] / len(df) * 100, 2), '%')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We see the data is highly skewed towards, Financially stable. If we train the model on this dataset, our prediction will be biased towards Financially stabled.\n\nWe will balance the dataset, to train our model.\n\nNotice: Notice how imbalanced is our original dataset! Most of the comapnies are Financially Stable. If we use this dataframe as the base for our predictive models and analysis we might get a lot of errors and our algorithms will probably overfit since it will \"assume\" that most of the companies are Financially Stable. But we don't want our model to assume, we want our model to detect patterns that give signs of Bankrupt!","metadata":{}},{"cell_type":"code","source":"## Visualizing the datas\n\nsns.set_theme(context = 'paper')\n\n\nplt.figure(figsize = (8,8))\nsns.countplot(x = 'Bankrupt?', data = df);\nplt.title('Class Distributions: \\n 0: Financially Stable & 1: Financially Unstable');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Splitting the Data (Original DataFrame)\n\n\nBefore proceeding with the <b> RandomUnderSampling </b> technique we have to seperate the original dataframe. \n\n<b>Why? </b>\n\nfor testing purposes, remeber although we are splitting the data when implementing Random UnderSampling or OverSampling techniques, we want to test our models on the original testing set not on the testing set created by either of these techniques. The main goal is to fit the model with the dataframes that were undersample and oversample (in order for our model to detect the patterns) and test it on the original testing set.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.model_selection import KFold, StratifiedKFold\n\nprint(\"Financially Stable:\", round(df['Bankrupt?'].value_counts()[0] / len(df) * 100, 2), '% of the dataset')\nprint(\"Financially Unstable:\", round(df['Bankrupt?'].value_counts()[1] / len(df) * 100,2),'% of the dataset')\n\nX = df.drop('Bankrupt?', axis = 1)\ny = df['Bankrupt?']\n\nsss = StratifiedKFold(n_splits = 5, random_state = None, shuffle = False)\n\nfor train_index, test_index in sss.split(X,y):\n    print(\"\\n Train\", train_index, \"Test\", test_index)\n    org_Xtrain, org_Xtest = X.iloc[train_index], X.iloc[test_index]\n    org_ytrain, org_ytest = y.iloc[train_index], y.iloc[test_index]\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## turn into an array\n\norg_Xtrain = org_Xtrain.values\norg_Xtest = org_Xtest.values\norg_ytrain = org_ytrain.values\norg_ytest = org_ytest.values\n\n## See if both the train and test label distribution are similarly distributed \ntrain_unique_label, train_counts_label = np.unique(org_ytrain, return_counts = True)\ntest_unique_label, test_counts_label = np.unique(org_ytest, return_counts = True)\n\nprint('Label Distirubtions: \\n')\nprint(train_counts_label / len(org_ytrain))\nprint(test_counts_label / len(org_ytest))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Random Under-Sampling and OverSampling\n\nIn this phase of the project we will implement \"Random Under Sampling\" which basically consists of removing data in order to have a more balanced dataset and this avoiding our models to overfitting.\n","metadata":{}},{"cell_type":"code","source":"## Lets shuffle the data before creating the subsamples\n\nxdf = df.sample(frac = 1)\n\n## amount of Financially unstable data is 220\n# sdf = Financially stable\n# ndf = Financially unstable\n\nsdf = df.loc[xdf['Bankrupt?'] == 0][:220]\nndf = df.loc[xdf['Bankrupt?']==1]\n\nnormal_distributed_df = pd.concat([sdf, ndf])\n\n# Shuffling again\n\nnxdf = normal_distributed_df.sample(frac = 1, random_state = 42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nxdf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Checking new dataframe\n\nprint(\"Distribution of the Classes in the subsample dataset\")\nprint(nxdf['Bankrupt?'].value_counts() / len(nxdf))\n\nsns.countplot('Bankrupt?', data = nxdf)\nplt.title(\"Equally Distributed Class\", fontsize = 14)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Correlation Matrices\n\nCorrelation matrices are the essence of understanding our data. We want to know if there are features that influence heavily in whether a specific transaction is a fraud. However, it is important that we use the correct dataframe (subsample) in order for use to see which features have a high positive or negative correlation with regards to fraud transactions.","metadata":{}},{"cell_type":"code","source":"## make sure we use the subsampe in our correlation\n\nf, (ax1, ax2) = plt.subplots(2,1, figsize = (54,50))\n\n## Entire data frame\n\ncorr = df.corr()\nsns.heatmap(corr, cmap = 'coolwarm_r', annot_kws = {'size': 20}, ax= ax1)\nax1.set_title(\"Imbalanced Correlated Matrix \\n\")\n\n\nsub_sample_corr = nxdf.corr()\nsns.heatmap(sub_sample_corr, cmap = 'coolwarm_r', annot_kws = {'size': 20}, ax = ax2)\nax2.set_title(\"SubSample Correlation Matrix\")\nplt.show()","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nxdf.hist(bins = 50, figsize = (35,20))\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see there are large number of blue square boxes and red square boxes which indicates, those column are has high or low correlation with one or other. So we will use PCA (Dimensionality Reduction) technqiue. \n\n<b> PCA vs Feature Selection? </b>\n\nhttps://stackoverflow.com/questions/16249625/difference-between-pca-principal-component-analysis-and-feature-selection#:~:text=The%20difference%20is%20that%20PCA,takes%20the%20target%20into%20consideration.&text=PCA%20is%20based%20on%20extracting,data%20shows%20the%20highest%20variability.\n\nJust to add to the very good answers above. The difference is that PCA will try to reduce dimensionality by exploring how one feature of the data is expressed in terms of the other features(linear dependecy). Feature selection instead, takes the target into consideration. It will rank your input variables in terms of how useful they are to predict the target value. This is true for univariate feature selection. Multi variate feature selection can also do something that can be considered a form of PCA, in the sense that it will discard some of the features in the input. But don't take this analogy too far.","metadata":{}},{"cell_type":"markdown","source":"## 3. Data Preprocessing\n\n- Split Training and Testing\n- Feature selection with RandomForest\n- PCA","metadata":{}},{"cell_type":"markdown","source":"#### Split Training and Testing","metadata":{}},{"cell_type":"code","source":"## this is equally sampled dataset (perfectly balanced target)\n\nX = nxdf.drop(['Bankrupt?'],1)\ny = nxdf['Bankrupt?']\n\nrf_fs_Xtrain, rf_fs_Xtest, rf_fs_ytrain, rf_fs_ytest = train_test_split(X,y, test_size = 0.1, random_state = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.feature_selection import SelectFromModel","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Feature selection with RandomForest","metadata":{}},{"cell_type":"code","source":"## modelling with balanced traget \n\nmodel = RandomForestClassifier(n_estimators=1000, random_state=0, n_jobs=-1)\nmodel.fit(rf_fs_Xtrain, rf_fs_ytrain)\n\nsel = SelectFromModel(model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## balanced target\n\nsel.fit(rf_fs_Xtrain, rf_fs_ytrain)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# balanced\n\nselected_feat= rf_fs_Xtrain.columns[(sel.get_support())]\nlen(selected_feat)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"selected_feat","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Creating a dataframe for only selected values to train later\n\nrf_fs = pd.DataFrame()\n\nfor column in selected_feat:\n    if column in nxdf:\n        rf_fs[column] = nxdf[column].values\n        \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rf_fs","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### PCA","metadata":{}},{"cell_type":"code","source":"from sklearn.decomposition import PCA","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_components = 2\npca = PCA(n_components = n_components)\npca.fit(nxdf)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_reduced_pca = PCA(n_components=2, random_state=42).fit_transform(rf_fs_Xtrain.values)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_pca = pca.transform(nxdf)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_pca.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# PCA scatter plot\nplt.figure(figsize = (8,8))\nplt.scatter(X_reduced_pca[:,0], X_reduced_pca[:,1], c=(rf_fs_ytrain == 0), cmap='coolwarm', label= 'Stable_Company', linewidths=2)\nplt.scatter(X_reduced_pca[:,0], X_reduced_pca[:,1], c=(rf_fs_ytrain == 1), cmap='coolwarm', label= 'Unstable_Company', linewidths=2)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Testing our Models\n\nWe will test our all the dataset (i.e normal, random forest feature selection and PCA dataset with each model.\n\nFor comparison we will make a new dataFrame, and comapre which method performed better\n\nAlso as it is classification problem, we will test it with following algorithms\n\n- Logistic Regression\n- Naive Bayes\n- KNN\n- Decision Trees\n- Random Forest\n- SVM","metadata":{}},{"cell_type":"markdown","source":"#### Preparing all the dataset for the models\n\n- <b> nxdf </b> is the original dataset.\n- <b> rf_fs </b> is the dataset with Feature Selection from Random Forest\n","metadata":{}},{"cell_type":"code","source":"## Splitting dataset for Normal data without feature selection\n\nX_train, X_test, y_train, y_test = train_test_split(nxdf.drop('Bankrupt?', axis = 1), nxdf['Bankrupt?'],test_size = 0.1, random_state = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nxdf.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rf_fs.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Since <b> rf_fs </b> target feature <b> Bankrupt? </b> has already been dropped. We know nxdf and rf_fs has same target value i.e ['Bankrupt'] so we will use the target value from nxdf for splitting Selected Dataset","metadata":{}},{"cell_type":"code","source":"## Splitting RandomForest Feature Selection dataset\n\nfs_Xtrain, fs_Xtest, fs_ytrain, fs_ytest = train_test_split(rf_fs, nxdf['Bankrupt?'], test_size = 0.1, random_state = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_score = pd.DataFrame(columns = (\"Original_Dataset\",\"Selected_Dataset\"))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Logistic Regression","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix, precision_score, recall_score, confusion_matrix","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Original Dataset","metadata":{}},{"cell_type":"code","source":"lrmodel1 = LogisticRegression(max_iter = 1000)\nlrmodel1.fit(X_train, y_train)\nscore1 = lrmodel1.score(X_test, y_test)\nlr_pred1 = lrmodel1.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Accuracy on Original Datset without Feature Selection:\n\nprint(\"Score:\", score1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr_cm1 = confusion_matrix(y_test, lr_pred1, labels = (1,0))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr_cm1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_axis_labels = [1,0]\ny_axis_labels = [1,0]\n\nsns.set(font_scale=1.4)\nsns.heatmap(lr_cm1, xticklabels = x_axis_labels, yticklabels = y_axis_labels, annot = True, annot_kws = {'size': 16})\nplt.xlabel(\"Actual Class\")\nplt.ylabel(\"Predicted Class\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n","metadata":{}},{"cell_type":"markdown","source":"### Feature Selection Dataset","metadata":{}},{"cell_type":"code","source":"lrmodel2 = LogisticRegression(max_iter = 1000)\nlrmodel2.fit(fs_Xtrain, fs_ytrain)\nscore2 = lrmodel2.score(fs_Xtest, fs_ytest)\nlr_ypred2 = lrmodel2.predict(fs_Xtest)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Score\", score2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr_cm2 = confusion_matrix(fs_ytest, lr_ypred2, labels = (1,0))\nprint(\"Confusion Matrix: \\n\", lr_cm2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_axis_labels = [1,0]\ny_axis_labels = [1,0]\n\nsns.set(font_scale=1.4)\nsns.heatmap(lr_cm2, xticklabels = x_axis_labels, yticklabels = y_axis_labels, annot = True, annot_kws = {'size': 16})\nplt.xlabel(\"Actual Class\")\nplt.ylabel(\"Predicted Class\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_score = model_score.append(pd.DataFrame({'Original_Dataset':[score1], 'Selected_Dataset': [score2]}, index = ['LogisticRegression']))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Naive Bayes","metadata":{}},{"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Original dataset","metadata":{}},{"cell_type":"code","source":"naiveb1 = GaussianNB()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"naiveb1.fit(X_train, y_train)\nscore1 = naiveb1.score(X_test, y_test)\nnb_pred1 = naiveb1.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Score:\", score1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nb_cm1 = confusion_matrix(y_test, nb_pred1, labels = (1,0))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Confusion Matrix: \\n\", nb_cm1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_axis_labels = [1,0]\ny_axis_labels = [1,0]\n\nsns.set(font_scale=1.4)\nsns.heatmap(nb_cm1, xticklabels = x_axis_labels, yticklabels = y_axis_labels, annot = True, annot_kws = {'size': 16})\nplt.xlabel(\"Actual Class\")\nplt.ylabel(\"Predicted Class\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Feature Selection Dataset","metadata":{}},{"cell_type":"code","source":"naiveb2 = GaussianNB()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"naiveb2.fit(fs_Xtrain, fs_ytrain)\nscore2 = naiveb2.score(fs_Xtest, fs_ytest)\nnb_pred2 = naiveb2.predict(fs_Xtest)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Score:\", score2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nb_cm2 = confusion_matrix(fs_ytest, nb_pred2, labels = [1,0])\nprint(\"Confusion Matrix: \\n\", nb_cm2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_axis_labels = [1,0]\ny_axis_labels = [1,0]\n\nsns.set(font_scale=1.4)\nsns.heatmap(nb_cm2, xticklabels = x_axis_labels, yticklabels = y_axis_labels, annot = True, annot_kws = {'size': 16})\nplt.xlabel(\"Actual Class\")\nplt.ylabel(\"Predicted Class\")\nplt.show()","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_score = model_score.append(pd.DataFrame({'Original_Dataset': [score1], 'Selected_Dataset': [score2]}, index = ['NaiveBayes']))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# KNN","metadata":{}},{"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Original Dataset","metadata":{}},{"cell_type":"code","source":"knn1 = KNeighborsClassifier(n_neighbors = 7)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"knn1.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"score1 = knn1.score(X_test, y_test)\nprint(score1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"knn_pred1 = knn1.predict(X_test)\nknn_cm1 = confusion_matrix(y_test, knn_pred1, labels = (1,0))\nprint(\"Confusion Matrix:\\n\", knn_cm1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_axis_labels = [1,0]\ny_axis_labels = [1,0]\n\nsns.set(font_scale=1.4)\nsns.heatmap(knn_cm1, xticklabels = x_axis_labels, yticklabels = y_axis_labels, annot = True, annot_kws = {'size': 16})\nplt.xlabel(\"Actual Class\")\nplt.ylabel(\"Predicted Class\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Hyperparameter tuning for KNN","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"error_rate = []\n\nfor i in range(1,40):\n    knn = KNeighborsClassifier(n_neighbors = i)\n    knn.fit(X_train, y_train)\n    pred_knn = knn.predict(X_test)\n    error_rate.append(np.mean(pred_knn != y_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (8,8))\nplt.plot(range(1,40), error_rate, color = 'blue', linestyle = 'dashed', marker = 'o', markerfacecolor = 'red', markersize = 10);\nplt.title('Error Rate vs K Value')\nplt.xlabel('K')\nplt.ylabel('Error Rate')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## let's see how much difference does it makes\n\ntuned_knn1 = KNeighborsClassifier(n_neighbors = 4)\ntuned_knn1.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tuned_score1 = tuned_knn1.score(X_test, y_test)\nprint(tuned_score1)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see, it's not that different","metadata":{}},{"cell_type":"markdown","source":"### Feature Selection Dataset","metadata":{}},{"cell_type":"code","source":"knn2 = KNeighborsClassifier(n_neighbors = 7)\nknn2.fit(fs_Xtrain, fs_ytrain)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"score2 = knn2.score(fs_Xtest, fs_ytest)\nprint(score2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"knn_pred2 = knn2.predict(fs_Xtest)\nknn_cm2 = confusion_matrix(fs_ytest, knn_pred2, labels = (1,0))\nprint(\"Confusion Matrix: \\n\", knn_cm2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_axis_labels = [1,0]\ny_axis_labels = [1,0]\n\nsns.set(font_scale=1.4)\nsns.heatmap(knn_cm2, xticklabels = x_axis_labels, yticklabels = y_axis_labels, annot = True, annot_kws = {'size': 16})\nplt.xlabel(\"Actual Class\")\nplt.ylabel(\"Predicted Class\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Hyperparamter tuning for this","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"error_rate = []\n\nfor i in range(1,40):\n    knn = KNeighborsClassifier(n_neighbors = i)\n    knn.fit(fs_Xtrain, fs_ytrain)\n    pred_knn = knn.predict(fs_Xtest)\n    error_rate.append(np.mean(pred_knn != fs_ytest))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (8,8))\nplt.plot(range(1,40), error_rate, color = 'blue', linestyle = 'dashed', marker = 'o', markerfacecolor = 'red', markersize = 10);\nplt.title('Error Rate vs K Value')\nplt.xlabel('K')\nplt.ylabel('Error Rate')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's test with, K = 5","metadata":{}},{"cell_type":"code","source":"tuned_knn2 = KNeighborsClassifier(n_neighbors = 14)\ntuned_knn2.fit(fs_Xtrain, fs_ytrain)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tuned_score2 = tuned_knn2.score(fs_Xtest, fs_ytest)\nprint(tuned_score2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tuned_knn_pred2 = knn.predict(fs_Xtest)\ntuned_cm2 = confusion_matrix(fs_ytest, tuned_knn_pred2, labels = (1,0))\nprint(\"Confusion Matrix: \\n\", tuned_cm2)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_axis_labels = [1,0]\ny_axis_labels = [1,0]\n\nsns.set(font_scale=1.4)\nsns.heatmap(tuned_cm2, xticklabels = x_axis_labels, yticklabels = y_axis_labels, annot = True, annot_kws = {'size': 16})\nplt.xlabel(\"Actual Class\")\nplt.ylabel(\"Predicted Class\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A bit better but not that great","metadata":{}},{"cell_type":"code","source":"model_score = model_score.append(pd.DataFrame({'Original_Dataset': [tuned_score1], 'Selected_Dataset': [tuned_score2]}, index = ['KNN']))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Decision Tree","metadata":{}},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dt1 = DecisionTreeClassifier()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Original data","metadata":{}},{"cell_type":"code","source":"dt1 = dt1.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"score1 = dt1.score(X_test, y_test)\nprint(score1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dt_pred1 = dt1.predict(X_test)\ndt_cm1 = confusion_matrix(y_test, dt_pred1, labels = [1,0])\nprint(\"Confusion Matrix: \\n\", dt_cm1)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_axis_labels = [1,0]\ny_axis_labels = [1,0]\n\nsns.set(font_scale=1.4)\nsns.heatmap(dt_cm1, xticklabels = x_axis_labels, yticklabels = y_axis_labels, annot = True, annot_kws = {'size': 16})\nplt.xlabel(\"Actual Class\")\nplt.ylabel(\"Predicted Class\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Feature Selection Data","metadata":{}},{"cell_type":"code","source":"dt2 = DecisionTreeClassifier()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dt2 = dt2.fit(fs_Xtrain, fs_ytrain)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"score2 = dt2.score(fs_Xtest, fs_ytest)\nprint(score2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dt_pred2 = dt2.predict(fs_Xtest)\ndt_cm2 = confusion_matrix(fs_ytest, dt_pred2, labels = [1,0])\nprint(\"Confusion Matrix: \\n\", dt_cm2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_axis_labels = [1,0]\ny_axis_labels = [1,0]\n\nsns.set(font_scale=1.4)\nsns.heatmap(dt_cm2, xticklabels = x_axis_labels, yticklabels = y_axis_labels, annot = True, annot_kws = {'size': 16})\nplt.xlabel(\"Actual Class\")\nplt.ylabel(\"Predicted Class\")\nplt.show()","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_score = model_score.append(pd.DataFrame({'Original_Dataset': [score1], 'Selected_Dataset': [score2]}, index = ['DecisionTrees']))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Random Forest","metadata":{}},{"cell_type":"code","source":"rfclf1 = RandomForestClassifier(n_estimators = 100)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### With Original dataset","metadata":{}},{"cell_type":"code","source":"rfclf1.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"score1 = rfclf1.score(X_test, y_test)\nprint(score1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rf_pred1 = rfclf1.predict(X_test)\nrf_cm1 = confusion_matrix(y_test, rf_pred1, labels = (1,0))\nprint(\"Confusion Matrix: \\n\", rf_cm1)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_axis_labels = [1,0]\ny_axis_labels = [1,0]\n\nsns.set(font_scale=1.4)\nsns.heatmap(rf_cm1, xticklabels = x_axis_labels, yticklabels = y_axis_labels, annot = True, annot_kws = {'size': 16})\nplt.xlabel(\"Actual Class\")\nplt.ylabel(\"Predicted Class\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### With Selected Features","metadata":{}},{"cell_type":"code","source":"rfclf2 = RandomForestClassifier(n_estimators = 100)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rfclf2.fit(fs_Xtrain, fs_ytrain)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"score2 = rfclf2.score(fs_Xtest, fs_ytest)\nprint(score2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rf_pred2 = rfclf2.predict(fs_Xtest)\nrf_cm2 = confusion_matrix(fs_ytest, rf_pred2, labels = [1,0])\nprint(\"Confusion Matrix: \\n\", rf_cm2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_axis_labels = [1,0]\ny_axis_labels = [1,0]\n\nsns.set(font_scale=1.4)\nsns.heatmap(rf_cm2, xticklabels = x_axis_labels, yticklabels = y_axis_labels, annot = True, annot_kws = {'size': 16})\nplt.xlabel(\"Actual Class\")\nplt.ylabel(\"Predicted Class\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Hyperparamter Tuning","metadata":{}},{"cell_type":"code","source":"# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 10, stop = 80, num = 10)\n               ]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n\n# Maximum number of levels in tree\nmax_depth = [2,4]\n\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5]\n\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2]\n\n# Method of selecting samples for training each tree\nbootstrap = [True, False]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create the param grid\nparam_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\nprint(param_grid)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tuned_rf = RandomForestClassifier()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nrf_Grid = GridSearchCV(estimator = tuned_rf, param_grid = param_grid, cv = 3, verbose=2, n_jobs = 4)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rf_Grid.fit(X_train, y_train)","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rf_Grid.best_params_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print (f'Train Accuracy - : {rf_Grid.score(X_train,y_train):.3f}')\nprint (f'Test Accuracy - : {rf_Grid.score(X_test,y_test):.3f}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tuned_score2 = rf_Grid.score(X_test, y_test)\nprint(tuned_score2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_score = model_score.append(pd.DataFrame({'Original_Dataset': [score1], 'Selected_Dataset': [tuned_score2]}, index = ['RandomForest']))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# XGBoost","metadata":{}},{"cell_type":"code","source":"from xgboost import XGBClassifier","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### With Original Dataset","metadata":{}},{"cell_type":"code","source":"xgb1 = XGBClassifier(n_estimators = 100)\nxgb1.fit(X_train, y_train)","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"score1 = xgb1.score(X_test, y_test)\nprint(score1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb_pred1 = xgb1.predict(X_test)\nxgb_cm1 = confusion_matrix(y_test, xgb_pred1, labels = [1,0])\nprint(\"Confusion Matrix: \\n\", xgb_cm1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_axis_labels = [1,0]\ny_axis_labels = [1,0]\n\nsns.set(font_scale=1.4)\nsns.heatmap(xgb_cm1, xticklabels = x_axis_labels, yticklabels = y_axis_labels, annot = True, annot_kws = {'size': 16})\nplt.xlabel(\"Actual Class\")\nplt.ylabel(\"Predicted Class\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### With Selected Dataset","metadata":{}},{"cell_type":"code","source":"xgb2 = XGBClassifier(n_estimators = 100)\nxgb2.fit(fs_Xtrain, fs_ytrain)","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"score2 = xgb2.score(fs_Xtest, fs_ytest)\nprint(score2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb_pred2 = xgb2.predict(fs_Xtest)\nxgb_cm2 = confusion_matrix(fs_ytest, xgb_pred2, labels = [1,0])\nprint(\"Confusion Matrix: \\n\", xgb_cm2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_axis_labels = [1,0]\ny_axis_labels = [1,0]\n\nsns.set(font_scale=1.4)\nsns.heatmap(xgb_cm2, xticklabels = x_axis_labels, yticklabels = y_axis_labels, annot = True, annot_kws = {'size': 16})\nplt.xlabel(\"Actual Class\")\nplt.ylabel(\"Predicted Class\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Hyperparamter Tuning","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params={\n \"learning_rate\"    : [0.05, 0.10, 0.15, 0.20, 0.25, 0.30 ] ,\n \"max_depth\"        : [ 3, 4, 5, 6, 8, 10, 12, 15],\n \"min_child_weight\" : [ 1, 3, 5, 7 ],\n \"gamma\"            : [ 0.0, 0.1, 0.2 , 0.3, 0.4 ],\n \"colsample_bytree\" : [ 0.3, 0.4, 0.5 , 0.7 ]\n    \n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tuned_xgb = XGBClassifier()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"random_search = RandomizedSearchCV(tuned_xgb, param_distributions = params, n_iter = 5, scoring = 'roc_auc', n_jobs = 1, cv = 5, verbose = 3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"random_search.fit(X_train, y_train)","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"random_search.best_estimator_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"random_search.best_params_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tuned_score1 = random_search.score(X_test, y_test)\nprint(tuned_score1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print (f'Train Accuracy - : {random_search.score(X_train,y_train):.3f}')\nprint (f'Test Accuracy - : {random_search.score(X_test,y_test):.3f}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tuned_xgb_pred1 = random_search.predict(X_test)\ntuned_xgb_cm1 = confusion_matrix(y_test, tuned_xgb_pred1, labels = [1,0])\nprint(\"Confusion Matrix: \\n\", tuned_xgb_cm1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_axis_labels = [1,0]\ny_axis_labels = [1,0]\n\nsns.set(font_scale=1.4)\nsns.heatmap(tuned_xgb_cm1, xticklabels = x_axis_labels, yticklabels = y_axis_labels, annot = True, annot_kws = {'size': 16})\nplt.xlabel(\"Actual Class\")\nplt.ylabel(\"Predicted Class\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Definitely this is biased towards, postive class. Since this is unbalanced dataset. We will hyeprtune with equally balanced dataset ","metadata":{}},{"cell_type":"markdown","source":"### Hypertuning for balanced dataset","metadata":{}},{"cell_type":"code","source":"tuned_xgb2 = XGBClassifier()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"random_search2 = RandomizedSearchCV(tuned_xgb2, param_distributions = params, n_iter = 5, scoring = 'roc_auc', n_jobs = 1, cv = 5, verbose = 3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"random_search2.fit(fs_Xtrain, fs_ytrain)","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"random_search2.best_estimator_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"random_search2.best_params_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tuned_score2 = random_search2.score(fs_Xtest, fs_ytest)\nprint(tuned_score2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print (f'Train Accuracy - : {random_search2.score(fs_Xtrain,fs_ytrain):.3f}')\nprint (f'Test Accuracy - : {random_search2.score(fs_Xtest,fs_ytest):.3f}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tuned_xgb_pred2 = random_search2.predict(fs_Xtest)\ntuned_xgb_cm2 = confusion_matrix(fs_ytest, tuned_xgb_pred2, labels = [1,0])\nprint(\"Confusion Matrix: \\n\", tuned_xgb_cm2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_axis_labels = [1,0]\ny_axis_labels = [1,0]\n\nsns.set(font_scale=1.4)\nsns.heatmap(tuned_xgb_cm2, xticklabels = x_axis_labels, yticklabels = y_axis_labels, annot = True, annot_kws = {'size': 16})\nplt.xlabel(\"Actual Class\")\nplt.ylabel(\"Predicted Class\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_score = model_score.append(pd.DataFrame({'Original_Dataset': [tuned_score1], 'Selected_Dataset': [tuned_score2]}, index = ['XGBoost']))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see, <b> XGBoost </b> performs best, on <b> selected features </b>. ","metadata":{}},{"cell_type":"code","source":"## Checking Classification report of the best model\n\nprint(classification_report(fs_ytest, tuned_xgb_pred2))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Biased model\n\nprint(classification_report(y_test, tuned_xgb_pred1))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Checking Classification report of the worst model\n\nprint(classification_report(y_test, lr_ypred2 ))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}