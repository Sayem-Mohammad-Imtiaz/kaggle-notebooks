{"cells":[{"cell_type":"markdown","source":"This is my first shot at doing PCA on a dataset (as well as one of the first few kernels I've run), all input/criticism is welcome! First lets load the libraries and dataset.","metadata":{"_cell_guid":"8c714b98-2223-412f-91a8-51ce4971a6cd","_uuid":"8babb7dbdb945542dbd1a5be8066345e268f83b7"}},{"outputs":[],"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport seaborn as sns\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.decomposition import PCA\n\ndata = pd.read_csv('../input/voice.csv')","metadata":{"_cell_guid":"36e5f47b-8f7d-4fcf-8d1c-a32009099226","_uuid":"5cc2039e635bfc6864bb03b810e23082245d4242"},"execution_count":1},{"cell_type":"markdown","source":"Splitting the data to features and labels, then standardizing the features...","metadata":{"_cell_guid":"9ae40e15-52b0-412d-b3e7-7299d5344686","_uuid":"28effe1bd5e1e6dc2bb8f15b12d169155982610c"}},{"outputs":[],"cell_type":"code","source":"x = data.iloc[:, :-1].values\ny = data.iloc[:,-1].values\n\nscaler = StandardScaler()\nx = scaler.fit_transform(x)","metadata":{"_cell_guid":"01eb8ace-2a28-4722-acae-e08e55562b98","_uuid":"fdee60f64963efcdbf736a14b86ca576863ddb6b"},"execution_count":2},{"cell_type":"markdown","source":"Checking to see if there are any correlations amongst the features...","metadata":{"_cell_guid":"93a34f2e-b7f8-458b-94d6-6cdcc2e7774d","_uuid":"754fc41e1b90272b5a4100b38364950304393917"}},{"outputs":[],"cell_type":"code","source":"sns.heatmap(np.corrcoef(x, rowvar = 0))","metadata":{"_cell_guid":"b9dd7ddc-a6d1-49aa-8d02-befad8ae23b9","_uuid":"ae428a7ec85467536ee06d62a716a7a35802e1be"},"execution_count":3},{"cell_type":"markdown","source":"Looks like there are some highly correlated features. This dataset intially had 20 features. Lets go ahead and do some PCA to determine how many dimensions we can reduce it by...","metadata":{"_cell_guid":"c33aee3d-6bfc-4a55-8d6f-a9f5e46aad23","_uuid":"5dac067284bb0b54de05c1947fdd13501a828d33"}},{"outputs":[],"cell_type":"code","source":"var_explain = []\nfor i in range(1,16):\n    pca = PCA(n_components= i)\n    x_pca = pca.fit_transform(x)\n    var_explain.append(np.sum(pca.explained_variance_ratio_))\n\nplt.plot(var_explain[:])\nplt.xlabel('Number of PCA Components')\nplt.ylabel('Explained Variance Ratio')\nplt.title('PCA')\nplt.yticks(np.arange(.4,1.05,.05))\nplt.xticks(np.arange(1,16,1))\nplt.show()","metadata":{"_cell_guid":"9c43fa27-5ac7-4a62-8296-94891bd65aaa","_uuid":"42ad93caab30279b89f8f124e987bf63bde44ca8"},"execution_count":4},{"cell_type":"markdown","source":"From my understanding a ratio between .9 and .95 is sufficient, so using 7- 10 priciple components will suffice for this dataset.","metadata":{"_cell_guid":"106377e5-2218-49a3-b944-3be62c6f7ad2","_uuid":"bedc1dd04c9d08888d564e4994fb11b0143de978"}}],"metadata":{"language_info":{"nbconvert_exporter":"python","version":"3.6.1","name":"python","pygments_lexer":"ipython3","mimetype":"text/x-python","codemirror_mode":{"version":3,"name":"ipython"},"file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat_minor":1,"nbformat":4}