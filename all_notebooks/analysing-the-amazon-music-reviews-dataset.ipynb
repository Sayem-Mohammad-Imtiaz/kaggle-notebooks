{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Exploratory analysis of amazon instrument reviews\nAt first we are gonna take a look at the given dataset. I dont know how the data was created or whether there was any kind of preprocessing before it was uploaded, but maybe we can make deductions on the prior processing of the data. \n\nThe data table contains 9 columns and 10261 rows. \n\n* reviewerID - ID of the reviewer, e.g. A2SUAM1J3GNN3B\n* asin - ID of the product, e.g. 0000013714\n* reviewerName - name of the reviewer\n* helpful - helpfulness rating of the review, e.g. 2/3\n* reviewText - text of the review\n* overall - rating of the product\n* summary - summary of the review\n* unixReviewTime - time of the review (unix time)\n* reviewTime - time of the review (raw)\n\nLets start with a quick first look over the data just to see if there is any data missing.","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd \nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"######## HELPER FUNCTIONS ##########\ndef look_at_categories(df,col):\n    # general look\n    print(df[col].describe())\n    # how often do reviewers review?\n    count_reviews = df[col].value_counts()\n\n    #print(count_reviews)\n    print('mean',count_reviews.mean())\n    print('min',count_reviews.min())\n    print('max',count_reviews.max())\n\n    print('0.5',count_reviews.quantile(0.5))\n    print('0.95',count_reviews.quantile(0.95))\n    print('0.99',count_reviews.quantile(0.99))\n\n    return count_reviews\n    \ndef look_at_numerics(df,col):\n    print(df[col].describe())\n    print('mean',df[col].mean())\n    print('min',df[col].min())\n    print('max',df[col].max())\n\n    print('0.5',df[col].quantile(0.5))\n    print('0.95',df[col].quantile(0.95))\n    print('0.99',df[col].quantile(0.99))\n    \n    return df[col]\n\n    \ndef draw_hist(count_reviews,ylabel,xlabel='number of reviews',bins=30):\n\n    fig  = plt.figure()\n    #hist\n    plt.hist(count_reviews,bins=bins)\n    # vertical lines\n    plt.axvline(count_reviews.mean(),color='yellow',label='mean')\n    plt.axvline(count_reviews.quantile(0.95),color='orange',label='Top 5%')\n    plt.axvline(count_reviews.quantile(0.99),color='red',label='Top 1%')\n    # labels\n    plt.legend()\n    plt.xlabel(xlabel)\n    plt.ylabel(ylabel)\n    # set size\n    fig.set_size_inches(18.5, 10.5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# read data\ndf= pd.read_csv('/kaggle/input/amazon-music-reviews/Musical_instruments_reviews.csv')#\n# Is the shape as anticipated?\nprint('Is the shape as anticipated? ',df.shape==(10261,9))\n# Are the columns as anticipated and what kind of index is used? \nprint('Are the columns as anticipated and what kind of index is used?')\nprint('index: ',df.head().index)\nprint('columns: ', df.columns)\n# Are there any NaN entries?\nprint('Number of nan entries? \\n',df.isna().sum())\n# Are there any duplicated rows?\nprint('Are there any duplicated rows? ',df.duplicated().sum()>0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# First Look\n## Has the data the expected shape?\nYes its (10261,9)\n## What do the columns and index look like?\nThe columns are exactly as descriped and the index is just ongoing numbers.\n## Are there any NaN entries in the table?\nYes. There are 27 missing reviewer names and 7 missing reviewer texts.\n\n### The number of columns are very manageable so we can take a quick look at all of them.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# can i fill up the missing names\nprint('can i fill up the missing names?')\nno_names_id = df[df['reviewerName'].isna()]['reviewerID'].unique()\ni_can='No'\nfor id in no_names_id:\n    if df[df['reviewerID'].isna()]['reviewerName'].shape[0]>0:\n        i_can='Yes'\nprint(i_can)\n    \n###\ncount_reviews_rvid = look_at_categories(df,'reviewerID')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Reviewer ID\n## How many reviewers are there?\nThere are 1429 reviewers writing 10261 reviews.\n## How many reviews are individual reviewers writing?\nReviewers write between 5 and 42 reviews. Oddly there arent any reviewers who wrote less than 5 reviews. Seems like they either removed all reviewers who didnt have atleast 5 reviews or their method of obtaining the reviews didnt allow them to capture infrequent reviewers.\n**This means we are most likely looking only at frequent reviewers instead of all reviewers**. We assume looking further into the data that there are actually reviewers that wrote less than 5 reviews, but were removed from the dataset or not captured.\n#### Reviewers in numbers\n* Minimum: 5\n* Mean: 7,18\n* Median: 6\n* 95% Quantile: 14\n* 99% Quantile: 23\n* Maximum: 42\n\n\n\n##### The data is visualized in the following histogram.\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"draw_hist(count_reviews_rvid,'number of reviewers')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The histogram shows us that most reviewers write only 5 reviews. This means these reviewers make up more than a third of all reviewers.\n\nMy first thought was that there might be an error in the data acquisition that converts reviewers with less than 5 reviews to reviewers with 5 reviews. In that case there would be a huge decrease in reviewers from 5 to 6, but there are 558 reviewers with 5 reviews and 296 reviewers with 6 reviews, which is a very reasonable progression.\n\nAnother notable thing to see is, that span of the upper 1% (23-42 -->19) is bigger than the span of the lower 99% (5-23 --> 18).\n\n### Conclusions:\n1. The data doesnt contain any reviewers with less than 5 reviews\n2. Most reviewers write 5 reviews\n3. There is a bigger difference in the top 1% of reviewers than in the lower 99% of reviewers\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"count_reviews_asin = look_at_categories(df,'asin')\n\n# Can a reviewer review a product multiple times? \n# (is there a duplicated row when only looking at asin and reviewerID?)\nprint('Can a reviewer review a product multiple times? ',df[['asin','reviewerID']].duplicated().sum()>0)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ASIN\n## How many different products are in the dataset?\nThere are 900 different products in the dataset with atleast 5 Reviews each. Again it seems like all products that dont have atleast 5 reviews are left out of the dataset either through method of obtaining or preprocessing of the data.\nIt's a bit odd that there are exactly 900 diffent products. Having exactly a round number of anything in a dataset tends to be a systemic reason, a human set limit or might just be a product of randomness. I dont think that there is a systemic reason within amazon to have a round number of musical instruments, but it also seems odd for a human to pick 900 as a limit instead of 1000. So i guess this 900 just occurs randomly, but there is a chance that there are actually more than 900 instruments on amazon.\n## Can a reviewer review a product multiple times?\nMaybe they can but no one did in the dataset.\n## How many reviews are there per product?\n#### Products in numbers\n* Minimum: 5\n* Mean: 11,4\n* Median: 8\n* 95% Quantile: 29\n* 99% Quantile: 67\n* Maximum: 163\n\n\n\n##### The data is visualized in the following histogram.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"draw_hist(count_reviews_asin,'number of products')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We get the same results we already got from looking at the reviewers. Most products get a low amount of reviews and the products with the highest amounts of reviews vary strongly in their number of reviews.\n\n\n### Conclusions:\n1. The data doesnt contain any products or any reviewers with less than 5 reviews\n2. Most products and reviewers have 5 reviews\n3. There is a bigger difference in the top 1% of products and reviewers than in the lower 99%\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# Can a reviewer review a product multiple times? \n# (is there a duplicated row when only looking at asin and reviewerID?)\nprint('Can a reviewer review a product multiple times? ',df[['asin','reviewerID']].duplicated().sum()>0)\n\n# how often do reviewers review?\ncount_reviews_rvn = look_at_categories(df,'reviewerName')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Reviewer name\nYou would expect reviewer name to be a very much like reviewer ID because it should be just a worse version of identifying a reviewer. \n\n### Reviewer name in numbers compared\n  \\  |Reviewer name| Reviewer ID\n---|-------------|-------------\n Minimum: | 1 | 5\nMean: | 7,32 | 7,18\nMedian: | 6 | 6 \n95% Quantile: | 14 | 14\n99% Quantile: | 25 | 23\nMaximum: | 66 | 42\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,ax = plt.subplots(2,sharex=True)\n\n\n#fig  = plt.figure()\n#hist\nax[0].hist(count_reviews_rvn,bins=60,label = 'number of reviewer names')\n# vertical lines\nax[0].axvline(count_reviews_rvn.mean(),color='yellow',label='mean')\nax[0].axvline(count_reviews_rvn.quantile(0.95),color='orange',label='Top 5%')\nax[0].axvline(count_reviews_rvn.quantile(0.99),color='red',label='Top 1%')\n# labels\nax[0].legend()\n\n\n#hist\nax[1].hist(count_reviews_rvid,bins=40,label='number of reviewer IDs')\n# vertical lines\nax[1].axvline(count_reviews_rvid.mean(),color='yellow',label='mean')\nax[1].axvline(count_reviews_rvid.quantile(0.95),color='orange',label='Top 5%')\nax[1].axvline(count_reviews_rvid.quantile(0.99),color='red',label='Top 1%')\n# labels\nax[1].legend()\n\nax.flat[0].set( ylabel='number of reviewer names')\nax.flat[1].set(xlabel='number of reviews', ylabel='number of reviewer IDs')\n\n# set size\nfig.set_size_inches(16.5, 12.5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The histograms are atleast very similar. The minimum of reviews by reviewer names is smaller than the minimum of reviews by reviewer IDs, so there have to be reviewer IDs with multiple names. Additionally the maximum of reviews by reviewer names is also higher than the maximum of reviews by reviewer IDs, so there have to be reviewer names with multiple IDs.\n\nAlright lets take a look at the names with multiple IDs.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"## names with multiple IDs\nfor name in df['reviewerName'].unique():\n    if df[df['reviewerName']==name]['reviewerID'].unique().shape[0]>1:\n        print(name)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ok so the reviewer names arent unique usernames. So there are just different users with the same reviewer name.\n\nSo lets look at the names with that share the same ID.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"        \n## multiple names with one ID\nfor ID in df['reviewerID'].unique():\n    if df[df['reviewerID']==ID]['reviewerName'].unique().shape[0]>1:\n        print(df[df['reviewerID']==ID]['reviewerName'].unique())\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ok so most reviewer IDs with multiple names are caused by nans and on some occasion its just a change in name like 'caffeinebrain' --> 'coffeebrain'.\n\nI just had wrong assumptions about the reviewer names.\n\n\n\n### Conclusions:\n1. The data doesnt contain any products or any reviewers with less than 5 reviews\n2. Most products and reviewers have 5 reviews\n3. There is a bigger difference in the top 1% of products and reviewers than in the lower 99%\n4. Reviewer names are not unique and can be changed\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"### helpful\n# make helpful ratio\nfrom ast import literal_eval\n\ndef ratio_fun(x):\n    evaled = literal_eval(x)\n    if evaled[1] == 0:\n        return float('NaN')\n    else:\n        return evaled[0]/evaled[1]\n\ndef all_votes_fun(x):\n    return literal_eval(x)[1]\n\ndf['helpful_ratio'] = df['helpful'].apply(ratio_fun)\ndf['helpful_all_votes'] = df['helpful'].apply(all_votes_fun)\ncount_reviews_hpr = look_at_numerics(df,'helpful_ratio')\ncount_reviews_hpav = look_at_numerics(df,'helpful_all_votes')\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Helpful\nHelpful is a string in the form of \"[\\*upvote\\*,\\* all votes\\*]\" (atleast this seems to make the most sense, because the first number is always smaller than the second). Because we got two number in relation to each other, we are gonna look at the ratio between them. In the case [0,0] we are just gonna give back NaN.\\\nThis means the helpful ratio is between 0 and 1 and the number of entries is the number of reviews that received votes. Just looking at the ratio doesnt give a picture on  how total votes are destributed over the reviews, so we are also gonna look at the number of all votes.\\\n\n### Helpful in numbers:\n\\  | Helpful ratio | Helpful\n----|--------|-----\n Minimum:| 0 | 0\n Mean:| 0,78 | 1,86\n Median:| 1 | 0\n 95% Quantile:| 1| 7\n 99% Quantile:| 1 | 30\n Maximum:| 1 | 300\n \nThere are 3465 (33,8%) reviews with votes out of a total of 10261 reviews.\n\n##### The data is visualized in the following histogram.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndraw_hist(df['helpful_ratio'].dropna(),'number of reviews','helpful ratios')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We are looking around 1/3 of all reviews and a majority of the reviews are completely helpful. We already know that we are only looking at frequent reviewers, so we can say frequent reviewers are getting mostly positiv feedback. If we assume that people give positive and negative votes at the same rate if they actually found a review helpful or not helpful, we can conclude that frequent reviewers write helpful reviews. I am not really willing to make that assumption just because i believe if people read an unhelpful review they just keep looking for a helpful one and if people read a really helpful review they might just give it an upvote. BUT this believe is mostly based on how i behave.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"draw_hist(df[df['helpful_all_votes']>0]['helpful_all_votes'],'helpful ratios', 'all votes',bins=80)\n_= look_at_numerics(df[df['helpful_all_votes']>0],'helpful_all_votes')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looking at the distribution of votes over the reviews with atleast one vote given, we see that the median number of votes is still only at 2 and the top 25% is getting atleast 4 votes. Seeing that we already removed the 2/3 of reviews that didnt get any votes, we can say that reviews get rarely a lot of feedback.\nSo we can update our Conclusion list.\n\n\n\n### Conclusions:\n1. The data doesnt contain any products or any reviewers with less than 5 reviews\n2. Most products and reviewers have 5 reviews\n3. There is a bigger difference in the top 1% of products and reviewers than in the lower 99%\n4. Reviewer names are not unique and can be changed\n5. Reviews get rarely more than 4 helpful votes, but the votes given are overwhelmingly positiv\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"count_reviews_ov = look_at_numerics(df,'overall')\n# are all entries integers?\nsum(df['overall']==df['overall'].apply(lambda x:int(x)))\n# how many 5 star reviews are there?\ndf['overall'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Overall (product rating)\nOverall shows the product rating as an integer between 1 and 5. Its the rating given by the corresponding review. \n\n### Overall in numbers:\n  \\  |Reviewer name\n---|-------------\n Minimum: | 1 \nMean: | 4,48 \nMedian: | 5 \n95% Quantile: | 5 \n99% Quantile: | 5 \nMaximum: | 5 \n\n\n##### The data is visualized in the following histogram.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"draw_hist(df['overall'],'number of reviews','overall')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see the average review of a frequent reviewer is pretty high at around 4,5 and nearly 7000 out of 10261 reviews give the product 5 stars. On the lower end you can see that there are barely any 1-2 star reviews.\n\n\n\n\n### Conclusions:\n1. The data doesnt contain any products or any reviewers with less than 5 reviews\n2. Most products and reviewers have 5 reviews\n3. There is a bigger difference in the top 1% of products and reviewers than in the lower 99%\n4. Reviewer names are not unique and can be changed\n5. Reviews get rarely more than 4 helpful votes, but the votes given are overwhelmingly positiv\n6. Frequent reviewers give in ~90% of cases a 4-5 star review\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['reviewText_len'] = df['reviewText'].apply(lambda x: len(str(x)))\n_ = look_at_numerics(df,'reviewText_len')\n# Minimum without \"NaN\"\nno_na_review = df[['reviewText']].dropna()\nno_na_review['reviewText_len'] = no_na_review['reviewText'].apply(lambda x: len(str(x)))\nprint('Minimum without \"NaN\": ',no_na_review['reviewText_len'].min())\n# Whats the shortest review?\nprint('Whats the shortest review? \\n',no_na_review[no_na_review['reviewText_len'] == no_na_review['reviewText_len'].min()]['reviewText'])\n# Is the shortest review helpful?\nprint('Is the shortest review helpful? ',df[df['reviewText_len'] == no_na_review['reviewText_len'].min()]['helpful'])\n# Whats the longest review?\n#print('Whats the longest review? \\n',no_na_review[no_na_review['reviewText_len'] == no_na_review['reviewText_len'].max()]['reviewText'].values)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ReviewText\nA quick evaluation of text is difficult-ish. If you wanted to look whether a text is just nonsense, you could run the text through a pretrained english language model and look at the average likeliness that the words are in the place that they are. I have never tried this. We are just gonna look at the length of each review. The length is the number of characters in the review.\n\n\n### Review text length in numbers:\n  \\  |Reviewer text length\n---|-------------\n Minimum: | 3 (9)*\nMean: | 485,93\nMedian: | 284 \n95% Quantile: | 1552\n99% Quantile: | 3027 \nMaximum: | 11310\n\n\\* without 'NaN'\n\n##### The data is visualized in the following histogram.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"draw_hist(df['reviewText_len'],'number of reviews','reviewer text length',bins=80)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Most reviews have less than 300 characters with the shortest one just being the word \"excellent\" and the longest one being a comparison between multiple articles. I looked at a few of the reviews and i didnt see any nonsense or anything that would point towards systemic error. So there seems to be no problem with the review texts.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['summary_len'] = df['summary'].apply(lambda x: len(str(x)))\n_ = look_at_numerics(df,'summary_len')\n# Whats a summary of a NaN review?\nprint(' Whats a summary of a NaN review? \\n',df[df['reviewText'].isna()]['summary'])\n# Whats the shortest summary?\nprint('Whats the shortest summary? \\n',df[df['summary_len']==df['summary_len'].min()]['summary'])\n# Whats the longest summary?\nprint('Whats the longest summary? \\n',df[df['summary_len']==df['summary_len'].max()]['summary'].values)\n#\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Summarys\nNow review texts but shorter. So there are no NaN entries here. That means that the missing reviews all have summarys and they look completely normal like:\n* The Pop Rocks with the Yeti\n* No power = No Sound, But It Sounds GREAT!\n\nWe are gonna do the same and look at the summary length again.\n\n\n### Summary length in numbers:\n  \\  |Summary length\n---|-------------\n Minimum: | 1\nMean: | 24,34\nMedian: | 21 \n95% Quantile: | 55\n99% Quantile: | 74 \nMaximum: | 128\n\n##### The data is visualized in the following histogram.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"draw_hist(df['summary_len'],'number of reviews','length of summary',bins=60)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Most summarys are below 25 characters. Some short ones are just like \"F\", \"A+\", \"OK\" or just a \"-\". The longest one is:\n\n\"Excellent, best design ever, but only get the SILVER aluminum one, not the thermoplastic as they break-- I've had 3 break on me.\"\n\nThe longest one is 128 characters long, which might be the limit, because its a power of 2.\nOtherwise no data irregularities to see here.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['reviewTime_dt'] = pd.to_datetime(df['reviewTime'])\n_ = look_at_numerics(df,'reviewTime_dt')\n# Is unixTime = Time?\nprint('Is unixTime = Time? ',sum(pd.to_datetime(df['reviewTime']) == pd.to_datetime(df['unixReviewTime'],unit='s'))>0)\n# Where is 95% of the data?\nprint('Where is 95% of the data? past ', df['reviewTime_dt'].quantile(0.05))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Time (unixReviewTime, ReviewTime)\nUnixtime and review time point to the same date. We are gonna look at the number of reviews over time as datetime.\n\n\n\n### ReviewTime numbers:\n  \\  |ReviewTime\n---|-------------\n First: | 2004-09-18\nMean: | 2013-02-11 \nMedian: | 2013-05-14 \n95% Quantile: | 2014-06-03\n99% Quantile: | 2014-07-08 \nLast: | 2014-07-22\n\n##### The data is visualized in the following histogram.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"draw_hist(df['reviewTime_dt'],'number of reviews','timeline')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So the data has reviews inbetween 2004 and 2015 with rarely any reviews before 2008. Half of the reviews made are just between 2013-05-14 and 2014-07-22.\nThis seems to fit to my presumption of amazons usage. \n\n\n### Conclusions:\n1. The data doesnt contain any products or any reviewers with less than 5 reviews\n2. Most products and reviewers have 5 reviews\n3. There is a bigger difference in the top 1% of products and reviewers than in the lower 99%\n4. Reviewer names are not unique and can be changed\n5. Reviews get rarely more than 4 helpful votes, but the votes given are overwhelmingly positiv\n6. Frequent reviewers give in ~90% of cases a 4-5 star review\n7. Around 95% of all reviews are made past 2011\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Relations\n\nAt last we are gonna look at the relationship between the numerical features.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looking at the unixReviewTime column we can see:\n* before 2010 there were barely close to no 1-2 star ratings (but there were also barely any reviews before 2010 and 1-2 star reviews are rare overall atleast by frequent reviewers)\n* helpful ratio before 2010 is in average higher than after 2010\n* the number of all helpful votes seem to drop near the end of the timeline (but these reviews might have had too little time to accumulate higher number of votes)\n* review text length and summary text length increases over time\n\nOther interesting things are:\n* long summarys and long reviews dont get a lot of helpful votes\n* reviews with an overall low rating seem to be short","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# All Conclusions\n* The data doesnt contain any products or any reviewers with less than 5 reviews\n* Most products and reviewers have 5 reviews\n* There is a bigger difference in the top 1% of products and reviewers than in the lower 99%\n* Reviewer names are not unique and can be changed\n* Reviews get rarely more than 4 helpful votes, but the votes given are overwhelmingly positiv\n* Frequent reviewers give in ~90% of cases a 4-5 star review\n* Around 95% of all reviews are made past 2011\n* before 2010 there were barely close to no 1-2 star ratings (but there were also barely any reviews before 2010 and 1-2 star reviews are rare overall atleast by frequent reviewers)\n* helpful ratio before 2010 is in average higher than after 2010\n* the number of all helpful votes seem to drop near the end of the timeline (but these reviews might have had too little time to accumulate higher number of votes)\n* review text length and summary text length increases over time\n* long summarys and long reviews dont get a lot of helpful votes\n* reviews with an overall low rating seem to be short","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}