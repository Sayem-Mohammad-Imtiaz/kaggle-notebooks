{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Making predictions on the NYC criminality dataset\n\nAntonio RÃ­os-Vila","metadata":{}},{"cell_type":"markdown","source":"In the previous notebook that I shared, we performed an exploratory analysis of the data contained mainly in the New York City Department historic of arrests and complaints. As I said in that notebook before, we have a great amount of data to perform experiments on making some predictions with simple Machine Learning techniques, those who are not based in deep learning approaches. So, in this notebook, it is time to explore how easy and/or hard could be to predict some data in this dataset.\nThe focus of this work will be to try to predict where the crimes occur (given some data) with both a classification approach (where we try to gess the borough where the crime has taken place) and a regression one, where we will try to predict the exact location of the complaint.","metadata":{}},{"cell_type":"markdown","source":"First, we import all the libraries and tools that we need to do this","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd\n\nfrom sklearn.linear_model import LogisticRegression, LinearRegression, Ridge, Lasso\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import LinearSVC, SVC, SVR, LinearSVR\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.decomposition import PCA\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier\n\n\nfrom sklearn.model_selection import StratifiedKFold, KFold \nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler, label_binarize\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn.metrics import f1_score, r2_score, roc_curve, auc\nfrom sklearn.utils.random import sample_without_replacement\nfrom sklearn.utils import shuffle\nfrom sklearn import preprocessing\n\nfrom itertools import cycle\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Utility Functions","metadata":{}},{"cell_type":"markdown","source":"Before diving into this approach, I coded some utility functions that will be called during this notebook and I preferred to wrap up in order to avoid code repetition.","metadata":{}},{"cell_type":"code","source":"# Function to train the classification problem with 10 - Fold CV\ndef trainClassificationCV(X,Y,numclasses, ylabel):\n    #We define the KFOLD separator\n    cv = StratifiedKFold(n_splits=10)\n    \n    #Some utility variables that will be useful during our train/test process\n    mean_score_logistic = 0\n    mean_score_NB = 0\n    mean_score_SVM = 0\n    mean_score_SVMK = 0\n    iteration = 0\n    \n    #I would like to know the F1 scores of the models, as precission and recall are important when predicting\n    #the targetted label we want to describe\n    f1_scores_logistic = []\n    f1_scores_NB = []\n    f1_scores_SVM = []\n    f1_scores_SVMK = []\n    \n    # I will also store all the historic variables of the models to obtain a ROC curve for each fold and method\n    roc_curve_logistic = []\n    roc_curve_NB = []\n    roc_curve_SVM = []\n    roc_curve_SVMK = []\n    \n    roc_curve_data = []\n    \n    for train_idx, test_idx in cv.split(X,Y):\n        #Split into train_test datasets\n        x_train, y_train = X.iloc[train_idx], Y.iloc[train_idx]\n        x_test, y_test = X.iloc[test_idx], Y.iloc[test_idx]\n        \n        #We define the models to work with: Logistic regression, Naive Bayes, Lineal SVM and Kernel SVM\n        model_logistic = LogisticRegression(C=100, max_iter=10000)\n        model_NB = GaussianNB()\n        model_SVM = LinearSVC(random_state=1, tol=1e-5)\n        model_SVMK = SVC(gamma=\"auto\", C=10000)\n        \n        #Train step\n        model_logistic.fit(x_train, y_train[ylabel].ravel())\n        model_NB.fit(x_train, y_train[ylabel].ravel())\n        model_SVM.fit(x_train, y_train[ylabel].ravel())\n        model_SVMK.fit(x_train, y_train[ylabel].ravel())\n        \n        ## VALIDATION AND METRIC OBTAINMENT METHODS ##\n        mean_score_logistic = mean_score_logistic + model_logistic.score(x_test,y_test[ylabel].ravel())\n        mean_score_NB = mean_score_NB + model_NB.score(x_test,y_test[ylabel].ravel())\n        mean_score_SVM = mean_score_SVM + model_SVM.score(x_test,y_test[ylabel].ravel())\n        mean_score_SVMK = mean_score_SVMK + model_SVMK.score(x_test,y_test[ylabel].ravel())\n        \n        score_roc_Logistic = model_logistic.decision_function(x_test)\n        score_roc_NB = model_logistic.decision_function(x_test)\n        score_roc_SVM = model_SVM.decision_function(x_test)\n        score_roc_SVMK = model_SVMK.decision_function(x_test)\n        \n        fprlog = dict()\n        tprlog = dict()\n        roc_auclog = dict()\n        \n        fprNB = dict()\n        tprNB = dict()\n        roc_aucNB = dict()\n        \n        fprSVM = dict()\n        tprSVM = dict()\n        roc_aucSVM = dict()\n        \n        fprSVMK = dict()\n        tprSVMK = dict()\n        roc_aucSVMK = dict()\n        \n        for i in range(numclasses):\n            fprlog[i], tprlog[i], _ = roc_curve(label_binarize(y_test,classes=[i for i in range(numclasses)])[:, i], score_roc_Logistic[:,i])\n            roc_auclog[i] = auc(fprlog[i], tprlog[i])\n            \n            fprNB[i], tprNB[i], _ = roc_curve(label_binarize(y_test,classes=[i for i in range(numclasses)])[:, i], score_roc_NB[:,i])\n            roc_aucNB[i] = auc(fprNB[i], tprNB[i])\n            \n            fprSVM[i], tprSVM[i], _ = roc_curve(label_binarize(y_test,classes=[i for i in range(numclasses)])[:, i], score_roc_SVM[:,i])\n            roc_aucSVM[i] = auc(fprSVM[i], tprSVM[i])\n            \n            fprSVMK[i], tprSVMK[i], _ = roc_curve(label_binarize(y_test,classes=[i for i in range(numclasses)])[:, i], score_roc_SVMK[:,i])\n            roc_aucSVMK[i] = auc(fprSVMK[i], tprSVMK[i])\n            \n        \n        roc_curve_data.append([[fprlog, tprlog, roc_auclog], [fprNB, tprNB, roc_aucNB], [fprSVM, tprSVM, roc_aucSVM], [fprSVMK, tprSVMK, roc_aucSVMK]])\n    \n        print(\"Iteration \", iteration)\n        iteration+=1\n        print(\"Accuracy Naive Bayes: \", model_NB.score(x_test,y_test[ylabel].ravel()))\n        print(\"Accuracy Logistic Regression: \", model_logistic.score(x_test,y_test[ylabel].ravel()))\n        print(\"Accuracy Linear SVM: \", model_SVM.score(x_test,y_test[ylabel].ravel()))\n        print(\"Accuracy Kernel SVM: \", model_SVMK.score(x_test,y_test[ylabel].ravel()))\n\n        y_predicted_lr = model_logistic.predict(x_test)\n        y_predicted_NB = model_NB.predict(x_test)\n        y_predicted_SVM = model_SVM.predict(x_test)\n        y_predicted_SVMK = model_SVMK.predict(x_test)\n    \n        precision_lr = f1_score(y_test.Borough.ravel(), y_predicted_lr, average=None)\n        precision_nb = f1_score(y_test.Borough.ravel(), y_predicted_NB, average=None)\n        precision_svm = f1_score(y_test.Borough.ravel(), y_predicted_SVM, average=None)\n        precision_svmk = f1_score(y_test.Borough.ravel(), y_predicted_SVMK, average=None)\n        \n        f1_scores_logistic.append(precision_lr)\n        f1_scores_NB.append(precision_nb)\n        f1_scores_SVM.append(precision_svm)\n        f1_scores_SVMK.append(precision_svmk)\n\n        print(\"F1-Score Naive Bayes: \", precision_nb)\n        print(\"F1-Score Logistic Regression: \", precision_lr)\n        print(\"F1-Score Linear SVM: \", precision_svm)\n        print(\"F1-Score Kernel SVM: \", precision_svmk)\n    \n\n        print(\"-----------------\")\n\n    result_lr = np.round(mean_score_logistic/10,3)\n    result_nb = np.round(mean_score_NB/10,3)\n    result_svm = np.round(mean_score_SVM/10,3)\n    result_svmk = np.round(mean_score_SVMK/10,3)\n    print(\"Accuracy in Cross validation Naive Bayes: \", result_nb) \n    print(\"Accuracy in Cross validation Logistic Regresion: \", result_lr)    \n    print(\"Accuracy in Cross validation Linear SVM: \", result_svm)   \n    print(\"Accuracy in Cross validation Kernel SVM: \", result_svmk)\n    \n    #We return the ROC data\n    return roc_curve_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to train the regression problem with 10 - Fold CV\n# The procedure is similar to the above function, even there are not that many parameters to obtain during\n# the training process (we only want the R2 score)\ndef trainRegressionCV(X,Y, ylabel):\n    cv = KFold(n_splits=10) \n    r2_linear = 0\n    r2_svr = 0\n    r2_rfo = 0\n    r2_ridge = 0\n    r2_lasso = 0\n    r2_knn = 0\n    iteration = 0\n    for train_idx, test_idx in cv.split(X,Y):\n        x_train, y_train = X.iloc[train_idx], Y.iloc[train_idx]\n        x_test, y_test = X.iloc[test_idx], Y.iloc[test_idx]\n\n        model_linear = LinearRegression()\n        model_SVR = SVR(max_iter=1000, C=50, kernel=\"poly\", coef0=2)\n        model_RandomForest = RandomForestRegressor(criterion=\"mse\", bootstrap=True)\n        model_Ridge = Ridge(alpha=1.0)\n        model_Lasso = Lasso(alpha=0.1)\n        model_KNN = KNeighborsRegressor(n_neighbors=100)\n    \n        model_linear.fit(x_train, y_train[ylabel].ravel())\n        model_SVR.fit(x_train, y_train[ylabel].ravel())\n        model_RandomForest.fit(x_train, y_train[ylabel].ravel())\n        model_Ridge.fit(x_train, y_train[ylabel].ravel())\n        model_Lasso.fit(x_train, y_train[ylabel].ravel())\n        model_KNN.fit(x_train, y_train[ylabel].ravel())\n    \n        print(\"Iteration \", iteration)\n        iteration+=1\n    \n        r2_linear = r2_linear + model_linear.score(x_test, y_test[ylabel].ravel())\n        r2_svr = r2_svr + model_SVR.score(x_test, y_test[ylabel].ravel())\n        r2_rfo = r2_rfo + model_RandomForest.score(x_test, y_test[ylabel].ravel())\n        r2_ridge = r2_ridge + model_Ridge.score(x_test, y_test[ylabel].ravel())\n        r2_lasso = r2_lasso + model_Lasso.score(x_test, y_test[ylabel].ravel())\n        r2_knn = r2_knn + model_KNN.score(x_test, y_test[ylabel].ravel())\n    \n        print(f\"R2 Linear Regression {model_linear.score(x_test, y_test[ylabel].ravel())}\")\n        print(f\"R2 SVR {model_SVR.score(x_test, y_test[ylabel].ravel())}\")\n        print(f\"R2 Random Forest {model_RandomForest.score(x_test, y_test[ylabel].ravel())}\")\n        print(f\"R2 KNN100 {model_KNN.score(x_test, y_test[ylabel].ravel())}\")\n        print(f\"R2 Ridge {model_Ridge.score(x_test, y_test[ylabel].ravel())}\")\n        print(f\"R2 Lasso {model_Lasso.score(x_test, y_test[ylabel].ravel())}\")\n    \n        print(\"-----------------\")\n\n    result_lm = np.round(r2_linear/10,3)\n    result_svr = np.round(r2_svr/10,3)\n    result_rfo = np.round(r2_rfo/10,3)\n    result_rid = np.round(r2_ridge/10,3)\n    result_las = np.round(r2_lasso/10,3)\n    result_knn = np.round(r2_knn/10,3)\n    print(\"Mean R2 Linear Regression: \", result_lm) \n    print(\"Mean R2 SVR: \", result_svr)\n    print(\"Mean R2 RandomForest: \", result_rfo) \n    print(\"Mean R2 Ridge: \", result_rid)\n    print(\"Mean R2 Lasso: \", result_las)     ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Boxplot function for our dataset\ndef boxes(xdata, ydata, labels, ylabel):\n    idx = 0\n    idy = 0\n    fig, axes = plt.subplots(2,4, figsize=(30, 20))\n    for label in labels:\n        sns.boxplot(ax=axes[idx, idy], x=xdata[label], y = ydata[ylabel])\n        idy+=1\n        if idy>3:\n            idy = 0\n            idx += 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Somewat analogous function for R software pairplot\n#However, we only make pairplots with our interest variable (represented by ylabel) as we want\n#to see if there is any kind of linear relationship between regressors and variables\ndef pairplots(xdata, ydata, labels, ylabel, subpl):\n    idx = 0\n    idy = 0\n    maxY = subpl[1]\n    fig, axes = plt.subplots(subpl[0],subpl[1], figsize=(15, 5))\n    for label in labels:\n        sns.scatterplot(ax=axes[idx, idy], x=xdata[label], y = ydata[ylabel])\n        idy+=1\n        if idy>maxY-1:\n            idy = 0\n            idx += 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Monstruous function to plot the ROC curves\ndef plot_roc_curves_CV(dataroc, foldsToSee, yencoder, methods):\n    plt.figure()\n    lw = 2\n    foldSee = foldsToSee if foldsToSee is not None else range(10)\n    for fold in foldSee:\n        fig, axs = plt.subplots(2, 2, figsize=(20,20))\n        fig.suptitle(f\"Fold {fold}\", fontsize=16)\n        fpr_complete = dataroc[fold]\n        tpr_complete = dataroc[fold]\n        rocauc_complete = dataroc[fold]\n        x=0\n        y=0\n        for method in range(4):\n            axis = axs[x,y]\n    \n            fpr = fpr_complete[method][0]\n            tpr = tpr_complete[method][1]\n            rocauc = rocauc_complete[method][2]\n\n            colors = cycle(['aqua', 'darkorange', 'cornflowerblue', 'red', 'yellow'])\n            for i, color in zip(range(5), colors):\n                axis.plot(fpr[i], tpr[i], color=color, lw=lw,\n                     label='ROC curve of class {0} (area = {1:0.2f})'\n                     ''.format(yencoder.inverse_transform([i])[0], rocauc[i]))\n\n            axis.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n            axis.axis(xmin=0.0, xmax=1.0, ymin=0.0, ymax=1.05)\n            axis.set_xlabel('False Positive Rate')\n            axis.set_ylabel('True Positive Rate')\n            axis.set_title(f\"ROC - {methods[method]}\")\n            axis.legend(loc=\"lower right\")\n            \n            y+=1\n            if y > 1:\n                y = 0\n                x+=1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Classification","metadata":{}},{"cell_type":"markdown","source":"In this section, we will try to classify in which borough belongs an input which, in our case, is a biased crime complaint. I said biased as we are avoiding some explicit or easily deductible location variables (such as the cartesian coordinates and the latitude/longitude data). This will make the problem more interesting as we will not be easing the model to provide direct solutions with coordinate variables.","metadata":{}},{"cell_type":"markdown","source":"Before starting the training of the models, let's prepare the data","metadata":{}},{"cell_type":"code","source":"#Read dataframe and drop null data, in my previous work i saw that it was somewhat residual (25.000 dropped data over 5.000.000 entries), but it helps to avoid future errors\ndataframe = pd.read_csv('../input/arrestsnypd/NYPDArrests.csv')\ndataframe = dataframe.dropna()\n# We get the descriptors that I do not conseader \"cheat\" too much the prediction of the borough. ARREST_PRECINCT we will see that traverses that thin line between cheat and non\n# trivial description, however, that is to be discussed for further analysis\ndata_criminality = dataframe[[\"ARREST_DATE\",\"OFNS_DESC\",\"LAW_CAT_CD\",\"ARREST_BORO\",\"ARREST_PRECINCT\", \"AGE_GROUP\", \"PERP_SEX\", \"PERP_RACE\"]]\n# There are some ages like 12005 and so that are not good to label classify, as introduce unwanted variability, so we directly discard them as they are also residual\n# We stick to the main five groups of ages in this dataset\ndata_criminality = data_criminality[data_criminality.AGE_GROUP.isin(['45-64', '25-44', '18-24', '<18', '65+'])]\n\n#This is just for the sake of my comfortability treating the data, it is not necessary to change names, but I think it is more descriptive\ndata_criminality.loc[data_criminality.ARREST_BORO == \"M\", \"ARREST_BORO\"] = \"Manhattan\"\ndata_criminality.loc[data_criminality.ARREST_BORO == \"B\", \"ARREST_BORO\"] = \"Bronx\"\ndata_criminality.loc[data_criminality.ARREST_BORO == \"K\", \"ARREST_BORO\"] = \"Brooklyn\"\ndata_criminality.loc[data_criminality.ARREST_BORO == \"Q\", \"ARREST_BORO\"] = \"Queens\"\ndata_criminality.loc[data_criminality.ARREST_BORO == \"S\", \"ARREST_BORO\"] = \"Staten Island\"\n\ndata_criminality.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Before stepping further, we must take into account a tiny detail about this dataset: it has 5.000.000 entries. As much as I'd love to process all data, i believe it is unfeasable (as Kaggle gives limited resources to freebies like me) to do so. Now then, we have to get a smaller dataset which we can work from. In my opinion, I think if we can get a 100.000 samples dataset can be enough to accomplish our goals (and would not turn my Kaggle space into a fire hazard).\n\nHowever, we have to be careful when reducing the dataset, as we have to obtain a balanced dataframe to not have unbalanced problems. The best way, in my opinion, to do so is to make a random sample between the subsets conformed by the target labels. In our case, as the model has to predict among five boroughs, we should make its correspondent subsets and analyse if we can do so.","metadata":{}},{"cell_type":"code","source":"manhattan_crimes = data_criminality.loc[data_criminality.ARREST_BORO == \"Manhattan\"]\nprint(len(manhattan_crimes))\nbronx_crimes = data_criminality.loc[data_criminality.ARREST_BORO == \"Bronx\"]\nprint(len(bronx_crimes))\nbrooklyn_crimes = data_criminality.loc[data_criminality.ARREST_BORO == \"Brooklyn\"]\nprint(len(brooklyn_crimes))\nqueens_crimes = data_criminality.loc[data_criminality.ARREST_BORO == \"Queens\"]\nprint(len(queens_crimes))\nstatenisland_crimes = data_criminality.loc[data_criminality.ARREST_BORO == \"Staten Island\"]\nprint(len(statenisland_crimes))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can observe by this projection that the dataset is already quite unbalanced (Staten Island has way less reports than the rest). However, we can observe that we can get a 100.000 reports dataset by randomly sampling 20.000 events from each subset, they have enough data to carry out this way. Even though, despite having enough data for the models we are using in this work to converge, we should take into account that we are deliberately increasing the variance from our solution to the optimal one by reducing the dataset. However, we have to do it for computational reasons, and I believe the results we obtain with this models will not be far from the ones obtained if we used all the dataset samples.\n\nSo, without further ado, we proceed to create our reduced dataset, which I call the \"Intermediate dataset\"","metadata":{}},{"cell_type":"code","source":"mhc = manhattan_crimes.iloc[sample_without_replacement(n_population=len(manhattan_crimes), n_samples = 20000, random_state=1)]\nbrc = bronx_crimes.iloc[sample_without_replacement(n_population=len(bronx_crimes), n_samples = 20000, random_state=1)]\nbxc = brooklyn_crimes.iloc[sample_without_replacement(n_population=len(brooklyn_crimes), n_samples = 20000, random_state=1)]\nquc = queens_crimes.iloc[sample_without_replacement(n_population=len(queens_crimes), n_samples = 20000, random_state=1)]\nsic = statenisland_crimes.iloc[sample_without_replacement(n_population=len(statenisland_crimes), n_samples = 20000, random_state=1)]\nintermediate_dataset = pd.concat([mhc, brc, bxc, quc, sic], ignore_index=True)\nintermediate_dataset = shuffle(intermediate_dataset, random_state=1)\nprint(len(intermediate_dataset))\nintermediate_dataset.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Once done that, we proceed to prepare, finally, our data to do the training process","metadata":{}},{"cell_type":"code","source":"X = intermediate_dataset[[\"ARREST_DATE\",\"OFNS_DESC\",\"LAW_CAT_CD\", \"ARREST_PRECINCT\", \"AGE_GROUP\",\"PERP_SEX\", \"PERP_RACE\"]]\nY = intermediate_dataset[[\"ARREST_BORO\"]]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As most of our data is categorical, we need to label encode it to make it numerical","metadata":{}},{"cell_type":"code","source":"le_OFNS = preprocessing.LabelEncoder()\nle_LAW = preprocessing.LabelEncoder()\nle_AGE = preprocessing.LabelEncoder()\nle_SEX = preprocessing.LabelEncoder()\nle_RACE = preprocessing.LabelEncoder()\nle_BOROUGH = preprocessing.LabelEncoder()\n\nle_BOROUGH.fit(Y.ARREST_BORO)\nle_OFNS.fit(X.OFNS_DESC)\nle_LAW.fit(X.LAW_CAT_CD)\nle_AGE.fit(X.AGE_GROUP)\nle_SEX.fit(X.PERP_SEX)\nle_RACE.fit(X.PERP_RACE)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scaler = MinMaxScaler()\nXDataframe = {'Month': pd.DatetimeIndex(X['ARREST_DATE']).month, \n             'Offense': le_OFNS.transform(X.OFNS_DESC),\n             'Law_Code': le_LAW.transform(X.LAW_CAT_CD),\n             'Precint': X.ARREST_PRECINCT,\n             'Age': le_AGE.transform(X.AGE_GROUP),\n             'Sex': le_SEX.transform(X.PERP_SEX),\n             'Race': le_RACE.transform(X.PERP_RACE)}\nX = pd.DataFrame(data=XDataframe)\nX_Scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\nY_Encoded = pd.DataFrame(data={'Borough':le_BOROUGH.transform(Y.ARREST_BORO)})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataroc = trainClassificationCV(X_Scaled,Y_Encoded, 5, \"Borough\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So, after the trainin process, we see that all the models are very accurate on predicting the Borough by the data we provide (both with the Accuracy and the F1 scores). However, it is hard to tell which one is the best, as they seem to be pretty equal. However, we can have a bigger picture by plotting the ROC curve (which takes) they describe in each fold. As they behave pretty consistently, I will only plot one fold to make this analysis, but you can see it all by adding a None to the folds parameter that it is given to the function.","metadata":{}},{"cell_type":"code","source":"plot_roc_curves_CV(dataroc, [0], le_BOROUGH, ['Logistic Regression', 'Naive Bayes', 'Linear SVM', 'Kernel SVM'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So, as we can see, even of having that good results during the training process, there exists a model that outperforms the rest in all classes, which is the Kernel SVM. As we can observe from the ROC traced by the Lineal SVM, the data that we are handling is not linearly sepparable (as it would have a 100% accuracy on test to be so). The Logistic Regression and the Naive Bayes method do confirm that. We can also observe that the most hard variable to predict is the Brooklyn borough. However, if we apply a non-linearity function to the distribution (the kernel that uses the SVM-Kernel method), we observe that the ROC has an area of 1 in all classes, which is the most optimal result we can obtain. In summary, the data is not linear and, with a simple non-linearity distribution, we can obtain the best performance of a model that has a great accuracy in test.\n\nHowever, having that good results somehow triggers one question on me: Does it need all the regressors we have inserted?\nTo inspect that, we should check the distribution of the data depending on each applied regression and analyze it.","metadata":{}},{"cell_type":"code","source":"boxes(X, Y, [\"Month\", \"Offense\", \"Law_Code\", \"Precint\", \"Age\", \"Sex\", \"Race\"], \"ARREST_BORO\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see, the regressor that better sepparates the classes is the Precinct one, as we can observe that we could tell the difference between the data. So, I believe, that using only the Precinct descriptor we could get the same best model (Kernel SVM) in less time (and with a simpler formula, which is always something to pursue in this problems). However, we can observe that Brooklyn (the less perfomrant class) can be somehow hard to distinguish between Bronx and Queens, so I believe we should add a supporting regressor that helps to make a difference between them. In this dataset, I believe the race of the offender can somehow draw a difference between these boroughs (as we can observe in their respective boxplot). So, I'm going to train the model but only considering these two descriptors.","metadata":{}},{"cell_type":"code","source":"dataroc_2 = trainClassificationCV(X_Scaled[[\"Precint\", \"Race\"]],Y_Encoded, 5, \"Borough\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_roc_curves_CV(dataroc_2,[0], le_BOROUGH, ['Logistic Regression', 'Naive Bayes', 'Linear SVM', 'Kernel SVM'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see, the Kernel SVM maintains its performance results.","metadata":{}},{"cell_type":"markdown","source":"## Regression","metadata":{}},{"cell_type":"markdown","source":"In this section, we will try to predict where exactly (by regression means) has occoured a complaint. In other words, we will try to predict the Latitude and the Longitude of the reported event by the other regressors the dataset contains, and we will analyze how this works and if it is feasable to do so. In order to achieve this goal, we will train the regression models both on the Latitude and the Longitude of the events.\n","metadata":{}},{"cell_type":"markdown","source":"The following code is just like in the classification section (including the Latitude and Longitude variables that we will try to predict).","metadata":{}},{"cell_type":"code","source":"data_criminality = dataframe[[\"ARREST_DATE\",\"OFNS_DESC\",\"LAW_CAT_CD\",\"ARREST_BORO\",\"ARREST_PRECINCT\", \"AGE_GROUP\", \"PERP_SEX\", \"PERP_RACE\", \"Latitude\", \"Longitude\"]]\ndata_criminality = data_criminality[data_criminality.AGE_GROUP.isin(['45-64', '25-44', '18-24', '<18', '65+'])]\ndata_criminality.loc[data_criminality.ARREST_BORO == \"M\", \"ARREST_BORO\"] = \"Manhattan\"\ndata_criminality.loc[data_criminality.ARREST_BORO == \"B\", \"ARREST_BORO\"] = \"Bronx\"\ndata_criminality.loc[data_criminality.ARREST_BORO == \"K\", \"ARREST_BORO\"] = \"Brooklyn\"\ndata_criminality.loc[data_criminality.ARREST_BORO == \"Q\", \"ARREST_BORO\"] = \"Queens\"\ndata_criminality.loc[data_criminality.ARREST_BORO == \"S\", \"ARREST_BORO\"] = \"Staten Island\"\nmanhattan_crimes = data_criminality.loc[data_criminality.ARREST_BORO == \"Manhattan\"]\nbronx_crimes = data_criminality.loc[data_criminality.ARREST_BORO == \"Bronx\"]\nbrooklyn_crimes = data_criminality.loc[data_criminality.ARREST_BORO == \"Brooklyn\"]\nqueens_crimes = data_criminality.loc[data_criminality.ARREST_BORO == \"Queens\"]\nstatenisland_crimes = data_criminality.loc[data_criminality.ARREST_BORO == \"Staten Island\"]\nmhc = manhattan_crimes.iloc[sample_without_replacement(n_population=len(manhattan_crimes), n_samples = 20000, random_state=1)]\nbrc = bronx_crimes.iloc[sample_without_replacement(n_population=len(bronx_crimes), n_samples = 20000, random_state=1)]\nbxc = brooklyn_crimes.iloc[sample_without_replacement(n_population=len(brooklyn_crimes), n_samples = 20000, random_state=1)]\nquc = queens_crimes.iloc[sample_without_replacement(n_population=len(queens_crimes), n_samples = 20000, random_state=1)]\nsic = statenisland_crimes.iloc[sample_without_replacement(n_population=len(statenisland_crimes), n_samples = 20000, random_state=1)]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"intermediate_dataset = pd.concat([mhc, brc, bxc, quc, sic], ignore_index=True)\nintermediate_dataset = shuffle(intermediate_dataset, random_state=1)\nintermediate_dataset = intermediate_dataset.reset_index()[[\"ARREST_DATE\",\"OFNS_DESC\",\"LAW_CAT_CD\",\"ARREST_BORO\",\"ARREST_PRECINCT\", \"AGE_GROUP\", \"PERP_SEX\", \"PERP_RACE\", \"Latitude\", \"Longitude\"]]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = intermediate_dataset[[\"ARREST_DATE\",\"OFNS_DESC\",\"LAW_CAT_CD\", \"ARREST_BORO\",\"ARREST_PRECINCT\", \"AGE_GROUP\", \"PERP_SEX\", \"PERP_RACE\"]]\nY_Lat = intermediate_dataset[[\"Latitude\"]]\nY_Long = intermediate_dataset[[\"Longitude\"]]\nXDataframe = {'Month': pd.DatetimeIndex(X['ARREST_DATE']).month, \n             'Offense': le_OFNS.transform(X.OFNS_DESC),\n             'Borough': le_BOROUGH.transform(X.ARREST_BORO),\n             'Law_Code': le_LAW.transform(X.LAW_CAT_CD),\n             'Precint': X.ARREST_PRECINCT,\n             'Age': le_AGE.transform(X.AGE_GROUP),\n             'Sex': le_SEX.transform(X.PERP_SEX),\n             'Race': le_RACE.transform(X.PERP_RACE)}\nX = pd.DataFrame(data=XDataframe)\nscaler = StandardScaler()\nX_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Once prepared all of our data, we train the regression models","metadata":{}},{"cell_type":"code","source":"trainRegressionCV(X_scaled, Y_Lat, \"Latitude\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainRegressionCV(X_scaled, Y_Long, \"Longitude\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So, as we can observe, the models show poor performance on the prediction task, except the Random Forest model in the Longitude prediction which unexpectably is able to explain the 98% of the variance present in the test set. \nThis was something expected, as there is a lot of categorical data which, sadly, does not draw any linear relationship between the predicted label and the regressors. We can, indeed, see this with some graphics: ","metadata":{}},{"cell_type":"code","source":"pairplots(X, Y_Lat, [\"Month\", \"Offense\", \"Borough\", \"Law_Code\", \"Precint\", \"Age\", \"Sex\", \"Race\"], \"Latitude\", (2,4))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see in the Latitude case, the data has poor variance and it cannot be seen any linear relationships between the latitude and the different regressors. It is logical to think that is impossible to perform a good regression task with this kind of data. Indeed, it is normal that this could happen as the majority of the regressors used in this problem are, except the Precint one, categorical data.","metadata":{}},{"cell_type":"code","source":"pairplots(X, Y_Long, [\"Month\", \"Offense\", \"Borough\", \"Law_Code\", \"Precint\", \"Age\", \"Sex\", \"Race\"], \"Longitude\", (2,4))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Suprprisingly in the case of the Longitude, we see that the Precint regressor somehow draws a tendency with multiple clusters within the target variable, which explains why the models perform better than in the Latitude. Indeed, this also explains somehow why the Random Forest Regressor does indeed perform very well on this case, as it shows good performance when the data can have multidimensional relationships with the targetted variable to predict. ","metadata":{}},{"cell_type":"markdown","source":"The first thing before using clustering or ensemble methods, is to try to reduce the number of regressors in these models, which usually benefit in performance from these simplifications.\nFirst of all, let's see if there is a multicolinearity problem, where a regressor already explains the variability of other and makes it useless for the model, by plotting any correlations between the regressors that we have in the X set.","metadata":{}},{"cell_type":"code","source":"correlation = X.corr()\ncorrelation\nmask = np.triu(np.ones_like(correlation, dtype=bool))\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(11, 9))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(correlation, mask=mask, cmap=cmap, vmax=1, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can observe, the Borough and the Precint are highly correlated (as expected from the classification problem), so we can say that we can discard one of them without losing relevant information. We can also say there is some close relation between Law Code and Offense, so we will also pick the latter as it has more categories than the other one.\n\nIn the case of Latitude is really hard, as there are no linear relationships. However, I believe that if we take those that seem to have more variability (Month, Precint, Offense, and Race), we could get some performance out of them. In the case of the Longitude, I think, from the scatterplots, that using the Precint and the Offense would be enough to get the best performance of the Random Forest method.","metadata":{}},{"cell_type":"code","source":"trainRegressionCV(X_scaled[[\"Offense\", \"Precint\", \"Month\", \"Race\"]], Y_Lat, \"Latitude\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainRegressionCV(X_scaled[[\"Offense\", \"Precint\"]], Y_Long, \"Longitude\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Bringing in ensembles","metadata":{}},{"cell_type":"markdown","source":"So, we have performed several classifications and regressions tasks in this document. However, we have seen that, for example, in the regression task we had problems to predict the location of a certain crime, as there was no tendency (correlation) between the regressors and the explained variable.\nHowever, could we try to solve that?\nWe can try different approaches. However, one of the most interesting ones is to bring in ensembles (a set of prediction models which combine their predictions, depending the method, to obtain better results). In this section, we are going to try to combine some ensemble options to solve our problem in both the classification and the regression contexts and observe if we obtain better results than in the previous methods. ","metadata":{}},{"cell_type":"markdown","source":"First, we should import the desired libraries to work with and redesign our utility functions to support ensembles.","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import StackingClassifier, AdaBoostClassifier, GradientBoostingClassifier, BaggingClassifier\nfrom sklearn.ensemble import StackingRegressor, AdaBoostRegressor, GradientBoostingRegressor, BaggingRegressor\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Then, we redefine our utility functions for classification to train the proposed ensemble models with a 10-fold Cross Validation method. ","metadata":{}},{"cell_type":"code","source":"def trainClassification_EnsemblesCV(X,Y,numclasses, ylabel):\n    #We define the KFOLD separator\n    cv = StratifiedKFold(n_splits=10)\n    \n    #Some utility variables that will be useful during our train/test process\n    mean_score_adaboost = 0\n    mean_score_gradientboost = 0\n    mean_score_stacking = 0\n    mean_score_bagging = 0\n    iteration = 0\n    \n    #I would like to know the F1 scores of the models, as precission and recall are important when predicting\n    #the targetted label we want to describe\n    f1_scores_adaboost = []\n    f1_scores_gradientboost = []\n    f1_scores_stacking = []\n    f1_scores_bagging = []\n    \n    # I will also store all the historic variables of the models to obtain a ROC curve for each fold and method\n    roc_curve_adaboost = []\n    roc_curve_gradientboost = []\n    roc_curve_stacking = []\n    roc_curve_bagging = []\n    \n    roc_curve_data = []\n    \n    for train_idx, test_idx in cv.split(X,Y):\n        #Split into train_test datasets\n        x_train, y_train = X.iloc[train_idx], Y.iloc[train_idx]\n        x_test, y_test = X.iloc[test_idx], Y.iloc[test_idx]\n        \n        model_adaboost = AdaBoostClassifier(n_estimators = 10)\n        model_gradientBoost = GradientBoostingClassifier(n_estimators = 10, learning_rate=0.8)\n        stacking_classifiers = [('lr', LogisticRegression(C=100, max_iter=10000)), ('knn', KNeighborsClassifier(n_neighbors=5)), ('tree', DecisionTreeClassifier())]\n        model_stacking = StackingClassifier(estimators= stacking_classifiers, final_estimator = LogisticRegression(C=100, max_iter=10000), cv=10)\n        model_bagging = BaggingClassifier(DecisionTreeClassifier(max_depth=10),n_estimators=10, max_samples=0.5, max_features=0.5)\n        #model_SVMK = SVC(gamma=\"auto\", C=10000)\n        \n        #Train step\n        model_adaboost.fit(x_train, y_train[ylabel].ravel())\n        model_gradientBoost.fit(x_train, y_train[ylabel].ravel())\n        model_stacking.fit(x_train, y_train[ylabel].ravel())\n        model_bagging.fit(x_train, y_train[ylabel].ravel())\n        \n        ## VALIDATION AND METRIC OBTAINMENT METHODS ##\n        mean_score_adaboost = mean_score_adaboost + model_adaboost.score(x_test,y_test[ylabel].ravel())\n        mean_score_gradientboost = mean_score_gradientboost + model_gradientBoost.score(x_test,y_test[ylabel].ravel())\n        mean_score_stacking = mean_score_stacking + model_stacking.score(x_test,y_test[ylabel].ravel())\n        mean_score_bagging = mean_score_bagging + model_bagging.score(x_test,y_test[ylabel].ravel())\n        \n        score_roc_adaboost = model_adaboost.decision_function(x_test)\n        score_roc_gradientboost = model_gradientBoost.decision_function(x_test)\n        score_roc_stacking = model_stacking.decision_function(x_test)\n        score_roc_bagging = model_bagging.predict_proba(x_test)\n        \n        fprada = dict()\n        tprada = dict()\n        roc_aucada = dict()\n        \n        fprgb = dict()\n        tprgb = dict()\n        roc_aucgb = dict()\n        \n        fprsk = dict()\n        tprsk = dict()\n        roc_aucsk = dict()\n        \n        fprbg = dict()\n        tprbg = dict()\n        roc_aucbg = dict()\n        \n        for i in range(numclasses):\n            fprada[i], tprada[i], _ = roc_curve(label_binarize(y_test,classes=[i for i in range(numclasses)])[:, i], score_roc_adaboost[:,i])\n            roc_aucada[i] = auc(fprada[i], tprada[i])\n            \n            fprgb[i], tprgb[i], _ = roc_curve(label_binarize(y_test,classes=[i for i in range(numclasses)])[:, i], score_roc_gradientboost[:,i])\n            roc_aucgb[i] = auc(fprgb[i], tprgb[i])\n            \n            fprsk[i], tprsk[i], _ = roc_curve(label_binarize(y_test,classes=[i for i in range(numclasses)])[:, i], score_roc_stacking[:,i])\n            roc_aucsk[i] = auc(fprsk[i], tprsk[i])\n            \n            fprbg[i], tprbg[i], _ = roc_curve(label_binarize(y_test,classes=[i for i in range(numclasses)])[:, i], score_roc_bagging[:,i])\n            roc_aucbg[i] = auc(fprbg[i], tprbg[i])\n            \n        \n        roc_curve_data.append([[fprada, tprada, roc_aucada], [fprgb, tprgb, roc_aucgb], [fprsk, tprsk, roc_aucsk], [fprbg, tprbg, roc_aucbg]]) \n    \n        print(\"Iteration \", iteration)\n        iteration+=1\n        print(\"Accuracy Adaboost: \", model_adaboost.score(x_test,y_test[ylabel].ravel()))\n        print(\"Accuracy GradientBoost: \", model_gradientBoost.score(x_test,y_test[ylabel].ravel()))\n        print(\"Accuracy Stacking: \", model_stacking.score(x_test,y_test[ylabel].ravel()))\n        print(\"Accuracy Bagging: \", model_bagging.score(x_test,y_test[ylabel].ravel()))\n\n        y_predicted_ada = model_adaboost.predict(x_test)\n        y_predicted_gb = model_gradientBoost.predict(x_test)\n        y_predicted_sk = model_stacking.predict(x_test)\n        y_predicted_bagging = model_bagging.predict(x_test)\n    \n        precision_ada = f1_score(y_test.Borough.ravel(), y_predicted_ada, average=None)\n        precision_gb = f1_score(y_test.Borough.ravel(), y_predicted_gb, average=None)\n        precision_sk = f1_score(y_test.Borough.ravel(), y_predicted_sk, average=None)\n        precision_bagging = f1_score(y_test.Borough.ravel(), y_predicted_bagging, average=None)\n        \n        f1_scores_adaboost.append(precision_ada)\n        f1_scores_gradientboost.append(precision_gb)\n        f1_scores_stacking.append(precision_sk)\n        f1_scores_bagging.append(precision_bagging)\n\n        print(\"F1-Score AdaBoost: \", precision_ada)\n        print(\"F1-Score GradientBoost: \", precision_gb)\n        print(\"F1-Score Stacking: \", precision_sk)\n        print(\"F1-Score Bagging: \", precision_bagging)\n    \n\n        print(\"-----------------\")\n\n    result_lr = np.round(mean_score_adaboost/10,3)\n    result_nb = np.round(mean_score_gradientboost/10,3)\n    result_svm = np.round(mean_score_stacking/10,3)\n    result_bg = np.round(mean_score_bagging/10,3)\n    print(\"Accuracy in Cross validation AdaBoost: \", result_nb) \n    print(\"Accuracy in Cross validation GradientBoost: \", result_lr)    \n    print(\"Accuracy in Cross validation Stacking: \", result_svm)   \n    print(\"Accuracy in Cross validation Bagging: \", result_bg)\n    \n    #We return the ROC data\n    return roc_curve_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the classification task, we saw that the key for obtaining the best predictions was by the precinct number, as it was a polignomically sepparable variable which allowed us to obtain nearly perfect predictions. However, it may seem pretty ovbious that this classifiers will have issues making this predictions if we removed this variable. This is what we are going to experiment here, we are going to see if classification methods ensembles are capable to deal with this issue.","metadata":{}},{"cell_type":"code","source":"dataroc_2 = trainClassification_EnsemblesCV(X_Scaled[[\"Month\", \"Offense\", \"Law_Code\",\"Age\", \"Sex\", \"Race\"]],Y_Encoded, 5, \"Borough\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_roc_curves_CV(dataroc_2,[0], le_BOROUGH, ['AdaBoost', 'Gradient Boost', 'Stacking', 'Bagging'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see from the accuracy, models are not very accurate when predicting in which borough a crime has been committed. However, from the ROC curves, we observe that, despite of being a little bit low, we could somehow tackle the problem from a one vs many perspective, as we observe that, when we observe each case, the model is able to differentiate one variable from the rest. We can try this approach by implementing the same method with a binary classification task (where we do not try to tackle all variables but we focus on one label only, getting then four models for each label and ensembling them in a pipeline).","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import LabelBinarizer\nfrom sklearn.metrics import plot_roc_curve","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def trainClassification_OVM_EnsemblesCV(X,Y,numclasses, ylabel):\n    #We define the KFOLD separator\n    cv = StratifiedKFold(n_splits=10)\n    \n    #Some utility variables that will be useful during our train/test process\n    mean_score_adaboost = 0\n    mean_score_gradientboost = 0\n    mean_score_stacking = 0\n    mean_score_bagging = 0\n    iteration = 0\n    \n    #I would like to know the F1 scores of the models, as precission and recall are important when predicting\n    #the targetted label we want to describe\n    f1_scores_adaboost = []\n    f1_scores_gradientboost = []\n    f1_scores_stacking = []\n    f1_scores_bagging = []\n    \n    # I will also store all the historic variables of the models to obtain a ROC curve for each fold and method\n    roc_curve_adaboost = []\n    roc_curve_gradientboost = []\n    roc_curve_stacking = []\n    roc_curve_bagging = []\n    \n    roc_curve_data = []\n    \n    for train_idx, test_idx in cv.split(X,Y):\n        #Split into train_test datasets\n        x_train, y_train = X.iloc[train_idx], Y.iloc[train_idx]\n        x_test, y_test = X.iloc[test_idx], Y.iloc[test_idx]\n        \n        model_adaboost = AdaBoostClassifier(n_estimators = 10)\n        model_gradientBoost = GradientBoostingClassifier(n_estimators = 10, learning_rate=0.8)\n        stacking_classifiers = [('lr', LogisticRegression(C=100, max_iter=10000)), ('knn', KNeighborsClassifier(n_neighbors=5)), ('tree', DecisionTreeClassifier())]\n        model_stacking = StackingClassifier(estimators= stacking_classifiers, final_estimator = LogisticRegression(C=100, max_iter=10000), cv=10)\n        model_bagging = BaggingClassifier(DecisionTreeClassifier(max_depth=10),n_estimators=10, max_samples=0.5, max_features=0.5)\n        #model_SVMK = SVC(gamma=\"auto\", C=10000)\n        \n        #Train step\n        model_adaboost.fit(x_train, y_train[ylabel].ravel())\n        model_gradientBoost.fit(x_train, y_train[ylabel].ravel())\n        model_stacking.fit(x_train, y_train[ylabel].ravel())\n        model_bagging.fit(x_train, y_train[ylabel].ravel())\n        \n        ## VALIDATION AND METRIC OBTAINMENT METHODS ##\n        mean_score_adaboost = mean_score_adaboost + model_adaboost.score(x_test,y_test[ylabel].ravel())\n        mean_score_gradientboost = mean_score_gradientboost + model_gradientBoost.score(x_test,y_test[ylabel].ravel())\n        mean_score_stacking = mean_score_stacking + model_stacking.score(x_test,y_test[ylabel].ravel())\n        mean_score_bagging = mean_score_bagging + model_bagging.score(x_test,y_test[ylabel].ravel())\n        \n        score_roc_adaboost = model_adaboost.decision_function(x_test)\n        score_roc_gradientboost = model_gradientBoost.decision_function(x_test)\n        score_roc_stacking = model_stacking.decision_function(x_test)\n        score_roc_bagging = model_bagging.predict_proba(x_test)\n        \n        #roc_curve_data.append([[fprada, tprada, roc_aucada], [fprgb, tprgb, roc_aucgb], [fprsk, tprsk, roc_aucsk], [fprbg, tprbg, roc_aucbg]]) \n        \n\n\n    result_lr = np.round(mean_score_adaboost/10,3)\n    result_nb = np.round(mean_score_gradientboost/10,3)\n    result_svm = np.round(mean_score_stacking/10,3)\n    result_bg = np.round(mean_score_bagging/10,3)\n    print(\"Accuracy in Cross validation AdaBoost: \", result_nb) \n    print(\"Accuracy in Cross validation GradientBoost: \", result_lr)    \n    print(\"Accuracy in Cross validation Stacking: \", result_svm)   \n    print(\"Accuracy in Cross validation Bagging: \", result_bg)\n    \n    #roc_curve_data = [roc_curve_adaboost, roc_curve_gradientboost,roc_curve_stacking,roc_curve_bagging]\n    #We return the ROC data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"label_bin = LabelBinarizer()\nlabel_binarized = label_bin.fit_transform(Y_Encoded)\nY_Borough0 = pd.DataFrame([el[0] for el in label_binarized])\nY_Borough0.columns = [\"Borough\"]\n\nY_Borough1 = pd.DataFrame([el[1] for el in label_binarized])\nY_Borough1.columns = [\"Borough\"]\n\nY_Borough2 = pd.DataFrame([el[2] for el in label_binarized])\nY_Borough2.columns = [\"Borough\"]\n\nY_Borough3 = pd.DataFrame([el[3] for el in label_binarized])\nY_Borough3.columns = [\"Borough\"]\n\nY_Borough4 = pd.DataFrame([el[4] for el in label_binarized])\nY_Borough4.columns = [\"Borough\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"BOROUGH 0 RESULTS\")\ntrainClassification_OVM_EnsemblesCV(X_Scaled[[\"Month\", \"Offense\", \"Law_Code\",\"Age\", \"Sex\", \"Race\"]],Y_Borough0, 1, \"Borough\")\nprint(f\"#################\")\nprint(f\"BOROUGH 1 RESULTS\")\ntrainClassification_OVM_EnsemblesCV(X_Scaled[[\"Month\", \"Offense\", \"Law_Code\",\"Age\", \"Sex\", \"Race\"]],Y_Borough1, 1, \"Borough\")\nprint(f\"#################\")\nprint(f\"BOROUGH 2 RESULTS\")\ntrainClassification_OVM_EnsemblesCV(X_Scaled[[\"Month\", \"Offense\", \"Law_Code\",\"Age\", \"Sex\", \"Race\"]],Y_Borough2, 1, \"Borough\")\nprint(f\"#################\")\nprint(f\"BOROUGH 3 RESULTS\")\ntrainClassification_OVM_EnsemblesCV(X_Scaled[[\"Month\", \"Offense\", \"Law_Code\",\"Age\", \"Sex\", \"Race\"]],Y_Borough3, 1, \"Borough\")\nprint(f\"#################\")\nprint(f\"BOROUGH 4 RESULTS\")\ntrainClassification_OVM_EnsemblesCV(X_Scaled[[\"Month\", \"Offense\", \"Law_Code\",\"Age\", \"Sex\", \"Race\"]],Y_Borough4, 1, \"Borough\")\nprint(f\"#################\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see, we have a high accuracy metric when performing a classification in a OVM perspective when we lack with a linearly sepparable variable variable between all classes. Of course, this involves more complexity on the creation of our pipeline, which could be implemented as an ensemble of both four best predictors and handle their resuls as a voting system, the highest probability of a class will be the predicted one by the system.","metadata":{}},{"cell_type":"markdown","source":"Now, let's jump into the regression problem, as it is our point of interest in this work, as we had trouble predicting the crime latitude","metadata":{}},{"cell_type":"code","source":"# Function to train the regression problem with 10 - Fold CV\n# The procedure is similar to the above function, even there are not that many parameters to obtain during\n# the training process (we only want the R2 score)\ndef trainRegression_EnsemblesCV(X,Y, ylabel):\n    cv = KFold(n_splits=10) \n    r2_st = 0\n    r2_ada = 0\n    r2_gb = 0\n    r2_bg = 0\n    r2_rfo = 0\n    iteration = 0\n    for train_idx, test_idx in cv.split(X,Y):\n        x_train, y_train = X.iloc[train_idx], Y.iloc[train_idx]\n        x_test, y_test = X.iloc[test_idx], Y.iloc[test_idx]\n        \n        stacking_regressors = [('lr', LinearRegression()), ('svr', SVR(max_iter=1000, C=50, kernel=\"poly\", coef0=2)), ('ridge', Ridge(alpha=0.1), ('lasso', Lasso(alpha=0.1)))]\n        model_ada = AdaBoostRegressor(n_estimators=100)\n        model_gb = GradientBoostingRegressor()\n        model_RandomForest = RandomForestRegressor(criterion=\"mse\", bootstrap=True)\n        model_stacking = StackingRegressor(estimators=stacking_regressors, final_estimator=RandomForestRegressor(criterion=\"mse\", n_estimators=10))\n        model_bagging = BaggingRegressor(base_estimator=DecisionTreeRegressor(), n_estimators=10)\n    \n        model_ada.fit(x_train, y_train[ylabel].ravel())\n        model_gb.fit(x_train, y_train[ylabel].ravel())\n        model_RandomForest.fit(x_train, y_train[ylabel].ravel())\n        model_stacking.fit(x_train, y_train[ylabel].ravel())\n        model_bagging.fit(x_train, y_train[ylabel].ravel())\n    \n        print(\"Iteration \", iteration)\n        iteration+=1\n    \n        r2_ada = r2_ada + model_ada.score(x_test, y_test[ylabel].ravel())\n        r2_gb = r2_gb + model_gb.score(x_test, y_test[ylabel].ravel())\n        r2_rfo = r2_rfo + model_RandomForest.score(x_test, y_test[ylabel].ravel())\n        r2_bg = r2_bg + model_bagging.score(x_test, y_test[ylabel].ravel())\n        r2_st = r2_st + model_stacking.score(x_test, y_test[ylabel].ravel())\n    \n        print(f\"R2 Ada Boost {model_ada.score(x_test, y_test[ylabel].ravel())}\")\n        print(f\"R2 Gradient Boost {model_gb.score(x_test, y_test[ylabel].ravel())}\")\n        print(f\"R2 Random Forest {model_RandomForest.score(x_test, y_test[ylabel].ravel())}\")\n        print(f\"R2 Stacking {model_stacking.score(x_test, y_test[ylabel].ravel())}\")\n        print(f\"R2 Bagging {model_bagging.score(x_test, y_test[ylabel].ravel())}\")\n    \n        print(\"-----------------\")\n\n    result_ada = np.round(r2_ada/10,3)\n    result_gb = np.round(r2_gb/10,3)\n    result_rfo = np.round(r2_rfo/10,3)\n    result_st = np.round(r2_st/10,3)\n    result_bg = np.round(r2_bg/10,3)\n    print(\"Mean R2 Ada Boost: \", result_ada) \n    print(\"Mean R2 Gradient Boost: \", result_gb)\n    print(\"Mean R2 RandomForest: \", result_rfo) \n    print(\"Mean R2 Stacking: \", result_st)\n    print(\"Mean R2 Bagging: \", result_bg)    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainRegression_EnsemblesCV(X_scaled[[\"Offense\", \"Precint\", \"Month\", \"Race\"]], Y_Lat, \"Latitude\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainRegression_EnsemblesCV(X_scaled[[\"Offense\", \"Precint\", \"Month\", \"Race\"]], Y_Long, \"Longitude\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can observe, we cannot improve the accuracy of the model when predicting the location of the occured crime, as the models still struggle with the problem related before (no presence of correlation between the predicted variable and the regressors). This brings up a clear idea for this work: more power does not have to bring better results, as these methods are completely data-driven and depend on premises that have to be accomplished if we desire to obtain good predictions from our models, which can have better results depending on their structure or the data properties.","metadata":{}},{"cell_type":"markdown","source":"## Clustering","metadata":{}},{"cell_type":"markdown","source":"As I have mentioned in my introduction, during my previous work I explored the criminality data on the used dataset during this project. In this section, we are going to apply clustering to try to unveil interesting information about how crimes and occurrencies can be grouped throughout the different boroughs that compose the dataset. Specifically, we are going to focus on how these boroughs are distributed in the drug-dealing crimes, as they are the most frequent ones in our dataset and it involved the majority of underage and young groups. \n\nThe clustering operation will try to generate groups (which will represent ideally the boroughs) from the number of events and the population of each borough per year. So to speak, we are going to divide the data by neighborhoods in each borough and use the yearly population as a second variable (as we need to somehow contextualize that amount of incidencies). We will observe if the clusters have any tendency to group boroughs or (if the method reaches an ideal state) they are able to isolate the boroughs in each cluster.","metadata":{}},{"cell_type":"markdown","source":"We first need to prepare the dataset based on the neighborhoods and filter it to only get the drug-related crimes.","metadata":{}},{"cell_type":"code","source":"neighborhoods = pd.read_csv('../input/nyc-neighborhoods-dataset/nynta.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_crim = dataframe[[\"ARREST_DATE\",\"OFNS_DESC\",\"ARREST_BORO\",\"Neighborhood\"]].dropna()\ndrugs_dataframe = data_crim[(data_crim.OFNS_DESC == 'DANGEROUS DRUGS') | (data_crim.OFNS_DESC == 'LOITERING FOR DRUG PURPOSES')]\ndrugs_dataframe.reset_index(inplace=True)\ndrugs_dataframe.OFNS_DESC = [\"Drugs\" for _ in range(len(drugs_dataframe.OFNS_DESC))]\ndrugs_dataframe[\"Inc\"] = [1 for _ in range(len(drugs_dataframe.OFNS_DESC))]\ndrugs_dataframe","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Then, we need to isolate the year in the date-related field, as we are going to cluster from neighborhood and year (as population changes from one another).","metadata":{}},{"cell_type":"code","source":"drugs_dataframe.ARREST_DATE = pd.to_datetime(drugs_dataframe.ARREST_DATE).dt.year\ndrugs_dataframe.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We remove the index column generated in the previous operations.","metadata":{}},{"cell_type":"code","source":"drugs_dataframe = drugs_dataframe[[\"ARREST_DATE\", \"OFNS_DESC\", \"ARREST_BORO\", \"Neighborhood\", \"Inc\"]]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, we have to prepare the population dataframe, as we need this reference to contextualize the incidencies between neighborhoods.","metadata":{}},{"cell_type":"code","source":"population = pd.read_csv('../input/new-york-city-population/new-york-city-population-by-borough-1950-2040.csv')\nborough_interest = population[[\"Borough\",\"2000\", \"2010\", \"2020\"]]\nborough_interest\npopulation_dataframe = pd.DataFrame(columns=[\"Year\",\"Total\", \"Bronx\", \"Brooklyn\", \"Manhattan\", \"Queens\", \"Staten Island\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(2000, 2021):\n    if i == 2000 or i == 2010 or i == 2020:\n        population_dataframe = population_dataframe.append({\"Year\": str(i),\"Total\": borough_interest[borough_interest[\"Borough\"] == \"NYC Total\"][str(i)][0],\n                              \"Bronx\": borough_interest[borough_interest[\"Borough\"] == \"   Bronx\"][str(i)][1],\n                              \"Brooklyn\": borough_interest[borough_interest[\"Borough\"] == \"   Brooklyn\"][str(i)][2],\n                              \"Manhattan\": borough_interest[borough_interest[\"Borough\"] == \"   Manhattan\"][str(i)][3],\n                              \"Queens\": borough_interest[borough_interest[\"Borough\"] == \"   Queens\"][str(i)][4],\n                              \"Staten Island\": borough_interest[borough_interest[\"Borough\"] == \"   Staten Island\"][str(i)][5]}, ignore_index=True)\n    else:\n        population_dataframe = population_dataframe.append({\"Year\": str(i),\"Total\": np.nan,\n                              \"Bronx\": np.nan,\n                              \"Brooklyn\": np.nan,\n                              \"Manhattan\": np.nan,\n                              \"Queens\": np.nan,\n                              \"Staten Island\": np.nan}, ignore_index=True)\npopulation_dataframe = population_dataframe.set_index(\"Year\")\npopulation_dataframe = population_dataframe.apply(pd.to_numeric)\npopulation_dataframe = population_dataframe.interpolate(method=\"linear\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"populations = []\ncorrespondence = {\"B\": \"Bronx\", \"K\": \"Brooklyn\", \"M\": \"Manhattan\", \"Q\":\"Queens\", \"S\":\"Staten Island\"}\nfor row in drugs_dataframe[['ARREST_DATE','ARREST_BORO']].itertuples(index=False):\n    #print(str(row.ARREST_DATE))\n    populations.append(population_dataframe.loc[str(row.ARREST_DATE)][correspondence[row.ARREST_BORO]])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Once obtained our dataframe with populations, we append that column into our drugs incidents dataframe to perform the groupby operation","metadata":{}},{"cell_type":"code","source":"drugs_dataframe[\"Population\"] = populations\ndrugs_dataframe","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We then proceed to do the Neighborhood group by operation","metadata":{}},{"cell_type":"code","source":"grouped_incidents = drugs_dataframe.groupby('Neighborhood').sum()[[\"Inc\", \"Population\"]]\ngrouped_incidents[\"Population\"] = grouped_incidents[\"Population\"] / grouped_incidents[\"Inc\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We plot the data to see if visually evaluate the difficulty of clustering it between different groups.","metadata":{}},{"cell_type":"code","source":"plt.scatter(grouped_incidents['Population'], grouped_incidents['Inc'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can observe, this data can be seppareted (at least) in three unique groups (by proximity). Even though, we are going to analitically determine in how many groups can be clustered these data and then observe if those groups correspond to each borough that represent this data. So to speak, we are going to determine if each borough is actually sepparable and differentiable from the others on drug events for each year.","metadata":{}},{"cell_type":"markdown","source":"I first scale the data to esase the complexity to the clustering method.","metadata":{}},{"cell_type":"code","source":"scaled_drugs = pd.DataFrame(StandardScaler().fit_transform(grouped_incidents))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.cluster import DBSCAN, KMeans\nimport seaborn as sns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We then define a function that applies the elbow rule to determine the best number of clusters in the dataframe with the KMeans method, which is one of the methods we are going to use to perform clustering.","metadata":{}},{"cell_type":"code","source":"def elbow_rule(dataframe):\n  plt.figure(figsize=(10, 8))\n  wcss = []\n  for i in range(1, 15):\n    kmeans = KMeans(n_clusters = i, init = 'k-means++')\n    kmeans.fit(dataframe)\n    wcss.append(kmeans.inertia_) #criterion based on which K-means clustering works\n  plt.plot(range(1, 15), wcss)\n  plt.title('The Elbow Method')\n  plt.xlabel('Number of clusters')\n  plt.ylabel('WCSS')\n  plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"elbow_rule(scaled_drugs)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Once executed the elbow rule, we observe that the curve is very soft, so it is hard to tell which is the best number of clusters in this dataset. However, in this case I will pick up four, as it somehow starts to differentiate the end of the curve with the WCSS drop, so we can say it is the elbow point of this graphic. ","metadata":{}},{"cell_type":"code","source":"kmeans = KMeans(n_clusters = 4, init = 'k-means++').fit(scaled_drugs)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I then perform another method to compare. In this case, I use the DBSCAN method, as it is based on the population density in the clusters and it determines automatically the best number of clusters to use. It also is able to isolate potential outliers that difficult the clustering method. I also execute this clustering technique to have a reference between both methods.","metadata":{}},{"cell_type":"code","source":"dbscan = DBSCAN(eps=0.5).fit(scaled_drugs)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We then plot the obtained clusters.","metadata":{}},{"cell_type":"code","source":"grouped_incidents['Cluster_dbscan'] = dbscan.labels_\ngrouped_incidents['Cluster_kmeans'] = kmeans.labels_\n#plt.scatter(grouped_incidents['Population'], grouped_incidents['Inc'], labels=)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"First, we plot the KMeans clusters.","metadata":{}},{"cell_type":"code","source":"fg = sns.FacetGrid(data=grouped_incidents, hue='Cluster_kmeans')\nfg.map(plt.scatter, 'Population', 'Inc').add_legend()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can observe, the KMeans clustering method produces four clusters that somehow can be logically splitted by distance in the dataset (except the third one, which picks up superior points in the graphic). ","metadata":{}},{"cell_type":"markdown","source":"And then the DBSCAN produced ones.","metadata":{}},{"cell_type":"code","source":"fg = sns.FacetGrid(data=grouped_incidents, hue='Cluster_dbscan')\nfg.map(plt.scatter, 'Population', 'Inc').add_legend()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In this case, five distinctive groups have been determined (and another one composed by potential outliers of the dataset). This is interesting, as it can be seen that the DBSCAN method could have sepparated the data into boroughs (as we are dealing with five different classes).","metadata":{}},{"cell_type":"markdown","source":"However, we need to determine the frequency of boroughs of each cluster to determine wether the methods were able to sepparate the data ideally or not.","metadata":{}},{"cell_type":"code","source":"borough_list = []\nfor row in grouped_incidents.reset_index().itertuples(index=False):\n    borough_list.append(neighborhoods[neighborhoods.NTAName == row.Neighborhood][\"BoroName\"].values[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grouped_incidents[\"Borough\"] = borough_list\ngrouped_incidents[\"Count\"] = [1 for _ in borough_list]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We plot the obtained clusters.","metadata":{}},{"cell_type":"code","source":"def plot_clusters(n_cluster, method):\n    clusteringmth = f\"Cluster_{method}\"\n    cluster = grouped_incidents[grouped_incidents[clusteringmth] == n_cluster]\n    grouped = cluster.groupby(\"Borough\").sum()[\"Count\"]\n    grouped = grouped.reset_index()\n    fig = plt.figure()\n    sns.barplot(x=grouped[\"Borough\"], y=grouped[\"Count\"])\n\nfor i in range(4):\n    plot_clusters(i, \"kmeans\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can observe, the KMeans clustering method is able to distinguish the Staten Island borough from the others, as it is the one with less criminality rates from all the presented data in our dataframe. However, it is not able to make a clear sepparation between the rest of the boroughs, as we observe that they are mixed-up in different clusters with no apparent sense (we do not have any economic or demographic dataset that could unveil these decissions).","metadata":{}},{"cell_type":"markdown","source":"We observe the results performed by the DBSCAN method.","metadata":{}},{"cell_type":"code","source":"for i in range(5):\n    plot_clusters(i, \"dbscan\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We observe again that the DBSCAN method is able to isolate Staten Island from the rest of the boroughs, doe to the same reason explained before. However, we see that the distribution of the rest of the boroughs are quite different from the method above. In this case, we can observe that two clusters groups the boroughs with the most criminality incidencies ratio and crime per capita in the dataset (as it is observed in my previous work): The Bronx and Manhattam, one being the most crowded one and the other one the most humble and tendent to have criminals for survival. The rest of the clusters get data from similar crime per capita boroughs, although it is able to partially isolate Brooklyn. Somehow, this is interesting, as the clustering method is able to differentiate the most conflictive boroughs from the rest, even of not being able to draw clear lines in the sepparation of them. Again, to evaluate the representativity of the presented clusters, we would need aditional data (such as economic demography) that we could not bring to the project doe to processing unfeasability (as Kaggle has its limitations) or lack of public resources to get the desired data.","metadata":{}}]}