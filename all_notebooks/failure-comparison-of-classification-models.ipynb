{"cells":[{"metadata":{},"cell_type":"markdown","source":"## â™¥ Failure : Comparison of 6 classification models"},{"metadata":{},"cell_type":"markdown","source":"![](https://patients.healthquest.org/wp-content/uploads/2018/05/congestive-heart-failure-feature2.jpg)"},{"metadata":{},"cell_type":"markdown","source":"## 1. Import Libraries and Dataset"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Data Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\n \n# Model Selection and utilities\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\nfrom sklearn.metrics import confusion_matrix, classification_report, plot_confusion_matrix\n\n# Model Libraries\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#READING DATASET\ndf = pd.read_csv(\"../input/heart-failure-clinical-data/heart_failure_clinical_records_dataset.csv\")\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In this dataset:\n* It contains 299 rows (patient information).\n* It contains 13 columns (12 features and DEATH_EVENT target variable).\n* 10 features are integer type.\n* 3 features are float type.\n\n***FEATURES***\n\n**age**: age of patient\n\n**anaemia**: Decrease of red blood cells or hemoglobin\n\n**creatinine_phosphokinase**: Level of the CPK enzyme in the blood (mcg/L)\n\n**diabetes**: If the patient has diabetes\n\n**ejection_fraction**: Percentage of blood leaving the heart at each contraction (percentage)\n\n**high_blood_pressure**: If the patient has hypertension platelets: Platelets in the blood\n\n**serum_creatinine**: Level of serum creatinine in the blood (mg/dL)\n\n**serum_sodium**: Level of serum sodium in the blood (mEq/L)\n\n**sex**: Woman or man (binary)\n\n**smoking**: If the patient smokes or not\n\n**time**: Follow-up period (days)\n\n**DEATH_EVENT**: If the patient deceased during the follow-up period"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Data analysis and Visualization"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Storing categorical and numerical features names in different Series\ncat_columns = [\"anaemia\",\"diabetes\",\"high_blood_pressure\",\"sex\",\"smoking\",\"DEATH_EVENT\"]\nnum_columns = pd.Series(df.columns)\nnum_columns = num_columns[~num_columns.isin(cat_columns)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Frequency distribution of Categorical Variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axs = plt.subplots(nrows=2, ncols=3, figsize=(10, 6))\ntitles = list(df[cat_columns])\n\nax_title_pairs = zip(axs.flat, titles)\n\nfor ax, title in ax_title_pairs:\n    sns.countplot(x=title, data=df, palette='muted', ax=ax)\n    ax.set_title(title)\n    ax.set_xlabel('')\n\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Frequency distribution of Continuous Variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_grouped = df.groupby(by='DEATH_EVENT')\nfig, axs = plt.subplots(nrows=3, ncols=3, figsize=(15, 8))\ntitles = list(df[num_columns])\n\nax_title_pairs = zip(axs.flat, titles)\n\nfor ax, title in ax_title_pairs:\n  sns.distplot(df_grouped.get_group(0)[title], bins=10, ax=ax, label='No')\n  sns.distplot(df_grouped.get_group(1)[title], bins=10, ax=ax, label='Yes')\n  ax.legend(title='DEATH_EVENT')\n\naxs.flat[-1].remove()\naxs.flat[-2].remove()\nfig.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. Modelling on raw dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_raw = df.iloc[:,:-1].to_numpy()\ny_raw = df['DEATH_EVENT'].to_numpy()\n\nX_train_r, X_test_r, y_train_r, y_test_r = train_test_split(X_raw, y_raw, test_size = 0.2, random_state =1)\n\nresult_dict = {}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Utility Functions"},{"metadata":{"trusted":true},"cell_type":"code","source":"def fit_model_with_grid_search(model, X_t, y_t, parameters, scoring='f1', verbose=1):\n  model = GridSearchCV(\n      model,\n      parameters,\n      scoring=scoring\n  )\n  \n  model.fit(X_t, y_t)\n  \n  if verbose:\n    print(f'\\nbest_params_: {model.best_params_}')\n    print(f'Mean cross-validated F1 score of the best_estimator: {model.best_score_:.4f}')\n      \n  return model\n\ndef print_metrics(cf, X_t, y_t):\n  y_pred = classifier.predict(X_t)\n\n  accuracy = accuracy_score(y_t, y_pred)\n  f1 = f1_score(y_t, y_pred, average='macro')\n  precision = precision_score(y_t, y_pred, average='macro')\n  recall = recall_score(y_t, y_pred, average='macro')\n  \n  print(f'\\nAccuracy (test set)\\t| {accuracy:.4f}')\n  print(f'F1 (test set)\\t\\t| {f1:.4f}')\n  print(f'Precision (test set)\\t| {precision:.4f}')\n  print(f'Recall (test set)\\t| {recall:.4f}\\n')\n  # print()\n  cm = confusion_matrix(y_t, y_pred)\n  plt.figure(figsize=(5,3))\n  sns.heatmap(cm,annot=True, linewidths=.5)\n  plt.show()\n\n  return {\n    'accuracy': accuracy,\n    'f1': f1,\n    'precision': precision,\n    'recall': recall,\n  }","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1. Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"*Logistic Regression*\")\n\nmodel_logistic_regression = LogisticRegression()\nparameters = {\n    'C': [0.01, 0.1, 1],\n}\nclassifier = fit_model_with_grid_search(\n    model_logistic_regression,\n    X_train_r,\n    y_train_r,\n    parameters,\n    scoring='f1',\n)\n\nresult_dict['Logistic Regression'] = print_metrics(classifier, X_test_r, y_test_r)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2. KNN"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"*K Nearest Neighours*\")\n\nmodel_knn = KNeighborsClassifier()\nparameters = {\n    \"n_neighbors\": list(range(2, 21)),\n    \"weights\": ['uniform', 'distance'],\n}\nclassifier = fit_model_with_grid_search(\n    model_knn,\n    X_train_r,\n    y_train_r,\n    parameters,\n    scoring='accuracy',\n)\n\nresult_dict['KNN'] = print_metrics(classifier, X_test_r, y_test_r)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list1 = []\nfor neighbors in range(2,21):\n  classifier = KNeighborsClassifier(n_neighbors=neighbors)\n  classifier.fit(X_train_r, y_train_r)\n  y_pred = classifier.predict(X_test_r)\n  list1.append(accuracy_score(y_test_r, y_pred))\nplt.plot(list(range(2,21)), list1)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3. SVM"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"*Support Vector Machine*\")\n\nmodel_svm = SVC()\nparameters = {\n    \"C\": [0.001, 0.01, 0.1, 1],\n}\nclassifier = fit_model_with_grid_search(\n    model_svm,\n    X_train_r,\n    y_train_r,\n    parameters,\n    scoring='accuracy',\n)\n\nresult_dict['SVM'] = print_metrics(classifier, X_test_r, y_test_r)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4. Decision Tree"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"*Decision Tree Classsifier*\")\n\nmodel_decision = DecisionTreeClassifier()\nparameters = {\n  \"max_depth\": [1, 2, 3, 5, 10, None], \n  \"max_leaf_nodes\": list(range(2, 15)),\n  \"criterion\": [\"entropy\"],\n}\nclassifier = fit_model_with_grid_search(\n    model_decision,\n    X_train_r,\n    y_train_r,\n    parameters,\n    scoring='f1',\n)\n\nresult_dict['Decision Tree'] = print_metrics(classifier, X_test_r, y_test_r)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5. Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"*Random Forest Classifier*\")\nmodel_rand_forest = RandomForestClassifier()\nparameters = {\n    \"n_estimators\": list(range(10,21)),\n}\nclassifier = fit_model_with_grid_search(\n    model_rand_forest,\n    X_train_r,\n    y_train_r,\n    parameters,\n    scoring='f1',\n)\n\nresult_dict['Random Forest'] = print_metrics(classifier, X_test_r, y_test_r)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 6. Naive Bayes"},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier = GaussianNB()\nclassifier.fit(X_train_r, y_train_r)\n\nprint(\"*Gaussian NaiveBayes*\")\nresult_dict['NaiveBayes'] = print_metrics(classifier, X_test_r, y_test_r)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4. Feature Selection"},{"metadata":{},"cell_type":"markdown","source":"### 1. Co-relation matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"ax, fig = plt.subplots(figsize=(12,12))\ncorr = df.corr()\nsns.heatmap(corr, vmin=-1, cmap='coolwarm', annot=True)\nplt.xticks(rotation=30, ha='right')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In above correlation matrix, we see features relationship each other. This relationships can be useful to set up model. If the relationship how is close and is strong, it can be impact to use them in order to set up true model. In this dataset, we will look relationship of DEATH_EVENT with other features. If relationship between them is big from 0.1, This features can be important features,which heart attack triggers. "},{"metadata":{"trusted":true},"cell_type":"code","source":"corr[abs(corr['DEATH_EVENT']) > 0.1]['DEATH_EVENT']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2. Extra Tree Classifier "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Feature Selection\n\nplt.rcParams['figure.figsize']=12,6 \nsns.set_style(\"darkgrid\")\n\nx1 = df.iloc[:, :-1]\ny1 = df.iloc[:,-1]\n\nfrom sklearn.ensemble import ExtraTreesClassifier\n\nmodel = ExtraTreesClassifier()\nmodel.fit(x1,y1)\n# print(model.feature_importances_) \nfeat_importances = pd.Series(model.feature_importances_, index=x1.columns)\nfeat_importances.nlargest(12).plot(kind='barh')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_histogram(dataset, feature, color, title, labels):\n  fig = px.histogram(\n    dataset, \n    x=feature,\n    color=color, \n    marginal=\"box\",\n    hover_data=dataset.columns,\n    title = title, \n    labels = labels,\n    width=800,\n    template=\"plotly_white\",\n  )\n  fig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_histogram(df, 'age', 'DEATH_EVENT', 'AGE Vs DEATH_EVENT', {\"age\": \"AGE\"})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_histogram(df, 'ejection_fraction', 'DEATH_EVENT', 'EJECTION FRACTION Vs DEATH_EVENT', {\"ejection_fraction\": \"EJECTION FRACTION\"})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_histogram(df, 'serum_sodium', 'DEATH_EVENT', 'SERUM SODIUM Vs DEATH_EVENT', {\"serum_sodium\": \"SERUM SODIUM\"})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_histogram(df, 'serum_creatinine', 'DEATH_EVENT', 'SERUM CREATININE Vs DEATH_EVENT', {\"serum_creatinine\": \"SERUM CREATININE\"})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5. Model Training and Prediction on selected features"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df[['ejection_fraction', 'serum_creatinine', 'serum_sodium', 'time', 'age']].to_numpy()\n\ny = df['DEATH_EVENT'].to_numpy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state =1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Data Scaling"},{"metadata":{"trusted":true},"cell_type":"code","source":"sc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1. K Nearest Neighours"},{"metadata":{"trusted":true},"cell_type":"code","source":"list1 = []\nfor neighbors in range(2,15):\n  classifier = KNeighborsClassifier(n_neighbors=neighbors)\n  classifier.fit(X_train, y_train)\n  y_pred = classifier.predict(X_test)\n  list1.append(accuracy_score(y_test,y_pred))\nplt.plot(list(range(2,15)), list1)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"*K Nearest Neighours (Transformed Data)*\")\n\nclassifier = KNeighborsClassifier(n_neighbors=11)\nclassifier.fit(X_train, y_train)\n\ny_pred = classifier.predict(X_test)\n\nresult_dict['KNN(Selected Features)'] = print_metrics(classifier, X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2. SVM"},{"metadata":{"trusted":true},"cell_type":"code","source":"list1 = []\nfor c in [0.001, 0.01, 0.05, 0.1, 0.2, 0.4, 0.5, 0.75, 0.8, 0.9, 1]:\n  classifier = SVC(C = c)\n  classifier.fit(X_train, y_train)\n  y_pred = classifier.predict(X_test)\n  list1.append(accuracy_score(y_test,y_pred))\nplt.plot([0.001, 0.01, 0.05, 0.1, 0.2, 0.4, 0.5, 0.75, 0.8, 0.9, 1], list1)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"*Support Vector machine (Transformed Data)*\")\n\nclassifier = SVC(C = 0.2)\nclassifier.fit(X_train, y_train)\n\ny_pred = classifier.predict(X_test)\n\nresult_dict['SVM (Selected Features)'] = print_metrics(classifier, X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 6. Model performance comparison"},{"metadata":{"trusted":true},"cell_type":"code","source":"Results = pd.DataFrame(result_dict).T\nResults","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}