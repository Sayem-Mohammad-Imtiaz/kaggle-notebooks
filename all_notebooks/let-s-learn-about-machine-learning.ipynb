{"cells":[{"metadata":{"_uuid":"0f5c19cb784cbe52a18b90f0f75400d25b3c5d1c","_cell_guid":"d58c3f5c-5d0b-4960-ad86-fee5a6634ec4"},"cell_type":"markdown","source":"# Level 1"},{"metadata":{"_uuid":"b6269c0e8f417f82daf093dda8fa0da6d2c57d86","_cell_guid":"e81ee64d-e474-4662-9036-ce23df615199"},"cell_type":"markdown","source":"# Introduction\n**This will be your workspace for Kaggle's Machine Learning education track.**\n\nYou will build and continually improve a model to predict housing prices as you work through each tutorial.  Fork this notebook and write your code in it.\n\nThe data from the tutorial, the Melbourne data, is not available in this workspace.  You will need to translate the concepts to work with the data in this notebook, the Iowa data.\n\nCome to the [Learn Discussion](https://www.kaggle.com/learn-forum) forum for any questions or comments. \n\n# Write Your Code Below\n\n"},{"metadata":{"collapsed":true,"_uuid":"1c728098629e1301643443b1341556a15c089b2b","_cell_guid":"86b26423-563a-4fa1-a595-89e25ff93089","trusted":false},"cell_type":"code","source":"import pandas as pd\n\nmain_file_path = '../input/house-prices-advanced-regression-techniques/train.csv'\ndata = pd.read_csv(main_file_path)\nprint('hello world')","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"9c34b9986f96c8df7267b60b5961b18f20f6a9b5","_cell_guid":"88d4a535-6747-4ed8-8514-a974b9b76c0e","trusted":false},"cell_type":"code","source":"data","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"7e7e7651be498d391c80fb827cec4f76a7e346ab","_cell_guid":"39892bab-d244-4bf4-b14f-ff862139c4f2","scrolled":true,"trusted":false},"cell_type":"code","source":"print(data.describe())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ad939e8dee3fe1d8b0516674c9498062a1a02664","_cell_guid":"fcce2599-2b4c-4664-868e-a30fcfa35b2f"},"cell_type":"markdown","source":"# Selecting and Filtering Data"},{"metadata":{"_uuid":"6123ac2f122ac715cc297e6b8e86259284cde580","_cell_guid":"d6e4ec11-43d3-472b-9477-27c0d1f26552"},"cell_type":"markdown","source":"## Selecting a Single Column"},{"metadata":{"collapsed":true,"_uuid":"c4616a4d3fdfb20a967af4f2ed4b56c94477e36d","_cell_guid":"3a4b160c-421c-4fe3-b866-76ef3f39b003","trusted":false},"cell_type":"code","source":"#Print a list of the columns\nprint (data.columns)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"e778a3077acb62964f944213f0271b46aa4ce91c","_cell_guid":"89dade5b-2169-418d-a97f-7ac2bfbe369c","trusted":false},"cell_type":"code","source":"# From the list of columns, find a name of the column with the sales prices of the homes. Use the dot notation to extract this to a variable (as you saw above to create melbourne_price_data.)\n#Use the head command to print out the top few lines of the variable you just created.\ndata_sale = data.SalePrice\nprint(data_sale.head())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ea9f7a18c2e34e30707ceed87dd63c59a008d72a","_cell_guid":"cc7ad6c1-1503-4930-897f-1b04edb612cc"},"cell_type":"markdown","source":"## Selecting Multiple Columns"},{"metadata":{"collapsed":true,"_uuid":"67ce75d422d3d13fa768d8f3a7e4824e1c5444ac","_cell_guid":"84996041-ef79-4321-aa8e-b27cd9cc17e5","trusted":false},"cell_type":"code","source":"#Pick any two variables and store them to a new DataFrame (as you saw above to create two_columns_of_data.)\n#Use the describe command with the DataFrame you just created to see summaries of those variables. \n\ncolumns_inter = ['SaleCondition','SaleType']\nprint(data[columns_inter].describe())\n# Categorical.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a91593a77b15622bda8c7a4043785b37039446a7","_cell_guid":"5d17f50d-222b-49df-b220-c0da4b09d9d1"},"cell_type":"markdown","source":"# My First Scikit-Learn Model"},{"metadata":{"_uuid":"b81f033f3de77cdea337d0b27f65f5da2acc0280","_cell_guid":"ccae9702-2047-4467-9b78-991df248f81b"},"cell_type":"markdown","source":"## Choosing the Prediction Target"},{"metadata":{"collapsed":true,"_uuid":"c877c0bae53e3a1fbee6f815e61e334b7d508b1e","_cell_guid":"9a981ea7-6921-4690-83f1-dc8cc8803a4b","trusted":false},"cell_type":"code","source":"# Select the target variable you want to predict. You can go back to the list of columns from your earlier commands to recall what it's called (hint: you've already worked with this variable). Save this to a new variable called y.\ny  = data.SalePrice","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9c9a5df71b59adcd54bb3a331e9ed9c4bd784046","_cell_guid":"909218fd-9a68-4762-95fe-046c7f004ddd"},"cell_type":"markdown","source":"## Choosing Predictors"},{"metadata":{"_uuid":"5d4c0374b7806537791dafa3bdeb702b18380e42","_cell_guid":"be72167e-ae5f-4130-ae42-24cc81c5f894"},"cell_type":"markdown","source":"It's possible to model with non-numeric variables, but we'll start with a narrower set of numeric variables."},{"metadata":{"collapsed":true,"_uuid":"674c3c858a24be59bf9809151c021092ae64001b","_cell_guid":"7ccf722a-52d7-4d23-b42a-ebde23506d74","trusted":false},"cell_type":"code","source":"#Using the list of variable names you just created, select a new DataFrame of the predictors data. Save this with the variable name X.\n# Create a list of the names of the predictors we will use in the initial model. Use just the following columns in the list (you can copy and paste the whole list to save some typing, though you'll still need to add quotes):\n\npredictors = ['LotArea','YearBuilt','1stFlrSF','2ndFlrSF','FullBath','BedroomAbvGr','TotRmsAbvGrd']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c87ab09831769e2ec777e831476258c3bc70cb9b","_cell_guid":"a3e41c2d-fb2d-4105-baec-b54f89ba8f18"},"cell_type":"markdown","source":"By convention, this data is called **X**"},{"metadata":{"collapsed":true,"_uuid":"8a4733149ca4985d7d7e9a86ac54aa793ef50780","_cell_guid":"1f31445c-4eb2-492a-ab5e-472b08fa66a9","trusted":false},"cell_type":"code","source":"X = data[predictors]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"285cea7205a1ebedfdf8d4f596f35e331852f1eb","_cell_guid":"5976b1ae-fef5-4b37-84be-121230a3c786"},"cell_type":"markdown","source":"## Building My Model"},{"metadata":{"_uuid":"d966c8c505f1ce6558b7b7f4c28d6ce311b8e6ea","_cell_guid":"7816d4af-eac3-4005-953e-5d8d0fd2ca8d"},"cell_type":"markdown","source":"I will use the scikit-learn library to create your models. When coding, this library is written as sklearn, as you will see in the sample code. Scikit-learn is easily the most popular library for modeling the types of data typically stored in DataFrames.\n\nThe steps to building and using a model are:\n\n* Define: What type of model will it be? A decision tree? Some other type of model? Some other parameters of the model type are specified too.\n* Fit: Capture patterns from provided data. This is the heart of modeling.\n* Predict: Just what it sounds like\n* Evaluate: Determine how accurate the model's predictions are.\nHere is the example for defining and fitting the model."},{"metadata":{"collapsed":true,"_uuid":"44075f48d29caad2b585650cb66ccdd0d0de533c","_cell_guid":"53e421d2-6e05-44ba-b3ea-1ec5830c5849","trusted":false},"cell_type":"code","source":" # Create a DecisionTreeRegressorModel and save it to a variable (with a name like my_model or iowa_model). Ensure you've done the relevant import so you can run this command.)\nfrom sklearn.tree import DecisionTreeRegressor\n# Define model\ndata_model = DecisionTreeRegressor()\n# Fit model\ndata_model.fit(X,y)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"745ca29e4b24caa0daa4868bd21c13d1a6f0c1cd","_cell_guid":"8147dae8-fb40-4afe-aebc-622536178e0c","trusted":false},"cell_type":"code","source":"# Make a few predictions with the model's predict command and print out the predictions.\nprint('Make predictors for the following 5')\nprint(X.head())\nprint('------------------------------------')\nprint('Predictions are')\nprint(data_model.predict(X.head()))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e33ac918ef4f2700a2a2c5e1b5c8185bf0d31a51","_cell_guid":"535c3b41-2860-4ddf-b9dd-619d9976c60f"},"cell_type":"markdown","source":"# Model Validation"},{"metadata":{"_uuid":"30b04b59ea4048d31ca8e32d00e59df11363df36","_cell_guid":"c347bfe9-f90b-49ee-9ba9-e0a947f53ebd"},"cell_type":"markdown","source":"In this step, you will learn to use model validation to measure the quality of your model. Measuring model quality is the key to iteratively improving your models."},{"metadata":{"_uuid":"f63b0f5fab6ad87d05ef76e79b0c99f56a641597","_cell_guid":"5fe67123-e1d8-4cf8-8141-bacc5e6366e2"},"cell_type":"markdown","source":"## What is Model Validation\nYou've built a model. But how good is it?\n\nYou'll need to answer this question for almost every model you ever build. In most (though not necessarily all) applications, the relevant measure of model quality is predictive accuracy. In other words, will the model's predictions be close to what actually happens.\n\nSome people try answering this problem by making predictions with their training data. They compare those predictions to the actual target values in the training data. This approach has a critical shortcoming, which you will see in a moment (and which you'll subsequently see how to solve).\n\nEven with this simple approach, you'll need to summarize the model quality into a form that someone can understand. If you have predicted and actual home values for 10000 houses, you will inevitably end up with a mix of good and bad predictions. Looking through such a long list would be pointless.\n\nThere are many metrics for summarizing model quality, but we'll start with one called Mean Absolute Error (also called MAE). Let's break down this metric starting with the last word, error.\n\nThe prediction error for each house is: \nerror=actual−predicted\n\nSo, if a house cost $150,000 and you predicted it would cost $100,000 the error is $50,000.\n\nWith the MAE metric, we take the absolute value of each error. This converts each error to a positive number. We then take the average of those absolute errors. This is our measure of model quality. In plain English, it can be said as\n\nOn average, our predictions are off by about X\n\nWe first load the Melbourne data and create X and y. That code isn't shown here, since you've already seen it a couple times."},{"metadata":{"collapsed":true,"_uuid":"db72f040e2a4ffde783aaff625f283d258e8edb0","_cell_guid":"f32361e5-d859-4e53-bb13-6288f6391631","trusted":false},"cell_type":"code","source":"from sklearn.metrics import mean_absolute_error\npredicted_home_prices = data_model.predict(X)\nmean_absolute_error(y, predicted_home_prices)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"47cade317108993334f4cd56f806851a954fbfe5","_cell_guid":"576feeea-d539-4eee-bd75-17d32932c7ef"},"cell_type":"markdown","source":"## The Problem with \"In-Sample\" Scores\nThe measure we just computed can be called an \"in-sample\" score. We used a single set of houses (called a data sample) for both building the model and for calculating it's MAE score. This is bad.\n\nImagine that, in the large real estate market, door color is unrelated to home price. However, in the sample of data you used to build the model, it may be that all homes with green doors were very expensive. The model's job is to find patterns that predict home prices, so it will see this pattern, and it will always predict high prices for homes with green doors.\n\nSince this pattern was originally derived from the training data, the model will appear accurate in the training data.\n\nBut this pattern likely won't hold when the model sees new data, and the model would be very inaccurate (and cost us lots of money) when we applied it to our real estate business.\n\nEven a model capturing only happenstance relationships in the data, relationships that will not be repeated when new data, can appear to be very accurate on in-sample accuracy measurements.\n\n## Example¶\nModels' practical value come from making predictions on new data, so we should measure performance on data that wasn't used to build the model. The most straightforward way to do this is to exclude some data from the model-building process, and then use those to test the model's accuracy on data it hasn't seen before. This data is called validation data.\n\nThe scikit-learn library has a function train_test_split to break up the data into two pieces, so the code to get a validation score looks like this:"},{"metadata":{"collapsed":true,"_uuid":"e49b49e280c151bf737871382abf641d356d883c","_cell_guid":"10ebff7c-d3e1-4aaf-b845-f0c7867302b8","trusted":false},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n## split data into training and validation data, for both predictors and target\n# The split is based on a random number generator. Supplying a numeric value to\n# the random_state argument guarantees we get the same split every time we\n# run this script.\ntrain_X, val_X, train_y, val_y = train_test_split(X,y, random_state = 0)\n#Define model\ndata_model = DecisionTreeRegressor()\n#Fit\ndata_model.fit(train_X,train_y)\n\n# get Prediction \nval_predictions = data_model.predict(val_X)\nprint(mean_absolute_error(val_y,val_predictions))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"376e765b5fcec024f4141c9e2d068762b0a82b99","_cell_guid":"89f5e643-a4b0-4ccb-9d7a-8915ce6afd1b"},"cell_type":"markdown","source":"# Underfitting, Overfitting and Model Optimization\n"},{"metadata":{"_uuid":"26d55b3ffb6d69b19849c032fca86e285a9817da","_cell_guid":"15f5488b-1644-46e5-802d-98b3d58e4212"},"cell_type":"markdown","source":"## Experimenting With Different Models\nNow that you have a trustworthy way to measure model accuracy, you can experiment with alternative models and see which gives the best predictions. But what alternatives do you have for models?\n\nYou can see in scikit-learn's documentation that the decision tree model has many options (more than you'll want or need for a long time). The most important options determine the tree's depth. Recall from page 2 that a tree's depth is a measure of how many splits it makes before coming to a prediction. This is a relatively shallow tree\n\n![Depth 2 Tree](http://i.imgur.com/R3ywQsR.png)\n\nIn practice, it's not uncommon for a tree to have 10 splits between the top level (all houses and a leaf). As the tree gets deeper, the dataset gets sliced up into leaves with fewer houses. If a tree only had 1 split, it divides the data into 2 groups. If each group is split again, we would get 4 groups of houses. Splitting each of those again would create 8 groups. If we keep doubling the number of groups by adding more splits at each level, we'll have  210  groups of houses by the time we get to the 10th level. That's 1024 leaves.\n\nWhen we divide the houses amongst many leaves, we also have fewer houses in each leaf. Leaves with very few houses will make predictions that are quite close to those homes' actual values, but they may make very unreliable predictions for new data (because each prediction is based on only a few houses).\n\nThis is a phenomenon called overfitting, where a model matches the training data almost perfectly, but does poorly in validation and other new data. On the flip side, if we make our tree very shallow, it doesn't divide up the houses into very distinct groups.\n\nAt an extreme, if a tree divides houses into only 2 or 4, each group still has a wide variety of houses. Resulting predictions may be far off for most houses, even in the training data (and it will be bad in validation too for the same reason). When a model fails to capture important distinctions and patterns in the data, so it performs poorly even in training data, that is called underfitting.\n\nSince we care about accuracy on new data, which we estimate from our validation data, we want to find the sweet spot between underfitting and overfitting. Visually, we want the low point of the (red) validation curve in\n\n![underfitting_overfitting](http://i.imgur.com/2q85n9s.png)\n\n## Example\nThere are a few alternatives for controlling the tree depth, and many allow for some routes through the tree to have greater depth than other routes. But the max_leaf_nodes argument provides a very sensible way to control overfitting vs underfitting. The more leaves we allow the model to make, the more we move from the underfitting area in the above graph to the overfitting area.\n\nWe can use a utility function to help compare MAE scores from different values for max_leaf_nodes:"},{"metadata":{"collapsed":true,"_uuid":"e650e9d86c51a25cdfdd082534c99107baeb0de7","_cell_guid":"be22afc0-51ba-4eca-a921-7efe82c95c6e","trusted":false},"cell_type":"code","source":"from sklearn.metrics import mean_absolute_error\n\ndef get_mae(max_leaf_nodes, predictors_train, predictors_val, targ_train, targ_val):\n    model = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=0)\n    model.fit(predictors_train, targ_train)\n    preds_val = model.predict(predictors_val)\n    mae = mean_absolute_error(targ_val, preds_val)\n    return(mae)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"6edeadb50e314cce95349425026cf05757fb53f7","_cell_guid":"891787e3-d035-4832-bedb-179f51c0fea7","trusted":false},"cell_type":"code","source":"for max_leaf_nodes in [5, 50, 500, 5000]:\n    my_mae = get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y)\n    print(\"Max leaf nodes: %d  \\t\\t Mean Absolute Error:  %d\" %(max_leaf_nodes, my_mae))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7d2537d33d5920220ba6984913f7cc9711c30769","_cell_guid":"4431c89a-3597-4dbf-a5f8-7a4695b29c39"},"cell_type":"markdown","source":"**Of the options listed, 50 is the optimal number of leaves. Apply the function to your Iowa data to find the best decision tree.**"},{"metadata":{"_uuid":"bfaddbc7d637a2e2af661abcd1e9f0ddd3dd97ec","_cell_guid":"955f13ca-9d9f-4e2e-8aff-ee008f9e92bc"},"cell_type":"markdown","source":"## Conclusion\nHere's the takeaway: Models can suffer from either:\n\n* Overfitting: capturing spurious patterns that won't recur in the future, leading to less accurate predictions, or\n* Underfitting: failing to capture relevant patterns, again leading to less accurate predictions.\n* We use validation data, which isn't used in model training, to measure a candidate model's accuracy. This lets us try many candidate models and keep the best one.\n\nBut we're still using Decision Tree models, which are not very sophisticated by modern machine learning standards."},{"metadata":{"collapsed":true,"_uuid":"763bb1491cc89d31a519f27b0232ef922d5c3d19","_cell_guid":"c536c64b-4010-402c-af7e-23fb5732c5dd"},"cell_type":"markdown","source":"# Random Forests"},{"metadata":{"collapsed":true,"_uuid":"0d4e62a56561fa981f6036ea47811ae973a64ce5","_cell_guid":"520d5134-afcc-46c1-8f18-35d4eb133e70","trusted":false},"cell_type":"code","source":"# sophisticated machine learning model, (정교한 기계핛브 모델)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d01739cb8a9d1dc4b006cbe298525cf5a4affa07","_cell_guid":"d96a545f-e055-432b-bd2f-720f8dd442d8"},"cell_type":"markdown","source":"## Introduction\nDecision trees leave you with a difficult decision. A deep tree with lots of leaves will overfit because each prediction is coming from historical data from only the few houses at its leaf. But a shallow tree with few leaves will perform poorly because it fails to capture as many distinctions in the raw data.\n\nEven today's most sophisticated modeling techniques face this tension between underfitting and overfitting. But, many models have clever ideas that can lead to better performance. We'll look at the random forest as an example.\n\nThe random forest uses many trees, and it makes a prediction by averaging the predictions of each component tree. It generally has much better predictive accuracy than a single decision tree and it works well with default parameters. If you keep modeling, you can learn more models with even better performance, but many of those are sensitive to getting the right parameters."},{"metadata":{"collapsed":true,"_uuid":"b042f9055f903522a95be09ac6c6435fa9798386","_cell_guid":"f39ca3cc-78de-4265-a8ff-ff96bb49a941","trusted":false},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\n\nforest_model = RandomForestRegressor()\nforest_model.fit(train_X,train_y)\ndata_predictions = forest_model.predict(val_X)\nprint(mean_absolute_error(val_y,data_predictions))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1cb4d354447de866e16dd0d5f8b024ae8fcb77b3","_cell_guid":"1cd97f3a-6328-465a-a7fc-93d6c521896d"},"cell_type":"markdown","source":"## Conclusion\nThere is likely room for further improvement, but this is a big improvement over the best decision tree error of 250,000. There are parameters which allow you to change the performance of the Random Forest much as we changed the maximum depth of the single decision tree. But one of the best features of Random Forest models is that they generally work reasonably even without this tuning.\n\nYou'll soon learn the XGBoost model, which provides better performance when tuned well with the right parameters (but which requires some skill to get the right model parameters)."},{"metadata":{"_uuid":"139183f58c818a9855cc36c6a96e62774be7fc68","_cell_guid":"b60e48e6-795e-4c7e-a04d-18e3f314cc0a"},"cell_type":"markdown","source":"# Submitting From A Kernel"},{"metadata":{"_uuid":"fd2107c8ab68e0ba3753b32701d34d1bf16f9bb0","_cell_guid":"d5a6505c-9e0a-4fbc-9457-827277d168c5"},"cell_type":"markdown","source":""},{"metadata":{"_uuid":"43f16d285e20573185e6705ac6351aafe1e3fa0f","_cell_guid":"91c4647d-3344-485e-909d-c0800fd97d08"},"cell_type":"markdown","source":"# Level 2"},{"metadata":{"_uuid":"24357e743904385d9e105f2e763846be48f458d3","_cell_guid":"be1ca786-151f-44c7-9c8b-06c533522105"},"cell_type":"markdown","source":"# Handling Missing Values"},{"metadata":{"_uuid":"134b7ee8ed586fd9a2b3cc037c121b8d265009f6","_cell_guid":"7d0f2ea7-0dee-4aaa-adf9-4dc833960531"},"cell_type":"markdown","source":"## Introduction\nThere are many ways data can end up with missing values. For example\n\n* A 2 bedroom house wouldn't include an answer for How large is the third bedroom\n* Someone being surveyed may choose not to share their income\n\nPython libraries represent missing numbers as nan which is short for \"not a number\". You can detect which cells have missing values, and then count how many there are in each column with the command:\n\n> > print(data.isnull().sum())\n\nMost libraries (including scikit-learn) will give you an error if you try to build a model using data with missing values. So you'll need to choose one of the strategies below."},{"metadata":{"collapsed":true,"_uuid":"8224addee1782c243ec395aa9e032e9f7016f8e8","_cell_guid":"359f0e81-832a-4cfb-bc16-34e9b3e4064e","trusted":false},"cell_type":"code","source":"print(data.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6c996ef014e244239dee36a458ccbe1acf2eb057","_cell_guid":"562db223-ea2a-4ac5-8bca-fcc5595f5cf6"},"cell_type":"markdown","source":"-------------------------------------------------"},{"metadata":{"_uuid":"31535c08e8b323795f8a1c3a38b3deb0a589bc1d","_cell_guid":"6ffc8151-738b-499f-876a-f2e19ae6d66f"},"cell_type":"markdown","source":"Solution은 처리할 수 있는 방법 들이다. 한번 숙지해보자."},{"metadata":{"_uuid":"8722bc4b62ff71051920018bf08c23e96c231e1a","_cell_guid":"a48ad080-d018-43af-890d-b97913d4e9d5"},"cell_type":"markdown","source":"## Solutions\n\n### 1) A Simple Option: Drop Columns with Missing Values\nIf your data is in a DataFrame called original_data, you can drop columns with missing values. One way to do that is\n\n    data_without_missing_values = original_data.dropna(axis=1)\n    \nIn many cases, you'll have both a training dataset and a test dataset. You will want to drop the same columns in both DataFrames. In that case, you would write\n\n    cols_with_missing = [col for col in original_data.columns \n                                 if original_data[col].isnull().any()]\n    redued_original_data = original_data.drop(cols_with_missing, axis=1)\n    reduced_test_data = test_data.drop(cols_with_missing, axis=1)\n\nIf those columns had useful information (in the places that were not missing), your model loses access to this information when the column is dropped. Also, if your test data has missing values in places where your training data did not, this will result in an error.\n\nSo, it's somewhat usually not the best solution. However, it can be useful when most values in a column are missing."},{"metadata":{"_uuid":"0a04f78a041ce2d7e8b43a516d926016e88c4e59","_cell_guid":"7f21f65a-b988-4719-8758-65622194d7af"},"cell_type":"markdown","source":"### 2) A Better Option: Imputation\nImputation fills in the missing value with some number. The imputed value won't be exactly right in most cases, but it usually gives more accurate models than dropping the column entirely.\n\nThis is done with\n\n    from sklearn.preprocessing import Imputer\n    my_imputer = Imputer()\n    data_with_imputed_values = my_imputer.fit_transform(original_data)\nThe default behavior fills in the mean value for imputation. Statisticians have researched more complex strategies, but those complex strategies typically give no benefit once you plug the results into sophisticated machine learning models.\n\nOne (of many) nice things about Imputation is that it can be included in a scikit-learn Pipeline. Pipelines simplify model building, model validation and model deployment.\n\n### 3) An Extension To Imputation\nImputation is the standard approach, and it usually works well. However, imputed values may by systematically above or below their actual values (which weren't collected in the dataset). Or rows with missing values may be unique in some other way. In that case, your model would make better predictions by considering which values were originally missing. Here's how it might look:\n\n    # make copy to avoid changing original data (when Imputing)\n    new_data = original_data.copy()\n\n    # make new columns indicating what will be imputed\n    cols_with_missing = (col for col in new_data.columns \n                                     if new_data[c].isnull().any())\n    for col in cols_with_missing:\n        new_data[col + '_was_missing'] = new_data[col].isnull()\n\n    # Imputation\n    my_imputer = Imputer()\n    new_data = my_imputer.fit_transform(new_data)\n    In some cases this approach will meaningfully improve results. In other cases, it doesn't help at all."},{"metadata":{"_uuid":"90e704bf5bbe4aa3aa236df5b5b309a9dfd399c1","_cell_guid":"f1ae0fcb-13ec-4615-8246-630f7d2565e9"},"cell_type":"markdown","source":"--------------------------------"},{"metadata":{"_uuid":"72930bbfc9ef33c6501190049b4b4fd0bb32d383","_cell_guid":"0b3277df-f68e-49a3-ac6f-4cecdbf84aa5"},"cell_type":"markdown","source":"   위의 Soultion을 가지고 한번 각각 진행해보자."},{"metadata":{"_uuid":"457efbf255784e779afd66602b85b64045b14c71","_cell_guid":"1acdaa32-3e2a-42fc-b982-b7b7d6a9004e"},"cell_type":"markdown","source":"## Example (Comparing All Solutions)"},{"metadata":{"collapsed":true,"_uuid":"a380633c2cbbb134752f7f6078573fd665045290","_cell_guid":"32483eb9-7707-4043-915e-60bf9f9b075b","trusted":false},"cell_type":"code","source":"main_file_path = '../input/house-prices-advanced-regression-techniques/train.csv'\ndata = pd.read_csv(main_file_path)\n\nhouse_data = data.copy()\nhouse_target = house_data.SalePrice\nhouse_predictors = house_data.drop(['SalePrice'],axis=1)\n\nhouse_numeric_predictors = house_data.select_dtypes(exclude=['object'])","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"a639e74a0f1b7098a7ef0cab47d93db6ffba37a8","_cell_guid":"dcacd3b2-e2fa-4b99-9b23-b4304e444dc3","trusted":false},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(house_numeric_predictors, \n                                                    house_target,\n                                                    train_size=0.7, \n                                                    test_size=0.3, \n                                                    random_state=0)\n\ndef score_dataset(X_train, X_test, y_train, y_test):\n    model = RandomForestRegressor()\n    model.fit(X_train, y_train)\n    preds = model.predict(X_test)\n    return mean_absolute_error(y_test, preds)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d99c104e524541e2566b4a6b9c4b6dc0c05f4bc7","_cell_guid":"5ec9af28-4d22-42d4-9d13-f504e05af693"},"cell_type":"markdown","source":"### Get Model Score from Dropping Columns with Missing Values"},{"metadata":{"collapsed":true,"_uuid":"32865719d3f599ebf6a7fcb193f0b3d406aff9d9","_cell_guid":"0e1c2731-bc34-4b84-9822-82a05ea2f396","trusted":false},"cell_type":"code","source":"cols_with_missing = [col for col in X_train.columns \n                                 if X_train[col].isnull().any()]\nreduced_X_train = X_train.drop(cols_with_missing, axis=1)\nreduced_X_test  = X_test.drop(cols_with_missing, axis=1)\nprint(score_dataset(reduced_X_train, reduced_X_test, y_train, y_test))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5f6ae0448468e9698cbe44bd902cacc15168375b","_cell_guid":"965669b9-9679-4ddc-945a-0da5751e45a7"},"cell_type":"markdown","source":"### Get Model Score from Imputation"},{"metadata":{"collapsed":true,"_uuid":"4878cc5348e8cd5d96648f699af6a8f170e415a4","_cell_guid":"854c3a3d-195d-43fc-b982-486072b3236e","trusted":false},"cell_type":"code","source":"from sklearn.preprocessing import Imputer\n\nmy_imputer = Imputer()\nimputed_X_train = my_imputer.fit_transform(X_train)\nimputed_X_test = my_imputer.transform(X_test)\nprint(\"Mean Absolute Error from Imputation:\")\nprint(score_dataset(imputed_X_train, imputed_X_test, y_train, y_test))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"96528dc5b28a86c62f2ffb909128a728a5f7a336","_cell_guid":"df9a334c-290f-48eb-8ad1-a5d609ff4a42"},"cell_type":"markdown","source":"### Get Score from Imputation with Extra Columns Showing What Was Imputed"},{"metadata":{"collapsed":true,"_uuid":"6738e4676472a0954b7df205c4e5ae25051c53e2","_cell_guid":"6f9189b1-2100-4033-a1fe-194bd3df5395","trusted":false},"cell_type":"code","source":"imputed_X_train_plus = X_train.copy()\nimputed_X_test_plus = X_test.copy()\n\ncols_with_missing = (col for col in X_train.columns \n                                 if X_train[col].isnull().any())\nfor col in cols_with_missing:\n    imputed_X_train_plus[col + '_was_missing'] = imputed_X_train_plus[col].isnull()\n    imputed_X_test_plus[col + '_was_missing'] = imputed_X_test_plus[col].isnull()\n\n# Imputation\nmy_imputer = Imputer()\nimputed_X_train_plus = my_imputer.fit_transform(imputed_X_train_plus)\nimputed_X_test_plus = my_imputer.transform(imputed_X_test_plus)\n\nprint(\"Mean Absolute Error from Imputation while Track What Was Imputed:\")\nprint(score_dataset(imputed_X_train_plus, imputed_X_test_plus, y_train, y_test))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b3165b61fb268ff77f137797f848f862a5f51d64","_cell_guid":"a4a8bfcf-a918-4a7f-9c9b-dc4c46633209"},"cell_type":"markdown","source":"# Using Categorical Data with One Hot encoding"},{"metadata":{"_uuid":"5cad57a0d9e9d360e162007f9a9acab8baf3b3df","_cell_guid":"55dc5118-9dc0-4e60-aca1-1e1c4b5a3320"},"cell_type":"markdown","source":"## Introduction\nCategorical data is data that takes only a limited number of values.\n\nFor example, if you people responded to a survey about which what brand of car they owned, the result would be categorical (because the answers would be things like Honda, Toyota, Ford, None, etc.). Responses fall into a fixed set of categories.\n\nYou will get an error if you try to plug these variables into most machine learning models in Python without \"encoding\" them first. Here we'll show the most popular method for encoding categorical variables."},{"metadata":{"_uuid":"4d9b3acd3004455ae11b649763f08356105d8cb4","_cell_guid":"9e219be4-7a26-4caf-a106-d751857e60c9"},"cell_type":"markdown","source":"## One-Hot Encoding : The Standard Approach for Categorical Data\nOne hot encoding is the most widespread approach, and it works very well unless your categorical variable takes on a large number of values (i.e. you generally won't it for variables taking more than 15 different values. It'd be a poor choice in some cases with fewer values, though that varies.)\n\nOne hot encoding creates new (binary) columns, indicating the presence of each possible value from the original data. Let's work through an example.\n\n<img src=\"https://i.imgur.com/mtimFxh.png\" alt=\"Imgur\">\n\nThe values in the original data are Red, Yellow and Green. We create a separate column for each possible value. Wherever the original value was Red, we put a 1 in the Red column."},{"metadata":{"_uuid":"a34ce4a29d59723a97f97dd03f0d33e31c3ab32e","_cell_guid":"94fb964d-6904-4a3a-84b3-6c52aa775dca"},"cell_type":"markdown","source":"## Example"},{"metadata":{"collapsed":true,"_uuid":"c528568cf69fca92125c10f31b31452c32266831","_cell_guid":"bc5c32ac-e098-4fbc-81d2-c0d9c512516f","trusted":false},"cell_type":"code","source":"import pandas as pd\ntrain_data = pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv')\ntest_data = pd.read_csv('../input/house-prices-advanced-regression-techniques/test.csv')\n\n# Drop houses where the target is missing\ntrain_data.dropna(axis=0, subset=['SalePrice'], inplace=True)\n\ntarget = train_data.SalePrice\n\n# Since missing values isn't the focus of this tutorial, we use the simplest\n# possible approach, which drops these columns. \n# For more detail (and a better approach) to missing values, see\n# https://www.kaggle.com/dansbecker/handling-missing-values\ncols_with_missing = [col for col in train_data.columns \n                                 if train_data[col].isnull().any()]      \n\ncandidate_train_predictors = train_data.drop(['Id', 'SalePrice'] + cols_with_missing, axis=1)\ncandidate_test_predictors = test_data.drop(['Id'] + cols_with_missing, axis=1)\n\n# \"cardinality\" means the number of unique values in a column.\n# We use it as our only way to select categorical columns here. This is convenient, though\n# a little arbitrary.\nlow_cardinality_cols = [cname for cname in candidate_train_predictors.columns if \n                                candidate_train_predictors[cname].nunique() < 10 and\n                                candidate_train_predictors[cname].dtype == \"object\"]\nnumeric_cols = [cname for cname in candidate_train_predictors.columns if \n                                candidate_train_predictors[cname].dtype in ['int64', 'float64']]\nmy_cols = low_cardinality_cols + numeric_cols\ntrain_predictors = candidate_train_predictors[my_cols]\ntest_predictors = candidate_test_predictors[my_cols]","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"27780d96b229ce05e4ef24fcf9776ff1be927d76","_cell_guid":"f34b2c17-c0b5-4311-aae0-df49c60fac8d","trusted":false},"cell_type":"code","source":"train_predictors.dtypes.sample(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2e4e47b75b56d4620e03580bf2dc6024f2baccf6","_cell_guid":"1e2e7cea-10d0-454f-9472-5881d2412ce4"},"cell_type":"markdown","source":"**Objec**t indicates a column has text (there are other things it could be theoretically be, but that's unimportant for our purposes). It's most common to one-hot encode these \"object\" columns, since they can't be plugged directly into most models. Pandas offers a convenient function called** get_dummies **to get one-hot encodings. Call it like this:"},{"metadata":{"collapsed":true,"_uuid":"981351e605a0db5eb156109ccc5d705d9ef5713e","_cell_guid":"fe70fceb-52de-4698-ac41-fc0090ce99d5","trusted":false},"cell_type":"code","source":"one_hot_encoded_training_predictors = pd.get_dummies(train_predictors)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"c13620296ac4c7ef94347f3f8300f5d41156f568","_cell_guid":"82e2d5a6-9b1d-48f1-a9a0-8bfacd384639","scrolled":true,"trusted":false},"cell_type":"code","source":"print(one_hot_encoded_training_predictors[:10])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8d3e9b299fb348ef43397d0d5532f6022a08a9f7","_cell_guid":"b78cb4ff-a6d8-45e8-a0d8-9a782dc65e37"},"cell_type":"markdown","source":"Alternatively, you could have dropped the categoricals. To see how the approaches compare, we can calculate the mean absolute error of models built with two alternative sets of predictors:\n\nOne-hot encoded categoricals as well as numeric predictors\nNumerical predictors, where we drop categoricals.\nOne-hot encoding usually helps, but it varies on a case-by-case basis. In this case, there doesn't appear to be any meaningful benefit from using the one-hot encoded variables."},{"metadata":{"collapsed":true,"_uuid":"45e909a5a8dbf6cf0ad9078c65063f9c55329485","_cell_guid":"9677ac2a-109e-47a8-93ab-2c4ef6beff1c","trusted":false},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestRegressor\n\ndef get_mae(X, y):\n    # multiple by -1 to make positive MAE score instead of neg value returned as sklearn convention\n    return -1 * cross_val_score(RandomForestRegressor(50), \n                                X, y, \n                                scoring = 'neg_mean_absolute_error').mean()\n\npredictors_without_categoricals = train_predictors.select_dtypes(exclude=['object'])\n\nmae_without_categoricals = get_mae(predictors_without_categoricals, target)\n\nmae_one_hot_encoded = get_mae(one_hot_encoded_training_predictors, target)\n\nprint('Mean Absolute Error when Dropping Categoricals: ' + str(int(mae_without_categoricals)))\nprint('Mean Abslute Error with One-Hot Encoding: ' + str(int(mae_one_hot_encoded)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0d9cf6f99cbe176ce0277cead6a02107ebc91164","_cell_guid":"08f9fb6c-7354-4c30-bf10-b403deb8cbf3"},"cell_type":"markdown","source":"## Applying to Multiple Files\nSo far, you've one-hot-encoded your training data. What about when you have multiple files (e.g. a test dataset, or some other data that you'd like to make predictions for)? Scikit-learn is sensitive to the ordering of columns, so if the training dataset and test datasets get misaligned, your results will be nonsense. This could happen if a categorical had a different number of values in the training data vs the test data.\n\nEnsure the test data is encoded in the same manner as the training data with the align command:"},{"metadata":{"collapsed":true,"_uuid":"e2a77975c5635153f9d6def2046c5f0e5b28088e","_cell_guid":"f864d1b9-e78e-4a64-950f-61422caf4cbb","trusted":false},"cell_type":"code","source":"one_hot_encoded_training_predictors = pd.get_dummies(train_predictors)\none_hot_encoded_test_predictors = pd.get_dummies(test_predictors)\nfinal_train, final_test = one_hot_encoded_training_predictors.align(one_hot_encoded_test_predictors,\n                                                                    join='left', \n                                                                    axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"49aebc16a61e2e588a2f5d559ab397fb4e226208","_cell_guid":"392554f3-d852-4fe1-9df2-b885fa6772c6"},"cell_type":"markdown","source":"The align command makes sure the columns show up in the same order in both datasets (it uses column names to identify which columns line up in each dataset.) The argument join='left' specifies that we will do the equivalent of SQL's left join. That means, if there are ever columns that show up in one dataset and not the other, we will keep exactly the columns from our training data. The argument join='inner' would do what SQL databases call an inner join, keeping only the columns showing up in both datasets. That's also a sensible choic"},{"metadata":{"_uuid":"1ca7b10fc6bf45facf242e7ef57e8bebe3a3c214","_cell_guid":"3303621e-7fa4-4c16-8592-c64f919c3885"},"cell_type":"markdown","source":"# What is XGBoost\nXGBoost is the leading model for working with standard tabular data (the type of data you store in Pandas DataFrames, as opposed to more exotic types of data like images and videos). XGBoost models dominate many Kaggle competitions.\n\nTo reach peak accuracy, XGBoost models require more knowledge and model tuning than techniques like Random Forest. After this tutorial, you'ill be able to\n\n* Follow the full modeling workflow with XGBoost\n* Fine-tune XGBoost models for optimal performance\nXGBoost is an implementation of the Gradient Boosted Decision Trees algorithm (scikit-learn has another version of this algorithm, but XGBoost has some technical advantages.) What is Gradient Boosted Decision Trees? We'll walk through a diagram.\n\n<img src=\"https://i.imgur.com/e7MIgXk.png\" alt=\"xgboost image\">\n\nWe go through cycles that repeatedly builds new models and combines them into an ensemble model. We start the cycle by calculating the errors for each observation in the dataset. We then build a new model to predict those. We add predictions from this error-predicting model to the \"ensemble of models.\"\n\nTo make a prediction, we add the predictions from all previous models. We can use these predictions to calculate new errors, build the next model, and add it to the ensemble.\n\nThere's one piece outside that cycle. We need some base prediction to start the cycle. In practice, the initial predictions can be pretty naive. Even if it's predictions are wildly inaccurate, subsequent additions to the ensemble will address those errors.\n\nThis process may sound complicated, but the code to use it is straightforward. We'll fill in some additional explanatory details in the model tuning section below."},{"metadata":{"_uuid":"492e8cc76ac66c3640d348a1c35af934c7e80348","_cell_guid":"e35e5a56-f671-41e8-b353-838369396f65"},"cell_type":"markdown","source":"## Example"},{"metadata":{"collapsed":true,"_uuid":"2cbe54c5065d833e7fadbcb1971871ac525afd95","_cell_guid":"a65ef2a4-63fd-487c-99cb-840e620bed51","trusted":false},"cell_type":"code","source":"from xgboost import XGBRegressor\ndata = pd.read_csv('../input/train.csv')\ndata.dropna(axis=0, subset=['SalePrice'],inplace=True)\ny = data.SalePrice\nX = data.drop(['SalePrice'],axis=1).select_dtypes(exclude=['object'])\ntrain_X, test_X, train_y, test_y = train_test_split(X.as_matrix(), y.as_matrix(), test_size=0.25)\n\nmy_imputer = Imputer()\ntrain_X = my_imputer.fit_transform(train_X)\ntest_X = my_imputer.transform(test_X)\n\nxg_model = XGBRegressor()\nxg_model.fit(train_X,train_y,verbose=False)\npredictions = xg_model.predict(test_X)\nprint(\"Mean Absolute Error : \" + str(mean_absolute_error(predictions, test_y)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7d84480d3aa7385140431ec8ee25d3e7e129e04a","_cell_guid":"99423880-d5dc-448a-b0e5-2fc4e03d4fe6"},"cell_type":"markdown","source":"## Model Tuning\nXGBoost has a few parameters that can dramatically affect your model's accuracy and training speed. The first parameters you should understand are:\n\nn_estimators and early_stopping_rounds\nn_estimators specifies how many times to go through the modeling cycle described above.\n\nIn the underfitting vs overfitting graph, n_estimators moves you further to the right. Too low a value causes underfitting, which is inaccurate predictions on both training data and new data. Too large a value causes overfitting, which is accurate predictions on training data, but inaccurate predictions on new data (which is what we care about). You can experiment with your dataset to find the ideal. Typical values range from 100-1000, though this depends a lot on the learning rate discussed below.\n\nThe argument early_stopping_rounds offers a way to automatically find the ideal value. Early stopping causes the model to stop iterating when the validation score stops improving, even if we aren't at the hard stop for n_estimators. It's smart to set a high value for n_estimators and then use early_stopping_rounds to find the optimal time to stop iterating.\n\nSince random chance sometimes causes a single round where validation scores don't improve, you need to specify a number for how many rounds of straight deterioration to allow before stopping. early_stopping_rounds = 5 is a reasonable value. Thus we stop after 5 straight rounds of deteriorating validation scores.\n\nHere is the code to fit with early_stopping:"},{"metadata":{"collapsed":true,"_uuid":"67c8de0a38ae549199ca3c89ddbf1cdb73183b9b","_cell_guid":"fbf3d20f-04d8-415f-807d-e1518f35196b","trusted":false},"cell_type":"code","source":"xg_model.fit(train_X,train_y,early_stopping_rounds=5,eval_set=[(test_X,test_y)],verbose=False)\npredictions = xg_model.predict(test_X)\nprint(\"Mean Absolute Error : \" + str(mean_absolute_error(predictions, test_y)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"55ee082971eeb7c0cb2f7b609e6b92cda2d7485a","_cell_guid":"a88d52b0-378e-49d1-a8cc-711fb7541345"},"cell_type":"markdown","source":"# What Are Partial Dependence Plots\nSome people complain machine learning models are black boxes. These people will argue we cannot see how these models are working on any given dataset, so we can neither extract insight nor identify problems with the model.\n\nBy and large, people making this claim are unfamiliar with partial dependence plots. Partial dependence plots show how each variable or predictor affects the model's predictions. This is useful for questions like:\n\n* How much of wage differences between men and women are due solely to gender, as opposed to differences in education backgrounds or work experience?\n\n* Controlling for house characteristics, what impact do longitude and latitude have on home prices? To restate this, we want to understand how similarly sized houses would be priced in different areas, even if the homes actually at these sites are different sizes.\n\n* Are health differences between two groups due to differences in their diets, or due to other factors?\n\nIf you are familiar with linear or logistic regression models, partial dependence plots can be interepreted similarly to the coefficients in those models. But partial dependence plots can capture more complex patterns from your data, and they can be used with any model. If you aren't familiar with linear or logistic regressions, don't get caught up on that comparison.\n\nWe will show a couple examples below, explain what they mean, and then talk about the code."},{"metadata":{"_uuid":"b0f941d88f7ee2011da89f16b0a66fa4144ff9a5","_cell_guid":"26554611-bc56-4b8d-a1a2-20f1b9735608"},"cell_type":"markdown","source":"## Interpreting Partial Dependence Plots¶\nWe'll start with 2 partial dependence plots showing the relationship (according to our model) between Price and a couple variables from the Melbourne Housing dataset. We'll walk through how these plots are created and interpreted."},{"metadata":{"collapsed":true,"_uuid":"4d16be8298bc937995653e72359637c605d73c04","_cell_guid":"111d6487-f35d-4a1c-a13e-ff11719f438a","trusted":false},"cell_type":"code","source":"import pandas as pd\nfrom sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifier\nfrom sklearn.ensemble.partial_dependence import partial_dependence, plot_partial_dependence\nfrom sklearn.preprocessing import Imputer\n\ncols_to_use = ['LotFrontage', 'LotArea']\n\ndef get_some_data():\n    data = pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv')\n    y = data.SalePrice\n    X = data[cols_to_use]\n    my_imputer = Imputer()\n    imputed_X = my_imputer.fit_transform(X)\n    return imputed_X, y\n    \n\nX, y = get_some_data()\nmy_model = GradientBoostingRegressor()\nmy_model.fit(X, y)\nmy_plots = plot_partial_dependence(my_model, \n                                   features=[0,1], \n                                   X=X, \n                                   feature_names=cols_to_use, \n                                   grid_resolution=10)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"89ae0fbc1eba712d1d339967fe156df338cdb69c","_cell_guid":"3a9ecbf2-4f93-43bc-a018-7e2defdb1a8e","trusted":false},"cell_type":"code","source":"titanic_data = pd.read_csv('../input/titanic-solution-a-beginners-guide/train.csv')\ntitanic_y = titanic_data.Survived\nclf = GradientBoostingClassifier()\ntitanic_X_colns = ['PassengerId','Age', 'Fare',]\ntitanic_X = titanic_data[titanic_X_colns]\nmy_imputer = Imputer()\nimputed_titanic_X = my_imputer.fit_transform(titanic_X)\n\nclf.fit(imputed_titanic_X, titanic_y)\ntitanic_plots = plot_partial_dependence(clf, features=[1,2], X=imputed_titanic_X, \n                                        feature_names=titanic_X_colns, grid_resolution=8)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"6c3669dd16b1ee54cce95c76fb6071a4e735aead","_cell_guid":"414a4b3b-bc23-4cc9-9c0f-7a859814322f"},"cell_type":"markdown","source":"# Pipelines"},{"metadata":{"_uuid":"8d58a740ba7088183dcf9e4a82be7eec0cee0f86","_cell_guid":"02b39c4c-aef8-471b-b8d9-877523f55255"},"cell_type":"markdown","source":"## What Are Pipelines\nPipelines are a simple way to keep your data processing and modeling code organized. Specifically, a pipeline bundles preprocessing and modeling steps so you can use the whole bundle as if it were a single step.\n\nMany data scientists hack together models without pipelines, but Pipelines have some important benefits. Those include:\n\n* Cleaner Code: You won't need to keep track of your training (and validation) data at each step of processing. Accounting for data at each step of processing can get messy. With a pipeline, you don't need to manually keep track of each step.\n* Fewer Bugs: There are fewer opportunities to mis-apply a step or forget a pre-processing step.\n* Easier to Productionize: It can be surprisingly hard to transition a model from a prototype to something deployable at scale. We won't go into the many related concerns here, but pipelines can help.\nMore Options For Model Testing: You will see an example in the next tutorial, which covers cross-validation."},{"metadata":{"collapsed":true,"_uuid":"2faa2930467052f0bfd5095dc361d86b0c998f73","_cell_guid":"775cbfd9-9a95-4e4e-a679-0b7d5c8dabb1","trusted":true},"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# Read Data\ndata = pd.read_csv('../input/melbourne-housing-snapshot/melb_data.csv')\ncols_to_use = ['Rooms', 'Distance', 'Landsize', 'BuildingArea', 'YearBuilt']\nX = data[cols_to_use]\ny = data.Price\ntrain_X, test_X, train_y, test_y = train_test_split(X, y)","execution_count":1,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"63d15e8a9d1dcb41667a5787a134864dc3aae5a8"},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import Imputer\n\nmy_pipeline = make_pipeline(Imputer(), RandomForestRegressor())","execution_count":2,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"d0db611a02c54b11c40cfbb4ae6433dc70c36a09"},"cell_type":"code","source":"my_pipeline.fit(train_X, train_y)\npredictions = my_pipeline.predict(test_X)","execution_count":3,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"6b66946649e56652c47fcbd64904c003147e1080"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}