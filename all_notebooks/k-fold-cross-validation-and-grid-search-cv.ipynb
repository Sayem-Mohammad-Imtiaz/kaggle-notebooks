{"cells":[{"metadata":{"_uuid":"35a8754583e0c2b6685ad796190b5b83f72e19d4"},"cell_type":"markdown","source":"# INTRODUCTION<br><br>\n* **In this kernel,we will see K-Fold Cross Validation and Grid Search Cross Validation with Machine Learning Classification Algorithms.**"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"46a0a6cb62aac67dac8672cf4ef19635ea1b1e57"},"cell_type":"markdown","source":"### Import Data"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"from sklearn.datasets import load_iris\n\niris = load_iris()\n\nx = iris.data\ny = iris.target","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8634a3392686dda1a04075076b1a39a0a9e89103"},"cell_type":"markdown","source":"### Normalization Data"},{"metadata":{"trusted":true,"_uuid":"ffdd552903346efb1677933ffe91fe2f20c39881"},"cell_type":"code","source":"x = (x-np.min(x))/(np.max(x)-np.min(x))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fa7f415488b7c84833daf7f56dc251b2f2b29b2f"},"cell_type":"markdown","source":"### Train and Test Split"},{"metadata":{"trusted":true,"_uuid":"6a1c3dff89802d4e588338095c104b9555885fc0"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.3)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6de3780b7c50064574411e066157107616526fc5"},"cell_type":"markdown","source":"### K-Fold Cross Validation with Machine Learning Classification Algorithms"},{"metadata":{"_uuid":"08224e72ce91a433e1b2336cd13b7a0a27f61ad0"},"cell_type":"markdown","source":"### Grid Search CV with K-Nearest Neighbors Classification"},{"metadata":{"trusted":true,"_uuid":"fd1c10ba42147856e76b8caf7115f7c5874c5af9"},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors = 3)# k = n_neighbors","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a1aa49ed3f098a0f01838547871774115ecf9c36"},"cell_type":"code","source":"#K Fold CV K=10\nfrom sklearn.model_selection import cross_val_score\naccuracies = cross_val_score(estimator = knn, X=x_train, y=y_train, cv=10)\naccuracies","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"73a353b8eaebe026f48e8aa5a62d47e89e48a483"},"cell_type":"code","source":"print(\"average accuracy: \",np.mean(accuracies))\nprint(\"average std: \",np.std(accuracies))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7f0eec1fe07993a48f5256825df63f883c900304"},"cell_type":"code","source":"knn.fit(x_train,y_train)\nprint(\"test accuracy: \",knn.score(x_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c74a0400cbe1a392f932f0cf1dad516723d7d134"},"cell_type":"code","source":"#grid search cross validation for knn\nfrom sklearn.model_selection import GridSearchCV\n\ngrid = {\"n_neighbors\":np.arange(1,50)}\nknn = KNeighborsClassifier()\n\nknn_cv = GridSearchCV(knn, grid, cv=10)#GridSearchCV\nknn_cv.fit(x_train,y_train)\n\n#%% print hyperparameter KNN algoritmasindaki K degeri\nprint(\"tuned hyperparameter K: \",knn_cv.best_params_)\nprint(\"tuned parametreye g√∂re en iyi accuracy (best score): \",knn_cv.best_score_)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"574748dff3d6a7f88dd090d591e119176e171177"},"cell_type":"markdown","source":"### Grid Search CV with Logistic Regression Classification"},{"metadata":{"trusted":true,"_uuid":"29c24a01358a61e19078835eb58057a63aaaf0da"},"cell_type":"code","source":"x = x[:100,:]\ny = y[:100]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"319407488f0382cda0e6309778645a8dbe2ff068"},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n#\"C\":Logistic Regression regularization parameter\n#if it is small it will be underfit.if it is big it will be overfit.\n#we need to choose good value for C.\n#loss functions\ngrid = {\"C\":np.logspace(-3,3,7),\"penalty\":[\"l1\",\"l2\"]} # l1 = lasso ve l2 = ridge\n\nlogreg = LogisticRegression()\nlogreg_cv = GridSearchCV(logreg, grid, cv=10)\nlogreg_cv.fit(x_train,y_train)\n\nprint(\"tuned hyperparameters: (best parameters): \",logreg_cv.best_params_)\nprint(\"accuracy: \",logreg_cv.best_score_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ed9c36e75c740c6400a53151fa069b6f71e2ebc7"},"cell_type":"code","source":"logreg2 = LogisticRegression(C=100,penalty=\"l1\")\nlogreg2.fit(x_train,y_train)\nprint(\"score: \",logreg2.score(x_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0d65d088456bbe9820a2e1c6d505e74491cc6ac9"},"cell_type":"markdown","source":"# Conclusion\n* **If you like it, Please upvote my kernel.**\n* **If you have any question, I will happy to hear it.**"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}