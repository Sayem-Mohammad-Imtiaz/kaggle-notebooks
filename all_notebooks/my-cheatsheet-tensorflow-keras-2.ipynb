{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Author: Yan Pan\n\nThis is the second part of TF/Keras cheatsheets, mainly collects recurrent models in language processing and time series prediction. For detailed data source, check Kaggle link above.","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom keras import layers\nfrom keras.models import Sequential\nfrom keras.preprocessing import image\n\nopt_verbose = 1 if 'runtime' in get_ipython().config.IPKernelApp.connection_file else 2\nmax_epochs = 2 if 'runtime' in get_ipython().config.IPKernelApp.connection_file else 15","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Training will be stopped once accuracy reaches 97%, and epochs are limited to max 15.","metadata":{}},{"cell_type":"code","source":"class accCallback(tf.keras.callbacks.Callback):\n  def on_epoch_end(self, epoch, logs={}):\n    if(logs.get('accuracy')>0.97):\n      print(\"--> Callback: Reached 97% accuracy. Cheers and relax! <--\")\n      self.model.stop_training = True\n\naccCallbacks = accCallback()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# NLP: Sentiment Analysis","metadata":{}},{"cell_type":"markdown","source":"Use in-memory dataset IMDB review. The dataset is also available from tensorflow_datasets\n\nKeywords: NLP, recurrent neural network, RNN, tokenize, embedding, Long Short Term Memory, LSTM,Gated Recurring Units, GRU\n\nReference: [Colab1](https://colab.research.google.com/github/lmoroney/dlaicourse/blob/master/TensorFlow%20In%20Practice/Course%203%20-%20NLP/Course%203%20-%20Week%203%20-%20Lesson%201c.ipynb) | [Colab2](https://colab.research.google.com/github/lmoroney/dlaicourse/blob/master/TensorFlow%20In%20Practice/Course%203%20-%20NLP/NLP%20Course%20-%20Week%203%20Exercise%20Answer.ipynb) | [Colab3](https://colab.research.google.com/github/lmoroney/dlaicourse/blob/master/TensorFlow%20In%20Practice/Course%203%20-%20NLP/Course%203%20-%20Week%203%20-%20Lesson%201b.ipynb)","metadata":{}},{"cell_type":"code","source":"imdb = pd.read_csv('../input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv')\nimdb_text= imdb['review']\nimdb_label = imdb['sentiment'].map(lambda x: 1 if x=='positive' else 0)\nlen(imdb_text), len(imdb_label)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Tokenizer and padding sequences\n\nPadded sequences can be tuned to smaller size using `maxlen`, the longest will be used if default (which is not necessary)","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(imdb_text)\nimdb_sequences = tokenizer.texts_to_sequences(imdb_text)\nimdb_padded = pad_sequences(imdb_sequences, maxlen=1000)\n\nword_index = tokenizer.word_index\nvocab_size = len(word_index) + 1 # note for +1\nvocab_size","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Embedding, Conv, GRU or LSTM?\n\nIt is very likely to get over-fitting with LSTM (or generally in NLP). Remeber Embedding size needs +1.","metadata":{}},{"cell_type":"code","source":"model1 = Sequential([\n    layers.Embedding(vocab_size, 64),\n    layers.Dropout(0.2),\n    layers.Conv1D(128, 5, activation='relu'),\n    layers.MaxPooling1D(pool_size=4),\n    # LSTM or GRU? This model seems to prefere GRU\n    # layers.LSTM(64),\n    layers.Bidirectional(layers.GRU(64)),\n    layers.Dense(1, activation='sigmoid')\n])\n\nmodel1.compile(optimizer='adam',loss='binary_crossentropy', metrics=['accuracy'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hisfit1=model1.fit(\n    imdb_padded, \n    imdb_label,\n    epochs=max_epochs, \n    validation_split=0.2,\n    callbacks=[accCallbacks], \n    verbose=opt_verbose,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mytext = [\"The movie is really crappy. It lacks contents and the story is old-fashioned\", \n          \"A reasonably attractive storyline. Scenes are nicely crafted and the ending is ok\"]\nx = tokenizer.texts_to_sequences(mytext)\nx = pad_sequences(sequences=x, maxlen=len(imdb_padded[0]))\nmodel1.predict(x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Pretrained Embeddings - GloVe","metadata":{}},{"cell_type":"markdown","source":"Example to use [GloVe: Global Vectors for Word Representation](https://nlp.stanford.edu/projects/glove/) from Stanford, For simplicity and performance, the Kaggle copy is used as input.\n\n> Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. GloVe: Global Vectors for Word Representation.\n\nThe tokenized and padded sequences are resued from above section.\n\nReferences: [Colab1](https://colab.research.google.com/github/lmoroney/dlaicourse/blob/master/TensorFlow%20In%20Practice/Course%203%20-%20NLP/Course%203%20-%20Week%203%20-%20Lesson%202d.ipynb#scrollTo=5NEpdhb8AxID)","metadata":{}},{"cell_type":"markdown","source":"## Processing GloVe txt data and mapped to tokenized sequences","metadata":{}},{"cell_type":"markdown","source":"The tokenized and padded sequences are resued from above section.","metadata":{}},{"cell_type":"code","source":"embedding_dim = 100; # depends on which GloVe is used\nembeddings_index = {};\nwith open('../input/glove6b/glove.6B.100d.txt') as f:\n    for line in f:\n        values = line.split();\n        word = values[0];\n        coefs = np.asarray(values[1:], dtype='float32');\n        embeddings_index[word] = coefs;\n\nembeddings_matrix = np.zeros((vocab_size, embedding_dim));\nfor word, i in word_index.items():\n    embedding_vector = embeddings_index.get(word);\n    if embedding_vector is not None:\n        embeddings_matrix[i] = embedding_vector;\n        \n        \nf\"the size should be equal to vocab {len(embeddings_matrix)} / {vocab_size}\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Add non-trainable embedding layer","metadata":{}},{"cell_type":"code","source":"model2 = Sequential([\n    layers.Embedding(vocab_size, embedding_dim, input_length=1000, weights=[embeddings_matrix], trainable=False),\n    layers.Dropout(0.2),\n    layers.Conv1D(64, 5, activation='relu'),\n    layers.MaxPooling1D(pool_size=4),\n    layers.LSTM(64),\n    layers.Dense(1, activation='sigmoid')\n])\n\nmodel2.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n\nhisfit2=model2.fit(\n    imdb_padded, \n    imdb_label,\n    epochs=max_epochs, \n    validation_split=0.2,\n    callbacks=[accCallbacks], \n    verbose=opt_verbose,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# NLP: predict next words","metadata":{}},{"cell_type":"markdown","source":"This is a toy example with very small training dataset. Blackpink Ice Cream.\n\nReference: [Colab1](https://colab.research.google.com/github/lmoroney/dlaicourse/blob/master/TensorFlow%20In%20Practice/Course%203%20-%20NLP/Course%203%20-%20Week%204%20-%20Lesson%201%20-%20Notebook.ipynb#scrollTo=Atey4zDdR0_C) | [Colab2](https://colab.research.google.com/github/lmoroney/dlaicourse/blob/master/TensorFlow%20In%20Practice/Course%203%20-%20NLP/Course%203%20-%20Week%204%20-%20Lesson%202%20-%20Notebook.ipynb#scrollTo=6Vc6PHgxa6Hm) | [Colab3](https://colab.research.google.com/github/lmoroney/dlaicourse/blob/master/TensorFlow%20In%20Practice/Course%203%20-%20NLP/NLP_Week4_Exercise_Shakespeare_Answer.ipynb)","metadata":{}},{"cell_type":"code","source":"ice_cream = \"Come a little closer 'cause you looking thirsty\\nI'ma make it better, sip it like a Slurpee\\nSnow cone chilly\\nGet it free like Willy\\nIn the jeans like Billie\\nYou be poppin' like a wheelie\\nEven in the sun, you know I keep it icy\\nYou could take a lick but it's too cold to bite me\\nBrr, brr, frozen\\nYou're the one been chosen\\nPlay the part like Moses\\nKeep it fresh like roses (oh)\\nLook so good yeah, look so sweet (hey)\\nLookin' good enough to eat\\nColdest with the kiss, so he call me ice cream\\nCatch me in the fridge, right where the ice be\\nLook so good yeah, look so sweet (hey)\\nBaby, you deserve a treat\\nDiamonds on my wrist, so he call me ice cream\\nYou can double dip 'cause I know you like me\\nIce cream, chillin', chillin'\\nIce cream, chillin'\\nIce cream, chillin', chillin'\\nIce cream, chillin'\\nI know that my heart can be so cold\\nBut I'm sweet for you, come put me in a cone\\nYou're the only touch, yeah, that get me meltin'\\nHe's my favorite flavor, always gonna pick him\\nYou're the cherry piece, just stay on top of me, so\\nI can't see nobody else for me, no\\nGet it, flip it, scoop it\\nDo it like that, ah yeah ah yeah\\nLike it, love it, lick it\\nDo it like la la la, oh yeah\\nLook so good, yeah, look so sweet (hey)\\nLookin' good enough to eat\\nColdest with the kiss, so he call me ice cream\\nCatch me in the fridge, right where the ice be\\nLook so good, yeah, look so sweet (hey)\\nBaby, you deserve a treat\\nDiamonds on my wrist, so he call me ice cream\\nYou can double dip 'cause I know you like me\\nIce cream, chillin', chillin'\\nIce cream, chillin'\\nIce cream, chillin', chillin'\\nIce cream, chillin'\\nIce cream, chillin', chillin'\\nIce cream, chillin'\\nIce cream, chillin', chillin'\\nIce cream\\nKeep it movin' like my lease up\\nThink you fly, boy, where your visa?\\nMona Lisa kinda Lisa\\nNeeds an ice cream man that treats her\\nKeep it movin' like my lease up\\nThink you fly, boy, where your visa?\\nMona Lisa kinda Lisa\\nNeeds an ice cream man that treats her (hey)\\nNa na na na na\\nNa na na na na (hey)\\nIce on my wrist, yeah, I like it like this\\nGet the bag with the cream\\nIf you know what I mean\\nIce cream, ice cream\\nIce cream, chillin'\\nNa na na na na\\nNa na na na na (hey)\\nIce on my wrist, yeah, I like it like this\\nAnd I'm nice with the cream\\nIf you know what I mean\\nIce cream, ice cream\\nIce cream\"\nice_cream = ice_cream.lower().split('\\n')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = Tokenizer()\ntokenizer.fit_on_texts(ice_cream)\nvocab_size = len(tokenizer.word_index) + 1 #added one!\n\nice_cream_seq = tokenizer.texts_to_sequences(ice_cream)\nice_cream_pad = pad_sequences(ice_cream_seq, padding='pre')\n\nf\"Vocab size={vocab_size}; Padded sequence maxlen={len(ice_cream_pad[0])}\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Determining features and labels","metadata":{}},{"cell_type":"code","source":"xs, labels = np.asarray(ice_cream_pad[:,:-1]), ice_cream_pad[:,-1]\nys = tf.keras.utils.to_categorical(labels, num_classes = vocab_size)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model3=Sequential([\n    layers.Embedding(vocab_size, 64),\n    layers.Bidirectional(layers.LSTM(20)),\n    layers.Dense(vocab_size, activation='softmax')\n])  \n\nmodel3.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# output is too large, use plot instead of verbose print\nhisfit3 = model3.fit(xs, ys, epochs=500, verbose=0)\nplt.plot(hisfit3.history['accuracy'])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Serving Prediction","metadata":{}},{"cell_type":"code","source":"seed_text = \"This is Blackpink Ice Cream\"\n  \nfor _ in range(20):\n    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n    token_list = pad_sequences([token_list], maxlen=len(ice_cream_pad[0])-1)\n    predicted = model3.predict(token_list, verbose=0)\n    predicted = [np.argmax(predicted)]\n    output_word = tokenizer.sequences_to_texts([predicted])\n    seed_text += \" \" + output_word[0]\nprint(seed_text)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Time Series","metadata":{}},{"cell_type":"markdown","source":"A simulated data with [0,1,2,3,...,9]; five windows are taken each has [n+1,n+2,...,n+5], and set the first 4 to be feature, last to be label. Finally, it is shuffled","metadata":{}},{"cell_type":"code","source":"dataset = tf.data.Dataset.range(10)\ndataset = dataset.window(5, shift=1, drop_remainder=True)\ndataset = dataset.flat_map(lambda window: window.batch(5))\ndataset = dataset.map(lambda window: (window[:-1], window[-1:]))\ndataset = dataset.shuffle(buffer_size=10)\ndataset = dataset.batch(2).prefetch(1)\n[print(f\"x={x.numpy()}\\ny={y.numpy()}\") for x,y in dataset]","metadata":{},"execution_count":null,"outputs":[]}]}