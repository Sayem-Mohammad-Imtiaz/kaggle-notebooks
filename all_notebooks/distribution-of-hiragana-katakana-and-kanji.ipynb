{"nbformat":4,"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"file_extension":".py","name":"python","codemirror_mode":{"name":"ipython","version":3},"nbconvert_exporter":"python","version":"3.6.1","mimetype":"text/x-python","pygments_lexer":"ipython3"}},"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"b0701800-a04a-4f22-b717-33e78af92974","_uuid":"72ef63d1d5c9b57f7d7170b5a1e10022fde4c83b"},"source":"Charaguana Comes to the Rescue\n===="},{"cell_type":"markdown","metadata":{"_cell_guid":"20fb63c3-dac6-4cb6-8275-54b155966273","_uuid":"57e715302f3093302d1636939882f7a30962c5b9"},"source":"To get find the distribution of `hiragana`, `katakata` and `kanji` within the list of lemmas, we can use the `charguana` library."},{"cell_type":"markdown","metadata":{"_cell_guid":"db054ba8-c521-40a3-b1bb-35ab284bc2bb","_uuid":"f932743fe4ebe6bbc37cff7bff6f95da1a593e5d"},"source":"[`Charaguana`](https://github.com/alvations/charguana) is a \"character vommitting\" library, i.e. it contains the manually labelled character sets that one would find useful when dealing with human language orthography in unicode."},{"cell_type":"markdown","metadata":{"_cell_guid":"77250af4-63de-4117-b22a-dd14399299e3","_uuid":"ff97ca70fc524aae3286fc24a2aa82f574d37c5c"},"source":"First let's retreive the relevant Japanese character sets.\n"},{"cell_type":"markdown","metadata":{"_cell_guid":"d84677c3-575c-4cac-8b96-77005a1ba318","_uuid":"7b455eda6239f382d5697f85b8c2bb369554f0b0"},"source":"```python\nfrom charguana import get_charset\n\n# Syllables. \nhiragana = set(get_charset('hiragana'))\nkatakana = set(get_charset('katakana'))\n\n# Punctuations.\ncjk_punct = set(get_charset('cjk_punctuation'))\n\n# Kanji characters are other characters in the \"japanese\" set.\nkanji = set(get_charset('japanese')) - hiragana - katakana - cjk_punct\n```"},{"cell_type":"markdown","metadata":{"_cell_guid":"1bcc3fbd-a96b-4a95-8460-872f73435902","_uuid":"f96afa074a8ebe721b1bee6295d607058fbf7851"},"source":"**Note:** Sadly, it's not supported by Kaggle kernels so we'll have to plug-in the lists manually"},{"cell_type":"code","metadata":{"_cell_guid":"78b99991-18e7-4a8b-bde9-88fd0e128747","_uuid":"6dbcd163727c15adc2a288e00478602ea2a1c399","collapsed":true},"outputs":[],"source":"hiragana = {'え', 'ぁ', 'で', 'め', 'を', 'こ', 'い', '゙', '゜', 'あ', 'ゎ', 'ぺ', 'げ', 'る', 'ず', 'ち', 'ぐ', 'ゖ', 'ゆ', 'ゞ', 'ご', 'ね', 'の', 'ゑ', 'し', 'っ', 'び', 'ぶ', 'へ', 'わ', 'ほ', 'ゅ', 'さ', 'べ', 'ぜ', 'も', 'だ', 'ぱ', 'ぇ', 'ろ', 'ゕ', 'く', 'ま', 'ば', 'ざ', 'は', 'お', 'ぼ', 'ゔ', 'よ', 'ぞ', 'つ', 'が', '\\u3097', 'り', 'そ', 'ん', 'ら', 'ゝ', 'れ', 'ぃ', 'ぅ', 'む', 'ぴ', '゛', 'す', 'ぽ', 'み', 'か', 'や', 'ゐ', 'ひ', 'ぷ', 'け', 'せ', '\\u3098', 'て', 'ゃ', 'づ', 'ふ', 'ょ', 'じ', '゚', '\\u3040', 'ゟ', 'ぉ', 'ぢ', 'な', 'に', 'た', 'ぎ', 'ぬ', 'ど', 'と', 'き', 'う'}\nkatakana = {'ヅ', 'プ', 'ヂ', 'ァ', 'ヵ', 'ヿ', 'ブ', 'ョ', 'ム', 'ポ', 'エ', 'ノ', 'カ', 'シ', 'ュ', 'モ', 'ナ', 'ト', 'ェ', 'ロ', 'ハ', 'オ', 'ヒ', 'ホ', 'ィ', 'ペ', 'コ', '・', '゠', 'ヶ', 'ク', 'メ', 'ギ', 'ゼ', 'ユ', 'パ', 'ビ', 'ソ', 'ピ', 'ヲ', 'ス', 'ゾ', 'ン', 'ヤ', 'リ', 'ォ', 'ッ', 'ウ', 'ツ', 'ザ', 'グ', 'ベ', 'フ', 'ヘ', 'ニ', 'ジ', 'ゥ', 'キ', 'セ', 'ヱ', 'ャ', 'ヽ', 'ケ', 'ゴ', 'ヾ', 'ラ', 'ヌ', 'タ', 'ヺ', 'サ', 'ヴ', 'ダ', 'レ', 'ヮ', 'テ', 'ヰ', 'ガ', 'ヹ', 'ヷ', 'マ', 'ミ', 'ー', 'ア', 'デ', 'ボ', 'ド', 'チ', 'ゲ', 'イ', 'バ', 'ル', 'ネ', 'ズ', 'ヨ', 'ヸ', 'ワ'}","execution_count":1},{"cell_type":"markdown","metadata":{"_cell_guid":"bd94bd5e-548f-417d-b9d0-642f1eed0caf","_uuid":"0d9734ed600f746acae57e0197c087e308e2e8fa"},"source":"To check whether all characters in string are of the same character set, we can loop through each character can check it as such:"},{"cell_type":"code","metadata":{"_cell_guid":"f5e380da-6a69-4ead-ad19-d3960f199db0","_uuid":"7809f9ff102372da3907db460ae1dd4a2ae7fe9c"},"outputs":[],"source":"# E.g. we want to check whether ある is hiragana.\ns = 'ある'\nall(True for ch in s if ch in hiragana)","execution_count":2},{"cell_type":"markdown","metadata":{"_cell_guid":"f377e75e-e539-448d-b364-157e90f54258","_uuid":"21708f65aa9c18db1e81d477c747d53bb6ad0ae1"},"source":"Alternatively, we can simply check the first character and assumes that a lemma doesn't contain mixed script:"},{"cell_type":"code","metadata":{"_cell_guid":"0fb4f092-9343-42b3-aef3-a2740fd9e7e6","_uuid":"df0805299cb3ae8c8f7ded8733a1b5a526d05ff9"},"outputs":[],"source":"s = 'ある'\ns[0] in hiragana","execution_count":3},{"cell_type":"markdown","metadata":{"_cell_guid":"937e5153-e231-4e86-b4c2-2bc8f3d4666f","_uuid":"b74867e1643238c8fdf34aa9fa6b8a409931770f"},"source":"Another way is to check for whether the intersection between all characters in string against the character set, e.g."},{"cell_type":"code","metadata":{"_cell_guid":"da7fad00-6abf-4d1d-aefd-4bfb6fa89bc4","_uuid":"76a5eb0d5155a5c7c4bf64efcc552ffe346860a6"},"outputs":[],"source":"len(set(s).intersection(hiragana)) == len(s)","execution_count":4},{"cell_type":"markdown","metadata":{"_cell_guid":"b80c6fde-fee4-49d7-9308-0584d065a051","_uuid":"6520f965578eadfb4f55c0622f5bf6db129ef079","collapsed":true},"source":"Phew, now the heavy lifting is over, let's count!"},{"cell_type":"code","metadata":{"_cell_guid":"d41bb1e0-2b12-4596-ab81-1220c3b807c7","_uuid":"a9e413fe4a3e8f61e13a4518ee3fd5293125fd21"},"outputs":[],"source":"from collections import defaultdict\n\nimport pandas as pd\n\ndf = pd.read_csv('../input/japanese-lemma-frequency/japanese_lemmas.csv')\ndf.head()","execution_count":5},{"cell_type":"markdown","metadata":{"_cell_guid":"339c98a4-2391-4623-8c88-7ed0496965e9","_uuid":"6c62657c0bb746891e402d5b1181da20acdba51c"},"source":"If it's not hiragana/katana, we can treat it as a kanji."},{"cell_type":"code","metadata":{"_cell_guid":"f77fef35-b760-4cd0-a6dc-532438404e80","_uuid":"edab8aa9457454ba7d719cb1aa61f55e9b7adfd4","collapsed":true},"outputs":[],"source":"def is_charset(s, charset):\n    return len(set(s).intersection(charset)) == len(s)\n\n# We'll store the charset as the keys\n# and list of row index as the values.\ncharset2idx = defaultdict(list)\n\nfor idx, row in df.iterrows():\n    lemma = row['lemma']\n    if is_charset(lemma, hiragana):\n        k = 'hiragana'\n    elif is_charset(lemma, katakana):\n        k = 'katakana'\n    else: # i.e. Kanji.\n        k = 'kanji'\n    charset2idx[k].append(idx)","execution_count":6},{"cell_type":"code","metadata":{"_cell_guid":"457073ea-05a7-4eb7-8ec0-b1cf1641f4e0","_uuid":"ab0e4898c526026d9f4cdfccbde4ed2643fe5f62"},"outputs":[],"source":"num_lemmas = len(df)\nprint(len(charset2idx['hiragana']), 'out of', num_lemmas, 'are hirgana.')\nprint(len(charset2idx['katakana']), 'out of', num_lemmas, 'are katakana.')\nprint(len(charset2idx['kanji']), 'out of', num_lemmas, 'are kanji.')","execution_count":7},{"cell_type":"markdown","metadata":{"_cell_guid":"d0f1b57e-e2d1-41f3-9109-34590c92a854","_uuid":"56a8e185fc98caa12fdce9b5b765c56b329d6fe4"},"source":"Numbers are nice but a picture tells 15,000 words ;P"},{"cell_type":"code","metadata":{"_cell_guid":"ffcaec6e-6743-4acc-a969-d6276309d65c","_uuid":"68239e5e1f28ee0d6286cf49d3f0dc0dfb6262bd"},"outputs":[],"source":"import matplotlib.pyplot as plt\nfrom matplotlib import rc\n \n# Data to plot\nlabels = 'Hiragana', 'Katakana', 'Kanji', \nsizes = [len(charset2idx['hiragana']),\n         len(charset2idx['katakana']),\n         len(charset2idx['kanji']), \n        ]\ncolors = ['lightcoral', 'yellowgreen', 'lightskyblue']\nexplode = (0.2, 0.1, 0.1)  # explode 1st slice\n \n# Plot\nplt.pie(sizes, explode=explode, labels=labels, colors=colors,\n        autopct='%1.1f%%', shadow=True, startangle=140)\nplt.axis('equal')\nplt.rcParams[\"figure.figsize\"] = [16,9]\n\nfont = {'size': 22}\n\nrc('font', **font)\nplt.show()","execution_count":8},{"cell_type":"markdown","metadata":{"_cell_guid":"3cf71741-a413-445f-9eea-9cb8d2edcd58","_uuid":"4a22c551260090aa12850d6e1d1fcabf46115a75"},"source":"## ちょっと待ってください (*wait a minute*), what if there lemmas with mixed character sets?\n\nLet's go through every character in each lemma and count them =)\n"},{"cell_type":"code","metadata":{"_cell_guid":"dd3eabba-b2e0-4c3b-a6b3-96c55b68cf7f","_uuid":"d0b970a39de809b49a9a2eab9d42939176396f7f","collapsed":true},"outputs":[],"source":"from collections import Counter\n\nidx2charset = defaultdict(Counter)\n\nfor idx, row in df.iterrows():\n    lemma = row['lemma']\n    # Iterate through each character in lemma.\n    for ch in lemma:\n        if ch in hiragana:\n            idx2charset[idx]['Hiragana'] += 1\n        elif ch in katakana:\n            idx2charset[idx]['Katakana'] += 1\n        else:\n            idx2charset[idx]['Kanji'] += 1","execution_count":9},{"cell_type":"markdown","metadata":{"_cell_guid":"c7f0404d-6d93-4f03-bdce-28b698a7eb84","_uuid":"33bc8ef18b618eeb2cd7fcad968a33f4c687a0a9"},"source":"So it seems that there are mixed scripts lemmas!"},{"cell_type":"code","metadata":{"_cell_guid":"5d25b6cc-6103-485e-8376-0299353a0ced","_uuid":"0b5e2f712c8c3dc943ba069fdba9187de75952ec"},"outputs":[],"source":"next((idx, idx2charset[idx]) for idx in idx2charset if len(idx2charset[idx]) > 1)","execution_count":10},{"cell_type":"code","metadata":{"_cell_guid":"720e9cd8-ada9-447b-ae4f-af248d1688f8","_uuid":"947af4c4daecbdd2aefd765c64eab3af373805b7"},"outputs":[],"source":"df.iloc[43]","execution_count":11},{"cell_type":"markdown","metadata":{"_cell_guid":"88821480-7de8-47a9-b95b-e42817fec858","_uuid":"794c4dc1164043eeae241e1c49d13af87365cb26"},"source":"Ah ha, we see words like **思う** where there's kanji mixed with hiragana!"},{"cell_type":"code","metadata":{"_cell_guid":"7132ce03-56cf-41c1-8c0d-29384c990824","_uuid":"873a63c262409125b44c425a7f24e48f64a57847"},"outputs":[],"source":"Counter(' + '.join(sorted(charset_in_lemma.keys())) \n        for idx, charset_in_lemma in idx2charset.items())","execution_count":12},{"cell_type":"code","metadata":{"_cell_guid":"15682a67-aefd-4530-bebf-e6fa5e173934","_uuid":"539a9a2a4360444fb194a63303d7f0dd445c6474"},"outputs":[],"source":"charset_counter = Counter(' + '.join(sorted(charset_in_lemma.keys())) \n                          for idx, charset_in_lemma in idx2charset.items())\n\nnum_lemmas = len(df)\nfor cs, count in charset_counter.most_common():\n    print(count, 'out of', num_lemmas, 'are', cs)","execution_count":13},{"cell_type":"code","metadata":{"_cell_guid":"d8a1f17d-eb9b-4e48-8cac-e6f5d313965e","_uuid":"05cee52021335416c39f9eefbc5f8eddba6b9516"},"outputs":[],"source":"import matplotlib.pyplot as plt\nfrom matplotlib import rc\n \n# Close the previous plot\nplt.close()\n\n# Data to plot\nlabels, sizes = zip(*charset_counter.most_common())\n\n# Red = Hiragana\n# Green = Katakana\n# Blue = Kanji\n# Purple = Kanji + Hiragana\n# Yellow = Hiragana + Katakana\n# Cyan = Kanji + Katakana\n\ncolors = ['lightskyblue', 'orchid', 'yellowgreen', \n          'lightcoral',  'cyan', 'yellow']\nexplode = (0.1, 0.1, 0.1, 0.1, 0.1, 0.0 )  # explode 1st slice\n \n# Plot\nplt.pie(sizes, explode=explode, labels=labels, colors=colors,\n        autopct='%1.1f%%', shadow=False, startangle=120)\nplt.axis('equal')\nplt.rcParams[\"figure.figsize\"] = [14,7]\n\nfont = {'size': 12}\n\nrc('font', **font)\nplt.show()","execution_count":14},{"cell_type":"markdown","metadata":{"_cell_guid":"9955232c-82ab-41bd-a8d4-f40f6dede186","_uuid":"db5d70a9b842372e0bb82c9028893b22747f149a","collapsed":true},"source":""},{"cell_type":"markdown","metadata":{"_cell_guid":"b621758c-4b1d-46cf-b13b-f279282ade1b","_uuid":"dfba339226b5b40d47a43ba717744a621ad4629c","collapsed":true},"source":"It seems like the katakana+kanji and katakana+hiragana lemmas are of very small percentage.\n\nLet's take a closer look at them. "},{"cell_type":"code","metadata":{"_cell_guid":"dc8f319a-1cc8-45a2-aff8-f9cf622e9c27","_uuid":"32d01533f9cfa6969bb68d33066d071fbb9c5452","collapsed":true},"outputs":[],"source":"charset2idx = defaultdict(list)\n\nfor idx, charset_in_lemma in idx2charset.items():\n    _charset = ' + '.join(sorted(charset_in_lemma.keys())) \n    charset2idx[_charset].append(idx)","execution_count":15},{"cell_type":"markdown","metadata":{"_cell_guid":"e7bfa5ac-6192-45c9-b5e1-377c0b025abf","_uuid":"1a3e69bbe66198caf7d1306f1843b2cb10ec5ae7"},"source":"Katana + Kanji lemmas\n===="},{"cell_type":"code","metadata":{"_cell_guid":"21a846da-bbf1-4a36-ba0c-d3a57b1549f5","_uuid":"bb196cd79831c9a3cba817ffc9f87b0381e4cb21"},"outputs":[],"source":"for idx in charset2idx['Kanji + Katakana']:\n    print(df.iloc[idx]['lemma'])","execution_count":16},{"cell_type":"markdown","metadata":{"_cell_guid":"efd5493e-397e-462a-b843-23464d00c106","_uuid":"8396a4c3c057636be9f488b5fe4e76b62a0938cb"},"source":"Katakana + Hiragana lemmas\n===="},{"cell_type":"code","metadata":{"_cell_guid":"41f274c1-9b11-4aa8-9aeb-1f73756102c3","_uuid":"0517ed900e65e871b9c63e2e9f45362c426a4273"},"outputs":[],"source":"for idx in charset2idx['Hiragana + Katakana']:\n    print(df.iloc[idx]['lemma'])","execution_count":17},{"cell_type":"markdown","metadata":{"_cell_guid":"a3c34f19-3342-4e5e-8532-fe28e0ffa0cb","_uuid":"d377fbcca7d80321bb73e5a7f690b73dc204489f"},"source":"The *ー* charset isn't really katana in these lemmas, they are the lengthen vowels of the previous hiragana syllable. "},{"cell_type":"markdown","metadata":{"_cell_guid":"febb35ae-1806-484a-8394-2e1bfd71139a","_uuid":"e152d12c51c4b5e7e904d42b1c9d1e8937ec046b"},"source":"Are kanji really kanjis? \n====\n\nIf take a look at the first 20 kanji characters, it seems like some of are romanji and punctuations!"},{"cell_type":"code","metadata":{"_cell_guid":"d520e47e-67ce-4f32-969d-2423c501429e","_uuid":"526dd036f5267b3d6fbe48648623cbf0aca06cfd"},"outputs":[],"source":"for idx in charset2idx['Kanji'][:20]:\n    print(df.iloc[idx]['lemma'])","execution_count":18},{"cell_type":"markdown","metadata":{"_cell_guid":"d56197bd-3889-48bf-89eb-47ce4e3ba211","_uuid":"aa1ed958fae87185b95ad069c72e0ddffe7bd81c","collapsed":true},"source":"Let's redo the counting by cheating a little, I've pickled the character sets from charguana and uploaded it as a Kaggle dataset."},{"cell_type":"code","metadata":{"_cell_guid":"c4aba59b-fe4c-40d5-adf3-a59c366799b8","_uuid":"d6004a505c7d53cec5beb06b3060ee2fdd24a8cc","collapsed":true},"outputs":[],"source":"import pickle\nwith open('../input/charguana/japanese.pkl', 'rb') as fin:\n    japanese = pickle.load(fin)\n\nkatakana = set(japanese['katakana'])\nhiragana = set(japanese['hiragana'])\nkanji = set(japanese['kanji'])\nromanji = set(japanese['romanji'])","execution_count":19},{"cell_type":"code","metadata":{"_cell_guid":"6e63785c-00de-4c31-8849-20807157b2fc","_uuid":"110397b34bbb065c9b18af33b79659766269b66c","collapsed":true},"outputs":[],"source":"from collections import Counter\n\nidx2charset = defaultdict(Counter)\notherchars = set()\nfor idx, row in df.iterrows():\n    lemma = row['lemma']\n    # Iterate through each character in lemma.\n    for ch in lemma:\n        if ch in hiragana:\n            idx2charset[idx]['Hiragana'] += 1\n        elif ch in katakana:\n            idx2charset[idx]['Katakana'] += 1\n        elif ch in romanji:\n            idx2charset[idx]['Romanji'] += 1\n        elif ch in kanji:\n            idx2charset[idx]['Kanji'] += 1\n        else:\n            otherchars.add(ch)","execution_count":20},{"cell_type":"markdown","metadata":{"_cell_guid":"8a3d9302-5e75-4ac3-a66e-52af7fa26429","_uuid":"18d7bf9b83fe711641a1822eaca762b47095bafa"},"source":"It seems like we're still letting some slip through the gaps...\nLooking at each one of the `otherchars`, they seem to fall into the **romanji** category."},{"cell_type":"code","metadata":{"_cell_guid":"05bcff4f-c654-473d-a72b-51d5fd898a8c","_uuid":"586acd98da052e6417c6a76cd6b518e373dcc184"},"outputs":[],"source":"''.join(sorted(otherchars))","execution_count":21},{"cell_type":"markdown","metadata":{"_cell_guid":"1d9dff15-78fd-4a3e-b260-7c888894ab4e","_uuid":"50f8a7815f2a0acf972cc212ffec0bd940b02379"},"source":"Let's add them to **romanji** and recount again."},{"cell_type":"code","metadata":{"_cell_guid":"5d659432-d312-4d67-8a26-540302816f99","_uuid":"21bcc6d34dfbc1a2b27dcc3dc701fa6450205539","collapsed":true},"outputs":[],"source":"romanji = set(japanese['romanji']).union(otherchars)","execution_count":22},{"cell_type":"code","metadata":{"_cell_guid":"e38685c1-4ff2-4f2a-ab00-433d9b739ba1","_uuid":"69cc5350994874b713bbef22887b92a8e40b766f","collapsed":true},"outputs":[],"source":"idx2charset = defaultdict(Counter)\notherchars = set()\nfor idx, row in df.iterrows():\n    lemma = row['lemma']\n    # Iterate through each character in lemma.\n    for ch in lemma:\n        if ch in hiragana:\n            idx2charset[idx]['Hiragana'] += 1\n        elif ch in katakana:\n            idx2charset[idx]['Katakana'] += 1\n        elif ch in romanji:\n            idx2charset[idx]['Romanji'] += 1\n        elif ch in kanji:\n            idx2charset[idx]['Kanji'] += 1\n        else: \n            # Now, we should have caught everything so \n            # there shouldn't be anything falling in these gaps. \n            print(ch)","execution_count":23},{"cell_type":"code","metadata":{"_cell_guid":"d48c61cc-1dfb-4575-ab98-e33242b83c3b","_uuid":"5a99048621325130e75b8844cfb812242d0bd565"},"outputs":[],"source":"charset_counter = Counter(' + '.join(sorted(charset_in_lemma.keys())) \n                          for idx, charset_in_lemma in idx2charset.items())\n\nnum_lemmas = len(df)\nfor cs, count in charset_counter.most_common():\n    print(count, 'out of', num_lemmas, 'are', cs)","execution_count":24},{"cell_type":"markdown","metadata":{"_cell_guid":"4bffbe33-7823-4c34-a7f7-8da347addab4","_uuid":"e8431696195c5a1a2ad415da7dc774c1173cd86c","collapsed":true},"source":"Now, that's kind of messy with so many slices of the pie.\n\n Except for  **Hiragana+Kanji**, let's group the other mixed scripts as **Others** since they're <1."},{"cell_type":"code","metadata":{"_cell_guid":"1b9544bb-bca5-4508-acc6-d45d57e91fe7","_uuid":"34709abd5bf9a6f025fbc4c7daf6acd9279795c0"},"outputs":[],"source":"import matplotlib.pyplot as plt\nfrom matplotlib import rc\n \n# Close the previous plot\nplt.close()\n\n# Data to plot\nlabels, sizes = zip(*charset_counter.most_common()[:5])\n_, size_of_others = zip(*charset_counter.most_common()[5:])\n\n# Added the label and counts of 'Others'\nlabels = ['Others'] + list(labels) \nsizes = [sum(size_of_others)] + list(sizes)\n\n# Blue = Kanji\n# Purple = Kanji + Hiragana\n# Green = Katakana\n# Red = Hiragana\n# Grey = Romanji\n# Light Grey = Others\n\ncolors = ['gainsboro', 'lightskyblue', 'orchid', 'yellowgreen', \n          'lightcoral',  'silver']\nexplode = (0.0, 0.1, 0.1, 0.1, 0.1, 0.1 )  # explode 1st slice\n \n# Plot\nplt.pie(sizes, explode=explode, labels=labels, colors=colors,\n        autopct='%1.1f%%', shadow=False, startangle=120)\nplt.axis('equal')\nplt.rcParams[\"figure.figsize\"] = [14,7]\n\nfont = {'size': 12}\n\nrc('font', **font)\nplt.show()","execution_count":25},{"cell_type":"markdown","metadata":{"_cell_guid":"83a3bfdb-ef69-4b2a-bb73-c3104b3750a1","_uuid":"34c43a1a181079dad0824da3a630026d8bd837e2","collapsed":true},"source":"Going back to the DataFrame\n====\n\nLet's added the counts of the characters per lemma back into the original dataframe."},{"cell_type":"code","metadata":{},"outputs":[],"source":"df.head()","execution_count":26},{"cell_type":"code","metadata":{"collapsed":true},"outputs":[],"source":"# Initialize the new columns with zeros\ndf['#Katakana'] = 0\ndf['#Hiragana'] = 0\ndf['#Kanji'] = 0\ndf['#Romanji'] = 0","execution_count":31},{"cell_type":"code","metadata":{},"outputs":[],"source":"df.head()","execution_count":32},{"cell_type":"code","metadata":{},"outputs":[],"source":"for idx, row in df.iterrows():\n    lemma = row['lemma']\n    for ch in lemma:\n        if ch in hiragana:\n            df.iloc[idx, df.columns.get_loc('#Hiragana')] += 1\n        elif ch in katakana:\n            df.iloc[idx, df.columns.get_loc('#Katakana')] += 1\n        elif ch in romanji:\n            df.iloc[idx, df.columns.get_loc('#Romanji')] += 1\n        elif ch in kanji:\n            df.iloc[idx, df.columns.get_loc('#Kanji')] += 1","execution_count":33},{"cell_type":"code","metadata":{},"outputs":[],"source":"df.head()","execution_count":34}],"nbformat_minor":1}