{"cells":[{"metadata":{"_uuid":"2069027e814059d61dd25521ae9975a3936b3c77","_cell_guid":"1be39542-723c-4424-82fc-073a425287f9","trusted":true},"cell_type":"code","source":"#Here are some standard libraries that are loaded when you \nimport cv2\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt # visualize satellite images\nimport matplotlib.patches as patches\nimport matplotlib.colors as mcolors\n\nfrom skimage.io import imshow # visualize satellite images\n\nfrom keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout # components of network\nfrom keras.models import Sequential # type of model\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"import sklearn\nfrom sklearn.metrics import confusion_matrix, classification_report\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"from skimage.filters import gaussian\nfrom scipy.interpolate import interp1d\nimport skimage\nimport pylab as pl\nimport scipy.signal\nfrom scipy import misc","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"41076d58fa5e54474aea8822ca6dbd3f3f6ff2cd","_cell_guid":"b3e9101f-1de5-4fdd-b865-b6664eb9038d"},"cell_type":"markdown","source":"## Get Input Data\nThe input data was encoded into CSV files. The X_test_sat4.csv flattened the images that were 28 x 28 x 4 that were taken from space. The first three channels are the standard red, green, and blue channels in normal images. The 4th is a near-infrared band. We are using the smaller test set because the training set is too big.\nAfter extracting the data from the csv files, we can reshape it into the original images. Then, we can see the images before we train on them.\nThe second file we are loading are the labels for each image. They can be one of 4: barren land, trees, grassland and other. Each row in the file looks like this [1,0,0,0], where only one of the 4 value is 1. If it is one, then it is that class respective to the order I showed above. If it was the above values, the image is a picture of barren land. If it was [0,1,0,0], then it would be trees. If it was [0,0,1,0], then it would be grassland and so on.","execution_count":null},{"metadata":{"_uuid":"71a03e051a5f17fce9848916f446d103b965fde1","_cell_guid":"d8fb5058-7afb-42df-b84b-f38e7a86b634","trusted":true},"cell_type":"code","source":"x_train_set_fpath = '../input/X_test_sat4.csv'\ny_train_set_fpath = '../input/y_test_sat4.csv'\nprint ('Loading Training Data')\nX_train = pd.read_csv(x_train_set_fpath)\nprint ('Loaded 28 x 28 x 4 images')\n\nY_train = pd.read_csv(y_train_set_fpath)\nprint ('Loaded labels')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Different Classes:\n- 0: Barren Land\n- 1: Trees\n- 2: Grasslands\n- 3: Urban","execution_count":null},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"target_class_dict = {\n    0: 'Barren Land',\n    1: 'Trees',\n    2: 'Grasslands',\n    3: 'Urban'\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"plt.figure(figsize=(10, 10))\np = pd.Series(Y_train.argmax(axis=1)).value_counts().plot(kind='pie',\n                    labels=['Barren Land', 'Trees', 'Grasslands', 'Urban'],\n                    autopct='%1.1f%%')\np.add_artist(plt.Circle((0,0), 0.7, color='white'))\nplt.title('Class Distribution of Satellite Images')\nplt.legend()\nplt.savefig('Class distribution.PNG')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e9094f9738ef4e26acd501ec1aa9e571433c566a","_cell_guid":"0b828d91-12f3-4601-9617-f12f0dbe282a"},"cell_type":"markdown","source":"## The values are in a pandas(data library) DataFrame. We need them as a numpy array\nYou can convert pandas dataframes to numpy arrays like this:","execution_count":null},{"metadata":{"_uuid":"74dd20d0aa2dcad14293c06f5a3c5afca36cd7e6","_cell_guid":"84b88149-7e8a-4d4d-824b-7eda05a5a2b7","trusted":true},"cell_type":"code","source":"X_train = X_train.as_matrix()\nY_train = Y_train.as_matrix()\nprint ('We have',X_train.shape[0],'examples and each example is a list of',X_train.shape[1],'numbers with',Y_train.shape[1],'possible classifications.')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8ffcf5bcc9c559142a5fc1e7298b96a66616c362","_cell_guid":"fdab3f76-0e67-48b6-8026-e84fa83a9ff9","trusted":true},"cell_type":"code","source":"#First we have to reshape each of them from a list of numbers to a 28*28*4 image.\nX_train_img = X_train.reshape([99999,28,28,4]).astype(float)\nprint (X_train_img.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"class_idxs = []\ny_targets = Y_train.argmax(axis=1)\nfor c in range(4):\n    class_idxs.append(np.where(y_targets==c))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"# dimensions\nw=28\nh=28\nc=4","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"fig=plt.figure(figsize=(12, 12))\ncolumns = 5\nrows = 4\ni = 0\nfor classs, c_idxs in enumerate(class_idxs):\n    n = c_idxs[0].shape[0]\n    s = np.random.randint(0, n-5)\n    \n    for idx in c_idxs[0][s:s+5]:\n        img = np.squeeze(X_train_img[idx,:,:,0:3]).astype(float)\n        fig.add_subplot(rows, columns, i+1)\n        i+=1\n        plt.title(str(idx)+':'+target_class_dict[classs])\n        plt.xticks([])\n        plt.yticks([])\n        plt.imshow(img)\n        \nplt.savefig('target class examples.PNG')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d986e3e2e4c72e2f0c904881d36a06f6df548b77","_cell_guid":"4e8b6540-58a0-48a3-b733-6f01e70b3997","scrolled":false,"trusted":true,"collapsed":true},"cell_type":"code","source":"#Let's take a look at one image. Keep in mind the channels are R,G,B, and I(Infrared)\nix = 5#Type a number between 0 and 99,999 inclusive\nimshow(np.squeeze(X_train_img[ix,:,:,0:3]).astype(float)) #Only seeing the RGB channels\nplt.show()\n#Tells what the image is\nif Y_train[ix,0] == 1:\n    print ('Barren Land')\nelif Y_train[ix,1] == 1:\n    print ('Trees')\nelif Y_train[ix,2] == 1:\n    print ('Grassland')\nelse:\n    print ('Other')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Fractal Augmentation","execution_count":null},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"def rgb2gray(rgb):\n    r, g, b = rgb[:,:,0], rgb[:,:,1], rgb[:,:,2]\n    gray = 0.2989 * r + 0.5870 * g + 0.1140 * b\n    return gray\n\ndef smooth_image(img):\n    return cv2.GaussianBlur(img, (3,3), 0)\n\ndef im2col(A,BLKSZ):   \n\n    # Parameters\n    M,N = A.shape\n    col_extent = N - BLKSZ[1] + 1\n    row_extent = M - BLKSZ[0] + 1\n\n    # Get Starting block indices\n    start_idx = np.arange(BLKSZ[0])[:,None]*N + np.arange(BLKSZ[1])\n\n    # Get offsetted indices across the height and width of input array\n    offset_idx = np.arange(row_extent)[:,None]*N + np.arange(col_extent)\n\n    # Get all actual indices & index into input array for final output\n    return np.take (A,start_idx.ravel()[:,None] + offset_idx.ravel())\n\ndef coltfilt(A, size):\n    \n    original_shape = np.shape(A)\n    a,b = 0, 0\n    if(size%2==0):\n        a, b = int(size/2)-1, int(size/2)\n    else:\n        a,b = int(size/2), int(size/2)\n    A = np.lib.pad(A, (a, b), 'constant')\n    Acol = im2col(A, (size, size))\n    rc = np.floor((Acol.max(axis=0) - Acol.min(axis=0))/float(size)) + 1\n    return np.reshape(rc, original_shape)\n\ndef matlab_style_gauss2D(shape=(3,3),sigma=0.5):\n    \"\"\"\n    2D gaussian mask - should give the same result as MATLAB's\n    fspecial('gaussian',[shape],[sigma])\n    \"\"\"\n    m,n = [(ss-1.)/2. for ss in shape]\n    y,x = np.ogrid[-m:m+1,-n:n+1]\n    h = np.exp( -(x*x + y*y) / (2.*sigma*sigma) )\n    h[ h < np.finfo(h.dtype).eps*h.max() ] = 0\n    sumh = h.sum()\n    if sumh != 0:\n        h /= sumh\n    return h\n\ndef mat2gray(mat):\n    maxI = np.max(mat)\n    minI = np.min(mat)\n    gray = (mat[:,:] - minI) / (maxI - minI)\n    return gray\n    \n#------- computing the slope using linear regression -------\ndef fractal_aug(image):\n    \n    image = smooth_image(image)\n    \n    n_channels = len(np.shape(image))\n\n    if(n_channels == 3):\n        image=rgb2gray(image)\n    \n    image = smooth_image(image)\n\n    image *= 255.0\n    imrows, imcols = np.shape(image)\n    \n    B = np.zeros((6, imrows, imcols))\n\n    #print(\"Calculating Differential Box Counting image\")\n\n    for r in range(2,8):\n        mask = matlab_style_gauss2D((r,r), r/2.0)\n        im = scipy.signal.convolve2d(image, mask, mode='same')\n        F = (coltfilt(im, r))*(49/(r**2))\n        B[r - 2] = np.log(F)\n\n    #print(\"Calculating FD image\")\n\n    i = np.log(range(2,8)) #Normalised scale range vector\n\n    Nxx = np.dot(i,i) - (np.sum(i)**2)/6\n    FD = np.zeros((imrows,imcols))\n\n    for m in range(1,imrows):\n        for n in range(1,imcols):\n            fd = [B[5,m,n], B[4,m,n], B[3,m,n], B[2,m,n], B[1,m,n], B[0,m,n]] #Number of boxes multiscale vector\n            Nxy = np.dot(i,fd) - (sum(i)*sum(fd))/6\n            FD[m,n] = Nxy/Nxx # slope of the linear regression line\n\n    tmp = np.zeros(np.shape(B))\n    for r in range(2,8):\n        tmp[r-2, :, :] = FD * np.log(m)\n\n    intercept = np.mean(B - tmp, axis=0)\n\n    FDB = mat2gray(FD);\n\n    intercept_image = mat2gray(intercept)\n    \n    #plt.imshow(intercept_image, cmap='gray')\n    #plt.show()\n    intercept_image = ((intercept_image - intercept_image.min()) * (1/(intercept_image.max() - intercept_image.min())) * 255).astype('uint8')\n    \n    return intercept_image","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"ix = 23\ntest_img = np.squeeze(X_train_img[ix,:,:,0:3]).astype(float)\nfractal_img = fractal_aug(test_img)\nprint(fractal_img.shape)\n\nplt.imshow(fractal_img, cmap='gray')\nplt.savefig('fractal-test.PNG')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"plt.imshow(test_img)\nplt.savefig('original-test.PNG')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"fig=plt.figure(figsize=(12, 12))\ncolumns = 5\nrows = 4\ni = 0\nfor classs, c_idxs in enumerate(class_idxs):\n    n = c_idxs[0].shape[0]\n    s = np.random.randint(0, n-5)\n    \n    for idx in c_idxs[0][s:s+5]:\n        img = fractal_aug(np.squeeze(X_train_img[idx,:,:,0:3]).astype(float))\n        fig.add_subplot(rows, columns, i+1)\n        i+=1\n        plt.title(str(idx)+':'+target_class_dict[classs])\n        plt.xticks([])\n        plt.yticks([])\n        plt.imshow(img, cmap='gray')\n        \nplt.savefig('target class fractal.PNG')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"def fractal_dimension(image, threshold=0.9):\n    # finding all the non-zero pixels\n    pixels=[]\n    for i in range(image.shape[0]):\n        for j in range(image.shape[1]):\n            if image[i,j]>0:\n                pixels.append((i,j))\n\n    Lx=image.shape[1]\n    Ly=image.shape[0]\n    #print (Lx, Ly)\n    pixels=pl.array(pixels)\n    #print (pixels.shape)\n\n    # computing the fractal dimension\n    #considering only scales in a logarithmic list\n    scales=np.logspace(0.001, 1, num=10, endpoint=False, base=2)\n    Ns=[]\n    # looping over several scales\n    for scale in scales:\n        # computing the histogram\n        H, edges=np.histogramdd(pixels, bins=(np.arange(0,Lx,scale),np.arange(0,Ly,scale)))\n        Ns.append(np.sum(H>0))\n\n    # linear fit, polynomial of degree 1\n    coeffs=np.polyfit(np.log(scales), np.log(Ns), 1)\n    return -coeffs[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"fig=plt.figure(figsize=(12, 12))\ncolumns = 2\nrows = 2\ni = 0\nfor classs, c_idxs in enumerate(class_idxs):\n    n = c_idxs[0].shape[0]\n    s = np.random.randint(0, n-5)\n    \n    join_indices = c_idxs[0][s:s+16]\n    img = (np.vstack((\n        np.hstack((np.vstack((np.hstack((np.squeeze(X_train_img[join_indices[0],:,:,0:3]).astype(float),\n                                         np.squeeze(X_train_img[join_indices[1],:,:,0:3]).astype(float))),\n                              np.hstack((np.squeeze(X_train_img[join_indices[2],:,:,0:3]).astype(float),\n                                         np.squeeze(X_train_img[join_indices[3],:,:,0:3]).astype(float))))),\n                   np.vstack((np.hstack((np.squeeze(X_train_img[join_indices[4],:,:,0:3]).astype(float),\n                                         np.squeeze(X_train_img[join_indices[5],:,:,0:3]).astype(float))),\n                              np.hstack((np.squeeze(X_train_img[join_indices[6],:,:,0:3]).astype(float),\n                                         np.squeeze(X_train_img[join_indices[7],:,:,0:3]).astype(float))))))),\n        np.hstack((np.vstack((np.hstack((np.squeeze(X_train_img[join_indices[8],:,:,0:3]).astype(float),\n                                         np.squeeze(X_train_img[join_indices[9],:,:,0:3]).astype(float))),\n                              np.hstack((np.squeeze(X_train_img[join_indices[10],:,:,0:3]).astype(float),\n                                         np.squeeze(X_train_img[join_indices[11],:,:,0:3]).astype(float))))),\n                   np.vstack((np.hstack((np.squeeze(X_train_img[join_indices[12],:,:,0:3]).astype(float),\n                                         np.squeeze(X_train_img[join_indices[13],:,:,0:3]).astype(float))),\n                              np.hstack((np.squeeze(X_train_img[join_indices[14],:,:,0:3]).astype(float),\n                                         np.squeeze(X_train_img[join_indices[15],:,:,0:3]).astype(float))))))))) )\n    \n    img = rgb2gray(img)\n    \n    # perform adaptive thresholding\n    t = skimage.filters.threshold_otsu(img)\n    mask = img > t\n    \n    fig.add_subplot(rows, columns, i+1)\n    i+=1\n    plt.title('FD of '+target_class_dict[classs]+':'+str(fractal_dimension(mask, 0.25)))\n    #plt.xticks([])\n    #plt.yticks([])\n    plt.imshow(img, cmap='gray')\n        \nplt.savefig('FD of classes 8.PNG')\n#plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"# FD simulation of all the classes\ni = 0\nlarge_fd_table = {\n    0: [],\n    1: [],\n    2: [],\n    3: []\n}\nfor classs, c_idxs in enumerate(class_idxs):\n    print('...'*15, classs, '...'*15)\n    for _ in range(300):\n        n = c_idxs[0].shape[0]\n        s = np.random.randint(0, n-16)\n\n\n        join_indices = c_idxs[0][s:s+16]\n        img = (np.vstack((\n            np.hstack((np.vstack((np.hstack((np.squeeze(X_train_img[join_indices[0],:,:,0:3]).astype(float),\n                                             np.squeeze(X_train_img[join_indices[1],:,:,0:3]).astype(float))),\n                                  np.hstack((np.squeeze(X_train_img[join_indices[2],:,:,0:3]).astype(float),\n                                             np.squeeze(X_train_img[join_indices[3],:,:,0:3]).astype(float))))),\n                       np.vstack((np.hstack((np.squeeze(X_train_img[join_indices[4],:,:,0:3]).astype(float),\n                                             np.squeeze(X_train_img[join_indices[5],:,:,0:3]).astype(float))),\n                                  np.hstack((np.squeeze(X_train_img[join_indices[6],:,:,0:3]).astype(float),\n                                             np.squeeze(X_train_img[join_indices[7],:,:,0:3]).astype(float))))))),\n            np.hstack((np.vstack((np.hstack((np.squeeze(X_train_img[join_indices[8],:,:,0:3]).astype(float),\n                                             np.squeeze(X_train_img[join_indices[9],:,:,0:3]).astype(float))),\n                                  np.hstack((np.squeeze(X_train_img[join_indices[10],:,:,0:3]).astype(float),\n                                             np.squeeze(X_train_img[join_indices[11],:,:,0:3]).astype(float))))),\n                       np.vstack((np.hstack((np.squeeze(X_train_img[join_indices[12],:,:,0:3]).astype(float),\n                                             np.squeeze(X_train_img[join_indices[13],:,:,0:3]).astype(float))),\n                                  np.hstack((np.squeeze(X_train_img[join_indices[14],:,:,0:3]).astype(float),\n                                             np.squeeze(X_train_img[join_indices[15],:,:,0:3]).astype(float))))))))) )\n\n        img = rgb2gray(img)\n\n        # perform adaptive thresholding\n        t = skimage.filters.threshold_otsu(img)\n        mask = img > t\n        \n        # add FD to dict\n        large_fd_table[classs].append(fractal_dimension(mask, 0.25))\n        \nfor k in large_fd_table.keys():\n    large_fd_table[k] = np.array(large_fd_table[k])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"large_fd_table_df = pd.DataFrame(large_fd_table)\nlarge_fd_table_df.plot(figsize=(30, 8), grid=True)\nplt.xlabel('Sample Set')\nplt.ylabel('Fractal Dimension')\nplt.title('FD of 300 Sample set of different classes')\nplt.legend(['Barren Lands', 'Trees', 'Grasslands', 'Urban'])\nplt.savefig('FD of 300 Sample set of different classes.PNG')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"f0 = interp1d(large_fd_table_df.index, large_fd_table_df[0],kind=33)\nf1 = interp1d(large_fd_table_df.index, large_fd_table_df[1],kind=33)\nf2 = interp1d(large_fd_table_df.index, large_fd_table_df[2],kind=33)\nf3 = interp1d(large_fd_table_df.index, large_fd_table_df[3],kind=33)\n\nlarge_fd_table_df2 = pd.DataFrame()\n\nnew_index = np.arange(0, 300)\nlarge_fd_table_df2[0] = f0(new_index)\nlarge_fd_table_df2[1] = f1(new_index)\nlarge_fd_table_df2[2] = f2(new_index)\nlarge_fd_table_df2[3] = f3(new_index)\n\nlarge_fd_table_df2.index = new_index\nlarge_fd_table_df2.plot(style='--', figsize=(25, 12), grid=True)\nplt.xlabel('Sample Set')\nplt.ylabel('Fractal Dimension')\nplt.title('FD of 300 Sample set of different classes - smoothened using cubic interpolation of degree to the power 33')\nplt.legend(['Barren Lands', 'Trees', 'Grasslands', 'Urban'])\nplt.savefig('FD of 300 Sample set of different classes - smoothened using cubic interpolation of degree to the power 33.PNG')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"large_fd_table_df.to_csv('large_fd_table.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c7b12392c052e8d11b959c4f9fc3ed300d68fac4","_cell_guid":"be71ab97-fa55-4db0-90d5-4e01a9999cf6"},"cell_type":"markdown","source":"## Let's now define our model\nThere are 2 different types of models we can choose from: A 'vanilla' artificial neural network we have been learning about, and a special Convolutional Neural Network we will learn about, which is very, very good at image recognition. For now we will use the simpler, vanilla artificial neural network. The network will only have one layer: the output one. This network will not be expected to be very powerful, and pretty slow.","execution_count":null},{"metadata":{"_uuid":"d8d3b8c5ec7170a540609652b8f34758c76605cc","collapsed":true,"_cell_guid":"f18bbf36-2ef5-40db-adca-b7f94a5bd8ed","trusted":true},"cell_type":"code","source":"model = Sequential([\n    Dense(4, input_shape=(3136,), activation='softmax')\n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"84575b82041b3487623c369e056412d51de3bff3","_cell_guid":"04e69069-122a-43bf-a425-20e8df580117"},"cell_type":"markdown","source":"Now that we have the data and model ready, there is one more thing we have to do. In neural networks, it is very important we normalize training data. This means we make the mean 0, and the standard deviation 1 for the best results. However, dividing the image by 255 is good enough. We will just divide the array by 255:","execution_count":null},{"metadata":{"_uuid":"0c1616cd587f4bb685552f7a9e96dce6e1e4c70d","collapsed":true,"_cell_guid":"c1d529ec-806f-4c88-bff2-b2a9fd4a9192","trusted":true},"cell_type":"code","source":"X_train = X_train/255","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a2b5cbaafa66580de3484ecd439b4d6033f39833","_cell_guid":"c3856083-839e-4330-806f-01854406565f"},"cell_type":"markdown","source":"## Now lets fit our model to the training data","execution_count":null},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"callbacks = [EarlyStopping(monitor='val_loss', patience=8),\n                ModelCheckpoint(filepath='best_model.h5',\n                                monitor='val_loss',\n                                save_best_only=True)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"def evaluate_model(history, X_test, y_test, model):\n    scores = model.evaluate((X_test), y_test, verbose=0)\n    print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n    \n    print(history)\n    fig1, ax_acc = plt.subplots()\n    plt.plot(history.history['acc'])\n    plt.plot(history.history['val_acc'])\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy')\n    plt.title('Model - Accuracy')\n    plt.legend(['Training', 'Validation'], loc='lower right')\n    plt.savefig('Accuracy graph.PNG')\n    plt.show()\n    \n    fig2, ax_loss = plt.subplots()\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.title('Model - Loss')\n    plt.legend(['Training', 'Validation'], loc='upper right')\n    plt.savefig('Loss graph.PNG')\n    plt.show()\n    target_names = [str(i) for i in range(5)]\n    \n    y_true = []\n    for element in y_test:\n        y_true.append(np.argmax(element))\n    prediction_proba = model.predict(X_test)\n    prediction = np.argmax(prediction_proba, axis=1)\n    cnf_matrix = confusion_matrix(y_true, prediction)\n    return cnf_matrix","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1ddf5b4520c1c5a66c14c9c995e7151b884e8652","_cell_guid":"6c91c908-feb5-4ed7-a0c3-23a3a17895fe","trusted":true},"cell_type":"code","source":"model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\nmodel.summary()\nhistory = model.fit(X_train,Y_train,batch_size=32,\n          callbacks=callbacks,\n          epochs=15, verbose=1,\n          validation_split=0.01)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"580d3eb9a4b1b1ec7a04a441ceed610b3bf9641b","_cell_guid":"18ec3494-24e6-4c36-a4f6-e4f2d0756e37"},"cell_type":"markdown","source":"Lets try to see what the model can do on a few images. Let's first get the predictions:","execution_count":null},{"metadata":{"_uuid":"f6748d34d5b720652348155e2c8d337e76f4b88f","_cell_guid":"40f2b736-a7ae-49ea-a6ad-760a60a2bcc3","trusted":true},"cell_type":"code","source":"preds = model.predict(X_train[-1000:], verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"conf_matrix = evaluate_model(history, X_train[-1000:], Y_train[-1000:], model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(Y_train[-1000:].argmax(axis=1), preds.argmax(axis=1)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"conf_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"df_cm = pd.DataFrame(conf_matrix, range(4), range(4))\nplt.figure(figsize=(10,7))\nsns.set(font_scale=0.9) # for label size\nsns.heatmap(df_cm, annot=True, annot_kws={\"size\": 16}) # font size\n\nplt.savefig('confusion_matrix.png')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"df_cm = pd.DataFrame(conf_matrix, range(4), range(4)).corr()\nplt.figure(figsize=(10,7))\nsns.set(font_scale=0.9) # for label size\nsns.heatmap(df_cm, annot=True, annot_kws={\"size\": 16}, cmap='summer') # font size\n\nplt.savefig('confusion_matrix corr plot.png')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4871b21ad6f1a9e47f38eb12aaa5ab1e50b3d1f9","_cell_guid":"68f7ea33-7615-47ae-8a75-99bde011e5c6","trusted":true,"collapsed":true},"cell_type":"code","source":"ix = 20 #Type a number between 0 and 999 inclusive\nimshow(np.squeeze(X_train_img[99999-(1000-ix),:,:,0:3]).astype(float)*255) #Only seeing the RGB channels\nplt.show()\n#Tells what the image is\nprint ('Prediction:\\n{:.1f}% probability barren land,\\n{:.1f}% probability trees,\\n{:.1f}% probability grassland,\\n{:.1f}% probability other\\n'.format(preds[ix,0]*100,preds[ix,1]*100,preds[ix,2]*100,preds[ix,3]*100))\n\nprint ('Ground Truth: ',end='')\nif Y_train[99999-(1000-ix),0] == 1:\n    print ('Barren Land')\nelif Y_train[99999-(1000-ix),1] == 1:\n    print ('Trees')\nelif Y_train[99999-(1000-ix),2] == 1:\n    print ('Grassland')\nelse:\n    print ('Other')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}