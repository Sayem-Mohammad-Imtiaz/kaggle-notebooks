{"cells":[{"metadata":{},"cell_type":"markdown","source":"![](https://i.pinimg.com/originals/09/53/81/0953813004d675ca814403fbb649f8b7.png)\n\n## Goals\nCreate a machine learning algorithm to predict if a patient has diabetes or not .\n\n## Data\nThe data sample is for female patients at least 21 years of age or older with Pima Native American heritage.\n\n## Conclusions\n- Major Takeaways: \n    - The patient's Glucose level has the highest impact on becoming diabetic\n    - Other features, such as high BMI, can increase the risk.  \n    \n- Final model to predict a patient's diagnosis of diabetes: Random Forest\n\n| Model    | RandomForest | (max_depth=5, random_state=123)             | ['Glucose','Age','BMI','insulin_glucose_cluster','DiabetesPedigreeFunction'] |\n|----------|--------------|---------------------------------------------|------------------------------------------------------------------------------|\n| DF       | Accuracy     | Recall on Positive (predicting diabetic) | Precision on Positive (predicting diabetic)                               |\n| Train    | 86%          | 75%                                         | 84%                                                                          |\n| Validate | 78%          | 64%                                         | 73%                                                                          |\n| Test     | 75%          | 63%                                         | 70%                                                                          |\n\n- Next Steps:\n    - Create more features with clustering/binning\n    - Statistically test more features\n\n## How to Reproduce:\n1. Go over the Readme.md file in the repository of this project [here](https://github.com/ThompsonBethany01/Predicting-Diabetes-Onset).\n2. Download Data_Analysis.ipynb, Prepare.py, and the dataset in your working directory.\n3. Run this notebook."},{"metadata":{},"cell_type":"markdown","source":"## Thought Process\nThe predictive variable is the patient being diabetic or not, 0 or 1, making it a classification problem. With a classification problem:\n- We create algorithms based on the labeled outcome variable.\n- This produces a decision rule to classify future data with.\n- We generalize the trends/patterns in the data to predict the future/unseen data."},{"metadata":{},"cell_type":"markdown","source":"# Table of Contents <a class=\"anchor\" id=\"top\"></a>\n1. [Acquire](#acquire)\n2. [Prepare](#prepare)\n3. [Explore](#explore)\n4. [Modeling](#model)\n5. [Final Conclusions](#fin)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# initial imports\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n#import modules\nimport pima_prepare as Prepare","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Acquire <a class=\"anchor\" id=\"acquire\"></a>\nDataset from UCI Machine Learning via Kaggle [here](https://www.kaggle.com/uciml/pima-indians-diabetes-database) saved in .csv file  \n#### Steps:\n1. Read csv file into df\n2. Summarize data\n3. Create data dictionary"},{"metadata":{"trusted":true},"cell_type":"code","source":"# needs saved csv file to continue\ndf = pd.read_csv('../input/pima-indians-diabetes-database/diabetes.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('The dataframe has', df.shape[0], 'rows and', df.shape[1], 'columns.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('The columns are named: ', df.columns.to_list())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# What are the data types and null counts for each column?\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### What do we learn from df.info()?\n- there are no null values\n- most columns are integers\n- BMI and DPF are decimals (floats)"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# what is the distribution of the numeric columns? (All columns)\ndf.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### What do we learn from df.describe()?\n- greatest variation in Insluin\n- Many features have a minimum of 0. Is this feasible?\n    - someone can not have 0 for BMI, Glucose, or BloodPressure\n- Insulin maximum is 846, is this possible or an anamoly cause by a typo? Need to research."},{"metadata":{"trusted":true},"cell_type":"code","source":"# looking at the distribution of features all at once simply with df.hist()\ndf.hist()\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Takeaways\n768 observations  \n- 8 columns and 1 predictive column as diabetic or not  \n\nAll numeric values, integers or floats  \n- Average diabetic diagnosis is 0, non-diabetic  \n- All continuous features except pregnancies and outcome which are discrete  \n\nNo null values  \n- Observations with 0 for values that cannot be, such as BMI and Blood Pressure, have 0 for multiple features  \n    - Could be null values that were replaced with 0"},{"metadata":{},"cell_type":"markdown","source":"[Table of Contents](#top)"},{"metadata":{},"cell_type":"markdown","source":"# Prepare <a class=\"anchor\" id=\"prepare\"></a>\nFor Exploration:\n- Create new features by bining demographics  or clustering\n    - age into 20s, 30s, etc\n    - bmi into low, middle, high\n    - blood pressure into low, good, high\n    - create features based on clustering\n\nFor Modeling:\n- Split into train, valideate, test\n- Scale the data - fitting on train df only\n- Determine if outliers/anomalies to remove (after MVP complete)\n\n## Prepare.py Module contains functions used below\n### Prepare.prep_df\n- Replaces values of 0 in...\n    - BMI\n    - Glucose\n    - BloodPressure\n    - SkinThickness\n    - Insulin  \n   with the mean of the feature\n   \n- Bins features with pd.qcut(cuts features into even bins based on number of bins specified)\n    - age\n    - bmi\n    - bloodpressure\n\n- Creates feature for patient having both high bloodpressure and bmi"},{"metadata":{"trusted":true},"cell_type":"code","source":"# prepping df before split with function\ndf = Prepare.prep_df(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# quality control, checking the df looks accurate\ndf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Prepare.split_df\nSplits Data into\n- 70% train\n- 20% validate\n- 10% test  \n\nprinting the returned shape of the split df"},{"metadata":{"trusted":true},"cell_type":"code","source":"# splitting df with function\ntrain, validate, test = Prepare.split_df(df)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# quality control, checking train df looks like the df above but with smaller mixed index\ntrain","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Prepare.scale_dfs\n- Scaling the Data Using Min-Max Scaler\n- transforms the range of data points to 0 - 1\n- fits scaler to train only, then transforms on all 3 dfs\n- returns the split dfs scaled"},{"metadata":{"trusted":true},"cell_type":"code","source":"# calling split df function\nX_train_scaled, X_validate_scaled, X_test_scaled = Prepare.scale_dfs(train, validate, test, 'Outcome')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# quality control, does df look the same as train but with scalled values?\nX_train_scaled","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Prepare.create_clusters\n- Creating Clusters on Scaled Data\n- multitude of parameters allow one function to create any cluster\n    - train, validate, and test scaled dfs to fit the cluster model to train only, then transform on all dfs\n    - train, validate, test to add the clusters to the unscaled dfs as well for exploration\n    - features = what to create the clusters on\n    - columns = name of the columns when adding the clusters to the dfs\n    - n = number of groups within the cluster to make\n    - cluster = name of the original cluster before splitting into dummies\n    \n#### For each cluster:\n1. visualize the number to set for n with elbow test\n2. call the function with n set from elbow test\n\n### Age and BMI Cluster"},{"metadata":{"trusted":true},"cell_type":"code","source":"# elbow test to determine n\nfrom sklearn.cluster import KMeans\n\n# features to predict cluster on, only fitting model on X(train)\nX = X_train_scaled[['age_bins','bmi_bins']]\n\nwith plt.style.context('seaborn-whitegrid'):\n    plt.figure(figsize=(9, 6))\n    pd.Series({k: KMeans(k).fit(X).inertia_ for k in range(2, 18)}).plot(marker='x')\n    plt.xticks(range(2, 18))\n    plt.xlabel('k')\n    plt.ylabel('inertia')\n    plt.title('Change in inertia as k increases')\n    \n# will start with 4 clusters","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# creating cluster with function from Prepare.py\n\nfeatures = ['age_bins','bmi_bins']\ncolumns = ['age_bmi_cluster1','age_bmi_cluster2','age_bmi_cluster3','age_bmi_cluster4']\nn = 4\ncluster = 'age_bmi_cluster'\n\nX_train_scaled, X_validate_scaled, X_test_scaled, train, validate, test = Prepare.create_clusters(X_train_scaled, X_validate_scaled, X_test_scaled, train, validate, test, features, n, columns, cluster)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Pregnancy Cluster"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# feature to create cluster on, only fitting model on X(train)\nX = X_train_scaled[['Pregnancies']]\n\nwith plt.style.context('seaborn-whitegrid'):\n    plt.figure(figsize=(9, 6))\n    pd.Series({k: KMeans(k).fit(X).inertia_ for k in range(2, 18)}).plot(marker='x')\n    plt.xticks(range(2, 18))\n    plt.xlabel('k')\n    plt.ylabel('inertia')\n    plt.title('Change in inertia as k increases')\n    \n# will start with 4 clusters","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# creating cluster with function from Prepare.py\n\nfeatures = ['Pregnancies']\ncolumns = ['pregnancy_cluster1','pregnancy_cluster2','pregnancy_cluster3','pregnancy_cluster4']\nn = 4\ncluster = 'pregnancy_cluster'\n\nX_train_scaled, X_validate_scaled, X_test_scaled, train, validate, test = Prepare.create_clusters(X_train_scaled, X_validate_scaled, X_test_scaled, train, validate, test, features, n, columns, cluster)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Insulin and Glucose Cluster"},{"metadata":{"trusted":true},"cell_type":"code","source":"# features to predict cluster on, only fitting model on X(train)\nX = X_train_scaled[['Insulin','Glucose']]\n\nwith plt.style.context('seaborn-whitegrid'):\n    plt.figure(figsize=(9, 6))\n    pd.Series({k: KMeans(k).fit(X).inertia_ for k in range(2, 18)}).plot(marker='x')\n    plt.xticks(range(2, 18))\n    plt.xlabel('k')\n    plt.ylabel('inertia')\n    plt.title('Change in inertia as k increases')\n    \n# will start with 5 clusters","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# creating cluster with function from Prepare.py\n\nfeatures = ['Insulin','Glucose']\ncolumns = ['insulin_glucose_cluster1','insulin_glucose_cluster2','insulin_glucose_cluster3','insulin_glucose_cluster4','insulin_glucose_cluster5']\nn = 5\ncluster = 'insulin_glucose_cluster'\n\nX_train_scaled, X_validate_scaled, X_test_scaled, train, validate, test = Prepare.create_clusters(X_train_scaled, X_validate_scaled, X_test_scaled, train, validate, test, features, n, columns, cluster)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# quality control, do we see the clusters added to the end of train scaled df?\nX_train_scaled.head(3).T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train.to_csv('train.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Takeaways\n- imputed 0 values that could not be 0 with the mean\n- created features based on binning\n- split the data for exploration and modeling\n- scaled the data based on split train df\n- created clusters based on split scaled train df\n\n### Next Steps\n- are there other clusters that could be more significant in modeling?\n- are there outliers/anomalies to deal with?"},{"metadata":{},"cell_type":"markdown","source":"[Table of Contents](#top)"},{"metadata":{},"cell_type":"markdown","source":"# Explore <a class=\"anchor\" id=\"explore\"></a>\n- determine trends in patient being diabetic or not\n    - X feature(s) vs. Outcome\n- test the significance with hypothesis testing, such as with:\n    - t-test\n    - chi-squared contingancy table\n    - peirson correlation test\n- explore interaction of independent features to determine what clusters to create\n- visualize clusters created"},{"metadata":{"trusted":true},"cell_type":"code","source":"# visualizing distribution of Y feature (predictive variable)\nplt.figure(figsize=(10,7))\ntrain.Outcome.value_counts().sort_index().plot.bar()\ndiabetic_rate = train.Outcome.mean()\nplt.title(f\"Overall diabetes diagnosis rate: {diabetic_rate:.2%}\", size=17)\nplt.xlabel('Is diabetic?', size=17)\nplt.ylabel('Count of Patients', size=17)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Overall, most patients are not diagnosed with diabetes."},{"metadata":{"trusted":true},"cell_type":"code","source":"# visualizing overall linear correlation of all features\ncorr = train.corr()\n\n# Generate a mask for the upper triangle\nmask = np.triu(np.ones_like(corr, dtype=bool))\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(11, 9))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, vmin=-1, vmax=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Outcome has strongest correlation with Glucose, which is also reflected later in model. This feature had the strongest importance for all modeling algorithms.\n#### Note: Features can have non-linear correlation, which would not be captured in this heatmap."},{"metadata":{},"cell_type":"markdown","source":"## Looking at Independent Features vs. Diabetic Outcome\n### Diagnosis Rate within Subgroups of Age, BMI, and BP Bins"},{"metadata":{"trusted":true},"cell_type":"code","source":"# visualizing subgroups within feature bins, is there a significant difference of diabetic diagnosis?\n# categorical features we can compare\nfeatures = ['age_bins', 'bmi_bins', 'bp_bins']\n\n# overall diagnosis of diabetes on whole train df\ndiabetic_rate = train.Outcome.mean()\n\n# plotting subgroups diagnosis rate and comparing to overal with dashed line\n_, ax = plt.subplots(nrows=1, ncols=3, figsize=(16, 6), sharey=True)\nfor i, feature in enumerate(features):\n    sns.barplot(feature, 'Outcome', data=train, ax=ax[i], alpha=.8)\n    ax[i].set_xlabel('')\n    ax[i].set_ylabel('Diabetes Diagnosis Rate', size=13)\n    ax[i].set_title(feature, size=15)\n    ax[i].axhline(diabetic_rate, ls='--', color='grey')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Notes:\n- Age bins have higher diagnosis rates in the older bins.\n- BMI bins have higher diagnosis rates at higher BMI\n- BloodPressure bins have higher diagnosis rates at higher bloodpressure\n\n## Statistically Testing these Hypothesis Seen in Visuals"},{"metadata":{},"cell_type":"markdown","source":"### Correlation Test\n- do two samples have a linear relationship?\n- null hypothesis is that there is no linear correlation between the two variables\n- the correlation coefficient is a unitless continuous numerical measure between -1 and 1, where 1 = perfect correlation and -1 = perfect negative correlation"},{"metadata":{},"cell_type":"markdown","source":"### Age vs. Diabetes Diagnosis\n$H_O$: There is no significant correlation between age and diabetes diagnosis.   \n$H_a$: Older populations correlate with a higher rate of diabetes (in female patients +21 with Pima Indian heritage)."},{"metadata":{"trusted":true},"cell_type":"code","source":"# not normally distributed, so we do not do a t-test\nplt.hist(train.Age)\nprint('Age average:', round(train.Age.mean(),2), '\\nAge median:', round(train.Age.median(),2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy import stats\n\nx = train.Age\ny = train.Outcome\n\ncorr, p = stats.pearsonr(x, y)\nprint('correlation coeeficient:', corr, '\\n\\np-value:', p)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### P is less than alpha (.01), we reject the null hypothesis.\nThere is a significant correlation between age and diagnosis rates of diabetes. The correlation coeeficient being a positive number tells us the rate increases as age increases."},{"metadata":{},"cell_type":"markdown","source":"### T-test\n- Compare the mean for a specific subgroup against the population mean.\n- One of the assumptions of the t-test is that the continuous variable is normally distributed. To check this, we can make a quick visualization."},{"metadata":{},"cell_type":"markdown","source":"### BMI vs. Diabetes Diagnosis\n$H_O$: There is no significant difference between BMI and diabetes diagnosis.  \n$H_a$: Populations with higher BMI have a significantly higher rate of diabetes (in female patients +21 with Pima Indian heritage).\n\n## alpha = .01"},{"metadata":{"trusted":true},"cell_type":"code","source":"# is the continuous variable normally distributed?\nplt.hist(train.BMI)\nplt.title('Distribution of BMI', size=15)\nprint('BMI average:', round(train.BMI.mean(),2), '\\nBMI median:', round(train.BMI.median(),2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from math import sqrt\n\n# tesing subgroup of bin 3 of BMI\ntop_BMI = train[train.bmi_bins == 3]\n\nμ = train.Outcome.mean()\nxbar = top_BMI.Outcome.mean()\ns = top_BMI.Outcome.std()\nn = top_BMI.shape[0]\ndegf = n - 1\nstandard_error = s / sqrt(n)\n\nt = (xbar - μ) / (s / sqrt(n))\nprint('t-value:', round(t,4))\n      \np = stats.t(degf).sf(t) * 2 # *2 for two-tailed test\nprint('p-value:', round(p,4))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### P is less than alpha (.01), we reject the null hypothesis.\nPopulations with higher BMI have a significantly higher rate of diabetes (in female patients +21 with Pima Indian heritage). This is specifically patients in the bmi_bins of 3, those with a BMI higher than 34.867."},{"metadata":{},"cell_type":"markdown","source":"### BloodPressure vs. Diabetes Diagnosis\n$H_O$: There is no significant difference between age and diabetes diagnosis.  \n$H_a$: Populations with higher blood pressure have a significantly higher rate of diabetes (in female patients +21 with Pima Indian heritage)."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# is the continuous variable normally distributed?\nplt.hist(train.BloodPressure)\nplt.title('Distribution of BP', size=15)\nprint('Blood Pressure average:', round(train.BloodPressure.mean(),2), '\\nBlood Pressure median:', round(train.BloodPressure.median(),2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# tesing subgroup of bin 3 of BMI\ntop_BP = train[train.bp_bins == 3]\n\nμ = train.Outcome.mean()\nxbar = top_BP.Outcome.mean()\ns = top_BP.Outcome.std()\nn = top_BP.shape[0]\ndegf = n - 1\nstandard_error = s / sqrt(n)\n\nt = (xbar - μ) / (s / sqrt(n))\nprint('t-value:', round(t,4))\n      \np = stats.t(degf).sf(t) * 2 # *2 for two-tailed test\nprint('p-value:', round(p,4))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### P is not lower than alpha(.01), we fail to reject the null hypothesis.\nThere is no significant difference between age and diabetes diagnosis. This is specific for patients in the bp_bin of 3. Could splitting the bins into smaller categories create a cluster that is significantly higher rates of diabetes?"},{"metadata":{},"cell_type":"markdown","source":"### Diagnosis Rates Within Subgroups of Pregnancy Count"},{"metadata":{"trusted":true},"cell_type":"code","source":"# overall diagnosis of diabetes on whole train df\ndiabetic_rate = train.Outcome.mean()\n\nplt.figure(figsize=(13,8))\n\n# plotting pregnancy count diagnosis rate and comparing to overall with dashed line\nsns.barplot('Pregnancies', 'Outcome', data=train, alpha=.8)\nplt.xlabel('Count of Pregnancies')\nplt.ylabel('Diabetes Diagnosis Rate', size=13)\nplt.title('Diagnosis Rate by Pregnancy Count', size=15)\nplt.axhline(diabetic_rate, ls='--', color='grey')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# note the few amount of pregnancies at 10 and higher\ntrain[train.Outcome == 1].Pregnancies.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Count of Pregnancies vs. Diabetes Diagnosis - Correlation Test\n$H_O$: There is no significant correlation between age and diabetes diagnosis.   \n$H_a$: Older populations correlate with a higher rate of diabetes (in female patients +21 with Pima Indian heritage)."},{"metadata":{"trusted":true},"cell_type":"code","source":"x = train.Pregnancies\ny = train.Outcome\n\ncorr, p = stats.pearsonr(x, y)\nprint('correlation coeeficient:', corr, '\\n\\np-value:', p)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### P is less than alpha (.01), we reject the null hypothesis.\nThere is a significant correlation between count of pregnancies and diagnosis rates of diabetes. The correlation coefficient being a positive number tells us the rate increases as the count increases. Howver, you can see the correlation is not as strong as age, as the coefficient is .19 as opposed to .24 in age."},{"metadata":{},"cell_type":"markdown","source":"### Visualizing Interaction of Age and BMI Bins"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(13,9))\nsns.swarmplot(x=\"bmi_bins\", y=\"Age\", data=train, hue=\"Outcome\", palette=\"Set2\")\nplt.legend()\nplt.title('Diabetes Diagnosis by Age and BMI Bins', size=15)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Visualzing Interaction of Glucose and Insulin"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.relplot(x=\"Glucose\", y=\"Insulin\", hue=\"Outcome\", data=train, height=6, aspect=1.6)\nplt.xlim(-5, 250)\nplt.title('Diabetes Diagnosis with BMI vs. Glucose', size=15)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Exploring Interaction of X Variables\n- are there any clear groupings within the independent features?\n- are groupings clearer when adding hue for diagnosis?\n- what clusters can be created?"},{"metadata":{},"cell_type":"markdown","source":"### Interaction of Features with Glucose and the Outcome"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,4))\n\nplt.subplot(131)\nsns.scatterplot(x=train.Glucose, y=train.BloodPressure, hue=train.Outcome)\n\nplt.subplot(132)\nsns.scatterplot(x=train.Glucose, y=train.Insulin, hue=train.Outcome)\n\nplt.subplot(133)\nsns.scatterplot(x=train.Glucose, y=train.BMI, hue=train.Outcome)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Scatterplots can show if there is a distinction between the variables\n- With all, it seems Glucose has the greater effect on the clustering of diabetic outcome\n- i.e., Farther right on the Glucose (higher glucose) has more diabetic\n- Higher up on the y axis (other feature) does not effect the clustering of diabetic"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.scatterplot(x=train.age_bins, y=train.bmi_bins, hue=train.Outcome)\nplt.title('Average Outcome by Age and BMI bins', size=15)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### How to interpret:\nExamples\n- when age_bin == 1 and bmi_bin == 3, average Outcome == 1 \"diabetic\"\n    - younger patients with higher bmi have an average of diabetic\n- when age_bin == 4 and bmi_bin == 1, average Outcome == 0 \"non - diabetic\"\n    - older patients with lower bmi have an average of non-diabetic\n- when age_bin == 2 and bmi_bin == 2, average Outcome == 1 \"diabetic\""},{"metadata":{},"cell_type":"markdown","source":"## Visualizing Clusters\n- Are there subgroups in the clusters that have a higher rate of daibetes diagnosis? Above the dashed line?"},{"metadata":{"trusted":true},"cell_type":"code","source":"# comparing the dummy variables created from the age_bmi_cluster\nfeatures = ['age_bmi_cluster1', 'age_bmi_cluster2', 'age_bmi_cluster3','age_bmi_cluster4']\n\n# overall diagnosis of diabetes on whole train df\ndiabetic_rate = train.Outcome.mean()\n\n# plotting subgroups diagnosis rate and comparing to overal with dashed line\n_, ax = plt.subplots(nrows=1, ncols=4, figsize=(16, 6), sharey=True)\nfor i, feature in enumerate(features):\n    sns.barplot(feature, 'Outcome', data=train, ax=ax[i], alpha=.8)\n    ax[i].set_xlabel('')\n    ax[i].set_ylabel('Diabetes Diagnosis Rate', size=13)\n    ax[i].set_title(feature, size=15)\n    ax[i].axhline(diabetic_rate, ls='--', color='grey')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# comparing the dummy variables created from the age_bmi_cluster\nfeatures = ['pregnancy_cluster1', 'pregnancy_cluster2', 'pregnancy_cluster3','pregnancy_cluster4']\n\n# overall diagnosis of diabetes on whole train df\ndiabetic_rate = train.Outcome.mean()\n\n# plotting subgroups diagnosis rate and comparing to overal with dashed line\n_, ax = plt.subplots(nrows=1, ncols=4, figsize=(16, 6), sharey=True)\nfor i, feature in enumerate(features):\n    sns.barplot(feature, 'Outcome', data=train, ax=ax[i], alpha=.8)\n    ax[i].set_xlabel('')\n    ax[i].set_ylabel('Diabetes Diagnosis Rate', size=13)\n    ax[i].set_title(feature, size=15)\n    ax[i].axhline(diabetic_rate, ls='--', color='grey')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# comparing the dummy variables created from the age_bmi_cluster\nfeatures = ['insulin_glucose_cluster1', 'insulin_glucose_cluster2', 'insulin_glucose_cluster3','insulin_glucose_cluster4','insulin_glucose_cluster5']\n\n# overall diagnosis of diabetes on whole train df\ndiabetic_rate = train.Outcome.mean()\n\n# plotting subgroups diagnosis rate and comparing to overal with dashed line\n_, ax = plt.subplots(nrows=1, ncols=5, figsize=(16, 6), sharey=True)\nfor i, feature in enumerate(features):\n    sns.barplot(feature, 'Outcome', data=train, ax=ax[i], alpha=.8)\n    ax[i].set_xlabel('')\n    ax[i].set_ylabel('Diabetes Diagnosis Rate', size=13)\n    ax[i].set_title(feature, size=15)\n    ax[i].axhline(diabetic_rate, ls='--', color='grey')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Takeaways\n- Glucose has the greatest correlation with diabetes diagnosis\n- Statistical testing found Age, BMI, and Pregnancies failed to reject the null hypothesis of \"{feature} does not significantly influence diabetes\"\n- Certain subgroups within clusters have a higher rate of diagnosis than the average\n\n### Next Steps\n- can create different clusters\n- bin more continuous variables"},{"metadata":{},"cell_type":"markdown","source":"[Table of Contents](#top)"},{"metadata":{},"cell_type":"markdown","source":"# Modeling <a class=\"anchor\" id=\"model\"></a>\n##### Outcome of patient being diabetic or not is the predictive feature, Y\n#### Steps\n1. Create the Baseline model for comparison based on most common diagnosis\n2. Create models fit to the train df only\n3. Validate on top 3 models, tuning hyperparameters\n4. Use final top model evaluated on test\n5. Determine next steps/conclusions\n\n[Skip to Modeling Summary](#model-summary)\n\n### Baseline"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.Outcome.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# taking a look at the same barplot again, overall rate of diabetes diagnosis\ntrain.Outcome.value_counts().sort_index().plot.bar()\ndiabetic_rate = train.Outcome.mean()\nplt.title(f\"Overall diabetes diagnosis rate: {diabetic_rate:.2%}\", size=15)\nplt.xlabel('Is diabetic?', size=13)\nplt.ylabel('Count of Patients', size=13)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = train[['Outcome']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# most common diagnosis is non-diabetic, this will be our baseline\ny_train['baseline_prediction'] = 0\n\nbaseline_accuracy = (y_train.baseline_prediction == train.Outcome).mean()\n\nprint(f'baseline accuracy: {baseline_accuracy:.2%}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Creating Classification Models\n#### Models Created\n- LogisticRegression\n- DecisionTree\n- RandomForest\n- KNN\n- RidgeClassifier Model\n- SGDClassifier\n\n#### Primary Evaluation Metric\nIs it more dangerous to predict diabetic when actually not, or not diabetic when actually diabetic? \n   - It is better to predict Diabetic because a patient not being diagnosed could lead to harm to the patient\n   - We want the model to predict 1 better, aka have a higher recall score and precision\n       - recall: \n       - TP / (TP + FN)\n       - % of acually positive cases that were predicted as positive\n       - Optimize for recall when missing actual positive cases is expensive or deadly\n       \n### Determine What Features to Model on Using:\n- SelectKBest\n- model.feature_importances_"},{"metadata":{"trusted":true},"cell_type":"code","source":"# modeling imports\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import RidgeClassifier\nfrom sklearn.linear_model import SGDClassifier\n\n# SelectKBest features\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.feature_selection import RFE\n\n# evaluation metrics\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Logistic Regression Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# does better on all features than when using Top 10 SelectKBest features\nX_train = X_train_scaled\ny_train = train[['Outcome']]\n\n# create model object\nlogit = LogisticRegression(C=10, random_state=123)\n\n# fit to train\nlogit.fit(X_train, y_train)\n\n# predict on train\ny_pred = logit.predict(X_train)\n\n#evaluate\nprint('Accuracy of Logistic Regression classifier on training set: {:.2f}'\n     .format(logit.score(X_train, y_train)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Coefficient: \\n', logit.coef_)\nprint('Intercept: \\n', logit.intercept_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(confusion_matrix(y_train, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_train, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Decision Tree Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = X_train_scaled\n\n# create the model\nclf = DecisionTreeClassifier(max_depth=5, random_state=123)\n\n# fit to train\nclf.fit(X_train, y_train)\n\ncol = X_train_scaled.columns\n\n#modelname.feature_importance_\ny = clf.feature_importances_\n\n#plot\nfig, ax = plt.subplots(figsize=(13,9)) \nwidth = .75 # the width of the bars \nind = np.arange(len(y)) # the x locations for the groups\nplt.barh(ind, y, width, color=\"green\")\nax.set_yticks(ind+width/10)\nax.set_yticklabels(col, minor=False)\nplt.title('Feature importance in Decision Classifier')\nplt.xlabel('Relative importance')\nplt.ylabel('feature')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# features to model on\nX_train = X_train_scaled[['Glucose','BMI','DiabetesPedigreeFunction','Age','SkinThickness']]\n\n# create the model\nclf = DecisionTreeClassifier(max_depth=5, random_state=123)\n\n# fit to train\nclf.fit(X_train, y_train)\n\n# predict on train\ny_pred = clf.predict(X_train)\n\n# evaluate\nprint('Accuracy of Decision Tree classifier on training set: {:.2f}'\n     .format(clf.score(X_train, y_train)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"confusion_matrix(y_train, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_train, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Random Forest Model\n- visualize feature importance for model\n- model on specific features"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = X_train_scaled","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create the model\nrf = RandomForestClassifier(max_depth=5, random_state=123)\n\n# fit to train\nrf.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"col = X_train_scaled.columns\n\n#modelname.feature_importance_\ny = rf.feature_importances_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plot\nfig, ax = plt.subplots(figsize=(13,9)) \nwidth = .75 # the width of the bars \nind = np.arange(len(y)) # the x locations for the groups\nplt.barh(ind, y, width, color=\"green\")\nax.set_yticks(ind+width/10)\nax.set_yticklabels(col, minor=False)\nplt.title('Feature importance in RandomForest Classifier')\nplt.xlabel('Relative importance')\nplt.ylabel('feature')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# features to model on\nX_train = X_train_scaled[['Glucose','Age','BMI','insulin_glucose_cluster','DiabetesPedigreeFunction']]\n\n# create the model\nrf = RandomForestClassifier(max_depth=5, random_state=123)\n\n# fit to train\nrf.fit(X_train, y_train)\n\n# predict on train\ny_pred = rf.predict(X_train)\n\n# evaluate\nprint('Accuracy of random forest classifier on training set: {:.2f}'\n     .format(rf.score(X_train, y_train)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_train, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(rf.feature_importances_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## KNN Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# initialize the ML algorithm\nlm = LinearRegression()\n\n# create the rfe object, indicating the ML object (lm) and the number of features to select\nrfe = RFE(lm, 12)\n\n# fit the data using RFE\nrfe.fit(X_train_scaled,y_train.Outcome)  \n\n# get the mask of the columns selected\nfeature_mask = rfe.support_\n\n# get list of the column names. \nrfe_feature = X_train_scaled.iloc[:,feature_mask].columns.tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Features elected by SelectKBest\nprint('SelectKBest Top 12 Features:')\nrfe_feature","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 10 features have the same rank of 1\nrfe.ranking_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# select features to model\nX_train = X_train_scaled[['Pregnancies','Glucose','BloodPressure','SkinThickness','Insulin','BMI', 'DiabetesPedigreeFunction','Age','age_bins','bp_bins','high_bmi_bp','age_bmi_cluster4']]\n\n# create the model\nknn = KNeighborsClassifier(n_neighbors=5)\n\n# fit to train\nknn.fit(X_train, y_train)\n\n# predict on train\ny_pred = knn.predict(X_train)\n\n# evaluate\nprint('Accuracy of KNN classifier on training set: {:.2f}'\n     .format(knn.score(X_train, y_train)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_train, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## RidgeClassifier Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# select features to model\nX_train = X_train_scaled[['Pregnancies','Glucose','BloodPressure','SkinThickness','Insulin','BMI', 'DiabetesPedigreeFunction','Age','age_bins','bp_bins','high_bmi_bp','age_bmi_cluster4']]\n\n# create the model object\nclf2 = RidgeClassifier(random_state=123)\n\n# fit to train only\nclf2.fit(X_train, y_train)\n\ny_pred = clf2.predict(X_train)\n\n# evaluate with score, returns the mean accuracy on the given test data and labels\nprint('Accuracy of Ridge classifier on training set:', round(clf2.score(X_train, y_train),2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_train, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## SGDClassifier Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# select features to model\nX_train = X_train_scaled[['Pregnancies','Glucose','BloodPressure','SkinThickness','Insulin','BMI', 'DiabetesPedigreeFunction','Age','age_bins','bp_bins','high_bmi_bp','age_bmi_cluster4']]\n\nclf3 = SGDClassifier(max_iter=1000, tol=1e-3, random_state=123)\n\nclf3.fit(X_train, y_train)\n\ny_pred = clf3.predict(X_train)\n\nprint('Accuracy of SGD classifier on training set:', round(clf3.score(X_train, y_train),2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_train, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Evaluating Top 3 on Validate - Tuning Hyperparameters\n1. RandomForest Model at .86 accuracy, .77 recall\n2. DecisionTree at .85 accuracy, .77 recall\n3. KNN at .82 accuracy, .72 recall"},{"metadata":{"trusted":true},"cell_type":"code","source":"# splitting into y\ny_validate = validate[['Outcome']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### RandomForest on Validate"},{"metadata":{"trusted":true},"cell_type":"code","source":"# features to model on\nX_validate = X_validate_scaled[['Glucose','Age','BMI','insulin_glucose_cluster','DiabetesPedigreeFunction']]\n\n# predict on validate\ny_pred = rf.predict(X_validate)\n\n# evaluate\nprint('Accuracy of random forest classifier on validate set: {:.2f}'\n     .format(rf.score(X_validate, y_validate)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_validate, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### DecisionTree on Validate"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_validate = X_validate_scaled[['Glucose','BMI','DiabetesPedigreeFunction','Age','SkinThickness']]\n\n# predict on validate\ny_pred = clf.predict(X_validate)\n\n# evaluate\nprint('Accuracy of Decision Tree classifier on validate set: {:.2f}'\n     .format(clf.score(X_validate, y_validate)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_validate, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### KNN on Validate"},{"metadata":{"trusted":true},"cell_type":"code","source":"# features created model on\nX_validate = X_validate_scaled[['Pregnancies','Glucose','BloodPressure','SkinThickness','Insulin','BMI', 'DiabetesPedigreeFunction','Age','age_bins','bp_bins','high_bmi_bp','age_bmi_cluster4']]\n\n# predict on train\ny_pred = knn.predict(X_validate)\n\n# evaluate\nprint('Accuracy of KNN classifier on validate set: {:.2f}'\n     .format(knn.score(X_validate, y_validate)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_validate, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Evaluating Top Model on Test - Determine if Overfit\n- RandomForest did better on recall"},{"metadata":{"trusted":true},"cell_type":"code","source":"# splitting into X and y\n# features to model on\nX_test = X_test_scaled[['Glucose','Age','BMI','insulin_glucose_cluster','DiabetesPedigreeFunction']]\ny_test = test[['Outcome']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# predict on validate\ny_pred = rf.predict(X_test)\n\n# evaluate\nprint('Accuracy of random forest classifier on test set: {:.2f}'\n     .format(rf.score(X_test, y_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modeling Summary <a class=\"anchor\" id=\"model-summary\"></a>\n##### Baseline Accuracy: 66%, recall 0%\n## All Models Tested on Train\n| Model Type          | Hyperparameters         | Features                                                                                                                                                            | Accuracy | Recall on True Positive (Diabetic Predicted Diabetic) |\n|---------------------|-------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------|-------------------------------------------------------|\n| Logistic Regression | C=10                    | All Features                                                                                                                                                        | 79%      | 62%                                                   |\n| Decision Tree       | max_depth=5             | ['Glucose','BMI','DiabetesPedigreeFunction', 'Age']                                                                                                                 | 84%      | 70%                                                   |\n| Random Forest       | max_depth=5             | ['Glucose','Age','BMI', 'insulin_glucose_cluster','DiabetesPedigreeFunction']                                                                                       | 85%      | 75%                                                   |\n| KNN                 | n_neighbors=5           | ['Pregnancies','Glucose','BloodPressure', 'SkinThickness','Insulin','BMI', 'DiabetesPedigreeFunction', 'Age','age_bins','bp_bins','high_bmi_bp','age_bmi_cluster4'] | 80%      | 62%                                                   |\n| Ridge Classifier    | None                    | ['Pregnancies','Glucose','BloodPressure', 'SkinThickness','Insulin','BMI', 'DiabetesPedigreeFunction', 'Age','age_bins','bp_bins','high_bmi_bp','age_bmi_cluster4'] | 78%      | 57%                                                   |\n| SGD Classifier      | max_iter=1000, tol=1e-3 | ['Pregnancies','Glucose','BloodPressure', 'SkinThickness','Insulin','BMI', 'DiabetesPedigreeFunction', 'Age','age_bins','bp_bins','high_bmi_bp','age_bmi_cluster4'] | 68%      | 7%                                                   |\n\n## All Models Tested on Validate\n| Model Type    | Hyperparameters | Features                                                                                                                                                            | Accuracy | Recall on True Positive (Diabetic Predicted Diabetic) |\n|---------------|-----------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------|-------------------------------------------------------|\n| Decision Tree | max_depth=5     | ['Glucose','BMI','DiabetesPedigreeFunction', 'Age']                                                                                                                 | 74%      | 58%                                                   |\n| Random Forest | max_depth=5     | ['Glucose','Age','BMI', 'insulin_glucose_cluster','DiabetesPedigreeFunction']                                                                                       | 78%      | 64%                                                   |\n| KNN           | n_neighbors=5   | ['Pregnancies','Glucose','BloodPressure', 'SkinThickness','Insulin','BMI', 'DiabetesPedigreeFunction', 'Age','age_bins','bp_bins','high_bmi_bp','age_bmi_cluster4'] | 75%      | 55%                                                   |\n\n## Final Model Metrics with Train, Validate, and Test: Random Forest\n\n| Model    | RandomForest | (max_depth=5, random_state=123)             | ['Glucose','Age','BMI','insulin_glucose_cluster','DiabetesPedigreeFunction'] |\n|----------|--------------|---------------------------------------------|------------------------------------------------------------------------------|\n| DF       | Accuracy     | Recall on Positive (predicting diabetic) | Precision on Positive (predicting diabetic)                               |\n| Train    | 85%          | 75%                                         | 84%                                                                          |\n| Validate | 78%          | 64%                                         | 73%                                                                          |\n| Test     | 76%          | 60%                                         | 69%                                                                          |"},{"metadata":{},"cell_type":"markdown","source":"[Table of Contents](#top)"},{"metadata":{},"cell_type":"markdown","source":"# Conclusions <a class=\"anchor\" id=\"fin\"></a>\n- final model outperforms baseline (64% accuracy, 0% recall)\n- most clusters created were not significant for random forest model\n- emphasis on modeling performance with True and False positives.\n    - diagnosising a patient early on prevents further harm to the patient if medicine/therapy is needed\n    - not diagnosing a patient can lead to dangerous levels of Blood Glucose\n\n## Next Steps\n- hypothesis testing on more features\n- create new clusters and test signifcance in modeling with visuals/hypothesis testing"},{"metadata":{},"cell_type":"markdown","source":"[Table of Contents](#top)"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}