{"cells":[{"metadata":{},"cell_type":"markdown","source":"[An online version of this notebook is available here](https://nbviewer.jupyter.org/github/lorenzodenisi/Heart-Failure-Clinical-Records/blob/master/Heart%20Failures.ipynb) <br>"},{"metadata":{},"cell_type":"markdown","source":"# Heart Failure clinical records \n\n\nHeart failure occurs when the heart is not able to pump enough blood to the body.<br>\nHF are only a subgroup of all the **cardiovascular diseases** that comprehend also coronary heart diseases (heart attacks), cerebrovascular diseases (strokes) and other pathologies that altogether kill every year  approximately **17 million people** around the world.<br>\n\n\n\nMachine learning applied to medical records can be useful to  predict the survival of a patient, highlighting patterns and even ranking the features to understand which are **risk factors**, possibly undetectable by doctors.<br>\nIn this notebook the analisys will be done starting from an **EDA** to understand the dataset and applying some **preprocessing** to be able to learn properly from it.<br>\nThen will follow a number of **machine learning models** trained on the preprocessed dataset, aiming to predict the **survival** of patients that suffered HF.<br>\nThe results are presented at the end of the notebook. ([click here to go to the end](#Results_and_conclusions))"},{"metadata":{},"cell_type":"markdown","source":"<a id='index'></a>\n### Index\n*  [Exploratory Data analisys](#Exploratory_data_analisys)\n\n    * [Imports](#Imports)\n    * [Dataset](#Dataset)\n    * [Feature distributions](#Feature_distributions)\n    * [Standardization](#Standardization)\n    * [Correlation matrix](#Correlation_matrix)\n    * [Normality Assumptions](#normality_assumptions)\n    * [Validate the models](#validate_the_models)\n    * [Feature selection](#Feature_selection)\n        * [Mutual information](#Mutual_information)\n        * [Chi-squared test](#Chi_squared_test)\n        * [KFold case](#KFold_case)\n    * [Class imbalance](#Class_imbalance)\n        * [Random oversampling](#oversampling)\n        * [Smote](#Smote)\n        * [Class-weight parameter](#Class_weight)\n        \n    \n*  [Classification models](#Classification_models)\n    * [Decision tree](#Decision_tree)\n    * [Random forest](#Random_forest)\n    * [Linear regression](#Linear_regression)\n    * [Logistic regression](#Logistic_regression)\n    * [Support vector machine](#Support_vector_machine)\n        * [Linear kernel](#Linear_kernel)\n        * [Polynomial and RBF kernels](#Polynomial_and_RBF_kernels)\n        * [Polynomial kernel](#Polynomial_kernel)\n        * [RBF kernel](#RBF_kernel)\n    * [K nearest neighbors](#K_nearest_neighbors)\n        * [weights=\"original\"](#weights_original)\n        * [weights=\"distance\"](#weights_distance)\n    * [Naive bayes](#Naive_bayes)\n        * [Kernel density estimation](#Kernel_density_estimation)\n        * [Naive bayes with KDE and bernoulli (Flexible bayes)](#naive_bayes_with_KDE_and_bernoulli)\n        * [Gaussian Naive Bayes](#Gaussian_naive_bayes)\n    \n    \n*  [Results and conclusions](#Results_and_conclusions)\n*  [References](#references)"},{"metadata":{},"cell_type":"markdown","source":"<a id='Exploratory_data_analisys'></a>\n\n# **Exploratory data analisys** <a style=\"text-decoration:none\" href=\"#index\">⤴</a>"},{"metadata":{},"cell_type":"markdown","source":"<a id='Imports'></a>\n## Imports <a style=\"text-decoration:none\" href=\"#Exploratory_data_analisys\">⤴</a>"},{"metadata":{"hide_input":false},"cell_type":"markdown","source":"Here are listed the main libraries used:\n* **Numpy**: standard library for math operations\n* **Scipy**: used to compute test statistics and distributions\n* **Pandas**: used to manipulate data inside dataframes and for basic computations\n* **Sklearn**: used to apply different ML models to the data\n* **Pyplot** to plot visualizations\n* **Seaborn** built on top of pyplot (nicer visualizations)\n\nOther libraries:\n* **random**: used to generate random numbers\n* **HTML and matplotlib.animation**: used for the animations"},{"metadata":{},"cell_type":"markdown","source":"<a id='Dataset'></a>\n##  Dataset <a style=\"text-decoration:none\" href=\"#Exploratory_data_analisys\">⤴</a>\n\n**Heart failure clinical records Data Set** contains the medical records of 299 patients who had heart failure.<br>\nThe dataset contains 11 clinical features (some of them are binary, others are numerical), the *follow-up* period and the label `DEATH_EVENT` that indicates whether or not the patient has died.<br>\nWe can find some features strictly related to medical aspects like levels of enzymes, sodium, creatinine and platelets in the blood and others that are more common like age, sex or smoking.\n\nThe dataset is collected in 2015 at the Allied Hospital in Faisalabad (Punjab, Pakistan)"},{"metadata":{"hide_input":true,"jupyter":{"source_hidden":true},"scrolled":false,"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport sklearn\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport matplotlib.animation as animation\nfrom IPython.core.display import HTML","execution_count":null,"outputs":[]},{"metadata":{"hide_input":true,"jupyter":{"source_hidden":true},"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"hf = pd.read_csv(\"../input/heart-failure-clinical-data/heart_failure_clinical_records_dataset.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"| Feature  | Explanation | Measurement | Range |\n| :------------- | :------------- | :------------- | :------------- |\n| `age`   | Age of the patient | Years | (40, ..., 95) |\n| `anaemia`  | Decrease of red blood cells or hemoglobin<br> (haematocrit levels were lower than 36%)  | Boolean  |  0, 1 |\n| `creatinine_phosphokinase` | Level of the CPK enzyme in the blood | mcg/L | (23, ..., 7861) |\n| `diabetes` | If the patient has diabetes | Boolean | 0, 1 |\n| `ejection_fraction` | Percentage of blood leaving the heart<br> at each contraction | Percentage | (14, ..., 80) |\n| `high_blood_pressure`  | If a patient has hypertension | Boolean | 0, 1 |\n| `platelets` | Platelets in the blood | kiloplatelets/mL | (25.01, ..., 850.00) |\n| `serum_creatinine` | Level of creatinine in the blood | mg/dL | (0.50, ..., 9.40) |\n| `serum_sodium` | Level of sodium in the blood |  mEq/L | 114, ..., 148 |\n| `sex` | Woman or man | Binary | 0, 1 |\n| `smoking` | If the patient smokes | Boolean | 0, 1 |\n| `time` | Follow-up period | Days | (4, ..., 285) |\n| `DEATH_EVENT` | If the patient died during the follow-up period |  Boolean | 0, 1 |"},{"metadata":{},"cell_type":"markdown","source":"To be consistent with the feature description let's represent the `platelets` as kiloplatelets/mL"},{"metadata":{"hide_input":true,"jupyter":{"source_hidden":true},"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"hf.platelets = hf.platelets/1000","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's take a look to the dataset"},{"metadata":{"hide_input":true,"jupyter":{"source_hidden":true},"scrolled":true,"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"hf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For brevity `creatinine_phosphokinase` will be renamed `CPK`."},{"metadata":{"scrolled":true,"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"hf[\"CPK\"] = hf[\"creatinine_phosphokinase\"]\nhf = hf.drop(\"creatinine_phosphokinase\", axis=1)","execution_count":null,"outputs":[]},{"metadata":{"code_folding":[],"hide_input":true,"jupyter":{"source_hidden":true},"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# PALETTE\n\nfrom matplotlib.colors import LinearSegmentedColormap\nimport numpy as np\nimport matplotlib\n\ndef colorFader(c1,c2,mix=0): #fade (linear interpolate) from color c1 (at mix=0) to c2 (mix=1)\n    c1=np.array(matplotlib.colors.to_rgb(c1))\n    c2=np.array(matplotlib.colors.to_rgb(c2))\n    return matplotlib.colors.to_hex((1-mix)*c1 + mix*c2)\n\nmeshPalette = []\nmeshPalette_rev = []\nnCol = 50\n\nfor i in range(nCol):\n    meshPalette.append(colorFader(\"#71706E\", \"#990303\", i/nCol))\n    meshPalette_rev.append(colorFader(\"#990303\",\"#9C9999\", i/nCol))\n\ncm = LinearSegmentedColormap.from_list(\"cmap_name\", meshPalette, N=nCol)\ncm_rev = LinearSegmentedColormap.from_list(\"cmap_name\", meshPalette_rev, N=nCol)\n\n#sns.palplot(meshPalette)\n#sns.palplot([\"#990303\", \"#9C9999\", \"#71706E\", \"#292323\", \"#FFFFFF\"]);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='Feature_distributions'></a>\n## Feature distributions <a style=\"text-decoration:none\" href=\"#Exploratory_data_analisys\">⤴</a>\nWe can see how features are distributed according to label."},{"metadata":{"jupyter":{"source_hidden":true},"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"numerical_features = [\"age\", \"CPK\", \"ejection_fraction\", \"platelets\", \"serum_creatinine\", \"serum_sodium\"]\ncategorical_features = [\"anaemia\", \"diabetes\", \"high_blood_pressure\", \"sex\", \"smoking\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Firstly we plot the **numerical** features (omitting `time` because is not used in the prediction).<br>\nIn this case we plot the kernel density estimation with a **kdeplot** to better see the distribution along with the **boxplot**."},{"metadata":{"jupyter":{"source_hidden":true},"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(18, 27))\n\nfor i, col in enumerate(numerical_features):\n    plt.subplot(6, 4, i*2+1)\n    plt.subplots_adjust(hspace =.25, wspace=.3)\n    \n    plt.grid(True)\n    plt.title(col)\n    sns.kdeplot(hf.loc[hf[\"DEATH_EVENT\"]==0, col], label=\"alive\", color = \"#990303\", shade=True, kernel='gau', cut=0)\n    sns.kdeplot(hf.loc[hf[\"DEATH_EVENT\"]==1, col], label=\"dead\",  color = \"#292323\", shade=True, kernel='gau', cut=0)\n    plt.subplot(6, 4, i*2+2) \n    sns.boxplot(y = col, data = hf, x=\"DEATH_EVENT\", palette = [\"#990303\", \"#9C9999\"])   ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see already from the unnormalized distribution plot of the features, the most informative ones seem to be `ejection_fraction` and `serum_creatinine`.<br>\nThis is confirmed by the fact that in the original paper by Chicco and Jurman [[1](#references)] the analisys has been conducted taking into account only these two features."},{"metadata":{},"cell_type":"markdown","source":"Here are reported the **categorical** features"},{"metadata":{"jupyter":{"source_hidden":true},"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12, 8))\n\nfor i, col in enumerate(categorical_features):\n    plt.subplot(2, 3, i+1)\n    plt.title(col)\n    plt.subplots_adjust(hspace =.5, wspace=.3)\n    sns.countplot(data=hf, x=col, hue=\"DEATH_EVENT\", palette = [\"#990303\", \"#9C9999\"], alpha=0.8, edgecolor=\"k\", linewidth=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='Standardization'></a>\n## Standardization <a style=\"text-decoration:none\" href=\"#Exploratory_data_analisys\">⤴</a>\nOne important step is standardization for numerical features that is performed via the standard `sklearn` function, removing the mean and scaling to unit variance.\n\n\\begin{align}\n{z = \\frac{(x−\\bar{x})}{\\hat{\\sigma}}}\n\\end{align}\n\nwhere $\\bar{x}$ is the mean of the training samples and ${\\hat{\\sigma}}$ is the sample standard deviation."},{"metadata":{"jupyter":{"source_hidden":true},"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler","execution_count":null,"outputs":[]},{"metadata":{"jupyter":{"source_hidden":true},"scrolled":false,"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"import matplotlib.image as mpimg\nfrom matplotlib import gridspec\n\nhf_norm = hf.copy()\n\nfor i, col in enumerate(numerical_features):\n    hf_norm[[col]] = StandardScaler(with_mean=True, with_std=True).fit_transform(hf_norm[[col]])\n    \nplt.figure(figsize=(16, 4))\ngs  = gridspec.GridSpec(1, 5, width_ratios=[1, 1 ,0.1, 1, 1])\nplt.subplot(gs[0])   \nplt.grid(True)\nplt.title(\"ejection fraction\")\nsns.kdeplot(hf.loc[hf[\"DEATH_EVENT\"]==0, \"ejection_fraction\"], label=\"alive\", color = \"#990303\", shade=True, kernel='gau', cut=0)\nsns.kdeplot(hf.loc[hf[\"DEATH_EVENT\"]==1, \"ejection_fraction\"], label=\"dead\",  color = \"#292323\", shade=True, kernel='gau', cut=0)\nplt.subplot(gs[1]) \nsns.boxplot(y = \"ejection_fraction\", data = hf, x=\"DEATH_EVENT\", palette = [\"#990303\", \"#9C9999\"])  \nplt.subplot(gs[2])\nplt.imshow(mpimg.imread(\"../input/heart-failure-clinical-records-images/right_arrow.png\"))\nplt.axis('off')\nplt.subplot(gs[3])\nplt.grid(True)\nplt.title(\"ejection fraction\")\nsns.kdeplot(hf_norm.loc[hf[\"DEATH_EVENT\"]==0, \"ejection_fraction\"], label=\"alive\", color = \"#990303\", shade=True, kernel='gau', cut=0)\nsns.kdeplot(hf_norm.loc[hf[\"DEATH_EVENT\"]==1, \"ejection_fraction\"], label=\"dead\",  color = \"#292323\", shade=True, kernel='gau', cut=0)\nplt.subplot(gs[4])\nsns.boxplot(y = \"ejection_fraction\", data = hf_norm, x=\"DEATH_EVENT\", palette = [\"#990303\", \"#9C9999\"]);  \nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='Correlation_matrix'></a>\n## Correlation matrix <a style=\"text-decoration:none\" href=\"#Exploratory_data_analisys\">⤴</a>\n\nTo see how features are correlated is useful to look at the correlation matrix that is a matrix in which are showed the correlation values of each couple of features according to the **Pearson's correlation coefficient**:"},{"metadata":{},"cell_type":"markdown","source":"${\\displaystyle \\rho _{X_1,X_2}={\\frac {\\operatorname {cov} (X_1,X_2)}{\\sigma _{X_1}\\sigma _{X_2}}}}$ \n\nWhere $cov$ stands for the covariance measure:\n\n${\\displaystyle \\operatorname {cov} (X_1,X_2)=\\operatorname {E}{{\\big [}(X_1-\\operatorname {E} [X_1])(X_2-\\operatorname {E} [X_2]){\\big ]}},}$\n\nThat is computed for every pair of features ${X_1}$ and ${X_2}$"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"all_features = categorical_features.copy()\nall_features.extend(numerical_features)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8, 7))\nsns.heatmap(hf_norm[all_features].corr(method='pearson'), vmin=-1, vmax=1, cmap='viridis', annot=True, fmt='.2f');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the heatmap we can see that in general, features are quite uncorrelated with the exception of `sex` and `smoking` that seems to be slightly positively correlated. <br>\nAs we will see during feature selection, this is not a problem since one of the two will be dropped."},{"metadata":{},"cell_type":"markdown","source":"<a id='normality_assumptions'></a>\n\n## Normality assumptions <a style=\"text-decoration:none\" href=\"#Exploratory_data_analisys\">⤴</a>\n\nSpeaking of the numerical features, it's interesting to notice whether or not they come from a normal distribution.<br>\nThis could be helpful for some models ([Gaussian Naive Bayes](#Gaussian_naive_bayes)) in which is assumed normality conditional to the class.<br>\nTo test it, a **Shapiro-Wilk test** is performed on all numerical features.<br>\nThe test statistic is:\n\n\\begin{align}\n{\nW = \\frac\n{\\Big( \\sum_{i=1}^{n} a_i x_{(i)}  \\Big)^2}\n{\\sum_{i=1}^{n} \\big( x_i - \\bar{x} \\big)^2}\n}\n\\end{align}\n\nwhere $x_{(i)}$ is the i-th smallest number in the sample, $\\bar{x}$ is the sample mean and $a_i$ are coefficients derived from a normal distributions.\n\nThe **null hypothesis** is that the samples are taken from a normal distribution, so with a sufficiently low value of the p-value we can consider the features as **not normal**."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"from scipy.stats import shapiro\n\n#sw_df = pd.DataFrame(columns=[\"DEATH_EVENT=0\", \"DEATH_EVENT=1\", \"Both\"])\nindex = [(feat, \"statistic\") for feat in numerical_features]\nindex.extend([(feat, \"p-value\") for feat in numerical_features])\n\nindex = pd.MultiIndex.from_tuples(index)\n\nsw_df = pd.DataFrame(index=index,  columns = [\"Both Classes\", \"DEATH_EVENT=0\", \"DEATH_EVENT=1\"])\n\nfor feat in numerical_features:\n    x = hf_norm[feat]\n    stat, p = shapiro(x)\n    \n    sw_df[\"Both Classes\"].loc[(feat, \"statistic\")] = stat\n    sw_df[\"Both Classes\"].loc[(feat, \"p-value\")] = p\n    \n    x = hf_norm.loc[hf[\"DEATH_EVENT\"]==0, feat]\n    stat, p = shapiro(x)\n    sw_df[\"DEATH_EVENT=0\"].loc[(feat, \"statistic\")] = stat\n    sw_df[\"DEATH_EVENT=0\"].loc[(feat, \"p-value\")] = p\n    \n    x = hf_norm.loc[hf[\"DEATH_EVENT\"]==1, feat]\n    stat, p = shapiro(x)\n    sw_df[\"DEATH_EVENT=1\"].loc[(feat, \"statistic\")] = stat\n    sw_df[\"DEATH_EVENT=1\"].loc[(feat, \"p-value\")] = p\n    \nsw_df = sw_df.unstack()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"pd.set_option('display.float_format', '{:.3g}'.format)\nsw_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In this case, applying an hypotethical alpha value equal to 0.05, all the numerical features could be considered non normal.<br>\nThis holds for the samples conditional on the class but also considering all of them, as we can see from the table."},{"metadata":{},"cell_type":"markdown","source":"<a id='validate_the_models'></a>\n\n## Validate the models <a style=\"text-decoration:none\" href=\"#Exploratory_data_analisys\">⤴</a>\nTo validate and test our models we can have different approaches.<br>\nThe first one is **holdout**, that consists in dividing the dataset into subsets dedicated to training, validation and test.\nThis is useful when we have lots of data.<br>\nIn this case we have 299 records so the same subset is used both for validation and testing.<br>\nAnother approach is **K-Fold** that consists in diving the dataset into K parts with an equal number of samples, using K-1 for training and the last one as validation.<br> \nThis is done K times and each part is used as validation exactly once.<br>\nThen the results are combined (averaged).<br>\n\n![kfold.png](attachment:kfold.png)","attachments":{"kfold.png":{"image/png":"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"}}},{"metadata":{},"cell_type":"markdown","source":"When the number on samples for each part is 1, the method is called **leave-1-out**."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split, StratifiedKFold","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"train_ratio = 0.75\nval_ratio = 0.25\n\nho_train_df, ho_val_df = train_test_split(hf_norm, train_size = train_ratio, random_state=42)\nunnorm_ho_train_df, unnorm_ho_val_df = train_test_split(hf, train_size = train_ratio, random_state=42)\n\nprint(\"Holdout split:\")\nprint(f\"Train samples: {len(ho_train_df)}\")\nprint(f\"Validation/Test samples: {len(ho_val_df)}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='Feature_selection'></a>\n## Feature selection <a style=\"text-decoration:none\" href=\"#Exploratory_data_analisys\">⤴</a>\n<a id='Mutual_information'></a>\n### Mutual information\n\nMutual information can be useful when it's needed to assess the dependence of a feature with respect to the target.<br>\n**MI** is always > 0  and higher values indicate stronger dependence.<br>\nFormally:\n\n\\begin{align}\n{I(X;Y) =\\sum_{y \\in Y}\\sum_{x \\in X}p_{X, Y}(x, y)log\\Bigg( \\frac {p_{X, Y}(x, y)}{p_X(x)p_Y(y)} \\Bigg)}\n\\end{align}\n\n\\begin{align}\n{I(X;Y) =\\int_{y}\\int_{x}p_{X, Y}(x, y)log\\Bigg( \\frac {p_{X, Y}(x, y)}{p_X(x)p_Y(y)} \\Bigg)dx dy}\n\\end{align}\n\n\nIn this case the mutual information is estimated for each feature with respect to the class label `DEATH_EVENT`.<br>\nMutual information is estimated with the function `mutual_info_classif` from `sklearn` that, according to the  [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_regression.html), exploits entropy estimation from k-nearest neighbors distances. "},{"metadata":{"scrolled":false,"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import mutual_info_classif\n\n    \nMI = (mutual_info_classif(ho_train_df[all_features],\n                             ho_train_df[\"DEATH_EVENT\"], n_neighbors=20,\n                             discrete_features=[True, True, True, True, True, False, False, False, False, False, False],\n                             random_state=42))\n\nplt.figure(figsize=(5.4, 4))\nplt.barh(y=all_features, width=MI, color=\"#990303\")\nplt.title(\"Mutual information w.r.t. DEATH_EVENT (whole training set)\");\nplt.xlabel(\"Mutual information\")\nplt.gca().xaxis.grid(True, linestyle=':');\nplt.tight_layout();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='Chi_squared_test'></a>\n### Chi-squared test\n\nFor the categorical features we can further test the dependence w.r.t. the target with a **chi-squared test**.<br>\nA lower value of the statistic means a stronger independence.\n\nChi-squared test is performed starting from the contingency table, for instance:"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"from scipy.stats import chi2_contingency\nfrom scipy.stats import chi2\n\nprint(\"Observed\")\nctable = pd.crosstab(ho_train_df[\"anaemia\"], ho_train_df[\"DEATH_EVENT\"])\nctable.columns = [\"DEATH_EVENT=0\", \"DEATH_EVENT=1\"]\nctable.index = [\"anaemia=0\", \"anaemia=1\"]\nctable.loc[\"Total\"] = ctable.sum()\nctable[\"Total\"] = ctable.sum(axis=1)\nctable","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"that indicates how samples are distributed among these two features. "},{"metadata":{},"cell_type":"markdown","source":"The **null hypothesis** is that `anaemia` and `DEATH_EVENT` are independent, so we compute the expected values considering them as under **the null hypothesis**, so:\n\n\\begin{align}\n{P(anaemia=0, DEATH\\_EVENT=0) = P(anaemia=0)\\times P(DEATH\\_EVENT=0) = \\frac{128 \\times 159}{224 \\times 224} = 0.406}\n\\end{align}\n\n\n\\begin{align}\n{E(anaemia=0, DEATH\\_EVENT=0) = P(anaemia=0, DEATH\\_EVENT=0) \\times N = 0.406 \\times 224 = 90.9}\n\\end{align}"},{"metadata":{"scrolled":true,"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"print(\"Expected\")\ncontingency_table = pd.crosstab(ho_train_df[\"anaemia\"], ho_train_df[\"DEATH_EVENT\"])\nstat, p, dof, expected = chi2_contingency(contingency_table)\nexpected = pd.DataFrame(expected)\nexpected.columns = [\"DEATH_EVENT=0\", \"DEATH_EVENT=1\"]\nexpected.index = [\"anaemia=0\", \"anaemia=1\"]\nexpected","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Then we know that:\n\n\\begin{align}\n{X^2 = \\sum_{i=1}^k \\frac{(o_i - e_i)^2}{e_i}}\n\\end{align}\n\nwhere $o_i$ is the observed value and $e_i$ is the expected value of the i-th combination of features.<br>\nAs the number of samples goes to infinity, $X^2$ tends to a $\\chi^2$ distribution with (columns-1)\\*(rows-1) degrees of freedom (so in this case 1).<br>\n\nIn the `scipy` implementation, the statistic is further corrected with **Yates correction** for continuity that, according to the [documentation](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.chi2_contingency.html) (and [definition](https://en.wikipedia.org/wiki/Chi-squared_test#Yates's_correction_for_continuity)), consists in subtracting 0.5 from the absolute difference, in the numerator.<br>\nIn this way, the statistic for `anaemia` is 0.617.\n<br>\n\nNow that we have the statistic we could compare it with a **chi2 distribution** with those given degrees of freedom, fixing an alpha value and keeping only the features that produce a lower p-value (rejected). \n\n"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def chi2_test(df, feat1, feat2):\n\n    contingency_table = pd.crosstab(df[feat1], df[feat2])\n    \n    stat, p, dof, expected = chi2_contingency(contingency_table)\n    prob = 0.95 # alpha=0.05\n    critical = chi2.ppf(prob, dof)\n    #print('alpha=%.3f, critical_value=%.3f,\\nstat=%.3f' % (1 - prob, critical, stat))\n\n    # interpret p-value\n    alpha = 1.0 - prob\n    \n    return stat, p","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"stats = []\np_values = []\n\nfor feat in categorical_features:\n        \n    stat, p = chi2_test(ho_train_df, feat, \"DEATH_EVENT\")\n    stats.append(stat)\n    p_values.append(p)        \n\nfig, axes = plt.subplots(ncols=2, sharey=True)\nfig.set_size_inches(8.5, 4)\naxes[0].barh(y=categorical_features, width=stats, color=\"#990303\", label=\"test statistic\", height=0.5)\naxes[0].set_title(\"Chi squared test statistics\")\n\naxes[1].barh(y=categorical_features, width=p_values, color=\"#9C9999\", label=\"p-value\", height=0.5)\naxes[1].set_title(\"Chi squared test p-values\")\n\naxes[0].xaxis.grid(True, linestyle=':');\naxes[1].xaxis.grid(True, linestyle=':');\n\naxes[0].legend(loc=1)\naxes[1].legend(loc=4)\n\nfig.subplots_adjust(wspace=0.06)\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Results\nFrom the results we can say that `smoking`, `high_blood_pressure`, `diabetes` and `platelets` can be easily dropped.<br>\nAltough `sex`and `anaemia` produce still very low values of the statistic, they are kept.\n(In this way we can see how models handle a mix of continouos and binary features)"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"kfold_train_df, kfold_val_df = ho_train_df.copy(), ho_val_df.copy()\n\nall_features = ['anaemia', 'sex', 'age', 'CPK',\n                'ejection_fraction', 'serum_creatinine', 'serum_sodium']\n\n#ho_train_df, ho_val_df = ho_train_df[all_features+[\"DEATH_EVENT\"]], ho_val_df[all_features+[\"DEATH_EVENT\"]]\n#unnorm_ho_train_df, unnorm_ho_val_df = unnorm_ho_train_df[all_features+[\"DEATH_EVENT\"]], unnorm_ho_val_df[all_features+[\"DEATH_EVENT\"]]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='KFold_case'></a>\n### KFold case\n\nWhen kfold crossvalidation is done, feature selection needs to be performed inside every iteration, in fact, performing it before would mean considering the whole training set and this can lead to an overestimation of the accuracy during cross validation.<br>\nIn this case, feature selection consists in dropping features that have a very low mutual information and chi squared statistic so it's possible to evaluate those for each iteration of KFold."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\nplt.figure(figsize=(12.5, 2.5))\n\nprint(\"Mutual information with respect to DEATH_EVENT (5 folds)\")\nfor i, (train_idx, val_idx) in enumerate(kf.split(kfold_train_df, kfold_train_df[\"DEATH_EVENT\"])):\n    MI=(mutual_info_classif(kfold_train_df.iloc[train_idx][categorical_features+numerical_features],\n                             kfold_train_df.iloc[train_idx][\"DEATH_EVENT\"], n_neighbors=20,\n                             discrete_features=[True, True, True, True, True, False, False, False, False, False, False],\n                             random_state=42))\n    plt.subplot(1, 5, i+1)\n    plt.title(f\"Iteration {i+1}\")\n    plt.barh(y=['anaemia', 'diabetes', 'h.b.p.','sex', 'smoking', 'age', 'CPK', 'e.f.', 'platelets',\n         's.c.', 's.s.'], width=MI, color=\"#990303\", label=\"test statistic\")\n    \nplt.tight_layout()  ","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\nplt.figure(figsize=(12.5, 2.5))\nprint(\"Chi squared test statistics (5 folds)\")\nfor i, (train_idx, val_idx) in enumerate(kf.split(kfold_train_df, kfold_train_df[\"DEATH_EVENT\"])):\n    stats = []\n    p_values = []\n\n    for feat in categorical_features:\n\n        stat, p = chi2_test(kfold_train_df.iloc[train_idx], feat, \"DEATH_EVENT\")\n        stats.append(stat)\n        p_values.append(p)\n    \n    plt.subplot(1, 5, i+1)\n    plt.title(f\"Iteration {i+1}\")\n    plt.barh(y=['anaemia', 'diabetes', 'h.b.p.', 'sex', 'smoking'], width=stats, color=\"#990303\", label=\"test statistic\")\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that for the mutual information, the vast majority of the cases the features are kept or dropped analogously to the holdout feature election so we stick with the original feature selection removing `platelets`. <br>\n\nFor the chi-squared test the results are quite different, so it's decided to keep the top 2 for each iteration."},{"metadata":{},"cell_type":"markdown","source":"|Iteration|Feature 1| Feature 2|\n|:---:|---|---|\n|**Iteration 1**|anaemia|diabetes|\n|**Iteration 2**|sex|smoking|\n|**Iteration 3**|anaemia|sex|\n|**Iteration 4**|anaemia|smoking|\n|**Iteration 5**|sex|smoking|"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"numerical_features = ['age', 'CPK', 'ejection_fraction', 'serum_creatinine', 'serum_sodium']\nall_features_kfold = [\n    [\"anaemia\", \"diabetes\"]+numerical_features,\n    [\"sex\", \"smoking\"]+numerical_features,\n    [\"anaemia\", \"sex\"]+numerical_features,\n    [\"anaemia\", \"smoking\"]+numerical_features,\n    [\"sex\", \"smoking\"]+numerical_features,\n]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='Class_imbalance'></a>\n## Class imbalance <a style=\"text-decoration:none\" href=\"#Exploratory_data_analisys\">⤴</a>\n\nOne thing to take into account is the possible class imbalance.<br>"},{"metadata":{"scrolled":true,"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(3, 3))\nplt.pie(hf[\"DEATH_EVENT\"].value_counts(),\n        labels = [\"alive\", \"dead\"],\n        colors = [\"#990303\", \"#9C9999\"], \n        wedgeprops={'edgecolor':'black', 'linewidth': 2}, \n        autopct = lambda y: str(round(y))+\"%\",\n        startangle=90);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see, even if not so strong, there is a class imbalance.<br>\nThis can leads to biased results that can be noticed by measures such as `recall`, `precision` or `f1`.<br>\nTo handle class inbalance it's possible to *re-balance* the dataset with different techniques.\n\n<a id='oversampling'></a>\n### Random oversampling\n\nRandom oversampling is a resample technique that consists in taking the under-represented class samples and sampling new samples from them until the classes are balanced.<br>\nDuring training, random oversampling needs to be done after the subdivision into train, validation and test to avoid **data leakage**.<br>\nThe idea is that, not being able to sample more samples from the true distribution, we sample them from the empirical distribution coming from the samples that we already have."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"n_to_sample = len(ho_train_df[ho_train_df.DEATH_EVENT==0]) - len(ho_train_df[ho_train_df.DEATH_EVENT==1])\nnew_samples = ho_train_df[ho_train_df.DEATH_EVENT==1].sample(n_to_sample, replace=True, random_state=42)\n\nho_train_df_rs = ho_train_df.append(new_samples)\n\nnew_samples = unnorm_ho_train_df[unnorm_ho_train_df.DEATH_EVENT==1].sample(n_to_sample, replace=True, random_state=42)\nunnorm_ho_train_df_rs = unnorm_ho_train_df.append(new_samples)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we can see the distribution of `ejection_fraction` for only under-represented class samples before and after oversampling. (the line is a kde)"},{"metadata":{"scrolled":true,"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"nbins = 10\nplt.figure(figsize = (8, 4))\n\nplt.subplot(1, 2, 1)\nplt.ylim(0, 1)\nplt.title(\"Distribution for class 1 before oversampling\")\nsns.distplot(ho_train_df[ho_train_df.DEATH_EVENT==1].ejection_fraction, bins=nbins)\nplt.subplot(1, 2, 2)\nplt.ylim(0, 1)\nplt.title(\"Distribution for class 1 after oversampling\")\nsns.distplot(ho_train_df_rs[ho_train_df_rs.DEATH_EVENT==1].ejection_fraction, bins=nbins);\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='Smote'></a>\n### Smote  \n\nSynthetic Minority Oversampling Technique or SMOTE is a useful technique used to deal with unbalanced datasets.\nIt consists in taking for each sample of the minority class the k-nearest neighbours an then synthesize new samples starting from the sample and one of the nearest neighbours (chosen randomly)"},{"metadata":{},"cell_type":"markdown","source":"To show how SMOTE works let's plot the samples according to `ejection_fraction` and `serum_creatinine`.\nIn the visualization below, SMOTE is performed on the whole dataset while then, when we use it during classification, we perform it only on training samples (as stated for random oversampling, using it on the whole dataset leads to data leakage).<br>The same happens in the kfold case in which resampling methods are applied inside each iteration and not before.\n<br>\n<br>\nA jittering is added on the x axis to better see the distribution."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def rand_jitter(arr):\n    np.random.seed(42)\n    stdev = .01*(max(arr)-min(arr))\n    return arr + np.random.randn(len(arr)) * stdev","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"norm = np.linalg.norm\nimport random\nfrom random import sample \n\ndef SMOTE(data, sampling_rate, n_neigh, random_state=42):\n    random.seed(random_state)\n    new_samples = []\n    \n    if sampling_rate==0:\n        return\n    \n    if sampling_rate>n_neigh: return      \n    data = data.reset_index(drop=True)\n\n    n_samples = data.count()[0]\n\n    for i in range(n_samples):\n        dists = []\n        for j in range(n_samples):\n            if i==j: continue\n            dists.append((j, norm(data.loc[i]-data.loc[j])))    \n        \n        topk = sorted(dists, key=lambda s: s[1])[:n_neigh]\n        neighs = sample(topk, sampling_rate)\n\n        for neigh in neighs:\n            alpha = random.random()\n            new_samples.append(data.loc[i] + alpha * (data.loc[neigh[0]]-data.loc[i]))\n            \n    return new_samples","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"My implementation of **SMOTE** is based on the original paper by N. V. Chawla *et al.* [[2]](#references).<br>\n\nWe can see how it works when the sample rate is higher. <br>\nIn this case the imbalance is not so strong so, just with a sample rate of 1 we obtain a good balance between classes."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"%matplotlib notebook\nplt.ioff()\nfig, ax = plt.subplots(1, 1);\n%matplotlib inline \nplt.ion()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig.set_size_inches(4, 4);\nfig.set_dpi(100)\n\ndef animate_func(i):\n    new_samples = SMOTE(hf[hf[\"DEATH_EVENT\"]==1][[\"ejection_fraction\", \"serum_creatinine\"]], sampling_rate = i, n_neigh = 20)\n    sm_hf = hf.copy()\n\n    if new_samples:\n        sm_hf = hf.append(new_samples)\n        \n    sm_hf[\"DEATH_EVENT\"].fillna(1, inplace=True)\n    \n    ax.clear();\n\n    ax.set_ylim(0, 17.5);\n    ax.set_xlim(10, 90);\n    \n    ax.scatter((sm_hf[sm_hf[\"DEATH_EVENT\"]==1][\"ejection_fraction\"]),\n            sm_hf[sm_hf[\"DEATH_EVENT\"]==1][\"serum_creatinine\"], s=5, label=\"DEAD\", color=\"#71706E\", alpha=1)\n    ax.scatter(rand_jitter(sm_hf[sm_hf[\"DEATH_EVENT\"]==0][\"ejection_fraction\"]),\n            sm_hf[sm_hf[\"DEATH_EVENT\"]==0][\"serum_creatinine\"], s=5, label=\"ALIVE\", color=\"#990303\", alpha=1)\n    ax.legend()\n    \n    ax.set_xlabel(\"ejection_fraction\")\n    ax.set_ylabel(\"serum_creatinine\")\n    ax.set_title(f\"Sampling rate: {i}, ALIVE = {sm_hf[sm_hf.DEATH_EVENT==0].count()[3]}, DEAD = {sm_hf[sm_hf.DEATH_EVENT==1].count()[3]}\")\n\n    return [fig]\n    \nanim = animation.FuncAnimation(\n                               fig, \n                               animate_func, \n                               frames = 10,\n                               interval = 100, # in ms\n                               );\n\nHTML(anim.to_jshtml())","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"n_to_sample = len(ho_train_df[ho_train_df.DEATH_EVENT==0]) - len(ho_train_df[ho_train_df.DEATH_EVENT==1])\nnew_samples = SMOTE(ho_train_df[ho_train_df[\"DEATH_EVENT\"]==1][all_features],\n                    sampling_rate = 1, n_neigh = 50)\n\n# categorical attributes need to be fixed\nfor s in new_samples:\n    s[\"anaemia\"] = np.round(s[\"anaemia\"])\n    s[\"sex\"] = np.round(s[\"sex\"])\n\nho_train_df_sm = ho_train_df.append(new_samples)\nho_train_df_sm[\"DEATH_EVENT\"].fillna(1, inplace=True)\n\n\nnew_samples = SMOTE(unnorm_ho_train_df[unnorm_ho_train_df[\"DEATH_EVENT\"]==1][all_features],\n                    sampling_rate = 1, n_neigh = 50)\n\n# categorical attributes need to be fixed\nfor s in new_samples:\n    s[\"anaemia\"] = np.round(s[\"anaemia\"])\n    s[\"sex\"] = np.round(s[\"sex\"])\n\nunnorm_ho_train_df_sm = unnorm_ho_train_df.append(new_samples)\nunnorm_ho_train_df_sm[\"DEATH_EVENT\"].fillna(1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"nbins = 10\nplt.figure(figsize = (8, 4))\n\nplt.subplot(1, 2, 1)\nplt.ylim(0, 1)\nplt.title(\"Distribution for class 1 before SMOTE\")\nsns.distplot(ho_train_df[ho_train_df.DEATH_EVENT==1].ejection_fraction, bins=nbins)\nplt.subplot(1, 2, 2)\nplt.ylim(0, 1)\nplt.title(\"Distribution for class 1 after SMOTE\")\nsns.distplot(ho_train_df_sm[ho_train_df_sm.DEATH_EVENT==1].ejection_fraction, bins=nbins)\nplt.tight_layout();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='Class_weight'></a>\n### Class-weight parameter\n\nAnother way to handle the class imbalance is class weighting.<br>\nThis is a parameter present in many models that allows to weight samples during training according to the imbalance.<br>\nThe configuration tested is `class-weight=\"balanced\"` that according to the [sklearn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) sets the weights in this way:<br>\n\n${w_i = \\frac {N} {m\\times n_i}}$\nwhere ${N}$ is the total number of samples, ${m}$ is the number of classes and ***${n_i}$*** is the number of samples belonging to class i.\n\nIn our case the weights will be:"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"w = len(ho_train_df) / (2 * np.bincount(ho_train_df.DEATH_EVENT))\nprint(f\"class 0: {w[0]} \\nclass 1: {w[1]}\")","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# KFOLD\n\nclass KFold():\n    \n    # caching smote results \n    smote_folds = None\n    smote_labels = None\n    \n    def __init__(self, all_features_kfold, random_state=42):\n        self.smote_folds = []\n        self.smote_labels = []\n        self.features = all_features_kfold\n        \n        self.kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n    \n    def fit_predict(self, model, X, y, threshold = None, resampling = None, cached = False):\n        acc, rec, pre, f1 = 0, 0, 0, 0\n        \n        if resampling==\"SMOTE\" and not cached:\n            self.smote_folds = []\n            self.smote_labels = []\n        \n        for i, (train_idx, val_idx) in enumerate(kf.split(X, y)):\n           \n            X_fold = X[self.features[i]] # for each fold we have different features\n        \n            X_train, X_val = X_fold.iloc[train_idx], X_fold.iloc[val_idx]\n            y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n\n            if resampling==\"oversampling\":\n                n_to_sample = len(X_train[y_train==0]) - len(X_train[y_train==1])\n                new_samples = X_train[y_train==1].sample(n_to_sample, replace=True, random_state=42)\n                \n                X_train = X_train.append(new_samples)\n                y_train = y_train.append(pd.Series([1]*len(new_samples)))\n                \n                X_train = X_train.reset_index(drop=True)\n                y_train = y_train.reset_index(drop=True)\n\n            if resampling==\"SMOTE\": \n                if not cached or len(self.smote_folds)<5 or len(self.smote_labels)<5:\n                    n_to_sample = len(X_train[y_train==0]) - len(X_train[y_train==1])\n                    new_samples = SMOTE(X_train[y_train==1], sampling_rate = 1, n_neigh = 50)\n\n                    # categorical attributes need to be fixed\n                    for s in new_samples:\n                        if \"anaemia\" in  s.index:\n                            s[\"anaemia\"] = np.round(s[\"anaemia\"])\n                        if \"high_blood_pressure\" in s.index:\n                            s[\"high_blood_pressure\"] = np.round(s[\"high_blood_pressure\"])\n                        if \"sex\" in  s.index:\n                            s[\"sex\"] = np.round(s[\"sex\"])\n                        if \"smoking\" in  s.index:\n                            s[\"smoking\"] = np.round(s[\"smoking\"])\n                        if \"diabetes\" in  s.index:\n                            s[\"diabetes\"] = np.round(s[\"diabetes\"])\n                                            \n                    X_train = X_train.append(new_samples, ignore_index=True)\n                    y_train = y_train.append(pd.Series([1]*len(new_samples)))\n                    \n                    X_train = X_train.reset_index(drop=True)\n                    y_train = y_train.reset_index(drop=True)\n                    \n                    # cache smoted folds\n                    self.smote_folds.append(X_train)\n                    self.smote_labels.append(y_train)\n                    \n                else:\n                    # use cached folds\n                    X_train = self.smote_folds[i]\n                    y_train = self.smote_labels[i]\n\n            model.fit(X_train, y_train)\n            preds = model.predict(X_val)\n\n            if threshold:\n                preds[preds>=threshold] = 1\n                preds[preds<threshold] = 0\n\n            acc += accuracy_score(y_val, preds)\n            pre += precision_score(y_val, preds)\n            rec += recall_score(y_val, preds)\n            f1 += f1_score(y_val, preds)\n\n        acc /= 5\n        pre /= 5\n        rec /= 5\n        f1 /= 5\n        return acc, pre, rec, f1","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# initialize kfold object\nkfold = KFold(all_features_kfold, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='Classification_models'></a>\n# Classification models <a style=\"text-decoration:none\" href=\"#index\">⤴</a>\n\nNow will follow a series of different models used to perform classification of the `DEATH_EVENT`\n\n* [Decision tree](#Decision_tree)\n* [Random forest](#Random_forest)\n* [Linear regression](#Linear_regression)\n* [Logistic regression](#Logistic_regression)\n* [Support vector machine (linear, poly, rbf)](#Support_vector_machine)\n* [K nearest neighbors](#K_nearest_neighbors)\n* [Naive bayes](#Naive_bayes)\n        \nAll models are evaluated considering the following metrics\n\n* ${accuracy = \\frac {TP+TN}{TP+TN+FP+FN}\\quad }$\n\n\n* ${precision = \\frac {TP}{TP+FP}\\quad}$\n\n\n* ${recall = \\frac {TP}{TP+FN}\\quad}$ \n\n\n* ${F_1 = 2 \\times \\frac{precision \\times recall}{precision+recall}}$\n\n\n\nMoreover, **ROC curve** is also evaluated.<br>\nReceiver operating characteristic is a plot that shows the True Positive and False positive rates applying different thresholds on the prediction (that needs to be a number between 0 and 1).<br>\nThen, model selection can be also performed according to the Area Under the Curve (**AUC**) that is the area under the roc curve. (the bigger the better)"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"from sklearn import tree\nfrom sklearn.tree import DecisionTreeClassifier, export_graphviz\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.svm import SVC, SVR\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import precision_score\n\nfrom sklearn.metrics import roc_curve","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"final_results = pd.DataFrame(columns=[\"Model\", \"Holdout Original\",\"Holdout Oversampling\", \"Holdout SMOTE\", \n                   \"Holdout class-weight=balanced\", \"KFold Original\", \"KFold Oversampling\",\n                   \"KFold SMOTE\", \"KFold class-weight=balanced\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='Decision_tree'></a>\n## Decision tree <a style=\"text-decoration:none\" href=\"#Classification_models\">⤴</a>\n\nDecision trees are one of the most widely known machine learning models.<br>\nThey are non-parametric models that learn by recursively split the predictor space (and so the train samples) according to the **best** feature (greedy approach) until the tree reaches a constrained depth, the subsets contain elements of only one class or it meets another stopping criterion (e.g. less than 5 samples in the subset).<br>\nThe **best** feature is the feature that, if used to discriminate samples, allows us to obtain the best possible split according to a measure.<br>\nSuch measure in this case is the **Gini Index** (in this sklearn implementation the unnormalized gini index is used)<br>\n\n${G = 1-\\sum_{j} p_j^2}$  \nwhere $p$ is the ratio between number of samples of class $j$ and total number of samples.\n\nGini index is evaluated on both the splits, weighted by the number of samples in each split and the feature that gives us the lower overall Gini index is chosen.<br>\n\nDue to the nature of this model, the explainability is quite high. Is in fact possible to plot the tree to see how the dataset is split at every step and it's easy to describe how the prediction works.<br>\nThe main drawback of decision tree is the fact that sometimes is a too simple model, that provides lower values of accuracy and it's easy to overfit.<br>\nTo tackle this problem it's possible to train an *ensemble* of decision trees, called **random forest**."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"p = np.linspace(1e-2, 1-1e-2, 100)\n\nplt.figure(figsize=(3, 3))\nplt.plot(p, [1-pj**2-(1-pj)**2 for pj in p], label=\"Gini Index\")\nplt.plot(p, [-((pj*np.log(pj))+((1-pj)*np.log(1-pj))) for pj in p], label=\"Shannon Index\")\nplt.grid()\nplt.legend();\nplt.xlabel(\"p0\")\nplt.title(\"Gini and shannon indexes for binary data\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is an example of how a two feature decisison tree work.<br>\ns.c stands for `serum creatinine` while e.j. stands for `ejection fraction`."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"%matplotlib notebook\nplt.ioff()\nfig, (ax1, ax2) = plt.subplots(1, 2);\n%matplotlib inline \nplt.ion()\n\ndepth = range(1, 6)\nh = 0.03\nx_min, x_max = hf[\"ejection_fraction\"].min() - 1, hf[\"ejection_fraction\"].max() + 1\ny_min, y_max = hf[\"serum_creatinine\"].min() - .5, hf[\"serum_creatinine\"].max() + .5\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                        np.arange(y_min, y_max, h))\n\nfig.set_size_inches(14, 7);\ndef animate_func(i):\n    \n    dt = DecisionTreeClassifier(criterion='gini', splitter='best', max_depth=depth[i], random_state=42, )\n    dt.fit(hf[[\"ejection_fraction\", \"serum_creatinine\"]], hf.DEATH_EVENT)\n    Z = dt.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n    \n    ax1.clear()\n    ax1.contourf(xx, yy, Z, alpha=0.5, cmap=cm_rev)\n    ax1.scatter(hf[hf[\"DEATH_EVENT\"]==0][\"ejection_fraction\"],\n                hf[hf[\"DEATH_EVENT\"]==0][\"serum_creatinine\"], label=\"alive\", color=\"#990303\", edgecolor='BLACK')\n    ax1.scatter(hf[hf[\"DEATH_EVENT\"]==1][\"ejection_fraction\"],\n                hf[hf[\"DEATH_EVENT\"]==1][\"serum_creatinine\"], label=\"dead\", color=\"#9C9999\", edgecolor='BLACK')\n\n    ax1.set_xlabel(\"ejection_fraction\")\n    ax1.set_ylabel(\"serum_creatinine\")\n    ax1.legend();\n    \n    tree.plot_tree(dt,  feature_names=[\"e.f.\", \"s.c.\"], filled=True,\n                   label='none', ax=ax2, rounded=True, proportion=True, impurity=False);\n    ax2.set_title(f\"Decision tree (depth: {i+1})\")\n    \n    fig.tight_layout()\n    return [fig]\n\nanim = animation.FuncAnimation(\n                               fig, \n                               animate_func, \n                               frames = len(depth),\n                               interval = 200, # in ms\n                               );\n\nHTML(anim.to_jshtml())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can train decision trees considering the rebalanced datasets and the original one. One further model can be trained with `class_weight=\"balanced\"`<br>\nFor decision trees and random forests, normalization is not necessary, so for the sake of visualization I use the unnormalized features."},{"metadata":{"scrolled":false,"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"depth = range(1, 12)\n\nrs_acc, rs_rec, rs_pre, rs_f1 = [], [], [], []\nsm_acc, sm_rec, sm_pre, sm_f1 = [], [], [], []\nno_rs_acc, no_rs_rec, no_rs_pre, no_rs_f1 = [], [], [], []\nw_acc, w_rec, w_pre, w_f1 = [], [], [], []\n\nfor d in depth: \n    # random oversampling \n    dt = DecisionTreeClassifier(criterion='gini', splitter='best', max_depth=d, random_state=42, class_weight=None)\n    dt.fit(unnorm_ho_train_df_rs[all_features], unnorm_ho_train_df_rs['DEATH_EVENT'])\n    pred = dt.predict(unnorm_ho_val_df[all_features])\n    rs_acc.append(accuracy_score(unnorm_ho_val_df[\"DEATH_EVENT\"], pred))\n    rs_rec.append(recall_score(unnorm_ho_val_df[\"DEATH_EVENT\"], pred))\n    rs_pre.append(precision_score(unnorm_ho_val_df[\"DEATH_EVENT\"], pred))\n    rs_f1.append(f1_score(unnorm_ho_val_df[\"DEATH_EVENT\"], pred))\n    \n    # SMOTE \n    dt = DecisionTreeClassifier(criterion='gini', splitter='best', max_depth=d, random_state=42, class_weight=None)\n    dt.fit(unnorm_ho_train_df_sm[all_features], unnorm_ho_train_df_sm['DEATH_EVENT'])\n    pred = dt.predict(unnorm_ho_val_df[all_features])\n    sm_acc.append(accuracy_score(unnorm_ho_val_df[\"DEATH_EVENT\"], pred))\n    sm_rec.append(recall_score(unnorm_ho_val_df[\"DEATH_EVENT\"], pred))\n    sm_pre.append(precision_score(unnorm_ho_val_df[\"DEATH_EVENT\"], pred))\n    sm_f1.append(f1_score(unnorm_ho_val_df[\"DEATH_EVENT\"], pred))\n    \n    # not resampled\n    dt = DecisionTreeClassifier(criterion='gini', splitter='best', max_depth=d, random_state=42, class_weight=None)\n    dt.fit(unnorm_ho_train_df[all_features], unnorm_ho_train_df['DEATH_EVENT'])\n    pred = dt.predict(unnorm_ho_val_df[all_features])\n    no_rs_acc.append(accuracy_score(unnorm_ho_val_df[\"DEATH_EVENT\"], pred))\n    no_rs_rec.append(recall_score(unnorm_ho_val_df[\"DEATH_EVENT\"], pred))\n    no_rs_pre.append(precision_score(unnorm_ho_val_df[\"DEATH_EVENT\"], pred))\n    no_rs_f1.append(f1_score(unnorm_ho_val_df[\"DEATH_EVENT\"], pred))\n \n    # weighted classes\n    dt = DecisionTreeClassifier(criterion='gini', splitter='best', max_depth=d, random_state=42, class_weight='balanced')\n    dt.fit(unnorm_ho_train_df[all_features], unnorm_ho_train_df['DEATH_EVENT'])\n    pred = dt.predict(unnorm_ho_val_df[all_features])\n    w_acc.append(accuracy_score(unnorm_ho_val_df[\"DEATH_EVENT\"], pred))\n    w_rec.append(recall_score(unnorm_ho_val_df[\"DEATH_EVENT\"], pred))\n    w_pre.append(precision_score(unnorm_ho_val_df[\"DEATH_EVENT\"], pred))\n    w_f1.append(f1_score(unnorm_ho_val_df[\"DEATH_EVENT\"], pred))","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"#sorted([(a, b) for a, b in zip(no_rs_acc, no_rs_f1)], key= lambda a: a[0]+a[1])[-1];","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10, 10))\n\n# accuracy\nplt.subplot(2, 2, 1)    \nplt.plot(list(depth), rs_acc, label=\"random oversampling\", marker='o', linewidth=2, markersize=4)\nplt.plot(list(depth), sm_acc, label=\"SMOTE\", marker='o', linewidth=2, markersize=4)\nplt.plot(list(depth), no_rs_acc, label = \"original dataset\", marker='o', linewidth=2, markersize=4)\nplt.plot(list(depth), w_acc, label = \"class_weight balanced\", marker='o', linewidth=2, markersize=4)\n\nplt.legend(loc=4)\nplt.grid(True)\nplt.ylim(0, 1)\nplt.ylabel(\"Validation accuracy\")\nplt.xlabel(\"Tree depth\")\nplt.title(\"Accuracy\")\n\n# precision\nplt.subplot(2, 2, 2)    \nplt.plot(list(depth), rs_pre, label=\"random oversampling\", marker='o', linewidth=2, markersize=4)\nplt.plot(list(depth), sm_pre, label=\"SMOTE\", marker='o', linewidth=2, markersize=4)\nplt.plot(list(depth), no_rs_pre, label = \"original dataset\", marker='o', linewidth=2, markersize=4)\nplt.plot(list(depth), w_pre, label = \"class_weight balanced\", marker='o', linewidth=2, markersize=4)\n\nplt.legend(loc=4)\nplt.grid(True)\nplt.ylim(0, 1)\nplt.ylabel(\"Validation precision\")\nplt.xlabel(\"Tree depth\")\nplt.title(\"Precision\")\n\n# recall\nplt.subplot(2, 2, 3)    \nplt.plot(list(depth), rs_rec, label=\"random oversampling\", marker='o', linewidth=2, markersize=4)\nplt.plot(list(depth), sm_rec, label=\"SMOTE\", marker='o', linewidth=2, markersize=4)\nplt.plot(list(depth), no_rs_rec, label = \"original dataset\", marker='o', linewidth=2, markersize=4)\nplt.plot(list(depth), w_rec, label = \"class_weight balanced\", marker='o', linewidth=2, markersize=4)\n\nplt.legend(loc=4)\nplt.grid(True)\nplt.ylim(0, 1)\nplt.ylabel(\"Validation recall\")\nplt.xlabel(\"Tree depth\")\nplt.title(\"Recall\")\n\n# f1 score\nplt.subplot(2, 2, 4)    \nplt.plot(list(depth), rs_f1, label=\"random oversampling\", marker='o', linewidth=2, markersize=4)\nplt.plot(list(depth), sm_f1, label=\"SMOTE\", marker='o', linewidth=2, markersize=4)\nplt.plot(list(depth), no_rs_f1, label = \"original dataset\", marker='o', linewidth=2, markersize=4)\nplt.plot(list(depth), w_f1, label = \"class_weight balanced\", marker='o', linewidth=2, markersize=4)\n\nplt.legend(loc=4)\nplt.grid(True)\nplt.ylim(0, 1)\nplt.ylabel(\"Validation f1 score\")\nplt.xlabel(\"Tree depth\")\nplt.title(\"F1 score\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The best model seems to be the decision tree on the original dataset with `class_weight = balanced` with a `depth` of 6."},{"metadata":{},"cell_type":"markdown","source":"Here we can see how the tree grows according to the depth constraint.<br>\nHaving trained the tree with unnormalized features, from the visualization we can see the split policies on the original feature values, providing so a clearer explaination."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# KFOLD\n\nrs_acc, rs_rec, rs_pre, rs_f1 = [], [], [], []\nsm_acc, sm_rec, sm_pre, sm_f1 = [], [], [], []\nno_rs_acc, no_rs_rec, no_rs_pre, no_rs_f1 = [], [], [], []\nw_acc, w_rec, w_pre, w_f1 = [], [], [], []\n\nfor d in depth: \n    # random oversampling \n    dt = DecisionTreeClassifier(criterion='gini', splitter='best', max_depth=d, random_state=42, class_weight=None)\n    acc, rec, pre, f1 = kfold.fit_predict(dt, ho_train_df, ho_train_df['DEATH_EVENT'],\n                                                          resampling=\"oversampling\")\n    rs_acc.append(acc)\n    rs_rec.append(rec)\n    rs_pre.append(pre)\n    rs_f1.append(f1)\n    \n    # SMOTE \n    dt = DecisionTreeClassifier(criterion='gini', splitter='best', max_depth=d, random_state=42, class_weight=None)\n    acc, rec, pre, f1 = kfold.fit_predict(dt, ho_train_df, ho_train_df['DEATH_EVENT'],\n                                                          cached=True, resampling=\"SMOTE\")\n    sm_acc.append(acc)\n    sm_rec.append(rec)\n    sm_pre.append(pre)\n    sm_f1.append(f1)\n    \n    # not resampled\n    dt = DecisionTreeClassifier(criterion='gini', splitter='best', max_depth=d, random_state=42, class_weight=None)\n    acc, rec, pre, f1 = kfold.fit_predict(dt, ho_train_df, ho_train_df['DEATH_EVENT'])\n    no_rs_acc.append(acc)\n    no_rs_rec.append(rec)\n    no_rs_pre.append(pre)\n    no_rs_f1.append(f1)\n \n    # weighted classes\n    dt = DecisionTreeClassifier(criterion='gini', splitter='best', max_depth=d, random_state=42, class_weight='balanced')\n    acc, rec, pre, f1 = kfold.fit_predict(dt, ho_train_df, ho_train_df['DEATH_EVENT'])\n    w_acc.append(acc)\n    w_rec.append(rec)\n    w_pre.append(pre)\n    w_f1.append(f1)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"#sorted([(a, b) for a, b in zip(w_acc, w_f1)], key= lambda a: a[0]+a[1]);","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"%matplotlib notebook\nplt.ioff()\nfig, ax = plt.subplots(1, 1);\n%matplotlib inline \nplt.ion()\n\nfig.set_size_inches(20, 15);\ndef animate_func(i):\n    dt = DecisionTreeClassifier(criterion='gini', splitter='best', max_depth=i+1, random_state=42, class_weight='balanced')\n    dt.fit(unnorm_ho_train_df[all_features], unnorm_ho_train_df['DEATH_EVENT'])\n    pred = dt.predict(unnorm_ho_val_df[all_features])\n    \n    tree.plot_tree(dt,  feature_names=all_features, filled=True, label='all', ax=ax, rounded=True, proportion=True);\n    ax.set_title(f\"Decision tree (depth: {i+1})\", fontsize=25)\n    # value are float variables due to class_weight=\"balanced\"\n    return [fig]\n\nanim = animation.FuncAnimation(\n                               fig, \n                               animate_func, \n                               frames = 11,\n                               interval = 200, # in ms\n                               );\n\nHTML(anim.to_jshtml())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='Random_forest'></a>\n## Random forest <a style=\"text-decoration:none\" href=\"#Classification_models\">⤴</a>\n\nRandom forest is an ensemble model based on a number of decision trees.<br>\nThe decision trees are trained with data, sampled **with repetition** from the original dataset (bagging).<br>\n**Bootstrapping** training data in fact allow us to decrease the variance of the model without increasing the bias (average of many trees is less sensitive to noise with respect to single tree) and helps us to train trees that are less correlated.<br>\n(Each tree will be trained on average on 63.2% of training data)<br>\n**Feature bagging** is also performed: this means that at each candidate split, only a subset of features (traditionally the square root or the log2 of the total number) is considered.<br>\nThis forces the models to select different features, increasing uncorrelation.<br>\nPrediction is then done by majority voting."},{"metadata":{"scrolled":true,"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# oob_score: Whether to use out-of-bag samples to estimate the generalization accuracy\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\nwarnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n\nmax_features='sqrt' # square root of number of features       \nmax_depth = 14\ndepth = range(1, max_depth)\nn_trees = [5, 10, 20, 50, 100]\nn_vals = len(n_trees)\n\nrs_acc, rs_oob_acc, rs_rec, rs_pre, rs_f1= [None]*n_vals, [None]*n_vals, [None]*n_vals, [None]*n_vals, [None]*n_vals\nsm_acc, sm_oob_acc, sm_rec, sm_pre, sm_f1 = [None]*n_vals, [None]*n_vals, [None]*n_vals, [None]*n_vals, [None]*n_vals\nno_rs_acc, no_rs_oob_acc, no_rs_rec, no_rs_pre, no_rs_f1= [None]*n_vals, [None]*n_vals, [None]*n_vals, [None]*n_vals, [None]*n_vals\nw_acc, w_oob_acc, w_rec, w_pre, w_f1 = [None]*n_vals, [None]*n_vals, [None]*n_vals, [None]*n_vals, [None]*n_vals\n\nfor i,n in enumerate(n_trees):\n    \n    rs_acc[i], rs_oob_acc[i], rs_rec[i], rs_pre[i], rs_f1[i] = [], [], [], [], []\n    sm_acc[i], sm_oob_acc[i], sm_rec[i], sm_pre[i], sm_f1[i] = [], [], [], [], []\n    no_rs_acc[i], no_rs_oob_acc[i], no_rs_rec[i], no_rs_pre[i], no_rs_f1[i] = [], [], [], [], []\n    w_acc[i], w_oob_acc[i], w_rec[i], w_pre[i], w_f1[i] = [], [], [], [], []\n\n    for d in depth: \n        # random oversampling\n        rf = RandomForestClassifier(n_estimators=n, criterion='gini', max_depth=d, max_features=max_features,\n                                    oob_score=True, random_state=42, class_weight=None)\n        \n        rf.fit(ho_train_df_rs[all_features], ho_train_df_rs['DEATH_EVENT']);\n        pred = rf.predict(ho_val_df[all_features]);\n        rs_acc[i].append(accuracy_score(ho_val_df[\"DEATH_EVENT\"], pred))\n        rs_oob_acc[i].append(rf.oob_score_)\n        rs_rec[i].append(recall_score(ho_val_df[\"DEATH_EVENT\"], pred))\n        rs_pre[i].append(precision_score(ho_val_df[\"DEATH_EVENT\"], pred))\n        rs_f1[i].append(f1_score(ho_val_df[\"DEATH_EVENT\"], pred))\n\n        # SMOTE\n        rf = RandomForestClassifier(n_estimators=n, criterion='gini', max_depth=d, max_features=max_features,\n                                    oob_score=True, random_state=42, class_weight=None)\n\n        rf.fit(ho_train_df_sm[all_features], ho_train_df_sm['DEATH_EVENT']);\n        pred = rf.predict(ho_val_df[all_features]);\n        sm_acc[i].append(accuracy_score(ho_val_df[\"DEATH_EVENT\"], pred))\n        sm_oob_acc[i].append(rf.oob_score_)\n        sm_rec[i].append(recall_score(ho_val_df[\"DEATH_EVENT\"], pred))\n        sm_pre[i].append(precision_score(ho_val_df[\"DEATH_EVENT\"], pred))\n        sm_f1[i].append(f1_score(ho_val_df[\"DEATH_EVENT\"], pred))\n        \n        # no random oversampling\n        rf = RandomForestClassifier(n_estimators=n, criterion='gini', max_depth=d, max_features=max_features,\n                                    oob_score=True, random_state=42, class_weight=None)\n\n        rf.fit(ho_train_df[all_features], ho_train_df['DEATH_EVENT']);\n        pred = rf.predict(ho_val_df[all_features]);\n        no_rs_acc[i].append(accuracy_score(ho_val_df[\"DEATH_EVENT\"], pred))\n        no_rs_oob_acc[i].append(rf.oob_score_)\n        no_rs_rec[i].append(recall_score(ho_val_df[\"DEATH_EVENT\"], pred))\n        no_rs_pre[i].append(precision_score(ho_val_df[\"DEATH_EVENT\"], pred))\n        no_rs_f1[i].append(f1_score(ho_val_df[\"DEATH_EVENT\"], pred))\n\n        # class weight\n        rf = RandomForestClassifier(n_estimators=n, criterion='gini', max_depth=d, max_features=max_features,\n                                    oob_score=True, random_state=42, class_weight='balanced')\n\n        rf.fit(ho_train_df[all_features], ho_train_df['DEATH_EVENT']);\n        pred = rf.predict(ho_val_df[all_features]);\n        w_acc[i].append(accuracy_score(ho_val_df[\"DEATH_EVENT\"], pred))\n        w_oob_acc[i].append(rf.oob_score_)\n        w_rec[i].append(recall_score(ho_val_df[\"DEATH_EVENT\"], pred))\n        w_pre[i].append(precision_score(ho_val_df[\"DEATH_EVENT\"], pred))\n        w_f1[i].append(f1_score(ho_val_df[\"DEATH_EVENT\"], pred))","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"%matplotlib notebook\nplt.ioff()\nfig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2);\n%matplotlib inline \nplt.ion()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig.set_size_inches(7, 7);\ndef animate_func(i):  \n    # accuracy\n    ax1.clear();\n    ax1.plot(list(depth), rs_acc[i], label=\"random oversampling\", linewidth=1.5, marker='o', markersize=3);\n    ax1.plot(list(depth), sm_acc[i], label=\"SMOTE\", linewidth=1.5, marker='o', markersize=3);\n    ax1.plot(list(depth), no_rs_acc[i], label = \"original dataset\", linewidth=1.5, marker='o', markersize=3);\n    ax1.plot(list(depth), w_acc[i], label = \"class_weight balanced\", linewidth=1.5, marker='o', markersize=3);\n\n    ax1.plot(list(depth), rs_oob_acc[i], label=\"random oversampling OOB\", linewidth=1, linestyle='dashed', alpha=0.8, color = \"#287CB7\");\n    ax1.plot(list(depth), sm_oob_acc[i], label=\"SMOTE OOB\", linewidth=1, linestyle='dashed', alpha=0.8, color = \"#FE8417\");\n    ax1.plot(list(depth), no_rs_oob_acc[i], label = \"original dataset OOB\", linewidth=1, linestyle='dashed', alpha=0.8, color = \"#34A334\");\n    ax1.plot(list(depth), w_oob_acc[i], label = \"class_weight balanced OOB\", linewidth=1, linestyle='dashed', alpha=0.8, color = \"#D62728\");\n\n    ax1.legend(fontsize=7, loc=3);\n    ax1.grid(True);\n    ax1.set_ylim(0, 1);\n    ax1.set_ylabel(\"Validation accuracy\");\n    ax1.set_xlabel(\"Single tree depth\");\n    ax1.set_title(f\"Accuracy (n_trees: {n_trees[i]})\");\n\n    # precision\n    ax2.clear();\n    ax2.plot(list(depth), rs_pre[i], label=\"random oversampling\", marker='o', linewidth=1.5, markersize=3);\n    ax2.plot(list(depth), sm_pre[i], label=\"SMOTE\", marker='o', linewidth=1.5, markersize=3);\n    ax2.plot(list(depth), no_rs_pre[i], label = \"original dataset\", marker='o', linewidth=1.5, markersize=3);\n    ax2.plot(list(depth), w_pre[i], label = \"class_weight balanced\", marker='o', linewidth=1.5, markersize=3);\n\n    ax2.legend(fontsize=7, loc=3);\n    ax2.grid(True);\n    ax2.set_ylim(0, 1);\n    ax2.set_ylabel(\"Validation precision\");\n    ax2.set_xlabel(\"Single tree depth\");\n    ax2.set_title(f\"Precision (n_trees: {n_trees[i]})\");\n\n    # recall\n    ax3.clear();\n    ax3.plot(list(depth), rs_rec[i], label=\"random oversampling\", marker='o', linewidth=1.5, markersize=3);\n    ax3.plot(list(depth), sm_rec[i], label=\"SMOTE\", marker='o', linewidth=1.5, markersize=3);\n    ax3.plot(list(depth), no_rs_rec[i], label = \"original dataset\", marker='o', linewidth=1.5, markersize=3);\n    ax3.plot(list(depth), w_rec[i], label = \"class_weight balanced\", marker='o', linewidth=1.5, markersize=3);\n\n    ax3.legend(fontsize=7, loc=4);\n    ax3.grid(True);\n    ax3.set_ylim(0, 1);\n    ax3.set_ylabel(\"Validation recall\");\n    ax3.set_xlabel(\"Single tree depth\");\n    ax3.set_title(f\"Recall (n_trees: {n_trees[i]})\");\n\n    # f1 score\n    ax4.clear();\n    ax4.plot(list(depth), rs_f1[i], label=\"random oversampling\", marker='o', linewidth=1.5, markersize=3);\n    ax4.plot(list(depth), sm_f1[i], label=\"SMOTE\", marker='o', linewidth=1.5, markersize=3);\n    ax4.plot(list(depth), no_rs_f1[i], label = \"original dataset\", marker='o', linewidth=1.5, markersize=3);\n    ax4.plot(list(depth), w_f1[i], label = \"class_weight balanced\", marker='o', linewidth=1.5, markersize=3);\n\n    ax4.legend(fontsize=7, loc=4);\n    ax4.grid(True);\n    ax4.set_ylim(0, 1);\n    ax4.set_ylabel(\"Validation f1 score\");\n    ax4.set_xlabel(\"Single tree depth\");\n    ax4.set_title(f\"F1 score (n_trees: {n_trees[i]})\");\n    \n    fig.tight_layout()\n    return [fig];\n\nanim = animation.FuncAnimation(\n                               fig, \n                               animate_func, \n                               frames = n_vals,\n                               interval = 100, # in ms\n                               );\nHTML(anim.to_jshtml())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Out Of Bag score (OOB)\nOut of bag score is equal to 1 - the **out of bag error**.<br>\nConsidering a sample ${z_i=(x_i, y_i)}$ the out of bag error is the average error for each ${z_i}$ evaluated using only trees that **do not** contain ${z_i}$ in their training set.<br>\nSo, for each sample, we compute the error considering only trees not trained on that sample.<br>\nThis allows us to do a sort of vaidation during training."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# KFOLD \n\nrs_acc, rs_oob_acc, rs_rec, rs_pre, rs_f1= [None]*n_vals, [None]*n_vals, [None]*n_vals, [None]*n_vals, [None]*n_vals\nsm_acc, sm_oob_acc, sm_rec, sm_pre, sm_f1 = [None]*n_vals, [None]*n_vals, [None]*n_vals, [None]*n_vals, [None]*n_vals\nw_acc, w_oob_acc, w_rec, w_pre, w_f1 = [None]*n_vals, [None]*n_vals, [None]*n_vals, [None]*n_vals, [None]*n_vals\n\nfor i,n in enumerate(n_trees):\n    \n    rs_acc[i], rs_oob_acc[i], rs_rec[i], rs_pre[i], rs_f1[i] = [], [], [], [], []\n    sm_acc[i], sm_oob_acc[i], sm_rec[i], sm_pre[i], sm_f1[i] = [], [], [], [], []\n    w_acc[i], w_oob_acc[i], w_rec[i], w_pre[i], w_f1[i] = [], [], [], [], []\n\n    for d in depth: \n        # random oversampling\n        rf = RandomForestClassifier(n_estimators=n, criterion='gini', max_depth=d, max_features=max_features,\n                                     random_state=42, class_weight=None)\n        \n        acc, rec, pre, f1 = kfold.fit_predict(rf, ho_train_df, ho_train_df['DEATH_EVENT'],\n                                                          resampling=\"oversampling\")\n        rf.fit(ho_train_df_rs[all_features], ho_train_df_rs['DEATH_EVENT']);\n        rs_acc[i].append(acc)\n        \n        rs_rec[i].append(rec)\n        rs_pre[i].append(pre)\n        rs_f1[i].append(f1)\n\n        # SMOTE\n        rf = RandomForestClassifier(n_estimators=n, criterion='gini', max_depth=d, max_features=max_features,\n                                    random_state=42, class_weight=None)\n\n        acc, rec, pre, f1 = kfold.fit_predict(rf, ho_train_df, ho_train_df['DEATH_EVENT'], \n                                               resampling=\"SMOTE\", cached=True)\n        sm_acc[i].append(acc)\n        sm_rec[i].append(rec)\n        sm_pre[i].append(pre)\n        sm_f1[i].append(f1)\n        \n        # no random oversampling\n        rf = RandomForestClassifier(n_estimators=n, criterion='gini', max_depth=d, max_features=max_features,\n                                     random_state=42, class_weight=None)\n\n        acc, rec, pre, f1 = kfold.fit_predict(rf, ho_train_df, ho_train_df['DEATH_EVENT'])\n        no_rs_acc[i].append(acc)\n        no_rs_rec[i].append(rec)\n        no_rs_pre[i].append(pre)\n        no_rs_f1[i].append(f1)\n\n        # class weight\n        rf = RandomForestClassifier(n_estimators=n, criterion='gini', max_depth=d, max_features=max_features,\n                                     random_state=42, class_weight='balanced')\n\n        acc, rec, pre, f1 = kfold.fit_predict(rf, ho_train_df, ho_train_df['DEATH_EVENT'])\n        w_acc[i].append(acc)\n        w_rec[i].append(rec)\n        w_pre[i].append(pre)\n        w_f1[i].append(f1)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"#acc = [a for b in w_acc for a in b]\n#f1 =  [a for b in w_f1 for a in b]\n#sorted([(a, b) for a, b in zip(acc, f1)], key=lambda a: a[0]+a[1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='Linear_regression'></a>\n## Linear Regression <a style=\"text-decoration:none\" href=\"#Classification_models\">⤴</a>\n\nLinear regression is one of the simplest models in machine learning.<br>\nIt is a generalized linear model capable of fitting a linear equation to observed data.<br>\n\n![simple_linear_regression_scaled2.png](attachment:simple_linear_regression_scaled2.png)\n\n**Generalized Linear Models** are based on the following equation:<br>\n${Y=X \\beta + \\epsilon}$ \n<br>\nin which $Y$ is the response vector, $X$ is the matrix of predictors, $\\beta$ is a set of unknown parameters and $\\epsilon$ is a set of unobservable random variables called errors.","attachments":{"simple_linear_regression_scaled2.png":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAZAAAAFbCAIAAAB08k/sAAAXdXpUWHRSYXcgcHJvZmlsZSB0eXBlIGV4aWYAAHjarZrpcSO7koX/w4oxAftiDtaI58GYP98BSYlSS/f1jZhWi6SKVYUlM88ClNn/+59j/od/2bdoYio1t5wt/2KLzXc+VPv41+6rs/G+vg6559Evx00az4+e98B7eHxR+uPddY6nzwtebbjx9bipz298fd7o+cXrhkEtez6s905y3D+Ou/i8UduPD7nV8t7V4R/v83ni7crzd+57a+uejelv834gFmZpJc4K3u/A4ftaHz0Ij9/Ob+XVBcd5LmQ+p9DM/aI8e8KEfBne693a9wn6MsmvT+b77K/nUL9Pvu/PM8K3uczPOeLDj1+49PPk3yl+azh89Mh//SIdV/4YzvP3nFXP2Y/R9ZiZ0fzMKGtes6NrOHEw5eFelvkp/CY+l/vT+Km220nIl5128DNdc56oHOOiW6674/Z9n27Sxei3L7x7P324x2oovvkZFKeoH3d8CS0sIujD9NuEwGH/0Rd32223vekqLS/Hqd5xM8clv/6Yf/ry3/yYc6amyGkyCb17BNgrc+mGIqdXziIg7jzjlu4Ev36e4bdviUWqEsF0p7kywG7H4xYjuc/cCjfOgfMS748Scqas5w2YItpOdIbkj85mF5LLzhbvi3PMYyVAnZ77EP0gAi4lv+ikjyFkb4qvXm1zTXH3XJ989joMNhGIRDUVYtNCJ1gxJvKnxEoO9RRSTCnlVFI1qaWeQ4455ZxLFsj1EkosqeRSSi2t9BpqrKnmWmqtrfbmWwADU8uttNpa692bTkOde3XO7xwZfoQRRxp5lFFHG32SPjPONPMss842+/IrLGBi5VVWXW317cwGKXbcaedddt1t90OunXDiSSefcuppp39E7RnVP37+RdTcM2r+RkrnlY+ocdSU8rqFE5wkxYyI+eiIeFEESGivmNnqYvSKnGJmm6cokqeTSbExyylihDBu5yn9j9h9Ru6v4mZS/au4+f8WOaPQ/X9EzhC6P+P2Q9SWeG7eiD2qUHNqA9XHOd1Xw6+1vPzX93lW3mfvEOfyO45Oj5MrKdTCNFUTR0tzbyCzzZpymNQOYMoLFQetODgd8C0zhlHyaMHOQVBDI9JM+Bqh5hnKNqfMaUvZQGE+zTNL82TuFojqJopzNR9TCJ4Epku1pVH2IdLJR3v8LEMwGDd5NO0W/7pld2nMxO659p3iqduvHI89uZ9OMFaP4OvJJbU4qNNCQlHhlUro3ax5mDAnLK0nhtqLPW6sNhEmYa3F3LhYXJidGPQMsEcfGy3P493wxPBOhzXkVmAuVkyOueqlEuGkPrlM9Ae3i2vaPjqdDG4cknWM7og4wOR9n2udWrc1wzt72iKtuPMaufmNQOnjQCF5EYZDGsEfjNm1PeYOGv5q3Y/GYGqcJArlYGCHvPqxYeQYcgZI5whn0IswyRCfI1jY3NQEutZcmSRzPYwanKT5nvZiRN7U41fZqTBf24V9SvKdEa7QZi5hIqXyWYx6rlkq6ewDd3Gpzz1n9kQmzV6jLyZQmM2S2DXTKBR7yICdIcxN+576JO82FdMtBds7ZcU89ji3mzldLh9UY+VGQfFvyr6Vkouz9pgaidiS3YkoBN9ODvF0yTrl+I/v5o8vum+buYkz25gPoDMyRFPcrvEUMoKS8YnTBuUH7lCJoY1DZpNlZ/tez4hxKKfCBkTOOrHpr5mYplTfrqSYHtfeS0nge7G5V79f++eV79f92qT5bJO68IDFGBUFOBoZy0yV7VRrdvayu10h7OX87elpc51TFkVzXIow7Y+tAjlpoWCQUzYVijwKzeewwDaqc5DWboVOxa/T2kndSII5d7jxXsen3ZBPhRmeJFfLAU1TbWqrAfiwRp11Ez9V9wD3aitnl0m2aGhjt6DO1t5/7Tjg0DfYTt6m6T2VNmwaaOkWqnPIsmQSFVCzEmlSSntEUHAyBFJg2U6/farg4izUPLN2KNKLF5PU7SuMV9Pme9u/zpmNd84quADeaNIYNxUPMfSQqxG41A0h0NhcqbU89+oMGg45LqgzMx3wJ4A/FAX8UJD+gF3+klfme2L9kle2hRlcj7H1quJrnJga+Bv7pAID7gh+mRYY4TqmCl0LRNh8eSIt0D0AU1yQxpjM906ivAhhE1dLAIu62ssx/5T2P/cu0malFaTGOuRFbmQVPZpgN3DkivILwiVofLR5z0zVj0NOxboOVwI1vSWIRpg1Z1AZ90VjyifzSqi/zafQ+UcIEbYpyUHAPhaKMbDxe4I+7vZ5r+edsKb3XjI73C2DN11/fV5q/r4vz6t/adb8u3Z/b9b81C401SIUG5FazKvgNrshyfNFWfRT3KGsFq0MgzE6stCud4KLHDqSM4Q9LXsTAWOjOW6AQ2JSY1iqyIF0BOCTTDY1kY6hSXVHCPblW/wvukj8A+WUb5fWcVB0yyPXuHBTvsn4Je9e1wroxzg+PjEXuijpW5rMZA2akNmppJkWAbAPj7uiI0sfICSCCxoac2XqdZaUuFOUCo+uhI4ytXfqck5huFKC2Gx6iG9IAXUGutLYFK0IhnSPvTEjUFwNQJjk64iLkkRHwFScC9aS2Lu37OElD1wirT0lW1cf01hpkjI2mrlISoWEcuCE7s/akgyLIZRZDkV6ZgCoaHhA3mN3Yd6ByfGl22x7NkIZ8lGPVV7bZiQwY1SkkMzVvSYZEbfQjcidCeyGmtAb/kD+zQYz3AYDkWVzN5/bDpw1cp1ogo3kQX2dakNCjAN/FPfugkhKPduAFDsTgPLgskEae0B4I4LQwKQTqFpQ1QiJxhyQO0i/mwcEKm+mOD5ehyKK9w6lkF/LzLCQO9ufSVaAchhdDtARZZ/Wgmi/SCpBHzpoGfL9DmBa9+L72kyWeCCIdnXNSYRoS1/2I+Ne90Zlwb2qirhhzd0QfFqSuZ3mDMMp916P2ze+WJrt4QRfjlgpE+XAP1sfGBU7kGwn2YWsZe7yMn4cTb0oH/UJ3uaEAMBq2Y2UYgpsHtKxBNaJYeJhboKPzCdz1KhLtxpzZkC6duBoMovLpyT4vi6ykzCtzIGI3zn5JGqCqRu3WhfXyR6AVsEhf+JdZJlSKC3WvhxyuziSCuu0KfNIZjjqPrrVn7NNCLBVdL74sX5MSB0Foh7HX4c/s9Hy5yBF6qnjAps06+2ZYhVMSmiL81M2TacgxlpGuRH5gDQGCY4I8eNn3piPbz8ShGEwDV8vfaEO8EQNLnfizSTfLHJ9op2IGrHUx3eAQVLVzMlYM8pGogjNgupGbxdA/EjgITcg44g4R+vAiAbhc5tIAexuBbBA9vSwUh1QLCYLzx0uHOsYUTxVN4OlFRXv/LRNbZi0hoqVLueDGMc9oMZlwKtsreRPmXgFNFbGebtQyDCyAR9PjS2yD4hKdNxgF+tc1HKRm7m2agSBZT8jJWAvUF6NxF61xYMkiXwFoVtsfSZdFxyBg3AmofGX6phkBhcKQLJ6AyD6U8R3ZP/REpwF3CrIhoHtj4Ok4Mdhc4+/aX8A7iHtZINIBcUS9xBEVYSTKkTfoJvlvCIUxPwVKhDpRxcqxpO5xGjahiDD8IY06SLXJLk97s/tApbYEipyuotjnvlFBjBQwo86fjAWcjlTBkwNF34wndhvbSjbPvqN07+/qrVnvjWloFmgz+fNMlq0kqs/5CCJ3tSMn2s7egNqQEodAMfR+WPgM7wo5ri6hADM3bqKkobOYI3art8FdZ5pqyZTBZIQooR6oJYQBYIgs/PC3ZEBaHBSbC9VRkKdiHNxl0h9Khz8j42yBu86YElaIda1hlO4vOMWmplpxogSB2ndqNS8vx4Wyq1hbSTGnqtgNq6goBUtnEB+JBvOEcVMZNHyecP9Y8aC8oSPSNs9qRumjQHEQZCAJWGgY5iS2OFoNQezhKntQu1NBwHClcyJpQh0xrTDOfmYBk4MMCdHbG5BSy1llY2wnHLf1u1jpahjSTuSwoLduEygCqv0hVP0g/5IEh6I8QVlMJaojIADiSd5HmF/lNIBUbkyUTGFM0s3NKdq+EyPvyyJz4roiQrEr7lxGNBioCGOYmGvyqBOAT2gAiragTChzXbnb4ljfiok86ok5nCVR6sHEJP0i2/SL7+I27unBvyszwu4RojbPkD47dsvBfC6FHHUl3gi19SzBEQmyDi5ZUgvx9AVWxDOWiJQSdXUUGOhDJ9Hh5830igz6oUnOllrzd5OoVoAD5FsVH/AEg1EkIUOsnRanXZHJzWaGgZEYYwDpPNjwLRonSluES8n+C1CzHd3CDxCJGKlgqXdAaRPpGPbWhuinOhIyAv1GOum1CgDQfPS3tjCzZ5SG4UwkVEQ5PYYuZSrn1yaxpIci161BuV73K00LqisBVeQQVIZvtDqGZUPsNE63tZwpVYF6SiKieHD2h1CIh9HIOcGogY60bIY9hUiRcfXKPWG8n/spVDjvRYTqTLka2L4uIzh4BM4qS0bptzDBl72SXIQZYMuA2BPoD16DYkLHaBppTwBfwJxSsub7AV8OiUXuQwuak6LjGgfmqbQEKv0C4WJuoAwIPem1V7cdGDCphH6Wcx8BMoCCjwSFF6aFbGHS7cXb+NDcKn4HizR+hXq0umNq83HX28n/VJZP1bhK4XNR+E8ywaRdxmI271XzrNumFhVzg91Y34snFs32rcEcKTEPqxRJviuz9VhrTwcaSVr18FAM0vGu2FztfQLAiPomWR/U71eg0U/UdJawiRJWuY+cQJy6HqbgG/rCYRNBuFHkHzHz2gxUtj/uHwuBlzbTuTDzRHAwmpBlDJEQOy4NCOtTkjLDceNHCSAxgY/Qe3qJuWKi3VdVou/fI+IO+RD0/aoFm0A8eS4Xy6TSKNgc8RCINULMAKJH8h3Qhl+L0oNQRtcDwldWiV6kI8g+4TxoUyKhL5C2s1iCwDyRK0VlIBtqlwfKTsUL3CJwNWabiAhlxbFFqGpBKUj3+7SPCMnsRfT2+gstGEo6505PqIy3MrM9DmBqCq3hkzeErPSf57R4brnHDOj0hTNRjNb+vpEQ8V8kDx9yrAtTEFoeMf9eW7Qe5USv0sXTxPSQ58+K0W9w7RchGQ0KJbgl0x+KxkTpRXEdWD/lJBdP3nl+IdXNm9aFjOYtXMh6ftGX5/3kNa6axAH/DxrujOfqwXumPPtqJp8XJYTbDwgZaxJIX7yNBkM03BGOihawr4ATEgsTzPQaCGXLonLjQgcFVkAP5U+ihXily1D3yIN+LPL5/5gJczDS2hXZHMqU35g+/hJXvEgZxBfen04OyZaS+vQC0quldBzouLRR1wwZ0Y84MOcLDOUPyt+agH7gULylfqG3zMMESnVmbuWChPpTfKRzvDOwdSg3ygtj3goK2RmdgXxBV1coC8khJlDS2XrCno/a70OOX97D9ISjoU3rCZoFRCTjkgbDH4CttB9QJRTCAPZT54PEQGDSiBBSNBkCQl1gwwjnnSWgAeDKvFKI/jCai2Yliw8Jf9BujVAbiWtaxT9p34w/pbySzZHgESLbU7rFd7gQCnxuxigrZnMvOJa0spwqjYsYOmNHGQ8uOUcbpF1aF4rLBQwcEzFhCZg6/A4akX+5UQitkn+4eSZK9yyehbNuymqR83B5hGew8GQHdJdWh+dMxi6q/WVqxaw4L5j7msEt0thBhTnBgYRL0rBzZiRTbA0SFt3u9TD2/apmYRW7LVuoIXUsLAWIULpd9EpJyO9sjaFCDhhDLNKgzimv+9AHZehVVQ8lHGIBL6IvsLKkDrzDOeXtqq2bxL5Xjg7ab0n30UOp3kgoiVeGYn0SVq3MJ8LFzoz0m/YYH1dtrgy9XUjztxZp+5yh6cRggfmyaD2UQnSrs8ljvtRS7aDtofunGb+IFN7Ve7n6d28sSd59g4Mv+OC++LxuSlCwHyuQq5/WCAAk+ybP3uuJ6Hvx0Y3NZS1QTSe2fiqRbBEK3Yh2LGkMfbQwyPaNg3zscHWCVPBHZUS4ESHfxhgCxrtBKPHEZAray8bw6XP4DFYKE8tfmB0C6IFiEJw4nNJSAEoeF7BGa/ldviYHBioWkqPPDqoO+28+lRrpSJt1VL68IAk+a6HDLia6e5XElPkvW8t5GASK26vmaa1FCoZJNKOrUc3MCpwFuE9U+jMBYJgB8w/+Xy3DqIehhGcomGB44YBmdsQ4AWB49XDQtB1oRB2mGIr9LvYyV8Fzwgvwc7AtZbVc9QyEiam7YxtK05zNAi9YHDkBVllfQb9EBZagIwFn4sEwIVPKaki7YNV+XNJ2PzzmvBfLgmT1ebFc/X7mrDaehmzSA/GY3n48xSX8KtHs4dGLoaMRvuRq29HrR43uoff+vtc0BI93ox95KuylQiuZuRsvDiroFiaOl+3prJp4Y5eqshXwcIvn4AJqT1OOtqnh/PwJoRuT8xxkyyOVnss+8FmxFiLfRE3DE3RuaXn1LTU148ebejY2YbJgWECwdhMp2/ODISbzH07sCJtDyoWuAcMRBCt77y0CdbHxqdbLTwQNC0AetiPDNbGE+HzhvPBceA2kr2zC2oebFHQk4Dh0NICZDVE4uBNSjI3OFPYVaq1JK+V0mrqSdKaUHy6u6LJXvGXu+wslQw4k8SrbRKZonWTBvQ8HSbYaffhbj5kuw00B/s4bWHbqMjmgPLT6XKJrw2IKs4imVMPeJxHGnnO1HMWOt8685l6IWn9U0+C+VZep3/k4qmcgOYVY96VbHl8nXUbRmh9epdHA/TyqPXrYNar3UePrR5SeaoxAA9bvEOk57BnM+GZr99a+LIO+9+WYbXwaX7aB/qyDRQxtNjW7SioDb/trUcLElVTtaLHQfoeyKOUtWS1D+bIdZf5j0qGG0qOOWv7EJvSHA6OHJViR3OloTXosU7V8lDpBBeE1AoWjB3btOucB+xH/tKeokZ9j4xAsPfRXu+26X0DX8/4aNfN4BSA/0c5Io5eG2/n90vby4hhicleiqcXYCSvWehr6HBdjgjJy1QDAk13Cy5/tvRs527R6WkrPcJ0OJ/7jWMeCyvVpdcW9Pn9Uv/CRSVw3No6KK0hmbA7Zmw9n4S3UvWpB0PCzn+53/ug/+iKekI/zMee4z9d6j9AGFuiBWwQv8641JmGkFk7GvXmQsIdWDurnQdGYkz1DuJ/7CVb6c7niiT5/aVP5jE5VxiAftCl1XMeDjnngSDtiIzWyLotrwPICnAQyjg6PXxxW71tmvNo8m5evzfxHgCMef8wQt+68jrPcCKKn56UvbSighxyzJZkawHOyGhtxidypZ+e98ELWwr8rUOPaTDf5+F7NN43gPvbMkYfX7PMfMmVKoA+EHLWk0aqtyrMzI5O1oLcyugtsJ2GFmC/w/NxsMTQ4K7EPW/ImS/sxcFcbyq4MItFSVlpaEoe/mj7AnZ1ZWdq42JA1msxxHEf7Wz8+GiPZggFDRadOZjG+1Rq4kZ6YiAYS6ETZXviVOAhxcHIAHSEi29YrtMIv3wPGgsU7C80e2zU7OibR50ck91jNzEXRDbEqP15XFrFMmwcEyNEGNGFJkkfz1LOxI6B7zlnMD0J5PAqJoekp9gWKYCUXk8tfSFTqxLcFIfRtZWLjls7UQWkpdOa3bNfs1aPXRdtDtdShhIJlJNjR4oA/5lgVfS7njLGC1BEemZilde+R09aivGNcMI1pjL6kqEC8Au/oEWAlPdtSfozLZAWiYpZkloT6GNK9TDaBJIxWKQQQ87RYJmc19p5ej0PMKW/PTOF3XWW1Bp6VKvVqEVVPeV50kTIkv3aItQ1kEEz+4QhK4eazs5zMXpwBzLCo5FED01PDgGy/TmkATswJ3qSDN8bo0zwLlGqFqfjHguTiHtfiXNzbUFEtA1NIxWAIa1e5iLJ+lrSQwoH1AckxOCm6SNgw5lubThbVNZ9SjroWUaRgCN0XUZSOl4rVnqW/z6tufQcYAnI9YuuxqPdk+Ry+qLz/vW7+fELh9LauEx0jtMqNSoJXY4+IisohKBnFhAU9pnMusjEd/v0l++U8tFStPk/jmM/RDC46D4AAAGFaUNDUElDQyBwcm9maWxlAAB4nH2RPUjDQBzFX1PFolUHO4g4ZKhOFkVFxEmrUIQKoVZo1cHk0i9o0pCkuDgKrgUHPxarDi7Oujq4CoLgB4iTo5Oii5T4v6TQItaD4368u/e4ewcI1SLTrLYxQNNtMxGLiqn0qtjxigC60IMZjMrMMuYkKY6W4+sePr7eRXhW63N/jm41YzHAJxLPMsO0iTeIpzZtg/M+cYjlZZX4nHjEpAsSP3Jd8fiNc85lgWeGzGRinjhELOaaWGliljc14knisKrplC+kPFY5b3HWimVWvyd/YTCjryxzneYgYljEEiSIUFBGAUXYiNCqk2IhQfvRFv4B1y+RSyFXAYwcCyhBg+z6wf/gd7dWdmLcSwpGgfYXx/kYAjp2gVrFcb6PHad2AvifgSu94S9VgelP0isNLXwE9G4DF9cNTdkDLneA/idDNmVX8tMUslng/Yy+KQ303QKda15v9X2cPgBJ6ip+AxwcAsM5yl5v8e5Ac2//nqn39wPCr3LHYXBnbQAAAAlwSFlzAAAOwwAADsMBx2+oZAAAAAd0SU1FB+QICw47CJ6HdfQAACAASURBVHja7V1neFTV1l7nzGRmSKEogURCF4JUAQEvWCkW5EoVCYgQpKooAn4YFeEiKFdERS8giChIlS6IIMVQvRSRACEhSklISEJ6mXLq/n5smTsmmWSSzJxZM9nvjzyTnSlv9j6zztprvXstjhACDAwMDL4Ank0BAwMDM1gMDAwMzGAxMDAwg8XAwMDADBYDAwMDM1gMDAzMYDEwMDAwg8XAwMDADBYDAwMzWAwMDAzMYDEwMDAwg8XAwMAMFgMDAwMzWAwMDAzMYDEwMDCDxcDAwMAMFgMDA0PF0LMp8FEQQhITE2VZ5nk+PDz8rrvuAoDLly+Lonj//fdX4Q0vXLgQFBTUsmXLcp5z8eJFQkjHjh0vXLgQEBBw3333+dOU/vHHH8XFxZ07d2ZXF+rrnsHnIIoiXb62bdsGBwfbTcmwYcOqvKYAEBMTU+Fz6PsbjcYRI0b42azOmjWLfSOQg3lYPons7GwAiIuLa926tdVqbdeu3Zw5c7Zt2xYUFESfMHTo0NDQ0Pbt258/f37IkCGhoaErVqwIDQ398MMPAeCZZ57p1asXx3HXr18fPXr0ww8/7PjmRUVFMTExgiD06tVr7NixZRKIjIw0mUwAMHz48KeffjolJeXmzZtDhgzp378/AJjN5rfeestms/Xo0WP8+PH0JQsWLLhx44bJZIqJibnnnnuSk5PnzZv3+eefjxkzZu3atYGBgQCQkJDw0UcfrVix4vnnn9+4caPJZPrqq69Onz599913L1y4kL6P1WqdOXNm06ZNW7VqtXfv3i+++CIhIeGzzz5bvXr10KFDN2/ebDQaly5dev78+QYNGixYsIC+6ueff96+fbuqqsOGDXviiScA4Pbt23PnzpUkqVGjRu+99x7P8waDwf4P7t+/f8eOHTzPjx07tnv37gAwduxYi8Xy+uuvf/vtt2FhYXPnztXpdOxSZB4WQ8VITk4GgKVLl545c8ZsNmdnZ6enpxNC+vXrR9cUAPR6/ahRo+wLPWTIEACYMWOG3VEaM2ZM165d6UbS0cPS6/UjR4787rvvAGDVqlVlelgA8Oyzz9pHxo8fX79+fQC4ceMGISQsLGzw4MEbNmwAgP/85z+EEGrIPv/882bNmgFAcXHxH3/8AQDU6pnNZvr+v//+O2UOAJIkvfXWWwCwadMm+inFxcWFhYX08aBBgwAgODjYbDafPHnS/m8qivLGG28AwObNm+mIxWKJi4ujFnPSpEkAcPToUcr80UcfXbZsGQBMmTKFEPLaa6/R/27Pnj0AMG7cOGptz549S59fr169nj170knev38/uw61BzNYvrol/OCDD9q0aVMiuuRosOxxLgC4evUqHWzZsiV9MGDAAEKIIAgA8MQTT9DB2bNnb926FQBOnDiRkZExdOjQEre0Mg3W7NmzCSHHjh0DgHXr1v3www8AcOTIkfT09HHjxtHnr127dsuWLUVFRfPmzaN2jRqsjRs3Or4/NVg7duwghOTm5gLAokWLsrKy6H+RlJREn0ANnF6vdzRYe/bsIYTcvn0bAD777LOsrKwLFy4AQEpKypEjRwDg4MGD+fn5S5YsOXXqFGUeHR2dnp6+c+dO+lq7wXK8l7dv377EYFFREQBMmjSJXYfag2UJfRIBAQExMTEJCQmEkEOHDoWHh3fo0KHMZ6qqCgA0JA8A9l1M48aNAYBuggoKCuggx3Hnzp0DgF69eoWFhW3btg0AZFkun0xISAgANGrUiH6fqTvz6KOPhoeHr169mno9hYWFzz33XEhIyJo1a+gH0ddSC1sCdNBqtQLAm2++GRoaSk1zcXFxUlJSREQEfdrixYtLv8psNgPAtGnTQkNDO3bsCAD5+fmdO3euXbt2375969at+/rrr9ON8/Lly7/55pvw8PBBgwZR00ZBjTg1rADQtGnTEvToHFY4LQyeADNYPomjR4927dr11q1bANC7d++RI0dW9h1SU1PptgsA7r77brv70KVLFwBIS0sjhJw5c+ann36iGzTXIwzUTCQnJ1OPae/evYIgvPrqq/PnzyeExMTEuPhWdLe4efNmemvdt29f8+bNW7ZsmZqaSk3Gl19+WfpVNBZGfTRZlvft29eiRYsrV67s2rUrPz//6tWrADB69OjMzMzw8PDc3NyMjIwZM2a8/fbb9ncwGo0A8N5779FfU1JS2PXGDBZDtfDAAw+cO3euUaNGb7zxRnR09OLFi2m4x3Xs3r17woQJjzzyCAB89dVXdFCWZboNnDp16rp167p160bdJdehquqzzz5rNBonT568YcOGzp07nz59OiAgAADefffdjz/+mEaFqN9XPu6666433njj+eefX7duXcuWLZ966imO46hjaDKZXnjhhYSEhNKvatCgwZQpU2gErXHjxvRVV69effzxx9euXUu3q2PGjNHr9YMGDRo6dOiBAwfOnDnTrVs3xzfZuXMnAEyePHnKlCkXL148deoUu+SYwWKoOgIDA1VVnT17dnFxsclk2rBhA03/derUie6MBg8e/OKLL9JNX3R0NM/zNDnYu3dv+g4vvfRSs2bN2rVrd+rUqXvuuQcAoqKi6IPc3NzQ0NDY2Nivv/6aZvrtGDNmTFRUFACMGDGibdu2ADBs2LDQ0FC6S/3nP/9Zt25dAMjMzGzatOkvv/yyYsWKOXPmBAQEXL58OTo6Oj8///bt29HR0bm5uY7EHL2bMWPG2Ac/+eSTpUuXxsbGDhs2TJblOnXqNGjQIDU1dfLkya1atVq6dCkhhOM4k8k0evRo+zZz2bJln3766eHDh8eOHauqamBg4PPPP//1119funTp0qVLq1evfv311+++++6kpKTIyMjY2NgOHTrs37+fbpP79u0LAAMHDty7dy/17P773//SLOGLL744ePBgup+Njo5u3rw5uw61B8da1dfEVee4V1999YsvvvA55pcuXZo0adKrr776+OOPh4eHg0M4nIF5WAx+Cx/VELVp02b+/Pmvv/56eHj4okWLaIidgXlYDH4Luo1i88DADBYDAwMD2xIyMDAwMIPFwMDADBYDAwMDM1gMDAzMYDEwMDAwg8XAwMDADBYDAwMzWAwMDAzMYDEwMDAwg8XAwMAMFgMDAwMzWAwMDAzMYDEwMDCDxcDAwIAEWBqpvv/++ydOnKiwTlNQUJCiKDabDQNng8FQu3Zt2tPU6yCENG7cmLaW8DoURWnatCkSMqqqhoWF0fZfGJapfv362dnZSEqShYWFpaWlaVDNMTAw8Pvvv6/+B2ExWHv27Nm9ezdteVIOZFkmhNCmBhi+lrIs0yYrGL4JVqu1wgnUDGaz2d6G2uuw2Wy0Bw8jUwJWq7VWrVoe38fx/EMPPSSKYvU/C4vBok0xK/y+YauWiYpPcHAwHjJBQUGMDCNjh7ucDB+LYVksFnuzcq9DEISsrCwkZFRVRbIFo0DVzi89PR0PmYyMDDxkaGtLH4Let+gGBQXhqelsNBpphysM4Hne3hIZA5o0aYKHDO2vgwRhYWF4yNDGbj4EH/OwbDYb7WCOAZIk2Zu8Y9ic4nH3AAAVmdzcXDxk8vLy8JDJyclhHpYn6eoREdbpdBoELF0Ex3HBwcF4JgcVGTzhfwDAkxjBtkx+6GHRbyYwoJ8ZtkxsZpjBAkVRRFHEQwaJIoxuCS0WC56VQtXiFE8YARuZal4zGzdu5O5AG0Gij20JDQYDnhtUQEBA7dq18dy369Wrh2el7rrrLjxk6tati4dMnTp1/GZmjh075i7b558eFpM1OIOqqmlpaXhWCpXGgskaPDQzNDFNfQht4ss+ZrCCgoLwODVGo7FBgwZY9vbIZA2NGzfGQ4bJGpyhmrKGt99+e8OGDYSQ4uJibRQSPmawrFYrnkiNKIp4UtSEECTH5SgyMzPxkEGVvEelsah+4CkqKooQolke1vdiWHiEo3q9Hk++nOM4VMERVGRCQkLwkEGlJMCzX/FPDwuPtULIR1VVNjOMjO9eM35osGiBBDyLLQgCnq8BHo0FIEve41kmRqbGbQnxkNHpdHjce2xbQlQaC1RbQlRk2JbQs7BYLMXFxUjIiKKIJ5qrqiqqk/eoZA2oMgCociOaaSyUmulhoToUhq1aAyolAavW4Aw1sVqDLL9465ZbQne+J2vAc+ZDFEU8KWpVVVHJI1G5e6icGlR1LLTwPfftg3HjDtau7ZYTKj5msAwGA57ysrRKKh4PC9VpGHY0xxlqUKgxPR2GDweeh7VrM930TfGxLSG2/DSqrLAkSUgKzAOAoih4ZoaRcb5Xkz2SyCIE9u6FtWth0ybg3ekV+ZiHpaoqHlkDIUSSJFQGCw8ZVPlytkyakklNBZ6HyEjYvNm91sr3PCwk/XIodDodqiQAqhQ1qo0PqmXy5zqLsgyrV0NQEAgCeEaB5HtBdyZrcOZ7sqC7M7Di0c7gzqD7jRvQsyc89BCMGgUe00v6mIeFqrwsa0JRDlBpLFApCRo2bIiHjHtkDYoCI0bA88/D6dMev859y2ChEo4KgsCEo86AqjhXjRWO0tYk8+fPj4mJSUhIKJ2zcoNw9Oef4YUXYOlSGDZMg//Ixzwsk8mEJ1FIW9Xj8bDq16+PZ6Xw+J6ATGOh5aEljuPsJdsWLlxY+rtz9913V+c+AK+9BqNGwcaNmv1HPmawaEpYp9MhcWpkWcaTBxAEAc9ZS1RkRFHEs0ySJOEhU8WZURQ4cgRWr4aNG92eB/SrLSEq6RMhhAl8fIIMKrmcxjNz/vx5+mD37t3uIVNQAHo9NGoE69ZpbK3AF/sS4mlCgaovIfh3vrx6QJWr0ZhMp06d6J21zH1J5QQfqgpLl0JwMFit4KUDJ77X+RnVWUI8JZJVVUUVWkbVakGbDlSYyTiLolQiA5CcDMOGQa9eEB0N3jsex2QNVYfRaMRzFIbn+UaNGuGZHFQaC1RKAlRkXKpjIcswZQp07Qrbt3v/Ovctg2WxWIqKipCQEQQBz62btfnyFXcPlSNcsdj4l1/guefgo49g8mQUN2bfMlgmkwnPMQuDwYCnDADP83h6jmHzI1AJPqqlJHA3ylOf5OXBmDFQXAw7dgCa+rG+J2sghCDJl6uqKoqiNv0jXYHVasWTL0dFxmaz4UkCCIKA6popo2SzLMOvv8KqVfDNN9rnAf3Kw2JdcxgZXwf2mbFa4e67ISQE1qzBZq2AyRqquQvDU00QanbyvsJIAiNTJkrqchYtAqMRMjMBE0mNPCxPdJ1CJWuQJCk/Px8JGVVVUZUBQFWVGFWzZaSdn69ehTFjoHdveO01tNbK/QZr6dKlzZs3z8jIGD16dK1atUaPHu32+zaeYITBYMATQOV5XqOGAq4BlcYCVToCFZmwsDCQZXjzTfj+e1izBrp2Rb7HcqfBOn78eEFBweXLlwcPHjxy5EhBENq3b3/27NkynxwfH594B0lJSbSOqL2GpyiKACDLMj2LYx+0WCwFBQXgUHrU/kxng4qi0PMHdJC+FR0khJQYVFW19CAhxE6M/qTEbDYb9SMq5OBssJrE6CD9VRCE1NTUEmydTWOl2FaBGAAkJyc7zpgniLm+vlTwUdn1rSaxMtfXXrassuvriWmUJOnWtWvqP/8J//qXMmtW1S48V4i58WiUOw2W1Wp98MEHa9Wq9fDDD7du3dpgMHTt2pVSL4HCwsL8vyM7O1sQBEVRaMdg+lMQBFmWFUWhu0ur1WoymXQ6nSzLsizbB+lWUVEUSZIEQSCE2N+ErqLjoM1mo9k9SZIcn0kIEQRBkiRVVUsP2olZLBY7MZ1ORyM1jmzLIWYfpMREUSxNTJIkURRLEBNFsfQg/Tg7W0EQwsLCbDYbnbES00i/wKWJOWPr+HGOxEr/C2USE0UxLCyMzlWZ0+hsfenHuX19Q0JCqrC+JYi5ZX1FUaxTp04JDq6sr8ViofdI96yvLJPCQsucOQ2vXTPv3KmaTJIour6+dBoru75uAefGnEVKSkrTpk0HDRrUtm3buLi4uXPnduvWLSUlxZVabg0bNkxISKiwDAidUySyBlmWBUHAowvLz8/HowtDRaaoqAhPv2Xvk1FVOHYM1qyBFSsKBaG2JjGWHj16xMbGVv/srTuzhE2aNLGbP7PZvHPnTkmS3Cs54TgOT1aY4zg8KUsaxmJkGJkKb7PwwAOwdCmsXg0APKZeIVobLEcEBQWNGjXK7W+LpBKW/crDU/IJWPLeOVAtkzfPny5eDPn5cOoU3OGA5zCsF2JYGkAQBLo9xgBJkgoLC5GQIYQgzZfj2J/iIeOdCh+JiTB6NPTuDe+/Dw5GCk+5ES97WB4CqvpTBoMBT+1djuNQtVpApbFAVa9ZazKSBIsWgSDAd9+V/iMqjYUfelhmsxmPUyMIAh55pKqqN2/exLNSKSkpeMigas+haTe2//4XunaFGTPgX/8q8++oKnz4ocEKDAzEk+4xGAx4ygAw4Wg5QFU6okynZtmyZStWrHDnx4giTJsG+fkQFwfOA1WovHI/3BJSWQOSSCFVnSAxoISQwsLCemjKgBQUFODZLxcXF+PpRG02m0s0W7r//vuvXr0qy/LkyZPd0y/j0CFYvRpWroSKNDdFRUV41Cd+6GHxPI8nRc1xHJ46IajIAACe2jLYyJRYJrPZHBcXV1xcbLPZOI6rbn5AFKF3b9DpYP16cEEhiGpm/NDD4nkej/QJm41AlbxHRQaz9QwKCurWrVt8fLyiKIIgVCskv2QJ/Pkn7N3r+ullnzNYPuZhiaKIR9YgyzKeNtSEEJa8L2d/iodM6azR6dOnd+/e/dNPP1VdFB0fD88/D/36wRdfVKrWAqqZ8UMPC5usAY8f4djjFwNQRXPxl0ju3bt3Fd9OkmD5csjIgM2bq/BqVIIPP/SwUMka7NUaMEBVVVR9H5KTk/GQ8VtZw8WLYDDA+PHwwQdVewMma/AsUMkaTCYTKlkDKiWBKyfea6a75x6NhaLAa69BZiZIElSjuKtLbb6YwaoyaMkOJGQkSUIVw0IVNkJFBo9XDgBu6FN36BAMGQLvvQd9+0L10j4+F8PyMYOl0+nwnH/meR5PkoXjOFQHWRkZZ6hW3NNigcGDgRDYtQvc4d373OFnHwu6cxzHdFhO15KRcX6f8wcyq1bB0aOwaRO4z8qgmhk/9LAkSfJEb4uqhhEUPB0xqNIdz0qh2mvg6RYOAFUJI1y6BAMGwEMPwdq14FafCNXM+KGHharKUkBAAJ4DHxzHoUpRo9JY4DkkBACVOz4lirB5M9y4Abt3gwck06jaUPuhh4VN1oCnsxar1lAONC2QUBEyMjJcfeqVK2A0wsCBMHs2eOaAB5M1eBbYZA14nBqe5yMiIvCsFCpZA6rkvUsaC0LglVcgJQUkCf5+Utq9QCWF8UODJQgCnhgWtoqjOTk5eFYKFRlUGouKT1AdPgwDBsDs2dCvH3g4d3H79u0DBw74kAXwvVb1eJpQ6HQ6PFlhjuPw9O8BZK3qUZ3oKi8OW1AA0dEwfjz8+KMGTLp3756SkpKZmUlDCqg6qviJwWKNasqfHDYz+Mk4VRJs3gybN8OmTaDVAdUWLVrk5eVRg3X79m1UZQ79ZEtob+eJAfa2kUi2hHg0FlC15L3HQFt+IkEZy5SYCI8/DvffD9u3g4bH6S9evNi3b1/62I3NmZmH9T8YjUY8fkRAQEBtTwZEK+teoUreoyqQgKqo5t/IiCL88AMkJ8PBg1CuhjMrKyszM5PjuHbt2rmLSXx8/BdffPHtt9/yPO8rhwp9zGBZLBZFUZCYCUEQCgoKkAiOVFVNS0vDk5ujbcCRkElPT8dT8D4jI+Mv63DtGrRsCbduwbBhFb7KfpldvXq1RYsW7iIzZMgQ30oU+tiWMCgoCI9TYzQamazBGZo0aYKHDDpZAyEwYQIkJoIkQSW5tWzZ0o1kUDUu8UODZbPZ8MQjRFHEcwCFEIJHxQoAeCqFAQCqFrN5587Bs8/C7NnQv38VVAsbN250IxlU6hM/NFh6vR5PkU+9Xo8nX85xHB5JLQCgIoNF8JGTA8OHB96+Dbt3QyU90LNnz7711lv16tUbMWKEGxkFBwf7mAUAX4NPqEW85WSxZcKL3bth2TL44QeuSvm4lJSUJUuWWK1WjuNQLTTzsMqDoiioZA14qgkSQvBoLKDM5L1XIwne/PjkZOjWDVq2hJ9+goCAKi8TfaHRaNy1a5e7qKG6ZvzQwzIYDKhkDaj6EuLpogrICiR4LVEjSbBjB2RlwYkTdoFV1Sp8CIIQEhJSVFQkCEL79u3dRRBPuRH/9LAsFgue43uCIKCq1oDq5D2qjhiVKJDgRty6BQYDdOoEr7ziKAetGpkRI0Z07tx57Nixubm5bkwUoqpj4YceFqrjckajEU/VJ57nURVIQEVG6+Q9ITBuHAwcCKpauixMlTUWR44ccTtTVq3Bs7BarXiCI6Io4smXE0K840c4AT2hhgSaOsJHjsDzz8PcuTBoUJlFrLKzs/HMDCr1iR8aLIPBgKdAgl6vx5MV5jgO7wEUb0OjGFZ2Nrz4ImRlwfffg3OVPyrBh8/FsHxsS4gtoYvqyKiiKGxmvEbml19g4UL48ccKtaCormFfOfPsqx6WoiiyLONZbEmS8JhyPBoL8LqSoNTmvZy/bt++febMmdOnT6/itzcnByIjoUED2L/fFeU6qmVCRcalnQQSe9+wYcOEhIQKc+GULRJlAyGEEIKn1pKiKHi6NqEio6pqOctELyee51VVLfF1sFgs//3vf/v06TNmzJhvv/225CtlGbZsgbw8eOkl15vZlE8G1cy4ET169IiNja3+yRDfkzXgKbQkiiKeo1iqqqJKUaPSWLiSASjzzm21Wvv06QMAW7ZsKVnfLj0dunWDVq3g5Zcr1XoLVZwbVaLGDw1WUFAQnpglq9ZQDnyoWsPChQupwSrnKLsgCG3btrXbNpgwAfbtg99/hwceqCwZl5pQaAVWrcGzwCZrYB6WM9y6dQsPmfKdmlmzZhUXF9+4caN0MtGeRFMU5ZdffgEAOH4cXngB5syB6OiqkUFVVAOV+sQV+N7RHDxkUFUc5Xke1WkYVB06K9RYBAUFlalJ/lvTk5wciImBxx6D9eurQwaVkgDVcS4/NFjYUsKKogQEBCDhI0kSHpEaKjKyLFfrVqeqcPo0zJ8Pu3ZBtTMJqNQn1Z0ZtiV0xUbgsZ54NBbURjAyzr6WVX9xcTG0aAEGA+zZA+7Ie7JlqkEeFh53BgB0Oh2q7nuoJNR4NstQnSOoq1dDQQEkJkI5zQQ1I+MB+FwBP98LujNZgzPfE1UAFVXQvSpx7owM6N8f7rsP3njDjdYK2FnCGuVhofJosMkaUJ28R1WtodJKgjfegLAw2LvXE2RQ9Sv1le5evuphoRKOCoKA526pqioqp8bnhKN/4cQJGDIEYmJg1qya4NQw4ahnYTKZ8PR9MBgMeGoS8DyPqncpHt8TXCx/WlgIr74KycmwfTt4sswZKiUBKvWJH24JaYoQySE1evgZT5VkQRDwpKgrRSYvL8+j5XFEUSwvXaOqcOYMfPABbN8Onr+0JEnCkzsSBAFVIsvfPCxCSI2rW+KbZFxXnxw7duyuu+6qV6+e53aR5c2MLEOXLiDLsHMnaHIjZFWAapCHpdfr8TSh4Hne5Nb8UTXho/nyRx55hD6IiIjwkDDYaa5m5UrIyYGTJ6GiZA4hxF0XHqrEESoyfuhh2Ww2VGcJ8/Ly8NwqWTQXABISEjiO4zhu5cqV9sEyciM3b8KLL0L79hATU761ou92/fp1dzFE1WwZlcbCFfhYPSwG/8ORI0dmz54NABs2bKh+wQlHP6jsa5sQmDcPBAE++MAN78bgGnygHpYndIwWi6WoqAjJGmBr84Wqs9bNmzddfOajjz569OjRo0ePeq48zv/cvVOnoHdveP11V6yVD30vqgxUUhitDVZeXl6vXr3i4+Pj4uI4jvv3v//duXNn98qmTCYTnkiNwWDAk6LmeR6VIrH6VZ927tz5888/V/ZV9oiBY1Os+g0agM0Gb78Nycnwyy/gcjrSXiFr/Pjx7poZVEoCPH3qXAVxHw4cOLBlyxZCyBtvvJGTk0MIWbdu3cmTJ0s/UxCEtLS0W3eQmZnZuHHjnJwc2vydEGKz2QghoigqiuI4KAiCxWKhld0dnykIgqqqsiyLolhiUJIkURRVVS0xKElS6UFZlu2Ddg6yLCuKUmJQURRRFAsLC0sMVpZY6cEyiTkOlklMEISCgoLSM1bmNNLnU7alOThj6woxVVXpx+Xl5ZXJtkJi9KMB4JlnngGApKSkSq0v/UkI+R8xRcm/eJE8+aRUWCgriuvrS5fYftG6a30drxnX17f8Gavy+hYWFtLZq9T6lvM1KZOYJEndu3e3WCzVNzLu9LBq1ap1+fJlenelt6bCwsIyNVM8zycnJ6fewc2bN2VZpvECmvSl2VZ71ID+qqoqx3H0p/2v9Pn2JE6ZgxT2dy79TDpIZ6T0p5fg4PgnWgbAPuj4ziUSxvTX0mwJIeX8C3Zijm9SJjFFURz/2RJs6Zs4DtoDNKWJ2efBdWKlJ9zZx5W/vo6D165d4ziudevWlVpf+gRZlv/3zH/8QzGbYd8+YjKRsj6oHA70Pa1WqxvX1/GZrq9vmR9X/fWlv2qzvuiC7oSQzZs3R0VFPfLII0ePHn3iiSc6duz44YcfuiKtdDHoTucRTxMKVVXxtFpApUisJhn3RLtXroSEBJg/X65VS4+m74Msy3jExpqRcVfQ3Z1cOY4bMWLEiBEjbt++XVxc3LBhQ7fHm6hTiqQmhiRJRUVFSEISqqpmZ2fjOcualZVVnXrh8fHxly5dAoBBgwZVRQN1/TosWADjx8PEiQCQe/s2nmBNXl4ennNLOTk5qEKfmnpY1QGTNTC4y3LD0qVw+TIsX84mAw9qaJsvs9nMZA3OPCxUsoaUlBQvfGpcHLRtC2PHlrBWqNpzaED5fgAAIABJREFUoCqQ4HOyBh87moOnVAMAGAwGPC4hz/M1sH/Upk2bOI4TBOHF0aNh9mzo0gXi40sfCUSVvEdVx8K39oO+Z7Bo9gdJdwOackYSUCOEFBcX4yl3owGZGzdujBw5khDyRkhImzFjuhcWgpMi0WazGU/JZovFgqeYNaqZ8cMtIZ4UISWDp+c4x3F48pXU49PgIxoQkghgKSr6ZsoUcG4FatrM+CgZPzRYOp0OT0qY53lULZLwtNUCAM/VsZg2bdqrr74KAI1+/HESwCCDYQVARrlRKrZMPkHGDw2WIAhUxYcBVLWMhAwhJDc3F89KeagmwWeffbZk5crENWsOcZyue/d/EbJ4x44bN27s2LGjnFfl5+fjmRlUZPCUG3F1J8FkDQy+FROYC/AAwABWPsGnUHNlDXicGkEQ8JSgUlXV9QIJGsAjsoYrV34F2AowoJKvQ5W8R6WxQNUrxBWwNl9Vh8FgwNP3ged5bZQELsLNPccIgblzoWvXdllZW3NyKvv+qAQfflZUQ+vr3LfoiqIoiiISMoqi4Cl/Sgix10LxIuLi4uLj4y9evOh6qPHjjz+myV+bzVb2M379FR56CF5+GZ59NqR+/cjIyMjIyEqpSfCIjQEAT586bDPjhwaL53lUSgI8KUuO47yeCzty5MiDDz7Yvn37jh077t6925WXTJw4kZYbLXuvJAgwZAjk5MCJE1ANx6T0MWxRFHft2sVxnMVi0XpTo0e0rfGtljk+abDwaGpQGSwMF59Op7N7SS56WDabzekcbtoEo0bBd9/BgAFutxEzZ84cNGgQx3FBQUEaH2lCZSOYwfL4ltDpxkFzyLKMx70nhHg9X96mTRu4Uxlm6NChrrxk7dq106ZNo4//d2bljz/gueegdWvYuhXcUfCjdKKGmjBCiF6v1/iKwpM1AoeSqsxgeQS1atXCE3dHVSKZ4zivn5irX7++KIpz585NT093/cDH+++/T0vKBQcHgyzDDz/AV1/B5s3QpYsbiZUY6XLnzWVZvvfee7WcJVQlklF1C/dDg4VK1mCz2VDJGjBUawgICHjvvffCwsKqImu4ehUCAuDBB+Gjj8CtkcrSsoYXXnhBvQONpwiVrMG3qzVQ/7BOnTpo6aKSNZhMJjxnPnied7OSwDWcPXv24MGDADBhwgRH36Fx48aV2tDCO+9Ajx4gCOCBKS0zee+tQ6moZA14Kj5W0cN6+eWXO3TocOzYMVRVe+wQRVEQBCRkJElCFcPyyjGLbt26xcTExMTEHD161HG8/HNCJ0+ePHny5F+/nDoFffrAxIkwcCB45gaAKlKDKoaF6pxQpQ1WnTp11q9ff/HixXPnzoWHhzdv3jwpKUmSJDx0dTodniyhTqfD42FxHOe588auYM6cOSXcT2cmvlGjRn379u3VuzfHcRAdDenpcPgwNGvmUV8YzzVcQ86oa+RhxcfHT5gwYcaMGRMnTvz999/379+/ZMkSPHRRVXQBZNU5vGXKaQzhp59++lusoZSSYNmyZY899lhISMitW7esVutsQdjbvj0sXQqDBvnrzDAynjVYt27dOnHixIwZMywWy4oVK+rWrTt16tROnTrhoStJEp4toaIo2ssOy9kSekW1TPtREkJKRNBKb3y2bNli3wbuATDVqtX/0iXQJCiJSlzOyLjNYN1zzz0TJ05s06aN406nX79+qDxYPHH3gIAAPAkKjuO8laLu1q1b6cHShYD7PfkkGAz/ATgGkLBsmen99zUrt4BHfYKNjM/VR2GyhqrDZrOhakKBvFrD2y+9dFwU3+zc+WGAmVOmTJ8xQzMyrAmFM7AmFJ4FNlkDngAqz/OVUxJ4GH8jQwjMnAkPP9xLECyVSVOoqjp9+vSAgABZlj/++OMqB1xQJe9rYK+QmmuwBEEghCDpnSNJksViQbIrJIRkZ2fj6ciSnZ39l/L+9Gn44AP46CNo3bqyb3LgwAF7zueJJ554+umnq0YmLy8Pz94nPz8fz64wNzcXlfLe37aEer0ez3FNnU6Hx8Oi53jxrFRwSAgUFsK4cXD1KuzcWQVrBQB79+61P/7xxx/9wzFH1aoO1cz4ocFC1TUHkMkaKjUzFoulQ4cOXbp0+frrrz1C5uhR6N8fli2DqKgqv8mnn35q/1J99tln2swMqmWqUWT80GBJkoSqgB+ejhiEkEpVEwwKCoqPj//999/Hjx/vZiqZmdCrV/G998Lx4+BEl+hifpDneULImTNnaFmFKjPCoz5hZKq7x/ItukajEc89ISAgAE8TSo7jvB+mkWX4+Wc4fx4OH7673OB6QkJCu3btAEBV1QoXtG3bttXkhae/LDYyqDQWfuhhWSwWVE0oUMkaKtVQwGazEUIefvjhdevWuYdBVhYEBEDTpvD222A03iy3WgO1VprtqZmswRmYrMGzQBVXNhqNeLJyPM9HRERUinxRUZHJZHJP0dSpU6FHD5BluKM8aNKkCZ6VYrIGZ/A5WYOPeVhWqxXPrlsURTyH3QkhlS3OFRwc7AZrdeYMREXB1KnwwgvgoJMqn8zLL79MH2gzgR7q6lo1oOp3m52dzQyWB2EwGPAUSNDr9XiywhzHaR1Qy8+HKVPg/HnYuLG0aqF8MkuXLqVVRrVRsYWEhOC5hivV76dGzYwfbgkJIT6XiNVycrT7sFOnYOZMOHTIWQUrVG2ZWY9ov4GPeViKouCpz6WqKp7SEYQQjTQW+fnw8MMAAMeOlVNvD4/gAwDwNC7BNjOoyPihh4VnP0i3hHjce47jPJ6iVlX44Qe4fBkOHICKCr+hypfjUZ9gI4O5Hro/eFgWiwVPr1pBEPDELCsra6iKY9W1K4SGwttvgwtlKjF0xLAjMzMTDxk8jUsAmeDDDz0sbLIGr3fW+t+dx6PVGmbMgFat4PffXX8FkzU4AypZg1cal9QgD8tqtVbqAIpHIYoinhQ1IcQjisTTpyEqCl55BSZP9t1bNx59LyBTEqBy9/zQw2IxLGfgOM7NZz4KC2HOHIiMhI0bq/BqVDEsVJEaFsOqQQYLW7Jc+zac5UBRFHf9Y3DuHLz2Ghw5AlVVlrqNDKqZcQdQXTOoyPjhllBRFFmW8RgsPBoLQoh76liIIjz2GIgiHD8O1dDB4xF80M07I4N/mdiW0LPQ6XR4kgDuUbpv3w5paXD4MNHpqinP1WavsW/fPgCQJOmf//xnOU9jSnef2J/6oYdlsVjwNCYSRRHPITVVVSuMcyuKkpmZmZmZWYY0JC9PfOCB7kOHcq+9xun1YrVvvBqUAUhJSenfv//TTz/97LPPll/Dg8kanAFng3f/MVhBQUF47pY+V61Br9eHhYWFhYUdOnTob3+YPRs+/TR20aIzdwZ27txZTT4adMRo2rQpjWnqdLrp06eX80wma3AGVq2BeVhe87Bcd2oGDx7816OzZ2HIEJg8GebNsxUVuTHPqIGHRdua1a9fX1GUVatW+YpTg0pjwTwsjzs1eGr4e7fi6Llz50p4WK63P9nxww9gscC778K5c7B9OzRqBADPPvvsu+++CwCffPJJmzZtSr/qo48+ojX1XZHCadCLJSIighDy4YcfVpg7ZhoLZ/C5RqpAcKBBgwY5OTkVPk0URVEUkXCWZdlisXjlo8tcvqKiogpfGBUVlZmRQS5dIj16EEGgg8XFxRW+9uWXX7ZnGJKSkir8IFfIaAaz2YyHjLeumTJRXFyszQd1797dLf8473PmFY9yhBDiFYHP4cOHaWVhnufXrl1rH3dF8LFhw4YGL7wAOTlw8iSttZCamhocHBwSEnL27NnSz9+3b19UVFR0dHRlK3/hEXy4ODM1kwwqhZor8DFZA56mhDTW65X9qSRJISEhBQUFtWvXdqy/WnG+fN06OH8etm0Dh52sPTrerVu30nsre+/S1atXf/jhh6+99pqLkVpUSgJUR1BRtQJENTN+GMOyWq2ogu55eXmOI9evX7969aqnP/fJJ5+kSbFp06ZNvnPET1XV8pL3t29DdDQ0agQffwxViruNGzdu6tSp1C135SpHFc1lZwmdXxc+dpaQQ3LYpWHDhgkJCb4XAnScSo7jOI7n+blz59LoNRYsXgyXL4OThqlXr1598sknqQ/1yCOPlP6n7Fc2Hg0Hg8+hR48esbGx1d+R+JjBslgsqqoi0QoLglBUVFS/fv0S323wxpnH2NhYURSPHj06a9as4ODgv8icOwdvvgmbNkFVbU1RUZHZbOY4rmHDhpV6YWpqaqW6+HgUGRkZeNRPmZmZlZ1MzyE9PV0bkZq7DJaPxbBMLpSO0wwGg6GEcInn+aCgoC5dumhP5uDBg5s2bbp69eqCBQsIISAIsHgxhIVBCY1oJRESElK1aBSeSmGgicbCdaDaRthvt74CDxosq9Xq9pg0TWpo032zQqiqKoqivVMWzf1nZWVt2bJFezLr1q3r3r37XxG0pCSIioJjx8B78V2bzYbn4KcoinjSNajICIKAKpFVITz4zW/RooXbk6bYKrqU2PotW7bMK9YKABYtWkTP050HgORkOHsWvJqNYkVUXLxmGBmvGazr169zHHfffffR8PPt27f1ev3JkydLP1OSpCNHjpy4g19//TUgIIDjOFVVaZ6e/hQEQVVVRVFobw+z2RwQEEATVYqi0FYoVHVNG6/LskzLZdgHAUCWZao1pYP0rSRJkiSpxKAoirIsq6pKBykHURQVRSlBjA7aF9sZWzpIiZXJVlXVhQsXBgYGchx3/PhxSqyybOmnPzt06LaxYzNnz+6Yk6P062cThLVr144bN47juI0bN1Ln1HVi9K+O01hZYoGBgaVnrML1tRNz7/rSiF5l19dOrMrrW+Y06nS6Kqxv1aaxQmKBgYFVW99KEXNjERt3GqzmzZsnJyc3bNjw+vXr1BUihPTs2bP0M81mc3p6eoYDjEYj/VLRakF0Qu3XjX3QarUWFRXRqlj2QTpfqqrar106KAgCbQtGRYz2QVVV7SthfxM6KMuy4yB9TJfHcZC+syiKBQUFjhwo2xIfR9nSjyvBQVGUmJiYcePGAcDbb79NLxHKwfGZZXKwDwqyDKmpliVLCrt3D3znHSk4WJUkURS/+eYbuilbv359bm6u4zTaiZVga59G1/+F0sTo09LT0+3PtLOtcH0dP86N60sPUVR2fR0Hq7a+pYnJskzLaldiff8+jaVnrJxpLH99RVHMzs6m12Gl1rdMYuWsrxtVxB7JEr7zzjtNmzadMmWK1Wp1MZDhB7KGKi4Ax9GfJc7cVALffAM//1y6kPHjjz9+5MgR+p75+fk+VwyXwZ/griyhR2JYCxYseOihh8aMGeP26Di2Nl/VVyTeuHEDqnzKJyEBOnSAgQNh40ZVVUt01tqzZ4/dAmpvrWgpBSRA1REDVXEuDYpquPkG71s6LBo9RZIlpNtenU7nhc9WFPj8cwgNhagouENAkiQ8GR9UZGRZ1uv1jIwXyaD2sDw6v3jOjtoDw1rj99+hQwcYNy6tb98Ch4NKjucKvQ5UZFA1ZPfONeMLM+OHBstRTY6BjNZ8CIEhQyAtLef48XrNmkWEhzsqV1FNDrtsGBlPwMeU7t7Zfzkz9jyvqTby++9h61b4+muoVw9ycvLz8+nwihUrJk2aNHv2bCpJ/7//+z8Mk4OqJgGqAxI0Ic7I1AgPSxAEPHsNSZKorMHjSE6GMWPgnnvg+++hXj34uxKyefPm/fr1mz9/vl6vnzVrVonQu7eAqkACngbdAFCiwgebGX/2sPDURwYAg8Hg8UNqqgrbtsGePfDtt+DgvYeGhr7yyitFRUU5OTlPPPHEd999x/P8jBkzAE1GAlV3A1QHG1EVvcBzDNvVPaxvZQnNZrOqqkiKwwmCUFhY6MHrLykJIiMhMxMq+r7RZOU333wTHR2NZEFTUlKaNGmC5CrXrCaBK0BVOiItLa1Ro0YafFANrdYQGBiI5/STwWCostJ1w4YNhJAuXbrcd999ZfyZEPjkE2jSBGw2cCHKwPN8RkZGZmYmnslhHpZPeFioeo75ocGipweQRAqprKEKxblSU1NHjRpFH9+6davkzf/CBYiKgp9+gsp4KA0aNEDVFruoqAhPrxqz2YynxbHFYsFTP7q4uNi3jkD4nqwBTyKW4zi9Xt+sWbPKUnIMM8XExPzvD7IMI0fCtWsQHw+V3E9RMnhWClU+F1tyGZHDovcxl4XJGqpFZvTo0cnJybTUhOvbMccc33/+85+/Hu3YAUuWwI4dUFXHBJWHha3UIiODn4wfeli0YAMSMoQQWoKqsofRIyIiDh48eODAgevXrwcHB0NyMowaBQ0aQGxsla0VIQRVvhxPT2wAsGvWMEAjKYxrQHXN+KGHhUrWwHGc3eOzN8sqbUcKCgrOnj3bo0cPx8hFnz59AAAUBX7+GbZuhTVroHrOOcdxqAKoeLJygCzOjaoqMap0hB96WGazmTo1SNy9r776ipY9okUXygxY1KtXr1+/fmUEfW/dAr0e2reHlSuh2qEEVVVRFUhITk7GQwZVTQJUpSPS0tKYh+VBoDrwYTAY6tevX4UYaoHZTFasqNuuHbFYODf5jDzPo1IS4GmZA8iS96i0mqgcYT/0sERRdGO51WpClmVaOrYcLF68mD6gvf8A4Lcvv9wWHFxvxgzuqac49+1w6d4Tz0qhChvh8coBAE9BN2wz44cGi+d5PIlCnucrzApPnz6dEJKYmLhv3z6w2WDs2J+mTHkJAACCg4M3b97sLjIcx6HK+KA6VYtqZlB1qWFZwhpksFyXPkVGRsKBA9CnD3z+eas7Rqq4uHj48OH++k1ARQaV2ojNTA0yWJIkaV//zJnASlGUYof6eU6Rng7Dh0OtWnDiBNSu3atnz507dwLAyZMn3SiCZVtCtgurCVtC1vm5YhQXF9McX3x8fNu2bR1vlRWcPlEUOH4cdu6Edevgju8dERERERHh9kN/HMehSt6jypejam6Cqg01KjJ+6GF5RdawatUq+qB9+/a//fabfdxms92+fdvpy7KzQa+HsDD49FPwfKQAm6whJSUFDxlUSgJUZHyuCQWTNVQCQUFBjhXlTSaT09DyvHnQogVIEmgVI+B5HpWSwJmS1itgsgZnYLIGz0IURe1jWL1796YPli1b1qNHD/u4JElluHuXLsHAgTB4MLzwwsJFiziOO3TokAYkCSGoqkeiIoMquocqbIQq1OiHHpZOp9P+sHunTp3KDDnpdLq/eVgWC8ycCQ88ALt2AcD06dPpXvKZZ545c+ZMhw4dPEqS4zhU55ZQkWE13X1iZvzQYKEqLwOOpUKOHoWYGNi/H+6Ux6pTpw5NTgmCoE3ymFV0qXiZ2MwgJuOHW0JJkvAo3RVFsQgCZGfDc88BIXDiBDgU85swYYL9cdOmTTXYErLkvTNUeCBBS7gkhamRZPzQw/KWBysIAv1om81md+kDeL7OuXOwfz+sXQuldkDh4eFaFizmOA5VGQBUGgs8tU+xkUEl+PBDD8tb1RrshvJ/FrO42Na3b1ZYGHzwAZQVr9F466qqKpIGXxRM1uAMGRkZeMgwWYNnERQUhKLPwuzZ0KyZaf9+49+PWSxevPjMmTObNm3ywp0HmawBT8scQJa8R6WxQFXhww89LJvNpr2sAQAWLVr0l0kCgAkTYPhweOkl6e/5co7jZs6cuXfv3scff1x7hoSQ7OxsPCvFGqk6AysMWx34WF9Cqtv0zolNRYGPPoI6deDll+27MEmS7CEtxz2gV2bVYrHgqReGiozNZsOTv0dFxmq1aiNAqaF9CUHz2NBfOHMGJk+GEyfg75daCTKVakXhJzPDyDAybEvo3MtRRFHU2FUggweDybRg+PAFn3xSgoxjRwxCyJ9//nnt2jWvRFUJISx5X467h4cMni4q2GbGDz0sg8Gg3T1BVeHQITh8mNu2jdPpaMPU+nffPWnSJPr3gICAEpXaW7Ro4cVbJatJ4Ax169bFQwZV41JUGgs/9LAsFotGsgazGXr0AJ0OPvwQeJ66DMXFxZMnT7Y/RRAEPKFlVVVRNRRApbFgsgZnYLIGzyIoKEiLj5k7F0wmOHUKyj3SYTQa8cgjWbWGcsBkDc7AZA0e3/97dtd94QJERcGIEfDWW47WqqCg4MyZM2fOnHHUMYiiiOewOyGkvOJcmiMzMxMPGVTJe1QaC1RSGD/0sAwGg6fScFYrLFwIQUGwcWPpP9auXfuBBx4oOXd6vUYenwvgOK6M1ofeA6pIjWMLW68j2OHAKZsZPzdYnrJWly7B8OFw/nxlS4MiUbExMoyMH5Dxwy2hoiiONT/dAFmGoUMhMxMuXaqstVJVFU/pCEKIV84AlLN5x0MG1czguWawkfHPLaE7327PHjh4EFavhirtX3R3tA5ItoQsX+4MqDbLqHZhqGbGDz0si8XiHkVicTEMHw4BAfDZZ1DV77koiniiuaqqokpRo5I1oMoAoMqNoBJ8+KGH5Z4g97//DRkZ8P331XwbbLIGVEoCVq3BGVDJGho1asQ8LA/CarVW7QDK/PnzOY5ryHHw+uvw3HPw6afVJyOKIp4UtaqqrJmVM6AqHYFKSYDK9/RDD6tqMazDhw/PXrRoKcBdAJ917jzNTQdo9Ho9nhgWz/OsrqYzoIruoQoboTq05IcGqypZWEKCAZILC5sC8By3wn1JRkKIqqp4JkdRFEaGkfFdMn64JVRVtdKyhqio7qIoXbsGACoh0dHRbjRYkiThmRyt61iUC1T5clTLxMjUIA8r4O8liSvArl2wfTssXw716rX0gEZOp9PhUbpj22ug2oWhEpczMjXIw7JarS7JGvLyYPRoMBhgzRrwWDAFm6wBVZwblcaCyRp8Ymb80MNyqerul1/Cr7/Cd995mgyr1lAOWLUGZ2DVGmqQh1WBcPTSJejfHwYMgDVrNCAjCAITjjoDquJczMNyBlTFufzQwzKZTGWHomQZVq0CSYIffwStSpIaDAY8YSOe51E1UkVFBlUtVtZItQYZLJqF1el0fxu9fBl69ICbN0FbUQlNWVYuD+Bhj8/NZy2rAcd+QhjI4FkmbGTwXDN+uCUsQ/o0ciRcuwYFBaC5BI4QwjQ1zuDmohp+NDOMDC4Pq6ioyPE8uqIoJR2i6tDV6//XhGLXLli7FlasAC/tPnQ6nTY93VwEy5c7A54OidjIoNLlaO1h2Wy2pk2bPvXUUxzHFRUVAcC2bdtOnDjh3o8otlrh9m0YPx5q1YJt28B7sRJRFPF08VVVFVVoGVU0l50ldAZUGQCtDdbJkyfnzZt34sSJ3377bcKECQBgMBjK7NJMN1N2AAB/p4A6jak7/nT8NTAwMHjXLhg9GlauJP36lfNMx5+uD7rCwf7TaDTSZlZVe3mliFXIluO4Ro0aVZaD54hFRERUh4N72TZs2NCjHCpFzJGMG6/GqhELDw/39NcEdQyLxlm7dOkycODA9evXOwsuSpKUmJiYdAdXrlxp3ry5yWRSVZUWh6Q/RVFUVVVRFHrOwxYfb5k6NffBB9X9+xVC6EkUWtlSFEVCiCzLjoOCINBBjuPGjRs3bty4W7du2Ww2OijLsr1KJ31/SZJkWS7BQZIkRVHKHLRarVRJUDbbO4POiCmKIkkSIaQE29LEZFlWFMU+aOfgyNZisaSlpVFipTmUSYwmDUpPozNiFbK1EyOEpKSkVDiN5RCzd8x1ZX0rJJaSklKF9XVxGiu1voqiUMFHZde3CsQqXF+bzZaenk4fV2p9XZzGEkvptsixu0BbyLzyyiuSJAmCMGnSpH/84x8nT5505bU9e/Y0m81O/yzLZOVKsmKFYjbTpa0U5syZYw+r/fnnn+76f1VVpWuMBPRyRAJ6xSMBqmWqmddM9+7dLRZL9d/HnUH3OnXqEEKysrJ0Op1er//yyy+vXLnihhDjlSvQpg2kpcE99yiSBIrC85VzDH/66ScaUwOAxMTEli1buitsJIpimXter8BqteLJl9tsNjz5cpvNhicJIAgCnmtGEAQ814wXtoQAEBoaak/kRUZGVveIxpgxkJAAqgr33OO4qXYF7733Hn2waNEi+6o888wzbvxnWQcUnyCDCmyZcBkst2HvXhgwABYtgkGD7OJ1vV7vyg1hy5YtHMe9//77HMfl5eU98sgjFy5cSEpKcm8rF57nTSYTngljyXtnQLVMqKQwqMj4rMG6dQvGjQOdDvbsgQYNSvj2rpRI3rt3r+NGCQDatGnTqlUr9164kiTh6fysqiqq5D2qfDmqzs+oyLDOz9XG/v3wxRewcyeUtc938b5NM8d2P8hDTA0GA5U1oLjz8Dyqk/eouhs4Xg9eR4O/34O9C1SlI3zJwyIA5MYNqFsX2rWDPXvASVTSbDbbw+flYOHChTabbeTIkQUFBZ5bEkEQ8Dg1qqqi6qx18+ZNPGRQVQpDJalFVeHDlzysNoJQ68wZyMiAcndtrm+5jUbj+vXrPcrZYDDgOezO8zyquyUqMnjKlgGyOhao3D1f8rAumkzW558v31qlpKSkpqbiyWsoioKqB3rVGqDVBDLuTbZUE6iuGYvFwgxWlbwVQrhyO9C0bt26adOmTzzxxIsvvoiEM8dxnFa1t1zkg+jC4nlGhi2T324Jy0deXh7d+f/xxx+5ublWqxVDOpbneVS1hFAl71GRQbVMeMqEYSPjSx5WhRdcUVGRwWAYMGDAo48+ikQ8IklSYWEhkikihLB8uTPgUZ/Quy8eMnhal/uVhxUUFCQIgtFo/OGHH/CUHEMVdOc4DlWrBVQaC1RBd1RkUAk+/MfDysrKMhqNHMdFRUVNnToVCStBEPDII1VVRaUkoAUSkABV8h6VrAFVrxD/MVi03i4hZOfOnXg2Pkw46iseFio/ApWHxYSjHkF4ePiqVasAwGq1PvXUU0hY0ZJYSMgQQvAE1ADAFX2vZmCCD2dwqS0xM1hVwEsvvURrj40cORIJJY7j8GSFOY7DU7QEAFDiePVyAAAJR0lEQVSRcWNXAT+bGVRk/Mpg2f0IPFPM8zyqWkIsee8TM4PqmvGtYli+Z7BEUcSzC5MkCc/Gx17xFQlQ5csLCgoYGfxkXHIJfYsuqvI92GQNrAyAM7Ce2M6AKgPghx6W2WzGE1rGJmtAVa0hOTkZDxlUsgZUpSN8TtbgYx4WqjqWBoMBz90Sm6whIiKCuXtlApXGApXY2JcMlsViSUlJcWXHZ7VakYjdeZ7X6/XubGFUvS1hYGAgkpQ5IaR27dp4AnxGo5G2wEJyn0NyzQBArVq1NEih8jzvrgNJHJJqLVu3bj1//nyFB9nz8/OvXLmCJFhjNputVisSJ8tms8myjKQ3DMdxhYWF9tZqXr+vpKWl3XPPPRgudZ7nr1+/3rx5c7Xc2iSaLVNmZmbPnj018ACMRuO7777rnpuhD+HChQu7du1CQiYxMXHx4sVIyOTm5s6dOxfPSr377rt4yEycOBEPmbFjx+IhM23aNN+yAL6nw8JTwA8VGQaGql3DvkWYZ2vGwMDADBYDAwNDzTZYiqLgqYdFCEFVLBzV4WdUR3xR6blRFfBDdUbdFXC+tYklhKAqiY2Kj6IoeE75qqqK52Q4tsuGXcA1xWAxMDCwLSEDAwMDM1gMDAwMzGAxMDAwg8XAwMDADBYDAwNDteEz5WVu3rxZv359x3IO586dsze2GjhwoMZ8kpKSWrdu7TiSmJiYmJjYpk2bNm3aaEbDZrMdOnQIAJ588kl78ejCwsLY2Fh6cigyMvK+++7TgElGRsapU6fq1av3yCOPOI6fPHkyKyurV69eWp4ST01N/e2330JDQ3v27GkfvH79+oULFwCA47h+/fppWQwyNzdXFMUSVW6OHDmSn5//2GOP1alTR8tLNzs7mxDiWLrvypUriYmJdGYGDBiAuX+9D3hYqqrm5eX16NGjRNXdRYsWBQYGmkwmjXtt2Wy21NTUiRMnOg6mpaXdd999DRo0eOaZZ7QspDd9+vS8vLzk5OSvvvrK8eu6YsWKWrVqGY1Gzb4MPXr0MJlMu3fvPnXqlH1wz549a9eurV27dmhoqM1m04aJLMuNGzcOCgpav379+fPn7eN79+69deuWyWQymUxaCtYKCgq+/fbbX3/91XFwzZo1hw4dMhgMERERmmmhaR3tTz/9NC4uznF8w4YNVqvVZDIFBQVhl2XhP5+dnJxMy4ylpaU5jnuL/PLly7t16zZkyBDHwV9++WXz5s2EkEOHDv373//WhokkSR07diw9G7/++uuiRYs0rhURExNDCKGacjqoquqIESP+/PNPQsiUKVNSUlK0IZORkTF//nxCSGZmZkREhGPNhitXrmh8tdhstv79+xsMhp07d9oHVVVt3LhxVlYWIWTAgAHFxcXakCkuLqb+76FDhxzHH3vssZycHFatwT1o0qTJrVu3unfvXvpPr7zySp8+fd5//30t+UyePPn06dMlDuXExcV16dIFABo0aJCdna0Nk+TkZHuXxscff1ySJPpYEIQ333zz448/5jguISFBAyYXL1589NFH4U7bKGpAVVW9fPky3Xo8+OCDmnXAvXDhwsMPPwwARqMxNTXVLo1OSkqaNm1aTExM//79NTtTZTQaf/zxx9WrV5dw0m/evEmLl/Xp00ezwzpBQUFHjhxZsmRJifHY2NhXXnllypQpY8aMwVCoyydjWElJSbRD/QMPPABl1cH4888/W7ZsCQDNmjWbNWuWR1s5Wa3Ws2fP8jzfrl27unXrll5U+7ZfVVVPN5Uym83nzp3jed6xYLQoivaWTe3atbNYLLVq1erevfvu3bs1iGFx3N+OTNi3FfZpURTFZDJpc+XYP51Ssv+6cuXKVq1aAcDcuXPT0tLuvfdeLcMa5fxJ45ZopckkJiZGRkYCwKRJk3Jzc1G1yfCZGFZ4eHiTJk1atGjhLC5gP9Hat29f2sveczCZTE2aNKGRkTKf0KlTp7Nnz9KIZt26dT1KplatWpRMu3btdu/eTQdPnDhhf8LNmzep8apXr56nZ4aiY8eOv/zyCzVM9kGdTte2bdusrCwAOH36tLOpczs6deoUGxtLjXj79u3tFtMeW4yMjNRmWsq5nJo1a0a7Lh86dMi7nQpEUbT3xejQoYN3Z8YfYlgU3bp1ozEsQRCmTp1Kb57Lly9fvnz5qFGjNCajKMrTTz9NH2/dujUpKSkjIwMAqM3KyMjQjMk777yzatWqDz/88JtvviGEbNiwITU19eDBg0899VRsbOz9999/+vRpbZj84x//2L9//4svvvj7778TQj744ANFUQ4fPjx69Ojjx4/TfZA2TGRZvvfeew8fPjxkyJD4+HhCyJw5cwghffr0eeutt/bu3QsARUVFWl4wa9eutcewaLBvx44dM2bM2LdvX6dOnWRZ1pLMp59+ao9hzZw5k36VFixYsHXr1latWimKgtkO+Mzh5wsXLtx7772BgYGyLMfFxXXt2tVmsyUkJBBC2rVrp7FTTQi5dOlShw4dACAhISEsLKxevXppaWnp6enh4eGNGjXSjIkkSZcuXeI4rkOHDjqdLj4+vnHjxrVr146Pj7fZbPXr12/atKk2TPLy8q5evRocHExVHb/99tv999+v0+mSkpIKCwsjIyO1LPGek5Nz/fr12rVrU+nJ2bNnu3TpwvN8XFycLMvNmjXTOLN88+ZNnU5H2xqdPXu2a9euHMddvnzZYrG0bdtWYw/r2rVrwcHBtDHCmTNnunXrRoOw1NBrrLGo9H6fVWtgYGDwFTClOwMDAzNYDAwMDMxgMTAwMIPFwMDAwAwWAwMDAzNYDAwMzGAxMDAwMIPFwMDAwAwWAwMDM1gMDAwMzGAxMDAwMIPFwMDADBYDAwMDM1gMDAwMzGAxMDAwg8XAUF2IojhgwICCgoL8/PwBAwbYG2cwMLgCVsCPQWvExcXFxsaKojhkyBDaRoSBgXlYDEjRqVOnjIwMi8XCrBVDZaFnU8CgPU6fPu3dVjEMbEvIwOAS5s2b9/TTT1ut1nPnzk2bNo1NCAPzsBiQQpKkXr16de3aVZIkWZZlWabNohkYmIfFwMDgV2BBdwYGBmawGBgYGJjBYmBgYAaLgYGBgRksBgYGBmawGBgYmMFiYGBgYAaLgYGBgRksBgYGZrAYGBgYmMFiYGBgYAaLgYGBGSwGBgYGFPh/767zStpf/xQAAAAASUVORK5CYII="}}},{"metadata":{},"cell_type":"markdown","source":"\\begin{align}\n{Y = \\begin{bmatrix}\n    y_{0}        \\\\\n    y_{1}       \\\\\n    \\vdots  \\\\\n    y_{n}   \n\\end{bmatrix}} \\space \\space \\space \\space\n{X = \\begin{bmatrix}\n    x_{11}       & x_{12} & x_{13} & \\dots & x_{1p} \\\\\n    x_{21}       & x_{22} & x_{23} & \\dots & x_{2p} \\\\\n    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n    x_{n1}       & x_{n2} & x_{n3} & \\dots & x_{np}\n\\end{bmatrix}} \\space \\space \\space \\space\n{\\beta = \\begin{bmatrix}\n    \\beta_{0}        \\\\\n    \\beta_{1}       \\\\\n    \\vdots  \\\\\n    \\beta_{p}   \n\\end{bmatrix}} \\space \\space \\space \\space\n{\\epsilon = \\begin{bmatrix}\n    \\epsilon_{0}        \\\\\n    \\epsilon_{1}       \\\\\n    \\vdots  \\\\\n    \\epsilon_{n}   \n\\end{bmatrix}}\n\\end{align}"},{"metadata":{},"cell_type":"markdown","source":"Our goal is to find $\\beta$ such that ${|| Y -  X \\beta ||^2}$ is minimized. <br>\n\n$X$ has as many rows as the number of training samples and $p+1$ columns where $p$ is the number of predictiors, that in this case, after feature selection is 7 plus one first column that is an all one column that is used for the intercept (${\\beta_{0}}$).<br>\n\nBasically, each prediction in  $X \\beta$ is made by a linear combination of the features, weighted by $\\beta$<br>\n${y_i = \\sum_{j=0}^p{x_j\\beta_j}}$\n<br>\nthat is the prediction for the i-th observation according to the $p$ predictors."},{"metadata":{},"cell_type":"markdown","source":"Usually, **linear regression is not suggested for binary classification** because it can output values below 0 or above 1 (nonsense, assuming probabilistic meaning).<br>\nIn this case I decided to perform it anyway to **compare the results** with another regression technique called **logistic regression** that instead is very suitable for binary classification."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# holdout\n\nthreshold=0.5\n\n# oversampled \nlr = LinearRegression(fit_intercept=True)\nlr.fit(ho_train_df_rs[all_features], ho_train_df_rs['DEATH_EVENT'])\npred = lr.predict(ho_val_df[all_features])\nraw_pred_rs = pred.copy()\npred[pred>=0.5]=1\npred[pred<0.5]=0\nrs_acc=(accuracy_score(ho_val_df[\"DEATH_EVENT\"], pred))\nrs_rec=(recall_score(ho_val_df[\"DEATH_EVENT\"], pred))\nrs_pre=(precision_score(ho_val_df[\"DEATH_EVENT\"], pred))\nrs_f1=(f1_score(ho_val_df[\"DEATH_EVENT\"], pred))\n    \n# SMOTE \nlr = LinearRegression(fit_intercept=True)\nlr.fit(ho_train_df_sm[all_features], ho_train_df_sm['DEATH_EVENT'])\npred = lr.predict(ho_val_df[all_features])\nraw_pred_sm = pred.copy()\npred[pred>=0.5]=1\npred[pred<0.5]=0\nsm_acc=(accuracy_score(ho_val_df[\"DEATH_EVENT\"], pred))\nsm_rec=(recall_score(ho_val_df[\"DEATH_EVENT\"], pred))\nsm_pre=(precision_score(ho_val_df[\"DEATH_EVENT\"], pred))\nsm_f1=(f1_score(ho_val_df[\"DEATH_EVENT\"], pred))\n    \n# not oversampled\nlr = LinearRegression(fit_intercept=True)\nlr.fit(ho_train_df[all_features], ho_train_df['DEATH_EVENT'])\npred = lr.predict(ho_val_df[all_features])\nraw_pred_no_rs = pred.copy()\npred[pred>=0.5]=1\npred[pred<0.5]=0\nno_rs_acc=(accuracy_score(ho_val_df[\"DEATH_EVENT\"], pred))\nno_rs_rec=(recall_score(ho_val_df[\"DEATH_EVENT\"], pred))\nno_rs_pre=(precision_score(ho_val_df[\"DEATH_EVENT\"], pred))\nno_rs_f1=(f1_score(ho_val_df[\"DEATH_EVENT\"], pred))","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# oversampled \nlr = LinearRegression(fit_intercept=True)\nrs_acc, rs_rec, rs_pre, rs_f1 = kfold.fit_predict(lr, ho_train_df, ho_train_df['DEATH_EVENT'], threshold=0.5, resampling=\"oversampling\")\n    \n# SMOTE \nlr = LinearRegression(fit_intercept=True)\nsm_acc, sm_rec, sm_pre, sm_f1 = kfold.fit_predict(lr, ho_train_df, ho_train_df['DEATH_EVENT'], threshold=0.5, resampling=\"SMOTE\", cached=True)\n    \n# not oversampled\nlr = LinearRegression(fit_intercept=True)\nno_rs_acc, no_rs_rec, no_rs_pre, no_rs_f1= kfold.fit_predict(lr, ho_train_df, ho_train_df['DEATH_EVENT'], threshold=0.5)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# https://python-graph-gallery.com/11-grouped-barplot/\n\nfig = plt.figure(figsize=(6, 4))\nbarWidth = 0.25\nspace=0.0\n \nbars1 = [no_rs_f1, no_rs_rec, no_rs_pre, no_rs_acc]\nbars2 = [rs_f1, rs_rec, rs_pre, rs_acc]\nbars3 = [sm_f1, sm_rec, sm_pre, sm_acc]\n\nr1 = np.arange(len(bars1))\nr2 = [x + barWidth + space for x in r1]\nr3 = [x + barWidth + space for x in r2]\n\nplt.barh(r3, bars1, label=\"Original\",height=barWidth, edgecolor='white')\nplt.barh(r2, bars2, label=\"Oversampling\", height=barWidth, edgecolor='white')\nplt.barh(r1, bars3, label=\"SMOTE\", height=barWidth, edgecolor='white')\n\nplt.title(\"Mean values on 5-Fold CV\")\nplt.yticks([r + barWidth for r in range(len(bars1))], [\"F1 score\", \"Recall\", \"Precision\", \"Accuracy\"])\nplt.xlim(0, 1)\nplt.gca().xaxis.grid(True, linestyle=':')\nplt.legend();","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# area under the curve evaluation\ndef roc_area(tpr, fpr):\n    area = 0\n    for i in range(len(tpr)-1):\n        base = fpr[i+1]-fpr[i]\n        h = tpr[i]\n        area += base*h\n        \n    return round(area, 3)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fpr_rs, tpr_rs, thresholds = roc_curve(ho_val_df[\"DEATH_EVENT\"], raw_pred_rs)\nfpr_no_rs, tpr_no_rs, thresholds = roc_curve(ho_val_df[\"DEATH_EVENT\"], raw_pred_no_rs)\nfpr_sm, tpr_sm, thresholds = roc_curve(ho_val_df[\"DEATH_EVENT\"], raw_pred_sm)\n\nplt.figure(figsize=(12, 4))\nplt.subplot(1, 3, 1)\nplt.plot(fpr_no_rs, tpr_no_rs, label=f\"Original (area={roc_area(tpr_no_rs, fpr_no_rs)})\", linewidth=3, color='#990303')\nplt.grid()\nplt.ylim(0, 1)\nplt.xlim(0, 1)\nplt.legend(loc=4)\nplt.xlabel(\"False positive rate\")\nplt.ylabel(\"True positive rate\")\nplt.title(\"ROC curve [Original]\")\nplt.fill_between(fpr_no_rs, 0, tpr_no_rs, alpha=0.05, color='#990303')\n\nplt.subplot(1, 3, 2)\nplt.plot(fpr_rs, tpr_rs, label=f\"Oversampling (area={roc_area(tpr_rs, fpr_rs)})\", linewidth=3, color='#990303')\nplt.grid()\nplt.ylim(0, 1)\nplt.xlim(0, 1)\nplt.legend(loc=4)\nplt.xlabel(\"False positive rate\")\nplt.ylabel(\"True positive rate\")\nplt.title(\"ROC curve [Oversampling]\")\nplt.fill_between(fpr_rs, 0, tpr_rs, alpha=0.05, color='#990303')\n\nplt.subplot(1, 3, 3)\nplt.plot(fpr_sm, tpr_sm, label=f\"SMOTE (area={roc_area(tpr_sm, fpr_sm)})\", linewidth=3, color='#990303')\nplt.grid()\nplt.ylim(0, 1)\nplt.xlim(0, 1)\nplt.legend(loc=4)\nplt.xlabel(\"False positive rate\")\nplt.ylabel(\"True positive rate\")\nplt.title(\"ROC curve [SMOTE]\")\nplt.fill_between(fpr_sm, 0, tpr_sm, alpha=0.05, color='#990303');\n\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='Logistic_regression'></a>\n## Logistic Regression <a style=\"text-decoration:none\" href=\"#Classification_models\">⤴</a>\n\nLogistic regression is a generalized linear model in which the link function is not the identity (as in the case of linear regression) but is the **logit**.<br>\n\n\\begin{align}\n{logit(p) = ln(\\frac {p}{1-p})}\n\\end{align}\n\nThe **link function** is a function connecting the expected value of the response and the linear combination of predictors.<br>\nPractically:<br>\n\n\\begin{align}\n{logit(p(X)) = X\\cdot\\beta}\n\\end{align}\n\n\\begin{align}\n{p(X) = logit^{-1}(X\\cdot\\beta)}\n\\end{align}\n\n\\begin{align}\n{p(X) = S(X\\cdot\\beta)}\n\\end{align}\n\nwhere $p$ is the expected value of the prediction that in this case (binary) can be modeled as a bernoulli.<br>\n\nThe logit \"stretches\" the interval (0, 1) into the whole real line.\nThe inverse of the logit is called **sigmoid**:\n\n\\begin{align}\n{\\displaystyle S(x)={\\frac {1}{1+e^{-x}}}={\\frac {e^{x}}{e^{x}+1}}.}\n\\end{align}\nso:\n\\begin{align}\n{p(X) = \\frac{e^{X \\beta}}{1+e^{X \\beta}}}\n\\end{align}\n\n![regression.png](attachment:regression.png)\n\nIn this way every prediction is bounded between 0 and 1, assuming a probabilistic meaning.<br>\nFor this reason, logistic regression is very suitable for binary classification.","attachments":{"regression.png":{"image/png":"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"}}},{"metadata":{},"cell_type":"markdown","source":"Here we can see the results on our dataset:"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"c = 1\npenalty = \"l2\"\nsolver = \"liblinear\"\nmulti_class = \"auto\"\n\n# oversampled\nlr = LogisticRegression(penalty=penalty, C=c, fit_intercept=True, class_weight=None,\n                   random_state=42, solver=solver, multi_class=multi_class)\n\nlr.fit(ho_train_df_rs[all_features], ho_train_df_rs['DEATH_EVENT'])\npred = lr.predict(ho_val_df[all_features])\nraw_pred_rs = lr.predict_proba(ho_val_df[all_features])[:, 1]\n\nrs_acc=(accuracy_score(ho_val_df[\"DEATH_EVENT\"], pred))\nrs_rec=(recall_score(ho_val_df[\"DEATH_EVENT\"], pred))\nrs_pre=(precision_score(ho_val_df[\"DEATH_EVENT\"], pred))\nrs_f1=(f1_score(ho_val_df[\"DEATH_EVENT\"], pred))\n    \n# SMOTE \nlr = LogisticRegression(penalty=penalty, C=c, fit_intercept=True, class_weight=None,\n                   random_state=42, solver=solver, multi_class=multi_class)\n\nlr.fit(ho_train_df_sm[all_features], ho_train_df_sm['DEATH_EVENT'])\npred = lr.predict(ho_val_df[all_features])\nraw_pred_sm = lr.predict_proba(ho_val_df[all_features])[:, 1]\n\nsm_acc=(accuracy_score(ho_val_df[\"DEATH_EVENT\"], pred))\nsm_rec=(recall_score(ho_val_df[\"DEATH_EVENT\"], pred))\nsm_pre=(precision_score(ho_val_df[\"DEATH_EVENT\"], pred))\nsm_f1=(f1_score(ho_val_df[\"DEATH_EVENT\"], pred))\n    \n# not oversampled\nlr = LogisticRegression(penalty=penalty, C=c, fit_intercept=True, class_weight=None,\n                   random_state=42, solver=solver, multi_class=multi_class)\n\nlr.fit(ho_train_df[all_features], ho_train_df['DEATH_EVENT'])\npred = lr.predict(ho_val_df[all_features])\nraw_pred_no_rs = lr.predict_proba(ho_val_df[all_features])[:, 1]\n\nno_rs_acc=(accuracy_score(ho_val_df[\"DEATH_EVENT\"], pred))\nno_rs_rec=(recall_score(ho_val_df[\"DEATH_EVENT\"], pred))\nno_rs_pre=(precision_score(ho_val_df[\"DEATH_EVENT\"], pred))\nno_rs_f1=(f1_score(ho_val_df[\"DEATH_EVENT\"], pred))\n\n# class-weight\nlr = LogisticRegression(penalty=penalty, C=c, fit_intercept=True, class_weight=\"balanced\",\n                   random_state=42, solver=solver, multi_class=multi_class)\n\nlr.fit(ho_train_df[all_features], ho_train_df['DEATH_EVENT'])\npred = lr.predict(ho_val_df[all_features])\nraw_pred_w = lr.predict_proba(ho_val_df[all_features])[:, 1]\n\nw_acc=(accuracy_score(ho_val_df[\"DEATH_EVENT\"], pred))\nw_rec=(recall_score(ho_val_df[\"DEATH_EVENT\"], pred))\nw_pre=(precision_score(ho_val_df[\"DEATH_EVENT\"], pred))\nw_f1=(f1_score(ho_val_df[\"DEATH_EVENT\"], pred))","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# oversampled\nlr = LogisticRegression(penalty=penalty, C=c, fit_intercept=True, class_weight=None,\n                   random_state=42, solver=solver, multi_class=multi_class)\n\nrs_acc, rs_rec, rs_pre, rs_f1 = kfold.fit_predict(lr, ho_train_df, ho_train_df['DEATH_EVENT'],\n                                                  threshold=0.5, resampling=\"oversampling\")\n    \n# SMOTE \nlr = LogisticRegression(penalty=penalty, C=c, fit_intercept=True, class_weight=None,\n                   random_state=42, solver=solver, multi_class=multi_class)\n\nsm_acc, sm_rec, sm_pre, sm_f1= kfold.fit_predict(lr, ho_train_df, ho_train_df['DEATH_EVENT'],\n                                                 threshold=0.5, resampling=\"SMOTE\", cached=True)\n    \n# not oversampled\nlr = LogisticRegression(penalty=penalty, C=c, fit_intercept=True, class_weight=None,\n                   random_state=42, solver=solver, multi_class=multi_class)\n\nno_rs_acc, no_rs_rec, no_rs_pre, no_rs_f1= kfold.fit_predict(lr, ho_train_df, ho_train_df['DEATH_EVENT'], threshold=0.5)\n\n# class-weight\nlr = LogisticRegression(penalty=penalty, C=c, fit_intercept=True, class_weight=\"balanced\",\n                   random_state=42, solver=solver, multi_class=multi_class)\n\nw_acc, w_rec, w_pre, w_f1= kfold.fit_predict(lr, ho_train_df, ho_train_df['DEATH_EVENT'], threshold=0.5)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"'''\nprint(f\"no_rs:{no_rs_acc},{no_rs_f1}\")\nprint(f\"rs:{rs_acc},{rs_f1}\")\nprint(f\"sm:{sm_acc},{sm_f1}\")\nprint(f\"w:{w_acc},{w_f1}\")\n''';","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# https://python-graph-gallery.com/11-grouped-barplot/\n\nfig = plt.figure(figsize=(6, 4))\nbarWidth = 0.2\nspace=0.01\n \nbars1 = [no_rs_f1, no_rs_rec, no_rs_pre, no_rs_acc]\nbars2 = [rs_f1, rs_rec, rs_pre, rs_acc]\nbars3 = [sm_f1, sm_rec, sm_pre, sm_acc]\nbars4 = [w_f1, w_rec, w_pre, w_acc]\n\nr1 = np.arange(len(bars1))\nr2 = [x + barWidth + space for x in r1]\nr3 = [x + barWidth + space for x in r2]\nr4 = [x + barWidth + space for x in r3]\n\nplt.barh(r4, bars1, label=\"Original\",height=barWidth, edgecolor='white', )\nplt.barh(r3, bars2, label=\"Oversampling\", height=barWidth, edgecolor='white',)\nplt.barh(r2, bars3, label=\"SMOTE\", height=barWidth, edgecolor='white', )\nplt.barh(r1, bars4, label=\"class-weight\", height=barWidth, edgecolor='white', )\n\nplt.title(\"Mean values on 5-Fold CV\")\nplt.yticks([r + barWidth*1.5 for r in range(len(bars1))], [\"F1 score\", \"Recall\", \"Precision\", \"Accuracy\", ])\nplt.xlim(0, 1)\nplt.gca().xaxis.grid(True, linestyle=':')\nplt.legend();","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fpr_rs, tpr_rs, thresholds = roc_curve(ho_val_df[\"DEATH_EVENT\"], raw_pred_rs)\nfpr_no_rs, tpr_no_rs, thresholds = roc_curve(ho_val_df[\"DEATH_EVENT\"], raw_pred_no_rs)\nfpr_sm, tpr_sm, thresholds = roc_curve(ho_val_df[\"DEATH_EVENT\"], raw_pred_sm)\nfpr_w, tpr_w, thresholds = roc_curve(ho_val_df[\"DEATH_EVENT\"], raw_pred_w)\n\nplt.figure(figsize=(8, 8))\nplt.subplot(2, 2, 1)\nplt.plot(fpr_no_rs, tpr_no_rs, label=f\"Original (area={roc_area(tpr_no_rs, fpr_no_rs)})\", linewidth=3, color='#990303')\nplt.grid()\nplt.ylim(0, 1)\nplt.xlim(0, 1)\nplt.legend(loc=4)\nplt.xlabel(\"False positive rate\")\nplt.ylabel(\"True positive rate\")\nplt.title(\"ROC curve [Original]\")\nplt.fill_between(fpr_no_rs, 0, tpr_no_rs, alpha=0.05, color='#990303')\n\nplt.subplot(2, 2, 2)\nplt.plot(fpr_rs, tpr_rs, label=f\"Oversampling (area={roc_area(tpr_rs, fpr_rs)})\", linewidth=3, color='#990303')\nplt.grid()\nplt.ylim(0, 1)\nplt.xlim(0, 1)\nplt.legend(loc=4)\nplt.xlabel(\"False positive rate\")\nplt.ylabel(\"True positive rate\")\nplt.title(\"ROC curve [Oversampling]\")\nplt.fill_between(fpr_rs, 0, tpr_rs, alpha=0.05, color='#990303')\n\nplt.subplot(2, 2, 3)\nplt.plot(fpr_sm, tpr_sm, label=f\"SMOTE (area={roc_area(tpr_sm, fpr_sm)})\", linewidth=3, color='#990303')\nplt.grid()\nplt.ylim(0, 1)\nplt.xlim(0, 1)\nplt.legend(loc=4)\nplt.xlabel(\"False positive rate\")\nplt.ylabel(\"True positive rate\")\nplt.title(\"ROC curve [SMOTE]\")\nplt.fill_between(fpr_sm, 0, tpr_sm, alpha=0.05, color='#990303')\n\nplt.subplot(2, 2, 4)\nplt.plot(fpr_w, tpr_w, label=f\"Class-weight (area={roc_area(tpr_w, fpr_w)})\", linewidth=3, color='#990303')\nplt.grid()\nplt.ylim(0, 1)\nplt.xlim(0, 1)\nplt.legend(loc=4)\nplt.xlabel(\"False positive rate\")\nplt.ylabel(\"True positive rate\")\nplt.title(\"ROC curve [class-weight balanced]\")\nplt.fill_between(fpr_w, 0, tpr_w, alpha=0.05, color='#990303');\n\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='Support_vector_machine'></a>\n## Support Vector Machine <a style=\"text-decoration:none\" href=\"#Classification_models\">⤴</a>\n\nSupport vector machine is a powerful model used for both classification and regression.<br>\nIt consists in trying to fit an hyperplane that best divides the dataset into the two classes by maximizing the margin (the distance between the hyperplane and the closest points).\n\n![svm.png](attachment:svm.png)\n#### Hard margin\nThe simplest implementation is the hard margin SVM in which data needs to be linearly separable to allow the algorithm to converge.<br>\nConsidering the hyperplane described by the vector **w** such that:<br>\n\n\\begin{align}\n{L=\\{v:\\langle w,v\\rangle+ b= 0\\}}\\space , \\space ‖w‖=1\n\\end{align}\n\nthe distance of a point fom the hyperplane L can be evaluated in that way:<br>\n\n\\begin{align}\n{d(x,L) =| \\langle w,x \\rangle+ b |}\n\\end{align}\n\nwhile the distance between two points of two different classes on the margin is:\n\n\\begin{align}\n\\frac{<x_{+} - x_{-}, W>}{\\left \\| W \\right \\|} = \\frac{(<x_{+}, W> + \\space b) - (<x_{-}, W> + \\space b)}{\\left \\| W \\right \\|} = \\frac{2}{\\left \\| W \\right \\|}\n\\end{align}\n\nSo we need to find w and b such that that distance is **maximized** (considering the whole training set) and at the same time all the points are **classified correctly**.<br>\nEquivalently:<br>\n\n\\begin{align}\n{min_{w,b} \\frac{1}{2}‖w‖^2 \\space\\space s.t. \\space\\space \\forall i,\\space\\space y_i \\big(\\langle w,x_i\\rangle+b \\big)>1}\n\\end{align}\n\nwhere $y_i$ is the true label and the prediction is the evaluated distance of the sample from the hyperplane.\nBasically if the label and the prediction have the same sign it means that the prediction is on the correct side of the margin.<br>\nWhen the algorithm has converged, then the model can be described using only the points on the margin (called **support vectors**). This means that SVM can scale well.<br>\nBeing able to respect this constraint means having a linearly separable problem.<br>\nThe main drawback is that, in the real world, the vast majority of the problems are not linearly separable, and an algorithm like this one would not converge.<br>\nFor this reason we can add a term on the constraints to relax them.\n\n#### Soft margin\nIn this implementation, a \n<span style=\"color:red\">relaxation</span>\nis added to the constraint.<br>\n\n\\begin{align}\n{\nmin_{w,b} \\Big(\\frac{1}{2} ‖W‖^2 + {\\color{red} {C  \\sum_{i=1}^{m} {\\xi_i}}}\\Big)\n\\space \\space\ns.t.\n\\space \\space\n\\forall i,\\space\\space y_i \\big(\\langle w,x_i\\rangle+b \\big)≥ 1 \\color{red}{−\\xi_i\\space\\space and \\space\\space \\xi_i≥0}\n} \n\\end{align}\n\n$\\xi_i$ is called **slack variable**, it is the distance of $x_i$ from the corresponding class's margin if $x_i$ is on the wrong side of the margin and 0 otherwise.<br>\nSmaller values of C allows more errors in exchange of a bigger margin, while higher values can be used where it's needed to be less permissive regarding misclassificatons, with an higher risk of overfitting.\n\nIn some cases, problems are not well separable on the original feature space but they are separable on another space.<br>\nTherefore it's possible to map our features in another higher dimensional space in which hopefully it will be easier to learn.<br>\nOne simple example is the polynomial mapping:<br>\n\nA degree k polynomial is \n$p(x)=\\sum_{j=0}^{k}w_jx^j\\space$\n\nthat can also be seen as:\n$\\space \\langle w,\\psi(x)\\rangle \\space \\space where \\space \\space \\psi(x) = (1,x,x^2,...,x^k)$<br>\n\n$\\psi(x)$ is the polynomial mapping function\n\nThe problems arise in the case of multiple predictors.<br>\nIn fact in a polynomial mapping on a $\\rm I\\!R^2$ space:\n\n$ \\psi(\\textbf{x}) = (x_1^2, x_2^2, \\sqrt2x_1x_2)$ <br>\n(considering only the quadratic features)\n\nThis means that, in the cases in which there are a lot of features, mapping every time from the original to the new space could be costly (second order features in 1000 dimensions are around $5\\cdot10^5$ numbers)<br>\nThe solution is using kernel functions.<br>\n\n#### Kernel trick\nTo solve the svm optimization problem, computing inner products it's needed.<br>\nIn fact, according to **representer theorem**, $w$ can be written as:\n\n${w = \\sum_{i} \\alpha_i\\psi(x_i)}\\space$\n\nso it follows that: <br>\n\n$\\space ‖w‖^2 = \\langle \\sum_j{\\alpha_j\\psi(x_j)}, \\sum_j{\\alpha_j\\psi(x_j)}\\rangle = \\sum_{i,j=1}^{m}\\alpha_i\\alpha_j\\langle\\psi(x_i),\\psi(x_j)\\rangle $\n\nand the problem now consists in maximizing the margin over $\\alpha$.<br>\nOnce $\\alpha$ vector is obtained, **support vectors** correspond to samples that have an alpha value that is greater than zero (so to describe the model, only those samples are needed).\n\nConsidering the mapping, this operation could be unfeasible.<br>\nA **kernel function** is a function that implements the inner product in the new feature space (given $\\psi$):<br>\n\n${ K(\\textbf{x},\\textbf{x}^\\prime) =\\langle\\psi(\\textbf{x}),\\psi(\\textbf{x}^\\prime)\\rangle}$\n\nIn this way we don't have to explicitly apply ${\\psi}$ on our data and then compute the inner product.<br>\n\nA symmetric function $K: X\\times X \\rightarrow \\rm I\\!R$ can be a kernel function if and only if it respects the **Mercer theorem** that says that the Gram matrix, that is the matrix such that:\n${G_{i,j} = K (\\textbf{x}_i, \\textbf{x}_j)}$ needs to be **positive semidefinite**.\n\nA matrix is semidefinite if and only if:\n\n${\\textbf{x}^T G \\textbf{x} > 0\\space \\space\\forall \\textbf{x}\\in R^n \\backslash 0}$<br>\nand the aigenvalues are **non-negative**.\n\n<br><br>\nThe most used kernels are:\n* the **polynomial kernel** (degree k)<br>\n${(\\langle \\textbf{x}, \\textbf{x}^\\prime \\rangle + 1)^k }$\n\n\n* the **radial basis function** (gaussian kernel)<br>\n${e^{-\\gamma ‖\\textbf{x} - \\textbf{x}^\\prime‖^2}}$\n\nLinear kernel means that no mapping is done and the kernel is simply ${\\langle \\textbf{x}, \\textbf{x}^\\prime\\rangle}$","attachments":{"svm.png":{"image/png":"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"}}},{"metadata":{},"cell_type":"markdown","source":"To better see how soft margin svm works with different kernels, we can see the classification according to `serum_creatinine` and `ejection_fraction`.<br>\nThen the same algorithm will be tested on all the 7 features of the original, oversampled and SMOTE dataset."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def make_meshgrid(x, y, h=.02):\n    x_min, x_max = x.min() - 1, x.max() + 1\n    y_min, y_max = y.min() - 1, y.max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                         np.arange(y_min, y_max, h))\n    return xx, yy\n\ndef plot_contours(ax, clf, xx, yy, **params):\n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n    out = ax.contourf(xx, yy, Z, **params)\n    return out","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"%matplotlib notebook\nplt.ioff()\nfig, (ax1, ax2, ax3) = plt.subplots(1, 3);\n%matplotlib inline \nplt.ion()\n\nC = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\nh = 0.03\nx_min, x_max = hf_norm[\"ejection_fraction\"].min() - .5, hf_norm[\"ejection_fraction\"].max() + .5\ny_min, y_max = hf_norm[\"serum_creatinine\"].min() - .5, hf_norm[\"serum_creatinine\"].max() + .5\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                        np.arange(y_min, y_max, h))\n\nfig.set_size_inches(11, 4);\ndef animate_func(i):  \n    # linear\n    svm = SVC(C=C[i], kernel='linear', random_state=42, class_weight='balanced', probability=True)\n    svm.fit(hf_norm[[\"ejection_fraction\", \"serum_creatinine\"]], hf_norm['DEATH_EVENT'])\n\n    Z = svm.predict_proba(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z[:, 1].reshape(xx.shape)\n    \n    ax1.clear()\n    ax1.contourf(xx, yy, Z, alpha=0.7, cmap=cm_rev, antialiased=True)\n    ax1.scatter(hf_norm[hf_norm[\"DEATH_EVENT\"]==0][\"ejection_fraction\"],\n                hf_norm[hf_norm[\"DEATH_EVENT\"]==0][\"serum_creatinine\"], label=\"alive\", color=\"#990303\", edgecolor='BLACK')\n    ax1.scatter(hf_norm[hf_norm[\"DEATH_EVENT\"]==1][\"ejection_fraction\"],\n                hf_norm[hf_norm[\"DEATH_EVENT\"]==1][\"serum_creatinine\"], label=\"dead\", color=\"#9C9999\", edgecolor='BLACK')\n\n    ax1.set_xlabel(\"ejection_fraction\")\n    ax1.set_ylabel(\"serum_creatinine\")\n    ax1.legend();\n    ax1.set_title(f\"Linear (C={C[i]})\")\n    \n    # poly\n    svm = SVC(C=C[i], kernel='poly', random_state=42, gamma='auto', degree=3, class_weight='balanced', probability=True)\n    svm.fit(hf_norm[[\"ejection_fraction\", \"serum_creatinine\"]], hf_norm['DEATH_EVENT'])\n\n    Z = svm.predict_proba(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z[:, 1].reshape(xx.shape)\n\n    ax2.clear()\n    ax2.contourf(xx, yy, Z, alpha=0.7, cmap=cm_rev, antialiased=True)\n    ax2.scatter(hf_norm[hf_norm[\"DEATH_EVENT\"]==0][\"ejection_fraction\"],\n                hf_norm[hf_norm[\"DEATH_EVENT\"]==0][\"serum_creatinine\"], label=\"alive\", color=\"#990303\", edgecolor='BLACK')\n    ax2.scatter(hf_norm[hf_norm[\"DEATH_EVENT\"]==1][\"ejection_fraction\"],\n                hf_norm[hf_norm[\"DEATH_EVENT\"]==1][\"serum_creatinine\"], label=\"dead\", color=\"#9C9999\", edgecolor='BLACK')\n\n    ax2.set_xlabel(\"ejection_fraction\")\n    ax2.set_ylabel(\"serum_creatinine\")\n    ax2.legend();\n    ax2.set_title(f\"Polynomial (C={C[i]})\")\n\n    # rbf\n    svm = SVC(C=C[i], kernel='rbf', random_state=42, gamma='auto', class_weight='balanced', probability=True)\n    svm.fit(hf_norm[[\"ejection_fraction\", \"serum_creatinine\"]], hf_norm['DEATH_EVENT'])\n\n    Z = svm.predict_proba(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z[:, 1].reshape(xx.shape)\n\n    ax3.clear()\n    ax3.contourf(xx, yy, Z, alpha=0.7, cmap=cm_rev, antialiased=True)\n    \n    ax3.scatter(hf_norm[hf_norm[\"DEATH_EVENT\"]==0][\"ejection_fraction\"],\n                hf_norm[hf_norm[\"DEATH_EVENT\"]==0][\"serum_creatinine\"], label=\"alive\", color=\"#990303\", edgecolor='BLACK')\n    ax3.scatter(hf_norm[hf_norm[\"DEATH_EVENT\"]==1][\"ejection_fraction\"],\n                hf_norm[hf_norm[\"DEATH_EVENT\"]==1][\"serum_creatinine\"], label=\"dead\", color=\"#9C9999\", edgecolor='BLACK')\n\n    ax3.set_xlabel(\"ejection_fraction\")\n    ax3.set_ylabel(\"serum_creatinine\")\n    ax3.legend();\n    ax3.set_title(f\"Radial Basis Function (C={C[i]})\")\n    \n    fig.tight_layout()\n    return [fig]\n\nanim = animation.FuncAnimation(\n                               fig, \n                               animate_func, \n                               frames = len(C),\n                               interval = 500, # in ms\n                               );\nHTML(anim.to_jshtml())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let's apply it on our actual classification task."},{"metadata":{},"cell_type":"markdown","source":"<a id='Linear_kernel'></a>\n### Linear kernel"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"C = 1\n\n# original\nsvm = SVC(kernel=\"linear\", random_state=42, probability=True, C = C)\nsvm.fit(ho_train_df[all_features], ho_train_df['DEATH_EVENT'])\n\npred = svm.predict(ho_val_df[all_features])\nlin_raw_pred_no_rs = svm.predict_proba(ho_val_df[all_features])[:, 1]\n\nlin_no_rs_acc = accuracy_score(ho_val_df[\"DEATH_EVENT\"], pred)\nlin_no_rs_pre = recall_score(ho_val_df[\"DEATH_EVENT\"], pred)\nlin_no_rs_rec = precision_score(ho_val_df[\"DEATH_EVENT\"], pred)\nlin_no_rs_f1 = f1_score(ho_val_df[\"DEATH_EVENT\"], pred)\n\n# oversampled\nsvm = SVC(kernel=\"linear\", random_state=42, probability=True, C = C)\nsvm.fit(ho_train_df_rs[all_features], ho_train_df_rs['DEATH_EVENT'])\n\npred = svm.predict(ho_val_df[all_features])\nlin_raw_pred_rs = svm.predict_proba(ho_val_df[all_features])[:, 1]\n\nlin_rs_acc = accuracy_score(ho_val_df[\"DEATH_EVENT\"], pred)\nlin_rs_pre = recall_score(ho_val_df[\"DEATH_EVENT\"], pred)\nlin_rs_rec = precision_score(ho_val_df[\"DEATH_EVENT\"], pred)\nlin_rs_f1 = f1_score(ho_val_df[\"DEATH_EVENT\"], pred)\n\n# SMOTE\nsvm = SVC(kernel=\"linear\", random_state=42, probability=True, C = C)\nsvm.fit(ho_train_df_sm[all_features], ho_train_df_sm['DEATH_EVENT'])\n\npred = svm.predict(ho_val_df[all_features])\nlin_raw_pred_sm = svm.predict_proba(ho_val_df[all_features])[:, 1]\n\nlin_sm_acc = accuracy_score(ho_val_df[\"DEATH_EVENT\"], pred)\nlin_sm_pre = recall_score(ho_val_df[\"DEATH_EVENT\"], pred)\nlin_sm_rec = precision_score(ho_val_df[\"DEATH_EVENT\"], pred)\nlin_sm_f1 = f1_score(ho_val_df[\"DEATH_EVENT\"], pred)\n\n# class-weight balanced\nsvm = SVC(kernel=\"linear\", random_state=42, probability=True, class_weight=\"balanced\", C = C)\nsvm.fit(ho_train_df[all_features], ho_train_df['DEATH_EVENT'])\n\npred = svm.predict(ho_val_df[all_features])\nlin_raw_pred_w = svm.predict_proba(ho_val_df[all_features])[:, 1]\n\nlin_w_acc = accuracy_score(ho_val_df[\"DEATH_EVENT\"], pred)\nlin_w_pre = recall_score(ho_val_df[\"DEATH_EVENT\"], pred)\nlin_w_rec = precision_score(ho_val_df[\"DEATH_EVENT\"], pred)\nlin_w_f1 = f1_score(ho_val_df[\"DEATH_EVENT\"], pred)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"'''\nprint(f\"no_rs:{lin_no_rs_acc},{lin_no_rs_f1}\")\nprint(f\"rs:{lin_rs_acc},{lin_rs_f1}\")\nprint(f\"sm:{lin_sm_acc},{lin_sm_f1}\")\nprint(f\"w:{lin_w_acc},{lin_w_f1}\")\n''';","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# original\nsvm = SVC(kernel=\"linear\", random_state=42, probability=True, C = C)\nlin_no_rs_acc, lin_no_rs_pre, lin_no_rs_rec, lin_no_rs_f1 = kfold.fit_predict(svm, ho_train_df, ho_train_df['DEATH_EVENT'], threshold=0.5, resampling=\"oversampling\")\n\n# oversampled\nsvm = SVC(kernel=\"linear\", random_state=42, probability=True, C = C)\nlin_rs_acc, lin_rs_pre, lin_rs_rec, lin_rs_f1 = kfold.fit_predict(svm, ho_train_df, ho_train_df['DEATH_EVENT'], threshold=0.5, resampling=\"SMOTE\", cached=True)\n\n# SMOTE\nsvm = SVC(kernel=\"linear\", random_state=42, probability=True, C = C)\nlin_sm_acc, lin_sm_pre, lin_sm_rec, lin_sm_f1 = kfold.fit_predict(svm, ho_train_df, ho_train_df['DEATH_EVENT'], threshold=0.5)\n\n# class-weight balanced\nsvm = SVC(kernel=\"linear\", random_state=42, probability=True, class_weight=\"balanced\", C = C)\nlin_w_acc, lin_w_pre, lin_w_rec, lin_w_f1 = kfold.fit_predict(svm, ho_train_df, ho_train_df['DEATH_EVENT'], threshold=0.5)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"'''\nprint(f\"no_rs:{lin_no_rs_acc},{lin_no_rs_f1}\")\nprint(f\"rs:{lin_rs_acc},{lin_rs_f1}\")\nprint(f\"sm:{lin_sm_acc},{lin_sm_f1}\")\nprint(f\"w:{lin_w_acc},{lin_w_f1}\")\n''';","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# https://python-graph-gallery.com/11-grouped-barplot/\n\nfig = plt.figure(figsize=(6, 4))\nbarWidth = 0.2\nspace=0.01\n \nbars1 = [lin_no_rs_f1, lin_no_rs_rec, lin_no_rs_pre, lin_no_rs_acc]\nbars2 = [lin_rs_f1, lin_rs_rec, lin_rs_pre, lin_rs_acc]\nbars3 = [lin_sm_f1, lin_sm_rec, lin_sm_pre, lin_sm_acc]\nbars4 = [lin_w_f1, lin_w_rec, lin_w_pre, lin_w_acc]\n\nr1 = np.arange(len(bars1))\nr2 = [x + barWidth + space for x in r1]\nr3 = [x + barWidth + space for x in r2]\nr4 = [x + barWidth + space for x in r3]\n\nplt.barh(r4, bars1, label=\"Original\",height=barWidth, edgecolor='white')\nplt.barh(r3, bars2, label=\"Oversampling\", height=barWidth, edgecolor='white')\nplt.barh(r2, bars3, label=\"SMOTE\", height=barWidth, edgecolor='white')\nplt.barh(r1, bars4, label=\"class-weight\", height=barWidth, edgecolor='white')\n\nplt.title(\"Mean values on 5-Fold CV\")\nplt.yticks([r + barWidth*1.5 for r in range(len(bars1))], [\"F1 score\", \"Recall\", \"Precision\", \"Accuracy\", ])\nplt.xlim(0, 1)\nplt.gca().xaxis.grid(True, linestyle=':')\nplt.legend();","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_curve\n\nfpr_rs, tpr_rs, thresholds = roc_curve(ho_val_df[\"DEATH_EVENT\"], lin_raw_pred_rs)\nfpr_no_rs, tpr_no_rs, thresholds = roc_curve(ho_val_df[\"DEATH_EVENT\"], lin_raw_pred_no_rs)\nfpr_sm, tpr_sm, thresholds = roc_curve(ho_val_df[\"DEATH_EVENT\"], lin_raw_pred_sm)\nfpr_w, tpr_w, thresholds = roc_curve(ho_val_df[\"DEATH_EVENT\"], lin_raw_pred_w)\n\nplt.figure(figsize=(8, 8))\nplt.subplot(2, 2, 1)\nplt.plot(fpr_no_rs, tpr_no_rs, label=f\"Original (area={roc_area(tpr_no_rs, fpr_no_rs)})\", linewidth=3, color='#990303')\nplt.grid()\nplt.ylim(0, 1)\nplt.xlim(0, 1)\nplt.legend(loc=4)\nplt.xlabel(\"False positive rate\")\nplt.ylabel(\"True positive rate\")\nplt.title(\"ROC curve [Original]\")\nplt.fill_between(fpr_no_rs, 0, tpr_no_rs, alpha=0.05, color='#990303')\n\nplt.subplot(2, 2, 2)\nplt.plot(fpr_rs, tpr_rs, label=f\"Oversampling (area={roc_area(tpr_rs, fpr_rs)})\", linewidth=3, color='#990303')\nplt.grid()\nplt.ylim(0, 1)\nplt.xlim(0, 1)\nplt.legend(loc=4)\nplt.xlabel(\"False positive rate\")\nplt.ylabel(\"True positive rate\")\nplt.title(\"ROC curve [Oversampling]\")\nplt.fill_between(fpr_rs, 0, tpr_rs, alpha=0.05, color='#990303')\n\nplt.subplot(2, 2, 3)\nplt.plot(fpr_sm, tpr_sm, label=f\"SMOTE (area={roc_area(tpr_sm, fpr_sm)})\", linewidth=3, color='#990303')\nplt.grid()\nplt.ylim(0, 1)\nplt.xlim(0, 1)\nplt.legend(loc=4)\nplt.xlabel(\"False positive rate\")\nplt.ylabel(\"True positive rate\")\nplt.title(\"ROC curve [SMOTE]\")\nplt.fill_between(fpr_sm, 0, tpr_sm, alpha=0.05, color='#990303')\n\nplt.subplot(2, 2, 4)\nplt.plot(fpr_sm, tpr_sm, label=f\"class-weight (area={roc_area(tpr_w, fpr_w)})\", linewidth=3, color='#990303')\nplt.grid()\nplt.ylim(0, 1)\nplt.xlim(0, 1)\nplt.legend(loc=4)\nplt.xlabel(\"False positive rate\")\nplt.ylabel(\"True positive rate\")\nplt.title(\"ROC curve [class-weight='balanced']\")\nplt.fill_between(fpr_w, 0, tpr_w, alpha=0.05, color='#990303')\n\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='Polynomial_and_RBF_kernels'></a>\n### Polynomial and RBF kernels\n<a id='Gridsearch on C and gamma'></a>\n#### Gridsearch on C and gamma\n\nGamma is a kernel coefficient used in both polynomial and RBF kernels.<br>\nFor RBF kernels, gamma is the coefficient multiplied to the squared norm of $x-x^\\prime$ while for polynomial kernel, in the `libsvm` implementation (the one used by sklearn library), the kernel function is ${(\\gamma \\langle \\textbf{x}, \\textbf{x}^\\prime \\rangle + 1)^k }$\n\nGridsearch model selection is done with KFold crossvalidation."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def _make_combinations(lenghts, n, comb, combs):\n    if len(lenghts)==n:\n        combs.append(comb.copy())\n    else:\n        for i in range(lenghts[n]):\n            combs = _make_combinations(lenghts, n+1, comb+[i], combs)\n    return combs\n\ndef make_combinations(lenghts):\n    combs = []\n    combs = _make_combinations(lenghts, 0, [], combs)\n    return combs","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def gridsearch(model, train_set, train_lab, params, starting_conf, resampling = None):\n    \n    results = []\n    lenghts = [len(params[l]) for l in params.keys()]\n    combs = make_combinations(lenghts)\n    n_params = len(params.keys())\n\n    for comb in combs:\n        conf = starting_conf.copy()\n        for i, param in enumerate(params.keys()):\n            conf[param]=params[param][comb[i]]\n        \n        m = model(**conf)\n        \n        acc, pre, rec, f1 = kfold.fit_predict(model=m, X = train_set, y = train_lab, resampling=resampling, cached=True)\n        results.append([acc, f1, conf])\n    return np.asarray(results)  ","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"variable_params = {\n    \"C\": [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n    \"gamma\": [0.0001, 0.001, 0.01, 0.1, 1, 10, 100]\n}","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# Original dataset\nplt.figure(figsize=(9, 8))\n\n# poly 3 degree\nplt.subplot(2, 2, 1)\n\nstarting_params = {\"class_weight\":None, \"random_state\": 42, \"kernel\":\"poly\" , \"degree\": 3, 'max_iter':100000}\nresults = gridsearch(SVC, ho_train_df, ho_train_df['DEATH_EVENT'], variable_params, starting_params)\n\nmatrix_res = results[:, 0].reshape(len(variable_params[\"C\"]), len(variable_params[\"gamma\"]))\nmatrix_res = matrix_res.astype(np.float64)\n\nplt.tight_layout()\nax = sns.heatmap(matrix_res, annot=True, cmap=\"viridis\")\nplt.xlabel(\"gamma\")\nplt.ylabel(\"C\")\nplt.title(\"Polynomial (degree: 3) [Original] accuracy\")\nax.set_xticklabels(variable_params[\"gamma\"])\nax.set_yticklabels(variable_params[\"C\"])\n\nplt.subplot(2, 2, 2)\n\nmatrix_res = results[:, 1].reshape(len(variable_params[\"C\"]), len(variable_params[\"gamma\"]))\nmatrix_res = matrix_res.astype(np.float64)\n\nplt.tight_layout()\nax = sns.heatmap(matrix_res, annot=True, cmap=\"inferno\")\nplt.xlabel(\"gamma\")\nplt.ylabel(\"C\")\nplt.title(\"Polynomial (degree: 3) [Original] f1 score\")\nax.set_xticklabels(variable_params[\"gamma\"])\nax.set_yticklabels(variable_params[\"C\"])\n\n# rbf\nplt.subplot(2, 2, 3)\n\nstarting_params = {\"class_weight\":None, \"random_state\": 42, \"kernel\":\"rbf\", 'max_iter':100000}\nresults = gridsearch(SVC, ho_train_df, ho_train_df['DEATH_EVENT'], variable_params, starting_params)\n\nmatrix_res = results[:, 0].reshape(len(variable_params[\"C\"]), len(variable_params[\"gamma\"]))\nmatrix_res = matrix_res.astype(np.float64)\n\nplt.tight_layout()\nax = sns.heatmap(matrix_res, annot=True, cmap=\"viridis\")\nplt.xlabel(\"gamma\")\nplt.ylabel(\"C\")\nplt.title(\"RBF [Original] accuracy\")\nax.set_xticklabels(variable_params[\"gamma\"])\nax.set_yticklabels(variable_params[\"C\"]);\n\nplt.subplot(2, 2, 4)\n\nmatrix_res = results[:, 1].reshape(len(variable_params[\"C\"]), len(variable_params[\"gamma\"]))\nmatrix_res = matrix_res.astype(np.float64)\n\nplt.tight_layout()\nax = sns.heatmap(matrix_res, annot=True, cmap=\"inferno\")\nplt.xlabel(\"gamma\")\nplt.ylabel(\"C\")\nplt.title(\"RBF [Original] f1 score\")\nax.set_xticklabels(variable_params[\"gamma\"])\nax.set_yticklabels(variable_params[\"C\"]);","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# random oversampling dataset\nplt.figure(figsize=(9, 8))\n\n# poly 3 degree\nplt.subplot(2, 2, 1)\n\nstarting_params = {\"class_weight\":None, \"random_state\": 42, \"kernel\":\"poly\" , \"degree\": 3, 'max_iter':100000}\nresults = gridsearch(SVC, ho_train_df, ho_train_df['DEATH_EVENT'], variable_params, \n                     starting_params, resampling=\"oversampling\")\n\nmatrix_res = results[:, 0].reshape(len(variable_params[\"C\"]), len(variable_params[\"gamma\"]))\nmatrix_res = matrix_res.astype(np.float64)\n\nplt.tight_layout()\nax = sns.heatmap(matrix_res, annot=True, cmap=\"viridis\")\nplt.xlabel(\"gamma\")\nplt.ylabel(\"C\")\nplt.title(\"Polynomial (degree: 3) [Oversampling] accuracy\")\nax.set_xticklabels(variable_params[\"gamma\"])\nax.set_yticklabels(variable_params[\"C\"])\n\nplt.subplot(2, 2, 2)\n\nmatrix_res = results[:, 1].reshape(len(variable_params[\"C\"]), len(variable_params[\"gamma\"]))\nmatrix_res = matrix_res.astype(np.float64)\n\nplt.tight_layout()\nax = sns.heatmap(matrix_res, annot=True, cmap=\"inferno\")\nplt.xlabel(\"gamma\")\nplt.ylabel(\"C\")\nplt.title(\"Polynomial (degree: 3) [Oversampling] f1 score\")\nax.set_xticklabels(variable_params[\"gamma\"])\nax.set_yticklabels(variable_params[\"C\"])\n\n# rbf\nplt.subplot(2, 2, 3)\n\nstarting_params = {\"class_weight\":None, \"random_state\": 42, \"kernel\":\"rbf\", 'max_iter':100000}\nresults = gridsearch(SVC, ho_train_df, ho_train_df['DEATH_EVENT'], variable_params,\n                     starting_params, resampling=\"oversampling\")\n\nmatrix_res = results[:, 0].reshape(len(variable_params[\"C\"]), len(variable_params[\"gamma\"]))\nmatrix_res = matrix_res.astype(np.float64)\n\nplt.tight_layout()\nax = sns.heatmap(matrix_res, annot=True, cmap=\"viridis\")\nplt.xlabel(\"gamma\")\nplt.ylabel(\"C\")\nplt.title(\"RBF [Oversampling] accuracy\")\nax.set_xticklabels(variable_params[\"gamma\"])\nax.set_yticklabels(variable_params[\"C\"]);\n\nplt.subplot(2, 2, 4)\n\nmatrix_res = results[:, 1].reshape(len(variable_params[\"C\"]), len(variable_params[\"gamma\"]))\nmatrix_res = matrix_res.astype(np.float64)\n\nplt.tight_layout()\nax = sns.heatmap(matrix_res, annot=True, cmap=\"inferno\")\nplt.xlabel(\"gamma\")\nplt.ylabel(\"C\")\nplt.title(\"RBF [Oversampling] f1 score\")\nax.set_xticklabels(variable_params[\"gamma\"])\nax.set_yticklabels(variable_params[\"C\"]);","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# SMOTE dataset\nplt.figure(figsize=(9, 8))\n\n# poly 3 degree\nplt.subplot(2, 2, 1)\n\nstarting_params = {\"class_weight\":None, \"random_state\": 42, \"kernel\":\"poly\" , \"degree\": 3, 'max_iter':100000}\n\nresults = gridsearch(SVC, ho_train_df, ho_train_df['DEATH_EVENT'], variable_params, \n                     starting_params, resampling=\"SMOTE\")\n\nmatrix_res = results[:, 0].reshape(len(variable_params[\"C\"]), len(variable_params[\"gamma\"]))\nmatrix_res = matrix_res.astype(np.float64)\n\nplt.tight_layout()\nax = sns.heatmap(matrix_res, annot=True, cmap=\"viridis\")\nplt.xlabel(\"gamma\")\nplt.ylabel(\"C\")\nplt.title(\"Polynomial (degree: 3) [SMOTE] accuracy\")\nax.set_xticklabels(variable_params[\"gamma\"])\nax.set_yticklabels(variable_params[\"C\"])\n\nplt.subplot(2, 2, 2)\n\nmatrix_res = results[:, 1].reshape(len(variable_params[\"C\"]), len(variable_params[\"gamma\"]))\nmatrix_res = matrix_res.astype(np.float64)\n\nplt.tight_layout()\nax = sns.heatmap(matrix_res, annot=True, cmap=\"inferno\")\nplt.xlabel(\"gamma\")\nplt.ylabel(\"C\")\nplt.title(\"Polynomial (degree: 3) [SMOTE] f1 score\")\nax.set_xticklabels(variable_params[\"gamma\"])\nax.set_yticklabels(variable_params[\"C\"])\n\n# rbf\nplt.subplot(2, 2, 3)\n\nstarting_params = {\"class_weight\":None, \"random_state\": 42, \"kernel\":\"rbf\", 'max_iter':100000}\nresults = gridsearch(SVC, ho_train_df, ho_train_df['DEATH_EVENT'], variable_params,\n                     starting_params, resampling=\"SMOTE\")\n\nmatrix_res = results[:, 0].reshape(len(variable_params[\"C\"]), len(variable_params[\"gamma\"]))\nmatrix_res = matrix_res.astype(np.float64)\n\nplt.tight_layout()\nax = sns.heatmap(matrix_res, annot=True, cmap=\"viridis\")\nplt.xlabel(\"gamma\")\nplt.ylabel(\"C\")\nplt.title(\"RBF [SMOTE] accuracy\")\nax.set_xticklabels(variable_params[\"gamma\"])\nax.set_yticklabels(variable_params[\"C\"]);\n\nplt.subplot(2, 2, 4)\n\nmatrix_res = results[:, 1].reshape(len(variable_params[\"C\"]), len(variable_params[\"gamma\"]))\nmatrix_res = matrix_res.astype(np.float64)\n\nplt.tight_layout()\nax = sns.heatmap(matrix_res, annot=True, cmap=\"inferno\")\nplt.xlabel(\"gamma\")\nplt.ylabel(\"C\")\nplt.title(\"RBF [SMOTE] f1 score\")\nax.set_xticklabels(variable_params[\"gamma\"])\nax.set_yticklabels(variable_params[\"C\"]);","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# class weight\nplt.figure(figsize=(9, 8))\n\n# poly 3 degree\nplt.subplot(2, 2, 1)\n\nstarting_params = {\"class_weight\":\"balanced\", \"random_state\": 42, \"kernel\":\"poly\" , \"degree\": 3, 'max_iter':100000}\nresults = gridsearch(SVC, ho_train_df, ho_train_df['DEATH_EVENT'], variable_params, \n                     starting_params)\n\nmatrix_res = results[:, 0].reshape(len(variable_params[\"C\"]), len(variable_params[\"gamma\"]))\nmatrix_res = matrix_res.astype(np.float64)\n\nplt.tight_layout()\nax = sns.heatmap(matrix_res, annot=True, cmap=\"viridis\")\nplt.xlabel(\"gamma\")\nplt.ylabel(\"C\")\nplt.title(\"Polynomial (degree: 3)\\n [class-weight balanced] accuracy\")\nax.set_xticklabels(variable_params[\"gamma\"])\nax.set_yticklabels(variable_params[\"C\"])\n\nplt.subplot(2, 2, 2)\n\nmatrix_res = results[:, 1].reshape(len(variable_params[\"C\"]), len(variable_params[\"gamma\"]))\nmatrix_res = matrix_res.astype(np.float64)\n\nplt.tight_layout()\nax = sns.heatmap(matrix_res, annot=True, cmap=\"inferno\")\nplt.xlabel(\"gamma\")\nplt.ylabel(\"C\")\nplt.title(\"Polynomial (degree: 3)\\n [class-weight balanced] f1 score\")\nax.set_xticklabels(variable_params[\"gamma\"])\nax.set_yticklabels(variable_params[\"C\"])\n\n# rbf\nplt.subplot(2, 2, 3)\n\nstarting_params = {\"class_weight\":\"balanced\", \"random_state\": 42, \"kernel\":\"rbf\", 'max_iter':100000}\nresults = gridsearch(SVC, ho_train_df, ho_train_df['DEATH_EVENT'], variable_params,\n                     starting_params)\n\nmatrix_res = results[:, 0].reshape(len(variable_params[\"C\"]), len(variable_params[\"gamma\"]))\nmatrix_res = matrix_res.astype(np.float64)\n\nplt.tight_layout()\nax = sns.heatmap(matrix_res, annot=True, cmap=\"viridis\")\nplt.xlabel(\"gamma\")\nplt.ylabel(\"C\")\nplt.title(\"RBF [class-weight balanced] accuracy\")\nax.set_xticklabels(variable_params[\"gamma\"])\nax.set_yticklabels(variable_params[\"C\"]);\n\nplt.subplot(2, 2, 4)\n\nmatrix_res = results[:, 1].reshape(len(variable_params[\"C\"]), len(variable_params[\"gamma\"]))\nmatrix_res = matrix_res.astype(np.float64)\n\nplt.tight_layout()\nax = sns.heatmap(matrix_res, annot=True, cmap=\"inferno\")\nplt.xlabel(\"gamma\")\nplt.ylabel(\"C\")\nplt.title(\"RBF [class-weight balanced] f1 score\")\nax.set_xticklabels(variable_params[\"gamma\"])\nax.set_yticklabels(variable_params[\"C\"]);","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"best_poly_no_rs = {\"class_weight\":None, \"random_state\": 42, \"kernel\":\"poly\" , \"degree\": 3, \"C\":0.1, \"gamma\":1, \"probability\":True}\nbest_rbf_no_rs = {\"class_weight\":None, \"random_state\": 42, \"kernel\":\"rbf\" , \"C\":10, \"gamma\":0.1, \"probability\":True}\n\nbest_poly_rs = {\"class_weight\":None, \"random_state\": 42, \"kernel\":\"poly\" , \"degree\": 3, \"C\":0.1, \"gamma\":1, \"probability\":True}\nbest_rbf_rs = {\"class_weight\":None, \"random_state\": 42, \"kernel\":\"rbf\" , \"C\":0.1, \"gamma\":1, \"probability\":True}\n\nbest_poly_sm = {\"class_weight\":None, \"random_state\": 42, \"kernel\":\"poly\" , \"degree\": 3, \"C\":100, \"gamma\":0.1, \"probability\":True}\nbest_rbf_sm = {\"class_weight\":None, \"random_state\": 42, \"kernel\":\"rbf\" , \"C\":1, \"gamma\":0.1, \"probability\":True}\n\nbest_poly_w = {\"class_weight\":\"balanced\", \"random_state\": 42, \"kernel\":\"poly\" , \"degree\": 3, \"C\":10, \"gamma\":0.1, \"probability\":True}\nbest_rbf_w = {\"class_weight\":\"balanced\", \"random_state\": 42, \"kernel\":\"rbf\" , \"C\":1, \"gamma\":0.1, \"probability\":True}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Best models are re-trained on the whole training set and then evaluated on the validation set.\n\n<a id='Polynomial_kernel'></a>\n### Polynomial kernel - best models"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# original\nsvm = SVC(**best_poly_no_rs)\nsvm.fit(ho_train_df[all_features], ho_train_df['DEATH_EVENT'])\n\npred = svm.predict(ho_val_df[all_features])\npoly_raw_pred_no_rs = svm.predict_proba(ho_val_df[all_features])[:, 1]\n\npoly_no_rs_acc = accuracy_score(ho_val_df[\"DEATH_EVENT\"], pred)\npoly_no_rs_pre = recall_score(ho_val_df[\"DEATH_EVENT\"], pred)\npoly_no_rs_rec = precision_score(ho_val_df[\"DEATH_EVENT\"], pred)\npoly_no_rs_f1 = f1_score(ho_val_df[\"DEATH_EVENT\"], pred)\n\n# oversampling\nsvm = SVC(**best_poly_rs)\nsvm.fit(ho_train_df_rs[all_features], ho_train_df_rs['DEATH_EVENT'])\n\npred = svm.predict(ho_val_df[all_features])\npoly_raw_pred_rs = svm.predict_proba(ho_val_df[all_features])[:, 1]\n\npoly_rs_acc = accuracy_score(ho_val_df[\"DEATH_EVENT\"], pred)\npoly_rs_pre = recall_score(ho_val_df[\"DEATH_EVENT\"], pred)\npoly_rs_rec = precision_score(ho_val_df[\"DEATH_EVENT\"], pred)\npoly_rs_f1 = f1_score(ho_val_df[\"DEATH_EVENT\"], pred)\n\n# SMOTE\nsvm = SVC(**best_poly_sm)\nsvm.fit(ho_train_df_sm[all_features], ho_train_df_sm['DEATH_EVENT'])\n\npred = svm.predict(ho_val_df[all_features])\npoly_raw_pred_sm = svm.predict_proba(ho_val_df[all_features])[:, 1]\n\npoly_sm_acc = accuracy_score(ho_val_df[\"DEATH_EVENT\"], pred)\npoly_sm_pre = recall_score(ho_val_df[\"DEATH_EVENT\"], pred)\npoly_sm_rec = precision_score(ho_val_df[\"DEATH_EVENT\"], pred)\npoly_sm_f1 = f1_score(ho_val_df[\"DEATH_EVENT\"], pred)\n\n# class-weight balanced\nsvm = SVC(**best_poly_w)\nsvm.fit(ho_train_df[all_features], ho_train_df['DEATH_EVENT'])\n\npred = svm.predict(ho_val_df[all_features])\npoly_raw_pred_w = svm.predict_proba(ho_val_df[all_features])[:, 1]\n\npoly_w_acc = accuracy_score(ho_val_df[\"DEATH_EVENT\"], pred)\npoly_w_pre = recall_score(ho_val_df[\"DEATH_EVENT\"], pred)\npoly_w_rec = precision_score(ho_val_df[\"DEATH_EVENT\"], pred)\npoly_w_f1 = f1_score(ho_val_df[\"DEATH_EVENT\"], pred)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"'''\nprint(f\"no_rs:{poly_no_rs_acc},{poly_no_rs_f1}\")\nprint(f\"rs:{poly_rs_acc},{poly_rs_f1}\")\nprint(f\"sm:{poly_sm_acc},{poly_sm_f1}\")\nprint(f\"w:{poly_w_acc},{poly_w_f1}\")\n''';","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# https://python-graph-gallery.com/11-grouped-barplot/\n\nfig = plt.figure(figsize=(6, 4))\nbarWidth = 0.2\nspace=0.01\n \nbars1 = [poly_no_rs_f1, poly_no_rs_rec, poly_no_rs_pre, poly_no_rs_acc]\nbars2 = [poly_rs_f1, poly_rs_rec, poly_rs_pre, poly_rs_acc]\nbars3 = [poly_sm_f1, poly_sm_rec, poly_sm_pre, poly_sm_acc]\nbars4 = [poly_w_f1, poly_w_rec, poly_w_pre, poly_w_acc]\n\nr1 = np.arange(len(bars1))\nr2 = [x + barWidth + space for x in r1]\nr3 = [x + barWidth + space for x in r2]\nr4 = [x + barWidth + space for x in r3]\n\nplt.barh(r4, bars1, label=\"Original [C:0.1, gamma:1]\",height=barWidth, edgecolor='white', )\nplt.barh(r3, bars2, label=\"Oversampling [C:0.1, gamma:1]\", height=barWidth, edgecolor='white',)\nplt.barh(r2, bars3, label=\"SMOTE [C:100, gamma:0.1]\", height=barWidth, edgecolor='white', )\nplt.barh(r1, bars4, label=\"class-weight [C:10, gamma:0.1]\", height=barWidth, edgecolor='white', )\n\nplt.yticks([r + barWidth*1.5 for r in range(len(bars1))], [\"F1 score\", \"Recall\", \"Precision\", \"Accuracy\", ])\nplt.xlim(0, 1)\nplt.gca().xaxis.grid(True, linestyle=':')\nplt.legend(bbox_to_anchor=(1, 1));","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_curve\n\nfpr_rs, tpr_rs, thresholds = roc_curve(ho_val_df[\"DEATH_EVENT\"], poly_raw_pred_rs)\nfpr_no_rs, tpr_no_rs, thresholds = roc_curve(ho_val_df[\"DEATH_EVENT\"], poly_raw_pred_no_rs)\nfpr_sm, tpr_sm, thresholds = roc_curve(ho_val_df[\"DEATH_EVENT\"], poly_raw_pred_sm)\nfpr_w, tpr_w, thresholds = roc_curve(ho_val_df[\"DEATH_EVENT\"], poly_raw_pred_w)\n\nplt.figure(figsize=(8, 8))\nplt.subplot(2, 2, 1)\nplt.plot(fpr_no_rs, tpr_no_rs, label=f\"Original (area={roc_area(tpr_no_rs, fpr_no_rs)})\", linewidth=3, color='#990303')\nplt.grid()\nplt.ylim(0, 1)\nplt.xlim(0, 1)\nplt.legend(loc=4)\nplt.xlabel(\"False positive rate\")\nplt.ylabel(\"True positive rate\")\nplt.title(\"ROC curve [Original]\")\nplt.fill_between(fpr_no_rs, 0, tpr_no_rs, alpha=0.05, color='#990303')\n\nplt.subplot(2, 2, 2)\nplt.plot(fpr_rs, tpr_rs, label=f\"Oversampling (area={roc_area(tpr_rs, fpr_rs)})\", linewidth=3, color='#990303')\nplt.grid()\nplt.ylim(0, 1)\nplt.xlim(0, 1)\nplt.legend(loc=4)\nplt.xlabel(\"False positive rate\")\nplt.ylabel(\"True positive rate\")\nplt.title(\"ROC curve [Oversampling]\")\nplt.fill_between(fpr_rs, 0, tpr_rs, alpha=0.05, color='#990303')\n\nplt.subplot(2, 2, 3)\nplt.plot(fpr_sm, tpr_sm, label=f\"SMOTE (area={roc_area(tpr_sm, fpr_sm)})\", linewidth=3, color='#990303')\nplt.grid()\nplt.ylim(0, 1)\nplt.xlim(0, 1)\nplt.legend(loc=4)\nplt.xlabel(\"False positive rate\")\nplt.ylabel(\"True positive rate\")\nplt.title(\"ROC curve [SMOTE]\")\nplt.fill_between(fpr_sm, 0, tpr_sm, alpha=0.05, color='#990303')\n\nplt.subplot(2, 2, 4)\nplt.plot(fpr_w, tpr_w, label=f\"class-weight (area={roc_area(tpr_w, fpr_w)})\", linewidth=3, color='#990303')\nplt.grid()\nplt.ylim(0, 1)\nplt.xlim(0, 1)\nplt.legend(loc=4)\nplt.xlabel(\"False positive rate\")\nplt.ylabel(\"True positive rate\")\nplt.title(\"ROC curve [class-weight balanced]\")\nplt.fill_between(fpr_w, 0, tpr_w, alpha=0.05, color='#990303')\n\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='RBF_kernel'></a>\n### RBF kernel - best models"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# original\nsvm = SVC(**best_rbf_no_rs)\nsvm.fit(ho_train_df[all_features], ho_train_df['DEATH_EVENT'])\n\npred = svm.predict(ho_val_df[all_features])\nrbf_raw_pred_no_rs = svm.predict_proba(ho_val_df[all_features])[:, 1]\n\nrbf_no_rs_acc = accuracy_score(ho_val_df[\"DEATH_EVENT\"], pred)\nrbf_no_rs_pre = recall_score(ho_val_df[\"DEATH_EVENT\"], pred)\nrbf_no_rs_rec = precision_score(ho_val_df[\"DEATH_EVENT\"], pred)\nrbf_no_rs_f1 = f1_score(ho_val_df[\"DEATH_EVENT\"], pred)\n\n# oversampling\nsvm = SVC(**best_rbf_rs)\nsvm.fit(ho_train_df_rs[all_features], ho_train_df_rs['DEATH_EVENT'])\n\npred = svm.predict(ho_val_df[all_features])\nrbf_raw_pred_rs = svm.predict_proba(ho_val_df[all_features])[:, 1]\n\nrbf_rs_acc = accuracy_score(ho_val_df[\"DEATH_EVENT\"], pred)\nrbf_rs_pre = recall_score(ho_val_df[\"DEATH_EVENT\"], pred)\nrbf_rs_rec = precision_score(ho_val_df[\"DEATH_EVENT\"], pred)\nrbf_rs_f1 = f1_score(ho_val_df[\"DEATH_EVENT\"], pred)\n\n# SMOTE\nsvm = SVC(**best_rbf_sm)\nsvm.fit(ho_train_df_sm[all_features], ho_train_df_sm['DEATH_EVENT'])\n\npred = svm.predict(ho_val_df[all_features])\nrbf_raw_pred_sm = svm.predict_proba(ho_val_df[all_features])[:, 1]\n\nrbf_sm_acc = accuracy_score(ho_val_df[\"DEATH_EVENT\"], pred)\nrbf_sm_pre = recall_score(ho_val_df[\"DEATH_EVENT\"], pred)\nrbf_sm_rec = precision_score(ho_val_df[\"DEATH_EVENT\"], pred)\nrbf_sm_f1 = f1_score(ho_val_df[\"DEATH_EVENT\"], pred)\n\n# class-weight balanced\nsvm = SVC(**best_rbf_w)\nsvm.fit(ho_train_df[all_features], ho_train_df['DEATH_EVENT'])\n\npred = svm.predict(ho_val_df[all_features])\nrbf_raw_pred_w = svm.predict_proba(ho_val_df[all_features])[:, 1]\n\nrbf_w_acc = accuracy_score(ho_val_df[\"DEATH_EVENT\"], pred)\nrbf_w_pre = recall_score(ho_val_df[\"DEATH_EVENT\"], pred)\nrbf_w_rec = precision_score(ho_val_df[\"DEATH_EVENT\"], pred)\nrbf_w_f1 = f1_score(ho_val_df[\"DEATH_EVENT\"], pred)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"'''\nprint(f\"no_rs:{rbf_no_rs_acc},{rbf_no_rs_f1}\")\nprint(f\"rs:{rbf_rs_acc},{rbf_rs_f1}\")\nprint(f\"sm:{rbf_sm_acc},{rbf_sm_f1}\")\nprint(f\"w:{rbf_w_acc},{rbf_w_f1}\")\n''';","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# https://python-graph-gallery.com/11-grouped-barplot/\n\nfig = plt.figure(figsize=(6, 4))\nbarWidth = 0.2\nspace=0.01\n \nbars1 = [rbf_no_rs_f1, rbf_no_rs_rec, rbf_no_rs_pre, rbf_no_rs_acc]\nbars2 = [rbf_rs_f1, rbf_rs_rec, rbf_rs_pre, rbf_rs_acc]\nbars3 = [rbf_sm_f1, rbf_sm_rec, rbf_sm_pre, rbf_sm_acc]\nbars4 = [rbf_w_f1, rbf_w_rec, rbf_w_pre, rbf_w_acc]\n\nr1 = np.arange(len(bars1))\nr2 = [x + barWidth + space for x in r1]\nr3 = [x + barWidth + space for x in r2]\nr4 = [x + barWidth + space for x in r3]\n\nplt.barh(r4, bars1, label=\"Original [C:10, gamma:0.1]\",height=barWidth, edgecolor='white', )\nplt.barh(r3, bars2, label=\"Oversampling [C:0.1, gamma:1]\", height=barWidth, edgecolor='white',)\nplt.barh(r2, bars3, label=\"SMOTE [C:1, gamma:0.1]\", height=barWidth, edgecolor='white', )\nplt.barh(r1, bars4, label=\"class-weight [C:1, gamma:0.1]\", height=barWidth, edgecolor='white', )\n\nplt.yticks([r + barWidth*1.5 for r in range(len(bars1))], [\"F1 score\", \"Recall\", \"Precision\", \"Accuracy\"])\nplt.xlim(0, 1)\nplt.gca().xaxis.grid(True, linestyle=':')\nplt.legend(bbox_to_anchor=(1, 1));","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fpr_rs, tpr_rs, thresholds = roc_curve(ho_val_df[\"DEATH_EVENT\"], rbf_raw_pred_rs)\nfpr_no_rs, tpr_no_rs, thresholds = roc_curve(ho_val_df[\"DEATH_EVENT\"], rbf_raw_pred_no_rs)\nfpr_sm, tpr_sm, thresholds = roc_curve(ho_val_df[\"DEATH_EVENT\"], rbf_raw_pred_sm)\nfpr_w, tpr_w, thresholds = roc_curve(ho_val_df[\"DEATH_EVENT\"], rbf_raw_pred_w)\n\nplt.figure(figsize=(8, 8))\nplt.subplot(2, 2, 1)\nplt.plot(fpr_no_rs, tpr_no_rs, label=f\"Original (area={roc_area(tpr_no_rs, fpr_no_rs)})\", linewidth=3, color='#990303')\nplt.grid()\nplt.ylim(0, 1)\nplt.xlim(0, 1)\nplt.legend(loc=4)\nplt.xlabel(\"False positive rate\")\nplt.ylabel(\"True positive rate\")\nplt.title(\"ROC curve [Original]\")\nplt.fill_between(fpr_no_rs, 0, tpr_no_rs, alpha=0.05, color='#990303')\n\nplt.subplot(2, 2, 2)\nplt.plot(fpr_rs, tpr_rs, label=f\"Oversampling (area={roc_area(tpr_rs, fpr_rs)})\", linewidth=3, color='#990303')\nplt.grid()\nplt.ylim(0, 1)\nplt.xlim(0, 1)\nplt.legend(loc=4)\nplt.xlabel(\"False positive rate\")\nplt.ylabel(\"True positive rate\")\nplt.title(\"ROC curve [Oversampling]\")\nplt.fill_between(fpr_rs, 0, tpr_rs, alpha=0.05, color='#990303')\n\nplt.subplot(2, 2, 3)\nplt.plot(fpr_sm, tpr_sm, label=f\"SMOTE (area={roc_area(tpr_sm, fpr_sm)})\", linewidth=3, color='#990303')\nplt.grid()\nplt.ylim(0, 1)\nplt.xlim(0, 1)\nplt.legend(loc=4)\nplt.xlabel(\"False positive rate\")\nplt.ylabel(\"True positive rate\")\nplt.title(\"ROC curve [SMOTE]\")\nplt.fill_between(fpr_sm, 0, tpr_sm, alpha=0.05, color='#990303')\n\nplt.subplot(2, 2, 4)\nplt.plot(fpr_w, tpr_w, label=f\"class-weight (area={roc_area(tpr_w, fpr_w)})\", linewidth=3, color='#990303')\nplt.grid()\nplt.ylim(0, 1)\nplt.xlim(0, 1)\nplt.legend(loc=4)\nplt.xlabel(\"False positive rate\")\nplt.ylabel(\"True positive rate\")\nplt.title(\"ROC curve [class-weight balanced]\")\nplt.fill_between(fpr_w, 0, tpr_w, alpha=0.05, color='#990303')\n\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='K_nearest_neighbors'></a>\n## K Nearest Neighbors <a style=\"text-decoration:none\" href=\"#Classification_models\">⤴</a>\nKNN model tries to classify new points according to the class of the nearest neighbors.<br>\nNearest neighbors are evaluated according to a **distance metric** function and for each new point, only a fixed number of neighbors are taken into account.<br>\nThis model is quite simple but it doesn't scale well.<br>\nIn fact, to predict a new point is necessary to store the entire dataset, so when the number of features or the number of records is very high, the computation could be heavy.<br>\nConsidering the way KNN learns, it is possible to re-train an already trained model on new data.<br>\n<br>\nThe parameters taken into account are, of course, the number of neighbors but also the distance metric that is the **minkowski distance**:<br>\n\n${\\Big(\\sum_{i=1}^{n}{|x_i-y_i|^p}\\Big)^{1/p}}$\n\nin which the parameter $p$ is changed.<br>\nThen is also possible to **weight** the neighbors according to their distance from the new point.(`weights=distance`)<br>"},{"metadata":{"scrolled":true,"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"n_neigh = [1, 5, 10, 20, 50]\nmetric = \"minkowski\"\np = [2, 3, 4, 5]\nh = 0.03\n\nx_min, x_max = hf_norm[\"ejection_fraction\"].min() - .5, hf_norm[\"ejection_fraction\"].max() + .5\ny_min, y_max = hf_norm[\"serum_creatinine\"].min() - .5, hf_norm[\"serum_creatinine\"].max() + .5\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"print(\"Evaluting different number of nearest neighbors\")\n%matplotlib notebook\nplt.ioff()\nfig, (ax1, ax2) = plt.subplots(1, 2);\n%matplotlib inline \nplt.ion()\n\nfig.set_size_inches(8, 4);\ndef animate_func(i):\n    knn = KNeighborsClassifier(n_neigh[i], weights=\"uniform\", metric=metric, p=2)\n    knn.fit(hf_norm[[\"ejection_fraction\", \"serum_creatinine\"]], hf_norm['DEATH_EVENT'])\n    Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n    \n    ax1.clear()\n    ax1.contourf(xx, yy, Z, alpha=0.5, cmap=cm_rev)\n\n    ax1.scatter(hf_norm[hf_norm[\"DEATH_EVENT\"]==0][\"ejection_fraction\"],\n                    hf_norm[hf_norm[\"DEATH_EVENT\"]==0][\"serum_creatinine\"], label=\"alive\", color=\"#990303\", edgecolor='BLACK')\n    ax1.scatter(hf_norm[hf_norm[\"DEATH_EVENT\"]==1][\"ejection_fraction\"],\n                    hf_norm[hf_norm[\"DEATH_EVENT\"]==1][\"serum_creatinine\"], label=\"dead\", color=\"#9C9999\", edgecolor='BLACK')\n\n    ax1.set_xlabel(\"ejection_fraction\")\n    ax1.set_ylabel(\"serum_creatinine\")\n    ax1.legend();\n    ax1.set_title(f\"KNN (weights=uniform) n_neigh={n_neigh[i]}\")\n    \n    # distance\n    knn = KNeighborsClassifier(n_neigh[i], weights=\"distance\", metric=metric, p=2)\n    knn.fit(hf_norm[[\"ejection_fraction\", \"serum_creatinine\"]], hf_norm['DEATH_EVENT'])\n    Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n    \n    ax2.clear()\n    ax2.contourf(xx, yy, Z, alpha=0.5, cmap=cm_rev)\n\n    ax2.scatter(hf_norm[hf_norm[\"DEATH_EVENT\"]==0][\"ejection_fraction\"],\n                    hf_norm[hf_norm[\"DEATH_EVENT\"]==0][\"serum_creatinine\"], label=\"alive\", color=\"#990303\", edgecolor='BLACK')\n    ax2.scatter(hf_norm[hf_norm[\"DEATH_EVENT\"]==1][\"ejection_fraction\"],\n                    hf_norm[hf_norm[\"DEATH_EVENT\"]==1][\"serum_creatinine\"], label=\"dead\", color=\"#9C9999\", edgecolor='BLACK')\n\n    ax2.set_xlabel(\"ejection_fraction\")\n    ax2.set_ylabel(\"serum_creatinine\")\n    ax2.legend();\n    ax2.set_title(f\"KNN (weights=distance) n_neigh={n_neigh[i]}\")\n    \n    fig.tight_layout()\n    \nanim = animation.FuncAnimation(\n                               fig, \n                               animate_func, \n                               frames = len(n_neigh),\n                               interval = 500, # in ms\n                               );\nHTML(anim.to_jshtml())","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"print(\"Evaluating different values of p for minkowski distance metric\")\n%matplotlib notebook\nplt.ioff()\nfig, (ax1, ax2) = plt.subplots(1, 2);\n%matplotlib inline \nplt.ion()\n\nfig.set_size_inches(8, 4);\ndef animate_func(i):\n    knn = KNeighborsClassifier(10, weights=\"uniform\", metric=metric, p=p[i])\n    knn.fit(hf_norm[[\"ejection_fraction\", \"serum_creatinine\"]], hf_norm['DEATH_EVENT'])\n    Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n    \n    ax1.clear()\n    ax1.contourf(xx, yy, Z, alpha=0.5, cmap=cm_rev)\n\n    ax1.scatter(hf_norm[hf_norm[\"DEATH_EVENT\"]==0][\"ejection_fraction\"],\n                    hf_norm[hf_norm[\"DEATH_EVENT\"]==0][\"serum_creatinine\"], label=\"alive\", color=\"#990303\", edgecolor='BLACK')\n    ax1.scatter(hf_norm[hf_norm[\"DEATH_EVENT\"]==1][\"ejection_fraction\"],\n                    hf_norm[hf_norm[\"DEATH_EVENT\"]==1][\"serum_creatinine\"], label=\"dead\", color=\"#9C9999\", edgecolor='BLACK')\n\n    ax1.set_xlabel(\"ejection_fraction\")\n    ax1.set_ylabel(\"serum_creatinine\")\n    ax1.legend();\n    ax1.set_title(f\"KNN (weights=uniform) p={p[i]}\")\n    \n    # distance\n    knn = KNeighborsClassifier(10, weights=\"distance\", metric=metric, p=p[i])\n    knn.fit(hf_norm[[\"ejection_fraction\", \"serum_creatinine\"]], hf_norm['DEATH_EVENT'])\n    Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n    \n    ax2.clear()\n    ax2.contourf(xx, yy, Z, alpha=0.5, cmap=cm_rev)\n\n    ax2.scatter(hf_norm[hf_norm[\"DEATH_EVENT\"]==0][\"ejection_fraction\"],\n                    hf_norm[hf_norm[\"DEATH_EVENT\"]==0][\"serum_creatinine\"], label=\"alive\", color=\"#990303\", edgecolor='BLACK')\n    ax2.scatter(hf_norm[hf_norm[\"DEATH_EVENT\"]==1][\"ejection_fraction\"],\n                    hf_norm[hf_norm[\"DEATH_EVENT\"]==1][\"serum_creatinine\"], label=\"dead\", color=\"#9C9999\", edgecolor='BLACK')\n\n    ax2.set_xlabel(\"ejection_fraction\")\n    ax2.set_ylabel(\"serum_creatinine\")\n    ax2.legend();\n    ax2.set_title(f\"KNN (weights=distance) p={p[i]}\")\n    \n    fig.tight_layout()\n    \nanim = animation.FuncAnimation(\n                               fig, \n                               animate_func, \n                               frames = len(p),\n                               interval = 500, # in ms\n                               );\nHTML(anim.to_jshtml())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let's apply it on our training set considering all features "},{"metadata":{},"cell_type":"markdown","source":"<a id='weights_original'></a>\n### weights = \"original\""},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"n_neigh = range(1, 21, 2)\nn = len(n_neigh)\n\nrs_acc, rs_rec, rs_pre, rs_f1 = [], [], [], []\nsm_acc, sm_rec, sm_pre, sm_f1 = [], [], [], []\nno_rs_acc, no_rs_rec, no_rs_pre, no_rs_f1 = [], [], [], []\nrs_raw_pred, sm_raw_pred, no_rs_raw_pred = [], [], []\n\nfor i,n in enumerate(n_neigh):\n       \n    # oversampling\n    knn = KNeighborsClassifier(n, weights=\"uniform\", metric=metric, p=2)\n    knn.fit(ho_train_df_rs[all_features], ho_train_df_rs['DEATH_EVENT']);\n    pred = knn.predict_proba(ho_val_df[all_features]);\n    pred=pred[:, 1]\n    rs_raw_pred.append(pred.copy())\n    pred[pred>=0.5]=1\n    pred[pred<0.5]=0\n        \n    rs_acc.append(accuracy_score(ho_val_df[\"DEATH_EVENT\"], pred))\n    rs_rec.append(recall_score(ho_val_df[\"DEATH_EVENT\"], pred))\n    rs_pre.append(precision_score(ho_val_df[\"DEATH_EVENT\"], pred))\n    rs_f1.append(f1_score(ho_val_df[\"DEATH_EVENT\"], pred))\n\n    # SMOTE\n    knn = KNeighborsClassifier(n, weights=\"uniform\", metric=metric, p=2)\n    knn.fit(ho_train_df_sm[all_features], ho_train_df_sm['DEATH_EVENT']);\n    pred = knn.predict_proba(ho_val_df[all_features]);\n    pred=pred[:, 1]\n    sm_raw_pred.append(pred.copy())\n    pred[pred>=0.5]=1\n    pred[pred<0.5]=0\n        \n    sm_acc.append(accuracy_score(ho_val_df[\"DEATH_EVENT\"], pred))\n    sm_rec.append(recall_score(ho_val_df[\"DEATH_EVENT\"], pred))\n    sm_pre.append(precision_score(ho_val_df[\"DEATH_EVENT\"], pred))\n    sm_f1.append(f1_score(ho_val_df[\"DEATH_EVENT\"], pred))\n        \n    #no oversampling\n    knn = KNeighborsClassifier(n, weights=\"uniform\", metric=metric, p=2)\n    knn.fit(ho_train_df[all_features], ho_train_df['DEATH_EVENT']);\n    pred = knn.predict_proba(ho_val_df[all_features]);\n    pred=pred[:, 1]\n    no_rs_raw_pred.append(pred.copy())\n    pred[pred>=0.5]=1\n    pred[pred<0.5]=0\n        \n    no_rs_acc.append(accuracy_score(ho_val_df[\"DEATH_EVENT\"], pred))\n    no_rs_rec.append(recall_score(ho_val_df[\"DEATH_EVENT\"], pred))\n    no_rs_pre.append(precision_score(ho_val_df[\"DEATH_EVENT\"], pred))\n    no_rs_f1.append(f1_score(ho_val_df[\"DEATH_EVENT\"], pred))","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"#sorted([(a, b) for a, b in zip(sm_acc, sm_f1)], key=lambda a:a[0]+a[1])","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# k fold\nrs_acc, rs_rec, rs_pre, rs_f1= [], [], [], []\nsm_acc, sm_rec, sm_pre, sm_f1= [], [], [], []\nno_rs_acc, no_rs_rec, no_rs_pre, no_rs_f1= [], [], [], []\n\nfor i,n in enumerate(n_neigh):\n       \n    # oversampling\n    knn = KNeighborsClassifier(n, weights=\"uniform\", metric=metric, p=2)\n    acc, pre, rec, f1 = kfold.fit_predict(knn, ho_train_df, ho_train_df['DEATH_EVENT'],\n                                          threshold=0.5, resampling=\"oversampling\")\n    rs_acc.append(acc)\n    rs_rec.append(rec)\n    rs_pre.append(pre)\n    rs_f1.append(f1)\n\n    # SMOTE\n    knn = KNeighborsClassifier(n, weights=\"uniform\", metric=metric, p=2)\n    acc, pre, rec, f1 = kfold.fit_predict(knn, ho_train_df, ho_train_df['DEATH_EVENT'],\n                                          threshold=0.5, resampling=\"SMOTE\", cached=\"True\")\n    sm_acc.append(acc)\n    sm_rec.append(rec)\n    sm_pre.append(pre)\n    sm_f1.append(f1)\n        \n    #no oversampling\n    knn = KNeighborsClassifier(n, weights=\"uniform\", metric=metric, p=2)\n    acc, pre, rec, f1 = kfold.fit_predict(knn, ho_train_df, ho_train_df['DEATH_EVENT'],\n                                          threshold=0.5, resampling=None)\n    no_rs_acc.append(acc)\n    no_rs_rec.append(rec)\n    no_rs_pre.append(pre)\n    no_rs_f1.append(f1)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"#sorted([(a, b) for a, b in zip(sm_acc, sm_f1)], key=lambda a:a[0]+a[1])","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"print(\"K-Fold cross-validation metrics (5 folds)\")\nplt.figure(figsize=(10, 10))\n\n# accuracy\nplt.subplot(2, 2, 1)    \nplt.plot(list(n_neigh), rs_acc, label=\"Oversampling\", marker='o', linewidth=3)\nplt.plot(list(n_neigh), sm_acc, label=\"SMOTE\", marker='o', linewidth=3)\nplt.plot(list(n_neigh), no_rs_acc, label = \"original dataset\", marker='o', linewidth=3)\nplt.xticks(n_neigh)\n\nplt.legend()\nplt.grid(True)\nplt.ylim(0, 1)\nplt.ylabel(\"Validation accuracy\")\nplt.xlabel(\"n neighbors\")\nplt.title(\"Accuracy\")\n\n# precision\nplt.subplot(2, 2, 2)   \nplt.plot(list(n_neigh), rs_pre, label=\"Oversampling\", marker='o', linewidth=3)\nplt.plot(list(n_neigh), sm_pre, label=\"SMOTE\", marker='o', linewidth=3)\nplt.plot(list(n_neigh), no_rs_pre, label = \"original dataset\", marker='o', linewidth=3)\nplt.xticks(n_neigh)\n\nplt.legend()\nplt.grid(True)\nplt.ylim(0, 1)\nplt.ylabel(\"Validation precision\")\nplt.xlabel(\"n neighbors\")\nplt.title(\"Precision\")\n\n# recall\nplt.subplot(2, 2, 3)    \nplt.plot(list(n_neigh), rs_rec, label=\"Oversampling\", marker='o', linewidth=3)\nplt.plot(list(n_neigh), sm_rec, label=\"SMOTE\", marker='o', linewidth=3)\nplt.plot(list(n_neigh), no_rs_rec, label = \"original dataset\", marker='o', linewidth=3)\nplt.xticks(n_neigh)\n\nplt.legend()\nplt.grid(True)\nplt.ylim(0, 1)\nplt.ylabel(\"Validation recall\")\nplt.xlabel(\"n neighbors\")\nplt.title(\"Recall\")\n\n# f1 score\nplt.subplot(2, 2, 4)    \nplt.plot(list(n_neigh), rs_f1, label=\"Oversampling\", marker='o', linewidth=3)\nplt.plot(list(n_neigh), sm_f1, label=\"SMOTE\", marker='o', linewidth=3)\nplt.plot(list(n_neigh), no_rs_f1, label = \"original dataset\", marker='o', linewidth=3)\nplt.xticks(n_neigh)\n\nplt.legend()\nplt.grid(True)\nplt.ylim(0, 1)\nplt.ylabel(\"Validation f1 score\")\nplt.xlabel(\"n neighbors\")\nplt.title(\"F1 score\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='weights_distance'></a>\n### weights = \"distance\""},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"n_neigh = range(1, 21, 2)\nn = len(n_neigh)\n\nrs_acc, rs_rec, rs_pre, rs_f1= [], [], [], []\nsm_acc, sm_rec, sm_pre, sm_f1= [], [], [], []\nno_rs_acc, no_rs_rec, no_rs_pre, no_rs_f1= [], [], [], []\nrs_raw_pred, sm_raw_pred, no_rs_raw_pred = [], [], []\n\nfor i,n in enumerate(n_neigh):\n         \n    # oversampling\n    knn = KNeighborsClassifier(n, weights=\"distance\", metric=metric, p=2)\n    knn.fit(ho_train_df_rs[all_features], ho_train_df_rs['DEATH_EVENT']);\n    pred = knn.predict_proba(ho_val_df[all_features]);\n    pred=pred[:, 1]\n    rs_raw_pred.append(pred.copy())\n    pred[pred>=0.5]=1\n    pred[pred<0.5]=0\n        \n    rs_acc.append(accuracy_score(ho_val_df[\"DEATH_EVENT\"], pred))\n    rs_rec.append(recall_score(ho_val_df[\"DEATH_EVENT\"], pred))\n    rs_pre.append(precision_score(ho_val_df[\"DEATH_EVENT\"], pred))\n    rs_f1.append(f1_score(ho_val_df[\"DEATH_EVENT\"], pred))\n\n    # SMOTE\n    knn = KNeighborsClassifier(n, weights=\"distance\", metric=metric, p=2)\n    knn.fit(ho_train_df_sm[all_features], ho_train_df_sm['DEATH_EVENT']);\n    pred = knn.predict_proba(ho_val_df[all_features]);\n    pred=pred[:, 1]\n    sm_raw_pred.append(pred.copy())\n    pred[pred>=0.5]=1\n    pred[pred<0.5]=0\n        \n    sm_acc.append(accuracy_score(ho_val_df[\"DEATH_EVENT\"], pred))\n    sm_rec.append(recall_score(ho_val_df[\"DEATH_EVENT\"], pred))\n    sm_pre.append(precision_score(ho_val_df[\"DEATH_EVENT\"], pred))\n    sm_f1.append(f1_score(ho_val_df[\"DEATH_EVENT\"], pred))\n        \n    # no oversampling\n    knn = KNeighborsClassifier(n, weights=\"distance\", metric=metric, p=2)\n    knn.fit(ho_train_df[all_features], ho_train_df['DEATH_EVENT']);\n    pred = knn.predict_proba(ho_val_df[all_features]);\n    pred=pred[:, 1]\n    no_rs_raw_pred.append(pred.copy())\n    pred[pred>=0.5]=1\n    pred[pred<0.5]=0\n        \n    no_rs_acc.append(accuracy_score(ho_val_df[\"DEATH_EVENT\"], pred))\n    no_rs_rec.append(recall_score(ho_val_df[\"DEATH_EVENT\"], pred))\n    no_rs_pre.append(precision_score(ho_val_df[\"DEATH_EVENT\"], pred))\n    no_rs_f1.append(f1_score(ho_val_df[\"DEATH_EVENT\"], pred))","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"#sorted([(a, b) for a, b in zip(rs_acc, rs_f1)], key=lambda a:a[0]+a[1])","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# k fold\n\nrs_acc, rs_rec, rs_pre, rs_f1= [], [], [], []\nsm_acc, sm_rec, sm_pre, sm_f1= [], [], [], []\nno_rs_acc, no_rs_rec, no_rs_pre, no_rs_f1= [], [], [], []\n\nfor i,n in enumerate(n_neigh):\n         \n    # oversampling\n    knn = KNeighborsClassifier(n, weights=\"distance\", metric=metric, p=2)\n    acc, pre, rec, f1 = kfold.fit_predict(knn, ho_train_df, ho_train_df['DEATH_EVENT'],\n                                          threshold=0.5, resampling=\"oversampling\")\n    rs_acc.append(acc)\n    rs_rec.append(rec)\n    rs_pre.append(pre)\n    rs_f1.append(f1)\n    \n    # SMOTE\n    knn = KNeighborsClassifier(n, weights=\"distance\", metric=metric, p=2)\n    acc, pre, rec, f1 = kfold.fit_predict(knn, ho_train_df, ho_train_df['DEATH_EVENT'],\n                                          threshold=0.5, resampling=\"SMOTE\", cached=\"True\")\n    sm_acc.append(acc)\n    sm_rec.append(rec)\n    sm_pre.append(pre)\n    sm_f1.append(f1)  \n    \n    # no oversampling\n    knn = KNeighborsClassifier(n, weights=\"distance\", metric=metric, p=2)\n    acc, pre, rec, f1 = kfold.fit_predict(knn, ho_train_df, ho_train_df['DEATH_EVENT'],\n                                          threshold=0.5, resampling=None)\n    no_rs_acc.append(acc)\n    no_rs_rec.append(rec)\n    no_rs_pre.append(pre)\n    no_rs_f1.append(f1)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"#sorted([(a, b) for a, b in zip(sm_acc, sm_f1)], key=lambda a:a[0]+a[1])","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"print(\"K-Fold cross-validation metrics (5 folds)\")\nplt.figure(figsize=(10, 10))\n\n# accuracy\nplt.subplot(2, 2, 1)    \nplt.plot(list(n_neigh), rs_acc, label=\"Oversampling\", marker='o', linewidth=3)\nplt.plot(list(n_neigh), sm_acc, label=\"SMOTE\", marker='o', linewidth=3)\nplt.plot(list(n_neigh), no_rs_acc, label = \"original dataset\", marker='o', linewidth=3)\nplt.xticks(n_neigh)\n\nplt.legend()\nplt.grid(True)\nplt.ylim(0, 1)\nplt.ylabel(\"Validation accuracy\")\nplt.xlabel(\"n neighbors\")\nplt.title(\"Accuracy\")\n\n# precision\nplt.subplot(2, 2, 2)   \nplt.plot(list(n_neigh), rs_pre, label=\"Oversampling\", marker='o', linewidth=3)\nplt.plot(list(n_neigh), sm_pre, label=\"SMOTE\", marker='o', linewidth=3)\nplt.plot(list(n_neigh), no_rs_pre, label = \"original dataset\", marker='o', linewidth=3)\nplt.xticks(n_neigh)\n\nplt.legend()\nplt.grid(True)\nplt.ylim(0, 1)\nplt.ylabel(\"Validation precision\")\nplt.xlabel(\"n neighbors\")\nplt.title(\"Precision\")\n\n# recall\nplt.subplot(2, 2, 3)    \nplt.plot(list(n_neigh), rs_rec, label=\"Oversampling\", marker='o', linewidth=3)\nplt.plot(list(n_neigh), sm_rec, label=\"SMOTE\", marker='o', linewidth=3)\nplt.plot(list(n_neigh), no_rs_rec, label = \"original dataset\", marker='o', linewidth=3)\nplt.xticks(n_neigh)\n\nplt.legend()\nplt.grid(True)\nplt.ylim(0, 1)\nplt.ylabel(\"Validation recall\")\nplt.xlabel(\"n neighbors\")\nplt.title(\"Recall\")\n\n# f1 score\nplt.subplot(2, 2, 4)    \nplt.plot(list(n_neigh), rs_f1, label=\"Oversampling\", marker='o', linewidth=3)\nplt.plot(list(n_neigh), sm_f1, label=\"SMOTE\", marker='o', linewidth=3)\nplt.plot(list(n_neigh), no_rs_f1, label = \"original dataset\", marker='o', linewidth=3)\nplt.xticks(n_neigh)\n\nplt.legend()\nplt.grid(True)\nplt.ylim(0, 1)\nplt.ylabel(\"Validation f1 score\")\nplt.xlabel(\"n neighbors\")\nplt.title(\"F1 score\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='Naive_bayes'></a>\n## Naive Bayes <a style=\"text-decoration:none\" href=\"#Classification_models\">⤴</a>\n\nNaive bayes classifier is a probabilistic model based on the Bayes theorem in which is assumed a strong independence between the features.<br>\nThe concept is that every feature independently contribute to the final prediction.<br>\nOur goal is to find:\n\n\\begin{align}\n{p(C=k|x_1, \\dots, x_n)}\n\\end{align}\n\nthat is the probability of having the class k, given the sample features (considered independent).<br>\n**Bayes theorem** tells us that:\n\n\\begin{align}\n{p(C=k|X=x)=\\frac{p(C=k)p(X=x|C=k)}{p(X=x)}}\n\\end{align}\n\nBasically we have to compute the **feature distributions given the class** $p(X=x|C=k)$, while $p(X=x)$ in this case is called **evidence** and is evaluated in this way: $p(X=x)=\\sum_k p(C=k)p(X=x|C=k)$.<br>\n$p(C=k)$ is the **prior**.\nThis represents our prior knowledge on the class distribution. We will see that, changing the prior, the results may be very different.\n\nOnce the class probabilities given the sample are computed, one common strategy is to pick the most probable one:\n\n\\begin{align}\n{\\hat{y} = argmax_{k \\in {1, \\dots, K}} \\space p(C=k) \\prod_{i=1}^{n}p(X_i=x_i|C=k)}\n\\end{align}\nwhere  the **evidence** is ignored because is the same for every class.<br>\nSince we assume feature independence:\n\n${p(X=x|C=k})=\\prod_{i=1}^{n}p(X_i=x_i| C=k)$"},{"metadata":{},"cell_type":"markdown","source":"To evaluate the probability of a sample given the class, it's important to know the feature distributions.\nThis is a fundamental prior knowledge on features, picking the wrong distributions can leads to wrong results.\n\nOur dataset is composed by a mix of binary and continouos features.<br>\nFor the first ones, a **bernoulli distribution** is the easy pick, while for the others an analisys has to be done.<br>\nNaive bayes can achieve good results with categorical features so one approach could be **binning** the numerical features into bins and then treat those bins as categories, but the problem is that in that way you risk to lose some important information.<br>\nAnother approach consists in estimating the continouos distributions.\nAltough some of them seems to be gaussians, others are not (`ejection_fractions` seems to be bimodal).<br>\nTo tackle this problem it's possible to estimate the distributions without trying to fit an already known one, using **kernel density estimation**.\n\nThis technique has been proposed in 1995 by **John and Langley** [[3](#references)] under the name of **Flexible Bayes**.\n\nSo, to evaluate Naive Bayes classifier, a model based on bernoulli and kde distributions is implemented.<br>\nThen, to compare the results, also the classic **Gaussian Naive Bayes** is tested."},{"metadata":{},"cell_type":"markdown","source":"Before applying Naive Bayes we need to test that all the features are mutually independent, conditional on the target.<br>\nWe already tested the linear independence of the features with the correlation matrix, now we can perform the same correlation matrix but considering the samples **given the target label**."},{"metadata":{"scrolled":false,"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"dead = hf_norm[hf_norm[\"DEATH_EVENT\"]==1][all_features].corr(method='pearson')\nalive = hf_norm[hf_norm[\"DEATH_EVENT\"]==0][all_features].corr(method='pearson')\n\nplt.figure(figsize=(12, 4))\nplt.subplot(1, 2, 1)\nplt.title(\"Mutual correlation given DEATH_EVENT=1\")\nsns.heatmap(dead, vmin=-1, vmax=1, annot=True, xticklabels=False, cmap='viridis', fmt='.2f')\n\nplt.subplot(1, 2, 2)\nplt.title(\"Mutual correlation given DEATH_EVENT=0\")\nsns.heatmap(alive, vmin=-1, vmax=1, annot=True, xticklabels=False, cmap='viridis', fmt='.2f')\n\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the correlation matrices we can see that, apart from some pairs of features (`anaemia`-`CPK`, `sex`-`ejection_fraction`), the vast majority seems to be strongly uncorrelated."},{"metadata":{},"cell_type":"markdown","source":"<a id='Kernel_density_estimation'></a>\n### Kernel density estimation\n\nKDE is a technique used to estimate the probability distribution of a continouos random variable starting from a finite set of observations.<br>\nA KDE weights a defined density around each observation.<br>\n\n\\begin{align}\n{\n\\hat{f_h}(x)=\\frac{1}{n} \\sum_{i=1}^{n}K_h(x-x_i) = \\frac{1}{nh} \\sum_{i=1}^{n}\\frac{K(x-x_i)}{h} \n}\n\\end{align}\n\nPractically, it consists in summing, for each observation, the same function K called kernel, centered on that observation.<br>\nA further parameter $h$ called **bandwidth** is used to control the *smothing effect* of the kde.\n\n![kde.png](attachment:kde.png)\n\nOne common example of kernel is a normal kernel but also other functions can be used such as triangular, Epanechnikov or uniform.<br>\n\nHere we can see how the *bandwidth* works.","attachments":{"kde.png":{"image/png":"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"}}},{"metadata":{"scrolled":true,"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"feats = [\"age\", \"ejection_fraction\", \"serum_creatinine\", \"serum_sodium\"]\n\nfig, axs = plt.subplots(1, 4)\nfig.set_size_inches(14, 4);\n\nfor j, ax in enumerate(axs):\n    ax.set_title(feats[j], fontsize=13)\n    sns.distplot(hf_norm.loc[hf[\"DEATH_EVENT\"]==0, feats[j]], ax=ax, label=\"alive\", color = \"#990303\", kde=False, rug=True)\n    sns.distplot(hf_norm.loc[hf[\"DEATH_EVENT\"]==1, feats[j]], ax=ax, label=\"dead\", color = \"#292323\", kde=False, rug=True)\n    ax.legend(prop={'size': 13})\n    \nfig.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"%matplotlib notebook\nplt.ioff()\nfig, axs = plt.subplots(1, 4);\n%matplotlib inline \nplt.ion()\n\nbws = [0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8]\nfeats = [\"age\", \"ejection_fraction\", \"serum_creatinine\", \"serum_sodium\"]\nylims = [0.8, 1, 1.75, 0.9]\n\nfig.set_size_inches(14, 4);\ndef animate_func(i):    \n    for j, ax in enumerate(axs):        \n        ax.clear()        \n        ax.set_ylim(0, ylims[j])\n        sns.kdeplot(hf_norm.loc[hf[\"DEATH_EVENT\"]==0, feats[j]], ax=ax,\n                label=\"alive\", color = \"#990303\", shade=True, kernel=\"gau\", cut=0, bw=bws[i])\n        sns.kdeplot(hf_norm.loc[hf[\"DEATH_EVENT\"]==1, feats[j]], ax=ax,\n                label=\"dead\",  color = \"#292323\", shade=True, kernel=\"gau\", cut=0, bw=bws[i])\n        ax.set_xlabel(f\"bandwidth: {bws[i]}\")\n        ax.set_title(feats[j])\n        \n    fig.tight_layout()\n    \nanim = animation.FuncAnimation(\n                               fig, \n                               animate_func, \n                               frames = len(bws),\n                               interval = 500, # in ms\n                               );\nHTML(anim.to_jshtml())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Bandwidth rule of thumb\nThere is also a rule of thumb for the selection of the best bandwidth in the case of **gaussian kernel** and **gaussian estimated distribution**.<br>\n\n\\begin{align}\nh = \\bigg(\\frac{4 \\hat\\sigma^5}{3n}\\bigg)^{\\frac{1}{5}} \\approx 1.06\\hat\\sigma n ^ {-\\frac{1}{n}}\n\\end{align}\n\nEven if gaussian assumptions seems to be [not respected](#normality_assumptions) we can see the results for our features:"},{"metadata":{"scrolled":true,"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"rot = lambda s: 1.06*np.std(s)*len(s)**(-0.2)\n\nfeats = [\"age\", \"ejection_fraction\", \"serum_creatinine\", \"serum_sodium\", \"CPK\"]\n\nfor feat in feats:\n    \n    h = rot(hf_norm.loc[hf[\"DEATH_EVENT\"]==0, feat])\n    print(f\"{feat}: {np.round(h, 2)}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Despite the rule of thumb, different bandwidths will be evaluated."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"from scipy.stats import gaussian_kde, norm\n\nclass bernoulli:  \n    prob0, prob1 = None, None\n    \n    def __init__(self, samples):\n        if isinstance(samples, list):\n            if len(set(samples))!=2:\n                print(\"not binary\")\n                return None\n            \n            counts = np.unique(samples, return_counts=True)\n            self.prob0 = counts[1][0]/len(samples)\n            self.prob1 = 1-self.prob0            \n        else:\n            if isinstance(samples, float):     \n                self.prob1 = samples\n                self.prob0 = 1-self.prob1\n\n    def evaluate(self, x):\n        if x==0:\n            return self.prob0\n        if x==1:\n            return self.prob1\n        return None\n    \nclass MyNaiveBayes:  \n    prior = None\n    categorical = []\n    distributions = {}\n    columns = []\n    \n    def __init__(self, prior, categorical, bw=None):\n        self.prior=bernoulli(prior)\n        self.categorical = categorical\n        \n        if bw is not None:\n            self.bw = bw\n        else:\n            self.bw=\"scott\"\n        \n    def fit(self, X, y):\n        self.columns = list(X.columns)\n        \n        for i, col in enumerate(self.columns):\n            data = X[col]\n            \n            if(self.categorical[i]):\n                distr0 = bernoulli(list(data[y==0]))\n                distr1 = bernoulli(list(data[y==1]))\n            else:\n                distr0 = gaussian_kde(data[y==0], bw_method=self.bw)\n                distr1 = gaussian_kde(data[y==1], bw_method=self.bw)\n            \n            self.distributions[col] = (distr0, distr1)\n    \n    def predict_proba(self, X):\n        probs = []\n        \n        for _, sample in X.iterrows():\n            score0 = 1\n            score1 = 1\n            \n            for col in self.columns:\n                score0 *= self.distributions[col][0].evaluate(sample[col])\n                score1 *= self.distributions[col][1].evaluate(sample[col])\n\n            score0 *= self.prior.evaluate(0)\n            score1 *= self.prior.evaluate(1)\n            \n            prob1 = score1/(score0+score1)        \n            probs.append(prob1)\n            \n        return np.array(probs)\n     \n    def predict(self, X):\n        probs = self.predict_proba(X)\n        probs[probs>=0.5] = 1\n        probs[probs<0.5] = 0\n        return probs","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='naive_bayes_with_KDE_and_bernoulli'></a>\n### Naive bayes with KDE and bernoulli (Flexible bayes)\n\nAs stated before, the conditional distributions are a mix of **bernoulli and kde estimations**.<br>\nThe probabilities are then multiplied to obtain the class probability given the sample.\n\nHere it's possible to see how the metrics change according the prior (on X axis) and bandwidth (on the slider).<br>\nOn the top left plot there is an example of a normal kernel (centered in zero) with the given bandwidth."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"gauss = norm(0, 1)\nprior = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n\nno_rs_acc, no_rs_pre, no_rs_rec, no_rs_f1= [], [], [], []\nrs_acc, rs_pre, rs_rec, rs_f1= [], [], [], []\nsm_acc, sm_pre, sm_rec, sm_f1= [], [], [], []\n\nfor i in range(len(bws)):\n    # original\n    for p in prior:\n        nb = MyNaiveBayes(prior = p, categorical=(True, True, False, False, False, False, False, False), bw=bws[i])\n        nb.fit(ho_train_df[all_features], ho_train_df['DEATH_EVENT'])\n        preds = nb.predict(ho_val_df[all_features])\n\n        no_rs_acc.append(accuracy_score(preds, ho_val_df['DEATH_EVENT']))\n        no_rs_pre.append(precision_score(preds, ho_val_df['DEATH_EVENT']))\n        no_rs_rec.append(recall_score(preds, ho_val_df['DEATH_EVENT']))\n        no_rs_f1.append(f1_score(preds, ho_val_df['DEATH_EVENT']))\n\n    # oversampling\n    for p in prior:\n        nb = MyNaiveBayes(prior = p, categorical=(True, True, False, False, False, False, False, False), bw=bws[i])\n        nb.fit(ho_train_df_rs[all_features], ho_train_df_rs['DEATH_EVENT'])\n        preds = nb.predict(ho_val_df[all_features])\n\n        rs_acc.append(accuracy_score(preds, ho_val_df['DEATH_EVENT']))\n        rs_pre.append(precision_score(preds, ho_val_df['DEATH_EVENT']))\n        rs_rec.append(recall_score(preds, ho_val_df['DEATH_EVENT']))\n        rs_f1.append(f1_score(preds, ho_val_df['DEATH_EVENT']))\n\n    # SMOTE\n    for p in prior:\n        nb = MyNaiveBayes(prior = p, categorical=(True, True, False, False, False, False, False, False), bw=bws[i])\n        nb.fit(ho_train_df_sm[all_features], ho_train_df_sm['DEATH_EVENT'])\n        preds = nb.predict(ho_val_df[all_features])\n\n        sm_acc.append(accuracy_score(preds, ho_val_df['DEATH_EVENT']))\n        sm_pre.append(precision_score(preds, ho_val_df['DEATH_EVENT']))\n        sm_rec.append(recall_score(preds, ho_val_df['DEATH_EVENT']))\n        sm_f1.append(f1_score(preds, ho_val_df['DEATH_EVENT']))     ","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"#sorted([(a, b) for a,b in zip(sm_acc, sm_f1)], key=lambda a: a[0]+a[1])","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"print(\"K-Fold cross-validation metrics (5 folds)\")\n%matplotlib notebook\nplt.ioff()\nfig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2);\n%matplotlib inline \nplt.ion()\n\nfig.set_size_inches(7, 7);\ndef animate_func(i):\n\n    x = np.linspace(-3, 3, 1000)\n    y = (gauss.pdf(x/bws[i]))/bws[i]\n    \n    ax1.clear()\n    ax1.set_title(\"Gaussian kernel\")\n    ax1.plot(x, y, color='#990303', linewidth=1.5, label=f\"bandwidth={bws[i]}\")\n    ax1.fill_between(x, 0, y, alpha=0.15, color='#990303')\n    ax1.set_ylim(0, 4.1)\n    ax1.set_xlim(-3.5, 3.5)\n    ax1.legend(loc=1, fontsize=8)\n    \n    # original\n    acc, pre, rec, f1= [], [], [], []\n    for p in prior:\n        nb = MyNaiveBayes(prior = p, categorical=(True, True, False, False, False, False, False), bw=bws[i])\n        accuracy, precision, recall, f1_s = kfold.fit_predict(nb, ho_train_df, ho_train_df['DEATH_EVENT'],\n                                          threshold=0.5, resampling=None)\n        acc.append(accuracy)\n        pre.append(precision)\n        rec.append(recall)\n        f1.append(f1_s)\n        \n    ax2.clear()\n    ax2.set_ylim(0, 1)\n    ax2.set_title(\"Original\")\n    ax2.set_xlabel(\"Prior class probability P(DEATH_EVENT=1)\")\n    \n    ax2.plot(list(prior), acc, label=\"accuracy\", marker='o', linewidth=2)\n    ax2.plot(list(prior), pre, label=\"precision\", marker='o', linewidth=1, alpha=0.5)\n    ax2.plot(list(prior), rec, label=\"recall\", marker='o', linewidth=1, alpha=0.5)\n    ax2.plot(list(prior), f1, label=\"f1 score\", marker='o', linewidth=1)\n    ax2.legend(loc=4, fontsize=8)\n    ax2.grid()\n\n    # oversampling\n    acc, pre, rec, f1= [], [], [], []\n    for p in prior:\n        nb = MyNaiveBayes(prior = p, categorical=(True, True, False, False, False, False, False), bw=bws[i])\n        \n        accuracy, precision, recall, f1_s = kfold.fit_predict(nb, ho_train_df, ho_train_df['DEATH_EVENT'],\n                                          threshold=0.5, resampling=\"oversampling\")\n        acc.append(accuracy)\n        pre.append(precision)\n        rec.append(recall)\n        f1.append(f1_s)\n        \n    ax3.clear()\n    ax3.set_ylim(0, 1)\n    ax3.set_title(\"Random oversampling\")\n    ax3.set_xlabel(\"Prior class probability P(DEATH_EVENT=1)\")\n    \n    ax3.plot(list(prior), acc, label=\"accuracy\", marker='o', linewidth=2)\n    ax3.plot(list(prior), pre, label=\"precision\", marker='o', linewidth=1, alpha=0.5)\n    ax3.plot(list(prior), rec, label=\"recall\", marker='o', linewidth=1, alpha=0.5)\n    ax3.plot(list(prior), f1, label=\"f1 score\", marker='o', linewidth=1)\n    ax3.legend(loc=4, fontsize=8)\n    ax3.grid()\n\n    # smote\n    acc, pre, rec, f1= [], [], [], []\n    for p in prior:\n        nb = MyNaiveBayes(prior = p, categorical=(True, True, False, False, False, False, False), bw=bws[i])\n        \n        accuracy, precision, recall, f1_s = kfold.fit_predict(nb, ho_train_df, ho_train_df['DEATH_EVENT'],\n                                          threshold=0.5, resampling=\"SMOTE\", cached=\"True\")\n        acc.append(accuracy)\n        pre.append(precision)\n        rec.append(recall)\n        f1.append(f1_s)\n        \n    ax4.clear()\n    ax4.set_ylim(0, 1)\n    ax4.set_title(\"SMOTE\")\n    ax4.set_xlabel(\"Prior class probability P(DEATH_EVENT=1)\")\n    \n    ax4.plot(list(prior), acc, label=\"accuracy\", marker='o', linewidth=2)\n    ax4.plot(list(prior), pre, label=\"precision\", marker='o', linewidth=1, alpha=0.5)\n    ax4.plot(list(prior), rec, label=\"recall\", marker='o', linewidth=1, alpha=0.5)\n    ax4.plot(list(prior), f1, label=\"f1 score\", marker='o', linewidth=1)\n    ax4.legend(loc=4, fontsize=8)\n    ax4.grid()\n    \n    fig.tight_layout()\n    return [fig]\n    \nanim = animation.FuncAnimation(\n                               fig, \n                               animate_func, \n                               frames = len(bws),\n                               interval = 500, # in ms\n                               );\nHTML(anim.to_jshtml())","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"#sorted([(a, b) for a,b in zip(sm_acc, sm_f1)], key=lambda a: a[0]+a[1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='Gaussian_naive_bayes'></a>\n### Gaussian Naive Bayes\nAnyway, even if the hypothesis seems to be not respected, a Gaussian Naive Bayes is performed.<br>\nIn the Gaussian Naive Bayes we assume that our continouos values associated with the class are distributed according to a normal distribution.\n\n\\begin{align}\np(X_i=x_i|C = k) = \\frac{1}{\\sqrt{2\\pi\\sigma_{k}^{2}}} e^{-\\frac{(x_i-\\mu_k)^2}{2\\sigma_k^2}}\n\\end{align}\nwhere $x_i$ is the observation value and $\\mu_k$ and $\\sigma_k^2$ are respectively the sample mean and sample variance of the values in $X_i$ associated with class $k$.\n\nAs we can see, the results are still quite comparable with the other models.<br>\nIn fact one very nice positive aspect of Naive Bayes in general is the good performances even in cases in which some of the hypothesis are not respected."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# original\nno_rs_acc, no_rs_pre, no_rs_rec, no_rs_f1= [], [], [], []\nfor p in prior:\n    nb = GaussianNB(priors=[1-p, p])\n    nb.fit(ho_train_df[all_features], ho_train_df['DEATH_EVENT'])\n    preds = nb.predict(ho_val_df[all_features])\n    no_rs_acc.append(accuracy_score(preds, ho_val_df['DEATH_EVENT']))\n    no_rs_pre.append(precision_score(preds, ho_val_df['DEATH_EVENT']))\n    no_rs_rec.append(recall_score(preds, ho_val_df['DEATH_EVENT']))\n    no_rs_f1.append(f1_score(preds, ho_val_df['DEATH_EVENT']))\n    \n\n# oversampling\nrs_acc, rs_pre, rs_rec, rs_f1= [], [], [], []\nfor p in prior:\n    nb = GaussianNB(priors=[1-p, p])\n    nb.fit(ho_train_df_rs[all_features], ho_train_df_rs['DEATH_EVENT'])\n    preds = nb.predict(ho_val_df[all_features])\n    rs_acc.append(accuracy_score(preds, ho_val_df['DEATH_EVENT']))\n    rs_pre.append(precision_score(preds, ho_val_df['DEATH_EVENT']))\n    rs_rec.append(recall_score(preds, ho_val_df['DEATH_EVENT']))\n    rs_f1.append(f1_score(preds, ho_val_df['DEATH_EVENT']))\n\n# smote\nsm_acc, sm_pre, sm_rec, sm_f1= [], [], [], []\nfor p in prior:\n    nb = GaussianNB(priors=[1-p, p])\n    nb.fit(ho_train_df_sm[all_features], ho_train_df_sm['DEATH_EVENT'])\n    preds = nb.predict(ho_val_df[all_features])\n    sm_acc.append(accuracy_score(preds, ho_val_df['DEATH_EVENT']))\n    sm_pre.append(precision_score(preds, ho_val_df['DEATH_EVENT']))\n    sm_rec.append(recall_score(preds, ho_val_df['DEATH_EVENT']))\n    sm_f1.append(f1_score(preds, ho_val_df['DEATH_EVENT']))","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"#sorted([(a, b) for a,b in zip(sm_acc, sm_f1)], key=lambda a: a[0]+a[1])","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# k fold\nprint(\"K-Fold cross-validation metrics (5 folds)\")\n\nplt.figure(figsize=(12, 4))\n\n# original\nacc, pre, rec, f1= [], [], [], []\n    \nfor p in prior:\n    nb = GaussianNB(priors=[1-p, p])\n    accuracy, precision, recall, f1_s = kfold.fit_predict(nb, ho_train_df, ho_train_df['DEATH_EVENT'],\n                                          threshold=0.5, resampling=None)\n    acc.append(accuracy)\n    pre.append(precision)\n    rec.append(recall)\n    f1.append(f1_s)\n    \nplt.subplot(1, 3, 1)\nplt.ylim(0, 1)\nplt.title(\"Original\")\nplt.xlabel(\"Prior class probability P(DEATH_EVENT=1)\")\n    \nplt.plot(list(prior), acc, label=\"accuracy\", marker='o', linewidth=2)\nplt.plot(list(prior), pre, label=\"precision\", marker='o', linewidth=1, alpha=0.5)\nplt.plot(list(prior), rec, label=\"recall\", marker='o', linewidth=1, alpha=0.5)\nplt.plot(list(prior), f1, label=\"f1 score\", marker='o', linewidth=1)\nplt.legend()\nplt.grid()\n\n# oversampling\nacc, pre, rec, f1= [], [], [], []\n    \nfor p in prior:\n    nb = GaussianNB(priors=[1-p, p])\n    accuracy, precision, recall, f1_s = kfold.fit_predict(nb, ho_train_df, ho_train_df['DEATH_EVENT'],\n                                          threshold=0.5, resampling=\"oversampling\")\n    acc.append(accuracy)\n    pre.append(precision)\n    rec.append(recall)\n    f1.append(f1_s)\n    \nplt.subplot(1, 3, 2)\nplt.ylim(0, 1)\nplt.title(\"Random oversampling\")\nplt.xlabel(\"Prior class probability P(DEATH_EVENT=1)\")\n    \nplt.plot(list(prior), acc, label=\"accuracy\", marker='o', linewidth=2)\nplt.plot(list(prior), pre, label=\"precision\", marker='o', linewidth=1, alpha=0.5)\nplt.plot(list(prior), rec, label=\"recall\", marker='o', linewidth=1, alpha=0.5)\nplt.plot(list(prior), f1, label=\"f1 score\", marker='o', linewidth=1)\nplt.legend()\nplt.grid()\n\n# smote\nacc, pre, rec, f1= [], [], [], []\n    \nfor p in prior:\n    nb = GaussianNB(priors=[1-p, p])\n    accuracy, precision, recall, f1_s = kfold.fit_predict(nb, ho_train_df, ho_train_df['DEATH_EVENT'],\n                                          threshold=0.5, resampling=\"SMOTE\", cached=True)\n    acc.append(accuracy)\n    pre.append(precision)\n    rec.append(recall)\n    f1.append(f1_s)\n\nplt.subplot(1, 3, 3)\nplt.ylim(0, 1)\nplt.title(\"SMOTE\")\nplt.xlabel(\"Prior class probability P(DEATH_EVENT=1)\")\n    \nplt.plot(list(prior), acc, label=\"accuracy\", marker='o', linewidth=2)\nplt.plot(list(prior), pre, label=\"precision\", marker='o', linewidth=1, alpha=0.5)\nplt.plot(list(prior), rec, label=\"recall\", marker='o', linewidth=1, alpha=0.5)\nplt.plot(list(prior), f1, label=\"f1 score\", marker='o', linewidth=1)\nplt.legend()\nplt.grid()\n\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"no_rs_acc, no_rs_pre, no_rs_rec, no_rs_f1= [], [], [], []\nrs_acc, rs_pre, rs_rec, rs_f1= [], [], [], []\nsm_acc, sm_pre, sm_rec, sm_f1= [], [], [], []\n\n\n# original\nfor p in prior:\n    nb = GaussianNB(priors=[1-p, p])\n    accuracy, precision, recall, f1_s = kfold.fit_predict(nb, ho_train_df, ho_train_df['DEATH_EVENT'],\n                                              threshold=0.5, resampling=None)\n    no_rs_acc.append(accuracy)\n    no_rs_pre.append(precision)\n    no_rs_rec.append(recall)\n    no_rs_f1.append(f1_s)\n\n    # oversampling\nfor p in prior:\n        nb = GaussianNB(priors=[1-p, p])\n\n        accuracy, precision, recall, f1_s = kfold.fit_predict(nb, ho_train_df, ho_train_df['DEATH_EVENT'],\n                                              threshold=0.5, resampling=\"oversampling\")\n        rs_acc.append(accuracy)\n        rs_pre.append(precision)\n        rs_rec.append(recall)\n        rs_f1.append(f1_s)\n\n        # smote\nfor p in prior:\n        nb = GaussianNB(priors=[1-p, p])\n\n        accuracy, precision, recall, f1_s = kfold.fit_predict(nb, ho_train_df, ho_train_df['DEATH_EVENT'],\n                                              threshold=0.5, resampling=\"SMOTE\", cached=\"True\")\n        sm_acc.append(accuracy)\n        sm_pre.append(precision)\n        sm_rec.append(recall)\n        sm_f1.append(f1_s)\n","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"#sorted([(a, b) for a,b in zip(sm_acc, sm_f1)], key=lambda a: a[0]+a[1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='Results_and_conclusions'></a>\n# Results and conclusions <a style=\"text-decoration:none\" href=\"#index\">⤴</a>\n\nHere we can see the results obtained with different models and different rebalancing techniques for the **Hearth Disease** dataset.<br>\nBoth **accuracy** and **f1 score** (inside parenthesis) are showed."},{"metadata":{},"cell_type":"markdown","source":"| Model  | Holdout Original | Holdout Oversampling | Holdout SMOTE | Holdout class-weight=balanced | KFold Original | KFold Oversampling | KFold SMOTE | KFold class-weight=balanced| \n| :------------- | :-------------: | :-------------: | :-------------: | :-------------: | :-------------: | :-------------: | :-------------: | :-------------: |\n| **Decision Tree** | 0.706 (0.607) | 0.733 (0.655) | 0.706 (0.645) | 0.747 (0.698) | 0.790 (0.560) | 0.794 (0.687) | 0.754 (0.598) | 0.785 (0.664) | \n| **Random Forest** | 0.707 (0.577) | **0.787 (0.733)** | 0.747 (0.667) | 0.733 (0.642) | 0.803 (0.625) | **0.808 (0.683)** | 0.794 (0.654) | 0.799 (0.617) | \n| **Linear Regression** | 0.667 (0.444) | 0.693 (0.635) | 0.707 (0.633) | - | 0.776 (0.508) | 0.727 (0.611) | 0.750 (0.614) | - | \n| **Logistic Regression** | 0.667 (0.444) | 0.707 (0.645) | 0.733 (0.667) | 0.707 (0.645) | 0.772 (0.501) | 0.736 (0.607) | 0.759 (0.630) | 0.740 (0.619) | \n| **Linear SVM** | 0.653 (0.458) | 0.720 (0.667) | 0.720 (0.644) | 0.706 (0.656) | 0.736 (0.594) | 0.759 (0.606) | 0.781 (0.519) | 0.718 (0.574) | \n| **Poly SVM** | 0.680 (0.538) | 0.693 (0.582) | 0.640 (0.542) | 0.706 (0.607) | 0.759 (0.503) | 0.759 (0.562) | 0.763 (0.589) | 0.754 (0.536) | \n| **RBF SVM** | 0.680 (0.571) | 0.680 (0.657) | 0.720 (0.657) | 0.747 (0.698) | 0.790 (0.542) | 0.781 (0.669) | 0.794 (0.680) | **0.799 (0.693)** | \n| **KNN original** | 0.640 (0.501) | 0.720 (0.644) | 0.680 (0.586) | - | 0.772 (0.471) | 0.737 (0.603) | 0.763 (0.604) | - | \n| **KNN distance** | 0.667 (0.510) | 0.733 (0.667) | 0.793 (0.610) | - | 0.776 (0.485) | 0.737 (0.599) | 0.759 (0.601) | - | \n| **Flexible Bayes** | 0.733 (0.730) | 0.733 (0.714) | 0.747 (0.716) | - | 0.799 (0.631) | 0.772 (0.616) | 0.785 (0.611) | - | \n| **Gaussian Naive Bayes** | 0.693 (0.667) | 0.707 (0.686) | 0.733 (0.688) | - | 0.781 (0.653) | 0.763 (0.619) | 0.727 (0.606) | - | "},{"metadata":{},"cell_type":"markdown","source":"We can clearly see how using some rebalancing techniques the **f1 score** increase substancially.<br>\nIn some cases SMOTE performs better with respect to random oversampling, and the opposite in others.<br>\nFurthermore, where is possible to apply it, also the use of the `class-weight` parameter increases the performances, sometimes outperforming the other techniques.\n\nThen we noticed how using Gaussian Naive Bayes, even without respecting the hypothesis, leads to good results and also with a Bayes Classifier with KDE, the results are in line.\n\nBest overall model seems to be the **random forest** trained on the oversampled dataset, that delivers the best results in terms of accuracy and f1 score.<br>\nAlso **RBF-SVM** with `class-weights=balanced` provides some good results on KFold.\n\nFor the models that allow it, it's possible to evaluate the **ROC curve** to select a threshold according to the main goal (minimize false positives or maximize true positives) but the results in the table are obtained by fixing the threshold at 0.5.\n\nThe overall results seem in line with the ones obtained in the reference paper [[1]](#references) but it's needed to keep in mind that the metrics are highly influenced by the small dimension of the dataset (75 samples in holdout validation set)."},{"metadata":{},"cell_type":"markdown","source":"<a id='references'></a>\n# References  <a style=\"text-decoration:none\" href=\"#index\">⤴</a>\n\n[[1]](https://bmcmedinformdecismak.biomedcentral.com/articles/10.1186/s12911-020-1023-5)  D. Chicco, G. Jurman. \"Machine learning can predict survival of patients with heart failure from serum creatinine and ejection fraction alone\", 2020\n\n\n[[2]](https://arxiv.org/abs/1106.1813) N. V. Chawla, K. W. Bowyer, L. O. Hall, W. P. Kegelmeyer. SMOTE: Synthetic Minority Over-sampling Technique, 2002\n\n\n[[3]](https://dl.acm.org/doi/10.5555/2074158.2074196) G. H. John, P. Langley. Estimating Continuous Distributions in Bayesian  Classifiers, 1995"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}