{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Paper - A tutorial on traffic forecasting with deep learning: [Tutorial Deep Learning](https://www.researchgate.net/profile/Giovanni-Buroni-2/publication/348930068_A_Tutorial_on_Network-Wide_Multi-Horizon_Traffic_Forecasting_with_Deep_Learning/links/6017c45a92851c2d4d0aa267/A-Tutorial-on-Network-Wide-Multi-Horizon-Traffic-Forecasting-with-Deep-Learning.pdf)\n","metadata":{}},{"cell_type":"markdown","source":"## General Import","metadata":{"_kg_hide-input":false}},{"cell_type":"code","source":"import numpy as np \nimport os \nimport pandas as pd \nfrom math import sqrt\nfrom sklearn.preprocessing import MinMaxScaler\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import regularizers\nimport tensorflow_probability as tfp\nimport gc\nimport time","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-07-05T15:03:11.377009Z","iopub.execute_input":"2021-07-05T15:03:11.377318Z","iopub.status.idle":"2021-07-05T15:03:17.248059Z","shell.execute_reply.started":"2021-07-05T15:03:11.37729Z","shell.execute_reply":"2021-07-05T15:03:17.247126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import geojson\nimport geopandas as gpd\nfrom fiona.crs import from_epsg\nimport os, json\nfrom shapely.geometry import shape, Point, Polygon, MultiPoint\nfrom geopandas.tools import sjoin\nimport matplotlib.cm as cm\nimport matplotlib.pyplot as plt # plotting\nimport seaborn as sns; sns.set()\nfrom IPython.display import Image\nfrom branca.colormap import  linear\nimport json\nimport branca.colormap as cm\nimport folium","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-07-05T15:03:17.249813Z","iopub.execute_input":"2021-07-05T15:03:17.250147Z","iopub.status.idle":"2021-07-05T15:03:18.295077Z","shell.execute_reply.started":"2021-07-05T15:03:17.250119Z","shell.execute_reply":"2021-07-05T15:03:18.294214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Seed","metadata":{}},{"cell_type":"code","source":"from numpy.random import seed\n\n# Reproducability\ndef set_seed(seed=31415):\n    \n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n    \nset_seed(31415)","metadata":{"execution":{"iopub.status.busy":"2021-07-05T15:03:18.296618Z","iopub.execute_input":"2021-07-05T15:03:18.296979Z","iopub.status.idle":"2021-07-05T15:03:18.304678Z","shell.execute_reply.started":"2021-07-05T15:03:18.296943Z","shell.execute_reply":"2021-07-05T15:03:18.304034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Check Files","metadata":{}},{"cell_type":"code","source":"for dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_kg_hide-input":false,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-07-05T15:03:18.306615Z","iopub.execute_input":"2021-07-05T15:03:18.30718Z","iopub.status.idle":"2021-07-05T15:03:18.348036Z","shell.execute_reply.started":"2021-07-05T15:03:18.307142Z","shell.execute_reply":"2021-07-05T15:03:18.347312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Import OBU File","metadata":{}},{"cell_type":"code","source":"# BXL_timeseries_kaggle.csv may have more rows in reality, but we are only loading/previewing the first 1000 rows\nnew_table = pd.read_csv('../input/obu-data-preprocessing/Flow_BEL_street_30min.csv')\nnRow, nCol = new_table.shape\nprint(f'There are {nRow} rows and {nCol} columns')","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2021-07-05T15:03:18.35302Z","iopub.execute_input":"2021-07-05T15:03:18.353266Z","iopub.status.idle":"2021-07-05T15:03:23.35763Z","shell.execute_reply.started":"2021-07-05T15:03:18.353241Z","shell.execute_reply":"2021-07-05T15:03:23.356721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visualize Street Network","metadata":{}},{"cell_type":"code","source":"df_belgium = gpd.read_file('/kaggle/input/belgium-obu/Belgium_streets.json')\n\nm = folium.Map([50.85045, 4.34878], zoom_start=9, tiles='cartodbpositron')\nfolium.GeoJson(df_belgium).add_to(m)\n\nm","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-07-05T15:03:23.360426Z","iopub.execute_input":"2021-07-05T15:03:23.360939Z","iopub.status.idle":"2021-07-05T15:03:37.319067Z","shell.execute_reply.started":"2021-07-05T15:03:23.360898Z","shell.execute_reply":"2021-07-05T15:03:37.317956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Select Streets based on Average Traffic Flow","metadata":{}},{"cell_type":"code","source":"mean_value = 10\n\ntable_index = new_table.iloc[:,1:]\nALL_STREETS = list(table_index.columns.values)\n\nmean_flow =[]\nnew_street=[]\n\n\nfor street in ALL_STREETS:\n    \n    single_street=table_index[street]\n    mean = np.mean(single_street)\n    mean_flow.append(mean)\n    new_street.append(street)\n    \ndf_mean_flow = pd.DataFrame({'street_index':new_street, 'mean_flow': mean_flow})\nprint('')\nprint(df_mean_flow.head())\nprint('')\n\nSTREETS = df_mean_flow[(df_mean_flow['mean_flow']>= mean_value)] \nSTREETS = STREETS.sort_values(by=['street_index'])\nSTREETS = list(STREETS.street_index)\n\nprint('considering a average traffic flow of ' + str(mean_value)+' per street')\nprint('')\nprint('mean traffic flow '+str(mean_value)+ ' ---> number of street segments: ' + str(len(STREETS)))\n","metadata":{"execution":{"iopub.status.busy":"2021-07-05T15:03:37.320448Z","iopub.execute_input":"2021-07-05T15:03:37.320858Z","iopub.status.idle":"2021-07-05T15:03:38.065683Z","shell.execute_reply.started":"2021-07-05T15:03:37.320814Z","shell.execute_reply":"2021-07-05T15:03:38.06462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Add time-based Covariates","metadata":{}},{"cell_type":"code","source":"new_table['Datetime'] = pd.to_datetime(new_table['datetime'])\n\nDATAFRAME = new_table\nDATAFRAME = DATAFRAME.drop(['datetime'],axis=1) \nDATAFRAME = DATAFRAME[DATAFRAME.columns.intersection(STREETS)]\n\n# Time-based Covariates\n\nDATAFRAME['minutes'] = new_table['Datetime'].dt.minute\nDATAFRAME['hour'] = new_table['Datetime'].dt.hour\n\nDATAFRAME['hour_x']=np.sin(DATAFRAME.hour*(2.*np.pi/23))\nDATAFRAME['hour_y']=np.cos(DATAFRAME.hour*(2.*np.pi/23))\n\nDATAFRAME['day'] = new_table['Datetime'].dt.day\nDATAFRAME['DayOfWeek'] = new_table['Datetime'].dt.dayofweek\n\nDATAFRAME['WorkingDays'] = DATAFRAME['DayOfWeek'].apply(lambda y: 2 if y < 5 else y)\nDATAFRAME['WorkingDays'] = DATAFRAME['WorkingDays'].apply(lambda y: 1 if y == 5 else y)\nDATAFRAME['WorkingDays'] = DATAFRAME['WorkingDays'].apply(lambda y: 0 if y == 6 else y)\n\nDATAFRAME = DATAFRAME.drop(['minutes','hour','day'],axis=1)\n\n# temporal features = 4\nfeat_time = 4\n\nDATAFRAME.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-05T15:03:38.067217Z","iopub.execute_input":"2021-07-05T15:03:38.067551Z","iopub.status.idle":"2021-07-05T15:03:38.258186Z","shell.execute_reply.started":"2021-07-05T15:03:38.067519Z","shell.execute_reply":"2021-07-05T15:03:38.257098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visualize Traffic Flow at Particular Time","metadata":{}},{"cell_type":"code","source":"STREETS = [int(float(s)) for s in STREETS]\n\ndf_belgium = df_belgium[df_belgium.index.isin(STREETS)]\ndf_belgium['Trucks_Flow'] =  DATAFRAME.iloc[2182,:-4].astype(float).values\n\nnbh_count_colormap = linear.YlOrRd_09.scale(0,200)\n\ncolormap_dept = cm.StepColormap(\n    colors=['#00ae53', '#86dc76', '#daf8aa',\n            '#ffe6a4', '#ff9a61', '#ee0028'],\n    vmin = 0,\n    vmax = 200,\n    index=[0, 20, 50, 80, 110, 150, 180])\n\npolygons = df_belgium\nm = folium.Map([50.85045, 4.34878], zoom_start= 9, tiles='cartodbpositron')\n\nstyle_function = lambda x: {\n    'fillColor': colormap_dept(x['properties']['Trucks_Flow']),\n    'color': colormap_dept(x['properties']['Trucks_Flow']),\n    'weight': 1.5,\n    'fillOpacity': 1\n}\nfolium.GeoJson(polygons,\n    style_function=style_function).add_to(m)\n\n\ncolormap_dept.caption = 'Traffic Flow (N#Trucks/30min) at (not real) 12:00 a.m.'\ncolormap_dept.add_to(m)\n\nm","metadata":{"execution":{"iopub.status.busy":"2021-07-05T15:03:38.259733Z","iopub.execute_input":"2021-07-05T15:03:38.260115Z","iopub.status.idle":"2021-07-05T15:03:43.829598Z","shell.execute_reply.started":"2021-07-05T15:03:38.260078Z","shell.execute_reply":"2021-07-05T15:03:43.828314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# SPLITTING Training/Testing","metadata":{}},{"cell_type":"code","source":"Image(\"/kaggle/input/image-lstm/MATRIX.jpg\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-07-05T15:03:43.831059Z","iopub.execute_input":"2021-07-05T15:03:43.831691Z","iopub.status.idle":"2021-07-05T15:03:43.846826Z","shell.execute_reply.started":"2021-07-05T15:03:43.831644Z","shell.execute_reply":"2021-07-05T15:03:43.845926Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_step = 168*2 + 168*2 # 1 WEEK\ntest_step = 168*2 # 1 WEEK\n\n# ATTENTION: anything you learn and is not known in advance, must be learnt only from training data!\nscaler = MinMaxScaler(feature_range=(0, 1))\nscaler_aux = MinMaxScaler(feature_range=(0, 1))\n\n# TRAINING --- (scaler/scaler_aux).fit_transform()\n# TESTING --- (scaler/scaler_aux).transform()\n\n# TRAINING SET\nTRAIN = DATAFRAME[: -val_step]\ntrain_feat = scaler.fit_transform(TRAIN.values[:,:-feat_time])\n\n# VALIDATION SET\nVAL = DATAFRAME[-val_step : -test_step]\nvalid_feat = scaler.transform(VAL.values[:,:-feat_time])\n\n# TESTING SET\nTEST = DATAFRAME[-test_step:]\ntest_feat = scaler.transform(TEST.values[:,:-feat_time])\n\n\n# AUX are known in advance\nAUX = scaler_aux.fit_transform(DATAFRAME.values[:,-feat_time:])\ntrain_aux = AUX[: -val_step]\nvalid_aux = AUX[-val_step: -test_step]\ntest_aux = AUX[-test_step:]\n\n\n# concate final results\ntrain_feat = np.hstack([train_feat, train_aux])\nvalid_feat = np.hstack([valid_feat, valid_aux])\ntest_feat = np.hstack([test_feat, test_aux])\n","metadata":{"execution":{"iopub.status.busy":"2021-07-05T15:03:43.849043Z","iopub.execute_input":"2021-07-05T15:03:43.849674Z","iopub.status.idle":"2021-07-05T15:03:43.989605Z","shell.execute_reply.started":"2021-07-05T15:03:43.849631Z","shell.execute_reply":"2021-07-05T15:03:43.988681Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def inverse_transform(forecasts, scaler):\n    # invert scaling\n    inv_pred = scaler.inverse_transform(forecasts)\n    return inv_pred","metadata":{"execution":{"iopub.status.busy":"2021-07-05T15:03:43.991115Z","iopub.execute_input":"2021-07-05T15:03:43.991654Z","iopub.status.idle":"2021-07-05T15:03:43.996084Z","shell.execute_reply.started":"2021-07-05T15:03:43.991619Z","shell.execute_reply":"2021-07-05T15:03:43.995069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Visualize testing set","metadata":{}},{"cell_type":"code","source":"nRow, nCol = DATAFRAME.shape\n\nplt.figure(figsize=(10,5))\n\nlength = list(range(DATAFRAME.shape[0]))\n\nplt.plot(length[:TRAIN.shape[0]], np.sum(TRAIN.iloc[:,:-feat_time], axis=1))\nplt.plot(length[TRAIN.shape[0]:TRAIN.shape[0]+VAL.shape[0]],np.sum(VAL.iloc[:,:-feat_time], axis=1))\nplt.plot(length[TRAIN.shape[0]+VAL.shape[0]:TRAIN.shape[0]+VAL.shape[0]+TEST.shape[0]],np.sum(TEST.iloc[:,:-feat_time], axis=1))\nplt.legend(['training','validation','testing'], loc='upper left')\nplt.show()\n\nprint(f'Consider {nRow} instances (rows) and {nCol} streets segments (columns)')\nprint('')\nprint('TRAIN SIZE: '+ str(TRAIN.shape))\nprint('')\nprint('VAL SIZE: '+ str(VAL.shape))\nprint('')\nprint('TEST SIZE: '+ str(TEST.shape))","metadata":{"execution":{"iopub.status.busy":"2021-07-05T15:03:43.997336Z","iopub.execute_input":"2021-07-05T15:03:43.997854Z","iopub.status.idle":"2021-07-05T15:03:44.413488Z","shell.execute_reply.started":"2021-07-05T15:03:43.997811Z","shell.execute_reply":"2021-07-05T15:03:44.412612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# LSTM encoder decoder model - Multivariate Multiple-step ahead Prediction Model","metadata":{}},{"cell_type":"markdown","source":"# Direct Approach","metadata":{}},{"cell_type":"markdown","source":"# Data Preparation\n\n## *{Batch_Sz, Input_Sq, Feature_Sz}*","metadata":{}},{"cell_type":"code","source":"Image(\"/kaggle/input/image-lstm/DATAPREP.png\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-07-05T15:03:44.415143Z","iopub.execute_input":"2021-07-05T15:03:44.415481Z","iopub.status.idle":"2021-07-05T15:03:44.432489Z","shell.execute_reply.started":"2021-07-05T15:03:44.415447Z","shell.execute_reply":"2021-07-05T15:03:44.431738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prep_data(dataframe, INPUT, OUTPUT, AUX, BATCH):\n    \n    TOTAL = INPUT + OUTPUT\n    \n    dataset_feat = tf.data.Dataset.from_tensor_slices(dataframe)    \n    aux = tf.data.Dataset.from_tensor_slices(dataframe[:,-AUX:])    \n    dataset_labels = tf.data.Dataset.from_tensor_slices(dataframe)\n\n    # features - past observations\n    feat = dataset_feat.window(INPUT,  shift=1,  stride=1,  drop_remainder=True) \n    feat = feat.flat_map(lambda window: window.batch(INPUT))\n    \n    # aux - temporal features\n    aux = aux.window(OUTPUT,  shift=1,  stride=1,  drop_remainder=True ).skip(INPUT)\n    aux = aux.flat_map(lambda window: window.batch(OUTPUT))\n    \n    # labels - future observations\n    label = dataset_labels.window(OUTPUT, shift=1,  stride=1,  drop_remainder=True).skip(INPUT)\n    label = label.flat_map(lambda window: window.batch(OUTPUT))\n    \n    dataset = tf.data.Dataset.zip(((feat, aux), label))\n    \n    dataset = dataset.batch(BATCH).prefetch(tf.data.experimental.AUTOTUNE)\n\n    return dataset\n","metadata":{"execution":{"iopub.status.busy":"2021-07-05T15:03:44.433827Z","iopub.execute_input":"2021-07-05T15:03:44.434655Z","iopub.status.idle":"2021-07-05T15:03:44.445969Z","shell.execute_reply.started":"2021-07-05T15:03:44.434616Z","shell.execute_reply":"2021-07-05T15:03:44.444881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Parameters","metadata":{}},{"cell_type":"code","source":"n_total_features = len(DATAFRAME.columns) \n\nsize_input = 12\nsize_forecast = 12\nsize_total = size_input + size_forecast\nsize_aux = feat_time\n\nbatch_size = 32\nbatch_train = batch_size\nbatch_valid = batch_size\nbatch_test = 1\n\nwindowed_train = prep_data(train_feat, size_input, size_forecast, size_aux, batch_train)\nwindowed_valid = prep_data(valid_feat, size_input, size_forecast, size_aux, batch_valid)\nwindowed_test = prep_data(test_feat, size_input, size_forecast, size_aux, batch_test)\n\nlatent_dim = 25\nEPOCHS = 250 #250 # in the paper 250","metadata":{"execution":{"iopub.status.busy":"2021-07-05T15:03:44.447366Z","iopub.execute_input":"2021-07-05T15:03:44.447719Z","iopub.status.idle":"2021-07-05T15:03:46.46278Z","shell.execute_reply.started":"2021-07-05T15:03:44.447682Z","shell.execute_reply":"2021-07-05T15:03:46.46202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# timestamp for predictions\ntimestamp = new_table['Datetime']\ntimestamp_pred = timestamp[-test_step+size_input-1:]","metadata":{"execution":{"iopub.status.busy":"2021-07-05T15:03:46.464755Z","iopub.execute_input":"2021-07-05T15:03:46.46555Z","iopub.status.idle":"2021-07-05T15:03:46.473997Z","shell.execute_reply.started":"2021-07-05T15:03:46.465505Z","shell.execute_reply":"2021-07-05T15:03:46.473108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# LSTM Encoder Decoder Architecture","metadata":{}},{"cell_type":"code","source":"Image(\"/kaggle/input/image-lstm/ECDEC.jpg\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-07-05T15:03:46.475728Z","iopub.execute_input":"2021-07-05T15:03:46.476108Z","iopub.status.idle":"2021-07-05T15:03:46.492679Z","shell.execute_reply.started":"2021-07-05T15:03:46.476074Z","shell.execute_reply":"2021-07-05T15:03:46.491867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Input Encoder","metadata":{}},{"cell_type":"code","source":"# the input for encoder\npast_inputs = tf.keras.Input(shape=(size_input, n_total_features), name = 'enc_inputs')\npast_inputs","metadata":{"execution":{"iopub.status.busy":"2021-07-05T15:03:46.49435Z","iopub.execute_input":"2021-07-05T15:03:46.494665Z","iopub.status.idle":"2021-07-05T15:03:46.507512Z","shell.execute_reply.started":"2021-07-05T15:03:46.494635Z","shell.execute_reply":"2021-07-05T15:03:46.5067Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Encoder","metadata":{}},{"cell_type":"code","source":"class Encoder(tf.keras.Model):\n    \n    def __init__(self, enc_units, sz_input, sz_tot,  batch_sz):\n        \n        super(Encoder, self).__init__()\n        \n        self.batch_sz = batch_sz\n        self.enc_units = enc_units\n        \n        self.lstm = tf.keras.layers.LSTM(self.enc_units,\n                                         return_sequences=False, #turn to False for one layer\n                                         return_state=True,\n                                          recurrent_initializer='glorot_uniform',\n                                          kernel_regularizer=regularizers.l2(0.001),\n                                          name='Encoder')\n\n                \n    def __call__(self, x):\n\n        output, state_h, state_c = self.lstm(x) # just one layer on the paper \n        \n        return output, state_h, state_c\n","metadata":{"execution":{"iopub.status.busy":"2021-07-05T15:03:46.509194Z","iopub.execute_input":"2021-07-05T15:03:46.509664Z","iopub.status.idle":"2021-07-05T15:03:46.517772Z","shell.execute_reply.started":"2021-07-05T15:03:46.509624Z","shell.execute_reply":"2021-07-05T15:03:46.516869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encoder = Encoder(latent_dim, size_input, n_total_features,  batch_size)\nout_enc, h, c = encoder(past_inputs)\nout_enc","metadata":{"execution":{"iopub.status.busy":"2021-07-05T15:03:46.519282Z","iopub.execute_input":"2021-07-05T15:03:46.519913Z","iopub.status.idle":"2021-07-05T15:03:47.032526Z","shell.execute_reply.started":"2021-07-05T15:03:46.519866Z","shell.execute_reply":"2021-07-05T15:03:47.031824Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Input Decoder","metadata":{}},{"cell_type":"code","source":"# the future input for decoder\nfuture_inputs = tf.keras.Input(shape=(size_forecast, size_aux), name='aux_inputs')\nfuture_inputs","metadata":{"execution":{"iopub.status.busy":"2021-07-05T15:03:47.033776Z","iopub.execute_input":"2021-07-05T15:03:47.034138Z","iopub.status.idle":"2021-07-05T15:03:47.043245Z","shell.execute_reply.started":"2021-07-05T15:03:47.034109Z","shell.execute_reply":"2021-07-05T15:03:47.042208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Decoder","metadata":{}},{"cell_type":"code","source":"class Decoder(tf.keras.Model):\n    \n    def __init__(self, dec_units, sz_forecast, sz_aux, tot_feat,  batch_sz):\n        \n        super(Decoder, self).__init__()\n        \n        self.batch_sz = batch_sz\n        self.dec_units = dec_units\n        self.feat = tot_feat\n        self.out = sz_forecast\n        self.sz_aux = sz_aux\n\n        \n        self.lstm = tf.keras.layers.LSTM(self.dec_units, return_sequences = True,\n                                        recurrent_initializer='glorot_uniform',\n                                         kernel_regularizer=regularizers.l2(0.001),\n                                         name ='Decoder') # \n        \n        self.dense1 = tf.keras.layers.Dense(self.dec_units, kernel_regularizer=regularizers.l2(0.001))\n        \n        self.drop = tf.keras.layers.Dropout(0.1)\n        \n        self.fc = tf.keras.layers.Dense(self.feat, kernel_regularizer=regularizers.l2(0.001)) #\n        \n\n    def __call__(self, x, h, c,  training=False):\n    \n        output_dec = self.lstm(x, initial_state= [h, c]) # just one layer on the paper \n        \n        if training:\n            \n            output_dec = self.drop(output_dec, training =training)\n\n        out = self.fc(output_dec)\n\n        return out\n        \n\n        ","metadata":{"execution":{"iopub.status.busy":"2021-07-05T15:03:47.045292Z","iopub.execute_input":"2021-07-05T15:03:47.045729Z","iopub.status.idle":"2021-07-05T15:03:47.060535Z","shell.execute_reply.started":"2021-07-05T15:03:47.045692Z","shell.execute_reply":"2021-07-05T15:03:47.059747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Decoder Output","metadata":{}},{"cell_type":"code","source":"decoder = Decoder(latent_dim, 12, size_aux, n_total_features,  batch_size)\nout_dec = decoder(future_inputs, out_enc, c)\nout_dec ","metadata":{"execution":{"iopub.status.busy":"2021-07-05T15:03:47.062161Z","iopub.execute_input":"2021-07-05T15:03:47.062417Z","iopub.status.idle":"2021-07-05T15:03:47.278954Z","shell.execute_reply.started":"2021-07-05T15:03:47.062393Z","shell.execute_reply":"2021-07-05T15:03:47.278245Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train Model","metadata":{}},{"cell_type":"markdown","source":"### Optimizer","metadata":{}},{"cell_type":"code","source":"opt = tf.keras.optimizers.Adam()","metadata":{"execution":{"iopub.status.busy":"2021-07-05T15:03:47.280177Z","iopub.execute_input":"2021-07-05T15:03:47.280507Z","iopub.status.idle":"2021-07-05T15:03:47.286104Z","shell.execute_reply.started":"2021-07-05T15:03:47.280469Z","shell.execute_reply":"2021-07-05T15:03:47.283802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Mean Absolute Loss function ","metadata":{}},{"cell_type":"code","source":"loss_fct = tf.keras.losses.MeanAbsoluteError(name='loss_function')\nvalid_loss_fct = tf.keras.losses.MeanAbsoluteError(name='valid_loss_function')","metadata":{"execution":{"iopub.status.busy":"2021-07-05T15:03:47.287406Z","iopub.execute_input":"2021-07-05T15:03:47.288021Z","iopub.status.idle":"2021-07-05T15:03:47.294661Z","shell.execute_reply.started":"2021-07-05T15:03:47.287982Z","shell.execute_reply":"2021-07-05T15:03:47.293739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Batch Loss","metadata":{}},{"cell_type":"code","source":"@tf.function\ndef batch_loss(inp, aux, targ, loss_funct, opt=None):\n    loss = 0\n    with tf.GradientTape() as tape:\n        context_vector, state_h, state_c = encoder(inp)\n        predictions = decoder(aux, state_h, state_c, training=True)\n        loss = loss_funct(targ, predictions)\n    if opt is not None:\n        variables = encoder.trainable_variables + decoder.trainable_variables\n        gradients = tape.gradient(loss, variables)\n        opt.apply_gradients(zip(gradients, variables))\n    return loss","metadata":{"execution":{"iopub.status.busy":"2021-07-05T15:03:47.296454Z","iopub.execute_input":"2021-07-05T15:03:47.296829Z","iopub.status.idle":"2021-07-05T15:03:47.306429Z","shell.execute_reply.started":"2021-07-05T15:03:47.296776Z","shell.execute_reply":"2021-07-05T15:03:47.305359Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### early stopping","metadata":{}},{"cell_type":"code","source":"def monitor_loss(val, epoch, delta, cnt):\n    if ((epoch > 0) and (val[epoch-1] - val[epoch])) > delta:\n        cnt = 0    \n    else:\n        cnt += 1 \n        \n    return cnt","metadata":{"execution":{"iopub.status.busy":"2021-07-05T15:03:47.308255Z","iopub.execute_input":"2021-07-05T15:03:47.308609Z","iopub.status.idle":"2021-07-05T15:03:47.319487Z","shell.execute_reply.started":"2021-07-05T15:03:47.308575Z","shell.execute_reply":"2021-07-05T15:03:47.318627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Training and Validation","metadata":{}},{"cell_type":"code","source":"print('')\nprint('Training & Validation')\nprint('')\n\n# early stopping\npatience = 20   \nmin_delta = 0.0001   \npatience_cnt = 0 \n\n# Keep results for plotting\ntrain_loss_results = []\nvalid_loss_results = []\nsteps_per_epoch = len(TRAIN) // batch_size\n\nstart = time.time()\n\nfor epoch in range(EPOCHS):\n    \n    ## training\n    step = 0\n    epoch_loss_avg = tf.keras.metrics.Mean()\n    \n    for (batch, (inp_tot, targ)) in enumerate(windowed_train.take(steps_per_epoch)):\n        \n        inp = inp_tot[0]\n        aux = inp_tot[1]\n        \n        batch_loss_results = batch_loss(inp, aux, targ, loss_fct, opt)\n        \n        # training progress\n        epoch_loss_avg.update_state(batch_loss_results)\n\n    # collect training loss values\n    train_loss_results.append(epoch_loss_avg.result())\n    \n    ## validation\n    step = 0\n    epoch_valid_loss_avg = tf.keras.metrics.Mean()\n    \n    for (batch, (inp_tot, targ)) in enumerate(windowed_valid.take(steps_per_epoch)):\n        \n        inp = inp_tot[0]\n        aux = inp_tot[1]\n        \n        batch_loss_results = batch_loss(inp, aux, targ, valid_loss_fct, None)\n        \n        # training progress\n        epoch_valid_loss_avg.update_state(batch_loss_results)\n\n    # collect training loss values\n    valid_loss_results.append(epoch_valid_loss_avg.result())\n    \n    if epoch % 10 == 0:\n        \n        print(\"Epoch {}: Loss MAE: {:.5f} --- Val Loss MAE: {:.5f}\".format(epoch,\n                                                                       epoch_loss_avg.result(),\n                                                                       epoch_valid_loss_avg.result()))\n        \n      # ----- EARLY STOPPING -------\n    \n    patience_cnt = monitor_loss(valid_loss_results, epoch, min_delta, patience_cnt)\n\n    if patience_cnt > patience:\n        \n        print(\"early stopping...\") \n        break  \n        \nprint('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))","metadata":{"execution":{"iopub.status.busy":"2021-07-05T15:03:47.322193Z","iopub.execute_input":"2021-07-05T15:03:47.322505Z","iopub.status.idle":"2021-07-05T15:04:01.740211Z","shell.execute_reply.started":"2021-07-05T15:03:47.32248Z","shell.execute_reply":"2021-07-05T15:04:01.739438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Plot Training Progress","metadata":{}},{"cell_type":"code","source":"fig, axes = plt.subplots(1, sharex=True, figsize=(12, 8))\n\nfig.suptitle('Training Metrics')\naxes.set_ylabel(\"Loss (MAE) - training and validation\", fontsize=14)\naxes.plot(train_loss_results)\naxes.plot(valid_loss_results)\naxes.set_xlabel(\"Epoch\", fontsize=14)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-05T15:04:01.743369Z","iopub.execute_input":"2021-07-05T15:04:01.743632Z","iopub.status.idle":"2021-07-05T15:04:02.03084Z","shell.execute_reply.started":"2021-07-05T15:04:01.743606Z","shell.execute_reply":"2021-07-05T15:04:02.029817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluate_forecasts(targets, forecasts, n_seq):\n    \n    list_rmse = []\n    list_mae = []\n    \n    for i in range(n_seq):\n        true = np.vstack([target[i] for target in targets])\n        predicted = np.vstack([forecast[i] for forecast in forecasts])\n        \n        rmse = np.sqrt((np.square(true - predicted)).mean(axis=0))\n        mae = np.absolute(true - predicted).mean(axis=0)\n        \n        list_rmse.append(rmse)\n        list_mae.append(mae)\n        \n    list_rmse = np.vstack(list_rmse)\n    list_mae = np.vstack(list_mae)\n    \n    return list_rmse, list_mae","metadata":{"execution":{"iopub.status.busy":"2021-07-05T15:04:02.033886Z","iopub.execute_input":"2021-07-05T15:04:02.034149Z","iopub.status.idle":"2021-07-05T15:04:02.044322Z","shell.execute_reply.started":"2021-07-05T15:04:02.034121Z","shell.execute_reply":"2021-07-05T15:04:02.041816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Test and Update Model","metadata":{}},{"cell_type":"code","source":"forecasts = []\ntargets = []\n\nrmse_list = []\nmae_list = []\n\nprint('Starting')\nprint('')\n\nfor (step, (inp_tot, targ)) in enumerate(windowed_test):\n           \n\n        inp = inp_tot[0]\n        aux = inp_tot[1]\n        \n        targ = tf.cast(targ, tf.float32)\n        \n        out_enc, state_h, state_c  = encoder(inp)\n        \n        pred = decoder(aux, state_h, state_c, training=False)\n        \n        truth = inverse_transform(targ[0][:,:-size_aux],  scaler)\n        pred = inverse_transform(pred[0][:,:-size_aux],  scaler)\n        \n        forecasts.append(pred)\n        targets.append(truth)\n        \n        rmse, mae = evaluate_forecasts(targets, forecasts, size_forecast)\n           \n        rmse_list.append(rmse)\n        mae_list.append(mae)\n        \n        \n        print('* Time step '+str(step))\n        print('* Timestamp '+str(timestamp_pred.iloc[step]))\n        print('* Prediction Accuracy (MAE) '+ str(np.absolute(truth - pred).mean()))\n        print('* After prediction UPDATE model with new streets observations')\n        \n        new_instance = test_feat[step,:].reshape(1,-1)\n    \n        train_feat = np.vstack([train_feat[1:,:], new_instance])\n    \n        windowed_new = prep_data(train_feat, size_input, size_forecast, size_aux, batch_size) \n\n        update_steps_per_epoch = len(train_feat)//batch_size\n        \n        UPDATE = 2\n        \n        for epoch in range(UPDATE):\n            \n            for (batch, (inp_tot_new, targ_new)) in enumerate(windowed_new.take(update_steps_per_epoch )):\n                \n                inp_new = inp_tot_new[0]\n                aux_new = inp_tot_new[1]\n                \n                targ_new = tf.cast(targ, tf.float32)\n                \n                batch_loss_results = batch_loss(inp_new, aux_new, targ_new, loss_fct, opt)\n                \n                # Track progress\n                epoch_loss_avg.update_state(batch_loss_results)\n                \n            # End epoch\n            train_loss_results.append(epoch_loss_avg.result())\n            \n            if epoch % UPDATE == 0:\n                print(\"UPDATE - Epoch {}: Loss MAE: {:.3f}\".format(epoch, epoch_loss_avg.result()))\n        \n        # ---> comment this to have full prediction\n        if step == 5:\n            break\n            \n        print('')     ","metadata":{"execution":{"iopub.status.busy":"2021-07-05T15:04:02.049175Z","iopub.execute_input":"2021-07-05T15:04:02.049431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Performance Metrics for each forecasting horizon (here for first 5 predictions steps).\n\nTo get metrics for the full test set, comment the above\n\n```\nif step == 5:\n            break\n```","metadata":{}},{"cell_type":"markdown","source":"# RMSE Metric","metadata":{}},{"cell_type":"code","source":"RMSE_MEAN = np.mean(rmse_list,axis=0).mean(axis=1)\nRMSE_STD =  np.std(rmse_list,axis=0).std(axis=1)\n\nfor i in range(len(RMSE_MEAN)):\n    print('t+'+str(i+1)+' RMSE MEAN ' +str(np.round(RMSE_MEAN[i],3))+' +- '+str(np.round(RMSE_STD[i],3)))\n    print('')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# MAE Metric","metadata":{}},{"cell_type":"code","source":"MAE_MEAN = np.mean(mae_list,axis=0).mean(axis=1)\nMAE_STD =  np.std(mae_list,axis=0).std(axis=1)\n\nfor i in range(len(MAE_MEAN)):\n    print('t+'+str(i+1)+' MAE MEAN ' +str(np.round(MAE_MEAN[i],3))+' +- '+str(np.round(MAE_STD[i],3)))\n    print('')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Example Visualizations Results for Time step t","metadata":{"execution":{"iopub.status.busy":"2021-07-05T13:54:52.527472Z","iopub.status.idle":"2021-07-05T13:54:52.528199Z"}}},{"cell_type":"markdown","source":"#### Matplotlib","metadata":{}},{"cell_type":"code","source":"t = timestamp_pred.iloc[step+1:step+13]\n        \n       \nplt.fill_between(t,  np.mean(pred, axis=1) + np.std(pred, axis=1), \n           (np.mean(pred, axis=1) - np.std(pred, axis=1)),\n           color = 'green', label = 'pred mean +- std', alpha=0.13,\n           linewidth = 2)\n\nplt.fill_between(t,  np.mean(truth, axis=1) + np.std(truth, axis=1), \n           (np.mean(truth, axis=1) - np.std(truth, axis=1)),\n           color = 'orange', label = 'targ mean +- std', alpha=0.13,\n           linewidth = 2)\n\nplt.plot(t, np.mean(pred, axis=1), color = 'green', lw=2, label='Mean Prediction') \nplt.plot(t, np.mean(truth, axis=1), color = 'orange', lw=2, label='Mean Truth')\n\nplt.title('$Mean\\ Traffic\\ Flow\\ in\\ Belgium$ at: '+str(timestamp_pred.iloc[step]))\nplt.ylabel('$Traffic\\ Flow$')\nplt.xlabel('$Forecasting\\ Horizon$')\nplt.xticks(rotation=45)\nplt.legend(loc='upper left')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### pydeck","metadata":{}},{"cell_type":"code","source":"!pip install pydeck","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### visualization for prediction at (t+1)","metadata":{}},{"cell_type":"code","source":"import pydeck as pdk\n\ndf_belgium['traffic_flow'] = pred[0]\nprint('Prediction for time: ' +str(timestamp_pred.iloc[step+1]))\n\ntooltip = {\"text\": \"Traffic Flow : {traffic_flow}\"}\n\nINITIAL_VIEW_STATE = pdk.ViewState(latitude=50.85045, longitude=4.34878, zoom=8, max_zoom=30, pitch=10, bearing=0)\ngeojson = pdk.Layer(\n\"GeoJsonLayer\",\ndf_belgium,\nstroked=False,\nfilled=True,\nextruded=True,\nwireframe=True,\nget_elevation = \"traffic_flow*5\",\nget_fill_color='[255, (1-traffic_flow/250)*255, 0]',\nget_line_color='[255, 255, 255]')\n\nr = pdk.Deck(map_style=pdk.map_styles.LIGHT, layers=geojson, initial_view_state=INITIAL_VIEW_STATE,tooltip={\"text\": \"Traffic Flow: {traffic_flow}\"},)\nr.to_html(\"geojson_layer.html\", notebook_display=True)\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Save Results Pickle","metadata":{}},{"cell_type":"code","source":"import pickle\n\n# Saving the objects:\nwith open('save_predictions_results.pkl', 'wb') as f: \n    pickle.dump([rmse_list, mae_list], f)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}