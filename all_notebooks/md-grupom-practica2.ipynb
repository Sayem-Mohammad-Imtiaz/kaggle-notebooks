{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Práctica 2: Aprendizaje y selección de modelos de clasificación\n\n### Minería de Datos: Curso académico 2020-2021\n\n### Profesorado:\n\n* Juan Carlos Alfaro Jiménez\n* José Antonio Gámez Martín\n\n### Alumnado:\n\n* Alejandro Gómez Escribano\n* Mykola Mandzyak\n\n\\* Adaptado de las prácticas de Jacinto Arias Martínez y Enrique González Rodrigo"},{"metadata":{},"cell_type":"markdown","source":"En esta práctica estudiaremos los modelos más utilizados en `scikit-learn` para conocer los distintos hiperparámetros que los configuran y estudiar los clasificadores resultantes. Además, veremos métodos de selección de modelos orientados a obtener una configuración óptima de hiperparámetros.\n\nLos conjuntos de datos utilizados son `Diabetes` y `Wisconsin`."},{"metadata":{},"cell_type":"markdown","source":"# 1. Preliminares"},{"metadata":{},"cell_type":"markdown","source":"Antes de comenzar cargamos las librerías para que estén disponibles posteriormente:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Third party\nfrom sklearn.base import clone\nfrom sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import HistGradientBoostingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\nfrom imblearn.pipeline import make_pipeline\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import KBinsDiscretizer\nfrom imblearn import FunctionSampler\nfrom sklearn.ensemble import IsolationForest\nimport pandas as pd\n\n# Local application\nimport miner_a_de_datos_aprendizaje_modelos_utilidad as utils","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ahora debemos fijar una semilla para que los experimentos sean reproducibles:"},{"metadata":{"trusted":true},"cell_type":"code","source":"random_state = 18453 #cambiarlo en el imputador de outliers del archivo utils","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1.1 Modelos e Hiperparámteros tenidos en cuenta "},{"metadata":{},"cell_type":"markdown","source":"Los iniciaremos con sus valores por defecto o los más básicos posibles para poder compararlos después."},{"metadata":{},"cell_type":"markdown","source":"### 1.1.1 Vecinos más cercanos"},{"metadata":{},"cell_type":"markdown","source":"* `n_neighbors`: Número de vecinos más cercanos (por defecto, `n_neighbors=5`). Es importante señalar que aumentar el número de vecinos más cercanos provoca que el clasificador sea más robusto al ruido, pero al mismo tiempo se obtienen fronteras de decisión menos distinguidas.\n\n* `weights`: Función de ponderación de los vecinos más cercanos (por defecto, `weights=\"uniform\"`). Si consideramos que todos los vecinos cercanos tienen la misma importancia utilizaremos`weights=\"uniform\"`, por el contrario, si consideramos que los vecinos más cercanos tienen más importancia y deben contribuir más a la regresión que los puntos lejanos, utilizaremos `weights=\"distance\"`.\n\n* `metric`: Distancia métrica que utilizará el árbol (por defecto, `metric=\"minkowski\"`, pero como el hiperparámetro `p=2` equivale a usa una distancia euclídea), las distancias a considerar serán `metric=\"euclidean\"` y `metric=\"manhattan\"`."},{"metadata":{"trusted":true},"cell_type":"code","source":"n_neighbors = 10\n\nk_neighbors_model = KNeighborsClassifier(n_neighbors)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1.1.2 Árboles de decisión"},{"metadata":{},"cell_type":"markdown","source":"* `criterion`: Criterio utilizado para medir la calidad de una partición (por defecto, `criterion=\"gini\"`).\n* `max_depth`: Altura máxima del árbol de decisión (por defecto, `max_depth=None`).\n* `ccp_alpha`: Parámetro de complejidad usado para el algoritmo de post-poda *Minimal Cost-Complexity Pruning* (por defecto, `ccp_alpha=0.0`).\n* `splitter`: La estrategia utilizada para elegir la división en cada nodo. Las estrategias admitidas son \"best\" para elegir la mejor división y \"random\" para elegir la mejor división aleatoria (por defecto, `splitter=best`).\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"decision_tree_model = DecisionTreeClassifier(random_state=random_state)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1.1.3 Adaptative Boosting (AdaBoost)"},{"metadata":{},"cell_type":"markdown","source":"* `base_estimator`: Estimador base que usa el ensemble (por defecto, `base_estimator=None`). Si no se especifica, se utiliza un árbol de decisión de profundidad uno.\n* `n_estimators`: Número de estimadores a aprender (por defecto, `n_estimators=50`).\n* `learning_rate`: Parámetro de regularización para controlar la contribución de cada estimador (por defecto, `learning_rate=1.0`).\n* `ccp_alpha`: Parámetro de complejidad usado para el algoritmo de post-poda *Minimal Cost-Complexity Pruning* (por defecto, `ccp_alpha=0.0`)."},{"metadata":{"trusted":true},"cell_type":"code","source":"adaboost_model = AdaBoostClassifier(random_state=random_state)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1.1.4 Bootstrap Aggregating (Bagging)"},{"metadata":{},"cell_type":"markdown","source":"* `base_estimator`: Estimador base que usa el ensemble (por defecto, `base_estimator=None`). Si no se especifica, se utiliza un árbol de decisión sin podar.\n* `n_estimators`: Número de estimadores a aprender (por defecto, `n_estimators=10`).\n* `bootstrap`: Cuándo el muestreo de instancias es con reemplazo (por defecto, `bootstrap=True`).\n* `bootstrap_features`: Cuándo el muestreo de características es con reemplazo (por defecto, `bootstrap_features=False`).\n* `random_state`: Semilla para controlar la reproducibilidad de los experimentos (por defecto, `random_state=None`)."},{"metadata":{"trusted":true},"cell_type":"code","source":"bagging_model = BaggingClassifier(random_state=random_state)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1.1.5 Random Forests"},{"metadata":{},"cell_type":"markdown","source":"* `n_estimators`: Número de estimadores a aprender (por defecto, `n_estimators=100`).\n* `criterion`: Criterio utilizado para medir la calidad de una partición (por defecto, `criterion=\"gini\"`).\n* `max_features`: Número de características a considerar en cada nodo del árbol (por defecto, `max_features=\"auto\"`).\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"random_forest_model = RandomForestClassifier(random_state=random_state)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1.1.6 Gradient Tree Boosting (Gradient Boosting)"},{"metadata":{"trusted":true},"cell_type":"code","source":"gradient_boosting_model = GradientBoostingClassifier(random_state=random_state)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* `learning_rate`: Parámetro de regularización para controlar la contribución de cada estimador (por defecto, `learning_rate=0.1`).\n* `n_estimators`: Número de estimadores a aprender (por defecto, `n_estimators=100`).\n* `criterion`: Criterio utilizado para medir la calidad de una partición (por defecto, `criterion=\"friedman_mse\"`).\n* `max_depth`: Altura máxima de los árboles de decisión (por defecto, `max_depth=3`).\n* `ccp_alpha`: Parámetro de complejidad usado para el algoritmo de post-poda *Minimal Cost-Complexity Pruning* (por defecto, `ccp_alpha=0.0`)."},{"metadata":{},"cell_type":"markdown","source":"### 1.1.7 Histogram-Based Gradient Boosting (Histogram Gradient Boosting)"},{"metadata":{},"cell_type":"markdown","source":"* `learning_rate`: Parámetro de regularización para controlar la contribución de cada estimador (por defecto, `learning_rate=0.1`).\n* `max_iter`: Número máximo de iteraciones del algoritmo (por defecto, `max_iter=100`).\n* `max_leaf_nodes`: Número máximo de nodos hoja (por defecto, `max_leaf_nodes=31`)."},{"metadata":{"trusted":true},"cell_type":"code","source":"hist_gradient_boosting_model = HistGradientBoostingClassifier(random_state=random_state)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1.2 Términos importantes"},{"metadata":{},"cell_type":"markdown","source":"### 1.2.1 Hiperparámetro\n\nParámetro del algoritmo de aprendizaje que modifica como se induce el modelo."},{"metadata":{},"cell_type":"markdown","source":"### 1.2.2 Sesgo\n\nDiferencia entre el valor esperado y el valor obtenido.\n\nSe debe a las suposiciones incorrectas del modelo.\n\nEste error suele reducirse usando modelos más complejos o una mayor dimensionalidad de los datos."},{"metadata":{},"cell_type":"markdown","source":"### 1.2.3 Varianza\n\nVariabilidad de la predicción esperada y la\npredicción obtenida.\n\nSe produce cuando el modelo aprende patrones espúreos debido al ruido de los datos. \n\nEste error suele reducirse aumentando el número de ejemplos del conjunto de datos de entrenamiento o usando modelos más simples."},{"metadata":{},"cell_type":"markdown","source":"![](https://bookdown.org/content/2031/images/sesgo_varianza.png)"},{"metadata":{},"cell_type":"markdown","source":"Tendremos modelos que tengan alto sesgo o varianza, por lo que deberemos tratar estos problemas dependiendo del error de cada modelo."},{"metadata":{},"cell_type":"markdown","source":"## 1.3 Evaluación de modelos"},{"metadata":{},"cell_type":"markdown","source":"Para la evaluación de modelos, utilizaremos una validación cruzada estratificada para asegurarnos buenas estimaciones y evitar así el sesgo y la varianza. Utilizaremos 10 carpetas y 5 repeticiones:"},{"metadata":{"trusted":true},"cell_type":"code","source":"n_splits = 10\nn_repeats = 5\n\ncv = RepeatedStratifiedKFold(n_splits=n_splits,\n                             n_repeats=n_repeats,\n                             random_state=random_state)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Además como criterio de evaluación utilizaremos la media entre `Accuracy`, `Recall` y `ROC`, aunque utilizamos `ROC`, consideramos `Recall` a parte para ponderar más los aciertos en positivos ya estamos ante conjuntos de datos que tratan efermedades y hemos hecho esta consideración.\n\nAplicaremos esto a los conjuntos de datos utilizados en la práctica anterior, comenzando por `Diabetes`."},{"metadata":{},"cell_type":"markdown","source":"# 2. Diabetes"},{"metadata":{},"cell_type":"markdown","source":"## 2.1 Carga de datos"},{"metadata":{},"cell_type":"markdown","source":"Cargamos datos y comprobamos que se han cargado correctamente"},{"metadata":{"trusted":true},"cell_type":"code","source":"filepath= \"../input/pima-indians-diabetes-database/diabetes.csv\"\ntarget = \"Outcome\"\n\ndata = utils.load_data(filepath,None,target)\ndata.index=range(data.shape[0])\ndata.sample(8,random_state=random_state)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A continuación, obtendremos nuestras variables predictoras por un lado y la variable objetivo por otro."},{"metadata":{"trusted":true},"cell_type":"code","source":"target = \"Outcome\"\n\n(X, y) = utils.divide_dataset(data, target)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ahora comprobamos que se han separado correctamente:"},{"metadata":{},"cell_type":"markdown","source":"**Variables Predictoras**"},{"metadata":{"trusted":true},"cell_type":"code","source":"X.sample(5, random_state=random_state)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Variable Clase**"},{"metadata":{"trusted":true},"cell_type":"code","source":"y.sample(5, random_state=random_state)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ahora, divididiremos el conjunto de datos en entrenamiento y prueba mediante un *holdout* estratificado (70% train y 30% test):"},{"metadata":{"trusted":true},"cell_type":"code","source":"stratify = y\ntrain_size = 0.7\n\n(X_train, X_test, y_train, y_test) = train_test_split(X, y,\n                                                      stratify=stratify,\n                                                      train_size=train_size,\n                                                      random_state=random_state)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ahora comprobamos que hemos separado correctamente el conjunto en train y test:"},{"metadata":{},"cell_type":"markdown","source":"**Train - Variables Predictoras**"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.sample(5, random_state=random_state)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Train - Variable Clase**"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train.sample(5, random_state=random_state)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Test - Variables Predictoras**"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test.sample(5, random_state=random_state)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Test - Variable Clase**"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test.sample(5, random_state=random_state)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Establecemos la 'X' y la 'y' de esta forma por comodidad para los modelos\n\nX = X_train\ny = y_train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Parámetros del pipeline**"},{"metadata":{"trusted":true},"cell_type":"code","source":"variables =['BMI','Glucose','BloodPressure']\nimputar = utils.imputador(variables)\n\nlista = ['Insulin','SkinThickness']\neliminar_columnas = utils.dropVar(lista)\n\neliminar_outliers = FunctionSampler(func=utils.outlier_rejection)#para cambiar la seed, en el utils\n\ndiscretizer = KBinsDiscretizer(n_bins=3, strategy=\"uniform\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.2 Selección de modelos"},{"metadata":{},"cell_type":"markdown","source":"Para esta selección, es muy importante escoger los mejores hiperparámteros para cada modelo, por ello utilizaremos un grid de hiperparámetros que evaluará diferentes combinaciones de los hiperparámetros y las ejecutará para luego poder ver el resultado de cada combinación, todo esto junto con un pipeline para el procesamiento de los datos."},{"metadata":{},"cell_type":"markdown","source":"### 2.2.1 Vecinos más cercanos"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Modelo\nestimator = clone(k_neighbors_model)\n\n#Hiperparámetros\nweights = [\"uniform\", \"distance\"]\nn_neighbors = [3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]\nmetric = [\"euclidean\",\"manhattan\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Sobre `weights`, poco que decir, esos son sus dos únicos valores, cuya principal diferencia es la ponderación o no dependiendo de la distancia, resulta un hiperparámetro bastante útil.\n\nSobre `n_neighbors`, lo primero, que hemos descartado los valores 1 y 2 debido a sobreajustes. Además, como los datos no son muy grandes (el train lo forman 537 datos), hemos establecido un array secuencial, ya que knn recomienda evaluar de forma secuencial desde 1 hasta máximo la raíz de N (23).\n\nSobre `metric` queríamos establecer al menos 2 métricas no muy complejas para el estudio de distancias, luego de la lista que proporciona scikit, escogimos estas 2."},{"metadata":{},"cell_type":"markdown","source":"**Sin discretizar**"},{"metadata":{"trusted":true},"cell_type":"code","source":"pipeline = make_pipeline(imputar, eliminar_columnas, eliminar_outliers, estimator)\n\nk_neighbors_clf = utils.optimize_params(pipeline,\n                                        X, y, cv,\n                                        kneighborsclassifier__weights=weights,\n                                        kneighborsclassifier__n_neighbors=n_neighbors,\n                                        kneighborsclassifier__metric=metric)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Según nuestro método de validación esta combinación acaba siendo la mejor, incluso si consideramos que al ser una enfermedad nos interesa más conocer el caso de positivos marcados como tal respecto al total de positivos (Recall), esta combinación se encuentra en el puesto 8, luego no es tampoco un mal resultado.\n\nTambién comentar que este conjunto de datos es pequeño, los datos se encuentran más o menos dispersos y sin discretizar, lo que provoca que tanto `uniform` como `distance`, estén más o menos reñidos. Además de que `metric=manhattan` acaba siendo la mejor métrica con diferencia.\n\nTambién comenta que aunque conseguimos buenos puestos validando con `Accuracy` y `REcall`, en `AUC` por desgracia no ocurre lo mismo:\n\n* Puesto `Accuracy` = 2\n* Puesto `AUC` = 57\n* Puesto `Recall` = 8"},{"metadata":{},"cell_type":"markdown","source":"**Discretizado**"},{"metadata":{"trusted":true},"cell_type":"code","source":"pipeline = make_pipeline(imputar, eliminar_columnas, eliminar_outliers, discretizer, estimator)\n\nk_neighbors_clf_ds = utils.optimize_params(pipeline,\n                                        X, y, cv,\n                                        kneighborsclassifier__weights=weights,\n                                        kneighborsclassifier__n_neighbors=n_neighbors,\n                                        kneighborsclassifier__metric=metric)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Discretizando, acabamos obteniendo peor `score` que sin discretizar, podemos observar que en este caso el `n_neighbors` es menor y además provoca que las 2 variables de `metric` estén más reñidas, tiene sentido ya que al discretizar dividimos en intervalos, luego esto afecta bastante a los 3 hiperparámetros utilizados, ya que todos están relacionados con las distancias.\n\nValidando obtenemos un caso parecido al anterior pero con algo que nos puede interesar, obtenemos peor `Accuracy` y mejor `Recall`, en `AUC` por desgracia sigue igual, esto es interesante porque como la base de datos trata una enfermedad, `Recall` podría interesarnos más al tener mejor puesto, aún así su score sigue siendo menor al caso sin discretizar:\n\n* Puesto `Accuracy` = 15\n* Puesto `AUC` = 79\n* Puesto `Recall` = 2"},{"metadata":{},"cell_type":"markdown","source":"### 2.2.2 Árboles de decisión"},{"metadata":{},"cell_type":"markdown","source":"**Árbol totalmente expandido**"},{"metadata":{"trusted":true},"cell_type":"code","source":"utils.plot_tree(decision_tree_model, X, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Modelo\nestimator = clone(decision_tree_model)\n\n#Hiperparámetros\ncriterion = [\"gini\", \"entropy\"]\nmax_depth = [2, 3, 4, 5, 6, 7, 8, 9]\nccp_alpha = [0.0, 0.015, 0.025, 0.035]\nsplitter = [\"best\", \"random\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"En relación con el árbol expandido totalmente podemos observar que la profundidad máxima es 12, de aquí establecemos un corte más o menos moderado en `max_depth` hasta un máximo de 9 para evitar sobreajustes, además de que omitimos profundidad 0 y 1 ya que es a partir de la profundidad 2 donde de verdad podemos empezar a considerar las particiones, podemos observar un clara división de variables a izquierda y derecha (en el subárbol izquierdo predomina el naranja mientras que en el derecho el azul)\n\nAdemás hemos cambiado los valores de `ccp_alpha`, los hemos establecido más pequeños debido a que en guías como las de scikit podemos observar que con valores muy bajos ya comienza a reducir el número de nodos, además de que también hemos observado de que conseguimos mejores scores que con los valores anteriormente por defecto.\n\nAdemás también hemos considerado `criterion` y `splitter`, ambos relacionados con las divisiones (`criterion ` mide la calidad y `splitter` escoge la mejor división dependiendo del valor seleccionado).\n"},{"metadata":{},"cell_type":"markdown","source":"**Sin discretizar**"},{"metadata":{"trusted":true},"cell_type":"code","source":"pipeline = make_pipeline(imputar, eliminar_columnas, eliminar_outliers, estimator)\n\ndecision_tree_clf = utils.optimize_params(pipeline,\n                                          X, y, cv,\n                                          decisiontreeclassifier__criterion=criterion,\n                                          decisiontreeclassifier__max_depth=max_depth,\n                                          decisiontreeclassifier__ccp_alpha=ccp_alpha,\n                                          decisiontreeclassifier__splitter=splitter)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Podemos observar que con este clasificador mejoramos el mejor caso de score anterior, respecto a las salidas, podemos ver que sin discretizar `criterion=entropy` domina los primeros puestos.\n\nLos mejores casos son a partir de `max_depth` mayores o iguales a 4, si nos fijamos en el árbol totalmente expandido podemos observar lo anteriormente comentado sobre la división de colores, donde en la izquierda predomina el naranja y en la derecha el azul, que hacen referencia a los 0's y 1's de nuestra variable clase, luego es normal que obtengamos mejores resultados a partir de esas profundidades.\n\nCon este clasificador mejoramos el mejor caso de score anterior, esto se puede ver reflejado en los puestos, con este clasificador conseguimos más filas debido a las configuraciones pero aún así acabamos obteniendo mejores puestos:\n\n* Puesto `Accuracy` = 2\n* Puesto `AUC` = 3\n* Puesto `Recall` = 8"},{"metadata":{},"cell_type":"markdown","source":"**Discretizado**"},{"metadata":{"trusted":true},"cell_type":"code","source":"pipeline = make_pipeline(imputar, eliminar_columnas, eliminar_outliers, discretizer, estimator)\n\ndecision_tree_clf_ds = utils.optimize_params(pipeline,\n                                          X, y, cv,\n                                          decisiontreeclassifier__criterion=criterion,\n                                          decisiontreeclassifier__max_depth=max_depth,\n                                          decisiontreeclassifier__ccp_alpha=ccp_alpha,\n                                          decisiontreeclassifier__splitter=splitter)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Aquí vuelve a pasar lo mismo que en el antiguo modelo, discretizando perdemos, pero aún así superamos `k_neighbors_model` discretizado.\n\nLa discretización acaba provocando que esta vez el `criteron` predominante sea gini, debido a la división que esta provoca. Antes también encontrábamos un `ccp_alpha` variado, mientras que aquí predominan los casos donde no se poda, además la división también provoca que los mejores árboles sean los de profundidad pequeña, de ahí que consigamos un peor score.\n\nEn este caso conseguimos muy buenos scores de `Accuracy` y `AUC` pero uno bastante malo de `Recall` que para este tipo de bases de datos resulta muy importante un buen score en este evaluador, sus puestos:\n\n* Puesto `Accuracy` = 1\n* Puesto `AUC` = 3\n* Puesto `Recall` = 25"},{"metadata":{},"cell_type":"markdown","source":"### 2.2.3 Adaptative Boosting (AdaBoost)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Modelo\nestimator = clone(adaboost_model)\nbase_estimator = clone(decision_tree_model)\n\n#Hiperparámetros\nbase_estimator = [base_estimator]\nlearning_rate = [0.85, 0.95, 1.0]\ncriterion = [\"gini\", \"entropy\"]\nmax_depth = [1, 2, 3, 4, 5]\nccp_alpha = [0.0, 0.015, 0.025, 0.035]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"En relación con el árbol totalmente expandido, y sabiendo que `adaboost_model` trabaja con árboles pequeños, hemos establecido un valor máximo para `max_depth` de 5.\n\nPara los árboles, seguimos teniendo en cuenta los 2 `criterion` anteriores y los valores de `ccp_alpha`.\n\nPara el `learning_rate`, donde se marca el valor de contribución de un modelo nuevo al ya existente, hemos utilizado valores altos ya que consideramos que deben tener un peso cercano a 1 en la contribución y poder analizar como varía según sea más alto o bajo."},{"metadata":{},"cell_type":"markdown","source":"**Sin discretizar**"},{"metadata":{"trusted":true},"cell_type":"code","source":"pipeline = make_pipeline(imputar, eliminar_columnas, eliminar_outliers, estimator)\n\nadaboost_clf = utils.optimize_params(pipeline,\n                                     X, y, cv,\n                                     adaboostclassifier__base_estimator=base_estimator,\n                                     adaboostclassifier__learning_rate=learning_rate,\n                                     adaboostclassifier__base_estimator__criterion=criterion,\n                                     adaboostclassifier__base_estimator__max_depth=max_depth,\n                                     adaboostclassifier__base_estimator__ccp_alpha=ccp_alpha)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Sin discretizar, podemos observar primeramente que el mejor `criterion` es `gini` y que al ser `adaboost_model`, la mejor `max_depht` acaba siendo 1. Podemos notar algo de similitud con el decision tree discretizado ya que su `max_depht` también es un valor pequeño y el `criterion` acaba siendo gini.\n\nTambién notamos que en los 3 métodos de validación utilizados, estamos consiguiendo cada vez resultados mejores, de ahí el aumeto de score respecto a los casos anteriores, los puestos:\n\n* Puesto `Accuracy` = 15\n* Puesto `AUC` = 8\n* Puesto `Recall` = 3"},{"metadata":{},"cell_type":"markdown","source":"**Discretizado**"},{"metadata":{"trusted":true},"cell_type":"code","source":"pipeline = make_pipeline(imputar, eliminar_columnas, eliminar_outliers, discretizer, estimator)\n\nadaboost_clf_ds = utils.optimize_params(pipeline,\n                                     X, y, cv,\n                                     adaboostclassifier__base_estimator=base_estimator,\n                                     adaboostclassifier__learning_rate=learning_rate,\n                                     adaboostclassifier__base_estimator__criterion=criterion,\n                                     adaboostclassifier__base_estimator__max_depth=max_depth,\n                                     adaboostclassifier__base_estimator__ccp_alpha=ccp_alpha)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Seguimos obteniendo peores resultados discretizando, la combinación ganadora sigue manteniendo un `criterion=gini` y `max_depth=1`, en este caso no poda y el `learning_rate=1`.\n\nTambién encontramos más combinaciones de los 2 tipos de criterio en los mejores casos y el significativo rechazo a la poda en los mejores casos, tiene sentido debido al discretizador, por la división de los datos.\n\n* Puesto `Accuracy` = 27\n* Puesto `AUC` = 23\n* Puesto `Recall` = 1"},{"metadata":{},"cell_type":"markdown","source":"### 2.2.4 Bootstrap Aggregating (Bagging)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Modelo\nestimator = clone(bagging_model)\nbase_estimator = clone(decision_tree_model)\n\n#Hiperparámetros\nbase_estimator = [base_estimator]\ncriterion = [\"gini\", \"entropy\"]\nn_estimators = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\nbootstrap_features = [True, False] \nbootstrap = [True, False] ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Debido a que `bagging_model`, trabaja mejor con árboles profundos, favorecemos las creaciones totales de los árboles, dejando los parámetros por defecto, con la excepción de `criterion`.\n\nDebido a que, teóricamente, cuantos más estimadores, mejor score obtenemos, `n_estimators` es el hiperparámetro que más nos interesa, de ahí las combinaciones, a parte, hemos tenido en cuenta `boostrap_feautres` y `boostrap`, ambas tratan las variables con reemplazo, para evaluar los posibles resultados.\n\nTambién se tuvieron en cuentra otros hiperparámetros pero se descartaron debido a que no podrucían ningún tipo de cambio, como `warm_start` que si se establece como `warm_start=True` reutiliza la solución de la llamada anterior para ajustar y agregar más estimadores al conjunto, otro que también se estimó pero no podrucía cambios era `oob_score` que marcado como `oob_score=True` tendría en cuenta las muestras fuera de la bolsa para estimar el error de generalización (Detallar que otros modelos tratados en esta práctica también poseen estos hiperparámetros pero el resultado seguía siendo el mismo, no producían cambios a pesar de que son muy interesantes)."},{"metadata":{},"cell_type":"markdown","source":"**Sin discretizar**"},{"metadata":{"trusted":true},"cell_type":"code","source":"pipeline = make_pipeline(imputar, eliminar_columnas, eliminar_outliers, estimator)\n\nbagging_clf = utils.optimize_params(pipeline,\n                                    X, y, cv,\n                                    baggingclassifier__base_estimator=base_estimator,\n                                    baggingclassifier__base_estimator__criterion=criterion,\n                                    baggingclassifier__n_estimators=n_estimators,\n                                    baggingclassifier__bootstrap_features =bootstrap_features,\n                                    baggingclassifier__bootstrap=bootstrap)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Continuando con la racha de que cada algoritmo obtiene un mejor score que el anterior, `bagging_model` sin discretzar obtiene mejor tasa que `adaboost_model`.\n\nEn este caso observamos significantes parecidos en los principales resultados, los cuáles son `criterion=entropy`, `bootstrap=True` y `boostrap_features=False` (estos 2 últimos son los valores por defecto).\n\nAdemás los mejores valores de `n_estimators`, el cuál por defecto es igual a 10, acaban siendo mucho mayores, aunque también podemos observar que el ganador acaba teniendo `n_estimators=80`, lo que deja por debajo los valores de 90 y 100, lo que nos podría hacer llegar a pensar de que hemos alcanzado el tope.\n\nSus puestos:\n\n* Puesto `Accuracy` = 1\n* Puesto `AUC` = 2\n* Puesto `Recall` = 1"},{"metadata":{},"cell_type":"markdown","source":"**Discretizado**"},{"metadata":{"trusted":true},"cell_type":"code","source":"pipeline = make_pipeline(imputar, eliminar_columnas, eliminar_outliers, discretizer, estimator)\n\nbagging_clf_ds = utils.optimize_params(pipeline,\n                                    X, y, cv,\n                                    baggingclassifier__base_estimator=base_estimator,\n                                    baggingclassifier__base_estimator__criterion=criterion,\n                                    baggingclassifier__n_estimators=n_estimators,\n                                    baggingclassifier__bootstrap_features =bootstrap_features,\n                                    baggingclassifier__bootstrap=bootstrap)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Seguimos obteniendo peores resultados en discretización, además la dicretización anterior, la de `adaboost_model` es mejor que esta.\n\nAcabamos obteniendo la misma solución que sin discretizar, excepto que el `n_estimators=90` discretizando.\n\nEn este caso las combinaciones de los hiperparámetros de los mejores casos, son muy parecidas a las de los resultados sin discretizar. \n\nTeniendo en cuenta estos resultados, podríamos omitir el uso de los hiperparámetros `bootstrap` y `bootstrap_features`, ya que no tienen peso significativo en los mejores resultados, aún así hemos deicido dejarlas a modo de datos interesantes ya que estos de verdad proporcionan variaciones en los resultados.\n\nLos puestos en este caso son:\n\n* Puesto `Accuracy` = 11\n* Puesto `AUC` = 21\n* Puesto `Recall` = 4"},{"metadata":{},"cell_type":"markdown","source":"### 2.2.5 Random Forests"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Modelo\nestimator = clone(random_forest_model)\n\n#Hiperparámetros\ncriterion = [\"gini\", \"entropy\"]\nmax_features = [\"sqrt\", \"log2\"]\nn_estimators = [100, 150, 200, 250]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Random Forests es otro de los modelos que trabaja mejor con árboles profundos, luego dejaremos los parámetros por cefecto y nos centraremos en los proporcionados por la práctica, además de aumentar el número de `n_estimators`."},{"metadata":{},"cell_type":"markdown","source":"**Sin discretizar**"},{"metadata":{"trusted":true},"cell_type":"code","source":"pipeline = make_pipeline(imputar, eliminar_columnas, eliminar_outliers, estimator)\n\nrandom_forest_clf = utils.optimize_params(pipeline,\n                                          X, y, cv,\n                                          randomforestclassifier__criterion=criterion,\n                                          randomforestclassifier__max_features=max_features,\n                                          randomforestclassifier__n_estimators=n_estimators)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"En este caso, aunque sea por un 0.001, `random_forest_model` sin discretizar acaba obteniendo peor score que el mejor de lo que llevamos de modelos.\n\nEntre los mejores casos, podemos ver que en `criterion` sus dos valores están bastante reñidos, en `max_features` acaba predominando el valor log2 y la mayoría de los mejores resultados, acaban siendo mayores al valor por defecto `n_estimators=100`, a pesar de esto una de las combinaciones con `n_estimators=100` acaba en segunda posición.\n\nDestacar también que en este modelo sin discretizar se han obtenido muy buenos resultados en general en el train de la validación `AUC`.\n\nLos puestos:\n\n* Puesto `Accuracy` = 1\n* Puesto `AUC` = 11\n* Puesto `Recall` = 1"},{"metadata":{},"cell_type":"markdown","source":"**Discretizado**"},{"metadata":{"trusted":true},"cell_type":"code","source":"pipeline = make_pipeline(imputar, eliminar_columnas, eliminar_outliers, discretizer, estimator)\n\nrandom_forest_clf_ds = utils.optimize_params(pipeline,\n                                          X, y, cv,\n                                          randomforestclassifier__criterion=criterion,\n                                          randomforestclassifier__max_features=max_features,\n                                          randomforestclassifier__n_estimators=n_estimators)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Discretizar sigue proporcionando peores scores, en este caso la solución es totalmente distinta a la anterior.\n\nAdemás podemos ver algo realmente interesante, una de los hiperparámetros no proporciona ningún tipo de variación debido a la discretización, este hiperparámetro es `max_features`, la cuál aquí podríamos suprimirla ya que no aporta nada.\n\nAdemás en este caso los `n_estimators` mayores tienen más peso que sin discretizar, lo que provoca que, aunque gini predomine en los mejores casos, entropy acaba siendo el mejor.\n\nAdemás esta discretización pierde contra la anterior de `bagging_model` discretizado.\n\nLos puestos:\n\n* Puesto `Accuracy` = 1\n* Puesto `AUC` = 1\n* Puesto `Recall` = 3"},{"metadata":{},"cell_type":"markdown","source":"### 2.2.6 Gradient Tree Boosting (Gradient Boosting)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Modelo\nestimator = clone(gradient_boosting_model)\n\n#Hiperparámetros\nlearning_rate = [0.01, 0.05, 0.1]\ncriterion = [\"friedman_mse\", \"mse\"]\nmax_depth = [1, 2, 3, 4]\nccp_alpha = [0.0, 0.5, 1]\nn_estimators = [100, 150]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"`gradient_boosting_model` trabaja mejor con árboles pequeños, de ahí que limitemos el tamaño a un máximo de 4, además hemos establecido valores intermedios en `learning_rate` y `ccp_alpha` para comprobar las variaciones, ya que para que no se extienda mucho este modelo no hemos modificado mucho los hiperparámetros ya establecidos por la práctica que no eran malos.\n\nAdemás de que queríamos establecer al menos 2 valores distintos de `n_estimators` ya que puede provocar mejorías en los resultados al aumentarlo."},{"metadata":{},"cell_type":"markdown","source":"**Sin discretizar**"},{"metadata":{"trusted":true},"cell_type":"code","source":"pipeline = make_pipeline(imputar, eliminar_columnas, eliminar_outliers, estimator)\n\ngradient_boosting_clf = utils.optimize_params(pipeline,\n                                              X, y, cv,\n                                              gradientboostingclassifier__learning_rate=learning_rate,\n                                              gradientboostingclassifier__criterion=criterion,\n                                              gradientboostingclassifier__max_depth=max_depth,\n                                              gradientboostingclassifier__n_estimators=n_estimators,\n                                              gradientboostingclassifier__ccp_alpha=ccp_alpha)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Hasta aquí, este acaba siendo el mejore score obtenido de todos los modelos tenidos en cuenta.\n\nPodemos observar también que los mejores parámetros no utilizan poda, aún así, no observamos sobreajuesta en ninguna de las 3 variaciones.\n\nLos mejores valores no pasan de`max_depth=3` debido a lo ya comentado y el `learning_rate` es bastante variado.\n\nTambién podemos observar que el hiperparámetro proporcionado de la práctica `criterion` en los mejores valores es irrelevante, ya que no produce cambios.\n\nA pesar de estos puestos las diferencias con los primeros son bastante despreciables:\n\n* Puesto `Accuracy` = 5\n* Puesto `AUC` = 5\n* Puesto `Recall` = 3"},{"metadata":{},"cell_type":"markdown","source":"**Discretizado**"},{"metadata":{"trusted":true},"cell_type":"code","source":"pipeline = make_pipeline(imputar, eliminar_columnas, eliminar_outliers, discretizer, estimator)\n\ngradient_boosting_clf_ds = utils.optimize_params(pipeline,\n                                              X, y, cv,\n                                              gradientboostingclassifier__learning_rate=learning_rate,\n                                              gradientboostingclassifier__criterion=criterion,\n                                              gradientboostingclassifier__max_depth=max_depth,\n                                              gradientboostingclassifier__n_estimators=n_estimators,\n                                              gradientboostingclassifier__ccp_alpha=ccp_alpha)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Discretizano seguimos obtenienod peores scores, en este caso los mejores scores no realizan poda.\n\n\nTambién podemos observar que en el hiperparámetro proporcionado de la práctica `criterion` ocurre lo mismo que sin discretizar en los mejores valores es irrelevante, ya que no produce cambios.\n\nLos puestos:\n\n* Puesto `Accuracy` = 21\n* Puesto `AUC` = 29\n* Puesto `Recall` = 8"},{"metadata":{},"cell_type":"markdown","source":"### 2.2.7 Histogram-Based Gradient Boosting (Histogram Gradient Boosting)"},{"metadata":{},"cell_type":"markdown","source":"No es posible hacer una discretización de este modelo"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Modelo\nestimator = clone(hist_gradient_boosting_model)\n\n#Hiperparámetros\nlearning_rate = [0.01, 0.02, 0.03, 0.04, 0.05]\nmax_leaf_nodes = [15, 31, 65, 127]\nmax_iter = [100, 150, 200]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Sin discretizar**"},{"metadata":{"trusted":true},"cell_type":"code","source":"pipeline = make_pipeline(imputar, eliminar_columnas, eliminar_outliers, estimator)\n\nhist_gradient_boosting_clf = utils.optimize_params(pipeline,\n                                                   X, y, cv,\n                                                   histgradientboostingclassifier__learning_rate=learning_rate,\n                                                   histgradientboostingclassifier__max_iter=max_iter,\n                                                   histgradientboostingclassifier__max_leaf_nodes=max_leaf_nodes)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Recalcar que aquí acaba empeorando bastante el score respecto a diferentes modelos que no disctrizan, los mejores casos los encontramos con los valores más bajos de `max_iter` mientras que en el resto de hiperparámetros encontramos más variedad en los mejores casos.\n\nLos puestos:\n\n* Puesto `Accuracy` = 13\n* Puesto `AUC` = 29\n* Puesto `Recall` = 1"},{"metadata":{},"cell_type":"markdown","source":"## 2.3 Construcción y validación del modelo final"},{"metadata":{"trusted":true},"cell_type":"code","source":"estimators = {\n    \"Nearest neighbors\": k_neighbors_clf,\n    \"Decision tree\": decision_tree_clf,\n    \"AdaBoost\": adaboost_clf,\n    \"Bagging\": bagging_clf,\n    \"Random Forests\": random_forest_clf,\n    \"Gradient Boosting\": gradient_boosting_clf,\n    \"Histogram Gradient Boosting\": hist_gradient_boosting_clf,\n    \"Nearest neighbors discretizado\": k_neighbors_clf_ds,\n    \"Decision tree discretizado\": decision_tree_clf_ds,\n    \"AdaBoost discretizado\": adaboost_clf_ds,\n    \"Bagging discretizado\": bagging_clf_ds,\n    \"Random Forests discretizado\": random_forest_clf_ds,\n    \"Gradient Boosting discretizado\": gradient_boosting_clf_ds\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"utils.evaluate_estimators(estimators, X, y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lo primero a destacar es que discretizano obtenemos peores resultados en `Diabetes`, eso si, algunas discretizaciones son mejores que otros modelos sin discretizar y podrían tenerse en cuenta.\n\nAdemás en los resultados marcábamos a `gradient_boosting_model` como el mejor, mientras que después de evaluar, acaba siendo `bagging_model` seguido muy de cerca de `random_forests_model`, como el mejor clasificador con un total de un 85%.\n\nOtro factor a tener cuenta, que hemos descartado tanto aquí como en `Wisconsin` es el tiempo, ya que estos conjuntos de datos son muy pequeños, luego al final acabamos despreciando los tiempos, cosa que en conjuntos de datos mucho mayores debe tenerse muy en cuenta."},{"metadata":{},"cell_type":"markdown","source":"# 3. Wisconsin"},{"metadata":{},"cell_type":"markdown","source":"## 3.1 Carga de datos"},{"metadata":{},"cell_type":"markdown","source":"Cargamos datos y comprobamos que se han cargado correctamente"},{"metadata":{"trusted":true},"cell_type":"code","source":"filepath= \"../input/breast-cancer-wisconsin-data/data.csv\"\ntarget = \"diagnosis\"\n\ndata = utils.load_data(filepath,None,target)\ndata = data.drop(columns=['Unnamed: 32'])\npd.get_dummies(data, \"diagnosis\")\ndata = pd.get_dummies(data, columns = [\"diagnosis\"], drop_first = True)\ndata.rename(columns={'diagnosis_M': 'diagnosis'}, inplace=True)\ndata.index=range(data.shape[0])\n\ndata.sample(8,random_state=random_state)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A continuación, obtendremos nuestras variables predictoras por un lado y la variable objetivo por otro."},{"metadata":{"trusted":true},"cell_type":"code","source":"target = \"diagnosis\"\n\n(X, y) = utils.divide_dataset(data, target)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ahora comprobamos que se han separado correctamente:"},{"metadata":{},"cell_type":"markdown","source":"**Variables predictoras**"},{"metadata":{"trusted":true},"cell_type":"code","source":"X.sample(5, random_state=random_state)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Variable clase**"},{"metadata":{"trusted":true},"cell_type":"code","source":"y.sample(5, random_state=random_state)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ahora, divididiremos el conjunto de datos en entrenamiento y prueba mediante un *holdout* estratificado (70% train y 30% test):"},{"metadata":{"trusted":true},"cell_type":"code","source":"stratify = y\ntrain_size = 0.7\n\n(X_train, X_test, y_train, y_test) = train_test_split(X, y,\n                                                      stratify=stratify,\n                                                      train_size=train_size,\n                                                      random_state=random_state)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ahora comprobamos que hemos separado correctamente el conjunto en train y test:"},{"metadata":{},"cell_type":"markdown","source":"**Train - Variables Predictoras**"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.sample(5, random_state=random_state)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Train - Variable Clase**"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train.sample(5, random_state=random_state)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Test - Variables Predictoras**"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test.sample(5, random_state=random_state)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Tes - Variable Clase**"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test.sample(5, random_state=random_state)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Establecemos la 'X' y la 'y' de esta forma por comodidad para los modelos\n\nX = X_train\ny = y_train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Parámetros del pipeline**"},{"metadata":{"trusted":true},"cell_type":"code","source":"eliminar_outliers = FunctionSampler(func=utils.outlier_rejection)#para cambiar la seed, en el utils\n\ndiscretizer = KBinsDiscretizer(n_bins=3, strategy=\"quantile\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3.2 Selección de modelos"},{"metadata":{},"cell_type":"markdown","source":"Para este conjunto de datos utilizaresmo los mismos hiperparámetros que en el conjunto de datos anterior para compararlos, a excepción de `n_neighbors` de `k_neighbors_model` debido a que hemos seguido un array secuencial que tiene como valor máximo la raíz del número de datos a tratar y de `max_depth` en los modelos que lo implementen debido a que el árbol es distinto (por razones obvias)."},{"metadata":{},"cell_type":"markdown","source":"### 3.2.1 Vecinos más cercanos"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Modelo\nestimator = clone(k_neighbors_model)\n\n#Hiperparámetros\nweights = [\"uniform\", \"distance\"]\nn_neighbors = [3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]\nmetric = [\"euclidean\",\"manhattan\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Sin discretizar**"},{"metadata":{"trusted":true},"cell_type":"code","source":"pipeline = make_pipeline(eliminar_outliers, estimator)\n\nk_neighbors_clf = utils.optimize_params(pipeline,\n                                        X, y, cv,\n                                        kneighborsclassifier__weights=weights,\n                                        kneighborsclassifier__n_neighbors=n_neighbors,\n                                        kneighborsclassifier__metric=metric)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Podemos observar que sin discretizar, conseguimos un score similar al de diabetes en este modelo, salvo que en este caso `n_neighbors` ha sido menor y el `weights` escogido ha sido `distance`.\n\nEn esta conjunto de datos, pasa lo mismo que el anterior en `metric`, `manhattan` sigue predominando y la mayor diferencia que podemos destacar es que `wights=distance` tiene más peso en los mejores casos en este conjunto de datos que en el anterior.\n\nLos puestos obtendios:\n\n* Puesto `Accuracy` = 1\n* Puesto `AUC` = 9\n* Puesto `Recall` = 1"},{"metadata":{},"cell_type":"markdown","source":"**Discretizado**"},{"metadata":{"trusted":true},"cell_type":"code","source":"pipeline = make_pipeline(eliminar_outliers, discretizer, estimator)\n\nk_neighbors_clf_ds = utils.optimize_params(pipeline,\n                                        X, y, cv,\n                                        kneighborsclassifier__weights=weights,\n                                        kneighborsclassifier__n_neighbors=n_neighbors,\n                                        kneighborsclassifier__metric=metric)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lo más llamativo de este caso es el score, conseguimos un gran score de un 95% de acierto, en comparación con el otro conjunto de datos, donde discretizando obteníamos muy malos scores, esto puede deberse principalmente a que solo imputamos outliers en este conjunto de datos, además de que también poseemos 32 columnas y otro tipo de discretización. Además de que provoca un empate en el primer puesto, dado que ambas métricas consiguen el mismo resultado.\n\nSobre el mejor resultado, acaba siendo el mismo que en el caso del conjunto de datos anterior discretizando, solo que el `n_neighbors` en este caso es mayor, además las `metric` están más reñidas, y el mejor `weights` por diferencia es `distance` si tenemos en cuenta el empate, ya que ocupa los primeros puestos.\n\nLos puestos obtendios:\n\n* Puesto `Accuracy` = 7\n* Puesto `AUC` = 45\n* Puesto `Recall` = 1"},{"metadata":{},"cell_type":"markdown","source":"### 3.2.2 Árboles de decisión"},{"metadata":{},"cell_type":"markdown","source":"**Árbol totalmente expandido**"},{"metadata":{"trusted":true},"cell_type":"code","source":"utils.plot_tree(decision_tree_model, X, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Modelo\nestimator = clone(decision_tree_model)\n\n#Hiperparámetros\ncriterion = [\"gini\", \"entropy\"]\nmax_depth = [2, 3, 4]\nccp_alpha = [0.0, 0.015, 0.025, 0.035]\nsplitter = [\"best\", \"random\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Sin discretizar**"},{"metadata":{"trusted":true},"cell_type":"code","source":"pipeline = make_pipeline(eliminar_outliers, estimator)\n\ndecision_tree_clf = utils.optimize_params(pipeline,\n                                          X, y, cv,\n                                          decisiontreeclassifier__criterion=criterion,\n                                          decisiontreeclassifier__max_depth=max_depth,\n                                          decisiontreeclassifier__ccp_alpha=ccp_alpha,\n                                          decisiontreeclassifier__splitter=splitter)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Respecto al modelo anterior sin discretizar, conseguimos un gran salto en el score, también se debe entre otras cosas a la complejidad del árbol, ya que si nos fijamos es un árbol bastante simple, no muy complejo como en el caso anterior.\n\nEn el modelo de `Diabetes` sin discretizar también coincide en que el mejor `criterio=entropy` y `splitter=best`, es normal que no coincida el `max_depth` como ya hemos comentado, y aquí utiliza un valor de poda superior.\n\nLos puestos obtendios:\n\n* Puesto `Accuracy` = 2\n* Puesto `AUC` = 25\n* Puesto `Recall` = 1"},{"metadata":{},"cell_type":"markdown","source":"**Discretizado**"},{"metadata":{"trusted":true},"cell_type":"code","source":"pipeline = make_pipeline(eliminar_outliers, discretizer, estimator)\n\ndecision_tree_clf_ds = utils.optimize_params(pipeline,\n                                          X, y, cv,\n                                          decisiontreeclassifier__criterion=criterion,\n                                          decisiontreeclassifier__max_depth=max_depth,\n                                          decisiontreeclassifier__ccp_alpha=ccp_alpha,\n                                          decisiontreeclassifier__splitter=splitter)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Discretizando también conseguimos un buen score, en este caso pierde contra el modelo sin discretizar pero ambos consiguen un score superior a los 2 obtenidos en diabetes.\n\nAdemás lo único que cambia en las 2 mejores scores es el `max_depth`.\n\nAquí el `criterion=entropy`es el más utilizado en los mejores casos, como sucede sin discretizar, `splitter` se comporta en ambos conjuntos de datos igual.\n\nComparando con la discretización de este modelo en diabetes, aquí poda y el `criterion` es distinto.\n\nLos puestos obtendios:\n\n* Puesto `Accuracy` = 6\n* Puesto `AUC` = 5\n* Puesto `Recall` = 1"},{"metadata":{},"cell_type":"markdown","source":"### 3.2.3 Adaptative Boosting (AdaBoost)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Modelo\nestimator = clone(adaboost_model)\nbase_estimator = clone(decision_tree_model)\n\n#Hiperparámetros\nbase_estimator = [base_estimator]\nlearning_rate = [0.85, 0.95, 1.0]\ncriterion = [\"gini\", \"entropy\"]\nmax_depth = [1, 2, 3, 4]\nccp_alpha = [0.0, 0.015, 0.025, 0.035]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Sin discretizar**"},{"metadata":{"trusted":true},"cell_type":"code","source":"pipeline = make_pipeline(eliminar_outliers, estimator)\n\nadaboost_clf = utils.optimize_params(pipeline,\n                                     X, y, cv,\n                                     adaboostclassifier__base_estimator=base_estimator,\n                                     adaboostclassifier__learning_rate=learning_rate,\n                                     adaboostclassifier__base_estimator__criterion=criterion,\n                                     adaboostclassifier__base_estimator__max_depth=max_depth,\n                                     adaboostclassifier__base_estimator__ccp_alpha=ccp_alpha)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Adaboost también consigue un muy buen score, comparando este caso con el de `Diabetes`, los mejores scores solo coinciden en el `max_depth`, no es de extrañar ya que este clasificador trabaja con árboles pequeños, y los de profundidad 1 son muy comunes.\n\nComparando con `Diabetes`, también podemos observar que mientras que en `Diabetes` el `criterion=gini` predominaba en los mejores scores, aquí acaba predominando `criterion=entropy`.\n\nLos puestos obtendios:\n\n* Puesto `Accuracy` = 12\n* Puesto `AUC` = 34\n* Puesto `Recall` = 1"},{"metadata":{},"cell_type":"markdown","source":"**Discretizado**"},{"metadata":{"trusted":true},"cell_type":"code","source":"pipeline = make_pipeline(eliminar_outliers, discretizer, estimator)\n\nadaboost_clf_ds = utils.optimize_params(pipeline,\n                                     X, y, cv,\n                                     adaboostclassifier__base_estimator=base_estimator,\n                                     adaboostclassifier__learning_rate=learning_rate,\n                                     adaboostclassifier__base_estimator__criterion=criterion,\n                                     adaboostclassifier__base_estimator__max_depth=max_depth,\n                                     adaboostclassifier__base_estimator__ccp_alpha=ccp_alpha)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"El score discretizando acaba siendo peor que sin discretizar, aún así nos colocamos en un 94%, las diferencias respecto a no discretizar son que aquí `criterion=gini` (antes los mejores casos eran `entropy` y discretizando produce lo contrario, los mejores casos utilizan `criterion=gini`) y realizamos poda.\n\nLa única similitud que podemos sacar del caso discretizado en `Diabetes` es que `max_depth=1` en los mejores casos, cosa que se debe por el clasificador como ya hemos comentado.\n\nLos puestos obtendios:\n\n* Puesto `Accuracy` = 1\n* Puesto `AUC` = 43\n* Puesto `Recall` = 1"},{"metadata":{},"cell_type":"markdown","source":"### 3.2.4 Bootstrap Aggregating (Bagging)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Modelo\nestimator = clone(bagging_model)\nbase_estimator = clone(decision_tree_model)\n\n#Hiperparámetros\nbase_estimator = [base_estimator]\ncriterion = [\"gini\", \"entropy\"]\nn_estimators = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\nbootstrap_features = [True, False] \nbootstrap = [True, False] ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Sin discretizar**"},{"metadata":{"trusted":true},"cell_type":"code","source":"pipeline = make_pipeline(eliminar_outliers, estimator)\n\nbagging_clf = utils.optimize_params(pipeline,\n                                    X, y, cv,\n                                    baggingclassifier__base_estimator=base_estimator,\n                                    baggingclassifier__base_estimator__criterion=criterion,\n                                    baggingclassifier__n_estimators=n_estimators,\n                                    baggingclassifier__bootstrap_features =bootstrap_features,\n                                    baggingclassifier__bootstrap=bootstrap)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Seguimos con buenos scores, además aquí se nota que cuánto mayor es el `n_estimators`, mejor es el score obtenido, además, comparando con `Diabetes` obtenemos una gran diferencia relativa a `bootstrap` y `bootstrap_features`, ya que la combinación con mejores scores son ambas a `True`, si bien es verdad que en `Wisconsin` podemos notar buenos scores también con `criterion=gini` cosa que en `Diabetes` no ocurre ya que predomina `entropy`\n\nLos puestos obtendios:\n\n* Puesto `Accuracy` = 1\n* Puesto `AUC` = 6\n* Puesto `Recall` = 1"},{"metadata":{},"cell_type":"markdown","source":"**Discretizado**"},{"metadata":{"trusted":true},"cell_type":"code","source":"pipeline = make_pipeline(eliminar_outliers, discretizer, estimator)\n\nbagging_clf_ds = utils.optimize_params(pipeline,\n                                    X, y, cv,\n                                    baggingclassifier__base_estimator=base_estimator,\n                                    baggingclassifier__base_estimator__criterion=criterion,\n                                    baggingclassifier__n_estimators=n_estimators,\n                                    baggingclassifier__bootstrap_features =bootstrap_features,\n                                    baggingclassifier__bootstrap=bootstrap)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Discretizando seguimos con buenos scores, se sigue manteniendo que cuánto mayor es el `n_estimators` mejor es el score, aunque en este caso haya sido 90 y sin discretizar 100. Las diferencias entre los mejores casos discretizando y sin discretizar lo marcan realmente `criterion=entropy` (ambos casos ganadores lo poseen) quien predomina en este caso y `bootstrap_features` que en este caso podemos encontrar valores False en los mejores casos.\n\nEl mejor resultado acaba coincidiendo exacatamente con el obtenido en `Diabetes` discretizando a pesar de que encontramos diferencias en los 2 hiperparámetros comentados en la comparación con `Wisconsin` sin discretizar.\n\nLos puestos obtendios:\n\n* Puesto `Accuracy` = 1\n* Puesto `AUC` = 34\n* Puesto `Recall` = 1"},{"metadata":{},"cell_type":"markdown","source":"### 3.2.5 Random Forests"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Modelo\nestimator = clone(random_forest_model)\n\n#Hiperparámetros\ncriterion = [\"gini\", \"entropy\"]\nmax_features = [\"sqrt\", \"log2\"]\nn_estimators = [100, 150, 200, 250]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Sin discretizar**"},{"metadata":{"trusted":true},"cell_type":"code","source":"pipeline = make_pipeline(eliminar_outliers, estimator)\n\nrandom_forest_clf = utils.optimize_params(pipeline,\n                                          X, y, cv,\n                                          randomforestclassifier__criterion=criterion,\n                                          randomforestclassifier__max_features=max_features,\n                                          randomforestclassifier__n_estimators=n_estimators)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Seguimos manteniendo buen score, los hiperparámetros se encuentran bastante reñidos en general en los mejores casos, aunque aquí tenemos el caso peculiar de que aunque aumentemos el `n_estimators`, en este caso el mejor acaba siendo el valor por defecto que quedó en segunda posicioon en `Diabetes` sin discretizar `n_estimators=100`.\n\n\nLos puestos:\n\n* Puesto `Accuracy` = 1\n* Puesto `AUC` = 9\n* Puesto `Recall` = 4"},{"metadata":{},"cell_type":"markdown","source":"**Discretizado**"},{"metadata":{"trusted":true},"cell_type":"code","source":"pipeline = make_pipeline(eliminar_outliers, discretizer, estimator)\n\nrandom_forest_clf_ds = utils.optimize_params(pipeline,\n                                          X, y, cv,\n                                          randomforestclassifier__criterion=criterion,\n                                          randomforestclassifier__max_features=max_features,\n                                          randomforestclassifier__n_estimators=n_estimators)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Aunque empeora un poco el score, tampoco lo podemos calificar de malo, destacar que respecto al caso sin discretizar, los hiperparámetros de los mejores casos, se comportan igual, salvo `n_estimators` ya que aquí si que tienen mayor peso los valores más grandes. \n\nEste comportamiento es similar al obtenido en los mejores resultados de `Diabetes` discretizando, con la salvedad de que en `Diabetes` el segundo puesto tiene `n_estimators=100`\n\nLos puestos:\n\n* Puesto `Accuracy` = 1\n* Puesto `AUC` = 3\n* Puesto `Recall` = 2"},{"metadata":{},"cell_type":"markdown","source":"### 3.2.6 Gradient Tree Boosting (Gradient Boosting)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Modelo\nestimator = clone(gradient_boosting_model)\n\n#Hiperparámetros\nlearning_rate = [0.01, 0.05, 0.1]\ncriterion = [\"friedman_mse\", \"mse\"]\nmax_depth = [1, 2, 3, 4]\nccp_alpha = [0.0, 0.5, 1]\nn_estimators = [100, 150]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Sin discretizar**"},{"metadata":{"trusted":true},"cell_type":"code","source":"pipeline = make_pipeline(eliminar_outliers, estimator)\n\ngradient_boosting_clf = utils.optimize_params(pipeline,\n                                              X, y, cv,\n                                              gradientboostingclassifier__learning_rate=learning_rate,\n                                              gradientboostingclassifier__criterion=criterion,\n                                              gradientboostingclassifier__max_depth=max_depth,\n                                              gradientboostingclassifier__n_estimators=n_estimators,\n                                              gradientboostingclassifier__ccp_alpha=ccp_alpha)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Seguimos manteniendo buenos scores.\n\nPodemos observar que los mejores parámetros no utilizan poda, no pasan de`max_depth=3` al igual que en `Diabetes`.\n\nEl `learning_rate=0.1` es el más utilizado en los mejores casos.\n\nTambién podemos observar que ne `Wisconsin`el hiperparámetro proporcionado de la práctica `criterion` en los mejores valores no es irrelevante, ya que no produce cambios aunque sean muy pequeños.\n\nLos puestos\n\n* Puesto `Accuracy` = 1\n* Puesto `AUC` = 1\n* Puesto `Recall` = 1"},{"metadata":{},"cell_type":"markdown","source":"**Discretizado**"},{"metadata":{"trusted":true},"cell_type":"code","source":"pipeline = make_pipeline(eliminar_outliers, discretizer, estimator)\n\ngradient_boosting_clf_ds = utils.optimize_params(pipeline,\n                                              X, y, cv,\n                                              gradientboostingclassifier__learning_rate=learning_rate,\n                                              gradientboostingclassifier__criterion=criterion,\n                                              gradientboostingclassifier__max_depth=max_depth,\n                                              gradientboostingclassifier__n_estimators=n_estimators,\n                                              gradientboostingclassifier__ccp_alpha=ccp_alpha)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Discretizando seguimos obteniendo buenos scores, aquí los mejores casos siguen mantiendo el criterio de no podar.\n\n\nEl resto de hiperparámetros se comportan de la misma forma que sin discretizar, salvo que en `max_depth` si seguimos mateniendo que los mejores casos utilizan profundidades de 1 a 3, estos incluidos, lo cumplen de manera descendente, es decir de 3 a 1.\n\nLos puestos:\n\n* Puesto `Accuracy` = 1\n* Puesto `AUC` = 17\n* Puesto `Recall` = 1"},{"metadata":{},"cell_type":"markdown","source":"### 3.2.7 Histogram-Based Gradient Boosting (Histogram Gradient Boosting)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Modelo\nestimator = clone(hist_gradient_boosting_model)\n\n#Hiperparámetros\nlearning_rate = [0.01, 0.02, 0.03, 0.04, 0.05]\nmax_leaf_nodes = [15, 31, 65, 127]\nmax_iter = [100, 150, 200]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Sin discretizar**"},{"metadata":{"trusted":true},"cell_type":"code","source":"pipeline = make_pipeline(eliminar_outliers, estimator)\n\nhist_gradient_boosting_clf = utils.optimize_params(pipeline,\n                                                   X, y, cv,\n                                                   histgradientboostingclassifier__learning_rate=learning_rate,\n                                                   histgradientboostingclassifier__max_iter=max_iter,\n                                                   histgradientboostingclassifier__max_leaf_nodes=max_leaf_nodes)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Seguimos obteniendo buenos scores, los mejores casos los encontramos con los valores más altos de `max_iter` además de que el mejor `learning_rate=0.05` mientras que en el resto de hiperparámetros encontramos más variedad en los mejores casos.\n\nLos puestos:\n\n* Puesto `Accuracy` = 1\n* Puesto `AUC` = 9\n* Puesto `Recall` = 1"},{"metadata":{},"cell_type":"markdown","source":"## 3.3 Construcción y validación del modelo final"},{"metadata":{"trusted":true},"cell_type":"code","source":"estimators = {\n    \"Nearest neighbors\": k_neighbors_clf,\n    \"Decision tree\": decision_tree_clf,\n    \"AdaBoost\": adaboost_clf,\n    \"Bagging\": bagging_clf,\n    \"Random Forests\": random_forest_clf,\n    \"Gradient Boosting\": gradient_boosting_clf,\n    \"Histogram Gradient Boosting\": hist_gradient_boosting_clf,\n    \"Nearest neighbors discretizado\": k_neighbors_clf_ds,\n    \"Decision tree discretizado\": decision_tree_clf_ds,\n    \"AdaBoost discretizado\": adaboost_clf_ds,\n    \"Bagging discretizado\": bagging_clf_ds,\n    \"Random Forests discretizado\": random_forest_clf_ds,\n    \"Gradient Boosting discretizado\": gradient_boosting_clf_ds\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"utils.evaluate_estimators(estimators, X, y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lo primero a destacar es que discretizano obtenemos mucho mejores resultados que en `Diabetes`, también destacar que disminuimos el n_bins debido a que sobreajustaba más de la cuenta y podemos ver que en algunos de `Wisconsin`, destacando el metodo de validación de `AUC` se sobreajusta bastante.\n\nEl mejor clasificador acaba siendo un triple empate entre `adabbost_model`, `bagging_model` y `Random_forests_model`. Luego por tiempos, aunque hayamos dicho que son despreciables en este conjunto de datos, nos quedaríamos con `adabbost_model`."},{"metadata":{},"cell_type":"markdown","source":"# 4. Estudio de un kernel de Kaggle"},{"metadata":{},"cell_type":"markdown","source":"Este kernel de Kaggle, está dedicado a personas que estás comenzando a utilizar la ciencia de datos, básicamente es un tutorial muy completo para que podamos conseguir bastante experiencia en este ámbito. (no ha dado tiempo a comprobar su ejecución)\n\n[A Data Science Framework: To Achieve 99% Accuracy](https://www.kaggle.com/ldfreeman3/a-data-science-framework-to-achieve-99-accuracy/notebook)"},{"metadata":{},"cell_type":"markdown","source":"**Tabla de contenidos**"},{"metadata":{},"cell_type":"markdown","source":"* Capítulo 1 - Definir el problema\n* Capítulo 2 - Preparar los datos para el consumo\n* Capítulo 3 - Limpieza de datos: corregir, completar, crear y convertir\n* Capítulo 4 - Datos del modelo\n* Capítulo 5 - Evaluar el desempeño del modelo\n* Capítulo 6 - Ajustar modelo con hiperparámetros\n* Capítulo 7 - Ajuste del modelo con selección de funciones\n* Capítulo 8 - Validar e implementar"},{"metadata":{},"cell_type":"markdown","source":"## 4.1 Definir el problema "},{"metadata":{},"cell_type":"markdown","source":"Nos encontramos ante un probema basado en el incidente de Titanic, debemos desarrollar un algoritmo para predecir los supervivientes de este navío. [DATASET](https://www.kaggle.com/c/titanic/data)"},{"metadata":{},"cell_type":"markdown","source":"## 4.2 Preparar los datos para el consumo "},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#COMIENZA REALIZANDO LA CARGA DE LIBERÍAS PARA EL PREPROCESAMIENTO\n\nimport sys #access to system parameters https://docs.python.org/3/library/sys.html\nprint(\"Python version: {}\". format(sys.version))\n\nimport pandas as pd #collection of functions for data processing and analysis modeled after R dataframes with SQL like features\nprint(\"pandas version: {}\". format(pd.__version__))\n\nimport matplotlib #collection of functions for scientific and publication-ready visualization\nprint(\"matplotlib version: {}\". format(matplotlib.__version__))\n\nimport numpy as np #foundational package for scientific computing\nprint(\"NumPy version: {}\". format(np.__version__))\n\nimport scipy as sp #collection of functions for scientific computing and advance mathematics\nprint(\"SciPy version: {}\". format(sp.__version__)) \n\nimport IPython\nfrom IPython import display #pretty printing of dataframes in Jupyter notebook\nprint(\"IPython version: {}\". format(IPython.__version__)) \n\nimport sklearn #collection of machine learning algorithms\nprint(\"scikit-learn version: {}\". format(sklearn.__version__))\n\n#misc libraries\nimport random\nimport time\n\n\n#ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')\nprint('-'*25)\n\n\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\n\n#ADEMÁS YA OBTIENE EL TRAIN Y EL TEST DIVIDIDO COMO PODEMOS COMPROBAR EN ELOS ARCHIBOS DE TITANIC\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#AQUÍ PODEMOS VER QUE CARGA LIBRERÍAS SOBRE MODELOS Y \n#GRÁFICAS PARA EL ANÁLISIS ANTES DE EJECUTAR LOS MODELOS\n\n#Common Model Algorithms\nfrom sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\nfrom xgboost import XGBClassifier\n\n#Common Model Helpers\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn import feature_selection\nfrom sklearn import model_selection\nfrom sklearn import metrics\n\n#Visualization\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport matplotlib.pylab as pylab\nimport seaborn as sns\n\n#Configure Visualization Defaults\n#%matplotlib inline = show plots in Jupyter Notebook browser\n%matplotlib inline\nmpl.style.use('ggplot')\nsns.set_style('white')\npylab.rcParams['figure.figsize'] = 12,8","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4.3 Limpieza de datos: corregir, completar, crear y convertir"},{"metadata":{},"cell_type":"markdown","source":"Aquí realiza la carga de los datos y podemos observar que tiene un total de 891 entradas, además de que muchas variables predictoras tienen valores núlos, la variable objetivo es `Survived` y puede tomar el valor 0 o 1, luego estamos ante un problema categórico."},{"metadata":{"trusted":true},"cell_type":"code","source":"#import data from file: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html\ndata_raw = pd.read_csv('../input/titanic/train.csv')\n\n\n#a dataset should be broken into 3 splits: train, test, and (final) validation\n#the test file provided is the validation file for competition submission\n#we will split the train set into train and test data in future sections\ndata_val  = pd.read_csv('../input/titanic/test.csv')\n\n\n#to play with our data we'll create a copy\n#remember python assignment or equal passes by reference vs values, so we use the copy function: https://stackoverflow.com/questions/46327494/python-pandas-dataframe-copydeep-false-vs-copydeep-true-vs\ndata1 = data_raw.copy(deep = True)\n\n#however passing by reference is convenient, because we can clean both datasets at once\ndata_cleaner = [data1, data_val]\n\n\n#preview data\nprint (data_raw.info()) #https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.info.html\n#data_raw.head() #https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.head.html\n#data_raw.tail() #https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.tail.html\ndata_raw.sample(10) #https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sample.html","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Aquí observaresmos lo aneterior comentado, observa qué variables tienen valores nulos y cuántos son, tanto en test como en train.\n\nLas columnas son `Age`, `Cabin` y `Embarked` en train y `Age`, `Cabin` y `Fare` en test."},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Train columns with null values:\\n', data1.isnull().sum())\nprint(\"-\"*10)\n\nprint('Test/Validation columns with null values:\\n', data_val.isnull().sum())\nprint(\"-\"*10)\n\ndata_raw.describe(include = 'all')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ahora pasa a la limpieza de los datos, obtando por rellenar los nulos de `Age`, `Fare` por la media de los valores de esas columnas.\n\nLas columnas `PassengerID` y `Ticket` son eliminadas debido a que son ids que no aportan nada.\n\n`Sex` y `Embarked` son realmente variables binarias, por lo que las establecerá por 1's y 0's"},{"metadata":{"trusted":true},"cell_type":"code","source":"###COMPLETING: complete or delete missing values in train and test/validation dataset\nfor dataset in data_cleaner:    \n    #complete missing age with median\n    dataset['Age'].fillna(dataset['Age'].median(), inplace = True)\n\n    #complete embarked with mode\n    dataset['Embarked'].fillna(dataset['Embarked'].mode()[0], inplace = True)\n\n    #complete missing fare with median\n    dataset['Fare'].fillna(dataset['Fare'].median(), inplace = True)\n    \n#delete the cabin feature/column and others previously stated to exclude in train dataset\ndrop_column = ['PassengerId','Cabin', 'Ticket']\ndata1.drop(drop_column, axis=1, inplace = True)\n\nprint(data1.isnull().sum())\nprint(\"-\"*10)\nprint(data_val.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Los datos nos proporcionan columnas relacionas con nombres (pudiendo derivar de aquí géneros del título, tamaños de la familia según el apelido y el SES de títulos como médico o maestro), los cuales poseen apellidos, número de padres, número de hijos, número de hermanos, número de cónyuges.\n\nAhora utilizará estos datos para crear nuevas columnas que podrán generar información que le interese."},{"metadata":{"trusted":true},"cell_type":"code","source":"###CREATE: Feature Engineering for train and test/validation dataset\nfor dataset in data_cleaner:    \n    #Discrete variables\n    dataset['FamilySize'] = dataset ['SibSp'] + dataset['Parch'] + 1\n\n    dataset['IsAlone'] = 1 #initialize to yes/1 is alone\n    dataset['IsAlone'].loc[dataset['FamilySize'] > 1] = 0 # now update to no/0 if family size is greater than 1\n\n    #quick and dirty code split title from name: http://www.pythonforbeginners.com/dictionary/python-split\n    dataset['Title'] = dataset['Name'].str.split(\", \", expand=True)[1].str.split(\".\", expand=True)[0]\n\n\n    #Continuous variable bins; qcut vs cut: https://stackoverflow.com/questions/30211923/what-is-the-difference-between-pandas-qcut-and-pandas-cut\n    #Fare Bins/Buckets using qcut or frequency bins: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.qcut.html\n    dataset['FareBin'] = pd.qcut(dataset['Fare'], 4)\n\n    #Age Bins/Buckets using cut or value bins: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.cut.html\n    dataset['AgeBin'] = pd.cut(dataset['Age'].astype(int), 5)\n\n\n    \n#cleanup rare title names\n#print(data1['Title'].value_counts())\nstat_min = 10 #while small is arbitrary, we'll use the common minimum in statistics: http://nicholasjjackson.com/2012/03/08/sample-size-is-10-a-magic-number/\ntitle_names = (data1['Title'].value_counts() < stat_min) #this will create a true false series with title name as index\n\n#apply and lambda functions are quick and dirty code to find and replace with fewer lines of code: https://community.modeanalytics.com/python/tutorial/pandas-groupby-and-python-lambda-functions/\ndata1['Title'] = data1['Title'].apply(lambda x: 'Misc' if title_names.loc[x] == True else x)\nprint(data1['Title'].value_counts())\nprint(\"-\"*10)\n\n\n#preview data again\ndata1.info()\ndata_val.info()\ndata1.sample(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ahora realizará los cambios necesarios para realizar esta ingeniería de funciones, como transformar los datos categóricos en `dummy variables` para los análisis matemáticos"},{"metadata":{"trusted":true},"cell_type":"code","source":"#CONVERT: convert objects to category using Label Encoder for train and test/validation dataset\n\n#code categorical data\nlabel = LabelEncoder()\nfor dataset in data_cleaner:    \n    dataset['Sex_Code'] = label.fit_transform(dataset['Sex'])\n    dataset['Embarked_Code'] = label.fit_transform(dataset['Embarked'])\n    dataset['Title_Code'] = label.fit_transform(dataset['Title'])\n    dataset['AgeBin_Code'] = label.fit_transform(dataset['AgeBin'])\n    dataset['FareBin_Code'] = label.fit_transform(dataset['FareBin'])\n\n\n#define y variable aka target/outcome\nTarget = ['Survived']\n\n#define x variables for original features aka feature selection\ndata1_x = ['Sex','Pclass', 'Embarked', 'Title','SibSp', 'Parch', 'Age', 'Fare', 'FamilySize', 'IsAlone'] #pretty name/values for charts\ndata1_x_calc = ['Sex_Code','Pclass', 'Embarked_Code', 'Title_Code','SibSp', 'Parch', 'Age', 'Fare'] #coded for algorithm calculation\ndata1_xy =  Target + data1_x\nprint('Original X Y: ', data1_xy, '\\n')\n\n\n#define x variables for original w/bin features to remove continuous variables\ndata1_x_bin = ['Sex_Code','Pclass', 'Embarked_Code', 'Title_Code', 'FamilySize', 'AgeBin_Code', 'FareBin_Code']\ndata1_xy_bin = Target + data1_x_bin\nprint('Bin X Y: ', data1_xy_bin, '\\n')\n\n\n#define x and y variables for dummy features original\ndata1_dummy = pd.get_dummies(data1[data1_x])\ndata1_x_dummy = data1_dummy.columns.tolist()\ndata1_xy_dummy = Target + data1_x_dummy\nprint('Dummy X Y: ', data1_xy_dummy, '\\n')\n\n\n\ndata1_dummy.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ahora que ha limpiado los datos, vuelve a comprobarlos"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Train columns with null values: \\n', data1.isnull().sum())\nprint(\"-\"*10)\nprint (data1.info())\nprint(\"-\"*10)\n\nprint('Test/Validation columns with null values: \\n', data_val.isnull().sum())\nprint(\"-\"*10)\nprint (data_val.info())\nprint(\"-\"*10)\n\ndata_raw.describe(include = 'all')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#split train and test data with function defaults\n#random_state -> seed or control random number generator: https://www.quora.com/What-is-seed-in-random-number-generation\ntrain1_x, test1_x, train1_y, test1_y = model_selection.train_test_split(data1[data1_x_calc], data1[Target], random_state = 0)\ntrain1_x_bin, test1_x_bin, train1_y_bin, test1_y_bin = model_selection.train_test_split(data1[data1_x_bin], data1[Target] , random_state = 0)\ntrain1_x_dummy, test1_x_dummy, train1_y_dummy, test1_y_dummy = model_selection.train_test_split(data1_dummy[data1_x_dummy], data1[Target], random_state = 0)\n\n\nprint(\"Data1 Shape: {}\".format(data1.shape))\nprint(\"Train1 Shape: {}\".format(train1_x.shape))\nprint(\"Test1 Shape: {}\".format(test1_x.shape))\n\ntrain1_x_bin.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ahora realizaría un anális exploratorio el cuál no vamos a tratar porque es bastante extenso y nos interesa más lo siguiente, aún así debemos tener en cuenta que es un paso muy importante.\n\nEs interesante verlo y estudiarlo detenidamente ya que utiliza diagramas muy variados y pueden resultar algo complejos."},{"metadata":{},"cell_type":"markdown","source":"## 4.4 Datos del modelo"},{"metadata":{},"cell_type":"markdown","source":"Ahora estudia varios modelos y métodos de validación a través de una validación cruzada de 10 repeticiones con 3 carpetas test y 6 carpetas train"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Machine Learning Algorithm (MLA) Selection and Initialization\nMLA = [\n    #Ensemble Methods\n    ensemble.AdaBoostClassifier(),\n    ensemble.BaggingClassifier(),\n    ensemble.ExtraTreesClassifier(),\n    ensemble.GradientBoostingClassifier(),\n    ensemble.RandomForestClassifier(),\n\n    #Gaussian Processes\n    gaussian_process.GaussianProcessClassifier(),\n    \n    #GLM\n    linear_model.LogisticRegressionCV(),\n    linear_model.PassiveAggressiveClassifier(),\n    linear_model.RidgeClassifierCV(),\n    linear_model.SGDClassifier(),\n    linear_model.Perceptron(),\n    \n    #Navies Bayes\n    naive_bayes.BernoulliNB(),\n    naive_bayes.GaussianNB(),\n    \n    #Nearest Neighbor\n    neighbors.KNeighborsClassifier(),\n    \n    #SVM\n    svm.SVC(probability=True),\n    svm.NuSVC(probability=True),\n    svm.LinearSVC(),\n    \n    #Trees    \n    tree.DecisionTreeClassifier(),\n    tree.ExtraTreeClassifier(),\n    \n    #Discriminant Analysis\n    discriminant_analysis.LinearDiscriminantAnalysis(),\n    discriminant_analysis.QuadraticDiscriminantAnalysis(),\n\n    \n    #xgboost: http://xgboost.readthedocs.io/en/latest/model.html\n    XGBClassifier()    \n    ]\n\n\n\n#split dataset in cross-validation with this splitter class: http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.ShuffleSplit.html#sklearn.model_selection.ShuffleSplit\n#note: this is an alternative to train_test_split\ncv_split = model_selection.ShuffleSplit(n_splits = 10, test_size = .3, train_size = .6, random_state = 0 ) # run model 10x with 60/30 split intentionally leaving out 10%\n\n#create table to compare MLA metrics\nMLA_columns = ['MLA Name', 'MLA Parameters','MLA Train Accuracy Mean', 'MLA Test Accuracy Mean', 'MLA Test Accuracy 3*STD' ,'MLA Time']\nMLA_compare = pd.DataFrame(columns = MLA_columns)\n\n#create table to compare MLA predictions\nMLA_predict = data1[Target]\n\n#index through MLA and save performance to table\nrow_index = 0\nfor alg in MLA:\n\n    #set name and parameters\n    MLA_name = alg.__class__.__name__\n    MLA_compare.loc[row_index, 'MLA Name'] = MLA_name\n    MLA_compare.loc[row_index, 'MLA Parameters'] = str(alg.get_params())\n    \n    #score model with cross validation: http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html#sklearn.model_selection.cross_validate\n    cv_results = model_selection.cross_validate(alg, data1[data1_x_bin], data1[Target], cv  = cv_split)\n\n    MLA_compare.loc[row_index, 'MLA Time'] = cv_results['fit_time'].mean()\n    MLA_compare.loc[row_index, 'MLA Train Accuracy Mean'] = cv_results['train_score'].mean()\n    MLA_compare.loc[row_index, 'MLA Test Accuracy Mean'] = cv_results['test_score'].mean()   \n    #if this is a non-bias random sample, then +/-3 standard deviations (std) from the mean, should statistically capture 99.7% of the subsets\n    MLA_compare.loc[row_index, 'MLA Test Accuracy 3*STD'] = cv_results['test_score'].std()*3   #let's know the worst that can happen!\n    \n\n    #save MLA predictions - see section 6 for usage\n    alg.fit(data1[data1_x_bin], data1[Target])\n    MLA_predict[MLA_name] = alg.predict(data1[data1_x_bin])\n    \n    row_index+=1\n\n    \n#print and sort table: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sort_values.html\nMLA_compare.sort_values(by = ['MLA Test Accuracy Mean'], ascending = False, inplace = True)\nMLA_compare\n#MLA_predict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#barplot using https://seaborn.pydata.org/generated/seaborn.barplot.html\nsns.barplot(x='MLA Test Accuracy Mean', y = 'MLA Name', data = MLA_compare, color = 'm')\n\n#prettify using pyplot: https://matplotlib.org/api/pyplot_api.html\nplt.title('Machine Learning Algorithm Accuracy Score \\n')\nplt.xlabel('Accuracy Score (%)')\nplt.ylabel('Algorithm')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4.5 Evaluar el desempeño del modelo"},{"metadata":{},"cell_type":"markdown","source":"En base a unas cuestiones planteadas por el autor de la libreta, empieza un estudio de los datos que está tratando para ver qué hiperpárametos y modelos pueden ajustarse mejor al problema"},{"metadata":{},"cell_type":"markdown","source":"**Pregunta 1:** ¿Estabas en el Titanic? En caso afirmativo, la mayoría (62%) murió. Tenga en cuenta que la supervivencia de nuestra muestra es diferente a nuestra población del 68%. No obstante, si asumimos que todos murieron, la precisión de nuestra muestra es del 62%.\n\n**Pregunta 2:** ¿Eres hombre o mujer? Hombre, la mayoría (81%) murió. Mujeres, la mayoría (74%) sobrevivió. Dándonos una precisión del 79%.\n\n**Pregunta 3A** (bajando por la rama femenina con recuento = 314): ¿Estás en la clase 1, 2 o 3? Clase 1, la mayoría (97%) sobrevivió y Clase 2, la mayoría (92%) sobrevivió. Dado que el subgrupo muerto es menor que 10, dejaremos de bajar por esta rama. Clase 3, está incluso en una división 50-50. No se obtiene nueva información para mejorar nuestro modelo.\n\n**Pregunta 4A** (bajando por la rama femenina clase 3 con recuento = 144): ¿Embarcó desde el puerto C, Q o S? Obtenemos un poco de información. C y Q, la mayoría sobrevivió, así que no hubo cambios. Además, el subgrupo muerto es menor que 10, así que pararemos. S, la mayoría (63%) murió. Entonces, cambiaremos a las hembras, clase 3, embarcadas S de asumir que sobrevivieron a asumir que murieron. La precisión de nuestro modelo aumenta al 81%.\n\n**Pregunta 5A** (bajando por la rama femenina de clase 3 embarcada S con recuento = 88): Hasta ahora, parece que tomamos buenas decisiones. Agregar otro nivel no parece obtener mucha más información. Este subgrupo 55 murió y 33 sobrevivieron, dado que la mayoría murió, necesitamos encontrar una señal para identificar a los 33 o un subgrupo para cambiarlos de muertos a sobrevivientes y mejorar la precisión de nuestro modelo. Podemos jugar con nuestras funciones. Una que encontré fue la tarifa 0-8, la mayoría sobrevivió. Es un tamaño de muestra pequeño 11-9, pero se usa a menudo en estadísticas. Mejoramos ligeramente nuestra precisión, pero no mucho para pasar del 82%. Entonces, pararemos aquí.\n\n**Pregunta 3B** (bajando por la rama masculina con recuento = 577): Volviendo a la pregunta 2, sabemos que la mayoría de los hombres murieron. Entonces, estamos buscando una característica que identifique un subgrupo al que la mayoría sobrevivió. Sorprendentemente, la clase o incluso el embarque no importaba como lo hacía para las mujeres, pero el título sí lo hace y nos lleva al 82%. Adivina y revisando otras características, ninguna parece empujarnos más allá del 82%. Entonces, pararemos aquí por ahora."},{"metadata":{},"cell_type":"markdown","source":"A raíz de estas preguntas empieza a establecer un código para plasmarlo e intentar mejorarlo"},{"metadata":{"trusted":true},"cell_type":"code","source":"#IMPORTANT: This is a handmade model for learning purposes only.\n#However, it is possible to create your own predictive model without a fancy algorithm :)\n\n#coin flip model with random 1/survived 0/died\n\n#iterate over dataFrame rows as (index, Series) pairs: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.iterrows.html\nfor index, row in data1.iterrows(): \n    #random number generator: https://docs.python.org/2/library/random.html\n    if random.random() > .5:     # Random float x, 0.0 <= x < 1.0    \n        data1.set_value(index, 'Random_Predict', 1) #predict survived/1\n    else: \n        data1.set_value(index, 'Random_Predict', 0) #predict died/0\n    \n\n#score random guess of survival. Use shortcut 1 = Right Guess and 0 = Wrong Guess\n#the mean of the column will then equal the accuracy\ndata1['Random_Score'] = 0 #assume prediction wrong\ndata1.loc[(data1['Survived'] == data1['Random_Predict']), 'Random_Score'] = 1 #set to 1 for correct prediction\nprint('Coin Flip Model Accuracy: {:.2f}%'.format(data1['Random_Score'].mean()*100))\n\n#we can also use scikit's accuracy_score function to save us a few lines of code\n#http://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html#sklearn.metrics.accuracy_score\nprint('Coin Flip Model Accuracy w/SciKit: {:.2f}%'.format(metrics.accuracy_score(data1['Survived'], data1['Random_Predict'])*100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#group by or pivot table: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html\npivot_female = data1[data1.Sex=='female'].groupby(['Sex','Pclass', 'Embarked','FareBin'])['Survived'].mean()\nprint('Survival Decision Tree w/Female Node: \\n',pivot_female)\n\npivot_male = data1[data1.Sex=='male'].groupby(['Sex','Title'])['Survived'].mean()\nprint('\\n\\nSurvival Decision Tree w/Male Node: \\n',pivot_male)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#handmade data model using brain power (and Microsoft Excel Pivot Tables for quick calculations)\ndef mytree(df):\n    \n    #initialize table to store predictions\n    Model = pd.DataFrame(data = {'Predict':[]})\n    male_title = ['Master'] #survived titles\n\n    for index, row in df.iterrows():\n\n        #Question 1: Were you on the Titanic; majority died\n        Model.loc[index, 'Predict'] = 0\n\n        #Question 2: Are you female; majority survived\n        if (df.loc[index, 'Sex'] == 'female'):\n                  Model.loc[index, 'Predict'] = 1\n\n        #Question 3A Female - Class and Question 4 Embarked gain minimum information\n\n        #Question 5B Female - FareBin; set anything less than .5 in female node decision tree back to 0       \n        if ((df.loc[index, 'Sex'] == 'female') & \n            (df.loc[index, 'Pclass'] == 3) & \n            (df.loc[index, 'Embarked'] == 'S')  &\n            (df.loc[index, 'Fare'] > 8)\n\n           ):\n                  Model.loc[index, 'Predict'] = 0\n\n        #Question 3B Male: Title; set anything greater than .5 to 1 for majority survived\n        if ((df.loc[index, 'Sex'] == 'male') &\n            (df.loc[index, 'Title'] in male_title)\n            ):\n            Model.loc[index, 'Predict'] = 1\n        \n        \n    return Model\n\n\n#model data\nTree_Predict = mytree(data1)\nprint('Decision Tree Model Accuracy/Precision Score: {:.2f}%\\n'.format(metrics.accuracy_score(data1['Survived'], Tree_Predict)*100))\n\n\n#Accuracy Summary Report with http://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html#sklearn.metrics.classification_report\n#Where recall score = (true positives)/(true positive + false negative) w/1 being best:http://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html#sklearn.metrics.recall_score\n#And F1 score = weighted average of precision and recall w/1 being best: http://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score\nprint(metrics.classification_report(data1['Survived'], Tree_Predict))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plot Accuracy Summary\n#Credit: http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\nimport itertools\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n\n# Compute confusion matrix\ncnf_matrix = metrics.confusion_matrix(data1['Survived'], Tree_Predict)\nnp.set_printoptions(precision=2)\n\nclass_names = ['Dead', 'Survived']\n# Plot non-normalized confusion matrix\nplt.figure()\nplot_confusion_matrix(cnf_matrix, classes=class_names,\n                      title='Confusion matrix, without normalization')\n\n# Plot normalized confusion matrix\nplt.figure()\nplot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True, \n                      title='Normalized confusion matrix')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Este sería el resutado del modelo mental, llegando a un 82%, ahora intentará mejorarlo utilizando una validación cruzada junto con los modelos e hiperparámetros más adecuados a través de un grid, como el que utilizamos en esta práctica"},{"metadata":{},"cell_type":"markdown","source":"## 4.6 Ajustar modelo con hiperparámetros"},{"metadata":{},"cell_type":"markdown","source":"El modelo base, al igual que el nuestro, será un `decision_tree_model`"},{"metadata":{"trusted":true},"cell_type":"code","source":"#base model\ndtree = tree.DecisionTreeClassifier(random_state = 0)\nbase_results = model_selection.cross_validate(dtree, data1[data1_x_bin], data1[Target], cv  = cv_split)\ndtree.fit(data1[data1_x_bin], data1[Target])\n\nprint('BEFORE DT Parameters: ', dtree.get_params())\nprint(\"BEFORE DT Training w/bin score mean: {:.2f}\". format(base_results['train_score'].mean()*100)) \nprint(\"BEFORE DT Test w/bin score mean: {:.2f}\". format(base_results['test_score'].mean()*100))\nprint(\"BEFORE DT Test w/bin score 3*std: +/- {:.2f}\". format(base_results['test_score'].std()*100*3))\n#print(\"BEFORE DT Test w/bin set score min: {:.2f}\". format(base_results['test_score'].min()*100))\nprint('-'*10)\n\n\n#tune hyper-parameters: http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier\nparam_grid = {'criterion': ['gini', 'entropy'],  #scoring methodology; two supported formulas for calculating information gain - default is gini\n              #'splitter': ['best', 'random'], #splitting methodology; two supported strategies - default is best\n              'max_depth': [2,4,6,8,10,None], #max depth tree can grow; default is none\n              #'min_samples_split': [2,5,10,.03,.05], #minimum subset size BEFORE new split (fraction is % of total); default is 2\n              #'min_samples_leaf': [1,5,10,.03,.05], #minimum subset size AFTER new split split (fraction is % of total); default is 1\n              #'max_features': [None, 'auto'], #max features to consider when performing split; default none or all\n              'random_state': [0] #seed or control random number generator: https://www.quora.com/What-is-seed-in-random-number-generation\n             }\n\n#print(list(model_selection.ParameterGrid(param_grid)))\n\n#choose best model with grid_search: #http://scikit-learn.org/stable/modules/grid_search.html#grid-search\n#http://scikit-learn.org/stable/auto_examples/model_selection/plot_grid_search_digits.html\ntune_model = model_selection.GridSearchCV(tree.DecisionTreeClassifier(), param_grid=param_grid, scoring = 'roc_auc', cv = cv_split)\ntune_model.fit(data1[data1_x_bin], data1[Target])\n\n#print(tune_model.cv_results_.keys())\n#print(tune_model.cv_results_['params'])\nprint('AFTER DT Parameters: ', tune_model.best_params_)\n#print(tune_model.cv_results_['mean_train_score'])\nprint(\"AFTER DT Training w/bin score mean: {:.2f}\". format(tune_model.cv_results_['mean_train_score'][tune_model.best_index_]*100)) \n#print(tune_model.cv_results_['mean_test_score'])\nprint(\"AFTER DT Test w/bin score mean: {:.2f}\". format(tune_model.cv_results_['mean_test_score'][tune_model.best_index_]*100))\nprint(\"AFTER DT Test w/bin score 3*std: +/- {:.2f}\". format(tune_model.cv_results_['std_test_score'][tune_model.best_index_]*100*3))\nprint('-'*10)\n\n\n#duplicates gridsearchcv\n#tune_results = model_selection.cross_validate(tune_model, data1[data1_x_bin], data1[Target], cv  = cv_split)\n\n#print('AFTER DT Parameters: ', tune_model.best_params_)\n#print(\"AFTER DT Training w/bin set score mean: {:.2f}\". format(tune_results['train_score'].mean()*100)) \n#print(\"AFTER DT Test w/bin set score mean: {:.2f}\". format(tune_results['test_score'].mean()*100))\n#print(\"AFTER DT Test w/bin set score min: {:.2f}\". format(tune_results['test_score'].min()*100))\n#print('-'*10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4.7 Ajuste del modelo con selección de funciones"},{"metadata":{},"cell_type":"markdown","source":"Ahora lo que realiza es una selección de características, eliminando las recursivas."},{"metadata":{"trusted":true},"cell_type":"code","source":"#base model\nprint('BEFORE DT RFE Training Shape Old: ', data1[data1_x_bin].shape) \nprint('BEFORE DT RFE Training Columns Old: ', data1[data1_x_bin].columns.values)\n\nprint(\"BEFORE DT RFE Training w/bin score mean: {:.2f}\". format(base_results['train_score'].mean()*100)) \nprint(\"BEFORE DT RFE Test w/bin score mean: {:.2f}\". format(base_results['test_score'].mean()*100))\nprint(\"BEFORE DT RFE Test w/bin score 3*std: +/- {:.2f}\". format(base_results['test_score'].std()*100*3))\nprint('-'*10)\n\n\n\n#feature selection\ndtree_rfe = feature_selection.RFECV(dtree, step = 1, scoring = 'accuracy', cv = cv_split)\ndtree_rfe.fit(data1[data1_x_bin], data1[Target])\n\n#transform x&y to reduced features and fit new model\n#alternative: can use pipeline to reduce fit and transform steps: http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html\nX_rfe = data1[data1_x_bin].columns.values[dtree_rfe.get_support()]\nrfe_results = model_selection.cross_validate(dtree, data1[X_rfe], data1[Target], cv  = cv_split)\n\n#print(dtree_rfe.grid_scores_)\nprint('AFTER DT RFE Training Shape New: ', data1[X_rfe].shape) \nprint('AFTER DT RFE Training Columns New: ', X_rfe)\n\nprint(\"AFTER DT RFE Training w/bin score mean: {:.2f}\". format(rfe_results['train_score'].mean()*100)) \nprint(\"AFTER DT RFE Test w/bin score mean: {:.2f}\". format(rfe_results['test_score'].mean()*100))\nprint(\"AFTER DT RFE Test w/bin score 3*std: +/- {:.2f}\". format(rfe_results['test_score'].std()*100*3))\nprint('-'*10)\n\n\n#tune rfe model\nrfe_tune_model = model_selection.GridSearchCV(tree.DecisionTreeClassifier(), param_grid=param_grid, scoring = 'roc_auc', cv = cv_split)\nrfe_tune_model.fit(data1[X_rfe], data1[Target])\n\n#print(rfe_tune_model.cv_results_.keys())\n#print(rfe_tune_model.cv_results_['params'])\nprint('AFTER DT RFE Tuned Parameters: ', rfe_tune_model.best_params_)\n#print(rfe_tune_model.cv_results_['mean_train_score'])\nprint(\"AFTER DT RFE Tuned Training w/bin score mean: {:.2f}\". format(rfe_tune_model.cv_results_['mean_train_score'][tune_model.best_index_]*100)) \n#print(rfe_tune_model.cv_results_['mean_test_score'])\nprint(\"AFTER DT RFE Tuned Test w/bin score mean: {:.2f}\". format(rfe_tune_model.cv_results_['mean_test_score'][tune_model.best_index_]*100))\nprint(\"AFTER DT RFE Tuned Test w/bin score 3*std: +/- {:.2f}\". format(rfe_tune_model.cv_results_['std_test_score'][tune_model.best_index_]*100*3))\nprint('-'*10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Graph MLA version of Decision Tree: http://scikit-learn.org/stable/modules/generated/sklearn.tree.export_graphviz.html\nimport graphviz \ndot_data = tree.export_graphviz(dtree, out_file=None, \n                                feature_names = data1_x_bin, class_names = True,\n                                filled = True, rounded = True)\ngraph = graphviz.Source(dot_data) \ngraph","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4.8 Validación e implementación"},{"metadata":{},"cell_type":"markdown","source":"Para comparar y elegir la mejor combinación de algoritmos de predicción, establece un análisis a través de un diagrama de correlación entre algoritmos de predicción."},{"metadata":{"trusted":true},"cell_type":"code","source":"#compare algorithm predictions with each other, where 1 = exactly similar and 0 = exactly opposite\n#there are some 1's, but enough blues and light reds to create a \"super algorithm\" by combining them\ncorrelation_heatmap(MLA_predict)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Al final acaba escogiendo un `voting classifier` para poder utilizarlos todos."},{"metadata":{"trusted":true},"cell_type":"code","source":"#why choose one model, when you can pick them all with voting classifier\n#http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html\n#removed models w/o attribute 'predict_proba' required for vote classifier and models with a 1.0 correlation to another model\nvote_est = [\n    #Ensemble Methods: http://scikit-learn.org/stable/modules/ensemble.html\n    ('ada', ensemble.AdaBoostClassifier()),\n    ('bc', ensemble.BaggingClassifier()),\n    ('etc',ensemble.ExtraTreesClassifier()),\n    ('gbc', ensemble.GradientBoostingClassifier()),\n    ('rfc', ensemble.RandomForestClassifier()),\n\n    #Gaussian Processes: http://scikit-learn.org/stable/modules/gaussian_process.html#gaussian-process-classification-gpc\n    ('gpc', gaussian_process.GaussianProcessClassifier()),\n    \n    #GLM: http://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n    ('lr', linear_model.LogisticRegressionCV()),\n    \n    #Navies Bayes: http://scikit-learn.org/stable/modules/naive_bayes.html\n    ('bnb', naive_bayes.BernoulliNB()),\n    ('gnb', naive_bayes.GaussianNB()),\n    \n    #Nearest Neighbor: http://scikit-learn.org/stable/modules/neighbors.html\n    ('knn', neighbors.KNeighborsClassifier()),\n    \n    #SVM: http://scikit-learn.org/stable/modules/svm.html\n    ('svc', svm.SVC(probability=True)),\n    \n    #xgboost: http://xgboost.readthedocs.io/en/latest/model.html\n   ('xgb', XGBClassifier())\n\n]\n\n\n#Hard Vote or majority rules\nvote_hard = ensemble.VotingClassifier(estimators = vote_est , voting = 'hard')\nvote_hard_cv = model_selection.cross_validate(vote_hard, data1[data1_x_bin], data1[Target], cv  = cv_split)\nvote_hard.fit(data1[data1_x_bin], data1[Target])\n\nprint(\"Hard Voting Training w/bin score mean: {:.2f}\". format(vote_hard_cv['train_score'].mean()*100)) \nprint(\"Hard Voting Test w/bin score mean: {:.2f}\". format(vote_hard_cv['test_score'].mean()*100))\nprint(\"Hard Voting Test w/bin score 3*std: +/- {:.2f}\". format(vote_hard_cv['test_score'].std()*100*3))\nprint('-'*10)\n\n\n#Soft Vote or weighted probabilities\nvote_soft = ensemble.VotingClassifier(estimators = vote_est , voting = 'soft')\nvote_soft_cv = model_selection.cross_validate(vote_soft, data1[data1_x_bin], data1[Target], cv  = cv_split)\nvote_soft.fit(data1[data1_x_bin], data1[Target])\n\nprint(\"Soft Voting Training w/bin score mean: {:.2f}\". format(vote_soft_cv['train_score'].mean()*100)) \nprint(\"Soft Voting Test w/bin score mean: {:.2f}\". format(vote_soft_cv['test_score'].mean()*100))\nprint(\"Soft Voting Test w/bin score 3*std: +/- {:.2f}\". format(vote_soft_cv['test_score'].std()*100*3))\nprint('-'*10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#WARNING: Running is very computational intensive and time expensive.\n#Code is written for experimental/developmental purposes and not production ready!\n\n\n#Hyperparameter Tune with GridSearchCV: http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\ngrid_n_estimator = [10, 50, 100, 300]\ngrid_ratio = [.1, .25, .5, .75, 1.0]\ngrid_learn = [.01, .03, .05, .1, .25]\ngrid_max_depth = [2, 4, 6, 8, 10, None]\ngrid_min_samples = [5, 10, .03, .05, .10]\ngrid_criterion = ['gini', 'entropy']\ngrid_bool = [True, False]\ngrid_seed = [0]\n\n\ngrid_param = [\n            [{\n            #AdaBoostClassifier - http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html\n            'n_estimators': grid_n_estimator, #default=50\n            'learning_rate': grid_learn, #default=1\n            #'algorithm': ['SAMME', 'SAMME.R'], #default=’SAMME.R\n            'random_state': grid_seed\n            }],\n       \n    \n            [{\n            #BaggingClassifier - http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html#sklearn.ensemble.BaggingClassifier\n            'n_estimators': grid_n_estimator, #default=10\n            'max_samples': grid_ratio, #default=1.0\n            'random_state': grid_seed\n             }],\n\n    \n            [{\n            #ExtraTreesClassifier - http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html#sklearn.ensemble.ExtraTreesClassifier\n            'n_estimators': grid_n_estimator, #default=10\n            'criterion': grid_criterion, #default=”gini”\n            'max_depth': grid_max_depth, #default=None\n            'random_state': grid_seed\n             }],\n\n\n            [{\n            #GradientBoostingClassifier - http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier\n            #'loss': ['deviance', 'exponential'], #default=’deviance’\n            'learning_rate': [.05], #default=0.1 -- 12/31/17 set to reduce runtime -- The best parameter for GradientBoostingClassifier is {'learning_rate': 0.05, 'max_depth': 2, 'n_estimators': 300, 'random_state': 0} with a runtime of 264.45 seconds.\n            'n_estimators': [300], #default=100 -- 12/31/17 set to reduce runtime -- The best parameter for GradientBoostingClassifier is {'learning_rate': 0.05, 'max_depth': 2, 'n_estimators': 300, 'random_state': 0} with a runtime of 264.45 seconds.\n            #'criterion': ['friedman_mse', 'mse', 'mae'], #default=”friedman_mse”\n            'max_depth': grid_max_depth, #default=3   \n            'random_state': grid_seed\n             }],\n\n    \n            [{\n            #RandomForestClassifier - http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier\n            'n_estimators': grid_n_estimator, #default=10\n            'criterion': grid_criterion, #default=”gini”\n            'max_depth': grid_max_depth, #default=None\n            'oob_score': [True], #default=False -- 12/31/17 set to reduce runtime -- The best parameter for RandomForestClassifier is {'criterion': 'entropy', 'max_depth': 6, 'n_estimators': 100, 'oob_score': True, 'random_state': 0} with a runtime of 146.35 seconds.\n            'random_state': grid_seed\n             }],\n    \n            [{    \n            #GaussianProcessClassifier\n            'max_iter_predict': grid_n_estimator, #default: 100\n            'random_state': grid_seed\n            }],\n        \n    \n            [{\n            #LogisticRegressionCV - http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegressionCV.html#sklearn.linear_model.LogisticRegressionCV\n            'fit_intercept': grid_bool, #default: True\n            #'penalty': ['l1','l2'],\n            'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'], #default: lbfgs\n            'random_state': grid_seed\n             }],\n            \n    \n            [{\n            #BernoulliNB - http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html#sklearn.naive_bayes.BernoulliNB\n            'alpha': grid_ratio, #default: 1.0\n             }],\n    \n    \n            #GaussianNB - \n            [{}],\n    \n            [{\n            #KNeighborsClassifier - http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier\n            'n_neighbors': [1,2,3,4,5,6,7], #default: 5\n            'weights': ['uniform', 'distance'], #default = ‘uniform’\n            'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']\n            }],\n            \n    \n            [{\n            #SVC - http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC\n            #http://blog.hackerearth.com/simple-tutorial-svm-parameter-tuning-python-r\n            #'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n            'C': [1,2,3,4,5], #default=1.0\n            'gamma': grid_ratio, #edfault: auto\n            'decision_function_shape': ['ovo', 'ovr'], #default:ovr\n            'probability': [True],\n            'random_state': grid_seed\n             }],\n\n    \n            [{\n            #XGBClassifier - http://xgboost.readthedocs.io/en/latest/parameter.html\n            'learning_rate': grid_learn, #default: .3\n            'max_depth': [1,2,4,6,8,10], #default 2\n            'n_estimators': grid_n_estimator, \n            'seed': grid_seed  \n             }]   \n        ]\n\n\n\nstart_total = time.perf_counter() #https://docs.python.org/3/library/time.html#time.perf_counter\nfor clf, param in zip (vote_est, grid_param): #https://docs.python.org/3/library/functions.html#zip\n\n    #print(clf[1]) #vote_est is a list of tuples, index 0 is the name and index 1 is the algorithm\n    #print(param)\n    \n    \n    start = time.perf_counter()        \n    best_search = model_selection.GridSearchCV(estimator = clf[1], param_grid = param, cv = cv_split, scoring = 'roc_auc')\n    best_search.fit(data1[data1_x_bin], data1[Target])\n    run = time.perf_counter() - start\n\n    best_param = best_search.best_params_\n    print('The best parameter for {} is {} with a runtime of {:.2f} seconds.'.format(clf[1].__class__.__name__, best_param, run))\n    clf[1].set_params(**best_param) \n\n\nrun_total = time.perf_counter() - start_total\nprint('Total optimization time was {:.2f} minutes.'.format(run_total/60))\n\nprint('-'*10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Hard Vote or majority rules w/Tuned Hyperparameters\ngrid_hard = ensemble.VotingClassifier(estimators = vote_est , voting = 'hard')\ngrid_hard_cv = model_selection.cross_validate(grid_hard, data1[data1_x_bin], data1[Target], cv  = cv_split)\ngrid_hard.fit(data1[data1_x_bin], data1[Target])\n\nprint(\"Hard Voting w/Tuned Hyperparameters Training w/bin score mean: {:.2f}\". format(grid_hard_cv['train_score'].mean()*100)) \nprint(\"Hard Voting w/Tuned Hyperparameters Test w/bin score mean: {:.2f}\". format(grid_hard_cv['test_score'].mean()*100))\nprint(\"Hard Voting w/Tuned Hyperparameters Test w/bin score 3*std: +/- {:.2f}\". format(grid_hard_cv['test_score'].std()*100*3))\nprint('-'*10)\n\n#Soft Vote or weighted probabilities w/Tuned Hyperparameters\ngrid_soft = ensemble.VotingClassifier(estimators = vote_est , voting = 'soft')\ngrid_soft_cv = model_selection.cross_validate(grid_soft, data1[data1_x_bin], data1[Target], cv  = cv_split)\ngrid_soft.fit(data1[data1_x_bin], data1[Target])\n\nprint(\"Soft Voting w/Tuned Hyperparameters Training w/bin score mean: {:.2f}\". format(grid_soft_cv['train_score'].mean()*100)) \nprint(\"Soft Voting w/Tuned Hyperparameters Test w/bin score mean: {:.2f}\". format(grid_soft_cv['test_score'].mean()*100))\nprint(\"Soft Voting w/Tuned Hyperparameters Test w/bin score 3*std: +/- {:.2f}\". format(grid_soft_cv['test_score'].std()*100*3))\nprint('-'*10)\n\n\n#12/31/17 tuned with data1_x_bin\n#The best parameter for AdaBoostClassifier is {'learning_rate': 0.1, 'n_estimators': 300, 'random_state': 0} with a runtime of 33.39 seconds.\n#The best parameter for BaggingClassifier is {'max_samples': 0.25, 'n_estimators': 300, 'random_state': 0} with a runtime of 30.28 seconds.\n#The best parameter for ExtraTreesClassifier is {'criterion': 'entropy', 'max_depth': 6, 'n_estimators': 100, 'random_state': 0} with a runtime of 64.76 seconds.\n#The best parameter for GradientBoostingClassifier is {'learning_rate': 0.05, 'max_depth': 2, 'n_estimators': 300, 'random_state': 0} with a runtime of 34.35 seconds.\n#The best parameter for RandomForestClassifier is {'criterion': 'entropy', 'max_depth': 6, 'n_estimators': 100, 'oob_score': True, 'random_state': 0} with a runtime of 76.32 seconds.\n#The best parameter for GaussianProcessClassifier is {'max_iter_predict': 10, 'random_state': 0} with a runtime of 6.01 seconds.\n#The best parameter for LogisticRegressionCV is {'fit_intercept': True, 'random_state': 0, 'solver': 'liblinear'} with a runtime of 8.04 seconds.\n#The best parameter for BernoulliNB is {'alpha': 0.1} with a runtime of 0.19 seconds.\n#The best parameter for GaussianNB is {} with a runtime of 0.04 seconds.\n#The best parameter for KNeighborsClassifier is {'algorithm': 'brute', 'n_neighbors': 7, 'weights': 'uniform'} with a runtime of 4.84 seconds.\n#The best parameter for SVC is {'C': 2, 'decision_function_shape': 'ovo', 'gamma': 0.1, 'probability': True, 'random_state': 0} with a runtime of 29.39 seconds.\n#The best parameter for XGBClassifier is {'learning_rate': 0.01, 'max_depth': 4, 'n_estimators': 300, 'seed': 0} with a runtime of 46.23 seconds.\n#Total optimization time was 5.56 minutes.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"De esta manera consigue lo que quería, a través de estos métodos, incrementar el score llegando hasta un 85.22%\n\nComparando este trabajo con el nuestro, resalta lo laborioso y más desarrollado que está, llegando incluso a establecer análisis muy profundos para los clasificadores y las validaciones, además de las preguntas previas.\n\nOtro punto también destacable se encuentra en el preprocesamiento, ya que nosostros no llegamos a esablecer relaciones entre variables, simplemente realizamos una limpieza.\n\nOtro dato interesante es la validación cruzada de 10 repeticiones con 3 carpetas test y 6 carpetas train, mientras que nosotros siempre nos hemos limitado a unas 10 repeticiones con 1 carpeta test y 5 carpetas train, hubiera estado también interesante este tipo de combinaciones.\n\nEn general este kernel de kaggle es muy completo y superior al trabajo que nosotros hemos realizado, ya que va más allá en muchas de las cosas que nosotros realizamos, lo que la convierte en una libreta muy interesante para estudiar detenidamente."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}