{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# EE226 - Coding 2\n## Streaming algorithm & Locality Sensitive Hashing","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Streaming: DGIM","metadata":{}},{"cell_type":"markdown","source":"DGIM is an efficient algorithm in processing large streams. When it's infeasible to store the flowing binary stream, DGIM can estimate the number of 1-bits in the window. In this coding, you're given the *stream_data.txt* (binary stream), and you need to implement the DGIM algorithm to count the number of 1-bits. Write code and ask the problems below.","metadata":{}},{"cell_type":"markdown","source":"### Your task","metadata":{}},{"cell_type":"markdown","source":"1. Set the window size to 1000, and count the number of 1-bits in the current window.","metadata":{}},{"cell_type":"code","source":"import time\nfrom math import *\n      \nwindow_size = 1000     #窗口大小\ntime_location = 1000  #当前时刻\nn_max_bucket = 2      #最大相同桶数量\ncontainer = {} \nlist_1 = []          \n\nfor i in range(int(log(window_size,2))+1):\n    key = int(pow(2,i))\n    list_1.append(key)\n    container[key] = []          #建桶\n\ndef DGIM(data,container,keylist,window_size,n_max_bucket,time_location):\n    start_time = time.time()\n    bit_num = 0\n    timestamp = 0   #时间戳标志该位进入流的时间\n    \n    for i in range(time_location):\n        timestamp = (timestamp + 1) % window_size      \n                                                      \n        for key in container:\n            for eachstamp in container[key]:\n                if eachstamp == timestamp:            \n                    container[key].remove(eachstamp)   #弃桶\n                                                     \n                    \n        if data[i] == '1':\n            container[1].append(timestamp)             #合并桶\n            for key in list_1:                             \n                if len(container[key]) > n_max_bucket:   \n                    container[key].pop(0)\n                    tmpstamp = container[key].pop(0)\n                    if key != list_1[-1]:\n                        container[key*2].append(tmpstamp)\n                    else:\n                        container[key].pop(0)\n                else:\n                    break\n                    \n    firststamp = 0                                \n    for key in list_1:\n        if len(container[key]) > 0:\n            firststamp = container[key][0]    \n        for tmpstamp in container[key]:\n            print(\"Bucket's size: {}.Timestamp: {}\".format(key,tmpstamp))\n    for key in list_1:\n        for tmpstamp in container[key]:\n            if tmpstamp != firststamp:           \n                bit_num += key\n            else:\n                bit_num += 0.5*key                   \n            \n    end_time = time.time()\n    return bit_num,end_time-start_time\n    \nwith open('../input/coding2/stream_data.txt','r') as f:\n    data = f.read().split('\\t')\n    res, cost_time = DGIM(data,container,list_1,window_size,n_max_bucket,time_location)\n    print(\"1s in the last 1000 window_size of {} bits: {}\".format(time_location,res))\n    print(\"Running time with DGIM algorithm:{}\".format(cost_time))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"2. Write a function that accurately counts the number of 1-bits in the current window, and compare the difference between its running time and space and the DGIM algorithm.","metadata":{}},{"cell_type":"code","source":"# Your code here, you can add cells if necessary\ndef BF(data,window_size,time_location):   #暴力算法\n    start_time = time.time()\n    bit_num = 0\n    \n    for i in range(time_location-window_size,time_location):\n        if data[i] == '1': bit_num += 1\n    end_time = time.time()\n    return  bit_num, end_time-start_time \n        \nwith open('../input/coding2/stream_data.txt','r') as f:\n    data = f.read().split('\\t')\n    res, cost_time = BF(data,window_size,time_location)\n    print(\"1s in the last 1000 window_size of  {} bits: {}\".format(time_location,res))\n    print(\"Running time with BF algorithm:{}\".format(cost_time))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Answer**:In ***running time*** aspect: BF algorithm is faster than DGIM algorithm.      \n           In ***space*** aspcet: BF algorithm cost more memory than DGIM algorithm.\n","metadata":{}},{"cell_type":"markdown","source":"### Locality Sensitive Hashing","metadata":{}},{"cell_type":"markdown","source":"The locality sensitive hashing (LSH) algorithm is efficient in near-duplicate document detection. In this coding, you're given the *docs_for_lsh.csv*, where the documents are processed into set of k-shingles (k = 8, 9, 10). *docs_for_lsh.csv* contains 201 columns, where column 'doc_id' represents the unique id of each document, and from column '0' to column '199', each column represents a unique shingle. If a document contains a shingle ordered with **i**, then the corresponding row will have value 1 in column **'i'**, otherwise it's 0. You need to implement the LSH algorithm and ask the problems below.","metadata":{}},{"cell_type":"markdown","source":"### Your task","metadata":{}},{"cell_type":"markdown","source":"Use minhash algoirthm to create signature of each document, and find 'the most similar' documents under Jaccard similarity. \nParameters you need to determine:\n1) Length of signature (number of distinct minhash functions) *n*. Recommanded value: n > 20.\n\n2) Number of bands that divide the signature matrix *b*. Recommanded value: b > n // 10.","metadata":{}},{"cell_type":"code","source":"# Your code here, you can add cells if necessary\n\nimport numpy as np\nimport csv\nimport random\n\n#数据预处理，将csv处理成需要的list\ntime = 0\ndata = []\nwith open('../input/coding2/docs_for_lsh.csv') as f:\n    csvmap = csv.reader(f)\n    for row in csvmap:\n        time += 1\n        if time == 1:              \n            pass\n        else:\n            data.append(row[1:])    \n\ndata = np.array(data)                       \nprint(data.T)\nprint('-------------------------------------------------------')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def MinHash(data, b, r):\n    n = b*r\n    signature = []\n    \n    for i in range(n):                          \n        permutation = []                        \n        signal_signature = []                  \n        for num in range(1,data.shape[0]+1):\n            permutation.append(num)\n        \n        random.shuffle(permutation)            #初始化向量 \n    \n        for j in range(data.shape[1]):\n            for k in range(data.shape[0]):\n                index = permutation.index(k+1)           \n                if data[index][j] == '1':\n                    signal_signature.append(k+1)\n                    break\n                else:\n                    pass\n                \n        signature.append(signal_signature)\n    return np.array(signature)\n\nb = 10\nr = 5\nres_signature = MinHash(data,b,r)\nprint(res_signature)\nprint('----------------------------------------')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import hashlib\nfrom sklearn.metrics import jaccard_score\ndef LSH(signature, b, r):\n    #计算两个signature向量MinHash值相等的比例，即可以估计原向量A，B的Jaccard相似度\n    length, docnum = signature.shape\n     \n    buckets = {}                     \n    \n    start = 0       #初始位置                 \n    \n    for i in range(b):\n        for j in range(docnum):\n            md5 = hashlib.md5()       \n            signal_band = str(signature[start:start+r,j])\n            hashed_band = md5.update(signal_band.encode())    \n            hashed_band = md5.hexdigest()\n            \n            if hashed_band not in buckets:                \n                buckets[hashed_band] = [j]                \n            elif j not in buckets[hashed_band]:           \n                buckets[hashed_band].append(j)\n        start += r\n    \n    return buckets\n\nLSH_table = LSH(res_signature,b,r)\nprint('-------------------------------------------------------')\n\ndef NNS(LSH_table, num):\n    res = {}\n    for key in LSH_table:\n        if num in LSH_table[key] and len(LSH_table) != 1:   \n            for docnum in LSH_table[key]:\n                if docnum == num:\n                    pass\n                else:\n                    if docnum in res:\n                        res[docnum] += 1\n                    else:\n                        res[docnum] = 1\n    return res\n\nresult = NNS(LSH_table,0)\nresult = sorted(result.items(),key=lambda item:item[1])    \n\nnearest_neighbor_num = 30\nnearest_neighbor = []\nfor i in range(len(result)-1,len(result)-nearest_neighbor_num-1,-1):   #找到距离最近的文件\n    nearest_neighbor.append(result[i])\nprint('time: {}. '.format(nearest_neighbor))\n\ncheck_data = data.T\n\nLSH_neighbor = []\nfor i in range(len(nearest_neighbor)):\n    check_doc = nearest_neighbor[i][0]\n    score = jaccard_score(check_data[check_doc],check_data[0], pos_label= '1', average = 'binary') #计算准确度\n    print(\"Number:{}        Score(with doc 0): {}\".format(check_doc,score))\n    LSH_neighbor.append((check_doc,score))\nprint('result: {}'.format(LSH_neighbor))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Problem: For document 0 (the one with id '0'), list the **30** most similar document ids (except document 0 itself). You can valid your results with the [sklearn.metrics.jaccard_score()](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.jaccard_score.html) function.\n\nTips: You can adjust your parameters to hash the documents with similarity *s > 0.8* into the same bucket.","metadata":{}},{"cell_type":"code","source":"# Your code here, you can add cells if necessary\ntrue_res = {}\nfor i in range(1,data.shape[1]):\n    score = jaccard_score(check_data[0],check_data[i],pos_label= '1', average = 'binary')\n    true_res[i] = score\ntrue_res = sorted(true_res.items(),key=lambda item:item[1])\n\ntrue_result = []\nfor i in range(len(true_res)-1,len(true_res)-nearest_neighbor_num-1,-1):\n    true_result.append(true_res[i])\nprint('results{}'.format(true_result))\nsamecnt = 0\nsame_neighbor = []\nfor i in range(len(LSH_neighbor)):\n    if LSH_neighbor[i] in true_result:\n        samecnt += 1\n        same_neighbor.append(LSH_neighbor[i])\n\nprint('The documents with score in both LSH result and brute force result are: {}'.format(same_neighbor))\nprint('The same documents number is {}, the accuracy is {}.'.format(samecnt,float(samecnt/nearest_neighbor_num)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}