{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Welcome to my Tutorial for Artificial Neural Network\n# Artificial Neural Network"},{"metadata":{},"cell_type":"markdown","source":"# Getting more underestanding about our dataset:\n* This is a dataset for a bank which collects information about the customers\n* RowNumber: is an irrelevent feature that we are going to get rid of it in the future \n* Customerid: is the identification key of each customer\n* Surname: is the surname of each customer\n* CreditScore: is the credit score of each customer\n* Geography: means the conutry which each customer lives in\n* Gender: is the gender of each customer\n* Age: is the age of each customer\n* Tenure:means the number of the year(s) which customer in the bank\n* Balance: means the amount of money that each customer has in their account\n* NumOfProducts: the number of the product(s) which customer uses like Credict card, Master Card Etc,\n* HasCrCard: means if the customer has the credit card, 1 means Yes and 0 means No\n* isActiveMemebr: means if the customer is active or not which 1 means is a active customer\n* EstimatedSalary: means the estimated salary of each customer\n# Target\n* Exited: means if the customer exists the bank or not, 1 means the customer left the bank "},{"metadata":{},"cell_type":"markdown","source":"# First, we are going to import the libraries\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tf.__version__","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Great! we have tensorflow 2 and this works much better than tensorflow 1 "},{"metadata":{},"cell_type":"markdown","source":"# Let's import the dataset into the kernel"},{"metadata":{},"cell_type":"markdown","source":"## Part 1 - Data Preprocessing"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"dataset = pd.read_csv('../input/churn-predictions-personal/Churn_Predictions.csv')\nX = dataset.iloc[:, 3:-1].values #we select all th features except RowNumber, Customerid, Surname, Exited\ny = dataset.iloc[:, -1].values #we select the target feature Exited for the y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Encoding categorical data"},{"metadata":{},"cell_type":"markdown","source":"# Label Encoding the \"Gender\" column"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nX[:, 2] = le.fit_transform(X[:, 2])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Great, we encoded the Gender feature, we have 0 code for Female category"},{"metadata":{},"cell_type":"markdown","source":"* One Hot Encoding the \"Geography\" column"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [1])], remainder='passthrough')\nX = np.array(ct.fit_transform(X))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Great, we encoded the Geography Column"},{"metadata":{},"cell_type":"markdown","source":"### Splitting the dataset into the Training set and Test set"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature Scaling\n* Notice: This is a very important and fundamental step for implementing the Deep Learning "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Part 2 - Building the ANN"},{"metadata":{},"cell_type":"markdown","source":"### Initializing the ANN"},{"metadata":{"trusted":true},"cell_type":"code","source":"ann = tf.keras.models.Sequential()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Adding the input layer and the first hidden layer"},{"metadata":{"trusted":true},"cell_type":"code","source":"ann.add(tf.keras.layers.Dense(units=6, activation='relu'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Adding the second hidden layer"},{"metadata":{"trusted":true},"cell_type":"code","source":"ann.add(tf.keras.layers.Dense(units=6, activation='relu'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Adding the output layer"},{"metadata":{"trusted":true},"cell_type":"code","source":"ann.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Part 3 - Training the ANN"},{"metadata":{},"cell_type":"markdown","source":"### Compiling the ANN"},{"metadata":{"trusted":true},"cell_type":"code","source":"ann.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ann.fit(X_train, y_train, batch_size = 32, epochs = 100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Part 4 - Making the predictions and evaluating the model"},{"metadata":{},"cell_type":"markdown","source":"### Predicting the result of a single observation"},{"metadata":{},"cell_type":"markdown","source":"Use our ANN model to predict if the customer with the following informations will leave the bank: \n\nGeography: France\n\nCredit Score: 600\n\nGender: Male\n\nAge: 40 years old\n\nTenure: 3 years\n\nBalance: \\$ 60000\n\nNumber of Products: 2\n\nDoes this customer have a credit card? Yes\n\nIs this customer an Active Member: Yes\n\nEstimated Salary: \\$ 50000\n\nSo, should we say goodbye to that customer?"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(ann.predict(sc.transform([[1, 0, 0, 600, 1, 40, 3, 60000, 2, 1, 1, 50000]])) > 0.5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Therefore, our ANN model predicts that this customer stays in the bank!\n\n**Important note 1:** Notice that the values of the features were all input in a double pair of square brackets. That's because the \"predict\" method always expects a 2D array as the format of its inputs. And putting our values into a double pair of square brackets makes the input exactly a 2D array.\n\n**Important note 2:** Notice also that the \"France\" country was not input as a string in the last column but as \"1, 0, 0\" in the first three columns. That's because of course the predict method expects the one-hot-encoded values of the state, and as we see in the first row of the matrix of features X, \"France\" was encoded as \"1, 0, 0\". And be careful to include these values in the first three columns, because the dummy variables are always created in the first columns."},{"metadata":{},"cell_type":"markdown","source":"### Predicting the Test set results"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = ann.predict(X_test)\ny_pred = (y_pred > 0.5)\nprint(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Making the Confusion Matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\naccuracy_score(y_test, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}