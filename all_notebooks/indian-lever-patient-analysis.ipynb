{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport matplotlib.mlab as mlab\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1.Read the dataset (tab, csv, xls, txt, inbuilt dataset)"},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndf = pd.read_csv(\"../input/indian-liver-patient-records/indian_liver_patient.csv\")\ndf.head()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.Summarize important observations from the data set "},{"metadata":{},"cell_type":"markdown","source":"#### Some pointers which would help you, but don’t be limited by these\n#### a.\tFind out number of rows; no. & types of variables (continuous, categorical etc.)\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# dataframe dimensions\nprint(\"This dataframe has {} rows and {} columns\".format(*df.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"numerical_cols = df.select_dtypes(include=\"number\").columns\nprint('There are',len(numerical_cols),'numeric columns')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_cols = df.select_dtypes(include=\"object\").columns\n\nprint('There are',len(categorical_cols),'categorical columns')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.rename(columns={'Dataset':'target'},inplace=True)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target_counts=df['target'].value_counts().values\ngender_counts=df['Gender'].value_counts().values\n\nfig1, axes=plt.subplots(nrows=1, ncols=2,figsize=(10,5))\n\n\ntarget_sizes=df.groupby('target').size()\naxes[0].pie(\n    x=target_counts,\n    labels=['patient({})'.format(target_sizes[1]),'not patient({})'.format(target_sizes[2])],\n    autopct='%1.1f%%'\n)\naxes[0].set_title(\"Percentage of liver patient\")\n\ngender_sizes=df.groupby('Gender').size()\naxes[1].pie(\n    x=gender_counts, \n    labels=['male({})'.format(gender_sizes['Male']), 'female({})'.format(gender_sizes['Female'])], \n    autopct=\"%1.1f%%\"\n)\naxes[1].set_title(\"Age wise liver patient\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Gender'].value_counts().plot.bar(color='peachpuff')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"No.of male is more than female."},{"metadata":{},"cell_type":"markdown","source":"#### b.\tCalculate five-point summary for numerical variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe().T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### c.\tSummarize observations for categorical variables – no. of categories, % observations in each category"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.dtypes.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_col = ['Gender','target']\nprint('Categorical columns in the dataset are :',cat_col)\nprint('No of values in each categories   ')\nfor i in cat_col:\n   print('Column = ',i)\n   print(df[i].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Percentage of categories in each variable')\nfor i in cat_col:\n   print('Column = ',i)\n   print(df[i].value_counts()/len(df[i]))    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3.Check for defects in the data. Perform necessary actions to ‘fix’ these defects (5 Marks)"},{"metadata":{},"cell_type":"markdown","source":"#### Some pointers which would help you, but don’t be limited by these\n#### a.\tDo variables have missing/null values?\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()     ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are four missing values in Albumin_and_Globulin_Ratio.Filling missing values with mean."},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Albumin_and_Globulin_Ratio'].fillna(df['Albumin_and_Globulin_Ratio'].mean(), inplace=True)\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### b.\tDo variables have outliers? "},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = df.select_dtypes(exclude='object')\nq=1\nplt.figure(figsize=(15,20))\nfor col in cols:\n   plt.subplot(5,5,q)\n   ax = sns.boxplot(df[col],color='red')\n   plt.xlabel(col)\n   q+=1\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are outliers in our dataset but we are not removing off as of now."},{"metadata":{},"cell_type":"markdown","source":"#### c.\tIs the Target distributed evenly? Is it a defect? If Yes, what steps are being taken to rectify the problem"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.subplot(1,2,1)\nax = sns.countplot(data=df, x='target')\nplt.title('Liver patient', fontsize=15)\nfor i in ax.patches:\n    \n    ax.text(i.get_x()+0.3, i.get_height(), str(round(i.get_height(), 2)), fontsize=15, color='red')\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can observe that data is not balanced so we will balance it by smote or nearmiss."},{"metadata":{"trusted":true},"cell_type":"code","source":"print('We will be using smote when we will splitting the dataset in train and test')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.Summarize relationships among variables (10 marks)               "},{"metadata":{},"cell_type":"markdown","source":"#### a.\tPlot relevant categorical plots. Find out which are the variables most correlated or appear to be in causation with Target? Do you want to exclude some variables from the model based on this analysis? What other actions will you take?"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20, 10))\nsns.heatmap(df.corr(), cbar = True,  square = True, annot=True, fmt= '.2f',annot_kws={'size': 15},\n           cmap= 'coolwarm')\nplt.title('Correlation between features');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### The above correlation also indicates the following correlation\n##### Total_Protiens & Albumin\n##### Alamine_Aminotransferase & Aspartate_Aminotransferase\n##### Direct_Bilirubin & Total_Bilirubin\n##### There is some correlation between Albumin_and_Globulin_Ratio and Albumin. But its not as high as Total_Protiens & Albumin"},{"metadata":{"trusted":true},"cell_type":"code","source":"g = sns.FacetGrid(df, col=\"Gender\", row=\"target\", margin_titles=True)\ng.map(plt.scatter,\"Direct_Bilirubin\", \"Total_Bilirubin\", edgecolor=\"w\")\nplt.subplots_adjust(top=0.9)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There seems to be direct relationship between Total_Bilirubin and Direct_Bilirubin. We have the possibility of removing one of this feature."},{"metadata":{},"cell_type":"markdown","source":"### b.\tPlot all independent variables with the target & find out the relationship? Perform the Relevant Tests to find out if the Independent variables are associated with the Target Variable."},{"metadata":{"trusted":true},"cell_type":"code","source":"# plotting all the dependent variables with target variable\n# plotting scatter plot of continous variable with target variable\nnum_cols=['Age', 'Total_Bilirubin', 'Direct_Bilirubin',\n     'Alkaline_Phosphotase', 'Alamine_Aminotransferase', 'Aspartate_Aminotransferase', 'Total_Protiens',\n     'Albumin', 'Albumin_and_Globulin_Ratio']\nq=1\nplt.figure(figsize=(15,25))\n\nfor i in num_cols:\n   plt.subplot(4,3,q)\n   plt.title(i)\n   plt.scatter(df[i],df[\"target\"])\n   plt.xlabel(i)\n   plt.ylabel(\"target\")\n   q+=1\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see the relationship of target variables with each variables in the dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"g = sns.FacetGrid(df, col=\"Gender\", row=\"target\", margin_titles=True)\ng.map(plt.scatter,\"Albumin_and_Globulin_Ratio\", \"Total_Protiens\",  edgecolor=\"w\")\nplt.subplots_adjust(top=0.9)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.jointplot(\"Albumin_and_Globulin_Ratio\", \"Albumin\", data=df, kind=\"reg\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above jointplots and scatterplots, we find direct relationship between the following features:\nDirect_Bilirubin & Total_Bilirubin\nAspartate_Aminotransferase & Alamine_Aminotransferase\nTotal_Protiens & Albumin\nAlbumin_and_Globulin_Ratio & Albumin"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Converting gender into dummies\ndf = pd.get_dummies(df,columns = ['Gender'],drop_first = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5.Split dataset into train and test (70:30) \n#### a.\tAre both train and test representative of the overall data? How would you ascertain this statistically?\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import scipy.stats as st","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X=df.drop('target', axis=1)\nY= df['target']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train ,X_test, Y_train , Y_test = train_test_split(X , Y , test_size = 0.30 , random_state =42)\nX_train.shape , Y_train.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"H0:mu of Y_train=mu of Y_test =mu of Y\nH1:any one of them differs"},{"metadata":{"trusted":true},"cell_type":"code","source":"st.f_oneway(Y_train,Y_test,Y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"P value is greater than 0.05 so we are fai to reject null hypothesis.so mean are same of all."},{"metadata":{},"cell_type":"markdown","source":"### Fit a base model and explain the reason of selecting that model. Please write your key observations.\n#### a.\tWhat is the overall Accuracy? Please comment on whether it is good or not. \n#### b.\tWhat is Precision, Recall and F1 Score and what will be the optimization objective keeping in mind the problem statement.\n#### c.\tWhich variables are significant?\n#### d.\tWhat is Cohen’s Kappa Value and what inference do you make from the model\n#### e.\tWhich other key model output parameters do you want to look at? \n\n"},{"metadata":{},"cell_type":"markdown","source":"### Using base model Logistic Regression "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\n\nfrom sklearn.metrics import classification_report,confusion_matrix\nfrom sklearn import linear_model\nfrom sklearn.linear_model import LogisticRegression","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" ## using Logistic Regression\n# Create logistic regression object\nlogreg = LogisticRegression()\n# Train the model using the training sets and check score\nlogreg.fit(X_train, Y_train)\n#Predict Output\nlog_predicted= logreg.predict(X_test)\n\nlogreg_score = round(logreg.score(X_train, Y_train) * 100, 2)\nlogreg_score_test = round(logreg.score(X_test, Y_test) * 100, 2)\n#Equation coefficient and Intercept\nprint('Logistic Regression Training Score: \\n', logreg_score)\nprint('Logistic Regression Test Score: \\n', logreg_score_test)\nprint('Coefficient: \\n', logreg.coef_)\nprint('Intercept: \\n', logreg.intercept_)\nprint('Accuracy: \\n', accuracy_score(Y_test,log_predicted))\nprint('Confusion Matrix: \\n', confusion_matrix(Y_test,log_predicted))\nprint('Classification Report: \\n', classification_report(Y_test,log_predicted))\n\nsns.heatmap(confusion_matrix(Y_test,log_predicted),annot=True,fmt=\"d\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Overall accuracy is the probability that an individual will be correctly classified by a test; that is, the sum of the true positives plus true negatives divided by the total number of individuals tested.**"},{"metadata":{},"cell_type":"markdown","source":"**Precision - Precision is the ratio of correctly predicted positive observations to the total predicted positive observations. The question that this metric answer is of all passengers that labeled as survived, how many actually survived? High precision relates to the low false positive rate. We have got 0.788 precision which is pretty good.**\n\nPrecision = TP/TP+FP"},{"metadata":{},"cell_type":"markdown","source":"**Recall (Sensitivity) - Recall is the ratio of correctly predicted positive observations to the all observations in actual class.**\n\nRecall = TP/TP+FN"},{"metadata":{},"cell_type":"markdown","source":"**F1 score - F1 Score is the weighted average of Precision and Recall.**\nF1 Score = 2*(Recall * Precision) / (Recall + Precision)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# reading the coefficient parameters\npd.DataFrame(zip(list(X_train.columns),list(logreg.coef_[0])), columns=[\"Variable\",\"Coefficient\"])\\\n.sort_values('Coefficient').style.background_gradient(cmap='viridis', low=0.2, high=0.2)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import cohen_kappa_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Cohen Kappa Score on this model',cohen_kappa_score(Y_test,log_predicted))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Cohen's kappa coefficient is a statistic that is used to measure inter-rater reliability for qualitative items. It is generally thought to be a more robust measure than simple percent agreement calculation, as κ takes into account the possibility of the agreement occurring by chance. "},{"metadata":{"trusted":true},"cell_type":"code","source":"\nprint(f'Coefficients: {logreg.coef_}')\nprint(f'Intercept: {logreg.intercept_}')\nprint(f'R^2 score: {logreg.score(X, Y)}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 7. How do you improve the accuracy of the model? Write clearly the changes that you will make before re-fitting the model. Fit the final model. \n#### Please feel free to have any number of iterations to get to the final answer. Marks are awarded based on the quality of final model you are able to achieve. \n\n"},{"metadata":{},"cell_type":"markdown","source":"## Iteration 1"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Hyper parameter tuning for logistic regression","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder, binarize\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score, balanced_accuracy_score, precision_score, recall_score, f1_score, cohen_kappa_score, roc_auc_score, roc_curve\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# creating parameter grid\ndict_params = {\"penalty\" : [\"l1\", \"l2\"],\n               \"C\" : [0.001, 0.01, 0.1, 1, 10, 100]}\n\n# hyperparameter tuning\nbase_model = LogisticRegression()\nmodel_tuning = GridSearchCV(base_model, param_grid=dict_params, scoring=\"roc_auc\", cv=4, return_train_score=True)\nmodel_tuning.fit(X,Y)\n\n# model results\ncv_results = pd.DataFrame(model_tuning.cv_results_)\ncv_results['train_test_diff'] = cv_results['mean_train_score'] - cv_results['mean_test_score']\ncv_results.sort_values('train_test_diff')[[\"param_C\",\"param_penalty\",\"mean_train_score\",\"mean_test_score\",'train_test_diff']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# visualizing training and testing accuracy\nplt.plot(cv_results.index+1, cv_results[\"mean_test_score\"], label=\"test score\")\nplt.plot(cv_results.index+1, cv_results[\"mean_train_score\"], label=\"train score\")\nplt.title(\"Training vs. Test score\")\nplt.ylabel(\"ROC AUC Score\")\nplt.xlabel(\"Iteration\")\nplt.legend()\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fitting model with best params\nmodel_logit = LogisticRegression(penalty='l1', C=100)\nmodel_logit.fit(X_train, Y_train)\ny_pred = model_logit.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# confusion matrix\ncm = pd.DataFrame(confusion_matrix(Y_test,y_pred), columns=['Predicted:0','Predicted:1'], index=['Actual:0','Actual:1'])\n\n# plotting test results confusion matrix\nplt.figure(figsize=(5,5))\nsns.heatmap(cm, cmap=\"RdYlGn\", annot=True, cbar=False, square=True)\nplt.xlabel(\"Predicted values\", fontsize=12)\nplt.ylabel(\"Actual values\", fontsize=12)\nplt.title(\"Test results\", fontsize=20)\nplt.show()\n\ncm_reference = pd.DataFrame(np.array([\"TN\",\"FP\",\"FN\",\"TP\"]).reshape(2,2), columns=['Predicted:0','Predicted:1'], index=['Actual:0','Actual:1'])\nprint(cm_reference)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# calculating TP,TN,FP,FN\nTN, FP, FN, TP = cm.iloc[0,0], cm.iloc[0,1], cm.iloc[1,0], cm.iloc[1,1]\n\n# print values\nprint(\"True positives:\", TP)\nprint(\"True negatives:\", TN)\nprint(\"False positives (Type I error):\", FP)\nprint(\"False negatives (Type II error):\", FN)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_results = pd.DataFrame(columns=['Description','Misclassifications','Type I errors','Type II errors','Precision','Recall','Accuracy','F1-score','ROC AUC'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# itereation results\ndescription = \"Logit with hyperparamter tuning\"\nmisclassifications = FP + FN\ntype1 = FP\ntype2 = FN\nprecision = round(precision_score(Y_test,y_pred),2)\nrecall = round(recall_score(Y_test,y_pred),2)\naccuracy = round(accuracy_score(Y_test,y_pred),2)\nf1 = round(f1_score(Y_test,y_pred),2)\nauc = round(roc_auc_score(Y_test,y_pred),2)\n\ndf_results = pd.concat([df_results,\n                        pd.DataFrame(np.array([description,\n                                     misclassifications,\n                                     type1,\n                                     type2,\n                                     precision,\n                                     recall,\n                                     accuracy,\n                                     f1,\n                                     auc]).reshape(1,-1), columns=['Description','Misclassifications','Type I errors','Type II errors','Precision','Recall','Accuracy','F1-score','ROC AUC'])\n                                  ], axis=0)\n\ndf_results","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Iteration 2"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\n\n# fitting base model with default params\nmodel_knn = KNeighborsClassifier()\nmodel_knn.fit(X_train, Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# predicting X_test\ny_pred = model_knn.predict(X_test)\n\n# checking for model overfit\nprint(\"Training accuracy:\", accuracy_score(Y_train,model_knn.predict(X_train)))\nprint(\"Test accuracy:\", accuracy_score(Y_test,y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# confusion matrix\ncm = pd.DataFrame(confusion_matrix(Y_test,y_pred), columns=['Predicted:0','Predicted:1'], index=['Actual:0','Actual:1'])\n\n# plotting test results confusion matrix\nplt.figure(figsize=(5,5))\nsns.heatmap(cm, cmap=\"RdYlGn\", annot=True, cbar=False, square=True)\nplt.xlabel(\"Predicted values\", fontsize=12)\nplt.ylabel(\"Actual values\", fontsize=12)\nplt.title(\"Test results\", fontsize=20)\nplt.show()\n\ncm_reference = pd.DataFrame(np.array([\"TN\",\"FP\",\"FN\",\"TP\"]).reshape(2,2), columns=['Predicted:0','Predicted:1'], index=['Actual:0','Actual:1'])\nprint(cm_reference)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# calculating TP,TN,FP,FN\nTN, FP, FN, TP = cm.iloc[0,0], cm.iloc[0,1], cm.iloc[1,0], cm.iloc[1,1]\n\n# print values\nprint(\"True positives:\", TP)\nprint(\"True negatives:\", TN)\nprint(\"False positives (Type I error):\", FP)\nprint(\"False negatives (Type II error):\", FN)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# itereation results\ndescription = \"Base kNN model\"\nmisclassifications = FP + FN\ntype1 = FP\ntype2 = FN\nprecision = round(precision_score(Y_test,y_pred),2)\nrecall = round(recall_score(Y_test,y_pred),2)\naccuracy = round(accuracy_score(Y_test,y_pred),2)\nf1 = round(f1_score(Y_test,y_pred),2)\nauc = round(roc_auc_score(Y_test,y_pred),2)\n\ndf_results = pd.concat([df_results,\n                        pd.DataFrame(np.array([description,\n                                     misclassifications,\n                                     type1,\n                                     type2,\n                                     precision,\n                                     recall,\n                                     accuracy,\n                                     f1,\n                                     auc]).reshape(1,-1), columns=['Description','Misclassifications','Type I errors','Type II errors','Precision','Recall','Accuracy','F1-score','ROC AUC'])\n                                  ], axis=0)\n\ndf_results","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Optimizing for optimal k value.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# k values for model complexity\nneig = np.arange(1, 25)\ntrain_accuracy, test_accuracy = [], []\n\n# loop over different values of k\nfor k in neig:\n    # k from 1 to 25(exclude)\n    knn = KNeighborsClassifier(n_neighbors=k)\n    # Fit with knn\n    knn.fit(X_train,Y_train)\n    #train accuracy\n    train_accuracy.append(knn.score(X_train, Y_train))\n    # test accuracy\n    test_accuracy.append(knn.score(X_test, Y_test))\n\n# Plot\nplt.figure(figsize=[12,8])\nplt.plot(neig, test_accuracy, label = 'Testing Accuracy')\nplt.plot(neig, train_accuracy, label = 'Training Accuracy')\nplt.legend()\nplt.title('Value VS Accuracy',fontsize=20)\nplt.xlabel('Number of Neighbors',fontsize=20)\nplt.ylabel('Accuracy',fontsize=20)\nplt.xticks(neig)\nplt.grid()\nplt.show()\nprint(\"Best accuracy is {} with k = {}\".format(np.max(test_accuracy),1+test_accuracy.index(np.max(test_accuracy))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fitting base model with k=6\nmodel_knn = KNeighborsClassifier(n_neighbors=6)\nmodel_knn.fit(X_train, Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# predicting X_test\ny_pred = model_knn.predict(X_test)\n\n# checking for model overfit\nprint(\"Training accuracy:\", accuracy_score(Y_train,model_knn.predict(X_train)))\nprint(\"Test accuracy:\", accuracy_score(Y_test,y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# confusion matrix\ncm = pd.DataFrame(confusion_matrix(Y_test,y_pred), columns=['Predicted:0','Predicted:1'], index=['Actual:0','Actual:1'])\n\n# plotting test results confusion matrix\nplt.figure(figsize=(5,5))\nsns.heatmap(cm, cmap=\"RdYlGn\", annot=True, cbar=False, square=True)\nplt.xlabel(\"Predicted values\", fontsize=12)\nplt.ylabel(\"Actual values\", fontsize=12)\nplt.title(\"Test results\", fontsize=20)\nplt.show()\n\ncm_reference = pd.DataFrame(np.array([\"TN\",\"FP\",\"FN\",\"TP\"]).reshape(2,2), columns=['Predicted:0','Predicted:1'], index=['Actual:0','Actual:1'])\nprint(cm_reference)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# calculating TP,TN,FP,FN\nTN, FP, FN, TP = cm.iloc[0,0], cm.iloc[0,1], cm.iloc[1,0], cm.iloc[1,1]\n\n# print values\nprint(\"True positives:\", TP)\nprint(\"True negatives:\", TN)\nprint(\"False positives (Type I error):\", FP)\nprint(\"False negatives (Type II error):\", FN)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# itereation results\ndescription = \"kNN with optimal k (6)\"\nmisclassifications = FP + FN\ntype1 = FP\ntype2 = FN\nprecision = round(precision_score(Y_test,y_pred),2)\nrecall = round(recall_score(Y_test,y_pred),2)\naccuracy = round(accuracy_score(Y_test,y_pred),2)\nf1 = round(f1_score(Y_test,y_pred),2)\nauc = round(roc_auc_score(Y_test,y_pred),2)\n\ndf_results = pd.concat([df_results,\n                        pd.DataFrame(np.array([description,\n                                     misclassifications,\n                                     type1,\n                                     type2,\n                                     precision,\n                                     recall,\n                                     accuracy,\n                                     f1,\n                                     auc]).reshape(1,-1), columns=['Description','Misclassifications','Type I errors','Type II errors','Precision','Recall','Accuracy','F1-score','ROC AUC'])\n                                  ], axis=0)\n\ndf_results","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Random Forest","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_rfc = RandomForestClassifier()\nmodel_rfc.fit(X_train, Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# predicting X_test\ny_pred = model_rfc.predict(X_test)\n\n# checking for model overfit\nprint(\"Training accuracy:\", accuracy_score(Y_train,model_knn.predict(X_train)))\nprint(\"Test accuracy:\", accuracy_score(Y_test,y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# confusion matrix\ncm = pd.DataFrame(confusion_matrix(Y_test,y_pred), columns=['Predicted:0','Predicted:1'], index=['Actual:0','Actual:1'])\n\n# plotting test results confusion matrix\nplt.figure(figsize=(5,5))\nsns.heatmap(cm, cmap=\"RdYlGn\", annot=True, cbar=False, square=True)\nplt.xlabel(\"Predicted values\", fontsize=12)\nplt.ylabel(\"Actual values\", fontsize=12)\nplt.title(\"Test results\", fontsize=20)\nplt.show()\n\ncm_reference = pd.DataFrame(np.array([\"TN\",\"FP\",\"FN\",\"TP\"]).reshape(2,2), columns=['Predicted:0','Predicted:1'], index=['Actual:0','Actual:1'])\nprint(cm_reference)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# calculating TP,TN,FP,FN\nTN, FP, FN, TP = cm.iloc[0,0], cm.iloc[0,1], cm.iloc[1,0], cm.iloc[1,1]\n\n# print values\nprint(\"True positives:\", TP)\nprint(\"True negatives:\", TN)\nprint(\"False positives (Type I error):\", FP)\nprint(\"False negatives (Type II error):\", FN)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# itereation results\ndescription = \"Base random forest\"\nmisclassifications = FP + FN\ntype1 = FP\ntype2 = FN\nprecision = round(precision_score(Y_test,y_pred),2)\nrecall = round(recall_score(Y_test,y_pred),2)\naccuracy = round(accuracy_score(Y_test,y_pred),2)\nf1 = round(f1_score(Y_test,y_pred),2)\nauc = round(roc_auc_score(Y_test,y_pred),2)\n\ndf_results = pd.concat([df_results,\n                        pd.DataFrame(np.array([description,\n                                     misclassifications,\n                                     type1,\n                                     type2,\n                                     precision,\n                                     recall,\n                                     accuracy,\n                                     f1,\n                                     auc]).reshape(1,-1), columns=['Description','Misclassifications','Type I errors','Type II errors','Precision','Recall','Accuracy','F1-score','ROC AUC'])\n                                  ], axis=0)\n\ndf_results","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**As we can see that logistic regression and KNN with k=6 is the best model for us as it gives best accuracy.**"},{"metadata":{},"cell_type":"markdown","source":"### 8. Summarize as follows \n#### 1.\tSummarize the overall fit of the model and list down the measures to prove that it is a good model\n#### 2.\tWrite down a business interpretation/explanation of the model – which variables are affecting the target the most and explain the relationship. Feel free to use charts or graphs to explain.\n#### 3.\tWhat changes from the base model had the most effect on model performance?\n#### 4.\tWhat are the key risks to your results and interpretation?\n"},{"metadata":{},"cell_type":"markdown","source":"1."},{"metadata":{},"cell_type":"markdown","source":"As we can see that KNN with k=6 is the best model for us as it gives best accuracy.We use optimum k value 6 by hyper parameter tuning.It gives 74% accuracy. Logistic regression is also giving almost same accuracy."},{"metadata":{},"cell_type":"markdown","source":"2."},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Correlation of selector with all independent variables')\ndf.corr()['target'].plot.barh()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Albumin and Albumin_and_Globulin ratio,Total_Protiens are the variables which are afftecting target variable most"},{"metadata":{},"cell_type":"markdown","source":"3."},{"metadata":{},"cell_type":"markdown","source":"We are changing value of k by doing hyper parameter tuning. We get optimum k value 6 for the model.it increases the accuracy compared to base model."},{"metadata":{},"cell_type":"markdown","source":"4."},{"metadata":{},"cell_type":"markdown","source":"- Mitigating Risk of Machine Learning\n- Data Difficulties.\n- Technology Troubles.\n- Security Snags.\n- Models Misbehaving.\n- Interaction Issues."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":1}