{"cells":[{"metadata":{"_uuid":"2065aa54bf92f05147f4be1a578abb67b88d4a5f"},"cell_type":"markdown","source":"# **Telco Customer Churn**"},{"metadata":{"_uuid":"6310901980fc6db17a54b3dc3f164d50d713c2de"},"cell_type":"markdown","source":"### One of the biggest challenges in the telecom service industry is to retain customers. Neural networks can be a powerfull toll to create a model to forecast if, based on custumer's profile, he or she will churn or not in the future. But one can ask, what are the effects of different network architectures? Here we will build visualizatios of both accuracy and loss of various neural networks. We will clearly see at which point the networks will overfit. \n\n### We begin by loading the most relevant packages and by building a plotting function. After data reading, we discharge the column customer ID, since it will be useless to us.  "},{"metadata":{"trusted":true,"_uuid":"f67fccc23546c9fd96b28e0f774bdb5239f3d22d"},"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom random import sample\nimport numpy as np\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"12b9826346d5ac3a08cfd5858858b5c34ba1966e"},"cell_type":"code","source":"def history_plot(hist_dict):\n    import matplotlib.pyplot as plt\n    import seaborn as sns\n    import itertools\n    \n    palette = itertools.cycle(sns.color_palette())\n\n    sns.set(style=\"darkgrid\")\n    sns.set_context(\"notebook\", font_scale=1.75)\n\n    plt.figure(figsize=(8,8))\n    \n    for label, hist in hist_dict.items():\n        plt.plot(hist.history['acc'], color=next(palette), \n                 linestyle='dashed', lw=2, label='{0} - train'.format(label))\n        plt.plot(hist.history['val_acc'], color=next(palette),\n                 lw=2, label='{0} - val'.format(label))\n\n    #plt.ylim([0.0, 1.05])\n    plt.xlabel('Epochs')\n    plt.ylabel('Accuracy rate')\n    plt.title('Accuracy')\n    #plt.legend(loc=\"lower right\")\n    plt.legend(bbox_to_anchor=(1.04,1), loc=\"upper left\")\n    plt.tight_layout()\n    plt.show()\n\n\n    plt.figure(figsize=(8,8))\n    \n    for label, hist in hist_dict.items():\n        plt.plot(hist.history['loss'], color=next(palette), \n                 linestyle='dashed', lw=2, label='{0} - train'.format(label))\n        plt.plot(hist.history['val_loss'], color=next(palette),\n                 lw=2, label='{0} - val'.format(label))\n\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.title('Loss')\n    #plt.legend(loc=\"upper right\")\n    plt.legend(bbox_to_anchor=(1.04,1), loc=\"upper left\")\n    plt.tight_layout()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"057a02c9dd57f234f53731d943125594d320bfd4"},"cell_type":"code","source":"data = pd.read_csv('../input/WA_Fn-UseC_-Telco-Customer-Churn.csv')\ndata.drop('customerID', axis='columns', inplace=True)\ndata.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8daf2a108996ca34150b7ccd05c2c8403cdb8886"},"cell_type":"markdown","source":"### The column TotalCharges was read as strings, because some of the rows the data is non-numerical. We will transform it into numerical data and discharge some few non-numerical rows. We also create a list with the name of the categorical columns."},{"metadata":{"trusted":true,"_uuid":"6b52f713de68a8480282699a90b4ae6ff7e1322b"},"cell_type":"code","source":"data['TotalCharges'] = pd.to_numeric(data['TotalCharges'], errors='coerce')\nprint(data.shape)\ndata.dropna(inplace=True)\nprint(data.shape)\n\ncolumns = list(data.columns)\ncolumns.remove('MonthlyCharges')\ncolumns.remove('tenure')\ncolumns.remove('TotalCharges')\nprint(columns)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"81c726ecbdd0f0357a7fe2101093b61906839ff0"},"cell_type":"markdown","source":"### The categorical data will be encoded into numbers."},{"metadata":{"trusted":true,"_uuid":"90eb199fa4408f89bacef066e725cedab6eb5b50"},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder, StandardScaler\nX = data[columns[:-1]]\n\nle = LabelEncoder()\ny = le.fit_transform(data['Churn'])\n\nX = pd.get_dummies(X)\n\nX['MonthlyCharges'] = data['MonthlyCharges']\nX['tenure'] = data['tenure']\nX['TotalCharges'] = data['TotalCharges']\nX[['MonthlyCharges', 'tenure', 'TotalCharges']] = StandardScaler().fit_transform(data[['MonthlyCharges', 'tenure', 'TotalCharges']])\n\nX.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"48b444246563183dc1db18416291497ca7950729"},"cell_type":"code","source":"from keras import utils\nX = X.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"13417fc41ab1151c8af4964432e3cb6355de56e4"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"775ac77417485a1afa92cb9df830557f07f3ed4a"},"cell_type":"code","source":"from sklearn.utils.class_weight import compute_class_weight\nclass_weights = compute_class_weight('balanced', np.unique(y_train), y_train)\ntrain_class_weights = dict(enumerate(class_weights))\nprint (train_class_weights, \"\\n\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"796e242237dd7e85bf241a0bc81ce328a707874d"},"cell_type":"code","source":"print(X_train.shape)\nprint(X_val.shape)\nprint(X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"aebc00f68388e2fa0d43e5df06c6e3239a7c0f98"},"cell_type":"code","source":"y_train_rs = utils.to_categorical(y_train, num_classes=2)\ny_val_rs = utils.to_categorical(y_val, num_classes=2)\ny_test_rs = utils.to_categorical(y_test, num_classes=2)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6544aac06fd6a5f7a0dfade5753c1957d745d4a4"},"cell_type":"markdown","source":"## **Number of neurons**"},{"metadata":{"_uuid":"af86fff44dca5ca4600070e241a47b4932c93f01"},"cell_type":"markdown","source":"### Here we will build a NN with two hidden layers and we will vary the number of neurons in the first one."},{"metadata":{"trusted":true,"_uuid":"2ca87d4fbb2f47b86230b36796c1524189718383"},"cell_type":"code","source":"from keras.models import Sequential, Model\nfrom keras.layers import Input, Dense, Dropout\n\nhistory = {}\ni=1\nn_list = [2**i for i in range(5,12)]\n\nfor neurons in n_list:\n    print('\\rModel {0} of {1}'.format(i, len(n_list)))\n    model = Sequential([\n        Dense(neurons, activation='relu', input_shape=(45,)),\n        Dense(8, activation='relu'),\n        Dropout(0.75),\n        Dense(2, activation='softmax')\n    ])\n    model.compile(loss='categorical_crossentropy', \n                  optimizer='sgd', metrics=['accuracy'])\n    history['{0}'.format(neurons)] = model.fit(X_train, y_train_rs,\n                                              batch_size=32, epochs=200, verbose=0,\n                                              class_weight = train_class_weights, \n                                              validation_data=(X_val, y_val_rs))\n    i+=1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"034eec7a478cd873e199f8d00eba2ed51f00f180"},"cell_type":"code","source":"history_plot(history)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c8f719d2aeb36ce5aaf5a754fee017e24910b2db"},"cell_type":"markdown","source":"## **Number of layers**"},{"metadata":{"_uuid":"ad50a5a3bff0830530d88d3583a702a8d171732f"},"cell_type":"markdown","source":"### In this experiment we begin with two hidden layers. After, we will add more hidden layers, with the same number of neurons as in the first one."},{"metadata":{"trusted":true,"_uuid":"97916f2606d6c8969c446482580ab62acf500dce"},"cell_type":"code","source":"from keras.models import Sequential, Model\nfrom keras.layers import Input, Dense, Dropout\n\nhistory = {}\ni=1\nn_list = [i for i in range(6)]\n\nfor n_layers in n_list:\n    print('\\rModel {0} of {1}'.format(i, len(n_list)))\n    model = Sequential()\n    model.add(Dense(32, activation='relu', input_shape=(45,)))\n    \n    for layer in range(n_layers):\n        model.add(Dense(32, activation='relu'))\n        \n    model.add(Dense(8, activation='relu'))\n    model.add(Dropout(0.75))\n    model.add(Dense(2, activation='softmax'))\n    \n    model.compile(loss='categorical_crossentropy', \n                  optimizer='sgd', metrics=['accuracy'])\n    history['{0}'.format(n_layers+2)] = model.fit(X_train, y_train_rs,\n                                              batch_size=32, epochs=200, verbose=0,\n                                              class_weight = train_class_weights, \n                                              validation_data=(X_val, y_val_rs))\n    i+=1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"aa1d24379cc722e69a02a57b335e4adf4307f6b5"},"cell_type":"code","source":"history_plot(history)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e7c2defd07dd1ec16a39c7fd8b710df945ef7112"},"cell_type":"markdown","source":"## **Regularization**"},{"metadata":{"_uuid":"f464ba0bf4f15e0c4c6d343c8e38a6f5513ece3b"},"cell_type":"markdown","source":"### Here we vary the regularization parameter of one of the three hidden layers."},{"metadata":{"trusted":true,"_uuid":"2514c013c32505ebb8a8c02b7eaab3e4d0052ded"},"cell_type":"code","source":"from keras.models import Sequential, Model\nfrom keras.layers import Input, Dense, Dropout\nfrom keras import regularizers\n\nhistory = {}\ni=1\nr_params = [10**(-i) for i in range(0,4)]\n\nfor r in r_params:\n    print('\\rModel {0} of {1}'.format(i, len(r_params)))\n    l2 = regularizers.l2(r)\n    model = Sequential([\n        Dense(32, activation='relu', input_shape=(45,)),\n        Dense(32, activation='relu', kernel_regularizer=l2),\n        Dense(8, activation='relu'),\n        Dropout(0.75),\n        Dense(2, activation='softmax')\n    ])\n    model.compile(loss='categorical_crossentropy', \n                  optimizer='sgd', metrics=['accuracy'])\n    history['{0}'.format(r)] = model.fit(X_train, y_train_rs,\n                                              batch_size=32, epochs=200, verbose=0,\n                                              class_weight = train_class_weights, \n                                              validation_data=(X_val, y_val_rs))\n    i+=1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e8763429dc926d37102f70db998e5510c0efe0f3"},"cell_type":"code","source":"history_plot(history)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}